

OHB, UK.
While on the face of it these two approaches appear
quite different, I will argue that model search meth­
ods based upon maximizing a local log-score can be

It is often stated in papers tackling the task

expressed

of selecting a Bayesian network structure
from data that there are these two distinct
approaches:

(i)

as

equivalent search methods employing lo­

cal conditional independence tests.
The plan of the paper is as follows. The next section

Apply conditional indepen­

dence tests . when testing for the presence

introduces notation together with some theoretical re­

or otherwise of

sults. Section

edges; (ii) Search the

model

3

states the assumptions made in later

Section 4 considers learning structure from

sections.

space using a scoring metric.

a known distribution, which is equivalent to learning

Here I argue that for complete data and a

from an infinite data set. Section 5 considers the more

given node ordering this division is largely a

realistic case of inferring model structure from finite

myth, by showing that cross entropy methods

data, from both a classical and a Bayesian perspec­

for checking conditional independence are

tive.

mathematically identical to methods based
upon discriminating between models by their
overall goodness-of-fit logarithmic scores.

2
Keywords Bayesian networks;

structural learning;

conditional independence test; scoring metric; cross
entropy.

1

tion of a Bayesian network; for a recent monograph

{X 1,

In this paper I consider learning Bayesian network
structures on a finite set of discrete variables, under
the restrictions of complete data and a given node­
ordering. The following quote (Cheng et al.

1997)

is

typical of statements made in articles either introduc­
ing a novel algorithm or reviewing current algorithms
for learning Bayesian networks.
Generally, these algorithms can be grouped
one category of algo­

rithms uses heuristic searching methods to
construct a model and then evaluates it using
a scoring method. . .. The other category of
algorithms constructs Bayesian networks by
analyzing dependency relationships between
nodes.

I will assume that readers are familiar with the no­
see Cowell et al.

Introduction.

into two categories:

Notation and background results.

.

.

.

, Xn}

of

(1999).
n

X

I consider a finite set

=

(finite) discrete random variables

X= {X1, ... , Xn} =
:= {1, .. . , n} denotes some in­

taking values in the state space

xj=1Xi.

If A � V

dex set, then XA will denote the subset of variables

{X a

:

a E

A}

and will take values in

Where convenient a variable
its index

v.

X11

XA

:=

XaEAXa.

may be referred to by

Particular configurations will be denoted

using lower case letters, for example,
or XA E XA.

x = (x1, .

.

.

, Xn),

In this paper I consider search algorithms constrained
by a given node ordering; without loss of generality I
will take the node ordering to be

(X1, ... , X,).

Let

9n denote the set of directed acyclic graphs (dags) on

X,

such that

Xi

For g E 9n let

can be a parent of

P9

Xj

only if i < j.

denote the set of distributions di­

rected Markov with respect to g. This means that for

any

P9

E

P9,

the probability mass function factorizes

92

COWELL

as

3

Pg(x)=
where
v in g.

Xpa(v:g)

IT Pg(Xv I Xpa(v:g)),

Assumptions made in learning a
network.

(1)

vEV

I shall make the following assumptions for the remain­

denotes the set of parents of the vertex

ing sections.

1. I will be looking for good predictive models, se­

P(X) and Q(X) be two probability distributions
X. The Kullback-Leibler divergence between P
and Q is defined to be

Let

lected according to a log-scoring rule, and choos­

over

K(P, Q)
It takes

a

=

p(X)
log
)
q(X

[
Ep

]

ing the simplest model among equally good pre­
dictive models.

"
p(x)
L-xp(x) log q(x).

=

2. The dataset is complete, and there are no latent

xE

variables.

non-negative value measuring the similarity

or closeness of the distribution Q to that of

P,

vanish­
3. The node ordering is given, and without loss of

ing if and only if the distributions are identical.

generality is

It was shown by Cowell (1996) (see also Cowell et al.

(1999)) that for a given graph g E Yn the distribu­

tion P9 E P9 which minimizes

distribution

P(X)

K(P, P9)

UAI2001

4.

for some fixed

assigns to every vertex v E V,

(XI,

.

.

.

, Xn)·

There are no logical constraints between the vari­
ous conditional probability tables to be estimated.

Assumption 1 emphasizes that I am not looking to
Let A, Band C be disjoint index subsets of V, and let

P( X)

be some distribution over

entropy

X.

Then the cross­

of X A and XB is defined to be

construct causal models from data, but simply seeking
good predictive models. The log scoring rule is unique

in that it is (for multi-attribute variables) the only
proper scoring rule whose value is determined solely by
the probability attached to the outcome that actually

occurs (Bernardo 1979). The final part of Assumption
whilst the cross-entropy of

Xc,

XA

and

XB

conditional on

1

is Occam's razor: without it I could choose the satu­

rated model, that is, the complete graph, which would
fit the data perfectly.

or conditional cross entropy, is defined as

Scoring based search methods

usually try and balance these two aspects - by penal­
izing a model's predictive score with some measure of
the model's complexity - as a way of reducing over­
fitting.
Assumption 2 is made for simplicity, to avoid approxi­

XA is conditionally independent of Xa
given Xc under the distribution P, written as
XAlLPXB I Xc, if and only if p(XA, XB I Xc)
p(XA I Xc )p(Xa I Xc) (Dawid 1979). The notation

to account for the pattern of missing data. It also im­

llp will be abbreviated to lL when the distribution

depending upon the node and its parents in the dag),

We say

=

P under consideration is clear from the context. Note
that if XAllPXB I Xc, then Hp (XA, XB I Xc) = 0,
and vice versa.

For g E

9n

any distribution

P9

E P9 has the directed

Markov property, that is, any node is conditionally in­

dependent of its non-descendants given its parents in

g:

plies that the logarithmic score of a dag decomposes
additively into functions (one function for each node,
thus making local search possible by enabling indepen­
dent optimizations of each node's parent set.
Assumption 3 implies that the dag I obtain might not
exhibit all of the conditional independence properties
of the data, but only those consistent with the order­
ing.
Assumption

XvllX nd(v:g) I X pa(v:g)
Note

mations being made to handle missing data, or having

in

particular

that

Hpg (Xu, X nd(v:g) I X pa(v:g)) = 0.

4

states that I am assuming local meta­

independence of the conditional probabilities associ­

·

ated with the families of any given graph considered
this

implies

(Dawid and Lauritzen 1993). These conditional prob­
abilities will be taken as parameters to be estimated.

UAI2001

4

Learning networks from a known
distribution.

In this section, I assume that the joint distribution
P(X) is known; this is equivalent to recovering P(X)
from its maximum likelihood estimate (MLE) for the
saturated model in the limit of an infinite amount of
data drawn from P(X). The task is to find the sim­
plest model g E 9n such that P9(X) P(X).
=

4.1

93

COWELL

sets R; to find the largest such set for which the cross
entropy vanishes. In practice, this is not usually possi­
ble because the search space is too large. Thus heuris­
tic searches are normally applied, usually based upon
evaluating (5) with S; singleton sets. An example of
such a search is:
1. Set Xpa(i)

•

independence tests.
•

=

Hp(X;, R; I X pa(i))
XJ

=

"" (

L..t P x;,r;,Xpa.(i)
XJ

[ p(x;,( I Xpa; (i)) ]
p( I Xpa i)) (r I Xpa(i) )
) log [p(x; I r;,Xpa(i))) ]
,
r;

p

p (X ·I Xpa(>)
.

=

"" (X;,s;,Xpa(i) ) log [p(x; I s;,Xpa(i)) ]
(X,·I Xpa.(t). )
p

Xpa(i) \ {Y }.

Model selection via Kullback-Leibler

Given a graph g E Yn, the distribution directed
Markov with respect to g (that is, factorizes as (1))
which has minimum Kullback-Leibler divergence from
P(X) obeys (2) for each node in the graph. In prin­
ciple, one could perform an exhaustive search over all
possible graphs g E Yn, finding their closest matching
distributions P9(X) in terms of Kullback-Leibler di­
vergence from P(X), selecting those graphs for which
the Kullback-Leibler divergence vanishes, and select­
ing among these graphs the one having the fewest num­
ber of edges.
Consider a graph g E 9n, and it associated distribu­
tion P9(X) which satisfies (2). The Kullback-Leibler
divergence is given by

(4)

vanishes, and conversely. Hence (4) forms the ba­
sis of a conditional independence test. Note that if
X;ll.R; I Xpa(i), then for any subset S; C R; it is also
true that X;ll.S; I Xpa(i), and

L..t P

-+

.

When (3) hold s the conditional cross entropy ( 4)

Hp(X;, S; I Xpa(i))

Xpa(il

divergence.

=

X;

Select S; E R; such that Hp(X;,S; I Xpa(i))
is maximized.
RemoveS; from R; and add it to Xpa(i)·

This is similar to the 'thickening and thinning' algo­
rithm of Cheng et al. (1997). More generally, S; could
represent a restricted set of subsets of R;, not just sin­
gleton sets.
4.2

The minimal set Xpa(i) may then be taken as the set
of parents of node xi in the sought for graph. If
found for each X;, the joint distribution will factorize
as (1). Let us write R; : = {X1, ... , X;_t} \ Xpa(i)•
and Xr
{X1, ... , Xi}. Then using the identity
P(X; I R;, Xpa(i))P(R; I Xpa(i)),
P(X;, R; I Xpa(i))
we have

L..t P ( X;, r;, Xpa(i)) log
_ ""

{Xt, ... ,X;-t}.

Xpa(i) such that X;ll.Y I Xpa(i) \
{Y} is TRUE do
•

which is equivalent to the independence statement

=

3. WHILE :3 Y E

n
P(X) =II P(X; I Xt, ... 'X;- 1)·
i=l
The goal of the model search using conditional inde­
pendence tests is to find for each node X; a minimal
set Xpa(i) � {X1, ... , Xi-t}, such that

0 and R;

2. WHILE X;ll.R; I Xpa(i) is FALSE do

Model selection via conditional

Given the ordering (X11 ... ,Xn), the joint distribu­
tion P(X) may be factorized as

:=

(5)

will vanish also (and conversely). In principle, one
could perform an exhaustive search over all possible

(6)

Note that (6) decomposes into a sum of terms, one
for each node, where the g-dependence of the term
on each node depends upon the family in g of that
node. In fact for the same graph g, the ith term in
the summation (6) is identical to the cross entropy
expression (4). Thus, an exhaustive search based upon
conditional independence is equivalent to an exhaustive

search which minimizes Kullback-Leibler divergence.

94

UAI2001

COWELL

Suppose in a stepwise search algorithm that g is our

current model and a candidate model g1 differs from

F(X;, Xpa(i:g), S;)

X; for which Xpa(i:g'} :::> Xpa(i:g}· Then
the difference in Kullback-Leibler divergence of the

L

gin one node

two models is found f rom

(6)

to be

Hp(X;, X pa(i:g') \ Xp a(i:g) I Xpa(i:g)),

(5)

with

S;

:=

fi(x;,Xpa(i:g)• s; )
l

l::.(g,i)=
which is

Xi,Xpa.(i;g) ,Si

and the conditional cross entropy

Xpa(i:g') \Xpa(i:g)·

(7)

Thus choos­

P(x; I Xpa(i:g))

where

og

=

X

[p (

x ; IXpa(i:g }, s;)

p' (X; I Xpa(i:g)

]'

p(x; , Xpa(i:g))/fi(xpa(i:g))

(8)
etc.

Then a search heuristic would employ a decision rule

which on the basis of the value of

would either

(8)

accept or reject the hypothesis that

X;JlS; IXpa(i)•

ing the g' differing from g by one or more edges which

and if the latter, decide which among the candidate

minimizes

algorithm.

maximizes

(7) is equivalent to choosing g' :::> g which
K(P, P9,). After adding parents to X; until

no further decrease in Kullback-Leibler divergence is

possible (on adding yet further nodes as parents), one
could thin the parents of node

for which

l::.(g, g')

X;,

remains zero.

by removing nodes

More generally, a decision criterion in the search al­
'

gorithm which moves to a model g from a submodel g

based upon the consideration of a set of pos sible sets

S,

and their associated conditional independence tests

could be used to give the same result (or move) based

S;

to add to Xpa(i) for the next iteration of the search
Two common decision heuristics are: (i)

if the value of

(8)

is below a fixed threshold value

E

then accept the conditional independence; (ii) perform

a classical significance test, using the null hypothesis
of conditional independence, under which a suitable
multiple

(8)

(the multiplier being twice the number

X�

of observations) will have a

distri b u tion for some

suitable k. Each of these has a counterpart in search
heuristics based upon a log-score.

upon the consideration of the same S; and the changes

5.2

in Kullback-Leibler divergence, because the numeri­

Let us write n(xA) for the marginal count of the num­
ber of cases in the dataset for which
XA. For

cal quantities entering into the decision process upon
which the decision is based are identical in the two
search frameworks.
Put another way, for every search heuristic based upon
using conditional independence tests, there is an equiv­

alent search heuristic based upon using changes in
Kullback-Leibler divergence, and vice versa.

Model selection via maximum likelihood.

XA

g E

=

Q,.. the log-likelihood of the data decomposes as

logL(p9)

=

L IT
i

There

Xi,Xpa(i:;9)

n(x;, Xpa(i:g) )log(p9 (x ;IXpa(i:g) )),

is no fundamental difference between the two ap­

proaches, only a difference in interpretation.

5

which yields the MLE

,
Pg(X; I Xpa(i:g))

Learning networks from finite data

5.1

Suppose, as in Section

Xpa(i:g')

independence tests.
The directed Markov property, and the completeness

of the data, allows conditional independence tests to
be performed locally on each node. The conditional in­

dependence tests employ MLEs. However due to sam­

pling variability the tests are not sharp, so typically
the requirement of the exact vanishing of (conditional)

cross entropy expressions is relaxed.
model, with node

suppose that g is

X;

from the data one obtains the ML Es

our current

Xpa(i:g), and
F(X;, Xpa(i:g))·

having parents

Furthermore, suppose that for some node or set of
nodes

S;

E

X/-l \ Xpa(i:g}

n(x;,Xpa.(i:g))
n (Xpa(i:g) ) ·

4.2,

one evaluates _the MLEs

:::>

Xpa(i:g}·

(9)

that ·g is our current

model and g' differs from g in one node

Model selection via conditional

Thus for example,

=

X;

for which

Then the difference in the log­

likelihoods of the two models evaluated at their MLEs

is given by
log

L(pg')
L(pg)

"
L..,;

=

Xi,Xpa.(i:gl)

log

n(x;, Xpa(i:g'})

X

n(x;, Xpa(i:g'))/n(xpa(i:g'))

n(x;, Xpa(i:g) ) /n(Xpa(i:g) )

(10)
.

Thus one could decide to move from g to g' in the
model search if this quantity is positive. However, this

will generally be the case with finite data, because the

larger model will fit the data better by virtue of having

extra parameters, hence the significance of the better

COWELL

UAI2001

fit needs assessing. One simple heuristic is to set a
threshold e such that if the change is greater than e
the difference is taken to be significant - to do this
we must first normalize (10) by the total number of
cases N = .L:x n(x) in the dataset. Doing this yields
1
N

log

L(fv)
L(pg)

"""
L

Xi,XpA-(i:,g')

=

p(x; I Xpa(i:g'))

,
p(x;,
Xpa(i:g')) log (
A

p

X; IXpa(i:g) )

,

where p represents the (marginal of the) MLE of the
saturated model. This is identical to (8), with Si =

Xpa(i:g') \ Xpa(i:g)·

A more formal approach would be based on hypoth­
esis testing. Note that twice the value of (10) is the
difference in the deviances of the two models, which
under the assumption that the larger model is true,
and that the smaller model is also true, will have a
X� distribution with k equal to the difference in the
degrees of freedom of the two nested models. Thus we
perform the same test, and obtain the same result, as
the formal conditional independence test described at
the end of Section 5.1. Alternatively, one could penal­
ize the deviance by some function of the number of
parameters, for example by using the Akaike Informa­
tion Criterion (Akaike 1973) which penalizes the more
complex model by twice the number of extra parame­
ters.

More generally, because of the equality of (8) and (9)
it follows as in the last paragraph of Section 4.2, that
for every search heuristic based upon testing for con­
ditional independence, there is an equivalent search
heuristic based upon using changes in log-maximum­
likelihood, and vice versa. There is no fundamental
difference between the two approaches, only a differ­
ence in interpretation.
5.3

The Bayesian approach.

Many belief network search algorithms using a scor­
ing metric tend to employ the Bayesian formalism,
with the score being the log-marginal likelihood. The
advantages are that for smaller data sets, where the
asymptotic distribution results required for the tests in
Section 5.1 and Section 5.2 may not apply (although
exact classical tests are available, see Chapter 4 of Lau­
ritzen (1996)), the results tend to be more robust and,
furthermore, generally less sensitive to the presence of
zeroes in marginal counts.
The Bayesian approach requires a prior on the space
of graphical structures - usually this is taken to be
uniform, but there are other alternatives (Beckerman
1998). For each graphical structure a prior on the

95

probability parameters is also required - usually these
are taken to be locally independent Dirichlet priors.
Under these assumptions and complete data the mar­
ginal likelihood may be evaluated explicitly and de­
composes into a product of terms, one for each node.
An early and important paper is Cooper and Ber­
skovits (1992), who gave an explicit formula for the
marginal likelihood under these conditions.
A common feature of the analyses given in Section 5.1
and Section 5.2 is that the global scores factorize into
local contributions from each node, and, moreover,
that in comparing two similar graphs their score dif­
ference is identical to quantities which arise when test­
ing conditional independence using cross entropy mea­
sures. I shall now show that a similar circumstance
arises in a Bayesian approach when globally indepen­
dent priors are employed. The key feature is that
global independence is preserved under updating with
complete data (Cowell et al. 1999).
Thus suppose each node v of a graph g E 9n has an
associated (vector) parameterization 8� of the condi­
tional probability table of v, and a globally indepen­
dent prior distribution over the parameters 89 := {8� :
v E V}. Global independence means that the prior
measure factorizes as d7r9(89) = flv d1r9(8�). Under
these conditions the marginal likelihood of the graph
g in the light of complete data D is

L(g) :=p(Dig) = J Pg(Dig,89)d7rg(B9)

=II 1
V

II Pg(Xv IXpa(v)' e�r(xv,Xp�(v))d1l'g(8�).

Bv Xu ,Xpa(v)

(11)

From (11) we see that the marginal likelihood factor­
izes into terms, one for each node and it parents. As
before, let g' be a graph identical to graph g except
for a difference in the parent set of the Xi. Then g'
will require a different parameterization and associated
prior, (see Cowell (1996), Beckerman et al. (1995) for
alternative strategies for doing this for Dirichlet pri­
ors), but we may take for every node other than Xi
the same local parameterization and contribution to
the prior as for the graph g (that is, for Xv =1- Xi,

e�' = e�, Pg'(Xv IXpa(v:g'),e�') = Pg(Xv IX pa(v:g)•e�)
andd7r9(8Z) = d1!' ,(8z')). If, furthermore, we take uni­
9
form priors over the alternative graphical structures
(ie, P(g) = P(g')), then after suitable cancellations
we obtain the ratio of posterior probabilities given in
(12) .

96

COWELL

p(g1 I D) - p(D I g1)
p(g I D) - p(D I g)

UA12001

- fe( fL xpa(i'g') Pg' (xi I Xpa(i:g')' e( t(x;,Xpa(;,g'))d1rg• (B( )
- for flx,,xpa(''Dl Pg(Xi IXpa(i:g)' Bf)n(x;,x.,a(;,gJ)d7r9(8f)
•.

The decision of a local score driven search to stay with

6

(12)

Conclusions

graph g or move to graph g' would depend upon the

(1994) use
(12) in a Markov chain Monte Carlo

value of this ratio. Madigan and Raftery

Under the conditions of complete data and given node

(the logarithm of )

ordering I have shown that conditional independence

based graphical model search procedure, which they

tests for searching for Bayesian networks are equiva­

apply to model selection and model averaging; see also

lent to local log-scoring metrics - they are two ways

Madigan and York

(1994).

of interpreting the same numerical quantities.

I am not aware of papers applying Bayesian tests of

conditional independence to Bayesian network model

selection, hence there is not a direct comparison I can
make of

(12)

to results extant in the current literature.

(This is not to say there are none; however Bayesian
methods - based on comparison of posterior proba­
bilities - for testing for independence in contingency

It is

possible to relax the node-ordering constraint by con­
sidering arc reversals in addition to arc removals and
additions; then the change in score (which will be local
to a pair of nodes) will be a combination of the terms
which would be considered using conditional indepen­
dence tests.

However, in the latter case, one would

have the extra option of deciding if the conditional in­

tables do exist, see for example Jeffreys ( 1961 ) , Good

dependence properties associated with each of the two

(1994).)

conditional independence searching can be more re­

(1 96 5) . See also the discussion in Madigan and Raftery

However, a formal Bayesian approach would

consider the following two hypotheses:

pendence tests were combined into a single test, then

p(Xi I Xpa(i:g')' e( )p(Xpa(i:g') I ¢>��(i:g'))
d1r(ef' )d1r( ¢>��(i=g'});

p(Xi, Xpa(i:g') IBH1 )d1r(BH1)
p(Xi IXpa(i:g)> Bf)p(Xpa(i:g') I ¢��(i:g'))

H1

d1r(Bf)d1r( ¢>��(i:g')).

model exhibiting conditional independence, and ¢
is a parameterization common to the two hypothe­
Then, from the (possibly equal) priors

P(Ho)

P(H1) and the data D, posterior probabilities
P (Ho I D) and P(H1 ID) are evaluated and compared.
It is left to the reader to verify that this leads to (12).
and

Thus if one were to do model search based upon lo­

cal conditional independence tests, then one should
use

(12)

the two procedures would again be equivalent under
the same decision rules.

