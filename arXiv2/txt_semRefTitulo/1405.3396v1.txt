
We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem
is an online model of learning with ordinal feedback of the form â€œA is preferred to
Bâ€ (as opposed to cardinal feedback like â€œA has value 2.5â€), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences.
In contrast to existing algorithms for the Dueling Bandits problem, our reductions
â€“ named Doubler, MultiSBM and Sparring â€“ provide a generic schema for
translating the extensive body of known results about conventional Multi-Armed
Bandit algorithms to the Dueling Bandits setting. For Doubler and MultiSBM
we prove regret upper bounds in both finite and infinite settings, and conjecture
about the performance of Sparring which empirically outperforms the other two
as well as previous algorithms in our experiments. In addition, we provide the first
almost optimal regret bound in terms of second order terms, such as the differences
between the values of the arms.

1

Introduction

When interacting with an online system, users reveal their preferences through the
choices they make. Such a choice â€“ often termed implicit feedback â€“ may be the click
or tap on a particular link in a web-search ranking, or watching a particular movie
among a set of recommendations. Connecting to a classic body of work in econometrics and empirical work in information retrieval Joachims et al. (2007), such implicit
feedback is typically viewed as an ordinal preference between alternatives (i.e., â€œA is
better than Bâ€), but it does not provide reliable cardinal valuations (i.e., â€œA is very
good, B is mediocreâ€).

1

To formalize the problem of learning from preferences, we consider the following
interactive online learning model, which we call the Utility-Based Dueling Bandits
Problem (UBDB) similar to Yue et al. (2012); Yue & Joachims (2011). At each iteration t, the learning system presents two actions xt , yt âˆˆ X to the user, where X
is the set (either finite or infinite) of possible actions. Each of the two actions has an
associated random reward (or utility) for the user, which we denote by ut and vt , respectively. The quantity ut (resp. vt ) is drawn from a distribution that depends on xt
(resp. yt ) only. We assume these utilities are in [0, 1]. The learning system is rewarded
the average utility Utav = (ut + vt )/2 of the two actions it presents, but it does not
observe this reward. Instead, it only observes the userâ€™s binary choice among the two
alternative actions xt , yt , which depends on the respective utilities ut and vt . In particular, we model the observed choice as a {0, 1}-valued random variable bt distributed
as
Pr[bt = 0|(ut , vt )]

= Ï†(ut , vt )

P r[bt = 1|(ut , vt )]

= Ï†(vt , ut ) ,

(1.1)

where Ï† : [0, 1] Ã— [0, 1] 7â†’ [0, 1] is a link function. Clearly, the link function has to
satisfy Ï†(A, B)+Ï†(B, A) = 1. Below we concentrate on linear link functions (defined
in Sec. 2). The binary choice is interpreted as a stochastic preference response between
the left alternative xt (if bt = 0) and the right alternative yt (if bt = 1). The utility
U av captures the overall latent user experience from the pair of alternatives. A concrete
example of this UBDB game is learning for web search, where X is a set of ranking
functions among which the search engine selects two for each incoming query; the
search engine then presents an interleaving Chapelle et al. (2012) of the two rankings,
from which it can sense a stochastic preference between the two ranking functions
based on the userâ€™s clicking behavior.
The purpose of this paper is to show how UBDB can be reduced to the conventional
(cardinal) stochastic Multi-Armed Bandit (MAB) problem1 , which has been studied
since 1952 Robbins (1952). In MAB, the system chooses only a single action xt âˆˆ X in
each round and directly observes its cardinal reward ut , which is assumed to be drawn
from a latent but fixed distribution attached to xt . The set X in the traditional MAB
game is of finite cardinality K. In more general settings Dani et al. (2008); Mannor &
Shamir (2011), this set can be infinite but structured in some way. Dani et al. (2008),
for example, assume a stochastic setting in which X is a convex, bounded subset of
Rn , and the expectation Âµ(x) of the corresponding value distribution is hÂµ, xi, where
Âµ âˆˆ Rn is an unknown coefficient vector and hÂ·, Â·i is the inner product with respect to
the standard basis. We refer to this as the linear expected utility setting. We study here
both the finite setting and the infinite setting.
Main results. We provide general reductions from UBDB to MAB. More precisely,
we use a MAB strategy as a black-box for helping us play the UBDB game. The art is in
exactly how to use a black-box designed for MAB in order to play UBDB. We present
one method, Doubler (Section 3) which adds an extra O(log T ) factor to the expected
regret function compared to that of the MAB black-box, assuming the MAB black-box
1 One armed bandit is a popular slang for slot machines in casinos, and the MAB game describes the
problem faced by a gambler who can choose one machine to play at each instance.

2

has polylogarithmic (in T ) regret, where T is the time horizon. When the MAB blackbox has polynomial regret, only an extra O(1) factor is incurred. This algorithm works
for infinite bandit spaces. We also present a reduction algorithm MultiSBM (Section 4) which works for finite bandit spaces and gives an O(log T ) regret, assuming the
MAB black-box enjoys an O(log T ) expected regret function with some mild higher
moment assumptions. These assumptions are satisfied, for example, by the seminal
UCB algorithm Auer et al. (2002). Our analysis in fact shows that for sufficiently large
T , the regret of MultiSBM is asymptotically identical to that of UCB not only in
terms of the time horizon T but in terms of second order terms such as the differences
between the values of the arms; it follows that MultiSBM is asymptotically optimal in the second order terms as well as in T . Finally, we propose a third algorithm
Sparring (Section 5) which we conjecture to enjoy regret bounds comparable to those
of the MAB algorithms hiding in the black boxes it uses. We base the conjecture on arguments about a related, but different problem. In experiments (Section 7) comparing
our reductions with special-purpose UBDB algorithms, Sparring performs clearly
the best, further supporting our conjecture.
All results in this extended abstract assume the linear link function (see Section 2),
but we also show preliminary results for other interesting link functions in Appendix D.
Contributions in relation to previous work. While specific algorithms for specific cases of the Dueling Bandits problem already exist Yue et al. (2012); Yue &
Joachims (2011, 2009), our reductions provide a general approach to solving the UBDB.
In particular, this paper provides general reductions that make it possible to transfer the
large body of MAB work on exploiting structure in X to the dueling case in a constructive and algorithmic way. Second, despite the generality of the reductions their
regret is asymptotically comparable to the tournament elimination strategies in Yue
et al. (2012); Yue & Joachims (2011) for the finite case as T â†’ âˆž, and better than the
regret of the online convex optimzation algorithm of Yue & Joachims (2009) for the
infinite case (albeit in a more restricted setting).
In our setting, the reward and feedback of the agent playing the online game are, in
some sense, orthogonal to each other, or decoupled. A different type of decoupling was
also considered in Avner et al.â€™s work Avner et al. (2012), although this work cannot
be compared to theirs. There is yet more work on bandit games where the algorithm
plays two bandits (or more) in each iteration, e.g. Agarwal et al. Agarwal et al. (2010),
although there the feedback is cardinal and not relative in each step. There is much
work on learning from example pairs Herbrich et al. (2000); Freund et al. (2003); Ailon
et al. (2012) as well as noisy sorting Karp & Kleinberg (2007); Feige et al. (1994),
which are not the setting studied here. Finally, our results connect multi-armed bandits
and online optimization to the classic econometric theory of discrete choice, with its
use of preferential or choice information to recover values of goods (see Train (2009)
and references therein).
Another important topic related to our work is that of partial monitoring games.
The idea was introduced by Piccolboni & Schindelhauer (2001). The objective in partial monitoring is to choose at each round an action from some finite set of actions,
and receive a reward based on some unknown function chosen by an oblivious process. The observed information is defined as some (known) function of the chosen
action and the current choice of the oblivious process. One extreme setting in which
3

the observed information equals the reward captures MAB. In the other extreme, the
observed information equals the entire vector of rewards (for all actions), giving rise to
the so-called full information game. Our setting is a strict case of partial monitoring as
it falls in neither extremes. Most papers dealing with partial monitoring either discuss
non-stochastic settings
âˆšor present problem-independent results. In both cases the regret
is lower bounded by T , which is inapplicable to our setting (see Antos et al. (2012)
for a characterization of partial monitoring problems). BartoÌk et al. BartoÌk et al. (2012)
do present problem dependent bounds. Using their work, a logarithmic (in T ) bound
can be deduced for the dueling bandit problem, at least in the finite case. However,
the dependence on the number of arms is quadratic, whereas we present a linear one in
what follows. Our algorithms are also much simpler and directly take advantage of the
structure of the problem at hand.

2

Definitions

The set of actions (or arms) is denoted by X. In a standard stochastic MAB (multiarmed bandit) game, each bandit x âˆˆ X has an unknown associated expected utlity
Âµ(x) âˆˆ [0, 1]. At each step t the algorithm chooses some xt âˆˆ X and receives from
â€œnatureâ€ a random utility ut âˆˆ [0, 1], drawn from a distribution of expectation Âµ(xt ).
This utility isP
viewed by the algorithm.2 The regret at time T of an algorithm is defined
T
as R(T ) = t=1 (Âµ(xâˆ— ) âˆ’ ut ). where xâˆ— is such that Âµ(xâˆ— ) = maxxâˆˆX Âµ(x) (we
assume the maximum is achievable). Throughout, for x âˆˆ X we will let âˆ†x denote
Âµ(xâˆ— ) âˆ’ Âµ(x) whenever we deal with MAB. (We will shortly make reference to some
key results on MAB in Section 2.1.)
In this work we will use MAB algorithms as black boxes. To that end, we define a
Singleton Bandit Machine (SBM) as a closed computational unit with an internal timer
and memory. A SBM S supports three operations: reset, advance and feedback. The
reset operation simply clears its state.3 The advance operation returns the next bandit
to play, and feedback is used for simulating a feedback (the utility). It is assumed that
advance and feedback operations are invoked in an alternating fashion. For example, if
we want to use a SBM to help us play a traditional MAB game we first invoke reset(S),
then invoke and set x1 â† advance(S), we will play x1 against nature and observe u1
and then invoke feedback(S, u1 ). We then invoke and set x2 â† advance(S), then
weâ€™ll play x2 against nature and observe u2 , then invoke feedback(S, u2 ) and so on.
For all SBMâ€™s S that will be used in the algorithms in this work, we will only invoke
the operation feedback(S, Â·) with values in [0, 1].
In the utility based dueling bandit game (UBDB), the algorithm chooses (xt , yt ) âˆˆ
X Ã— X at each step, and a corresponding pair of random utilities (ut , vt ) âˆˆ [0, 1] are
given rise to, but not observed by the algorithm. We assume ut is drawn from a distribution of expectation Âµ(xt ) and vt independently from a distribution of expectation
Âµ(yt ). The algorithm observes a choice variable bt âˆˆ {0, 1} distributed according to
the law (1.1). This random variable should be thought of as the outcome of a duel,
2 It

is typically assumed that this distribution depends on xt only, but this assumption can be relaxed.
assume the bandit space X is universally known to all SBMâ€™s.

3 We

4

or match between xt and yt . The outcome bt = 1 (resp. bt = 0) should be interpreted as â€œyt is chosenâ€™ (resp. â€œxt is chosenâ€).4 The link function Ï†, which is assumed
to be known, quantitatively determines how to translate the utilities ut , vt to winning
probabilities. The linear link function Ï†lin is defined by
Pr[bt = 1|(ut , vt )] = Ï†lin (ut , vt ) :=

1 + vt âˆ’ u t
âˆˆ [0, 1] .
2

The unobserved reward is Utav = (ut + vt )/2, and the corresponding regret after T
PT
steps is Rav (T ) := t=1 (Âµ(xâˆ— ) âˆ’ Utav ), where xâˆ— = argmaxxâˆˆX Âµ(x). This implies
that expected zero regret is achievable by setting (xt , yt ) = (xâˆ— , xâˆ— ). In practice,
these two identical alternatives would be displayed as one, as would naturally happen
in interleaved retrieval evaluation Chapelle et al. (2012). It should be also clear that
playing (xâˆ— , xâˆ— ) is pure exploitation, because the feedback is then an unbiased coin
with zero exploratory information.
We also consider another form of (unobserved) utility, which is given as Utchoice :=
ut (1 âˆ’ bt ) + vt bt . We call this choice-based utility, since the utility that is obtained
depends on the userâ€™s choice. Accordingly, we define Rtchoice := Âµ(xâˆ— ) âˆ’ Utchoice . In
words, the player receives reward associated with either the left bandit or the right bandit, whichever was actually chosen. The utility U choice captures the userâ€™s experience
after choosing a result. In an e-commerce system, U choice may capture conversion,
namely, the monetary value of the choice. Although both utility modelings U av and
U choice are well motivated by applications, we avoid dealing with choice based utilities
and regrets for the following reason: regret bounds with respect to U av imply similar
regret bounds with respect to U choice .
Observation 2.1. Assuming a link function where u > v implies Ï†(u, v) > 1/2, for
any xt , yt , E[Rtchoice |(xt , yt )] â‰¤ E[Rtav |(xt , yt )].
(Due to lack of space, the proof can be found in Appendix E.) The observationâ€™s
assumption on the link function in words is: when presented with two items, the item
with the larger utility is more likely to be chosen. This clearly happens for any reasonable link function. We henceforth assume utility U av and regret Rav and will no longer
make references to choice-based versions thereof.

2.1

Classic Stochastic MAB: A Short Review

We review some relevant classic MAB literature. We begin with the well known UCB
policy (Algorithm 1) for MAB in the finite case. The commonly known analysis of
UCB provides expected regret bounds. For the finite X case, we need a less known,
robust guarantee bounding the probability of playing a sub-optimal arm too often.
Lemma 2.2 is implicitly proved in Auer et al. (2002). For completeness, we provide an
explicit proof in Appendix A.
4 We have just defined a two-level model in which the distribution of the random variable b is determined
t
by the outcome two other random variables ut , vt . For simplicity, the reader is encouraged to assume that
(ut , vt ) is deterministically (Âµ(xt ), Âµ(yt )). Most technical difficulties in what follows are already captured
by this simpler case.

5

Algorithm 1 UCB algorithm for MAB with |X| = K arms. Parameter Î± affects tail
of regret per action in X.
âˆ€x âˆˆ X, set ÂµÌ‚x = âˆž
âˆ€x âˆˆ X, set tx = 0
set t = 1
while true do
q
ln(t)
let x be the index maximizing ÂµÌ‚x + (Î±+2)
2tx
play x and update ÂµÌ‚x as the average of rewards so far on action x; increment tx
by 1.
tâ†t+1
end while
P
Lemma 2.2. Assume X is finite. Fix a parameter Î± > 0. Let H := xâˆˆX\{xâˆ— } 1/âˆ†x .
When running the UCB policy (Algorithm 1) with parameter Î± for T rounds the expected regret is bounded by
2(Î± + 2)H ln(T ) + K

Î±+2
= O(Î±H ln T ) .
Î±

Furthermore, lex x âˆˆ X denote some suboptimal arm and let s â‰¥ 4Î± ln(T )/âˆ†2x .
Denote by Ïx (T ) the random variable counting the number of times arm x was chosen
up to time T . Then Pr[Ïx (T ) â‰¥ s] â‰¤ Î±2 Â· (s/2)âˆ’Î± .
For the infinite case, we will review a well known setting and result which will
later be used to highlight the usefulness of Algorithm 2 (and the ensuing Theorem 3.1).
In this setting, the set X of arms is an arbitrary (infinite) convex set in Rd . Here, the
player chooses at each time point a vector x âˆˆ X and observes a stochastic reward
with an expected value of hÂµ, xi, for some unknown vector Âµ âˆˆ Rd .5 This setting was
dealt with by Dani et al. (2008). They provide an algorithm for this setting that could
be thoughtâˆšof as linear optimization under noisy feedback. Their algorithm provides
(roughly) T regret for general convex bodies and polylog(T ) regret for polytopes.
Formally, for general convex bodies, they prove the following.
Lemma 2.3 (Dani et al. 2008). Algorithm C ONFIDENCE
BALL1 (resp.
p
 C ONFIDENCE
p BALL2 ) 
of Dani et al. (2008), provides an expected regret of O
dT log3 T (resp. O
d2 T log3 T
) for any convex set of arms.
In case X is a polytope with vertex set V and there is a unique vertex v âˆ— âˆˆ V
achieving maxxâˆˆX hÂµ, xi, and any other vertex v âˆˆ V satisfies the gap condition
hÂµ, vi â‰¤ hÂµ, v âˆ— i âˆ’ âˆ† for some âˆ† > 0, we say we are in the âˆ†-gap case.
Lemma 2.4 (Dani et al. 2008). Assume the âˆ†-gap case. Algorithm C ONFIDENCE BALL1
(resp. C ONFIDENCE
 BALL2 ) of Dani et al.
 (2008), provides an expected regret of
O âˆ†âˆ’1 d2 log3 T (resp. O âˆ†âˆ’1 d3 log3 T ).
5 Affine

linear functions can also be dealt with by adding a coordinate fixed as 1.

6

Algorithm 2 (Doubler): Reduction for finite and infinite X with internal structure.
1: S â† new SBM over X
2: L â† an arbitrary singleton in X
3: i â† 1, t â† 1
4: while true do
5:
reset(S)
6:
for j = 1...2i do
7:
choose xt uniformly from L
8:
yt â† advance(S)
9:
play (xt , yt ), observe choice bt
10:
feedback(S, bt )
11:
tâ†t+1
12:
end for
13:
L â† the multi-set of arms played as yt in the last for-loop
14:
iâ†i+1
15: end while

3

UBDB Strategy for Large or Structured X

In this section we consider UBDB in the case of a large or possibly infinite set of arms
X, and the linear link function. The setting where X is large typically occurs when
some underlying structure for X exists through which it is possible to gain information
regarding one arm via queries to another. Our approach, called Doubler, is best explained by thinking of the UBDB strategy as a competition between two players, one
controlling the choice of the left arm and the other, the choice of the right one. The objective of each player is to win as many rounds possible, hence intuitively, both players
should play the arms with the largest approximated value. Since we are working with a
stochastic environment it is not clear how to analyze a game in which both players are
adaptive, and whether such a game would indeed lead to a low regret dueling match
(see also Section 5 for a related discussion). For that reason, we make sure that at all
times one player has a fixed stochastic strategy, which is updated infrequently.
We divide the time axis into exponentially growing epochs. In each epoch, the
left player plays according to some fixed (stochastic) strategy which we define shortly,
while the right one plays adaptively according to a strategy provided by a SBM. At
the beginning of a new epoch, the distribution governing the left arm changes in a way
that mimics the actions of the right arm in the previous epoch. The formal definition of
Doubler is given in Algorithm 2.
The following theorem bounds the expected regret of Algorithm 2 as a function of
the total number T of steps and the expected regret of the SBM that is used.
Theorem 3.1. Consider a UBDB game over a set X. Assume the SBM S in Line 1
of Doubler (Algorithm 2) has an expected regret of c logÎ± T after T steps, for all
Î±
logÎ±+1 T . If the expected
T . Then the expected regret of Doubler is at most 2c Î±+1
regret of the SBM is bounded by some function f (T ) = â„¦(T Î± ) (with Î± > 0), then the
expected regret of Doubler is at most O(f (T )).

7

The proof is deferred to Appendix B. By setting the SBM S used in Line 1 as
the algorithms C ONFIDENCE BALL1 or C ONFIDENCE BALL2 of Dani et al. (2008), we
obtain the following:
Corollary 3.2. Consider a UBDB game over a set X. Assume that the SBM S in Line 1
of Doubler is algorithm C ONFIDENCE BALL2 (resp. C ONFIDENCE BALL
q 1 ). If X is a

compact convex set, then the expected regret of Doubler is at most O( dT log3 (T ))
q
(resp. O( d2 T log3 (T ))). In the âˆ†-gap setting (see discussion leading to Lemma 2.4),


the expected regret is bounded by O âˆ†âˆ’1 d2 log4 (T ) (resp. O âˆ†âˆ’1 d3 log4 (T ) ).
In the finite case, one may set the SBM S to the standard UCB, and obtain:

Corollary 3.3. Consider a UBDB game over a finite set X of cardinality K. Let âˆ†i
be the difference between the reward of the best arm and the iâ€™th best arm. Assume the
SBM S in Line 1 of Doubler
PisKUCB. Then the expected regret of Doubler is at most
O(H log2 (T )) where H := i=2 âˆ†âˆ’1
i
Memory requirement issues: A possible drawback of Doubler is its need to store
the history of yt from the last epoch in memory, translating to a possible memory
requirement of â„¦(T ). This situation can be avoided in many natural cases. The first
is the case where X is embedded in a real linear space and the expectation Âµ(x) is
a linear function. Here, there is no need to store the entire history of choices of the
left arm but rather the average arm (recall that here the arms are thought of as vectors
in Rd , hence the average is well defined). Playing the average arm (as xt ) instead
of picking an arm uniformly from the list of chosen arm gives the same result with
memory requirements equivalent to storage of one arm. In other cases (e.g., X is not
even geometrically embedded) this cannot be done. Nevertheless, as long as we are in
a âˆ†-gap case, as T grows, the arm played as yt is the optimal one with increasingly
higher probability. In more detail, if the regret incurred in a time epoch is R, then the
number of times a suboptimal arm is played is at most R/âˆ†. As R is polylogarithmic
in T , the required space is polylogarithmic in T as well. We do not elaborate further
on memory requirements and leave this as future research.

4

UBDB Strategy for Unstructured X

In this section we present and analyze an alternative reduction strategy, called MultiSBM,
particularly suited for the finite X case where the elements of X typically have no
structure. MultiSBM will not incur an additional logarithmic factor as our previous
approach did. Unlike the algorithms in Yue & Joachims (2011); Yue et al. (2012), we
will avoid running an elimination tournament, but just resort to a standard MAB strategy by reduction. Denote K = |X|. The idea is to use K different SBMs in parallel,
where each instance is indexed by an element in X. In step t we choose a left arm
xt âˆˆ X in a way that will be explained shortly. The right arm, yt is chosen according
to the suggestion on the SBM indexed by xt , and the binary choice is fed back to that

8

Algorithm 3 (MultiSBM): Reduction for unstructured finite X by using K SBMs
in parallel.
1: For all x âˆˆ X: Sx â† new SBM over X, reset(Sx )
2: y0 â† arbitrary element of X
3: t â† 1
4: while true do
5:
xt â† ytâˆ’1
6:
yt â† advance(Sxt )
7:
play (xt , yt ), observe choice bt
8:
feedback(Sxt , bt )
9:
tâ†t+1
10: end while

SBM. In the next round, xt+1 is set to be yt , namely, the right arm becomes the left
one in the next step. Algorithm 3 describes MultiSBM exactly.
Naively, the regret of the algorithm can be shown to be at most K times that of a
single SBM. However, it turns out that the regret is in fact asymptotically competitive
with that of a single SBM, without the extra K factor. Specifically, we show that the total regret is in fact dominated solely by the regret of the SBM corresponding to the arm
with maximal utility. To achieve this, we assume that the SBMâ€™s implement a strategy
with a certain robustness property that implies a bound not only on the expected regret,
but also on the tail of the regret distribution. More precisely, an inverse polynomial tail
distribution is necessary. Interestingly, the assumption is satisfied by the UCB algorithm Auer et al. (2002) (as detailed in Lemma 2.2). Recall that xâˆ— âˆˆ X denotes an
arm with largest valuation Âµ(x), and that âˆ†x := Âµ(xâˆ— ) âˆ’ Âµ(x) for all x âˆˆ X. Assume
âˆ†x > 0 for all x 6= xâˆ— .6
Definition 4.1. Let Tx be the number of times a (sub-optimal) arm x âˆˆ X is played
when running the policy T rounds. A MAB policy is said to be Î±-robust when it has the
2
âˆ’Î±
following property: for all s â‰¥ 4Î±âˆ†âˆ’2
.
x ln(T ), it holds that Pr[Tx > s] < Î± (s/2)
Recall that as discussed in Section 2.1, in Auer et al.â€™s (2002) classic UCB policy
this property can be achieved by slightly enlarging the confidence region.
Theorem 4.2. The total expected regret of MultiSBM (Algorithm 3) in the UBDB
game is



X
O HÎ± ln T + HÎ± K ln K +K ln ln T âˆ’
ln
âˆ†
,
x
âˆ—
x6=x

assuming the policy of the SBMs defined in Line 1 is Î±-robust for Î± = max(3, ln(K)/ ln ln(T )).
The robustness can be ensured by choosing the UCB policy (Algorithm 1) for the SBM
with parameter Î±.
Note that achieving (Î± = 3)-robustness requires implementing a variant of UCB
with a slight modification of the confidence interval parameter in each SBM. Therefore,
6 If this is not the case, our statements still hold, yet the proof becomes slightly more technical. As there
is no real additional complication to the problem under this setting, we ignore this case.

9

if the horizon T is large enough so that ln ln T > (ln K)/3, then the total regret is
comparable to that of UCB in the standard MAB game.
The proof of the theorem is deferred to Appendix C. The main idea behind the
proof is showing that a certain â€œpositive feedback loopâ€ emerges: if the expected regret
incurred by the right arm at some time t is low, then there is a higher chance that xâˆ—
will be played as the left arm at time t + 1. Conversely, if any fixed arm (in particular,
xâˆ— ) is played very often as the left arm, then the expected regret incurred by the right
arm decreases rapidly.

5

A Heuristic Approach

In this section we describe a heuristic called Sparring for playing UBDB, which
shows extremely good performance in our experiments. Unfortunately, as of yet we
were unable to prove performance bounds that explain its empirical performance. Sparring
uses two SBMs, corresponding to left and right. In each round the pair of arms is chosen according to the strategies of the two corresponding SBMs. The SBM corresponding to the chosen arm receives a feedback of 1 while the other receives 0. The formal
algorithm is described in Algorithm 4.
The intuition for this idea comes from analysis of an adversarial version of UDBD,
in which it can be easily shown that the resulting expected regret of Sparring is at
most a constant times the regret of the two SBMs which emulate an algorithm for adversarial MAB. (We omit the exact discussion and analysis for the adversarial counterpart
of UDBD in this extended abstract.) We conjecture that the regret of Sparring is
asymptotically bounded by the combined regret of the algorithms hiding in the SBMâ€™s,
with (possibly) a small overhead. Proving this conjecture is especially interesting for
settings in which X is infinite and a MAB algorithm with polylogarithmic regret exists.
Indeed, previous literature based on tournament elimination strategies does not apply
to infinite X, and Doubler presented earlier is probably suboptimal due to the extra
log-factor it incurs.
Proving the conjecture appears to be tricky due to the fact that the left (resp. right)
SBM does not see a stochastic environment, because its feedback depends on nonstochastic choices made by the right (resp. left) SBM. Perhaps there exist bad settings
where both strategies would be mutually â€˜stuckâ€™ in some sub-optimal state. We leave
the analysis of this approach as an interesting problem for future research. Our experiments will nevertheless include Sparring.

6

Notes

Lower Bound: Our results contain upper bounds for the regret of the dueling bandit
problem. We note that a matching lower bound, up to logarithmic terms can be shown
via a simple reduction to the MAB problem. This reduction is the reverse of the others
presented here: simulate a SBM by using a UBDB solver. It is an easy exercise to
obtain such a reduction whose regret w.r.t. the MAB problem is at most twice the

10

Algorithm 4 (Sparring): Reduction to two SBMs.
1: SL , SR â† two new SBMs over X
2: reset(SL ), reset(SR ), t â† 1
3: while true do
4:
xt â† advance(SL ); yt â† advance(SR )
5:
play (xt , yt ), observe choice bt âˆˆ {0, 1}
6:
feedback(SL , 1bt =0 ); feedback(SR , 1bt =1 )
7:
tâ†t+1
8: end while
regret of the dueling bandit problem. It follows that the same lower bounds of the
classic MAB problem apply to the UBDB problem.
Adversarial Setting: One may also consider an adversarial setting for the UBDB
problem. Here, utilities of the arms that are assumed to be constant in the stochastic
case are assumed to change each round in some arbitrary way. We do not elaborate
âˆš
on this setting due to space constraints but mention that (a) a lower bound of KT
matching that of the MAB problem is valid in the UDBD setting, and (b) the Sparring
algorithm, when using SBM solvers for the adversarial setting, can be shown to obtain
the same regret bounds of said SBM solvers.

7

Experiments

We now present several experiments comparing our algorithms with baselines consisting of the state-of-the-art I NTERLEAVED F ILTER (IF) Yue et al. (2012) and B EAT T HE
M EAN BANDIT (BTMB) Yue & Joachims (2011). Our experiments are exhaustive, as
we include scenarios for which no bounds were derived (e.g. nonlinear link functions),
as well as the much more general scenario in which BTMB was analyzed Yue &
Joachims (2011).
Henceforth, the set X of arms is {A, B, C, D, E, F }. For applications such as the
interleaving search engines Chapelle et al. (2012), 6 arms is realistic. We considered 5
choices of the expected value function Âµ(Â·) and 3 link functions78 .
linear
natural
logit
Name
1good
2good
3good
arith
geom

Âµ(A)
0.8
0.8
0.8
0.8
0.8

Ï†(x, y) = (1 + x âˆ’ y)/2
Ï†(x, y) = x/(x + y)
Ï†(x, y) = (1 + exp{y âˆ’ x})âˆ’1
Âµ(B)
0.2
0.7
0.7
0.7
0.7

Âµ(C)
0.2
0.2
0.7
0.575
0.512

Âµ(D)
0.2
0.2
0.2
0.45
0.374

Âµ(E)
0.2
0.2
0.2
0.325
0.274

Âµ(F )
0.2
0.2
0.2
0.2
0.2

7 To be precise, the actual expected utility vector Âµ was a random permutation of the one given in the
table. This was done to prevent initialization bias arising from the specific implementation of the algorithms.
8 Note that in row â€™arithâ€™, Âµ(2)..Âµ(6) form an arithmetic progression, and in row â€™geomâ€™ they form a
geometric progression.

11

For each 15 combinations of arm values and link function we ran all 5 algorithms
IF, BTMB, MultiSBM, Doubler, and Sparring with random inputs spanning
a time horizon of up to 32000.
We also set out to test our algorithms in a scenario defined in Yue & Joachims
(2011). We refer to this setting as YJ. Unlike our setting, where choice probabilities are
derived from (random) latent utilities together with a link function, in YJ an underlying
unknown fixed matrix (Pxy ) is assumed, where Pxy is the probability of arm x chosen
given the pair (x, y). The matrix satisfies very mild constraints. Following Yue &
Joachims (2011), define xy = (Pxy âˆ’ Pyx )/2. The main constraint is, for some
unknown total order  over X, the imposition x  y â‡â‡’ (x, y) > 0. The optimal
arm xâˆ— is maximal in the total order. The regret incurred by playing the pair (xt , yt ) at
time t is 12 (xâˆ— xt + xâˆ— yt ).
The BTMB algorithm Yue & Joachims (2011) proposed for YJ is, roughly speaking, a tournament elimination scheme, in which a working set of candidate arms is
maintained. Arms are removed from the set whenever there is high certainty about
their suboptimality. Although the YJ setting is more general than ours, our algorithms
can be executed without any modification, giving rise to an interesting comparison with
BTMB. For this comparison, we shall use the same matrix (xy )x,yâˆˆX as in Yue &
Joachims (2011), which was empirically estimated from an operational search engine.
A
B
C
D
E
F

A
0
âˆ’0.05
âˆ’0.05
âˆ’0.04
âˆ’0.11
âˆ’0.11

B
0.05
0
âˆ’0.05
âˆ’0.04
âˆ’0.08
âˆ’0.10

C
0.05
0.05
0
âˆ’0.04
âˆ’0.01
âˆ’0.06

D
0.04
0.06
0.04
0
âˆ’0.04
0

E
0.11
0.08
0.01
0.04
0
âˆ’0.01

F
0.11
0.10
0.06
0
0.01
0

(Note that xâˆ— = A  B  C  D  E  F .)
Experiment Results and Analysis Figure 1 contains the expected regrets of these
described scenarios as a function of the log (to the base 2) of the time, averaged over
400 executions, with one standard deviation confidence bars. The experiments reveal
some interesting results. First, the heuristic approach is superior to all others in all of
the settings. Second, among the other algorithms, the top two are the algorithms IF
and MultiSBM, where MultiSBM is superior in a wide variety of scenarios.

8

Future work

We dealt with choice in sets of size 2. What happens in cases where the player chooses
from larger sets? We also analyzed only the linear choice function. See Appendix D
for an extension of the results in Section 4 to other link functions.
Both algorithms Doubler and MultiSBM treated the left and right sides asymmetrically. This did not allow us to consider distinct expected valuation functions for
the left and right positions. 9 Algorithm Sparring is symmetric, further motivating
the question of proving its performance guarantees.
9 Such a case is actually motivated in a setting where, say, the perceived user valuation of items appearing
lower in the list are lower, giving rise to bias toward items appearing at the top.

12

600
400
200
0
10

11

12
13
14
linear/1good

15

800

800

2000

600

1500

400

1000

200

500

0
10

11

12
13
14
natural/1good

15

12
13
logit/1good

14

15

11

12
13
logit/2good

14

15

11

12
13
logit/3good

14

15

11

12
13
logit/arith

14

15

11

12
13
logit/geom

14

15

1000

400

500
500

200
11

12
13
14
linear/2good

15

0
10

11

12
13
14
natural/2good

15

800

0
10

1500
1000

600

1000

400

500

500

200
0
10

11

1500

1000

600

0
10

0
10

11

12
13
14
linear/3good

15

800

0
10

11

12
13
14
natural/3good

15

1000

0
10

1500

600
400

1000

500

500

200
0
10

11

12
13
linear/arith

14

15

0
10

11

12
13
14
natural/arith

15

0
10

800
600

1000

400
500

200
0
10

1500

1000

11

12
13
linear/geom

14

15

0
10

500
11

12
13
14
natural/geom

15

Doubler
IF
DoubleSBM
BTMB
Sparring
MultiSBM

400
200
0
10

0
10

11

12

13

14

15

YJ

Figure 1: Expected regret plots, averaged over 400 runs for each of the 16 scenarios, and 5
algorithms. The x-axis is the log to the base 2 of the time, and the y-axis is the regret, averaged
over 400 executions (with 1 standard deviation confidence bars).

Proving (or refuting) the conjecture in Section 5 regarding the regret of Sparring
is an interesting open problem. Much like our proof idea for the guarantee of MultiSBM,
there is clearly a positive feedback loop between the two SBMâ€™s in Sparring: the

13

more often the left (resp. right) arm is played optimally, the right (resp. left) arm
would experience an environment which is closer to that of the standard MAB, and
would hence incur expected regret approximately that of the SBM it implements.

Acknowledgments
The authors thank anonymous reviewers for thorough and insightful reviews. This
research was funded in part by NSF Awards IIS-1217686 and IIS-1247696, a Marie
Curie Reintegration Grant PIRG07-GA-2010-268403, an Israel Science Foundation
grant 1271/33 and a Jacobs Technion-Cornell Innovation Institute grant.

