 : This paper advocates the usefulness of new theories of uncertainty for the
purpose of modeling some facets of uncertain knowledge, especially vagueness, in A.l.. It can
be viewed as a partial reply to Cheeseman's (among others) defense of probability.
0.

In troduction

1.

Representing . uncertainty
Let fP be a Boolean algebra of propositions denoted by a, b,c .. . We assume that an

In spite of the growing bulk of works dealing with deviant models of uncertainty in
artificial intelligence (e.g. [141), there is a strong reaction of classical probability tenants
((1]-[2] and (16] for instance), claiming that new uncertainty theories are "at best
unnecessary, and at worst misleading" [1]. Interestingly enough, however, the trend to go
beyond probabilistic models of subjective uncertainty is emerging even in the orthodox field
of decision theory in order to account for systematic deviations of human behavior from the
expected utility models. This paper tries to reconcile the points of view of probability
theory and those of two presently popular alternative settings : possibility theory [22], [51
and the theory of evidence [19]. The focus is precisely on the representation of subjective
uncertain knowledge. We try to explain why probability measures cannot account for all
facets of uncertainty, especially partial ignorance, imprecision, vagueness, and how the
other theories can do the job, without rejecting the laws of probability when they apply.

•.

uncertain piece of information can be represented by means of a logical proposition to which
a number, conveying the level of uncertainty, is attached. The meaning of this number is
partly a matter of context. Following Cheeseman [1], g(a ) is here supposed to reflect an
"entity's opinion about the truth of a, given the available evidence". Let g(a) denote the
number attached to a. g is supposed to range over (0, 1J, and satisfies limit conditions g(O) =
0 and g(1) = 1, where 0 and 1 denote the contradiction and the tautology respectively.

1.1 Some limitatjons of a probabilistic model of subjectjve uncertainty
If g is assumed to be a probability measure, then g(a) = 1 means that a is certainly true,
while g(a ) = 0 means that a is certainly false. This convention does not allow for the
modeling of partial ignorance in a systematic way. Especially it may create some problems
in the presence of imprecise evidence, when part of it neither supports nor denies a. First,
to allocate an amount of probability P(a) to a proposition a compels you to allocate
1-P(a) to �he converse proposition 'not a' (-,a). It implies that P(a)=1 means "a is true" and
P(a)=O means "a _ is false". ln case of total ignorance about a, you are bound to let P{a) =
1-P(-.a)=.S. If you are equally ignorant about the truth of another proposition b then you must
do it again : P(b )=1 - P(-.b) = .5. Now assuming that a logically entails b (-.a v b is the
tautology 1), you come up with the result that P(-.a ..... b)= P (b) - P(a ) = 0 ! This is a paradox :
how can your ignorance allow you to discard the possibility that ...,a ..... b be true ! For instance
a::"horse no 4 will win the race" b:"horse with an even number wins the race", and the
probabilistic modelling of ignorance leads you to the surprising statement "no horse with an
even number other than 4 will win the race". What is pointed out is that probability theory
81

.I
I
offers no stable, consistent reference value for the modelling of ignorance. This issue is
important since even experts are often in situation of partial ignorance (e.g. on the precise
value of a probability !) . The way a question is answered depends upon the existence of
further questions which reveal more about the structure of the set of possible answers. This
is not psychologically satisfying.
Of course probability theory tenants would argue that this is not the proper way of
applying probability theory. First you should make sure about how many horses are in the
race and then your ignorance leads you to uniformly distributing your probabilities among the
horses following the principle of maximum entropy (13]. This is good for games of chance,
when it is known that dice have only 6 facets. Sut in the expert system area, they usually
play with objects without exactly knowing their number of facets. An important part of
probabilistic modelling consists of making up a set of exhaustive, mutually exclusive
alternatives before assessing probabilities. For instance, in the horse example, this set must
account for the constraint "a implies b"; it leaves only 3 alternatives: a, ...,a"' b, ...,b to which
probabilities must be assigned. The difficulty in A.l. is that this job is generally impossible
to do, at least to do once for all. Knowledge bases must allow for evolving representations of
the universe, that is the discovery of new unexpected events, without having to reconstruct
a new knowledge base out of nothing, upon such occurrences. At a given point in the
construction of a knowledge base, the expert may not be aware of all existing alternatives.
Of course you can always use a special dummy alternative such as "all other possibilities".
But the uniformly distributed probability assignment derived from Laplace's insufficient
reason principle is not very convincing in that case ! Moreover the expert may not always be
able to precisely describe the alternatives he is aware of, so that these descriptions may
overlap each other and the mutual exclusiveness property needed in probability theory, is
lost. Despite this not very nice setting, one must still be able to perform uncertain reasoning
in knowledge bases ! Quoting Cheeseman [1], "if the problem is undefined probability theory
cannot say something useful". But artificial intelligence tries to address partially undefined
problems which human experts can cope with. This presence of imprecision forces us out of
the strict probabilistic setting - this do�s not mean rejecting it, but enlarging it.
Even· when an exhaustive set of mutually exclusive alternatives is available it is
questionable to represent the state of total or partial ignorance by means of an uniformly
distributed probability measure. The most recent justification for this latter approach
seems to be the principle of maximum entropy (e.g. (13]). However, in the mind of some
entropy principle tenants, there seems to be a confusion between two problems : one of
representing a state of knowledge and one of making a decision. The kind of information
measured by Shannon's entropy is the amount of uncertainty (pervading a probability
assignment) regarding which outcome is likely to occur next, or what is the best decision to
be made on such ground. The word "information" refers here to the existence of reasons to
choose one alternative aqajnst another. Especially you are equally uncertain about what will
happen when flipping a coin whether you totally ignore the mechanical properties of this
particular coin, or you have made 1 ,000,000 experiments with the coin, and it has led to an
equal amount of heads and tails. You are in the same state of uncertainty but certainly not in
the same state of knowledge : in the case of total ignorance, you have !1Q. evidence about the
coin ; in the second situation, you are not far from having the maximal amount of evidence,
although being uncertain due to randomness. In artificial intelligence problems we want to
have a model of our state of knowledge in which ignorance is carefully distinguished from
randomness. This is important because when uncertainty is due to ignorance, there is some
- hope of improving the situation by getting more evidence (flip the coin, for instance). When
uncertainty is due to experimentally observed randomness, it seems difficult to remove it by
getting more information. Distinguishing between ignorance and randomness may be useful
for the purpose of devising knowledge based systems equipped with self-explanation
82

I
I
I
I
I

I

I
I
I
I
I
I
I
I
I

I
I.
I
I
I
J

I
I
I
I
I
,,
I

I
I
I
I
I
I

capabilities. The maximal entropy principle was not invented to take care of this distinction,
and fooks irrelevant as far as the representation of knowledge is concerned. It may prove
more useful tor decision support-systems than tor approximate reasoning systems.
1.2 Beljef functjons
In the situation of partial ignorance the probability of a is only imprecisely known, and
can be expressed as an interval range [C(a), Pl(a)] whose lower bound can be viewed as a
degree of certainty (or belief) of a, while the upper bound represents a grade of plausibility
(or possibility) of a, -i.e. the extent to which a cannot be denied. Total ignorance about a is
observed when there is a total lack of certainty (C(a) = 0) and complete possibility (PI(a)=1)
for a. A natural assumption is to admit that the evidence which supports a also denies 'not a'
(-.a). This modeling assumption leads to the convention
C(a) = 1 - Pl(-.a)
(1)
which is in agreement with g(a) = 1 - g(-.a), where g(a) and g(-.a ) are imprecisely known
probabilities. This equality also means that the certainty of a is equivalent to the
impossibility of -.a. The framework of probability theory does not allow for modelling the
difference between possibility and certainty, as expressed by (1) . Functions C and PI are
usually called lower arid upper probabilities, when considered as bounds on an unknown
probability measure. See [20] and [8] for surveys on lower and upper probabilities.
Using (1), the knowledge of the certainty function C over the Boolean algebra of
propositions f? is enough to reconstruct the plausibility function Pl. Especially the amount of
uncertainty pervading a is summarized by the two numbers C(a) and C(--.,a). They are such that
C(a) + C(-.a ) � 1, due to (1 ) . The above discussion leads to the following conventions, for
interpreting the number C(a) attached to a :
i ) C(a)=1 means that a is certainly true: ii) C(-.a)=1 ¢:::> Pl(a)=O means that a is certainly false.
i i i ) C(a)=C(-.a) "' 0 (i.e. Pl(a)=1) means total ignorance about a. In other words a is neither
s upported nor denied by ·any piece of available evidence. This is a self-consistent,
absolute reference point for expressing ignorance.
i v ) C(a)=C(-.a)=0.5 (i.e. Pl(a)=0.5) means maximal . probabilistic uncertainty about a. In other
words the available evidence can be shared in two equal parts : one which supports a and
the other which ·denies it. This is the case of pure randomness in the occurence of a.
Note that total ignorance implies that we are equally uncertain about the truth of a and -.a,
as well as when C(a)=C(-.a)=.S. In other words ignorance implies uncertainty about the truth
of a, but the converse is not true. Namely, in the probabilistic case, we have a lot of
information, but we are still completely uncertain. Total uncertainty is more generally
observed whenever C(a)=C(-.a)e(O,O.SJ; then the amount of ignorance is assessed by 1-2 C(a).
The mathematical properties of C depend upon the way the available evidence is
modelled and related to the certainty function. In Shafer theory , a body of evidence (� ,m) is
composed of a subset � � f? of n focal propositions, each being attached a relative weight of
confidence m(ai). For all ai E �, m(ai) is a positive number in the unit interval, and it holds
m(O) = 0
(3)
(2)
I i = 1,n m(a i) = 1
(3) expresses the fact that no confidence is committed to the contradictory proposition. The
weight m(1 ) possibly granted to the tautology represents the amount of total ignorance
since the tautology does not support nor deny any other proposition. The fact that a
proposition a supports another
proposition b is formally expressed by the logical
entailment, i.e. a .... b ( = ...,a 'V b ) = 1. Let S(a) be the set of propositions supporting a other
than the contradiction 0. The function C(a) is called a belief function in the sense of Shafer
(and denoted 'Bel' ) if and only if there is a body of evidence (�,m) such that
83

I

_,
S(a) m(a1) (4) ; 1:1 a, Pl(a) = La1 e S(...,a)c· {O} m(a1) (5)
where 'c' denotes complementation. Clearly, when the focal elements are only atoms of the
Boolean algebra fP (i.e. S(a1)={a1}, for all i=1,n) then l:la, S(-.a)=S(a)c·{O}, and Pl(a)=Bel(a),l::la.
1:1 a , Bel(a) = La1

e

We recover a probability measure on fP. In the general case the quantity Pl(a ) -Bel(a )
represents the amount of imprecision about the probability of a. Interpreting the Boolean---­
algebra fP as the set of subsets of a referential set n, the atoms of fP can be viewed as the
singletons of n and interpreted as possible worlds, one of which is the actual world. Then a
focal element a1 whose model is the subset M(a i) = A1 � n corresponds to the statement : the
actual world is in A1 with probability m(A1). When Ai is not a singleton, this piece of
information is said to be imprecise, because the actual world can be anywhere within Ai.
When A1 is a singleton, the piece of information is said to be precise. Clearly, Bel = PI is a
probability measure if and only if the available evidence is precise (but generally scattered
between several disjoint focal elements viewed as singletons).
Note that although Bel(a) and Pl(a) are respectively lower and upper probabilities, the
converse is not true, that is any interval-valued probability cannot be interpreted as a pair
of belief and plausibility functions in the sense of (4) and (5) (see [20], [8]).
1.3. Pgssibi!ity measyres
When two propositions a and b are such that a e S(b), we write a 1- b, and 1- is called the
entailment relation. Note that 1- is reflexive and transitive and equips the set fF of focal
elements with a partial ordering structure. When fF is linearly ordered by 1-, i.e fF ={a1 .... ,an}
.•

where a1

1-

a1+ 1• i = 1,n-1, the belief and plausibility functions Bel and PI satisfy [21]

'v'a, 'v'b, Bel(a A b) = m!n (Bel(a). Bel(b))
(6)
Pl(a .., b) = max (PI(a), Pl(b))
Formally, the plausibility function is a possibility measure in the sense of Zadeh [22]. Then
max (PI(a), Pl(-,a)) = 1 (7) ; min (Bel(a), Bel(-,a)) = 0 (8) ; Bel(a) > 0 � Pl(a) = 1 (9)
In the following possibility measures are denoted IT for the sake of clarity. The dual measure
through (1) is then denoted N and calle·d a necessity measure [5]. Zadeh [22] introduces
possibility. measures from so-called possibility distributions, which are mappings from n to
[0, 1], denoted 1t. A possibility and the dual necessity measure are then obtained as
1::1 Ac:.O, IT( A) = sup {1t(w) 1 w e A} (10) ; N(A) = inf {1 - 1t(w) 1 w e Ac} (11)
and we then have 1t(w) = IT({w}), 1::1 w. The function 1t can be viewed as a generalized
characteristic function, i.e. the membership function of a fuzzy set F. Let Fo: be the o:-cut of
F. i.e., the subset {w I llF(w) � o:} with 1t = llF· It is easy to check that in the finite case the
set of o:-cuts { Fo: 1 o: e (0,1]} is the set fF of focal elements of the possibility measure IT.
Moreover, let 1t1 = 1 > � ... > 1tm be the set of distinct values of 1t(w), let 1tm + 1 = 0 by
convention. and Ai be the 1ti - cut of F, i = 1,m. Then the basic probability assignment m
underlying IT is completely defined in terms of the possibility distribution 1t as [4] :
(12)
m(A1) = 1t1 - 1ti+1 i = 1,m ; m(A) = 0 otherwise

tl.Ji.. : Interpreting N(a) as a belief degree as in MYCIN and N(-,a) as a degree of disbelief in a,
(6), (7) and (8) are assumed 'in MYCIN to be valid. Hence, as pointed out in [5]. MYCIN's
treatment of uncertainty is partly consistent with possibility theory.
Among the unjustified criticisms of possibility theory is the statement by Cheeseman
[2) that it "contains rules such as IT(a A b) = min(IT(a). IT(b ))". This is wrong. Only an
inequality, IT(a ,., b) s min(IT(a), IT(b)) is valid, generally, just as in probability theory. The
above equality holds in very special cases [5], namely when a and b pertain to variables
which do not interact with each other. Possibility logic relies on (6) but certainly not on the
equality mentioned by Cheeseman [2). Possibility logic, just as probabilistic logic is not
truth-functional. It can even be proved t� in the presence of uncertainty, the two
84

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

I
I
I
I
I
I
I

I
I
I
I
I
I
I
I
I
I
I
I

equalities Il(a ..., b) • max ( Il(a), Il(b)) and Il(a .... b) = min(Il(a), Il(b)) Cia C:lb, are inconsistent.
So it is wrong to claim, as Cheeseman [2] does, that possibility theory "assumes a state of
maximal dependence between components".
The conventions adopted in this paper to quantify uncertainty can be visually illustrated
by a rectangle triangle in a Cartesian coordinate system that is reminiscent of Rollinger
[18]'s conventions (see Figure 1). Any point in the Viangle has coordinates (8el(a), 8el(-, a)) .
The x-axis (resp. y-axis) quantifies support for a (resp.
B
...,a). Hence any point in the triangle A08 expresses a
state of knowledge, viewed as a convex combination of
total certainty for a (vertex A), for ...,a (vertex 8), and
total ignorance (vertex 0). Probabilistic knowledge lies
.5
on the A-8 segment, and possibilistic knowledge lies on
Bel(-.a)
the coordinate axes. Totally uncertain states are
located on the segment bounded by 0 and R, R being the
mid-point of the probabilistic segment.
ignorance oc.c'-=-1=-::o--=-=--• A
,

,

,

Bel(a)

2

.

A

short

.5

discussion

of

Cox's axiomatic

framework

for

probability

Traditionally, a degree of probability can be interpreted as
- either the ratio between the number of outcomes which realize an event over the number of
possible outcomes. This is good for games of chance.
- or the frequency of occurrence of an event, after a sufficient (theoretically infinite)
number of trials. This is the frequentist view.
- or a numerical translation of an entity's opinion about the truth of a proposition, given the
which has been
available evidence as termed in [2]. This is the subjectivist view,
expressed in various settings : axiomatic [3], pragmatic (this is the Bayesian approach to
betting behavior, and qualitative comparative probability (see [11]) ; we shall focus only on
Cox's axiomatic view and ask whether such a view exist for belief and possibility
functions ; see [10] for details. See [9] for other views of these functions.
Cox [3] tried to prove that probability measures were the only possible numerical model
of "reasonable expectation". He started from the following requirements. Letting f (b 1 a) be a
measure of the reasonable credibility of the proposition b when the proposition a is known
to be true. Cox proposes two basic axioms :
C1. there is some operation
such that f(c .... b 1 a) = f(c 1 b .... a) f(b 1 a)
•

•

C2. there is a function S such that f(...,b 1 a) = S ( f(b 1 a))
The following additional technical requirements are used in [3] :
C3. and S have both continuous second order derivatives
Then, f is proved to be isomorphic to a probability measure. The purely technical assumption
(C3) is very strong and cannot be justified on common sense arguments . For instance
minimum is a solution of C1 Which does not violate the algebra of propositions, but certainly
violates C3. In fact the unicity result does not require C3, which can be relaxed into a more
intuitive continuity and monotoriicity assumption. Cox's unicity result � holds under �
monotonicity assumption [10]. Cheeseman [2] proposes Cox's results as a formal proof that
only probability measures are reasonable for the modeling of subjective uncertainty. His
claim can be disputed. Although C1 sounds very sensible as a definition of conditional
credibility function, C2 explicitly states that only one number is enough to describe both the
uncertainties of b and -,b. Clearly, this . statement rules out the ability to distinguish
between the notions of posSibility and certainty. This distinction is the very purpose of
belief functions, possibility measures, and any kind of upper and lower probability system.
Hence the unicity result is not so surprizing, and Cox's setting, although being an interesting
attempt at recovering probability measures from a purely non frequentist argument does not
provide the ultimate answer to uncertainty � delling problems.
•

•

=

I

I
3

.

Modeling vagueness

Cheeseman [2] has proposed a nice definition of vagueness which we shall adopt. namely:
"vagueness is uncertainty about meaning and can be represented by a probability distribution
over possible meanings". However, contrastedly with [2], we explain why this view is
completely consistent with fuzzy sets and with the set-theoretic view of belief functions.

3. 1 MembershiP functjons and intermediary grades of truth
Consider the statement "Mary is young", represented by the logical formula a = young(x)
where x stands for the actual value of Mary's age. The set of possible worlds is an age scale
n. A rough model of "young" consists in letting M(a ) = I0, an interval contained in n, for
instance the interval [0,25] years. I0 is called the meaning of "young" the subscript c
indicates the context where the information arises. Then "Mary is young" is true if x e I0 and
false otherwise. Vagueness arises when there is uncertainty regarding which interval in n
properly translates "young". Let m(A) be the probability that I0 = A. m(A) can be a

I
I
I
J,
I

subjective probability obtained by asking a single individual, or can be a frequency if it
reflects a proportion of individuals thinking that A properly expresses "young".
Knowing the age x of Mary, the grade of truth of the statement "Mary is young" is defined
by the grade of membership J.l.young(x) defined

I

( 1 3)
IJ.young(x) = 2: A : xe A m(A)
llyoung(x) estimates the extent to which the value x is compatible with the meaning of
"young", formally expressed under the form of a random W i.e. a body of evidence in the
sense of Shafer [15], [6]. This view is a translation of Cheeseman [2]'s definition of
vagueness. It becomes exactly Zadeh's definition of a fuzzy set as soon as the family {Ai 1
m ( A i) > 0} is a nested family· so that the knowledge of the membership function lly oun g is
equivalent to that of the probabiliti�s m(Ai), because (12) is equivalent to (13) (see [6]). Of

I

course this nested property is not always completely valid in practice, especially if the Ai's
come from different individuals. However consonant (nested) approximations of dissonant
bodies of evidence exist [6], which are especially very good when n i A i ;: 0, a usually

satisfied consistency property which expresses that there is an age in n, totally compatible
with "young". Hence a fuzzy set, with membership function tJ. : n .... [0, 1 ], can always be used
as an approximation of a random set. However the word "random" may be inappropriate when
the m(Ai)'s do not express frequencies. One might prefer the term "convex combination of
sets" , which is more neutral but lengthy.
Cheeseman [2] claims that a membership function IJ.young is nothing but a conditional
probability P(young 1 x). This claim is not very well founded. Indeed the existence of a
quantity such as P(young 1 x) underlies the assumption that 'young' represents an event in the
usual sense of probability. Hence 'young' is a given subset 10 of n. But then the value of
P(young I x) is either .Q (x

e

10) or 1 (x

e

10). This is because x and 10 belong to the same

universe n. Admitting that P(young 1 x) e (0, 1) leads to admit that 'young' is not a standard
event but has a membership function ; Zadeh [21] has introduced the notion of the probability
of a fuzzy event, defined, in the tradition of probability· theory, as the expectation of the
membership function llyoun g·

(14)
P(young) = fn IJ.young(u) d P(u)
This definition assumes that the a p�iCiri knowledge on Mary's age u is given by a probability
measure P and is exactly equation (5) of Cheeseman [2] p. 95. When calculating P(young 1 x),
Mary's age is known (u = x) and P(young) P(young I x) = llyoung(x). But because in that case
the available information (u
x) is deterministic, P(. 1 X!) is a Dirac measure (P(ulx) = 1 if
=

=

86

I
I
I
I
I
I
I
I
I
I

I
I
I
I
I
I
I

I
I
I
a:
I
I
I
I
I
I
I
I

u =x, and 0 otherwise) and writing P(young 1 x) is just a matter of convention, one could write
Bel(young 1 x), IT(young 1 x) etc ... as well, since a Dirac measure is also a particular case of
belief function, possibility measure, etc ... ! If one admits as Hisdal [12] that concepts like
'young' have clear-cut meanings for single individuals, but become fuzzy when a group of
individuals is considered, then P(younglx) reflects the proportion of individuals that claim
that x agre.es-wltiL their.. notion of young; then f.l.young can be given a probabilistic

interpretation (as a likelihood function). This is also in accordance with (13) if m(A) denotes
the proportion of individuals for whom 'young' means A.

3.2 Uncertainty and graded truth
Note that f.l.young(x) is really a grade of truth of the statement "Mary is young" knowing
that x is the actual age of Mary. It is not a grade of uncertainty about the truth of the
statement. The statement is partially true (f.l.young(x) e (0,1)) as long as x is a borderline age
for the concept young (e.g. x = 30 in a given context). We can get grades of uncertainty about
truth when reversing the problem, namely finding whether Mary's age (a variable u) is x,
given that "Mary is young". The statement "Mary's age is x" can only be true or .false but its
truth may not be known. Let g(young,x) be the grade of possibility (or upper probability) that
Mary is young aru1 her age is x. We now wish to compute g(xlyoung) i.e. the degree of
possibility that Mary's age is x gjyen that she is young. Using Cox's axiom C1 we get
g(young , x) = g(young I x) g(x) = g(x I young) g(young)
(15)
•

•

where " is some unspecified (non-decreasing, associative) combination

operation such that

a " 1 = a, 0 .. 0 = 0. This decomposition is the basis of conditional probability definition,
with g = P,

•

= product. It is by definition valid for any measure of uncertainty, consistently

with Cox [3]'s proposal. Let us evaluate various terms. in order to calculate g(x 1 young).
g(young I x) f.l.young(x) as shown above, regardless of the nature of g.
"'

expresses the a priori knowledge about Mary's. age belonging to A. We shall assume
the state of total ignorance, so that g(A) = 1 A "' 0, since g(A) is a degree of
plausibility, consistently with section 1. Particularly g(x) = 1 and g(young) = 1.
As a consequence (15) leads to a well known equation in likelihood theory :
(16)
g(x I young) = g(young I x) = f.l.young(x)
When g is, stricto sensu, a possibility measure, (16) is exactly Zadeh's basic identity [22]
which translates a verbal statement (such as "Mary is young") into a possibility distribution
1t restricting the age u of Mary, defined by
7t(U = x) = f.l.young(x)
(17)
i.e. f.l.young(x) is interpreted as the grade of possibility that u = x given that "Mary is young" :
g(A)

it is .o.sll the degree of truth of 'u = x'. Contrastedly, the degree of certainty C(x 1 young) can
be computed using (1) and Dempster-Shafer framework as
(18)
C(x 1 young) 7 � A,;. {x} m(A) = m ({x})

i.e. C(x 1 young) = 0 (no certainty) generally because "young" usually refers to an age interval,
and not to a precise age, so �that m seldom bears on singletons. Hence, contrary to what is
claimed in [12], conditional distributions of xiA. and A.lx (where A. is a fuzzy predicate) are
distinct in possibility theory, provided that we consider both measures of possibility and
necessity.
So, Cheeseman [2]'s view of vagueness looks .consistent with possibility theory, much
more than with probability theory when the a priori knowle<;Jge about the variable underlying
a vague predicate is not available. Note that if this a priori knowledge were indeed available
under the form of a probability measure P then (15) is specialized into
(19)
P(x I young) = f.l.young(x) . [P(x) I P(young)]
which indeed defines a probability measure on n that is a revision of the a priori probability
87

I

I
P on the basis of the fuzzy event "Mary is young". If this vague statement were precise, i.e.
"Mary's age is in 10" then (19) becomes the well known conditioning formula
P(l0 ) if x e 10

P(xllcl • P(x) I
; P(xll0) = 0 otherwise
(20)
.
so (20), (19) and (17) can be regarded as particular consequences of Cox's axiom C1.
3.3 Uncertain eyjdence
Suppose as in Cheeseman [21 that the following information about Mary is that "Mary is
probably young". This statement is denoted S. Cheeseman [21 models this statement by means
of a conditional probability P(young 1 S) where once more the events appearing in the
probability are not very well defined. Surprizingly, no attempt is made to apply the maximum
entropy principle to this case. Indeed if we want to compute g(x 1 S) as a probability, a
natural approach would be to solve the following program (.0 is finite) :
find p which maximize - Iw. en. p(wi) log p(wi) under the constraint P(lc) � ex
(21)
I

where [ex, 1] is a numerical. translation of "probable" appearing in S. The interval lc
supposedly expresses the term ·"young". The solution of the above problem is simply, where 1.1
denotes cardinality : P(xiS) = ex /llcl if x e 10 ; P(x!S)= (1 - ex) I (101 llcl) if x E 10
(22)
-

When ex =1, we get a particular case of (20) where the a priori probability distribution is
uniform. The above representation is good to decjde what is Mary's age, not to represent our
state of knowledge, when the a priori probability distribution is not available. In tha't91 case,
statement S translates into C(l0)= ex where C(l0) is a lower probability degree, here a degree
of belief in the sense of Shafer. We need some least information principle, which, in the
scope of simply representing knowledge, would maximize the imprecision contained in the
belief function Bel-C. A simple evaluation of the imprecision of the statement "Mary is
young", where "young" translates into· the interval 10, is the cardinality of 10. Wheh llc1=1 then
we know Mary's age, when 10=0 we have no information. More generally if Mary's age is known
under the form of a random set (ff ,m). the imprecision is measured by
(23)
J(ff,m)l =LA_. n. m(A) IAI
•

which is an expected cardinality. Hence the problem of representing the meaning of a
statement such as "Mary is probably young", where "probably" is viewed as specifying a lower
bound on a probability value and "young" is viewed as a clearcut category approximated by a
subset 10, is that of solving the following program
find m which maximizes LA c 0 m(A).IAI under the constraint Bel(lc) = LA c 1 c m(A) = ex (24)
This is the principle of minimum specificity [7]. The solution of (24) is easily found
(25)
m(l0 I S) = ex ; m(O I S) : 1 - ex
Note that this solution is a possibility measure whose
distribution is rt(xiS) =1 if x e 10, rt(xJS) = 1 ex otherwise
-

(see Fig. 2). Moreover it encompasses the solution given by
the maximum entropy principle as a particular case, namely
l:f A.- 0
Bei(AIS) ::;; P(AIS) ::;; Pf(AIS)
(26)
1 -« 1---:1
i.e. the probability measJre obtained by (22) is in the set of
O"-------•
� probability measures implicitly defined by Bel and PI,
I c =young
viewed as upper and lower probab ilities.
This approach to the representation of uncertain knowledge can be extended to the case
when "young" is expressed, not as an interval, but as a fuzzy interval (e.g. [17]). Note that the
maximum entropy principle can also accommodate fuzziness in the statement of constraints
changing P(l0) into P(young) as defined by (14).

88

I
I
I
.I

I
I
I
I
I
I
I
I
I
I
I
I
I

I
I
I
I

I

Con c lusion

This paper has tried to show that as far as knowledge representation is concerned, there
should not be any dispute regarding the well-foundedness of possibility and fuzzy set theory,
belief functions, versus probability theory. The latter is older and is presently far more
developed than the two former. So, they are easy to criticize from the stand-point of
probability theory. However the development of new models of uncertainty for knowledge
representation seems to be an important issue, because of limitations of probability theory
in terms of descriptive power. The new models such as possibility and evidence theories are
not built aaaj nst probability. theory, but in the same spirit ; indeed most of the new
uncertainty measures can be viewed as upper and lower probability measures. Hence
probability theory itself is a basic tool for the construction of new models of uncertain
knowledge. The present situation of probability theory is similar to the situation of
classical logic in the mid-seventies. Classical logic has been given up by some researchers,
when modelling common sense knowledge, but the idea of a logic has been preserved, and new
logics (default logics for instance) have arised. In the case of uncertainty models, it seems
important to go beyond probability but consistently with probability theory itself. Of course
the authors are aware of the bulk of work needed to bring possibility and evidence theories
to the level of development of probability theory. In that sense many results reported in the
literature about foundations and combination rules are certainly preliminary. But we
strongly question dogmatic attitudes disputing alternative theories of uncertainty on behalf
of rationality. Modelling uncertainty, and especially subjective uncertainty, cannot be but a
compromise between the ideally optimal Bayesian theory and the limited precision and
vagueness of the available often subjective knowledge.
