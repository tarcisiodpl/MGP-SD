
We present a new anytime algorithm that
achieves near-optimal regret for any instance
of finite stochastic partial monitoring. In particular, the new algorithm achieves the minimax regret, within logarithmic factors, for
both “easy” and “hard” problems. For easy
problems, it additionally achieves logarithmic
individual regret. Most importantly, the algorithm is adaptive in the sense that if the
opponent strategy is in an “easy region” of
the strategy space then the regret grows as if
the problem was easy. As an implication, we
show that under some reasonable additional
√
assumptions, the algorithm enjoys an O( T )
regret in Dynamic Pricing, proven to be hard
by Bartók et al. (2011).

1. Introduction
Partial monitoring can be cast as a sequential game
played by a learner and an opponent. In every time
step, the learner chooses an action and simultaneously
the opponent chooses an outcome. Then, based on the
action and the outcome, the learner suffers some loss
and receives some feedback. Neither the outcome nor
the loss are revealed to the learner. Thus, a partialmonitoring game with N actions and M outcomes is
defined with the pair G = (L, H), where L ∈ RN ×M
is the loss matrix, and H ∈ ΣN ×M is the feedback
matrix over some arbitrary set of symbols Σ. These
matrices are announced to both the learner and the
opponent before the game starts. At time step t, if
It ∈ N = {1, 2, . . . , N } and Jt ∈ M = {1, 2, . . . , M }
denote the (possibly random) choices of the learner
and the opponent, respectively then, the loss suffered
by the learner in that time step is L[It , Jt ], while the
Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012.
Copyright 2012 by the author(s)/owner(s).

feedback received is H[It , Jt ].
The goal of the learner (or player) is to minimize his
PT
cumulative loss t=1 L[It , Jt ]. The performance of the
learner is measured in terms of the regret, defined as
the excess cumulative loss he suffers compared to that
of the best fixed action in hindsight:
RT =

T
X
t=1

L[It , Jt ] − min
i∈N

T
X

L[i, Jt ] .

t=1

The regret usually grows with the time horizon T .
What distinguishes between a “successful” and an “unsuccessful” learner is the growth rate of the regret. A
regret linear in T means that the learner does not approach the performance of the optimal action. On the
other hand, if the growth rate is sublinear, it is said
that the learner can learn the game.
In this paper we restrict our attention to stochastic
games, adding the extra assumption that the opponent generates the outcomes with a sequence of independent and identically distributed random variables.
This distribution will be called the opponent strategy.
As for the player, a player strategy (or algorithm) is
a (possibly random) function from the set of feedback
sequences (observation histories) to the set of actions.
In stochastic games, we use a slightly different notion
of regret: we compare the cumulative loss with that of
the action with the lowest expected loss.
RT =

T
X
t=1

L[It , Jt ] − T min E[L[i, J1 ]] .
i∈N

The “hardness” of a game is defined in terms of the
minimax expected regret (or minimax regret for short):
RT (G) = min max E[RT ] ,
A p∈∆M

where ∆M is the space of opponent strategies, and
A is any strategy of the player. In other words, the

Adaptive Stochastic Partial Monitoring

minimax regret is the worst-case expected regret of the
best algorithm.
A question of major importance is how the minimax
regret scales with the parameters of the game, such as
the time horizon T , the number of actions N , the number of outcomes M . In the stochastic setting, another
measure of “hardness” is worth studying, namely the
individual or problem-dependent regret, defined as the
expected regret given a fixed opponent strategy.
1.1. Related work
Two special cases of partial monitoring have been
extensively studied for a long time: full-information
games, where the feedback carries enough information for the learner to infer the outcome for any
action-outcome pair, and bandit games, where the
learner receives the loss of the chosen action as feedback. Since Vovk (1990) and Littlestone & Warmuth
(1994) we know that for full-information
games, the
√
minimax regret scales as Θ( T log N ). For bandit
games,
√ the minimax regret has been proven to scale as
Θ( N T ) (Audibert & Bubeck, 2009).1 The individual regret of these kind of games has also been studied:
Auer et al. (2002) showed that given any opponent
strategy,
P the expected regret can be upper bounded
by c i∈N :δi 6=0 δ1i log T , where δi is the expected difference between the loss of action i and an optimal
action.
Finite partial monitoring problems were introduced by
Piccolboni & Schindelhauer (2001). They proved that
a game is either “hopeless” (that is, its minimax regret scales linearly with T ), or the regret can be upper
bounded by O(T 3/4 ). They also give a characterization of hopeless games. Namely, a game is hopeless
if it does not satisfy the global observability condition
(see Definition 5 in Section 2). Their upper bound
for non-hopeless games was tightened to O(T 2/3 ) by
Cesa-Bianchi et al. (2006), who also showed that there
exists a game with a matching lower bound.
Cesa-Bianchi et al. (2006) posted the problem of characterizing partial-monitoring games with minimax regret less than Θ(T 2/3 ). This problem has been solved
since then. The first steps towards classifying partialmonitoring games were made by Bartók et al. (2010),
who characterized almost all games with two outcomes.
They proved that there are only
√ four categories: games
e T ), Θ(T 2/3 ), and Θ(T ),
with minimax regret 0, Θ(
and named them trivial, easy, hard, and hopeless, re1

The Exp3 algorithm due to Auer et al. (2003) achieves
almost the same regret, with an extra logarithmic term.

spectively.2 They also found that there exist games
that are easy, but can not easily be “transformed” to
a bandit or full-information game. Later, Bartók et al.
(2011) proved the same results for finite stochastic partial monitoring, with any finite number of outcomes.
The condition that separates easy games from hard
games is the local observability condition (see Definition 6). The algorithm Balaton introduced there
works by eliminating actions that are thought to be
suboptimal with high confidence. They conjectured in
their paper that the same classification holds for nonstochastic games, without changing the condition. Recently, Foster & Rakhlin (2011) designed the algorithm
NeighborhoodWatch that proves this conjecture to
be true. Foster & Rakhlin prove an upper bound on a
stronger notion of regret, called internal regret.
1.2. Contributions
In this paper, we extend the results of Bartók et al.
(2011). We introduce a new algorithm, called CBP
for “Confidence Bound Partial monitoring”, with various desirable properties. First of all, while Balaton
only works on easy games, CBP can be run on any
non-hopeless game, and it achieves (up to logarithmic
factors) the minimax regret rates both for easy and
hard games (see Corollaries 3 and 2). Furthermore, it
also achieves logarithmic problem-dependent regret for
easy games (see Corollary 1). It is also an “anytime”
algorithm, meaning that it does not have to know the
time horizon, nor does it have to use the doubling trick,
to achieve the desired performance.
The final, and potentially most impactful, aspect of
our algorithm is that through additional assumptions
on the set of opponent strategies, the minimax regret
√
e T )!
of even hard games can be brought down to Θ(
While this statement may seem to contradict the result of Bartók et al. (2011), in fact it does not. For the
precise statement, see Theorem 2. We call this property “adaptiveness” to emphasize that the algorithm
does not even have to know that the set of opponent
strategies is restricted.

2. Definitions and notations
Recall from the introduction that an instance of partial
monitoring with N actions and M outcomes is defined
by the pair of matrices L ∈ RN ×M and H ∈ ΣN ×M ,
where Σ is an arbitrary set of symbols. In each round
t, the opponent chooses an outcome Jt ∈ M and simultaneously the learner chooses an action It ∈ N . Then,
2

Note that these results do not concern the growth rate
in terms of other parameters (like N ).

Adaptive Stochastic Partial Monitoring

the feedback H[It , Jt ] is revealed and the learner suffers the loss L[It , Jt ]. It is important to note that the
loss is not revealed to the learner.

+
Note that the neighborhood action set Ni,j
naturally
+
contains i and j. If Ni,j contains some other action k
then either Ck = Ci , Ck = Cj , or Ck = Ci ∩ Cj .

As it was previously mentioned, in this paper we deal
with stochastic opponents only. In this case, the choice
of the opponent is governed by a sequence J1 , J2 , . . .
of i.i.d. random variables. The distribution of these
variables p ∈ ∆M is called an opponent strategy, where
∆M , also called the probability simplex, is the set of
all distributions over the M outcomes. It is easy to
see that, given opponent strategy p, the expected loss
of action i can be expressed as `>
i p, where `i is defined
as the column vector consisting of the ith row of L.

In general, the elements of the feedback matrix H can
be arbitrary symbols. Nevertheless, the nature of the
symbols themselves does not matter in terms of the
structure of the game. What determines the feedback
structure of a game is the occurrence of identical symbols in each row of H. To “standardize” the feedback
structure, the signal matrix is defined for each action:

The following definitions, taken from Bartók et al.
(2011), are essential for understanding how the structure of L and H determines the “hardness” of a game.
Action i is called optimal under strategy p if its expected loss is not greater than that of any other ac>
tion i0 ∈ N . That is, `>
i p ≤ `i0 p. Determining which
action is optimal under opponent strategies yields the
cell decomposition 3 of the probability simplex ∆M :
Definition
1
(Cell
decomposition). For
every action i
∈
N , let Ci
=
{p
∈
∆M
:
action i is optimal under p}.
The sets
C1 , . . . , CN constitute the cell decomposition of ∆M .
Now we can define the following important properties
of actions:
Definition 2 (Properties of actions).
• Action i is
called dominated if Ci = ∅. If an action is not
dominated then it is called non-dominated.
• Action i is called degenerate if it is nondominated and there exists an action i0 such that
Ci ( Ci0 .
• If an action is neither dominated nor degenerate then it is called Pareto-optimal. The set of
Pareto-optimal actions is denoted by P.
From the definition of cells we see that a cell is either
empty or it is a closed polytope. Furthermore, Paretooptimal actions have (M − 1)-dimensional cells. The
following definition, important for our algorithm, also
uses the dimensionality of polytopes:
Definition 3 (Neighbors). Two Pareto-optimal actions i and j are neighbors if Ci ∩ Cj is an (M − 2)dimensional polytope. Let N be the set of unordered
pairs over N that contains neighboring action-pairs.
The neighborhood action set of two neighboring ac+
tions i, j is defined as Ni,j
= {k ∈ N : Ci ∩ Cj ⊆ Ck }.
3
The concept of cell decomposition also appears in Piccolboni & Schindelhauer (2001).

Definition 4. Let si be the number of distinct symbols in the ith row of H and let σ1 , . . . , σsi ∈ Σ
be an enumeration of those symbols. Then the signal matrix Si ∈ {0, 1}si ×M of action i is defined as
Si [k, l] = I{H[i,l]=σk } .
The idea of this definition is that if p ∈ ∆M is the opponent’s strategy then Si p gives the distribution over
the symbols underlying action i. In fact, it is also true
that observing H[It , Jt ] is equivalent to observing the
vector SIt eJt , where ek is the k th unit vector in the
standard basis of RM . From now on we assume without loss of generality that the learner’s observation at
time step t is the random vector Yt = SIt eJt . Note
that the dimensionality of this vector depends on the
action chosen by the learner, namely Yt ∈ RsIt .
The following two definitions play a key role in classifying partial-monitoring games based on their difficulty.
Definition 5 (Global observability (Piccolboni &
Schindelhauer, 2001)). A partial-monitoring game
(L, H) admits the global observability condition, if for
all pairs i, j of actions, `i − `j ∈ ⊕k∈N Im Sk> .
Definition 6 (Local observability (Bartók et al.,
2011)). A pair of neighboring actions i, j is said to
be locally observable if `i − `j ∈ ⊕k∈N + Im Sk> . We
i,j
denote by L ⊂ N the set of locally observable pairs of
actions (the pairs are unordered). A game satisfies the
local observability condition if every pair of neighboring actions is locally observable, i.e., if L = N .
The main result of Bartók et al.
√ (2011) is that loe T ) minimax regret.
cally observable games have O(
It is easy to see that local observability implies global
observability. Also, from Piccolboni & Schindelhauer
(2001) we know that if global observability does not
hold then the game has linear minimax regret. From
now on, we only deal with games that admit the global
observability condition.
A collection of the concepts and symbols introduced
in this section is shown in Table 1.

Adaptive Stochastic Partial Monitoring
Table 1. List of basic symbols

Symbol
N, M ∈ N
N
∆ M ⊂ RM
p∗ ∈ ∆M
L ∈ RN ×M
H ∈ ΣN ×M
`i ∈ RM
C i ⊆ ∆M
P⊆N
N ⊆ N2
+
Ni,j
⊆N
Si ∈ {0, 1}si ×M
L⊆N
Vi,j ⊆ N
vi,j,k ∈ Rsk , k ∈ Vi,j
Wi ∈ R

Definition
number of actions and outcomes
{1, . . . , N }, set of actions
M -dim. simplex, set of opponent strategies
opponent strategy
loss matrix
feedback matrix
`i = L[i, :], loss vector underlying action i
cell of action i
set of Pareto-optimal actions
set of unordered neighboring action-pairs
neighborhood action set of {i, j} ∈ N
signal matrix of action i
set of locally observable action pairs
observer actions underlying {i, j} ∈ N
observer vectors
confidence width for action i ∈ N

3. The proposed algorithm
Our algorithm builds on the core idea underlying algorithm Balaton of Bartók et al. (2011), so we start
with a brief review of Balaton. Balaton uses
sweeps to successively eliminate suboptimal actions.
This is done by estimating the differences between
the expected losses of pairs of actions, i.e., δi,j =
(`i −`j )> p∗ (i, j ∈ N ). In fact, Balaton exploits that
it suffices to keep track of δi,j for neighboring pairs of
actions (i.e., for action pairs i, j such that {i, j} ∈ N ).
This is because if an action i is suboptimal, it will
have a neighbor j that has a smaller expected loss
and so the action i will get eliminated when δi,j is
checked. Now, to estimate δi,j for some {i, j} ∈ N one
observes that under thePlocal observability condition,
it holds that `i − `j = k∈N + Si> vi,j,k for some veci,j

tors vi,j,k ∈ Rσk . This yields that δi,j = (`i − `j )> p∗ =
P
def
>
∗
∗
+ v
i,j,k Sk p . Since νk = Sk p is the vector
k∈Ni,j
of the distribution of symbols under action k, which
can be estimated by νk (t), the empirical frequencies of
the individual symbols
under k up to time
P observed
>
νk (t) to estimate δi,j .
t, Balaton uses k∈N + vi,j,k
i,j

+
Since none of the actions in Ni,j
can get eliminated
before one of {i, j} gets eliminated, the estimate of
δi,j gets refined until one of {i, j} is eliminated.

The essence of why Balaton achieves a low regret is
as follows: When i is not a neighbor of the optimal
action i∗ one can show that it will be eliminated before all neighbors j “between i and i∗ ” get eliminated.
Thus, the contribution of such “far” actions to the re-

Found in/at

Definition
Definition
Definition
Definition
Definition
Definition
Definition
Definition
Definition

1
2
3
3
4
6
7
7
7

gret is minimal. When i is a neighbor of i∗ , it will
−2
be eliminated in time proportional to δi,i
∗ . Thus the
contribution to the regret of such an action is propordef
tional to δi−1 , where δi = δi,i∗ . It also holds that the
contribution to the regret of i cannot be larger than
δi T . Thus, the contribution
of i to the regret is at
√
most min(δi T, δi−1 ) ≤ T .
When some pairs {i, j} ∈ N are not locally observable,
+
one needs to use actions other than those in Ni,j
to
construct anP
estimate of δi,j . Under global observability, `i −`j = k∈Vi,j Si> vi,j,k for an appropriate subset
Vi,j ⊂ N and an appropriate set of vectors vi,j,· . Thus,
if the actions in Vi,j are kept in play,
Pone can >estimate
the difference δi,j as before, using k∈N + vi,j,k
νk (t).
i,j
This motivates the following definition:
Definition 7 (Observer sets and observer vectors).
The observer set Vi,j ⊂ N underlying a pair of neighboring actions {i, j} ∈ N is a set of actions such that
`i − `j ∈ ⊕k∈Vi,j Im Sk> .
The observer vectors (vi,j,k )P
k∈Vi,j are defined to satisfy the equation `i − `j = k∈Vi,j Sk> vi,j,k . In particular, vi,j,k ∈ Rsk . In what follows, the choice
of the observer sets and vectors is restricted so that
Vi,j = Vj,i and vi,j,k = −vj,i,k . Furthermore, the ob+
server set Vi,j is constrained to be a superset of Ni,j
and in particular when a pair {i, j} is locally observ+
able, Vi,j = Ni,j
must hold. Finally, for any action
S
+
k ∈ {i,j}∈N Ni,j
, let Wk = maxi,j:k∈Vi ,j kvi,j,k k∞ be
the confidence width of action k.
+
The reason of the particular choice Vi,j = Ni,j
for lo-

Adaptive Stochastic Partial Monitoring

cally observable pairs {i, j} is that we plan to use Vi,j
(and the vectors vi,j,· ) in the case of locally observable
pairs, too. For not locally observable pairs, the whole
action set N is always a valid observer set (thus, Vi,j
can be found). However, whenever possible, it is better to use a smaller set. The actual choice of Vi,j (and
vi,j,k ) is postponed until the effect of this choice on the
regret becomes clear.
With the observer sets, the basic idea of the algorithm
becomes as follows: (i) Eliminate the suboptimal actions in successive sweeps; (ii) In each sweep, enrich
the set of remaining actions P(t) by adding the observer actions underlying theS remaining neighboring
pairs {i, j} ∈ N (t): V(t) = {i,j}∈N (t) Vi,j ; (iii) Explore the actions in P(t) ∪ V(t) to update the symbol
frequency estimate vectors νk (t). Another refinement
is to eliminate the sweeps so as to make the algorithm
enjoy an advantageous anytime property. This can be
achieved by selecting in each step only one action. We
propose the action to be chosen should be the one that
maximizes the reduction of the remaining uncertainty.
√
This algorithm could be shown to enjoy T regret for
locally observable games. However, if we run it on a
non-locally observable game and the opponent strategy is on Ci ∩ Cj for {i, j} ∈ N \ L, it will suffer linear
regret! The reason is that if both actions i and j are
optimal, and thus never get eliminated, the algorithm
+
will choose actions from Vi,j \ Ni,j
too often. Furthermore, even if the opponent strategy is not on the
boundary the regret can be too high: say action i is
optimal but δj is small, while {i, j} ∈ N \ L. Then
a third action k ∈ Vi,j with large δk will be chosen
proportional to 1/δj2 times, causing high regret. To
combat this we restrict the frequency with which an
action can be used for “information seeking purposes”.
For this, we introduce the set of rarely chosen actions,
R(t) = {k ∈ N : nk (t) ≤ ηk f (t)} ,
where ηk ∈ R, f : N → R are tuning parameters to be
chosen later. Then, the set of actions available at time
t is restricted
N + (t) ∪ (V(t) ∩ R(t)), where
S to P(t) ∪ +
+
N (t) = {i,j}∈N (t) Ni,j . We will show that with
these modifications, the algorithm achieves O(T 2/3 )
regret in the general
√ case, while it will also be shown
to achieve an O( T ) regret when the opponent uses
a benign strategy. A pseudocode for the algorithm is
given in Algorithm 1.
It remains to specify the function getPolytope. It
gets the array halfSpace as input. The array halfSpace stores which neighboring action pairs have a
confident estimate on the difference of their expected
losses, along with the sign of the difference (if confi-

Algorithm 1 CBP
Input: L, H, α, η1 , . . . , ηN , f = f (·)
Calculate P, N , Vi,j , vi,j,k , Wk
for t = 1 to N do
Choose It = t and observe Yt
{Initialization}
nIt ← 1
{# times the action is chosen}
νIt ← Yt
{Cumulative observations}
end for
for t = N + 1, N + 2, . . . do
for eachP
{i, j} ∈ N do
νk
>
δ̃i,j ← k∈Vi,j vi,j,k
nk q{Loss diff. estimate}
P
t
{Confidence}
ci,j ← k∈Vi,j kvi,j,k k∞ α nlog
k
if |δ̃i,j | ≥ ci,j then
half Space(i, j) ← sgn δ̃i,j
else
half Space(i, j) ← 0
end if
end for
[P(t), N (t)] ← getPolytope(P, N , half Space)
+
N + (t) = ∪{i,j}∈N (t) Nij
V(t) = ∪{i,j}∈N (t) Vij
R(t) = {k ∈ N : nk (t) ≤ ηk f (t)}
S(t) = P(t) ∪ N + (t) ∪ (V(t) ∩ R(t))
W2
Choose It = argmaxi∈S(t) nii and observe Yt
νIt ← νIt + Yt
nIt ← nIt + 1
end for
dent). Each of these confident pairs define an open
halfspace, namely

∆{i,j} = p ∈ ∆M : half Space(i, j)(`i − `j )> p > 0 .
The function getPolytope calculates the open polytope defined as the intersection of the above halfspaces.
Then for all i ∈ P it checks if Ci intersects with the
open polytope. If so, then i will be an element of P(t).
Similarly, for every {i, j} ∈ N , it checks if Ci ∩ Cj intersects with the open polytope and puts the pair in
N (t) if it does.
Note that it is not enough to compute P(t) and then
drop from N those pairs {k, l} where one of k or l is
excluded from P(t): it is possible that the boundary
Ck ∩ Cl between the cells of two actions k, l ∈ P(t) is
included in the rejected region. For an illustration of
cell decomposition and excluding cells, see Figure 1.
Computational complexity The computationally
heavy parts of the algorithm are the initial calculation
of the cell decomposition and the function getPolytope. All of these require linear programming. In the
preprocessing phase we need to solve N + N 2 linear

Adaptive Stochastic Partial Monitoring
(0, 0, 1)

(0, 0, 1)

(0, 1, 0)

(0, 1, 0)

(1, 0, 0)

(1, 0, 0)

(a) Cell decomposition.

(b) Gray indicates
cluded area.

ex-

Figure 1. An example of cell decomposition (M = 3).

programs to determine cells and neighboring pairs of
cells. Then in every round, at most N 2 linear programs are needed. The algorithm can be sped up by
“caching” previously solved linear programs.

short explanation of the different terms in the bound.
The first term corresponds to the confidence interval
failure event. The second term comes from the initialization phase of the algorithm. The remaining four
terms come from categorizing the choices of the algorithm by two criteria: (1) Would It be different if R(t)
was defined as R(t) = N ? (2) Is It ∈ P(t) ∪ Nt+ ?
These two binary events lead to four different cases in
the proof, resulting in the last four terms of the bound.
An implication of Theorem 1 is an upper bound on the
individual regret of locally observable games:
Corollary 1. If G is locally observable then


X
1
E[RT ] ≤
2|Vi,j | 1 +
2α − 2
{i,j}∈N

+

N
X

δk + 4Wk2

k=1

d2k
α log T .
δk

4. Analysis of the algorithm
The first theorem in this section is an individual upper
bound on the regret of CBP.
Theorem 1. Let (L, H) be an N by M partialmonitoring game. For a fixed opponent strategy p∗ ∈
∆M , let δi denote the difference between the expected
loss of action i and an optimal action. For any time
horizon T , algorithm CBP with parameters α > 1,
2/3
νk = Wk , f (t) = α1/3 t2/3 log1/3 t has expected regret

2|Vi,j | 1 +

X

E[RT ] ≤

{i,j}∈N
N
X

+

k=1
δk >0

+

X

4Wk2

1
2α − 2


+

N
X

δk

k∈V\N +

d2l(k)
2
δl(k)

α log T,
!

α
+

1/3

2/3
Wk T 2/3

X

log

1/3

The following corollary is an upper bound on the minimax regret of any globally observable game.
Corollary 2. Let G be a globally observable game.
Then there exists a constant c such that the expected
regret can be upper bounded independently of the choice
of p∗ as
E[RT ] ≤ cT 2/3 log1/3 T .

k=1

d2k
α log T
δk

δk min 4Wk2

Proof. If a game is locally observable then V \N + = ∅,
leaving the last two sums of the statement of Theorem 1 zero.

T

2/3

δk α1/3 Wk T 2/3 log1/3 T

k∈V\N +

+ 2dk α1/3 W 2/3 T 2/3 log1/3 T ,
where W = maxk∈N Wk , V = ∪{i,j}∈N Vi,j , N + =
+
∪{i,j}∈N Ni,j
, and d1 , . . . , dN are game-dependent constants.
The proof is omitted for lack of space.4 Here we give a
4
For complete proofs we refer the reader to the supplementary material.

The following theorem is an upper bound on the minimax regret of any globally observable game against
“benign” opponents. To state the theorem, we need a
new definition. Let A be some subsetTof actions in G.
We call A a point-local game in G if i∈A Ci 6= ∅.
Theorem 2. Let G be a globally observable game. Let
∆0 ⊆ ∆M be some subset of the probability simplex
such that its topological closure ∆0 has ∆0 ∩ Ci ∩ Cj = ∅
for every {i, j} ∈ N \ L. Then there exists a constant
c such that for every p∗ ∈ ∆0 , algorithm CBP with
2/3
parameters α > 1, νk = Wk , f (t) = α1/3 t2/3 log1/3 t
achieves
p
E[RT ] ≤ cdpmax bT log T ,
where b is the size of the largest point-local game, and
dpmax is a game-dependent constant.
In a nutshell, the proof revisits the four cases of the
proof of Theorem 1, and shows that the terms which
would yield T 2/3 upper bound can be non-zero only
for a limited number of time steps.

Adaptive Stochastic Partial Monitoring

Remark 1. Note that the above theorem implies that
CBP does not√need to have any prior knowledge about
∆0 to achieve T regret. This is why we say our algorithm is “adaptive”.
An immediate implication of Theorem 2 is the following minimax bound for locally observable games:
Corollary 3. Let G be a locally observable finite partial monitoring game. Then there exists a constant c
such that for every p ∈ ∆M ,
p
E[RT ] ≤ c T log T .
Remark 2. The upper bounds in Corollaries 2 and 3
both have matching lower bounds up to logarithmic factors (Bartók et al., 2011), proving that CBP
achieves near optimal regret in both locally observable
and non-locally observable games.

5. Experiments
We demonstrate the results of the previous sections using instances of Dynamic Pricing, as well as a locally
observable game. We compare the results of CBP to
two other algorithms: Balaton (Bartók et al., 2011)
which is, as mentioned earlier
√ in the paper, the first
e T ) minimax regret for all
algorithm that achieves O(
locally observable finite stochastic partial-monitoring
games; and FeedExp3 (Piccolboni & Schindelhauer,
2001), which achieves O(T 2/3 ) minimax regret on
all non-hopeless finite partial-monitoring games, even
against adversarial opponents.
5.1. A locally observable game
The game we use to compare CBP and Balaton has
3 actions and 3 outcomes. The game is described with
the loss and feedback matrices:




1 1 0
a b b
L = 0 1 1 ;
H = b a b .
1 0 1
b b a
We ran the algorithms 10 times for 15 different
stochastic strategies. We averaged the results for each
strategy and then took pointwise maximum over the 15
strategies. Figure 2(a) shows the empirical minimax
regret calculated the way described above. In addition,
Figure 2(b) shows the regret of the algorithms against
one of the opponents, averaged over 100 runs. The
results indicate that CBP outperforms both FeedExp
and Balaton. We also observe that, although the
asymptotic performace of Balaton is proven to be
better than that of FeedExp, a larger constant factor
makes Balaton lose against FeedExp even at time
step ten million.

5.2. Dynamic Pricing
In Dynamic Pricing, at every time step a seller (player)
sets a price for his product while a buyer (opponent)
secretly sets a maximum price he is willing to pay. The
feedback for the seller is “buy” or “no-buy”, while his
loss is either a preset constant (no-buy) or the difference between the prices (buy). The finite version of the
game can be described with the following matrices:




0 1 ··· N − 1
y y ··· y
 c 0 · · · N − 2
n y · · · y 




L = . .
H = . .

.
.
..
..
. . . . . ... 
.. 
 ..
 ..

c ···
c
0
n ···
n y
This game is not locally observable and thus it is
“hard” (Bartók et al., 2011). Simple linear algebra
gives that the locally observable action pairs are the
“consecutive” actions (L = {{i, i + 1} : i ∈ N − 1}),
while quite surprisingly, all action pairs are neighbors.
We compare CBP with FeedExp on Dynamic Pricing with N = M = 5 and c = 2. Since Balaton
is undefined on not locally observable games, we can
not include it in the comparison. To demonstrate the
adaptiveness of CBP, we use two sets of opponent
strategies. The “benign” setting is a set of opponents
which are far away from “dangerous” regions, that is,
from boundaries between cells of non-locally observable neighboring action pairs. The “harsh” settings,
however, include opponent strategies that are close or
on the boundary between two such actions. For each
setting we maximize over 15 strategies and average
over 10 runs. We also compare the individual regret of
the two algorithms against one benign and one harsh
strategy. We averaged over 100 runs and plotted the
90 percent confidence intervals.
The results (shown in Figures 3 and 4) indicate that
CBP has a significant advantage over FeedExp on
benign settings. Nevertheless, for the harsh settings
FeedExp slightly outperforms CBP, which we think is
a reasonable price to pay for the benefit of adaptivity.

