
We consider a setting where an agent’s uncertainty is represented by
a set of probability measures, rather than a single measure. Measureby-measure updating of such a set of measures upon acquiring new information is well-known to suffer from problems; agents are not always
able to learn appropriately. To deal with these problems, we propose using weighted sets of probabilities: a representation where each measure is
associated with a weight, which denotes its significance. We describe a
natural approach to updating in such a situation and a natural approach
to determining the weights. We then show how this representation can
be used in decision-making, by modifying a standard approach to decision making—minimizing expected regret—to obtain minimax weighted
expected regret (MWER). We provide an axiomatization that characterizes preferences induced by MWER both in the static and dynamic case.

1

Introduction

Agents must constantly make decisions; these decisions are typically made in a
setting with uncertainty. For decisions based on the outcome of the toss of a fair
coin, the uncertainty can be well characterized by probability. However, what
is the probability of you getting cancer if you eat fries at every meal? What if
you have salads instead? Even experts would not agree on a single probability.
Representing uncertainty by a single probability measure and making decisions by maximizing expected utility leads to further problems. Consider the
following stylized problem, which serves as a running example in this paper.
∗ The authors thank Joerg Stoye for useful comments. Work supported in part by NSF
grants IIS-0534064, IIS-0812045, and IIS-0911036, by AFOSR grants FA9550-08-1-0438 and
FA9550-09-1-0266, and by ARO grant W911NF-09-1-0281.

1

cont
back
check

1 broken
10,000
0
5,001

10 broken
-10,000
0
-4,999

Table 1: Payoffs for the robot delivery problem. Acts are in the leftmost column.
The remaining two columns describe the outcome for the two sets of states that
matter.
The baker’s delivery robot, T-800, is delivering 1, 000 cupcakes from the bakery
to a banquet. Along the way, T-800 takes a tumble down a flight of stairs and
breaks some of the cupcakes. The robot’s map indicates that this flight of stairs
must be either ten feet or fifteen feet high. For simplicity, assume that a fall of
ten feet results in one broken cupcake, while a fall of fifteen feet results in ten
broken cupcakes.
T-800’s choices and their consequences are summarized in Table 1. Decision
theorists typically model decision problems with states, acts, and outcomes: the
world is in one of many possible states, and the decision maker chooses an act,
a function mapping states to outcomes. A natural state space in this problem is
{good,broken}1000 , where each state is a possible state of the cupcakes. However,
all that matters about the state is the number of broken cakes, so we can further
restrict to states with either one or ten broken cakes.
T-800 can choose among three acts: cont: continue the delivery attempt;
back : go back for new cupcakes; or check : open the container and count the
number of broken cupcakes, and then decide to continue or go back, depending
on the number of broken cakes. The client will tolerate one broken cupcake, but
not ten broken cupcakes. Therefore, if T-800 chooses cont, it obtains a utility of
10, 000 if there is only one broken cake, but a utility of −10, 000 if there are ten
broken cakes. If T-800 chooses to go back , then it gets a utility of 0. Finally,
checking the cupcakes costs 4, 999 units of utility but is reliable, so if T-800
chooses check , it ends up with a utility of 5, 001 if there is one broken cake, and
a utility of −4, 999 if there are ten broken cakes.
If we try to maximize expected utility, we must assume some probability
over states. What measure should be used? There are two hypotheses that T800 entertains: (1) the stairs are ten feet high and (2) the stairs are fifteen feet
high. Each of these places a different probability on states. If the stairs are ten
feet high, we can take all of the 1, 000 states where there is exactly one broken
cake to be equally probable, and take the remaining states to have probability
0; if the stairs are fifteen feet high, we can take all of the C(1000, 10) states
where there are exactly ten broken cakes to be equally probable, and take the
remaining states to have probability 0. One way to model T-800’s uncertainty
about the height of the stairs is to take each hypothesis to be equally likely.
However, not having any idea about which hypothesis holds is very different
from believing that all hypotheses are equally likely. It is easy to check that
taking each hypothesis to be equally likely makes check the act that maximizes

2

utility, but taking the probability that the stairs are fifteen feet high to be .51
makes back the act that maximizes expected utility, and taking the probability
that the stairs are ten feet high to be .51 makes cont the act that maximizes
expected utility. What makes any of these choices the “right” choice?
It is easy to construct many other examples where a single probability measure does not capture uncertainty, and does not result in what seem to be
reasonable decisions, when combined with expected utility maximization. A
natural alternative, which has often been considered in the literature, is to represent the agent’s uncertainty by a set of probability measures. For example, in
the delivery problem, the agent’s beliefs could be represented by two probability measures, Pr1 and Pr10 , one for each hypothesis. Thus, Pr1 assigns uniform
probability to all states with exactly one broken cake, and Pr10 assigns uniform
probability to all states with exactly ten broken cakes.
But this representation also has problems. Consider the delivery example
again. Why should T-800 be sure that there is exactly either one broken cake or
ten broken cakes? Of course, we can replace these two hypotheses by hypotheses
that say that the probability of a cake being broken is either .001 or .01, but this
doesn’t solve the problem. Why should the agent be sure that the probability
is either exactly .001 or exactly .01? Couldn’t it also be .0999? Representing
uncertainty by a set of measures still places a sharp boundary on what measures
are considered possible and impossible.
A second problem involves updating beliefs. How should beliefs be updated if
they are represented by a set of probability measures? The standard approach
for updating a single measure is by conditioning. The natural extension of
conditioning to sets of measure is measure-by-measure updating: conditioning
each measure on the information (and also removing measures that give the
information probability 0).
However, measure-by-measure updating can produce some rather counterintuitive outcomes. In the delivery example, suppose that a passer-by tells T-800
the information E: the first 100 cupcakes are good. Assuming that the passerby told the truth, intuition tells us that there is now more reason to believe that
there is only one broken cupcake.
However, Pr1 | E places uniform probability on all states where the first
100 cakes are good, and there is exactly one broken cake among the last 900.
Similarly, Pr10 | E places uniform probability on all states where the first 100
cakes are good, and there are exactly ten broken cakes among the last 900.
Pr1 | E still places probability 1 on there being one broken cake, just like Pr1 ,
Pr10 | E still places probability 1 on there being ten broken cakes. There is no
way to capture the fact that T-800 now views the hypothesis Pr10 as less likely,
even if the passer-by had said instead that the first 990 cakes are all good!
Of course, both of these problems would be alleviated if we placed a probability on hypotheses, but, as we have already observed, this leads to other
problems. In this paper, we propose an intermediate approach: representing
uncertainty using weighted sets of probabilities. That is, each probability measure is associated with a weight. These weights can be viewed as probabilities;
indeed, if the set of probabilities is finite, we can normalize them so that they
3

are effectively probabilities. Moreover, in one important setting, we update
them in the same way that we would update probabilities, using likelihood (see
below). On the other hand, these weights do not act like probabilities if the set
of probabilities is infinite. For example, if we had a countable set of hypotheses,
we could assign them all weight 1 (so that, intuitively, they are all viewed as
equally likely), but there is no uniform measure on a countable set.
More importantly, when it comes to decision making, we use the weights
quite differently from how we would use second-order probabilities on probabilities. Second-order probabilities would let us define a probability on events (by
taking expectation) and maximize expected utility, in the usual way. Using the
weights, we instead define a novel decision rule, minimax weighted expected regret (MWER), that has some rather nice properties, which we believe will make
it widely applicable in practice. If all the weights are 1, then MWER is just
the standard minimax expected regret (MER) rule (described below). If the set
of probabilities is a singleton, then MWER agrees with (subjective) expected
utility maximization (SEU). More interestingly perhaps, if the weighted set of
measures converges to a single measure (which will happen in one important
special case, discussed below), MWER converges to SEU. Thus, the weights
give us a smooth, natural way of interpolating between MER and SEU.
In summary, weighted sets of probabilities allow us to represent ambiguity
(uncertainty about the correct probability distribution). Real individuals are
sensitive to this ambiguity when making decisions, and the MWER decision
rule takes this into account. Updating the weighted sets of probabilities using
likelihood allows the initial ambiguity to be resolved as more information about
the true distribution is obtained.
We now briefly explain MWER, by first discussing MER. MER is a probabilistic variant of the minimax regret decision rule proposed by Niehans [13]
and Savage [17]. Most likely, at some point, we’ve second-guessed ourselves
and thought “had I known this, I would have done that instead”. That is, in
hindsight, we regret not choosing the act that turned out to be optimal for
the realized state, called the ex post optimal act. The regret of an act a in a
state s is the difference (in utility) between the ex post optimal act in s and a.
Of course, typically one does not know the true state at the time of decision.
Therefore the regret of an act is the worst-case regret, taken over all states. The
minimax regret rule orders acts by their regret.
The definition of regret applies if there is no probability on states. If an
agent’s uncertainty is represented by a single probability measure, then we can
compute the expected regret of an act a: just multiply the regret of an act a
at a state s by the probability of s, and then sum. It is well known that the
order on acts induced by minimizing expected regret is identical to that induced
by maximizing expected utility (see [8] for a proof). If an agent’s uncertainty
is represented by a set P of probabilities, then we can compute the expected
regret of an act a with respect to each probability measure Pr ∈ P, and then
take the worst-case expected regret. The MER (Minimax Expected Regret) rule
orders acts according to their worst-case expected regret, preferring the act that
minimizes the worst-case regret. If the set of measures is the set of all probability
4

measures on states, then it is not hard to show that MER induces the same
order on acts as (probability-free) minimax regret. Thus, MER generalizes both
minimax regret (if P consists of all measures) and expected utility maximization
(if P consists of a single measure).
MWER further generalizes MER. If we start with a weighted set of measures,
then we can compute the weighted expected regret for each one (just multiply
the expected regret with respect to Pr by the weight of Pr) and compare acts
by their worst-case weighted expected regret.
Sarver [16] also proves a representation theorem that involves putting a
multiplicative weight on a regret quantity. However, his representation is fundamentally different from MWER. In his representation, regret is a factor only
when comparing two sets of acts; the ranking of individual acts is given by
expected utility maximization. By way of contrast, we do not compare sets of
acts.
It is standard in decision theory to axiomatize a decision rule by means of
a representation theorem. For example, Savage [18] showed that if an agent’s
preferences  satisfied several axioms, such as completeness and transitivity,
then the agent is behaving as if she is maximizing expected utility with respect
to some utility function and probabilistic belief.
If uncertainty is represented by a set of probability measures, then we can
generalize expected utility maximization to maxmin expected utility (MMEU).
MMEU compares acts by their worst-case expected utility, taken over all measures. MMEU has been axiomatized by Gilboa and Schmeidler [7]. MER was
axiomatized by Hayashi [8] and Stoye [20]. We provide an axiomatization of
MWER. We make use of ideas introduced by Stoye [20] in his axiomatization
of MER, but the extension seems quite nontrivial.
We also consider a dynamic setting, where beliefs are updated by new information. If observations are generated according to a probability measure that is
stable over time, then, as we suggested above, there is a natural way of updating
the weights given observations, using ideas of likelihood. The idea is straightforward. After receiving some information E, we update each probability Pr ∈ P
to Pr | E, and take its weight to be αPr = Pr(E)/ supPr′ ∈P Pr′ (E). If more than
one Pr ∈ P gets updated to the same Pr | E, the sup of all such weights is used.
Thus, the weight of Pr after observing E is modified by taking into account the
likelihood of observing E assuming that Pr is the true probability. We refer to
this method of updating weights as likelihood updating.
If observations are generated by a stable measure (e.g., we observe the outcomes of repeated flips of a biased coin) then, as the agent makes more and
more observations, the weighted set of probabilities of the agent will, almost
surely, look more and more like a single measure. The weight of the measures
in P closest to the measure generating the observations converges to 1, and
the weight of all other measures converges to 0. This would not be the case
if uncertainty were represented by a set of probability measures and we did
measure-by-measure updating, as is standard. As we mentioned above, this
means that MWER converges to SEU.
We provide an axiomatization for dynamic MWER with likelihood updat5

ing. We remark that a dynamic version of MMEU with measure-by-measure
updating has been axiomatized by Jaffray [10], Pires [14], and Siniscalchi [19].
Likelihood updating is somewhat similar in spirit to an updating method
implicitly proposed by Epstein and Schneider [5]. They also represented uncertainty by using (unweighted) sets of probability measures. They choose a
threshold α with 0 < α < 1, update by conditioning, and eliminate all measures
whose relative likelihood does not exceed the threshold. This approach also
has the property that, over time, all that is left in P are the measures closest
to the measure generating the observations; all other measures are eliminated.
However, it has the drawback that it introduces a new, somewhat arbitrary,
parameter α.
Chateauneuf and Faro [2] also consider weighted sets of probabilities (they
model the weights using what they call confidence functions), although they
impose more constraints on the weights than we do. They then define and
provide a representation of a generalization of MMEU using weighted sets of
probabilities that parallels our generalization of MER. Chateauneuf and Faro
do not discuss the dynamic situation; specifically, they do not consider how
weights should be updated in the light of new information.
The rest of this paper is organized as follows. Section 2 introduces the
weighted sets of probabilities representation, and Section 3 introduces the MWER
decision rule. Axiomatic characterizations of static and dynamic MWER are
provided in Sections 4 and 5, respectively. We conclude in Section 7.

2

Weighted Sets of Probabilities

A set P + of weighted probability measures on a set S consists of pairs (Pr, αPr ),
where αPr ∈ [0, 1] and Pr is a probability measure on S.1 Let P = {Pr :
∃α(Pr, α) ∈ P + }. We assume that, for each Pr ∈ P, there is exactly one α such
that (Pr, α) ∈ P + . We denote this number by αPr , and view it as the weight
of Pr. We further assume for convenience that weights have been normalized
so that there is at least one measure Pr ∈ P such that αPr = 1.2 We remark
that, just as we do, Chateaunef and Faro [2] take weights to be in the interval
[0, 1]. They impose additional requirements on the weights. For example, they
require that the weight of a convex combination of two probability measures is
at least as high as the weight of each one. This does not seem reasonable in
our applications. For example, an agent may know that one of two measures is
generating his observations, and give them both weight 1, while giving all other
distributions weight 0.
1 In this paper, for ease of exposition, we take the state space S to be finite, and assume
that all sets are measurable. We can easily generalize to arbitrary measure spaces.
2 While we could take weights to be probabilities, and normalize them so that they sum to
1, if P is finite, this runs into difficulties if we have an infinite number of measures in P. For
example, if we are tossing a coin, and P includes all probabilities on heads from 1/3 to 2/3,
using a uniform probability, we would be forced to assign each individual probability measure
a weight of 0, which would not work well in the definition of MWER.

6

As we observed in the introduction, one way of updating weighted sets of
probabilities is by using likelihood updating. We use P + | E to denote the
+
result of applying likelihood updating to P + . Define P (E) = sup{αPr Pr(E) :
′
+
Pr ∈ P}; if P (E) > 0, set αPr,E = sup{Pr′ ∈P:Pr′ |E=Pr|E} αPr′+Pr (E) . Note that
P (E)

given a measure Pr ∈ P, there may be several distinct measures Pr′ in P such
that Pr′ | E = Pr | E. Thus, we take the weight of Pr | E to be the sup of the
+
possible candidate values of αPr,E . By dividing by P (E), we guarantee that
αPr,E ∈ [0, 1], and that there is some measure Pr such that αPr,E = 1, as long as
+
+
there is some pair (αPr , Pr) ∈ P such that αPr Pr(E) = P (E). If P (E) > 0,
we take P + | E to be
{(Pr | E, αPr,E ) : Pr ∈ P}.
+

If P (E) = 0, then P + | E is undefined.
In computing P + | E, we update not just the probability measures in P, but
also their weights. The new weight combines the old weight with the likelihood.
Clearly, if all measures in P assign the same probability to the event E, then
likelihood updating and measure-by-measure updating coincide. This is not
surprising, since such an observation E does not give us information about
the relative likelihood of measures. We stress that using likelihood updating
is appropriate only if the measure generating the observations is assumed to
be stable. For example, if observations of heads and tails are generated by
coin tosses, and a coin of possibly different bias is tossed in each round, then
likelihood updating would not be appropriate.
It is well known that, when conditioning on a single probability measure,
the order that information is acquired is irrelevant; the same observation easily
extends to sets of probability measures. As we now show, it can be further
extended to weighted sets of probability measures.
Proposition 1. Likelihood updating is consistent in the sense that for all E1 , E2 ⊆
S, (P + | E1 ) | E2 = (P + | E2 ) | E1 = P + | (E1 ∩ E2 ), provided that
P + | (E1 ∩ E2 ) is defined.
Proof. By standard results, (Pr | E1 ) | E2 = (Pr | E2 ) | E1 = Pr | (E1 ∩ E2 ).
Since the weight of the measure Pr | E1 is proportional to αPr Pr(E1 ), the weight
of (Pr | E1 ) | E2 is proportional to αPr Pr(E1 ) Pr(E2 | E1 ) = αPr Pr(E1 ∩ E2 ).
Likewise, the weight of (Pr | E2 ) | E1 is proportional to αPr Pr(E2 ) Pr(E1 |
E2 ) = αPr Pr(E1 ∩ E2 ). Since, in all these cases, the sup of the weights is
normalized to 1, the weights of corresonding measures in P + | (E1 ∩ E2 ), (P + |
E1 ) | E2 and (P + | E2 ) | E1 must be equal.

3

MWER

We now define MWER formally. Given a set S of states and a set X of outcomes,
an act f (over S and X) is a function mapping S to X. For simplicity in this
paper, we take S to be finite. Associated with each outcome x ∈ X is a utility:
7

u(x) is the utility of outcome x. We call a tuple (S, X, u) a (non-probabilistic)
decision problem. To define regret, we need to assume that we are also given
a set M ⊆ X S of feasible acts, called the menu. The reason for the menu is
that, as is well known (and we will demonstrate by example shortly), regret can
depend on the menu. Moreover, we assume that every menu M has utilities
bounded from above. That is, we assume that for all menus M , supg∈M u(g(s))
is finite. This ensures that the regret of each act is well defined.3 For a menu
M and act f ∈ M , the regret of f with respect to M and decision problem
(S, X, u) in state s is


reg M (f, s) = sup u(g(s)) − u(f (s)).
g∈M

That is, the regret of f in state s (relative to menu M ) is the difference between
u(f (s)) and the highest utility possible in state s (among all the acts in M ). The
regret of f with respect to M and decision problem (S, X, u) is the worst-case
regret over all states:
max reg M (f, s).
s∈S

(S,X,u)
(f ),
reg M

and usually omit the superscript (S, X, u) if it
We denote this as
is clear from context. If there is a probability measure Pr over the states, then
we can consider the probabilistic decision problem (S, X, u, Pr). The expected
regret of f with respect to M is
X
reg M,Pr (f ) =
Pr(s)reg M (f, s).
s∈S

If there is a set P of probability measures over the states, then we consider the
P-decision problem (S, X, u, P). The maximum expected regret of f ∈ M with
respect to M and (S, X, u, P) is
!
X
Pr(s)reg M (f, s) .
reg M,P (f ) = sup
Pr∈P

s∈S

Finally, if beliefs are modeled by weighted probabilities P + , then we consider
the P + -decision problem (S, X, u, P + ). The maximum weighted expected regret
of f ∈ M with respect to M and (S, X, u, P + ) is
!
X
Pr(s)reg M (f, s) .
reg M,P + (f ) = sup αPr
Pr∈P

s∈S

The MER decision rule is thus defined for all f, g ∈ X S as
(S,X,u)

f S,X,u
M,P g iff reg M,P

(S,X,u)

(f ) ≤ reg M,P

(g).

3 Stoye [21] assumes that, for each menu M , there is a finite set A
M of acts such that M
consists of all the convex combinations of the acts in AM . Our assumption is clearly much
weaker than Stoye’s.

8

cont
back
check

1 broken cake
Payoff Regret
10,000 0
0
10,000
5,001
4,999

10 broken cakes
Payoff
Regret
-10,000 10,000
0
0
-4,999
4,999

Table 2: Payoffs and regrets for delivery example.

cont
back
check
new

1 broken cake
Payoff Regret
10,000 10,000
0
20,000
5,001
14,999
20,000 0

10 broken cakes
Payoff
Regret
-10,000 10,000
0
0
-4,999
4,999
-20,000 20,000

Table 3: Payoffs and regrets for the delivery problem with a new choice added.
That is, f is preferred to g if the maximum expected regret of f is less than that
(S,X,u)
S,X,u
of g. We can similarly define M,reg , S,X,u
M,Pr , and M,P + by replacing reg M,P
(S,X,u)

(S,X,u)

(S,X,u)

by reg M
, reg M,Pr , and reg M,P + , respectively. Again, we usually omit the
superscript (S, X, u) and subscript Pr or P + , and just write M , if it is clear
from context.
To see how these definitions work, consider the delivery example from the
introduction. There are 1, 000 states with one broken cake, and C(1000, 10)
states with ten broken cakes. The regret of each action in a state depends only
on the number of broken cakes, and is given in Table 2. It is easy to see that the
action that minimizes regret is check , with cont and back having equal regret.
If we represent uncertainty using the two probability measures Pr1 and Pr10 ,
the expected regret of each of the acts with respect to Pr1 (resp., Pr10 ) is just
its regret with respect to states with one (resp. ten) broken cakes. Thus, the
action that minimizes maximum expected regret is again check .
As we said above, the ranking of acts based on MER or MWER can change
if the menu of possible choices changes. For example, suppose that we introduce
a new choice in the delivery problem, whose gains and losses are twice those of
cont, resulting in the payoffs and regrets described in Table 3. In this new setting, cont has a lower maximum expected regret (10, 000) than check (14, 999),
so MER prefers cont over check . Thus, the introduction of a new choice can
affect the relative order of acts according to MER (and MWER), even though
other acts are preferred to the new choice. By way of contrast, the decision rules
MMEU and SEU are menu-independent ; the relative order of acts according to
MMEU and SEU is not affected by the addition of new acts.
We next consider a dynamic situation, where the agent acquires information. Specifically, in the context of the delivery problem, suppose that T800 learns E—the first 100 items are good. Initially, suppose that T-800

9

has no reason to believe that one hypothesis is more likely than the other,
so assigns both hypotheses weight 1. Note that P1 (E) = 0.9 and Pr10 (E) =
C(900, 10)/C(1000, 10) ≈ 0.35. Thus,
P + | E = {(Pr1 | E, 1), (Pr10 | E, C(900, 10)/(.9C(1000, 10))}.
We can also see from this example that MWER interpolates between MER
and expected utility maximization. Suppose that a passer-by tells T-800 that
the first N cupcakes are good. If N = 0, MWER with initial weights 1 is the
same as MER. On the other hand, if N ≥ 991, then the likelihood of Pr10 is 0,
and the only measure that has effect is Pr1 , which means minimizing maximum
weighted expected regret is just maximizing expected utility with respect to
Pr1 . If 0 < N < 991, then the likelihoods (hence weights) of Pr1 and Pr10 are
1000
9
1 and C(1000−N,10)
C(1000,10) × 1000−N < ((999 − N )/999) . Thus, as N increases, the
weight of Pr10 goes to 0, while the weight of Pr1 stays at 1.

4

An axiomatic characterization of MWER

We now provide a representation theorem for MWER. That is, we provide a
collection of properties (i.e., axioms) that hold of MWER such that a preference order on acts that satisfies these properties can be viewed as arising from
MWER. To get such an axiomatic characterization, we restrict to what is known
in the literature as the Anscombe-Aumann (AA) framework [1], where outcomes
are restricted to lotteries. This framework is standard in the decision theory
literature; axiomatic characterizations of SEU [1], MMEU [7], and MER [8, 20]
have already been obtained in the AA framework. We draw on these results to
obtain our axiomatization.
Given a set Y (which we view as consisting of prizes), a lottery over Y
is just a probability with finite support on Y . Let ∆(Y ) consist of all finite
probabilities over Y . In the AA framework, the set of outcomes has the form
∆(Y ). So now acts are functions from S to ∆(Y ). (Such acts are sometimes
called Anscombe-Aumann acts.) We can think of a lottery as modeling objective
uncertainty, while a probability on states models subjective uncertainty; thus,
in the AA framework we have both objective and subjective uncertainty. The
technical advantage of considering such a set of outcomes is that we can consider
convex combinations of acts. If f and g are acts, define the act αf + (1 − α)g
to be the act that maps a state s to the lottery αf (s) + (1 − α)g(s).
In this setting, we assume that there is a utility function U on prizes in Y .
The utility of a lottery l is just the expected utility of the prizes obtained, that
is,
X
l(y)U (y).
u(l) =
{y∈Y : l(y)>0}

This makes sense since l(y) is the probability of getting prize y if lottery l is
played. The expected utility of an act f with respect to a probability Pr is then

10

P
just u(f ) = s∈S Pr(s)u(f (s)), as usual. We also assume that there are at least
two prizes y1 and y2 in Y , with different utilities U (y1 ) and U (y2 ).
Given a set Y of prizes, a utility U on prizes, a state space S, and a set P +
S,∆(Y ),u
of preference
of weighted probabilities on S, we can define a family M,P +
orders on Anscombe-Aumann acts determined by weighted regret, one per menu
M , as discussed above, where u is the utility function on lotteries determined
S,∆(Y ),u
by U . For ease of exposition, we usually write S,Y,U
.
M,P + rather than M,P +
We state the axioms in a way that lets us clearly distinguish the axioms for
SEU, MMEU, MER, and MWER. The axioms are universally quantified over
acts f , g, and h, menus M and M ′ , and p ∈ (0, 1). We assume that f, g ∈ M
when we write f M g.4 We use l∗ to denote a constant act that maps all states
to l.
Axiom 1. (Transitivity) f M g M h ⇒ f M h.
Axiom 2. (Completeness) f M g or g M f .
Axiom 3. (Nontriviality) f ≻M g for some acts f and g and menu M .
Axiom 4. (Monotonicity) If (f (s))∗ {(f (s))∗ ,(g(s))∗ } (g(s))∗ for all s ∈ S, then
f M g.
Axiom 5. (Mixture Continuity) If f ≻M g ≻M h, then there exist q, r ∈ (0, 1)
such that
qf + (1 − q)h ≻M∪{qf +(1−q)h} g ≻M∪{rf +(1−r)h} rf + (1 − r)h.
Menu-independent versions of Axioms 1–5 are standard. Clearly (menuindependent versions of) Axioms 1, 2, 4, and 5 hold for MMEU, MER, and
SEU; Axiom 3 is assumed in all the standard axiomatizations, and is used to
get a unique representation.
Axiom 6. (Ambiguity Aversion)
f ∼M g ⇒ pf + (1 − p)g M∪{pf +(1−p)g} g.
Ambiguity Aversion says that the decision maker weakly prefers to hedge her
bets. It also holds for MMEU, MER, and SEU, and is assumed in the axiomatizations for MMEU and MER. It is not assumed for the axiomatization of SEU,
since it follows from the Independence axiom, discussed next. Independence also
holds for MWER, provided that we are careful about the menus involved. Given
a menu M and an act h, let pM + (1 − p)h be the menu {pf + (1 − p)h : p ∈ M }.
4 Stoye [21] assumed that menus were convex, so that if f, g ∈ M , then so is pf + (1 − p)g.
We do not make this assumption, although our results would still hold if we did (with the
axioms slightly modified to ensure that menus are convex). While it may seem reasonable to
think that, if f and g are feasible for an agent, then so is pf + (1 − p)g, this not always the
case. For example, it may be difficult for the agent to randomize, or it may be infeasible for
the agent to randomize with probability p for some choices of p (e.g., for p irrational).

11

Axiom 7. (Independence)
f M g iff pf + (1 − p)h pM+(1−p)h pg + (1 − p)h.
Independence holds in a strong sense for SEU, since we can ignore the menus.
The menu-independent version of Independence is easily seen to imply Ambiguity Aversion. Independence does not hold for MMEU.
Although we have menu independence for SEU and MMEU, we do not have
it for MER or MWER. The following two axioms are weakened versions of menu
independence that do hold for MER and MWER.
Axiom 8. (Menu independence for constant acts) If l∗ and (l′ )∗ are constant
acts, then l∗ M (l′ )∗ iff l∗ M ′ (l′ )∗ .
In light of this axiom, when comparing constant acts, we omit the menu.
An act h is never strictly optimal relative to M if, for all states s ∈ S, there
is some f ∈ M such that (f (s))∗  (h(s))∗ .
Axiom 9. (Independence of Never Strictly Optimal Alternatives (INA)) If every
act in M ′ is never strictly optimal relative to M , then f M g iff f M∪M ′ g.
Axiom 10. (Boundedness of menus) For every menu M , there exists a lottery
∗
l ∈ ∆(Y ) such that for all f ∈ M and s ∈ S, (f (s))∗  l .
The boundedness axiom enforces the assumption that we made earlier that every
menu has utilities that are bounded from above. Recall that this assumption is
necessary for regret to be finite.
We now present our representation theorem for MWER. Roughly, the representation theorem states that a family of preferences satisfies Axioms 1–10 if
and only if it has a MWER representation with respect to some utility function and weighted probabilities. In the representation theorem for SEU [1], not
only is the utility function unique (up to affine transformations, so that we can
replace U by aU + b, where a > 0 and b are constants), but the probability is
unique as well. Similarly, in the MMEU representation theorem of Gilboa and
Schmeidler [7], the utility function is unique, and the set of probabilities is also
unique, as long as one assume that the set is convex and closed.
To get uniqueness in the representation theorem for MWER, we need to consider a different representation of weighted probabilities. Define a sub-probability
measure p on S to be like a probability measure (i.e., a function mapping measurable subsets of S to [0, 1] such that p(T ∪ T ′ ) = p(T ) + p(T ′ ) for disjoint sets
T and T ′ ), without the requirement that p = 1. We can identify a weighted
probability distribution (Pr, α) with the sub-probability measure α Pr. (Note
that given a sub-probability measure p, there is a unique pair (α, Pr) such that
P = α Pr: we simply take α = p(S) and Pr = p/α.) A set C of sub-probability
measures is downward-closed if, whenever p ∈ C and q ≤ p, then q ∈ C. We
get a unique set of sub-probability measures in our representation theorem if
we restrict to sets that are convex, downward-closed, closed, and contain at
least one (proper) probability measure. (The latter requirement corresponds to
12

having αPr = 1 for some Pr ∈ P + .) For convenience, we will call a set regular
if it is convex, downward-closed, and closed.
We identify each set of weighted probabilities P + with the set of subprobability measures
C(P + ) = {α Pr : (Pr, αPr ) ∈ P + , 0 ≤ α ≤ αPr }.
Note that if (α, Pr) ∈ P + , then C(P + ) includes all the sub-probability measures
between the all-zero measure and αPr Pr.
We need to restrict to closed and convex sets of sub-probability measures to
get uniqueness in the representation of MWER for much the same reason that we
need to restrict to closed and convex sets to get uniqueness in the representation
of MMEU. To see why convexity is needed, consider the delivery example and
the expected regrets in Table 2, and the distribution a Pr1 +(1 − a) Pr10 , for
some a ∈ (0, 1). The weighted expected regret of any act with respect to
a Pr1 +(1 − a) Pr10 is bounded above by the maximum weighted expected regret
of that act with respect to Pr1 and Pr10 . Therefore, adding a Pr1 +(1−a) Pr10 to
P + for some weight a ∈ (0, 1) does not change the resulting family of preferences.
Similarly, we need to restrict to closed sets for uniqueness, since if we start with
a set C of sub-probability measures that is not closed, taking the closure of C
would result in the same family of preferences.
While convexity is easy to define for a set of sub-probability measures, there
seems to be no natural notion of convexity for a set P + of weighted probabilities.
Moreover, the requirement that P + is closed is different from the requirement
that C(P + ) is closed. The latter requirement seems more reasonable. For
example, fix a probability measure Pr, and let P + = {(1, Pr)} ∪ {(0, Pr′ ) : Pr′ 6=
Pr}. Thus, P + essentially consists of a single probability measure, namely Pr,
with weight 1; all the weighted probability measures (0, Pr′ ) have no impact.
This represents the uncertainty of an agent who is sure that that Pr is true
probability. Clearly P + is not closed, since we can find a sequence Prn such
that (0, Prn ) → (0, Pr), although (0, Pr) ∈
/ P + . But C(Pr+ ) is closed.
Restricting to closed, convex sets of sub-probability measures does not suffice to get uniqueness; we also need to require downward-closedness. This is so
because if p is in C, then adding any q ≤ p to the set leaves all regrets unchanged. Finally, the presence of a proper probability measure is also required,
since for any a ∈ (0, 1], scaling each element in the set C by a leaves the family
of preferences unchanged.
In summary, if we consider arbitrary sets of sub-probability measures, then
the set of sub-probability measures that represent a given family of MWER
preferences would be unique if we required the set to be regular and contain a
probability measure.
Theorem 1. For all Y , U , S, and P + , the family of preference orders S,Y,U
M,P +
satisfies Axioms 1–10. Conversely, if a family of preference orders M on
the acts in ∆(Y )S satisfies Axioms 1–10, then there exist a a utility U on Y
and a weighted set P + of probabilities on S such that C(P + ) is regular and
+
M =S,Y,U
M,P + . Moreover, U is unique up to affine transformations, and C(P )
13

is unique in the sense that if Q + represents M , and C(Q + ) is regular, then
C(Q + ) = C(P + ).
Showing that S,Y,U
M,P + satisfies Axioms 1–10 is fairly straightforward; we leave
details to the reader. The proof of the converse is quite nontrivial, although it
follows the lines of the proof of other representation theorems. We provide an
outline of the proof here; details can be found in the appendix.
Using standard techniques, we can show that the axioms guarantee the existence of a utility function U on prizes that can be extended to lotteries in the
obvious way, so that l∗  (l′ )∗ iff U (l) ≥ U (l′ ). We then use techniques of Stoye
[21] to show that it suffices to get a representation theorem for a single menu,
rather than all menus: the menu consisting of all acts f such that U (f (s)) ≤ 0
for all states s ∈ S. This allows us to use techniques in the spirit of those used
by by Gilboa and Schmeidler [6] to represent (unweighted) MMEU. However,
there are technical difficulties that arise from the fact that we do not have a
key axiom that is satisfied by MMEU: C-independence (discussed below). The
heart of the proof involves dealing with the lack of C-independence; as we said,
the details can be found in the appendix.
It is instructive to compare Theorem 1 to other representation results in
the literature. Anscombe and Aumann [1] showed that the menu-independent
versions of axioms 1–5 and 7 characterize SEU. The presence of Axiom 7 (menuindependent Independence) greatly simplifies things. Gilboa and Schmeidler [7]
showed that axioms 1–6 together with one more axiom that they call Certaintyindependence characterizes MMEU. Certainty-independence, or C-independence
for short, is a weakening of independence (which, as we observed, does not hold
for MMEU), where the act h is required to be a constant act. Since MMEU is
menu-independent, we state it in a menu-independent way.
Axiom 11. (C-Independence) If h is a constant act, then f  g iff pf + (1 −
p)h  pg + (1 − p)h.
As we observed, in general, we have Ambiguity Aversion (Axiom 6) for
regret. Betweenness [3] is a stronger notion than ambiguity aversion, which
states that if an agent is indifferent between two acts, then he must also be
indifferent among all convex combinations of these acts. While betweenness
does not hold for regret, Stoye [20] gives a weaker version that does hold. A
menu M has state-independent outcome distributions if the set L(s) = {y ∈
∆(Y ) : ∃f ∈ M, f (s) = y} is the same for all states s.
Axiom 12. If h is a constant act, and M has state-independent outcome distributions, then
h ∼M f ⇒ pf + (1 − p)h ∼M∪{pf +(1−p)h} f.
The assumption that the menu has state-independent outcome distributions
is critical in Axiom 12.
Stoye [20] shows that Axioms 1–9 together with Axiom 12 characterize
MER.5 Non-probabilistic regret (which we denote REG) can be viewed as a
5 Stoye

actually worked with choice correspondences; see Section 7.

14

cont
1
1
2 cont + 2 back
back
check

1 broken cake
Payoff Regret
10,000 0
5,000
5,000
0
10,000
5,001
4,999

10 broken cakes
Payoff
Regret
-10,000 10,000
-5,000
5,000
0
0
-4,999
4,999

Table 4: Payoffs and regrets for the delivery problem, with cont mixed with the
constant act back .

cont
1
1
2 cont + 2 back
back
check 1
check 2

1 broken cake
Payoff
Regret
10,000 0
5,000
5,000
0
10,000
-5,000
15,000
-10,000 20,000

10 broken cakes
Payoff
Regret
-10,000 20,000
-5,000
15,000
0
10,000
5,000
5,000
10,000
0

Table 5: Payoffs and regrets for the delivery problem, with state-independent
outcome distributions.
special case of MER, where P consists of all distributions. This means that it
satisfies all the axioms that MER satisfies. As Stoye [21] shows, REG is characterized by Axioms 1–9 and one additional axiom, which he calls Symmetry.
We omit the details here.
The assumption that the menu has state-independent outcome distributions
is critical in Axiom 12. For example, suppose that we change the payoffs in the
delivery problem so that cont has the same maximum expected regret as back
(10, 000). However, as seen in Table 4, 21 cont + 21 back has lower maximum expected regret (5, 000) than cont (10, 000), showing that the variant of Axiom 12
without the state-independent outcome distribution requirement does not hold.
Although Axiom 12 is sound for unweighted minimax expected regret, it is
no longer sound once we add weights. For example, suppose that we modified
the delivery problem so that all states we care about have the same outcome
distributions, as required by Axiom 12. Then the payoffs and regrets will be
those shown in Table 5. Suppose that the weights on Pr1 and Pr10 are 1 and
0.5, respectively. Then cont has the same maximum weighted expected regret as
back (10, 000). However, 21 cont + 21 back has lower maximum weighted expected
regret (7, 500) than cont, showing that Axiom 12 with weighted probabilities
does not hold.
Table 6 describes the relationship between the axioms characterizing the
decision rules.

15

Ax. 1-6,8-10
Ind
C-Ind
Ax. 12
Symmetry

SEU
X
X
X
X
X

REG
X
X

MER
X
X

X
X

X

MWER
X
X

MMEU
X
X

Table 6: Characterizing axioms for several decision rules.

5

Characterizing MWER with Likelihood Updating

We next consider a more dynamic setting, where agents learn information. For
simplicity, we assume that the information is always a subset E of the state
space. If the agent is representing her uncertainty using a set P + of weighted
probability measures, then we would expect her to update P + to some new set
Q + of weighted probability measures, and then apply MWER with uncertainty
represented byQ + . In this section, we characterize what happens in the special
case that the agent uses likelihood updating, so that Q + = (P + | E).
For this characterization, we assume that the agent has a family of preference
orders E,M indexed not just by the menu M , but by the information E. Each
preference order E,M satisfies Axioms 1–10, since the agent makes decisions
after learning E using MWER. Somewhat surprisingly, all we need is one extra
axiom for the characterization; we call this axiom MDC, for ‘menu-dependent
dynamic consistency’.
To explain the axiom, we need some notation. As usual, we take f Eh to be
the act that agrees with f on E and with h off of E; that is

f (s) if s ∈ E
f Eh(s) =
h(s) if s ∈
/ E.
In the delivery example, the act check can be thought of as (cont)E(back ),
where E is the set of states where there is only one broken cake.
Roughly speaking, MDC says that you prefer f to g once you learn E if and
only if, for any act h, you also prefer f Eh to gEh before you learn anything.
This seems reasonable, since learning that the true state was in E is conceptually
similar to knowing that none of your choices matter off of E.
To state MDC formally, we need to be careful about the menus involved.
Let M Eh = {f Eh : f ∈ M }. We can identify unconditional preferences with
preferences conditional on S; that is, we identify M with S,M . We also need
to restrict the sets E to which MDC applies. Recall that conditioning using
+
likelihood updating is undefined for an event such that P (E) = 0. That is,
αPr Pr(E) = 0 for all Pr ∈ P. As is commonly done, we capture the idea that
conditioning on E is possible using the notion of a non-null event.
Definition 1. An event E is null if, for all f, g ∈ ∆(Y )S and menus M with
16

f Eg, g ∈ M , we have f Eg ∼M g.
MDC. For all non-null events E, f E,M g iff f Eh MEh gEh for some
h ∈ M .6
The key feature of MDC is that it allows us to reduce all the conditional preference orders E,M to the unconditional order M , to which we can apply
Theorem 1.
Theorem 2. For all Y , U , S, and P + , the family of preference orders S,Y,U
P + |E,M
+

for events E such that P (E) > 0 satisfies Axioms 1–10 and MDC. Conversely, if a family of preference orders E,M on the acts in ∆(Y )S satisfies
Axioms 1–10 and MDC, then there exists a utility U on Y and a weighted
set P + of probabilities on S such that C(P + ) is regular, and for all non-null
E, E,M =S,Y,U
P + |E,M . Moreover, U is unique up to affine transformations, and
C(P + ) is unique in the sense that if Q + represents E,M , and C(Q + ) is regular, then C(Q + ) = C(P + ).
Proof. Since M =S,M satisfies Axioms 1–10, there must exist a weighted set
P + of probabilities on S and a utility function U such that f M g iff f S,Y,U
M,P +
+

g. We now show that if E is non-null, then P (E) > 0, and f E,M g iff
(S,X,u)
f M,P + |E g.
+

For the first part, it clearly is equivalent to show that if P (E) = 0, then E
+
is null. So suppose that P (E) = 0. Then αPr Pr(E) = 0 for all Pr ∈ P. This
means that αPr Pr(s) = 0 for all Pr ∈ P and s ∈ E. Thus, for all acts f and g,
reg M,P + (f Eg)P

= supPr∈P αPr P
s∈S Pr(s)reg M (f Eg, s)

αPr
= supP
M (f, s)
Pr∈P
s∈E Pr(s)reg

+ s∈E c Pr(s)reg
(g,
s)

P M
= supPr∈P αPr s∈S Pr(s)reg M (g, s)
= reg M,P + (g).
Thus, f Eg ∼M g for all acts f, g and menus M containing f Eg and g, which
means that E is null.
+
For the second part, we first show that if P (E) > 0, then for all f, h ∈ M ,
we have that
+
reg MEh,P + (f Eh) = P (E)reg M,P + |E (f ).
6 Although we do not need this fact, it is worth noting that the MWER decision rule has
the property that f Eh M Eh gEh for some act h iff f Eh M Eh gEh for all acts h. Thus,
this property follows from Axioms 1–10.

17

We proceed as follows:
=
=
=
=

reg MEh,P + (f Eh)

P
supPr∈P αPr s∈S Pr(s)reg
MEh (f EH, s)
P
supPr∈P αPr Pr(E) s∈E Pr(s | E)reg M (f, s)
P
+αPr s∈E c Pr(s)reg {h} (h, s)

P
supPr∈P αPr Pr(E) s∈E Pr(s|E)reg M (s, f ) 
P
+
supPr∈P P (E)αPr,E s∈E Pr(s|E)reg M (f, s)
[since αPr,E = sup{Pr′ ∈P:Pr′ |E=Pr|E}

=

+

αPr′ Pr′ (E)
]
+
P (E)

P (E) · reg M,P + |E (f ).

Thus, for all h ∈ M ,
reg MEh,P + (f Eh) ≤ reg MEh,P + (gEh)
+

+

iff P (E) · reg M,P + |E (f ) ≤ P (E) · reg M,P + |E (g)
iff reg M,P + |E (f ) ≤ reg M,P + |E (g).
It follows that the order induced by P + satisfies MDC.
Moreover, if 1–10 and MDC hold, then for a weighted set P + that represents
M , we have
f E,M g
iff
for some h ∈ M, f Eh MEh gEh
iff reg M,P + |E (f ) ≤ reg M,P + |E (g),
as desired.
Finally, the uniqueness of C(P + ) follows from Theorem 1, which says that
the family S,M of preferences is already sufficient to guarantee the uniqueness
of C(P + ).
Analogues of MDC have appeared in the literature before in the context of
updating preference orders. In particular, Epstein and Schneider [4] discuss a
menu-independent version of MDC, although they do not characterize updating in their framework. Sinischalchi [19] also uses an analogue of MDC in his
axiomatization of measure-by-measure updating of MMEU. Like us, he starts
with an axiomatization for unconditional preferences, and adds an axiom called
constant-act dynamic consistency (CDC), somewhat analogous to MDC, to extend the axiomatization of MMEU to deal with conditional preferences.

6

Dynamic Inconsistency

There is an important issue when one attempts to apply MWER with likelihood
updating to dynamic decision problems. If you want to execute a plan, at every
step you’ll need to stick with that plan and execute the corresponding part of
the plan. However, after following the initial steps of an ex-ante optimal plan,
18

a MWER agent may no longer wish to adhere to the plan. In such a situation, the agent is said to have dynamically inconsistent preferences. Dyanmic
inconsistency is well known to hold for regret. Indeed, as Epstein and Le Breton [4] show, dynamic inconsistency arises for any non-Bayesian approach to
decision making (i.e., any approach that does not involve maximizing expected
utility) that satisfies certain minimal assumptions. Not surprisingly, it arises
for MWER as well. In the rest of this section, we discuss the problem and some
standard approaches to dealing with it, and illustrate some subtleties that arise
in dealing with it in the context of MWER.
To understand the problem in the context of regret, consider the two-stage
decision problem of having dinner, represented as a decision tree in Figure 1.
Solid circles denote decision points, and empty circles denote points where nature reveals information to the agent. The decision tree also includes information about what states are considered possible at each node. The set of states
considered possible at the root is always the entire state space, and nature’s
actions at each nature decision point partitions the set of possible states.
{m, b}

Italian restaurant

Chinese restaurant

stirfry

plain rice

{m}
0

{b}

{m}

{b}

0

-2

3

{m}

{b}

5

-3

Figure 1: Dynamic inconsistency example.
First, you have to choose between a Chinese restaurant and an Italian restaurant. Once you have arrived at a particular restaurant, you cannot change your
mind and go to another; so in the second stage you must order something from
the menu at the chosen restaurant. Your utility is a combination of how much
you enjoy the food, and whether you get an allergic reaction. Initially, you know
that there are two possible states: you must be either allergic to MSG (state
m) or to basil (state b), but not both. Assume that all Italian foods will have
traces of basil, and Chinese stir-fry has MSG but plain rice does not. However,
you do not enjoy eating plain rice, so the utility of ordering rice is 0.
Suppose that you make decisions using the minimax regret decision rule,
viewing a plan (i.e., a strategy) as an act. A straightforward computation
19

shows that, ex ante, going to the Chinese restaurant and ordering plain rice has
the lowest regret (5). However, if you go to the Chinese restaurant, the choice of
going to the Italian restaurant is now irrelevant. If we now compute regret with
respect to the menu of the two remaining choices, then the regret of ordering
stir-fry is lower (2) than that of ordering rice (3). You thus end up ordering
the stir-fry. The plan of going to the Chinese restaurant and ordering plain rice
cannot be carried out.
More generally, dynamic consistency requires that the plan considered optimal ex ante continues to be considered optimal at any later time. As we said
earlier, Epstein and Le Breton [4] show that dynamic inconsistency will arise for
essentially all non-Bayesidan decision rules. A standard approach for dealing
with this lack of dynamic consistency in the literature is to consider ‘sophsticated’ agents, who are aware of the potential for dynamic inconsistency, and
thus use backward induction to determine the feasible plans. In the restaurant
example, a sophisticated agent believes correctly that she will prefer stir-fry over
rice, once she is at the Chinese restaurant. Therefore, she no longer considers
going to the Chinese restaurant and ordering plain rice a viable plan. The only
feasible options are going to the Italian restaurant, or having stir-fry at the
Chinese restaurant.
A subtlety arises when trying to apply backward induction to menu-dependent
decision rules: which menu do we use when comparing the viable plans? For
example, in the restaurant example, do we use the menu consisting of all three
plans, or the menu consisting of just the viable plans. It turns out not to matter
in this example—with respect to both menus, going to the Italian restaurant
minimizes regret. However, in general, the choice of menu can matter. Hayashi
[9] uses the menu of viable plans in computing for minimax expected regret
agents in optimal stopping problems, but it seems to us that both choices (and
perhaps others) can be justified.
A second subtlety that arises when considering sophisticated agents: What
choice do they make when they are indifferent between two plans? Sinischalchi
[19] axiomatizes consistent planning (with menu-independent preferences over
plans), which augments backward induction with a tie-breaking assumption.
This tie-breaking assumption in consistent planning allows an agent to commit
to a plan as long as each stage of the plan is considered to be one of the best at
each local decision node.
In order to axiomatize consistent planning, Siniscalchi must assume that the
agent has preferences that are more general than preferences over plans. Rather,
the agent must be assumed to have preferences over decision trees (such as that
in Figure 1). Plans are the special case of decision trees with no branching at
decision nodes; we can identify a decision tree with a set of plans (essentially, the
branches in the decision tree). Sophistication is captured by an axiom that says,
roughly, that the agent is indifferent between a decision tree and the same tree
with a non-optimal (based on backward-induction) plan removed. Preferences
over decision trees are similar in spirit to preferences over menus [11].
If we try to apply Siniscalchi’s approach to regret, we encounter further
difficulties. In a menu-independent setting, we can compare two decision trees
20

by comparing the best plans in each decision tree (if we identify a decision
tree with a set of plans). But once menus become relevant, we must decide
what menu to use when making this comparison. It is not clear which menu to
choose. What we really have here are menus over menus; it is not even clear how
to apply regret in this setting. Defining and axiomatizing consistent planning
in a regret-based setting remains an open problem.

7

Conclusion

We proposed an alternative belief representation using weighted sets of probabilities, and described a natural approach to updating in such a situation and a
natural approach to determining the weights. We also showed how weighted sets
of probabilities can be combined with regret to obtain a decision rule, MWER,
and provided an axiomatization that characterizes static and dynamic preferences induced by MWER.
We have considered preferences indexed by menus here. Stoye [21] used a
different framework: choice functions. A choice function maps every finite set
M of acts to a subset M ′ of M . Intuitively, the set M ′ consists of the ‘best’
acts in M . Thus, a choice function gives less information than a preference
order; it gives only the top elements of the preference order. The motivation
for working with choice functions is that an agent can reveal his most preferred
acts by choosing them when the menu is offered. In a menu-independent setting,
the agent can reveal his whole preference order; to decide if f ≻ g, it suffices
to present the agent with a choice among {f, g}. However, with regret-based
choices, the menu matters; the agent’s most preferred choice(s) when presented
with {f, g} might no longer be the most preferred choice(s) when presented with
a larger menu. Thus, a whole preference order is arguably not meaningful with
regret-based choices. Stoye [21] provides a representation theorem for MER
where the axioms are described in terms of choice functions. The axioms that
we have attributed to Stoye are actually the menu-based analogue of his axioms.
We believe that it should be possible to provide a characterization of MWER
using choice functions, although we have not yet proved this.
Finally, we briefly considered the issue of dynamic consistency and consistent
planning. As we showed, making this precise in the context of regret involves a
number of subtleties. We hope to return to this issue in future work.

A

Proof of Theorem 1

We show here that if a family of menu-dependent preferences M satisfies axioms 1-10, then M can be represented as minimizing expected regret with
respect to a set of weighted probabilities and a utility function. Since the proof
is somewhat lengthy and complicated, we split it into several steps, each in a
separate subsection.

21

A.1

Simplifying the Problem

Our proof starts in much the same way as the proof by Stoye [21] of a representation theorem for regret. Lemma 1 guarantees the existence of a utility
function U on prizes that can be extended to lotteries in the obvious way, so
that l∗  (l′ )∗ iff U (l) ≥ U (l′ ). In other words, preferences over all constant acts
are represented by the maximization of U on the corresponding lotteries that
the constant acts map to. Lemma 1 is a consequence of standard results. Our
menus are arbitrary sets of acts, as opposed to convex hulls of a finite number
of acts in [21]; Lemma 3 shows that Stoye’s technique can be adapted to work
when menus are arbitrary sets of acts. Finally, following Stoye [21], we reduce
the proof of existence of a minimax weighted regret representation for the family
M to the proof of existence of a minimax weighted regret representation for a
single menu-independent preference ordering  (Lemma 4).
Lemma 1. If Axioms 1-3, 5, 7, and 8 hold, then there exists a nonconstant
function U : X → R, unique up to positive affine transformations, such that for
all constant acts l∗ and (l′ )∗ and menus M ,
X
X
l′ (y)U (y).
l(y)U (y) ≥
l∗ M (l′ )∗ ⇔
{y: l′ (y)>0}

{y: l∗ (y)>0}

Proof. By menu independence for constant acts, the family of preferences M all
agree when restricted to constant acts. The lemma then follows from standard
results (see, e.g., [12]), since menu-independence for constant acts, combined
with independence, gives the standard independence (substitution) axiom from
expected utility theory.
P
As is commonly done, given U , we define u(l) = {y: l(y)>0} l(y)U (y). Thus,
u(l) is the expected utility of lottery l. We extend u to contsant acts by taking
u(l∗ ) = u(l). Thus, Lemma 1 says that, for all menus M , l∗  (l′ )∗ iff u(l∗ ) ≥
u(l′ ). If c is the utility of some lottery, let lc∗ be a constant lottery that u(lc∗ ) = c.
The following is now immediate. We state it as a lemma so that we can refer
to it later.
Lemma 2. u(lc∗ ) ≥ u(lc∗′ ) iff lc∗  lc∗′ ; similarly, u(lc∗ ) = u(lc∗′ ) iff lc∗ ∼ lc∗′ , and
u(lc∗ ) > u(lc∗′ ) iff lc∗ ≻ lc∗′ .
The key step in showing that we can reduce to a single menu is to show that,
roughly speaking, for each menu, there exists a menu-dependent function gM
such that u(gM (s)) = − supf ∈M u(f (s)). Stoye [21] proved a similar result, but
he assumed that all menus were obtained by taking the convex hull of a finite
set of acts. Because we allow arbitrary bounded menus, this result is not quite
true for us. For example, suppose that the range of u is (−1, ∞]. Then there
may be a menu M such that supf ∈M u(f (s)) = 5, so − supf ∈M u(f (s)) = −5.
But there is no act g such that u(g(s)) = −5, since u is bounded below by −1.
The following weakening of this result suffices for our purpose.

22

Lemma 3. There exists a utility function U such that for every menu M ,
there exists ǫ ∈ (0, 1] and constant act l∗ such that for all f, g ∈ M , f M
g ⇔ t(f ) t(M) t(g), where t has the form t(f ) = ǫf + (1 − ǫ)l∗ and t(M ) =
{t(f ) : f ∈ M }. Moreover, there exists an act gt(M) such that u(gt(M) (s)) =
− supf ∈t(M) u(f (s)) for all s ∈ S.
Proof. The nontriviality and monotonicity axioms imply there must exist prizes
x and y such that U (x) > U (y). We consider four cases.
Case 1: The range of U is bounded above and below. Then we can rescale
so that the range of U is [−1, 1]. Thus, there must be prizes x and y such that
U (x) = 1 and U (y) = −1. For all c ∈ [−1, 1], there must be a prize x′ that is
a convex combination of x and y such that u(x′ ) = c, so we can clearly define
a function gM such that, for all s ∈ S, we have u(gM (s)) = − supf ∈M u(f (s)).
Furthermore, we know that such a gM exists because it can be formed as an act
which maps each state to an appropriate lottery over the prizes x and y. More
generally, we know that an act with a certain utility profile exists if its utility
for each state is within the range of U . This fact will be used in the other cases
as well.
Thus, in this case we can take t to be the identity (i.e., ǫ = 1).
Case 2: The range of U is (−∞, ∞). Again, for all c ∈ (∞, ∞), there must
exist a prize x such that u(x) = c. Since menus are assumed to be bounded
above, we can again define the required function g and take ǫ = 1.
Case 3: The range of U is bounded above and unbounded below. Then we
can assume without loss of generality that the range is (−∞, 1], and for all c in
the range, there is a prize x such that u(x) = c. For all menus M , ǫ > 0, and
acts f, g ∈ M , by Independence, we have that
f M g ⇔ ǫf + (1 − ǫ)l1∗ ǫM+(1−ǫ)l∗1 ǫg + (1 − ǫ)l1∗ .
There exists an ǫ > 0 such that for all s ∈ S,
1 ≥ sup ǫu(f (s)) + (1 − ǫ) ≥ −1.
f ∈M

Let t(f ) = ǫf +(1−ǫ)l1∗. Clearly there exists an act gt(M) such that u(gt(M) (s)) =
− supf ∈t(M) u(f (s)) for all s ∈ S.
Case 4: The range of U is bounded below and unbounded above. By the
upper-boundedness axiom, every menu has an upper bound on its utility range.
Therefore, for every menu M , ǫ > 0, and all acts f and g in M , by Independence,
∗
∗
f M g ⇔ ǫf + (1 − ǫ)l−1
ǫM+(1−ǫ)l∗−1 ǫg + (1 − ǫ)l−1
.

There exists ǫ > 0 such that for all s ∈ S,
∗
sup ǫu(f (s)) + (1 − ǫ)u(l−1
(s)) ≤ 1.

f ∈M

∗
Let t(f ) = ǫf + (1 − ǫ)l−1
. Again, it is easy to see that gt(M) exists.

23

In light of Lemma 3, we henceforth assume that the utility function u derived
from U is such that its range is either (−∞, ∞), [−, 1, 1], (−∞, 1], or [−1, ∞).
In any case, its range always includes [−1, 1].
Before proving the key lemma, we establish some useful notation for acts
and utility acts. Given a utility act b, let fb , the act corresponding to b, be the
act such that fb (s) = lb(s) , if such an act exists. Conversely, let bf , the utility
act corresponding to the act f , be defined by taking bf (s) = u(f (s)). Note that
monotonicity implies that if fb = gb , then f ∼M g for all menus M . That is,
only utility acts matter. If c is a real, we take c∗ to be the constant utility act
such that c∗ (s) = c for all s ∈ S.
Lemma 4. Let M ∗ be the menu consisting of all acts f such that (−1)∗ ≤ bf ≤
+
0∗ . Then (U, P + ) represents M ∗ (i.e., M ∗ =S,X,U
M ∗ ,P + ) iff (U, P ) represents
M for all menus M .
Proof. Our arguments are similar in spirit to those of Stoye [21].
By Lemma 3, there exists t such that t(f ) = ǫf + (1 − ǫ)h for a constant
function h such that
f M g iff t(f ) t(M) t(g);
moreover, for this choice of t, the act gt(M) defined in Lemma 3 exists.
By Independence,
1
1
1
1
t(f ) + gt(M)  21 t(M)+ 12 gt(M ) t(g) + gt(M) .
2
2
2
2
Let M ∗ be the menu that contains all acts with utilities in [−1, 0]. By INA,
we know that for all acts f and g, and menus M for which gM is defined, we
have
1
1
1
1
f M g iff f + gM M ∗ g + gM .
2
2
2
2
t(f ) t(M) t(g) iff

This is because acts of the form 21 f + 12 gM are never strictly optimal with respect
to the menu 21 M + 12 gM . At every state there must be some act in 21 M + 21 gM
that has utility 0 (namely, the mixture that involves the act argmaxf ∈M u(f (s)).
Thus,
1
1
1
1
f M g iff t(f ) + gt(M) M ∗ t(g) + gt(M) .
2
2
2
2
Since the MWER representation also satisfies Independence and INA, we
know that for all menus M , and acts f and g in M ,
1
1
1
1
t(f ) + gt(M) S,X,U
t(g) + gt(M) .
∗ ,P +
M
2
2
2
2
Therefore, to show that M has a MWER representation with respect to
(U, P + ), it suffices to show that M ∗ has a MWER representation with respect
to (U, P + ).
S,X,U
f S,X,U
M,P + g ⇔ t(f ) t(M),P + t(g) ⇔

In the sequel, we drop the menu subscript when we refer to the family of
preferences, and just write  (to denote M ∗ ); by Lemma 4, it suffices to
consider M ∗ .
24

A.2

Defining a functional on utility acts

As we said, Stoye [20] also started his proof of a representation theorem for
MER by reducing to a single preference order M ∗ . He then noted that, the
expected regret of an act f with respect to a probability Pr and menu M ∗ is just
the negative of the expected utility of f . Thus, the worst-case expected regret
of f with respect to a set P of probability measures is the negative of the worstcase expected utility of f with respect to P. Thus, it sufficed for Stoye to show
that M ∗ had an MMEU representation, which he did by showing that M ∗
satisfied Gilboa and Schmeidler’s [6] axioms for MMEU, and then appealing to
their representation theorem.
This argument does not quite work for us, because now M ∗ does not satisfy
the C-independence axiom. (This is because our preference order M ∗ is based
on weighted regret, not regret.) However, we can get a representation theorem for weighted regret by using some of the techniques used by Gilboa and
Schmeidler to get a representation theorem for MMEU, appropriately modified
to deal with lack of C-independence. Specifically, like Gilboa and Schmeidler,
we define a functional I on utility acts such that the preference order on utility
acts is determined by their value according to I (see Lemma 6). Using I, we can
then determine the weight of each probability in ∆(S), and prove the desired
representation theorem.
Recall that u represents  on constant acts, and that only utility acts matter
to . The space of all utility acts is the Banach space B of real-valued functions
on S. Let B − be the set of nonpositive functions in B, where the function b is
nonpositive if b(s) ≤ 0 for all s ∈ S.
We now define a functional I on utility acts in B − such that for all f, g with
bf , bg ∈ B − , we have I(bf ) ≥ I(bg ) iff f  g. Let
Rf = {α′ : lα∗ ′  f }.
If 0∗ ≥ b ≥ (−1)∗ , then fb exists, and we define
I(b) = inf(Rfb ).
For the remaining b ∈ B − , we extend I by homogeneity. Let ||b|| = | mins∈S b(s)|.
Note that if b ∈ B − , then 0∗ ≥ b/||b|| ≥ (−1)∗ , so we define
I(b) = ||b||I(b/||b||).
∗
.
Lemma 5. If bf ∈ B − , then f ∼ lI(b
f)
∗
Proof. Suppose that bf ∈ B − and, by way of contradiction, that lI(b
≺ f . If
f)
∗
f ∼ l0 , then it must be the case that I(bf ) = 0, since I(bf ) ≤ 0 by definition
of inf, and f ∼ l0∗ ≻ lǫ∗ for all ǫ < 0 by Lemma 2, so I(bf ) > ǫ for all ǫ < 0.
∗
Therefore, f ∼ lI(b
. Otherwise, since bf ∈ B − , by monotonicity, we must have
f)
∗
∗
∗
l0 ≻ f , and thus l0 ≻ f ≻ lI(b
. By mixture continuity, there is some q ∈ (0, 1)
f)
∗
such that q · l0∗ + (1 − q) · lI(b
∼
l(1−q)I(bf ) ≺ f , contradicting the fact that I(b)
f)
is the greatest lower bound of Rf .

25

∗
∗
If, on the other hand, lI(b
≻ f , then lI(b
≻ f  lc∗ for some c ∈ R. If
f)
f)
f ∼ lc∗ then it must be the case that I(bf ) = c. I(bf ) ≤ c since lc∗  lc∗ , and
I(bf ) ≥ c since for all c′ < c, lc∗′ ≺ f ∼ lc∗ .
∗
Otherwise, lI(b
≻ f ≻ lc∗ , and by mixture continuity, there is some q ∈ (0, 1)
f)
∗
such that q ·lI(bf ) +(1−q)lc∗ ≻ f . Since qI(bf )+(1−q)c < I(bf ), this contradicts
the fact that I(bf ) is a lower bound of Rf . Therefore, it must be the case that
∗
lI(b
∼ f.
f)

We can now show that I has the required property.
Lemma 6. For all acts f, g such that bf , bg ∈ B − , f  g iff I(bf ) ≥ I(bg ).
∗
∗
Proof. Suppose that bf , bg ∈ B − . By Lemma 5, lI(b
∼ f and g ∼ lI(b
. Thus,
g)
f)
∗
∗
∗
∗
f  g iff lI(bf )  lI(bg ) , and by Lemma 2, lI(bf )  lI(bg ) iff I(bf ) ≥ I(bg ).

In order to invoke a standard separation result for Banach spaces, we extend
the definition of I to the Banach space B. We extend I to B by taking I(b) =
I(b− ) for b ∈ B − B − , where for all b ∈ B, b− is defined as
(
b(s), if b(s) ≤ 0,
−
b (s) =
0, if b(s) > 0.
Clearly b− ∈ B − and b = b− if b ∈ B − .
We show that the axioms guarantee that I has a number of standard properties. Since we have artificially extended I to B, our arguments require more
cases than those in [6]. (We remark that such an “artificial” extension seem unavoidable in our setting.) Moreover, we must work harder to get the result that
we want. We need different arguments from that for MMEU [6], since the preference order induced by MMEU satisfies C-independence, while our preference
order does not.
Lemma 7.

(a) If c ≤ 0, then I(c∗ ) = c.

(b) I satisfies positive homogeneity: if b ∈ B and c > 0, then I(cb) = cI(b).
(c) I is monotonic: if b, b′ ∈ B and b ≥ b′ , then I(b) ≥ I(b′ ).
(d) I is continuous: if b, b1 , b2 , . . . ∈ B, and bn → b, then I(bn ) → I(b).
(e) I is superadditive: if b, b′ ∈ B, then I(b + b′ ) ≥ I(b) + I(b′ ).
Proof. For part (a), If c is in the range of u, then it is immediate from the
defintion of I and Lemma 2 that I(c∗ ) = c. If c is not in the range of u, then
since [−1, 0] is a subset of the range of u, we must have c < −1, and by definition
of I, we have I(c∗ ) = |c|I(c∗ /|c|) = c.
For part (b), first suppose that ||b|| ≤ 1 and b ∈ B − (i.e., 0∗ ≥ b ≥ (−1)∗ ).
∗
Then there exists an act f such that bf = b. By Lemma 5, f ∼ lI(b)
. We
now need to consider the case that c ≤ 1 and c > 1 separately. If c ≤ 1, by
26

∗
Independence, cfb + (1 − c)l0∗ ∼ clI(b)
+ (1 − c)l0∗ . By Lemma 6, I(bcfb +(1−c)l∗0 ) =
I(bcl∗I(b) +(1−c)l∗0 ). It is easy to check that bcfb +(1−c)l∗0 = cb, and bcl∗I(b) + (1 −
c)l0∗ = cI(b)∗ . Thus, I(cb) = I(cI(b)∗ ). By part (a), I(cI(b)∗ ) = cI(b). Thus,
I(cb) = cI(b), as desired.
If c > 1, there are two subcases. If ||cb|| ≤ 1, since 1/c < 1, by what we have
just shown I(b) = I( 1c (cb)) = 1c I(cb). Crossmultiplying, we have that I(cb) =
cI(b), as desired. And if ||cb|| > 1, by definition, I(cb) = ||cb||I(bc/||cb||) =
c||b||I(b/||b||) (since bc/||cb|| = b/||b||). Since ||b|| ≤ 1, by what we have shows
1
I(b). Again, it follows
I(b) = I(||b||(b/||b||) = ||b||I(b/||b||), so I(b/||b||) = ||b||
that I(cb) = cI(b).
Now suppose that ||b|| > 1. Then I(b) = ||b||I(b/||b||). Again, we have two
subcases. If ||cb|| > 1, then

I(cb) = ||cb||I(cb/||cb||) = c||b||I(b/||b||) = cI(b).
And if ||cb|| ≤ 1, by what we have shown for the case ||b|| ≤ 1,
1
1
I(b) = I( (cb)) = I(cb),
c
c
so again I(cb) = cI(b).
For part (c), first note that if b, b′ ∈ B − . If ||b|| ≤ 1 and |b′ || ≤ −1, then the
acts fb and fb′ exist. Moreover, since b ≥ b′ , we must have (fb (s))∗  (fb′ )∗ (s)
for all states s ∈ S. Thus, by Monotocity, fb  fb′ . If either ||b|| > 1 or
||b′ || > 1, let n = max(||b||, ||b′ ||). Then ||b/n|| ≤ 1 and ||b′ /n|| ≤ 1. Thus,
I(b/n) ≥ I(b′ /n), by what we have just shown. By part (b), I(b) ≥ I(b′ ).
Finally, if either b ∈ B − B − or b′ ∈ B − B − , note that if b ≥ b′ , then b− ≥ (b′ )− .
By definition, I(b) = I(b− ) and I(b′ ) = I(b′ )− ; moreover, b− , (b′ )− ∈ B − . Thus,
by the argument above, I(b) ≥ I(b− ).
For part (d), note that if bn → b, then for all k, there exists nk such that
bn − (1/k)∗ ≤ bn ≤ bn + (1/k)∗ for all n ≥ nk . Moreover, by the monotonicity
of I (part (c)), we have that I(b − (1/k)∗ ) ≤ I(bn ) ≤ I(b + (1/k)∗ ). Thus, it
suffices to show that I(b − (1/k)∗ ) → I(b) and that I(b + (1/k)∗ ) → I(b).
To show that I(b − (1/k)∗ ) → I(b), we must show that for all ǫ > 0, there
exists k such that I(b − (1/k)∗ ) ≥ I(b) − ǫ. By positive homogeneity (part
(b)), we can assume without loss of generality that ||b − (1/2)∗ || ≤ 1 and that
||b|| ≤ 1. Fix ǫ > 0. If I(b − (1/2)∗ ) ≥ I(b) − ǫ, then we are done. If not, then
I(b) > I(b) − ǫ > I(b − (1/2)∗ ). Since ||b|| ≤ 1 and ||b − (1/2)∗ || ≤ 1, fb and
fb−(1/2)∗ exist. Moreover, by Lemma 6, fb ≻ f(I(b)−ǫ)∗ ≻ fb−(1/2)∗ . By mixture
continuity, for some p ∈ (0, 1), we have pfb + (1 − p)f(b−(1/2)∗ ≻ f(I(b)−ǫ)∗ . It is
easy to check that bpfb +(1−p)fb−(1/2)∗ = b − (1 − p)(1/2)∗ . Thus, by Lemma 6,
fb−(1−p)(1/2)∗  f(I(b)−ǫ)∗ , and I(b − (1 − p)1/2)∗ ) > I(b) − ǫ. Choose k such
that 1/k < (1 − p)(1/2). Then I(b − (1/k)∗ ) ≥ I(b − (1 − p)1/2)∗ ) > I(b) − ǫ,
as desired.
The argument that I(b + (1/k)∗ ) → I(b) is similar and left to the reader.
For part (e), first suppose that b, b′ ∈ B − . If ||b||, ||b− || ≤ 1, and I(b), I(b′ ) 6=
b
b′
b
b′
0, consider I(b)
and I(b
′ ) . Since I( I(b) ) = I( I(b′ ) ) = 1, it follows from Lemma 5
27

that f
p)f

b
I(b)

b′
I(b′ )

∼ f

f

b′
I(b′ )

. By Ambiguity Aversion, for all p ∈ (0, 1], pf
′

b
I(b)

+ (1 −

b
I(b)

I(b )
I(b)
b
b
b
b
. Thus, I( I(b)+I(b
′ ) I(b) + I(b)+I(b′ ) I(b′ ) ) ≥ I( I(b) ) = I( I(b′ ) ) = 1.
′

′

Hence, I(b + b′ ) ≥ I(b) + I(b′ ).
If b, b− ∈ B − and either ||b|| > 1 or ||b′ || > 1, and both I(b) 6= 0 and
′
I(b ) 6= 0, then the result easily follows by positive homogeneity (property (b)).
∗
∗
If b, b− ∈ B− and either I(b) = 0 or I(b′ ) = 0, let bn = b− n1 and b′n = b′ − n1 .
′
′
′
Clearly ||bn || > 0, ||bn || > 0, bn → b, and bn → bn . By our argument above,
I(bn + b′n ) ≥ I(bn ) + I(b′n ) for all n ≥ 1. The result now follows from continuity.
Finally, if either b ∈ B − B − or b′ ∈ B − B − , observe that
 −
= b (s) + b′− (s), if b(s) ≤ 0, b′ (s) ≤ 0



= b− (s) + b′− (s), if b(s) ≥ 0, b′ (s) ≥ 0
(b + b′ )− (s)
≥ b− (s) + b′− (s), if b(s) > 0, b′ (s) ≤ 0


 −
≥ b (s) + b′− (s), if b(s) ≤ 0, b′ (s) > 0.

Therefore, (b + b′ )− ≥ b− + b′− . Thus, I(b + b′ ) = I((b + b′ )− ) ≥ I(b− + b′− ) by
the monotonicity of I, and I(b− + b′− ) ≥ I(b− ) + I(b′− ) by superadditivity of I
on B − . Therefore, I(b + b′ ) ≥ I(b) + I(b′ ).

A.3

Defining the weights

In this section, we use I to define a weight αPr for each probability Pr ∈ ∆(S).
The heart of the proof involves showing that the resulting set P + so determined
gives us the desired representation.
Given a set P + of weighted probability measures, for b ∈ B − , define
X
NWREG(b) = inf αPr (
b(s) Pr(s)).
Pr∈P

s∈S

Note that NWREG is the negative of the weighted regret when the menu is B − .
Define
X
b(s) Pr(s).
NREG(b) = inf
Pr∈P

and

NREG Pr (b) =

X

s∈S

b(s) Pr(s) = EPr b.

s∈S

For each probability Pr ∈ ∆(S), define
αPr = sup{α ∈ R : αNREG Pr (b) ≥ I(b) for all b ∈ B − }.

(1)

Note that αPr ≥ 0 for all distributions Pr ∈ ∆(S), since 0 ≥ I(b) for b ∈ B −
(by monotonicity); and αPr ≤ 1, since NREG Pr ((−1)∗ ) = I((−1)∗ ) = −1 for
all distributions Pr. Thus, αPr ∈ [0, 1]. Moreover, it is immediate from the
definition of αPr that αPr NREG Pr (b) ≥ I(b) for all b ∈ B − . The next lemma
shows that there exists a probability Pr where we have equality.
28

Lemma 8.

(a) For some distribution Pr, we have αPr = 1.

(b) For all b ∈ B − , there exists Pr such that αPr NREG Pr (b) = I(b).
Proof. The proofs of both part (a) and (b) use a standard separation result: If
U is an open convex subset of B, and b ∈
/ U , then there is a linear functional
λ that separates U from b, that is, λ(b′ ) > λ(b) for all b′ ∈ U . We proceed as
follows
For part (a), we must show that for some Pr, for all b ∈ B − , NREG Pr (b) ≥
I(b). Since NREG Pr (b) = EPr b, it suffices to show that EPr (b) ≥ I(b) for all
b ∈ B−.
Let U = {b′ ∈ B : I(b′ ) > −1}. U is open (by continuity of I), and convex
(by positive homogeneity and superadditivity of I), and (−1)∗ ∈
/ U . Thus, there
exists a linear functional λ such that λ(b′ ) > λ((−1)∗ ) for b′ ∈ U .
We want to show that λ is a positive linear functional, that is, that λ(b) ≥ 0
if b ≥ 0∗ . Since 0∗ ∈ U , and λ(0∗ ) = 0, it follows that λ((−1)∗ ) < 0. Since λ
is linear, we can assume without loss of generality that λ((−1)∗ ) = −1. Thus,
for all b′ ∈ B − , I(b′ ) > −1 implies λ(b′ ) > −1. (The fact that I(cb′ ) = I(0∗ )
follows from the definition of I on elements in B − B −.) Suppose that c > 0 and
b′ ≥ 0∗ . From the definition of I, it follows that I(cb′ ) = I(0∗ ) = 0 > −1. So
cλ(b′ ) = λ(cb′ ) > −1, so λ(b′ ) > −1/c. Since this is true for all c > 0, it must
be the case that λ(b′ ) ≥ 0. Thus, λ is a positive functional.
Define the probability distribution Pr on S by taking Pr(s) = λ(1s ). To
see that Pr is indeed a probability distribution, note
P that since 1s ≥ 0 and λ
is positive, we must have λ(1s ) ≥ 0. Moreover, s∈S Pr(s) = λ(1∗ ) = 1. In
addition, for all b′ ∈ B, we have
X
X
λ(b′ ) =
λ(1s )b′ (s) =
Pr(s)b′ (s) = EPr (b′ ).
s∈S

s∈S

Next note that, for b ∈ B − ,
for all c < 0, if I(b) > c, then λ(b) > c.

(2)

For if I(b) > c, then I(b/|c|) > −1 by positive homogeneity, so λ(b/|c|) > −1
and λ(b) > c. The result now follows. For if b ∈ B − , then I(b) ≤ I(0∗ ) = 0 by
monotonicity. Thus, if c < I(b), then c < 0, so, by (2), λ(b) > c. Since λ(b) > c
whenever I(b) > c, it follows that EPr (b) = λ(b) ≥ I(b), as desired.
The proof of part (b) is similar to that of part (a). We want to show that,
given b ∈ B − , there exists Pr such that αPr NREG Pr (b) = I(b). First supose
that ||b|| ≤ 1. If I(b) = 0, then there must exist some s such that b(s) = 0, for
otherwise there exists c < 0 such that b ≤ c∗ , so I(b) ≤ c. If b(s) = 0, let Prs
be such that Prs (s) = 1. Then NREG Prs (b) = 0, so (b) holds in this case.
If ||b|| ≤ 1 and I(b) < 0, let U = {b′ : I(b′ ) > I(b)}. Again, U is open and
convex, and b ∈
/ U , so there exists a linear functional λ such that λ(b′ ) > λ(b) for
′
b ∈ U . Since 0∗ ∈ U and λ(0∗ ) = 0, we must have λ(b) < 0. Since (−1)∗ ≤ b,
(−1)∗ is not in U , and therefore we also have λ((−1)∗ ) < 0. Thus, we can
29

assume without loss of generality that λ((−1)∗ ) = −1, and hence λ((1)∗ ) = 1.
The same argument as above shows that λ is positive: for all c > 0 and b′ ≥ 0∗ ,
I(cb′ ) = 0 as before. Since I(b) < 0, it follows that I(cb′ ) > I(b), so cb′ ∈ U
and λ(cb′ ) > λ(b) ≥ λ((−1)∗ ) = −1. Thus, as before, for all c > 0, b′ ≥ 0∗ ,
λ(b′ ) > −1
c , so λ is a positive functional.
Therefore, λ determines a probability distribution Pr such that, for all b′ ∈
B − , we have λ(b′ ) = EPr (b′ ). This, of course, will turn out to be the desired distribution. To show this, we need to show that αPr = I(b)/NREG Pr (b). Clearly
αPr ≤ I(b)/NREG Pr (b), since if α > I(b)/NREG Pr (b), then αNREG Pr (b) <
I(b) (since NREG Pr (b) = λ(b) < 0). To show that αPr ≥ I(b)/NREG Pr b, we
must show that (I(b)/NREG Pr (b))NREG Pr (b′ ) ≥ I(b′ ) for all b′ ∈ B − . Equivalently, we must show that I(b)λ(b′ )/λ(b) ≥ I(b′ ) for all b′ ∈ B − .
Essentially the same argument used to prove (2) also shows
for all c > 0, if I(b′ ) > cI(b), then λ(b′ ) > cλ(b).
′

In particular, if I(b′ ) > cI(b), then by positive homogeneity, I(bc ) > I(b), so
b′
b′
′
c ∈ U , and λ( c ) > λ(b) and hence λ(b ) > cλ(b).
′
Thus, if I(b )/(−I(b)) > c and c < 0, then I(b′ ) > −cI(b), and hence
′
λ(b )/(−λ(b)) > c. It follows that λ(b′ )/(−λ(b)) ≥ I(b′ )/(−I(b)) for all b′ ∈ B − .
Thus, I(b)λ(b′ )/λ(b) ≥ I(b′ ) for all b′ ∈ B − , as required.
Finally, if ||b|| > 1, let b′ = b/||b||. By the argument above, there exists
a probability measure Pr such that αPr NREG Pr (b/||b||) = I(b/||b||). Since
NREG Pr (b/||b||) = NREG Pr (b)/||b||, and I(b/||b||) = I(b)/||b||, we must have
that αPr NREG Pr (b) = I(b).
We can now complete the proof of Theorem 1. By Lemma 8 and the definition of αPr , for all b ∈ B − ,
I(b) =

inf

Pr∈∆(S)

=

inf

αPr NREG(b)

Pr∈∆(S)

= sup
Pr∈P

αPr

X

−αPr

!

b(s) Pr(s)

s∈S

X

(3)

!

b(s) Pr(s) .

s∈S

Recall that, by Lemma 6, for all acts f, g such that bf , bg ∈ B − , f  g iff
I(bf ) ≥ I(bg ). Thus, f  g iff
!
!
X
X
−αPr
−αPr
sup
u(f (s)) Pr(s) ≤ sup
u(g(s)) Pr(s) .
Pr∈∆(S)

s∈S

Pr∈∆(S)

s∈S

Note that, for f ∈ M ∗ = B − , we have reg M ∗ ,Pr (f ) = sup(−u(f (s)) Pr(s), since
+
0∗ dominates all acts in M ∗ . Thus, =S,Y,U
= {(Pr, αPr : Pr ∈
M ∗ ,P + , where P

30

∆(S)}. By Lemma 4, this means (U, P + ) represents M for all menus M , as
required.
We have already observed that U is unique up to affine transformations,
so it remains to show that P + is maximal. This follows from the defini′
′ +
tion of αPr . If M =S,Y,U
M,(P ′ )+ , and (α , Pr) ∈ (P ) , then we claim that
α′ ∈ {α ∈ R : αNREG Pr (b) ≥ I(b) for all b ∈ B − }. If not, there would be
some b ∈ B − with ||b|| ≤ 12 , such that α′ NREG Pr (b) < I(b), which, by the
S,Y,U
S,Y,U
∗
∗
definition of ≺S,Y,U
M ∗ ,(P ′ )∗ , means that l−1 ≺M ∗ ,(P ′ )+ fb ≺M ∗ ,(P ′ )+ lI(b) . Recall that I(bf ) = inf{γ : lγ∗ M ∗ f }. Moreover, since ≺S,Y,U
M ∗ ,(P ′ )+ satisfies
the Mixture Continuity, there exists some p ∈ (0, 1) such that fb ≺S,Y,U
M ∗ ,(P ′ )+
S,Y,U
∗
∗
∗
pl−1
+ (1 − p)lI(b)
≺S,Y,U
M ∗ ,(P ′ )+ ≺M ∗ ,(P ′ )+ lI(b) . This contradicts the definition of
I(b). Therefore, α′ ∈ {α ∈ R : αNREG Pr (b) ≥ I(b) for all b ∈ B − }, and hence
α′ ≤ αPr .

A.4

Uniqueness of Representation

In the preceding sections, we have shown that if a family of menu-dependent
preferences M satisfies axioms 1 − 10, then M can be represented as minimizing weighted expected regret with respect to a canonical set P + of weighted
probabilities and a utility function. We now want to show uniqueness.
In this section, we show that the canonical set of weighted probabilities we
constructed, when viewed as a set of subnormal probability measures, is regular and includes at least one proper probability measure. Moreover, this set of
sub-probability measures is the only regular set that induces a family of preferences M that satisfies axioms 1 − 10. Our uniqueness result is analogous
to the uniqueness results of Gilboa and Schmeidler [7], who show that the convex, closed, and non-empty set of probability measures in their representation
theorem for MMEU is unique.
By Lemma 4, it suffices to consider the preference relation M ∗ . The argument is based on two lemmas: the first lemma says that the canonical set of
sub-probability measures is regular; and the second lemma says that a set of subprobability measures representing M ∗ that is regular and contains at least one
proper probability measure is unique. The proof of this second lemma, like the
proof of uniqueness in Gilboa and Schmeidler [7], uses a separating hyperplane
theorem to show the existence of acts on which two different representations
must ‘disagree’. However, a slightly different argument is required in our case,
since our acts in M ∗ must have utilities corresponding to nonpositive vectors in
R|S| .
Lemma 9. Let P + be the canonical set of weighted probability measures representing M ∗ . The set C(P + ) of sub-probability measures is regular.
Proof. It is useful to note that, by definition, p ∈ C(P + ) if and only if
Ep (b) ≥ I(b) for all b ∈ B −

31

(where expectation with respect to a subnormal probability measure is defined
in the obvious way).
Recall that a set is regular if it is convex, closed, and downward-closed.
We first show that C(P + ) is downward-closed. Suppose that p ∈ C(P + ) and
q ≤ p (i.e., q(s) ≤ α Pr(s) for all s ∈ S. Since p ∈ C(P + ), Ep (b) ≥ I(b)
for all b ∈ B − . Since q ≤ p and, if b ∈ cB − , we have b ≤ 0∗ , it follows that
Eq (b) ≥ Ep (b) ≥ I(b) for all b ∈ B − , and thus q ∈ C(P + ).
To see that C(P + ) is closed, let p = limn→∞ pn , where each pn ∈ C(P + ).
Since pn ∈ C(P + ) it must be the case that Epn (b) ≥ I(b) for all b ∈ B − . By
the continuity of expectation, it follows that Ep (b) ≥ I(b) for all b ∈ B − . Thus,
p ∈ C(P + ).
To show that C(P + ) is convex, suppose that p, q ∈ C(P + ). Then Ep (b) ≥
I(b) and Eq (b) ≥ I(b) for all b ∈ B − . It easily follows that for all a ∈ (0, 1),
Eap+(1−a)q (b) ≥ I(b) for all b ∈ B − . Thus, ap + (1 − a)q ∈ C(P + ).
Lemma 10. A set of sub-probability measures representing M ∗ that is regular,
and has at least one proper probability measure is unique.
Proof. Suppose for contradiction that there exists two regular sets of subnormal
probability distributions, C1 and C2 , that represent M ∗ and have at least one
proper probability measure.
First, without loss of generality, let q ∈ C2 \C1 . We actually look at an
extension of C1 that is downward-closed in each component to −∞. Let C 1 =
{p ∈ R|S| : p ≤ p′ }. Note an element p of C 1 may not be subnormal probability
measures; we do not require that p(s) ≥ 0 for all s ∈ S. Since C 1 and {q} are
closed, convex, and disjoint, and {q} is compact, the separating hyperplane
theorem [15] says that there exists θ ∈ R|S| and c ∈ R such that
θ · p > c for all p ∈ C 1 , and θ · q < c.

(4)

By scaling c appropriately, we can assume that |θ(s)| ≤ 1 for all s ∈ S. Now we
argue that it must be the case that θ(s) ≤ 0 for all s ∈ S (so that θ corresponds
to the utility profile of some act in M ∗ ). Suppose that θ(s′ ) > 0 for some s′ ∈ S.
By (4), θ · p > c for all p ∈ C 1 . However, consider p∗ ∈ C 1 defined by
(
0, if s 6= s′
p∗ (s) = −|c|
′
θ(s) , if s = s .
Clearly, θ · p∗ ≤ c, contradicting (4). Thus it must be the case that θ(s) ≤ 0 for
all s ∈ S.
Consider the θ given by the separating hyperplane theorem, and let f be
an act such that u ◦ f = θ. By continuity, f ∼M ∗ ld∗ for some constant act ld∗ .
Since C1 and C2 both represent M ∗ , and C1 and C2 both contain a proper
probability measure,
min p · (u ◦ f ) = min p · (u ◦ ld∗ ) = d = min p · (u ◦ f ).

p∈C1

p∈C1

p∈C2

32

However, by (4),
min p · (u ◦ f ) > c > min p · (u ◦ f ),
p∈C1

p∈C2

which is a contradiction.

