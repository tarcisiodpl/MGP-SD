

Irrelevance-based partial MAPs are useful
constructs for domain-independent explana­
tion using belief networks. We look at two
definitions for such partial MAPs, and prove
important properties that are useful in de­
signing algorithms for computing them effec­
tively. We make use of these properties in
modifying our standard MAP best-first algo­
rithm, so as to handle irrelevance-based par­
tial MAPs.
1

INTRODUCTION

Probabilistic explanation, finding causes for observed
facts (or evidence), is an extremely important aspect
of Artificial Intelligence in general, and probabilistic
reasoning in particular. For example, [ Charniak and
Goldman, 1988], views the understanding of stories
as finding high probability facts given the evidence as
an explanation of the natural language input text. In
automated medical dia�nosis (for example the work
of [ Cooper, 1984], and [Peng and Reggia, 1987]), one
wants to find the disease or set of diseases of highest
probability given the observed symptoms. In vision
processing, recent research formulates the problem in
terms of finding some set of objects that have the high­
est probability given the evidence (the image).
There is, however, no agreement on what should be
maximized in finding a good explanation. In fact,
Poole discussed six different schemes of probabilistic
explanation in [Poole and Provan, 1990], and even
these are not exhaustive. One of the schemes discussed
was Maximum A-Posteriori models (MAP), which was
presented in [Pearl, 1988] by the name of Maximum
Probability Explanation (MPE). A MAP is a maxi­
mum probability (given the evidence) assignment to
all the variables. We call such assignments complete
MAPs, as opposed to partial MAPs which are max­
imum probability assignments to some of the vari­
ables. MAPs are useful for finding best globally con-

sistent explanations, as argued by Pearl. In [Char­
niak and Shimony, 1990], we showed that MAPs are
useful for explanation by demonstrating that MAP ex­
planations are equivalent to complete assignment cost­
based abduction. Cost based abduction is a variant of
Hobbs' and Stickel's weighted abduction (see [Hobbs
and Stickel, 1988]), which they used for natural lan­
guage story understanding.
In the rest of this section, we will present the essence
of earlier papers: [ Shimony and Charniak, 1990] (an
algorithm for complete MAPs), and [Shimony, 1991]
(definitions of irrelevance-based assignments). The fol­
lowing sections will deal with how we modify the al­
gorithm for complete MAPs to handle partial MAPs.
We assume here, as well as in related papers, that
the world knowledge is represented as a belief network
(Bayesian network).
In [ Shimony, 1991], we proposed a new, domain­
independent method of highest likelihood explanations
called irrelevance-based (partial) MAPs. The idea is
that the standard MAP solution, that of finding the
most-probable complete model given the evidence, suf­
fers from the over specification problem (an instance of
which appears in [Pearl, 1988], and [Shimony, 1991]).
Our solution is a generalization of Pearl's idea of "cir­
cumscribing explanations". Pearl claimed that there
is no need to consider the assignment to nodes which
have no evidence coming in from below (evidential sup­
port).
In many cases the evidential support is insufficient
as a criterion for deciding which nodes are irrele­
vant, as shown in [ Shimony, 1991]. In that paper, we
defined irrelevance-based assignments as assignments
where every unassigned node is irrelevant. We then
defined our notion of explanation, irrelevance-based
MAP, as the irrelevance-based partial assignment of
highest probability (given the evidence).

Algorithms for Irrelevance-Based Partial MAPs

1.1

ery node v E S, A{•} is independent of all its ancestors
that are not inS, given ASi(v). 2

IRREL EVANCE-BASED
ASSIGNMENTS

We proceeded to give irrelevance a more formal
footing, using statistical independence as a crite­
rion for irrelevance. There were two such definitions
of irrelevance-based assignments: independence-based
partial assignments, and a-independence based partial
assignments; the second being a more general concept
that was introduced because independence-based as­
signments were too restrictive and captured the intu­
itive meaning of irrelevance only in special cases.

If v is independent of its unassigned parents, as in def­
inition 1, we say that the IB constraint holds at v.
The idea behind this definition is that the unassigned
nodes above each assigned node v should remain unas­
signed if they cannot affect v (and cannot be used to
explain v). Nodes that are not above v are never used
as an explanation of v anyway, as we stated implicitly
earlier.

We introduced the notion of independence given a
(partial) assignment1 As. We use the notation
In(a, biAs) to mean that a and b are independent
given an assignment As, where a and bare either as­
signments (assignments are used interchangeably with
sample-space events), or sets of nodes. Equivalently,
we can say that P(aiAs) = P(alb,As). The latter
constraint actually is a set of simple constraints, one
for each possible assignments to the nodes of a and b.
This is similar to Pearl's notation of I( a,S, b) stating
that a and b are independent given S, where a,S, bare
sets of variables. The difference is that the latter im­
plies the former, but not vice versa. That is because
our notion only states that independence occurs given
a particular assignment to S, whereas Pearl's notion
states that independence occurs given any assignment
to S. That is, I( a,S, b) stands for a set-wise larger set
of constraints than In(a, biAs).

Definition 2

An assignment can be seen as a set of pairs, where a
pair (v, V) means that node v is assigned the value V.
The function nodes( As) evaluates to the set of nodes
assigned (in this case S). We say that assignment A
subsumes assignment B iff A s;; B. The evidence £ is
assumed to be an assignment. We say that an assign­
ment As is evidentially supported by £ iff every node
v E S is either an evidence node or there exists a path
form v to some evidence node. Likewise, As is prop­
erly evidentially supported by £ iff every node v E S
is either an evidence node or there exists a path form
v to an evidence node that traverses only nodes in S.
Our definition of irrelevance-based assignments relies
on the directionality of belief networks, the "cause and
effect" directionality. The potential causes of a node v
are its parents, j (v ), and we do not assign (i.e. are not
interested in) variables that are irrelevant to the evi­
dence given the causes. We defined our first notion of
an irrelevance-based partial assignment formally (and
called it independence-based partial assignment):
An assignment As is an independence­
based assignment (IE assignment for short) iff for ev-

Definition 1

1 The

superscript over the assignment symbol denotes
the set of variables assigned by A. If the assignment assigns
values to all the nodes of S, we say that

s.

A is complete w.r.t.

An IE assignment As is an indepen­
dence based MAP (IE-MAP) w.r.t. to evidence £ iff
As is evidentially supported and subsumed by £1 and
there is no other IE assignment assignment eviden­
tially supported and subsumed by £ of greater probabil­
ity given the evidence.
Clearly, since As is subsumed by£, then P(£1As) = 1,
whenever P(As) f. 0. In fact, we are only interested in
MAPs that are maximal w.r.t. subsumption, because
they assign fewer variables and thus lead to "simpler"
explanations. This distinction is immaterial if the dis­
tribution of the belief network is strictly positive, be­
cause then if assignment A subsumes B (with A f. B)
then it also has a strictly greater probability.
We need to maximize P(Asl£), the posterior proba­
bility. Using Bayes rule, we can write:
(1)
Since the denominator P(£1As ) is 1 and P(£) is a
constant for all the assignments we are comparing, it is
sufficient to maximize the prior probability As, which
is much easier to compute (see section 2).
Definition 2 handles the case where the assigned vari­
ables are ezactly statistically independent of the unas­
signed variables. There remained the problem that
modifying the conditional probabilities very slightly
would have a major effect on the solution. In fact,
if the correlation factor between effects and poten­
tial causes is nearly 0, we would also want to con­
clude that these potential causes are irrelevant. In
order to achieve that, we relax the exact indepen­
dence constraint by requiring that the equality hold
only within a factor of a. That is (evaluating over all
possible assignments for a set of variables), if the max­
imum conditional probability is within a factor of a of
the minimum conditional probability, then we have a­
independence. Formally:
2

( )

We use j v to denote the set of immediate predeces­
sors of v. We omit the set-intersection operator between

sets whenever unambiguous, thus S j

( v) is the intersection

of S with the immediate predecessors of

v.

371

372

Shimony

We say that a is a-independent of b
given As, where a, b and S are sets of variables (writ­
ten In0(a, biAs) for short}, iff

Definition 3

minP(AaiAS I A b) 2': (1- a) maxP(AaiAS I A b) (2)

A'

A'

We expand the definition to include the case of a be­
ing a (possibly partial) assignment rather than a set of
variables, by substituting a for Aa in the above defi­
nition. Likewise for the case of b being an assignment.
This definition is parametric, i.e. a can vary between
0 and 1. We define a a-independent based assignment
as an assignment where each node is a-independent
of its unassigned ancestors given its assigned parents.
Formally:
An assignment As is a-independence
based iff for every v E s, In,(A{•} I r+ (v)-SIAST'•l ).3

Definition 4

Queue evidence
onto agenda

Get item of max H,

Expand "current",

the "current" item

Queue result items

The case of a = 0 reduces to the independence-based
assignment criterion. A a-independence based MAP is
defined in the same way as independence-based MAPs,
using In0 in place of In.
1.2

No

BEST-FIRST MAP ALGORITHM

We presented an algorithm for finding complete
(rather than partial) MAP assignments to belief net­
works in [Shimony and Charniak, 1990]. The algo­
rithm finds MAP assignments in linear time for belief
networks that are polytrees (when appropriate book­
keeping, not discussed here, is used). The algorithm
is potentially exponential time in the general case, as
the problem is provably NP-hard.
An agenda of states is kept (or assignments), sorted
by current probability, which is a product of all con­
ditional probabilities seen in the current expansion.
The operation of the algorithm is shown in the figure
1. An agenda item is complete iff all the variables are
assigned. Expansion consists of selecting a fringe node
(i.e. a node that has unassigned neighbors) and creat­
ing a new agenda item for each of the possible assign­
ments to neighboring nodes. The heuristic evaluation
function for an agenda item, which is an assignment
As to the set of nodes S, is the following product:
H(As)

=

IT

vEG(S)

P(A{•}IAT(•l)

(3)

where G(S) = {v lv E S /\ Vw E j (v), w E 5}, i.e.
the product is over all assigned nodes which have all
their parents assigned as well. Clearly the evaluation
function is precise for complete assignments, as the
product reduces to exactly the joint distribution of the
network in that case. H is also optimistic, because if
3We user+ to denote the non-reflexive transitive closure
ofi. Thus i+
is the set of ancestors ofv.

(v)

Figure

1:

Top Level of Algorithm for Finding MAPs

some nodes are not assigned, it essentially assumes
that their probability is 1. Thus, the evaluation func­
tion H is heuristically admissible.
The advantage of this best-first algorithm is that it can
be easily modified to produce the next-best complete
assignments in order of decreasing probability. This is
done in the following manner (see figure 1): instead of
ending with the first complete assignment, output it ,
and simply continue to loop (getting the next agenda
item).
In the following sections, we discuss properties of
independence-based and a-independence-based partial
assignments that allow us to use essentially the same
algorithm (with only local modifications) to compute
them. We then present the modifications required to
produce the IB-MAP algorithm. A formal specifica­
tion of the IB-MAP algorithm and a proof of its cor­
rectness follows. We conclude with suggestions of how
to modify the algorithm to find a-independence based
MAPs.

Algorithms for Irrelevance-Based Partial MAPs

2

IB-MAP ALGORITHM

F

We begin by informally introducing the changes re­
quired to convert the complete MAP algorithm to an
IB-MAP algorithm. We then proceed to define the
terms and the algorithm formally, and prove its cor­
rectness.
2.1

ALGORITHM MODIFICATIONS

The algorithm modifications needed to compute the
independence-based partial MAP are in checking
whether an agenda item is complete, and in the ex­
pansion of an agenda item. Completeness checking in
the modified algorithm is different in that an agenda
item may be complete even if not all variables are as­
signed. Specifically, an agenda item is complete iff it is
an independence-based (possibly partial) assignment.
The other conditions for the agenda item being an IE­
MAP are guaranteed because the evidence nodes are
assigned initially. Checking whether an assignment is
independence-based is easy, due to the following theo­
rem ( the locality theorem ) :
If As is a complete assignment to all the
nodes of subset S of a belief network B, and for every
node v E s, In(A{•}, r (v)- SIAST(•l), then As is an
independence-based partial assignment to B.

Theorem 1

The claim is essentially that if conditional indepen­
dence holds locally ( i.e. with respect to just the im­
mediate predecessors, as opposed to all ancestors, as
in the definition of independence-based partial assign­
ments ), then it also holds globally. The theorem al­
lows us to test whether an assignment is independence­
based in time linear in the size of the network, and is
thus an important theorem to use when we are con­
sidering the development of an algorithm to compute
independence-based partial MAPs. The following the­
orem allows us to compute P(As) easily:
If In(v, r (v)- SIAST(v)) holds for every
S, then the probability of the assignment is:
P(AS) = II P(A{v}IAST(v))
(4)
vES

Theorem 2

node v

E

Theorem 2 allows us to calculate P(As) in linear
time for independence-based partial assignments, as
the terms of the product are simply conditional prob­
abilities that can be read off from the conditional dis­
tribution array (or any other representation) of nodes
given their parents.
Another modification is required because, when ex­
tending a node, we may want to leave some of the par­
ents unassigned, as we will show presently. Also, only
nodes with unassigned parents are considered fringe
nodes, since we do not need to assign nodes with no
evidence nodes below them.

P(vl

some parent true)

P(vl

all parents false)

0.9

�

�

0.1

Figure 2: Expanding a Node
To take advantage of theorem 1, we precompute for
each node v a set of all the cases where partial inde­
pendence occurs. We do that in the following man­
ner. The space defined by an assignment to a node
v and some of its parents, ( where the other parents
are not assigned) , defines a hypercube 1{ of possible
value assignments. If the probability of v is the same
given any assignment in fi, then 1{ is an independence­
based hypercube. Another way to look at this is: an
independence-based hypercube is a sub-space of the
conditional distribution array ( of v given its parents)
with equal conditional probability entries. Consider,
for example the "leaky" OR node v of figure 2, where
P(v = Tiui = T) = 0.9 for 1 :S: i::; 4 is independent of
Uj, j '# i. This defines four 3-dimensional hypercubes
of "don't-care" values. We also have the 1-dimensional
hypercube where all Ui = F . When the algorithm ex­
pands v, it only assigns values to parents of which v
is not independent ( given the assignment to its other
parents ), i.e. it generates one agenda item for each of
the above independence-based hypercubes.
Naturally, since a belief net is not always a tree, some
nodes may already be assigned. Consider, for example,
figure 2. We are at the OR node v, with parents u1,
u2, u3, u4, where v has the value T, and u1 has already
been assigned F . We now have to expand all the states
of the nodes ui, the parents of v. We would, however,
like to generate as few new assignments as possible,
while guaranteeing that the IB-MAP is still reachable.
In the complete MAP case, we add the following
assignments for the nodes (u2, u3, u4):

8

{( F, F, F), ( F , F, T), ( F, T, F), ( F, T, T),

(T, F, F), (T, F, T), (T, T, F), (T, T, T)}
That is, all possible complete assignments to these
three variables. When we need to find the partial

373

374

Shimony

MAP, however, only the following 4 assignments are
added:
{(T, U, U ), (U,T, U, ) , ( U, U,T), (F, F, F)}
where U stands for "unassigned" . If a hypercube is
ruled out by a prior assignment to a parent node ( as
is the case u1 =T here), it is ignored. Otherwise, the
hypercubes are unified with the prior assignment, as
in this case, the 3-dimensional hypercubes are reduced
to 2-dimensional hypercubes by the prior assignment
of u1 = F. All the other assignments are redundant,
because they would assign values to variables that can­
not change the probability of v, and are subsumed by
the 4 assignments listed above.
Finally, to compute next-best partial assignments in
decreasing order, we perform the same simple mod­
ification as for the complete MAP algorithm: simply
continue to run, producing independence based partial
assignments. A useful termination condition is now a
probability threshold, i.e. stop producing assignments
once the probability of an assignment is below some
fraction of that of the first partial MAP produced.
2.2

FORMAL DEFINITION OF THE
ALGORITHM

We define the algorithm in terms of an input assign­
ment £, the evidence, and and output IB assignment.
We shall define an expansion operator r, and a termi­
nation condition, and show that the algorithm termi­
nates with an IE-MAP.
We assume a total ordering 0 on the nodes, such that
no node comes before its ( possibly indirect) descen­
dents. That is always possible, because belief net­
works are directed acyclic graphs ( DAGs). A fringe
node w is minimal in an assignment if it is the first
node w.r.t. the ordering 0 that has unassigned par­
ents. If w is a fringe node in an assignment, such that
the independence-based assignment condition holds at
w w.r.t. the assignment, then it is an independence­
based inactive (or just inactive, for short) fringe node.
If the latter does not hold, then it is an active fringe
node. If w is the first active node in the assignment,
it is called a minimal active fringe node. Given an as­
signment and an ordering, the minimal active fringe
node is unique. Unless otherwise specified, we shall
assume that an implicit ordering 0 is present, and de­
fine the function index :nodes( B) --+ N, the index of
a node w.r.t. 0.
An assignment A{w}uX to a node and a subset of its
parents (X �i (w)) is called a hypercube based on w. If
A{w}uX is complete w.r.t. wand X and P(A{w} lAX)
is independent of the nodes i(w)- X, that is4:
:lp 'fAV E AJ(w) -X P(A{w}IAX 1 AV )
4

Al(w)-X is the
i(w)- X

nodes

=

p

(5)

set of all complete assignments to the

then A{w}uj(w) is an independence-based hypercube
( acronym IB hypercube), and p is the conditional
probability of the hypercube.
An IB hypercube A{w}ux based on
w is maximal if there does not
ezist a different
independence-based hypercube B{w}UY based on W that
subsumes it (i.e. it is mazimal with respect to sub­
sumption).

Definition 5

The maximal IB hypercube based on w is not always
unique. Note also that a maximal IB hypercube has
the smallest set of nodes assigned. We currently as­
sume, for computation of hypercubes, that the distri­
bution is strictly positive.
If independence-based assignment A5 is
subsumed by the evidence £, but is not evidentially
supported w.r.t. £, then there ezists an independence­
based assignment AS' that subsumes As and is evi­
dentially supported w.r.t. £.

Theorem 3

Proof: By construction: we show that we can drop all
the nodes that have no evidence nodes below them
from the assignment As. Since the belief network
structure is a DAG, then so is any subgraph. Or­
der nodes of S that are not ancestors of some node
in E ( nodes in E are considered to be ancestors here)
in a list such that no node precedes its descendents.
Now, proceed to eliminate nodes from the list ( and
from the assignment), in order of the elements of S.
As each node is eliminated, the assignment remains
independence-based, as only nodes with no children
are eliminated, and the independence-based assign­
ment criterion for each node depends only on ances­
tor nodes. We can thus eliminate the entire list, and
remain with an assignment that is evidentially sup­
ported, is still subsumed by £, and is independence­
based. Q.E.D.
If As is an independence-based assign­
ment that is subsumed by £, then there exists an
independence-based assignment As' that subsumes As
and is properly evidentially supported w.r.t. £.

Theorem 4

Proof: By construction: we show that we can delete
from the assignment As, all the nodes that have no ev­
idence nodes below them, as well as all nodes for which
no path to an evidence node ( that traverses only nodes
inS) exists. Remove from the assignment As all nodes
that are not ancestors of E as in the proof of theorem
3. Then, remove all the nodesT that have no path to
a node in E that lies entirely inS, in a similar manner:
sort the nodes of T into a list such that no node pre­
cedes its descendents. Removing the nodes of T will
achieve a properly evidentially supported assignment,
if we preserve the independence-based assignment con­
dition. But removing the nodes ofT in sequence will
always preserve the criterion, because no node v is re­
moved if it has children in the resulting assignment ( if

Algorithms for Irrelevance-Based Partial MAPs

it did, then the node 11 would not have been inT, as
there would be a path from 11 to a node in E). Q.E.D.
Theorems 3 and 4 are further support for our intuition
for requiring that IB-MAPs be properly evidentially
supported.
Let Ap be the set of all possible (either partial or com­
plete) assi§nments. We define our expansion operator
T : Ap U 2 • -> 2.A•, as follows:
r(S) consists of ezactly the assignments
As that obey the following conditions:

Definition 6
•

If SE Ap, then S subsumes As and there
ists a fringe node w E S and a mazimal IB
percube B{"'}ux (based on w, where ezactly
nodes X <;j (w) are assigned), such that both
following conditions hold:

ez­
hy­
the
the

S = nodes(S) U X
2. As= S U H{w}uX
1.

•

If SE 2.A•, then ezists an assignment As' E S
that subsumes As, such that ezist a fringe node
w E S' and a mazimal IB hypercube B{w}uX
(based on w, where ezactly the nodes X <;j (w)
are assigned), such that both the following condi­
tions hold:
1. S = S' UX
2. As= As' U H {w}uX

In both cases, the hypercube should not contradict the
assignment to variables that are already assigned.
We say that w is the fringe node expanded by r. Essen­
tially, applying the r operator is equivalent to taking
all the IB-hypercubes at a certain node w, and creat­
ing a new assignment for each hypercube. The new
assignment is a union of the old assignment and the
hypercube.
If assignment As is r-reachable from £,
then it is properly evidentially supported by £.

Theorem 5

Proof: By induction on the number of applications of
the tau operator. The theorem clearly holds for 0 ap­
plications, as the only assignment in that case is {£},
which is clearly properly evidentially supported. Now,
assuming that the theorem holds for n applications of
r, then another application ofT can only assign values
to nodes that are in some IB-hypercube based on a
node w already assigned. IB-hypercubes assign only
direct parents of w, and w is either in E or there ex­
ists a path from w to E passing only through assigned
nodes, by the induction hypothesis. Hence, there will
always by a path from the nodes assigned in the n + 1
application of r to E. The theorem follows by induc­
tion. Q.E.D.
An assignment As is IB-terminated when each as­
signed node w E S either has no parents, or the IB

condition holds at w. The latter is true iff the as­
signment for every wE S, A{w}uST( v) is subsumed by
some IB hypercube based on w.
Theorem 6

Every mazimal (w.r.t. subsumption)
independence-based assignment As that is properly ev­
identially supported w.r.t. £ is r-reachable from £.

Proof outline: We show that there exists a sequence
of assignments As� of sufficient length, such that each
assignment subsumes As, As� E r(As�_, ) , and if 11k
is the node expanded by r, then all the nodes 11;, i :-:::;
k , are assigned exactly as in As. Thus, for some k ,
As� =As. The proof of this theorem shows that it is
actually sufficient to expand only the minimal fringe
node at each state, rather than all fringe nodes.
Using an agenda S (a set of states, or assign­
ments), evaluation function H, evidence £ (where
E = nodes( £)) and expansion operator T, the algo­
rithm is defined formally as follows:
1. SetS={£}, and i= minvEE index(v).
2. Set As to be a member of S of maximum H(As),
and remove it from S.
3. If As, is IE-terminated, halt (As is an IB-MAP).
4. SetS =(S U r(As))- As, and go to step 2.
The evaluation function H is similar the one for the
complete MAP algorithm. The only difference is that
the (conditional) probability of a node 11 is included in
the product if the IB condition holds at v, as well as
when all its parents are assigned:
H(As) =

II

vEG(S)

P(A{"}IAT("))

(6)

{ v l 11E S Aln(A{ v}, j(11)- SIAST(v))}
U {11!11E S !\ VwE j(11), wE S}
H is obviously optimistic, and because of theorem 2,
it is exact for IB assignments (the goal states). As
the algorithm is implemented, H is actually computed
before adding an assignment to the agenda, and the
agenda is always kept sorted (e.g. using a heap). We
now show that the algorithm is correct.
G(S)

Theorem 7

The IB-MAP algorithm terminates, and
when it halts it does so with As being the most-probable
IB assignment that is properly evidentially supported
and subsumed by £.
Proof: The algorithm terminates, because the num­
ber of states added to the agenda in step 3 is finite,
and since it always adds nodes to each assignment
As, it will eventually assign all the nodes above E,
in which case the IB condition is vacuously true. Nat­
urally, the runtime may be exponential. The assign­
ment found when the algorithm terminates is IB (that

375

376

Shimony

P(X)=0.1

P(Z)

=

algorithm, because of theorem 6.

0.1

3

P(BI XZ)=P(B I YZ)

=

I

P(BI other combo)=0
P(A I XY)

=

P(AI XZ)

=

P(A I other combo)= 0

I

P(EI AB)= I
P(EI other combo)=0

Ordering 0=(E. A. B,Y, X, 7:)
Minimal assignment: [E,A,B,X, Z}
Expanded node:

Assignment (one possiblity)

E

[E,A,B}

A

[E,A,B,X, Y]

B

[E. A,B,X, Y,

8-IB MAP ALGORITHM

In the case of 6-independence based MAPs, we use es­
sentially the same algorithm again, where we need to
pre-compute 6-independence hypercubes, rather than
independence hypercubes, as in the previous case.
However, once that is done, we can again employ local
checking:
If As is a complete assignment to all
the nodes of subset S of a belief network B, and for
every node v E S, In0(A{•}, j ( v ) - SJAST(v)) (for
0 � 6 � 1}, then As is a 6-independence-based partial
assignment to B.

Theorem 8

Computing the exact probability of a 6-independence
based partial assignment seems to be hard ( since we
cannot use theorem 2, and would need to find poste­
rior probabilities of non-root nodes ) , but the following
easily computable bound inequalities are always true:

(7)

Z]

Figure 3: How can a non-maximal assignment occur?

is the termination condition) . It is properly eviden­
tially supported ( from theorem 5) and the fact that
all assignments generated are T accessible from E. The
evaluation function admissible, and all possible max­
imal properly evidentially supported IB assignments
are T-accessible. The theorem follows from the latter
two properties, and from the correctness condition of
heuristic search w.r.t. evaluation functions. Q.E.D.
Continuing to run the algorithm after finding a first
assignment will find next-best IB-assignments, in de­
creasing order of probability. Note that theorem 7
does not guarantee a mazimal ( w.r.t. subsumption)
IB-MAP. In fact, figure 3 shows a simple counterex­
ample, where all the nodes are binary, E is the evi­
dence node, and is known to be true. Given the set
of agenda states shown, the non-maximal assignment,
{E, A, B, X, Y, Z} is reached. The latter assignment is
not maximal w.r.t. subsumption, because the assign­
ment {E, A, B, X, Z} subsumes it, and is both IB and
properly evidentially supported.
However, for positive distributions, subsumption also
implies a higher probability, which guarantees that the
IB-MAP found is indeed maximal. For other distribu­
tions, to find the maximal IB-MAPs, we need to com­
pare all IB-MAPs with equal probability, which is not
hard in most cases. We are assured that the maximal
IB-MAP will indeed appear if we continue to run the

P(AS)

>
-

II _4UT(•)
min P(A{ v}JAS ' AUT(v))

vES
These bounds get better as 6 approaches
ratio is at least (1- 6) 1SI.

0,

as their

Since getting the exact probability is hard, but the
upper and lower bounds above ( denoted U(A)s and
£(A)s respectively ) are easily computable, a post­
processing step may be needed, to select the most­
probable assignment from a number of possible candi­
dates. During the first part of the algorithm, the as­
signments are sorted in the agenda according to U(A).
We need to collect the set of assignments F such that
for all assignments A not in F, U(A) is smaller than
£(A)', for some A' in F. This assures us that the
most-probable assignment is indeed in F. Hopefully,
F is a small set ( as indeed it will prove to be in almost
all cases where one explanation clearly stands out ) .
Then, we evaluate the exact probability of the assign­
ments in F in parallel by adding AND nodes for all of
them and evaluating the diagram exactly once. Natu­
rally, ifF happens to contain only one assignment, we
do not need to post-process the results.
4

FUTURE WORK

The locality property of IB assignments and 6-IB as­
signments, i.e. the fact that local testing is sufficient to
determine whether an assignment is an IB assignment,
together with a quick way of computing its probabil­
ity, makes it possible, in principle, to use other types

Algorithms for Irrelevance-Based Partial MAPs

of algorithm. Future research will determine whether
random simulation techniques will prove useful.
Another possibility is to reduce IB-MAP computation
to complete MAP computation (on a different belief
network). That will allow us to use any complete MAP
algorithm, such as belief updating ([Pearl, 1988]), to
be used. This may prove to be faster than our algo­
rithms for networks with a small maximal clique size
(for using clustering), or a small cutset size (for using
conditioning). The algorithm presented in [Santos Jr.,
1991a] and [Santos Jr., 1991b] may also prove useful
for our purposes, if it can be easily extended to work
on multiple-valued nodes.
5

SUMMARY

We introduced irrelevance-based partial MAPs in or­
der to solve the overspecification problem inherent in
complete MAP explanation. We defined two methods
for defining what nodes are irrelevant, one based on
exact independence, the other based on approximate
independence. We then discussed properties of the re­
sulting partial MAPs that allowed us to transform an
existing best-first search for complete MAPs into one
that computes irrelevance-based partial MAPs. The
modifications required were minimal.
For independence-based assignments, we showed that
we can check whether an assignment is independence
based using only local information, (i.e. they are "lo­
cally recognizable" ). Computation of the probability
of such an assignment is also easy, and can be done us­
ing only lSI conditional probability array entries. We
showed how to adapt our best-first MAP algorithm to
compute IB-MAPs, and proved the correctness of the
resulting algorithm.
a-independence-based assignments were shown to be
locally recognizable. Computing the exact probability
is hard, but good, easily computable bounds are avail­
able. It is possible to compute the exact probabilities
in parallel, using only one belief-network evaluation
(with extra nodes).
We have implemented algorithms for finding IB-MAPs
and for finding oiB-MAPs, and their running time
seems roughly comparable to the running time of our
complete MAP algorithm running on the same net­
works. That is not surprising, as the additional work
required for expansion and for completion testing in
small, and the number of states expanded is usually
smaller than for complete MAPs.
A ckn ow led gement s

This work has been supported in part by the Na­
tional Science Foundation under grants IST 8416034
and IST 8515005 and Office of Naval Research under
grant N00014-79-C-0529. The author is funded by a

Corinna Borden Keen Fellowship. Special thanks to
Eugene Charniak for he! pful suggestions and for re­
viewing drafts of the paper.
