
We describe how we selectively reformulate
portions of a belief network that pose difficul­
ties for solution with a stochastic-simulation
algorithm. With employ the selective con·
ditioning approach to target specific nodes
in a belief network for decomposition, based
on the contribution the nodes make to the
tractability of stochastic simulation. We re­
view previous work on BNRAS algorithms­
randomized approximation algorithms for
probabilistic inference. We show how selec­
tive conditioning can be employed to refor­
mulate a single BNRAS problem into multiple
tractable BNRAS simulation problems. We
discuss how we can use another simulation
algorithm-logic sampling-to solve a com­
ponent of the inference problem that provides
a means for knitting the solutions of individ­
ual subproblems into a final result. Finally,
we analyze tradeoffs among the computa­
tional subtasks associated with the selective­
conditioning approach to reformulation.

. 1

INTRODUCTION

We have developed a method for identifying and refor­
mulating variables in a belief network to maximize the
efficiency of probablistic inference with a stochastic­
simulation algorithm. The approach is based on the se­
lection of nodes for decomposition through condition­
ing by considering how the decomposition will affect
inference efficiency. Although we focus on simulation­
based inference with belief networks, we believe our
approach has application to the solution of other diffi­
cult computational problems by providing a method­
ology for intelligently decomposing the most difficult
components of a problem instance, and for directing
subproblems to the most suitable solution procedures.
We shall first describe BNRAS algorithms for proba­
bilistic inference. These include the BNRAS algorithm
by Chavez and Cooper [3, 2], 'D-BNRAS by Dagum and

Chavez [6], and CS-BNRAS by Dagum et al. [9]. We
shall discuss how we can parameterize the runtime of
these algorithms in terms of a parameter 'D, a func­
tion of the dependency structure and the conditional
probabilities of a belief network. We shall then show
how we can decrease the maximal
associated with
a belief network by targeting nodes of a network that
contribute significantly to the value of 'D for decompo­
sition through conditioning.

V

Selective conditioning decomposes complex portions
of a belief network into subproblems that can be
solved efficiently with BNRAS algorithms. Solving the
global inference problem requires taking a weighted
sum of the results of the subproblem inferences. Thus,
the final solution requires a method for computing
the weights on each subproblem.
Although infer­
ence within subproblems is amenable to approxima­
tion with BNRAS, the algorithm cannot be used to effi­
ciently compute the weights on subproblems. We show
that the weights are ideally suited for efficient approx­
imation by a different simulation algorithm: we apply
a modification of Henrion's logic sampling [15) for this
task. Our modification of logic sampling employs a
Dirichlet stopping rule [8] that allows us to generate
the needed probability distribution over conditioned
nodes in optimal time .

2

RELATED WORK

The method of conditioning was introduced by Pearl
for decomposing multiply connected belief networks
into a set of singly connected belief network problems
through identification of the cutset [20). In our work,
we do not seek to identify and instantiate a cutset to
completely decompose a multiply connected network.
Rather we employ selective conditioning to identify
portions of a belief network which pose the most diffi­
cult problems to solution with a simulation algorithm.
Through selective conditioning, we compose a refor­
mulation search space of subsets of multiply connected
nodes, and seek to choose a set of nodes to decompose
the network most effectively.
In related work on reformulation, Cooper and Chin

50

Dagum and

Horvitz

have examined the reformulation of a belief network
through Bayesian arc-reversal in an attempt to eradi­
cate small-valued conditional probabilities in the net­
work (4]. Breese and Horvitz examined the ideal trade­
off in reformulation versus solution-execution effort in
searching for cutsets for application of the method of
conditioning [1]. In other work, Horvitz posed the
use of selective conditioning as a. means for topologi­
cally editing belief network-problem instances into sets
of singly and multiply connected belief-network sub­
problems for analysis by combinations of algorithms,
each best suited to the alternative subproblems [16].
Horvitz et al. employed the principles of cutset condi­
tioning to develop a flexible inference algorithm that
allows for varying amounts of incompleteness in con­
ditioning [17]. Suermondt et al. describe the value of
combining conditioning with the clique-tree method­
ology of Lauritzen and Spiegelhalter [18] for solving
problems with special nodes (e.g., disease nodes in
medical belief networks) that play a role of primary
causation, as ancestor to almost all other nodes in the
belief network [22].

3

RANDOMIZED
APPROXIMATION SCHEMES

The BNRAS algorithms, including BNRAS, V-BNRAS,
and cs-BNRAS, represent a family of algorithms that
provide approximations to probabilistic inferences sat­
isfying Equation 1. The operation of BNRAS algo­
rithms can be decoupled into a trial-generation p hase
and a scoring phase. The trial-generation phase gener­
ates belief network instantiations consistent with the
observed evidence. Thus, for unobserved nodes Z and
evidence E, the instantiation Z = z is generated with
probability Pr[Z = z]E]. If we desire to approximate
the inference Pr(X = x]E], the scoring phase com­
putes the fraction of trials that produce instantiations
consistent with the inference Pr[X =x]E].
Dagum and Chavez showed that the efficiency of BN­
algorithms is independent of the inference query,
but is critically dependent on the efficiency of the trial­
generation phase [6]. In addition they showed that the
efficiency of the trial-generation phase depends on the
dependence value, a n easily computable quantity of the
belief network.
RAS

5

We shall use B to denote a binary-valued belief net­
work on n nodes {Xt. . ..,Xn}· For any node Xi. and
parents ux;, a belief network specifies a conditional
probability function Pr[X;IuxJ The full joint prob­
ability distribution specified by a belief network can
be calculated by taking the product of the conditional
probabilities. Thus,
n

II Pr[Xdux.J.

Pr[Xt, ..., Xn] =

i=l

Probabilistic inference in belief networks refers to the
computation of Pr[X = xlE], for some set of nodes X
instantiated to x and conditioned on evidence E.
Randomized approximation schemes (RAS) for prob­
abilistic inference [3, 6] are a class of stochastic­
simulation algorithms. Simulation procedures for in­
ference estimate the value of an exact result by deter­
mining the fraction of successes of a Bernoulli process.
Let ¢ denote the value of Pr[X = xlE]. Stochastic
simulation algorithms for probabilistic inference pro­
vide an estimate JJ of ¢. Beyond randomized approx­
imation schemes, simulation algorithms include logic
sampling [15], straight simulation [19], and likelihood
weighting [21, 13].
A simulation algorithm is a randomized approximation
sc heme if, on input parameters f and 6, the algorithm
outputs an estimate JJ that satisfies

(1)

RAS ALGORI THMS FOR
INFERENCE

4

DEPENDENCE VALUE OF
BELIEF NE TWORKS

Dagum and Chavez (6], parameterize belief networks
by their dependence value, VE � 1. The dependence
value of a belief network depends on the evidence E
that has been observed. The dependence value pro­
vides a measure of the cumulative strength of the
dependencies among nodes in a belief network that
are encoded by the conditional probabilities associated
with each node.
For each node X;, we define l; and u; as the greatest
and smallest numbers, respectively, such that, for in­
stantiation x; of X;, and for all instantiations of the
nodes in ux, that are not evidence nodes,

(2)

l; � Pr[xdux.] � u;.

It follows that
(1- u; ) �

x;

Pr[x;luxJ �

(1 -1; ) ,

(3)

denotes 1 - x;. Note that l; > 0 and u; < 1,
where
since we are assuming that no complete instantiation
of the network has zero probability. If X; is not an
evidence node, then we define A; =max
X; is an evidence node, and X;=
X;

=

when

x; then A;

ux,

=

t=�·, .

x;,

( ¥:-• f=�·,). If

then A; = ¥:"· If

When X; is a prior node, or

contains only evidence nodes, then A;

Definition For a belief network
value is given by

n

=

1.

B, the dependence

Reformulating Inference Problems Through Selective Conditioning

By definition, 'DE 2: 1. The trivial case where Ve = 1
occurs when the variables representing the nodes of
the belief network are all mutually independent; that
is, the belief network does not contain any arcs.
6

DEPENDENCE VALUE AND
TRACTABILI TY

The time required to approximate an inference with V­
or with CS-BNRAS, is the product of V� and a
polynomial in the number of nodes in the belief net­
work. Thus, the dependence value is a measure of the
tractability of approximation, where increases in Ve
render approximations more intractable. The depen­
dence value of a belief network is dominated by bounds
on the conditional probabilities, given by Equations 2
and 3, that are close to 0 and 1. When the number of
observed nodes E increases, the bounds on the condi­
tional probabilities move away from 0 and 1. Thus,
with increasing evidence, the dependence value VE
decreases, and approximations that are otherwise in­
tractable are rendered tractable.
BNRAS,

7

PROBLEM REFORMULATION

We show how approximation of probabilistic inference
for belief network problem instances with large depen­
dence values can be reformulated into the approxima­
tion of a set of inference problems with small depen­
dence values.
In the preceding section we observed that large evi­
dence sets resulted typically in small dependence val­
ues. We achieve the greatest reduction of the depen­
dence value when we instantiate selectively the par­
ents of the nodes with the largest AiS. However, when
we instantiate nodes, we change the inference that is
approximated by the BNRAS algorithm. For example,
suppose we wish to approximate Pr[XIEJ, but, because
the value of VE is too large for tractable approxima­
tion, we are led to instantiate nodes B'. The resultant
dependence value, VE ,'il, allows tractable approxima­
tion, but of the inference Pr[XIE, <;.$]. We can recover
the correct inference Pr[XIE] if we observe that
Pr[XIE]

=

2: Pr[XIE,<;S] Pr[<;SIE],

(4)

Pr[X IEJ. We can approximate Pr[X, E] - and simi­
larly, Pr[E]
if we observe that
-

Pr[X, E ] =

I: Pr(X, E!B'] Pr(<;S].

(5)

'il

The choice of the nodes in <;)< guarantees that the in­
ferences Pr[X, EI<;SJ are approximated readily using a
BNRAS algorithm. However, use of Equation 5 poses
two problems. First, to evaluate the sum requires us
to approximate 21131 inferences, where PI denotes the
number of nodes in <;.$. Thus, crucial to the success
of problem reformulation is the existence of small sets
B' that, when instantiated, effectively reduce the de­
pendence value. The second challenge we encounter in
Equation 4 is the efficient approximation of inferences
Pr[<;S]. We cannot use a BNRAS algorithm because the
dependence value for the case of no evidence is at least
as large as VE, and by assumption, the size of 'DE pro­
hibits tractable approximations.
8

DIRICHLE T DISTRIBU TIONS

In other work [7, 8}, we exploited the conjugate rela­
tion between multinomial distributions and Dirichlet
distributions to derive stopping rules for multinomial
stochastic processes that appear in stochastic simula­
tion algorithms. We give a brief review of the material
in [8], and we explore how the stopping rules can be
employed to generate approximations to the probabil­
ity distribution Pr[<;S].
We simulate the belief network using logic sampling.
The output of each trial is a complete instantiation
x1, .. . , Xn of the nodes generated with probability dis­
tribution Pr[x1, , xn]· Let I; denote the ith instanti­
ation of the nodes in <;)< and let cPi = Pr[I;]. Consider
the stochastic process generated by the random vari­
able ( = ((Ii) whose outcome must belong to one of
the I< = 21131 mutually exclusive and exhaustive cate­
gories that label all possible instantiations of <;.$. The
probability that the outcome belongs to the ith cate­
gory is given by c/J; i= 1, ... , K. Assume we observe N
outcomes of (. Let ni, i = 1, ... , K, denote the num­
ber of these outcomes that belong to the ith category.
The random vector ii= (n1, .. ., nK) has a multinomial
distribution with parameters N and ¢= {cPb ..., cPK ),
that is,
.• .

where the summation is over all instantiations of<;.$.
The problem reformulation by conditioning requires
that we pose the inference problem Pr[XIE] as the two
inference problems Pr[X, E] and Pr[E]. From Bayes'
Rule we can express
Pr[X, E]
P r[X IE]=
Pr[E]
Furthermore, a property of RAS algorithms is that the
ratio of estimates that satisfy Equation 1 for Pr[X, E]
and for Pr(E], is an estimate of
that also sat­
isfies Equation 1
and is, therefore, an estimate of

P�(J£jl

�

For a random vector with a multinomial distribution,
the conjugate distribution is provided by a Dirichlet
distribution. Thus, for the preceding example, let Jli =
1f, i = 1, . . . , I<, and j1= (J.tl, ... , JLK). The distribution
of ¢, having observed j1 and N, is given by the Dirichlet
distribution with parameters j1 and N,
(N- 1)!
6(¢li1, N)
(JLlN -1)!· ( JLK N -1)!
A.,IJ.IN-1 . A,IJ.KN-1
X '1'1
(6)
. "'f'K
·

·

·

51

52

Dagum and Horvitz

For i= 1, . .. , K, the Dirichlet distribution mean of tPi
is J.li, and the variance of tPi is,
V6(tPi)=

1

N +1

J.li( 1- J.li ).

The Dirichlet distribution tells us how $is distributed
as a function of the sample size N and the estimate j1.
If the prior distribution of$is given by a Dirichlet dis­
tribution then, because the Dirichlet is conjugate with
respect to sampling from a multinomial distribution,
the posterior distribution of $after sampling is also a
Dirichlet distribution (e.g., see [11]).
9

DIRICHLET STOPPING RULES

The distribution j1 is computed from the outcomes of
N instantiations generated by logic sampling. For ex­
ample, J.li is the fraction of outcomes which instantiate
the nodes �to It. The Law of Large Numbers guar­
antees that in the limit of infinite N, j1 converges to $
- or equivalently, to Pr[�]. For finite N, j1 is only an
approximation of the distribution of i, We consider
j1 to be a satisfactory approximation of $ if, for all
i, J.li and tPi satisfy Equation 1. We use the Dirichlet
distribution to establish a stopping rule for the num­
ber of outcomes N required for j1 to be a satisfactory
approximation of $.
Given $, the distribution of j1 after observing N out­
comes of (is given by multinomial distribution. How­
ever, since we do not know J and we do know ji, we
would like to have a distribution for $given j1 after ob­
serving N outcomes. We assume that the distribution
of $prior to making any observations on the outcome
of ( has a Dirichlet distribution-we consider the im­
plications of this assumption in Section 10. Then, by
the conjugate nature of the Dirichlet distribution, the
distribution of J after observing an outcome of ( is
also a Dirichlet distribution. In particular, using the
unbiased-Dirichlet prior 6($10, 0) to represent the prior
distribution on$, $has distribution 6(ilfi, N) after N
experiments.
Equation 1 is equivalent to

Pr(ji(1 + t:)-1 � $ � jt(1 +c)]> 1-6.

(7)

Because $is described by a Dirichlet distribution, the
probability term in Equation 7 is given by the cumula­
tive mass of 6($1;1, N) that lies inside the convex poly­
tope defined by the following set of equations:

i
J

!::

;1(1 + t:')-1

(8)

ji(1 + /).
(9)
Conversely, the failure probability 6 is given by the
cumulative mass that lies outside the convex polytope,
�

{
6($1;1, N)d$
Jo'5.i<ii(1 +e')-•

(10)

Equation 10 allows us to formulate a general prob­
abilistic stopping rule for stochastic simulation alg�
rithms. To achieve an estimate jt of J that satisfies
Equation 1, the stochastic simulation algorithm stops
when the left side of Equation 10, evaluated at the
current N and ji, is less than or equal to the input 6.
Details of this analysis can be found in [8].
10

STRUCTURE AND EFFECTS
OF PRIOR PROBABILITIES

Let us consider the knowledge that an agent has about
J prior to observing the outcome. Possible values
of $ must lie in the K -cube [0, 1]K. Before experi­
mentation, an agent might believe that all values in
[0, l]K are equiprobable. Such a prior distribution
is given by a uniform distribution, or, equivalently,
by 6(¢!(-k, . .. ,k),K). In the discipline of Bayesian
statistics, the distribution 6( ¢15', 0) is considered to be
the unbiased prior (see, e.g., [14, 12]). The unbiased­
Dirichlet prior effectively partitions its mass equally at
the vertices of the /{-cube, reflecting complete uncertainty in¢.
Analyses of a preferred prior distribution are ren­
dered immaterial by noting the general insensitiv­
ity of results to these alternative prior distributions.
The information necessary to update an agent's prior
distribution on ¢ from complete uncertainty-that
is, 6(¢!0, 0)-to the uniform distribution-that is,
6(¢1Ck, . . . , :k ) , K)-is provided by the first I< out­
comes. Thus, for large samples, the rate of conver­
gence of the estimate to the mean is insensitive to the
choice of an informationless prior distribution on ¢.
11

ANALYSIS OF
REFORMULATION
TRADEOFFS

Without reformulation of the inference problem
Pr[XIE], BNRAS-algorithms have a runtime propor­
tional to Vi; [6, 9]. Reformulation requires us to in­
dependently approximate Pr[X, E} and Pr[E]. The
time required to approximate these inferences is pr�
portional to
( 1 1)
The first term in Equation 11 is the time required
to approximate the 29 inferences Pr[X, El�] of Equa­
tion 5. The second term is the number of instantia­
tions dictated by the stopping rule that guarantees jj
is a satisfactory approximation of i = Pr[�].
Let <PM denote the minimum probability Pr[J,:] over all
instantiations I;. Using results in [7] it is straightfor-

Reformulating Inference Problems Through Selective Conditioning

ward to show that

29

N < £24JM

log

2

(12)

6"

Equations 11 and 12 imply that the time required for
approximation is proportional to

(13)
The choice of nodes 9 that minimizes the time given
in Equation 13 requires searching over the space of all
possible sets �- However, even suboptimal selections
can be useful; inference based on suboptimal reformu­
lations can be significantly faster than inference on the
original problem instance.
We outline a greedy search algorithm that highlights
the optimization constraints present in Equation 13.
Initially, 9 is empty and the first term in Equa­

tion 13 dominates the runtime. Assume that for some
nonempty set 9, the first term in Equation 13 contin­
ues to dominate. We add ux; to� only if

(14)
and if, for all j #; i,

We have described a means of decomposing belief net­
works by selectively reformulating topologies and con­
ditional probabilities which pose difficult challenges to
a stochastic simulation algorithm. Perhaps the most
significant aspect of our method is the recruitment of
a parameter, developed in a formal analysis of the run­
time of an inference approximation algorithm, to serve
as an intelligent sentry in targeting the most difficult
components of a problem instance for decomposition.
There are opportunities for extending selective condi­
tioning for use in simulation-based inference. For ex­
ample, we ca.n introduce additional flexibility by inte­
grating b ounded conditioning [17] with selective condi­
tioning. W ith bounded conditioning, we focus the at­
tention of a system on the solution of the most relevant
set of subproblems, and consider additional subprob­
lems as time allows. Beyond refining the details of
our work with stochastic-simulation-based inference,
the general approach of identifying and reformulat­
ing troublesome regions of a problem instance holds
promise for solving other inference problems, and, per­
haps, for tackling difficult computational problems be­
yond inference.

Acknowledgments
>.�

-·

2IUx1l

>

..\�

1_

(15)

__

2IUxi1·

Equation 14 guarantees that, if we add ux to�. the
computation time, given by the term 2!l'D in Equa­
tion 13, is decreased. Equation 15 guarantees that
adding ux 1 achieves the best reduction of the compu­
tation time. The overall computation time in Equa­
tion 13 is also decreased because, by assumption, the
first term in Equation 13 dominates the running time.
The behavior of
and
is complementary in the

!

'D�

i/J/./

sense that, augmenting

�decreases

'D�

This work was supported in part by the National Sci­
ence Foundation under Grant IRI-9108385 and Rock­
well International Science Center IR&D funds.

