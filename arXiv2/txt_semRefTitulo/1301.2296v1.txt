
The Factored Frontier (FF) algorithm is a
simple approximate inference algorithm for
Dynamic Bayesian Networks (DBNs). It is
very similar to the fully factorized version
of the Boyen-Koller (BK) algorithm, but in­
stead of doing an exact update at every
step followed by marginalisation (projection),
it always works with factored distributions.
Hence it can be applied to models for which
the exact update step is intractable. We show
that FF is equivalent to (one iteration of)
loopy belief propagation (LBP) on the origi­
nal DBN, and that BK is equivalent (to one
iteration of) LBP on a DBN where we clus­
ter some of the nodes. We then show em­
pirically that by iterating more than once,
LBP can improve on the accuracy of both
FF and BK. We compare these algorithms on
two real-world DBNs: the first is a model of
a water treatment plant, and the second is a
coupled HMM, used to model freeway traffic.
1

Introduction

Dynamic Bayesian Networks (DBNs) are directed
graphical models of stochastic processes. They gener­
alise hidden Markov models (HMMs) by representing
the hidden (and observed) state in terms of state vari­
ables, which can have complex interdependencies. The
graphical structure provides an easy way to specify
these conditional independencies, and hence to pro­
vide a compact parameterization of the modeL See
Figure 1 for some examples.
In this paper, we will be concerned with the task of
offline probabilistic inference in DBNs, i.e., computing
P(Xfly1,r) fort= 1, . . . , T and i 1, . . . ,N, where
Xi is the i'th hidden node at timet, and Yt is evidence
vector at time t; this is often called "smoothing". We
=

will assume that all the hidden nodes are discrete and
each has Q possible values. The observed nodes can
be discrete or continuous.
The simplest way to perform exact inference in a
DBN is to convert the model to an HMM and ap­
ply the forwards-backwards algorithm [18]. This takes
O(TQ2N) time. By exploiting the conditional inde­
pendencies within a slice, it is possible to reduce this
to fl(TNQN+F) time, where F is the maximum fan-in
of any node. Unfortunately, this is still exponential in
N. In fact, this is nearly always the case (assuming
the graph is connected), because even if there is no
direct connection between two nodes in the same or
neighboring "time slices," they will become correlated
over time by virtue of sharing common influences in
the past. Hence, unlike the case for static networks,
we need to use approximations even for "sparse" mod­
els.
In Section 3.1, we present a new approximation, called
the "factored frontier" (FF) algorithm, which repre­
sents the belief state as a product of marginals. The
FF algorithm is thus very similar to the fully fac­
torized version of the Boyen-Koller (BK) algorithm,
which we summarise in Section 3.2. FF, however, is a
more aggressive approximation, and can therefore be
applied when even BK is intractable: FF will always
take O(TNQF+l) time, whereas BK can take more,
depending on the graph.
In Section 4, we show how both FF and BK are related
to loopy belief propagation (LBP) [15, 21, 20, 6, 7, 13],
which is the method of applying Pearl's message pass­
ing algorithm [16] to a Bayes net even if it contains
(undirected) cycles or loops. In Section 5, we exper­
imentally compare all four algorithms - exact, FF,
BK , and LBP - on a number of problems, and in
Section 7, we conclude.

UAI 2001

MURPHY &

WEISS

2.2

379

The frontier algorithm

If Xt is a vector of N hidden nodes, each with Q pos­
sible values, then X can be in S QN possible states,
so the FB algorithm becomes intractable. The fron­
tier algorithm [23] is a way of computing a1 from O:'t-l
(and similarly for the {31's) without needing to form
the QN x QN transition matrix, yet alone multiply by
it.
=

a
Figure 1: Some
chains and T

b
DBNs. (a) A coupled HMM with

N

=

5

3 timeslices. Clear nodes are hidden,
shaded nodes are observed. In the freeway traffic appli­
cation in Section 5, Xl represents the hidden traffic sta­
tus (free-flowing or congested) at location i on the free­
way at time t; this is assumed to generate a local noisy
measurement of traffic speed, Yt', and to depend on its
previous state and the previous state of its upstream and
downstream neighbors. (b) A DBN designed to monitor
a waste water treatement plant. This model is originally
from [9], and was modified by [2] to include (discrete) evi­
dence nodes.
=

2

Exact inference

We start by reviewing the forwards-backwards (FB)
algorithm [18] for HMMs, and then the frontier algo­
rithm [23] for DBNs, since this will form the basis of
our generalisation.

The basic idea is to "sweep" a Markov blanket across
the DBN, first forwards and then backwards. We shall
call the nodes in the Markov blanket the "frontier set",
and denote it by :F; the nodes to the left and right of
the frontier will be denoted by £ and 'R. At every
step of the algorithm, :F d-separates [, and 'R. We
will maintain a joint distribution over the nodes in :F.
We can advance the frontier from slice t - 1 to t as
follows. We move a node from R to :F as soon as all
its parents are in :F. To keep the frontier as small
as possible, we move a node from :F to £ as soon
as all its children are in :F. Adding a node entails
multiplying its conditional probability table (CPT)
P (Xf iPa(Xf )) onto the frontier, and removing a node
entails marginalising it out of the frontier.
This is best explained by example (see Figure 2). Con­
sider the coupled HMM (CHMM) shown in Figure 1.
The frontier initially contains all the nodes in slice t-1:
Ft,o �r O:'t-1 = P(Xl.:..'YIY1:t-1)· We then advance the
frontier by moving Xl from R to :F. To do this, we
multiply in its CPT P(XllXl_1, Xf-1):

Ft,l

=

P(Xf, Xf:_lfiYu-d = P(XfiXf-1, Xf_1) x Ft,o

Next we add in
2.1

Ft,2 = P(Xf'2, X[!'fiYl:t-d
P(X[txf_,,x[_1,Xf_1)

The forwards backwards algorithm

The basic idea of the FB algorithm is to compute
d f
def P(X
t = t.1 Y1:t) .m the £orwards pass, ,Bit =e
ati =
P(Yt+l:rlXt = i) in the backwards pass, and then
to combine them to produce the final answer: 'Yt def
=
P(Xt i\Yl:T) ex a�f3�- Let M(i,j) � P(Xt+l =
j[Xt = i) be the transition matrix, and Wt(i,i) �f
P(yt[Xt = i) be a diagonal matrix containing the con­
ditional likelihood of the evidence at time t. The
algorithm is just repated matrix-vector multiplica­
tion. Specifically, in the forwards pass we compute
at ex WtMT O:'t- , and in the backwards pass we com­
1
pute f3t ex MWt+1.8t+l; the constants of proportional­
ity are simply the normalizing constants. The bound­
ary conditions are a1 = W17l" and f3r = 1, where
7l"; d�,f P(X1 = i) is the prior. If X can be inS possible
states, the FB algorithm clearly takes O(S2T) time.
·

=

Xf:
x

Ft,l

Now all of the nodes that depend on Xl_1 are in the
frontier, so we can marginalize Xl_1 out (move it from
:F to £):
F.t,3=

'2 X2'
P(x1t'
L...., Ft,2
t-1NIY l:t-1 ) - """

The process continues in this way until we compute

Finally, we weight this factor by the likelihood:

O:t = P(Xf'NIYl:t) oc P(yt\Xf'N)

x

Ft,N

It is clear that in this example, exact inference takes
O(TNQN+2) time and space, since the frontier never

380

MURPHY &

WEISS

UA/ 2001

0 0
00

0 0

0 0

0 0

0 0

0 0

00

0 0

add Xl(t)

0 0

0 0
add X2(t)

0 0

0 0

0 0

removeXl(t-1)

0 0

remove

add XJ(I)

X2(t-1)

0 0

Figure 2: The frontier algorithm applied to a CHMM; observed leaves are omitted for clarity. Nodes inside the
box are in the frontier. The node being operated on is shown shaded; only connections with its parents and
children are shown; other arcs are omitted for clarity. See text for details
.

contains more than N + 2 nodes, and it takes O(N)
steps to sweep the frontier from t 1 to t. In general,
the running time of the frontier algorithm is exponen­
tial in the size of the largest frontier; this quantity
is also known as the induced width of the underlying
or moral graph. We would therefore like to keep the
frontiers as small as possible. Unfortunately, comput­
ing an order in which to add and remove nodes so as
to minimize the sum of the frontier sizes is equivalent
to finding an optimal elimination ordering, which is
known to be NP-hard. Nevertheless, heuristics meth­
ods, such as greedy search [10], often perform as well
as exhaustive search using branch and bound [23].
-

A special case of the frontier algorithm, applied to fac­
torial HMMs, was published in Appendix B of [8]. (In
an FHMM, there are no cross links between the hidden
nodes, so there are no constraints on the order in which
nodes are added to or removed from the frontier.) For
regular1 DBNs, the frontier algorithm is equivalent to
the junction tree algorithm [3, 11, 19] applied to the
"unrolled" DEN. In particular, the frontier sets cor­
respond to the maximal cliques in the moralized, tri­
angulated graph; in the junction tree, these cliques
are connected together in a chain, possibly with some
smaller cliques "hanging off the backbone" to accomo­
date the non-persistent observed leaves. Despite this
equivalence to junction tree, the frontier algorithm is
1 A regular DBN has certain restrictions on its topol­
ogy. Let Ht denote all the hidden nodes in time-slice t,
and 01 all the observed nodes. A regular DBN can have
connections from H1 to 01 and to Ht+!, but to nowhere
else. In particular, there cannot be any intra-slice connec­
tions within the H1 nodes. Furthermore, we assume each
node in Ht connects to one or more nodes in Ht+l (i.e., is
persistent). All the DENs in this paper are regular.
The frontier algorithm works for non-regular DBNs, but
it may be less efficient that junction tree in this case. The
factored frontier and loopy belief propagation algorithms
also work for non-regular DENs.

appealingly simple, and will form the basis of the ap­
proximation algorithm discussed in the next section.
3
3.1

Approximate inference
The factored frontier algorithm

The problem with the frontier and junction tree algo­
rithms is that they need exponential space just to rep­
resent the belief states, and hence need at least that
much time to compute them. The idea of the fac­
tored frontier (FF) algorithm is to approximate the
belief state with a product of marginals: P(Xt IYu) �
f1�1 P(XfiYt:t). (The backward messages f3t are ap­
proximated in a similar way.)
The algorithm proceeds as follows: when we add a
node to the frontier, we multiply its CPT by the prod­
uct of the factors corresponding to its parents; this
creates a joint distribution for this family. We then
immediately marginalize out the parent nodes. The
backwards pass is analogous. This is like the frontier
algorithm except that we always maintain the joint dis­
tribution over the frontier nodes in factored form. This
algorithm clearly takes O(TNQF+l) time, no matter
what the topology.
3.2

The Boyen-Koller algorithm

The Boyen-Koller algorithm [2] represents the belief
state, O:t = P(Xt!Yt:t), as a product of marginals over
C "clusters", P(Xt IYI:t) � TI�=l P( XfiYl:t), whe re Xf
is a subset of the variables {Xi}. (The clusters do not
need to be disjoint.) Given a factored prior, O:t-1, we
do one step of exact Bayesian updating to compute
the posterior, Ot. In general, Ot will not be factored
as above, so we need to project to the space of factored
distributions by computing the marginal on each clus-

MURPHY &

UAI 2001

WEISS

381

0·-,

.
.

:

.
.
.

-�oi�
\..�·o�-<l�·
:a:
.

'

.

.

.
.
'-•

(a)

Figure 3: Illustration of the clustering process. (a) This is a modified version of a CHMM with 4 chains. The big
"mega no?es" con� ain the joint distribution on the whole slice. We have omitted the observed leaves for clarity.
LBP apphed to th1s graph is equivalent to BK. (b) This is like (a), except we have created overlapping clusters
of size 2, for additional accuracy.
ter. The product of these marginals then gives the ap­
proximate posterior, Ext. We can use a similar method
for computing the backward messages in an efficient
manner [1]. Boyen and Koller prove, roughly speak­
ing, that if the error introduced by the projection step
isn't much greater than the error incurred by using
an approximate prior, both errors relative to the true
(uncomputable) distribution, then the overall error is
bounded.

the previous slice (but not on its neighbors within a
slice) - the largest clique has size n, and hence the
running time of BK is O(T NQYN), even in the fully
factorized case.

The accuracy of the BK algorithm depends on the size
of the clusters that we use to approximate the belief
state. Exact inference corresponds to using a single
cluster, containing all the hidden variables in a time­
slice. The most aggressive approximation corresponds
to using N clusters, one per variable; we call this the
"fully factorized" approximation.

Pearl's belief propagation algorithm [16] is a way of
computing exact marginal posterior probabilities in
graphs with no undirected cycles (loops). Essentially it
generalises the forwards-backwards algorithm to trees.
W hen applied to a graph with loops, the algorithm
is sometimes called "loopy belief propagation" (LBP);
in this case, the resulting "posteriors" may not be cor­
rect, and can even oscillate. Nevertheless, the out­
standing empirical success of turbo decoding, which
has be shown to be equivalent to LBP [ 13], has cre­
ated great interest in the algorithm.

It is clear that the fully factorized version of BK is
very similar to the FF algorithm, but there is one im­
portant difference: BK assumes that we update the
factored prior exactly (using, say, junction tree) be­
fore computing the marginals, whereas FF computes
the (approximate) marginals directly. BK is obviously
more accurate than FF, but sometimes it cannot be
used, because even one step of exact updating is too
expensive.

-;

The cost of using BK is determined by the size of the
maximal cliques of the moralized, triangulated ver­
sion of the two-slice DBN. (Unrolling the DBN for
many slices induces long-distance correlations, and re­
sults in cliques that span the whole time-slice, as we
saw above.) For the coupled HMM (CHMM) model
in Figure 1, the cliques just correspond to the fami­
lies (nodes and their parents), so the algorithm takes
O(T NQF+l) time, the same as FF. But for the wa­
ter model (see Figure 1), we also get extra "non-local"
cliques due to triangulation. For more complex mod­
els, such as the 2D generalisation of a CHMM- where
each time slice is now an N = n x n lattice, and each
cell depends on all the nodes in its "receptive field" in

4

BK and FF as special cases of loopy
belief propagation

LBP has been empirically shown to work well on sev­
eral kinds of Bayesian networks which are quite differ­
ent from turbo codes [15, 7]. In addition, a number of
theoretical results have now been proved for networks
in which all nodes are Gaussian [21], for networks in
which there is only a single loop [20], and for general
networks but using the max-product (Viterbi) version
instead of the sum-product (forwards-backwards) ver­
sion of the algorithm [6].
The key assumption in LBP is that the messages com­
ing into a node are independent. But this is exactly the
same assumption that we make in the FF algorithm!
Indeed, we can show that both algorithms are equiv­
alent if we use a specific order in which to send mes­
sages. Normally we implement LBP using a decentral­
ized message passing protocol, in which, at each step,
every node computes its own ..\ and 7!' in parallel (based
on the incoming message at the previous step), and
then sends out ..\ and 7l' messages to all its neighbors.
However, we can also imagine a forwards-backwards

MURPHY & WEISS

382

(FB) protocol, in which each node first sends 1r (a)
messages from left to right, and then sends >. (,B) mes­
sages from right to left. A single pass of this FB pro­
tocol is equivalent to FF.2
The fixed points of LBP are the same, no matter what
protocol is used. If there is not a unique fixed point,
the algorithms may end up at different answers. They
can also have different behavior in the short term. In
particular, if the D BN is in fact an HMM, then a single
FB iteration (2TN message computations) will result
in the exact posteriors, whereas it requires T iterations
of the decentralized protocol (each iteration comput­
ing 2TN messages in parallel) to reach the same result;
hence the centralized algorithm is more efficient [17].
For loopy graphs, it is not clear which protocol is bet­
ter; it depends on whether local or global information
is more important for computing the posteriors. In
this paper, we use the centralized (FB) protocol.
It is also easy to see that the fully-factorized version
of BK is equivalent to a single FB pass of LBP applied
to a modified DBN, as shown in Figure 3. Fo r each
slice, we create two "mega nodes" that contains all
the (hidden) nodes in that slice. The messages corn­
ing into the first mega node are assumed independent;
they are then multiplied together to form the (approx­
imate) prior3; a single message is then sent to the sec­
ond mega node, corresponding to an exact update step
using the QN x QN transition matrix; finally, the indi­
vidual marginals are computed, and the process is re­
peated. Of course, BK does not actually construct the
mega nodes, and does the exact update using junction
tree, but the two algorithms are functionally equiva­
lent. To simulate BK when the clusters contain more
than one node, we simply create new clustered nodes,
in addition to the mega-nodes, and run LBP on the
new graph, as illustrated in Figure 3.
Since FF and BK are equivalent to one iteration of
LBP, on the regular and clustered graphs respectively,
we can improve on both of these algorithms by iter­
ating more than once. This gives the algorithm the
opportunity to "recover" from its incorrect indepen­
dence assumptions. We will see in the Section 5 that
even a small number of iterations can help dramati­
cally.
2

In the case of noisy-or nodes, there are efficient ways to
1r messages without having to do work

compute the>. and

which is exponential in the number of parents [16]. This
reduces the overall complexity of FF from

O(TNFQ).
3For

a

O(TNQF+l)

directe d graph, naive Pearl would take

to

O(QN)

time to compute 1r for the mega-node, but we can do this in
tim e by exploiting the fact that the CPT factorizes.

O(QN)

,

Alternatively, we can use an undirected graph in which
the computation of messages always takes time linear in

the number

of neighbors.

4.1

A free

UAI2001

energy for iterated

BK

BK and a single iteration of
on the clustered graph allows us to utilize the re­
cent result of Yedidia et al [22] to obtain a free energy
for "iterated" BK. We define the "iterated" BK algo­
rithm as running LBP on the clustered graph using a
FB schedule until convergence. The first iteration of
iterated BK is equivalent to BK but in subsequent it­
erations, the a and (3 messages interact to improve the
quality of approximation. The analysis of [22] shows
that iterated BK can only converge to zero gradient
points of the Bethe free energy.
The equivalence between
LBP

This sheds light over the relationship between iterated
BK and the mean field (MF) approximation. The MF
free energy is the same as the iterated BK free en­
ergy when joint distributions over pairs of nodes are
replaced by a product of marginal beliefs over individ­
ual nodes: iterated BK captures dependencies between
nodes in subsequent slices while MF does not. While
this result only holds for iterated BK, ordinary BK can
be thought of as a first approximation to iterated BK.
5

Experimental results

In this section, we compare the BK algorithm with
k iterations of LBP on the original graph, using the
FB protocol (k = 1 iteration corresponds to FF).
We used a CHMM model with 10 chains trained on
some real freeway traffic data using exact EM [12].
.
Q
N Ls=l
We define the Lt error as Dot
Li=t
JP(X; =
sJYt:T)- F(Xf = s!Yt:r)J, where P(·) is the exact pos­
terior and F(-) is the approximate posterior. In Fig­
ure 4, we plot this against t for 1-4 iterations of LBP.
Clearly, the posteriors are oscillating, and this hap­
pens on many sequences with this model. We there­
fore used the damping trick described in [15]. In this
case, each new message is defined to be a convex com­
bination of the usual expression and the old messsage,
with weight J.L given to the old message. Hence J.L = 0
corresponds to undamped propagation, and J.L = 1 cor­
responds to not updating the messages at all, i.e., only
using local evidence. It is easy to show that any fixed
points reached using this algorithm are fixed points of
the original set of (undamped) equations. It is clear
from Figure 5 that damping helps considerably. The
results are summarised in F igure 6, where we see that
after a small number of iterations, LBP with J.L = 0.1
is doing better than BK. Other sequences give similar
behavior.
=

To check that these results are not specific to this
model/ data set, we also compared the algorithms on
the water DBN shown in Figure 1. We generated ob­
servation sequences of length 100 from this model us-

MURPHY & WEISS

UAI2001

6

x10-4

g4
Q)

:i

00

.I

l

200

Jl

1 iter

400

x10....

6

m

383

I

00

600

6

2

.._

6

x10....

i
200

H

400

�.

2 iter

600

g4
Q)

:i

00

600

3 iter

2

600

4 iter

Figure 4: £1 error on marginal posteriors vs. timeslice after iterations 1-4 of undamped
traffic

CHMM.

LBP

applied to the

The £1 error oscillates with a period of 2 (as seen by the similarity between the graphs for

iterations 1/3 and 2/4); this implies that the underlying marginals are oscillating with the same period.

6

6

6

x10"""

1 iter

6

Q)

:i

2

00
Figure
traffic

5: iterations
CHMM.

1

-4

2 iter

g4

g4

---i

X

Q)

:i

I

I

200

1, 2, and 18 of

18 iter

LBP

400

d

l

600

2

00

200

BK

400

600

with damping factor f.l = 0.1, and after using 1 iteration of BK, on the

384

MURPHY & WEISS

0.08

- - 0.1

I I
-- !

0.07

�

10
12x

-- . 0.6
,

0.06

0.05

10

�0.04

9

...

e
Q; 8
:::i

0.03

7

0.02

'·.

0.01

6

00

5
6:

iterations

error incurred by LBP using damping factor J.L = 0;

0.8.

=

0.1 and the high­

1.5

1

Results of applying LBP to the traffic

the lowest curve corresponds to J.L

·-

4

CHMM with 10 chains. The lower solid horizontal line

=

._\

5

is the error incurred by BK. The oscillating line is the

est to f..L

-3

11

'

::;

Figure

UAI2001

2

2.5

3

iterations

Figure 7: Same as Figure

6,

4

3.5

4.5

5

but for the water network.

The do tted lines, from top to bottom , represent f..l = 0,
f..L = 0.1 and f..L = 0.2. The solid line represents BK.

The upper horizontal line corresponds

to not updating the messages at all

(J.L =

1), and gives

an indication of performance based on local evidence
alone.
0.5

,.------,
o

·

...,._

0.45

jtree

number of iterations and damping factor.

we see that there is no oscillation, and that as few as

-�

ing random parameters and binary nodes, and then
compared the marginal posteriors as a function of

The results
for a typical sequence are shown in Figure 7. This time
two iterations of LBP can outperform BK.

how the algorithms compare in terms of speed.

We

therefore generated random data from CHMMs with
=

1, 3, . ..

, 11 chains, and computed the posteri­

ors using the different methods.
are shown in Figure

8.

The running times

It is clear that both BK and

FF /LBP have a running time which is linear in

: ..

·

!!.

-�

- .

0.3

Cij

a; 0.25

�<I)

0.

In addition to accuracy, it is also interesting to see

N

+·

0

bk

loopy 1
� 0.4 -+ loopy 2
!:
-+- loo
py 3
0
�0.35 + loopy4
·

0.2

�0.15

�

/

0.

,·

..

,·

.�·
. '

.·

.

. ..

_.+
--+

0.1

+

0.05

N (for

the CHMM model), but the constant factors of BK

11

are much higher, due to the complexity of the algo­
rithm, and in particular, the need to perform repeated
marginalisations. This is also why BK is slower than
exact inference for N < 11, even though it is asymp­
totically more efficient.4

Figure

8:

Running time on CHMMs as a function of

the number of chains.

The vertical axis is the total

running time divided by the length of the sequence.
The horizontal axis is the number of chains. The dot­
ted curve is junction tree, the steep straight line is
BK, and the shallow straight lines are LBP; "loopy k"

4 All algorithms were implemented in Matlab and are in­
cluded in the Bayes Net Toolbox, which can be downloaded
from YY'Il. cs. berkeley. edu/ ""murphyk/Bayes/bnt. html.

means k iterations of LBP.

UAI 2001

6

MURPHY &

Related work

WEISS

[8]

F. V. Jensen, U. Kjaerulff, K. G. Olesen, and J. Ped­
ersen. An expert system for control of waste water
treatment - a pilot project. Technical report, Univ.
Aalborg, Judex Datasystemer, 1989. In Danish.

[9]

U. Kjaerulff. Triangulation of graphs - algorithms
giving small total state space. Technical Report R90-09, Dept. of Math. and Comp. Sci., Aalborg Univ.,
Denmark, 1990.

[10]

U. Kjaerulff. A computational scheme for reasoning
in dynamic probabilistic networks. In UAI�B, 1992.

[11]

J. Kwon and K. Murphy.

We have already discussed in detail the connections
between LBP, BK and FF. However, there are several
other approximate inference algorithms with a very
similar "flavor".

Perhaps the closest is the expecta­

tion propagation algorithm [ 14] . This is also an itera­

tive message passing algorithm, but now the messages
encode moments of the variables computed with re­
spect to some approximating distribution. The mini­

bucket algorithm [4] also approximates joint distribu­

tions over collections of nodes as a product of smaller

hence cannot correct for erroneous independence as­

nia, Berkeley,

[ 1 2]

2000.

R. J. McEliece, D. J. C. MacKay, and J. F. Cheng.
Turbo decoding as an instance of Pearl's 'belief prop­
agation' algorithm.

7

Comm.,

Conclusions

[13]

tions.

J. Pearl.
mann,

[16]

U AI

reviewers for their

[17]

N00014-00-l-0637 and NSF IIS-9988642.

[18]

[2]

X. Boyen and D. Koller. Tractable inference for com­
plex stochastic processes. In U AI, 1998.

[3]

R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J.
Spiegelhalter. Probabilistic Networks and Expert Sys­

[5]

[6]
[7]

1999.

R. Dechter. Mini-buckets: a general scheme of ap­
proximating approximations in automated reasoning.
IJCAI,

Artifi�

77(2):257-286, 1989.

P. Smyth, D. Beckerman, and M. I. Jordan. Prob­
abilistic independence networks for hidden Markov
Neural Computation,

9(2):227-

269, 1997.

X. Boyen and D. Koller. Approximate learning of dy­
namic models. In NIPS-11, 1998.

In

Fusion and propagation

L. R. Rabiner. A tutorial on Hidden Markov Models
and selected applications in speech recognition. Proc.

probability models.

