
Many AI researchers argue that probability theory is only
capable of dealing with uncertainty in situations where a
fully specified joint probability distribution is available,
and conclude that it is not suitable for application in
AI systems. Probability intervals, however, constitute a
means for expressing incompleteness of information. We
present a method for computing probability interval! for
probabilities of interest from a partial specification of a
joint probability distribution.

Our method improves on

erties of this graph are then exploited for computing
precise intervals.
In Section 2 we introduce the notion of a partial
specification of a joint probability distribution. Fur­
thermore, the foundation for our method for comput­
ing probability intervals from such a partial specifi­
cation is layed. In Section 3 we discuss how indepen­
dency constraints can be taken into consideration. In
Section 4 we briefly point out that our method can
be of real help in the process of knowledge acquisition
for so-called belief networks.

earlier approaches by allowing for independency relation­
ships between statistical variables to be exploited .

1

Introduction

The adversaries of probability theory for dealing with
uncertainty in AI systems often argue that it is not
expressive enough to cope with the different kinds
of uncertainty that are encountered in real-life sit­
uations. More in specific, it has been argued that
probability theory is not able to distinguish between
uncertainty and ignorance due to incompleteness of
information. The suitability of probability intervals
for expressing incompleteness has been pointed out
decisively by J. Pearl in [Pearl, 1988a]. In this pa­
per, we present a framework for computing probabil­
ity intervals from an incomplete set of probabilities.
The general idea of our approach is to take the ini­
tially given probabilities as defining constraints on a
yet unknown joint probability distribution. Several
authors have already addressed the problem of com­
puting probability intervals, see for example (Cooper,
1984] and (Nilsson, 1986]. Our approach differs from
the mentioned ones by taking independency relation­
ships between the statistical variables discerned into
consideration. In order to do so, we assume that the
independencies in the unknown distribution are spec­
ified in a special type of graph. The topological prop-

2

Computing

Probability In­

tervals
In this section, we concentrate ourselves on the notion
of a partial specification of a joint probability distri­
bution and develop a method for computing proba­
bility intervals for probabilities of interest from such
a partial specification. For the moment, we assume
that no independencies between the statistical vari­
ables discerned exist. We begin by introducing some
terminology.
Let 8( a 1, .. .,a,.) be a free Boolean algebra gen­
erated by a set of atomic propositions A =
{al, ...,a,.}, n � 1; alternatively, the Boolean al­
gebra 8(a 1 ,...,a,.) may be viewed as a sample space
being 'spanned' by a set of statistical variables Ai,
i = 1, . .. ,n, each taking values from {ai,-.ai}· A
partial specification of a joint probability distribution

on B(a1, ... , a,.) is a total function P : C - [0, 1]
where C � 8( a 1 , ... , a,.). We call such a partial
specification consistent if there exists at least one
joint probability distribution Pr on 8 such that Pr
is an extension of P onto B(a1, ...,a,.) (notation:
Pr lc = P); otherwise, P is said to be inconsis­
tent. Furthermore, we say that P (uniquely) defines
Pr if Pr is the only joint probability distribution on
B(at, ...,a,.) such that Prlc = P. Now, let Bo be

492

I
I

the subset of B(a1, ..., an) consisting of its 'smallest

following inhomogeneous system of linear

elements', that is, let

...

+

equations

+

P1

I

n
Bo = {/\ Ld L; =a; or£; =.,a;, a; E .A}
i=l

We state some convenient p rop er ties of this set Bo.
Let th e elements of Bo be enumera ted as b;, i =
1, ..., 2n. Then, for any joint probability distribu­
tion Pr on B(a1, ..., an) we have that

(I)

0
1

if j � I.;
if j E Ie,
1, .. . ' 2n, in which Ie; is the index set for Cj E
Th is system of linear equations has the 2" unkn o wn

where d;,j =

Xt,

I: Pr(b;) = 1

+

{

I

.. . , x2 ... Now, let

right-hand sides and

p

:ll

denote the column

Fur�-

vector

the vector of unknowns.

cl
of

thermore, let D denote the coefficient matrix of th
sys tem. We will use the matrix equation D:�: = p t
The probabilitie s Pr(b;) for the elements b; E 80 will denote the system of linear equations obtained from
be called the constituent probabilities of Pr. Fur ther- a partial specification P as described
We ha ve th e follo wing relation between extension
more, for each element b E B(a1, ..., an) there exists
a uniqu e set of indic es Ib E { 1, ... , 2n} such that of a consistent partial specification of a joint probabil­
b = V b;; this set Ib will be called the index set ity distribution and solutions to the matrix equatio
i=l

above.

iEZ'�

for b. For each joint p rob ability distribution Pr on
B( a1, ..., a n) we then have that
Pr(b) =

obtained from it:
•

n

i = 1, ... , 2 , is a solution to D:ll

i E Z'b
•

In addition, it can easily be shown that any consistent
partial specification P : 80 - [0, 1] defined on 80

uniquely defines a joint probability distribution Pr
8(a 1, ... , an). In the sequel,
, an) as long as
we will write 8 instead of 8(a11
•

•

ambiguity cannot occur.
We exploit the set 8o and its properties for com­
puting probability intervals from an arbitrary partial
specification. Suppose that we are given probabilities
for a number of arbitrary elements of the Boolean al­
gebra

8, that is, we consider the case in which we are

given a consistent partial specification P of a joint
probability distribution on 8 that is defined on an
arbitrary subset C � 8. The problem of computing
probability intervals from P will now be transformed
into an equivalent problem in linear algebra. The

Pr on

I
l

= p.

�

Pr on B

such

that Pr I c

=

C = {c11 1 cm}1 m ;::: 1, be a subset of 81
P: C - [0, 1] be a consistent partial speci­
fication of a joint probability distribution on B. We
Let

•

•

Note that although every joint probability distri

l

�

Dz =p corresponds with a 'probabilistic' extension
P: Dz = p may have solutions in which at leas
one of the x; 's is less than zero.
It can easily be shown that the problem of find
ing for a given 6 E B the least upper bound to the
probability of b relative to a partial sp eci fic a ti on Pi.

of

equivalent to the following linear programming pro
lem:

2"

•

constituent probabilities Pr(b;)1 6; E 80, of Pr be de­
noted by z; 1 i = 11
1 2n, and let the initially speci­
1m,
fied probabilities P(c;) = Pr(c;)1 e; e C1 i = 11

L CjZj(= Pr(b))

•

•

•

•

be denoted by

p;.

Using

(1)

and

(2), we

•

i=l

subject to
2"

(i)

"'diJZi =p;,
�
i=1

•

obtain the

(ii)

z;

lll

I

maximize

and let

now consider an arbitrary (yet unknown) joint prob­
ability distribution Pr on B with Pr I c= P. Let the

Rl

bution Pr which is an extension of P corresponds
uniquely with a solution to the matrix equatio
Dz = p obtained from P, not every solution t

general idea is to take the initially given probabili­
ties as defining constraints on a yet unknown joint
probability distribution.

B

For any non ega ti v e solution vector :ll with com
n
ponents z;, t = 1, . . ., 2 1 to Dz = p, we hav
that Pr(6;) = x;, b; E Bo1 de fin es a joint proba­
bility distribution

on the entire algebra

•

For any joint p roba bility distribution

such tha t Pr I c = P, we have that the vector
of constituent probabilities x; = Pr(b;), b; E 6

(2)

L Pr(b;)

I
l

fori=1, . . . , I Cl+l,

;::: 0 , for J·=1, . . . , 2n

I
I

I

493

I

I

I

I

I

I

I
I

I
I

I

I

I

I
I

I
I

I

I

{

if j ¢ Ib
& and di,j constitute the
if j E I
matrix D. A similar statement can be made for the
greatest lower bound to the probability of b relative
to P. Note that this linear programming approach
can deal with conditional probabilities in the same
way in which it handles prior ones; furthermore, the
approach allows for initial specifications of bounds to
probabilities instead of point estimates.
It is well-known that an LP-problem can be solved
in polynomial time, that is, polynomial in the size of
the problem, (Papadimitriou, 1982). The size of an
LP-problem is dependent, among other factors, upon
the number of variables it comprises. Now note that
the specific type of problem discussed in the foregoing
has exponentially many variables, that is, exponential
in the number of statistical variables discerned in the
problem domain. Therefore, computing probability
intervals requires an exponential number of steps.
0

tionships exist and that the cliques are interrelated
only through their intersections. In order to be able
to exploit these properties, we further assume that all
initially given probabilities are local to the cliques of
G. Once more we introduce some new terminology.
Let G = (V(G), E(G)) be a decomposable graph
with the vertex set V(G) = {V1, ... , Vn}, n 2: 1, and
the clique set C/ (G) = {C/1, ..., Clm}, m 2: 1, to be
taken as a decomposable 1-map of an unknown joint
probability distribution Pr. We take the graph from
Figure l(a) as our running example. Let B be the free
Boolean algebra generated by {V; IV; E V(G)}; fur­
thermore, for each clique C/i, let B(Cli) � 6 be the
free Boolean algebra generated by {Vj I Vj E V(C/i)}.
Now, let P be a partial specification of a joint proba­
bility distribution on B (recall that all initially given
probabilities are local to the cliques of G). We say
that P is consistent with respect to G if P can be
extended in at least one way to a joint probability
distribution Pr on B such that Pr is decomposable
relative toG, that is, such that Pr can be expresssed
3
Exploiting
Independency in terms of marginal distributions on the cliques of
G. The initi ally given probabilities being local now
Relationships
allows us to apply the notions introduced in the pre­
In the preceding section we have presented a linear ceding section separately to marginal distributions on
programming method for computing probability in­ th e cliques of G. We begin by taking the definition
tervals from a consistent partial specification of a of a partial spec ification of a joint probability distri­
joint probability distribution. The initially assessed bution to apply to margin al distr�butions: a partial
probabilities were viewed as defining constraints on specification of a marginal distribution on B( Cli) is a
an unknown probability distribution. We assumed total function met; : Ci - [0, 1) where Ci � B(Cli).
that no independency relationships existed between Note that we may now view Pas been defined by a
the statistical variables discerned. In this section, set of partial specifications of marginal distributions
the linear programming approach is extended with M ={met; I C/i E C/(G)}. Furthermore, we take the
an additional method for representing and exploiting notion of consistency to apply to partial sp ec ifi ca ti ons
tions: we call such a partial spec­
independency relationships. Note that representing of marginal
it can be extended in at least
independency relationships in a straightforward man­ ification consistent
tual marginal dist rib u tion.
ner yields nonlinear constraints and therefore is not one way
The analogy between the notions of a consistent
suitable for our purposes.
where Cj

=

1

·

·

·

We assume that the independency relationships be.

partial specification of a joint probability distribution

tween the statistical variables have been specificed as

and a consistent partial specification of

an 1-map of the unknown joint probability distribu­

distribution suggest that we may apply the linear

tion

programming method presented in the preceding

Pr.

Informally speaking, an 1-map of

Pr

is an

undirected graph in which the vertices represent the

a marginal
sec­

tion separately to each of the partial specifications

statistical variables discerned and the missing edges

of marginal distributions associated with the

indicate the independencies holding between the vari­

of G.

ables. Furthermore, we assume that the fill-in algo­

of marginal distributions have been specified consis­

rithm by R.E. Tarjan and

M.

Yannakakis has been

applied to yield a decomposable 1-map G of

Pr.

An

cliques

However, even if all partial specifications

tently, they might still not give rise to a

joint prob­

ability distribution that respects the independency

I- map is decomposable if it does not contain any ele­

relationships shown in G. We therefore

mentary cycles of length four or more without a short­

additional notions of consistency:

define

some

cut. For further information, the reader is referred to

[Pearl,

1988b] . We will show that we can take advan­

. •

The set M of partial specifications of

marginal
if each

tage of the topology of G by observing that between

distributions is called locally consistent

the variables in a clique of G no independency rela-

mcz, E M, i

=

1,

... , m,

is c o ns i ste nt .

494

I

I

I

I

I

I

I

I
I
I

(a)

I
separate systems of constraints �re subseque�tly
1
bined into one large system of
constramts; thts

Figure 1

•

M is called globally consistent if there exists

a set M = {J.'cJ, I J.'CJ, : 8( Cl;) - [0, lj} of
marginal distributions IJCI, on B(Cl;) such that
for each clique Cl, E C/(G), IJCI; is an ex­
tension of mcz; e M, and furthermore that
for each pair of cliques Cl;, Cli E Cl(G) with
V(Cii)n V(CI;) ¢ 0 we have that J.'CI,(V(Cl;)n
V(Clj)) = IJcz;(V(CI;) n V(Cij)); such a set M
is called a global extension of M.

It can be shown that global consistency of M is a
necessary and sufficient condition for P being consis­
tent with respect to G; further details are provided
in [Gaag, 1990].
We now apply the linear programming method
from the preceding section separately to each of the
partial specifications of marginal distributions asso­
ciated with the cliques of G. For each clique Cl; E
Cl( G) we now define a vector z; of constituent proba­
bilities of a yet unknown marginal distribution J.'CI, in
the manner described in Section 2. From the partial
specification met, associated with clique Cl; we then
obtain an appropriate system of linear constraints
with the constituent probabilities as unknowns. This
system will be denoted by D;z; = m;, z; 2 0. The

com-

hnear

�
l

system will be denoted by Dz = m, z !:: 0. To
guarantee that every nonne�a.tive solution to the t �u
obtained system of constramts defines an extenston
of the initially given probabilities to a joint proba­
bility distribution that is decomposable relative to
G, we have to augment the system with some ad­
ditional constraints, called independency constraints,
expressing that the set M of partial specifications
marginal distributions has to be globally consistent.
In theory we now have to obtain for each pair o
cliques Cli, Cl; E C/(G) with V(Cli) n V( Cli) # 0, a
number of constraints specifying that IJcz,(V(Cli)
V(Clj)) = J.'Ct·(V(CI;) n V(Clj)). However, if we
do so, we get �any redundant constraints; in fact,
the reader may verify that it suffices to obtain independency constraints from the clique intersections
represented in a join tree of G only. Figure l(b)
shows a join tree Ta of our example graph. Note
that the resulting independency constraints each in­
volve variables from two cliques only. In the sequel,
the system of independency constraints for the intersection of two cliques Cli and Clj will be denoted

o­
nl
�

l
�

495

I

I

I

I

I

I

I

I

I

I

I

I

I
I

I

I

I

I
I

by T;,j�i- Tj,i�i = 0; the system of independency
constraints obtained from an entire clique tree of G
will be denoted by T� = 0. From now on we will call
D� = m, T� = 0, � 2: 0, the joint system of con­
straints. Analogous to our observations in Section
2, we have that the problem of finding for a given
b e B (which is local to a clique of G), the least
upper bound to the probability of b is equivalent to
maximizing the probability of b subject to this joint
system of constraints. Again, a similar statement can
be made concerning the greatest lower bound to the
probability of b. It should be evident that in the
resulting probability interval the independency rela­
tionships shown in G have been taken into account
properly.
We can solve the linear programming problem dis­
cussed above using a traditional LP-program or a de­
composition method like Dantzig-Wolfe decomposi­
tion, [Papadimitriou, 1982]. In such a straightfor­
ward approach, however, the modular structure of
the problem at hand is not fully exploited. We will
present an algorithm for solving the problem in which
the computations are restricted to local computations
per clique only. First, we describe its basic idea in­
formally for our running example.
Consider Figure 2 in which the join tree Ta of
G has been depicted once more, this time explicitly
showing the clique intersections. We view Ta as a
computational architecture in which the vertices are
autonomous ohjuts holding the local systems of con­
straints as private data. These objects are only able
to communicate with their direct neighbours and only
'through' the independency constraints: the edges
are viewed as communication channels. The indepen­
dency constraints are used for relating variables from
one clique to variables from another one. Now, sup­
pose that we are interested in the least upper bound
to a probability of interest which is local to a specific
clique, like the one shown in the figure. The object
corresponding with the clique now sends a request
for information about further constraints, if any, to
its neighbours and then waits until it has received the
requested information from all of them. For the mo­
ment, each 'interior' object in the join tree just passes
the request on to its other neighbours and awaits the
requested information. As soon as a leaf( or the root)
of the tree receives such a request for information, a
second pass through the tree is started. The leaf com­
putes the feasible set of its local system of constraints
and derives from it (by means of projection) the set
of feasible values for the probabilities which are the
constituent probabilities for the intersection(s) with
its neighbour(s). This information then is passed on
to these neighbours via the appropriate communica-

tion channels using the independency constraints for
'translation' of the variables. This results in the ad­
dition of extra constraints to the local system of con­
straints of these neighbours. These computations are
performed by the interior vertices as well until the ob­
ject that started the computation has been reached
again. The arcs in Figure 2 represent the flow of com­
putation from this second pass through the join tree.
From its (extended) local system of constraints, the
object that started the computation may now com­
pute the least upper bound to our probability of inter­
est. The result obtained is the same as when obtained
directly from the joint system of constraints. The in­
tuition of this property is that when the process has
again reached the object that started the computa­
tion, this object has been 'informed' of all constraints
of the entire joint system. By directing the same pro­
cess once more towards the root and the leaves of the
tree, all objects can be brought into this state. So, in
three passes through a join tree, each object locally
has a kind of global knowledge concerning the joint
system of constraints. It will be evident that for any
probability of interest which is local to a clique we
can now compute a probability interval locally.
The

following algorithm describes

these

three

Without loss of generality we assume that
the computation is started by the root Cl, of the
clique tree Ta. It performs the following actions:

passes.

1. Send a request for information to all neighbours

and wait.
2.

If a return message, having the form of a system
of constraints, has been received from all neigh­
bours, then add these systems of constraints to
the local system of constraints D,�. = m,,
z, 2: 0; compute the feasible set F, of the re­
sulting system and derive from it the (convex)
set {T,Jz,l z, E F,}, for each neighbour Cli of
Cl,.

Clj, send this
information as a system of constraints to Cli us­
ing T,J�•- T1,,�i = 0.

3. For each such neighbouring clique

Each leaf Cli of Ta performs the following actions:
1. Wait for a message.

2. If a request for information is received, then com­
pute the feasible set F; of the local system of
constraints DiZi = m;, z; � 0, and derive from
it the set {TiJZil Zi E Fi}, for the neighbour
Cl1.
3. Send this information as a system of constraints

to Cli using TaJZi- Tj,&ZJ
for a message.

=

0, and then wait

496

I

I

I

I

I

maximize
Pr(v� V -,116)

I

I

I

c:ompuce

I

add

add

I

compute

I

add

compute

I

I

I
l

Figure 2

4. If a system of linear constraints is received, then

add this system to the local system of constraints

D;z;

=

m;, z;

� 0.

Cit be
defined as the vertex on the path from Cl; to Cl, and
let Cli be defined as the set of all other neighbours of
Cl;. Each interior vertex Cl; performs the following
For each interior vertex

Cl,,

let the vertex

actions:

1. Wait for a message.

2.

3. If systems of constraints have been received from
all neighbours Cl�c E Cli (or from Ctt, respec­
tively), then add these additional systems of con­
straints to D;z; = m;, z; � 0; compute the feasible set F, of the resulting system of constraints
and derive from it the set {TiJ z; lza E Fi} for
Cli =Cit (or for each Clj E Cli, respectively).

1

.

4. For each such clique

Clj,

send this information

as a system of constraints to Cli using

Tj,iZj

=

�

T;,jZ; -

0.

.

I
l

The correctness of the algorithm has been proven in
If a request for information is received from the

[Gaag, 1990]. In general, the algorithm may take ex­
ponential time. However, if the maximal clique size

other neighbours Cli e Cli.

is small compared to the number of statistical vari-

neighbour Cit, then pass this message on to all

497

I

I
I

I

I

I

I

I

I
I

I

I

I

I

I

I

I

I

.I

ables, the algorithm will take polynomial time. An
important question for the algorithm to be of prac­
tical use is the question whether it is likely that the
mentioned restriction will be met in practice. Con­
cerning this, J. Pearl argues that sparse, irregular
graphs are generally appropriate in practical applica­
tions, [Pearl, 1988b].
The probability intervals obtained after application
of the algorithm may be rather wide, in fact they
may be too wide for practical purposes. However,
the intervals are precise and in a sense 'honest': they
just reflect the lack of knowledge concerning the joint
probability distribution.

4

Conclusion

intervals can guide the expert in providing further
information.

