arXiv:1402.6863v3 [stat.ML] 14 Aug 2014

The Annals of Statistics
2014, Vol. 42, No. 4, 1689–1691
DOI: 10.1214/14-AOS1217
c Institute of Mathematical Statistics, 2014

ADDENDUM ON THE SCORING OF GAUSSIAN DIRECTED
ACYCLIC GRAPHICAL MODELS
By Jack Kuipers, Giusi Moffa and David Heckerman
Regensburg University, Regensburg University and Microsoft Research
We provide a correction to the expression for scoring Gaussian
directed acyclic graphical models derived in Geiger and Heckerman
[Ann. Statist. 30 (2002) 1414–1440] and discuss how to evaluate the
score efficiently.

Gaussian directed acyclic graph (DAG) models represent a particular type
of Bayesian networks where the node variables are assumed to come from a
multivariate Gaussian distribution. The Bayesian Gaussian equivalent (BGe)
score was introduced in Geiger and Heckerman (1994, 2002), Heckerman and
Geiger (1995) for learning such networks.
For brevity, we omit formal definitions and refer the reader to Geiger and
Heckerman (2002), while following their notation in considering DAG models
m with n nodes corresponding to the set of variables X = {X1 , . . . , Xn }. Let
mh be the model hypothesis that the true distribution of X is faithful to
the DAG model m, meaning that it satisfies only and all the conditional
independencies encoded by the DAG. For a complete random data sample
d = {x1 , . . . , xN } with N observations and a complete DAG model mc , the
marginal likelihood is [Geiger and Heckerman (2002), Theorem 2]
(1)

p(d | mh ) =

n
Y
p(dPai ∪{Xi } | mh )
c

i=1

p(dPai | mhc )

,

where Pai are the parent variables of the vertex i and dY is the data restricted to the coordinates in Y ⊆ X. The BGe score is the posterior probability of mh which is proportional to the marginal likelihood in (1) and the
graphical prior; see equation (2) of Geiger and Heckerman (2002).
Different DAGs which encode the same set of conditional independencies
are said to belong to an equivalence class. Along with ensuring that all
Received February 2014.
AMS 2000 subject classifications. 62-07, 62F15, 62H99.
Key words and phrases. Gaussian DAG models, Bayesian network learning, BGe score.

This is an electronic reprint of the original article published by the
Institute of Mathematical Statistics in The Annals of Statistics,
2014, Vol. 42, No. 4, 1689–1691. This reprint differs from the original in
pagination and typographic detail.
1

2

J. KUIPERS, G. MOFFA AND D. HECKERMAN

DAGs in the same equivalence class are scored equally, the modularity of the
score allows the steps in structure MCMC [Madigan and York (1995)] to be
evaluated much more efficiently. Order MCMC [Friedman and Koller (2003),
on the related space of triangular matrices] as well as the edge reversal move
of Grzegorczyk and Husmeier (2008) would not be possible without it.
For Gaussian DAG models, the likelihood is a multivariate normal distribution with mean µ and precision matrix W . The need for global parameter
independence, so that the expression of the score in (1) holds, implies that
the prior distribution of (µ, W ) must be normal-Wishart [Geiger and Heckerman (2002)]. The parameter µ is taken to be normally distributed with
mean ν and precision matrix αµ W , for αµ > 0. W is Wishart distributed
with positive definite parametric matrix T (the inverse of the scale matrix)
and degrees of freedom αw , with αw > n − 1. As detailed in the supplementary material [Kuipers, Moffa and Heckerman (2014)], one finds
(2)

p(dY | mhc )
l/2

Γl ((N + αw − n + l)/2) |TYY |(αw −n+l)/2
αµ
,
=
N + αµ
π lN /2 Γl ((αw − n + l)/2) |RYY |(N +αw −n+l)/2

where l is the size of Y, AYY means selecting the rows and columns corresponding to Y of a matrix A,
(3)



 
l
Y
x+1−j
x
l(l−1)/4
Γ
=π
Γl
2
2
j=1

is the multivariate Gamma function and
(4)

R = T + SN +

N αw
(ν − x̄)(ν − x̄)T
(N + αw )

is the posterior parametric matrix involving
(5)

N
1 X
x̄ =
xi ,
N
i=1

N
X
(xi − x̄)(xi − x̄)T
SN =
i=1

the sample mean and sample variance multiplied by (N − 1).
The result in (2) is identical to equation (18) of Geiger and Heckerman
(2002), once some factors are cancelled, apart from the manner in which
the matrix elements are chosen. The result in Geiger and Heckerman (2002)
replaces the TYY and RYY by TY and RY , where AY = ((A−1 )YY )−1 .
Inverting the matrices before the elements are selected and then inverting
again [as in Geiger and Heckerman (2002)] we found inconsistent behavior
on simulated data.

ADDENDUM ON SCORING GAUSSIAN DAG MODELS

3

We may further compare to equation (24) of Heckerman and Geiger
(1995), which with the current notation becomes

l/2
αµ
Γl ((N + αw )/2) |TYY |αw /2
Y
h
(6)
p(d | mc ) =
N + αµ
π lN /2 Γl (αw /2) |RYY |(N +αw )/2
while incorrectly defining the SN in the R in (4) as the sample variance.
However, the same terminology, with the correct formula for SN , is used in
Geiger and Heckerman (1994) whose equation (15) is otherwise identical to
(6) aside from having π replaced by 2π.
The difference in the powers of the determinants between (2) and (6)
could lead to a subtle, and hard to predict, change in the scores. There
is also the same loss of l-dependence in the arguments of the multivariate
gamma functions. The ratio of gamma functions for each node now actually
decreases with l while the ratio from (2) increases instead. As discussed in
the supplementary material [Kuipers, Moffa and Heckerman (2014)], using
(6) instead of (2) effectively penalises each node with l parents by a factor
∼ N l , giving a substantial bias toward sparse DAGs. This bias is likely to
be present in early works implementing the score of Heckerman and Geiger
(1995) and possibly remains in legacy code.
SUPPLEMENTARY MATERIAL
Deriving and simplifying the BGe score (DOI: 10.1214/14-AOS1217SUPP;
.pdf). We detail the steps used to derive (2) and simplify the ratios appearing
in (1) to improve the numerical computation of the score.
