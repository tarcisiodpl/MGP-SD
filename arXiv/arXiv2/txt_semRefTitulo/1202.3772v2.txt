
When data is sampled from an unknown subspace, principal component analysis (PCA)
provides an effective way to estimate the subspace and hence reduce the dimension of the
data. At the heart of PCA is the EckartYoung-Mirsky theorem, which characterizes
the best rank k approximation of a matrix.
In this paper, we prove a generalization of
the Eckart-Young-Mirsky theorem under all
unitarily invariant norms. Using this result,
we obtain closed-form solutions for a set of
rank/norm regularized problems, and derive
closed-form solutions for a general class of
subspace clustering problems (where data is
modelled by unions of unknown subspaces).
From these results we obtain new theoretical
insights and promising experimental results.

1

Introduction

Real world data, while typically being very high dimensional, often only depends intrinsically on a few
parameters (Seung and Lee, 2000). It is therefore desirable to identify the low dimensional subspace (manifold) from which the data is sampled. Principal component analysis (PCA) (Jolliffe, 2002) is a classical,
yet still popular, method to perform such dimensionality reduction. If the data is sampled from a single subspace, PCA is provably correct in identifying
the underlying subspace. Algorithmically, the success of PCA depends critically on the Eckart-YoungMirsky theorem (Eckart and Young, 1936; Mirsky,
1960), which characterizes, in closed-form, the optimal
rank k approximation of an arbitrary matrix, under all
unitarily invariant norms.
However, in practice it is more likely that data is sampled, not from a single subspace, but from a union of

Dale Schuurmans
dale@cs.ualberta.ca
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
subspaces (Vidal, 2011). Had we known the membership of the data, the task would be easy: we would just
apply PCA to each subset. Unfortunately, one does
not normally have such useful membership information
a priori. The subspace clustering problem, therefore,
is to find the dimension and basis for each subspace
while segmenting/clustering the points accordingly. Of
course, the primary difficulty is that estimation and
segmentation must occur simultaneously, even though
either task can be easily accomplished given the result
of the other. For applications of subspace clustering,
we refer the reader to the recent survey (Vidal, 2011).
Many algorithms have been proposed for subspace clustering, including factorization methods
(Costeira and Kanade, 1998; Zelnik-Manor and Irani,
2006), generalized principal component analysis
(GPCA) (Vidal et al., 2005), and agglomerative
lossy compression (Ma et al., 2007), as well as
the more recent sparse subspace clustering (SSC)
(Elhamifar and Vidal, 2009) and low rank representation (LRR) (Liu et al., 2010b), to name a few. GPCA
is provably correct while SSC and LRR are provably
correct under the independent subspace assumption.
However, most current algorithms are computationally
extensive, requiring sophisticated numerical optimization routines.
To develop simple algorithms for subspace clustering,
we start by generalizing the Eckart-Young-Mirsky theorem (Eckart and Young, 1936; Mirsky, 1960), since it
is the foundation for PCA in the single subspace scenario. In Section 2, we prove a generalized version
of the Eckart-Young-Mirsky theorem under all unitarily invariant norms. The motivation for considering
all unitarily invariant norms is not solely for mathematical completeness, it also arises from the increasing popularity of the trace norm, a special unitarily
invariant norm. Using similar techniques, we are also
able to provide closed-form solutions for some interesting rank/norm regularized problems. The subsequent
connections to known results are discussed.

In Section 3, we then apply the results established in
Section 2 to the subspace clustering problem. Previous
work by Liu et al. (2010b) has proven that minimizing
the trace norm of the reconstruction matrix yields a
suitable block sparse matrix that will reveal the membership of the points in the ideal noiseless setting. We
prove that this is not only true for the trace norm,
but for essentially all unitarily invariant norms and
the rank function. Progressing to the noisy case, we
propose to choose suitable combinations of unitarily
invariant norms and the rank function in the objective,
as it will lead to very simple algorithms that remain
provably correct if the data is clean and obeys the
independent subspace assumption. Interestingly, the
algorithms we propose are intimately related to classic factorization methods (Costeira and Kanade, 1998;
Zelnik-Manor and Irani, 2006). Experiments on both
synthetic data and the hopkins155 motion segmentation dataset (Tron and Vidal, 2007) demonstrate that
the simple algorithms we devise perform comparably
with the state-of-the-art algorithms in subspace clustering, while being computationally much cheaper.
Notations: We use Mm×n to denote m × n matrices,
although generally we will not be very specific about
the sizes. k · kF , k · ksp , k · ktr denote the Frobenius norm,
spectral norm, and trace norm, respectively. For any
matrix A, A∗ denotes its conjugate transpose, A† its
pseudo-inverse, and A(k) its truncation, where the singular vectors are kept but all singular values are zeroed
out except the k largest.
In this extended version of the paper, all proofs for the
main results stated in the next section are provided in
the appendix.

2

Main Results

First we require some definitions. A matrix norm k · k
is called unitarily invariant if kU AV k = kAk for all
A ∈ Mm×n and all unitary matrices U ∈ Mm×m , V ∈
Mn×n . We use k · kUI to denote unitarily invariant
norms while k·kAUI means (simultaneously) all unitarily
invariant norms.
Perhaps the most important examples for unitarily invariant norms are:
kAk(k,p) :=

k
X
i=1

σip

1/p

,

(1)

where p ≥ 1, k is any natural number smaller than
rank(A), and σi is the i-th largest singular value of A.
For k = rank(A), (1) is known as Schatten’s p-norm;
while for p = 1, it is called Ky Fan’s norm. Some
special cases include the spectral norm (p = ∞), the
trace norm (p = 1, k = rank(A)), and the Frobenius

norm (p = 2, k = rank(A)). Note that all three norms
belong to the Schatten’s family while only the first two
norms are in the Ky Fan family.
The following theorem is well-known:
Theorem 1 Fix N ∋ k ≤ rank(A), then A(k) is a
minimum Frobenius norm solution of
min

X: rankX≤k

kA − XkAUI .

(2)

The solution is unique iff the k-th and (k+1)-th largest
singular values of A differ.
Theorem 1 was first1 proved by Eckart and Young
(1936) under the Frobenius norm; and then generalized to all unitarily invariant norms by Mirsky (1960).
The remarkable aspect of Theorem 1 is that although
the rank constraint is highly nonlinear and nonconvex, one is still able to solve (2) globally and efficiently
by singular value decomposition (SVD). Moreover, the
optimal solution under the Frobenius norm remains
optimal under all unitarily invariant norms.
The Frobenius norm seems to be very different from
other unitarily invariant norms, since it is induced by
an inner product and block decomposable. Therefore
it is usually much easier to work with the Frobenius
norm, and much stronger results can be obtained in
this case. For instance, we have the following generalization of Theorem 1 (though less well-known).
For an arbitrary matrix B with rank r, we denote its
thin SVD as: B = UB ΣB VB∗ . Define two projections
PB,L := UB UB∗ and PB,R := VB VB∗ . Let UB⊥ and VB⊥
be the orthogonal complement of UB and VB , respectively.
Theorem 2 Fix N ∋ k ≤ rank(PB,L APC,R ), then
B † (PB,L APC,R )(k) C † is a minimum Frobenius norm
solution of
min

X: rankX≤k

kA − BXCkF .

(3)

The solution is unique iff the k-th and (k+1)-th largest
singular values of PB,L APC,R differ.
Theorem 2 was first proved by Sondermann (1986),
but largely remained unnoticed. It was rediscovered
recently by Friedland and Torokhti (2007). One may
prove Theorem 2 fairly easily, for instance, by adapting
our proof for Theorem 3 below (plus the observation
that the Frobenius norm is block decomposable).
One natural question is whether we can replace the
Frobenius norm in Theorem 2 with other unitarily invariant norms, as in Theorem 1. Unfortunately, Example 2 below shows that it is impossible in general.
1
Erhard Schmidt proved a continuous analogue as early
as 1907.

However, by putting assumptions on A, B and C, we
are able to generalize Theorem 2 in meaningful ways.
Simultaneous Block Assumption (SB): Assume
(UB⊥ )∗ AVC = 0 and UB∗ AVC⊥ = 0.
Theorem 3 Fix N ∋ k ≤ rank(PB,L APC,R ). Under
the SB assumption, B † (PB,L APC,R )(k) C † is a minimum Frobenius norm solution of
min

X: rankX≤k

kA − BXCkAUI .

(4)

The solution is unique iff the k-th and (k+1)-th largest
singular values of PB,L APC,R differ.
The next proposition plays a key role in the proof of
Theorem 3, and may be of some independent interest.
Proposition 1 If it exists, any minimizer of
min kXkAUI

(5)

X∈X

remains optimal for

X
min
0
X∈X

0
B



AUI

for any constant matrix B.
Remark 1 It is our incapability
Propo of extending

X C
sition 1 to full block matrices,
, that prevents
D B
us from fully generalizing Theorem 2.
One interesting case where the SB assumption is trivially satisfied can be summarized as:
Corollary 1 Fix N ∋ k ≤ rank(A), then
B † (BAC)(k) C † is a minimum Frobenius norm solution of
min
kBAC − BXCkAUI .
(6)
X: rankX≤k

The solution is unique iff the k-th and (k+1)-th largest
singular values of BAC differ.
Setting B and C to identities, we recover Theorem 1.
Note that a special case of this corollary (where B
is identity and C is a projection) has been previously established in (Piotrowski and Yamada, 2008;
Piotrowski et al., 2009) in the context of reduced-rank
estimators. However, our corollary is stronger and obtained by a much simpler proof. We will apply Corollary 1 to subspace clustering in the next section.
We briefly illustrate these results with some examples.

Example 1 Consider


 

1 0
1
A=
, B=
, C= 1 0 .
0 1
0
The SB assumption is satisfied, therefore one can apply Theorem 3 to conclude that x = 1 is the unique
(rank 1) optimal solution under all unitarily invariant
norms. However, Corollary 1 does not apply to this
trivial example.
By now one might be tempted to hope that the SB
assumption is just a removable artifact of the proof.
This is not true: as the next example shows, the optimal solution under the Frobenius norm need not remain optimal under other unitarily invariant norms.
This observation is in sharp contrast with Theorem 1.
Example 2 Consider


 

1 1
1
A=
, B=
, C= 1 0 .
1 2
0
Here the SB assumption is deliberately falsified. X is
now just a scalar and we require rank(X) ≤ 1. Under the Frobenius norm, it is easy to see X = 1 is the
(unique) optimal solution. However, under the trace
norm, X = 0.5 yields the optimal objective value 2.5,
√
strictly better than X = 1 whose objective value is 2 2.
Note that this example also demonstrates that Penrose’s result, that is, B † AC † is the minimum Frobenius norm solution of minX kA − BXCkF , does not
generalize to other unitarily invariant norms (but see
Remark 4 below).
We currently do not know if problem (3), without the
SB assumption, can or cannot be solved in polynomial
time if the Frobenius norm is replaced by any other
unitarily invariant norm.2
The last example shows that even one of B and C is
restricted to identity, Theorem 2 still cannot be generalized to all unitarily invariant norms.
Example 3 Consider





a 0
1 0
1
A =  0 b  , B = 0 1  , C =
0
0 1
0 0


0
.
1

Now X is 2 by 2 and we require rank(X) ≤ 1. Set
a ∨ b = 1 and a ∧ b =1/2. Under 
the Frobenius norm,
1a>b
0
is the (unique) opit is easy to see X =
0
1a<b
timal solution. However,

 under the trace
 (resp.
 speca 0
0 0
tral) norm, X =
(resp. X =
) yields
0 0
0 b
2
We have found a positive result for the spectral norm.
Details can be found in the complete version of the paper.

a strictly smaller objective value if a < b (resp. a > b).
Interestingly, Penrose’s result, that B † A is the minimum Frobenius norm solution of minX kA − BXkF ,
generalizes to all unitarily invariant norms in this case
(Marshall et al., 2011, Theorem 10.B.7).
Remark 2 So far, we have restricted attention to
rank constrained problems. Fortunately, rank regularized problems
min

X∈Mm×n

f (X) + λ · rank(X)

(7)

can always be efficiently reduced to an equivalent constrained version
min

X: rank(X)≤k

f (X),

(8)

since the rank function can only take integral values
between 0 and min{m, n}. Here one need only solve
(8) for each admissible value of k, then pick the best
according to the objective of (7). Hence, it is clear that
if one can efficiently solve (8) for all admissible values
of k, then (7) can be efficiently solved for all values of
λ (even negative ones, which promote the rank).
Note that, due to the discreteness of the rank function,
the optimal solution changes discontinuously when
tuning the constant λ. Therefore, it is usually desirable to smooth the solution, even when one can optimally solve the rank regularized problem. This is
usually done by replacing the rank function with a
suitable norm. The next theorem states that Theorem
3 has a close counterpart for unitarily invariant norms,
although under a stronger assumption.
Simultaneous Diagonal Assumption (SD): In addition to the SB assumption, assume furthermore that
UB∗ AVC is diagonal.3
Theorem 4 Let λ > 0. Under the SD assumption,
the matrix problem
min kA − BXCkUI + λ · kXkUI′
X

(9)

has an optimal solution of the form X ⋆ = VB ΣX UC∗ ,
where ΣX is diagonal.
Note that the two unitarily invariant norms in (9) need
not be the same.
Remark 3 A couple of observations are in order:
• The SD assumption is considerably stronger than
the SB assumption. This is because the rank function has more invariance properties that we can
exploit: it is not only unitarily invariant, but also
3

Rectangular matrix A is diagonal if Aij = 0, ∀i 6= j.

scaling invariant. In contrast, norms, by their
definition, cannot be scaling invariant.
• Unlike the rank constrained problem (4), the norm
regularized problem (9) can always be solved in
polynomial time as long as one can evaluate the
norms in polynomial time. After all (9) is a convex program. However, the point of Theorem 4 is
to characterize situations where the problem can
be solved in nearly closed-form.
• Sometimes rather than regularizing, one might
prefer to constrain the norm to be smaller than
some constant. One can easily adapt the proof of
Theorem 4 to the constrained version. Moreover,
by choosing suitable constants, regularized problems and their constrained counterparts can yield
the same solutions.
• Theorem 4 obviously remains true if one asserts (possibly different) monotonically increasing
transforms around the two norms.
We now discuss two interesting cases where the
SD assumption is trivially satisfied.
Let A =
Prank(A)
∗
σ
U
V
be
the
thin
SVD.
i i i
i=1

Corollary 2 Let λ > 0. The matrix problem
min kA − Xk(k,p) + λ · kXk(k′ ,p′ )
X

X⋆

has a (nearly) closed-form solution
Prank(A) ⋆
xi Ui Vi∗ , where x⋆ solves
i=1
min

rank(A)

x∈R+

k
hX
i=1

|σ −

x|p[i]

i1/p

+λ·

k′
hX
i=1

′

xp[i]

i1/p′

=

.

(Here x[i] is the i-th largest element of the vector x.)
If we set the first norm in Corollary 2 to the squared
Frobenius norm (see the last item in Remark 3)
and the second to the trace norm, we recover the
SVD thresholding algorithm for matrix completion
(Cai et al., 2010; Ma et al., 2011). Note that the existing correctness proof for SVD thresholding relies on
the complete characterization of the subdifferential of
the trace norm, while our proof takes a very different
path and avoids deep results in convex analysis.4 One
can also easily derive closed-form solutions for other
variants, for instance, if the first norm is the p-th power
of Schatten’s p-norm while the regularizer is still the
trace norm, similar thresholding algorithms ensue.
4

Upon completing the paper, we discovered that a similar argument tailored for the trace norm has appeared in
(Ni et al., 2010).

Corollary 3 Let λ > 0. The matrix problem
min kA − AXk(k,p) + λ · kXk(k′ ,p′ )
X

X⋆

has a (nearly) closed-form solution
Prank(A) ⋆
xi Vi Vi∗ , where x⋆ solves
i=1
min

rank(A)

x∈R+

k
hX
i=1

|σ − σ ⊙

x|p[i]

i1/p

+λ·

k′
hX

′

xp[i]

i=1

=

i1/p′

.

Here x[i] denotes the i-th largest element and ⊙ is the
Hadamard (elementwise) product.
We apply Corollary 3 to the subspace clustering problem in the next section.
We close this section with a reversed version of Corollary 1. Let B = UB ΣB VB∗ and C = UC ΣC VC∗ be the
corresponding thin SVDs. Define Â := VB∗ AUC .
Theorem 5 Let λ > 0, then ∃ r ∈ {0, . . . , rank(Â)}
such that VB (Â − Â(r) )UC∗ is a minimum Frobenius
norm solution of
min rank(BAC − BXC) + λkXkRUI ,
X

(10)

where k · kRUI is either the rank function or a unitarily
invariant norm.
Remark 4 Theorem 5 also implies a closed-form solution for the following problem:
min

X:A=BXC

kXkRUI .

(11)

By a classic result of Penrose (1956), if the feasible set
is not empty,5 then B † AC † is a feasible point, hence
we may write A as BB † AC † C. Using B † AC † as A
in (10), and letting λ → 0, one obtains the closedform solution for (11): X ⋆ = B † AC † . Another way
to derive this fact is through Corollary 1 by setting k
large. With slightly more effort one can also prove the
uniqueness of the solution if k · kRUI is a Schatten pnorm (p < ∞). When k · kRUI = k · kF , we recover the
classic result of (Penrose, 1956); for k · kRUI = k · ktr ,
we recover the recent result in (Liu et al., 2010a);6 and
for k · kRUI = rank(·), we recover a (weaker) result in
(Tian, 2003). Surprisingly, all other cases appear to
be new.

3

Subspace Clustering

Subspace clustering considers the following problem:
Given a set of points X = [X1 , . . . , Xk ]Γ in RD , where
5

Of course, this can be easily and efficiently checked.
6
Although the formula in Theorem 3.1 of (Liu et al.,
2010a) looks different from ours, it can be verified that
they are indeed the same.

Xi = [xi1 , . . . , xini ] is drawn from some unknown subspace Si with unknown dimension di (i.e., xij ∈ RD
is the j-th sample from subspace Si ) and Γ is an unknown permutation matrix; we want to identify the
number of subspaces k, the dimension di and basis
Vi for each subspace, while simultaneously segmenting
the points accordingly (i.e., estimating Γ and ni ). In
general, this is a ill-posed problem, but if some prior
knowledge of k (the number of subspaces) or di (the
dimension of subspace Si ) is provided, one can solve
the subspace clustering problem in a meaningful way.
For example, if we assume k = 1, then subspace clustering reduces to classic principal component analysis,
which has been well-studied and widely applied.
Subspace clustering is very challenging because one
has to simultaneously estimate the subspaces and segment the points, even though each subproblem can be
easily solved given the result of the other. Practical issues like computational complexity, noise, and outliers
make the problem even more challenging. We refer the
reader to the excellent survey (Vidal, 2011) for details.
Recently, under the independent 7 subspace assumption (and assuming clean data), Elhamifar and Vidal
(2009) successfully recover a block sparse matrix to
reveal data membership, by resorting to the sparsest
reconstruction of each point from other points. The
key observation is that each point can only be represented by other points from the same subspace, due
to the independence assumption. Liu et al. (2010b,a)
subsequently showed that similar block sparse matrix
can be obtained by minimizing the trace norm, instead
of the ℓ1 norm in (Elhamifar and Vidal, 2009).
Specifically, Liu et al. (2010b) considered the following
problem:8
min rank(Z)
Z

s.t. X = XZ.

(12)

The idea is that, given the independence assumption,
if one reconstructs each point through other points, the
reconstruction matrix Z must have low rank. However,
(12) was thought to be hard, hence Liu et al. (2010b)
turned to a convex relaxation:
min kZktr
Z

s.t. X = XZ.

(13)

Under the independence assumption, Liu et al.
(2010b) successfully proved that Zij = 0 if points xi
and xj come from different subspaces. Our result in
Remark 4 then immediately yields a generalization to
all unitarily invariant norms (and the rank function,
7
A set of subspaces is called independent if the dimension of their direct sum equals the sum of their dimensions.
8
Instead of the data X itself, in principle one could also
choose other dictionaries to reconstruct X. As long as the
dictionary spans X, our results in this section still apply.

which was thought to be hard in (Liu et al., 2010b)).
Recall that we use k · kRUI to denote either the rank
function or an arbitrary unitarily invariant norm.
Theorem 6 Assume the subspaces are independent
and the data is clean, then Z ⋆ := X † X, being a minimum Frobenius norm solution of
min kZkRUI
Z

s.t.

X = XZ,

(14)

⋆
is block sparse, that is, Zij
= 0 if points xi and xj
come from different subspaces.

Proof: As shown in (Liu et al., 2010b), (14) under
the trace norm has a unique solution that satisfies the
block sparse property. But Remark 4 shows that X † X
is the unique solution and moreover remains optimal
under all unitarily invariant norms and the rank function.

We noted that X † X is called the shape interaction matrix (SIM) in the computer vision literature,
and was known to have the block sparse structure
(Costeira and Kanade, 1998). The surprising aspect of
Theorem 6 is that, at least in the ideal noiseless case,
there is nothing special about the trace norm. Any
unitarily invariant norm, in particular, the Frobenius
norm, leads to the same closed-form solution.

where one of k · kRUI and k · kRUI′ is the rank function, or
both are the trace norm.
Proof: The case k · kRUI′ = rank follows from Corollary
1 and Remark 2; the case k · kRUI = rank follows from
Theorem 5; and the last case k · kRUI = k · kRUI′ = k · ktr
follows from Corollary 3.

∗
Interestingly, V(r) V(r)
was known to be an effective
heuristic for handling noise in the computer vision literature (Zelnik-Manor and Irani, 2006). Here we provide a formal justification for this heuristic by showing that it is an optimal solution of some reasonable
optimization problem(s). This new interpretation is
important for model selection purposes in the unsu∗
pervised setting. Intuitively, the idea behind V(r) V(r)
is also simple: If the amount and magnitude of noise is
moderate, the SIM will not change significantly hence
by thresholding small singular values, which usually
are caused by noise, one might still be able to recover
∗
the SIM, approximately. We shall call V(r) V(r)
the discrete shrinkage shape interaction matrix (DSSIM).

As remarked previously, the discrete nature of the
DSSIM might lead to instability, hence it might be
preferable to consider the following variant
min kX − XZkUI + λ · kZkUI′ ,
Z

Of course, in practice, data is always corrupted by
noise and possibly outliers. To account for this, one
can consider:
min ρ(X − XZ) + λ · kZkREG ,
Z

(15)

where ρ(·) measures the discrepancy between X and
XZ, k·kREG is a regularizer, and λ is the parameter that
balances the two terms. In this case, popular choices
of ρ include the (squared) Frobenius norm, ℓ1 norm,
the ℓ2 /ℓ1 norm or even the rank function, depending
on our assumption of the noise. Typical regularizers
include the trace norm, Frobenius norm or the rank
function. For instance, Liu et al. (2010a) considered
ρ = ℓ1 (if the noise is sparsely generated) or ρ = ℓ2 /ℓ1
(if the noise is sample specific), and k·kREG = k·ktr . The
resulting convex program was solved by the method of
augmented Lagrangian multipliers.
When such prior information about the noise is not
available, it becomes a matter of subjectivity to choose
ρ and k·kREG . Our next result shows that, by choosing ρ
and k · kREG appropriately, one can still obtain a closedform solution for (15):
Theorem 7 Let X = U ΣV ∗ be the thin SVD. Then
∗
∃r ∈ {0, . . . , rank(X)} such that V(r) V(r)
is a minimum
Frobenius norm solution of
min kX − XZkRUI + λ · kZkRUI′ ,
Z

(16)

which has also been shown to have a (nearly) closedform solution in Corollary 3. Specifically, we have the
following result:
P
∗
Corollary
X =
i σi Ui Vi be the thin SVD.
P 4 Let
λ
∗
Then i (1 − 2σ2 )+ Vi Vi is an optimal solution of
i

min kX − XZk2F + λ · kZktr ,
Z

(17)

where (·)+ = max(0, ·) denotes the positive part.
We shall call the solution in the above corollary
the continuous shrinkage shape interaction matrix
(CSSIM). For comparison purposes, we also consider
P σi2
∗
i σ2 +λ Vi Vi , the closed-form solution of
i

min kX − XZk2F + λ · kZk2F .
Z

(18)

We shall call it the smoothed shape interaction matrix
(SSIM).
Finally, we show that for all choices of the discrepancy
measure ρ, and regularizers k · kRUI of the rank function
or a unitarily invariant norm, the optimal solutions of
(15) share some common structure. To see this let us
consider the equivalent problem:
min ρ(R) + λ · kZkRUI
Z,R

s.t. X = XZ + R.

(19)

The first observation is that R must lie in the range
space of X due to the equality constraint, hence we
can let R := XE. Moreover, given R, using results in
Remark 4, we obtain

Table 1: Segmentation Accuracy (%) on Hopkins155.
Method
Mean
λ
Time
Best

SIM
75.56
0
3.56s
75.56

DSSIM
95.51
10−2
3.29s
99.27

CSSIM
96.05
10−3
3.61s
99.29

SSIM
96.82
10−2
3.61s
99.46

LRR1
96.37
10
695.1s
99.40

LRR2
96.52
1
734.6s
99.32

Z = X † (X − R) := X † X(I − E).
Therefore, we see that no matter how we choose ρ, the
resulting optimal solution is a modification of the SIM.

4

Experiments

In this section, we compare the closed-form solutions
(SIM, DSSIM, CSSIM and SSIM) derived here with
the low rank subspace clustering algorithm proposed
in (Liu et al., 2010b,a). The latter has been shown
to achieve the state-of-the-art for subspace clustering problems. Specifically, two variants in (Liu et al.,
2010a), which we denote LRR1 (ρ is the ℓ2 /ℓ1 norm)
and LRR2 (ρ is the ℓ1 norm), respectively, are compared. For all methods9 except SIM, we tune the regularization constant λ within the range {10i , i = −4 : 1 :
4}. SIM does not have such a parameter (i.e., λ ≡ 0).
After obtaining the reconstruction matrix Z, an affinity matrix Wij = |Zij | + |Zji | is built. Standard
spectral clustering techniques (Luxburg, 2007) are applied to segment the points into different clusters (subspaces). We count the number of misclassified points,
and report the accuracy of each method.
4.1

Synthetic Data

Following (Liu et al., 2010a), 5 independent random
subspaces {Si }5i=1 ⊆ R100 , each with dimension 10,
are constructed. Then 40 points are randomly sampled from each subspace. We randomly choose p%
points and corrupt them with zero mean Gaussian
noise, whose standard deviation is 0.3 times the length
of the point. We repeat the experiment 10 times and
the averaged results are reported in Figure 1.
When p = 0, i.e., in the ideal noiseless setting, all
methods achieve perfect segmentation, as expected
from Theorem 6. As we increase the noise level p,
SIM degrades quickly since it has no protection against
noise. LRR1 performs best in the range of p = 30 ∼
70, probably because its discrepancy measure matches
the noise generation process the best. However, we
note that the advantage of LRR1 over other methods
is rather small. When most data points are corrupted
(p = 80 ∼ 100), DSSIM and CSSIM start to prevail.
Overall, CSSIM, based on the trace norm regularizer,
9

For DSSIM, we use the objective in Theorem 7 (the
trace norm case) to tune λ as we find this yields better
performance than tuning the rank r directly.

performs slightly better than SSIM, which is based on
the Frobenius norm regularizer.
On the computational side, SIM, DSSIM, CSSIM and
SSIM all have closed-form solutions and only require
a single call to SVD, while LRR generally requires 300
steps to converge on this dataset; that is, 300 SVDs,
since each step involves the SVD thresholding algorithm. Moreover, LRR pays extra computational cost
in selecting the regularization constant. In total, LRR
is orders of magnitude slower than all other methods.
We noted in experiments that if the noise magnitude
is larger than a threshold, all methods will degrade to
the level of SIM.

4.2

Hopkins155 Motion Segmentation

The hopkins155 dataset is a standard benchmark
for motion segmentation and subspace clustering
(Tron and Vidal, 2007). It consists of 155 sequences of
two or three motions. The motions in each sequence
are regarded as subspaces while each sequence is regarded as a separate clustering problem, resulting in
total 155 subspace clustering problems. The outliers in
this dataset has been manually removed, hence we expect the independent subspace assumption to hold approximately. For a fair comparison, we apply all methods to the raw data, even though we have observed in
experiments that the performances can be further improved by suitable preprocessing/postprocessing.
The results are tabulated in Table 1. All methods perform well on this dataset, even SIM achieves 75.56%
accuracy, confirming that this dataset only contains
small amount/magnitude of noise and obeys the independent subspace assumption reasonably well. The
numbers in the “Mean” row are obtained by first averaging over all 155 sequences and then selecting the best
out of the 9 regularization constants, with the best λ
tabulated below. The results are close to the state-ofthe-art as reported in (Tron and Vidal, 2007). If we
are allowed to select the best λ individually for each
sequence, we obtain the “Best” row, which is surprisingly good. It is clear that LRR is significantly slower
than all other methods. Note that the running time
is not averaged over sequences, nor does it include the
spectral clustering step or the tuning step.

SIM
DSSIM
CSSIM
SSIM
LRR1
LRR2

100
90

Segmentation Accuracy

80
70
60
50
40
30
20
10
0

0

10

20

30

40

50
60
Noise Level p

70

80

90

100

Figure 1: Results for synthetic data.

5

Conclusion

We have generalized the celebrated Eckart-YoungMirsky theorem, under all unitarily invariant norms.
Similar techniques are used to provide closed-form
solutions for some interesting rank/norm regularized
problems. The results are applied to subspace clustering, resulting in very simple algorithms. Experimental
results demonstrate that the proposed algorithms perform comparably against the state-of-the-art in subspace clustering, but with a significant computational
advantage.

