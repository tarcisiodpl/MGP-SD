
We introduce a new framework for web page
ranking—reinforcement ranking—that improves
the stability and accuracy of Page Rank while eliminating the need for computing the stationary distribution of random walks. Instead of relying
on teleportation to ensure a well defined Markov
chain, we develop a reverse-time reinforcement
learning framework that determines web page authority based on the solution of a reverse Bellman equation. In particular, for a given reward
function and surfing policy we recover a well defined authority score from a reverse-time perspective: looking back from a web page, what is the
total incoming discounted reward brought by the
surfer from the page’s predecessors? This results in a novel form of reverse-time dynamicprogramming/reinforcement-learning problem that
achieves several advantages over Page Rank based
methods: First, stochasticity, ergodicity, and irreducibility of the underlying Markov chain is no
longer required for well-posedness. Second, the
method is less sensitive to graph topology and more
stable in the presence of dangling pages. Third,
not only does the reverse Bellman iteration yield a
more efficient power iteration, it allows for faster
updating in the presence of graph changes. Finally, our experiments demonstrate improvements
in ranking quality.

1 Introduction
Page Rank is a dominant link analysis algorithm for web page
ranking [27, 24, 26], which has been applied to a wide range
of problems in information retrieval and social network analysis [32, 5, 35, 2]. Under Page Rank, authoritativeness is defined by the stationary distribution of a Markov chain constructed from the web link structure [30, 20, 6, 8, 12, 26, 34].
On each page, a model surfer follows a random link, jumping
to the linked page and continuing to follow a random link.
Thus, pages are treated as arriving in a Markov chain—the
next page visited depends only on the page where the surfer
currently visits. The rank of a web page is then defined as

the probability of visiting the page in a long run of this random walk. Unfortunately, this simple protocol does not allow the surfer to proceed from a page that has no outgoing
links—such pages are called dangling pages. In these cases,
the Markov chain derived from the link structure of the Web
is not necessarily irreducible or aperiodic, which are required
to guarantee the existence of the stationary distribution. To
circumvent these problems, a teleportation operator is introduced that allows the surfer to escape dangling pages by following artificial links added to the Web graph. Teleportation
has been widely adopted by literature, leading to the well accepted stationary distribution formulation of authority ranking, see e.g. [22, 23, 20, 10, 30, 16].
However, if we consider real search behavior, teleportation
is obviously artificial. It is unnatural to propagate the score of
a page to other unlinked pages, thus teleportation contributes
a blind regularization effect rather than any real information.
In fact, teleportation contradicts the basic hypothesis of Page
Rank: through teleportation, pages that are not linked by a
page still receive reinforcement from the page. Teleportation
was primarily introduced to guarantee the existence of the stationary distribution. In this paper, we show that teleportation
is in fact unnecessary for identifying authoritative pages on
the Web. First, contrary to widely accepted belief, teleportation is not required to derive a convergent power iteration
for global Page Rank style authority scores. Second, as has
been widely adopted in the random surfer interpretation for
Page Rank, teleportation or even random walk is also unnecessary conceptually. We introduce a new approach to defining web page authority that is based on a novel reinforcement
learning model that avoids the use of teleportation while remaining well defined. We prove that the authority function
is well posed and satisfies a reverse Bellman equation. We
also prove that the induced reverse Bellman iteration, which
is more efficient than the Page Rank procedure, is guaranteed
to converge for any positive discount factor.
In addition to establishing theoretical soundness, we also
show that the reinforcement based authority function is less
sensitive to link changes. This allows us to achieve faster
updates under graph changes, addressing the Page Rank updating problem [4] in an efficient new way. As early as 2000,
it was observed that 23% of the web pages changed their index daily [1]. Unfortunately, the Page Rank power iteration
does not benefit significantly from initialization with the pre-

Algorithm 1 Standard procedure for computing Page Rank:
efficient power iteration method that exploits G’s structure.
Initialize x0
repeat
xk+1 = cH T xk
ω = ||xk ||1 − ||xk+1 ||1
xk+1 = xk+1 + ωv
until desired accuracy is reached
vious stationary distribution [25]. We prove that our authority
function can take better advantage of initialization, and yield
faster updates to graph changes. Furthermore, we demonstrate that reinforcement ranking can improve on the authority scores produced by Page Rank in a controlled case study.

2 Page Rank
We first briefly review the formulation of Page Rank [6, 8,
12, 26]. Suppose there are N pages in the Web graph under consideration. Let L denote the adjacency matrix of the
graph; i.e., L(i, j) = 1 if there is a link from page i to page j,
otherwise L(i, j) = 0. Let H denote the row normalized matrix of L; let e be the vector of all ones; and let v denote the
teleportation vector, which is a normalized probability vector (assume column vectors). Finally, let S be a stochastic
matrix such that S = H + auT , where the vector a indicates ai = 1 if page i is dangling and 0 otherwise. Here u
is a probability vector that is normally set to either e/N or v.
Note that adding auT to the H matrix artificially “patches”
the dangling pages that block the random surfer.
The transition probability matrix used by Page Rank is
G = cS + (1 − c)ev T ,
for a convex combination parameter c ∈ (0, 1). The matrix
G is stochastic, irreducible and aperiodic, and thus its stationary distribution exists and is unique according to classical
Markov chain theory. In fact, the Page Rank (denoted by π̄) is
precisely the stationary distribution vector for G, which satisfies π̄ = GT π̄. Page Rank can be interpreted as follows: with
probability c the surfer follows a link, otherwise with probability 1 − c the surfer teleports to a page according to the
distribution v; the rank of a page is then given by its long run
visit frequency. Teleportation is key, since it ensures the chain
is irreducible and aperiodic, thus guaranteeing the existence
of a stationary distribution for the surfing process.
Unfortunately, the introduction of teleportation causes the
matrix G to become completely dense. Power iteration is
therefore impractical unless one exploits its special structure
in G; namely that it is a sparse plus two rank one matrices.
An efficient procedure for computing Page Rank is given in
Algorithm 1 [22, 23]. This algorithm evaluates an equivalent
update to π̄k+1 = GT π̄k , but it avoids using G by implicitly
incorporating the scores of the dangling pages and teleportation in computing ω. Note that the issue of accommodating
dangling pages in Page Rank has been considered a challenging research issue [13, 6].

3 MDPs and the Value Function
A Markov Decison Process (MDP) is defined by a 5-tuple
(S, A, P A , RA , γ); where S denotes a state space; A is the
action space; P A is a transition model with P a (s, s′ ) being
the probability of transitioning to state s′ after taking action
a at state s; RA is a reward model with Ra (s, s′ ) being the
reward of taking action a in state s and transitioning to state
s′ ; and γ ∈ [0, 1) is a discount factor [28, 21, 7, 33].
A policy π maps a state s and an action a into a probability
π(s, a) of choosing that action in the state. The value of a
state s under a policy π is the discounted long-term future
rewards received following the policy
o
nP
∞
t
V π (s) = Eπ
t=0 γ rt s0 = s, at≥0 ∼ π ,
where rt is the reward received by the agent at time t, and Eπ
is the expectation taken with respect to the distribution of the
states under the policy.
The value function satisfies an equality called Bellman
equation. In particular, for any state s ∈ S
P
V π (s) = γ s′ ∈S P π (s, s′ )V π (s′ ) + r̄π (s),
(1)

where P π (s, s′ ) is the probability of transitioning from s to s′
following the policy, and the r̄π (s) is the expected immediate
reward of leaving state s following the policy.

4 The Reinforcement Ranking Framework
We now introduce the reinforcement ranking framework,
which models search and ranking in terms of an MDP. The
framework is composed of the following elements.
The agent and the environment. The agent is a surfer model
and the environment is a set of hyperlinked documents on
which the surfer explores. That is, we consider the Web to be
the environment; the surfer acts by sending requests that are
processed by servers on the Web. This is a simple model of
everyday surfing that stresses the subjectiveness of the surfer
as well as the objective structure of the Web, in contrast to
Page Rank which models surfing as a goal-less random walk.
The rewards. According to [33], a reward function “maps
each perceived state (or state-action pair) of the environment
to a single number, a reward, indicating the intrinsic desirability of that state”. Intuitively, a reward is a signal that
evaluates an action. A surfer can click many hyperlinks on
a page. If a clicking leads to a page that satisfies the surfer’s
needs, then a large reward is received; otherwise it incurs a
small reward. From the perspective of information retrieval,
the reward represents information gain from reading a page.
The introduction of rewards to search and ranking is important because it highlights the fact that a page has intrinsic importance to users. In fact, this is a key difference from what
has been pursued in the link analysis literature, which does
not normally model pages as having intrinsic values. The reward hypothesis is also important because surfing and search
is purposeful in this model—actions are taken to achieve rewards. In this paper, we will be considering a special reward
function, in which Ra (s, s′ ) = r(s′ ), where r is a function
mapping from the state space to real numbers. This means
the reward of transitioning to a state is uniquely determined
by the state itself.

The actions and the states. An action is the click of a hyperlink on a page. A state is a web page. The current state
is the current page being visited by the surfer. After a clicking on a hyperlink, the surfer can observe the linked page or
a failed connection. For simplicity, we assume that all links
are good in this paper. That is, the next state is always the
page that an action leads to. Therefore, the state space S is
the set of the web pages. The action space on a page s, denoted A(s), is the set of actions that lead to the linked pages
from s. The overall action space is defined by the union of
the actions available on each page, i.e., A = ∪s∈S A(s).
The surfing policy and transition model. A surfer policy
specifies how hyperlinks are followed at web pages. Based on
the above definitions relating web search to an MDP model,
we can equate a surfer with a standard MDP policy as specified in Section 3. For web search, we also assume the transitions are deterministic; that is, clicking a hyperlink on a particular page always leads to the same successor page, hence
P a (s, s′ ) = 1 for all a ∈ A(s). This treatment simplifies
the problem without losing generality—it is straightforward
to extend our work to the other cases.

4.1

The Authority Function

Given these associations established between web surfing and
an MDP, we can now develop a web page authority function
in the framework of reinforcement ranking. In particular, we
define the authority score of a page to be the rewards accumulated by its predecessors under the surfing policy. That is,
for a page s ∈ S, its authority score under surfing policy π is
Rπ (s)

= r(s) + γr(1) (s) + γ 2 r(2) (s) + · · ·
∞
X
γ k r(k) (s),
=

(2)

k=0

where γ is the discount factor, r(s) is a reward that is dependent on s, and r(k) (s) is the reward carried from the k-step
predecessors of s to s by the policy. Note that in the second
equation, r(0) = r, and if a page s has no predecessor, Rπ (s)
can be set to r(s) or some other default value. The k-step
historical rewards to a state s are defined as follows:
r(1) (s) captures the one-step rewards propagated into s
X
π
r(1) (s) =
Pp,s
r(p) ;
p∈S

r(2) (s) captures the rewards from the two-step predecessors
X
X
π
r(2) (s) =
Pp,s
Ppπ′ ,p r(p′ ) ;
p∈S

p′ ∈S

r(3) (s) captures the 3-step rewards propagated into s
X
X
X
π
r(3) (s) =
Pp,s
Ppπ′ ,p
Ppπ′′ ,p′ r(p′′ ); etc.
p∈S

p′ ∈S

p′′ ∈S

Note that the discount factor γ plays an important role in
this model, since it controls the effective horizon over which
reward is accumulated. If γ is large, the authority score will
consider long chains of predecessors that lead into a page. If
γ is small, the authority score will only consider predecessors

that are a within a few steps of the page. This gives a new interpretation for the dampening-factor-like in PageRank. Previously it is commonly recognized that the larger the dampening factor in PageRank, the closer the score vector reflects
the true link structure of the graph, e.g. see [9, 10, 3, 14, 11].
While the two interpretations do not contradict each other,
viewing the dampening/discount factor as a control over the
distance of looking back from pages is surely both essential
and intuitive.

4.2

The Reverse Bellman Equation

Although the authority score function Rπ appears to be similar to a standard value function V π , they are not isomorphic
concepts: the value function (1) is defined in terms of the forward accumulated rewards. The reverse function (2) cannot
be reduced to the forward definition (1) because the transition probabilities are not normalized in both directions; they
are only normalized in the forward direction. In particular,
(1) is an expectation, whereas (2) cannot be an expectation in
general. Despite this key technical difference, it is interesting
(and ultimately very useful) that the authority function also
satisfies a reverse form of Bellman equation.
Theorem 1 (Reverse Bellman Equation) The
authority
function Rπ satisfies the reverse Bellman equation for all s:
X
π
Rπ (s) = γ
Pp,s
Rπ (p) + r(s).
(3)
p∈S

Proof: First observe that the k-step rewards can be expressed
in terms of the (k − 1)-step rewards; that is
X
X
π
π (1)
r(1) (s) =
Pp,s
r(p) , r(2) (s) =
Pp,s
r (p) , etc.
p∈S

p∈S

π

Therefore, from the definition of R in (2), one obtains
h
i
Rπ (s) = r(s) + γ r(1) (s) + γr(2) (s) + . . .
hX
i
X
π
= r(s) + γ
Pp,s
r(p) + γ
Pp,s r(1) (p) + . . .
p∈S

p∈S

h
i
r(p) + γr(1) (p) + . . .

= r(s) + γ

X

π
Pp,s

= r(s) + γ

X

π
Pp,s
Rπ (p). 

p∈S

p∈S

The standard Bellman equation (1) looks forward from a
state to define its value, but equation (3) looks backward from
a state in define its authority. Therefore, we call equation (3)
the reverse Bellman equation (RBE for short). Similar to Page
Rank, Rπ determines the authority of a page based on its back
links. However, Rπ is well defined without teleportation. In
particular, the surfer model P π is defined on the link structure
only, without any teleportation. Notice that P π is not necessarily irreducible or aperiodic, in fact it is not even stochastic
on rows for dangling pages. Yet, perhaps surprisingly, one is
still able to achieve a well defined authority score Rπ , which
is not possible from the classical Markov chain theory.
Theorem 2 For γ ∈ [0, 1), any policy π and any bounded
reward function r, Rπ is finite.

Algorithm 2 Reverse Bellman iteration for computing Rπ :
no special treatment is required for dangling pages.
Initialize R0
repeat
Rk+1 = γ(P π )T Rk + r
until desired accuracy is reached
Proof: It can be shown that the spectral radius of γP π is
strictly smaller than that of any well defined policy. Therefore
IP− γ(P π )T is invertible. Additionally, (I − γ(P π )T )−1 =
∞
π T t
π
=
t=0 (γ(P ) ) by [31, Theorem 1.5]. Therefore, R
π T −1
π
(I − γ(P ) ) r, hence R is finite for any policy and any
bounded reward function r. 
The practical significance of the RBE is that it yields an
efficient algorithm for computing Rπ , based on a backward
version of value iteration as used in dynamic programming
and reinforcement learning; see Algorithm 2. To establish
the correctness of this algorithm we first need a lemma. Let
PN
k · k be the L-2 norm, kR1 k = ( i=1 R1 (i)2 )1/2 .
Lemma 1 For any R ∈ RN , we have k(P π )T Rk ≤ kRk.

Proof:
k(P π )T Rk2 =

N
N X
X
i=1

=

h=1

N X
N
X

h=1 i=1

N X
N
2 X
π
π
R(h) ≤
Phi
Phi
R(h)2

π
R(h)2 =
Phi

i=1 h=1

N
X

R(h)2 = kRk2 .

h=1

Note here we used the ordinary L-2 norm rather than the
weighted L-2 norm, as is common in reinforcement learning.
Theorem 3 For γ ∈ [0, 1) and finite r, Algorithm 2’s update
has a unique fixed point to which the iteration must converge.
Proof: The proof follows the Banach fixed-point theorem
given in [7]. Define T π : RN → RN be a mapping by,
T π (Rπ ) = γ(P π )T Rπ + r. T π is a contraction mapping in
the L-2 norm because
kT π (R1 ) − T π (R2 )k = γkP π (R1 − R2)k ≤ γkR1 − R2 k,
according to Lemma 1. It follows that the iteration converges
to the unique fixed point Rπ = T π (Rπ ). 
This approach to computing an authority ranking has several advantages over Page Rank. First, Algorithm 2 does
not compute an additional ω factor (which requires 2N additional flops per iteration). Second, no special treatment is
required for dangling pages, which has generally been considered tricky for Page Rank [13, 6]. Finally, there is a significant improvement in computation cost and sensitivity for
Algorithm 2 over Algorithm 1.

5 Sensitivity
To assess the relative sensitivities of Page Rank and reinforcement ranking to changes in the graph topology, we establish
a few useful facts. First, an important feature of the reinforcement based authority function is that it decomposes over
disjoint subgraphs.

2
1
3

Historical
Rewards

6

4

subgraph 2
subgraph 1

5

Figure 1: A small graph example.
Proposition 1 (Disjoint Independence) For a graph consisting of separate subgraphs, the Rπ vector is given by the
union of the local Rπ vectors over the disjoint subgraphs.
(Straightforward consequence of the definition.)
As the Web grows, subgraphs are often added that have
only limited connection to the remainder of the web. In such
cases, the Rπ score remains largely unchanged, whereas Page
Rank is globally affected due to teleportation. In fact, merely
increasing graph size affects the Page Rank scores for a fixed
subgraph, since the teleportation vector changes.
Another independence property of reinforcement ranking
is that the Rπ score for altruistic subgraphs (subgraphs with
only outgoing and no incoming links) is not affected by any
external changes to the graph that do not impact altruism.
Proposition 2 (Altruistic Independence) The local Rπ vector for an altruistic subgraph cannot be affected by external
graph changes, provided no new incoming links to the subgraph are created. (Immediate consequence of the definition.)
Again, Page Rank cannot satisfy altruism independence
due to the global effect introduced by teleportation.
Intuitively, separate websites (i.e. separate subgraphs)
grow in a nearly independent manner. Reinforcement ranking is more stable with respect to independent subgraph
changes, since the stationary distribution of Page Rank must
react globally to even local changes. To illustrate the point,
consider the example in Figure 1. First, suppose the link
from 5 to 6 is not present; in which case the graph consists
of two disjoint subgraphs. For reinforcement ranking, any
local changes within the subgraphs (including adding new
pages) cannot affect the authority scores in the other subgraph, provided no connecting links are introduced between
them. However, the stationary distribution for Page Rank
must be affected even by disjoint updates. Next, consider
the effect of adding a link from 5 to 6, which connects the
two subgraphs. In this case, changes to the right subgraph
will still not affect the reinforcement scores of the left subgraph if no new links are introduced from the right to the left,
whereas Page Rank is affected. Finally, deleting the link from
node 1 to node 4 has no influence on node 2 under reinforcement ranking (only the successors of node 4 are influenced),
whereas the Page Rank of node 2 will generally change.

2

2

10

10

0

10

−2

10

Relative Error

Relative Error

0

uniform−Jan 2011
Oct 2010−Jan 2011
Nov 2009−Jan 2011
Oct 2008−Jan 2011
uniform−Oct 2010
Nov 2009−Oct 2010

−4

10

uniform−Jan 2011
Oct 2010−Jan 2011
Nov 2009−Jan 2011
Oct 2008−Jan 2011
uniform−Oct 2010
Nov 2009−Oct 2010

0

10

−2

10

−4

5

10
15
20
Number of Iterations

25

30

10

0

5

10
15
20
Number of Iterations

25

30

Figure 2: Convergence rate of Page Rank.

Figure 3: Convergence rate of reinforcement ranking.

The implication is that the reinforcement based authority
score is more stable to innocuous changes to the Web graph
than Page Rank, which has consequences for both the efficiency of the update algorithms as well as the quality of their
respective authority scores, as we now demonstrate.

the plots show the results for the (initialization, target) pairs:
color initializer
target ∆nodes% ∆links%
red Oct-2010 Jan-2011 +12, -4
+17, -8
green Nov-2009 Oct-2010 +18, -5
+39, -20
blue Nov-2009 Jan-2011 +19, -5
+46, -24
magenta Oct-2008 Jan-2011 +49, -18
+65, -41
Here, + indicates the percentage of new nodes/links added,
and - denotes the percentage nodes/links deleted between the
intial and target graphs. Figures 2 and 3 compare the relative
rate of convergence of Page Rank versus reinforcement
ranking. Note that, given its sensitivity, Page Rank is not
able to exploit a previous solution to significantly improve
the time taken to converge to a new solution for an updated
graph: uniform initialization performs as well. This confirms
Google’s report that historical update based power iteration
does not improve the accuracy for Page Rank [25]. By
contrast, reinforcement ranking exhibits far less sensitivity
and therefore demonstrates significantly faster convergence
when initialized from a previous graph’s score function.
Practically this means that, initialized with a historical
update from three months prior, the reinforcement score can
be computed about 10 times more accurately than with a
uniform initialization.
Ranking Quality. To assess the ranking quality of the two
methods we performed an experiment on the DBLP graph
[36], which consists of 1, 572, 278 nodes and 2, 083, 947
links. We chose this network because citation links are usually reliable, reducing the effects of spam and low quality
links. For this experiment, we used the same parameters as
before, except that for reinforcement ranking we used a history depth of 3.
To illustrate the ranking quality achieved by Page Rank and
reinforcement ranking, we show the highest ranked papers
according to each method in Tables 1 and 2 respectively. We
used the latest number of citation data retrieved from Google
Scholar on March 24, 2013 as the ground truth for paper
quality. Note that this oracle considers future citations that
are received four years later than the time of the link graph
was extracted. In addition, Google Scholar considers much

6 Experimental Results
We conducted experiments on real world graphs (Wikipedia
and DBLP) to evaluate two aspects of reinforcement ranking
and Page Rank. First, we compared these methods on the
updating problem: how quickly can the score function be updated given changes to the underlying graph? Second, we investigated the overall quality of the score functions produced.
Sensitivity and the Updating Problem. Intuitively, the
speed with which an iterative method can update its scores for
a modified graph is related to the sensitivity of its score function. If the score is not significantly affected by the graph update, then initializing the procedure from the previous scores
reduces the number of iterations needed to converge. Conversely, if the new score is significantly different than its predecessor, one expects that many more iterations will be required to converge. Indeed, we find that this is the case: Page
Rank demonstrates far more score sensitivity to graph modification, and consequently it is significantly outperformed by
reinforcement ranking in the updating problem.
To investigate this issue, we ran experiments on a set of
real world graphs extracted from Wikipedia dumps taken at
different times. In particular, we used graphs extracted from
dumps on Oct-2008, Nov-2009, Oct-2010 and Jan-2011.
These are large and densely connected graphs; for example, the Jan-2011 graph contains 6, 832, 616 articles and
144, 231, 297 links. For both methods, we used a unform random surfer policy, and a discount/dampening factor of 0.85.
For Page Rank, we set the teleportation vector to uniform,
and for reinforcement ranking we used uniform rewards. To
evaluate a given method’s ability to cope with graph updates,
we measured its rate of convergence to the new solution,
as well as the relative advantage of initializing from the
previous solution verus initializing uniformly. In particular,

Table 2: Top papers according to R3 (3-step history).

Table 1: Top papers according to Page Rank.
Rank
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

Paper Title
A Unified Approach to Functional Dependencies and Relations
On the Semantics of the Relational Data Model
Database Abstractions: Aggregation and Generalization
Smalltalk-80: The Language and Its Implementation
A Characterization of Ten Hidden-Surface Algorithms
An algorithm for hidden line elimination
Introduction to Modern Information Retrieval
C4
Introduction to Algorithms
Compilers: Princiles, Techniques, and Tools
Congestion avoidance and control
A Stochastic Parts Program and Noun Phrase Parser for ...
Illumination for Computer Generated Pictures
Graph-Based Algorithms for Boolean Function Manipulation
Programming semantics for multiprogrammed computations
Time, Clocks, and the Ordering of Events in a Distributed ...
Reentrant Polygon Clipping
Computational Geometry - An Introduction
A Computing Procedure for Quantification Theory
A Machine-Oriented Logic Based on the Resolution Principle
Beyond the Chalkboard: Computer Support for Collaboration
A Stochastic Approach to Parsing
Report on the algorithmic language ALGOL 60

#Cites
51
167
1518
5496
847
73
9056
20913
30715
11598
6078
1314
2504
8252
777
7720
373
8558
2579
4077
1079
42
646

more citation sources than DBLP. Although the results exhibit some noise, it is clear that the Page Rank scores in Table
1 are generally inferior: observe the prevalence of “outlier”
papers (italicized) that have very few citations. By contrast,
the reinforcement based ranking in Table 2 completely avoids
papers with low citation counts. Due to the relative purity of
the links in this graph, it is reasonable to expect a shallow history depth of 3 should be sufficient to safely identify influential papers in the reinforcement approach. On the other hand,
Page Rank which considers long term random walks, appears
to be derailed by noise in the graph and produces more erratic
results.

7 Discussion
A key challenge faced by Page Rank is coping with dangling
pages. Although some dangling pages genuinely do not have
any outlinks, many are left “dangling” simply because crawls
are incomplete. In practice, the number of dangling pages
can even dominate the number of non-dangling pages [13].
Page et al. (1998) first removed dangling pages (and the links
to them) before computing the Page Rank for the remaining graph, re-introducing dangling pages afterward. Such a
process, however, does not compute the Page Rank on the
original graph. Moreover, removing dangling pages produces
more dangling pages. In general, many approaches have been
proposed to solve this problem, but it does not appear to be
definitively settled for Page Rank; see, e.g., [13, 6]. This is
not a challenge for reinforcement ranking.
Recently, versions of Page Rank have been formulated using linear system theory (e.g., see [15, 6, 25]). However, the
justification for these formulations inevitably returns to random walks, teleportation, and the resulting stationary distributions. As we have observed, such foundations tend to lead
to globally sensitive ranking methods. Our work explains
and justifies a linear system formulation in a different way.
We generalize the teleportation vector to rewards that evaluate the intrinsic importance of individual pages. Moreover,

Rank
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

Paper Title
C4
Introduction to Algorithms
Introduction to Modern Information Retrieval
Smalltalk-80: The Language and Its Implementation
Compilers: Princiles, Techniques, and Tools
Graph-Based Algorithms for Boolean Function Manipulation
Computational Geometry - An Introduction
Congestion avoidance and control
Time, Clocks, and Ordering of Events in Distributed Sys...
Induction of Decision Trees
Mining Association Rules between Sets ...
A Performance Comparison of Multi-Hop Wireless ...
Fast Algorithms for Mining Association Rules ...
Highly Dynamic Destination-Sequenced ... Routing ...
A Stochastic Parts Program and Noun Phrase ...
Support-Vector Networks
A Machine-Oriented Logic Based on Resolution Principle
A Theory for Multiresolution Signal Decomposition...
An information-maximization approach to blind separation ...
The Anatomy of a Large-Scale Hypertextual Web Search ...
The Complexity of Theorem-Proving Procedures
Combinatorial Optimization: Algorithms and Complexity
A Computing Procedure for Quantification Theory

#Cites
20913
30715
9056
5496
11598
8252
8558
6078
7720
11561
12342
4936
13827
6731
1314
10523
4077
15897
5871
10122
4876
7050
2579

we have related the linear systems formulation to work in dynamic programming and reinforcement learning, via an accumulative rewards-based score function. It has previously
been observed that using a c near 1 in this linear formulation
still “often” converges, but the reason has not been well understood [15, 16]. However, we have shown that the authority
function can be well defined and guaranteed to converge for
any discount factor in (0, 1) and any well-defined surfing policy, without using teleportation.
There have been many attempts to formulate teleportation for more sophisticated ranking, such as personalization
[27, 20], query-dependent [30, 29], context-sensitive [18, 19],
and battling-link-spam ranking [17]. For example, the personalized Page Rank surfer teleports to the bookmarks of a
user. However, these practice still rely on the the stationary
distribution formulation for convergence. In fact, all these can
be even more naturally expressed in a reinforcement ranking framework, and thus convergence guaranteed. For example, the preferences of different users can be modeled by
different reward functions over pages, influenced by bookmarks. (Such reward functions can even be learned via inverse reinforcement learning, allowing convenient generalization across a large portion of the graph.) We can also explain why the pages linked by the bookmarked pages also
receive a high ranking, a fact first observed by [27]. In particular, the nonzero rewards received by a user on their bookmarked pages are also the historical rewards of the successor
pages of the bookmarked pages, hence the successor pages
are also rewarded.

8 Conclusion
Formuating and viewing Page Rank as the stationary distribution of random walks has been long recognized and practiced.
However, to gurantee the existence, stochasticity, ergodicity,
and irreducibility of the underlying Markov chain has to be
ensured. This is tricky for the case of Web, where there are
many dangling pages, sinks, and pages without any incoming
links. These problems are important to the theory and prac-

tice of Page Rank, for which there are many solutions and
discussions.
We proposed an authority function based on historical rewards. We used rewards to capture the intrinsic importance
of pages, without the need of teleportating and constructing
well behaved Markov chains. We related the authority function to the value function in dynamic programming and reinforcement learning, and showed that the authority function
satisfies a reverse Bellman equation. Thus, at a high level,
our work establishes a theoretical foundation for the recent
linear system formulation of Page Rank. We proved that our
authority function is well defined for any discount factor in
(0, 1) and any surfing policy, by referring not to the stationary distribution theory but to the contration mapping technique. Given that random walk models, a generalization of
Page Rank, have been used in various contexts, we believe
our work will contribute to the fields of information retrieval
and social networks.

