

I

In almost all situation assessment problems,
it is useful to dynamically contract and ex­
pand the states under consideration as assess­
ment proceeds. Contraction is most often used
to combine similar events or low probability
events together in order to reduce computa­
tion. Expansion is most often used to make
distinctions of interest which have significant
probability in order to improve 'the quality of
the assessment. Although other uncertainty
calculi, notably Dempster-Shafer [4], have ad­
dressed these operations, there has not yet been
any approach of refining and coarsening state
spaces for the Bayesian Network technology.

I
I
I
I
I
I
I
I
I
I
I
I
I

This paper presents two operations for refin­
ing and coarsening the state space in Bayesian
Networks. We also discuss their practical im­
plications for knowledge acquisition.

1

Introduction

Bayesian Networks [1], [2] is a technology for reason­
ing under uncertainty and has been used primarily to
address situation assessment problems (e.g., medical di­
agnosis, battlefield assessment). In situation assessment,
the problem is to infer the strength-of-belief (i.e., proba­
bility) in certain propositions given a set of internal be­
liefs (e. g., rules) and external evidence. In general, the
evidence about a situation does not come in all at once,
instead it is received over a period of time. As evidence
is received and beliefs are updated, some distinctions
which were previously irrelevant become relevant and
some distinctions which were previously relevant become
irrelevant. In general, it will be infeasible to consider all
possible relevant distinctions throughout the assessment
process due to resource limitations. Therefore an op­
portunistic approach is needed in which new states can
be added and existing states which are similar or in­
significant can be combined or removed dynamically as
the assessment proceeds. Other uncertainty calculi have
also recognized the importance of this problem, notably
Dempster-Shafer [4].
The introduction of new distinctions to a state space
refine• the state space and the removal of distinctions
coar1en1 the state space. These operations must fulfill

the intuitive constraint that their use must not afFect
beliefs which do not directly involve the refined or coars­
ened state space. This paper presents operations for re­
fining and coarsening the state spaces of Bayesian Net­
works. The inputs to the operations are a target node
and the desired refinement or coarsening. The outputs
of the operation are revised conditional probability dis­
tributions for the target node and for the target node's
successors which correspond to the modified state space
of the node.·
There are three important observations about these
operations. First, to satisfy the constraint that refine­
ment and coarsening operations do not affect the prob­
ability of states not involved in the operation, it is suf­
ficient that the operations do not affect the probability
of states in the "neighborhood" of the node under con­
sideration. It can be easily shown that this "neighbor­
hood" of a node is the Markov boundary of the node,
namely, the node's predecessors, successors, and succes­
sors' predecessors. In other words, if the joint probabil­
ity distribution of the blanket (other than node itself) is
not changed by the operation, then the joint probability
distribution of the entire network (other than the node
itself) will also be unchanged by the operation.
Second, since refinement operations "introduce" in­
formation to the network some judgements need to be
made about the relative weights of the new distinctions.
This can be done by modifying the relationships ( i.e.,
the conditional probabilities) between the refined vari­
able, its predecessors, and its successors. In order to
satisfy the Markov boundary condition described above I
cer tatn �onstraints need to be met in modifying these re.
lationships. The degree of freedom in assigning the new
probabilities is limited.
•

Third, while coarsening operations can always be ex­
act (i.e., satisfy the Markov boundary condition), the as­
.
soctated costs are high enough that it may be desirable to
make the operation approximate. In such circumstances
information may be lost due to the approximation. B
the loss of information, we mean that the resulting net­
work will have a different probability distribution than
the original one. However, if the states to be coarsened
are "similar" enough, the resulting impact will be small.

;

This paper is organized as follows. Section 2 describes
the refinement and coarsening operations. Section
presents some detailed examples. Some discussions and

3

I

476

I
I
I
I
Figure

1:

concluding remarks are given in Section

2

I

A Simplified Network

4.

I

The Refinment and Coarsening
Operations

In this section, two sets of related operations, one called
"external" and one calle d "internal", for refining and
coarsening a node's state space are presented. The node
to be refined or coarsened will be referred to as the "tar­
get" node. First set of operations is called "external"
since a new node is added "externally' to the target node
which is a successor to the target node and whose state
space is the desired modified state space. In the "in­
ternal" operation, however, the operation works "inter­
nally" on the node without changing the topology of the
original network.
The external operation is straight-forward. An exter­
nal node is added to the diagram which has no successors
and has the target node as its only predecessor. The
state space of the external node is the desired refine­
ment or coarsened state space of the target node. The
arc (conditional probabilities) between these two nodes
describe the mapping, either refinement or coarsening,
between their state spaces. The target node is then re­
moved from the graph probabilistically based on the arc
reversal and node removal operations [3]. This leaves
the external node in place of the target node in the new
graph. By doing so, an extra arc is introduced between
the predecessors and the successors of the target node.
The advantage of this approach is its simplicity. The
disadvantage of the approach is the change in the net­
work topology. In a dense network, this operation may
introduce many extra arcs.
An example of the operation is shown in Figures 1 and
2. Figure shows the original network. Suppose that z
is the target node. We first add an external node z1 as
the original node's successor with the desired new state
space. Figure 2 shows the resulting network after the
removal of the target node z. As can be seen, an extra
arc has been introduced between the predecessor and the
successor of z.
The internal operation refines and coarsens the state
space of a node without changing the topology of the

1

Figure

2:

Modifying Network with External Operation

networks. This operation takes four inputs:

• a state node (z) whose state space (0.) is to be
refined or coarsened,

I
I

• a relationship between (o.) and (0�) which specifies

I

• • auxiliary information, the Markov blanket of the

I

• a new state space (0�),
which values
which values

w.
w�

in o. are refined or coarsened into
in 0�.

node. The Markov blanket of z requires that the
state spaces of it's predecessors P. and successor's
predecessors P,. be specified as well as the proba­
bility distributions of it's successors s. ( see Figure

1).

The output of the operation are two sets of probability
distributions:
•

the new conditional distribution for

•

modified

z, p'(ziP.)

distributions for the successors

p'(•.lz, P,.)

and

of

z,

We specify the relationship between o. and 0� with
two mappings. The refinement mapping Refine maps
a single value in o. into multiple values in n� and the
coarsening mapping Coa?•en maps a single value in 0�
into multiple values in o•.
In the refinement operation, for those values w. in
o. which are refined into w� E Refine(w.) in 0�, an
obvious constraint of the new probability distribution is

I
I
I
I
I

(1)

I

Since w� does not have to be equally weighted, one needs
to make the judgements about the relative weights of the
new distribution.

I

p(w,.jP.)

=

L

c.��ER(c.�.j

p(�&��IP.)

I

477

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

The Markov boundary of the state node II' includes P.,

s., and Ps . They "shield" the node II' f'rom the rest of
the network. n can be easily shown that if the joint

probability distribution of the Markov boundary is not
aft'eded by the refinement operation, then the rest of the
network will not be afFected. To keep the joint probabil­
ity distribution of the Markov boundary the same before
and after the refinement operation, the constraint to be
satisfied is chosen as p(S.!P.,Ps.), namely,
p(S.!P.,Ps.)

In

=

:E. p(•.I��',P,.)p(��'IP.)

=

:E•• p(•.I��'',P,.)p(��''IP.•)

(2)

other words, for the value "'• to be refined,

p (••l"'•' P,.)p("'•IP.) =

:E

p(••l"'�' P,.)p("'�IP. )

(3)
need to be satisfied for all values of P.. An obvious solu­
tion satisfies the above constraints regardless ofp("'�IP.)
is,
(4)
p(••l"'�' P,.) = p(••l"'•' P,.)
This solution states that, regardless of how the condi­
tional probabilities p("'�IP.) being assigned, as long as
they satisfy eqn. (1), then i!the conditional probabilities
of the successors s. given the refined values are set to be
the same as that o!the original value,then the constraint
(3) is satisfied. This solution allows us to assign arbi­
trary proportions in the upper arc p("'�IP.) but leaves
no f'reedom in determining the lower arc p(••l"'�' P,.).
In general, the above solution may be too restridive.
In fact, if the proportions p("'�IP.) assigned in eqn. (1)
are the same for all the predecessor values, namely, if
p("'�IP.)
:Ew�eR(w.) p("'� IP.)

=

K("'�)

(5)

rhere K("'�) is a fundion depending only on "'�' then
qn.(3) can be reduced to a single constraint, i.e.,
p(••l"'•' P,.)

=

�

w�eR(w.)

p(••l"'�' P,.)K("'� ).

{6)

In this case, p (••l"'�' P,.) do not need to be the same as
p (••l"'•' P,.) and as long as they satisfy the constraint
(6), we have f'reedom to assign their numbers. In other
words, by imposing one more restridion (5) in obtaining
the upper arc, more f'reedom is allowed in choosing the
lower arc.
In the internal coarsening operation, for those values
"'• in o. which are coarsened, two constraints similar to
(1) and (3) need to be satisfied,

p("'�IP.) =
and
p(••l"'�' P,.) =

\

�

w.ec(w�)

p("'.IP.)

(7)

� p(••l"'•' P,.)p("'•IP.)
P.
p("'• •) w.ec(w�)
(8)

I! both of these constraints are satisfied, then the coars­
ening procedure is exact and the rest of the network
will not be afFected.
However, if no single value of
p('•'"'�,P,. ) can be found to satisfy (8) for all values
of P., then that means those "'• in C("'� ) cannot be
coarsened without changing the joint probability of the
network. In other words, some information may be lost
when "aggregating" those state values together and the
new network will be "inconsistent" with the old one. I!
that is desirable, one can either use the external opera­
tion described earlier or use the internal operation with
some approximation. If' the values to be coarsened are
"similar", namely, the values of p( '•'"'�, P,.) calculated
based on the right hand side of (8) with different val­
ues of P. are close, then the approximation will have
small impact on the rest of the graph. A reasonable
approximation under such situation will be to calculate
p(••I"'�,P,. ) as the average of the values obtained f'rom
(8).
3

Dlustration of the Operations

We illustrate the refining and coarsening operations for
both the external and internal approaches with the fol­
lowing examples. First consider the graph given in Fig­
ure 3. In this example, the root node M has two values,
Military Unit Type A and Type B. The second node V
has two values representing whether a vehicle exists in
a particular place and time. The terrain condition node
T has two values, good and bad. The feature node F
has'two predecessors, vehicle V and terrain condition T,
and has three values feature A, feature B, and feature
of Others. The original probability distribution of the
graph as well as the computed posterior probabilities of
each node given the evidence are also given in Figure 3.
In this example,suppose we are only interested in dis­
tinguishing whether there is a vehicle or not, which can
help us identifying the type of military unit. When the
posterior probability of the presence of vehicle becomes
very high as supported by evidence, we may become in­
terested in more details about the vehicle. Suppose, we
are interested in what type of vehicle it is, first we refine
the state value Y of node V into two values, Tank A and
Truck U. With the external operation, we can add an
artificial node VI, in which tank and truck are split, say
in a one to four ratio (see Figure 4). After removing the
original node V, the resulting graph, the corresponding
conditional probabilities and the posterior probabilities
of each node are also shown in Figure 4.

With the internal approach, first we assign probabili­
ties for the upper arc. As in the external approach, we
split vehicle into tank and truck with one to four ratio
and we assume it is independent of military unit type.
The new conditional probabilities of refined node V given
M is shown in Figure 5. In this case, the condition given
in eqn.(5) is satisfied, we therefore have f'reedom in as­
signing the conditional probabilities of the lower arc as
long as they satisfy the constraint given in eqn. (6),

478

namely,
[p(F;IV., 7J )p(V.IM•) + p( F;IV., 7J)p(V.IM•)]
p(V.IM•)
(9)
where F, is the i th value of node F. It can be seen
that this is a line equation in a tw�dimensional space.
For instance, for F, = a, 7j = g we have,

p(F.·,IV.•• T.:' )

=

-

0.45 = 0.2p(F.jV., T1) + O.Sp(F.)V., T1)
(10)
Therefore
any
pair
of values p(F.!V., T1) and p(F.!V., T1) between 0.0 and
0.9 (because p(F.IV., T1) = 0.1) and satisfy eqn. (10) are
feasible. Based on the constraints, we choose the feasible
conditional probabilities of node F as given in Figure 5.
The idea of choosing those numbers is that given vehicle
is a Tank, the probability of detecting feature A is much
higher than detecting feature B. On the other hand,
there is a slightly higher probability to detect feature B
than feature A from Truck. With these new arcs, the
posterior probabilities of each node given the evidence
are shown in Figure 5. As can be seen, other than node
V, the probabilities are the same as the one in Figure
4. Apparently, because of the new ares and because the
evidence favor feature B, the new posterior probability
of Tank is smaller.
We may also choose the upper are in such a way that
the split of vehicle between tank and truck depends on
the military unit. For example, as given in Figure 6, the
percentage of tank in type A military unit is assumed
to be much more than that in type B military unit. In
this case, the condition given in eqn. (5) is not satisfied,
the only solution that can satisfy eqn. (6) is the obvi­
ous solution given in eqn. (4), namely, the conditional
probabilities of node F given the refined values v. and
v. must be the same as that of the original value v,
as shown in Figure 6. The resulting posterior probabili­
ties also given in Figure 6 show visible changes in node
V. Note that, while it is possible in refinement to have
dift'erent ratio of splitting in the upper arc with the in­
ternal approach, it can not be done using the extemal
operation. As shown in Figure 4, the external operation
always produces the same ratio of splitting in the upper
arc which may not be desirable in certain cases.
With the refined network given in Figure 5 or Figure
6, if we coarsen the state values v. and V11 back into
v, using the internal operation, obviously, the results
will be the same as the one in Figure 3. However, in
many cases, no coarsening can be done without chang­
ing the joint probability distribution of the network. For
instance, if the conditional probabilities of the same net­
work is set to be the one given in Figure 7, then no single
value of p(F;Il'J, T1) can be found to satisfy (8) for all
values of M;. That means we either have to approximate
the coarsening or we can use the external operation. The
approximated values we use for p(F,Il'J, T,) is to take av­
erage of the values obtained from (8) as described in the
previous section. The resulting conditional probabilities
between F and V and the computed posterior probabil­
ities are also given in Figure 7. The results of extemal
operation which combine v. and Vu into v, are shown in
p(F.jV,, T,)

EV

Figure

3:

An Example Network

=

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

479

I
I
I
I
I
I

Concltllnlll

(i;i] l......,.�zw I
VA
v o.z
N 0.0
ll:.K::J

u
0.8
0.0

l'ftlll tlr HIIM VI

�-!

[W �

�

N

0.0
1.0
..d

I
I
I
I
I
I
I
I
I
I
I
I
I

Figure 4: Refined Network with External Operation

480
Figure 8. As can be seen, the results in Figures 7 and 8
are very similar, except in the ease of Figure 8, we have
introduced an extra are between node F and M.

4

Figure 5: Refined Network with Internal Operation I

Figure

TV A
G A 0.45
8 A 0.3
G U 0.45
e u 0.3
G N 0.1

0.45
0.3
0.1

0.4
0.1
0.4
0.8

e N o.z

o.z

0.6

6:

B
0.45
0.3

0
0.1

Refined Network with Internal Operation ll

Discussion and Conclusion

This paper has presented two operations, the external
operation and the internal operation for refining and
coarsening the state spaces of nodes in Bayesian Net­
works. The operations satisfy the constraint of leav­
ing unrelated probabilities in the network unchanged.
Through the refining or coarsening operations, one can
"emphasize" the important states in the analysis by re­
fining them or "de-emphasize" less important, similar
sta.tes by combining them at any point during the as­
sessment. These operations are especially useful when
the network is large and local changes are desired which
do not affect the rest of the network.
The refinment and coarsening operations have a dual
relationship. In general, information is being removed
in coarsening, and in refining information is being a.dded
to the network. Coarsening can "undo" refinement and
if no information is lost in coarsening, a refinement can
"undo" ·a coarsening. The refinement and coarsening
operations developed in this paper can be thought of as
"change of classification" operations in that they revise
the classification (e.g., discretization) scheme for a given
"axis" in a joint state space. A concrete example of this
is the splitting ofihe state Vehicle into the two substates
Tank and Truck as shown in Figure 4. This "change of
classification" operation is only one type of state space
modification. Another useful type of state space mod­
ification is the introduction or removal of classification
axes. This is easily accomplished by a.dding or removing
nodes from the network. For example, any new node
that has no successors will not change any of the rela­
tionships between existing nodes.
The external and internal operations are closely re­
lated. For the erlemal refinement operation one only
needs to specify the splitting ratio between the new val­
ues. The conditional probabilities of the upper arcs and
lower ares are then generated automatically. The new
ares created by the extemal operation are always redun�and can be removed wtfli:out any change to the

_]@it diStril:llitiea (see i'i� the m\etne:l teftne=

ment operation one can specify more information than in
the external operation since both the conditional proba­
bilities of the upper ares and lower ares can be specified
subject to certain constraints. Thus for refinement, the
external operation is a special case of the internal oper­
ation.
However, this relationship is reversed for the coarsen­
ing operation-the internal operation is a special case of
the extemal operation. The ability of the external oper­
ation to change the topology of the network allows any
states to be coarsened whereas in the internal operation
only i£ the constraints shown in equations (7) and (8)
are satisfied can coarsening be performed. It is intuitive
that only in such cases, the extra arcs create in the ex­
ternal operation are redundant. However since the main
idea in coarsening is to reduce the state space, the intro­
duction of new arcs, which is required in general, seems

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

481

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

Figure
ation

7:

Approximate Coarsening with Internal Oper­

Figure

8:

Coarsening with External Operation

I

482

counterproductive.
We feel an important application of this work is to the
knowledge acquisition process. For Bayesian Networks,
it is typical to first acquire, from an expert, the struc­
ture of a network. After the structure is determined,
the state space of each node is acquired from the expert
and lastly the probability distribution for each node is
acquired. The structure is acquired first since this knowl­
edge is the most robust cognitively. "Evidently, the no­
tion of relevance and dependence are far more basic to
human reasoning than the numerical values attached to
probability judgements ... Once asserted, these depen­
dency relationships should remain a part of the repre­
sentation scheme, impervious to variations in numerical
inputs."1• However this research shows there are definite
constraints between structure, states, and probabilities.
Consider the example shown in Figures and
Imag­
ine that the structure in F igure has been acquired and
a decision is being made about the state space of node V.
Consider the two possibilities: the state space of node V
is Y and N or the state space of node V is A, U, and
N. Imagine that we acquire the conditional probabili­
ties for each possibility and assume the expert gives his
"true" probabilities. Surprisingly, in general, the asso­
ciated joint probability spaces for these two possibilities
will be inconsistent. This leaves the issue of which etate
space possibility should be used. The intuitive answer
is one should choose the state space which contains the
"most information" but does not contain any "indistin­
guishable" states. In other words, a state space which is
big enough but not too big! We call this the "maximumly
distinguished" state space. The refining and coarsening
operations introduced in the paper allow a formal defi­
nition of this term.
A "maximumly distinguished" state space is a state
space which is both "irreducible" and "complete". An
"irreducible" state space is one in which no coarsen­
ing operation can be performed without making the
joint probability inconsistent (for the internal operation)
or without introducing unremovable new arcs (for the
external operation). Conceptually, a "complete" state
space is one which contains enough distinctions to cap­
ture all the expert's knowledge. Stated in another way,
a "complete" state space is a state space in which if
any state is refined into substates, then the expert can­
not distinguish between the substates. Formally then, a
"complete" state space is one in which the expert proba­
bilities on that state space can be reached by an internal
coarsening operation on the expert probabilities of any
more refined state.
This can be translated into broad guidelines for knowl­
edge acquisition as related to network structure and state
spaces. The knowledge engineer should first determine
the structure of the network. Second, he should order the
nodes such that the predecessors of a node are always be­
fore the node. He should then determine the state space
and probability distribution of each node according to
the order. This should be done by refining the state
space of each node step-by-step, eliciting probabilities
for each candidate state space. When the probabilities

7

1

[2), p.

79

7

8.

of a refined state space are "consistent" with a coarser
state, prefer the coarser state. If a more refined state
C:annot be found after some search, then that acquisition
of knowledge for that node can be considered complete.

I
I

