

and those that exploit structures in the probability ta­
bles (e.g.

[3], [1]).

We are interested in exploiting structures in the prob­

This paper explores the role of independence

ability tables induced by independence of causal influ­

of causal influence (ICI) in Bayesian network

ence

inference. ICI allows one to factorize a con­

(ICI).

Heckerman

ditional probability table into smaller pieces.

The concept of ICI was first introduced by

[3]

under the name causal independence.

It refers to the situation where multiple causes inde­

We describe a method for exploiting the fac­

pendently influence a common effect. We use the term

torization in clique tree propagation (CTP)

"independence of causal influence" instead of "causal

- the state-of-the-art exact inference algo­

independence" because many researchers have come

rithm for Bayesian networks. We also present

to agree that it captures the essence of the situation

empirical results showing that the resulting

better than the latter.

algorithm is significantly more efficient than
the combination of CTP and previous tech­

Knowledge engineers had been using specific models of

niques for exploiting ICI.

ICI in simplifying knowledge acquisition even before
the inception of the concept

Keywords:

Bayesian networks,

causal influence

(causal

independence of

independence),

inference,

clique tree propagation.

[13}

and Heckerman

be used to simplify the structures of Bayesian networks

[8])

[16],

made the observation that ICI

into smaller pieces and showed how the
Howard and Matheson

are a knowledge representation framework widely

used by AI researchers for reasoning under uncertainty.
They are directed acyclic graphs where each node rep­
resents a random variable and is associated with a
conditional probability table of the node given its par­
ents. This paper is about inference in Bayesian net­
works.

([23])

enables one to factorize a conditional probability table

INTRODUCTION

Bayesian networks (Pearl

Olesen et al

([5], [1 3]).

have also shown how ICI can

so that inference can be more efficient.

Zhang and Poole

1

[3]

There exists a rich collection of algorithms.

The state-of-the-art is an exact algorithm called clique

VE algorithm

- another exact inference algorithm - can be
tended to take advantage of the factorization.

ex­

This

paper extends CTP to exploit conditional probability
table factorization. We also present empirical results
showing that the extended CTP is more efficient than
the combination of CTP and the network simplifica­
tion techniques. In comparison with Zhang and Poole

[23], this paper presents a deeper understanding ofiCI.
The theory is substantially simplified.

tree propagation1 (CTP) (Lauritzen and Spiegelhalter

[12],

Jensen et al

[10],

and Shafer and Shenoy

[20]).

Unfortunately, there are applications that CTP can­
not deal with or where it is too slow (e.g. [18]). Much
recent effort has been spent on speeding up inference.
The efforts can be classified into those that approxi­
mate (e.g.
1

[15), [2), [9], [6], [7], [17], [22], [11],

Also known

as

junction tree propagation.

and

[19])

2

BAYESIAN NETWORKS

A Bayesian network (BN) is an annotated directed
acyclic graph, where each node represents a random
variable and is attached with

a

conditional probabil­

ity of the node given its parents. In addition to the
explicitly represented conditional probabilities, a BN
also implicitly represents conditional independence as-

482

Zhang and Yan

sertions. Let x1, x2,
, Xn be an enumeration of all
the nodes in a BN such that each node appears before
its children, and let rrz, be the set of parents of a node
x,. The following assertions are implicitly represented:

is conditionally independent of all other
all other ej's given c;, and

• • •

For i=l, 2, ... n, x; is conditionally indepen­
dent of variables in {xt,X2,····x•-d\rrz,
given variables in 1l",w
The conditional independence assertions and the con­
ditional probabilities t ogether entail a joint proba bility
over all the variables. As a matter of fact, by the chain
rule, we have
n

P(z1, x2, . . . , zn.)

=

IJ P(z;lxll x2, . . . , Xi-1 )

i=1
n

IT P(z;l1l"z1),

(1)

i=1

where the second equation is true because of the con­
ditional independence assertions and the conditional
probabilities P(x;l'lr:r:,) are given in the specification of
the BN. Consequently, one can, in theory, do arbitrary
probabilistic reasoning in a BN.
3

Bayesian networks place no restriction on how a node
depends on its parents. Unfortunately this means that
in the most general case we need to specify an expo­
nential (in the number of parents) number of condi­
tional probabilities for each node. There are many
cases where there is structure in the probability ta­
bles. One such case that we investigate in this paper
is known as independence of causal influence (ICI).
The concept of ICI was first introduced by Heckerma.n
[4]. The following definition first appeared in Zhang
and Poole [24].
In one interpretation, arcs in a BN represent causal
relationships; the parents c1, c2, . .. , Cm of a node e
are viewed as causes that jointly bear on the effect e.
,
ICI refers to the situation where the causes c1o c2
and Cm contribute independently to the effect e. In
other words, the ways by which the c;'s influence e are
independent.
•

• •

More precisely, Ct, c2
, and Cm are said to influence
e independently if there exist random variables �1, �2
... , and �m that have the same frame- set of possible
values - as e such that
• • •

C;

and

There exists a commutative and associative bi­
nary op erato r * over the frame of e such that
e == el *6* ... *�m·

We shall refer to ei as the co ntrib ution of e; to e. In less
technical terms, causes influence their common effect
independently if individual contributions from differ­
ent causes are independent and the total influence is a
combination of the individual contributions.
We call the variable e a convergent variable for it
is where independent contributions from different
sources are collected and combined (and for the lack of
a better name). Non-convergent variables will simply
be called regular variables. We also call * the base com­
bination operator of e. Different convergent variables
can have difference base combination operators.
The reader is referred to [24] for more detailed expla­
nations and examples of ICI.
The conditional probability table P(elct, . .. , em ) of a
convergent variable e can be factorized into smaller
pieces. To be more specific, let /; (e, e;) be the function
defined by

fi(e= a, e;) = P(ei=aie;),

INDEPENDENCE OF CAUSAL
INFLUENCE

1. For each i, e. probabUistically depends on

2.

Cj 's

and

for each possible value o of e. It will be referred to
as the contributing factor of e; to e. Zhang and Poole
[24] have shown that

P(elc11 .. . , Cm)

=

®�di(e, c;),

where ® is an operator for combining factors to be
defined in the following.
Assume there is a fixed list of variables, some of which
are designated to be convergent and others are desig­
nated to be regular. We shall only consider functions
of variables on the list.
Let f(et, ... ,e�c , A, B) and g(e1, . . . ,e�c,A,C) be two
functions that share convergent variables et, .. . , e,.
and a list A of regular variables. B is the list of vari­
ables that appear only in I, and C is the list of vari­
ables that appear only in g. Both B and C can contain
convergent variables as well as regular variables. Sup­
pose *i is the base combination operator of ei· Then,
the combination f®g off and g is a function of vari­
ables e1,
, e1c and of the variables in A, B, and C.
It is defined by
. • .

j®g(et=a11

• • •

,e�c=a�c, A,B, C)

f(et =au, . . .,e�c=akl, A, B)
g(et =Otz, . . . , e�c=a�cz, A, C), (2)

Independence of Causal Influence and Clique Tree Propagation

constitute

483

heterog e­

a

neous factorization of P(a, b,c, e1,e2, e3 ) because the

joint probability can be obtained by combining those

factors in a proper order using either multiplication or
the operator

®. The word heterogeneous is

to signify

the fact that different factor pairs might be combined

in different ways. We shall refer to the factorization
as the heterogeneous factorization represented by the

BN in Figure 1.
The heterogeneous factorization is of finer grain than
the homogeneous factorization. The purpose of this

Figure 1: A Bayesian network.

paper is to exploit such finer-grain factorizations to
speed up inference.

write

e,. We shall sometimes
/ ®gas /(e1,••. ,e�:,A,B)®g(eb ... ,e�:, A,C) to
make explicit the arguments of f and g.

5

The operator ® is associative and commutative. W hen

In a heterogeneous factorization, the order by which
factors can be combined is rather restrictive. The con�

for each possible value ai of

f and g do not share convergent variables,

j®g is sim­

ply the multiplication fg.

4

tributing factors of a convergent variable must be com�
bined with themselves before they can be multiplied

FACTORIZATION OF JOINT
PROBABILITIES

A BN represents a factorization of a joint probability.
For example, the Bayesian network in Figure 1 factor­
izes the joint probability

DEPUTATION

P(a,b,c,et,e2,ea)

into the

following list of factors:

P(a), P (b),P (c), P(e1la,b, c), P(e2la ,b, c), P(ealel,e2).

with other factors. This is the main issue that we need
to deal with in order to take advantage of conditional
probability table factorizations induced by

To alleviate the problem, we introduce the concept of
deputation. It was originally defined in term of BNs
[24]. In this paper, we define it in terms of heteroge­

neous factorizations themselves.

In the heterogeneous factorization represented by
e is to make

BN, to depute a convergent variable
copy

e1

of

e and

ing factors of
The joint probability can be obtained by multiply­
ing the factors.

We say that this factorization is

multiplication-homogeneous because all the factors are
combined in the same way by multiplication.
Now suppose the e,'s are convergent variables. Then
their conditional probabilities can be further factorized

as

follows:

replace

e with e 1

e. The variable

a
a

in all the contribut­

e' is called

the

deputy of

e and it is designated to be convergent. After depu­

tation, the original convergent variable e is no longer
convergent and is called a new regular variable. In con�
trast, variables that are regular before deputation are

called old regular variables.

After deputing all convergent variables, the heteroge­
neous factorization represented by the BN in Figure 1
becomes the following list of factors:

P(e1la, b, c)

=

P(e2!a,b,c)

=

P (ealell e2)

=

where the

ICI.

/u (e1, a)®!I2{e11 b)®ha(el! c) ,
h1(e2, a)®!22 (e2 , b)®ha(e2, c),

/u(ei,a), /1 2(e;,b), !Ia( e;,c), !21(e;,a),
/22(e�,b), /23(e;,c), /31(e�,e1), /32{e�,e2),
P(a), P(b), P(c).

/a1(ea, et)®/a2(ea,e2),

factor !11 {e1,a),
a to e1•

for instance, is the con­

tributing factor of

We say that the following list of factors

The rest of this section is to show that deputation
renders it possible to combine the factors in arbitrary
order.

fu (e1, a),i12(e11 b),fta(el, c),

5.1

/2I(e2,a),/22(e2,b),f2a(e2,c),
fai(ea,e1), /a2(ea, e2),
P(a),P(b), and P(c)

replace it with the corresponding new regular variable

ELIMINATING DEPUTY VARlABLES
IN FACTORS

Eliminating a deputy variable e1 in a factor f means to

484

e.

Zhang and Yan

The resulting factor will be denoted by

be more specific, for any factor
a list A of other variables,

f(e,e',A)

fle•=e·

Procedure

To

of e, e' and

1. If x1 is a new regular variable, remove
from F all the factors that involve the

fle•=e(e=a, A) = f(e=a, e'=a, A),
for each possible value
of

e'

and a list

A of

a

of

e.

deputy x� of :lh, combine them by 0 re­
sulting in, say, f . Add the new factor

For any factor f(e', A)

flz�=z1

other variables not containing e,

e.

volve x1, combine them by using ® re­
sulting in, say, g. Add the new factor g

For any factor f not

to

3.

Suppose f involve two deputy variables e� and e� and
we want to eliminate both of them. It is evident that
the order by which the deputy variables are eliminated
does not affect the resulting factor. We shall denote
the resulting factor by

5.2

/le� =e1 ,e;=e2

Return

:F.

Theorem 2 The
list
of
factors
returned
by sumoutc(:F, x1) is a ®-homoogeneous factorization
of P(xz, ... , Xn ) = E:�:1 P(xt. x2, ... ,xn)·
7

MODIFYING CLIQUE TREE
PROPAGATION

For later convenience, we introduce the concept of ®­
homogeneous factorization in term of joint potentials.

, Xn be a list of variables. A joint poten­
tialP(xl, x2, . . . , Xn ) is simply a non-negative function
• • •

o f the variables. Joint probabilities are special joint
potentials that sum to one.
Consider a joint potential P(et, ... , e�::, x1::+1, . . . , Xn )
of new regular variables e, and old regular variables
Xi.

:F.

•

®-HOMOGENEOUS
FACTORIZATIONS

Let :z:1, :z:2,

to F. Endif

2. Remove from F all the factors that in­

fle•=e(e=a, A) = f(e'=a, A ),
for each possible value o of
invol ving e', fle•=e =f.

sumoutc(F, x1)

A list of factors /t, ..., fm of the e,'s, their

Theorem 2 allows one to exploit ICI in many inference
algorithms, including VE and CTP. This paper shows
how CTP can be modified to take advantage of the
theorem. The modified algorithm will be referred to
as CTPI. As CTP, CTPI consists of five steps; namely
clique tree construction, clique tree initialization, ev­
idence absorption, propagation, and posterior proba­
bility calculation. We shall discuss the steps one by

one. Familiarity with CTP is assumed.

'

deputies eL and the xi s is a ®-homogeneous factor­

ization (reads circle cross homogeneous factorization)
, Xn ) if

of P(el! ... , e�:, Xk+l•

P(el,.

. .

'e�::, Xk+l•

7.1

CLIQUE TREE CONSTRUCTION

• . •

• • •

' Xn )

=

(®�di)le�=et,... ,e�=e....

Theorem 1 Let :F be the heterogeneous factorization
represented by a BN and let :F' be the list of factors
obtained from :F by deputing all convergent variables.
Then :F' is a ®-homogeneous factorization of the joint
probability entailed by the BN.
All proofs are omitted due to space limit. Since the
operator ® is commetative and associative, the theo­
rem states that factors can be combined in arbitrary
order after deputation.

A clique is simply a subset of nodes.

A clique tree

is a tree of cliques such that if a node appear in two
different cliques then it appears in all cliques on the
path between those two cliques.
A clique tree for a BN is constructed in two steps: first
obtain an undirected graph and then build a clique tree
for the undirected graph. CTPI and CTP differ only
in the first step. CTP obtains an undirected graph
by marrying the parents of each node (i.e. by adding
edges between the parents so that they are pairwise
connected) and then drop directions on all arcs. The
resulting undirected graph is called a moral graph of
the BN.

6

SUMMING OUT VARJABLES

Summing out a variable from a factorization is a funda­
mental operation in many inference algorithms. This
section shows how to sum out a variable from a ®­
homogeneous factorization of a joint potential.
Let

:F be a ®-homogeneous factorization of a joint po­

In CTPI, only the parents of regular nodes (represent­
ing old regular variables) are married. The parents of
convergent nodes (representing new regular variables)
are not married. The clique tree constructed in CTPI
has the following properties: (1) for any regular node
there is a clique that contain the node as well as all its
parents and (2) for any convergent node e and each of

tential P(x1, x2, . . , xn). Consider the following pro­

its parents x there is a clique that contains both e and

cedure.

x.

.

485

Independence of Causal Influence and Clique Tree Propagation

7.2

7.3

CLIQUE TREE INITIALIZATION

EVIDENCE ABSORPTION

Suppose a variable x is observed to take value a. Let
Xz=:a:(:z:) be the function that takes value 1 when :t==a

CTPI initializes a clique tree as follows:
1. For each regular node, find one clique that con­

tains the node as well as all it s parents and attach
the conditional probability of the node to that
clique.

and
that

0 otherwise. CTPI absorbs the piece of evidence
x=a as follows: find all factors that involve x and

multiply X:�:=a(x) to those factors.
Let

Xm+l,

O'm+b •

2.

,

Xn

be all the observed variables and

be their observed values.

Let Xt,

. • •

,

After evidence ab­

sorption, the factors associated with the cliques con­

(a) H there is a clique that contains the node and
all its parents, regard e as a regular node and

stitute a ®-homogenous factorization of joint poten­
tialPx
( t, ... 1Xm,Xm+l=am+11
,Xn=an) ofx11 . . . ,
• • •

Xm•

1.

( b) Otherwise for each parent

• • •

, an

Xm be all unobserved variables.

For each convergent node e

proceed as in step

. .

x

be the contributing factor of

of e, let

x to e.

f (x, e)

Find one

clique that contains both e and x, attached
to that clique the factor f(x,e'), where e' is
the deputy of e.

7.4

CLIQUE TREE PROPAGATION

Just as in CTP, propagation in CTPI is done in two
sweeps. In the first sweep messages are passed from
the leaf cliques toward a pivot clique and in the sec­
ond sweep messages are passed from the pivot clique

After initialization, a clique is associ ated with a list

toward the leaf cliques.

Unlike in CTP where mes­

sages passed between neighboring cliques are factors,

(possibly empty) of factors.

in CTPI messages passed between neighboring cliques

A couple of notes are in order. Factorizing the condi­

are lists of factors.

tional probability table of a convergent variable e into

Let C and C' be two neighboring cliques. Messages
can be passed from C to C' when C has received mes­

smaller pieces can bring about gains in inference ef­

fidency because the smaller pieces can be combined
with other factors before being combined with them­
selves, resulting in smaller intermediate factors.

H

there is a clique that contains e and all its parents,

then all the smaller pieces are combined at the same
time when processing the clique. In such a case, we are
better off to regard

e

as a regular node (representing

Procedure

this intuition.
On the other hand, if all variables that appear in one
factor f in the list also appear in another factor g in
the list, it does not increase complexity to combine f
and g.

Thus we can reduce the list by carrying out

such combinations. Thereafter, we keep the reduced
list of factors and combine a factor with others only
when we have to.

Since ® is commutative and associative, the factors
associated the cliques constitute a ®-homogenousfac­
torization of the joint probability entailed by the BN.

End­

2. Red uce the list F offactors and send

the

reduced list to C'.

visable to do the same in CTPI because the factors in­

and leads to inefficiency. Experiments have confirmed

sumoutc(F, Xi ) ,

=

for

combined at initialization and the resulting factor still
involves only those variables in the clique. It is not ad­

of new regular variables in the clique. Combining them

,x1

send.Message(C, C')

1. For i=l to l, :F

Second, in CTP all factors associated with a clique are

all right away can create an unnecessarily large factor

• • •

all the variables in C\C'. Let :F b e the list of the
factors associated with C and the factors sent to C
from all o ther neighbors of C. Messages are passed
from C t o C' by using the following subroutine.

an old regular variable).

volve not only variables in the clique but also deputies

Xt,

sages from all the other neighbors. Suppose
are

7.5

POSTERIOR PROBABILITIES

a clique and let x1,
, x1 be
all unobserved variables in C. Then the factors asso­

Theorem 3 Let C be

• • .

ciated with C and the factors sent to C from all its
neighbors constitute a ®-homogeneous factor ization of

P(Xt, ... 1 Xl1 Xm+l ='Om+11

• • •

, Xn=an).

Because of Theorem 3, the posterior probability of any

unobserved variable x can be obtained as follows:
Procedure

getProb(x)

1. Find a clique C that contains x. (Letx2,
... , x1 be all other variables in C. Let F
be the list of the {actors associated with

486

Zhang and Yan

a from

summing out variable

the list h of factors. It

is the following list of one factor:

{P.H4(e�,e;)},
where /.'l-+ 4(eL e;) = Ea J>(a)fu(ei, a)ht (e�,a).
Messages from cliques 2 and 3 to 4 are similar.
To figure out the message from clique
3

and sent to clique

Figure

2:

cliques

4

2 and 3 is:

from the list of factors. Summing out

=

for

f

ea

Hence the message is the following list of factors:

be the

{J.t2-+4(e�,e;)®J.ts-+4 (e�,e;),tJI(et,e2)}.

resulting factor.

4. If x is a new regular
flz•=f E:r: / l:r:•=:r:·
5. Else return f / E:r: f.

results in

tJI(et,e2)= L J>(ealet,e2)Xe1=a-(et)·

sumoutc(:F, x,), End­

Combine all factors in :F. Let

e3

a new factor

neighbors.)

2. For i= 2 to l, :F

variable return

where the first two factors are combined due to factor
list reduction. Messages from clique

3 are

4 to

cliques

2 and

similar.

Consider computing the posterior probability of

AN EXAMPLE

ea .

T he only clique where we can do this computation is
clique

A clique tree for the BN in Figure 1 is shown in Figure

2.

4 from

The message is obtained by summing out the variable

C and the factors sent to C from all its

8

1, we

{J>(ealet, e2)X<!1=a-(et), /.'2-+4(e�,e;),f.l3-+4 (e�,e;)}.
A clique tree for the BN in Figure 1.

e3

3.

4 to clique

notice that the list of factors associated with clique

4.

The list of factors associated with clique 4

and factors sent to clique

4 from all its

neighbors is

After initialization, the lists of factors associated

{J>(ealet,e2)Xe1=a-(et),
/.'2--+4(eJ., e2), 1'3-+4(e1,e2)}

with the cliques are as follows:

lt =
l2 =
la =
l4 =

{J>(a)ftt(e�,a),/2t{e;,a)},
{J>(b)!t2(e�,b),/22(e;,b)},
{J>(c)fta(e�,c),/2a(e;,c)},
{J>(ea!eh e2)}.

There are two variables to sum out, namely
Assume

tion because they do not share convergent variables.
and all its parents appear in clique

4,

its conditional probability is not factorized. It is hence
regarded as an old regular variable.

ing the piece of ev idence changes the list of factors

4 to

Then

e1

And then

e;

=

e1

is eliminated, yielding a new factor

e2

=

chosen to be the pivot. Then mes-

sages are first propagated from cliques
clique

4

and then from clique

The message from clique

1

4

1, 2,

to cliques 1,

to clique

4 is

and

3

to

2, and 3.

obtained by

Finally,

cf>2(e2, e;,ea)le;=w

is summed out, yielding a new factor

¢4(ea)

4 is

yielding a new

l: P(ealel,e2)Xe1=a-(et)¢t(et,e�).

¢a(e2,ea)

the following:
And then

Suppose clique

e1 and e2.

The first step

itself is summed out, yielding a new factor

Since

J>(eslell e2) is the only factor that involves et, absorb­
associated with clique

is to eliminate

e 2.
e i,

¢1(e1,e;) =
{mul-+4(e� ,e�)®mu2-+4(e�,e;)®mua-+4(ei,e2)]le�=e1•

¢2(e2,e;,ea)

Suppose e1 is observed to t ake value a.

e'

factor

tion and combination of factors reduces to multiplica­

e3

is summed out before

in summing out

Several factors are combined due to factor list reduc­

Also because

e1

==

L ¢a(e2,ea).
e2

487

Independence of Causal Influence and Clique Tree Propagation

9

EMPIRICAL COMPARISONS

100

WITH OTHER METHODS

This section empirically compares CTPI with CTP.
We also compare CTPI with PD&CTP, the combina­
tion of the parent-divorcing transformation [14] and
CTP, and with TT &CTP, the combination temporal
transformation [4] and CTP.

'CTP'

�
:;::

NN
145
145
245
245

NN:

number of nodes;
average number of parents;
average number of possible values of
a node.

AN-PN:
AN-PVN:

AN-PN
1.14
1.14
1.45
1.45

AN-PVN
2.0
2.27
2.0
2.25

Since clique tree construction and initialization need
to be carried out only once for each network, we shall
not compare in detail the complexities of algorithms
in those two steps, except saying that they do not dif­
fer significantly. Computing posterior probabilities af­
ter propagation requires very little resources compared
to propagation. We shall concentrate on propagation
time.
In standard CTP, incoming messages of a clique are

combined in the propagation module after message
passing. In CTPI , on the other hand, incoming mes­
sages are not combined in the propagation module.
For fairness of comparison, the version of CTP we
implemented postpones the combination of incoming
messages to the module for computing posterior prob­
abilities.
Let us define a case to consist of a list of observed vari­
ables and their observed values. Propagation time and
memory consumption varies from case to case. In the
first three networks, the algorithms were tested using
150 randomly generated cases consisting of 5, 10, or
15 observed variables. In the fourth network, only 15
cases were used due to time constraints. Propagation
times and maximum memory consumptions across the
cases were averaged. The statistics are in Figure 3,
where the Y-axises are in logscale. All data were col­
lected using a SPARC20.

10

i

l

The CPCS networks [19] are used in the comparisons.
They are a good testbed for algorithms that exploits
ICI since all non-root nodes are convergent. The net­
works vary in the number of nodes, and the average
number of parents of a node, and the average number
of possible values of a node (variable). Their specifi­
cations are given in the following table.
Networks
Network 1
Network 2
Network 3
Network 4

·K-­

'POCTP' ·9·-­
'TTCTP' -+-­
'CTPI' -

0.1

0.01

1

1e+08

:::0

f

3

4

3

4

Networlls
"CTP' ·1+­
'PDCTP" -ra-·­
'TTCTP' -+-·­

'CTPI'-+-

1e+07

t

2

1e+06

100000

10000

1

2

Networl<s

Figure 3: Average space and time complexities of CTP,
PD&CTP, TT&CTP, and CTPI on the CPCS net­
works.
We see that CTPI is faster than all other algorithms
and it uses much less memory. In network 4, for in­
stance, CTPI is about 5 faster than CTP, 3 times faster
than TT&CTP, a.nd 3.5 times faster than PD&CTP.
On average it requires 7MB memory, while CTP re­
quires 15MB, TT&CTP requires 22MB, and PD&CTP
require 17MB.
The networks used in our experiments are quite sim­
ple in the sense that the nodes have a average number
of less than 1.5 parents. As a consequence, gains due
to exploitation of ICI and the differences among the
different ways of exploiting ICI are not very signifi­
cant. Zhang and Poole [24] have reported experiments
on more complex versions of the CPCS networks with
combinations of the VE algorithm and methods for ex­
ploiting ICI. Gains due to exploitation of ICI and the
differences among the different ways of exploiting ICI
are much larger. Unfortunately, none of the combina­
tions of CTP and methods for exploiting ICI was able
to deal with those more complex network; they all ran
out memory when initializing clique trees.
The method of exploiting ICI described in this paper
is more efficient than previous method because it di-

488

Zhang and Yan

rectly takes advantage of the fact that ICI implies con­

[9]

F., Jensen and S. K. Andersen (1990), Approxima­
tions in Bayesian belief universes for knowledge-based
systems. in Proceedings of the Sixth Conference on Un­
certainty in Artificial Intelligence, Cambridge, MA,
pp. 162-169.

[10]

F. V. Jensen, K. G. Olesen, and K. Anderson (1990),
An algebra of Bayesian belief universes for knowledge­
based systems, Networks, 20, pp. 637 - 659.

[11]

U. Kjrerulff (1994), Reduction of computational com­
plexity in Bayesian networks through removal of weak
dependences. in Proceedings of the Tenth Conference
on Uncertainty in Artificial Intelligence, pp. 374-382.

ditional probability factorization, while previous meth­
ods make use of implications of the fact.

10

CONCLUSIONS

We have proposed to method for

exploiting

ICI

in

CTP. The method has been empirically shown to

be more efficient than the combination of CTP and
the network simplification methods for exploiting ICI.
Theoretical underpinnings for the method have their
roots in Zhang and Poole

[24]

and are significantly

simplified due a deeper u nderstanding of ICI.

[12)

ACKNOWLEDGEMENT
This

paper

has

David Poole.

Kong Research
and

Sino

benefited

from

discussions

with

[13]

K. G. Olesen, U. Kjrerulff, F . Jensen, B. Falck, S.
Andreassen, and S. K. Andersen (1989), A MUNIN
network for the median nerve - a case study on loops,
Applied Artificial Intelligence 3, pp. 384-403.

[14]

K. G. Olesen and S. Andreassen (1993), Specifica­
tion of models in large expert systems based on
causal probabilistic networks, Artificial Intelligence in
Medic ine 5, pp. 269-281.

[15]

J. Pearl {1987), Evidential reasoning using stochastic
simulation of causal models, Artificial Intelligence, 32,
pp. 245-257.

[16]

J. Pearl (1988), Probabilistic Reasoning in Intelligence
Systems: Networks of Plausible Inference, Morgan
Kaufmann Publishers, Los Altos, CA .

[17]

D. Poole

Research was supported by Hong
Council under grant

HKUST658/95E
grant

Software Research Center under

SSRC95 /96.EG01.
