
The paper describes aHUGIN, a tool for cre­
ating adaptive systems. aHUGIN is an exten­
sion of the HUG IN shell, and is based on t he
methods repo r ted by Spiegelhalter and Lau­
r itzen {1990a). The adaptive systems resu lt­
ing from aHUGIN are able to adj u s t the con­
ditional probabi lities in the modeL A short
analysis of the adap tation task is gi ven and
the fe atures of aHUGIN are des cribed. Fi­
nally a sess i on with experiments is reported
and the results are discussed.

1

Introduction

With the revival of Bayesian methods in decision sup­

port systems ( Shachter 1986; Pearl 1 988; Shafer and
Pearl 1990; Andreassen ei al. 1991b) mainly due to

the construction of e ffi cien t methods for belief re vi­

sion in causal probabilistic networks (Pearl 1988; L au­
ritzen and Spiegelhalter 1988; Andersen d a/. 1989;

et al. 1990; Shenoy and Shafer 1990), the
process of knowledge acq uisit ion under the Bayesi an
Jensen

paradigm has become increasingly important. Wh en
construc ti ng causal probabilistic network models, var­
ious sources may be used, ranging from ignorance over
experts' subjective assessments to well established sci­
entific theories and statistical models based on large
database s . Very often a mod el is a mixtu re of contri­
butions from sources of different epistemological char­
acter.

Sometimes these contributions do not coin ci de , and

the model is a mediation between them; sometimes the
(ignorance has, for ex­
ample, forced crude 'guesses' on certain distributions);
sometimes the model must vary with contexts which
cannot be specified beforehand; sometimes the domain
is drifting ov er time - requiring the mo del to drift

r esu lt in g model is incomplete

along with i t, and sometimes the model quite s i mply
does not reflect the real world pr op erly.
All the proble ms listed above call for pr ocedures which
enable the sy stem to mod ify the mo d el t h ro ugh ex p eri-

Finn V . .Tens en

ence. We call such an a ctiv ity adaptation, and sy stems

p erfo rm in g automatic adaptation
tems.
we

Note that

we

call adaptwe

sys­

have chosen t.o distinguish adaptation

which we usc to describe the activity
or creat in g models by hatch-pmccssing of large data.
from training,

( 10D2)

bases. In Spiegelhalter el rd

called

leaming and they

arc

both activities are

dist-inguished as sequent-i a l

learning and batch learning.

W h en using adaptat.ion

we <He us i ng the analogy to tlw n ot i on

of adaptive
reg u lato rs in control t h eo r y . llopefully, t h is abundance
of te rm i n o logy will not. confuse the reader completely.
The present paper descri hes all UG IN, a tool for cre­
ating adaptive systems. Tlw system, which is an ex­
tension of

IJUGIN ( Andersen

ct a/.

198\J), is based

on methods reported in Spiegellta.lt.er <Hid Lau ntzen
(l!.l!.lOa), see also Spiegelhaltcr and Lamit.zcn (1090b),

and th e adapt ive s y s te ms result in g from aliUGIN arc
able to adjust the con d i t i on a l probabilities in
mo d el .

sented

the

In a l l UGIN the mo d el is compactly repre­

by

a

contingency

table of i m a gi nary counts,

the adaptation p roced u re is
counts in this table.

a

and

proccs� of 111odifying

the

In se ction 2 we give a short analysis of the adaptation
discuss various simple adapt.ation methods
lead ing up to a dcscri ption of the one used in all U GIN.

task and

In section 3

we

d cscri h e the f"caturcs ofalllJCJN, a11d

in section '1 a session with t·xpcrillwnts
the

results

ar<'

disciiSSt'd.

i� r···portr·d

and

( 1 \JU2), r�nd Cowell ( I\J\J2) clt'­
difl'erent
ma.in difl'ercnre bctii'('CII t. h is sys­

Spiegclhalt.cr and Cc)\\"cll

scribe

a similar systt'lll a.nd rt'sult.s of s l ig h tly

experiments.

The

tem and t he i r system, is that. we allow an extra facilit.y,
\ailed fadwy, that. makf's the systcrn forget. t.h<' past at.
an ex p onenti a l rate, t.herel>y 111aking them 11101-e prone

to adapt in changing cnvir-onmcnt.s.

2

Analysis of adaptation

CPN m o dels have both a quantit.at.ive a n d a. qualita­
tive aspect. Througl1 t.hc eli reeled aTcs, the networ k
reflects the only ways in which variables mny have im-

224

Olesen, Lauritzen, and Jensen
pact on each other. The st.rengt.h of the impact is mod­
e ll ed through conditional probability tables. We shall
here describe how the probabil ity tables are modifi ed
in the adaptation process.
So, consider a causal probabilistic network into which
information on the state of the variables can be en­
t ered
If the state of only some of the variables is
known, then the probability distributions for the re­
maining variables are calculated. Each time this is
done, we have described a case. Now, a large number
of cases is at hand, an d we want to improve the model
.

by adjusting the conditional probability tables to the
set of cases.
2.1

Direct modelling of table uncertRinty

In Fig u re 1 ( a ) , the state of the variable A is influenced
direct ly by the states of B and C, and the strength

of this influence is modelled by P(AIB, C). If the
this may, for exam­
stren gth is subject to doubt
ple, be due to d ifferent estimates from experts, or it
may be due to a context influence not mod elled (like
soil quality of corn fields or ge ne t i c disposition for a
disease)
then this doubt may be modelled directly
by introducing an extra parent, T, for A ( F igu re 1
( b)) . This variable can be c onsi d e r ed as a ty pe vari­
able modelling, for example, types of context or differ­
ent experts' assessments. To reflect credibility of the
experts or frequencies of the co ntext types, a prior dis­
tribution forT could be given. When a case is entered
-

-

ma.y be neces,;ar·y. These <lrffi•renl. t.ypes rnay !w lw:n·­
ily interdependent (for example, assessments from Vilr­
ious intersecting sets of experts) and it. may he neces­
sar y to construct a COirlplcx t.ype nel.work with t.he risk

A simplifying assump­
ti o n would be global independence: the context depen­
dence for the conditional probabilities are mutually in­
dependent. In that case, each variable can he given its
own parent of ty p es and retrieval and dissemination
are completely lo c al (perfonned as above). However,
the pro ce d u re is still v u l nera b l e t.o combinatorial PX­
plosion. Take for instance the variahlc A in Figure I.
For each parent configurat.ion a type dcpcnclcncc on
the distribu tion for A shall be described. These de­
pe nden c ies may vary a lot with tile p a rticu la r parent
con figu rations and all kinds of intcr-dcpcndrncics rnay

of a combinatorial explosion.

,

be prcsellt.

Tlw1·d'or<'

WI'

('orc1�d

""'Y lw

in<'l'\';1"''

to

the p mbabil i ty tables by a. factor which is the prodtrct
of the number of states in l11c parents. So, a further

simplification wo uld he lorn/ independence: tlw ron­
text dependence for the various p<HCII1 coni\gtrratinns
ar e mutually independent.

Indirect moddliug of table 111H:1�rtaint.y

2.2

If nothing is known

a p ossibl e

011

t.!w structure of llw

inadequacy of the rnodello

the

GlliSI�S

case

for

set, the

uncertainty can not he represented directly through
a

network of discr e t e types,

and we rnust. leave

roorn

for all kinds of types witlt <dl kinds of distrdartions.

The learning process h e re is

as

everywhere else in t.he

B ayesi a n p ar a d igm : Specif"y a prior distribution

of the

ty pes and calculate the posteri or given the case ob­
served . It remai ns to find a natural way of spccifyi11g
,

and

such a p robabil i ty model. Sricgelh a ltc r

(Hl90a)

give

a

Lauritu·n

range of pos;,ihi\ities, incl11ding normal­

l og istic models.

The simplest probability n1odel which is convenient. f"or
cue I

(a)

case 2
(b)

Figure 1: Ad apt ation th ro ugh a type variable.
to the CPN, the calculation of updated probabilities
will yield a new distribution on T, an d we may say that
the change of these probabi litie s reflects what we have
learnt from the case. This process is called retrieval
of experience. The new dist ri bution may now be used
as prior probabilities for the next case and its impact
on the conditional probabilities found by summing out
the type variable. This process is called dzssemination
of experience. The technique has, for example, been
used in Andreassen et al. (199la), where the system
con tain s a model for metab ol i sm in patients suffering
from diabetes. Through a type variable, the syst em
adapts to the characteristics of the individual patient.
Several conditional probabilities in the CPN may be
context dependent, and a whole set of type variables

th is

situation assu rncs that each set of entries i 11 the

conditional probability tahlcs for

configuration follows

( .Johnson

and Kotz

distribution has �:
sit y

f(pt,····Pk-ll
f•or

p; > ()

I

atl<

The simplicity is
p ro p er )

a

so-called

a

part.icular parent

/Jmchlr!-rhslnh·ulnm

l!li2). A k:-dilll<"nsiOJd Dirichl,'t­
paJ<It\\cV•rs, (n1, . . . , rq.) <�ttd d<'ll­

(X

(

k-J

1-

LJil

)"k-1

•=I

k-1

II;;;··-!.
•=1

(I)

"\'"'k- I
L,=1 p, <I.
in

the

distribution with

itt\.crprcl.a.tion�
tri

=

0 for all

lf

i

the (im­

is

consid­

specified
may be interpreted a.� representing past exper ie nce as a
coni.ingency iable (cl'J, .. . . nk) of counl.� of pas! casrs.

ered

a

noninformativc prior, the distribution

s = L; rt; i:; thcr·eforc refet·red to as 1/ic
equivalent sample size. The updating p roced ure con­

The quanti ty

sists of modifying the cou nt.s as new cases
observed.

We shall

not give details,

hut. just

state

arc

that

bci 11g

the frac-

aHUGIN: A System Creating Adaptive Causal Probabilistic Networks

tion a;/ s = m; is t he mean for the ith outcome, and
for each i the variance of the probability for the ith
outcome is

m;(l- m;)

v;= --'---.:...

(2)

s +l

Hence v; is a measure of the uncertainty of the prob­
ability m;.
Using this interpretation we also have a tool to model
expert opinion s of the type "the probability is some­
where between p and q, but I believe it is about 1·" . In
the case of two states a and b, consider, for example,
the statement that the probability of a is between 0.3
an d 0.4 and that it is about .35. If we, as in Spiegel­
halter et al. ( 1990), interpret the statement so that
the mean is 0.35 and the standard deviation is 0.05,
then it can be modelled by a 2-dimensional Dirichlet­
distribution (which is called a Beta-distribution). We
then have to determine two counts na an d Ob whi ch
satisfy the equations
Oa
--- =

aa+ab

0.35 and

0.35

·

0.65

aa+ab+l

=

0.0025

(3)

which we solve to get Oa = 31.5 and Ob = 58.5. This
can be an attractive alternative to modelling second
order u n certainty by intervals of lower and upper prob­
abilities.
learning: Let (m1, . . . , mn) and a sample size
s be a given specification of the conditional probabil­
ity table P(Aib, c) . We can then act as if we had a
contingency table of counts (sm1, .. . , smn)· lf we ge t
a case in the configuration (b, c ) and a;, then retrieval
qu ite simply consist in adding 1 to the count for a;,
and dissemination is just to calculate the new frequen­
cies. If global and local independence can be assumed
the scheme is applicable to all tables.
Back to

The scheme only works if both the states of A an d its
parents are known. In general we may anticipate that
the provided evidence, E, may leave uncertainty on
both the states of A and of its parents.
A nai ve approach in the general case could be to add a
count of P( a;, b, cl E) to the counts for a;. This scheme
is known as fractional updating (Titterington 1976).
However, the scheme has several drawbacks. For ex­
ample, if P(A I b, c) = P(A I E) then the scheme may
give unjustified counts yielding a false accuracy. If, for
example, E = (b, c), then nothing can be learned on
the distribution of A, but nevertheless the sample size
will be increased by one. See further discussion of this
issue in Spiegelhalter and Lauritzen (1990a) as well as
in Spiegelhalter and Cowell (1992).
A mathematically correct updating of the distribu­
tions under our interpretation results in a mixture of
Dirichlet-distributions rather than in a si ngl e one (a
mixture is a linear combination with non-negative co­
efficients summing to 1). This complicates the calcu­
lations intractably - in particular when adapting from
the next case where mixtures of Dirichlet-distributions
are to be updated. Eventually the process will yet

agam result in a combinatorial explosion. Instead,
the correct distribution is approximated by a single
Dirichlet-distribution (keeping the approach of modi­
fying counts). First of all, we want the approximated
distribution to have the correct means, and the new set
of probabilities (mj, ... , m;) is set to be the means of
the correct distribution. Secondly, it would be prefer­
able also to give the distrihu t.ion t.he c orrect varian cPs.
However, this is not possible since only one free parant­
eter is left, namely the equivalent sam ple size. Instead,
the equivalent sample size is give n a value such t.h<-tt.
t he

'average v<�riancc'

v =

L

111.1: v;

i=l

(" )

ts correct. The r csu lt.i ng »chcrnc, wh ich is used i11
aHUGIN, is t.hc followittg: 1'- in;t. the ltwans art' chang('d
as if a full count W<\.� oht.aitwd:

mi

=

m;s

P(a;, b,c II,· ')+ w;{ I- P(b, c I!�')}

+

s+l

The last term

(rl)

be undcrst.oocl so that it clistr ibut .cs
that. fJ and C arc not in st.ates (b, c)
according to their present p ro babili t ies .
may

the probability

over the

a;

's

Next, the sam pie size is determined:
s

•

=

L�-l rn;-" (1- mi) - 1
""k
. • •
L..- i=l 7ll.; V;

((i)

where v; is the variance of l'i in t.he mixture (the for­
mulae may be found in Spiegelhalt.er and Lauritzen
(19!)0a)). The new r.ounts iii'C s"mi.
3

Features of aHUGIN

The program a ii U G IN , wltich is currently 1111der int­
plementation, is an ex t.cnsion of II U GIN (A nders<'n
et at. 1989). JIUGIN is a shell which allows the user
to edit CPNs over finit.e st.at.c variahlcs, and wh en t.ll<'
CPN is specified, TIUG!N creates <1. runtime sysl<'llt
for entering findings and ttpd<�t.inr; prohahilit.i<�s of tlw
variables in the network.
In aiJUGIN each variable lllilY be declared t.o be in
adaptation mode. If, for cx<�mplc, the variab le A with
states a1, . . . , an has parents B, . . . , C, th e n the con·
ditiona! probability table P( A I B, . .. , C) is modified
by declaring A of ndapt.at.ion t.y p e . The t.ahle is i n ­
terpreted as a contingency table such that for ead1
parent configuration b, .. . , c, the set P(Ajb, ... , c) is
interpreted as a set of frequencies based on a sampl e
of cases. Therefore the user will for each p arent config­
uration be prompted t.o specify F:QUIVALENT SAMPLE
SIZE. The l arger the ESS, t.hc more conservative t.hc
adaptation will be. The default val l !!! of ESS is Gk·,
where k is the number of states in 11.

Alternatively t h e ttscr will he M>ked t.o specify an in­
terval for cac\1 of the prohahililics in t ���� conditional

225

226

Olesen, Lauritzen, and Jensen

probability tables. These in terva ls will th en be trans­
lated to sample sizes using the equivalent of (3). The
ESS used for the given parent configur at io n will now
be chosen as the minimum of the t r anslate d sam p le
sizes for the individual entries .
3.1

Fading

Variables in ada p tati on mode have an extra feature,
fading, which makes them tend to ignore th in gs they
have learnt a long time ago, considering them as less
relevant. Each time a new case is taken into account,
the equivalent sample si ze is discounted by a fading
factor q, which is a re al number less than one but typ­
ically close to one. From the expression ( 1) for the
Dirichlet density, it is seen that the fading scheme es­
sentially corresponds to flattening the density by r ais­
ing it to the power q, known as power-steady d ynamic
modelling (Smith 1979; Smith 1981).

SIZE. In the case o f a change from accumulating to
fading the EQUIVALENT SAMPLE SlZE is kept but the
MAXIMAL SAMPLE SIZE provided by the user

ually claim its influence.

4

Experiments with aHUGIN

To

in v es tigate

the

strengths

and

will grad­

limitations

of

se r ies of experiments were carried out..
The investigation was designed as a complete fa<"lo­
rial simulation experilllent 011 t.IH' now classical "Chest

aiiUGIN, a

clinic" example (Figure 2) originat.ing frorn

and Spiegelhalt.er

( 1088).

Laurit.�•�n

Each experiment simulates

If s is the initial ESS, then the maximal ESS after
adaptation from a case is qs + 1. Running n cases will
result in a maximal ESS of
1- q"
q"s + ---

1- q

This gives that 1/(1 - q) is the maximal sam ple size
in the long run.
Therefore the user is given the choice between ACCU­
MULATING (fading factor 1) and FADING. If fa din g is
selected, the user is prompted for MAXIMAL SAMPLE
SIZE, MSS, and the fading factor is then computed as
(MSS- 1)/MSS. Defau lt value is lOOm, w h e re m is the
number of entries in the table.
Note: The result of fad in g is not only that the sample

size is reduced. Consider namely an entry with count
a- and with samp le size s, an d suppose that ret r ie va l of
a case results in an increase of the sample size by 1 and
o f the count by x. Without fading the ratio between
counts from present and past is x ( a-, but with fading
the ratio is xjqa-. This tells us that with fading the

present counts are given more weight. This can also be
seen by assuming that the entry wilt never receive more
counts. Without fadi n g the p roba bili ty will vanis h at
the speed of a-f(s + n ) while wi th fadi ng, the speed of
vanishing is in the order of a-f(s + q-").
3.2

Runtime mode

Figure 2:
10,000

cases,

The

"Ciwst clinic" •'X<llllpk.

and fo ur factors, dc110I.l'd ll, 0, P

are considered. Tine•� r<�ndom

ated from

Rl: Probabilities

close to the

R2: Probabilities v<�ry

san1pks (R)

o rigi n a l

are

and

L.

�enn­

ones.

difk.renL from the migina! o1ws.

R3: Probabilities "drifting over time", starting as the
original ones.

To control difTerences due t.o chance va riat ions , the
samples are reused . Thus, for example, all experimcTit.s
with probabilities as in Rl are based on identical dat.a.
Two different observational schemes ( 0) are investi­
gated, the first one is lll<lillly included for control pur­
poses.

01: Complete observations.

The ad a pta tion starts with the CPN in the initial con­
figuration. Findings are entered, and wh en all infor­
mation on the case has been entered , the adaptation
takes place changing the tables for the variables of

02: Data observed only on the variables "Visit. t.o
Asia?", "Smoker?", "Positive X - r ay ? " and " Dys­

adaptation type.

The P factor des c ribes difl'erent weights on the prior
distributions, expressed as v a ryi n g eq11ivalcnt. sR.rnpk
sizes. Two cases are consid•�rcd

At any time between two cases the user can choose
to change the adaptation type of any variable. When
the adaptation type of a variable has been chan ge d ,
the user is prompt ed for p ossibl e mi s s i n g information
on EQUIVALENT SAMPLE SIZE and MAXIMAL SAMPLE

pnoea?".

PI: Low precision,
P2: ll igh pn�cision,

ESS
ESS

=

=

10.
I Oll

aHUGIN: A System Creating Adaptive Causal Probabilistic Networks

Finally, three different learning schemes (L) are inves­

the random s amp le .

tigated

11: All variables except
ac c u mu latin g mode .

"Th ber c ulos i

s or cancer" in

Experiment ,22�

a

12: As 11 for the first 1000 cases, then t he mod e is

postet ior probability intervals �or p(bls)

"'\...:··----.--- ..

1000).

�

13: A s 12, b ut with short memory (MSS = 100).
"Tuberculosis or can ce r " is always in fixed mode as
it is a pure logical tran sit ion . As can be seen, the
whole in ves tigati on consists of 3 x 2 x 2 x 3 = 36
experiments . For each experiment a plot is generate d ,
sho w in g the current value of th e conditional probabil­
ities after each case has been processed, t ogether with
ap proximate 95% p oste ri o r probability intervals.

Results in accumulating mode

95%

!\

change d to fading, with long memory (MSS

4.1

·

· •'--__ -..---'--'---"- �. :: :_::. : :d:._

2000

•ooo

Experiment 1222

b

6000

.

sooo

I

,0000

95% posterior probability intervals tor p(bls)

These experiments are very similar to those performed

by Spie gelhalter and Cowell (1992). However, we al­
low uncertainty on all conditional probability tables.
In general our results show the same pattern. For com­

plete data the correct values are obtained quite fast,
and the influence of the initial specifications vanishes
after a f e w hundred cases.
Figures 3 {a) and 4 (a) show an interesting phe­

nomenon w h en learning from incomplete data ( 02).
In these experiments, it can only be observed from t he
given data that a maj ori ty of smokers suffer f rom dys­
pnoea (shortness of breath). It can not be inferred
f rom the data whether this correlation is due to the
presence or absence of bronchitis. In the fi rst exper­
iment, where all variables are in accumulating mode,
th e frequency of bronchitis is overestimated (Figure 3
(a)). To compensate for th is , the condition al pr oba­
bility for dy spn oe a given bronchitis and none of the
other diseases, is underestimated ( F igure 4(a)). Thus
t he correlation b etw een what can a ct u ally be observed
in the data is determined correctly, but the intermedi­
ate expl an ation is slightly incorrect.
From these experiments we conclude, not su rpri singly ,
that the method has difficulties learning about con­

ce pts on which dat a are indirect. In such situations the
system rel ies str ongly on p ri or k nowledge . This con­
clusion was also reached by Spiegelhalter and Cowell

(1992).

4.2

Results in fading mode

Figures 3 (b)-(c) and 4 (b)-(c) dis p l ay the results for
the same exp er i m ent s as in F ig ures 3 (a) and 4 (a), but
with the variab l es ch anged to fading with l ong memory
after the first 1000 cases. The same effect on esti mat­
ing intermediate variables can be observed. Note also,
that the two curves vary syn ch rono u sly. Most proba­
bly this is a r e s u l t of variations in freq uenc ies due to

" -------r--�--�
,.,.,
0000

Experimenl I 223

c

·

95% poslerior probabilily

intervals tor p(bjs)

� -

..

i

"

.

0

.
0

;: ·_�---r-----�----.-------,�· · ,__._j
8000

'0000

Figure 3: Exp erimen ts with in c ompl ete data. The con­
diti on al probability of bt·onchitis given the p a t.ient. is a
smo k e r is learnt in (a) <lccurnulating mode; (b) fading
m ode with long memory: (c) fading mode with short.
memory.
ln the

third experiment

(Figures 3

(c) and

4 (c))

the

maximal samplesizes are reduced to 100. ThisexJwri­
ment reveals the
mode. Figure 4

limit of the applicability of the frtdin)!;
(c) shows t.hat t.lw dat.<1 <He lwst. l'X­
ass u ming t.hat. <�.II pa.tients wi t. h bronchit.is

plained by
suffer from dyspnoea. To 1naint.ain t.he consistP.ncy
with the d ata , the frequency of s1nokcrs sufT'ering from
b ronchitis is

underestimated acc o r dingly. This pat.t.ern

227

22H

Olesen,

Lauritzen, and

Jensen

is general for fading with short memory for high and
low probabilities. We conclude that special attention
must be directed towards systematically m issi ng data
and the choice of MSS if such variables are fading.
Figure 5 shows a series of experiments with a declin­

Experiment 3221

a

��

ing probability of being a smoker. The first 1000 cases
are identical for the three plots, the variable being in

-

95% posterior probability intervals tor p(s)

accumulating mode. In Figure 5 (a) the variable re­

mains in this mode and it is seen how the probability
is becomin g increasingly conservative as the ESS m­
creases.

Experiment 1221

a

. 95% posterior probabili!y 1ntervals for p(djnot e.b)

2000

b

Experiment 3222- 956/o posterJor probabilrty interva!s for p(s}

c

Exp&rimen\ '3223- 9-5°/.g poslerior probability ln•ervals tor p(s)

eooo

Experiment 1222 · 95% posterio r probability intervals lor p(djnot e.b)

b

I

�-��!�'""""'�-��

(

r

6

gl

Q �-----,----�--,---�--�
2000

c

Experiment 1223

10000

·

95% posterior

probabili!y intervals for p(dlnot e,b)

10000

Figure 5: Learning about.
bei ng a smoker.

. ��

J .

a

declining probability of

·0 ;

I

;:I
� �-----,----.--,---,�
10000

Figure 4: The same experiment as in Figure 3 but for
"Dyspnoea" given the patient has bronchitis but none
of the other diseases.

In Fig u re 5 (b) the variable is changed to fading with
lo ng memory (MSS = lOOO) after t.he first 1000 ca�es.
This i n creases the dynami<: behaviour of the system
an d an almost correct adaptation is obtained. De­

creasing the MSS to 100 (Figure 5 (c)) increases the
dynamic behaviour further, re!>ult.ing in stronger fluc­
tuations around the correct value. The general expe­
r i e nc e is that the !V1SS shonld not he sd too low, nnd

that the experiments confirm th<.:
of aiiUGIN.

expected behaviour

aHUGIN: A System Creating Adaptive Causal Probabilistic Networks

To s u mmarize, aH U G I N seems to be able to adapt
to chang i n g environments, thereby extending H U G I N
with a valuable fun ctional i ty . Howeve r , special atten­
tion must be directed to the choice of M SS and to
var i ables with systematically missing d ata.

Andersen, S . K . , O lesen , K. G . , Jensen , F . V . , and
J ensen, F. ( 1 989). H U G I N - A she l l for building
Bayesian belief u ni verses for expert systems. In
Proceedings of t h e 1 1 t h int ern ational joint confe r­
ence o n artificial intellig e n ce , p p .

reprinted i n S hafer and Pearl

1080-5.
( 1 990).

Also

Andreassen , S., Benn , J . J . , Hovorka, R . , Olesen ,
K . G . , and Carso n , E . R. ( 19 9 1 a ) . A probabilis­
tic approach to glucose prediction and i nsulin dose
adj us t ment . Techn i c al report , Inst i t u te for Elec­
tronic Sys tems , A a l b org U n i versity .

A n d reassen, S . , J ensen, F. V . , and O lesen , K . G .
( 1991 b) . Medical expert systems b ased on causal
probabilistic networks. In ternational Journ al of
Bi omedical Computation, 2 8 ,

1 -30 .

Cowell, R. G . ( 1992). BAlES - a probab i l i s t i c ex­
pert system shell with qual i tative and quantita­
tive learning . In Bayesian st a tistics 4, ( ed . J . M.
Bern ardo , J . 0. Berger, A . P. D awi d , and A . F. M .
Smith) , p . i n press. Clarendon Press, Oxford , UK .
Jensen, F . V . , Lauri tzen , S . L . , and Olese n , K . G .
( 1990) . Bayesian up dat i n g i n causal probabil istic
networks by local c o mpu tations . Co m p u t a t ion a l
St atistics Q u a rt e rly, 4,

Johnson , N . L . and Kotz ,

269-82.
S . ( 1972).

Distri b u t ions i n

statistics. Co ntinu ous multivariate dis t ri b u ti o n s .

J oh n Wiley and Sons, New York .
Lauritzen, S . L . and S p iegel h a l t er , D. J . ( 1988). Lo­
cal computations with probabilities on graphical
st r u ctures and their appli cation to expert systems
( with discussi on ) . Journ al of the Royal Sta tistical
Society, Series B, 50,

( 1988 ) .

1 57-224 .

Probab ilis t i c inference

m

i n t ellig e n t

syste ms. Morgan Kaufmann , San Mateo.

Shachter, R. D. ( 1986). Eval u at i n g influence d i agrams.
Opera t i o ns Research, 34,

87 1 -82.
( 1990).

Sh afer , G . R. and Pearl , J . (ed . )

Rea dmg s

in u n certain reas on ing. Morgan Kaufm an n , San

M ateo , Califor n i a.
Shenoy, P. P. and Shafer , G . R. ( 1 990 ) . Axioms for
probab i l i ty and belief-fu nction propagation . In
Uncert a inty in artificial intelligence I V, ( ed . R. D .
Shachter, T . S . Le v i t t , L. N . K an a! , an d J . F . Lem­
mer) , pp . 1 69-98. North - Hol l and , Amsterdam .
Smith, J . Q. ( 1979 ) . A general ization of the Bayesian
steady forecasting mo d el . Journal of t h e Royal
Statistical So ci e t y, Series B, 4 1 ,

Smith, J .
model .

Q. ( 1 98 1 ) .

3 75-87.

The m u l ti parameter st ead y

Journal of t h e Ro yal St atistical Society,

Series B, 4 3 ,

Spiegel halter, D . and Lau r i tzen , S . L. ( 1 9906 ) . Tech­
n i ques fo r Bayesi a n a n alysis i n e x p e r t. syste m s .
A n n als of

