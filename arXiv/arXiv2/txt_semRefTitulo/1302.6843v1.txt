

In this paper we propose a new approach
to probabilistic inference on belief networks,
global conditioning, which is a simple gener­
alization of Pearl's (1986b) method of loop­
cutset conditioning. We show that global
conditioning, as well as loop--cutset condition­
ing, can be thought of as a special case of the
method of Lauritzen and Spiegelhalter (1988)
as refined by Jensen et al (1990a; 199Gb).
Nonetheless, this approach provides new op­
portunities for parallel processing and, in the
case of sequential processing, a tradeoff of
time for memory. We also show how a hybrid
method (Suermondt and others 1990) com­
bining loop-cutset conditioning with Jensen's
method can be viewed within our framework.
By exploring the relationships between these
methods, we develop a unifying framework in
which the advantages of each approach can
be combined successfully.
Keywords: causality, belief networks, causal net­
works, planning under uncertainty, troubleshooting.
1

Peter Szolovits

Dept. of Medical Informatics
Laboratory for Computer Science
and Image Analysis
Massachusetts Institute of Technology
Aalborg University
545 Technology Square
DK-9220 Aalborg SO DENMARK
Cambridge, MA 02139
ska@miba.auc.dk
psz@lcs.mit.edu

INTRODUCTION

In recent years, there have been intense research efforts
to develop efficient methods for probabilistic inference
on belief networks. A number of different exact meth­
ods have been proposed and implemented to solve this
general class of problems. In this paper, we introduce
a solution method we call the Clustering Algorithm,
a variant of the HUGIN algorithm (Jensen and others
1990a; Jensen and others 1990b), and closely related
to a variety of algorithms (Cannings and others 1976;
Cannings and others 1978; Lauritzen and others 1990;
Shafer and Shenoy 1990; Shenoy 1986). The Cluster­
ing Algorithm is not an improvement over the methods
listed above, but rather a framework for comparison
and integration with the Polytree Algorithm (Kim and
Pearl 1983; Pearl 1986b) and Loop-Cutset Condition­
ing (Pearl 1986a; Peot and Shachter 1991). We show

how these techniques are special cases of the Clustering
Algorithm. Although the strong relationship between
the Clustering and Polytree Algorithms is widely un­
derstood, the same has not been true of Loop-Cutset
Conditioning, despite our announcements to that ef­
fect several years back at the Conference on Uncer­
tainty in Artificial Intelligence.
In the process of demonstrating results about Loop­
Cutset Conditioning, we generalize it to an algorithm
we call Global Conditioning. Even though Global Con­
ditioning is shown to be a special case of the Cluster­
ing Algorithm, the insights it provides suggest new
applications with parallel processing and under mem­
ory restrictions. We also provide a Clustering Algo­
rithm interpretation to a hybrid algorithm which com­
bines Jensen's method with Loop-Cutset Conditioning
(Suermondt and others 1990).
Section 2 defines the notation and terms to be used
throughout the paper. Sections 3 and 4 introduce
the Clustering Algorithm and the Method of Global
Conditioning, respectively, and explore their connec­
tion with the Polytree and Loop-Cutset Conditioning
Methods. Section 5 presents a parallelizable approach
based on Global Conditioning and shows how it can
also trade compute time for memory. Section 6 shows
dynamic restructuring of cluster trees based on the
logic of conditiong, while conclusions and suggestions
for future research are presented in Section 7.
2

NOTATION AND BASIC
FRAMEWORK

In this section, we present our notation and some stan­
dard definitions. In particular we introduce the frame­
work for random variables and evidence, and their rep­
resentation in directed and undirected graphs. We
also present an example problem which will be used
throughout the paper.
We have a finite set of elements N {1, . , n}, cor­
responding to the nodes in a directed acyclic graph.
Such a directed graph has many names in the litera­
ture, including belief network (Pearl 1986b), prol:r
=

. .

Global Conditioning for Probabilistic Inference in Belief Networks

515

Figure 1: Chest clinic belief network example.

abilistic influence diagram (Shachter
causal probabilistic network (Jensen

1988), and
and others
1990b). We refer to a particular element with a lower

case letter, j E N, and a set of elements using up­
per case letters, J � N. The parents of a node are
denoted by Pa(j), and the parents of a set of nodes,
J) is simply the union of the parents of the nodes
in the set. The family of a node j consists of j and its
parents, {j} U Pa(j). An example of a belief network
is shown in Figure la. It is taken from Lauritzen and

Pa(

Spiegelhalter (1988).

Associated with each element

j

is a random variable
Xj· A
vector of variables X1 is associated with the set of el­
ements J. Each variable Xj has a prior conditional
probability distribution, Pr{XjiXPa(j)}· Lett. be the

xj having a finite number of possible values

evidence which has been obtained about the variables
XN. Each observation t.�o, whether an exact obser­
vation or indirect evidence about a variable or set of
variables, can be characterized by a likelihood func­
tion, Pr { t. �oi Xh }, of the variables XJ.,.
The elements N can also be represented as the nodes

in an undirected graph, called a moral graph. The
moral graph is constructed by adding an undirected
arc between all nodes with common children, and then
replacing the directed arcs with undirected arcs. The
moral graph corresponding to the belief network shown
in Figure la is displayed in Figure 1 b, without the
dashed (3, 5) arc. Moralizing arcs (3, 4) and (5, 6)
were added. A moral graph is said to be chordal
or triangulated if there is no cycle of length four or
more which does not have an arc between two nonad­
jacent nodes in the graph. Although the graph shown
in Figure 1 b is not chordal without the (3, 5) arc, it
is chordal with it, or with other arcs which could have
been added instead such as (2, 6).
Graphs can also be constructed in which each node
corresponds to a set of elements rather than just a
single element. A set of elements for this purpose will
be called a cluster. An undirected tree of clusters will
be called a join tree if every element which appears
in more than one cluster appears in every cluster on
the path between them (Beeri and others 1983).
A

cluster tree for

a particular belief network is a join
tree in which every family from the belief network is
contained in at least one cluster. For example, the

Figure 2: Cluster trees for the chest clinic example.

four trees drawn in Figure 2 are cluster trees for the
belief network drawn in Figure 1a. Cluster trees have
been recognized before (as "junction trees") and some
of their special properties are well known (Jensen and
others 1990a). The moral graph corresponding to a
cluster tree consists of nodes for each element with an
undirected arc between any two nodes whose corre­
sponding elements appear in the same cluster. Such a
moral graph will always be chordal and consistent with
the original belief network and its moral graph. The
independence properties represented in the cluster tree
can also be revealed if, for every pair of neighboring
clusters, an intervening cluster containing their inter­
section is inserted on the arc between them.

After

the addition of these new clusters, called separation

sets,

the new graph is still a cluster tree, but now all of

the conditional independence is explicitly represented,
namely, any two clusters in a cluster tree are condi­
tionally independent given any intervening cluster or
the separation set corresponding to any arc in between
them. The graph drawn in Figure 2d was constructed
from the cluster tree shown in Figure 2a by the addi­
tion of the separation sets followed by the deletion of
redundant clusters for (3, 6).

3

THE CLUSTERING ALGORITHM

The Clustering Algorithm performs probabilistic in­
ference by passing messages around cluster trees and
propagating the global effects of local information. It
is a variation on the algorithm in HUGIN (Jensen and
others 1990a; Jensen and others 1990b) but also closely
related to other algorithms (Cannings and others 1976;
Cannings and others 1978; Lauritzen and others 1990;
Shafer and Shenoy 1990; Shenoy 1986). We show that
the Polytree Algorithm (Kim and Pearl 1983; Pearl
1986b; Peot and Shachter 1991) can be viewed as a
special case of the Clustering Algorithm.

516

Shachter, Andersen, and Szolovits

There are only two k inds of data which are truly local
in a cluster tree: the conditional probability distribu­
tions and likelihood functions for variables in the clus­
ter. Those functions can be set independently for each

variable at any time, although there is a global restric­
tion on what constitutes a consistent or coherent set
of data for the entire cluster tree, namely Pr{ c} > 0.
Each variable Xj in the model has a conditional prob­
ability distribution Pr{XjiXPa(j)} which is assigned
to exactly one cluster containing j 's family. Likewise
for each observation €/c there is a likelihood function
Pr{t:.�ciXJ�o} showing how the probability for the evi­
dence depends on the variables XJ�o· Each likelihood
function is assigned to exactly one cluster containing
J�c. The product of the conditional probability dis­
tributions and likelihood functions assigned to cluster
Si is c alled the potential function \IIi
\lli(Xs;)
=

for cluster S;., and it contains all of the local infor­
mation for that cluster. (The arguments for w. will
be omitted to improve readability whenever they are

unambiguous.)

The joint distribution for the model is the global in­
formation obtained by multiplying together all of the
local information. For example, the posterior joint dis­
tribution for all of the variables is just
Pr {XN ,€}

II Pr{XiiXPa(j)} II Pr{t:.�ciXJ�o}

=

jEN

( L II

N\S; /cEK;1;

W�c)( L

Because all of the local data are partitioned into clus­
ters indexed by K-;.jj and Kjli• the two messages not
only contain all relevant information, but they can also
be computed asynchronously. In pr acti ce , the mes­
sages can be computed in terms of other messages,

S,\S;

All of the incoming cluster messages combined with
the local data also allow us to compute the posterior
probability distribution P,
Pr{Xs.,t} for any
clusters.,
=

P;

=

2::: II wk

N\S; k

=

Pr{XJ, t:.}

=

L Pr{XN , t:.}

N\J

=

L II \II;,

N\J

i

where LN\J signifies a summation over all possible
values of all argument variables except XJ.
The local information within the cluster tree is prop­
agated throughout the network via local messages in

order to compute these global expressions. Consider
two adjacent clusters S; and Si. If Kijj is the set of

c luster indices for all clusters on S, 's side of the ( i , j)
arc and Kjli is the complementary set of clusters on

S;'s side of the arc, define the
sent from Si to Sj to be

M;.j

=

cluster message M;j

L II

N\S; kE:K;1;

\II�c.

Because the cluster tree is a join tree, the only ele­
ments in common between Sj and all of the clusters
in Kili must be in Si , so M;j is a function of variables
Xs,ns; . As a result, we can compute the separation
set probability P;j, the posterior joint distribution
for the separation set for two adjacent clusters, as the
product of the two cluster messages on the arc con­
necting them:

P;j

Pr{Xs,ns;, t:.}

=

L

II Wt.:

N\(S;nS;) /e

II

w.

M/ei,

lcEKi

where K; is the set of all clusters adjacent to S;. From
these expressions it is easy to confirm that for any
adjacent clusters S; and Si,

Pr{Xs,ns;,t}

=

L

P1

L

pj

S;\S;

S;\S,

XJ

lcEKi-i

where Ki-j is the set of all clusters adjacent to S;,
excepting sj.

=

The posterior joint distribution over the variables

\ll�c)

M,jMji·

k

can then be computed by marginalizing,

II

N\S; leEK;!;

=

=

M;jMji
MjiMij

=

=

P;j
Pji·

There are a number of different strategies to control
message passing that constitute the Clustering Al­
gorithm, First, because a cluster message summa­
rizes all of the local information on one side of an arc,
any change to the local information can require updat­
ing of the message. Conversely, if there is no change
to any of the local information on one side of an arc,
then there will be no change to the cluster message.
Second, if all that is required is the computation of the
posterior distribution for a particular cluster S;, that
can be performed with local computations through a
collect operation (Jensen and others 1990b):
1. 81 sends request messages to all of its neighbors;
2. They in turn send request messages to all of their
other neighbors and so forth;
3. Eventually a request message reaches a cluster with
only one neighbor. It either computes and sends a new
cluster message in reply or indicates that the old mes­
sage is still valid;
4. Clusters with more than one neighbor either com­
pute and send a new message or, if they have no
changes and neither do any of their other neighbors,

indicate that the old message is unchanged;
5. Eventually, s. will receive a message from each of
its neighbors and thus it can compute P;.
Third, if all clusters are to be updated, then a dis­
tribute operation can also be performed with local
operations (Jensen and others 1990b): S;. sends up­
dated cluster messages to each of its neighbors, and

Global Conditioning for Probabilistic Inference in Belief Networks

517

is not M;j but rather Pl;ew = M;.jMj;, the separation
set joint distribution, which is of the same dimensions.
The actual update is just the element-by-element ratio
_

PiJ

-

pnew
i.j

.
-:-

nold
'•i ,

where any value can be used for 0 -7-0. The posterior
distribution for cluster S; is

Ptew

Figure 3: The Polytree Algorithm applied to the chest
clinic example.
they in turn send them to their other neighbors and
so forth until every cluster has received such a mes­
sage.
Finally, initialization simply requires multiplying the
factors in each W; and marking the cluster as changed.
If there are no probability factors for a cluster S;, then
its '11;. should be a scalar 1 and it should be marked as
not changed. If there is some evidence to begin with,
then it should also be factored into the appropriate W;
and the cluster should be marked as changed.

and
MjiMJi

<-

...._

piold II

kEK;

L P,

p;j

for j

S;\S;

E

K;,

where K; is the set of all clusters adjacent to S;.. There
are also tremendous opportunities for savings in the
compiled compression of zeros from all of the distribu­
tions (Jensen and Andersen 1990}, which means that
most components of a sparse message and joint distri­
bution are never computed. The only cost comes when
evidence is retracted, and the whole network might
have to be reinitialized. Initialization is not difficult
however, with Pij1d set to scalar 1, and the remaining
initialization process similar to the factored form.

The Polytree Algorithm

4

The Polytree Algorithm for singly-connected belief
networks (Kim and Pearl 1983; Pearl 1986b) forms
a cluster tree with the same topology as the original
network, but with each node j replaced by its family,
S i = {j} u Pa(j). Because the belief network is singly
connected, the resulting cluster tree is a join tree. The
cluster conditional probability is simply the node's
conditional probabilities, P3 = Pr{X3IXPa(j)}, and
the cluster likelihood function is just the evidence per­
taining directly to X3, Lj
Pr{EjiXJ}· The Cluster­
ing Algorithm is then performing precisely the Revised
Polytree Algorithm, in which causal support messages
are unconditional rather than conditional (Peot and
Shachter 1991}. Consider the belief network shown in
Figure 3a, in which the (2, 3) arc has been eliminated
to render the network singly-connected. The cluster
tree for the Polytree method is shown in Figure 3b. In
general, the following theorem applies.

The Methods of Global and Loop-Cutset Conditioning
are presented in this section. Loop-Cutset Condition­
ing (Pearl 1986a) is one of the oldest methods for solv­
ing multiply-connected belief networks. We show that
both conditioning methods can be viewed as special
cases of the clustering algorithm.

=

Theorem

1

(The Polytree Algorithm)

The Polytree Algorithm {Kim and Pearl 1983; Pearl
1986b} in modified form (Peot and Shachter 1991) is
a special case of the Clustering Algorithm.

There are also choices in the representation of the
cluster messages. We have presented the messages
in factored form, which simplifies the revision of
prior probability distributions and the retraction of ev­
idence. The messages can also be represented in joint
form for more efficient calculations. Each cluster S;
remembers its posterior joint distribution P; and each
arc (or separation set) from cluster S; to cluster Sj
remembers the posterior joint distribution last sent,
Pij1d. The message sent between clusters S; and Sj

GLOBAL CONDITIONING

The Method of Global Conditioning provision­
ally observes a set of variables XK, the conditioning
set, in order to simplify the belief network, rendering
it easier to solve. We will call the original problem the
master problem, and the simplified subproblems the
instantiated problems. Although we could conceiv­
ably use any technique to solve the instantiated sub­
problems, we will assume in this paper that they will
be solved with the Clustering Algorithm. Although
we will not explore methods to select the conditioning
set, it is critical to the success of the method and a
subject worthy of papers by itself.

Having "observed" XK, we can cut the outgoing arcs
from K in the belief network, which is equivalent to
separation by Kin the moral graph (Shachter 1990a).
For example, the multiply-connected graph shown in
Figure la becomes the singly-connected graph shown
in Figure 4a after X2 has been "observed." We can
compute the overall desired result by considering all
possible cases for XK and weighting by their probabil­
ities, using the "Law of Total Probability,"

Pr{XJ, E}

=

L Pr {XJ
XK

,

XK

=

xK,

E}.

The main problem with the method is that the num­
ber of terms in the sum grows exponentially with the
number of elements in the conditioning set K.

518

Shachter, Andersen, and Szolovits

Suermondt and Cooper 1988; Suermondt and Cooper

1990).
Theorem

3 (Loop-Cutset Conditioning) The
Method of Loop-Cutset Conditioning {Pearl 1986a) in
modified form {Peot and Shachter 1991) is a special
case of the Clustering Algorithm. It is less efficient
than the Clustering Algorithm unless the conditioning
set already appears in every family in the belief net­
work.

We know from Theorem 1 that the Polytree
Algorithm is a special case of the Clustering Algo­
rithm, with clusters corresponding to the families in
the belief network, so the results follow from Theo­
rem 2, after we construct the cluster tree. Start with
whatever singly connected network will be used for
the Polytree Algorithm. The cluster tree has the same
topology, with the cluster corresponding to a node j in
the tree formed from the union of node j, its parents
in the tree, and the conditioning set. This is a cluster
tree because (1) it is a tree (or forest) by construction;
(2) it is a join tree because the conditioning set K is
in every cluster and the cluster corresponding to every
node j E K must be adjacent to the clusters of j's chil­
dren (every other cluster in which j appears); and (3)
every family is contained in some cluster, because the
only arcs cut are from nodes in K which are included
in every cluster anyway. D

Proof:

Figure 4: The Loop-Cutset Conditioning Method ap­
plied to the chest clinic example.

Another way to view Global Conditioning is that we
are adding arcs in the moral graph from K to all other
nodes or, equivalently, that we are adding the condi­
tioning set K to every cluster in a cluster tree, since we
must iterate over the possible values of X K. Consider
for example the belief network shown in Figure 4a, in
which the arcs from 2 have been cut. Because we will
be iteratively solving the new problem for every pos­
sible case of X2, it is really as if an undirected arc
had been added from 2 to all other nodes in the moral
graph shown in Figure 4b, making it a chordal graph.
A cluster tree for that graph is shown in Figure 4c.
It could also have been obtained by building the clus­
ter tree corresponding to the singly-connected network
shown in Figure 4a (if we omit node 2) and then adding
2 to every cluster. This is not a particularly efficient
cluster tree because only the (2, 6) arc was needed
for triangulation. This cluster tree contains clusters
with four elements, while the cluster trees for the same
problem shown in Figure 2 have no clusters containing
more than three elements. This result is formalized in
the following theorem.

It might seem counterintuitive that Loop-Cutset Con­
ditioning is related to the Clustering Algorithm. Con­
sider the network shown in Figure Sa suggested
by Pearl in the discussion following (Lauritzen and
Spiegelhalter 1988). A "standard" chordal graph for
this problem in shown in Figure 5b. Although condi­
tioning on X 1 appears to be superior, it in fact cor­
responds exactly to the chordal graph shown in Fig­
ure 5c, which has the same number of cliques, but one
with four elements instead of three. Pearl's suggestion
that combining nodes, as shown in Figure Sd, might be
superior, actually results in the chordal graph shown
in Figure 5e , which is even less efficient than the other
two chordal graphs.

Theorem

5

2

(Global Conditioning)

The Method of Global Conditioning is a special case
of the Clustering Algorithm. It is less efficient than
the Clustering Algorithm unless the conditioning set
already appears in every cluster.

The Method of

Loop-Cutset Conditioning

(Pearl 1986b; Peot and Shachter 1991) is closely re­
lated to the Method of Global Conditioning: the in­
stantiated problems are solved using the (modified)
Polytree Algorithm, so the conditioning set must ren­
der the original belief network at most singly con­
nected. The set is called a loop-cutset because every
undirected cycle (or loop) in the network contains at
least one arc emanating from one of the nodes in the
conditioning set (Pearl 1 986a; Peot and Shachter 1 991;

PAR ALLEL IMPLEMENTATION
OF GLOBAL CONDITIONING

In the preceding section it was shown that the Method
of Global Conditioning is never more efficient than the
Clustering Algorithm, but it can nonetheless present
some natural opportunities for parallel implementa­
tion. In this section we show some of the technical
details and some efficiencies possible in such an imple­
mentation.
We start with a master cluster tree, on which we
would like to perform inference. From this master tree,
we build instantiated cluster trees for each possi­
ble case xK of the conditioning variables XK. After
distributing evidence within the instantiated cluster

Global Conditioning for Probabilistic Inference in Belief Networks

Figure

5:

we construct a new master tree equivalent to
what the original master tree would have been if we
had performed message passing directly on it. Each
instantiated cluster tree has the same topology as the
master tree, but without the variables XK. We fill in
the table for each cluster (and separation set) in the
instantiated tree by copying the table from the master
tree corresponding to the particular values of Xs,nK·
W henever there are some conditioning variables in a
cluster in the master tree, SinK # 0, the correspond­
ing cluster in the instantiated tree should be marked
as changed, since it now contains likelihood evidence
about XK. After passing messages within the instanti­
ated tree, we can add the resulting tables into the rna&
ter tree simply by summing out the variables XK\S;
that do not appear in the master tree,
trees,

Pr{Xs,,€}

=

L

K\S;

Pr{Xs,uK,€} .

The choice of master cluster tree depends on the con­
ditioning set which will be used to construct the in­
stantiated cluster trees. Consider the example shown
in Figure la. If we are going to condition on node
2, we might want the master tree shown in Figure 6a
rather than the one shown in Figure 6c or any of the
ones shown in Figure 2. Although the tree shown in
Figure 6a is a valid cluster tree based on the belief
network, it could not be obtained directly from any
of the cluster trees in Figure 2. Its corresponding in­
stantiated cluster tree conditioned on node 2 is shown
in Figure 6d. The instantiated cluster tree shown in
Figure 6e corresponds to the master tree shown in
Figure 6b conditioned on node 6. The instantiated
tree shown in Figure 6f corresponds to the master tree
shown in Figure 6c conditioned on nodes 3 and 6.
W hen we build an instantiated cluster tree, any sepa­
ration sets which were subsets of the conditioning set
become empty, as shown by the dashed arcs in Fig­
ure 6e and Figure 6f. This can be handled within
the Clustering Algorithm as scalar separation sets,

��
&-

I

I

0

Pearl's Example for Clique Formation.

519

0

F igure 6: Master and Instantiated Cluster Trees.

but there is some efficiency to be gained by exploit­
ing the fact that the problem has been separated
into "islands." Instead of passing the scalar messages
throughout the network, where we will have to update
the tables (in all but one island) twice, we can recog­
nize the cumulative contributions among islands at a
higher level and perform the scalar update only once,
as we copy the table back into the master problem.
To perform this, we need to construct a scalar «island
tree" with a node for each island. We can compute
the scalar value for any island by summing over its
smallest cluster (or separation set). The messages sent
between islands are then computed by the Clustering
Algorithm, obtaining an update factor for each island,
to be applied when the island's tables are copied back
into the master cluster tree If we ever encounter a
zero scalar value for any island, then we can skip the
rest of the processing for the entire instantiated clus­
ter tree, because our model says that this particular
instantiation is not possible given our observations €.
.

Trading Time for Memory

Another application of the Method of Global Con­
arises when there is insufficient memory to
store the master cluster tree. The procedure from the
previous section can be changed to trade extra com­
pute time for memory savings. Thus, as in matrix
algorithms, it is possible to transform a computation
over a spatial dimension into an iteration of the values
of that dimension over a spatially reduced problem.
ditioning

If the master cluster tree cannot be explicitly con­

structed because of space limitations, we can mod­
ify the parallelized algorithm. First we must deter­
mine a good conditioning set and a good structure for
the instantiated cluster tree, with cluster tables small
enough for the restricted memory. We then serially
consider each case of the conditioning variables. For

520

Shachter, Andersen, and Szolovits

each instantiation we initialize the cluster tree from the
input distributions rather than from a master tree. If
our goal is to compute Pr{XJ, E} for some set of vari­
ables J, then we accumulate the answer into these ta­
bles instead of into a master tree, but using the same
kind of mapping.

6

RECOGNIZING NEW CLUSTER
TREES

Although Global Conditioning is only performing the
same operations as the Clustering Algorithm, it can
provide additional insight about a problem. In this
section, we explore a sub tl e trick that emerges from
an ingenious application of Loop-Cutset Conditioning
(Suermondt and others 1990) to a "star-shaped" diag­
nostic network constructed using similarity networks
(Heckerman 1990a; Heckerman 1990b).
In that problem there is a single variable (disease)
whose observation splits the belief network into many
small disconnected pieces. By conditioning on that
variable, each piece can be solved independently.
(They are still dependent, but that dependence is re­
flected in their likelihood update for the disease node.)
An abstract representation of the situation in cluster
trees is shown in Figure 7, in which Xo corresponds to
the disease variable. Using the parallelization method
in Section 5, we would be splitting this network into
three "islands." The graphs shown in Figure 7a and
Figure 7b are both valid cluster trees. If our goal is to
collect at cluster 0 based on evidence recorded in clus­
ters 067 and 12, the cluster tree shown in Figure 7b
is superior to the one shown in Figure 7a. Of course,
there is no way to anticipate exactly what evidence
will be observed, so we can us e the following resul t to
restructure the cluster tree dynamically.

Theorem 4 (Dynamic Restructuring) Given

that there are two nonadjacent clusters A and B in
a cluster tree, such that A n B is equal to the separa­
tion set on some arc e on the path between them, arc
e can be replaced by a new arc connecting A and B.
The change is completely reversible and messages for
the old arc will be valid for the new one. If joint-form
messages are used, then all messages in the network
are still valid; otherwise, the messages along the old
path from A to B might require updating.
A n B must be
contained in every cluster between A and B. The arc
Proof: Because we have a join tree,

e can be replaced by an arc between any of the clusters
along the path from A until e and any of the clusters
along the path from B until e without violating the

join tree property, and hence maintaining the cluster
tree conditions. Because any pair of those clusters
satisfy the same conditions as A and B, the new arc can
be drawn between A and B without loss of generality.
The change is reversible, because the conditions will
be satisfied by the two clusters which were incident

Figure 7: Dy namic Restructuring of Cluster Trees.
to e with respect to the new arc. The message in
one direction along the old arc, with a slight abuse of
notation, is
MAB

=

L II

N\BkEKAB

\Ilk,

and it does not change because KAB is maintained.
If joint-form messages are used, they can be inter­
preted as posterior joint distributions over the sepa­
ration sets, and the separation sets are maintained. If
factored form messages are used, their values depend
on the Kij partition of the clusters in the network.
The only place where these partitions change is along
the old path between A and B. D
This theorem can be easily applied. Consider the clus­
ter tree shown in Figure 7c. Ope rationally, we can
view the arc between 0 and the three clusters above
it as "flexible," because any one of the three clusters
can be connected to 0 without changing the messages
on the arc. Likewise, there is a flexible arc between
0 and the three clusters below it. This is particularly
powerful in combination with joint-form messages, al­
though the updating formula for factored form mes­
sages is simple to compute because

p�Id
IJ

=

pn.ew "
IJ

Not all of the new messages need to be computed if
the only operation is to collect and not to distribute.
Another example of the theorem is in the cluster trees
shown in Figure 2. In the tree shown in Figure 2b, the
separation set between 67 and 346 is just 6, so the arc
between them can be replaced by one from 67 to any
other cluster containing 6. Thus we can immediately
obtain the tree shown in Figure 2c, as well as one in
which 67 is adjacent to 568 instead.

Global Conditioning for Probabilistic Inference in Belief Networks

7

Conclusions

521

