
We examine two types of similarity networks each based on a distinct notion of relevance. For both types of similarity networks we present an eﬃcient inference algorithm that works under the assumption that every event has a nonzero probability of occurrence. Another inference algorithm is developed for type 1 similarity networks that
works under no restriction, albeit less eﬃciently.

1

INTRODUCTION

Similarity networks were invented by Heckerman
(1991) as a tool for constructing large Bayesian networks from the judgments of domain experts. Heckerman used them to construct a diagnostic system for
lymph-node pathology. The main advantages of similarity networks are their ability to improve the quality
of the domain expert’s judgments and to utilize statements of conditional independence that are not represented in a Bayesian network in order to reduce the
number of probabilities the expert needs to specify.
In (Geiger and Heckerman, 1991), we removed several
technical restrictions imposed by the original development, and showed how to use a similarity network directly for inference without converting it to a Bayesian
network as proposed in (Heckerman, 1991). In do so,
we showed how to take advantage of asymmetric independence assertions to speed up inference.
In this paper, we define two types of similarity networks each based on a distinct notion of relevance.
We specify more fully the inference algorithm outlined
in (Geiger and Heckerman, 1991), and prove that it
works for type 1 similarity networks. We also develop
a faster inference algorithm that works for both types of similarity networks, under the assumption that
every event has a nonzero probability of occurrence.

David Heckerman
Microsoft Research Center and
Department of Computer Science, UCLA
One Microsoft Way, 9S/1024
Redmond, WA 98052-6399
<heckerma@microsoft.com>
We assume the reader is familiar with the definition
and usage of Bayesian networks. For details consult
(Pearl, 1988).

2

DEFINITION OF SIMILARITY
NETWORKS

Consider a mutually exclusive and exhaustive set of
hypotheses such as the list of possible identifications
in a classification task. We will represent these hypotheses as values of a variable called h. The variable
h is the focus of construction for similarity networks, and sometimes is called the distinguished variable
or hypothesis variable. We refer to other variables in
a given domain as nondistinguished variables. Each
value of h is called a hypothesis.
Definition A cover of a set of hypotheses H is a collection {A1 , . . . , Ak } of nonempty subsets of H whose
union is H. Each cover is a hypergraph, called a similarity hypergraph, where the Ai are hyperedges and
the hypotheses are nodes. A cover is connected if the
similarity hypergraph is connected.
A similarity network is a set of Bayesian networks,
each constructed under the assumption that h draws
its value from a specific subset of its domain. The
event “h draws its value from the set Ai ” is denoted
by [[Ai ]].
Definition Let P (h, u1 , . . . , un ) be a probability distribution and A1 , . . . , Ak be a connected cover of the
values of h. A directed acyclic graph Di is called a local network of P associated with Ai if Di is a Bayesian
network of P (h, v1 , . . . , vm | [[Ai ]]), where {v1 , . . . , vm }
is the set of all variables in {u1 , . . . , un } that “help to
discriminate” the hypotheses in Ai . The set of k local
networks is called a similarity network of P .
We define “help to discriminate” formally in the next
section.
Let us examine this definition, using the following example taken from (Geiger and Heckerman, 1991):

Spy/Visitor

✗

✔

✗

Visitor/Worker
h

✔

✖
✕
✖
✕
°
°
° ✗✔
❄
✗✔
✠
°
° ✗✔
❄
✗✔
✠
°
✛
b
b
g
g
✖✕✖✕
✖✕✖✕

h

✗

✔

Worker/Executive
h

✖

✕

❄
✗✔

✖✕

l

Figure 1: A similarity network representation of the
secured-building story.

A guard of a secured building expects four
types of persons to approach the building’s
entrance: executives, regular workers, approved visitors, and spies. As a person approaches the building, the guard can note its
gender (g), whether or not the person wears a
badge (b), and whether or not the person arrives in a limousine (l). We assume that only
executives arrive in limousines and that male
and female executives wear badges just as
do regular workers (to serve as role models).
Furthermore, we assume that spies are mostly men. Spies always wear badges in an attempt to fool the guard. Visitors don’t wear
badges because they don’t have one. Femaleworkers tend to wear badges more often than
do male-workers. The task of the guard is to
identify the type of person approaching the
building.
This problem is represented by the similarity network
shown in Figure 1. The similarity network in this
figure is based on the cover {spy, visitor}, {visitor,
worker}, {worker, executive} of the hypotheses set.
This cover is connected, because it consists of the
three links spy—visitor—worker—executive which form a connected hypergraph. This similarity network
contains three local networks: one local network helps
to discriminate spies from visitors, another local network helps to discriminate visitors from workers, and a
third local network helps to discriminate workers from
executives.
In each local network, we include only those variables

that help to discriminate among the hypotheses covered by that local network. For example, badge worn
and gender are not included in the local network for
workers versus executives, because they do not help to
distinguish between these hypotheses. Similarly, the
variable representing arrival in a limousine is not included in the local networks for spies versus visitors or
visitors versus workers.
The relationship between gender and the hypothesis
variable h is an example of subset independence, whereby a nondistinguished variable is independent of h,
given h draws its values from a subset of hypotheses {worker, executive}. In general, an assertion of
subset independence is represented in a similarity network whenever a link between the hypothesis node
and a nondistinguished variable exists in some local
networks, but not in all networks. The relationship
between badge worn and h, when h is restricted to
{worker, executive}, and the relationship of arrival in
limousine and h, when h is restricted to {spy, visitor},
are additional examples of subset independence.
The relationship between gender and badge worn is
an example of hypothesis-specific independence, whereby two variables are independent given some hypotheses { spies, visitors}, but dependent given others {visitors, workers}. In general, a hypothesis-specific
independence assertion is represented in a similarity
network whenever a link between two nondistinguished
variables exists in some local networks, but does not
exists in other local networks.1
The definition of similarity networks does not specify how to select a connected cover of hypotheses. Although any selection of a connected cover yields a valid
similarity network, some selections yield similarity networks that display more subset independence assertions than do other selections. An analogous situation
exists when constructing a Bayesian network where
some construction orders yield Bayesian networks that
display more symmetric independence assertions than
do other Bayesian networks. The practical solution for
constructing a Bayesian network is to choose a construction order according to cause-eﬀect relationships. This selection tends to maximize the information
about symmetric independence encoded in the resulting network. The practical solution for constructing
the similarity hypergraph is to choose a connected cover by grouping together hypotheses that are “similar”
to each other by some criteria under our control (e.g.,
spies and visitors). This choice tends to maximize the
number of subset independence assertions encoded in
a similarity network. Hence the name for this representation.

1
Heckerman (1991) coined the terms subset independence and hypothesis-specific independence.

3

TWO TYPES OF SIMILARITY
NETWORKS

The definition of similarity networks is not complete
without attributing a precise meaning to the utterance
“helps to discriminate” used in the definition of a local network. We give two possibilities based on the
study of three relations coupled, related, and relevant
presented in (Geiger and Heckerman, 1990).
Definition Let P (u1 , . . . , un | e) be a probability
distribution where e is a fixed event. Variables ui and
uj are unrelated given e, if ui and uj are disconnected
in every minimal Bayesian network of P (u1 , . . . , un |
e). Otherwise, ui and uj are related given e, denoted
related(ui , uj | e).
This definition states that two variables ui an uj are
unrelated given e, if there exists no trail connecting
them—that is, there exists no sequence of variables
ui , . . . , uj such that every two consecutive variables in
this sequence are connected with a link. The requirement that ui and uj be disconnected in every minimal
network is not as strong as it may seem, because if ui
and uj are disconnected in one minimal Bayesian network of P , then ui and uj are disconnected in every
minimal Bayesian network of P (Geiger and Heckerman, 1990).
Definition Let P (u1 , . . . , un | e) be a probability distribution where e is a fixed event. Variables ui and uj are mutually irrelevant given e, if
P (ui | uj , v1 , . . . , vm , e) = P (ui | v1 , . . . , vm , e)
or P (v1 , . . . , vm | e) = 0 for every value of
v1 , . . . , vm , where {v1 , . . . , vm } is an arbitrary subset
of {u1 , . . . , un }\{ui , uj }. Otherwise, ui and uj are mutually relevant given e, denoted relevant(ui , uj | e).
This definition states that two variables ui and uj are
mutually irrelevant given e if for any consistent assignment for the variables of interest knowing the value of
one variable does not change the knowledge about the
values of the other.
To use the concepts of relatedness and relevance for
defining those variables that are included in a local
network, we distinguish one variable as the hypothesis
variable (call it h), and define the event e to be [[Ai ]]—
namely, a disjunction over a subset of the values of
h.
If e is the event [[A]], where A is the set of all values of h—that is, e states that variable h is assigned
one of its values—then instead of related(ui , uj | [[A]]),
we write related(ui , uj ) and say that ui and uj are
related. Similarly, instead of relevant(ui , uj | [[A]]) we
write relevant(ui , uj ) and say that ui and uj are mutually relevant.
Definition A similarity network constructed by including in each local network Di only those variables
u that satisfy related(u, h | [[Ai ]]) is said to be of type 1.

A similarity network constructed by including in each
local network Di only those variables u that satisfy
relevant(u, h | [[Ai ]]) is said to be of type 2.
An equivalent definition for type 1 similarity networks
is entailed by the following theorem:

Theorem 1 (Geiger and Heckerman, 1990) Let
P (u1 , . . . , un | e) be a probability distribution where
U = {u1 , . . . , un } and e be a fixed event. Then, ui
and uj are unrelated given e iﬀ there exist a partition U1 , U2 of U such that ui ∈ U1 , uj ∈ U2 , and
P (U1 , U2 | e) = P (U1 | e)P (U2 | e). 2
We can now associate relatedness and relevance.
Theorem 2 Let P (u1 , . . . , un | e) be a probability distribution where e is a fixed event. Then, for every ui
and uj , relevant(ui , uj | e) implies related(ui , uj | e).
Proof: Suppose ui and uj are not related given e. Let
U1 , U2 be a partition of U such that ui ∈ U1 , uj ∈ U2
and I(U1 , U2 | ∅) holds for P . Theorem 1 guarantees
the existence of these conditions. We show that ui
and uj must be mutually irrelevant given e. Let Z
be an arbitrary subset of {u1 , . . . , un } \ {ui , uj }. Let
Z1 = Z ∩ U1 and Z2 = Z ∩ U2 . Since I(U1 , U2 | ∅)
holds in P (u1 , . . . , un | e), it follows, by summing over
the variables not in {ui , uj } ∪ Z1 ∪ Z2 , that I({ui } ∪
Z1 , {uj } ∪ Z2 | ∅) holds. Consequently, I({ui }, {uj } |
Z1 ∪Z2 ) must hold as well (Pearl 1988, Equation 3.6c).
Thus, I({ui }, {uj } | Z) holds for every Z. Hence, ui
and uj are mutually irrelevant given e. (This proof is
a slight modification of the one appearing in (Geiger
and Heckerman, 1990)). ✷
In particular, Theorem 2 holds when e is the event
[[Ai ]] where Ai is a subset of values of some variable
in U . Consequently, a type 2 similarity network of P
always includes in each local network at least all the
variables included in that local network by a type 1
similarity network. Moreover, whenever a variable u
does not satisfy related(u, h), it will not be included
in any local network of a type 1 or a type 2 similarity
network.
It is not hard to construct an example showing that
related does not imply relevant. For example, consider
a Markov chain P (x, y, z) = P (x)P (y | x)P (z | y)
where x and z are binary variables, and y is a trinary
variable. Any two matrices P (y | x) and P (z | y)
whose lines and columns are linearly independent and
whose product yields a matrix with identical lines will
render x and z (marginally) independent. Since x and
z are also independent given y, it follows that x and z
are mutually irrelevant, although x and z are related.
A necessary and suﬃcient condition for the converse
of Theorem 2 to hold is that P (u1 , . . . , un | e) be tran2

In (Geiger and Heckerman, 1990), P (U | e) is replaced
with P (U ). Since e is a fixed event, this shift of notation
does not alter the proof of this theorem.

sitive. Namely, for every three variables ui , uj and uk ,
relevant(ui , uj | e) & relevant(uj , uk | e)
⇒ relevant(ui , uk | e)

(1)

(Geiger and Heckerman, 1990).
Indeed, in the previous example, transitivity is violated, because x and y are mutually relevant and y and
z are mutually relevant, but x and z are not mutually
relevant. Notably, the relation related is always transitive, because connectivity in graphs is transitive.

4

INFERENCE USING
SIMILARITY NETWORKS

The main task similarity networks are designed for is
to compute the posterior probability of each hypothesis given a set of observations, as is the case in diagnosis. In this section, we show that under reasonable assumptions, the computation of the posterior probability of each hypothesis can be done in each local network
and then be combined coherently according to the axioms of probability theory. We analyze the complexity
of our algorithm demonstrating its superiority over inference algorithms that operate on Bayesian networks.
We assume that any instantiation of the variables in
a similarity network of P has a nonzero probability of
occurrence. Such a probability distribution is said to
be strictly positive. This assumption can be reasonable
in such applications as medical diagnosis, where given
an arbitrary collection of clinical findings, each potential disease retains some probability of occurrence.
Subject to this assumption, we develop an inference
algorithm that operates both on type 1 and type 2
similarity networks. We will remove this assumption
latter at the cost of obtaining an inference algorithm
that operates only on type 1 similarity networks and
whose complexity is higher.
The inference problem at hand can be stated as follows: Given a similarity network of P (h, u1 , . . . , un )
that is based on a partition A = {A1 , . . . , Ak } of the
values of h, and given a set of assignments v1 , . . . , vm
for a set v1 , . . . , vm of variables that is a subset of
{u1 , . . . , un } compute P (hj | v1 , . . . , vm )— the posterior probability of hj —for every hj .
In order to compute the posterior probability of each
hj we use the procedure INFER. This procedure has
two parameters, one specifying a query of the form
“compute P (X | Y)” and the second is a Bayesian
network where X and Y are sets of variables that appear in the network and Y is a value of Y . We do
not need to specify INFER’s operational details in order to demonstrate how this procedure is extended to
operate on similarity networks. We now describe this
new inference algorithm.
First, for each hi we identify a set of hypotheses
Aj ∈ A to which hi belongs and compute the posterior probability of hypothesis hi under the addi-

tional assumption that one of the hypotheses in Aj
holds true. In other words, we compute P (hi |
v1 , . . . , vm , [[Aj ]]). Second, the posterior probabilities
P (hj | v1 , . . . , vm ) are computed from the probabilities P (hj | v1 , . . . , vm , [[Ai ]]) by solving a set of linear
equations:
P (hj | v1 , . . . , vm ) = P
P(hj | v1 , . . . , vm , [[Ai ]])·
hj ∈Ai P (hj | v1 , . . . , vm )

that relate these quantities. We shall see later that
these equations have a unique solution.

It remains to show how to compute the query P (hi |
v1 , . . . , vm , [[Aj ]]). It seems that one can merely call
the procedure INFER to compute this query using the
local network Dj which corresponds to Aj . The query
P (hi | v1 , . . . , vm , [[Aj ]]), however, may include variables that do not appear in Dj in which case INFER
is not applicable.
Fortunately, for both type 1 and for type 2 similarity
networks the following equality will be shown to hold:
P (hi | v1 , . . . , vl , [[Aj ]]) = P (hi | v1 , . . . , vm , [[Aj ]])
(2)
where v1 , . . . , vl are the variables in {v1 , . . . , vm }
that appear in Dj and v1 , . . . , vl are their values.
Thus to compute P (hi | v1 , . . . , vm , [[Aj ]]) we use
the procedure INFER to compute the query P (hi |
v1 , . . . , vl , [[Aj ]]) using the network Dj . Equation 2
tells us that the two computations yield identical answers.
For type 2 similarity networks, the justification of Equation 2 is that vl+1 , . . . , vm are conditionally independent of hi given every value of the variables
v1 , . . . , vl that appear in Dj where vl+1 , . . . , vm are the
variables in {v1 , . . . , vm } that do not appear in Dj . If
Equation 2 does not hold, some of the variables in
{vl+1 , . . . , vm } would have appeared in the local network Dj , contrary to our assumption that Dj contains
only v1 , . . . , vl . Moreover, if some variable appears in
a type 2 similarity network, then it will also appear
in a type 1 similarity network (see the comment after
Theorem 2). Therefore, this equality holds for type 1
similarity networks as well.
This algorithm is summarized below.
Algorithm (Inference in similarity networks)
Input: A similarity network of P (u1 , . . . , un , h) based
on a connected cover A1 , . . . , Ak of h’s values.
Output: P (h | v1 , . . . , vm ) where v1 , . . . , vm are
values of variables v1 , . . . , vm and {v1 , . . . , vm } is a subset of {u1 , . . . , un }.

Notation: Dj denotes the local network that corresponds to Aj and Vj are the variables that appear in
Dj .
1
2

For each Aj
Let {v1 , . . . , vl } be the variables in

Vj ∩ {v1 , . . . , vm }
though some equations might be redundant, these eFor each hi ∈ Aj
quations are always consistent. When the set of loαij := INFER(P (hi | v1 , . . . vl , [[Aj ]]), Dj ) cal networks is constructed from expert judgments, as
done in practice, consistency is not guaranteed. HeckIf αij = 0, then
erman (1991) describes an algorithm that helps a usReturn “P is not strictly positive”
er to construct a consistent set of local networks by
6
Solve the following set of linear equations:
prompting to his attention all probabilities that have
7
For all iP
and j, P (hi | v1 , . . . , vm ) =
already been assigned previously in another local netαij · hi ∈Aj P (hi | v1 , . . . , vm )
P
work and verifying with him that these probabilities
8
P
(h
|
v
,
.
.
.
,
v
)
=
1
i
m
i
1
are acceptable.
9
Return P (h | v1 , . . . , vm )
It remains to analyze the complexity of this inference
algorithm. For simplicity, we assume that all variWe have argued already that the solution to the eables are binary in which case the procedure INFER
quations listed in Lines 7 and 8 provides the desired
has a complexity of O(2n ). In the worst case, the
posterior probability. It remains to show that there
proposed inference algorithm may not perform more
exists a unique solution. Let us examine a local neteﬃciently, because all n variables may appear in each
work Dj that corresponds to Aj . Assume Aj consists
local network. In practice, however, each local netof h1 , . . . , hr . Since v1 , . . . , vm remain fixed throughwork contains a small percentage, say c, of the n variout the computations we denote P (hi | v1 , . . . , vm )
ables, because all other variables are irrelevant given
by Q(hi ). Consider the following equations:
the context of a specific local network.3 If O(n) local
networks are given, the complexity of applying INFER
Q(h1 ) = α1,j [Q(h1 ) + Q(h2 ) + . . . + Q(hr )](3)
to these local networks is O(n · 2cn ), which is smaller
Q(h2 ) = α2,j [Q(h1 ) + Q(h2 ) + . . . + Q(hr )](4)
than O(2n ) obtained by applying INFER on a single
...
Bayesian network generated from these local networks. The complexity of solving the equations on Line 7
Q(hr ) = αr,j [Q(h1 ) + Q(h2 ) + . . . + Q(hr )](5)
and 8 is ignored, because it is linear in n. Thus from
These are the subset of the equations defined in Line 7
2100 calculations, for example, we reduce to 100 · 220 .
which correspond to the local network Dj . By dividing every pair of consecutive equations, we obtain the
following ratios:
5 INFERENTIAL AND
αr,j
DIAGNOSTIC COMPLETENESS
Q(hr ) =
Q(hr−1 )
(6)
αr−1,j
αr−1,j
An important property of Bayesian networks is that
Q(hr−1 ) =
Q(hr−2 )
αr−2,j
their parameters encode the entire joint distribution
...
through the product rule. This property guarantees
α2,j
Q(h2 ) =
Q(h1 )
that any inference task can in principle be computed
α1,j
from the parameters encoded in a Bayesian network.
Motivated by this observation we establish the followHence, the solution of these equations provides the raing definition.
tios of the posterior probabilities between every pair
3
4
5

of hypotheses in Aj . Since we repeat this process for
every Aj and since the cover defined by A1 , . . . Ak is
connected, the ratio of every pair of hypotheses is established. To obtain the absolute values of each Q(hi ),
it remains to normalize their sum to one, using the Equation on Line 7 of the algorithm.

Definition
A
similarity network S for P (u1 , . . . , un , h) is inferentially
complete if the distribution P (u1 , . . . , un , h) can be recovered from the parameters of S.

Theorem 3 Let P (h, u1 , . . . , un ) be a probability distribution and A = {A1 , . . . , Ak } be a partition of the
values of h. Let S be a similarity network based on A.
Let v1 , . . . , vm be a subset of variables whose value is
given. There exists a single solution for the set of equations defined by Line 7 and 8 of the above algorithm
and this solution determines uniquely the conditional
probability P (h | v1 , . . . , vm ).

Clearly not all similarity networks are inferentially
complete. For example if P (u1 , . . . , un , h) factors into
the product P (u1 )P (u2 . . . , un , h) then the variable u1
will not be included in any local network. Therefore,
it will be impossible to recover P (u1 ) from the parameters encoded in the similarity networks of P . The
information about P (u1 ) that is lost in the process of
producing a similarity network of P , however, is never
needed in order to compute the posterior probability of
any hypothesis. Evidently, inferential completeness is
too strong a requirement for the purpose of computing
the posterior probability of each hypothesis.

An important observation to make is that the equations on Lines 7 and 8 are derived from a given probability distribution P (h, u1 , . . . , un ). Consequently, al-

3
A reasonable number for c in the lymph-nodepathology domain is 0.2.

Consequently we have proven the following theorem.

Definition
A
similarity
network S for P (h, u1 , . . . , un ) is diagnostically complete if the conditional distribution P (h | v1 , . . . , vm )
can be recovered from the parameters of S for every
subset {v1 , . . . , vm } of {u1 , . . . , un }.

In the previous section, we showed that every type
1 or type 2 similarity network of a strictly positive
probability distribution P is diagnostically complete
(Theorem 5). The inference algorithm we presented
shows how to compute P (h | v1 , . . . , vm ) for every
value of v1 , . . . , vm . If P is not strictly positive, one
can construct examples where the equations defined by
Lines 7 and 8 of our inference algorithm do not have a
single solution. Nevertheless, we will prove that, under minor restrictions, every type 1 similarity network
is diagnostically complete. We conjecture that every
type 2 similarity network is diagnostically complete,
under the same restrictions.
Before proving diagnostic completeness we resort to an
example where our inference algorithm fails, and examine how the posterior probability can be computed
in an alternative way. This computation highlights the
general approach. Suppose S is a similarity network
for P (h, y) where h has three values {h1 , h2 , h3 } having equal apriori probability; and suppose that y has
two values +y, −y. Also assume that S is based on the
cover {{h1 , h2 }, {h2 , h3 }}, and that P (+y | h2 ) = 0.

When we apply our algorithm to compute P (hi |
+y), the algorithm generates three equations P (h1 |
+y, [[h1 , h2 ]]) = 1, P (h2 | +y, [[h2 , h3 ]]) = 0, and
P (h3 | +y, [[h2 , h3 ]]) = 1. From these three equations,
we cannot compute the relative magnitude of the posterior probability of h1 versus h3 . All three equations
merely show that P (h2 | +y) is zero.

P (hi | +y), however, can be computed from the parameters that quantify S. These parameters include
the following: P (h1 | h1 ∨ h2 ), P (h2 | h2 ∨ h3 ), P (h3 |
h2 ∨ h3 ), and P (+y | h1 , h1 ∨ h2 ), P (+y | h2 , h1 ∨ h2 ),
and P (+y | h3 , h2 ∨ h3 ). From the first three parameters, P (hi ), i = 1 . . . 3, can be recovered provided none
of the prior probabilities is zero. The restriction that
all prior probabilities are nonzero is quite reasonable.
If the prior probability of some hypothesis were zero,
there would be little reason to include that hypothesis
in the model.
The other three parameters are equal to P (+y | h1 ),
P (+y | h2 ), and P (+y | h3 ), respectively, because hi
entails hi ∨ hj . Thus P (hi | +y) can be computed by
Bayes rule:
P (+y | hi )P (hi )

P (hi | +y) = P3

j=1

P (+y | hj )P (hj )

In the case of type 1 similarity networks, this example indicates a general methodology for computing the
posterior probability of each hypothesis. The general method is based on the proof of the following two
theorems.

Theorem 4 (restricted inferential completeness)
Let S be a type 1 similarity network of P (h, u1 , . . . , un )
based on the connected cover A1 , . . . , Ak of the values of h. Let {v1 , . . . , vl } be a subset of variables in
{u1 , . . . , un } that satisfy relevant(vi , h). Then, the distribution P (h, v1 , . . . , vl ) can be computed from the parameters encoded in S provided P (hi ) 6= 0 for every
value hi of h.
Proof: To show that the distribution P (h, v1 , . . . , vl )
can be computed from the parameters of S, we will
show how to compute P (h) and then we will show how
to compute P (v1 , . . . , vl | h). The product of these two
probability distributions is equal to P (h, v1 , . . . , vl ).
For each hypothesis hi , let αij equal INFER( P (hi |
[[Aj ]]), Dj ), where Aj contains hi and Dj is the local
network corresponding to Aj . The prior probability
of each hi is computed by solving the following set of
linear equations:
P (hi ) = αij ·

X

hi ∈Aj

P (hi ),

n
X

P (hi ) = 1

1

In the previous section, we solved these equations and
showed that the solution Equation 6 is unique provided
P (hi ) 6= 0 for all hi .
Due to the chaining rule, P (v1 , . . . , vl |hi ) can be factored as follows:
P (v1 , . . . , vl |hi ) =
P (v1 |hi ) · P (v2 |v1 , hi ) . . . P (vl |v1 , . . . , vl−1 , hi )

(7)
Thus, it suﬃces to show that for each variable vj ,
P (vj |v1 , . . . , vj−1 , hi ) can be computed from the parameters encoded in S. Furthermore we can assume
that the conditioning event is consistent, lest the entire
product is zero, and the equality holds.
Let Di denote a local network in S, Ai be the hypotheses associated with Di , and hi be a hypothesis in Ai .
Each variable vj is depicted in some local network, because it satisfies relevant(vj , h). Let Ai , Ai+1 , . . . , Am
be a path in the similarity hypergraph where Am is
the only hyperedge on this path associated with a local network that depicts vj as a node. Such a path
exists, because the similarity hypergraph is connected
and vj is depicted in one of the local networks. If vj is
depicted in Ai then the entire path has one hyperedge:
Ai .
Let Dk be the local network associated with Ak for
k = i+1 . . . m and let hi+1 , hi+2 , . . . , hm be a sequence
of hypotheses such that hk ∈ Ak−1 ∩ Ak . Due to the
definition of type 1 similarity networks, since vj is not
depicted in Dk where k < m, vj is unrelated to h given
[[Ak ]]. Whenever vj is unrelated to h given [[Ak ]], vj is
also mutually irrelevant to h given [[Ak ]]. Thus,
P (vj |v1 , . . . , vj−1 , hk−1 , [[Ak ]]) =
P (vj |v1 , . . . , vj−1 , hk , [[Ak ]])

Since h implies [[A]] whenever h ∈ A it follows that

P (vj |v1 , . . . , vj−1 , hk−1 ) = P (vj |v1 , . . . , vj−1 , hk )

This equation holds for every k between i + 1 and m,
thus we obtain,
P (vj |v1 , . . . vj−1 , hi ) = P (vj |v1 , . . . vj−1 , hm )

Furthermore,

P (vj |v1 , . . . vj−1 , hm ) = P (vj |v10 , . . . vl0 , hm )

where
subset

v10 , . . . , vl0 are the variables
of {v1 , . . . , vj−1 }).

(8)

depicted in Dm (a

If Equation (8) did not hold, then related(v, vj | [[Am ]])
would hold, where v is some variable not appearing in
Dm . Nonetheless, together with related(vj , h | [[Am ]]),
which holds because vj is depicted in Dm , these two assertions imply by transitivity that related(v, h | [[Am ]])
holds too, contradicting the fact that v is assumed not
to be included in Dm . Finally,
P (vj |v10 , . . . vl0 , hm ) = P (vj |v10 , . . . vl0 , hm , [[Am ]]), (9)

because hm logically implies the disjunction over all
hypotheses in Am .

The latter probability can be computed using INFER
on the local network Dm . Thus, due to the equalities above, P (vj |v1 , . . . , vj−1 , hi ) can be computed as
needed. ✷
The above theorem shows that type 1 similarity networks are inferentially complete subject to the restriction that only features that help to discriminate between some hypotheses are included in the model and
that all hypotheses which are included in the model
have a probability greater than zero. Consequently,
diagnostic completeness is guaranteed too.
Theorem 5 (Diagnostic Completeness) Let S be
a type 1 similarity network of P (h, u1 , . . . , un ). Then
the conditional distribution P (h | v1 , . . . , vm ) can be
computed from the parameters of S for every subset
{v1 , . . . , vm } of {u1 , . . . , un } provided P (hi ) 6= 0 for
every value hi of h.
Proof: To compute P (h | v1 , . . . , vm ) observe that
P (h | v1 , . . . , vm ) = P (h | v10 , . . . , vl0 ) where v10 , . . . , vl0
is the subset of variables in v1 , . . . , vm that are relevant to h. Theorem 4 states that the joint distribution
P (h, v10 , . . . , vl0 ) can be computed from the parameters
of S. The conditional probability P (h | v10 , . . . , vl0 ) can
be computed from this joint distribution. ✷
The above two theorems provide us with a naive computation of the posterior probability of each hypothesis. This computation does not take into account the
fact that P (h, v10 , . . . , vl0 ) might be too large to be explicitly computed or stored as a table. Moreover, the
computation suggested by these proofs ignore the crucial observation that, in practice, all local networks
are often constructed according to a common order,
say h, v10 , . . . , vl0 , which usually reflects cause-eﬀect relations or time constrains.

When such a common ordering exists some computations become much easier. In particular, Equation 9
can be further developed. That is,
P (vj |v10 , . . . vl0 , hm , [[Am ]]) = P (vj |πDm (vj ), [[Am ]])
(10)
where v10 , . . . , vl0 are the variables depicted in the local
network Dm and πDm (vj ) are the parents of vj in Dm .
Consequently, P (vj |v10 , . . . vl0 , hm , [[Am ]]) need not be
computed using INFER as done in the proof. It is
stored explicitly at node vj in the local network Dm .
Equation 10 defines a Bayesian network Mi of
P (v1 , . . . , vm | hi ), because, for each vj , πMi is set
to be vj parents’ set in Dm excluding h, and the parameters associated with vj in Mi are merely those
associated with vj in Dm . The collection of these
local networks, one network for each hypothesis hi ,
forms a structure that we have called hypothesisspecific Bayesian multinet of P (h, v1 , . . . , vm ) (Geiger
and Heckerman, 1991).
Definition Let P (h, u1 , . . . , un ) be a probability distribution and h1 , . . . , hk be the values of h. A directed
acyclic graph Di is called a comprehensive local network of P associated with hi if Di is a Bayesian network of P (u1 , . . . , un | hi ). The collection of k comprehensive local networks is called an hypothesis-specific
Bayesian multinet of P .
The algorithm below summarizes how to bulid this
multinet. Afterwards, we shall show how to use it
compute the posterior probability of each hypothesis.
The first step below uses arc-reversal transformations
in order to reorient all local networks according to a
common construction order. This step is given for the
purpose of completeness, namely, to enable the algorithm to process similarity networks that are not constructed according to a common construction order.
In practice, however, this step usually is not needed,
because similarity networks are constructed according
to a common order of all relevant variables.
Algorithm (Similarity network to Bayesian
Multinet Conversion)
Input:
A
type
1
similarity
network S of P (h, u1 , . . . , un ) based on a connected
cover A1 , . . . , Ak of the values of h.
Output: A hypothesis-specific Bayesian multinet of
P (h, v1 , . . . , vl ) where each vi is depicted in some local
network of S.
Notation:
• Mi is the comprehensive local network associated
with hypothesis hi
• Di is the local network associated with Ai
• πG (u) are the parents of u in a graph G

• The conditional probability associated with node
u in Mi is PMi (u | πMi (u), hi ) and the condition-

✎

al probability associated with node u in Dm is
PDm (u | πDm (u), hm ).
1
2
3
4
5
6

Reorient all local networks in S according
to a common construction order
For each hi construct Mi as follows
For each vj taken in order v1 , . . . , vl
Find a path Ai , . . . , Am such that
hi ∈ Ai and vj is depicted
only in Am
Set πMi (vj ) to be πDm (vj ) \ {h}
Set PMi (vj | πMi (vj ), hi ) to be
PDm (vj | πDm (vj ), hm )

We now examine how the algorithm processes the similarity network S in Figure 1. Because the node ordering h, g, b, l is common to all local networks of S, the
algorithm performs no arc reversals. Suppose the algorithm first builds the comprehensive local network Me
for the hypothesis executive. Because l appears in the
local network for {worker, executive} with only h as a
parent, the algorithm makes l a root note in Me , and
sets PMe (l | executive) to be Pw∨e (l | executive), where
w ∨e denotes the local network for {worker, executive}.
The local network for {visitor, worker} is the closest
neighbor to the local network for {worker, executive}
that depicts g and b. Because the only parent of
g in the local network for {visitor, worker} is h, the
algorithm makes g a root node in Me . Because g
and h are the parents of b in the local network for
{visitor, worker}, the algorithm makes g a parent of
b in Me . The algorithm sets PMe (g | executive) to
be Pv∨w (g | worker) and PMe (b | g, executive) to be
Pv∨w (b | g, worker), where v ∨ w denotes the local network for {visitor, worker}. The algorithm constructs
the comprehensive local networks for worker, visitor
and spy similarly.
We can now use the inference algorithm stated below
to compute the posterior probability of each hypothesis.
Algorithm
(Hypothesis-specific
Bayesian-Multinet Inference)
Input:
• A
hypothesis-specific
Bayesian
multinet of P (v1 , . . . , vm | h), where Mi is the
comprehensive local network associated with hi
• The apriori probability distribution P (h)
• Instances v1 , . . . , vl for a set of variables
{v1 , . . . , vl } ⊆ {v1 , . . . , vm }
Output: The posterior probability distribution P (h |
v1 , . . . , vl )
1

For each hypothesis hi

✍
✡
✡
✢
✎☞
b✛
✍✌

h

☞
PP ✎☞
q l
✌P
❏
✍✌
❏
✎☞
✍✌

g

Figure 2: A Bayesian network representation of the
secured-building story.

2
3
4

βi = INFER( P (v1 , . . . , vl | hi ), Mi )
For each hypothesis hi
Compute P (hi | v1 , . . . , vl ) = PP (hi )·βi
P (hi )·βi
i

Line 2 is the normal computation performed by an
inference algorithm for Bayesian networks. Line 4 encodes Bayes rule.

The advantage of computing P (v1 , . . . , vl | h) via this algorithm versus using INFER on a Bayesian
network of P (v1 , . . . , vl , h) (see Figure 2) stems from
hypothesis-specific independence assertions represented in some local networks, but not represented in Figure 2.
For example, suppose the guard of our securedbuilding problem sees a person wearing a badge (b)
approach the building, but does not notice the person’s
gender or whether the person arrives in a limousine.
Using the Bayesian network of Figure 2, INFER computes the posterior probability of each possible identification (executive, worker, visitor, spy) as follows:
P (h | b) = k · P (h) ·

X
g

P (g | h) · P (b | g, h) (11)

where k is the normalizing constant that makes P (h |
b) sum to unity. Since the Bayesian network representing this problem does not encode any statement
of conditional independence among b, g, and h, the
above computation is done by any reasonable realization of INFER.
Alternatively, our inference algorithm computes the
posterior probability of each hypothesis more eﬃciently, using the appropriate hypothesis-specific Bayesian
multinet, as follows:
P (spy | b) = k · P (spy) · P (b | spy)
P (visitor | b) = k · P (visitor)
·P (b | visitor)
P (worker | b) = k · P (worker)
X
·
P (g | [[worker, executive]])
g
·P (b | g, [[worker, executive]])

(12)
(13)
(14)

P (executive | b) = k · P (executive)
X
·
P (g | [[worker, executive]])
g
·P (b | g, [[worker, executive]])

(15)

Equations 12 and 13 take advantage of hypothesisspecific independence. In particular, the two equations
incorporate the fact that g and b are conditionally independent given h = spy and h = visitor, respectively.
Thus, we do not have to sum over the variable gender
as we do when using the Bayesian network of Figure 2
(Equation 11). These savings are achieved by our inference algorithm for Bayesian multinets, because the
algorithm applies INFER on a local network that encodes this independence information.
Note that the sums in Equations 14 and 15 are equal
due to subset independence. Our inference algorithm,
however, does not use this fact or any other assertion of subset independence. The inference algorithm
for strictly positive distributions, on the other hand,
computes this sum only once.

6

OPEN PROBLEMS

We conjecture that every type 2 similarity network
also satisfies Theorems 4 and 5, and that the above algorithm is applicable to these networks as well. We
further believe that there exists an inference algorithm that uses both subset independence as well as
hypothesis-specific independence, even when the distributions are not strictly positive.

Acknowledgments
The second author thanks Mark Peot for suggesting
an early version of the inference algorithm for strictly
positive distributions. This research was supported in
part by the fund for the promotion of research in the
Technion.

