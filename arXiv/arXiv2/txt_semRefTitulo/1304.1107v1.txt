

Structure

2

IDEAL (Influence Diagram Evaluation and Analysis
in Lisp) is a software environment for creation and
evaluation of belief networks and influence diagrams.
IDEAL is primarily a research tool and provides an
implementation of many of the latest developments
in belief network and influence diagram evaluation in
a unified framework.

This paper describes IDEAL

and some lessons learned during its development.

IDEAL is written in Common Lisp. Lisp was chosen
as the implementation language since it is most suited
to exploratory programming and quick development.
In addition, the software is portable across a wide
variety of platforms.
IDEAL is a library of Lisp functions that pro­
vides the following features:
•

Data structures for representing influence dia­
grams and belief networks.

•

1

grams and belief networks.

Introduction

Over the last few years influence diagrams
lief networks

[5] and be­

•

tation schema for domains where uncertainty plays

•

process them

as

well

[10,8,14].

as

as

the semantics of these

on efficient algorithms to

•

lief networks.
•

systems. IDEAL is a software package that was de­
veloped

as

a platform for research in belief networks

and influence diagrams. IDEAL also can be used to

•

Algorithms for p erforming inference in influence
diagrams and inference and belief propagation in
belief networks.

library of functions that provides the belief network
and influence diagram methodology for embedded use

Routines that perform some basic transforma­
tions of influence diagrams.

create intermediate sized run-time systems and as a

by other applications.

Utilities that provide many useful services like
consistency checking and creation of random be­

This work has now matured to the point where
these techniques are finding their way into production

Utilities that are of use in coding influence dia­
gram manipulation algorithms etc.

an important role. There has been a wealth of work
representations

Facilities for copying, saving (to file) and loading
influence diagrams and belief networks.

[10] have emerged as attractive represen­

on both basic issues such

Facilities for creating and editing influence dia­

•

Influence diagram evaluation algorithms.

IDEAL incorporates, in a unified framework,

These functions can be used interactively by a

many of the latest developments in algorithms for

user typing to a Lisp interpreter or embedded in

evaluation of belief networks and influence diagrams.

code by other applications. To preserve portability,

In addition , it provides a complete environment for

IDEAL has only a simple character terminal based

creating, editing and saving belief networks and influ­

user interface. However, it provides hooks (or easy

ence diagrams. In the rest of the paper any reference

development of a graphic interface layered over it on

to 'diagrams' can be taken to refer to influence dia­

any specific platform. A graphic interface has been

grams and belief networks unless stated otherwise.

developed for the Symbolics environment.

213

I
I

3
3.1

Facilities in I DEAL
Data structures

IDEAL provides abstract data structures for rep­
resenting influence diagrams and belief networks.
These data structures and a tool kit of associated
functions provide all the basic low level functionali­
ties required for the creation of belief networks and
influence diagrams. This includes creation of directed
acyclic graph topologies, creation of probability ma­
trices and other matrices and vectors that are indexed
and sized by the states of the nodes in the graph, ac­
cessing these matrices and vectors, manipulation of
the graph topology, control constructs that allow easy
traversal of these node matrices, etc. These are low
level features that can be used by programmers to
develop functionalities that are not available directly
in IDEAL. A user who does not need any additional
functionalities can interact with IDEAL with higher
level functions described below.
3.2

Creating and Editing diagrams

The functions used to create and edit diagrams are
at a higher level than the functions that manipulate
the low level data structures. These functions expect
fully specified diagrams as input and return consis­
tent diagrams after they are done. Some of these
functions require interactive input from the user.
Functions to do the following are available: Cre­
ation of complete diagrams, adding arcs, deleting
arcs, adding nodes, deleting nodes, adding states to
a node, deleting states from a node and editing node
distributions. These functions make suitable assump­
tions that guarantee consistency of the diagram after
they are done. For example, adding an arc between
two nodes extends the distribution of the child node.
This extension of the distribution is done such that
the child node is independent of the new parent, i.e,
the child node has the same distribution given its
predecessors regardless of the state of the new node.
Most of these functions can be used embedded in
code to create diagrams on the fly. These functions
provide the right hooks into IDEAL for a user who is
interested primarily in the existing functionality and
does not need to go into the low level implementation
details.
3.3

Copying and Saving Diagrams

The copy function in IDEAL makes a complete copy
of a fully specified diagram. This is frequently useful
when one wants to make some transformation that
might destructively modify the diagram. The copy-

ing mechanism provides a means of keeping an un­
modified original in the Lisp environment.
IDEAL also has functions that allow the user to
save a diagram to file and to reload diagrams from
these saved files. IDEAL saves the diagram in text
files and so they can easily be exchanged between
users at remote sites or on different platforms by elec­
tronic mail or other means. The saving function can
be made to recognize any extensions that the user
may make to the abstract diagram data structures.
Thus, any custom information that a user may want
to associate with the diagram can also be saved and
retrieved.
3.4

Utility functions

IDEAL provides a wide variety of utility functions
that are of use in conjunction with belief networks
and influence diagrams. Consistency checking func­
tions for the following are available: To check whether
a diagram is consistent (i.e., it is acyclic, the proba­
bility distributions sum to 1, etc), to check whether
a diagram is acyclic (a lower level function), to check
whether a diagram has a strictly positive distribution
and to check whether a diagram is a belief network.
User interface utilities are available for display­
ing a description of the diagram in text format, for
easily accessing nodes in the diagram and for describ­
ing the contents of particular nodes of a diagram.
A set of utility functions is available for creat­
ing 'random' belief networks. This set of functions is
useful for creating examples for testing of belief net­
work algorithms and for quickly creating test belief
networks that satisfy certain user defined criteria (for
example, see [1]).
In addition to these there are miscellaneous util­
ity functions. Some examples: a function for sort­
ing the nodes in the diagram by graph order and
a function that modifies the distributions of a non­
strictly positive diagram slightly (as specified by an
argument) to make the distribution strictly positive.
3.5

Diagram transformations

This is a set of functions, each of which take a con­
sistent diagram as input and return a consistent di­
agram. These transformations are used in reduction
style algorithms [14,13]. They can also be used to
make changes in diagrams or to preprocess them be­
fore passing them to an inference scheme.
Some of the transformation functions are: Re­
moval of a particular barren node from a diagram,
Removal of all barren nodes from a diagram, absorb­
ing a chance node in a diagram, reversing an arc,

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

214

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

reducing a deterministic node etc. The transforma­
tion functions (as implemented) change the input di­
agram destructively to yield the result. Details of
these transformations can be found in [9,14].
3.6

Graphic Interface and documen­
tation

As mentioned before, IDEAL is designed to be a
portable tool and so it does not include any imple­
mentation specific graphics features. On the other
hand, hooks are available for in IDEAL for easily lay­
ering a graphics interface over it.
Such an interface has been developed for IDEAL
on Symbolics machines. In addition to standard
graphic manipulation commands this interface pro­
vides most of the functionalities described above ei­
ther through mouse driven graph manipulation (for
eg, reversing an arc) or through convenient menu
driven commands. The interface allows convenient
access to the Lisp environment in a separate window
and can be a very effective programming tool when
developing applications based on IDEAL .IDEAL and
the Symbolics interface to IDEAL are documented in
detail in [17].

same for all algorithms of the latter three classes. So,
if need be, the actual algorithm used can be a deci­

sion that is transparent to the end user or any calling
function which needs an inference mechanism whose
details are irrelevant.
4.1

Reduction algorithms

Influence diagram evaluation algorithms as described
by Shachter [14] and Rege and Agogino [13] are avail­
able. Inference algorithms applicable to both influ­
ence diagrams and belief networks are also available
as described in the same sources.
These algorithms operate by making a series of
transformations (see above) to the input diagram.
The input diagram is destructively modified.
4.2

Message passing algorithms

Message passing algorithms model each node as a pro­
cessor that communicate by means of messages. A
distributed algorithm from Pearl that applies to poly­
trees [10] is implemented in IDEAL. This implemen­
tation also utilizes work by Peat and Shachter [11). A
conditioning algorithm that works for all belief net­
works is also available. The conditioning algorithm
calculates cutset weights as described by Suermondt
and Cooper [18]. A variation of the conditioning a.l­
Algorithms in IDEAL
4
gorithm from Peot and Shachter [11] is also available.
The conditioning algorithms find cutsets as described
IDEAL provides many different evaluation and in­ by Suermondt and Cooper [19].
ference algorithms. The implementation emphasis is
on clarity rather than speed. Each of the algorithms
make extensive input checks and also explicitly de­ 4.3 Clustering algorithms
tects error conditions such as impossible evidence (see Clustering algorithms aggregate the nodes in a belief
Sec 5.2).
network into a join tree of 'meta' nodes and then run
The algorithms implemented in IDEAL fa]) into an update scheme on this tree. The updated beliefs
four classes- reduction algorithms (14,13], message for each of the belief network nodes is then calculated
passing algorithms (10,14), clustering algorithms [8,6) from the 'meta' nodes.
and simulation algorithms [10]. The algorithms in
IDEAL implements two variations of the ba­
each class are closely related to each other but differ sic clustering algorithm described by Lauritzen and
in complexity or are applicable to only specific kinds Spiegelhalter [8]. The first considers the join tree as a
of belief networks. Reduction algorithms are used for 'meta' belief network and runs a variation of the poly­
influence diagram evaluation (i.e., solving an influ­ tree algorithm [10] on it. The second variation uses
ence diagram for the optimal decision strategy) and an update scheme that operates on clique potentials
for inference. When used for inference they answer as described by Jensen et al [6].
specific queries, i.e, they give the updated belief of
Two methods are available for making the fill-in
a specific target node given a set of evidence nodes. for use in construction of the join tree - Maximum
The algorithms in the latter three classes ( as imple­ Cardinality Search [20] and a heuristic elimination
mented) can be used only for inference in belief net­ ordering heuristic from Jensen et. al. [7,12].
works. They give updated beliefs for all the nodes
in the network given evidence. The data structures
Simulation Algorithms
4.4
for declaring evidence before an algorithm is called
and the data structures where the updated beliefs are IDEAL implements a simulation algorithm from
found after the algorithm has finished running are the Pearl [10]. This implementation can only operate on

215

I
I

We have calibrated the estimates yielded by
these functions against actual time measurements of
how long it takes to solve the corresponding problems.
The correlations have been strong (see Sec 5.1).

belief networks with strictly positive distributions.
4.5

Estimator functions

IDEAL provides run time estimator functions for
some of the algorithms implemented in it. Given an
algorithm and a particular belief network with a par­
ticular state of evidence, the estimator function gives
a quick estimate of the complexity of the update pro­
cess.
In general, belief net inference algorithms con­
sist of two kinds of operations. The first kind are
graph operations that are polynomial in the number
of nodes in the graph (eg, triangulating a graph for
clustering, conversion of a multiply connected net­
work into a singly connected network by instanti.
atmg a cutset) 1 . The other class of operations are
the actual numerical calculations that are carried out
over the probability and potential matrices associ­
ated with the graphs. We will refer to this as the
update process. The overall exponential complexity
algorithm derives from the fact that these matrix op­
erations carried out during the update process take
exponential time. The estimator functions in IDEAL
give a quick estimate of the complexity of these ma­
trix operations.
The complexity count that is returned is a count
of the number of steps the algorithm will spend in
spanning the state spaces of the nodes or cliques in­
volved. For example, if a binary node A has a lone
binary node B as a predecessor then the complexity
count of setting the probability distribution of A is
four since one has to cover a state space of 2 x 2 states.
The complexity of normalizing the belief vector of A
is again 4 since one has to cover the state space of
the node A twice, once for summing the beliefs and
once for normalizing them.
An estimator function for a particular algorithm
takes an inference problem as input, i.e, a belief net­
work and associated evidence. The estimator per­
forms the polynomial time graph manipulations that
are necessary for initialization before the actual up­
date process can begin. It then applies embedded
knowledge of the update process to give an exact
count of the number of steps that the update pro­
cess will take. A step is defined as explained in the
previous paragraph. This estimate is made in linear
time. So overall, the estimator function runs in time
polynomial in the size of the input.
1 Here

we refer to the actual graph algorithm implemented

as against the algorithm which would give optimal results. For
example, the algorithm implemented in IDEAL for finding a

loop cutset for conditioning

runs

in polynomial time while the

p roblem of finding the minimal loop cutset is NP-hard ( see

for both results ) .

[3]

5

Discussion

IDEAL has been a success from the experimental
point of view. It has been used both for in-house ap­
plications and research both within and outside Rock­
well. Some examples of the uses of IDEAL include a
decision aiding model for pilots that helps to sort the
vast flow of information that comes to the cockpit
from the sensors on the plane, a life cycle costs anal­
ysis system for Rocket engines, embedded use in a
natural language system for story understanding [2]
and an implementation of interval influence diagrams
[4].
One of the lessons we learned in the process of
implementing IDEAL was that many of the algorithm
papers do not describe the algorithms in standard al­
gorithmic style. In addition they leave many details
incompletely specified. From an engineering point of
view, it would be very useful if we had both a more
complete description of algorithms and in a more con­
ventional style. IDEAL's emphasis on code readabil­
ity and explicitness were of great help in detecting
and correcting any problems that came up.
5.1

Estimator functions

As explained in the previous section, the estimator
functions carry out the polynomial time graph ma­
nipulations that precede the update process and then
give an estimate of the complexity of the update pro­
cess. The results of the graph manipulation are re­
quired to make the estimate. The actual estimate is
the result of applying a formula to the results of the
graph manipulation. These formulae were derived by
analysis of the update process of each algorithm. The
estimator functions in IDEAL apply only to exact al­
gorithms (as opposed to approximation algorithms).
As an example of an estimator function consider
the estimat�r for the Jensen method [6] of clustering.
.
G1v�n a behef network the complexity of initializing
.
the JOm tree by the Jensen method if given by:

L (3 + N(U))S(U)
UEJ

where U represents a Bayesian belief universe J
is the join tree made up of Bayesian belief univers�
N(U) is the number of neighbors of U in the joi�
tree and S(U) is the size of the joint state space of
the belief network nodes that are members of U.

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

216

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

Update phase of Jensen algorithm
300 .------.--.

This formula is easily derived as follows.

For

each belief universe the potential distribution has to
be set up by multiplying the distributions of the com­
ponent belief network nodes.

This has complexity

S(U).
When a belief universe absorbs from its neigh­
bors the complexity of the operation is

S(U).

When

it updates a neighboring sepset, again the complex­

ity of the operation is S(U). During the collect­
evidence operation, each universe absorbs from its
'child' neighbor sepsets and then updates its 'parent'
neighbor sepsets. Thus, for each universe the com­
plexity of the operation is

Iii

"C
c
0
u 200
Cll

!!.
Cll

�
c

2
iii
::I

100

0
<

2S(U).

During the dist.ribute-evidence operation, each
universe first absorbs from its 'parent' sepset neigh­
The complexity of the operation is

N(U)S(U)

1 &+5

5e+4

bor and then updates all the 'child' sepset neighbors.

Complexity Estimate (steps)

for

each universe U. Summing the terms for initializa­

Figure 1:

tion of the join tree, the collect-evidence operation

example

Performance of estimator functions:

2e+5

An

and the distribute-evidence operation gives the com­
plexity formula above.
An approximate formula that gives the complex­

once. This could be the case, for example, in a sys­

ity of the update process in the Jensen algorithm is:

tem that constructs belief networks dynamically and

L (2 + N(U))S(U) + L [S(Us) + S(i)]
iEB

UEJ

where

U,

is the smallest universe (in terms of

state space size) that contains node i of the network.
The update process consists of one collect
evidence-operation,

one distribute-evidence opera­

tion and a marginalization operation for setting the
belief vectors of the belief network nodes. These fac­
tors add up to the formula above. The formula does
not take into account the fact that some optimiza­
tion can be made based on the position of evidence in
the join tree. It also does not include the operations
needed to declare e vidence in the join tree. However,

uses each network only once. When the same network
is used repeatedly with different evidence pieces, the
clustering algorithms are superior. The construction
of the join tree can be considered as a compilation
step of the belief network ·that needs to be carried
·
out only once.
·

·

Though IDEAL is an experimental tool it gives
reasonable response times for medium size problems.
As an example, a

50

node network developed as part

of a decision aid system for aircraft pilots takes about

17 seconds to solve on a Symbolics 3645. IDEAL's
speed is limited both by the choice of implementation
language and its implementation style, where explicit
code rather than speed has been the top priority.

leaving out these terms does not introduce significant
error.
We have obtained excellent correlations between
the complexity estimates given by the estimator func­

5.2

Handling determinacy and inconsistency

tions for various algorithms and the actual run time.

In all the algorithms, gains can be made by explicitly

F ig 1 demonstrates the correlation for the update
phase of the Jensen algorithm. The data in the graph

done as a pre-processing step [15](in which case the

was collected by running tests on randomly created

network topology itself is modified) or, more gener­

belief networks.

ally, in the propagation phase of the algorithm.

detecting determinacy in the network. This can be

As expected, particular algorithms suit particu­

When the joint probability distribution of a be­

lar types of problems well. When choosing what algo­

lief net (i.e, the joint distribution of all the variables

rithm to use, in addition to the type or size of prob­

in the belief net) is not strictly positive it means that

lem, one needs to consider whether the belief network

some particular configuration of the belief net is im­

involved needs to be solved just once or solved mul­

possible.

tiple times with different evidence sets. Conditioning

of nodes of the belief net have non-strictly positive

algorithms are competitive (though not necessarily

joint distributions, i.e., the unconditional probability

faster) when the problems needs to be solved only

of some joi�t state of the subset is zero. The actual

This in turn implies that some subset(s)

217

I
I

makeup of these subsets depends on the conditional

conditional distribution in which the condition­

independencies in the network.
Let the network I (or some subset of nodes of

ing node set consists of some belief net nodes

the network) have an impossible state I= i. Then,
obviously, any conditional probability distribution

P(X/I

=

i) where X

is another subset of nodes of

the network cannot be assigned meaningfully. If an
implementation of any probabilistic inference algo­
rithm does not account for such circumstances, this
leads to a divide by zero error if the implementa­

P(X/ I = i).
P(X/I = i) as

tion tries to calculate the distribution
This occurs either when calculating

P(X, I= i) / P(I= i) or when normalizing the repre­
sentation of P(X/ I= i), say R(X/I= i) for all states
x of X where each R(X = x/I = i) has been found
to be zero. Note that the representation is inconsis­
tent and cannot represent a conditional probability
distribution that sums to

1.

An impossible state can occur due to two things:
1. Inconsistent Evidence: The evidence that the
user has declared may be inconsistent with the

Reduction algorithms

5.2.1

In reduction algorithms a divide by zero error can
occur when we try and find new conditional distri­
butions. This happens only during arc reversal and
node absorption.

In inference algorithms node ab­

sorption is just a special case of arc reversal and so
we need to look only at arc reversal.
When performing arc reversal to find a new dis­
tribution
and

B

P(A/B

==

b) where

A

is a single node

is a set of nodes the basic method is to

marginalize

P(A, B = b)

and then normalize it us­

ing the marginal. We hit a divide by zero error if
the marginal

a case IDEAL
tribution.

b) happens to be zero . In such
makes P(A/B = b) a uniform dis­

P( B

==

This is justified because any subsequent

manipulation of the distribution P(A/ B = b) by a re­
duction algorithm always involves multiplying it into

P(B =b) first. We know that P(B =b) is zero and
P(A/B =b) can be anything. The advantage of

belief net. Let us say that the probabilities en­
coded in the belief net are such that for a subset

so

of nodes A of the belief net P(A = a) is zero
where a is some joint state of the nodes A. If

consistent (i.e., the numbers still constitute a valid

the evidence we declare happens to be exactly

probability distribution) even after the tr;insforma­

a

or some superset of it (i.e

some nodes outside

a

plus evidence for

A) then obviously we will hit

a divide by zero error when performing inference
to find some distribution P(B/A =a) where

2.

which are not evidence nodes.

B is

this uniform assignment is that the diagram remains

tion.

The disadvantage is that if the user's query

to the system was

P(A/ B =b)

and

P(B =b)

hap­

pens to be zero for some state of B then the user
will not realize it and may ascribe some meaning to

some other set of nodes in the belief net. This is

the distribution

because the distribution we are seeking is hypo­

meaning. Note that this effectively amounts to out­

thetical, unassignable or meaningless, depending

putting garbage when the evidence is impossible (the

on how we look at the problem.

evidence being that particular state b of

Nature of algorithm: An impossible state may
also be caused by the nature of the inference al­
gorithm.

Consider the conditioning algorithm,

for example. It performs whatever inference we
are interested in conditioned on every possible

5.2.2

P(A/ B = b)

even though it has no

B).

Message passing algoritluns

The polytree algorithm,

as

implemented in IDEAL,

cannot hit the divide by zero error during the prop­
agation phase since it calculates only joint probabil­

joint state of a set of cutset nodes which make

ities. However, when normalizing the beliefs of each

the belief net singly connected. The results ob­

belief network node after the propagation is done, it

tained from each of these conditionings are then
'weighted' to get the results. Thus if the cutset

is possible to find that the marginal is zero. This di­
rectly implies that the evidence declared before the

is

propagation is impossible (i.e.,

A

and the evidence is

node(s) is

B then

we find

E = e and the target
P(B/A =a,E =e) for

a of A and then weight these
If P(A =a,E =e) is zero for some state

the marginal is nothing but

P(E = e) = O) since
P(E = e). IDEAL de­

all possible states

tects this situation explicitly and tells the user that

results.

the evidence is impossible.

a of A it is easy to see that we have an impossible

This conditioning algorithm makes the belief net

state which would lead to a divide by zero error
in general, an algorithm can hit an impossible

a poly tree by clamping the states of a cycle cutset
of nodes S. The evidence is propagated as by the
polytree algorithm for each of the evidence pieces and

situation (which cannot be attributed to incon­

then the result is weighted to get the beliefs of each

sistent evidence) if the algorithm calculates any

node given the evidence alone.

when calculating

P(B/A = a,E = e).

Thus,

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

218

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

IDEAL supports two conditioning implementa­

described in the original paper. After propagation, if

The first calculates cutset weights explic­

a zero marginal is encountered when normalizing the

In other words, for every node A we calcu­
P(A/S = s, E = e) and then use that to cal­
culate P( A/E = e) as the marginal of the prod­
uct P(A/S = s, E = e)P(S = sfE = e), where
P(S = s/E = e) is a 'mixing' probability. We will
hit the divide by zero error when P(S = s,E =e) is
zero and we try and calculate P(A/S= s, E =e).

beliefs this implies that the evidence was impossible.

tions.
itly.
late

In this implementation, a cutset conditioning
case

s

for which

P(S = s, E = e) = 0

contribute to the overall belief.

does not

So to avoid an er­

ror the cutset algorithm checks for the occurrence of

P(S = s, E = e) = 0

process that determines

during the recursive update

P(S = s/E= e).

If the con­

dition occurs then that cutset conditioning case

s

is

IDEAL signals the fact explicitly in both clustering
implementations.

5.2.4

Simulation Algorithms

The simulation algorithm coded in IDEAL cannot
handle non-strictly positive belief networks. If such a
belief network is given as input the algorithm breaks
with an appropriate warning.

skipped. Other than being a graceful technique to de­
tect an impossible situation, this step, in conjunction
with Suermondt and Cooper's

[19]

technique for cal­

culating cutset weights, can lead to substantial com­
plexity gains since whole classes of impossible cutset
cases can be detected and skipped with very little

Further developments

6

effort. For example, if the cutset consists of three bi­
nary nodes A, B and C (in graph order (A, B, C)),
then knowing that P(A = t) = 0 immediately elimi­
nates 4 cutset cases, one for each state combination
of

B

and

C

in conjunction with

A= t.

In the second conditioning implementation

[11]

no conditional probabilities are calculated during the
propagation phase and so no divide by zero errors are
possible. However, it is possible that when marginal­
izing the belief vectors of the nodes after the propa­
gation, the marginals are zero. This implies that the
evidence that has been propagated is impossible (see
previous subsection).

IDEAL detects this situation

explicitly in both conditioning implementations.

We foresee more work on developing efficient estima­
tor functions.

Each estimator function may be ex­

panded into a class of functions where one may trade
off the accuracy of the estimate with the time re­
quired to make the estimate. It may be p ossible to
use these estimator functions to help choose between
competing algorithms for a given problem or to use
them as a search function to search through a space
of competing alternative solutions.
IDEAL, has incorporated almost all the pub­
lished work to date on exact belief network and influ­
ence diagram algorithms. We will probably include
any promising new methods that come up (for exam­
ple, nested dissection

5.2.3

[3])

so that we can choose the

best possible method for the applications we have in

Clustering Algorithms

IDEAL supports two clustering algorithm implemen­
tations. The first implementation creates a join tree
of cliques and calculates the conditional probabili­
Consider a clique A with a
B. We hit the divide by zero er­
ror when P(B = b) is 0 and we try and calculate
P( A/ B = b). When creating the join tree we assign
P(A = afB = b) = 0 (we could assign anything, in
fact) for all states a of A when P(B =b)= 0. After

mind.
We will also be including some approximation
algorithms such as Likelihood weighting

[16].

ties in the join tree.

parent clique

7

Acknowledgements

the join tree is created the clustering algorithm uses a
variant of the polytree algorithm for evidence propa­

We would like to thank Robert Goldman for being

gation and so the divide by zero problem cannot come

an invaluable source of suggestions, bug reports and

up.
The second implementation from

[6]

handles a
divide by zero condition during the propagat.ion as

enhancements.

We would also like to thank Bruce

D'ambrosio, Keiji Kanazawa, Mark Peot and other
users of IDE�L for their suggestions and help.

219

I
I

