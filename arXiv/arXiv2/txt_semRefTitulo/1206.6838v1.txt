
A central task in many applications is reasoning about processes that change over continuous time. Recently, Nodelman et al. introduced
continuous time Bayesian networks (CTBNs), a
structured representation for representing Continuous Time Markov Processes over a structured
state space. In this paper, we introduce continuous time Markov networks (CTMNs), an alternative representation language that represents a
different type of continuous-time dynamics, particularly appropriate for modeling biological and
chemical systems. In this language, the dynamics of the process is described as an interplay
between two forces: the tendency of each entity to change its state, which we model using a
continuous-time proposal process that suggests
possible local changes to the state of the system
at different rates; and a global fitness or energy
function of the entire system, governing the probability that a proposed change is accepted, which
we capture by a Markov network that encodes
the fitness of different states. We show that the
fitness distribution is also the stationary distribution of the Markov process, so that this representation provides a characterization of a temporal
process whose stationary distribution has a compact graphical representation. We describe the
semantics of the representation, its basic properties, and how it compares to CTBNs. We also
provide an algorithm for learning such models
from data, and demonstrate its potential benefit
over other learning approaches.

1

Introduction

In many applications, we reason about processes that
evolve over time. Such processes can involve short time
scales (e.g., the dynamics of molecules) or very long ones
(e.g., evolution). In both examples, there is no obvious discrete “time unit” by which the process evolves. Rather, it is
more natural to view the process as changing in a continuous time: the system is in some state for a certain duration,

Raz Kupferman
Institute of Mathematics
The Hebrew University
raz@math.huji.ac.il

and then transitions to another state. The language of continuous time Markov processes (CTMPs) provides an elegant mathematical framework to reason about the probability of trajectories of such systems. Unfortunately, when we
consider a system with multiple components, this representation grows exponentially in the number of components.
Thus, we aim to construct a representation language for
CTMPs that can compactly encode natural processes over
high-dimensional state spaces. Importantly, the representation should also facilitate effective inference and learning.
Recently, Nodelman et al. [8, 9, 10, 11] introduced the
representation language of continuous time Bayesian networks (CTBNs), which provides a factorized, componentbased representation of CTMP: each component is characterized by a conditional CTMP dynamics, which describes
its local evolution as a function of the current state of its
parents in the network. This representation is natural for
describing systems with a sparse structure of local influences between components. Nodelman et al. provide algorithms for efficient approximate inference in CTBNs, and
for learning them from both complete and incomplete data.
In this paper, we introduce continuous time Markov networks, which have a different representational bias. Our
motivating example is modeling the evolution of biological
sequences such as proteins. In this example, the state of the
system at any given time is the sequence of amino acids encoded by the gene of interest. As evolution progresses, the
sequence is continually modified by local mutations that
change individual amino acids. The mutations for different
amino acids occur independently, but the probability that
these local mutations survive depends on global aspects of
the new sequence. For example, a mutation may be accepted only if the new sequence of amino acids folds properly into a functional protein, which occurs only if pairs of
amino acids that are in contact with each other in the folded
protein have complementary charges. Thus, although the
modifications are local, global constraints on the protein
structure and function introduce dependencies.
To capture such situations, we introduce a representation where we specify the dynamics of the process using
two components. The first is a proposal process that attempts to change individual components of the system. In
our example, this process will determine the rate of random

mutations in protein sequences. The second is an equilibrium distribution, which encodes preferences over global
configurations of the system. In our example, an approximation of the fitness of the folded protein. The equilibrium distribution is a static quantity that encodes preferences among states of the system, rather than dynamics of
changes. The actual dynamics of the system are determined
by the interplay between these two forces: local mutations
and global fitness. We represent the equilibrium distribution compactly using a Markov network, or, more generally, a feature-based log-linear model.
Importantly, as we shall see, the equilibrium distribution parameter is indeed the equilibrium distribution of
the process. Thus, our representation provides an explicit
representation of both the dynamics of the system and its
asymptotic limit. Moreover, this representation ensures
that the equilibrium distribution has a pre-specified simple structure. Thus, we can view our framework as a
continuous-time Markov network (CTMN) — a Markov
network that evolves over continuous time. From a different perspective, our representation allows us to capture a
family of temporal processes whose stationary distribution
has a certain locality structure. Such processes occur often
in biological and physical systems. For example, recent results of Socolich et al. [13] suggest that pairwise Markov
networks can fairly accurately capture the fitness of protein
sequences.
We provide a reduction from CTMNs to CTBNs, allowing us to use CTBN algorithms [7, 11] to perform effective
approximate inference in CTMNs. More importantly, we
also provide a procedure for learning CTMN parameters
from data. This procedure allows us to estimate the stationary distribution from observations of the system’s dynamics. This is important in applications where the stationary
distribution provides insight about the domain of application. In the protein evolution example, the stationary distribution provides a description of the evolutionary forces
that shape the protein and thus gives important clues about
protein structure and function.

2

Reversible Continuous Time Markov
Processes

We now briefly summarize the relevant properties of continuous time Markov processes that will be needed below.
We refer the interested reader to Taylor and Karlin [14] and
Chung [2] for more thorough expositions. Suppose we have
a family of random variables {X(t) : t ≥ 0} where the
continuous index t denotes time. A joint distribution over
these random variables is a homogeneous continuous time
Markov process (CTMP) if it satisfies the Markov property

Pr(X(tk+1 )|X(tk ), . . . , X(t0 )) = Pr(X(tk+1 )|X(tk ))

for all tk+1 > tk > . . . > t0 , and time-homogeneity,
Pr(X(s + t) = y|X(s) = x) =
Pr(X(s0 + t) = y|X(s0 ) = x)
for all s, s0 and t > 0.
The dynamics of a CTMP are fully determined by the
Markov transition function,
px,y (t) = Pr(X(s + t) = y|X(s) = x),
where time-homogeneity implies that the right hand side
does not depend on s. Provided that the transition function
satisfies certain analytical properties (see [2]) the dynamics
are fully captured by a constant matrix Q — the rate, or
intensity matrix — whose entries qx,y are defined by
qx,y = lim
h↓0

px,y (h) − 1 {x = y}
,
h

(1)

where 1 {} is the indicator function which takes the value 1
when the condition in the argument holds and 0 otherwise.
The Markov process can also be viewed as a generative process: The process starts in some state x. After spending a
finite amount of time at x, it transitions, at a random time,
to a random state y 6= x. The transition times to the various states are exponentially distributed, with rate parameters qx,y . The diagonal elements of Q are set to ensure the
constraint that each row sums up to zero.
If the process satisfies certain conditions (reachability)
then the limit
π x = lim py,x (t)
t→∞

exists and is independent of the initial state y. That is, in
the long time limit, the probability of visiting state x is independent of the initial state at time 0. The distribution
π x is called the stationary distribution of the process. A
CTMP is called stationary if P (X(0) = x) = π x , that
is, if the initial state is sampled from the stationary distribution. A stationary CTMP is called reversible if for every
x, y, and t > 0
Pr(X(t) = y|X(0) = x) = Pr(X(0) = y|X(t) = x).
This condition implies that the process is statistically
equivalent to itself running backward in time. Reversibility is intrinsic to many physical systems where the microscopic dynamics are time-reversible. Reversibility can be
formulated as a property on the Markov transition function,
where for every x, y, and t > 0
π x px,y (t) = π y py,x (t).
This identity is known as the detailed balance condition.
To better understand the constraint, we can examine the
implications of reversibility on the rate matrix Q.

Proposition 2.1: A CTMP is reversible if and only if its rate
matrix can be expressed as
qx,y = π y sx,y ,
where sx,y are the entries of a symmetric matrix (that is,
sx,y = sy,x ).
In other words, in a reversible CTMP, the asymmetry in
transition rates can be interpreted as resulting entirely from
preferences of the stationary distribution.

3

Continuous Time Metropolis Processes

We start by considering a reformulation of reversible
CTMPs as a continuous time version of the Metropolis
sampling process. We view the process as an interplay between two factors. The first is an unbiased random process that attempts to transition between states of the system,
and the second is the tendency of the system to remain in
more probable states. This latter probability is taken to be
the stationary distribution of the process. The structure of
the process can be thought of as going through iterations
of proposed transitions that are either accepted or rejected,
similar to the Metropolis sampler [6].
To formally describe such a process, we need to describe these two components. The first is the unbiased proposal of transitions. These proposals occur at fixed rates.
We denote by rx,y the rate at which proposals to transition
x → y occur. This in effect defines a CTMP process with
rate matrix R. To ensure an unbiased proposal, we require
R to be symmetric. (The stationary distribution of a symmetric rate matrix is the uniform distribution.)
The second component is a decision whether to accept
or reject the proposed transition. The decision whether to
accept the transition x → y depends on the probability
ratio of these states at equilibrium. We assume that we are
given a target distribution, which should coincide with the
equilibrium distribution π. As we shall see, to reach the
target equilibrium distribution, the acceptance probability
should satisfy a simple condition. To make this precise, we
assume we have an acceptance function f that takes as an
argument the ratio π y /π x and returns the probability of
accepting transition x → y. This function should return a
value between 0 and 1, and satisfy the functional relation
 
1
f (z) = zf
.
(2)
z
Two functions that satisfy these conditions are
fMetropolis (z)

=

flogistic (z)

=

min(1, z)
1
.
1 + z1

The function fMetropolis is the standard one used in Metropolis sampling. The function flogistic is closely linked to logistic regression. It is continuously differentiable, which, as
we shall see, facilitates the subsequent analysis.

Formally, a continuous time Metropolis process is defined by a symmetric matrix R, a distribution π, and a
real-valued function f . The semantics of the process are
defined in a generative manner. Starting at an initial state
x, the system remains in the state until receiving a proposed
transition x → y with rate rx,y . This proposal is then accepted with probability f (π y /π x ). If it is accepted, the
system transitions to state y; otherwise it remains in state
x. This process is repeated indefinitely.
To formulate the statistical dynamics of the system,
consider a short time interval h. In this case, the probability
of a proposal of the transition x → y is roughly h · rx,y .
Since the proposed transition is accepted with probability
f (π y /π x ), we have:


πy
.
px,y (h) ≈ h · rx,y · f
πx
Plugging this into Eq. (1) we conclude that the off-diagonal
elements of Q are


πy
qx,y = rx,y · f
.
(3)
πx
Proposition 3.1: Consider a continuous time Metropolis
process defined as in Eq. (3). Then, this CTMP is reversible, and its stationary distribution is π.
Proof: The condition on f implies that




πy
1
πx
1
f
=
f
,
πy
πx
πx
πy
Thus, it follows that qx,y is of the form sx,y π y , i.e., that
the process is reversible. Moreover, it implies that the stationary distribution of the process is π.
The inverse result is also easy to obtain.
Proposition 3.2: Any reversible CTMP can be represented
as a continuous time Metropolis process.
Proof: According to Proposition 2.1 we can write qx,y =
π y sx,y for a symmetric matrix sx,y . Define
πy
rx,y = sx,y   ,
π
f πxy
 
π
so that qx,y = rx,y · f πxy . Together, sx,y = sy,x and
Eq. (2) imply that rx,y = ry,x . Thus, R is symmetric and
together with π defines a continuous time Metropolis process which is equivalent to the original reversible CTMP.
We conclude that continuous time Metropolis processes
are a general reparameterization of reversible CTMPs.

4

Continuous Time Markov Networks

We are interested in dealing with structured, multicomponent systems, whose state description can be viewed

as an assignment to some set of state variables X =
hX1 , X2 , . . . , Xn i, where each Xi assumes a finite set of
values. The main challenge is dealing with the large state
space (exponential in n). We aim to find succinct representations of the system’s dynamics within the framework
of continuous time Metropolis processes. We do so in two
stages, first dealing with the proposal rate matrix R, and
then with the equilibrium distribution π.
Our first assumption is that proposed transitions are local. Specifically, we require that, for x 6= y
 i
rxi ,yi
(xj = yj ) ∀j 6= i
rx,y =
(4)
0
otherwise
where Ri = {rxi i ,yi } are symmetric local transition rates
for Xi . Thus, we allow only one component to change at
a time and the proposal rates do not depend on the global
state of the system.
The second assumption concerns the structure of the
stationary distribution π. Log-linear models or Markov
networks provide a general framework to describe structured distributions. A log-linear model is described by a set
of features, each one encoding a local property of the system that involves few variables. For example, the function
1 {X1 = X2 } is a feature that only involves two variables.
A feature-based Markov network is defined by a vector of features, s = hs1 , . . . , sK i, where each feature sk
assigns a real number to the state of the system. We further assume that each feature sk is a function of a (usually
small) subset D k ⊆ X of variables. We use the notation x|Dk to denote the projection of x on the subset of
variables D k . Thus, sk is a function of x|Dk ; however,
for notational convenience, we sometimes use sk (x) as a
shorthand for sk (x|Dk ).
Based on a set of features, we define a distribution by
assigning different weights to each feature. These weights
represent the relative importance of each feature. We use
the notation θ = hθ1 , . . . , θK i ∈ IRK to denote the vector of weights or parameters. The equilibrium distribution
represented by s and θ takes the log-linear form
(
)
X
1
πx =
exp
θk · sk (x|Dk ) ,
(5)
Z(θ)
k

where the partition function Z(θ) is the normalizing factor.
The structure of the equilibrium distribution can be represented as an undirected graph G — the nodes of G represent the variables {X1 , . . . , Xn }. If Xi , Xj ∈ D k for
some k, then there is an edge between the corresponding
nodes. Thus, for every feature sk , the nodes that represent
the variables in D k form a clique in the graph G. We define
the Markov Blanket, NG (i), of the variable Xi as the set of
neighbors of Xi in the graph G [12].
Example 4.1 :
Consider a four-variable process
{X1 , X2 , X3 , X4 }, where each variable takes binary

X1

X1
X4

(a)

X4

X2
X3

(b)

X2
X3

Figure 1: (a) The Markov network structure for Example 4.1. (b) The corresponding CTBN structure.
values, with the following set of features:
s1 (X1 ) = 1 {X1 = 1}

s5 (X1 , X2 ) = 1 {X1 = X2 }

s2 (X2 ) = 1 {X2 = 1}

s6 (X2 , X3 ) = 1 {X2 = X3 }

s3 (X3 ) = 1 {X3 = 1}

s7 (X3 , X4 ) = 1 {X3 = X4 }

s4 (X4 ) = 1 {X4 = 1}

s8 (X1 , X4 ) = 1 {X1 = X4 }

Note that all these features involve at most two variables.
The corresponding graph structure is shown in Figure 1(a).
In this example N(1) = {X2 , X4 }, N(2) = {X1 , X3 },
N(3) = {X2 , X4 }, and N(4) = {X1 , X3 }.
We now take advantage of the structured representation
of both R and π to get a more succinct representation of
the rate matrix Q of the process. We exploit the facts that
π appears explicitly in the rate only as a ratio π y /π x , and
moreover that the proposal process includes only transitions that modify a single variable. Thus, we only examine
ratios where y and x agree on all variables but one. It is
straightforward to show that if x and y are two states that
are identical except for the value of Xi and ui = x|N(i) ,
then
π y /π x = gi (xi → yi |ui ),
where
gi (xi → yi |ui ) =
(
)
X
exp
θk [sk (yi , ui ) − sk (xi , ui )] .
k:Xi ∈D k

Note that, if Xi ∈ D k , then D k ⊆ N(i) ∪ {Xi }. Thus, the
function gi is well defined.
Thus, the acceptance probability of a change in Xi depends only on the state of variables in its Markov blanket.
This property is heavily used for Gibbs sampling in Markov
networks. Depending on the choice of features, these dependencies can be very sparse, or involve all the variables
in the process.
To summarize, assuming a local form for R and a loglinear form for π, we can further simplify the definition of
the rate matrix Q. If x and y are two states that differ only
in the i’th variable, then
qx,y = rxi i ,yi f (gi (xi → yi |ui )),

(6)

where ui = x|N(i) . All other off-diagonal entries are 0,
and the diagonal entries are set to ensure that the sum of

each row is 0. We call a process with a Q matrix of the
form Eq. (6) a Continuous time Markov Network (CTMN).
One consequence of the form of the CTMN rate matrix
Eq. (6) is that the dynamics of the i’th variable depend directly only on the dynamics of its neighbors. As we can
expect, we can use this property to discuss independencies
among variables in the network. However, since we are
examining a continuous process, we need to consider independencies between full trajectories (see also [8]).
Theorem 4.2: Consider a CTMN with a stationary distribution represented by a graph G. If A, B, C are subsets
of X such that C separates A from B in G, then the trajectories of A and B are conditionally independent, given
observation of the full trajectory of C.
Proof: (sketch) Using the global independence properties
of a Markov network (see for example, [12]), we have that
π can be written as a product of two function each with its
own domain X 1 and X 2 such that X 1 ∩ X 2 = C and
A ⊆ X 1 and B ⊆ X 2 . Once the trajectories of variables in C are given, the dynamics of variables in X 1 − C
and X 2 − C are two independent CTMNs, each with its
own stationary distribution. As a consequence we get the
desired independence.
That is, the usual conditional separation criterion in
Markov networks [12] applies in a trajectory-wise fashion
to CTMNs.
It is important to note that although we can represent
any reversible CTMP as a continuous time Metropolis process, once we move to CTMNs this is no longer the case.
The main restriction is that, in CTMNs as we have defined them, each transition involves a change in the state
of exactly one component. Thus, although the language
of Markov networks allow to describe arbitrary equilibrium distributions (potentially with an exponential number
of features), the restrictions on R limit the range of processes we can describe as CTMNs. As an example of a
domain where CTMNs are not suitable, consider reasoning
about biochemical systems, where each component of the
state is the number of molecules of a particular species and
transitions correspond to chemical reactions. For example,
a reaction might be one that takes an OH molecule and
an H molecule and replace them by an H2 O molecule. If
reactions are reversible (i.e., H2 O can break into OH and
H molecules), then this process might be described by a
reversible CTMP. However, since reactions change several
components at once, we cannot describe such system as a
CTMN.

5

Connection to CTBNs

The factored form of Eq. (6) allows us to relate CTMNs
with CTBNs. A CTBN is defined by a directed (often
cyclic) graph whose nodes correspond to variables of the
process, and whose edges represent direct influences of
one variable on the evolution of another. More precisely,

a CTBN is defined by a collection of conditional rate matrices (also called conditional intensity matrices). For each
Xi , and for each possible value ui of its direct parents in
the CTBN graph, the matrix QXi |ui is a rate matrix over
the state space of Xi . These conditional rate matrices are
combined into a global rate matrix by a process Nodelman
et al. [9] call amalgamation. Briefly, if x and y are identical
except for the value of Xi , then
qx,y = qxXii,y|ui i

(7)

where ui = x|Pai is the assignment to Xi ’s parents in
the state x. That is, the rate of transition from x to y is
the conditional rate of Xi changing from xi to yi given the
state of its parents. Again, all other off-diagonal elements,
where more than one variable changes, are set to 0.
This form is similar to the rate matrix of CTMNs shown
in Eq. (6). Thus, given a CTMN, we can build an equivalent
CTBN by setting the parents of each Xi to be N(i), and
using the conditional rates:
qxXii,y|ui i = rxi i ,yi gi (xi → yi | x|N(i) )

(8)

Figure 1(b) shows the CTBN structures corresponding to
the CTMN of Example 4.1. In general, the CTBN graph
corresponding to a given CTMN is built by replacing each
undirected arc by a pair of directed ones. This matches the
intuition that if Xi and Xj appear in the context of some
feature, then they mutually influence each other.
As this transformation shows, the class of processes
that can be encoded using CTMNs is a subclass of CTBNs.
In a sense, this is not surprising, as a CTBN can encode any
Markov process where at most one variable can transition
at a time. However, the CTMN representation imposes a
particular parametrization of the system dynamics in terms
of the local proposal process and the global equilibrium
distribution. This parametrization violates both local and
global parameter independence [5] in the resulting CTBN.
In particular, a transition between xi and yi is proposed at
the same rate, regardless of whether it is globally advantageous (in terms of equilibrium preferences). As we shall
see, this property is important for our ability to effectively
estimate these rate parameters.
Moreover, as we have seen, this parametrization guarantees that the stationary distribution of the process factorizes as a particular Markov network. In general, even
a fairly sparse CTBN gives rise to a fully entangled stationary distribution that cannot be factorized. Indeed, even
computing the stationary distribution of a given CTBN is a
hard computational problem. By contrast, we have defined
a model of temporal dynamics that gives rise to a natural
and interpretable form for the stationary distribution. This
property is critical in applications where the stationary distribution is the key element in understanding the system.
Yet, the ability to transform a CTMN into a CTBN allows us to harness the recently developed approximate in-

ference methods for CTBNs [11, 7], including for the Estep used when learning CTMNs for partially observable
data.

6

Parameter Learning

We now consider the problem of learning the parametrization of CTMNs from data. Thus, we assume we are given
the form of π, that is, the set of features s, and need to
learn the parameters θ governing π and the local rate matrices Ri that govern the proposal rates for each variable.
We start by considering this problem in the context of complete data, where our observations consist of full trajectories of the system. As we show, we define a gradient ascent
procedure for learning the parameters from such data.
This result also enables us to learn from incomplete
data using the standard EM procedure. Namely, we can use
existing CTBNs inference algorithms to perform the E-step
effectively when learning from partially observable data to
compute expected sufficient statistics. The M-step is then
an application of the learning procedure for complete data
with these expected sufficient statistics. This combination
is quite standard and follows the lines of similar procedure
for CTBNs [10], and therefore we do not expand on it here.
6.1

(a)

The Likelihood Function

A key concept in addressing the learning problem is the
likelihood function, which determines how the probability
of the observations depends on the parameters.
We assume that the data is complete, and thus our observations consist of a trajectory of the system that can be
described as a sequence of intervals, where in each interval
the system is in one state. Using the relationship to CTBNs,
we can use the results of Nodelman et al. [9] to write the
probability of the data as a function of sufficient statistics
and entries in the conditional rate matrices of Eq. (8). A
problem with this approach is that the entries in the conditional rate matrix involve both parameters from Ri and
parameters from θ. Thus, the resulting likelihood function
couples the estimation of these two sets of parameters.
However, if we had additional information, we could
decouple these two sets of parameters. Suppose we observe
not only the actual trajectories, but also the rejected proposals; see Figure 2. With this additional information, we can
estimate the rate of different proposals, independently of
whether they were accepted or not. Similarly, we can estimate the equilibrium distribution from the accepted and
rejected proposals. Thus, we are going to view our learning problem as a partial data problem where the annotation
of rejected proposals is the missing data.
To formalize these ideas, assume that our evidence is a
trajectory annotated with proposal attempts. We describe
such a trajectory using three vectors; see Figure 2. The first
vector, τ = hτ [1], . . . , τ [M + 1]i, represents the time intervals between consecutive proposals. Thus, the first pro-

(b)
τ[8]

x[8]

y[8]
1

2 3

4 5

6

7

8

9

10

11 12 13

14

15

Figure 2: An illustration of training data. (a) A complete
trajectory. The x-axis denotes time and the y-axis denotes
the state at each time. Filled circles denote transitions. (b)
A trajectory annotated with accepted and rejected proposals (closed and open circles, respectively). (Remember that
accepted proposals lead to a transition.) The marks on the
x-axis denote the index of the proposal. We illustrate the
notation we use in the text, where τ [i] denotes the time interval before the i’th proposal, x[i] denote the actual state
after the i’th proposal, and y[i] denote the proposed state
in the i’th proposal.

posal took place at time τ [1], the second at time τ [1] + τ [2],
and so on. The last entry in this vector is the time between the last proposal and the end of the observed time
interval. The second vector, Ξ = hx[0], x[1], . . . , x[M ]i,
denotes the actual state of the system after each proposal
was made. Thus, x[0] is the initial state of the system,
x[1] is the state after the first proposal, and so on. Finally, Υ = hy[1], . . . , y[M ]i denotes the sequence of proposed states. Clearly, the m’th proposal was accepted if
y[m] = x[m] and rejected otherwise. We denote these
event using the indicators S[m] = 1 {x[m] = y[m]}.
The likelihood of these observations is the product of
the probability density of the duration between proposals,
and the probability of accepting or rejecting each proposal.
Plugging in the factored form of R and π we can write this
likelihood in a compact form.
Proposition 6.1: Given an augmented data set, τ , Ξ, and
Υ, the log-likelihood can be decomposed as
`(θ, {Ri } : τ , Ξ, Υ) =

n
X

`r,i (Ri : τ ) + `s (θ : Ξ, Υ),

i=1

such that
`r,i (Ri : τ ) =

X
xi 6=yi


M [xi , yi ] ln rxi i ,yi − rxi i ,yi T [xi ]

and
`s (θ : Ξ, Υ) =
n X X
X
i=1 ui xi 6=yi
n X X
X

M a [xi , yi |ui ] ln f (gi (xi → yi |ui )) +
M r [xi , yi |ui ] ln(1 − f (gi (xi → yi |ui )))

i=1 ui xi 6=yi

where M a [xi , yi |ui ] is the number of accepted transitions
of Xi from xi to yi when N(i) = ui , M r [xi , yi |ui ] is the
count of rejected proposals to make the same transition,
M [xi , yi |ui ] = M a [xi , yi |ui ]+M r [xi , yi |ui ], and T [xi ]
is the time spent in states where Xi = xi .
Note that if we use flogistic , then, as ln((1 + e−x )−1 )
is concave, the likelihood function `s (θ : Ξ, Υ) is concave
and has a unique maximum.
6.2

Maximizing the Likelihood Function

Under the Maximum Likelihood Principle, our estimated
parameters are the ones that maximize the likelihood function given the observations. We now examine how to maximize the likelihood. The decoupling of the likelihood into
several terms allows us to estimate each set of parameters
separately.
The estimation of Ri is straightforward: imposing the
symmetry condition, the maximum likelihood estimate is
rxi i ,yi =

M [xi , yi ] + M [yi , xi ]
.
T [xi ] + T [yi ]

Finding the maximum likelihood parameters of π is
somewhat more involved. Note that the likelihood `s (θ :
Ξ, Υ) is quite different from the likelihood of a log-linear
distribution given i.i.d. data [3]. The probability of acceptance or rejection involves ratios of probabilities. Therefore, the partition function Z(θ) cancels out, and does not
appear in the likelihood.
In a sense, our likelihood is closely related to the
pseudo-likelihood for log-linear models [1]. Recall that
pseudo-likelihood is a technique for estimating the parameters of a Markov network (or log-linear model) that uses
a different objective function. Rather than optimizing the
joint likelihood, one optimizes a sum of log conditional
likelihood terms, one for each variable given its neighbors.
By considering the conditional probability of a variable
given its neighbors, the partition function cancels out, allowing the parameters to be estimated without the use of inference. At the large sample limit, optimizing the pseudolikelihood criterion is equivalent to optimizing the joint
likelihood, but the results for finite sample sizes tend to
be worse. In our setting, the generative model is defined in
terms of ratios only. Thus, in this case the exact likelihood
turns out to take a form similar to the pseudo-likelihood
criterion. As for pseudo-likelihood, this form allows us to

perform parameter estimation without requiring inference
in the underlying Markov network.
In the absence of an analytical solution for this equation we learn the parameters using a gradient-based optimization procedure to find a (local) maximum of the likelihood. The derivation of the gradient is a standard exercise;
for completeness, we provide the details in the appendix.
When using flogistic we are guaranteed that such a procedure finds the unique global maximum.
6.3

Completing the Data

Our derivation of the likelihood and the associated optimization procedure relies on the assumption that rejected
transition attempts are also observed in the data. As we can
see from the form of the likelihood, these failures play an
important role in estimating the parameters. The question
is how to adapt the procedure to the case where rejected
proposals are not observed. Our solution to this problem is
to use Expectation Maximization, where we view the proposal attempts as the unobserved variables.
In this approach, we start with an initial guess of the
model parameters. We use these to estimate the expected
number of rejected proposals; we then treat these expected
counts as though they were real, and maximize the likelihood using the procedure described in the previous section.
We repeat these iterations until convergence.
The question is how to compute the expected number
of rejected attempts. It turns out that this computation can
be done analytically.
Proposition 6.2: Given a CTMN, and an observed trajectory τ , Ξ. Then,
IE [M r [xi , yi |ui ] |D]
=T

[xi |ui ] rxi i ,yi

(9)
(1 − f (g(xi , yi |ui )))

where T [xi |ui ] is the total amount of time the system was
in states where Xi = xi and N(i) = ui .
We see that, in this case, the E-step of EM is fairly
straightforward. The harder step is the M-step which requires an iterative gradient-based optimization procedure.
To summarize the procedure, to learn from complete
data we perform the following steps: We first collect sufficient statistics T [xi |ui ] and M a [xi , yi |ui ]. We then initialize the model with some set of parameters (randomly,
or using prior knowledge). We then iterate over the two
steps of EM until convergence: in the E-step, we complete the sufficient statistics with the expected number of
rejected attempts, as per Eq. (9); in the M-step, we perform
maximum likelihood estimation using the expected sufficient statistics, using gradient descent with the gradient of
Eq. (10).

7

A Numerical Example

To illustrate the properties of our CTMN learning procedure, we evaluated it on a small synthetic data set. We

Correct structure
0.14

0.14
CTMN
CTBN
Markov Network

0.12

0.1

0.08

(c)

0.06

KL distance

(b)

0.06

0.08
0.06

0.04

0.04

0.04

0.02

0.02

0.02

0

125

250
500
1000
Expected number of transitions

0

2000

CTMN
CTBN
Markov Network

0.12

0.1

0.08

KL distance

KL distance

0.1

(a)

0.14
CTMN
CTBN
Markov Network

0.12

125

250
500
1000
Expected number of transitions

0

2000

125

250
500
1000
Expected number of transitions

2000

Partial structure
CTMN
CTBN
Markov Network

0.25

(e)

0.2

0.15

(f)

KL distance

0.15

0.15

0.1

0.1

0.1

0.05

0.05

0.05

0

125

250
500
1000
Expected number of transitions

Equilibrium distribution

2000

0

CTMN
CTBN
Markov Network

0.25

0.2
KL distance

KL distance

0.2

(d)

CTMN
CTBN
Markov Network

0.25

125

250
500
1000
Expected number of transitions

2000

Uniform distribution (long)

0

125

250
500
1000
Expected number of transitions

2000

Uniform distribution (short)

Figure 3: Comparison of estimates of the equilibrium distribution by the CTMN learning procedure (solid lines), the CTBN
learning procedure (dashed lines) and a Markov Network parameter learning procedure applied to the frequency of time
spent in each state (dotted lines). The x-axis denotes the total length of training trajectories (measured in units of expected
number of observed transitions). The y-axis denotes the KL-divergence between the equilibrium distribution of the true
model and the estimated model. The curves report the median performance among 50 data sets, and the error bars report
25% − 75% percentiles. (a-c) report performance when learning with the true structure from which the data was generated,
and (d-f) report results when learning the parameters of a structure without the edges between X1 and X4 . In (a) and (d)
p(X(0)) is the equilibrium distribution. In (b) and (e) p(X(0)) is uniform and each trajectory is of length 25 time units.
In (c) and (f) p(X(0)) is uniform and each trajectory is of length 10 time units.
generated data from the CTMN model of Example 4.1
with θ = h−0.2, −2.3, 0.7, 0.7, −1.2, −1.2, −1.2, −1.2i
1
2
3
and proposal rates r0,1
= 1, r0,1
= 2, r0,1
= 3, and
4
r0,1 = 4.
The goal of our experiments is to test the ability of the
CTMN learning procedure to estimate stationary distributions from data in various conditions. As a benchmark, we
compared our procedure to two alternative methods:
• A procedure that estimates the stationary distribution
directly from the frequency of visiting each state. This
procedure is essentially the standard parameter learning method for Markov networks, where the weight of
each state (instance) is proportional to the duration in
which the process spends in that state. This procedure
uses gradient ascent to maximize the likelihood [3].
When the process is sampling from the stationary distribution, the relative time in each state is proportional
to its stationary probability, and in such situations we
expect this procedure to perform well.
• A procedure that estimates the Q-matrix of the associated CTBN shown in Figure. 1. Here we used the
methods designed for parameter learning of CTBNs
in [9]. Once that the Q-matrix has been estimated, the

estimated stationary distribution is the only normalized vector in its null space.
We examined these three procedures in three sets of
synthetic trajectories. The first set was generated by sampling the initial state X(0) of each trajectory from the stationary distribution and then sampling further states and
durations from the target model. In this data set the system is in equilibrium throughout the trajectory. The second
data set was generated by sampling the initial state from a
uniform distribution, and so the system starts in a distribution that is far from equilibrium. However, the trajectory is
long enough to let the system equilibrate. The third data set
is similar to the second, except that trajectories are shorter
and thus do not have sufficient time to equilibrate. To evaluate the effect of training set size, we repeated the learning
experiments with different numbers of trajectories. We report the size of the training set in terms of the total length of
training trajectories. Time is reported in units of expected
transition number. That is, one time unit is equal to the average time between transitions when the process is in equilibrium. The short and long trajectories in our experiments
are of length 10 and 25 expected transitions, respectively.
To evaluate the quality of the learned distribution, we

measured the Kullback-Leibler divergences from the true
stationary distribution to the estimated ones. Figures 3(ac) show the results of these experiments. When sampling
from the stationary distribution, the three procedures tend,
as the data size increases, toward the correct distribution.
For small data size, the performance of the CTMN learning procedure is consistently superior, although the error
bars partially overlap. We start seeing a difference between
the estimation procedures when we modify the initial distribution. As expected, the Markov network learning procedure suffers since it is learning from a biased sample. On
the other hand, the performance of the CTMN and CTBN
learning procedures is virtually unchanged, even when we
modify the length of the trajectories. These results illustrate
the ability of the CTMN and CTBN learning procedures to
robustly estimate the equilibrium distribution from the dynamics even when the sampled process is not at equilibrium.
To test the robustness to the network structure, we also
tested the performance of these procedures when estimating using a wrong structure. As we can see in Figures 3(df), while the three procedures converge to the wrong distribution, their relative behavior remains similar to the previous experiment, and the performance of the CTMN learning procedure is still not affected by the nature of the data.

8

Discussion and Future Work

In this paper, we define the framework of continuous time
Markov networks, where we model a dynamical system as
being governed by two factors: a local transition model,
and a global acceptance/rejection model (based on an equilibrium distribution). By using a Markov network (or
feature-based log-linear model) to encode the equilibrium
distribution, we naturally define a temporal process guaranteed to have an equilibrium distribution of a particular, factored form. We showed a reduction from CTMNs
to CTBNs that illustrates the differences in the expressive
powers of the two formalisms. Moreover, this reduction
allows us to reason in CTMNs by exploiting the efficient
approximate inference algorithms for CTBNs. Finally, we
provided learning algorithms for CTMNs, which allow us
to learn the equilibrium distribution in a way that exploits
our understanding about the system dynamics. We demonstrated on that this learning procedure is able to robustly estimate the equilibrium distribution even when the sampled
process is not at equilibrium. These results can be combined for learning from partial observations, by plugging
in the learning procedure as the M-step in the EM procedure for CTBNs [10].
This work opens many interesting questions. A key
goal in learning these models is to estimate the stationary
distribution. It is interesting to analyze, both theoretically
and empirically, the benefit gained in this task by accounting for the process dynamics, as compared to learning the
stationary distribution directly from a set of snapshots of

the system (e.g., a set of instances of a protein sequence
in different species). Moreover, so far, we have tackled
only the problem of parameter estimation in these models. In many applications, the model structure is unknown,
and of great interest. For example, in models of protein
evolution, we want to know which pair of positions in the
protein are directly correlated, and therefore likely to be
structurally interacting. Of course, tackling this problem
involves learning the structure of a Markov network, a notoriously difficult task. From the perspective of inference,
our reduction to CTBNs can lose much of the structure of
the model. For example, if the stationary distribution is
a pairwise Markov network, the fact that the interaction
model decomposes over pairs of variables is lost in the induced CTBN. It is interesting to see whether one can construct inference algorithms that better exploit this structure.
Finally, one important limitation of the CTMN framework
is the restriction to an exponential distribution on the duration between proposed state changes. Although such a
model is a reasonable one in many systems (e.g., biological sequence evolution), there are other settings where it is
too restrictive. In recent work, Nodelman et al. [10] show
how one can expand the framework of CTBNs to allow a
richer set of duration distributions. Essentially, their solution introduces a “hidden state” internal to a variable, so
that the overall transition model of the variable is actually
the aggregate of multiple transitions of its internal state. A
similar solution can be applied in our setting, but the resulting model would not generally encode a reversible CTMP.
One major potential field of application for this class of
models is sequence evolution. The current state of the art in
phylogenetic inference is based on continuous time probabilistic models of evolution [4]. Virtually all of these models assume that sequence positions evolve independently of
each other (although in some models, there are global parameters that induce weak dependencies). Our models provide a natural language for modeling such dependencies. In
this domain, the proposal process corresponds to mutation
rates within the sequence, and the equilibrium distribution
is proportional to the relative fitness of different sequences.
The latter function is of course very complex, but there is
empirical evidence that modeling pairwise interactions can
provide a good approximation [13]. Thus, in these systems,
both the local mutation process and a factored equilibrium
distribution are very appropriate, making CTMNs a potentially valuable tool for modeling and analysis. We hope to
incorporate this formalism within phylogenetic inference
tools, and to develop a methodology to leverage these models to provide new insights about the structure and function
of proteins.
Acknowledgments
We thank A. Jaimovich, T. Kaplan, M. Ninio, I. Wiener,
and the anonymous reviewers for comments on earlier versions of this manuscript. This work was supported by

grants from the Israel Science Foundation and the USIsrael Binational Science Foundation, and by DARPA’s
CALO program, under sub-contract to SRI International.

with a discontinuity when gi (xi → yi |ui ) = 1. We see
that, in this case, the updates are asymmetric, with maximal
weight to updates of accepted transitions.

A

