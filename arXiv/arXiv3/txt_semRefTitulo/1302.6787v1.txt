
We show how to find a small loop cutset in a
Bayesian network. Finding such a loop cutset
is the first step in the method of condition­
ing for inference. Our algorithm for finding
a loop cutset, called MGA, finds a loop cut­
set which is guaranteed in the worst case to
contain less than twice the number of vari­
ables contained in a minimum loop cutset.
We test MGA on randomly generated graphs
and find that the average ratio between the
number of instances associated with the algo­
rithms' output and the number of instances
associated with a minimum solution is 1. 22.

1

Introduction

Most inference algorithms for the computation of a
posterior probability in general Bayesian networks
have two conceptual phases. One phase handles op­
erations on the graphical structure itself and the other
performs probabilistic computations. For example, the
clique tree algorithm requires us to first find a "good"
clique tree and then perform probabilistic computa­
tions on the clique tree [LS88]. Pearl's method of con­
ditioning requires us first to find a "good" loop cutset
and then perform a c alcula tion for each loop cutset
[Pe86, Pe88]. F inally, Shachter's algorithm requires us
to find a "good" sequence of transformations and then,
for each transformation, to compute some conditional
probability tables [Sh86].
In the three algorithms just mentioned the first phase
is to find a good discrete structure, namely, a clique
tree, a cutset, or a sequence of transformations. The
goodness of the structure depends on a chosen param­
eter that, if selected appropriately, reduces the proba­
bilistic computations done in the second phase. Find­
ing a structure that optimizes the selected parameter
is usually NP-hard and thus heuristic methods are ap­
plied to find a reasonable structure. Most methods
in the past had no guarantee of performance and per­
formed very badly when presented with an appropriate

example. For example, the greedy algorithms of [St90]
and [SC90] for the method of conditioning may in the
worst case perform as bad as a factor of n /4 where n is
the number of variables in a Bayesian network. That
is to say, the size of their solution instead of being 2
variables may include as many as n/2 variables-a dis­
astrous outcome. Similar situations occur with other
inference algorithms.
However, recently, among other results, Bar-Yehuda
et al. (1994 ) have developed an algorithm that finds
a loop cutset that is guaranteed in the worst case to
contain less than 4 times the number of variables con­
tained by a minimum loop cutset. This guarantee is
given only when the number of values of every vari­
able in the network is the same. Note that this result
means that the number of instances associated with
a loop cutset F found by their algorithm (e.g., ,IF I if
the number of values of every variable is r) is no more
than the number of instances associated with a mini­
mum loop cutset raised to the forth power. Note also
that, the problem of finding a minimum loop cutset
was shown to be NP-hard in [SC90].
Our paper offers a new algorithm for finding a loop
cutset, called MGA, that finds a loop cutset which
is guaranteed in the worst case to contain less than
twice the number of variables contained in an optimal
loop cutset. That is, the number of instances associ­
ated with a loop cutset found by our algorithm is no
more than the number of instances associated with an
optimal loop cutset raised to the second power. The
complexity of MGA is O(m + nlogn) where� and
n are the number of edges and vertices respectively.
Unlike [BGNR94], our result holds even when the ar­
ities of the variables are arbitrary. Like [BGNR94},
our solution is based on a reduction to the Weighted
Vertex Feedback Set Problem, defined in the next sec­
tion. We should emphasize that all these performance
guarantees are for the worst case.
In Section 4 we test MGA on randomly generated
graphs and find that the average ratio between the
number of instances associated with the algonthms'
output and the number of instances associated with a
minimum solution is 1.2 2.

61

Approximation Algorithms for the Loop Cutset Problem

From a theoretical point of view, Bar-Yehuda et. a!.
( 1994) note that as the number of variables grows to
infinity the worst case ratio between the size of a loop
cutset found by any polynomial algorithm and the size
of an optimal loop cutset cannot be less than two un­
less the unlikely event that a similar result is obtained
for the weighted vertex cover problem (WVC/. Conse­
quently, we conjecture that no polynomial algorithm
for the loop cutset problem performs better in the
worst case than the algorithm presented in this paper
as graphs grow to infinity in size.
The rest of the paper is organized as follows. In
2 we outline the method of conditioning, ex­
plain the related loop cutset problem and describe
the reduction from the loop cutset problem to the
Weighted Vertex Feedback Set (WVFS) Problem. In
Section 3 we provide two approximation algorithms for
t he WVFS problem which is by itself an NP-Complete
problem [GJ79, pp. 191-192]. Finally, in Section 4 we
present experiments that test the average performance
of our algorithms.
Section

2

The Loop Cutset Problem

Pearl's method of conditioning is one of the known
inference methods for Bayesian networks. A short
overview of the method of conditioning and definitions
of Bayesian networks are needed. The reader is re­
ferred to [ Pe88) for more details.
Let P(u1, . . . , Un ) be a probability Ji:stribution where
each ui draws values from a finite set called the domain
of Ui- A directed graph D with no directed cycles
is called a Bayesian network of P if there is a 1-1
mapping between {u1, . . , un} and vertices in D, such
that u; is associated with vertex i and P can be written
as follows:
.

P(u1, . . . , un )

"

=IT P(u; I u;,,

.

.

.,

u;;,,)

(1 )

i=l

where i1, .. ., ij(i) are the source vertices of the incom­
edges to vertex i in D.

ing

Suppose now that some variables { v1, ... , v1} among
{u1, . . . , Un } are assigned specific values {v1, . .., vt}
respe ctivel y. The updating problem is to compute
the probability P(u; I v1
v1, . . . , v,
=
v, ) for
i 1, . . . , n.
=

=

A trail in a Bayesian network is a subgraph whose un­

derlying graph is a simple path. A vertex b is called a
sink with respect to a trail t if there exist two consec­
utive edges a -+ b and b r- c on t . A trail t is active
by a set of vertices Z if ( 1) every sink with r esp ect to
t either is in Z or has a descendant in Z and (2) every
other vertex along t is outside Z. Otherwise, the trail
is said to be blocked (d-separated) by Z.
1 The WVC problem is finding a set of vertices that con­
tains an endpoint of every edge in a given undirected graph

and which has

a.

minimum weight among all such sets.

Verma and Pearl [VP88] have proved that if D
is a Bayesian network of P(u1, ... , un) and all
trails between a vertex in { r1, .. . , r1} and a ver­
tex in {s1, ... ,sk } are blocked by {t1, ... ,tm}, then
the corresponding sets of variables { Ur1,
, Ur1 }
U3k } are independent conditioned on
and { u$1,
,
{Ut" . . ' Ut } Furthermore, Geiger and Pearl [G P90]
proved a converse to this theorem. Both results are
presented and extended in [GV P90].
•

.

.

•

m

.

•

•

.

Using the close relationship between blocked trails and
conditional independence, Kim and Pearl [K P83] de­
veloped an algorithm UPDATE-TREE that solves the
updating problem on Bayesian networks in which ev­
ery two vertices are connected with at most one trail
(singly-connected). Pearl then solved the updating
problem on any Bayesian network as follows [Pe86].
First, a set of vertices S is selected such that any two
vertices in the network are connected by at most one
active trail in S U Z, where Z is any subset of ver­
tices. Then, UPDATE-TREE is applied once for each
combination of value assignments to the variables cor­
responding to S, and, finally, the results are combined.
This algorithm is called the method of conditioning
and its complexity grows exponentially with the size
of S. The setS is called a loop cutset. Note that when
the domain size of the variables varies, th en UPDATE­
TREE is called a number of times equal to the product
of the domain sizes of the variables whose correspond­
ing vertices participate in the loop cutset. If we take
the logarithm of the domain size (number of values)
as the weight of a vertex, then finding a loop cutset
such that the sum of its vertices weights is minimum
optimizes Pearl's updating algorithm in the case where
the domain sizes may vary.
We now give an alternative definition for a loop cutset
S and then provide an approximation algorithm for
finding it. This definition is borrowed f rom [BGNR94].
The underlying graph G of a directed graph D is the
undirected graph formed by ignoring the directions of
the edges in D. A cycle in G is a path whose two
terminal vertices coincide. A loop in D is a subgraph
of D whose underlying graph is a cycle. A vertex v
is a sink with respect to a loop r if the two edges
adjacent to v in r are directed into v. Every loop
must contain at least one vertex that is not a sink
with respect to that loop. Each vertex that is not a
sink with respect to a loop r is called an allowed vertex
with respect to r. A loop cutset of a directed graph D
is a set of vertices that contains at least one allowed
vertex with respect to each loop in D. The weight of
a set of vertices X is denoted by w(X) and is equal to
L, x w(v) where w(x) = log(lxl) and lxl is the size
of tt e domain associated with vertex x. A minimum
loop cutset of a weighted directed graph D is a loop
cutset F• of D for which w(F*) is minimum over all
loop cutsets of G. The Loop Cutset Problem is defined
as finding a minimum loop cutset of a given weighted
directed graph D.
The approach we take is to reduce the weighted

loop

62

Becker and Geiger

cutset problem to the weighted vertex feedback set
problem, as done by [BGNR94]. We now define the
weighted vertex feedback set problem and then the re­
duction.
Let G = (V, E) be an undirected graph, and let w :
V --+ m+ be a weight function on the vertices of G. A
vertex feedback set of G is a subset of vertices F C V
such that each cycle in G passes through at least�ne
vertex in F. In other words, a vertex feedback set F
is a set of vertices of G such that by removing F from
G, along with all the edges incident with F, we obtain
a set of trees (i.e., a forest). The weight of a set of
vertices X is denoted (as before) by w(X) and is equal
to 2::v€X w(v). A minim·um vertex feedback set of a
weighted graph G with a weight function w is a vertex
feedback set F* of G for which w(F*) is minimum over
all vertex feedback sets of G. The Weighted Vertex
Feedb a ck Set (WVFS) Problem is defined as finding
a minimum vertex feedback set of a given weighted
graph G having a weight function w. Application of
this problem for constraint satisfaction is described in
[DP90].
In the next section we offer an algorithm, called MGA,
for approximately solving the weighted vertex feedback
set problem. The algorithm is guaranteed to output
a weighted vertex set whose weight is less than twice
the optimal weight.
The reduction is as follows. Given a weighted directed
graph (D, w ) (e.g., a Bayesian network), we define the
splitting weighted undirected graph D, with a weight
function w. as follows. Split each vertex v in D into
two vertices V;n and Vout in D, such that all incoming
edges to v in D become undirected incident edges with
V;n in Ds, and all outgoing edges from v in D become
undirected incident edges with Vout in D,. In addition,
connect V;0 and Vout in D, by an undirected edge. Now
set w,(v;n )
w ( v ) . For a set of
oo and w,(vout )
vertices X in D,, we define "1/l(X) as the set obtained by
replacing each vertex V;n or Vout in X by the respective
vertex v in D from which these vertices originated.
=

=

Our algorithm can now be easily stated.
Algorithm LC

Input: A Bayesian network D;
Output: A loop cutset of D;
1. Construct the splitting graph D.

with weight function

w,;

2. Apply MGA on (D5,w,) to obtain
a vertex feedback set F;
3. Output "1/J(F).

It is immediately seen that if MGA outputs a vertex
feedback set F whose weight is no more than twice the

weight of a minimum vertex feedback set of Ds, then
D with weight no more than
twice the weight of a minimum loop cutset of D. This
observation holds because there is an obvious one-to­
one and onto correspondence between loops in D and
cycles in D, and because MGA never chooses a vertex
that has an infinite weight.

"1/l(F) is a loop cutset of

3

Algorithms For The WVFS problem

Recall that the weighted vertex feedback set problem
is defined as finding a minimum vertex feedback set of
a given weighted graph G.
3.1

The Greedy Algorithm

We first analyze the simplest of all approximation algo­
rithms for the weighted vertex feedback set problem­
the greedy algorithm. Assume we are given a weighted
undirected graph G with a weight function w. The
greedy algorithm starts with G after removing all ver­
tices with degree 0 or 1 and repeatedly chooses to in­
sert a vertex v into the constructed vertex feedback
set if the ratio between v's weight w( v) and v's degree
d( v) in the current graph is minimal across all vertices
in the current graph. When v is selected, it is removed
from the current graph and then all vertices with de­
gree 0 or 1 are repeatedly removed as well. This step
is repeated until the graph is exhausted.
This algorithm and parts of its analysis are influenced
by the work of Chvatal (1979) who analyzed the greedy
algorithm for the Weighted Set Cover problem (WSC)
and by Lovisz (1975) and Johnson (1974) who ana­
lyzed the unweighted version of this problem.
ALGORITHM GA
Input: A weighted undirected graph
Output: A vertex feedback set F.

G(V, E, w).

F.-0
i .- 1

Repeatedly remove all vertices with
degree 0 or 1 from V and insert
the resulting graph into G;
While G; is not the empty graph do
1. Pick a vertex v; for which
w(v;)
d(v•) 1s mmtmum m G i
2. F +- F u { Vj}
3. V ..- V \ {v;}
4. i- i + 1
5. Repeatedly remove all vertices
with degree 0 or 1 from V
and insert the resulting
graph into G;
·

•

·

·

end.

In the rest of this section we prove that the perfor­
mance ratio of this greedy algorithm is bounded by
2(log d + 1) where d = m axv EVd( v) is the degree of the

Approximation Algorithms for the Loop Cutset Problem

graph. Recall that the performance ratio of an approx­
imation algorithm is the worst case ratio between the
weight of the algorithm's output and the weight of an
optimal solution. In Section 4, we show experimentally
that even this simple algorithm when combined with
the reduction algorithm LC convincingly outperforms
the algorithms given by [ SC90, St90].
Let F* be an optimal weighted feedback set of
G(V, E, w) and let Y V\F*. Note that the vertices
in F (the output ofGA) are denoted by {v1, v2, . . . , vt}
where v; are indexed in the order in which they are in­
serted into F by GA and where t = !FI- Let d;(v)
denote the degree of vertex v in G;-the graph gen­
erated in iteration i of GA-and let V; be the set of
vertices of G;. An edge is covered by the algorithm if
for some i = 1, . . . , t, one of its endpoints is v; and the
edge exists in G;. Let r1 ( v ) denote the set of edges in
G1 for which at least one endpoint is v. Note that the
set of vertex feedback sets of G and G1 is the same
and that the degree of every vertex in G1 is smaller or
equal to the degree of that vertex in G.
=

Let c; = w(vi)jd;(v;) and let C(e) = c; for every edge
e removed at iteration i.
Note that for every j ::=; i
we have w(vj )jdj (vj ) S w(vi )J dj ( v ; ) because vertices
are selected in decreasing order of these ratios. Also
note that for j S i, dj(v;) 2 d;(v;) since the algorithm
never adds edges. Thus,
(2)
Cj = w(vi)/di(vj) S w(v;)/d;(v;) = c;
for 1 S j SiSIFI, as originally claimed by [Ch79] in
the context of the W SC problem.
To analyze the performance ratio we use a lemma that
bounds the number of edges in G; covered by the al­
gorithm until its termination. We need the following
definitions. Let dx(v) be the number of edges whose
one endpoint is v and the other is a vertex in X. De­
note F;* = F* n V; and F'; = Y n V;. A linkpoint is
a vertex that has a degree 2 and A branchpoint is a
vertex that has a degree larger than 2. (A self-loop
adds 2 to the degree of a vertex ) .
Lemma 1
t

Ldi(vj)::; 2
j=i

L d;(v),

(3)

t

L(d;(v)- 2) +21FtIS2

(or equal) than the number of vertices, we have,
L:vEF: dr,(v)/2 :'::: IF'; I. Thus L:vEr,(d:p;(v)- 2) :S
0. Consequently, L:vEv.(d;(v)- 2) +21Ft I is less than
or equal to
L dp; (v) + L d;(v) :S 2 L

d;(v).

The proof of the first part of Eq. 4 is constructive.
We repeatedly apply the following procedure on G;
selecting in each step a vertex Vj E F; and showing
that there are terms in the right hand side (RHS) of
Eq. 4 that contribute dj (vi) to the RHS and have not
been used for any other v E F;. Set H = G; and for
k = i . . . t do as follows:
Pick the vertex Vk. If Vk is a linkpoint in H then
follow the two paths p1 and p2 in H emanating from
vk until the first branchpoint on each side is found.
There are three cases to consider. Either two distinct
branchpoints b1 and b2 are found, one branchpoint b1
(in which case p1 and p2 define a cycle) or none (if
the cycle is isolated). In the first case the two edges
on Pl and P2 whose endpoints are b1 and b2, respec­
tively, are associated with the terms d�r: (bl) - 2 > 0
and dk(b2)- 2 > 0 in the RHS and so each of these
terms contributes 1 to the sum L:vEV (d;(v)- 2). In
the second case, similarly, the two edges on p1 and
P2 whose endpoints is b1 are associated with the term
d�r:(bt)- 2 > 0 and so, if d�c(b1) > 3, this term con­
tributes 2 to the sum l:vEV;(d;(v) - 2). If d k ( bl ) = 3
we continue to follow the third path from b1 (i.e. , not
Pl or P2) until another branchpoint b2 is found and the
last edge on that path is associated with dk (b2 ) - 2
which contributes the extra missing 1 to the RHS. Fi­
nally, if no branchpoint is found, then on the cycle in
which Vk resides there must exist a vertex from F;*
that resides on no other cycles of H. Now, if Vk is
a branchpoint, then the term dk(vk)- 2 appears in
both sides of the inequality. In this case, sequentially
remove d�c(v�;:)- 2 of the d�c(v�c) edges adjacent to Vk
such that after each removal the vertices with degree 0
or 1 are removed from H as well. Thus, Vk remains a
linkpoint in which case the procedure for a linkpoint is
applied. Finally, remove Vk, and repeatedly remove all
the vertices with degree 0 or 1 from H. Repeat until
F; is exhausted. D
We now show that w(F)::; 2 (logd + 1) w(F*).

Proof: We will actually prove that,
L dj ( Vj ) S
j=i

63

·

·

L d;(v).

vEVi

w(F)

(4)
According to our notations , L:v EV (d;(v)- 2) equals
,
L (dF:(v)- 2) + L dp; (v) + L (d;(v)- 2).

Furthermore, the graph induced by F'; is a forest
and since the number of edges in a forest is smaller

t

=

L w(vi)
i=l

t

=

t

c1 L d;(v;)

i=l

Since

c;

2::

Ci-1,

w(F) S2c1

+

L c; d;(v;) =
i=1
·

t

t

i=2

j=i

L(c;- c;_l) L dj{Vj)

(5)

we can apply Eq. 3 and so,

L d1(v)

t

+

L2(c;- c;_l)

i=2

L d;(v)

=

64

Becker and Geiger

t

t-1

L 2 c; L di(v)- L 2c; L

We now present a modified greedy algorithm, called
MGA, whose performance ratio is bounded by the con­
stant 2. The changes we introduce into the greedy al­
gorithm are quite minor and so it is interesting that
such a vast improvement in the performance ratio is
obtained. A similar phenomenon is reported in the
context of the weighted vertex cover problem [Cl83].

Thus,
t

w(F) :S

L 2c;

t

L2c;
i=l

L

vEFi\Fi+1

d;(v)-

L

d;(v)+

t-1

L 2c;

MGA has two phases. In the first phase MGA repeat­

v

2(� (c;
L d; (v) +
i=l
vEFi\Fi+1
+ct

The Modified Greedy Algorithm

3.2

di+l(v)

c;

L (d;(v)- d;+I(v)))

vEFi+1

L dt(v))

vEF;"

However, since the last sum on the right hand side
merely counts the edge weights according to the iter­
ation they are assigned a weight, we get,

w(F) :S 2
Now, for every

L L

vEF" eEr1(v)

C(e)

(6)

·

2:

eEr1(v)

C(e),

(7)

where H (m) =
as shown in [Ch79] using the
following argument. Let s be the largest superscript
such that
> 0 then

2::::�1 1/i,

and the plain greedy algorithm is the revision of some
weights in each step instead of just revising the current
degrees. The second phase removes redundant vertices
ALGORITHM MGA

L

•

C(e)

=

l:(d;(v)- d;+l(v))
i=l

·

F'

eEr1(v)

weighted undirected graph G(V, E, w ) .

A

0

(w(vi)jd;(vi))

s

�t�S is minimum in G;

2. F' <--- F' U {vi}
3. V <--- V\ {vi }
4. i<--- i + 1

5.

"

w(v) L[H(d;(v))- H(di+1(v))].
i=l

Since the right hand side is equal to w(v) H(d( v)),
Eq. 7 follows. Combining Eqs. 6, and 7 yields,

Repeatedly remove all vertices with
degree 0 or
from V and their
adjacent edges from E and insert
the resulting graph into G;.

1

For every edge e = ( ut, u2 )
removed in this process do

·

w(F)::::;
Thus, since

d = 1),

2

L

vEF"

Theorem 2 The
by 2(Iog d + 1).

C(e)

H( d ( v )) · w(v) :S 2H(d) w( F*).

log d + 1 (equality holds only when

performance ratio of G A

is

w(u2)

end

F +-F1

�(�:$

<---

<---

1

w(ut)- C(e)
w(u2)- C(e)

For i
IFI to
do {Phase 2}
If every cycle in Gi that intersects
with {v; also intersects
with
\{
then,

bounded

We have an example in which the ratio between GA's
output and the optimal output is 2 log d. Our exam­
ple is similar to the example for the vertex cover prob­
lem given in [Mo92, pp. 47]. Consequently, the upper
b ound given in Theorem 2 is rather tight.

<---

w(u t )

·

H(d) ::::;

vertex feedback set F.

Repeatedly remove all vertices with degree 0
or 1 from V and their adjacent edges from
E and insert the resulting graph into G;.
While G; is not the empty graph do
1. Pick a vertex v; for which

where the inequality is due to Eq. 2. Furthermore, by
induction,

C(e)::::;

<---

i<--- 1

:S w(v) L(d;(v)- d;+t(v))jd;(v)

L

A

Input:

Output:

d,(v)

eEr1(v)

v

d(

from the constructed vertex feedback set.

v E F*,

H(d(v)) w(v) ;::

edly chooses to insert a vertex
into the constructed
vertex feedback set if the ratio between 's weight w(v)
v ) in the current graph is minimal
and v 's degree
across all vertices in the current graph. When v is se­
lected, it is removed from the current graph and then
all vertices with degr ee 0 or 1 are repeatedly removed
as well.
For every edge removed in this process, a.
weight of w(v)jd(v) is subtracted from its endpoint
vertices. These steps are repeated until the graph is
exhausted. The only difference between this phase

=

F

,__

endfor
end

}
F v;}
F \{vi}

65

Approximation Algorithms for the Loop Cutset Problem

Clearly F' computed at the first phase of MGA is a
vertex feedback set of G and F created from F' by
removing all redundant vertices is a mi n im a l vertex
feedback set of G, that is, if a vertex is removed from
F, then F ceases to be a vertex feedback set of G. Fur­
thermore, as a result of removing redundant vertices
the inequality 2::}=; dj(Vj) :S: 2 l:veF· d;(v) (Eq. 3),
proven to hold for the greedy algorith� becomes,

L d;(v) :S: 2 L d;(v),

(8)

veF;

vEFi

where F;" are the vertices in F that appear in graph
G;. The proof of this equation is postponed to Sec­
tion 3.3. From the description of the algorithm we
have for every vertex v in G1,

L

eer,(v)

C(e) ::; w(v)

(9)

and if v E F equality must hold. Eq. 9 replaces the in­
equality l::eer,(v) C(e) :S: H(d(v))·w(v) (Eq. 7) proven
for the greedy algorithm. By analogy with the previ­
ous section and using similar lines of reasoning, it is
clear that Eqs. 8 and 9 which replace Eqs. 3 and 7 show
that the bound on the performance ratio drops from
2 H(d) for the greedy algorithm to 2 for the modified
greedy algorithm.

Let a; ::: 1 if v; E F and a; ::: 0 if v; fl. F. That is, a; is
1 if v; is not removed from F in the final stage of MGA
and 0 otherwise. We now prove that w(F) ::; 2 w(F"').
·

t

w(F)

o; ·

·

d;(v;)

+

�

'i

·

l

(d;(v;)- d;+l(v;))

t

t

c1 I: a;· d1(v;) + L(c;- c;_t) Lai

i=l
Furthermore ,
j=i

·

·

d;(vi)

i=i

t

L ai

di(vj)

=

L d;(v) :S: 2 L

d;(v).

(12)

Since c; 2: ci-t, we can apply Eq. 12 and so, analo­
gously to the derivation of Eq. 6, we get,
w(F) :S
2ct

Theorem 3 Algorithm MGA always outputs a vertex
whose weight is no more than twice the

2

L

t

dt(v) + L2(c;- c;_l)

I: I:

L

di(v) :S:

C(e)

( 13)

of the optimal vertex feedback set.

As in Section 3.1, F* denotes a m1mmum
feedback set of G(V, E, w) and Y ::: V \ F*. Re ­
call that the vertices in the constructed set F' are
{ V t , v2, ..., Vt} where v; are indexed in the order in
which they are inserted into F by MG A and t ::: I F'l·
Also, w;(v) and d; (v) denote the weight and degree,
respectively, of vertex v in G;-the graph generated in
iteration i of Step 5 of MGA-and Vi denotes the set
of vertices of G;.
Proof.

As in the greedy algorithm, for every j ::; i
we have Wj(Vj)/dj(Vj) :S: Wj(v;)/dj(v;) and also
wi(v;)/d1(v;)::; w; (v;)jd;(v;) due to the way that the
current weights and degrees are updated in the algo­
rithm. Thus,
:=

Wj(Vj)/dj(Vj)::; W;(v;)/d;(v;)

for 1::; j ::;

i

:= Cj

(10)

:S: IF'l·

We also have,

L

[<;

w(F) is equal to

t

feedback set

Cj

Eq. ll,

which in turn equals to

·

weight

C(e)

i=l

Now, due to

t.

t

=I: a;· w(v;) =La; L

i-1

C(e) ::: c; d;(vi) + L:cj · (dj(v;)- dH1(v;))

Now, Eqs. 9 and 13 yield the claimed inequality,
w(F) :S: 2I:va• w(v)::: 2w( F*). 0
complexity of the first phase of MGA is O(IEI +
lVI log lVI) using a Fibonacci heap (e.g., [FT87]) be­
cause finding and deleting a vertex with minimum ra­
tio w(v)/d(v) from the heap is done lVI times at the
cost ofO(log lVI) and decreasing a weight from a ver­
tex in the heap is done lEI times at an amortized cost
ofO(l). The complexity of the second phase ofMGA
is also is O(IEI + lVI log lVI) using a simple implemen­
tation of the union-find algorithm because we need to
do at most lVI union operations at an amortized cost
of O(log lVI) and at most lEI find operations at the
cost of 0(1) [CLR90, pp. 445].

The

Interestingly, if the second phase is removed from
MGA (making MGA even closer to GA), then it can
be shown that the performance ratio becomes 4 rather
than 2. Hence the vast improvement in the worst­
case performance of MGA compared toGA stems from
changing the vertices' weights in each step rather than
from removing redundant vertices.

·

(ll)

because the right hand side simply groups edges ac­
cording to the iteration in which they are assigned a
weight.

3.3

A Theorem about Minimal Vertex
Feedback Sets

In this section we prove Eq. 8 which has been used
in the analysis of the modified greedy algorithm. Let

66

Becker and Geiger

be a weighted graph for which every vertex has a
degree strictly greater than 1, F be a minimal ver­
tex feedback set of G and F* be an arbitrary vertex
feedback set of G (possibly a minimum weight vertex
feedback set). Let d(v) be the degree of vertex v and
dx(v) be the number of edges whose one endpoint is
v and the other is in a set of vertices X.
G

4 Let G, F and F* be
LvEF d(v) :S 2 LvEP d(v).

Theorem
Then,

defined as above.

This theorem is interesting by its own sake since it
relates the number of edges adjacent to any minimal
weighted vertex feedback set to the number of edges
adjacent to any minimum weighted vertex feedback
set. Note that Ft is a minimal vertex feedback set of
Gi and therefore Theorem 4 proves Eq. 8.
To prove this theorem we divide l:vEF d(v) into the
sum 2IFI+ LvEF(d(v)-2) and provide an upper bound
for each term.
Lemma 5 Let

G, F

and F*

be defined as above.

Then,

2IFI::;
Proof:

G,

L

L d(v)- 2IF n F*l + 2IF n F*l
vEF

(14)

First note that for every set of vertices B in

d(v)-21FnYn BI-2I(FnY)\ BI (15)

vEF\B
However, the degree of every vertex in G satisfies

d(v)::::: 2 and therefore LveF\B d(v)::::: 2I(FnY)\ BI.
Consequently,

L d(v)-2jFnF*I::::: L

d(v)-21FnYnBI. (16)

Thus, and since IF n F*l::::: IF n F* n Bland dB( v) ::;
d(v), to prove the lemma it suffices to show that

2IF I::;

L dB(v)- 2IF n F n Bl + 2IF n F* n Bl,

or equivalently,

2IFI :S

L (dB(v)- 2) + 2IF" n Bl,

(17)
(18)

vEFnB

holds for some set of vertices B. We now define a set
B for which this inequality can be proven. Since F
is minimal, each vertex in F can be associated with a
cycle in G that contains no other vertices of F. We
define a graph H that consists of the union of these
cycles-one cycle per each vertex. Note that every
vertex in F is a linkpoint in H, i.e., a vertex with
degree 2. Let B be the vertices of H.

The proof of Eq. 18 is constructive. We repeatedly
apply the following procedure on H selecting in each
step a vertex v E F and showing that there are terms
in the right hand side (RHS) of Eq. 18 that contribute
2 to the RHS and have not been used for any other
v E F.
Set H' = H. Pick a vertex v E F and follow the
two paths Pt and p2 in H' emanating from v (which
is a linkpoint) until the first branchpoint on each side
is found. There are three cases to consider. Either
two distinct branchpoints bi and b2 are found, one
branchpoint bt (in which case Pi and p2 define a cy­
cle) or none (if the cycle is isolated). In the first
case the two edge�on Pi and P2 whose endpoints are
bt E F and b2 E F, respectively, are associated with
the terms dB(bi)- 2 > 0 and d B(b2 ) -2 > 0 in the
RHS and so each of these terms contributes 1 to the
sum L vEFnB ( dB(v) - 2). In the second case, simi­
larly, j_he two edges on Pi and p2 whose endpoints is
bt E F are associated with the term dB(b1)- 2 > 0
and so, if dB(bt) > 3, this term contributes 2 to the
sum LvEFnB(dB(v)- 2). If d B(bt) = 3 we continue
to follow the third path from bi (i.e., not Pi or P2)
until another branchpoint b2 E F is found and the last
edge on that path is associated with d8 ( b2) - 2 which
contributes the extra missing 1 to the RHS. Finally, if
no branchpoint is found, then on the cycle in which v
resides there must exist a vertex from F* that resides
on no other cycles of H'. Thus, the third case could
not occur more than IF* n Bl times. Now remove the
paths Pi and P2 from H' obtaining a graph in which
still each vertex in F resides on a cycle that contains
no other vertices of F. Continue the process until F
is exhausted. D
Lemma 6 Let G, F and F* be defined as above. Then
the sum LvEF(d(v)- 2) is upper bounded by,

L

dp•(v)+

L

(d(v)-2)-

L

(dr(v)-2)

vEFnF'

Proof:

First note that ,

L(d(v) - 2)

vEF

=

(dp- (v)- 2)+

L
vEFnF'

L dp(v) + L

vEFilF•

vEFnF•

(d(v)- 2) +

L (dp- (v) - 2)- L (dF'(v)- 2) (19)

vEFnF'

vEFn'F"

claim th at Lv EFnF' (dp- (v) - 2) +
LvEFnr(dr(v)-2) is less or equal than 0 and there­

We now

fore can be omitted from the inequality and conclude
this proof. The graph induced by F is a forest and
since the number of edges in a forest is smaller than the
number of vertices, we have, Z::vEF' dr(v)/2 :S IYI.
Thus l:vEF'" (dy• (v) - 2) :S 0 which is equivalent to
the stated claim. D

67

Approximation Algorithms for the Loop Cutset Problem

Using the bounds given by Lemmas 5 and 6 we have,

L d(v):::; L d(v)- 2 IF n Yl+
vEF
2IFnF*I+ L dF·(v)

vEF

vEFn'F"

+

L

(d(v)- 2) -

L (dr(v)- 2)

vEFnF•

2) + 2IF n F*l
LvEFnF• (d(v)
*
and
LvEFnF"(d:p·(v)-2)+21Fn'F I
LvEFnF• d(v)
LvEFnF" dF"(v). Thus, LvEF d(v) is bounded by

However,

-

L d(v) +

vEF

=

L d(v)vEFnF•
vEFnF"

Now, LvEFd(v)- LvEFnF" dF•(v) actually equals to
L'FnF" d(v) + LvEFnF" dp(v) and therefore

L d(v):::; L

vEF

dF·(v) + L d(v)::; 2 L d(v)

which concludes the proof of Theorem 4.

4

Experimental Results

Below we denote by A1 the algorithm described in
[SC90] and by A2 the algorithm described in [St90].
We performed six experiments. In the first two ex­
periments we tested how the outputs of the four al­
gorithms, A 1, A2, GA, and MGA, compare to a min­
imum loop cutset. In two additional experiments we
checked how the algorithms' outputs compare to each
other when given larger graphs for which a minimum
loop cutset is hard to obtain. In the above four ex­
periments we have chosen all variables to be binary.
The final two experiments compare the performance
of these algorithms when the number of values in each
vertex is randomly chosen between 2 and 6, 2 and 8,
and between 2 and 10. Each instance of the six exper­
iments is based on 100 graphs generated as described
by [SC90].
In the first experiment each of the 100 graphs gener­
ated had 15 vertices and 25 edges. MGA made only
one mistake producing 6 vertices instead of the mini­
mum of 5 vertices. G A made 4 mistakes each by one
vertex off. A2 made 7 mistakes one of which was two
vertices off the minimum and the other six mistakes
were one vertex off. A1 made 11 mistakes one of which
was 2 vertices off and the other 10 mistakes were one
vertex off. The minimum loop cutsets were between 3
and 6 vertices. Note that the ratio between the num­
ber of instances associated with a loop cutset found by
MGA in this experiment and the number of instances
associated with a minimum loop cutset is 1.002 which
is far less than the theoretical ratios guaranteed by

Theorem 4 for this experiment which lie between 8
when the minimum loop cutset contains 3 binary vari­
ables and 64 when the minimum loop cutset contains
6 binary variables.
In the second experiment we generated 100 networks
each with 25 vertices and 25 edges and tested how the
output of the four algorithms compare to a minimum
loop cutset when the graphs have a small number of
loops. This case is interesting because the conditioning
inference algorithm is most appropriate for these net­
works. MGA made no mistakes while the other three
algorithms made between 4 and 5 mistakes each by one
vertex (the minimum loop cutsets contained between
2 and 4 vertices).
Next we tested larger graphs. The first portion of the
table below compares between GA and A2 showing
that GA performs better than A2 in 53 of the 6 1 graphs
(87%) in which the algorithms disagree (out of 600
graphs tested). Each line in the table is based on 100
randomly generated graphs. The output columns show
the number of graphs for which the two algorithms had
an output of the same size and the number of graphs
each algorithm performed better than the other. Thus
even our simple greedy algorithm GA performs much
better than A2. The reason for this is the reduction
from the loop cutset problem to the weighted vertex
feedback set problem which allows the algorithm to se­
lect vertices that have parents while A2 unjustifiably
does not select such vertices (unless they have no pair
of parents residing on the same loop). Similar empir­
ical results and the same explanation applies to Al.
The second portion of the table shows that MGA per­
forms better than GA in 67 of the 75 graphs (89%) in
which the algorithms disagreed. Comparing MGA and
A2 in the same fashion (600 graphs) showed that MGA
performed better than A2 in 109 of the 116 graphs in
which the algorithms disagreed. Similarly, MGA per­
formed better than A1 in 135 of the 137 graphs in
which these algorithms disagreed.

lV I
25
25
25
55
55
55

lEI
25

A2

GA

75
55
75

1

0

8
15
2

50

105

0

1
4

2

8

1

10

17
53

Eq.
99
91
85

97

86
81
539

GA

MGA

Eq.

0

8
7

92
92

0

1

0
1
6

8

4
9

18

21

67

96

91

83
83
525

Finally, we repeated some of the experiments except
that now each vertex was associated with a random
number of values (between 2 and 6, 2 and 8, and 2
and 10). The results are summarized in the table be­
low. The two algorithms, A1 and MGA, output loop
cutsets of the same size in 55% of the graphs and when
the algorithms disagreed, then in 81% of these graphs
MGA performed better than Al. The ratio obtained
between the number of instances of the algorithms so­
lution and a minimum solution was 1.22 for MGA and

68

Becker and Geiger

1.44 for Al (using the 300 graphs in the table below
for which the number of vertices is 15 and number of
edges 25).

JVI
15
15
15
55
55
55

l Ei

25
25
25
105
105
105

values
2-6
2-8
2-10
2-6
2-8
2-10

Al
1
2
2
13
17
15
50

MGA
17
17
19
58
51
55
2 17

Eq.

82
81
79
29
32
30
333

To repeat this experiment with A2 required us to make
a small change in A2 because it is not designed to
run with vertices having different number of values.
We adopted the approach of A1 which selects vertices
(with at most one parent) according to their degree
and if there are several candidates the one with the
least number of values is selected for the loop cutset.
Combining this idea with the A2 algorithm defines an
algorithm we call the weighted A2 algorithm. The re­
sults obtained were that MGA performed better than
WA2 in 175 of the 224 graphs in which the algorithms
disagreed (out of 600). The ratio obtained between
the number of instances of the algorithms' solution
and a minimum solution was 1.22 for MGA and 1.33
for WA2.
Remark.

While this work was at its final stages of preparation
we became aware of a different method for the WVFS
problem that achieves a performance ratio of 2 [Be94].
A quick examination of our own work in light of this
information revealed that our method also achieves a
performance ratio of 2 .
Referen ces

[BGNR94] Bar-Yehuda R., Geiger D., Naor J . , and
Roth R. Approximation algorithms for the vertex
feedback set problems with applications to con­
straint satisfaction and Bayesian inference. In proc.
of the 5th A nnual A CM-Siam Symposium On Dis­
cre t e A lgorithms, Arlington, Virginia, January 1994.
[Be94] Berman P. Pennsylvania state university. Per­
sonal communication . February 1 994.
[Ch79] Chvatal V . A greedy heuristic for the set­
covering problem. Mathematics of operations re­
search, 4(3) , 1979, pp. 233-235.
[Cl83] Clarkson K.L. A modification of the greedy al­
gorithm for vertex cover, Inform ation Processing
Letters, 16 (1983), pp. 23-25.
[CLR90] Carmen T.H., Leiserson C.E . , and Rivest
R.L. Introduction t o algorithms, The MIT press,
London, England, 1990.

[DP90] Dechter R. and Pearl J . ,
schemes for constrain t processing:
learning, and cu tset decomposi tion,
telligence, 4 1 (1990), 273-312.

Enhancement
backjumping,

A rt ificial In-

[FT87] Fredman M. L . and Tarjan R.E . Fibonacci
heaps and their uses in improved network optimiza­
tion algorithms. Journal of the ACM, 34(3) , 1987,
pp. 596-6 15.
[GJ79] Garey M.R. and Johnson D.S., Computers
and Intractability: A Guide to the Theory of NP­
comp/eteness, W. H. Freeman, San Francisco, Cali­

fornia, 1 979.
[GP90] Geiger, D. and Pearl, J ., On the logic of causal
models, In Uncert ainty in A rtifi cial Intelligence 4,
Eds. Shachter R.D. , Levitt T.S. , Kana! L.N. , and
Lemmer J.F. , North-Holland, New York , 1990 , 314.
[GVP90] Geiger, D . , Verma, T.S. , and Pearl, J., Iden­
tifying in dependence in Bayesian networks,
works, 20 (1990), 507-534.

[KP83] Kim H. and Pearl J., A

Net­

comp u tational model

for combined causal an d diagnostic reasoning in in­

ference systems, In Proceedings of the Eighth IJCAI,
Morgan-Kaufmann, San Mateo, California, 1 983,
190-193.
[LS88] Lauritzen, S.L. and Spiegelhalter, D.J. Lo­
cal Computations with Probabilities on Graphical
Structures and Their Application to Expert Systems
(with discussion). Journal Royal Statistical Society,
B, 1988, 50(2) :157-224.
[Mo92] Motwani R.. Lecture notes on approximation
algorithms. Computer Science Department, Stan­
ford University, Report STAN-CS-92-1435.
[Pe88} Pearl, J ., Probabilistic reasoning in intelligent
systems: Networks of plausible infe re nce. Morgan
Kaufmann, San Mateo, California, 1988.
[Pe86] Pearl, J . , Fusion , propagation and structur­
ing in belief networks, A rt ificial Intelligence, 29:3
(1986), 241-288.
[Sh86] Shachter R.D . , Evaluating Influence Diagr ams.
Operations Research, 1986, 34 :871-882.
[SC90] Suermondt H.J. and Cooper G.F., Probabilis­
tic inference in m ultiply connected b elief networks
using loop cu tsets,

(1990) , 283-306.
[St90] Stillman, J . ,

Int. J. Approx. Reasoning,

4

On heuristics for fin ding loop c u t­

belief networks, In Pro­
ceedings of the Sixth Conference on Uncert ainty in
A rtificial Int ellige n ce, Cambridge, Massachusetts,

sets in mul tiply connected

1990 , 265-272.
[VP88] Verma, T. and Pearl, J. ,
Semantics and expressiveness,

Causal networks:
Proceedings of

In

Fourth Workshop on Uncertainty in Artificial Intel­
ligence, Minneapolis, Minnesota (published by the

Association for Uncertainty in Artificial Intelligence,
Mountain View, California) , 1988, 352�359.


