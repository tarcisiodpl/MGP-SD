

a must in real-world applications.
approximation methods (e.g.

This

paper

is

concerned

in stochastic domains by

with

planning

means of par­

tially observable Markov decision processes
(POMDPs). POMDPs are difficult to solve.
This paper identifies a subclass of POMDPs
called region observable POMDPs, which are
easier to solve and can be used to approxi­
mate general POMDPs to arbitrary accuracy.

Most previous

Cheng 1988, Lovejoy

1991b, and Parr and Russell1995) are value function
approximation methods in the sense that they approx­

imate optimal value functions of POMDPs directly.
We advocate model approximation methods.

Such a

method approximates a POMDP itself by another that
is easier to solve and uses the solution of the latter
to construct an approximate solution to the original
POMDP.
Model approximation can be in the form of a more
informative observation model, or a more deterministic

Keywords:

planning under uncertainty,

action model, or an aggregation of the state space,

partially observable Markov decision pro­

or a combination of two or all of them.

cesses, problem characteristics.

investigates the first alternative.

This paper

The idea of approximating a POMDP by assuming a

1

more informative observation model is not new. Cas­

INTRODUCTION

sandra et al (1996) have proposed to approximate

To plan is to find a policy that will lead an agent to
achieve a goal with minimum cost.

W hen the envi­

ronment of the agent, henceforth referred to as the
world, is completely observable and the effects of ac­
tions are deterministic, planning is reduced to finding
the shortest sequence of actions that leads the agent
to the goal.

POMDPs by using MDPs. This paper generalizes the
idea. We transform a POMDP by assuming that, in
addition to the observations obtained by itself, the
agent also receives a report from an oracle who knows
the true state of the world. The oracle does not report
the true state itself. Rather, he selects, from a list of
candidate regions, a region that contains the true state
and reports that region. The transformed POMDP is

In real-world applications, however, the world is rarely

said to be region observable because the agent knows

completely observable and effects of actions are almost

for sure that the true state is in region reported by the

always nondeterministic. For this reason, a growing

oracle.

number of researchers concern themselves with plan­
ning in stochastic domains (e.g.

Dean and Wellman

1991, Cassandra et al 1994 , Boutillier et al1995, Parr

and Russell 1995). Partially observable Markov deci­

sion processes (POMDPs) can be used as a model for
planning in such domains.

In this model, nondeter­

minism in effects of actions is encoded by transition
probabilities, partial observability of the world by ob­
servation probabilities, and goals and criteria for good
plans by reward functions.
POMDPs are difficult to solve and approximation is

W hen all candidate regions are singletons, the oracle
actually reports the true state of the world. In such
a case, the region observable POMDP reduces to an
MDP. MDPs are much easier to solve than POMDPs.
One would expect the region observable POMDP to
be solvable when all candidate regions are small.

In terms of quality of approximation, the larger the
candidate regions, the less extra information the ora­
cle provides and hence the more accurate the approx­
imation. In the extreme case when there is only one

473

An Approximation Scheme for Decision-Theoretic Planning

candidate region and it consists of all possible states of
the world, the oracle provides no extra information at

all. Hence the region observable POMDP is identical

to the original POMDP.
A way to determine the quality of approximation will
be described. This allows one to make the tradeoff be­
tween approximation quality and computational com­

plexity

as

follows: start with small candidate regions

and increase their sizes gradually until the approxima­

As a background example, consider path planning for
a robot who acts in an office environment. Here S
is the set of all location-orientation pairs, 0 is the
set of possible sensor readings, and A consists of
actions move-forward, tum-left, turn-right,
declare-goal.

and

The current observation o depends on the current state

of the world s. Due to sensor noise, this dependency is

uncertain in nature. The observation o sometimes also

tion becomes accurate enough or the region observable

depends on the action that the robot has just taken a_.

POMDP becomes untractable.

The minus sign in the subscript indicates the previous

In many applications, the agent often has a good idea
about the true state of the world.
planning

as an

example.

Take robot path

Observing a landmark,

a

room number for instance, would imply that the robot

is at the proximity of that landmark.

Observing a

feature about the world, a corridor T-junction for in­
stance, might imply the robot is in one of several re­
gions. Taking history into account, the robot might
be able to determine a unique region for its current
location. Also, an action usually moves the true state
of the world to only a few "nearby" states. Thus if
the robot has a good idea about the current state of
world, it should continue to have a good idea about it

in the next few steps.

time point.
of

o

upon

s

In the POMDP model, the dependency

and

a..

is numerically characterized by

P(ois, a.. ) , which
observation probability.

a conditional probability
referred to as the

is usually
It is the

observation model.

In a region observable POMDP, the current observa­
tion also depends on the previous state of the world
The observation probability for this case can be

8-.

P(o!s,a.., 8-).

written

s+ the world will be in after taking an ac­
a depends on the action and on the current state

The state
tion

8.

The plus sign in the subscript indicates the next

time point.

This dependency

is

again uncertain in

nature due to uncertainty in the actuator.

In the
a is

W hen the agent has a good idea about the true state

POMDP model, the dependency of 8+ upon 8 and

at all time, accurate approximation can be achieved

numerically characterized by a conditional probability

with small candidate regions.
We shall begin with a brief review of planning under
uncertainty and POMDPs. We shall then formally in­

P(s+l8, a), which is usually referred to
tion probability. It is the action model.

as the

transi­

We will often need to consider the joint conditional

P(s+, o+ is, a) of the next state of the world

troduce region observable POMDPs as an approxima­

probability

tion to general POMDPs.

and the next observation given the current state and

Thereafter, we shall de­

scribe a way to determine the quality of approxima­

the current action. It is given by

tion. Finally, we shall report empirical results, which
suggest that when there is not much uncertainty, a

P(8+,o+l8,a) = P(s+ls,a)P(o+l8+,a,s).

POMDP can be approximated accurately by a region
observable POMDP that has small candidate regions
and can hence be solved exactly.

The POMDP model encodes the starting state by a
probability mass function

Po over S. The planning
function such as the fol­

goal is encoded by a reward

2

lowing:

PLANNING UNDER
UNCERTAINTY AND POMDPs

r(s' a)=

To specify a planning problem, one needs to give a set

{1
0

if

a=d�lcare-goal and 8=goal,

otherwise.

(1)

DECISION MAKING IN POMDPs

S of possible states of the world, a set 0 of possible
observations, and a set A of possible actions. In this

3

paper, all those three sets are assumed to be finite.

The agent chooses and executes an action at each time

One needs also to give an observation model, which
describes the relationship between an observation and

edge about the true state of the world, which is sum­

the state of the world; and an action model, which

marized by a probability distribution over the set of

point. The choice is made based on the agent's knowl­

describes the effects of each action. Furthermore, one

possible states and called a

needs to specify the initial state of the world and

lief state

goal state.

a

and a

belief state.

The initial be­

b is the current belief state,
is the current action. H the observation o+ is
is P0• Suppose

474

Zhang and Liu

obtained at the next time point, then the next belief
state b+ is given by

b+(s+) = k LP(s+,o+ls,a)b(s),

(2)

8

where k=1/ La,s+ P(s+, o+ls, a)b(s) is the normaliza­
tion constant (Cassandra et al1994). To signify the
dependence of b+ upon b, a, and o+, we shall some-.
times write it as b+(.lb,a, o+)·
A policy 1r prescribes an action for each possible belief
state. Formally it is a mapping from the set B of all
possible belief states to A. For each belief state b,
1r(b) is the action prescribed by 1r for b. The value
function of 11" is defined for all belief states b by v11" (b) =
Eb(L:o ·lrt], where 0<-y<1 is the discount factor and
rt is the reward received at the tth step in the future.
Intuitively, it is the expected discounted reward the
agent can expect to receive starting from belief state
b if it behaves according to policy 1r. An policy 1r* is
optimal if v,..• (b);::: V,..(b) for all b and all other policies
1r.
The value function of an optimal policy is called
the optimal value function and is usually denoted by
v•.
Policies for POMDPs can be found through value it­
eration (Bellman 1957). Value iteration begins with
an arbitrary initial function "Y(t{b) and improves it by
using the following equation

Vt(b) = maxa[r(b,a) +-y LP(o + lb,a) Vt 1( b+)], (3)
__

0+

where P(o+ lb ,a) = Ls,s+ P(s+, o+ls,a)b(s), and b+ is
a shorthand for b+(-lb,a, o+)· If V0*=0, 'V;* is called
the t-step optimal value function.
It is well known that when the Bellman residual
maxbEBIV't*(b) - yt�1 ( b)l becomes small, l't* is close
to V* and the greedy policy based on vt*

1r(b) = arg maxa[r(b,a) + ')' LP(o+lb,a)l/t* (b+)] (4)
0+

is a good approximation of the optimal policy (e.g.
Puterman 1990).
Since there are uncountably infinite many belief states,
value iteration cannot to carried out explicitly. For­
tunately, it can be carried out implicitly due to the
piecewise linearity of the t-step optimal value function
(Sondik 1971). More specifically, there exists a list Vt
of function of s, usually referred to simply as vectors,
such that for any belief state

vt*(b) = maxvEV1 L V(s)b(s).

(5)

8

Exact methods for solving POMDPs (Monahan 1992,
Eagle 1984, and Larke 1991 (see W hite 1991), Sondik

1971, Cheng 1988,Cassandra et al 1994) attempt to
find a minimum list of vectors that satisfies the
above equation. Unfortunately, even the most effi­
cient algorithm can only solve POMDPs with no more
than twenty states and fifteen observations exactly
{Littman et al1995, Cassandra et al1997). Approxi­
mation is a must for real-world problems.
Most previous approximate methods (e.g. Cheng 1988,
Lovejoy 1991b, and Parr and Russell 1995) attempt to
find a list of vectors that satisfies equation (5) approxi­
mately. This paper proposes to approximate POMDPs
themselves by others that have more informative ob­
servations and hence are easier to solve.
4

PROBLEM CH ARACTERJSTICS
AND APPROXIMATIONS

We make the following assumption about problem
characteristics. Even though in a POMDP M the
agent does not know the true state of the world, he
often has a good idea about it. See the introduction
for justifications of this assumption.
Consider another POMDP M' which is the same as
M except that in addition to the observation made by
itself, the agent also receives a report from an oracle
who knows the true state of the world. The oracle does
not report the true state itself. Rather he selects, from
a list of candidate regions, a region that contains the
true state and report that region.
More information is available to the agent in M' than
in M; extra information is provided by the oracle.
When the agent already has a good idea about the true
state of the world, the oracle does not provides much
extra information even when the candidate regions are
small. In such a case, M' is a good approximation of
M.
In M', the agent knows for sure that the true state of

the world is in the region reported by the oracle. For
this reason, we say that it is region obseroable. The
region observable POMDP M' can be much easier to
solve than M when the candidate regions are small.
For example, if the oracle is allowed to report only sin­
gleton regions, then he actually reports the true state
of the world and hence M' is an MDP. MDPs are much
easier the solve than POMDPs.
We now set out to make the idea more concrete. Let
us begin with the concept of region systems.
4.1

A

Region Systems

region is simply
region system is a

a subset of states of the world. A
collection of regions such that no
region is a subset of other regions in the collection and

475

An Approximation Scheme for Decision-Theoretic Planning

the union of all regions equals the set of all possible

states of the world. We shall use R to denote a region
and n to denote a reg ion system. Region systems are

contain the true state of the world, one that supports
the function P(s, ols_,a.) of s to the maximum degree.
Where there is more than one such regions, choose

to be used to restrict the regions that the oracle can

the one that comes first in a predetermined orde ri ng

choo se to report .

among the regions.

There are many p o ssi ble ways to construct a region

Here are the intuitions.

system. A natural way is to create a region for each

state by in clud ing its "nearby" states.
this more precise.

Each action has an

Let us make

intended effect.

If the previous world st at e
a. were known to the agent, then his current belief
state b(8) would be proportional to P(s,ols.,aJ. In
this case, the rule minimizes extra information in the

The intended effect of move-forward, for instance, is

sense th at it supports the current belief state to the

to move one step forward. We say a sta te

maximum degree.

reachable in one step from another state

inform ative enough, being a landmark for instance, to

an

action whose intended effe<:t

8 is ideally
81 if there is

is, when the world

is

currently in state s', to take the world into state s.

A state

8k

is ideally reachable ink steps from another

state so if there are state

s1,

• • •

, BA:-1

such that si+l is

Also if the current observation is

ensure that the world state is in
regio n

a.

certain region, then

chosen using the rule fully supports the current

belief st ate. In such a case, no extra information is
provided.

ideally reachable from Si in on e step for all 0:5i:5k-1.
Any state is ideally reachable from itself in 0 step.

We do not claim that the rule described above is op­

For any non-neg ative integer k, the radius-k region

is still an open problem.

centered at a state

ally reachable from

s

8

consists of states that are ide­
in k or less steps.

A radius-k

region system is the one obtained by creating a radius­
k re gion for each state and then removing, one after
another, regions that are subsets of others.
W hen k is

0,

the radius-k region system consists of

singlet on regi on s.

On the

other hand, if

there is

timal. Finding a rule that minimize extra information
The

probability P(Ris, o, s_, a_)

P(Ris,o,s.,a.)

=

a k

such t hat any st ate is ideally reachable from any other

state in k or less steps, then there is only one region
in the radius-k region system , which is the set of all
possible states.
4.2

Region Observable POMDPs

the system. This subsection discusses how t he oracle
should choose regions from the system. The main issu e

minimizes the amount of extra information.
little

extra information as possible, the

oracle should consider what the agent alr eady knows.

However, he c annot take the entire history of past ac­

tions

and observations into ac count because if h e did,

M' would not be a POMDP. We suggest the foll ow ing
rule.

For

any

non- negative

any region R,

function f(s) of s and

we call t he quantity supp(f, R)=

EseR f(s)f'EseS f(s) the degree of support of f by
R. If R supp orts f to degree 1, we say that R fully
supports f.
Let s. b e the previous true state of the world,

11

being

sER
R'

if R is the first region s.t.
and for any other region

Es'eRP(s',ols.,a.):2:
Es'ER' P(s', ols., a.)
0 otherwise.

The region observable POMDP M' differs from the
original POMDP M o nly in terms of observation; in

addition to the observation o made by himself, the
an o bser vation in M' by z and
Observat ion model ofM' is given by

den ote

and t he oracle is al low ed to choose region only from

To provi de a.s

R

agent also receives a report R from the oracle. We shall

To complete the definition o£ the region observable
POMDP M', assume a region system has been given

is to

of a region

chosen under the above scheme is given by

P(zis,a_, s.)
4.3

=

P(o, Rls, a., 8. ) = P(ols, a.)P(Ris, o, s_, a_).

S olving Region Observable POMDPs

For any region R, let

8R be the set of belief states that

are fully supported by R. For any region system 'R,
let

BR-

=

UReR-Bn.

Let n be the region system underlying the region

servable POMDP M'.

be the

previous action, and o be the current observation. The
oracle should choose, among all the regions in n that

ob­

It is easy to see that no m atter

what the current belief state b is, the next belief s t at e
b+ must be in f3n. We assume that in M' the initial

belief state is in Bn. Then all possible belief states the
agent

Bn. This implies that poli­
8n and value
restricted to the subset 8 n of 8.

might have are

in

cies for M' need only be defined over

iteration for .M'

can

Restricting value iteration for M' to
a.

write z=(o,R).

8n

implies that

the t- s tep optimal value function Ui of M' is de­

fined only over

Bn and the
u;_l (b)J.

maxbEBR IU; (b)-

Bellman residual is now

Zhang and Liu

476

Like value iteration, restricted value iteration can be
Due to region observability, re­
stricted implicit value iteration in M' can be done
more efficiently than implicit value iteration in M. See
Zhang and Liu (1996) for details.

carried out implicitly.

Implicit restrict value iteration gives us a vectors,
which will be henceforth denoted by Ut. It repre­
sents the t-step optimal value function Ui(b) of M' in
the sense that Ut(b)=maxveu, L8 b(s)V(s) for any
bEBn. The greedy policy for M' based on Ut is as
follows: for any beBn

1r'( b)

=

arg max0[r(b,a) +1 LP(z+lb,a)Ui(b+)] , (6)
Z+

where z+ stands for observation of the next time
point a.od b+ is a shorthand for the next belief state
b+(-lb,a,z+)·
5

POLICY FOR THE ORIGINAL

POMDP

Suppose we have solved the region observable POMDP
M'. The next step is to construct a policy 1r for the
original POMDP M based on the solution forM'.
Even though it is our assumption that in the original
POMDP M the agent has a good idea about the state
of the world at all time, there is no guarantee that its
belief state will always be in B"R.· There is n o oracle in
M. A policy should prescribes actions for belief states
in Bn as well as for belief states outside BR.. An is­
sue here is that the policy 1r' for M' is defined only
for belief states in BR.. Fortunately, 1r1 can be natu­
rally extended to the entire belief space by ignoring
the constraint bEB"R. in equation (6). We hence define
an policy 1f forM as follows: for any bEB,
1r(b)

=

arg m axa[r(b, a)+ 1 L P(z+lb, a)Ui(b+)]. (7)
Z+

Let k be the radius of the region system underlying
M'. The policy 1r for M given above will be referred
to as the mdius-k approximate policy for M. The en­
tire process of obtaining the policy, including the con­
struction and solving of the region observable POMDP
M', will be referred to as region-based approximation.
It is wor t hwhile

to compare this equation with equa­
tion (4). In equation (4), there are two terms on the
right hand side. The first term is the immediate re­
ward for t aking action a and the second term is the
discounted future reward the agent can expect to re­
ceive if it behaves optimally. Their sum is the total
expected reward for taking action a. The action with
the highest total reward is chosen.

The second term is difficult to obtain. In essence,
equation (7) approximates the second term using the
optimal expected future reward the agent can receive
with the help of the oracle, which is easier to compute.
It should be emphasized that the presence of the oracle
is assumed only in the process of computing the radius­
k approximate policy. The oracle is not present when
executing the policy.
QUALITY OF APPROXIMATION

6

AND SIMULATION
In general, the quality of an approximate policy 1f is
measured by the distance between the optimal value
function V"'(b) and the value function V..-(b) of 1f. This
measurement does not consider what the agent might
know about the initial state of the world. As such, it is
not appropriate for a policy obtained through region­
based approximation. One cannot expect such a policy
be of good quality if the agent is very uncertain about
the initial state of the world because it is obtained
under the assumpti on that the agent has a good idea
about the state of the world at all time.

This section describes a scheme for determining the
quality of an approximate policy in cases where the
agent knows the initial state of the world with cer­
tainty. The scheme can be generalized to cases where
there is a small amount of uncertainty about the ini­
tial state; for example, cases where the initial state is
known to be in some small region.

The agent might need to reach the goal fro m dif­
ferent initial states at different times. Let P(s) be
the frequency it will start from state sl. The qual­
ity of an approximate policy 1r can be measured by
Ls IV*(s)- V��"(s)IP(s), where V*(s) and V��" denote
the rewards the agent can expect to receive starting
from state s if it behaves optimally or according to 1f
respectively.
By definition v• (s);?: V"��" (s) for all s. Let u• be
the optimal value function of the region observable
POMDP M'. Since more information is available
to the agent in M', U*(s);?:V•(s) for all s. There­
fore, 'E.[U • (s) - V��"(s)]P(s) is an upper bound on
L8[V*(s)- V��"(s)]P(s).
Let

1r1 be the policy for M' given by (6). When the
Bellman residual is small, 7r1 is close to optimal for M 1
and the value function v.,..' of 1f1 is close to u·. Con­
sequently, L:8[V��"' (s)- V��"(s)]P(s) is an upper bound
on L:,[V*(s)- v1r(s)]P(s) when the Bellman residual
is small enough.

1This is not to be confused with the initial belief state
Po.

477

An Approximation Scheme for Decision-Theoretic Planning

One

way to estimate the quantity

V1r(s)]P(s)

'Z:,[V1r' ( s)

-

is to conduct a large number of simula­
tion trials. In each trial, an initial state is randomly
generated according to P(s). The agent is informed of
the initial state. Simulation takes place in bothM and
M'. In M, the agent chooses, at each step, an action
using 1r based on the its current belief state. The ac­
tion is passed to a simulator which randomly generates
the next state of the world and the next observation
according to the transition and observation probabili­
ties. The observation (but not the state) is passed to

Environment A

the agent, who updates its belief state and chooses the
next action. And so on and

so

forth. The trial termi­

nates when the agent chooses the action declare-goal
or a maximum number of steps is reached. Simulation
inM 1 takes place in a similar manner except that the

C•cut.h)

bvira�Hnt B

observations and the observation probabilities are dif­
ferent and actions are chosen using 1r'.

Figure 1: Synthetic Office Environments.

H the goal is correctly declared at the end of a trial,
the agent receives a reward of the amount "Yn, where
n is the number of steps. Otherwise, the agent receive

no reward. The quantity

'E.,[V1r' (s)- v1r(s)]P(s)

can

be estimated using the difference between the average
reward received in the trials for M' and the average
reward received in the trials fo rM .

7

of region system and (2) where there is not much un­
certainty, a POMDP can be accurately approximated
by a region-observable POMDP that can be solved ex­
actly. This section reports on the experiments.

8.1

Synthetic Office Environments

TRADEOFF BETWEEN

Our experiments were carried using two synthetic

QUALITY OF APPROXIMATION

office environments borrowed from Cassandra et al

(1996)

AND COMPLEXITY
Intuitively, the larger the radius of the region system,
the less the amount of extra information the oracle pro­

with some minor modifications. Layouts of the

environments are shown in Figure 1, where squares
represent locations. Each location is represented as
four states in the POMDP model, one for each ori­

vides. Hence the closerM' is toM and the narrower
the gap between 'E. v1r' (s)P(s) and Es V7r(s)P(s).

entation. The dark locations are rooms connected to

pirical results (see the next section) do suggest that
Ls V,.-(s)P(s) increases with the radius of the region

goal location with the correct orientation.

Although we have not theoretically proved this, em­

system while

Ls v1r' (s)P(s)

the extreme case when there

decreases with it.

is

At

one region in the re­

gion system that contains all the possible states of
the world, M and M' are identical and hence so are

corridors by doorways.

In each environment, a robot needs to reach the

tions: move-forward, tum-left, tum-right, and
declare-goal. The two sets of action models given

Action

Those discussions lead to the following scheme for
making the tradeoff between complexity and quality.

move-forward

Start with the radius-0 region system and increases
the radius gradually until the quantity 'E .. [V,..' (s) -

tum-left

V,..(s)]P(s)

becomes sufficiently small or the region ob­

SIMULATION EXPERIMENTS

Simulation experiments have been carried out to show
that (1) quality of approximation increased with radius

Standard

Noisy outcomes

outcomes

tum-right
declare -:_g_oal

8

in

the following table were used.

E. v1r' (s)P(s) and E. V1r(s)P(s).

servable POMDP M' becomes untractable.

At each

step, the robot can execute one of the following ac­

N(O.ll), F(0.88),
F-F(0.01)
N(0.05), L(0.9),
L-L(0.05)
N(0.05), R(0.9),
R-R(0.05)
N_(l.O)

N(0.2), F(0.7),
F-F(0.1)
N(0.15), L (0 . 7) ,
L-L(O.l5)
N(0.15), R(O.7),
R-R(0.15)
N(LO}

For the action move-forward, the term

F-F (0.01)

means that with probability 0.01 the robot actually
moves two steps forward. The other terms are to be
interpreted similarly. H an outcome cannot occur in a

478

Zhang and Liu

certain

state of the

world, then the robot is left i n the

In each state, the robot is able to perceive

in each
of t hree nominal directions (front, left, and right)
whether there is a doorway, wall, open, or it is
undetermined. The following two sets of observation
mo dels were used:
Actual

Standard observations

case
wall

wall

(0.90),

undetermined
open

doorwa

8.2

(0.02)

(0.02),
open (0.90),
doorway (0.06),
undetermined (0.02)
wall (0.15),
open (0.15),
doorway (0.69),
undetermined (0.01)
wall

I

l
�

15
e

�

Noisy observations

·
····

1100
800
.

.g

I

"3
e
z

•
..

•

..

.rO-ot'acle"

···-··

"rff

-

'r1-<>n�cW•n" --

800
500

///

�··.r

20

1000
J/1.

..

/.

..

(0.70),
open (0.19),
doorway (0.09),
undetermined (0.02
wall (0.19),
open (0.70),
doorway (0.09),
undetermined (0.02
wall (0.15),
open (0.15),
doorway (0.69),
undetermined (0.01

..

/·

../

700

<100
15

wall

(0.04),
doorway (0.04),

open

Env�nmon\A

1000

last state before the impossible outcome.

25

Sleps
Envin>n"""'t B

35

30

llOO

800
700

•rO-oracle"' •••••.
•n-orac'-• .r1. -·-

800

'r(f"-

"

500
<100
10

15

20

Sleps

25

30

F igure 2: Experiments with standard action and noisy
models. The POMDPs are accurat el y approximated
by region observable POMDPs with radius zero or one.

Complexity of Solving the POMDPs

8.3

One of the POMDPs have 280 possible states while
the other has 200. They both have 64 possible ob­
servations and 4 possible actions. Since the l argest
POMDPs that researchers have been able to solve exactly so far have less than 20 states and 15 observations, it is safe to say no existing en.ct algorithms can
solve those two POMDPs.

Quality of Approximation for Standard
Models

To determine the quality of the radius-0 and radius-1
approximate policies for the POMDPs with s tand ard
action and observation models, 1000 simulation trials
were conducted using the scheme described in Section
6. It was assumed that the agent is equally likely to
We were be able to solve the radius-O and radius-1
start from any state. Instead of the average reward
approximations (region observable POMDPs) of the
over the trials, the performance of the agent is sumtwo POMDPs on a SUN SPARC2o computer. The
marized by the distribution of the numbers of steps it
threshold for the Bellman residual was set at 0.001
took to successfully complete the trials, i.e. by a funcand the discount factor at 0_99_ The amounts of time
tion g(n) of st eps n, where for each n, g(n) is the numit took in CPU seconds are collected in the following
her
of trials where the goal was reached and declared
table.
r-:::--..,---.-..,---=---:-...,...,=---r---:-::-:---�,-----. in n or less steps. The average reward over the tri­
t-;::----;o:--�-:;::-.....-;---,.--+-;;:;:--..--;<-..-.,---=-:-:-1 als can be computed by E�o In (g (n)-g(n-1)) /1000.
F.==�=:::;::::=l=
::::;:
::::;::=:===:l==
=:::==:===¥==::::;:====l We choose the function g(n) instead of the average re­
ward because it is more informative than the latter.

I

.:. __..
...:
;;.... L..._...:.:.::.
.;. _.J.._....;_;...;___J
..: -'-___;....;..:.._
---J...---.:.....:.L-.

We see that the radius-1 approximations took much
longer time to solve than the radius-0 approximations.
Also notice that the region observable POMDPs with
noisy action and observation models took more time
to solve that those with the standard models.
We were unable to solve the radius-2 approximations.
Other approximation techniques need to be incorpo­
rated in order to solve th e approximations based on
region sy stems with radius larger than or equal to 2.

Simulation results are shown in Figure 2. The curves
instance, represent the g-functions for
simulations in the radius-0 region observable POMDPs
(i.e. with the help of the oracle) using their opti­
mal policies. In contrast, the curves rO represent the
g-functions for simulations in the original POMDPs
(without the help of the oracle) using radius-0 approx­
imate policies. For readability, only top portions of the
g-functions are shown.
rO-oracle, for

We see that the gap between rO-oracle and rO is quite

An Approximation Scheme for Decision-Theoretic Planning

small in both cases. This indicates that the radius-0
region observable POMDPs (MDPs) are quite accu­
rate approximations of the original POMDPs. The

Environ..-! A

1000

...-··

1100

I

radius-0 approximate policies are close to optimal for

I

the original POMDPs.
The gaps between the curve'3 rl-oracle and rl are

'l5
E
"
z

even narrower. For environment A, there is essentially

no gap. Also n otice that the curves rl lie above rO

..

700
aoo

500
<100

creases with radius of region system.

,./
/
/
;

•ro-o,.c-.- -·� -·
·
"r1-onoc•·­
•n• -·­

20

80

'
..

300

!

"rfl'-

I

�
0

·

/
!

900

/

100

and the curves rl-oracle lie below rO-oracle. Those
support our claim that quality of approximation in­

479

..
0

100

1000
1100

There is a couple other facts worth mentioning. The

..

BOO

)

BOO

..e

gaps are larger in environment B than in environment
A. This is because environment B is more symmet­
ric and consequently observations a.re less effective in

700

�

8
0

disambiguating uncertainty in the agent's belief about

----.
"10-orac'-"
•r1-oi'IIC.. •n• -·­

�

"rfl'-

z

the state of the world.
There were a few failures in environment A even with

the presence of the oracle (curve rl-oracle). The
occurred due to uncertainties in the actions

failures

models: The agent was one step away from the goal

and had an very good idea about the state of the world.
An action towards the goal was taken and afterwards
the agent believed strongly that the world is in the
goal state. However, the action failed to effect any

Figure
models.

3:

Experiments with noisy action and noisy

The POMDPs are not accurately approxi­

mated by region observable POMDPs with radius zero
or one.

movement and the orcale's report did point this out2•
So a failure.

8.4

POMDPs exactly.

Quality of Approximation for Noisy
Models

Tracing through the trials, we learned some interesting
facts. In environment B, the agent, under the guidance
of the radius-1 approximate policy, was able to quickly

One thousand trials were also conducted for the

get to the neighborhood of the goal even when starting

POMDPs with noisy action and observation models.

from far way. The fact that the environment around

Results are shown in F igure

the g oal is highly symmetric was the cause of the poor
performance. Often the agent was not able to deter­

3.

We see that the gaps between rl-oracle and rl is sig­
nificantly narrower than the gaps between rO-oracle
and rO, especially for environment A. The curves rl
lie above the curves rO and the curves rl-oracle lie
below rO-oracle. Again, those support our claim that
quality of approximation increases with radius of re­

gion system.
As far as absolute quality of approximation is con­
cerned, the radius-0 POMDPs are obviously very poor
approximations of the original POMDPs since the gaps
gaps between the curves rO-oracle and rO are very
wide. For Environment A, the radius-1 approxima­
tion is fairly accurate. However, the radius-1 ap­
proximation remains poor for environment B. The ra­
dius of region system needs to be increased. Unfortu­
nately, increasing the radius beyond 1 renders it com­
putationally impossible to solve the region observable
2The oracle reported a region that contains both the

goal and the actual state.

mine whether it was at the goal location (room), or
in the opposite room, or in the left most room, or in

the room to the right of the goal location. The perfor­
mance would be close to optimal if the goal location
had some distinct features.

In environment A, the agent, again under the guidance
of the radius-! approximate policy, was able to reach
and declare the goal successfully once it got to the
neighborhood. However, it often took many unneces­
sarily steps before reaching the neighborhood due t o
the undesirable effects o f the turning actions.

Take

the lower left corner as an example. When the agent
reached the corner from above, it was facing down­
ward. The agent executed the action turn_left. Fif­
teen percent of the time, it ended up facing upward
instead of to the right - the desired direction. The
agent then decided to move-forward, thinking that it
was approaching the goaL But it was actually moving
upward and did not realize this until a few steps later.

Zhang and

480

Liu

The agent would perform much better there were in­
formative landmarks around the corners.
9

[6] II. T. Cheng (1988), Algorithms for partially ob­
servable Markov decision processes, PhD thesis,
University of British Columbia, Vancouver, BC,
Canada.

CONCLUSIONS

We propose to approximate a POMDP by using a
region observable POMDP. The region observable
POMDP has more informative observations and hence
is easier to solve. A method for determining the qual­
ity of approximation is also described, which allows
one to make the tradeoff between quality of approxi­
mation and computational complexity by starting with
a coarse approximation and refining it gradually. Sim­
ulation experiments have shown that when there is not
much uncertainty in the effects of actions and obser­
vations are informative, a P OMD P can be accurately
to approximated by a region observable POMDP that

be solved exactly. However, this becomes infeasi­
the degree of uncertainty increases. Other ap­
proximate methods need to be incorporated in order to
solve region observable POMDPs whose radiuses are
not small.

can

ble

as

Acknowledgement

Research was supported by Hong Kong Research
Council under grants HKUST 658/95E and Hong
Kong University of Science and Technology under
grant DAG96/97.EG01 (Rl).
