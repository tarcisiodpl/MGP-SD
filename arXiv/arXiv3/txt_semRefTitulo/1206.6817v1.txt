
We consider in this paper the formulation of
approximate inference in Bayesian networks
as a problem of exact inference on an approximate network that results from deleting edges
(to reduce treewidth). We have shown in earlier work that deleting edges calls for introducing auxiliary network parameters to compensate for lost dependencies, and proposed
intuitive conditions for determining these parameters. We have also shown that our earlier method corresponds to Iterative Belief
Propagation (IBP) when enough edges are
deleted to yield a polytree, and corresponds
to some generalizations of IBP when fewer
edges are deleted. In this paper, we propose a different criteria for determining auxiliary parameters based on optimizing the KL–
divergence between the original and approximate networks. We discuss the relationship
between the two methods for selecting parameters, shedding new light on IBP and its
generalizations. We also discuss the application of our new method to approximating
inference problems which are exponential in
constrained treewidth, including MAP and
nonmyopic value of information.

1

INTRODUCTION

The complexity of algorithms for exact inference on
Bayesian networks is generally exponential in the network treewidth (Jensen, Lauritzen, & Olesen, 1990;
Lauritzen & Spiegelhalter, 1988; Zhang & Poole, 1996;
Dechter, 1996; Darwiche, 2001). Therefore, networks
with high treewidth (and no local structure, Chavira &
Darwiche, 2005) can be inaccessible to these methods,
necessitating the use of approximate algorithms. Iterative Belief Propagation (IBP), also known as Loopy

Belief Propagation (Pearl, 1988; Murphy, Weiss, &
Jordan, 1999), is one such algorithm that has been critical for enabling certain classes of applications, which
have been intractable for exact algorithms (e.g., Frey
& MacKay, 1997). We have proposed in previous work
a new perspective on this influential algorithm, viewing it as an exact inference algorithm on a polytree approximation of the original network (Choi & Darwiche,
2006; Choi, Chan, & Darwiche, 2005). The approximate polytree results from deleting edges from the
original network, where the loss of each edge is offset
by introducing new parameters into the approximate
network. We have shown that the iterations of IBP can
be understood as searching for specific values of these
parameters that satisfy intuitive conditions that we
characterized formally (Choi & Darwiche, 2006). This
has led to a number of implications. On the theoretical
side, it provided a new, network–specific, characterization of the fixed points of IBP. On the practical side, it
has led to a concrete framework for improving approximations returned by IBP by deleting fewer edges than
those necessary to yield a polytree; that is, we delete
enough edges to obtain a multiply connected network
which is still tractable for exact inference.
In this paper, we consider another criterion for determining the auxiliary parameters introduced by deleting edges, which is based on minimizing the KL–
divergence between the original and approximate network. This proposal leads to a number of interesting
results. First, we provide intuitive, yet necessary and
sufficient, conditions that characterize the stationary
points of this optimization problem. These conditions
suggest an iterative procedure for finding parameters
that satisfy these conditions, leading to a new approximate inference method that parallels IBP and its generalizations. Second, the sufficiency of these conditions
lead to new results on IBP and its generalizations,
characterizing situations under which these algorithms
will indeed be optimizing the KL–divergence.
We seek to optimize the form of the KL–divergence

U

U

SE(U)

S’

PM(U’)

U’

X

X

Figure 1: Deleting edge U → X by adding a clone U 0
of U and a binary evidence variable S 0 .
that uses weights from the original distribution, and as
it turns out, the update equations for our new method
are more expensive than those for IBP and its generalizations, requiring the availability of true node marginals in the original network. This means that the
method as described is, in general, applicable only to
networks whose treewidth is manageable, but whose
constrained treewidth is not.1 That is, this approximation will typically be useful for problems which remain
hard even if treewidth is manageable. This includes
MAP (Park & Darwiche, 2004), inference in credal
networks (Cozman, de Campos, Ide, & da Rocha,
2004), and the computation of nonmyopic value of information (Krause & Guestrin, 2005). In complexity theoretic terms, computing node marginals is PP–
complete, while computing MAP is NPPP –complete.
Hence, our proposed method can be used to approximate NPPP –complete problems, while IBP and its generalizations approximate PP–complete problems.
This paper is structured as follows. Section 2 reviews the framework of approximating networks by
edge deletion. Section 3 treats the characterization of
auxiliary parameters introduced by deleting edges, discussing the new characterization proposed in this paper, and comparing it to the one corresponding to IBP
and its generalizations. Section 4 considers the problem of selecting which edges to delete in order to optimize the quality of approximations. Section 5 presents
empirical results, Section 6 discusses related work, and
Section 7 closes with some concluding remarks. Proofs
of theorems are sketched in Appendix A.

2

DELETING AN EDGE

Let U → X be an edge in a Bayesian network, and
suppose that we wish to delete this edge to make the
network more amenable to exact inference algorithms.
This deletion will introduce two problems. First, variable X will lose its direct dependence on parent U .
1

Networks may admit elimination orders with manageable treewidths, but certain queries may constrain these
orders, leading to constrained treewidths.

Second, variable U may lose evidential information received through its child X. To address these problems, we propose to add two auxiliary variables for
each deleted edge U → X as given in Figure 1. The
first is a variable U 0 which is made a parent of X,
therefore acting as a clone of the lost parent U . The
second is an instantiated variable S 0 which is made a
child of U , meant to provide evidence on U in lieu of
the lost evidence.2 Note that the states u0 of auxiliary
variable U 0 are the same as the states u of variable U ,
since U 0 is a clone of U . Moreover, auxiliary variable
S 0 is binary as it represents evidence.
The deletion of an edge U → X will then lead to introducing new parameters into the network, as we must
now provide conditional probability tables (CPTs) for
the new variables U 0 and S 0 . Variable U 0 , a root node
in the network, needs parameters θu0 representing the
prior marginal on variable U 0 . We will use PM (U 0 ) to
denote these parameters, where PM (u0 ) = θu0 . Variable S 0 , a leaf node in the network, needs parameters θs0 |u representing the conditional probability of
s0 given U . We will use SE (U ) to denote these parameters, where SE (u) = θs0 |u . Moreover, we will
collectively refer to PM (U 0 ) and SE (U ) as edge parameters. Figure 2 depicts a simple network with a
deleted edge, together with one possible assignment of
the corresponding edge parameters.
We have a number of observations about our proposal
for deleting edges:
• The extent to which this proposal is successful
will depend on the specific values used for the parameters introduced by deleting edges. This is a
topic which we address in the following section.
• If the deleted edge U → X splits the network into
two disconnected networks, one can always find
edge parameters which are guaranteed to lead to
exact computation of variable marginals in both
subnetworks (Choi & Darwiche, 2006).
• The auxiliary variable S 0 can be viewed as injecting a soft evidence on variable U , whose strength
is defined by the parameters SE (U ). Note that for
queries that are conditioned on evidence s0 , only
the relative ratios of parameters SE (U ) matter,
not their absolute values (Pearl, 1988; Chan &
Darwiche, 2005).
Our goal now is to answer the following two questions.
First, how do we parametrize deleted edges? Second,
which edges do we delete?
2
Our proposal for deleting an edge is an extension of
the proposal given by (Choi et al., 2005), who proposed
the addition of a clone variable U 0 but missed the addition
of evidence variable S 0 .

A

A
Θs0 |A
θs0 |a
θs0 |ā

S’
A’

B

C

B

C

D

ΘA0
θa0
θā0

SE (A)
0.3437
0.6562
PM (A0 )
0.8262
0.1737

D

Figure 2: A network N (left), an approximate network N 0 found after deleting A → B (center), along with
parameters for auxiliary evidence variable S 0 and clone A0 (right).

3

PARAMETRIZING EDGES

Finally, these update equations lead to fixed points
characterized by the following conditions:

Given a network N and evidence e, our proposal is
then to approximate this network with another N 0 that
results from deleting some edges U → X as given earlier. Moreover, when performing inference on network
N 0 , we will condition on the augmented evidence e0 ,
composed of the original evidence e and all auxiliary
evidence s0 introduced when deleting edges. More formally, if Pr and Pr 0 are the distributions induced by
networks N and N 0 , respectively, we will use Pr 0 (X|e0 )
to approximate Pr (X|e) where X is a set of variables
in the original network N .
To completely specify our approximate network N 0 ,
we need to specify parameters PM (u0 ) and SE (u) for
each edge that we delete. We have proposed in (Choi
& Darwiche, 2006) an iterative procedure that uses the
following update equations to parametrize edges:
PM (u0 ) = α
SE (u) = α

∂Pr 0 (e0 )
∂θs0 |u
∂Pr 0 (e0 )
,
∂θu0

(1)

where α is a normalizing constant.3 This procedure,
which we call ed-bp, starts with some arbitrary values for PM (U 0 ) and SE (U ), leading to an initial approximate network N 0 . This network can then be
used to compute new values for these parameters according to the update equations in (1). The process
is then repeated until convergence to a fixed point
(if at all). We have also shown that when deleting
enough edges to yield a polytree, the parametrizations
PM (U 0 ) and SE (U ) computed in each iteration correspond precisely to the messages passed by IBP. Moreover, if the edges deleted do not yield a polytree, edbp corresponds to a generalization of IBP (simulated
by a particular choice of a joingraph; see also Aji &
McEliece, 2001; Dechter, Kask, & Mateescu, 2002).
3
This is an alternative, but equivalent, formulation of
the update equations given by (Choi & Darwiche, 2006).

Pr 0 (u|e0 )
Pr 0 (u|e0 \ s0 )

= Pr 0 (u0 |e0 ),
= Pr 0 (u0 ).

(2)

The first condition says that variables U 0 and U should
have the same posterior marginals. The second condition, in light of the first, says that the impact of
evidence s0 on variable U is equivalent to the impact
of all evidence on its clone U 0 . Indeed, these conditions
correspond to the intuitions that motivated ed-bp.
3.1

A VARIATIONAL APPROACH

We propose now a variational approach to parametrizing deleted edges, based on the KL–divergence:
Pr (w|e)
def X
,
KL(Pr (.|e), Pr 0 (.|e0 )) =
Pr (w|e) log
Pr 0 (w|e0 )
w
where w is a world, denoting an instantiation over
all variables. Note that the KL–divergence is not
symmetric: the divergence KL(Pr (.|e), Pr 0 (.|e0 )) is
weighted by the true distribution while the divergence
KL(Pr 0 (.|e0 ), Pr (.|e)) is weighted by the approximate
one. Common practice weighs the KL–divergence using the approximate distribution, which is typically
more accessible computationally (e.g., Yedidia, Freeman, & Weiss, 2005). In contrast, we will weigh by
the true distribution in what follows.
Before we proceed to optimize the KL–divergence, we
must ensure that the domains of the distributions
Pr (.|e) and Pr 0 (.|e0 ) coincide. One way to ensure this
is to use the following construction, demonstrated in
Figure 3. Given a Bayesian network N ? , we can replace each edge U → X to be deleted with a chain
U → U 0 → X, where the equivalence edge U → U 0
denotes an equivalence constraint: θu0 |u = 1 iff u0 = u.
The resulting augmented network N will then satisfy
three important properties. First, it is equivalent to
the original network N ? over common variables. Second, it has the same treewidth as N ? . Finally, when

U

U

U

SE(U)

S’
U’

X

For the remainder of this paper, we will only be dealing with augmented networks N , leaving the original
network N ? implicit.
PM(U’)

U’

X

3.2

Figure 3: The edge U → X in N ? is replaced with a
chain U → U 0 → X in N . We delete the equivalence
edge U → U 0 in N to get N 0 .
we delete equivalence edges U → U 0 from N , we get
an approximate network N 0 that does not require the
introduction of clone variables U 0 as they are already
present in N 0 .4 We can therefore compute the KL–
divergence between the augmented network N and its
approximation N 0 .

Lemma 1 Let N be an (augmented) Bayesian network and N 0 be the network that results from deleting
equivalence edges U → U 0 . Then:

U →U 0 u=u0

Pr 0 (e0 )
1
.
+ log
θu0 θs0 |u
Pr (e)

Since Pr 0 (e0 ) is a function of our edge parameters, the
KL–divergence is thus also a function of our edge parameters. This result will be used later to derive update
equations for our variational method, and to develop
a heuristic for choosing edges to delete.
Before we proceed though, we observe the following. Let X be the variables of the original network N ? and U0 be the clone variables introduced
via the equivalence edges U → U 0 in N . The KL–
divergence of Lemma 1 is then over the variables
XU0 . One would normally prefer to optimize the
KL–divergence KL(Pr (X|e), Pr 0 (X|e0 )) over the original variables X, but our method will seek to optimize KL(Pr (XU0 |e), Pr 0 (XU0 |e0 )) instead. In fact,
the properties of the KL–divergence tell us that
KL(Pr (X|e), Pr 0 (X|e0 ))
≤ KL(Pr (XU0 |e), Pr 0 (XU0 |e0 )).

(3)

In our experimental results in Section 5, we will report
results on both versions of the KL–divergence, referring to KL(Pr (X|e), Pr 0 (X|e0 )) as the exact KL, and
to KL(Pr (XU0 |e), Pr 0 (XU0 |e0 )) as the KL bound.
We still need to add a new child S 0 for each U however. Since variables S 0 are observed, they do not prohibit
us from computing the KL–divergence between N and N 0
even though they are not present in network N .
4

We have the KL–divergence KL(Pr (.|e), Pr 0 (.|e0 )) as
a function of our edge parameters PM (u0 ) = θu0 and
SE (u) = θs0 |u . If we set to zero the partial derivatives of the KL–divergence with respect to each edge
parameter, we get the following.
Theorem 1 Let N be a Bayesian network and N 0 be
the network that results from deleting equivalence edges
U → U 0 . The edge parameters of N 0 are a stationary
point of KL(Pr (.|e), Pr 0 (.|e0 )) if and only if
Pr 0 (u|e0 ) = Pr 0 (u0 |e0 ) = Pr (u|e),

(4)

for all deleted edges U → U 0 .

We can now state the following key result:

KL(Pr (.|e), Pr 0 (.|e0 ))
X X
Pr (uu0 |e) log
=

THE APPROXIMATE NETWORK

X

That is, if we delete the edge U → U 0 , then the marginals on both U and U 0 must be exact in the approximate network N 0 . Note, however, that this does not
imply that other node marginals must be exact in N 0 :
only those corresponding to deleted edges need be.
Theorem 1 has a number of implications. First, the
necessity of Condition (4) will be exploited in the following section to provide an iterative method that
searches for parameters that are a stationary point for
the KL–divergence. Second, the sufficiency of Condition (4) implies that any method that searches for
edge parameters, regardless of the criteria chosen, will
yield parameters that are a stationary point for the
KL–divergence, if the parameters give rise to exact
marginals for variables corresponding to deleted edges.
For example, if we search for parameters using ed-bp
(Choi & Darwiche, 2006), and the parameters found
lead to exact marginals, then these parameters will
indeed be a stationary point for the KL–divergence.
Before we show how to identify parameters satisfying Condition (4), we note that parameters satisfying
Condition (2) do not necessarily satisfy Condition (4)
and, hence, are not necessarily a stationary point for
KL(Pr (.|e), Pr 0 (.|e0 )). We provide a simple network
with four nodes in Appendix B demonstrating this
point. Recall that Condition (2) characterizes IBP and
some of its generalizations (Choi & Darwiche, 2006).
3.3

SEARCHING FOR PARAMETERS

Having characterized stationary points of the KL–
divergence, we now proceed to develop an iterative
procedure for finding a stationary point. Our method
is based on the following result.

Theorem 2 Let N be a Bayesian network and N 0 be
the network that results from deleting equivalence edges
U → U 0 . The edge parameters of N 0 are a stationary
point of KL(Pr (.|e), Pr 0 (.|e0 )) if and only if:


∂Pr 0 (e0 )
0
0 0
, (5)
PM (u ) = Pr (u|e) Pr (e )/
∂θu0


∂Pr 0 (e0 )
SE (u) = Pr (u|e) Pr 0 (e0 )/
. (6)
∂θs0 |u
We have a number of observations about this theorem. First, if we have access to the true marginals Pr (u|e), then this theorem suggests an iterative
method that starts with some arbitrary values of parameters PM (u0 ) and SE (u), leading to some initial
approximate network N 0 . Using this network, we can
0
0
(e0 )
(e0 )
and ∂Pr
compute the quantities Pr 0 (e0 ), ∂Pr
∂θu0
∂θs0 |u ,
which can then be used to compute new values for
the parameters PM (u0 ) and SE (u), one set at a time.
The process can then be repeated until convergence
(if at all). We will refer to this method as ed-kl, to
be contrasted with ed-bp given earlier (Choi & Darwiche, 2006). Note that since the KL–divergence is
non-negative, there exists a set of edge parameters that
are globally minimal. However, a stationary point of
the KL–divergence is not necessarily a global minima.
Second, the availability of the true marginals Pr (u|e)
typically implies that the network treewidth is small
enough to permit the computation of these marginals.
Hence, ed-kl is in general applicable to situations
where the treewidth is manageable, but where the constrained treewidth is not. In these situations, the goal
of deleting edges is to reduce the network constrained
treewidth, making it amenable to algorithms that are
exponential in constrained treewidth, such as MAP
(Park & Darwiche, 2004), inference in credal networks
(Cozman et al., 2004), and the computation of nonmyopic value of information (Krause & Guestrin, 2005).
Third, we have the following result which is critical for
the practical application of ed-kl:
Theorem 3 Let N be a Bayesian network and N 0 be
the network that results from deleting a single equivalence edge U → U 0 . We then have
Pr 0 (e0 ) =

∂Pr (e)
,
∂θu0 |u

X

θs0 |u θu0

=

X

θs0 |u

=

X

θu0

uu0

which implies:
∂Pr 0 (e0 )
∂θu0
∂Pr 0 (e0 )
∂θs0 |u

u

u0

∂Pr (e)
,
∂θu0 |u

∂Pr (e)
.
∂θu0 |u

The main observation here is that ∂Pr (e)/∂θu0 |u is a
function of the original network and, therefore, is independent of the parameters θu0 and θs0 |u —this is why
the second and third equations above follow immediately from the first. Given the above equations, we can
apply ed-kl to a single deleted edge, without the need
for inference. That is, assuming that we have computed ∂Pr (e)/∂θu0 |u , we can use the above equations
to compute updated values for edge parameters from
the old values in constant time. This result will have
implications in the following section, as we present a
heuristic for deciding which edges to delete.

4

CHOOSING EDGES TO DELETE

Our method for deciding which edges to delete is based
on scoring each network edge in isolation, leading to a
total order on network edges, and then deleting edges
according to the resulting order. For example, if we
want to delete k edges, we simply delete the first k
edges in the order.
The score for edge U → U 0 is based on the KL–
divergence between the original network N and an approximate network N 0 which results from deleting the
single edge U → U 0 . The KL–divergence is computed
using Lemma 1. This lemma requires some quantities
from the original network N , which can be computed
since the network is assumed to have a manageable
treewidth. The lemma also requires that we have the
parameters θu0 and θs0 |u for the deleted edge U → U 0 ,
and the corresponding probability Pr 0 (e0 ). These can
be computed relatively easily using Theorem 3, assuming that we have computed ∂Pr (e)/∂θu0 |u as explained
in the previous section. Given these observations, all
edge parameters, together with the corresponding KL
scores, can be computed simultaneously for all edges
using a single evaluation of the original network. Moreover, the computed parameters have another use beyond scoring edges: when used as initial values for
ed-kl, they tend to lead to better convergence rates.
We indeed employ this observation in our experiments.

5

EMPIRICAL ANALYSIS

We present experimental results in this section on a
number of Bayesian networks, to illustrate a number
of points on the relative performance of ed-kl and edbp. We start with Figure 4 which depicts the quality
of computed approximations according to the exact
KL–divergence5 ; see Equation 3. For each approximation scheme, we consider two methods for deleting
edges. For ed-kl, we delete edges randomly (ed-kl5
To compute the exact KL–divergence, see, for example,
(Choi et al., 2005).

alarm

win95pts

1.6

0.16
ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

3.5
avg KL−divergence (exact)

1.8
avg KL−divergence (exact)

emdec

4
ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

1.4
1.2
1
0.8
0.6

3

0.14
avg KL−divergence (exact)

2

2.5
2
1.5
1

0.12

ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

0.1
0.08
0.06
0.04

0.4
0.5

0.2
10

20
30
edges deleted

0.02

0
0

40

20

40

tcc

0
0

100

1

0.2

0.15

0.1

0.05

50

100
edges deleted

100
150
edges deleted

20

0.8

0.6

0.4

0
0

150

200

250

25

ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

15

10

5

0.2

0
0

50

chain

1.2
ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

avg KL−divergence (exact)

avg KL−divergence (exact)

80

grid

0.3

0.25

60
edges deleted

avg KL−divergence (exact)

0
0

10

20

30
40
50
edges deleted

60

70

0
0

80

5

10

15
20
25
edges deleted

30

35

40

Figure 4: Comparing ed-kl and ed-bp using the exact KL, with two methods for edge deletion.
alarm

win95pts

7

16
avg KL−divergence (bound)

avg KL−divergence (bound)

8

emdec

18
ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

6
5
4
3
2

14

3
ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

2.5
avg KL−divergence (bound)

9

12
10
8
6
4

2

1.5

1

0.5
1
0
0

2
10

20
30
edges deleted

0
0

40

20

40

60
edges deleted

80

0
0

100

50

100
150
edges deleted

200

250

Figure 5: Comparing ed-kl and ed-bp using the KL bound, with two methods for edge deletion.
alarm

win95pts

70

160

120
ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

100

140

40
30

100
80

60

40

60

20

ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

80

120

iterations

iterations

50

180

iterations

60

emdec

200
ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

40

20

10
20
0
0

10

20
30
edges deleted

40

0
0

20

40

60
edges deleted

80

100

0
0

50

100
150
edges deleted

Figure 6: Comparing ed-kl and ed-bp according to number of iterations to converge.

200

250

tcc

0.95

0.99

0.9

0.85

0.8

0.75

0.7
0

ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided
20

40

80

0.97

0.96

0.94
0

100

0.9

0.98

0.95

60
edges deleted

grid
1

relative difference

1

relative difference

relative difference

win95pts
1

ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided
50

0.8
0.7
0.6
0.5
0.4

100
edges deleted

150

0

ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided
50

100

150
200
edges deleted

250

300

350

Figure 7: Approximating MAP using ed-kl and ed-bp.
win95pts

tcc

30

25
ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

20

15

15

10

15

10

5

10

5
0

ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

20
constrained treewidth

constrained treewidth

25
constrained treewidth

grid

20
ED−BP−Rand
ED−BP−MI
ED−KL−Rand
ED−KL−Guided
ED−BP−Guided

20

40

60
edges deleted

80

100

5
0

0
50

100
edges deleted

150

0

50

100

150
200
edges deleted

250

300

350

Figure 8: Reducing constrained treewidth by deleting edges. The horizontal line denotes network treewidth, as
approximated using a min-fill heuristic.
rand) and according to the heuristic of Section 4 (edkl-guided). For ed-bp, we delete edges randomly
(ed-bp-rand) and according to a heuristic based on
mutual information (ed-bp-mi) given in (Choi & Darwiche, 2006). As is clear from the figures, ed-kl is
overwhelmingly superior as far as minimizing the KL–
divergence, sometimes even using random deletion of
edges.
Figure 5 depicts sample results for the quality of approximations according to the KL–divergence bound;
see Equation 3. As mentioned earlier, ed-kl searches
for stationary points of this bound instead of stationary points of the exact KL–divergence, yet empirically
one does not see much difference between the two on
these networks.
Figures 4 and 5 depict another approximation scheme,
ed-bp-guided, which deletes edges based on the
heuristic of Section 4, but then uses ed-bp to search
for parameters instead of ed-kl. This is a hypothetical method since the mentioned heuristic assumes that
the network treewidth is manageable, a situation under which one would want to apply ed-kl instead of
ed-bp. Yet, our results show that ed-bp is consistently very close to ed-kl in this case as far as minimizing the KL–divergence. This observation is critical

as it highlights the great importance of heuristics for
deleting edges. In particular, these results show that
ed-bp can do quite well in terms of minimizing the
KL–divergence if the right edges are deleted!
Figure 6 depicts sample results on the speed of convergence for both ed-kl and ed-bp, again using the
different methods for edge deletion. In two of these
networks, ed-kl consistently converges faster than edbp. In the three omitted figures, due to space limitations, ed-kl is also superior to ed-bp.
We consider also sample results from using approximations identified by ed-kl to approximate MAP. Figure 7 depicts the relative difference p/q, where p is the
value of the MAP solution found in the approximate
network N 0 and q is the value of a MAP solution in
the original network N . It is clear from the figure that
ed-kl-guided produces the superior approximations,
and can provide accurate solutions even when many
edges are deleted. Again, based on the hypothetical
method ed-bp-guided, we see that it is possible for
ed-bp to produce good MAP approximations as well
if the right edges are deleted.
Figure 8 highlights how effective deleting edges is in
reducing the constrained treewidth (approximated using a min-fill heuristic), and thus how effective deleting

edges is in reducing the complexity of computing MAP.
We see that good approximations can be maintained
even when the constrained treewidth is reduced to the
network treewidth. When we further delete every network edge, we have a fully factorized approximation of
MAP, where the constrained (and network) treewidth
corresponds to the size of the largest network CPT.

ture from ed-kl and can lead to much worse behavior
for less likely evidence. That is, these approaches approximate a network once for all queries, while ed-kl
can approximate a network for each specific query.

The plots given in this section correspond to averages
of at least 50 instances per data point, where each
instance correspond to evidence over all leaf nodes
drawn from the network joint. We have also experimented with evidence drawn randomly (not from the
joint), leading to similar results. Networks tcc and
emdec are courtesy of HRL Labs, LLC. The grid and
chain networks are synthetic and available from the authors. Networks alarm and win95pts are available at
http://www.cs.huji.ac.il/labs/compbio/Repository.

We proposed a method, ed-kl, for approximating
Bayesian networks by deleting edges from the original network and then finding stationary points for the
KL–divergence between the original and approximate
networks (while weighing the divergence by the true
distribution). We also proposed an efficient heuristic
for deciding which edges to delete from a network, with
the aim of choosing network substructures that lead to
high quality approximations.

6

RELATED WORK

Many variational methods pose the problem of approximate inference as exact inference in some approximate model, often seeking to minimize the KL–
divergence, but weighing it by the approximate distribution (e.g., Jordan, Ghahramani, Jaakkola, & Saul,
1999; Jaakkola, 2000; Wiegerinck, 2000; Geiger &
Meek, 2005). One example is the mean–field method,
where we seek to approximate a network N by a
fully disconnected N 0 (Haft, Hofmann, & Tresp, 1999).
If we delete all edges from the network and try to
parametrize edges using ed-kl, we would be solving
the same problem solved by mean–field, except that
our KL–divergence is weighted by the true distribution, leading to more expensive update equations.
Other variational approaches typically assume particular structures in their approximate models, such as
chains (Ghahramani & Jordan, 1997), trees (Frey, Patrascu, Jaakkola, & Moran, 2000; Minka & Qi, 2003),
or disconnected subnetworks (Saul & Jordan, 1995;
Xing, Jordan, & Russell, 2003). In contrast, ed-kl
works for any network structure which is a subset of
the original. In fact, the efficient edge deletion heuristic of Section 4 tries to select the most promising
subnetworks and is quite effective as illustrated earlier. Again, most of these approaches weigh the KL–
divergence by the approximate distribution for computational reasons, with the notable exceptions of (Frey
et al., 2000; Minka & Qi, 2003).
Other methods of edge deletion have been proposed
for Bayesian networks (Suermondt, 1992; Kjærulff,
1994; van Engelen, 1997), some of which can be rephrased using a variational perspective. All of these
approaches, however, approximate a network independent of the given evidence, which is a dramatic depar-

7

CONCLUSION

The update equations of ed-kl require exact posteriors from the original network. This means that edkl is, in general, applicable to problems that remain
hard even when treewidth is manageable, including
MAP, nonmyopic value of information, and inference
in credal networks. This is to be contrasted with our
earlier method ed-bp, which updates parameters differently, coinciding with IBP and some of its generalizations.
Our empirical results provide good evidence to the
quality of approximations returned by ed-kl, especially when compared to the approximations returned
by ed-bp. Moreover, our results, both theoretical
and empirical, shed new and interesting light on edbp (and, hence, IBP and some of its generalizations),
showing that it can also produce high quality approximations (from a KL–divergence viewpoint), when
deleting the right set of network edges.
Acknowledgments
This work has been partially supported by Air Force
grant #FA9550-05-1-0075-P00002 and by JPL/NASA
grant #1272258.

A

Proof Sketches

Note that u ∼ w signifies that u and w are compatible
instantiations.
Proof of Lemma 1 Deleting edges U → U 0 , we have:
KL(Pr (.|e), Pr 0 (.|e0 )) =

X
w

Pr (w|e) log

Pr (w|e)
Pr 0 (w|e0 )

Pr 0 (e0 )
Pr (w, e)
+ log
=
Pr (w|e) log
0
0
Pr (w, e )
Pr (e)
w
X
Y θu0 |u
Pr 0 (e0 )
+ log
=
Pr (w|e) log
θu0 θs0 |u
Pr (e)
0
w
X

uu ∼w

=

X X

Pr (w|e) log

w uu0 ∼w

=

X X X

θu0 |u
Pr 0 (e0 )
+ log
θu0 θs0 |u
Pr (e)

Pr (w|e) log

U →U 0 uu0 w|=uu0

=

X X

Pr (uu0 |e) log

U →U 0 uu0

=

θu0 |u
Pr 0 (e0 )
+ log
θu0 θs0 |u
Pr (e)

1
Pr 0 (e0 )
Pr (uu0 |e) log
.
+ log
θu0 θs0 |u
Pr (e)
0

U →U 0 u=u

The last equation follows, since when u does not agree
with u0 , we have that Pr (uu0 |e) log θu0 |u = 0 log 0,
which we assume is equal to zero, by convention. 
Proof of Theorem 1 Note that when u = u0 , we
have Pr (uu0 |e) = Pr (u|e) = Pr (u0 |e).
First direction of theorem. Let f be the KL–divergence
as given in Lemma 1. Setting ∂f /∂θu0 to zero, we get:
0

Pr 0 (u0 , e0 )
θu0 ∂Pr 0 (e0 )
=
= Pr 0 (u0 |e0 )
0
0
Pr (e ) ∂θu0
Pr 0 (e0 )

Similarly, to show Pr (u|e) = Pr 0 (u|e0 ). Note that
constraints such as normalization are inactive here.
Second direction of theorem. Given a network N 0
where marginals on U and U 0 are exact, we want to
verify that the edge parameters are stationary points.
If we take the partial derivative with respect to θu0 :


∂f
θu0 ∂Pr 0 (e0 )
1
−Pr (u|e) +
=
∂θu0
θu0
Pr 0 (e0 ) ∂θu0


1
Pr 0 (u0 , e0 )
=
−Pr (u|e) +
θu0
Pr 0 (e0 )
1
(−Pr (u|e) + Pr 0 (u0 |e0 )) .
=
θu0
We are given Pr 0 (u0 |e0 ) = Pr (u|e), thus ∂f /∂θu0 = 0.
Similarly, to show ∂f /∂θs0 |u = 0.

Proof of Theorem 2 Equation 5 follows easily from
Equation 7. Equation 6 follows from ∂f /∂θs0 |u .

Proof of Theorem 3 First, we have:
X
Pr 0 (e0 ) =
Pr 0 (uu0 , e0 )
uu0

uu0

θs0 |u θu0

X2

U2

X2

S’

U’1

X1

X
∂Pr 0 (u, e)
∂ 2 Pr 0 (e0 )
=
.
θs0 |u θu0
∂θs0 |u ∂θu0
∂θu0
0
uu

Note that the distribution induced by a network where
a single edge U → U 0 has been deleted is equivalent to

U2

Figure 9: An example network N (left) where deleting
a single edge (right) may have infinitely many ed-bp
fixed points.

the distribution induced by another network N 0 that
is identical in structure to N , except that θu0 |u = θu0
for all u. We then have:

(7)

where u agrees with u0 . We then have

=

U1

Pr 0 (e0 ) =

0

Pr (u|e)
1
∂Pr (e )
∂f
=−
+
= 0,
∂θu0
θu0
Pr 0 (e0 ) ∂θu0

X

X1

θu0 |u
Pr 0 (e0 )
+ log
θu0 θs0 |u
Pr (e)

X X

Pr (u|e) =

U1

X

θs0 |u θu0

uu0

∂Pr 0 (e) X
∂Pr (e)
=
.
θs0 |u θu0
∂θu0 |u
∂θu0 |u
0

The other relations follow easily.

B

uu



Example

We demonstrate here an example where ed-bp fixed
points are not necessarily stationary points of the
KL–divergence. This example also shows that even
if we delete a single edge, ed-bp can have infinitely
many parametrizations satisfying Condition (2), even
though there exists an ed-bp (and ed-kl) fixed point
minimizing the KL bound, KL(Pr (.|e), Pr 0 (.|e0 )) (as
well as minimizing the exact KL). This example also
corresponds to an instance of IBP (with a particular
message passing schedule), since edge deletion renders
the network a polytree (Choi & Darwiche, 2006).
Our example is depicted in Figure 9. Variables Ui
have parameters θui = θūi = 0.5. Variables Xj are
fixed to states xj , and assert the equivalence of U1
and U2 : θxj |u1 u2 = 1 iff u1 = u2 . Conditioning on evidence e = x1 x2 , we have Pr (u1 |e) = Pr (u2 |e) = 0.5.
If we delete the edge U1 → X1 (implicitly, we delete
an edge U1 → U10 ), then any non-zero parameterization of our edge parameters satisfies the ed-bp fixed
point conditions given by Condition (2). For example,
when θs0 |u1 = θs0 |ū1 = 0.5, and θu01 = θū01 = 0.5, the
KL–divergence is zero and thus minimized, yielding
parent and clone marginals that are exact. By Theorem 1, edges parameters are then a stationary point
of the KL–divergence. However, when θu01 6= 0.5, the
parent and clone marginals are not exact, and thus
edges parameters are not a stationary point of the KL–
divergence, again by Theorem 1.


