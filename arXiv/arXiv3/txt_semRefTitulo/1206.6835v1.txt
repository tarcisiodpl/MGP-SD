
Continuous-time Bayesian networks (CTBNs)
are graphical representations of multi-component
continuous-time Markov processes as directed
graphs. The edges in the network represent direct influences among components. The joint
rate matrix of the multi-component process is
specified by means of conditional rate matrices
for each component separately. This paper addresses the situation where some of the components evolve on a time scale that is much shorter
compared to the time scale of the other components. We prove that in the limit where the
separation of scales is infinite, the Markov process converges (in distribution, or weakly) to a
reduced, or effective Markov process that only
involves the slow components. We also demonstrate that for a reasonable separation of scales
(an order of magnitude) the reduced process is
a good approximation of the marginal process
over the slow components. We provide a simple
procedure for building a reduced CTBN for this
effective process, with conditional rate matrices
that can be directly calculated from the original
CTBN, and discuss the implications for approximate reasoning in large systems.

1

Introduction

Continuous-time Markov processes are a framework used
in the modeling of a huge range of stochastic dynamical systems, e.g., chemical kinetics, population dynamics,
stock markets, and many more (Gardiner [2004]). We consider Markov processes that are homogeneous in time and
have a finite state space. Such systems are fully determined
by the state space S, the distribution of the process at the
initial time, and a description of the dynamics of the process. These dynamics are specified by a rate matrix Q,
whose off-diagonal entries qa,b are exponential rate inten-

Raz Kupferman
Institute of Mathematics
The Hebrew University
raz@math.huji.ac.il

sities for transitioning between states. Intuitively, we can
think of the entry qa,b as the rate parameter of an exponential distribution whose value is the duration of time spent in
state a before transitioning to b. When more than one transition is possible, the shortest duration determines the next
is the expected duration in state a before
state. Thus, q‚àí1
a,b
transitioning to state
P b (assuming it were the only possible
transition), and ( b,a qa,b )‚àí1 is the expected duration before transitioning out of state a.
In many applications, the state space is of the form of a
product space S = S1 √ó S1 √ó ¬∑ ¬∑ ¬∑ √ó SM , where M is the
number of components (such processes are called multicomponent). Even if each of the Si is of low dimension, the
dimension of the state space is exponential in the number of
components, which often poses computational difficulties,
e.g., in learning applications. Continuous-time Bayesian
networks (Nodelman et al. [2002, 2003]) are a graphical
representation for Markov processes that have extra structure, therefore allowing for more a compact representation
with fewer parameters. The first assumption is that transitions only occur in one component at a time. Second,
the transition rates associated with each component are assumed to only depend on the state of a collection of ‚Äúparent components‚Äù. The CTBN is a directed, possibly cyclic
graph whose nodes are the components of the process, and
whose edges represent parent-child relations.
It is also often the case that certain components are considerably faster than the others. Mathematically, it is assumed that the (conditional) rates associated with the fast
components are larger by a factor of 1/ than the (conditional) rates associated with the other, slow components,
with   1. Systems having such property are said to have
a separation of scales, or to be singularly perturbed. Such
situations are ubiquitous, for example, in chemical kinetics,
where some reactions may occur much faster than other. In
such situations, the fast components tend to reach ‚Äúlocal
equilibrium‚Äù, relative to the slow components, and under
certain conditions, reduced Markovian dynamics can be derived for a lower dimensional system that only involves the
slow components (see van Kampen [1985] for a classical

review on dimension reduction in scale separated systems;
see Givon et al. [2004] for a recent review).
In this paper we derive the limiting Markov process and
show how to reduce a CTBN with fast components into
a smaller CTBN that involves only the slow components.
We discuss the implications of this result for inference in
CTBNs with different time scales.

2

Continuous-time Bayesian networks

In this section we briefly review the CTBN model (Nodelman et al. [2002]). Consider an M-component Markov process
X(t) = (X1 (t), X2 (t), . . . XM (t))
with state space
S = S1 √ó S2 √ó ¬∑ ¬∑ ¬∑ √ó SM .
A notational convention: vectors are denoted by boldface
symbols, e.g., X, a, and matrices are denoted by blackboard style characters, e.g., Q. The states in S are denoted by vectors of indexes, a = (a1 , . . . , aM ). The indexes
1 ‚â§ i, j ‚â§ M are used to enumerate the components.
The dynamics of a time-homogeneous continuous-time
Markov process are fully determined by the Markov transition function,
pa,b (t) = Pr(X t+s = b|X s = a),
where time-homogeneity implies that the right hand side
does not depend on s. Provided that the transition function satisfies certain analytical properties (continuity, and
regularity; see Chung [1960]) the dynamics are fully captured by a constant matrix Q‚Äîthe rate, or intensity matrix‚Äîwhose entries qa,b are defined by
qa,b = lim
h‚Üì0

pa,b (h) ‚àí Œ¥a,b
,
h

where Œ¥a,b is a multivariate Kronecker delta (an alternative notation using an indicator function is 1(a = b)). The
Markov process X t can also be given a pathwise characterization. Suppose the process starts in a state a. After
spending a finite amount of time in state a it transitions,
at a random time, to a random state b , a. The transition times to the potential new states are exponentially
distributed, with qa,b , a , b, being the exponential rate
for transitioning from state a to state b. The diagonal elements of Q satisfy the condition that each row sums up
to zero. Suppose, for simplicity, that each component X j
takes values in a d-dimensional space. Then, the state space
is dM -dimensional, and the Q-matrix involves dM (dM ‚àí 1)
parameters.
The time-dependent probability distribution of the process,
p(t), whose entries are defined by
pa (t) = Pr(X(t) = a),

a ‚àà S,

satisfies the so-called master equation,
dp
= QT p.
dt

(1)

It is important to note that the master equation (1) encompasses all the statistical properties of the Markov process. There is a one-to-one correspondence between the
description of a Markov process by means of a master equation, and by means of a ‚Äúpathwise‚Äù characterization (up to
stochastic equivalence of the latter; see Gikhman and Skorokhod [1975]).
We are concerned here with processes for which every transition involves a single component. In such case, the most
general rate matrix takes the form
qa,b =

M
X

qia,bi

Y

i=1

Œ¥a j ,b j ,

(2)

j,i

where the qia,bi are the entries of a conditional rate matrix
Qi for Xi transitioning from ai to bi given that the state of
the system is a. The structure (2) represents the fact that
each component undergoes transitions independently from
the other components, but at a rate that depends on the current state of the entire system. A Q-matrix of the form (2)
requires MdM (d ‚àí 1) independent parameters, which may
still be a large number.
Further reduction in the number of parameters is obtained if
additional structure is incorporated. CTBNs are applicable
to situations where each of the conditional rate matrices Qi
is only influenced by a subset of component. Specifically, a
parent-child relation is introduced between ordered pairs of
components. To every 1 ‚â§ i ‚â§ M we define the (possibly
empty) set of indexes
n
o
Par(i) = 1 ‚â§ j ‚â§ M : X j is a parent of Xi ,
and the state space associated with the parents of Xi ,

SPar(i) =
S j.
j‚ààPar(i)

We then introduce a restriction operator Pi : S ‚Üí SPar(i) ,
which extracts from the state of the system the state of the
subsystem that consists of the parents of Xi ,
Pi (a) = (am1 , am2 , . . . , ami ),
where Par(i) = {m1 , m2 , . . . , mi }.
The conditional rate matrix associated with the i-th component only depends on the state of the parent components.
To make this dependence explicit, we denote the condii| Par(i)
tional rate matrices by Qi| Par(i) with entries qa ,b | P (a) . Thus,
i

i

i

the joint rate matrix of the whole process assumes the reduced form
qa,b =

M
X

i| Par(i)

qa ,b | P (a)
i

i=1

i

Y

i

Œ¥a j ,b j .

(3)

X2

X3

j,i

Equation (3) is, using the terminology of Nodelman et al.
[2002], the ‚Äúamalgamation‚Äù of the M conditional rate matrices. Note the compact representation which is valid for
both diagonal and off-diagonal entries. It is also noteworthy that amalgamation is a summation, rather than a product; indeed, independent exponential rates are additive. If,
for example, every component has k parents, the rate matrix
requires now only Mdk+1 (d ‚àí 1) parameters.
The dependency relations between components can be represented graphically as a directed graph, G, in which each
node corresponds to a component, and each directed edge
defines a parent-child relation. A CTBN consists of such a
graph, supplemented with a set of M conditional rate matrices Qi| Par(i) , and an initial distribution. Formally, we define
a CTBN as a tuple
D
E
C = G, {Qi| Par(i) }M
i=1 , P0 ,
where P0 is the initial distribution over X(0).
As stated in Nodelman et al. [2002], the graph structure
has two main roles: (i) it provides a data structure to which
parameters are associated; (ii) it provides a qualitative description of dependencies among the various components
of the system. The graph structure also reveals statistical (possibly conditional) independencies between sets of
components. An example of a four-component CTBN is
shown in Figure 1.
For later use, we note that if we substitute the structure (3)
of the rate matrix into the master equation (1), the latter
takes the particular form,

X4

Figure 1: A four-component process, in which X1 is a
Markov process (i.e., its rate matrix does not depend on
other components), X2 and X3 are influenced by X1 , and
in turn influence X4 . Thus, the construction of a joint rate
matrix requires the prescription of a rates matrix Q1 and
conditional rates matrices Q2|1 , Q3|1 , and Q4|2,3 . This structure implies, for example, that X1 and X4 are statistically
independent given the entire trajectories of X2 and X3 .
the conditional rate matrices of the complementary components. Intuitively, this means that every fast component has
undergone many transitions during a time interval characteristic of a single transition in the slow components. For
the sake of mathematical analysis, we will consider CTBNs
that are parametrized by a small parameter  that represents
the ratio of a characteristic time of residence in a state of a
fast component, and a characteristic time of residence in a
state of a slow component.
We now introduce definitions pertinent to the classification
of components into fast ones and slow ones: let
Ifast = {1 ‚â§ i ‚â§ M : Xi is a fast component}
Islow = {1 ‚â§ i ‚â§ M : Xi is a slow component}
be the sets of indexes of fast and slow components, respectively, and let


Sfast =
Si
and
Sslow =
Si
i‚ààIfast

M

dpb X X i| Par(i)
qa ,b | P (b) p(b1 ,...,bi‚àí1 ,ai ,bi+1 ,...,bM ) .
=
i i i
dt

(4)

i=1 ai ‚ààSi

3

X1

Singularly perturbed CTBNs

In many situations, it is possible to partition the M components into two sets: ‚Äúfast‚Äù components and ‚Äúslow‚Äù components. A standard measure for the ‚Äúspeed‚Äù of a Markov
process is the rate at which it equilibrates, which is commonly taken to be the absolute value of the second largest
eigenvalue of the Q matrix. In the context of CTBNs every
component is assigned a conditional rate matrix. Scale separation holds if all the equilibration rates associated with
the conditional rate matrices of a subset of components are
much faster than all the equilibration rates associated with

i‚ààIslow

be the state spaces associated with the two sets of components. For a ‚àà S we define the restriction operator
Fast : S ‚Üí Sfast ,
Fast(a) = (ai1 , ai2 , . . . , aim ),
with {i1 , i2 , . . . , im } = I f . The restriction operator Slow :
S ‚Üí Sslow is defined similarly. Below we will derive expressions that involves states in S, Sslow and Sfast . For the
sake of readability, we will use the symbols a, b for states
in S, Œ±, Œ≤ for states in Sslow , and Œ∂ for states in Sfast .
We make the following assumptions:
Assumption 3.1 The conditional rate matrices associated
with fast components can be expressed as 1/ times an independent rate matrix, while the conditional rate matrices associated with the slow components do not depend on

. Furthermore, for every fixed state of the slow components, the Markov process defined by the conditional rate
matrices of the fast components is ergodic over Sfast .
Note that even if the entire system is ergodic, the subsystem that consists only of the fast components, with the
slow components fixed, is not necessarily ergodic. Thus,
we need to explicitly require this additional property. This
condition is automatically satisfied, for example, if the conditional rate matrices have strictly positive off-diagonal entries. We will denote the (conditional) equilibrium distribution of the fast components, given the state of the slow comfast |Islow
ponents, by œÄIfast |Islow with entries œÄIŒ∂|Œ±
(where, as stated
above, Œ∂ ‚àà Sfast and Œ± ‚àà Sslow ).
Graphically, we will mark fast components by nodes with
shaded fillings. For example, Figure 1 represents a fourcomponent CTBN in which only X3 is a fast component.
Consequently,
Ifast = {3}
Sfast = S3
Fast(a) = a3

Remark. When interpreting (7) note that Œ± and Œ≤ are both
elements in Sslow , whereas the summation variables Œ∂ are
elements in Sfast . The concatenation (Œ±, Œ∂) is identified as
an element of the full space S, and Pi ((Œ±, Œ∂)) restricts the
state (Œ±, Œ∂) to only those components that are parents of Xi .
The expression (6) for the effective rates can be written in
an alternative form. Writing
Q=

1 fast
Q + Qslow ,


where Qfast and Qslow are -independent and have entries
qfast
a,b =

X

i| Par(i)

qa ,b | P (a)
i

i

Y

i

i‚ààIfast

qslow
a,b =

Islow = {1, 2, 4}
Sslow = S1 √ó S2 √ó S4
Slow(a) = (a1 , a2 , a4 ),

Œ¥a j ,b j

j,i

X

i| Par(i)

qa ,b | P (a)
i

i

Y

i

i‚ààIslow

Œ¥a j ,b j ,

j,i

the entries of QÃÉ can be written as

and the conditional rate matrices can be written as Q1 ,
Q2|1 , 1 Q3|1 and Q4|2,3 , where each of the Qi| Par(i) is independent.
The goal is to study the limiting behavior of the system as
 ‚Üí 0. Our main theorem, whose proof is sketched in
Appendix A, is:
Theorem 3.1 Let X(t) be an M-component Markov process satisfying Assumption 3.1. Then as  ‚Üí 0, the distribution p(t) converges to a product distribution of the form,
pa (t) =

The fact that the marginal distribution over Sslow satisfies
a master equation means that the limiting behavior of the
slow components is Markovian.

qÃÉŒ±,Œ≤ =

X
Œ∂‚ààSfast

fast |Islow slow
œÄIŒ∂|Œ±
q(Œ±,Œ∂),(Œ≤,Œ∂) .

(8)

In simple words, the reduced rate matrix associated with
the limiting dynamics of the slow components is the full
rate matrix, averaged over the conditional equilibrium distribution of the fast components. If we denote by E f |s
expectation with respect to the conditional distribution
œÄIfast |Islow , then (8) takes the more suggestive form,
QÃÉ = E f |s [Qslow ].

Ifast |Islow
œÄFast(a)|
pÃÉ
(t),
Slow(a) Slow(a)

where pÃÉ is the marginal distribution of the slow components. The latter, satisfies the reduced, or effective master
equation,
d
pÃÉ = QÃÉT pÃÉ,
(5)
dt
where QÃÉ is a rate matrix over Sslow . Its entries are given by
X
Y
qÃÉŒ±,Œ≤ =
qÃÉiŒ±,Œ≤
Œ¥Œ± j ,Œ≤ j ,
(6)
i‚ààIslow

Islow 3j,i

where QÃÉi with entries
X
Ifast |Islow i| Par(i)
qÃÉiŒ±,Œ≤ =
œÄŒ∂|Œ±
qŒ±i ,Œ≤i | Pi ((Œ±,Œ∂))

(7)

Œ∂‚ààSfast

is the effective conditional rate matrix associated with the
slow component Xi .

Example 3.1 As the simplest illustration of Theorem 3.1,
consider a two-component system X(t) = (X1 (t), X2 (t))
with rate matrix of the form
Q=

1 1
Q + Q2|1 .


That is, X1 is a fast component and it influences the slow
component X2 . The equilibrium distribution of X1 is denoted by œÄ1 . Theorem 3.1 asserts that as  ‚Üí 0, X2 (t) converges in a weak sense (i.e., in distribution) to a Markov
process with effective rate QÃÉ, whose entries are given by
qÃÉŒ±2 ,Œ≤2 = qÃÉ2Œ±2 ,Œ≤2 =

X
Œ∂1 ‚ààS1

œÄ1Œ∂1 q2|1
.
Œ±2 ,Œ≤2 |Œ∂1

The next section addresses the systematic derivation of reduced CTBNs.

4
4.1

Dimension reduction of CTBNs

X1

Segregated fast components
X2

We start by considering CTBNs in which fast components
are segregated: there are no parent-child relations between
two fast components. In such case the conditional equilibrium distribution of the fast components factors into a
product distribution on the form
Y
i| Par(i)
fast |Islow
œÄIŒ∂|Œ±
=
(9)
œÄŒ∂i | Pi (Œ±) ,
i‚ààIfast

where, with a slight abuse of notations, Pi (Œ±) extracts from
the vector Œ± ‚àà Sslow those components that belong to
Par(i). That is, the marginal equilibrium distribution of
each fast components depends only on the state of its parent components, which by assumption are all slow components.
Substituting the factorization (9) into the effective rate matrix (7), we obtain
Ô£Æ
Ô£π
X Ô£ØÔ£ØY
Ô£∫ i| Par(i)
k| Par(k) Ô£∫
i
Ô£Ø
Ô£∫Ô£∫ q
Ô£ØÔ£Ø
qÃÉŒ±,Œ≤ =
œÄ
Ô£∫Ô£ª Œ±i ,Œ≤i |(Pi (Œ±),Œ∂)) ,
Œ∂
|
P
(Œ±)
Ô£∞
k k
Œ∂

=‚áí

X1

X3

X3

Figure 2: Left: a three-component CTBN with one fast
component. Right: the reduced two-component CTBN in
the limit  ‚Üí 0.
X1

X2

X3

X4

X6

=‚áí

X5

X1

X2

X6

X5

k

where the product is over k ‚àà Ifast ‚à© Par(i) (i.e., fast parents of Xi ) and the sum is over Œ∂ in the corresponding state
space Sfast ‚à© SPar(i) . Note that QÃÉi is only conditional on
those components that are either slow parents of Xi , or
(non-exclusively) slow parents of fast parents of Xi .
Remark. In such cases where the fast dynamics can be factored into independent components, there is no necessity
for all fast components to evolve on the same fast scale.
The results remain unchanged if each fast component has
its own time scale i , as long as i ‚Üí 0 for all i ‚àà Ifast .
Example 4.1 Consider the three-component system depicted in Figure 2 (left), which consists of a chain of three
components, the one in the middle being fast. The dynamics are defined by the conditional rate matrices Q1 , Q2|1 and
Q3|2 . Let œÄ2|1 be the equilibrium distribution of X2 given a
fixed state of X1 . Theorem 3.1 implies that as  ‚Üí 0, the
joint distribution pÃÉ of the slow components X1 , X3 tends to
the solution of a master equation which corresponds to the
reduced two-state CTBN shown in Figure 2 (right). The effective rate matrix QÃÉ is determined by the conditional rate
matrices QÃÉ1 and QÃÉ3|1 given by
qÃÉ1Œ±1 ,Œ≤1 = q1Œ±1 ,Œ≤1
X
qÃÉ3|1
=
œÄ2|1
q3|2
.
Œ±3 ,Œ≤3 |Œ±1
Œ∂2 |Œ±1 Œ±3 ,Œ≤3 |Œ∂2
Œ∂2 ‚ààS2

Example 4.2 Consider the CTBN shown in Figure 3 (left).
The dynamics are defined by the conditional rate matrices

Figure 3: Left: a six-component CTBN with two fast components. Right: the reduced four-component CTBN in the
 ‚Üí 0 limit.
Q1 , Q2 , Q3|1 , Q4|1,2 , Q5|1,3,4 and Q6|3 . The components
X3 , X4 are assumed to be fast, and have conditional equilibrium distributions œÄ3|1 and œÄ4|1,2 , respectively. As  ‚Üí 0,
the slow components weakly converge to a four-component
Markov process on Sslow defined by the conditional rate matrices QÃÉ1 , QÃÉ2 , QÃÉ5|1,2 , and QÃÉ6|1 whose entries are given by
qÃÉ1Œ±1 ,Œ≤1 = q1Œ±1 ,Œ≤1
qÃÉ2Œ±2 ,Œ≤2 = q2Œ±2 ,Œ≤2
X
X
3|1
qÃÉ5|1,2
=
œÄ
œÄ4|1,2
q5|1,3,4
Œ±5 ,Œ≤5 |(Œ±1 ,Œ±2 )
Œ∂3 |Œ±1
Œ∂4 |(Œ±1 ,Œ±2 ) Œ±5 ,Œ≤5 |(Œ±1 ,Œ∂3 ,Œ∂4 )
Œ∂3 ‚ààS3

qÃÉ6|1
Œ±6 ,Œ≤6 |Œ±1

=

X
Œ∂3 ‚ààS3

Œ∂4 ‚ààS4

œÄŒ∂3|13 |Œ±1 qŒ±6|36 ,Œ≤6 |Œ∂3 .

Note that although X6 and X5 are statistically dependent,
both being descendants of X3 , they do not directly influence
each other in the reduced CTBN; in general, the elimination
of a fast component does not introduce graphical connections between its descendants, in contrast to node elimination in Bayesian networks. This point is highly non-trivial:
for finite  the knowledge of X5 influences the posterior distribution of X3 , which in turn affects the evolution of X6 .
In the limit of extreme scale separation, X3 equilibrates between successive transitions of the slow components, therefore the effective rate matrix of X6 is only affected by the

X1

X1

X3

X1

X2

X4

X2

X3 + X4

=‚áí

X5

X5

=‚áí

X1

X2

X2

X5

X4

X5

Figure 4: Left: a five-component CTBN with two interacting fast component. Center: equivalent CTBN with the
interacting fast components grouped together. Right: the
reduced three-component CTBN in the  ‚Üí 0 limit.
stationary distribution of X3 , which, in turn, is only affected
by X1 .
4.2

X3

Connected fast components

In situations where parent-child relations exist between fast
components, the equilibrium distribution on Sfast conditional of Sslow does not factor into a product over fast components. A simple minded solution is to group together fast
components that are connected together into a compound
component, as illustrated in the following example:
Example 4.3 Consider the CTBN shown in Figure 4 (left),
which is defined by the conditional rate matrices Q1 , Q2 ,
Q3|1 , Q4|3,2 and Q5|4 . The component X3 , X4 are assumed
to be fast. This situation can be adapted to the framework
considered in the previous subsection by grouping together
X3 and X4 into a single component with state space S3 √óS4 .
This intermediate CTBN is depicted in Figure 4 (center).
Let then œÄ3,4|1,2 denote the equilibrium distribution of the
pair X3 , X4 given the pair X1 , X2 . As  ‚Üí 0, the slow
components weakly converge to the Markov processes depicted in the right, with conditional rate matrices QÃÉ1 , QÃÉ2 ,
and QÃÉ5|1,2 , whose entries are given by
qÃÉ1Œ±1 ,Œ≤1 = q1Œ±1 ,Œ≤1
qÃÉ2Œ±2 ,Œ≤2 = q2Œ±2 ,Œ≤2
X X
3,4|1,2
qÃÉŒ±5|1,2
=
œÄ(Œ∂
q5|4
.
5 ,Œ≤5 |(Œ±1 ,Œ±2 )
3 ,Œ∂4 )|(Œ±1 ,Œ±2 ) Œ±5 ,Œ≤5 |Œ∂4
Œ∂3 ‚ààS3 Œ∂4 ‚ààS4

This solution, however, misses some of the structure in the
reduced process.
Example 4.4 Consider the CTBN shown in Figure 5 (left),
which is defined by the conditional rate matrices Q1 , Q2 ,
Q3|1 , Q4|3,2 , Q5|3 , and Q6|4 . The components X3 , X4 are

=‚áí

X6

X1

X2

X5

X6

Figure 5: Left: a six-component CTBN with two interacting fast component. Right: the reduced four-component
CTBN in the  ‚Üí 0 limit.
assumed to be fast. If we group X3 and X4 , we again have
that œÄ3,4|1,2 describes the equilibrium distribution of X3 and
X4 given X1 and X2 . This would imply that X5 depends
on both X1 and X2 in the reduced CTBN. However, if we
examine the pattern of influence in the original CTBN, the
intuition is that there is no path of influence from X2 to X5 .
The situation becomes clearer once we realize that we can
represent the equilibrium distribution of X3 and X4 as
œÄ3,4|1,2
= œÄ3|1
œÄ4|1,2,3
.
(Œ∂3 ,Œ∂4 )|(Œ±1 ,Œ±2 )
Œ∂3 |Œ±1 Œ∂4 |(Œ±1 ,Œ±2 ,Œ∂3 )
To see this, note that X4 does not influence X3 . Thus, if
we fix the values of X1 , the process X3 is Markovian and
reaches the same equilibrium distribution regardless of the
state of X2 . Repeating the same argument as in the previous example, we conclude that as  ‚Üí 0, the slow components weakly converge to the Markov processes depicted
on the right, with conditional rate matrices QÃÉ1 , QÃÉ2 , QÃÉ5|1 ,
and QÃÉ6|1,2 , whose entries are given by
qÃÉ1Œ±1 ,Œ≤1 = q1Œ±1 ,Œ≤1
qÃÉ2Œ±2 ,Œ≤2 = q2Œ±2 ,Œ≤2
X
qÃÉ5|1
=
œÄ3|1
q5|3
Œ±5 ,Œ≤5 |Œ±1
Œ∂3 |Œ±1 Œ±5 ,Œ≤5 |Œ∂3
Œ∂3 ‚ààS3

qÃÉ6|1,2
Œ±6 ,Œ≤6 |(Œ±1 ,Œ±2 )

=

X X
Œ∂3 ‚ààS3 Œ∂4 ‚ààS4

œÄ3,4|1,2
q6|4
.
(Œ∂3 ,Œ∂4 )|(Œ±1 ,Œ±2 ) Œ±6 ,Œ≤6 |Œ∂4

Note that in the last equation we sum over Œ∂3 , for the purpose of finding the marginal probability of X4 in œÄ3,4|1,2 .

The point of the last example is that although we cannot
factor the equilibrium distribution of the two fast components X3 and X4 , the marginal distribution of one variable,
X3 in this example, does not depend on the conditional rate
matrix of the other variable.

To generalize this line of reasoning, we need to characterize
which marginal distributions of the equilibrium distribution
can be computed independently of rates of the other components. To analyze such situations, we consider a more
general result about marginal distributions in CTBNs.
Definition 4.1 Let C be an M-component CTBN, and let J
be a subset of the components 1, . . . , M; we denote by
X J (t) = {Xi (t) : i ‚àà J}
the corresponding sub-process. We say that J is upward
closed if for every i ‚àà J, we have that Par(i) ‚äÜ J. We define
the upward closure Up(J) to be the minimal upward closed
set that contains J.
Example 4.5 In Example 4.4 (Figure 5) J = {1, 3, 5} is an
upward closed subset of components, and
Up({4}) = {1, 2, 3, 4} .
Suppose we are given a CTBN C. Given an upward closed
subset of components, J, we can define the sub-CTBN
spanned by J. Formally, we define


o
n
C J = G| J , Qi| Par(i)
, P0 | J ,
i‚ààJ

where G| J is the sub-graph of G restricted to the components
in J, and P0 | J is the marginal distribution of P0 over X J (0).
The sub-CTBN spanned by J, contains the conditional rate
matrices from the original CTBN for all the components in
J. Since J is upward closed, this results in a well defined
CTBN, as the parents of every component in J appear in the
sub-CTBN.
Theorem 4.1 Let C be an M-component CTBN, and let
J be an upward closed subset of components. Then, the
marginal distribution over X J (t) in C is identical to the distribution over X J (t) in C J .
The proof is straightforward, as we can show that the probability over any trajectory of X J is the same in both distributions. This follows, for example, if we sum the master
equation (4) over all indexes, bi , i < J.
Corollary 4.1 Let C be an M-component CTBN, and let
J be an upward closed subset of components. Then, the
marginal equilibrium distribution over X J does not depend
on the rates associated with the remaining variables.
Using this result we can return to the question of elimination of fast components. Suppose we have a connected
set of fast components. Since they are much faster than the
slow components, we can view their behavior as though the
slow components are fixed. This implies that the equilibrium distribution over the connected fast components has

the behavior of the conditional-CTBN defined by their conditional rate matrices, with the slow components fixed. In
this conditional CTBN, we can apply Corollary 4.1 and find
the set of conditional rate matrices that determine the equilibrium distribution of any particular subset of fast components.
For example, this result implies that in Example 4.4 the
equilibrium distribution over X3 in the ‚Äúfast‚Äù conditional
CTBN depends on the rate matrix Q3|1 but not on Q4|2,3 .
As a consequence the child, X5 , of X3 depends on X1 in
the reduced CTBN, but not on X2 .
We now use this intuition to define the reduced CTBN in
a precise manner. Consider a CTBN with fast and slow
components. Given a set, J ‚äÜ Ifast of fast components, we
define Up f (J) to be the upward closure of J in the subgraph that only consists of fast component; Up f (J) is the
smallest subset of fast components that contains J, such that
if i ‚àà J, then j ‚àà J for all j ‚àà Par(i) ‚à© Ifast ; to shorten the
terminology, we will call Up f (J) the fast-upward closure
of J. We then define for J ‚äÜ Ifast the set
n
o
sPar(J) = i ‚àà Islow : ‚àÉj ‚àà Up f (J), i ‚àà Par(j) .
That is, sPar(J) are the slow components that are parents of
components in J, or components in the fast upward closure
of J. We will call sPar(J) the set of last slow ancestors of
J.
Example 4.6 Consider once again Example 4.4 (Figure 5).
There,
Up f ({4}) = {3, 4}
is the upward closure of the subset of components {4}, in
the subgraph that only contains the fast components. Moreover,
sPar({4}) = {1, 2} ,
since X2 is a parent of X4 and X1 is a parent of X3 , which
belongs to fast-upward closure of {4}.
We now can formally define the procedure of building a
reduced CTBN. Assume we are given a CTBN C with scale
separation. We define the reduced CTBN CÃÉ as follows:
1. GÃÉ is the graph over Islow such that for each i ‚àà Islow
g = (Par(i) ‚à© Islow ) ‚à™ sPar(Par(i) ‚à© Ifast )
Par(i)
In other words, the parents of each slow component Xi
in the reduced CTBN CÃÉ are its slow parents in C supplemented by the last slow ancestors of its fast parents
in C. Consistently with out notations we define PÃÉi (Œ±)
to be the restriction of Œ± ‚àà Sslow to those components
g
that belong to Par(i).

2. For each i ‚àà Islow we define the conditional rate matrix
g
QÃÉi|Par(i) with entries
g
i|Par(i)

qÃÉŒ± ,Œ≤ |PÃÉ (Œ±) =
i

i

i

X
Œ∂

g
Up (Par(i)‚à©Ifast )|Par(i)
i| Par(i)
qŒ±i ,Œ≤i | Pi ((Œ±,Œ∂)) ,
i

œÄŒ∂|PÃÉ f(Œ±)

where the summation variable Œ∂ takes values in the
state space spanned by the fast-upper closure of the
fast parents of Xi . While the last equation may be difficult to parse, it bears a simple interpretation. The
conditional rate matrix of the i-th component in the
reduced CTBN is obtained by averaging over Qi| Par(i)
with respect to the marginal equilibrium distribution
of the fast-upward closure of the fast parents of Xi ,
conditioned by the last slow ancestors of this fastclosure. The effective conditional rate matrix depends, as a result, only on the slow parents of Xi and
on the last slow ancestors of its fast parents.
3. PÃÉ0 = P0 |Islow .
Our main Theorem 3.1, reformulated in the language of
CTBNs, implies:
Theorem 4.2 Let C be an M-component CTBN with conditional rate matrices satisfying Assumption 3.1. Then,
as  ‚Üí 0, the marginal distribution of the sub-process
spanned by the slow components Islow converges to the distribution induced by the reduced CTBN CÃÉ.

5

Numerical examples

Example 5.1 Consider Example 4.1 with state space S j =
{0, 1}, j = 1, 2, 3, and conditional rate matrices
!
‚àí1 1
1
Q =
2 ‚àí2
!
!
‚àí2 2
‚àí3 3
2|1
2|1
Q¬∑|0 =
Q¬∑|1 =
3 ‚àí3
2 ‚àí2
!
!
‚àí3 3
‚àí5 5
3|2
Q3|2
=
Q
=
¬∑|0
¬∑|1
4 ‚àí4
6 ‚àí6
The conditional equilibrium distribution œÄ2|1 is
!
!
1 2
1 3
2|1
2|1
œÄ¬∑|1 =
.
œÄ¬∑|0 =
5 2
5 3
As  ‚Üí 0 the slow components X1 , X3 weakly converge
to a Markov process with effective rate matrices QÃÉ1 = Q1 ,
and QÃÉ3|1 given by
!
1 ‚àí19 19
2|1 3|2
2|1 3|2
QÃÉ3|1
=
œÄ
Q
+
œÄ
Q
=
¬∑|0
0|0 ¬∑|0
1|0 ¬∑|1
5 24 ‚àí24
!
1 ‚àí21 21
3|1
2|1 3|2
2|1 3|2
QÃÉ¬∑|1 = œÄ0|1 Q¬∑|0 + œÄ1|1 Q¬∑|1 =
.
5 26 ‚àí26

The total effective matrix is
Ô£∂
Ô£´
1
3.75
0 Ô£∑Ô£∑
Ô£¨Ô£¨‚àí4.75
Ô£¨Ô£¨ 2
‚àí6.25
0
4.25 Ô£∑Ô£∑Ô£∑Ô£∑
Ô£¨
QÃÉ = Ô£¨Ô£¨Ô£¨
Ô£∑,
0
‚àí5.75
1 Ô£∑Ô£∑Ô£∑
Ô£¨Ô£¨ 4.75
Ô£≠
Ô£∏
0
5.25
2
‚àí7.25
with a lexicographic ordering of the states.
To test the accuracy of the procedure we have generated
paths of length T = 50000 and used standard maximum
likelihood to estimate the rate matrix, assuming that the
process (X1 (t), X3 (t)) is a Markov process. For  = 0.05
we obtained
Ô£´
Ô£∂
3.7965
0 Ô£∑Ô£∑
Ô£¨Ô£¨‚àí4.7907 0.9942
Ô£¨Ô£¨ 1.9958 ‚àí6.1869
0
4.1911 Ô£∑Ô£∑Ô£∑Ô£∑
Ô£¨Ô£¨
QÃÉ=0.05
=
Ô£¨
Ô£∑,
est.
Ô£¨Ô£¨ 4.8004
0
‚àí5.8066 1.0062 Ô£∑Ô£∑Ô£∑
Ô£¨Ô£≠
Ô£∏
0
5.1638
1.9886 ‚àí7.1524
i.e., deviations of about one percent. For  = 0.2 we obtained
Ô£´
Ô£∂
3.8304
0 Ô£∑Ô£∑
Ô£¨Ô£¨‚àí4.8259 0.9955
Ô£¨Ô£¨Ô£¨ 2.0033 ‚àí6.2026
0
4.1993 Ô£∑Ô£∑Ô£∑Ô£∑
Ô£¨
QÃÉ=0.2
Ô£∑,
est. = Ô£¨
Ô£¨Ô£¨ 4.8555
0
‚àí5.8644 1.0089 Ô£∑Ô£∑Ô£∑
Ô£¨Ô£≠
Ô£∏
0
5.2026
1.9936 ‚àí7.1962
i.e., deviations of about two percent. In either case the main
source of error seems to be statistical. The fact that parameter estimation converges as T ‚Üí ‚àû is by itself not
surprising, since the whole process is ergodic. The question is to what extent is the reduced process (X1 (t), X3 (t))
Markovian? To test this we re-evaluated the entries of the
rate matrix conditional on the preceding state. This yielded
changes of about one percent, which again, could be attributed to a lack of statistics.
Example 5.2 Consider next Example 4.4 with S j = {0, 1},
j = 1, . . . , 6, and conditional rate matrices,
!
!
‚àí1 1
‚àí2 2
Q1 =
Q2 =
2 ‚àí2
1 ‚àí1
!
!
‚àí2 2
‚àí3 3
3|1
3|1
Q¬∑|0 =
Q¬∑|1 =
3 ‚àí3
2 ‚àí2
!
!
‚àí3 3
‚àí4 4
4|2,3
4|2,3
Q¬∑|(0,0) =
Q¬∑|(1,0) =
4 ‚àí4
3 ‚àí3
!
!
‚àí1 1
‚àí2 2
4|2,3
Q4|2,3
=
Q
=
¬∑|(0,1)
¬∑|(1,1)
2 ‚àí2
1 ‚àí1
!
!
‚àí1 1
‚àí3 3
5|3
5|3
Q¬∑|0 =
Q¬∑|1 =
3 ‚àí3
1 ‚àí1
!
!
‚àí1 1
‚àí4 4
6|4
6|4
Q¬∑|0 =
Q¬∑|1 =
.
4 ‚àí4
1 ‚àí1
The slow component X5 is only influenced by the fast component X3 , whose conditional equilibrium distribution only

depends on its slow ancestor X1 :
!
!
1 3
1 2
3|1
3|1
œÄ¬∑|0 =
œÄ¬∑|1 =
.
5 2
5 3
As  ‚Üí 0 the conditional rate matrix associated with X5
converges to
!
1 ‚àí9
9
3|1 5|3
3|1 5|3
5|1
QÃÉ¬∑|0 = œÄ0|0 Q¬∑|0 + œÄ1|0 Q¬∑|1 =
5 11 ‚àí11
!
1 ‚àí11 11
3|1 5|3
3|1 5|3
=
Q
+
œÄ
Q
=
œÄ
QÃÉ5|1
.
1|1 ¬∑|1
0|1 ¬∑|0
¬∑|1
‚àí9
5 9
To test the reduction procedure, we generated a trajectory
of the full process with 107 transitions and  = 0.05. We
used maximum likelihood to estimate the rates associated
with the reduced process that only consists of the slow
components X1 , X2 , X5 , X6 . For example, we estimated the
rates associated with X5 transitioning from 0 to 1; one can
estimate these rates assuming that they depend on the state
of all other (slow) components. The estimation procedure
gives
5|1,2,6
q0,1|(0,0,0)
= 1.78

5|1,2,6
q0,1|(1,0,0)
= 2.19

5|1,2,6
q0,1|(0,1,0)
= 1.77

5|1,2,6
q0,1|(1,1,0)
= 2.20

5|1,2,6
q0,1|(0,0,1)
= 1.77

5|1,2,6
q0,1|(1,0,1)
= 2.18

5|1,2,6
q0,1|(0,1,1)
= 1.80

5|1,2,6
q0,1|(1,1,1)
= 2.18.

These results are in very good agreement with our prediction that the rates in the left column be equal to 9/5 = 1.8
and the rates in the right column to 11/5 = 2.2, irrespectively of the values of X2 and X6 .
5|1
Finally, we show in Table 1 the estimated values of qÃÉ0,1|0

and qÃÉ5|1
for various values of . The case  = 1 means
0,1|1
that all components evolve on comparable time scales, in
which case the reduction procedure does not hold. This
table confirms the intuitive feeling that the approximation
gets reasonably accurate for  of the order of 0.1. Note that
the estimate of qÃÉ5|1
for  = 0.025 is less accurate than for
0,1|1
 = 0.05; this is attributed to the fact that the smaller , the
less (relatively) probable it is to observe a transition in the
slow variables, resulting in poor statistics.

6

Discussion

In this paper we have proved a theorem about dimension
reduction of CTBNs in the limit of an infinite separation
of scales between fast and slow components. We showed
the implications of this theorem for constructing a reduced
CTBN that captures the dynamics of the slow components
without explicitly dealing with the fast ones.
Our results show that the elimination of fast components
has a counter intuitive property. The typical intuition is that


1
0.5
0.25
0.1
0.05
0.025
‚Üí0

qÃÉ5|1
0,1|0
1.638
1.700
1.742
1.766
1.782
1.796
1.800

qÃÉ5|1
0,1|1
1.875
1.992
2.077
2.145
2.189
2.174
2.200

Table 1: Estimated values of entries of the (effective) conditional rate matrix QÃÉ5|1 for various values of . The maximum likelihood estimation was applied to trajectories (of
the full system) with 107 transitions.

integrating out a variable introduces dependencies among
its children. However, when eliminating fast components
this intuition does not apply directly, and in some cases we
end with a simpler CTBN than we started with. Thus, our
reduction leads to further simplifications than one might
expect from basic intuitions about Bayesian networks.
In practice, one seldom encounters systems in which a
small parameter  is explicitly given. In many applications
there exists a range of characteristic rates, and one has to
verify to what extent the dimension reduction is a good approximation. Since equilibration is exponentially fast, dimensional reduction is expected to be a good approximation when the equilibration rates associated with a subset
of components are larger, by at least an order of magnitude, than the equilibration rates associated with the other
components.
To put the results we introduce here to use we need to develop them into concrete approximation algorithms. The
appeal of such results is that they give us a strategy to use
separation of scales to reason about the system at different levels of time granularity. For reasoning about coarse
time scales, our results allow to reduce the system to examine only the slow components. To reason about fine time
scales we can then assume that most of the slow components are fixed, and then reason about the dynamics of the
fast components. Clearly this intuition can be extended to
a hierarchy of time scales.
Given a CTBN, we can assess the characteristic equilibration rate of each conditional Q-matrix by computing the
absolute value of its second largest eigenvalue. There are,
however, multiple ways of using these values to separate
the system into an approximate hierarchy of scales. Another issue deals with evidence. Clearly, once we find a
reduced CTBN we can incorporate evidence and reason
about the posterior probability of slow components and
consequently fast components. However, it is also fairly
clear that the frequency of observations and the time scale
of the observed variables can make important impact on the
approximation.

The results we presented here provide solid foundations for
introducing scale-based approximation in real applications.
Clearly, these initial results are only the first step in the development of promising approximate inference procedures.

fast |Islow
ga,b = œÄIFast(b)|
Œ¥
.
Slow(b) Slow(a),Slow(b)

Acknowledgments
Nir Friedman was supported in part by grants from the Israel Science Foundation (ISF) and from the Binational USIsrael Science Foundation (BSF). Raz Kupferman was supported in part by a grant from the Israel Science Foundation
(ISF).

K.L. Chung. Markov chains with stationary transition
probabilities. Springer Verlag, Berlin, 1960.
C.W. Gardiner. Handbook of stochastic methods. SpringerVerlag, New-York, third edition, 2004.
I.I Gikhman and A.V. Skorokhod. The theory of Stochastic
processes II. Springer Verlag, Berlin, 1975.
D. Givon, R. Kupferman, and A.M. Stuart. Extracting
macroscopic dynamics: model problems and algorithms.
Nonlinearity, 17:R55‚ÄìR127, 2004.
U. Nodelman, C.R. Shelton, and D. Koller. Continuous
time Bayesian networks. In Eighteenth Conference on
Uncertainty in Artificial Intelligence, pages 378‚Äì387,
2002.
U. Nodelman, C.R. Shelton, and D. Koller. Learning continuous time Bayesian networks. In Nineteenth Conference on Uncertainty in Artificial Intelligence, pages
451‚Äì458, 2003.
N.G. van Kampen. Elimination of fast variables. Phys.
Rep., 124:69‚Äì160, 1985.

Proof sketch of Theorem 3.1

Using the partition of the Q-matrix as the sum of fast and
slow components, the master equation (1) takes the form
1
d
p ‚àí (Qfast )T p = (Qslow )T p.
dt

If we treat the right hand side as an inhomogeneous term,
this equation can be integrated, resulting in an integral
equation,
p(t) = e

As  ‚Üí 0, the distribution p(t) tends to the solution of the
limiting equation,
Z t
T
GT (Qslow )T p(s) ds,
p(t) = G p(0) +
0

which is equivalent to the differential equation,

