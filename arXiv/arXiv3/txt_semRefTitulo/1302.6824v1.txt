
We present an approach to the solution of de­
cision problems formulated as influence dia­
grams.

This approach involves a special tri­

angulation of the underlying graph, the con­
struction of a junction tree with special prop­

tials normalized. Ndilikilikesha (Shachter and Ndiliki­
likesha, 1993; Ndilikilikesha, 1994) modified the node­
removal/ arc-reversal algorithm to avoid these extra di­
visions; the result is an algorithm that is equivalent to
Shenoys algorithm with respect to computational effi­
ciency.

erties, and a message passing algorithm op­

Our work builds primarily on the work of Shenoy

erating on the junction tree for computa­

(1992) and Shachter and Peat {1992), in addition

tion of expected utilities and optimal decision

to our previous work on propagation algorithms for

policies.

the expert system shell Hugin (Andersen et al., 1989;
Jensen et al., 1990) .

1

INTRODUCTION

Influence diagrams were introduced by Howard and
Matheson (1981) as a formalism to model decision

2

INFLUENCE DIAGRAMS

An

influence diagram is a belief network augmented

problems with uncertainty for a single decision maker.

with decision variables and a utility function.

The original way to evaluate such problems involved

The structure of a decision problem is determined by

unfolding the influence diagram into a decision tree
and using the "average-out and fold-back" algorithm
on that tree. Shachter (1986) describes a way to eval­

uate an influence diagram without tranforming it into
a decision tree. The method operates directly on the
influence diagram by means of the node-removal and
arc-reversal operations. These operations successively
transform the diagram, ending with a diagram with
only one utility node that holds the utility of the op­
timal decision policy; the policies for the individual
decisions are computed during the operation of the
algorithm (when decision nodes are removed).
Shenoy (1992) describes another approach to the eval­
uation of influence diagrams: the influence diagram is
converted to a valuation network, and the nodes are
removed from this network by fusing the valuations
bearing on the node (variable) to be removed. Shenoys
algorithm is slightly more efficient than Shachters al­
gorithm in that it maintains a system of valuations,

an acyclic directed graph G. The vertices of G repre­
sent either random variables (also known as chance or
probabilistic variables) or decision variables, and the
edges represent probabilistic dependencies between
variables. Decision variables represent actions that are
under the full control of the decision maker; hence, we
do not allow decision variables to have parents in the
graph.
Let

UR

be the set of random variables, and let the

set of decision variables be

Uo

=

{0 1, .

.

.

, Dn},

with

the decisions to be made in the order of their index.
Let the universe of all variables be denoted by U

UR U Uo.

sets lo, . ..

We partition

, In;

for

0<

UR

=

into a collection of disjoint

Ik is the set of variables
Dk and Dk+ 1;
variables, and In is the set

k <

n,

that will be observed1 between decision

Io is the initial evidence

of variables that will never be observed (or will be
observed after the last decision). This induces a partial
order..:: on

U:

whereas Shachters algorithm maintains a system of
conditional probability functions (in addition to the
utility functions), and some extra work (some division
op erations) is required to keep the probability paten-

1 By

'observed,'

will be revealed.

we

mean that the true state of the variable

368

Jensen, Jensen, and Dittmer

We associate with each random variable A a condi­
tional probability function ¢A=P(AI:PA), where 'J'A
denotes the set of parents of A in G .
The state space Xv for V � U i s defined a s the Carte­
sian product of the sets of possible outcomes/decision
alternatives for the individual variables in V. A po­
tential ¢v for a set V of variables is a function from
Xv to the set of real numbers.
The potential <Pv can be extended to a potential

¢w (V � W) by simply ignoring the extra variables:
¢w(w) =¢v(v) ifv is the projection ofw on V.
Given two potentials, <P and tf!. The product ¢ * tV
and the quotient ¢/tf! are defined in the natural way,
except that 0/0 is defined to be 0 (x/0 for x -1= 0 is
undefined) .
The (a priori) joint probability function <Pu is defined
as
¢A·
<Pu
=

IT

AEUR

For each instance of Uo (i.e., each element of
<Pu defines a joint probability function on UR.

Xu0 ),

A solution to the decision problem consists of a series
of decisions that maximizes some objective function.
Such a function is called a utility function. Without
loss of generality, we may assume that the utility func­
tion tfJ is a potential that may be written as a sum of
(possibly) simpler potentials:

The independence restriction imposed on the decision
problem can be verified by checking that, in the influ­
ence diagram, there is no directed path from a deci­
sion Ok to a decision Di (i < k).
3

DECISION MAKING

Assume we have to choose an alternative for deci­
sion On (i.e., the last decision). We have already
observed the random variables 10, . . . , In-1, and we
have chosen alternatives for decisions D1 , ..., 0 n-1.
The maximum expected utility principle2 says that
we should choose the alternative that maximizes the
expected utility. The maximum expected utility for
decision Dn is given by

Pn =max L P(Inllo, · · ·, In-1, 01, ..., Dn) * tf!.
On

I,

Obviously, Pn is a function of previous observations
and decisions. We calculate the maximum expected
utility for decision Dk (k < n) in a similar way:

Pk =Df,ax L P(Ikllo, ... , Ik-1, o, . . . , Dk) *Pk+1·
k

I<

We note that Pk is well-defined because of ( 1).
By expansion of ( 2 ) , we get

Pk =max L P(Ikllo,... , Ik-1, D 1, ... , Dk)
Dk

m

Ik

*max
Dk+l

=max
We need to impose a restriction on the decision prob­
lem, namely that a decision cannot have an impact on
a variable already observed. This translates into the
property

P(Ikllo, ... , Ik-1, D1, ... , On)
=P(Ikllo, ... ,Ik-1.01, ... ,0kl·

(1)

In words: we can calculate the joint distribution for I k
without knowledge of the states of Dk+ 1, ... , Dn (i.e.,
the future decisions).
2.1

GRAPHICAL REPRESENTATION

In Figure 1, an example of an influence diagram is
shown. Random variables are depicted as circles, and
decision variables are depicted as squares. Moreover,
each term of the utility function is depicted as a dia­
mond, and the domain of the term is indicated by its
parent set. The partial order-< is indicated by making
I k-1 the parent set of D k, and we shall use the con­
vention that the temporal order of the decisions are
read from left to right.

(2 )

o.

L
h+•

P(Ik+11Io, ... ,Ik,
D1, ... ,0k+1 ) *Pk+2

L max L P(Ik, Ik+11Io, ... , Ik-1,
l•

Dk+l

Ik+l

D1, ... , 0k+1) *Pk+2·

The last step follows from ( 1) and the chain rule of
probability theory: P(AjB,C)P(BIC) =P(A,BIC). By
further expansion, we get

Pk =max L ···max L P(Ik,..., Inllo, ... , Ik-1,
Dk h
Dn
DnJ * t!J.
D1,
In
•

• •

1

From this formula, we see that in order to calculate
the maximum expected utility for a decision, we have
to perform a series of marginalizations (alternately
sum- and max-marginalizations), thereby eliminating
the variables.
When we eliminate a variable A from a function ¢,
expressible as a product of simpler functions, we par­
tition the factors into two groups: the factors that
involve A, and the factors that do not; call (the prod­
uct of) these factors ¢ :t_ and ¢A, respectively. The
marginal LA¢ is then equal to <PA: *LA¢:t_; LA <Pt
2There are good arguments for adhering to this principle.
See, e.g.,

(Pearl, 1988).

From Influence Diagrams to Junction Trees

369

FIGURE 1.
An influence diagram for a decision problem with four decisions. The set of variables is partitioned into the

sets:

I0

=

{b}, I 1

=

{ e, f}, Iz

=

0,

I3 = {g}, and I4 =

{a, c, d, h, i, j, k, e}.

four local utilities, three of which are associated with single variables (D 1 ,
the pair

(j, k).

then becomes a new factor that replaces the prod­
uct

cj:J:t_ in the expression for ¢1.

This also holds true for

max-marginalizations, provided

cPA.

does not assume

negative values.

The utility function is a sum of

D3, and €), and one associated with

marginalizations; but- in general- we cannot inter­
change the order of a max- and a sum-marginalization;
this fact imposes some restrictions on the elimination
order.

The product cjJ may be represented by an undirected
graph, where each maximal complete set (clique) of
nodes (the nodes being variables) corresponds to a fac­

4

INFLUENCE DIAGRAMS

tor (or a group of factors) of cjJ with that set as its do­
main. Marginalizing a variable

A out of ¢1 then corre­

COMPILATION OF

We first form the moral graph of G. This means adding

sponds to the following operation on the graph: the set

(undirected) edges between vertices with a common

A is

child. We also complete the vertex sets corresponding

of neighbors of A in the graph is completed, and

removed. It is a well-known result that all variables

to the domains of the utility potentials. Finally, we

can be eliminated in this manner without adding edges

drop directions on all edges.

if and only if the graph is triangulated (Rose, 1970).

Next, we triangulate the moral graph in such a way

Obviously, it is desirable to eliminate all variables

that it facilitates the computation of the maximum

without adding extra edges to the graph since this

expected utility. This is equivalent to the selection of

means that we do not create new factors with a larger

a

domain than the original factors (the complexity of

verse of the elimination order must be some extension

representing and manipulating a factor is exponen­

of -< to a total order.

tial in the number of variables comprising its domain).
However, in most cases, this is not possible: we have
to add some edges, and the elimination order chosen

special elimination order for the moral graph: the re­

Finally, we organize the cliques of the triangulated
graph in a strong junction tree:

A tree of cliques

optimal elimination order for all reasonable criteria of
optimality.

( C 1 , C2l of
C2 is contained in every clique on the
path connecting C1 and C2. For two adjacent cliques,
C1 and C2, the intersection C, n Cz is called a sepa­

W hen we perform inference in a belief network (i.e.,

least one distinguished clique R, called a strong root,

will determine how many and hence also the size of
the cliques.

Unfortunately, it is :N'Jl-hard to find an

is called a junction tree if for each pair

cliques,

C1

n

rator. A junction tree is said to be strong if it has at

(C1, Cz) of adjacent cliques in
C1 closer to R than C2, there exists
an ordering of C2 that respects -< and with the ver­
tices of the separator C1 n Cz preceding the vertices
of C2\C1 . This property ensures that the computation

calculation of the marginal probability of some vari­

such that for each pair

able given evidence on other variables), the computa­

the tree, with

tion only involves sum-marginalizations. In this case,
we can eliminate the variables in any order, since the
order of two marginalizations of the same kind can be
interchanged.

However, the calculation of the max­

imum expected utility involves both max- and sum-

of the maximum expected utility can be done by local
message passing in the junction tree (see Section 5).

370

Jensen, Jensen, and Dittmer

that no two cliques have the same index. Moreover,
unless index( C) = 1, the set

{ vE C I !X(v) <index( C)}

will be a proper subset of some other clique with a

lower index than

C.

Let the collection of cliques of the triangulated graph
be

C1, ... , Cm, ordered in

increasing order according

to their index. As a consequence of the above construc­
tion, this ordering will have the running intersection
property (Beeri et

al., 1983),

meaning that

k-1
for all

FIGURE 2.
The moral graph for the decision problem in Figure 1.
Edges added by the moralization process are indicated
by dashed lines.

k

>

1: Sl<

=

C 1< n

U

Ci � C;

for some

i=1

j < k.

It is now easy to construct a strong junction tree: we
start with
each clique

C1 (the root); then we successively attach
Ck to some clique C; that contains Sl<.

Consider the decision problem in Figure 1. Figure

2

shows the moral graph for this problem: edges have

been added between vertices with a common child (in­
cluding utility vertices), utility vertices have been re­
moved, and directions on all edges have been dropped.
Note that the time precedence edges leading into de­
cision vertices are not part of the graph and are thus
not shown.
Figure

shows the strong triangulation of the graph

3

2 generated by the elimination sequence e,
j, k, i (fill-ins: D2
04 and g
04), lt (fill-in:
f
03), a, c (fill-in: b
e), d (fill-ins: 01
e,
01 - f, b f, and e
f), 04, g (fill-in: e --. Oz),
03, 02, f, e, 01, and b. This graph has the fol­
lowing cliques: C16
{04,i,e}, C,s = {lt,k,j},
C14 = {03,lt,k}, C11
{b,c,a}, C10
{b,e,d,c},
Cs
{Oz, g, 04, i}, C6
{f, 03, lt}, Cs
{e, 02, g},
and C1 = {b,O,e,f,d}. Using the above algorithm,
in Figure

�

�

�

�

�

FIGURE 3.

�

�

=

The triangulated graph of the moral graph in Figure 2.
Fill-in edges added during triangulation are indicated
by dashed lines.

=

=

=

=

=

we get the strong junction tree shown in Figure

4

for

this collection of cliques. (There exists another strong

4.1
Let

CONSTRUCTION OF STRONG JUNCTION TREES

!X be

a numbering of

{1, ... , lUI})

U

such that for all

plies !X(u) <

!X(v).

!X: U H
U, u -< v im­

(i.e., a bijection

u,v

E

We assume that !X is the elimi­

nation order used to produce the triangulated graph

of G: vertices with higher numbers are eliminated be­
fore vertices with lower numbers.
Let C
vE C

the edge

Cs

--1

C1

by the edge

Cs

--1

C10.

This tree is

computationally slightly more efficient, but- unfor­
tunately- it cannot be constructed by the algorithm
given in this paper.)
In general, previous observations and decisions will be
relevant when making a decision. However, sometimes
only a subset of these observations and decisions are

be a clique of the triangulated graph, and let
be the highest-numbered vertex such that the

{wE C I !X(w) < !X(v)} have a common neigh­
bor u (j. C with !X(u) < !X(v). If such a vertex v exists,
we define the index for Cas index( C)
!X(v); other­
wise, we define index( C)
1. Intuitively, the index for
a clique C identifies the step in the elimination process
that causes C to "disappear" from the graph. It is easy
to see that the index for a clique C is well-defined, and
vertices

=

=

junction tree for this collection, obtained by replacing

needed to make an optimal decision. For example, for
the decision problem in Figure 1, the variable

e

sum­

marizes all relevant information available when deci­
sion

02

has to be made: although

before decision

0 2,

f

is observed just

it has no relevance for that deci­

sion (it does, however, have relevance for decision 03).
This fact is detected by the compilation algorithm: the
only link from 02 to past observations and decisions
goes to

e.

371

From Influence Diagrams to Junction Trees

FIGURE 4. A strong junction tree for the cliques of the graph in Figure 3.
5

USING THE STRONG JUNCTION TREE

Now, letT be a strong junction tree, and let C1 and C2

FOR COMPUTATIONS

be adjacent cliques with separator S inT. We say that

We perform computations in the junction tree as a spe­
cial'collect' operation from the leaves of the junction

C1 absorbs from Cz if
and tVc as follows:

Q:Jc,

and tVc, change to

,

cPc,

tree to some strong root of the tree.
To each clique

C in the junction tree, we associate a

probability potential QJc and a utility potential tVc
defined on

Xc.

Let e be the set of cliques. We define

where

the joint potentials q, and tV for the junction tree as

tVs

=

M

Cz\S

cPCz *tV c z

·

Note that this definition of absorption is 'asy mmetric'
We initialize the junction tree as follows: each variable

A E U R is assigned to a clique that contains A U PA.
T he probability potential for

a

clique is the product of

the conditional probability functions for the variables
assigned to it. For cliques with no variables assigned
to them, the probability potentials are unit functions.
In this way, the joint probability potential for the junc­
tion tree becomes equal to the joint probability func­
tion for the influence diagram. Similarly, each utility

in the sense that information only flows in the direc­
tion permitted by the partial order -<:. It is possible
to generalize this definition of absorption to a sym­
metric definition similar to the one given in (Jensen
et

al.,

1990} for the case of pure probabilistic influence

diagrams.
Clearly, the complexity of an absorption operation is

O{I Xc,l + IXsl + 1Xc21).

Note in particular that the
contribution from the division operation plays a much
smaller role than in (Shenoy, 1992), since division op­

function tVk is assigned to some clique that can ac­
commodate it. The utility potential for a clique is the

erations are performed on separators only.

sum of the utility functions assigned to it; for cliques

We will need the following lemma, which we shall state

with no utility functions assigned to them, the utility

without proof.

potential is a null function.
We need a generalized marginalization operation that
acts differently on random and decision variables. We
denote the operation by

'M'

.

For random variable

A

and decision variable D, we define

M q,
D

For a set

V of variables,

we define

=

maxQ:l.
D

Mv q,

as a series of

single-variable marginalizations, in the inverse order
as determined by the relation -<:. Note that although
-<: is only a partial order,

Mv q,

is well-defined.

Lemma 1 Let D be a decision variable, and let

V

a set of variables that includes all descendants of
in G .
of

D

Then

Mv\{ D} Q:lu,

be

0

considered as a function

alone, is a non-negative constant.

Let T be a strong junction tree with at least two
cliques; let

cPT

be the joint probability potential and

tVT the joint utility potential on T. Choose a strong
root R for T and some leaf l (=I R); let T \ l denote
the strong junction tree obtained by absorbing l into

its neighbor N and removing l; denote the separator
between N and l by S.

372

Jensen, Jensen, and Dittmer

Theorem 1 After absorption of l into T, we have

M <Pr

tl>r = <PT\L

*

*

(2) Xk

is a decision variable. By induction, we get

tVT\L·

L\S
Proof: Let

<iJL

IJ

=

1
Because of Lemma 1, <P ( k+ l, considered as a

<Pc;

function of xk alone, is a non-negative constant,

CEe\{L}

and we get
Since <PL does not assume negative values, we get

(mxax<P(k+1l)

*

,

L\S

L\S

(

maxx,

tj>lk+1l

maxx. <j)lk+ 1 J

- .+,(k)
-�

*

We have to show that

M <PL
L\S

*

(ti>L + lj}L)

=

<Ps

*

(�5�S

+ 'iJL

),

+\i))

( tj>(k)
<tJ(k)

L

+

::1:

�L

)

·

I

By successively absorbing leaves into a strong junction
tree, we obtain probability and utility potentials on
the intermediate strong junction trees that are equal
to the marginals of the original potentials with respect

where

to the universes of these intermediate trees.

This is

ensured by the construction of the junction tree in
which variables to be marginalized out early are lo­
We shall prove this by induction. Let X1

,

... , Xt be

some ordering of l \ S that respects -<. Now, consider

the equation:

cated farther away from the root than variables to be
marginalized out later.

The optimal p olicy for a decision variable can be deter­

mined from the potentials on the clique that is closest

to the strong root and contains the decision variable
(that clique may be the root itself), since all variables
that the decision variable may depend on will also be
members of that clique.

where

For our example decision problem (Figure 4), we can
determine the optimal policy for 01 from {the poten­
tials on) clique C 1 (the root), and the optimal policies
for the remaining decisions can be determined from

(For k = 1, (3) is equivalent to the desired result.) For
k > e, (3) is clearly true; for 1 :s k :s e, we have two

cases:

(1) Xk

cliques Cs (decision 02), C6 (decision 03), and Cs

(decision 0 4).

If only the maximum expected utility is desired, it
is a random variable. By induction, we get

should be noted that only storage for the 'active' part
of the junction tree during the collect operation needs
to be reserved; this means that storage for at most
two adjacent cliques and each clique that corresponds
to a branch point on the currently active path from

the root to a leaf

must be reserved. Since elimination

of a group of variables can be implemented more effi­

ciently than the corresponding series of single-variable
eliminations, it is still useful to organize the computa­
tions according to the structure of the strong junction
tree as compared to (Shenoy,

The correctness of the last step follows from the
fact that

<P(kl (x)

=

0 implies tj>lkl (x)

=

0 (so that

our division-by-zero convention applies).

6

1992).

CONCLUSION

We have described an algorithm to transform a deci­
sion problem formulated as an influence diagram into a

From Influence Diagrams to Junction Trees

secondary structure, a strong junction tree, that is par­
ticularly well-suited for efficient computation of max­

networks by local computations.
4:269-282.

373

Computational

Statistics Quarterly,

imum expected utilities and optimal decision policies.
The algorithm is a refinement of the work by Shenoy
{1992) and Shachter and Peot {1992); in particular,
the construction of the strong junction tree and its
use for computations has been elaborated upon.

Kjcerulff, U. {1990). Triangulation of graphs-algo­
rithms giving small total state space. Research
Report R-90-09, Department of Mathematics and
Computer Science, Aalborg University, Denmark.

T he present work forms the basis for an efficient com­
puter implementation of Bayesian decision analysis in
the expert system shell Hugin (Andersen et al., 1989).

Lauritzen, S. L. and Spiegelhalter, D. J. (1988). Lo­
cal computations with probabilities on graphical
structures and their application to expert systems.

We have not given an algorithm to construct the elim­
ination sequence that generates the strong triangula­
tion. However, the triangulation problem is simpler
than for ordinary probability propagation, since the
set of admissible elimination sequences is smaller; at
this stage, it appears that simple adaptations of the
heuristic algorithms described by Kjcerulff (1990) work
very well. Moreover, even given a triangulation, there
might exist several strong junction trees for the collec­
tion of diques.
Besides the use of the strong junction tree for compu­
tation of expected utilities and optimal decision poli­
cies, it should be possible to exploit the junction tree
for computation of probabilities for random variables
that only depend on decisions that have already been
made. Ideally, this should be done through a 'dis­
tribute' operation from the root towards the leaves of
the junction tree.
Work regarding these problems is in progress.
Acknowledgements

This work has been partially funded by the Danish
research councils through the PIFT programme.
