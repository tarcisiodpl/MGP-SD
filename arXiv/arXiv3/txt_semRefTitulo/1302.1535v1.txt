

We present a method for calculation of my­
opic value of information in influence dia­
grams (Howard & Matheson, 1981) based on
the strong junction tree framework (Jensen
et al., 1994).
An influence diagram specifies a certain or­
der of observations and decisions through its
structure. This order is reflected in the corre­
sponding junction trees by the order in which
the nodes are marginalized. This order of
marginalization can be changed by table ex­
pansion and use of control structures, and
this facilitates for calculating the expected
value of information for different information
scenarios within the same junction tree. In
effect, a strong junction tree with expanded
tables may be used for calculating the value
of information between several scenarios with
different observation-decision order.
We compare our method to other methods
for calculating the value of information in in­
fluence diagrams.
Influence diagrams, value of in­
formation, strong junction tree, table expan­
sion, dynamic programming.

Keywords:

1

INTRODUCTION

Influence diagrams were introduced by Howard &
Matheson ( 1981) as a formalism to model decision
problems with uncertainty for a single decision maker.
An influence diagram can be considered a Bayesian
network augmented with decision variables and a util­
ity function. The decision variables, D1, . . . , Dn, in
the influence diagrams are partially ordered and the
chance variables are divided into information sets, !0 ,

... , In· The information set !;_1 is observed immedi­
ately before decision D; is made, and the information
set In consists of the chance variables that are observed
later than the n'th decision is made, if ever.

Let v; be the set of variables preceding D;, t hat is,
v; contains the past relevant for D;. The solution
of a decision problem modeled by an influence dia­
gram is a sequence of decisions that maximizes the
expected utility. Shachter (1986) describes a method
to solve an influence diagram without unfolding it
into a decision tree; rather, the influence diagram
is transformed through a series of node-removal and
arc-reversal operations. Shenoy (1992) describes an­
other approach to the problem of solving influence dia­
grams by conversion into valuation networks. This ap­
proach is slightly more efficient than that of (Shachter,
1986). (Shachter & Ndilikilikesha, 1993) and (Ndiliki­
likesha, 1994) modified the node-removal/arc-reversal
algorithm and achieved a method that is equivalent to
the algorithm presented in (Shenoy, 1992) with respect
to computational efficiency.
Jensen et al. (1994) describes an efficient method for
solving influence diagrams using strong junction trees.
This is an extension to the junction trees used for com­
putation in pure Bayesian decision analysis. It is on
this framework we base the present work.
We are about to choose among a set of k options.
These options are packed into the decision node D. We
have already received some information e, and now we
can either choose among the options or we can look for
more information. The 'looking for more information'
is to consult some source which will provide the state
of a chance variable. Let the chance variables in ques­
tion be the set r = {AlI ... 'Am}. We want to calcu­
late what we can expect to gain from consulting the
information source. For all the considerations in this
paper we deal with the myopic value-of-information
question: At any time, we can ask for the state of at
most one of the variables in r.

Myopic Value of Information in Influence Diagrams

As basis for the considerations we have
expected utilities for

D

EU(Die),

143

the

given the evidence e, and the

decision d of maximal expected utility is chosen. If
A; E r is observed to be in state a, then EU(Die, A; =
a ) is the new basis. Now, before observing A; we have
probabilities P(A; le), and the expected utilities of the
optimal action after having observed A; is

EUO(A;, Die)= LP(A;]e)· mgxEU(Die, A;)

F igure

The

value of observing A;

1:

The scenario with one non-intervening deci­

sion node.

A;

is the difference

VOI(A;, Die)= EUO(A;, Die)- maxEU(Die)

For this scenario we have

D,

Value of information is a core element in decision anal­
ysis, and a method for efficient calculation of myopic
value of information in Bayesian networks (augmented

VOI(A;, Die)

(L:H P(HIA;, e)· U(D, H))
-mgx(� P(Hie) · U(D,H))

with a utility function) is described by {Jensen &
Jiangmin 1.,

1995).

Also, (Beckerman et al.,

= LP(Ade)

1992)

·

max

A,

describes a method for calculating the utility-based
myopic value of information.
Methods for computing the value of information in
influence diagrams have been described by {Ezawa,

1994)

based on the arc-reversal/node-removal meth­

ods. (Poh & Horvitz,

1996) approach a notion of qual­

itative value of information through graph-theoretic
considerations yielding a partial order of the chance
nodes in the model.

For

the

P(HIA;, e)

calculation

of

for all variables

VOI(A;, Die)
A; in r. These

The value of information can be viewed as the dif­
differing in the observation-decision sequence in the
influence diagram. We present a single-model frame­
work for calculating the exact value of information of
a chance node.

need

probabilities can be achieved through entering and
propagating each state of

A;.

Using Bayes' rule, the

requirement is transformed to a need for

ference in expected value between two models only

we

conditional

all

A;

P(A; IH, e) for

in r. They can be achieved all by entering and

propagating the states of

H.

So, the number of propa­

gations necessary for solving the value-of-information
task for this scenario is the minimum of the number of
states of
in r.

H and

the sum of the states of the variables

For the considerations in this paper, the network is of
considerable size so that a propagation in the network
is a heavy (but feasible) task.

2.2

THE NUMBER OF H IS LARGE

This means that the

methods presented shall be evaluated in the light of

The assumptions in Section

their propagation demand.

we would like to relax them. Often

2.1

are very crude and

D has an impact
P(H/D). Also,

on H and in that case we will need

2

the number of states of

SIMPLE SCENARIOS

We shall first describe a couple of simple scenar­
ios which have efficient solutions.

The first scenario

is standard and has been treated more detailed by
(Jensen,

H

as well as the sum of all

states of r may be very large

1996).

(H may be

propagations (see Figure

2).

The following method reduces the number of propa­
gations to the number of states in

D.

is a modification of a trick by Cooper
2.1

ONE NON-INTERVENING DECISION

There is one decision node

D

which has no impact

on any of the chance nodes in the model. The utility
function

U is a function of D

and the chance variable H

which may actually be a set of variables (see Figure

1).

a large set of

variables), and we will look for methods requiring less

The method

(1988).

The

utility function is transformed to a normalized util­
ity NV through a linear transformation such that

0 S NU S 1. NU

is represented in the influence dia­

gram by a binary node NU with the argument vari­

ables H (which might include D) as parents
P(NU = yiH)= NU(H) (see Figure 3).

and with

144

Dittmer and Jensen

ENU(Die) can be
NU = y.

calculated by entering and propa­

gating

Now, let A be a variable in r. Assume that A is ob­

served to be in the state

a.

Then we have

ENU(Dia, e)
)
= P(NU:::: YID, a, e
) P(aiNU = y, D, e)
= P(NU = y ID'e .
P(aJD, e)
P(aiNU = y, D, e)
= ENU(Die).
P(aiD, e)
Figure

2:

A scenario where the method of Section

2.1

is inadequate.

and the expected normalized utility after observing A

is

2::A (maxn ENU(DIA, e))· P(AID, e ) .

The required probabilities

P(AID, e)

P(AINU = y, D, e)

and

can be achieved by entering and propagat­

ing the states of Din a network conditioned one and in

one conditioned on

(e, NU = y).

Hence, the number of

propagations required for this calculation is twice the
number of states in

D,

that is, with

2k

propagations

we can calculate the value of observation for all vari­

ables. It should be noted that there were no structural
assumptions for this result.

In most cases the information e as well as the variables

D are not descendants
P(AID, e) = P(Aie) and the

which may be observed prior to

Figure

3:

in Figure

The Cooper transformation of the scenario

2.

of

D. In

method only requires

3
The normalized value of information is defined as

NVOI(A;, Die)
=

A,

- �x
and

(L::H P(HIA;, D, e)· NU(D))
(L:: P(HID,e) NU(H) )

L P(A;Ie)

VOl

· mtx

·

H

can be calculated from

NVOI

transformation.

by the inverse

The expected normalized utility of a decision d, given

the evidence

e

can be calculated as

ENU LNU(H)
=

·

P(HJd,e)

H
=

L P (NU = yiH) · P(Hid,e)
H

=

L P(NU

=

y, Hid, e)

H

=

P(NU

=

D

The

propagations.

k

A SEQUENCE OF DECISIONS
next

scenario

to

consider

is

the

following:

We have a sequence of decisions and observations
Io, D1,

I1, . . . , Dn, In where each I; is a set of chance

variables

(In

observed).

is the set of variables which are never

The variables are structured in an influ­

ence diagram (see Figure

4

for an example). We are

in the middle of this sequence, we have observed /;-1

and are about to decide on

ther option of observing
Let

VOI(X, Di, ,jiVi)

one

(where

D;

but we have a fur­

variable of the set r.

(i

<

j)

denote the dif­

ference in maximal expected utility for

D;

between

observing chance node X immediately before deciding

D; and immediately before deciding on Dj. That is
VOI(X, Di,jJVi) denotes the difference between hav­
ing X in /;_1 and in Ij-1 at the time of deciding on
D;.
on

The standard dynamic programming technique for
solving an influence diagram is to perform a sequence

of marginalizations in reverse order (Shenoy,

Shachter & Peat,

yid, e)

Using Bayes' rule and giving

these cases

1992).

1992;

Chance nodes are marginal­

ized through a summation and decision nodes are

the even distribution,

maximized.

Since summation and maximization do

not commute, the order of marginalization is impor-

145

Myopic Value of Information in Influence Diagrams

Figure 5: A strong junction tree for the influence dia­
gram in Figure 4. The strong root is the c lique G0 at
the far left.
the chance variable B. The model for this observation­
decision sequence is shown in Figure 6.
Figure 4: An influence diagram with the observation­
decision sequence Dt, C, D2, {A,E}, Da, B. Note
that A and E may be observed in mutually arbitrary
order but both will be observed.

B r-------�

tant and it is performed in the following order: First
marginalize In (in any order), then D... , then In-1 (in
any order) , etc. When /; has been marginalized, we
have a representation of the expected utility of the
various options of D; given the past.
It is tempting to use this technique to condense the
future into a utility function over a subsei; of the cur­
rently unknown variables and the decision node D;
and to use this condensed future for the calculation of
value of information. However, the condensed future
contains max-expected-utility decisions, and observing
a variable from r may affect these decisions. This can
be avoided by assuming that the future is independent
of r given D; (and the past) . Such an assumption will
rarely hold, and instead we will introduce a technique
which does not have that kind of assumption.
In (Jensen et al., 1994) the junction tree technique is
used to solve influence diagrams. A so-called strong
junction tree is constructed with a so-called strong
root. This means that there is a clique Co such that
when a collect-operation to Co is performed, then all
marginalizations can be performed in the proper order
(see Figure 5). Note that the strong junction tree in
itself does not ensure that marginalizations are per­
formed in a proper order. When marginalizing in a
clique we need a control structure giving the order of
rnarginalizations. The " proper order" need not be the
reversed temporal order. It is sufficient that each vari­
able is eliminated in reverse temporal order with re­
spect to its Markov blanket. The Markov blanket of a
node X is the minimal set of nodes covering X from
influence from other nodes, that is, the Markov blan­
ket for node X consists of X 's parents, children, and
children's parents.
In Figures 4 and 5, B is not observed (or rather: B
is not observed until after the last decision is made).
Now, assume that before deciding on D1, we observe

Figure 6: An influence diagram with the observation­
decision sequence B, D1, C, D2, {A,E}, D3.
The difference in expected utility when solving the two
influence diagrams is VOI(B, D1, oolv;), that is, the
value of observing B before D1 rather than never ob­
serving B. The difference between the two scenarios
can be seen on the strong junction trees in Figures 7a
and 7b.

a

b

c

Figure 7: Strong junction trees for the two scenarios
of Figures 4 and 6, and a junction tree adequate for
both scenarios.
It is possible to construct a junction tree capable of

146

Dittmer and Jensen

solving both scenarios and in effect calculate the value
of information between the two information scenarios.
The crucial thing about a strong junction tree is that
it allows marginalization in a proper (reverse) tem­
poral order and this can be done for both temporal
orders in the strong junction tree shown in Figure 7c.
This strong junction tree is obtained from the junction
tree in Figure 7a by adding B to the cliques down to
(D1, C).
This observation can be used in general: To obtain a
strong junction tree with strong root Co for calculat­
ing VOI(A, D;, jiVi), construct a strong junction tree
for the scenario with A in Ij -1 · Then Co imposes a
(partial) order < for the cliques, such that C < C' if
and only if C is on the path from C' to C0. Identify
the cliques C; and CA . C; is the clique closest to the
Co containing D;, and CA is the clique closest to Co
containing D;. Let C;A be the "greatest lower bound"
of C; and CA . That is, C;A is the clique furthest away
from Co such that C;A < C; and C;A < CA (when the
temporal order is strict, then C;A = C;). Finally, ex­
tend all cliques on the path between C;A and CA with
the variable A.
As mentioned earlier, a control structure is associated
with the (strong) junction tree. This structure handles
the order of marginalization, and therefore we can use
the expanded junction tree (and the associated con­
trol structure) in Figure 7c to marginalize B from any
clique of our chaise. After B has been marginalized
from a clique, the table space reserved for B in cliques
closer to the strong root is obsolete. Clever use of
the control structures will prevent calculations to take
place in the remaining table expansions, and the num­
ber of table operations in the remaining subtree equals
that of an ordinary strong junction tree.
3.1

Figure 8: An influence diagram with temporal order
from left to right (no-forgetting arcs are not included) .
It discloses temporal independence between D3 and
{D2, D4}. (From (Jensen et al., 1994))

a

NON-STRICT TEMPORAL ORDERS

As mentioned previously, a proper elimination order
of an influence diagram is an order where the elimi­
nation order of each node and its Markov blanket is a
reverse temporal order. This means that although the
influence diagram in the offset requires a linear tem­
poral order of the decisions, then the actual diagram
may disclose temporal independencies which can be
exploited when solving it.
The influence diagram in Figure 8 has a temporal order
of the decision nodes with increasing index. However,
when f has been observed, then Da can be decided at
any time independently of the observations and deci­
sions on e, g, D2, and D4. This is also reflected in
the strong junction tree in Figure 9a where the branch
containing D3 can be marginalized independently of
the other branches.

b

Figure 9: Strong junction trees (derived from Figure 8
illustrating the difference between never observing h
(a) and observing h immediately before decision D2
(b) when decisions are not strictly ordered.

Myopic Value of Information in Influence Diagrams

The value of information technique is illustrated on
the influence diagram in Figure 8 through Figures 9a
and 9b. The strong junction tree in Figure 9a can also
be used to solve an influence diagram with h observed
before deciding on D3. The difference between the
two scenaria is reflected in the control structure for
the collect operation rather than in the junction tree.
A strong junction tree also being able to handle the
situation where h is observed before deciding on D2 is
shown in Figure 9b.
3.2

NOTATION

In Figure 10, we present an extended version of the in­
fluence diagram from (Jensen et al., 1994). The origi­
nal influence diagram notation has been extended with
triangular nodes, observation nodes. An observation
node designates that the chance node associated with
it will be observed within some interval of information
sets.

g

147

would have the observation interval [ h; Is].

3.3

ALTERNATIVE METHODS

There are other methods for calculating the value of
information in influence diagrams. These can be sep­
arated into multiple-model methods and single-model
methods.
The value of information in influence diagrams can be
viewed as the difference in expected utility between a
set of influence diagrams each implementing a specific
scenario of the desired observation-decision sequences.
In that view Ezawa (1994) creates and solves multi­
ple models for calculating the value of information in
influence diagrams. However, as the construction of
strong junction trees is a complex task it is preferable
to reduce the number of different junction trees. Also,
to cover all desired observation-decision sequences the
decision analyst may be facing a considerable task in
constructing the needed influence dia grams .
Instead, the different decision models in Figure 4 and
Figure 6 can be combined into a single influence dia­
gram which gives us the power to calculate whether or
not to observe B. Such a model is shown in Figure 11.

Figure 10: Influence diagram from (Jensen et al., 1994)
with extended notation.
Though there may not be any computational difficul­
ties associated with observing variables at an earlier
time than modeled, there may be some conceptual
problems. It does not make sense to observe, say, the
state of a fungus attack on your crop in May before
deciding whether or not to apply fungicide in April.
In other words: We cannot observe a variable prior to
making a decision that influences it.
Hence, a variable is modeled in the influence dia­
gram as belonging to the last information set possible,
and the observation node is associated with a "lower
boundary" for the observation. For node c in Figure 10
the lower boundary is !0, yielding the observation in­
terval to be [!0; !4] whereas the lower boundary for
node j is h and hence the observation interval for j is
[!1;!4]. If associated with an observation node, node

Figure 11: General model capable of handling the sce­
narios of Figures 4 and 6.

The resulting model consists of the original model
without observation on B (f ro m Figure 4) with an ad­
ditional two nodes; a decision node, Do and the chance
node B'.
Do will consist of the decisions (B) and (--,B) and the

observed node, B' will have the same states as its un­
observed counterpart, B, plus an additional state, (No
observation). If the optimal decision, d0, is (B), then
B ' is observed and set to the true state of B; if the op­
timal decision is (-.B), B' is set to (No observation).
The probability table for B is equal to the one specified
in Figures 4 and 6 and the behavior of B' is specified

148

Dittmer and Jensen

as
B'
B'

=

No observation

=

B otherwise

for

d0

=

(-,B)

This type of modeling cannot be called neither simple
nor intuitive. Furthermore, as can be seen from Figure
12, the junction tree for the general model in Figure 11
is larger than the junction tree produced by expansion
(Figure 7c).

Figure 12: Strong junction tree constructed from the
general model of Figure 11.
It is also worth noting that the model in Figure 11
and its corresponding junction tree in Figure 12 are
made for the case where B is either unobserved or ob­
served before D1. The junction tree in Figure 7c is
capable of calculating the expected utility for the de­
cision problem with B belonging to any information
set. Should the model in Figure 11 be extended to the
same flexibility, we are facing a larger and consider­
ably less intuitive model with little resemblance to the
original decision problem.
4

CONCLUSION

For specific influence diagrams, such as scenarios with
non-intervening decisions, we have presented a simple
method for calculating the value of information. This
method is simple in construction and cheap in terms
of time and space requirements, but is restricted in
the structure of the influence diagram. It is based
on methods developed by (Cooper, 1988) and ( Jensen
& Jiangmin L., 1995). For certain, well-defined tasks
there may be advantages in using this method but in
the general case we propose to use the method pre­
sented for influence diagrams with sequences of inter­
vening decisions.
In strong junction trees constructed for decision prob­
lems formulated as general influence diagrams we are
able to calculate the value of information for a given
chance node, that is, the gain in expected utility
from observing variable X before making a decision
In other words, we can calculate the differ­
Di.
ence in expected utility between models that differ in
observation-decision sequence, using the same junction
tree structure with only a number of tables expanded
but not recalculated. We find this method far more in­
tuitive than modeling all possible outcomes in a gen­
eral influence diagram as the structure of the model

will not change even when chance nodes (within lim­
its) are observed prior to the latest possible observa­
tion time. Also, modeling observations as intervening
decisions may seem unappealing to decision analysts.
In addition to this, we experienced that the junction
trees produced from the general models are larger than
those produced by table expansion.
Using our method is not for free as in its worst case
(modeling a chance node as never observed and ob­
serving it before the first decision D1) all tables in the
junction tree will be expanded (assuming that the de­
cisions are strictly ordered). This means that with a
states in the node in question, the resulting junction
tree will be almost a: times larger than the original
junction tree. This corresponds to performing a: prop­
agations in the strong junction tree and the gain is
therefore minimal.
However, the method presented will only expand the
tables needed, that is, only part of the junction tree
becomes larger (by a factor of a: ) which consequently
reduces the number of operations performed during
a propagation. Also, clever use of the control struc­
tures associated with the strong junction tree will pre­
vent excess operations in the expanded tables after
marginalization of the node in question. Still, if for
example r is very large and if all A E r are placed in
In, we may very well face an intractable problem as we
expand the cliques beyond the capacity of computers.
Topics for further research include the possibility for
utilizing independence assumptions in order to further
reduce complexity.
