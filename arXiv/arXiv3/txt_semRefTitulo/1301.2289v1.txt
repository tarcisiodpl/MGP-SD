

The most commonly used type of hybrid BN is the

Many real life domains contain a mixture of discrete
and continuous variables and can be modeled as hy­
brid Bayesian Networks (BNs).

koller@cs.stanford.edu

An important sub­

class of hybrid BNs are conditional linear Gaussian
(CLG) networks, where the conditional distribution of
the continuous variables given an assignment to the
discrete variables is a multivariate Gaussian. Lau­
ritzen's extension to the clique tree algorithm can be
used for exact inference in CLG networks. However,
many domains include discrete variables that depend
on continuous ones, and CLG networks do not allow

ditional Linear Gaussian ( CLG) model.

Con­

In CLGs, the dis­

tribution of the continuous variables is a linear function
of their continuous parents, with Gaussian noise.
ritzen

Lau­

[6, 7] showed that the standard clique tree algorithm

can be extended to handle CLG networks, allowing the
structure of the network to be exploited for inference, as
in discrete BNs. Lauritzen's algorithm is "exact", in the
sense that it computes the correct distribution over the dis­
crete variables, and the correct first and second moments
for the continuous ones. (It does not always compute the

such dependencies to be represented. In this paper, we
propose the first "exact" inference algorithm for aug­

exact densities of the continuous variables, as these may be

Our algorithm is based on Lauritzen's algorithm, and
is exact in a similar sense: it computes the exact dis­

model does not allow discrete variables to have continuous

CLG

networks - CLG networks augmented
by allowing discrete children of continuous parents.

mented

tributions over the discrete nodes, and the exact first
and second moments of the continuous ones, up to in­
accuracies resulting from numerical integration used

complex multi-modal distributions.)
Perhaps the main weakness of CLGs is that the graphical
parents, a dependency that arises in many domains. For ex­
ample, consider a feedback control loop involving a ther­
mostat, which controls the room temperature by turning on

softmax

or off a heating device and a cooling system. The thermo­

ciently, and that using the first two moments leads to a
particularly accurate approximation. We show empiri­
cally that our algorithm achieves substantially higher
accuracy at lower cost than previous algorithms for
this task.

on", "cooling on", and "idle") which depends on the con­

within the algorithm. In the special case of

CPDs, we show that integration can often be done effi­

stat should be modeled using a discrete variable ("heating
tinuous variable representing the room temperature.
We can define a class of

augmented CLG networks, which

uses CLG dependencies for the continuous variables, but
also allows dependencies of discrete variables on continu­

1

ous parents

Introduction

Bayesian Networks (BNs) provide a compact and natural
representation for multivariate probability distributions in a
wide variety of domains. Recently, there has been a grow­
ing interest in domains which contain both discrete and
continuous variables, called

hybrid

domains. Examples of

such domains include target tracking

[1],

where the con­

tinuous variables represent the state of one or more targets
and the discrete variables might model the maneuver type;
visual tracking (e.g.,

[13]),

where the continuous variables

represent the positions of various body parts of a person and
the discrete variables the type of movement; and fault di­
agnosis

[4].

The conditional probability distributions

(CPDs) of these nodes are often modeled as

[I 0], where a physical system contains continuous

variables such as flows and pressures and discrete variables
such as failure events.

softmax func­

tions, which include as a special case a "soft" threshold
function for a continuous parent (i.e., a noisy indicator
whether the value of the continuous parent is greater than
some constant). There are many domains that can be mod­
eled very naturally using augmented CLG networks, in­
cluding our thermostat example above.
Unfortunately, there is no exact inference algorithm
known for augmented CLG networks. One can always re­
sort to the use of approximate inference, such as discretiza­
tion (e.g.,

ing [15]

or

[5]) or sampling (either Likelihood Weight­
Gibbs Sampling [12]), but these approaches

have some serious limitations.

It is often hard to find a

good discretization: Sometimes any reasonable discretiza­
tion demands too fine a resolution, and often requires the

320

LERNER ET AL.

UAI2001

X).

Sets of variables are denoted with boldface (e.g.,
Bayesian networks consist of two parts:

a qualitative

component given by a directed acyclic graph whose nodes

,_,

correspond to the random variables and a quantitative com­
- P(ActW:.n.H•IIIITotm�tuflo)
-P(Ao::tiQn•ldo!oiT•mpr•hl,.l
P<""""''""IT� �,

....

I

ponent given by a set of conditional probability distribu­

I

,_.

tions (CPDs) which define the conditional distribution of
every node given its parents in the graph. In principle, there
is no restriction on the form of the CPDs. However, if we
want to perform efficient inference on hybrid BNs, some
(b)

(a)

restrictions are necessary.

Figure 1: Examples of softmax CPDs: (a) The thermostat
(b) Multi-transition softmax

cn

The most widely used subclass of hybrid BNs is the o ­
ditional linear Gaussian (CLG) model. Let X be a con­

.

tinuous node, A be its discrete parents and Y1, . . , Yk its

continuous parents. The CLG CPD defines X to be a linear

handling of intractable intermediate factors (especially in
high dimensions). The convergence of sampling algorithms
can be quite slow, and is very sensitive to the network
parameters and the configuration of the evidence.

Mur­

Gaussian function of the Yi 's, with a different set of param­
eters for the linear Gaussian for every instantiation of A:

k

p(X I

a,

y)

=

L Wa,iYi; cr!).

N(wa,O +

i=l

phy [II] proposed a variational approximation for a class
of augmented CLG networks, based on the observation that

The CLG model does not allow discrete children of contin­

a Gaussian can be a good approximation to the product of

uous parents. This model has the appealing property that

a Gaussian and a softmax. Wiegerinck [I6] showed how

it defines a conditional Gaussian joint distribution: for any

this approach can be adapted to deal with multi-modal dis­

assignment to the discrete variables, the distribution over

tributions. However, this approach is currently limited to

the continuous variables is a multivariate Gaussian. The

binary discrete variables and softmax CPDs. More impor­

reason is that, given such an assignment, the CPDs of the

tantly, it does not provide any performance guarantees on

continuous nodes reduce to simple linear Gaussians, induc­

the quality of the resulting approximation.

ing a multivariate Gaussian.

In this paper we propose the first "exact" algorithm for
augmented CLG networks.

Our algorithm is based on

the following simple, yet powerful, idea.

Consider the

As discussed in the introduction, the inability of the CLG
model to represent the dependence of discrete nodes on

continuous ones severely restricts their range of applica­

case where the discrete children are modeled with softmax

bility. In this paper, we define a class of augmented CLGs,

CPDs. As in [11], we approximate the product of a Gaus­

where the CPDs of the continuous nodes are CLGs, as dis­

sian and a softmax as a Gaussian, but rather than using a

cussed above, but where we also allow discrete nodes to

variational approach, we find the approximation directly

depend on continuous parents. We use continuous-discrete

using numerical integration. We embed this idea within

(CD) to refer to such CPDs.

the general framework of Lauritzen's algorithm for CLG

There are many possible functional forms that can be

networks, leading to a simple algorithm, which is roughly

used to represent CD CPDs. One of the most useful is a

comparable in its complexity to Lauritzen's algorithm.
We prove that our algorithm is exact, in a sense that is
analogous to Lauritzen's algorithm: It computes the exact

softmax or logistic function. Let
a1,

...

P(A =a; I Yl, ... , Yk)

=

the possible values
ents. We define:

, am,

distributions over the discrete nodes, and the exact first and
second moments of the continuous ones, up to inaccura­
cies resulting from numerical integration used within the
algorithm. We also show empirically that it achieves ex­

A be a discrete node with

.

and let Yt, .. ,

(b;

exp
m

+

(

.

Yk be its par­

2-.:�=t wivz)
k

·

Lj=l exp bJ + Llo::t u1 Yl

)

·

A also has discrete parents is modeled as

tremely high accuracy for "reasonable" numerical integra­

The case where

tion schemes, leading to results that are significantly better

in CLGs: we define a different softmax function for every

than current approximate inference algorithms.

combination of the discrete parents. It is possible to elim­
inate one of the linear combinations by dividing both nu-

2

Hybrid Bayesian Networks

A hybrid BN represents a probability distribution over a
set of random variables where some are discrete and some
are continuous. We denote discrete variables with letters
from the beginning of the alphabet

(A ,B, C, and D) and

continuous ones with letters from the end (X, Y, and Z).

ing us with

m

( bi

L::7,1 wi Yl) , leav­
- 1 sets of parameters. In particular, when

merator and denominator by exp

+

A is binary, this new form simplifies to a standard sigmoid

function:

P(A=atiYt, .. . ,yk)=

1

+e

b

(
xp

1
+

k

Ll=l W!YI

).

UAI2001

LERNER ET AL.

Fig. l(a) shows a softmax CPD for our thermostat exam·
pie from the introduction. The CPD parameters control the
location and the slope of the transitions. It is possible to
generalize the softmax functions to express fairly complex
distributions, as in Fig. l(b); see [4] for discussion.

3

Inference in CLGs

We present a brief review of Lauritzen's algorithm for in·
ference in CLG networks, on which our algorithm is based.
The algorithm has two versions: the original one [6] and
an improvement to it [7]. Both versions are based on the
clique tree algorithm [8]. The clique tree algorithm begins
by transforming the BN into a clique tree. The first step is
to generate the moralized graph, where the parents of each
node are connected directly, and all edges are undirected.
The moralized graph is then transformed into a clique tree
using a process called triangulation (see [8] for details). In
the clique tree, each node (also called a clique) is associated
with a data structure, called a potential, that can represent
a function over the possible values of the variables in the
clique (in general, these data structures are called factors).
In a purely discrete BN, the factor is typically a table with
one entry (a number) for each assignment to the variables
in the clique. In the message passing phase of the algo·
rithm, factors are passed between neighboring cliques. At
the end of this phase, every clique potential contains the
correct marginal distribution over the clique variables.
Several issues arise when extending the clique tree al­
gorithm to CLG networks. Most obviously, the factors
in the cliques and the messages are functions over both
discrete and continuous variables and cannot be repre­
sented by tables as in the discrete case. Lauritzen's al­
gorithm deals with this issue by defining a factor as a
table which specifies a continuous function for every in­
stantiation of the discrete variables. The two versions of
the algorithm use these continuous functions in different
ways. In the original version, the functions are treated as
canonical forms, which can represent any function of the
form
g, h, K) = exp [g + x' h + x' J{ x], where g is
a constant, h is a vector and K is a full-rank square rna·
trix. Note that a multivariate Gaussian, whose density is
exp
is a special case

P(x;

�

(-(x- J.t)t�-1(x- �t)/2),

of this form (see [6] for formulae to convert from a mul·
tivariate Gaussian to canonical form). However, not ev­
ery function representable in canonical form is a multivari­
ate Gaussian (e.g., exp(x2)). In fact, canonical forms can
represent functions which are not probability distributions:
They do not necessarily have a finite integral and their mo­
ments may not be defined. In particular, CLG CPDs, which
represent a conditional rather than a joint distribution, are
representable in canonical form but not as a Gaussian. In
the new version of the algorithm, the factors represent con­
ditional Gaussians, i.e., they represent a conditional distri­
bution of a subset of the variables given the rest.

321

The clique tree algorithm manipulates factors in vari­
ous ways, such as multiplying, dividing and marginalizing.
Lauritzen shows how all these operations can be carried out
exactly in both versions of the algorithm, with the notable
exception of summing out a discrete variable. For example,
consider a factor over the variables A and X (where A is
discrete and X is continuous) and assume we need to com·
pute its marginal over X in order to send a message to a
neighboring clique (i.e., sum out the variable A). Since the
message contains only one Gaussian, we need to collapse
the two Gaussian components in the original mixture, while
maintaining the correct first and second moments. W hile
we can collapse Gaussians using their moments, the op­
eration is not defined for a general canonical form or for
a conditional Gaussian in which the moments may not be
well defined. Thus, we must ensure that when the message
passing algorithm calls for collapsing, our factors will rep­
resent Gaussians.
To ensure this property, Lauritzen's algorithm imposes
certain constraints on the form of the clique tree. These
lead to the notion of strong triangulation. While the ex­
act details are not important for the purposes of this paper
(see [6, 7]), one of the implications of strong triangulation
is important for our analysis. Define a continuous con·
nected component as a set of continuous variables X such
that every two variables X1 X2 E X are connected in the
moralized graph via a chain consisting only of continuous
variables. We define DN(X), the discrete neighbors of X,
as the set of discrete variables that are adjacent to some
variable in X in the moralized graph. Strong triangulation
implies that all the variables in DN( X) necessarily appear
together in some clique in the tree. The intuition for this
requirement is that the distribution over X is a mixture of
Gaussians with one mixture component for every assign­
ment to DN(X); hence, we must consider all the combina­
tions of the discrete neighbors together.

,

The cost of Lauritzen's algorithm is polynomial in the
size of the factors in the cliques. This size grows exponen­
tially with the number of discrete variables in the clique,
and quadratically with the number of continuous variables
in the clique. Thus, the strong triangulation property, al­
though unavoidable, is a major computational limitation of
Lauritzen s algorithm. (See [9] for further discussion.)
'

4

Inference in Augmented CLGs

We now extend Lauritzen's algorithm to the class of aug­
mented CLG networks defined in Section 2. We present
our algorithm in the context of the original version of
Lauritzen's algorithm and later show how it can be easily
adapted for the modified version.
We first motivate our algorithm with a simple example.
Consider the network X -+ A, where X has a Gaussian
distribution given by P(X) = N(J.t, <7) and the CPD of A
is a softmax given byP(A= liX=x)= 1/(l+ea"'+b).
The clique tree has a single clique (X, A), whose factor

322

LERNER ET AL.

should contain the product of these two CPDs.
should contain two continuous functions-

1 I x) and P(x)P(A = 0 I x)-

Thus, it

P(x)P(A

=

each of which is a

product of a Gaussian and a sigmoid.

UAI2001

to accomodate this insertion operation. Since we now have
integrable distributions, we can perform the approximation.
Our solution to this problem raises the following ques­
tion: Can we use the prior distribution over the CLG com­

[11] that the

ponent as our integration distribution? Unfortunately, there

product of a Gaussian and a sigmoid can be approximated

are several reasons why the use of this distribution is an ap­

Our algorithm is based on the observation

quite well by a Gaussian distribution. We can compute the

proximation which can lead to errors. We now discuss each

best Gaussian approximation to this function by computing

of these, and show how to correct them.

the marginal distribution of
moments of

X

A

and of the first and second

from the joint distribution:

P(A ;=:a)
f�oo P(A =a I x)P(x)dx
E[X I A:=: a]
f�oo xP(x I A= a)dx
= P(l-=a) J�= xP(A :=:a I x)P(x)dx
Similarly, we compute the second moment E[X2 I A:=:

ample, the network

(1)
a].

This basic idea leads us to the following outline for an
verging only in cases where a clique contains CD CPDs; in
this case, we approximate its factor as a mixture of Gaus­
sians, where the mixture has one Gaussian -with the cor­
rect first and second moments - for each instantiation of
the discrete variables (no matter their configuration). In the
remainder of this section, we "fill in" the details of this al­
gorithm, addressing the subtleties that arise.

(1) compute expectations relative to P(x ) : To

evaluate these expressions at a clique, we must have a prob­

ability distribution over X at that clique. Unfortunately, the
message passing algorithm does not guarantee that these

distributions are available initially. Consider the network
Y

A. The clique tree for this network consists of
two cliques: (X , Y) and (Y, A). In Lauritzen's algorithm,
--+

-+

the clique tree is initialized by incorporating all CPDs into
their corresponding cliques. In our case, we should incor­
porate

P(A

1 Y) into the clique ( Y, A) by computing the

relevant expectations. However, at the initialization phase,
the message passing has not yet been performed. As such,
Y is given in canonical form and does not yet represent a
Gaussian distribution, preventing us from performing this
integration; thus, we cannot multiply the CPD

P(A I

Y)

into the clique a t this stage.
We address this

problem by introducin g a preprocessing

phase, which serves to guarantee that all cliques contain an

integrable distribution - a

Y

-+

A, and assume that X is
(X , Y) and ( Y, A). Fol­

lowing our current algorithm, we would insert the CPD for
I Y) and calibrate the tree, approximating it as a CLG

network. If we now enter the evidence observed for

X,

we

would be incorporating it into an approximate distribution
rather than the true one, potentially leading to sub-optimal
approximations. Fig. 2(a) shows an example of this phe­
nomenon, where the approximation obtained by first inte­
grating the CD CPD and then conditioning on our evidence
is a sub-optimal approximation. T he optimal approxima­
tion uses the posterior over Y directly as our integration
distribution.

Our solution to this problem is straightfor­

ward: We not only ensure that each clique has a Gaussian
distribution in it, we ensure that it has the posterior Gaus­

The first difficulty arises from the observation that the

X

__,.

sian. T hus, we incorporate the evidence and propagate it

The algorithm

equations in

X

observed. T he minimal cliques are

P (A

algorithm. We roughly follow Lauritzen's algorithm, di­

4.1

The first difficulty is that our prior distribution is com­
puted before incorporating the evidence. Consider, for ex­

Gaussian distribution relative

to which we can compute the relevant expectations, rather
than a non-Gaussian canonical form. To do so, we build
the standard clique tree for our BN, but do not initialize
the clique potentials. We then insert all the CPDs except
for the CD CPDs. The resulting network is equivalent to a
CLG network, so we can calibrate it using Lauritzen's algo­
rithm, resulting in probability distributions in each clique.
Finally, we insert the remaining CD CPDs and re-calibrate
the tree. Note that the cliques in our tree were designed

before entering the softmax CPDs.
A more subtle problem with our choice of integration dis­
tribution relates to the use of collapsing within Lauritzen's
algorithm. Consider a network

A __,. X

-->

Y

assume that the clique tree has the cliques

_,.

B, and

(X , Y, B) and
(A, X , B) (note that the tree (A, X ), (X, Y), (Y, B) is in­

consistent with strong triangulation). According to our cur­
rent algorithm, we calibrate the clique tree with the CPDs
for A,

X , and Y , and then insert the CPD P( B

I Y) into the

clique (X , Y, B). However, the distribution in this clique is

not the correct prior distribution over Y. The correct prior
distribution of Y has two modes (one for every value of
A); but, as A does not appear in the clique, Lauritzen's al­
gorithm collapses the two modes into a single Gaussian,
losing its bimodal nature. Although Lauritzen shows that
this approximation can be used without introducing new
errors if the functions are linear, the CPD

P( B

I Y) is not

linear, and we may not get the best approximation

for the

first two moments. Fig. 2(b) shows an example, where our
approximation is worse when we use the collapsed distri­
bution for Y as the integration distribution.
Once again, the solution is to enter the CD CPD into a
clique where the integration distribution over the continu­
ous parents is correct. Let
the CD CPD, let

X be the continuous parents of
Y 2 X be the variables of their contin­

uous connected component (as defined in Section
let A= DN( Y ) . As we discussed,

3),

and

X will have one mode

for every assignment to A. Hence, if we want to represent
the exact multi-modal distribution for

X, it is necessary

LERNER ET AL.

UAI2001

--- Evidence First

\

·�r-------�--�
-- OplimaJ III'P"'--- Suboptinll approximation

'\

Softmax F1rst

-·-

323

- Sharp .sepe:te.te
-&··- Shatp corrtJinod
-·-- Flat separate
---· Flat combined

·.

\

\

\

\

- -=---.--= '-----7:., --------:::--0-�'::-

\
\\

\

\

\.:""--'
:,:;
, •·•-----;-,o.�,c.,�..:;...�----. "'
.•.. .

••

(a)

X

(b)

(c)

2: (a) Incorporating softmax CPDs before and after the evidence. (b) Error introduced if the discrete neighbors are
not in the same clique as the sigmoid CPDs. (c) Error introduced if the sigmoid CPDs are entered separately.

Figure

and sufficient to have a clique containing the variabl es in
both X and A. Of course, this requirement could result
in a larger clique tree; however, the overhead is not large.
As we discussed earlier, A must be in some clique in the
optimal tree. Thus, at worst , we only add some continuous
variables to some of the cliques. Since the represntation of
canonical forms (and multivariate Gaussians) is quadratic
in the number of variables, the size of the tree can only
grow by a polynomial factor at worst.
Note that this modification to the clique tree is necessary
only if we want to guarantee the optimal approximation.
The algorithm remains coherent if we use an approximate
integration distribution, only the quality of our approxima­
tion can degrade. Therefore, we can use a clique tree where
the clique that contains X contains only some subset of the
variables in A.
The final problem arises when there is more than one CD
CPD. Most simply, we can insert each CD CPDs sequen­
tially. We insert each CD CPD, appro ximate the resulting
joint distribution as a mixture of Gaussians, and proceed
to use that mixture as t he basis for inserting the next one.
The obvious problem with this approach is that the inte­
gration distribution used for the CD CPDs inserted later is
only an approximation to the correct non-Gaussian distri­
bution resulting from the insertion of the earlier CD CPDs.
The solution to the problem is to integrate all the CD CPDs
in the same continuous connected component in one oper­
ation. In Fig. 2(c), we show the difference on the network
A - X -+ Y -+ B. We tried inserting both softmax
CPDs into the clique containing X and Y together, and
separately. We experimented both with step-like transitions
("sharp" sigmoids), and smoother transitions. The latter al­
lows for a better approximation as a Gaussian, and there­
fore less error by doing the approxi mation step by step.
This difference is clearly manifested in the figure.
While the idea of joint integration seems expensive, we
note that the relevant CPDs must already be in the same
clique (with their discrete neighbors), so we do not increase
the size of the tree. However, we do pay the price of com-

Construct a strongly triangulated cliqu e tree such that for every
maxi mal connected component X 1 there exists a clique C;
such that C; contains X 1 and its discrete neighbors
Insert all CPDs except for softmax CPDs
Calibrate the tree u sing Lauritzen's algorithm
Insert the (continuous and discrete) evidence and re-calibrate
Instantiate the CD CPDs with the continuous evidence

for each maximal connected component X;
Find all softmax CPDs S1, ... , Sn that can go into C;
Insert S1, . . . , Sn into Cl using multi-dimension integration
Re-calibrate the tree
Return the distribution over Q

Figure 3: Outline of full algorithm
puting integrals in higher dimensions. We can reduce this
cost by integrating only some of the CD CPDs together.
This scheme induces a spectrum of approximations, with
a tradeoff between complexity and accuracy: If doing the
high dimensional integration is intractable, we can approx­
imate it either by inserting the CPDs separately or by using
a more efficient and less accurate integration method.
The full algorithm for inference in augmented CLG net­
works is presented in Fig. 3. We are given a hybrid
Bayesian network B, evidence e and a query Q and wish
to compute P( Q, e). Note that the CD CPDs should be in­
tegrated together to achieve the best approximation but can
also be integrated separately as discussed above.

4.2

In-clique integration

Having defined the overall structure of the algorithm, it re­
mains only to discuss the integration process within each
clique. There is a wide range of numerical integration
methods t hat can be applied in our setting. We focus our at­
tention on one that seems particularly suitable in our frame­
work-- Gaussian quadrature integration [2]. Gaussian
quadrature approximates a general integral as follows:

j W(x)f(x)dx
b

a

N

�

2: Wjj(xj)

j=l

324

Where

LERNER ET AL.

W(x)

is a known function and the points Xj and

UAI2001

approximation, e.g., inserting the CD CPDs one at a time

weights Wj are selected such that the integral would be

rather than all at once, thereby losing the optimality guar­

Quadrature is particularly well suited in our setting, since

selves to the use of other numerical integration methods,

exact if

f(x)

we can set

is a polynomial of degree 2N- 1 or less.

W(x)

antees of our algorithm. Altem!ltively, we can resign our­

to be a Gaussian, for which good points

such as Monte Carlo integration, that scales better with the

and weights are known (see [14] for code to generate the

dimensionality of the (continuous) space. Another possi­

points and weights). The main drawback of this method is

bility is the use of adaptive integration methods, described

that the size of the grid grows exponentially with the di­

in [2]. Here we assume that we have available an integra­

mension of the integral, which seems to be a real problem,

tion procedure that also outputs an error estimate. We then

as the cliques in our algorithm often contain many contin­

use this procedure to adaptively focus the computational

uous variables (particularly if we keep a cliq u e which con­

efforts to regions which produce larger estimated errors (in

tains an entire continuous connected component). Fortu­

our case we would use most of our resources in areas where

nately, we can use the properties of our augmented CLG

the sigmoids transition sharply between 0 and

networks to significantly reduce the computational burden.
Our first observation exploits the fact that we are dealing

4.3

1).

Using Lauritzen's modified algorithm

with Gaussian distributions. Assume that the continuous

Lauritzen's original algorithm suffers from some well

variables can be partitioned into the sets Y and

known numerical instabilities which our extension to it

Z where

only variables from Y appear in CD CPDs. Recall that

we can represent the multivariate Gaussian over the vari­

would inherit.

Lauritzen's modified algorithm improves

upon the original one by providing better numerical sta­

Y, Z as a linear Gaussian network with the structure
Y --+ Z (i.e., there are no edges from a variable in Z to a

like the original version, it maintains conditional distribu­

variable in Y). The CD CPO changes the distribution over

tions in the cliques, except for the strong root, in which a

ables in Y, having the first two moments for Y is enough

merical stability, we can adapt our algorithm to work with

numerical integration. Thus, to incorporate the variable

ensure that CD CPDs are inserted to a clique which is a

ables

Y, and since any variable in Z depends linearly on the vari­
to infer the first two moments for

Z without any further
A

bility, and also by dealing with determinstic CPDs. Un­

multivariate Gaussian is kept. To enjoy the benefit of nu­
Lauritzen's modified algorithm.

All we need to do is to

into the CPD, the required integration dimension is exactly

strong root, guaranteeing it has an integrable distribution.

the number of continuous parents of

If the strong root does not naturally contain the continuous

A.

Surprisingly, w e can substantially improve even o n this

variables from the CD CPDs, we can redesign the clique

A is a softmax. The

tree to ensure that this property holds. (This process can be

idea, in the case where the CPD of

a soft threshold defined via a set

accomplished using PUSH operations [7].) The only possi­

of linear functions Ia over the continuous parents Y of A;

ble consequence is the addition of continuous variables to

softmax for a node

A is

we have one function for each value

a

of

A,

although we

can eliminate one of them as discussed in Section 2. We can
now define a set of new variables Za which are a (determin­
istic) linear function of the variables Y:

Za

=

fa ( Y ) . We

can then reinterpret the softmax as a CPD whose parents
are the variables Za. (More generally, we can use any set of

Z which are linear combinations of Y such that
every Ia can be presented as a linear combination of Z.)
Note that, as the Za 's are linear functions of the parents Y,
variables

a Gaussian distribution over Y induces a Gaussian distri­
bution over the Za 's. We can use the distribution over the
Za 's as our integration distribution, and then propagate the

result to the actual parents Y using the linearity between
the Y's and the Za 's. The dimension of the integrals we
have to perform is at most
of values of

A.

[A[- 1, where [AI is the number

W hen dealing with binary variables, this

approach can result in dramatic savings.

Of course, one can still construct networks where the in­
tegration dimension is very large: networks where the dis­
crete variable

A has

many values and continuous parents,

or where there are many CD CPDs that all need t() be in­
tegrated into the same clique. When forced to deal with
these cases, we have several choices. We can resort to some

some cliques. Therefore, the added complexity is quadratic
in the number of continuous variables in the worst case.

5

Analysis

We now show that our algorithm is "exact", up to errors
caused by numerical integration.

We use "exact" in the

same sense used in Lauritzen's algorithm: It computes the
correct distribution over the discrete nodes, and the correct
first and second moments for the continuous ones.

1 Let Q be a query such that Q s;:; C where C
is some clique in the tree and let e be some evidence. The
above algorithm computes a distribution P ( Q, e) which is
exact ior discrete variables in Q and has the correct first
two moments for continuous variables in Q, up to inaccu­
racies caused by numerical integration.
Theorem

Proof: We start by showing that the algorithm is exact
when the

moralized graph contains one continuous con­

nected component and the clique tree has just one node.
The algorithm has three steps, and we analyze the result of
applying each one of them. The first step involves insert­
ing discrete and CLG CPDs into the clique tree. Since

all

the variables are in one clique (hence there is no need for

UAI2001

LERNER ET AL.

325

- KL on (A. B) using lllill sil,lnOid
.......... KL on (X, Y) U$lng Nat sigmOO
-............ Kl on (X, Y) using sharp sigmolds
.- KL on (A, B) using sharp sigrnok:ls

(a)
Figure

4:

(b)

(c)

(a) The Emission network. Ovals are continuous variables and rectangles are discrete. (b) The extended crop

network. (c) Error caused by inserting CD CPDs separately.

collapsing) we get that the clique potential represents the
exact product of the CPDs that were inserted. Note that

moralized graph has more than one continuous connected
component. In this case, strong triangulation guarantees

the product of the clique potential and the CD CPDs is the

that no clique contains continuous variables from two dif­

exact prior distribution.

ferent continuous connected components. Thus, the mes­

The next step involves incorporating the evidence. The

sages passed between different continuous connected com­

discrete evidence can be viewed as extra factors that are

ponents are discrete factors which do not change the con­

multiplied into the tree (the entry in the factor correspond­

ditional distribution of the continuous variables given their

ing to the evidence is 1 and the rest are

discrete neighbors.

these factors into the clique tree is

an

0). Multiplying

does not introduce any inaccuracies. We now consider the
continuous evidence. Setting the evidence is equivalent to
setting the relevant values in every function of the product
of the tree and the CD CPDs. In the clique potential this
can be done using Lauritzen's standard algorithm. In the
CD CPDs we are guaranteed that any continuous evidence
variables that appear in the CPD must be parents; Thus, we
can simply set their value and get a new conditional dis­
tribution not involving them, which is either a CD CPD or
just a discrete factor.
The last step is the insertion of the CD CPDs into the
clique potential. Had we been able to represent the answer
exactly, then the clique potential would have been the exact
posterior distribution. However, we approximate each of
the continuous components in the potential as a Gaussian,
computed in a way that guarantees it has the correct first
two moments (up to numerical integration inaccuracies).
We now remove the various assumptions. The clique

C

where the CD CPDs are entered need not contain discrete
variables which are not discrete neighbors of the continu­
ous connected component, since it can represent the exact
distribution over the continuous variables without them. It
also need not contain continuous variables which are not
involved in any CD CPD. The key insight here is that after
incorporating all the non-linear functions into

C,

the rest

of the cliques have linear functions. Therefore, in order to
find the correct first two moments, it suffices to use mes­

sage passing, where each message has the correct first two
moments.

I

exact operation, and

(This claim is similar to the proof of correct­

ness for Lauritzen's algorithm.) Finally, assume that the

6

Experimental Results

We tested various aspects of our algorithm and compared it
to other approaches. As we discussed, Gaussian quadrature
does not scale up well in high dimensions. To demonstrate
the effects of integrating in high dimensions we tested our
algorithm on networks where the continuous variables form
a chain

X1

-+ Xn, with each X; a parent of a dis­
A. We varied n, thus simulating integration

-+

crete variable

·

· ·

problems in different dimensions. Fig. 5(a) and (b) show
the result of performing the integration in one and eight di­
mensions using both Gaussian quadrature and Monte Carlo
integration. In one dimension, Gaussian quadrature is ex­
tremely efficient, achieving good accuracy with as few as 5
integration points. In eight dimensions, Gaussian quadra­
ture needs many more points to achieve a similar accuracy.
Note that we need at least two points in every dimension,
for a total of at least 256 points in 8 dimensions. As we dis­

cussed, it is possible to dramatically speed up the integra­
tion by taking advantage of the sigmoid function, represent­
ing the linear combinations of the eight parents as one vari­
able; indeed, we get a reduction of approximately three or­
ders of magnitudes in the number of points required. Note
that this case still converges more slowly than simple inte­
gration in one dimension. The reason is that the variance
of the dummy parent is larger than the variance we used in
one dimension (we further discuss this issue in Section

7).

Finally, we can see that although Monte Carlo integration
converges quite slowly, it is almost unaffected by the higher
dimension, and can be used in cases where we have no

326

LERNER ET AL.

UAI2001

...,. ,..----.

..... ,.-------.,
- Gaussian quadrature
------ Monte carlo lnteg<ation

\

\

..,,,

�

W O.I)Qot

-

-

.- --

\
\

\

'·""

"'"'

..•

O.OCJ:!

j5
fi
i

\

\

I
\.--.

u

i

!
I
I

(l.Q01

\

\,

\

o.0D1

&.0

fog10(1-01)

(a)

5.0

-·

\

'·---�, -- - 3.0

LWerror on Prlce
LW .rror on Rein I Profit,..Even
\ - Atu. em;�r on AUl I Prolt.evtn
-·-··

·,

\\

-.,

l

8 Oirnenaional integration
MC Integration

\

\�

i

Speci•l aigrnoid integration

-·-·

-

·
�---·- -·---·-·-·-·-·-·- ·-·-·-·-·------

1.0

(b)

2.0

3.0

Time (soc:on<ls)

4.0

S.D

(c)

Figure 5: (a) Integration in one dimension. (b) Integration in eight dimensions. (c) Comparison with Likelihood Weighting

(w

choice but to integrate in high dimension.
We then considered the effect of inserting the CD CPDs
together versus one at a time. We experimented with the
network

A

X

B used in Fig. 2(c).

=

2, b

=

-5 .6).

We experimented with various queries using our algo­
rithm and compared it to a few runs of Gibbs sampling

In

using BUGS. We found that BUGS seemed unstable and

Fig. 4(c) we show the KL-distance, for both the discrete

produced results which differed significantly from ours. As

--+

Y

--+

--+

and the continuous variables, between the distributions ob­
tained by inserting the softmax CPDs together versus one at

an example, we queried for the distribution over the emis­

sion of dust after setting both the metal emission sensor and

a time. The error depends on the strength of the correlation

the C02 sensor to High. Our algorithm returned a mean of

and Y, so we considered different correlation

3 .4 1 9 and a variance of 1 .007. B UGS converged after about

values ranging from 0 to 1 . We also changed the parame­

500,000 samples to a mean of 3 .3 1 and a variance of 0.3 1 ,

between

X

ters of the softmax CPDs, allowing for smooth transitions

which did not change substantially even after 1 ,000,000

in one

samples.

case (a flat sigmoid) and step-like transitions in an­

To understand the discrepancy, we used likeli­

other case. We see that the error is larger when there is a

hood weighting on the same query.

strong correlation between variables influenced by differ­

ples, the estimated mean was

After 500,000 sam­

3.418 and the estimated vari­

0.999 which agree quite closely with the results

ent CD CPDs, and when the sigmoids are sharp, making

ance was

the Gaussian approximation worse. Interestingly, however,

produced by our algorithm.' We note that our algorithm

the accuracy is very high in all cases; Thus, even if we

achieved these results with only 3 quadrature points per di­

insert CD CPDs separately, with the associated computa­

mension, and with the highest integration dimension being

tional benefits, we often get very accurate results.

2. Hence, our algorithm was almost instantaneous, and was

We compared the accuracy of our algorithm to that of oth­

much

faster than both B UGS and likelihood weighting.

ers on more realistic examples that have been considered by

As a final example, we tried our algorithm with a network

other researchers. We first tested our algorithm on the Crop

containing non-softmax CD CPDs. We augmented the crop

network presented by Murphy in [ 1 1 ] . We compared our

network (see Fig. 4(b)) with three more variables.

results with the results of Murphy and with those of Gibbs

of them is the

One

sampling using BUGS [3] . It turns out that Murphy's vari­

Profit variable, which depends on a prod­
uct of the Crop and Price variables. The parameters of the

ational algorithm performs quite poorly when the posterior

extended network appear in Appendix A. Having experi­

distribution is multi-modal, achieving Ll -errors over the
binary discrete variables

of 0.28--0.38. On the other hand,

both our algorithm and BUGS performed very well on this
simple network, giving the correct result almost instanta­
neously. We also note that Wiegerinck [ 1 6] reports good
results for this network using his variational approach.
To test the algorithm on a larger network, we used the

enced problems using BUGS, we compared our results to

without
evidence, and one with the evidence Profit= Even, and com­
likelihood weighting. We tested two scenarios , one

pared the accuracy of both algorithms on various queries.

We used numerical integration with 150 points per dimen­
sion

as our ground truth. We ran LW and our algorithm

for the same amount of time, and then measured the KL­

[6], which models the emis­

distance between the "ground truth" and the results. For

sion of heavy metals from a waste incinerator. The origi­

LW, we averaged over 1 0-500 runs (we used more runs

Emission network described in
nal network is

a CLG. We augmented it

wi

th three extra

discrete binary variables as shown in Fig. 4(a). The addi­
tional variables correspond to various emission sensors and

(w = 1 , b = -3), a
6) and a metal emission sensor

each has a CD CPD: a dust sensor
C02 sensor

(w

=

3,

b

=

1

We further investigated this discrepancy and discovered that.

BUGS also returns answers that disagree with those appearing
in [6] even for the ori ginal Emission network without the CD
CPDs. For ex.ample the standard deviation for DustEmission con­
verged to 0.85 instead of 0.77 (the mean was correct).

UAI 2001

LERNER ET AL.

for smaller number of samples where the variance is bi g­
ger). Fig. 5(c) shows the results for the KL-error for
with no evidence and for Rain given

Price

Profit=Even. Only

three lines are visible: the KL-error of our algorithm in the

327

Another problem for the algorithm relates to unlikely dis­
crete evidence (unlikely continuous evidence is not prob­
lematic), because even slight errors in the distribution

P( Q , e) are magnified by the renormalization process.

We

in the
raph.
It
is
clear
that
our
algorithm
converges
much
faster
g
than LW, especially when we have evidence. This is to be

can reduce the effect of unlikely evidence by a simple two­

expected, as it is well known that sampling methods can

allocating more of our resources to the mixture components

c ase of no evidence is too dose to zero to be visible

t<lk:e a lot of time to converge, and

the performance of LW

step process : we run our algorithm to obtain a first estimate

of the posterior over the discrete variables and then rerun it,
that are more likely in the posterior distribution . We plan

deteri orates when there is evidence.

to test this approach in future work.

7

works can be divided into three classes:

Discussion

In this paper, we presented the first exact inference algo­
rithm for augmented CLG networks. We use numerical in­
tegration to compute the first two moments of every mix­
ture component and thus approximate it as a multivariate
Gaussian. We show how this approach can be incorporated
into Lauritzen' s clique tree algorithm (both the original and
the modified version), which enables us to take advantage
of the properties of the network to speed up the computa­

tion. In particular, our algorithm exploi ts both the linearity

of the CLG part of the network, and the properties of the
softmax CPDs (when appl icable) , to reduce the dimension

of the integrati on . We proved that ou r algorithm p roduces
the correct distribution over the discrete variables, and the

correct first two moments of the continuous variables , up to

inaccurac ies resulting from numerical integration . Thus, it

gives the best approximati o n within our expressive power.

Our algorithm is not restricted to any special class of the
conditional distributions in the CD CPDs - we can always
compute the correct first two moments , resulting in a Gaus­
sian approximation . However, the quality of the Gaussian
approximation varies for different classes of conditional
distributions. In the common case of softmax CPDs, the
Gaussian approximation is often an excellent approxima­
tion to the true posterior distribution.

As our algorithm relies heavily on numerical integration,
its performance is directly related to the quality and effi­
ciency of the n umerical integrati on procedure. The Gaus­
sian quadrature method works particularly well in many
networks but ru ns into problems in high dimensions and
when the sigmoids are sharp relative to the variance of the

Gaussian (i .e. , they resemble a step function rather than
a smooth transition). The reason for this problem is that
Gaussian quadrature tries to find a set of points which op­

timizes the performance for fu nctions which are polynomi­

als. Since the smoother the function is, the better its ap­
proximation as a l ow degree polyn omial , Gaussian quadra­

ture is more accurate with smooth sigmoids. We point out
that, in the case of sharp sigmoids , a Gaussian approxima­
tion of the posterior can be quite bad and one may not want

to use it regardless of the numerical integration accuracy.

In any c ase, when Gaussian quadrature is not well suited

for the problem, we can use other integration methods such
as adaptive i ntegration and Monte Carlo methods.

Existi ng methods

for i nference in augmented CLG net­
discretization,

samp1ing methods and variational methods. Discretization
is conceptual ly simple: we discretize every variable and
then use standard discrete inference.

Unfortunately, dis­

cretization requires a fine resolution for an

adequate repre­

sentation even of simple distributions and the situation de­

grades exponentially with the number of dimensions, mak­
ing the approach intractable for large clique trees.

Sampling is a general method that can handle non ­
standard distributions such as CD CPDs; it has a low space

complexity, and is guaranteed to converge as the number of
s ampl es

N goes to infinity. There are two main classes of

sampling algorithms: those based on likelihood weighting

(LW) and those based on MCMC. The advantages of LW

are its generality and simplicity. However, it suffers from a
few problems.
der of

First , the convergence rate is slow ( on the or­

1/VN).

In contrast, our algorithm converges much

faster in c ases where the integration dimension is low. Both

LW and our algorithm have problems when dealing with
unlikely evidence, but the problems are much worse in LW.

Continuous evidence i s very problematic in LW, due to the
exponential decay of the Gaussian distribution. This type
of evidence has no impact on the accuracy of our algorithm.

While there is some i mpact in the case of unlikely discrete
evidence, it is much Jess si gnificant than in LW; as shown
in

Section 6, our algorithm achieves substantially higher

accuracies than LW in the same amount of running ti me.
On top of the slow convergence of

1/VN of sampling

methods, MCMC methods converge very slowly when the
mixing rate of the Markov chain

is slow, which depends

i n unpredictable ways on the network parameters. In addi­

tion,

MCMC may run into problems in arbitrary complex

CD CPDs. To correctly sample a value for some variable,
one has to combine all the CPDs in which it is involved
into a samp ling distribution - if the CPDs are complex,

this task is not

trivial. It would have been very interest­

ing to compare the results of our algorithm to BUGS, but
problems in BUGS prevented us from doing so. However,
even o ur partial results imply that, at least in

the case of

the Emission network , BUGS requires many samples to
achieve convergence, while our algorithm produced instan­

taneous answers. We do not know whether the wrong con­
vergence is a simple implementation problem, or whether
it results from a more fundamental difficulty.

LERNER ET AL.

328

Our algorithm appears closer in spirit to the variational
approximation approach proposed in

[ 1 1 , 1 6] . However,

the variational approach is limited to binary softmax distri­
butions, while our algorithm is completely general. More
importantly, our algorithm is guaranteed to give the correct
answer (up to numerical integration errors), while the varia­
tional approach has no guarantees. It seems that variational
approximation would suffer from the same problems of un­
likely evidence as our approach. It is interesting to explore
whether the variational approximation is sensitive to issues
such as the slope of the sigmoids as is our algorithm. (I.e.,
is it more difficult to find a good setting for the variational
parameters when the sigmoids are sharp and as a result the
quality of the approximation degrades.)
The most significant limitation of our algorithm, shared
with the variational approach, results from its relationship
to Lauritzen's algorithm.

As shown in [9], even simple

CLG networks can lead to clique trees that are intractably
large. An important open problem is to devise approxima­
tion schemes that are suitable for hybrid networks where
Lauritzen's algorithm cannot be applied.
Acknowledgments.

We thank Ofer Levi-Tsabari

for use­

ful discussions regarding numerical integration methods.

This research was supported by ONR Young Investigator
(PECASE) under grant number NOOO 1 4-99- 1 -0464, and by
ONR under the MURI program "Decision Making under
Uncertainty", grant number N0001 4-00- 1 -0637. Eran Se­
gal was also supported by a Stanford Graduate Fellowship.

A

Parameters for Extended Crop Network

Policy variable takes the values Liberal and Conserva­
tive. The Rain variable takes the values Drought, Average,
and Floods. The Profit variable takes the values Loss, Even,
and Profit. The Subsidize and Buy variables are both binary.
The

Node
Policy
Rain
Subsidize

Crop

Price
Buy
Profit

Distribution

Drought
Drought
Average
Average
Floods
Floods
Drought
Average
Floods
Yes
No

Liberal
Conservative
Liberal
Conservative
Liberal
Conservative

Sub=Yes

Buy= Yes

Sub=Yes

Buy=No

Sub=No

Buy=Yes

Sub=No

Buy=No

(0.5, 0.5)
(0.35,0.6,0.05)
(0.4, 0.6)
(0.3, 0.7)
(0.95, 0.05)
(0.95, 0.05)
(0.5, 0.5)
(0.2, 0.8)
N(3,0.S)
N(S , l )
N(2,0.2S)
N(9-C, l )
N(12-C, l )
b=- 1 , w=7
f!=exp(l3-2P-PC)
fp=exp(3P+PC-23)
h=exp( 13-2P)
fp=exp(3P-23)
j,=exp(l 3-PC)
fp=exp(PC-23)
j,=exp(l3)
fp=exp(-23)

Where
/1

+{+]p ,

UA1 2001

Jr+{�1P , P (Profit;.;;;Even)
and P (Profit=Profit)
!<-It"+

P(Profit=Loss)

=

=

fp .

