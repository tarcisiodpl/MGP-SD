

This paper discusses how conflicts (as used by
the consistency-based diagnosis community) can
be adapted to be used in a search-based algorithm
for computing prior and posterior probabilities
in discrete Bayesian Networks. This is an "any­
time" algorithm, that at any stage can estimate the
probabilities and give an error bound. Whereas
the most popular Bayesian net algorithms exploit
the structure of the network for efficiency, we ex­
ploit probability distributions for efficiency; this
algorithm is most suited to the case with extreme
probabilities. This paper presents a solution to
the inefficiencies found in naive algorithms, and
shows how the tools of the consistency-based
diagnosis comniunity (namely conflicts) can be
used effectively to improve the efficiency. Em­
pirical results with networks having tens of thou­
sands of nodes are presented.
1

Introduction

There have been two, previously disparate, communities
working on model-based diagnosis. The first is in the UAI
community, where Bayesian networks have become the rep­
resentation of choice for modelling. The second is the
community built on logic-based notions of diagnosis, and
is typified by the use of consistency-based diagnosis [Gene­
sereth, 1984; Reiter, 1987; de Kleer and Williams, 1987;
de Kleer et al., 19901.
The basis of consistency based diagnosis is the use of
the conflict [Reiter, 1987; de Kleer and Williams, 1987;
de Kleeret al., 1990]. A conflict is a set of assumptions, the
conjunction of which is inconsistent with the observations
and the system description. The model-based diagnosis
community has recently seen the need for the use of proba­
bilities in reducing the combinatorial explosion in the num­
ber of logical possibilities [de Kleer and Williams, 1987;
de Kleer, 1991]. This brings their work closer to that of
the uncertainty community. The efficiency of these algo­
rithms, and the other issues faced by this community (e.g.,
*Scholar, Canadian Institute for Advanced Research

the use of abstraction [Genesereth, 1984]) mean that their
work cannot be ignored by the uncertainty community.
This paper provides a general purpose search-based tech­
nique for computing posterior probabilities in arbitrarily
structured discrete 1 Bayesian networks. It is intended to be
used for the case where there are extreme probabilities (see
[Poole, 1993]). This paper shows how a major problem
of practical efficiency can be solved by the use of a proba­
bilistic analogue of the 'conflict' used in consistency-based
diagnosis.
The main contributions of this paper are:
1. For the Bayesian net community, this paper provides a
new search-based mechanism for computing probabil­
ities in discrete Bayesian networks, that has practical
significance for networks with extreme probabilities
(i.e., each conditional probability is close to one or
zero). This has been tested for networks with tens of
thousands of nodes.
2. For the model-based diagnosis community, this pa­
per provides a new representation for problems that is
more general and more natural than previous represen­
tations. The algorithm gives a way to determine the
accuracy of probability estimates.
3. It provides a way to bring the model-based diagno­
sis and probabilistic diagnosis communities together,
with a common set of problems, and a common set of
solutions.

Implementations of Bayesian networks have been placed
into three classes [Pearl, 1988; Henrion, 1990]:
1. Exact methods that exploit the structure of the network
to allow efficient propagation of evidence rPearl, 1988;
Lauritzen and Spiegelhalter, 1988; Jensenet at., 19901.
2. Stochastic simulation methods that give estimates of
probabilities by generating samples of instantiations
of the network (e.g., [Henrion, 1988; Pearl, 1987]).
3. Search-based approximation techniques that search
through a space of possible values to estimate proba­
bilities (e.g., [Henrion, 1991; D'Ambrosio, 1992]).
1 All of the variables have a finite set of possible values. We
do not consider variables with an infinite set of possible values.

360

Poole

The method presented in this paper falls into the last class.
While the efficient exact methods exploit aspects of the net­
work structure, we instead exploit extreme probabilities to
gain efficiency. The exact methods work well for sparse
networks (e.g., are linear for singly-connected networks
[Pearl, 1988]), but become inefficient when the networks
become less sparse. They do not take the distributions into
account. The method in this paper uses no information
on the structure of the network, but rather has a niche for
classes of problems where there are "normality"2 condi­
tions that dominate the probability tables (see Section 3).
The algorithm is efficient for these classes of problems,
but becomes very inefficient as the distributions become
less extreme (see [Poole, 1993] for a detailed average-case
complexity analysis of the simple version of the algorithm
presented here (without conflicts)). This algorithm should
thus be seen as having an orthogonal niche to the algorithms
that exploit the structure for efficiency.

2

Background

A Bayesian network [Pearl, 1988] is a graphical represen­
tation of (in)dependence amongst random variables. A
Bayesian network is a directed acyclic graph where the
nodes represent random variables. If there is an arc from
variable B to variable A, B is said to be a parent of A. The
independence assumption of a Bayesian network says that
each variable is independent of its non-descendents given
its parents.
Suppose we have a Bayesian network with random vari­
ablesXl, .. . , Xn. The parents of X; arewrittenasllx, =

( X;"···, X;k,)

vals(X;) is the set of possible
values of random variable X; .
. Suppose

Associated with the Bayesian network are conditional prob­
ability tables which gives the conditional probabilities of
the values of X; depending on the values of its parents
llx,. This consists of, for each v; E vals(Xj) and
Vi; E vals(X;J, probabilities of the form

P(X; = v;IX;1

=

v;1

1\

···I\ X;k·
.

=

v;k.)

values to variables and the standard logical connectives)
in a possible world is determined using the standard truth
tables.

3

Searching possible worlds

For a finite number of variables with a finite number of
values, we can compute the probabilities directly, by enu­
merating the possible worlds. This is however computa­
tionally expensive as there are exponentially many of these
(the product of the sizes of the domains of the variables).
The idea behind the search method presented in this paper
can be obtained by considering the questions:
•

Can we estimate the probabilities by only enumerating
a few of the possible worlds?

•

How can we enumerate just a few of the most probable
possible worlds?

•

Can we estimate the error in our probabilities?

•

How can we make this search efficient?

This paper sets out to answer these questions, for the case
where the distribution is given in terms of Bayesian net­
works.

3.1

Ordering the variables

The first thing we do is to impose a total ordering on the
variables that is consistent with the ordering of the Bayesian
network. We index the random variables X1 , .. . , Xn so that
the parents of a node have a lower index than the node. This
can always be done as the nodes in a Bayesian network
form a partial ordering. If the parents of X; are llx, =

( X;,, ···, X;k,)

'

.

the total ordering preserves ii <

i.

The reason that we are interested in this ordering is that we
can determine the probability of any formula given just the
predecessors of the variables in the total ordering (as the
parents of variables are amongst their predecessors).

.

For any probability distribution, we can compute a joint
distribution by

n

P(X1, ··, Xn) ==.IT P(X;IITx.).

3.2

Search Tree

We are now in a position to determine a search tree for
Bayesian networks3•

·

i=l

This is often given as the formal definition of a Bayesian
network.
We call an assignment of values to all the variables a pos­
sible world, and write 'w f= X; = v;' if X; is assigned
value v; in world w. Let n be the set of all possible worlds.
The truth value of a formula (made up of assignments of
2This should not be confused with "normal" as used for Gaus­
sian distributions. We consider systems that have normal operat­
ing conditions and only rarely deviate from this normality (i.e., we
are assuming abnormality [McCarthy, 1986] is rare). As we are
only considering discrete variables, there should be no confusion.

Definition 3.1 A partial description is a tuple of values
( v1, ·· · , vi) where each v; is an element of the domain of
variable X;.
The search tree has nodes labelled with partial descriptions,
and is defined as follows:
•

The root of the tree is labelled with the empty tuple
(where j
0).

()

=

3This search tree is the same as the probability tree of [Howard
and Matheson, 1981] and corresponds to the semantic trees used
in theorem proving [Chang and Lee, 1973, Section 4.4], but with
random variables instead of complementary literals.

The use of conflicts in searching Bayesian networks

361

Note that each partial description can only be generated
once. There is no need to check for multiple paths or loops
in the search. This simplifies the search, in that we do
not need to keep track of a CLOSED list or check whether
nodes are already on the OPEN list (Q in Figure 1) [Pearl,
1984].

Q:={()};
W:={};
While Q # {} do

choose and remove (VI, ···, vj) from Q;
if j = n
then W := W U {(vi, ···, Vj}}
else Q : = Q U {(vi, . . . , Vj, v) : v E vals(Xj+l)} No matter which element is chosen from the queue at each
.
time, this algorithm halts and when it halts W is the set of
all tuples corresponding to possible worlds.
Figure 1: Basic search algorithm

•

The children of node labelled with (VI, ··, v j) are
the nodes labelled with (vi, ···, Vj, v) for each v E
vals(Xj+l)· In other words, the children of a node
correspond to the possible values of the next variable
in the total ordering.
·

Estimating the Probabilities

4

be

Tuple (v1 , ··· , vj) corresponds to the variable assignment

If we let the above algorithm run to completion we have an
exponential algorithm for enumerating the possible worlds
that can
used for computing the prior probability of any
proposition or conjunction of propositions. This is not,
however, the point of this algorithm. The idea is that we·
want to stop the algorithm part way through, and use the set
of possible worlds generated to estimate the probabilities
we need.

We associate a probability with each node in the tree. The
probability of the node labelled with (v1 , ···, vi) is the
probability of the corresponding proposition which is

be

•

The leaves of the tree are tuples of the form
(v1 , ···, vn)· These correspond to possible worlds.

Xt = Vl 1\ ... 1\ Xj = Vj.

P(Xt = VII\ . . ·I\ Xj = Vj)
j
= IJ P(X; = v;IX;, = v;1
i=l

We use W, at the start of an iteration of the while loop, as
an approximation to the set of all possible worlds. This can
done irrespective of the search strategy used.

4.1
1\

· · ·1\ X;k. = v;k.)
.

.

This is easy to compute as, by the ordering of the variables,
all of the ancestors of every node have a value in the partial
description.

be

Prior Probabilities

P(g) =

The following lemma can
trivially proved, and is the
basis for the search algorithm.

The probability of a node is equal to the sum
of the probabilities of the leaves that are descendents ofthe
node.

Lemma 3.2

This lemma lets us bound the probabilities of possible
worlds by only generating a few of the possible worlds
and placing bounds on the sizes of the possible worlds we
have not generated.
3.3

Searching the Search Tree

To implement the computation of probabilities, we carry
out a search on the search tree, and generate some of the
most likely possible worlds. Figure 1 gives a generic search
algorithm that can
varied by changing which element is
chosen from the queue. There are many different search
methods that can
used [Pearl, 1984].

be
be

The idea of the algorithm is that there is a priority queue
Q of nodes. We remove one node at any time, either it is
a total description (i.e., where j = n) in which case it is
added to W, the set of generated worlds, or else its children
are added to the queue.

bebe

Suppose we want to compute P(g). At any stage (at the
start of the while loop), the possible worlds can
divided
into those that are in W and those that will
generated
fromQ.

=

wEOI\Lwf=g
)
CEFu
)
Cwbegene£fromQwFu
P(w)

P(w)

P(w)

+

We can easily compute the first of these sums, and can
bound the second. The second sum is greater than zero
and is less than the sum of the probabiliti�s of the partial
descriptions on the queue (using Lemma 3.2). This means
that we can bound the probabilities of a proposition based
on enumerating just some of the possible worlds. Let

g wEWI\Lwf=g
tLEQ

Pw-

­

Pq =

P(w)

P(t)

Lemma 4.1

Pfv 5o p(g) 5o Pfv

+

Q

p

362

Poole

can be determined whether a is true in that partial
description. When conditioning on our observations
we can prune any partial description that is inconsistent
with the observations.

As the computation progresses, the probability mass in the
queue PQ approaches zero and we get a better refinements
on the value of P(g). Note that PQ is monotonically non­
increasing through the loop (i.e PQ stays the same or gets
smaller through the loop). This thus forms the basis of an
"anytime" algorithm for Bayesian networks.

4.2

•

[Korf, 19851 As we are not concerned with finding
the most likely possible world, but a set of most likely
worlds, we can carry out depth-bounded depth-first
searches (not generating nodes whose probability is
below a threshold), without worrying too much about
decreasing the threshold to the maximum value it could
obtain.

Posterior Probabilities

The above analysis was for finding the prior probability
of any proposition. If we want to compute the posterior
probability of some g given some observations obs, we can
use the definition of conditional probability, and use

P(g I0b8)

_

-

P(g 1\ obs)
P(obs)

f:

Given estimates of P(g 1\ obs) and P(obs) , (namely P

b

o s

and Pf/JS), it can be proved [Poole, 1993] that P(globs) has
the following bound:

pgl\
o
b
s
Pobs

Theorem4.2

w

w

+ P.Q

- pgl\pobosbs
(pf:'obs Pf:'obs )

<
-

P(globs)

<

w

w

+ P.Q

+ P.Q

If we choose the midpoint as an estimate, the maximum

error is

!

+

pQ
+
PV(}8 PQ

2
=

_

P V(}8 + PQ

PQ
2(PV(}8 + Pq)

What is interesting about this is that the error is independent

of g. Thus when we are generating possible worlds for some
observation, and want to have posterior estimates within
some error, we can generate the required possible worlds
independently of the proposition that we want to compute
the probability of.

Discussion

5
5.1

Refinements to the Search Algorithm

There are a number of refinements that can be carried out
to the algorithm of Figure 1, independently of the search
strategy. Some of these are straightforward, and work well.
The most straightforward refinements are:
•

If we know our query and the conditioning variables,
we can prune those variables that cannot affect the an­
swers. We can prune any variables that are d-separated
from the query variables by the observations [Pearl,
1988]. We can prune any variable that is not an an­
cestor of the observations or the query. The above
two pruning steps can be done repeatedly [Baker and
Boult, 1990]

•

If we are trying to determine the value of P(a), we
can stop enumerating the partial descriptions once it

Another alternative is an iterative-deepening search

5.2

Extreme Probabilities

The improvements to the search algorithm below assume
we have extreme probabilities. This means that for each
variable X; and for each instantiation of the parents of X;,
there is one value v; for which

P(X; = v;IX;,

=

v;1

1\

· · ·

1\

X;k;

=

v;k;)

has a probability close to one (and so the conditional prob­
abilities of the other values for

X; are all close to zero).

X; is normal in possible world w
v;IX;1 = v;1 1\
1\ X;k· = v;k.) � 1 where
w f= X; = v; and w f= X;j = ;ij for 0 < j ::; k;.
Otherwise we say X; is a fault in possible world w.

Definition 5.1 Variable
if P(X;

=

· · ·

See [Poole, 1993] for a more detailed discussion of the use
of extreme probabilities, and why these extreme probabili­
ties guarantee convergence of the search.

6

A Diagnosis Example

In this section we describe how the search procedure can
be applied to a simple circuit diagnosis problem (as in [de
Kleer, 1991]), from which we can learn what problems
arise. The translation of the circuit into a Bayesian network
will follow that of Pearl [1988, Section 5.4].
The circuit is a sequence of a one-bit adders, cascaded to
form a multiple-bit adder4.
6.1

Representation

Figure 2 shows a one bit adder. Figure 3 shows the corre­
sponding Bayesian network.

In this Bayesian network the random variable out-a2 is a
binary random variable that has two values on meaning that

4There is actually an efficient algorithm for such an example
using a clique hypertree representation [Lauritzen and Spiegelhal­
ter, 1988; Jensen eta/., 19901 This exploits the local nature of

the propagation, which we do not exploit. These would not work
so well when the structure cannot be exploited as well as for the
cascaded adders, for example, if we add to the circuit another cir­
cuit to find the parity of the resulting bits. We chose this example

as it is simple to extend to large systems and also because it was
used in [de Kleer, 19911.

The use of conflicts in searching Bayesian networks

i1 -----,,-----\
i2 -r-1--/

xl

x2

,------}v-

a2ok

i3

out-xi

ok
ok
ok
ok
stuck1
stuckO

on
on

on

off
off

off

on

- -off
- -

363

out-a2
on off
1
0
1
0
1
0
1
0
1
0
1
0

Figure 4: Conditional probability table for variable out-a2.

ok
0.99999

a2ok

I stuck1 I stuckO
1 o.oooo5o 1 o.ooooo5

Figure 5: Conditional probability table for variable a2ok.
Figure 2: 1 bit adder
the output of gate a2 is on, and off meaning the output of
the gate a2 is off. The random variable a2ok has three
values: ok meaning that the gate a2 is working correctly,
stuck1 meaning the gate a2 is broken, and always has on
and stuckO meaning the gate a2 is broken, and always has
off.
The value of out-a2 depends on the values of three other
variables, i3, out-x 1, and a2ok. The values for the variable
out-a2 follow the table in Figure 4. The tables for the other
outputs of gates is similar.
The value of a2ok does not depend on any other variables.
The values for the variable follow the table in Figure 5.5
The tables for the status of other gates is similar.
These one-bit adders can be cascaded for form multiple bit
adders. This is done in the circuit by connecting the output
of gate o1 in one adder to input i3 of the following adder.
In the Bayesian network, this is done by having multiple
instances of the network for the one-bit adder with the value
of i3 depending on the variable out-a 1 for the previous
instance of the adder. The table for the probabilities is
given in figure 6. The value of the output of gate x2 of bit
k, is called the output of bit k; the value of the output of o1
is called the carry.
6.2

Figure 3: Bayesian network for a 1 bit adder

Computation

Suppose we apply the algorithm of Figure 1 to our cascaded
adder example with the partial description with the highest
prior probability chosen each time through the loop. First
the world with all gates being ok is generated followed
by the worlds with single faults. Most of these can be
pruned quickly (see Section 5.1). Then the double stuck­
at faults are generated, etc. The probability in the queue
converges very quickly [Poole, 1993]. Each of the elements
of the queue can be characterized by what errors are in the
5The numbers are purely made up. It may seem as though
these probabilities are very extreme, but a 1000 bit adder (with
5000 components), is only 95% reliable, if all of the gates are as
reliable as that given in this table.

364

Poole

out-o1k-I
on
off

i3k
on off
1
0
1
0

Figure 6: Conditional probability table for input 3 of adder

k.

g((vi,···,vj))
= P(XI = VI 1\ ... 1\ Xj = Vj)
j
= JI P(X; = v;jX;1 = v;1 1\ ···A X;k, = v;k,)
i=I
= P(Xj = Vji Xii = Vjl 1\ ... 1\ Xjk. = Vjk.)
J

partial description. We typically only generate the partial
descriptions with only a few of the errors.
This is essentially the candidate generator phase of [de
Kleer, 1991]. From this candidate generation, we can com­
pute all of the probabilities that we need to.
To see what computational problem arises, consider a 1000bit adder. Suppose all the inputs are zero, and all outputs,
except bit k, are zero, and bit k outputs one (this example
is from [de Kleer, 1991]). Fork > 1 there are five most
likely possible worlds (that correspond to x2okk =stuck1,

xlokk =stuck!, olokk-I =stuck!, a2okk-I =stuckl,

stuckl). We first choose the most likely
and a1okk-l
values of all variables (i.e., the ok state for all of the status
nodes). W hen we get to the output of bit k, which is pre­
dicted to be zero, we find that our prediction is inconsistent
with the observations. At this stage, we prune the search
and consider the single-fault possible worlds. For each bit
after bit k, we have already assigned a single fault (to ac­
count for the error in bit k ), thus for each of these gates, we
only consider the ok state. For all of the gates before bit
k, we consider each of the failure states. When generating
worlds with just single faults, there is no point in trying each
of the failure states for the gates before bit k - 1 as each of
these failure states will have to be combined with another
failure state to account for the error. We would like to not
consider faults that we know will have to be combined with
other faults until we need to (when considering double fault
worlds these may have to be considered). Learning what
we can about expectation failure and using this information
for pruning the search is the idea behind the use of cQnflicts.
=

7

probability of the corresponding proposition:

Search Strategy and Conflicts

T he above example assumed a simple search strategy. We
can carry out various search strategies, to enumerate the
most likely possible worlds. Here ·we present one that
incotporates an analogous notion of conflict to that used
in consistency-based diagnostic community [de Kleer and
Williams, 1987; Reiter, 1987; de Kleer eta!., 1990].
We carry out a multiplicative version6 of A* search [Pearl,

1984] by choosing the node m from the queue with the
highest value of f(m) = g(m) x h(m). Here g(m) is the
6This is an instance of Z* where, instead of adding the costs

and choosing the minimum we multiply and choose the maximum.
This can be transformed into a more traditional A* algorithm by
taking the negative of the logarithms of the probabilities. We do
not do this explicitly as we want the probabilities to add after the
search.

3

xg((vr, ·· · , Vj-I})
The heuristic function h( (VI, ···,vi)) is the product of the
maximum probabilities that can be obtained by variables
Xi+l ···Xn (for any values of the predecessors of these
variables). Initially, these can be computed by a linear scan
(from Xn to X1) keeping a table of the maximum products.
We use a notion of conflicts to refine the heuristic function
as computation progresses. This is defined in terms of
normality (Section 5.2), and is closest to that of [Reiter,

19871.

Definition 7.1 Given an observation o, a conflict is a set C

of random variables such that there is no possible world in
which o is true and all elements of C are normal. In other
words, if o is true, one of the elements of C is a fault (and
so has probability close to zero, no matter what values are
assigned to variable outside of C).

Associated with a conflict is a maximum probability which
is an upper bound on the prior probability of any assigmnent
of values to variables in the conflict that is consistent with
the observation.
Two conflicts are independent if there is no single variable
that can account for both conflicts. That is, CI and C2 are
independent if in all possible worlds in which the observa­
tion is true there are at least two faults, one in CI and one
in C2. This happens, for example, if the conflicts have no
variables in common.
Example 7.2 In our example of Section 6, with all inputs

zero, and bit 50 having output one and all other outputs
being zero, there is one minimal conflict, namely:

{ out-x2so. x2okso, i3so. out-ol49, olok49, out-al49,
alok49, i249, out-a249, a2ok49, out-xl49, xlok49, il49, i249,
out-x1so. xlokso. i1so. i2so}.

A conflict corresponds to a set of normal values that cannot
consistently coincide given the observation. Conflicts dis­
covered in the search can be used to prune the search earlier
than it could be pruned without the coirllict. There are a
number of issues to be discussed:

1. How can conflicts be used by the search algorithm?
2. How can conflicts be discovered?
3. How does the use of conflicts affect the estimation of
probabilities?

4. How much does the use of conflicts save in search
time?

5. In practice, how often can we detect a small set of
variables that form a conflict?

365

The use of conflicts in searching Bayesian networks

In this paper we answer all but the last of these questions.

without doing lots of search. This is where the extraction

7.1

predicted in the current partial description (otherwise X;
would have been predicted to have value o ). We use the

The last question we cannot answer until we have built
many more systems for many diverse applications.

Refining the heuristic function

We can use a conflict to update the heuristic function. In
particular a conflict can change the bound on the probability
that the rest of the variables can take on.

The simplest idea is that h( (v1,
, Vj)) is the product of
the maximum probabilities of the independent conflicts that
involve only variables after variable Xj7•
· · ·

A discovered conflict updates the heuristic function for all

the variables before (in the total variable ordering) the con­
flict. The heuristic function evolves as computation pro­
gresses and conflicts are found.

7.2

Finding confticts

We would like a way to extract conflicts from expectation
failures in our search. By the nature of the search, we first
consider the most likely values of variables. We only want
to consider faults that are forced upon us by conflicts.
Suppose that in the current partial description variable

X;

from the failure of an expectation comes into play. For
each conjunction, there was a conjunct whose negation was

procedure recursively to extract a counter to that assignment
from the current partial description.
The procedure extracLcounter(X;, o, 8) where X; is a
variable, o is a value and 8 is a partial description such
that X; = o is not true in 8, is defined as follows. Sup-

pose the parents of
each tuple

(v;,,

X;

are

)

(X;"

· · ·

, X;k,

)

.

Consider

, v;k, of values to the parents such
that P(X;
oiX;, = v;, 1\
1\ X;k. = v;k.) � 1.
Choose9 ij such that X;j =P v;j in the c.:m.ent pa'rtial de­
scription (i.e. X;j = v;j is not predicted). Recursively
call extracLcounter(X;j, v;j, 8). This returns the set of
variables all of which cannot be normal if X;j = v;j. The
value returned for extracLcounter(X;, o, 8) is then
· · ·

=

· · ·

U extracLcounter(X;i' v;i, 8)

{X;} U

P(X; = oiX;, = v;, 1\
1\ X;k· =
'
and 8 f= X;j =P v;j
· · ·

v;k· )

'

�

1

So when we have a failure of expectation caused by the

is assigned a value v; and it has been observed that X; = o
where o =P v;. We say that the value v; is predicted, and

observation

a conflict from the current partial description.

N.B. sometimes extracLcounter may fail to find a counter

we have a failure of expectation. We would like to extract

We want to extract conflicts as fast as possible, and do not
necessarily want to build the whole infrastructure of our

diagnosis system around :finding conflicts (as does de Kleer
[1991]). We would like to extract the conflicts from the
expectation failure directly. We are not particularly con­
cerned about :finding minimal conflicts8• Whether these are
reasonable design goals, or achievable in practice remains
to be seen.

A set of variables C is a counter to X; = o if there is
no possible world in which every variable inC is normal
and X; = o. A counter to a conjunction of assignments to
variables is defined analogously.

To generate a counter to

(

v;1, •

oiX;,

• •

=

)

X; = o,

we consider each tuple

, v;k, of values to the parents such that P(X; =
v;1 1\ . . 1\ X;k. = v;k.) � 1. A counter to X; = o
·

must contain a counter to. each of these conjunctions of
assignments of values to the parents of X;.

The idea of the algorithm extracLcounter that finds coun­
ters is that we recursively find counters of these assignments
to the parents, union them, add
counter to X; = o.

X; and return this set as a

The problem is how to :find the counter to the conjunction,
7 A more sophisticated version may count conflicts that contain
variables before Xi, (and do not include Xj) as long as they are
assigned normal values in ( v�, · · · , vi). We have only tested the
simpler idea.
8
Correctness does not depend on a conflict being minimal.

X;

=

return a conflict.

if 8

o,

then

extracLcounter(X;, o, 8)

will

contains a fault that produces the expectation failure.

In this case we cannot extract a conflict that is independent
of the conflict that forced the fault in

7.3

8.

Estimating probabilities

Naive use of the above procedure gives error estimates that
are too large. The problem is that there are quite large
probabilities on the queue that are not chosen because of
their heuristic values. Thus the value of PQ is much larger
than we may expect.

Suppose

m is an element on the queue, that is not chosen

because f( m) = g( m) x h( m) is too low. Although the set
of possible world rooted at m has probability g( m), most of
these are impossible if there is a conflict. We know at least
( 1 - h( m)) of the weighted sum of these possible worlds
must be inconsistent with the observations (by the definition
of h). This we should only count f(m) = g(m) x h(m)
as part of PQ. rather than g( m). This can then be used to
estimate probabilities, and gives a much better accuracy.

7.4

Experimental Results

The experiments we carried out were limited to understand­
ing the behaviour of the algorithm on cascaded n-bit adder
9Any choice will lead to a conflict. A bad choice may lead
to a non-minimal conflict. Our experiments were with a greedy
algorithm that chooses the first one found. There is a tradeoff

between the computational effort in finding minimal conflicts,
and the extra pruning that minimal conflicts allow.

366

Poole

I error bit
run time (no conflicts)
run time (with conflicts)
Figure 7: Runni ng time as a function of error bit in a 1 00-bit
adder.
#bits
#gates
#nodes
run time

100
500
1300
10

500
2500
6500
46

1000
5000
13000
92

2000
10000
26000
183

3000
15000
39000

275

Figure 8: Running time as a function of size of multiple-bit
adder.
example, with all inputs zero and all output bits being zero,
except for the output of bit k (i.e., the value of x2k) which
had value one. Note that an n-bit adder has 5n gates and
corresponds to a Bayesian network with 13n nodes. We ran
the program using a bounded depth-first search (pruning the
depth-first search when the /-value gets below a threshold),
generating the 5 most likely possible worlds.
All times are based on a SICStus Prolog program running
on a NeXTstation. All times are in seconds. The code is
available from the author.
The main problem with the search algorithm without con­
flicts, for our example, was how the runtime depended on
the bit k that was faulty. Figure 7 shows how run time
depends on the bit chosen for the program with no con­
flicts and for the program with conflicts. This was for the
100-bit adder (Bayesian network with 1300 nodes). The
difference in times for error bit 2 indicates the overhead in
using conflicts (as conflicts for this case gives us nothing).
Consider how the program runs; we pursue one world until
bit k, then pursue 5 worlds separately from bits k to n. Thus
we may estimate the time as proportional to k + 5(n - k).
This fits the experimental results extremely well.
The second experiment was with the asymptotic behaviour
as the size of the network was increased. Figure 8 shows
the run-time for finding the 5 most likely possible worlds,
as a function of circuit size. In each of these the error bit
was the middle bit of the circuit (i.e., k = '!). This was
chosen as it is the average time over all of the error bits (see
Figure 7). Note the linear time that was predicted by the
k + 5(n- k) formula.
Finally, the results from double errors, are very similar. For
a 100-bit adder, with ones observed at bits 3 0 and 70, the
program took 34 seconds to find the 25 most likely possible
worlds.

8

Comparison with other systems

The branch and bound search is very similar to the candidate
enumeration of de Kleer's focusing mechanism [de Kleer,

1991l We have considered a purely probabilistic version of
de Kleer's conflicts. We have extended the language to be
for Bayesian networks, rather than for the more restricted
and less well-defined language that de Kleer uses. We also
can bound the errors in our probabilistic estimates, which
de Kleer cannot do. One of the features of our work is that
finding minimal conflicts is not essential to the correctness
of the program, but only to the efficiency. Thus we can
explore the idea of saving time by finding useful, but non­
minimal conflicts quickly.
Shimony and Charniak [1990], Poole [1992a] and
D'Ambrosio [1992] have proposed back-chaining search al­
gorithms for Bayesian networks. None of these are nearly
as efficient as the one presented here. Even if we con­
sider finding the single most normal world, the algorithm
here corresponds to forward chaining on definite clauses
(see [Poole, 1992b]), which can be done in linear time,
but backward chaining has to search and takes potentially
exponential time.
This paper should be seen as a dual to the TOP-N algo­
rithm of Henrion [ 1991]. We have a different niche. We
take no account of the noisy-OR distribution that Henrion
concentrates on.
This paper deliberately takes the extreme position of seeing
how far we can get when we exploit the distributions and
not the structure of the network. Hopefully this can shed
light on the algorithms that use both the structure and the
distribution to gain efficiency (e.g., [D'Ambrosio, 1992]).

9

Conclusion

This paper has considered a simple search strategy for com­
puting prior and posterior probabilities in Bayesian net­
works. This uses a variation on A* search, and uses a
notion of 'conflict' to refine the heuristic function. One of
the aims of this work is to bring together the model-based
diagnosis community (e.g., [de Kleer, 1991]) and the un­
certainty in AI community, with a common set of problems
and tools.
In some sense this is preliminary work. We have not tested
this beyond the single example. It is not clear how easy it
will be in other examples to find conflicts without searching
for counters, nor how much the use of conflicts can save
us. The use of counters seems to be very different to ex­
ploitation of structure in other algorithms, but there may be
some, as yet undiscovered, relationship there.

Acknowledgements
Thanks to Craig Boutilier, Nevin Zhang, Runping Qi and
Michael Horsch for valuable comments on this paper. This
research was supported under NSERC grant OGP0044121,
and under Project BS of the Institute for Robotics and In­
telligent Systems.

The use of conflicts in searching Bayesian networks

