

without employing extensive terminology, thus mak­
ing the algorithms accessible to researchers working

This paper describes a class of probabilistic
approximation algorithms based on bucket
elimination which offer adjustable levels of
accuracy and efficiency. We analyze the ap­
proximation for several tasks: finding the
most probable explanation, belief updat­

in diverse areas.

More important, their uniformity

facilitates the transfer of ideas, applications, and
solutions between disciplines.

Indeed, all bucket­

elimination algorithms are similar enough to make
any improvement to a single algorithm applicable to
all others in its class.

ing and finding the maximum a posteriori

Normally, the input to a bucket-elimination algo­

hypothesis.

rithm consists of a knowledge-base theory specified

We identify regions of com­

pleteness and provide preliminary empiri­

by a collection of functions or relations, (e.g., clauses

cal evaluation on randomly generated net­

for propositional satisfiability, constraints, or condi­

works.

tional probability matrices for belief networks) . In

its first step, the algorithm partitions the functions

1

into buckets, each associated with a single variable.

Overview

Given a variable ordering, the bucket of a partic­
ular variable contains the functions defined on that

Bucket elimination, is a unifying algorithmic frame­

variable, provided the function is not defined on vari­

work that generalizes dynamic programming to en­

ables higher in the ordering. Next, buckets are pro­

able many complex problem-solving and reasoning
activities.

Among the algorithms that can be ac­

commodated within this framework are directional
resolution for propositional satisfiability, adaptive
consistency for constraint satisfaction, Fourier and

cessed from top to bottom. When the bucket of vari­
able

X

in its bucket. The result is a new function defined
over all the variables mentioned in the bucket, ex­

Gaussian elimination for linear equalities and in­

cluding

equalities, and dynamic programming for combina­

of

torial optimization

[7].

Many algorithms for proba­

bilistic inference, such as belief updating, finding the

is processed, an elimination procedure or an

inference procedure is performed over the functions

X

X.

This function summarizes the "effect"

on the remainder of the problem.

The new

function is placed in a lower bucket. For illustration
we include algorithm elim-mpe, a bucket-elimination

most probable explanation, finding the maximum a

algorithm for computing the maximum probable ex­

posteriori hypothesis, and calculating the maximum

planation in a belief network

expected utility, also can be expressed as bucket­
elimination algorithms

[3].

The main virtues of this framework are simplic­
ity and generality.

By simplicity, we mean that

complete specification of these algorithms is feasible
Thls work was partially supported by NSF grant
IRI-9157636 and by Air Force Office of Scientific Re­

( Figure 1) [3].

An important property of variable elimination algo­
rithms is that their performance can be predicted
using a graph parameter called induced width

( also called

[5},

tree-width[l ]) , which is the largest clus­

ter in an optimal tree-embedding of the graph. In
general, a given theory and its query can be asso­
ciated with an interaction graph describing various

search grant, AFOSR 900136, Rockwell International

dependencies between variables. The complexity of

and Amada of America.

bucket-elimination algorithms is time and space

ex-

133

A scheme for approximating probabilistic inference
ponential in the induced width of the problem's in­
teraction graph.

Depending on the variable order­

ing, the size of the induced width will vary and this
leads to different performance guarantees.
When a problem has a large induced width bucket­
elimination is unsuitable because of its extensive
memory demand. Approximation algorithms should
be attempted instead. We present here a collection
of parameterized approximation algorithms for prob­
abilistic inference that approximate bucket elimina­
tion with varying degrees of accuracy and efficiency.
In a companion paper

[4],

we presented a similar ap­

proach for dynamic programming algorithms, solv­
ing combinatorial optimization problems, and belief
updating. Here we focus on two tasks: finding the
most probable explanation and finding the maxi­
mum a posteriori hypothesis. We also show under
what conditions the approximations are complete
and provide preliminary empirical evaluation of the
algorithms on randomly generated networks.
After some preliminaries ( section

2),

we develop the

approximation scheme for the most probable expla­
nation task

4), and for
(section 5).

( section 3 ) ,

for belief updating (section

the maximum a posteriori hypothesis
We summarize the results of our em­

pirical evaluation in section

6.

Related work and

conclusions are presented in section

2

7.

Preliminaries

Definition 2.2 (belief networks)

Let X = {X1,... , Xn} be a set of random variables
over multivalued domains D1, ..., Dn. A belief net­
work (BN) is a pair (G, P) where G is a directed
acyclic graph and P = {P;}. P; = {P(X,Ipa(X,))}
are the conditional probability matrices associated
with X,. An a11signment (X1 = :llt, .. .,Xn = :!ln }
can be. abbreviated to :z: = ( :ct. . . .,:cn)· The BN
represents a probability distribution P(zt,. .. . , :en ) =
n�=lP(:z:,l:z:pa(x,)), where, Zs i!l the projection of z
over a sub11et S. if u is a tuple over a &ubset X,
then us denotes that assignment, which is restricted
to the variables in S n X. An evidence set e is an
instantiated subset of variables. We use (us, :cp) to
denote the tuple us appended by a value :z:P of Xp,
where Xp is not in S. We define i, = (:c1,...,z,)
and i{ = (z;,:z:i+l• ... , Zj).
Definition 2.3 (elimination functions) Given a
function h defined over subset of variables S,
where X E S, the functions (minxh), (ma:cxh),
(meanxh), and CL:x h) are defined over U
u,
For every U
S - {X} as follows.

mi� h(u,z), (ma:z:xh)(u)
(minxh)(u)
I: .. h(u, :z:), and
h(u,:z:), (L:x h)(u)
(meanxh)(u) = L: .. h�ij), where lXI i6 the car­
dinality of X 's domain. Given a 6et of functions
ht,..., h; defined over the 11ubset6 81, . . . , S1, the prod­
uct function (Tij hi) and L: 1 h1 are defined over U =
UjSj. For every U = u, (TI1h1)(u) = TI1hi(us3) and
(L:1 hj)(u) = Lj hj(us;)·

m�

Definition 2.1 (graph concepts)

A directed graph is a pair, G

A poly-tree is an acyclic directed graph whose under­
lying undirected graph (ignoring the arrows) has no
loops. The moral graph of a directed graph G is the
undirected graph obtained by connecting the parents
of all the node11 in G and then removing the arrows.

=

{V,E}, where

V = {X1, ... , Xn} is a set of elements and E =
{(X,,Xi)IX,,Xj E V } is the set of edges. If
(X;,Xj) E E, we say that X; points to Xi. For each
variable X;, pa(X;) or pa;, is the 11et of variable11
pointing to X, in G , while the. 11et of child node11 of
X,, denoted ch(X,), compri11e!l the variables that X1
point11 to. The family of X;, F,, includes X1 and its

child variable11. A directed graph is acyclic if it ha11
no directed cycle11. An ordered graph i!l a pair (G, d)
where G i11 an undirected graph and d = X11 ... ,Xn is
an ordering of the nodes. The width of a node in an
ordered graph i!l the number of the node 18 neig hbors
that precede it in the ordering. The width of an or­
dering d, denoted w(d), is the maximum width over
all nodes. The induced width of an ordered graph,
w * (d), is the width of the induced ordered graph ob­
tained by processing the node!l from last to first,· when
node X is p rocessed, all its neighbor11 that precede it
in the ordering are connected. The induced width of
a graph, W*, is the minimal induced width over all
it!l ordering!lj it is al!lo known as the tree-width [1].

Definition

2.4

(probabilistic tasks)

The mo11t probable explanation (mpe) task is to find
an anignment :z:0 = ( :z:011 . . . , :Z:0n) such that p(:z:0) =
ma.x.z,. Ilf=1 P(:c,, el:z:p4;). The belief assessment task
of X = :z: is to find bel(:z:) = P(X = :z:le) . Given
a set of hypothe11ize.d variables A = {At, ..., A�:},
A � X, the maximum a posteriori hypothesis (map)
task is to find an as6ignment a0 = (a01, ..., a0�c ) such
that p(a0) = ma.x;,11 L:'".x-A Ilf:::::1P(:z:;l:cpa,,e ) .
3

Approximating the mpe

Figure

mpe [3]

1

shows bucket-elimination algorithm

for computing mpe.

elim­

Given a variable or­

dering and a partitioning of the conditional proba­
bilities into their respective buckets, the algorithm

134

Dechter and Rish

Algorithm elim-mpe
Input: A belief network BN = {P1, ..., Pn}; an or­
dering of the variables, d; observations e.
Output: The most probable assignment.
1. Initialize: Partition BN into buckeh, ..., bucketn,
where bucket; contains all matrices whose highest vari­
able is X,. Put each observed variable into its appro­
priate bucket. Let S1, .. . , S; be the subset of variables
in the processed bucket on which matrices (new or old)
are defined.
2. Backward: For p +- n downto 1, do
for h1,h3, ...,h; in bucket,, do
• (bucket with observed variable) If bucket, contains
X, = :c,, BIIBign X, = a:, to each h; and put each
resulting function into its appropriate bucket.
.
• Else, generate the functions h": h" = ma:cx ll�
,. =l h;
and :r:; = argm.azx,.h". Add h" to the bucket of the

UL,

largest-index variable in U, +S;- {X,}.
3. Forward: Assign values in the ordering d using
the recorded functions :c0 in each bucket.

Figure 1: Algorithm

bottom. When processing the bucket of Xp, a new
function is generated by taking the maximum rel­
ative to Xp, over the product of functions in that
bucket. The resulting function is placed in the ap­
propriate lower bucket. The complexity of the al­
gorithm which is determined by the complexity of

2},

is time and space

exponential in the number of variables in the bucket
(namely the bucket's variable induced-width) and
is, therefore, time and space exponential in the
induced-width

w•

of the network's moral graph

[3].

Since the complexity of processing a bucket is tied
to the arity of the functions being recorded, we pro­
pose to approximate these functions by a collection
of smaller arity functions. Let

ht,..., h;

be the func­

tions in the bucket of Xp, and let St, ... , Sj be the
variable subsets on which those functions are d�
fined. When

elim-mpe processes the bucket of Xp, it
hP: hJ' = mazx,.II{=1�. One

computes the function

brute-force approximation method involves generat­
ing, instead, by migrating the maximization oper­
ator inside the multiplication, a new function

gP

=

rr{=lmazx,.hs.

Since each function

product of hJ' is replaced by

uct defining

gP, hJ' � gP.

mazx,.hs

hs

gP:

in the

in the prod­

We see that

gP has

a

product form in which the maximizing elimination
operator

gP's

mazx,. hs

This idea can be generalized to yield a collection
of parameterized approximation algorithms having
varying degrees of accuracy and efficiency. Instead
of applying the elimination operator (i.e., multiplica­
tion and maximization) to each singleton function in
a bucket as suggested in our brute-force approxima­
tion above, it can be applied to a more coerced par­
titioning of the buckets into mini-buckets. Let

{Qt, ...,Q,.}

Ql

=

be a partitioning into mini-buckets of

ht, ... , hi in Xp's bucket. The mini­
Q, contains the functions h11, ..., hlr· Algo­
rithm elim-mpe computes hP: (l index the mini­
buckets) hP = rnazx,.IIf=t� = mazx,.II/=1II,,h,,.
the functions

bucket

By migrating the maximization operator into each
mini-bucket, we get:

�'

=

II1=1 mazx,.II,,h,,.

As

the partitionings are more coerced, both the com­
plexity and the accuracy of the algorithm increase.

elim-mpe

starts processing buckets successively from top to

processing each bucket (step

mpe.

is applied separately to each of

component functions. The resulting functions

will never have dimensionality higher than

�.

and

each of these functions is moved, separately, to a

Definition 3.1 Partitioning Q' i! a refinement of
Q" iff for every !et A E Q' there ea:i!U a !et B E Q"
such that A C B.
Proposition 3.2 I'! in the bucket of Xp, Ql i! a
refinement of Q", then hP� �, � �·. 0
Algorithm

approz-mpe{i,m) is described

in Figure

2.

It is parameterized by two indexes that control the
partitionings.

Let H be a collection of function.
ht. ..., hi defined on subseu of variables, 81, ... , Si.
A partitioning of H u canonical if any function
whose argumenu are sub!umed by another function
belong! to a bucket with one of those !ubsuming
function.. A partitioning Q into mini-buckeu u
an (i, m)-partitioning iff 1. it u canonical, 2. at
most m nomubsumed function! participate in each
mini-bucket, 3. the total number of variables in a
mini-bucket doe! not ezceed i, and 4. the partition­
ing is refinement-maximal, namely, there is no other
( i,m)-partitioning that it refines.
Definition 3.3

If indez i is at least as large a! a
family size, then there ea:i!t an (i, m)-partitioning of
each bucket. 0

Proposition 3.4

Algorithm approz-mpe(i,m) com­
putes an upper bound to the mpe in time O(m
ezp(2i)) and space O(m e:z:p(i)), where i � n and
m� 2i.
Theorem 3.5

·

·

as m

lower bucket. When the algorithm reaches the first

Clearly, in general,

variable, it has computed an upper bound on the

accurate approximations.

and

i

increase we get more

135

A scheme for approximating probabilistic inference

Algorithm approx-rnpe(i,rn)
Input: A belief network BN =

{Pt, .. . ,Pn}i

ordering of the variables, d;
Output: An upper bound on the most probable as­

signment, given evidence e.
1. Initialize: Partition into buclcet1, ..., bucketn,
where bucket; contains all matrices whose highest vari­

able is X;. Let St, ... , S; be the subset of variables in
bucketp on which matrices old or new are defined.

2.
•

)

(

For p f- n downto 1, do
lfbucketp contains Xp = Zp, assign Xp =

(Backward)

h; and put each in appropriate bucket.
• el s e, for h1, h,, .. . ,h; in buclcetp, do:
Generate an

{Q�, ... , Qr}.

Zp

to each

(i, m)-mini-bucket-partitioning, Q'

For each

do,

Qr

E

Q

1

containing hr11

=

hr,

•••

1
1
1
Generate function h , h = mazxPII!=1hr,. Add h
to the bucket of the largest-index variable in Ur f-

ULl s,, - {Xp}•

3. (Forward) For i = 1 to n do, given Zt, •.• ,:z: p-1
choose a value :&p of Xp that maximizes the product
of all the functions in Xp's bucket.

Figure

2:

algorithm

In
maznP(H[E, F) h1(H, G), and &o on.
bucket(B) obtain the mpe value mazBP(B). hc(B),
and then can generate the mpe tuple while going for­
ward. If we proceu by approx-mpe ( n,l ) in6tead,
we get (we denote by 'Y the function& computed by
appro:z:..elim{n, 1} that differ from tho6e generated by
elim-mpe):
bucket(!) = P(IIH, G)
bucleet(H} = P(H\E, F), h1(H,G)
bucket{ G} = P(G[E , D),"fH(G)
bucket{F) = P(F[B), 'YH (E,F),
bucket{E) = P(E\C,B),y(E),-yG (E,D)
bucleet{D) = P(D\C),"fE(D)
bucket{C) = P(C),-yE(C,B),'YD(C)
bucket(B) = P(B),-yc(B),"fF (B).
Algorithm& elim-mpe and approx-mpe ( n,l ) fir6t dif­
fer in their processing of bucket( G). There, in6tead of recording a function on three variable6,
hH(E, F, G), iu6t like elim-mpe, appro:z:..mpe(n,l)
record6 two function6, one on G alone and one on
E and F. Once approz-mpe{n,l} ha6 proceued all
bucket!, we can generate a tuple in a greedy fa6h­
ion a6 in elim-mpe: we choo6e the value of B tiWJt
marimize6 the product of function6 in B 's bucket,
then a value of C marimizing the product-function6
in bucket(C}, and so on.
·

and an

appro:c-mpe{i,m)

There is no guarantee on the quality of the tuple
we generate. Nevertheless, we can bound the error

3: A belief network P(i,h,g,e,d,c,b)=
P(i\h, g )P( h\e , f )P(gje,d)P (e \c, b)P(d\c)P(b)P(c)

Figure

of

appro:z:-mpe

by evaluating the probability of the

generated tuple against the derived upper bound,
since the tuple generated provides a lower bound on
the mpe.

Example 3.6 Con,ider the network in Figure 3.
A.uume we we the ordering (B, C, D, E,F, G,H,I)
to which we apply both algorithm elim-mpe and it!
6imple6t approrimation where m = 1 and i = n. Ini­
tially the bucket of each variable will have at mod
one conditional probability: bucket{!) = P(I \H,G),
bucleet(H) = P(H \E, F), bucket( G) = P(G\E,D),
bucleet(F) = P(F \B ) , bucket{E) = P(E\C,B),
bucket(D) = P(D \ C), bucket(C) = P(C) , bucleet(B}
= P(B). Proce66ing the buckeb from top to bottom
by elim-mpe generate6 functiom that we denote by h
function,: buclcet(I) = P(I\H, G)
bucket(H) = P(H \E, F), h1(H,G)
bucleet(G) = P(GIE,D),hH(E,F,G)
bucket(F) = P(F\B),hG(E, F, D)
bucket(E) = P(E\C, B), h F(E,B, D)
bucket(D) = P(D\C),hE(C,B,D)
bucket{C) = P(C),hn(C, B)
bucleet{B) = P(B),hc(B)
Where h1(H,G) = mazrP(I\H,G), hH(E,F,G) =

Alternatively, we can use the recorded bound in each
bucket as heuristics in subsequent search.
the functions computed by

approz-mpe{i, m}

Since
are al­

ways upper bounds of the exact quantities, they
can be viewed

as

over-estimating heuristic func­

tions in a maximization problem. We can associate

ip-1 = {z1, ... , :l:p-t )
f(i,_l) = (g h)(z,_1)
rrr.:� P(zi\Zpo.) and h(ip-1) =

with each partial assignment
an evaluation function

·

g(ip-1) =
IT;ebuclcetp_1h;. It is easy to see that the evaluation
function f provides an upper bound on the mpe re­
stricted to the assignment i,_1• Consequently, we
where

can conduct a best first search using this heuristic
evaluation function.

From the theory of best first

search we know that

(1)

when the algorithm ter­

minates with a complete assignment, it has found
an optimal solution;

(2)

the sequence of evaluation

functions of expanded nodes are non-increasing;
as

(3)

the heuristic function becomes more accurate,

fewer nodes will be expanded; and

(4)

if we use the

136

Dechter

and Rish

full bucket-elimination algorithm, best first search
will become a greedy and complete algorithm for the
mpe task [10].

3.1

Cases of completeness

Clearly, approz-mpe(n, n) is identical to elim-mpe
because a full bucket is always a refinement-maximal
(n, n)-partitioning. There are additional cases for i
and m where the two algorithms coincide, and in
such cases approz-mpe(i, m} is complete. One case
is when the ordering d used by the algorithm has
induced width less than i. Formally,
Theorem 3. 7

Algorithm appro:z:.mpe(i, n} i! com­
plete for ordered network! having w• (d) � i.

Another interesting case is when m = 1. Algo­
rithm approz-mpe(n, 1) under some minor modifica­
tions and if applied to a poly-tree along some legal
orderings coincides with Pearl's poly-tree algorithm
[11]. A legal ordering of a poly-tree is one in which
observed variables appear last in the ordering and
otherwise, each child node appears before its par­
ents, and all the parents of the same family are con­
secutive.
Algorithm approz-mpe{n, 1} will solve
the mpe task on poly-trees with a legal variable­
ordering in time and space O(ezp(IFI)), where IFI
is the cardinality of the maximum family size. In
other words, it is complete for poly-trees and, like
Pearl's algorithm, it is tractable. Note, however,
that Pearl's algorithm records only unary functions
on a single variable, while ours records intermediate
results whose arity is at most the size of the fam­
ily. To restrict space needs, we modify elim-mpe
and approz-mpe{i, m} as follows. Whenever the al­
gorithm reaches a set of consecutive buckets from the
same family, all such buckets are combined into one
!uper-bucket indexed by a.ll the constituent buckets'
variables. In summary,

in variable X1, namely, to compute P(:z: 1, e) =
L-z=:!;" II�1P(:z:i, e[:z:pa.).
When processing each
bucket, we multiply all the bucket's matrices,
At. ... , >..i> defined over subsets S1, ... , S;, and then
eliminate the bucket's variable by summation [3].
In [4] we presented the mini-bucket approximation
scheme for belief updating. For completeness, we
summarize this scheme next. Let Ql = {Q1, , Q,.}
be a partitioning into mini-buckets of the func­
tions >.1, ... , >.; in Xp 's bucket. Algorithm elim­
bel com.putes )..P: (l index the mini-buckets) )..P =
Lx, II�=l >..i = l:x, III=1 lit, >..1,. Separating the
processing of one mini-bucket (call it first) from
the rest, we get ).P
L:x, (ITt, >.1,) (W==ziTt,>.t.),
and migrating the summation into each mini-bucket
yields, �, = rrr=l Lx, rr,,>..l,. This, however,
amounts to computing an unnecessarily bad upper
bound on P because the product IIt,>..l, for i > 1
is bounded by .L:x, II,,>.,,. Instead of bounding a
function of X by 1ts sum over X, we can bound
by its maximizing function, which yields �' =
.L:x,.((Iltl>.h) rrr=2ma:z:x,.III,AI.;)· Clearly, for ev­
ery partitioning Q, )..P::; yq · In summary, an upper
bound gP of )..P can be obtamed by processing one of
Xp 's mini� buckets by summation, and then process­
ing the rest of Xp's mini-buckets by maximization.
In addition to approximating by an upper bound, we
can approximate by a lower bound by applying the
min operator to each mini-bucket or by computing
a mean-value approximation using the mean-value
operator in each mini-bucket. Algorithm, appro:c­
bel-maz(i, m) , that uses the maximizing elimination
operator is described in [4]. In analogy to the mpe
task, we can conclude that, approz-bel-ma:c(i,m} has
time complexity O(m · ezp(2i)), is complete when,
(1) w"(d) � i, and, (2) when m = 1 and i = n, if
given a poly-tree.
•••

=

·

0

Prop o s ition 3.8 Algorithm approz-mpe{n,I} with

5

4

The bucket-elimination algorithm for computing the
map, elim-map, presented in [3] is a combination
of elim-mpe and elim-bel; some of the variables are
eliminated by summation, others by maximization.
Consequently, its mini-bucket approximation is com­
posed of approz-mpe{i,m} and appro:c-bel-ma:c{i,m).

the 8uper-bucket modification, applied along a legal
ordering, i! complete for poly-tree! and i! identical
to Pearl'! poly-tree algo'l'ithm for mpe. The modi­
fied algorithm'! complexity i! time ezponential in the
family 1ize, but it require! only linear 1pace. D
Approximating belief updating

The algorithm for belief assessment, elim-bel, is iden­
tical to elim-mpe with one change: it uses sum­
mation rather than maximization. Given some
evidence e,
the probl em is to assess the belief

Approximating the map

Given a belief network BN = {Pt, . .. . , P,,J, a. sub­
set of hypothesis variables A :::: {A1, ... , Ar.}, and
some evidence e, the problem is to find an assign­
ment to the hypothesized variable that maximizes

A scheme for approximating probabilistic inference

137

their probability. Formally, we wish to compute

m axP(a�ole) = (max L rr�=lP(:z:;,el:z:pa.))/P(e)
a � ..
4 ..
:l!k+l

when :z: = (a1, ... , a�o, :Z:Jo+l• ... , :Z:n ) · Algorithm elim­
map, the bucket-elimination algorithm for map, as­
sumes only orderings in which the hypothesized vari­
ables appear first. The algorithm has a backward
and a forward phase, but its forward phase is only
The ap­
relative to the hypothesized variables.
plication of the mini-bucket scheme to elim-map is
a straightforward extension of approz-mpe(i,m} and
approz-bel-maz(i,m}. We partition each bucket into
mini-buckets as before. If the bucket's variable is
a summation variable, we apply the rule we have
in appro:c-bel-maz(i,m}, that is, one mini-bucket is
approximated by summation and the rest by maxi­
mization. When the algorithm reaches the buckets
with hypothesized variables, their processing is iden­
tical to that of approz-mpe(i,m). Algorithm approz­
map(i,m} is described in Figure 4.

Theorem 5.1 Algorithm approz-map(i, m} com­
pute"' an upper bound of the map, in time 0( e:cp(m ·
e:cp(2i})) and &pace O(e:cp(m ezp(i))). Algorithm
approx-map(i, n} i.! complete when w * (d) :S i,
and algorithm approx-map(n, 1} u complete for poly­
tree&. D
·

Consider a belief network appropriate for decoding
a multiple turbo-code, that has M code fragments
(see Figure 5, which is taken from Figure 9 in [2]).
In this example, the Ufs are the information bits, the
X;'s are the code fragments, and the Yi's and Y,,'s
are the output of the channel. The task is to assess
the most likely values for the U's given the observed
Y's. Here, the X's are summation variables, while
the U's are maximization variables. After the ob­
servation's buckets are processed, (lower case char­
acters denoted observed variables) we process the
first three buckets by summation and the rest by
maximization using appro:c-map(n, 1}, we get that
all mini-buckets are full buckets due to subsurnp­
tion. The resulting buckets are:
bucket(X1) = P (y1IX1), P (X1I Ut, U:z:, U3, U4)
bucket(X2)
P (y:z:IX2), P (X2I Ut, U2, U3, U4)
f3x1 (Ut, U:z:, U3, U4)
bucket(X3)
P (y3IX2), P (X2IU11 U2, U3),
/3x2(Ull U2, U3, U4)
bucket(Ut) = P (Ut ), P(y,11Ut), f3x• (U11 U2, U3, U4)
bucket(U:z:) = P (U2), P(y,. IU2 ), {3u1 (U2 , U3, U4)
u
bucket(U3) = P (U3), P (y,,IU3), {3 2 (U3, U4)
u
bucket(U4) = P (U4), P(y,�IU4), f3 •(U4) ,
Therefore, approz-map(n, 1} coincides with elim­
map for this network.

Algorithm approx-map(i,m)
Input: A belief network BN

{Pt, ... ,P,.}; a sub­
,A�o}; an ordering of the
=

set of variables

A

=

{At1

•••

variables, d, in which the A's are first in the ordering;

evidence

e.

Output:

An upper bound maximum a posteriori hy­

pothesis, A = a.
1. Initialize: Partition BN into

bucket1, ••• , bucket,.,

where bucket, contains all matrices whose highest vari­
able is X,.

2. Backward: For p <f- n downto 1, do
in bucketp, do
for all the matrices f' t , f:Jl ,
• (bucket with observed variable) if bucketp contains
the observation Xp = zp1 then assign Xp = Xp to each
{3; and put each resulting function into its appropriate
bucket.

... ,{3;

A, for {3111 •••1{3; in bucketp1 do
'
(i, m)-partitiorung q of the matrices fl•
into miru-buckets Q,, ... 1 Qr.
'
(processing first bucket) For Q1 first in q containing
•

else, if Xp is not in

generate an

f3t17 ···� f' t; 1 do

{31

II1=1 {31,.

Add {:J1
=
x
,.
<f­
to the bucket of the largest-index variable in
•

generate function

L:

U1
{Xp}
•
U1=l St,
I
1f3r; 1 do
• For ea� Qr 1 l > 1 in Q contairung f3•1,
{:J1
functions
the
Generate
{Xp}·
Sr,
Ur � U� =l
•••

nf=1{:J,,.

Add
maxx,.
index variable in U,.
•

else, if

{31

to the bucket of the largest­

Xp E A, for f3t,f3l, ...

an
{Qt , ...1Qr}·

1{3j in bucketp1 do

(i, m )-miru-bucket-partitiorung

generate

For each

do
generate function

Q,

{:J11 {:J1

E
=

'
q

contairung

f:J111

maxx,.m=1{:Jr,.

3.

s,,

- {Xp}•

�orward:

'
q
•••

=

,{:Jr.,

Add

to the bucket of the largest-index variable in

U1=

=

U,

Assign values, in the ordering d

{31

�
=

At, ... , A1o using the information recorded in each
bucket.

Figure 4: Algorithm approz-map-maz(i,m)

Figure 5: Belief network for decoding multiple turbo
codes

138
6

Dechter and Rish

Experimental evaluation

diagnosis purposes, we also recorded the maximum
family size in the input network, F;,, and the max­

Our preliminary empirical evaluation is focused on
the trade-off between accuracy and efficiency of the
approximation algorithms for the mpe task. We wish

1.

to understand

the sensitivity of the approxima­

tions to the parameters

i

and m,

2.

The effective­

ness of the approximations on sparse networks vs
dense networks, and on uniform probability tables
vs.

structured ones (e.g., noisy-ORs), and

3,

the

extent to which a practitioner can tailor the approx­
imation level to his own application.

approz­
approz:-mpe{m), as­

We focused on two extreme schemes of

mpe {i, m):

the first one, called

i and varying m, while the sec­
approz:-mpe{i}, assumes unbounded
i.

sumes unbounded
ond one, called
m and varying
Given

the

values

of

i

and

m,

many

(i, m)­

partitionings are feasible, and preferring a particu­
lar one may have a significant impact on the quality
of the result.

Instead of trying to optimize parti­

tioning, we settled on a simple strategy.

We first

created a canonical partitioning in which subsumed
functions are combined into mini-buckets.

appro:rJ-mpe{m)

Then,

combines each m successive mini­

buckets into one mini-bucket, while

approz-mpe{i)

generates an i.-partitioning by processing the canoni­
cal mini-bucket list sequentially, merging the current
mini-bucket with a previous one provided that the
resulting number of variables in the resulting mini­
bucket does not exceed

i.

imum arity of the recorded functions,

F0•

We also

report the maximum number of mini-buckets that
occurred in any bucket during processing

6.1

(mb).

Results

We report on four sets of uniform random networks
(we had experimented with more sets and observed
similar behavior):

30

having

a.

set of

80

nodes and

instances having

60

200

hundred instances

1),

edges (set

nodes and

90

200
2), a

a set of

edges (set

100 instances having 100 nodes and 130 edges
3) and a set of 100 instances having 100 nodes
200 edges (set 4). The first and the forth sets

set of
(set
and

represent dense networks while the second and the
third represent sparse networks. For noisy-OR net­

30
100 edges; set 5 has 90 instances and uses
one evidence, set 6 has 140 instances and uses three
evidence nodes and set 7 has 130 instances and uses
works we experimented with three sets having

nodes and

ten evidence nodes.

6.1.1

Uniform random networks

On the relatively small networks (sets
we applied

elim-mpe and

1

and

2)

compared its performance

with the approximations. The results on these two

1-3.

sets appear in Tables

Table

1 reports averages,
i. Rather than

where the first column depicts m or

displaying the mpe, the lower bound, and the upper
bound (often, these values are very small, of order

The algorithms were evaluated on belief networks

10-6

generated randomly. The random acyclic-graph gen­

accuracy of the approximation. Thus, the second

n,

column displays Ml1, the ratio between the value of

erator, takes as an input the number of nodes,
and the number of edges,
ates

e

e,

and randomly gener­

directed edges, ensuring no cycles, no parallel

and less), we report ratios which capture the

an mpe tuple

(Ma:z:)

(Upper) and Ma:z:; and the fourth col­
time ratio, TR between the CPU
running times for elim-mpe and appro:rJ-mpe(m) or
approx- mpe(i). The next column gives the CPU
time, T,., of appro:rJ-mpe(m) or approz:-mpe(i). Fi­
nally, F,, F0 and mb, are reported.

edges, and no self-loops. Once the graph is available,

upper bound

for each node :z:;, a conditional probability function

umn contains the

P(z•l:z:,.,..) is generated. For uniform random net­
works the tables were created by selecting a random
number between 0 and 1 for each combination of val­
ues of :z:, and :z:,.,.,, and then normalizing. For ran­
dom noisy- OR networks the conditional probability
functions were generated as noisy-OR gates by se­

lecting a random probability q1c for each "inhibitor".
Algorithm

approz-mpe(i,m)

computes

an

upper

and the lower bound (Lower);

the third column shows the U IM ratio between the

Table

2

gives an alternative summary for the same

appro�-mpe{m} only. Three
UIM ratios, vs. the Time Ratio, are

two sets, focusing on
statistics MIL,

reported. For each bound and for each m, we display

200)

bound and a lower bound on the mpe. The latter is

the percent of instances (out of total

provided by the probability of the generated tuple.

the corresponding ratio (MI1 for the lower bound,

For each problem instance, we computed the mpe by

U IM for the upper bound) belongs to the interval

elim-mpe,

[E- 1, E] where the threshold value, E, changes from

the upper bound and the lower bound by

the approximation (either

approx-mpe{m} or approx­

mpe(i}) , and the running time ofthe algorithms.

For

1 to 4.

on which

We also display the corresponding mean T R.

For example, from Table 2's first few lines we see

A scheme for approximating probabilistic inference

that 8.5 % instances out of the 200 were solved by
appro:tJ-mpe(m=1} with accuracy factor of 2 or less,
48% achieved this accuracy with m = 2. The speed­
up over m= 1 instances was 176 while the speed-up
for m= 2 was 20.8.
Table 1: elim-mpe vs. appro:tJ-mpe(i,m) on 200 in­
stances of random networks with 30 nodes, 80 edges,
and with 60 nodes, 90 edges
_Ln•tance•
4ppNe-mpt(m form- 1,�.3

Mean value• on 200

•lim-mp• v•·

m

MJL

1
2
3

43.2
t.O
1.3

1
2
3

9.9
1.a
1.0

MfL

3

55.5
29.2
1'1'.3
15.0

12
3

I
II

12

UJM

I
I

41.2
3.3
1.1
21.'1'
2.a
1.1

elim-m.pe

i

I
II

I

!0

V• •

UfM

I

TR

I

I
I

80 nod••·
2118.1
25.0
1.t
eo nod••·
131.5
27.9
1.3

I
1

,.
T

for

ma>:
mb

max

4
2
1

9

3
2

5
5

1

1

-

I� I ':.�

�

9

5

31 t5 1 II 1:1

�

max
P.
n
n
n

12
n

12
ma>:
P.

0.1
0.1
0.2
0.8

4
3
3
2

9
9
9
9

12
12
12
12

0.1
0.1
0.2
0.5

3
2
2
2

5
5
5
5

12
12

I1 I
I

309.2
254.8
1111.0
U.3
80 node11 90 ed1e11
18.5
138.2
1.1
112.8
8.1
2.8
'11.'1
2.1
1.9
24.2
1.8
1.4

"'•

max
P,
.,
...-aluel per node

T

node11 AO edae•1

41.4
20,'1'
'1'.5
3.0

I
I

ao •da••
0.1
2.2
21.4
90 •dsu
0 .1
0.1
11.9

CpprOit-fi'I,J'8 ,;
TR

I

Yalue• per node

12

12

From these runs we observe a considerable efficiency
gain (2-3 orders of magnitude) relative to elim-mpe
for 50% of the probelm instances for which the ac­
curacy factor obtained was bounded by 4. We also
observe that, as expected, sparser networks require
lower levels of approximations than those required
by dense networks, in order to get similar levels of
accuracy. In particular, the performance of appro:tJ­
mpe(i=a) gave a 1-2 orders of magnitude perfor­
mance speedup while accompanied with an accuracy
factor bounde by 4, to 80 percent of the instances on
dense networks, and to 97 percent of the sparse net­
works. From table 1 we also observe that controlling
the approximation by i provides a better handle on
accuracy vs efficiency tradeoff. Finally, we observe
that approz-mpe(m=l} can be quite bad for arbi­
trary networks.
We experimented next with larger networks (sets
3 and 4), on which running the complete elimina­
tion algorithm was sometimes computationally pro­
hibitive. The results are reported in Tables 4 and
5. Since we did not run the complete algorithm on
those networks, we report the ratio U/L. We see that
the approximation is still effective (a factor of accu­
racy bounded by 10 achieved very effectively) for
sparse networks (set 3). However, on set 4, appro:tJ-

139

Table 2: Summary of the results: M/L, U /M and
TR statistics for the algorithm approz - mpe(m)
with m = 1, 2, 3 on random networks
[� -l,�J

Random network• w1th

��·�l
[2,3]
[3,4]

[i, oO]

��·�l

[2,3]
\3,
r.. ·��
""
l1
· �!
[2, 3]
(3,4}
[t, oO]

!�·

�l

(2, 3]

4., 00�l
r!M

!�· �!

[2, 3]
\3, " l
r4, �
""

!�· �!
[:1, 3]

[3,t]
[4 , oO]

30

1.5!'
9.0%
1.6%
'14%

2
2
2
3
3
3
3

11%
'1'.6%
211.5%
92,..
5%
1%
3%

tU'o

1'1'8.4.
339.11
221.3
313.1
20.1
211.'1'
113.1
25.3
1.4
2.0
1.2
1.1

Random notworko W1th eo
m

26.11:'18%
9Yo

2
2
2
2
3
3
3
3

'1'9.5,..
10%
6.5%
15%
1ooy;
o%
o%
0%

U.IIY,

10

edc••

Upper bound
UfM
Mean TR

0!'
o%
0%
100%
211.5,..
2'1.11%
17%
21%
9'1'7i
3%
1%
0"'
nodoo, 90 •da••

Lower bound
M ...uTR

MfL

1
1
1
1

nodes,

Lower bound
Moan TK
MfL

1
1
1
1

2

1, •J

L•

m

1'1'2.8
84.3
43.5
H7.5
:1 1 . 1
:11.0
-!:1.4
40.5
1.3
1.0
0.0
0.0

o.o
o.o
0.0
291.1
10.11
22.2
22.1
41.0
1.4
4.9
1.3
o.o

Upper bound
M...uTR

UfM
o,.,
oYo
!Yo
119%
41�
31%
14%
U%
100yj
1Yo
oYo
OYo

o.o
o.o
17.4
132.'1'
:11.2
!:I.e
24.t
40.3
1.3
1.0
0.0
0.0

mpe(m} was too expensive to run for m = 3, 4, and
too inaccurate for m = 1, 2. For this difficult class,
an acceptable accuracy was not obtained.
6.1.2

Noisy-OR networks

We experimented with several sets of random noisy­
OR networks and we report on three sets with 30
variables and 100 edges.
The results are summa­
rized in Figure 6 and Table 6. In the first, we display
all instances of set 5 plating the accuracy (M/L and
U /M) vs T R, for all 90 instances. In the second
we display the results on sets 6 and 7 in a manner
similar to Table 2. T.,1 gives the time of elim-mpe.
The results for the noisy-OR networks are much
more impressive than for the uniform random net­
works. The approximation algorithms often get a
correct mpe while still accompanied by 1-2 orders of
magnitue of speed-up (see cases when i = 12 and
i = 15.) Although the mean values of U /M and
M/1 can be large on average due to rare instances
(see Figure 6), in many of the cases both ratios are
close or equal to 1.
In summary, for random uniform and noisy-OR net­
works, 1. we observe that very efficient approxi­
mation algorithms can obtain good accuracy for a
considerable number of instances, 2. appro:tJ-mpe(i)
allows a more gradual control of the accuracy vs. ef-

Dechter and Rish

140

Table

3:

Summary of the results: M/L, U /M and

TR statistics for the algorithm appr ox -mpe ( i) with

i = 3, 6, 9, 12
[•

1 , •l

��
�·
�[2
, 3]
[3 , 4]
[4, oo]
!1• 2!
[2, 3)
(3 , 4)

[!, ..;]

!�·
�!]
[2, 3

�3 , 4�
[4 , 001

!•

1, •J

�
��·
[2 , 3�]
[3 , 4]
r:· . �
�
��·
��
[2, 3]
[3 , 4]
[4 , oo)
��· ��
[2 , 3]
(3, -!J
[4 , oo]

on random networks

Random net....orkl Wlth 30 nodelt eo edge•
Lower bound
I
Upper bound

MfJ..

Mean

111.5 ,,
9%
6 .5%
69%
31!'i
10%
10.5%
41.6%
61 !'!
15%
11%
23%

6
6
6
3

9
9

9
9

n
n

12
6

210.1
266.6
2-18.7
250.1
150.1
1 00.5
114.7
169.8
4 1 .3
41.3
69.2
u.s
•ith 60

Random network•

Lower bound

I

MfJ..

o.s,,
o%
0.&%
99%
2.6'ro
7%
12.6%
78%
29!'i
32%
IT%
22%

UfM.

3,.
15.5%
17.!i%
64%
3S.&!'i
2&%
21"
u.s%
I I ,.

13.&%
&%
0.5 %

3
6
9
12
1&
18
21

1 1 .3
0.0
14.4
256 .11
33.0
1 0 1 .1
132.9
162.1
27.0
60.5
U.4
60.6

T.,

1350t27.6
234561.7
9064,4
2 5 U .9
7]{.1
•o1.a

0.�
0.3
0.&
1.8
10.6
75.3

99.6

mb

max

Pi

"

7
T

3
3
3
3
3

5&0.2

'T

7
7

1
1

2

Accuracy vs efficiency:

�

;:;>
.,
•
..

�

c MIL
• UIM

1000
00

100 '9

0 0

10

500

H. O

ficiency relative to the complete elimination by one

max

MIL and U/M vs TR, I :12

2&.&
71.0
57.2
U2.0
35.9
72.0
96.3
124.5
23.5
29.1
3 7 .3

approx-mpe(m); 3. on random
appro:c-mpe(i} obtains a good ap­
M/L < 1.5) while still improving ef­

100 ln•tance•

UfL

10000 ��------�

mean .....,.

noisy-OR networks

TR

1000

1 500

6: Time Ratio versus M/L and U /M bounds
appro:c-mpe{m) with i = 12 on noisy-OR ne­
towrks with 30 nodes, 100 edges, and one evidence
Figure

for

node

or two orders of magnitude.

7

Mean vlllne• on

'

:R

ficiency tradeoff than
proximation (

on

Upp er bound

91.4
1511.3
U.3
157.2
54.9
88.9
27.4
158.-!
2-!.4
29.7
1 1 .4
21.1

15%
9%
1&.5%
ao,,
1 1 .5%
3%
5.5%
au;:'!
u.s%
0.5%
2.&%

Me•n

edgea

node11 SID

M.ean .... .,.

&1.�:'·

6
6
6
6
9
9
9
9
12
12
12
12

UfM

:R

5: elim-mpe vs. approz- mpe{i) for i = 3 - 2 1
100 instances of random networks with 100 nodes
and 200 edges

Table

z1

=

1

elimination framework, both the parameterized al­

Conclusions and related work

The paper describes a collection of parameterized
algorithms that approximate bucket elimination al­
gorithms.

gorithms and their approximations will apply uni­
formly across many areas. We presented and ana­

Due

to the

generality of the bucket-

lyzed the approximation algorithms in the context
of several probabilistic tasks.

We identified regions

of completeness and provided preliminary empirical
evaluations on randomly generated networks.
Our empirical evaluations have interesting negative

4: elim-mpe vs. appro:c-mpe(i, m) on 100 in­
stances of random networks with 100 nodes and 130
Table

edges

Mean vala••
m

on 100 l�•tan.aea

�mj_

tdim-m.pe v•. e&ppr-'rt:.-mp
UfL

T•

mn
mb

1
2
3
4
i

7111.1
10.t
1.2
1.0

0.1

3.4

13�.5
T..

12

''T&.S
36.3
14.5
1.1

11!

l.T

0.1
0.2
0.3
o.s
3.7
H.ll

I

u

1

1
209.5
elim.-m.pa v•. 4ppPoc-mpe i
UfL

3
6

3
�

3.0

max
mb

3
2
l
2
l
1

max

.!1'.;

5

5

6

5

max
.!1'.;

5
5
5
&
5
5

and positive results.

On the negative side, we see

that when the approximation algorithm coincides
with Pearl's poly-tree propagation algorithm (i.e.,
when we use

approz-mpe(m=l}) ,

it can produce ar­

bitrarily bad results, which contrasts recent suc­
cesses with Pearl's poly-tree algorithm when ap­
plied to examples coming from coding problems

9].

[2;

On the positive side, we see that on many prob­

lem instances the approximations can be quite good.
As theory dictates, we observe substantial improve­
ments in approximation quality as we increase the
parameters

(m

or

i).

This allows the user to an­

alyze in advance, based on memory considerations
and given the problem's graph, what would be the
best

m and i he can effort to use.

In addition, the ac-

A scheme for approxim ating probabilistic inference

Table 6: Summary of the results: M/1, U /M and
TR statistics for the appro:t!-mpe(i) on noisy-OR net­
works with 30 nodes, 100 edges
3
ranae

1
[ 1 , 2]
[2 , 3]
[ 3 , 4]

[,, .,.;]
1
[ 1 , 2]
(2, 3]
(3 , 4 ]

[i, .,.;]
1
[ 1 , 2]
[2 , 3]
[ 3 , i]

[,, .,.;]
1
[ 1 , 2]
[2 , 3]
[3 , 4 ]

[,, .,.;]
ran1e

1

[1, 2]

[ 2 , 3]
[ 3 , i]
[i , .,.;]
1
[1 , 2]
[2 , 3]
( 3 , i]
[, , .,.;]
1
[1 , 2]
[ 2 , 3)
[3, 4]
[4 , oo]
1
[1, 2]
[2 , 3]
[3 , 4)
[4, oo)

I

i

10

I

8
I

I

6
9
9
9
9
9
12
n

12
12
12
1&
1&
15

1&
15

upper and lower bounds approximations can be de­
rived for sigmoid belief networks. Specifically, each
Sigmoid function in a bucket, is approximated by a
Gaussian function.

e.,.tdenee node11 l of O problem 1nd•nce1

M,IL

Lower bound

5
6
6
5
6
9
9
9
9
9
12
12
12
1:1
1:1
1&
1&
1&
1&
1&

141

20.79�
10.0%
&.0%
4.3%
60.0%
46.4"
1&.0%
10.0%
1 .4%
27.1 Yo
70.7Y,
10.7%
4.3%
2.9%
11.4%
16.4,.,
10.0%
1.4%
0.7%
2.1"

rR

&07.9
6&4.2
494.1
730.&
929.1
461.0
&23.2
431.4
411.1
&3&.3
129.0
202.6
12.4
69.1
224.2

22.7
21.0
2&.&
34.1
43.2
33.7
39.0
34.3
27.2
40.0
32.0
&0.1
2 1 .2
17.2
58.7
34.1
4&.0
17.9
13.1
62.5

:n.t

36.0
13.2
9.0

u.r

e·ndence node•,

130

Lower bound
'.1:11.

MfL

28.5!!
17.3%
6.0%
6.8%
43.8%
39.1'Jio
19.&%
9.1%
6.0%
2&.8%
&4.9!!
19.&%
3 .8%
4.&%
17.3%
73.7'Jio
12.1%
3.0%
3.1%
7,&%

423.0
312.1
43!.9
436.0
454.1
311.1
212.8
2U.2
222.7
260.&
87.2
82.1
74.1
1 11 . 1

12.9
18.6
18.3
18.1
21.4
31.11

r,,

UfM

Upper bound

'!

1.4
16.4%
1 7.1 %
10.0%
&&.0%
1.4'!
40.7%
22.1"
1&.0%
20.7%

u.sy,

&6.4%
1 1 .<1%
7.1%
6.4%
40.7%
&2.9%
&.0%
0.7%
1.4%

rR

&21.2
616.1
611.6
421.&
939.0
&10.1
319.4
&12.3
730.&
402.1
11&.5
1&1.1
11&.3
1111.7
149.1
11.7
38.8
28.0
1 1.11
35.6

T,l

24.3
21.2
34.1
11.1
i ].7
40.9
21.\1
n.2
&&.&
30.7
25.1
u. s

31.6

u.s

37.&
22.2
47.1
34.7
17.1
31.5

