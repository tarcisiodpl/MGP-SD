

We investigate the value of extending the
completeness of a decision model along dif­
ferent dimensions of refinement. Specifically,
we analyze the expected value of quantita­
tive, conceptual, and structural refinement of
decision models. We illustrate the key dimen­
sions of refinement with examples. The anal­
yses of value of model refinement can be used
to focus the attention of an analyst or an au­
tomated reasoning system on extensions of a
decision model associated with the greatest
expected value.
1

Introduction

The quality of recommendations for action generated
by decision analyses hinges on the fidelity of deci­
sion models. Indeed, the task of framing a decision
problem-enumerating feasible actions, outcomes, un­
certainties, and preferences-lies at the heart of deci­
sion analysis. Decision models that are too small or
coarse may be blind to details that may have signif­
icant effects on a decision recommendation. Unfor­
tunately, the refinement of decision models can take
a great amount of time, and can be quite costly in
time and expense. In some cases, actions are taken
well before a natural stopping point is reached in the
modeling process. In other cases, important distinc­
tions about actions and outcomes are recognized days
or months after a model is developed.
We have developed methods for probing the value of
key dimensions of decision-model refinement. We pose
the techniques as tools that can direct the attention of
an analyst or of an automated reasoning system to re­
fine aspects of a decision model along dimensions that
have the highest expected payoff. The methods also
can provide guidance on when it is best to cease addi­
tional refinement and to take immediate action in the
world. Our work differs from previous studies of the
*Currently at the Department of Industrial & Systems
Engineering, National University of Singapore.

Eric J. Horvitz

Palo Alto Laboratory
Rockwell International Science Center
444 High Street, Palo Alto, CA 94301
value of modeling (Watson & Brown, 1978; Nickerson
& Boyd, 1980) in that we develop a unifying framework
for probing the values of different classes of refinement,
and consider issues surrounding the direction of model
building and improvement under resource constraints.
Three fundamental dimensions of decision-model re­
finement are (1) quantitative refinement, (2) concep­
tual refinement, and (3) structural refinement. We will
explore methods for making decisions about which di­
mensions to refine, and the amount of effort to expend
on each form of refinement.
Quantitative refinement is the allocation of effort to re­
fine the uncertainties and utilities in a decision model.
There are two classes of quantitative refinement: (1)
uncertainty refinement, and (2) preference refinement.
Uncertainty refinement is effort to increase the accu­
racy of probabilities in a decision model. For example,
assessment may be focused on the tightening of bounds
or second-order probabilities over probabilities in a de­
cision model. Preference refinement is refinement of
numerical values representing the utilities associated
with different outcomes. For example, an analyst may
work to refine his uncertainty about the value that a
decision maker will associate with an outcome that has
not been experienced by his client.
Conceptual refinement is the refinement of the seman­
tic content of one or more distinctions in a decision
model. With conceptual refinement, we seek to modify
the precision or detail with which actions, outcomes,
and related random variables are defined. For exam­
ple, for a decision m:1ker deliberating about whether
to locate a party inside his home versus outside on
the patio, it may be important to extend the distinc­
tion "rain" to capture qualitatively different types of
precipitation, using such conceptually distinct notions
as "drizzle," "intermittent showers," and "downpour."
Likewise, with additional deliberation, he may real­
ize that there are additional options available to him.
Many of these additional alternatives are those that
would not be taken if there were no uncertainty about
the weather. For example, he might consider having
the party on the porch, or renting a tent to shelter the
guests in his yard.

Value of Decision-Model Refinement

Structural refinement is modeling effort that leads
to the addition or deletion of conditioning variables
or dependencies in a decision model. For exam­
ple, a decision maker may discover that an expensive
telephone-based weather service gives extremely accu­
rate weather forecasts, and wish to include the results
of a query to the service in his decision analysis.
These classes of refinement represent distinct dimen­
sions of effort to enhance a decision model. In the next
sections, we will develop equations that describe the
expected value of continuing to refine a model for each
dimension of refinement.
2

Expected Values of Decision-Model
Refinement

Let us now formalize measures of the expected value
of refinement (EVR) 1. For any dimension of EVR, we
seek to characterize our current state of uncertainty
about the outcome of an expenditure of effort to re­
fine a decision model. Experienced decision analysts
often have strong intuitions about the expected bene­
fits of refining a decision model in different ways. This
knowledge is based on expertise, and is conditioned
on key observables about the history and state of the
modeling process. Assume that we assess and repre­
sent such knowledge in terms of probability distribu­
tions over the value of the best decision available fol­
lowing model refinement, conditioned on key modeling
contexts.
To compute the E V:I}, we first determine the expected
value associated with the set of possible models we cre­
ate after refinement. We sum together the expected
utility of the best decision recommended by each pos­
sible revised model, weighted by the likelihood of each
model. Finally, we subtract this revised expected value
from the expected value of the decision recommended
by the unrefined model.
2.1

General Analysis

the weather and A represents the decision on party lo­
cation. The expected value of taking action ak, given
background information e' is

The expected value of the decision offered by this de­
cision model is
E[vle] = mfCLP(xile)v(ak,xi)·

(2)

i

Suppose the decision model can be refined via one of
several refinement procedures R. In general, R can
be parameterized by amount of effort (e.g., as char­
acterized by time) expended on the refinement. We
shall simplify our presentation by initially overlooking
such a parameterization. Note that e represents the
state prior to any refinement consideration; R repre­
sents information about the refinement prior to actual
refinement. Let R(e) denote the state of information
after a refinement requiring some prespecified effort.
Let J-lk denote the expected utility that will be ob­
tained for action ak. Before the refinement is carried
out, the values of J-lk are unknown. However, we can
assess a probability distribution over each of the val­
ues, given information about R and e. We denote this
distribution as p(J-lkiR,e). The expected utility given
refinement R is

If we cease model-refinement activity, we commit
to an action in the world based on all information
available-including P(J-lkIR,e). The expected utility
without refinement is

The EVR is
=

],

J-lk P(J-lkiR,e).

(4)

E[viR(e)]- E[viR,e].

(5)

E[viR,e] = max
k

EVR( R)

Figure 1: A basic decision model

175

l'k

In practice, the values J-lk and distributions P(J-lkIR, e)
are dependent on the specific type of refinement and
the amount of effort allocated. We shall now describe
specific properties of the three types of model refine­
ment and give examples of the detailed analysis of
computing the EVR for each. In each case, we shall
show how each of the analyses is related to the general
formulation captured in Equation (5).

Consider the simple decision problem with a single
state variable X and a single decision variable A, as
shown in Figure 1. In the party problem, X represents

2.2

1 We shall use the EVR to refer generally to the expected
value of refinement, but shall use more specific terms to
refer to alternate classes of refinement.

We start with a consideration of the value of efforts to
refine quantitative measures of likelihoods and prefer­
ences.

Expected Value of Quantitative
Refinement

176

Poh and Horvitz

2.2.1

Uncertainty Refinement

Consider the quantitative refinement on the state vari­
able X of the party-location problem. What is the
value of "extending the conversation" through expend­
ing effort to refine the probability distribution p(XIe)
with additional assessment. Let us first consider the
general case where the distrioution p(XIe) is continu­
ous. Assume that a continuous distribution is char­
acterized or approximated by a named distribution
and a parameter or a vector of parameters. Specifi­
cally, assume a functional form f for the probability
density function, such that for every reasonable dis­
tribution p(XIe), there exists a parameter or a set
of parameters {3, so that the the numerical approxi­
mation p(XIe) � /p(X) is within satisfactory limits.
Before the assessment is carried out, we cannot be cer­
tain about the outcome distribution; however, its out­
come might be described by a distribution of the form
p(f31R,e), which represents the decision maker's un­
certainty about the primary distribution parameter {3.
The expected value of the refinement is

l.

/p(x)v(ak,x)] (6)
{ p(f31R,e)[max
k
J{j
The expected value without performing the quantita­
tive refinement but taking account of knowledge an
agent lias about the potential outcome of refinement
procedure R is
E[viR(e)] =

E[viR,e] =

where

X

l. }fp /p(x)p(f31R,e)v(ak,x)
max
(7)
p(xiR,e)v(ak,x)
k l.

max
k

f.,(x2) = 17r = 'ii'. The

Hence f is linear in 1r and therefore
expected value given that quantitative
refinement is performed is, E[viR(e)]
=

1 p(TriR,e)max[Trv(ak,x!) +(1-Tr)v(ak,x2)]. (8)
k

.,

The expected value without the refinement but with
knowledge about the potential performance of R is
E[viR,eJ = max [7rv(a k' x!) + (1- 'ii')v(ak' X2)]. (9)
k
The above analysis can be extended to the general case
where the state variable X has possible states. In
this case, f3 consists of n-1 parameters ( 1r1, . . . ,Trn-1)·
Our analysis of EVRQU (R) can be related to the gen­
eral formulation in Equation (5) by defining the vari­
able
n

(10)
J.lk =7rv(ak,x1) + (1-7r)v(ak,x2)
for each action ak E A. The distributions p(J.lkIR,e)
can be derived from p(TriR,e).

We shall illustrate the concept of quantitative refine­
ment with a example drawn from the party problem.
Consider the problem of selecting a location for the
party given uncertainty about the weather. Let the al­
ternatives for the location be "Outdoor" (at) and "In­
door" (a2), and let the weather conditions be "Rain"
(xl) or "Sunny" (a2). Let 1r denote the probability
that it will rain. The utility values are,

X

Outdoor
Indoor

X

fi(XIR,e) =

i fp(x)p(f31R,e)

is the operative distribution for the authentic distribu­
tion p(XIe) (Tani, 1978; Logan, 1985).
The operative distribution is the distribution which
the decision maker should use if no further assess­
ment is performed. Let {3 be the parameter that
best approximates the operative distribution p(XIe),
i.e., the numerical approximation fi(XIe) � fp(X)
is within satisfactory limits. This is different from
� = fp /3 p(f31R,e) which denotes the mean of the sec­
ondary distribution.
The expected value of quantitative refinement on the
uncertainty on X with respect to assessment procedure
R, denoted EVRQU (R) is the difference between (6)
and (7).
Let us consider the case where the state variable X
is discrete with two states { :e1, :e2}. We are interested
in the value of improving the probabilities assessed for
p(x1le) and p(x2le). We denote the assessed values
of p(x1le) and p(x2le) by 1r and 1 - 1r, respectively.
The parameter which describes the primary distribu­
tion over X is f3 = 1r, and we have f.,(xl) = 1r and

1r.

0.00
0.67

Sunny (1- 1r)
1.00
0.57

The optimal locations as a function of 1r are
a*(1r) =

Outdoor
{ Indoor

if 7r � 0.38
if 7r > 0.38

1
�

g
�

.57

Figure 2: The optimal party location as a function of
the probability of rain (1r)

Let us suppose that the current uncertainty about 1r
can be described by a probability distribution whose
mean is 0.4. In this case, the optimal decision, without
further assessment, is to hold the party indoors, with
an expected utility of 0.61. However, a more accurate
assessment of the value of 1r might change the optimal

Value of Decision-Model Refinement

decision resulting in a potentially higher utility. With
refinement,

1 p(1riR,e)v(a*(1r),1r)

E[viR(e)]=

where

1-?r
0.57 + O.l?r

v(a*(1r), 1r)={

if 7r � 0.38
if 7r > 0.38

.50 5(0.57 O.l1r)d1r- 0.61
!.3
8 o.o57r2U8 - o.61
5[0.5711"

+

+

Notice that the above analysis was performed in the 7r­
domain. An alternative analysis and perspective which
will produce equivalent results can be performed in the
p-domains. This is done by a change of variables from
1r to p1 and p2 via Equation (10) . The resulting anal­
ysis would have to be displayed as a two-dimensional
graph.
2.2.2

Outdoor
Indoor

+

U +
0.6324- 0.610 0.0224
=

Let us again use the party problem to illustrate the
value of refining preferences. Since we can fix the util­
ity for the worst outcome (outdoor and rain) at zero,
and the utility for the best outcome (outdoors and
sunny) at one, we need only to consider the uncer­
tainty over further assessment of the values <jJ21 (in­
door and rain), and <P22 (indoor and sunny). Let the
uncertainty over these values be:
U[0.62, 0.72)
</J21
<P22 = U [0.52, 0.62)
The operative values for the preference values are,
=

Consider the case where 1r is uniformly distributed be­
tween the interval [0.3,0.5) . The expected value of re­
finement, EVRQU (R) , is then,

.!3038 5(1 - 1r)d1r
5.[7r-o.57r2 8

177

Preference Refinement

Let us now consider the expected value of quantitative
refinement of preference EVRQP(R). We seek to im­
prove the values of v(ak,x;) for each k and i. Let <Pki
denote the value that will be assessed, given that the
refinement is carried out. Let p(</JkiiR,e) denote the
uncertainty over the assessment for each v(ak. Xi)· The
expected value, given that the quantitative refinement
on preference is carried out, is E[viR(e)]

lw·<l>mn p(</Ju,...,<Pmn IR,e)[mkax�p(xde)<Pki]·

Sunny (.6
1.00
0.57

eu
0.60
0.61

The default choice without any further assessment is
to hold the party indoors with an expected utility of
0.61.
/l1 = 0.60
Jl2 = 0.4<P21 + 0.6</J22
In the example, there is no uncertainty over p1. The
utility, Jl2, displayed in Figure 3, is a linear sum of
two uniformly distributed variables, with a triangular
distribution p(J.t21R,e),
if .56 � /l2 � .61
400(J.t2 - .56)
0 - 400(J.t2 - .61) if .61 � Jl2 � .66
=
otherwise
and an expected value of 0.61.

{�

=

( 11)
The expected value without quantitative refinement on
preference but with knowledge about the performance
of R is
( 12)
E[viR,e]=m:xi:>(x;le)�ki

Rain (.4
0.00
0.67

Figure 3: The pdf for p2
Figure ( 4) shows the optimal value p* = maxk Jlk
as function of Jl2, where Jl1 is fixed at 0.60. The
EVRQP(R) is

i

.60 400(J.t2 - 0.56)(0.6)dJ.t2
!. 6
/.56.061 [20 - 400(J.t2 - 0.61)](0.6)dJ.t2
.66 [20 - 400(J.t2 - 0.61)]J.t2dJ.t2- 0.61
!.61
0.63733- 0.610=0.02733
+

where

r <Pki p(<PkiIe)
1</>ki
is the operative utility value for v(ak,aki). The
EVRQP(R) is the difference between ( 11) and (12).
This analysis can be related to the general formula­
tion in Equation ( 5) by defining the variable
�ki=

(13)
for each action ak E A. The distributions p(J.tkIR,e)
can be derived from the distributions p(<Pk;IR,e) .

+

=

2.3

Expected Value of Conceptual
Refinement

We shall now explore measures of the value of con­
ceptual refinement: ( 1) the value of refinement of the

178

Poh and Horvitz

The expected value given that the refinement is not
carried out, but with knowledge about the perfor­
mance of R is E[viR, e]

J.l.*
.66
.60

�'5[p(xule)¢k1 + p(x12le)<fok2 +

=

.56

.60

.66

definitions of state variables, EVRc8(R) and (2) the
refinement of definitions of actions, EVRCA(R).
2.3.1

Assume that our current decision model has a state
variable X
{ x1,x2} and decisions A
{a1,a 2}.
Now, let us consider the value of refining the state
x1 into x u and x12, such that the resulting state vari­
able is X' {xu,X12,x2}. We further assume that
the probability of the refined states p(xulx1,e) and
p(:r:12\x1,{) are known. As a result of the refinement,
we need to assess the utilities v(ak,x1i) for k 1, 2
and j 1, 2. Before these assessments are carried out,
the values v(ak, Xlj) are unknown. Let </lkj represent
the utilities (ak,x1i) that will be assessed if the as­
sessment is performed. In addition, we assume that
the decision maker is able to assess a set of probability
distributions p(¢ki IR,e), k 1,2 and j 1, 2 over
these utilities.
To assess the probabilities over the utilities, a possi­
ble conversation between the analyst and the decision
maker might be as follows:
=

=

=

=

p(xule)¢kl + p(x121e)¢k2 + p(x2ie)v(ak, x2), (16)
for action ak, k = 1,2 and deriving the distributions
P(J-!k I R,e) from the distributions p(¢kjiR, e).

To illustrate conceptual refinement, consider the ex­
pansion of the state of "Rain" into "Downpour" and
"Drizzle". Assume that a decision maker's assessment
of his uncertainty over the values of ¢12 (outdoor and
drizzle), </121 (indoor and downpour) , and ¢22 (indoor
and drizzle) are as follows:
v(¢12\ R,e)
vC<P21IR,e)
p(<J!22IR,e)

v

=

=

=

State-Variable Refinement

=

(15)
= 1,2)

1,2 and j
where <foki
is the operative value to be used when no refinement is
carried out. The EVRc8(R) for refining state variable
X to X' is just the difference between (14) and (15).
As before, we can simplify this analysis and relate it
to the general formulation of Equation (5) by defining
the variable J-lk
=

Figure 4: The optimal value JJ* as a function of jJ2

p(x2i{)v(ak,x2)]
Jrf>ki ¢ki p(¢kjl{), (k

=

In our previous conversation, you assigned a
utility u for outcomes at your point of in­
difference between an outcome and a lottery
with probability u for the best prospect and
probability 1- u for the worst prospect. Sup­
pose I were to ask you to assess the utility of
each of the refined outcomes. As we do not
have an unlimited amount of time to assess
these utilities, please give us an estimate now
of the probabilities describing the utility val­
ues assessed if you were to have enough time
to thoroughly reflect on your preferences and
2
knowledge about the outcomes.

The expected value resulting from the conceptual re­
finement of X to X' is E[vjR(e)]

f
p(¢u, </112, </121,¢n\R,e)
}</>u </>I2<!>21 </>22
(14)
�t1 [p(xule)¢k1+ p(xde)¢k2 +
p(x2le)v(ak,x2)] .

-----

2We could also perform this assessment in terms of
the utilities that would be assessed after some predefined
amount of time for reflection.

U[0.05,0.15)
U[0.67, 0.77]
U[0.57, 0.67)

=
=

The operative utilities are as follows:

Outdoor
Indoor

Rain (.4)
Down- Dnzzle
pour (.2)
(.2)
0.00
0.72

0.10
0.62

Sunny
(.6)
1.00
0.57

EV
0.62
0.61

Without refinement, the expected utility of holding
the party outdoors is 0.62 and the utility of having
the party indoors is 0.61. Since the two expected val­
ues are very close, further refinement might lead to
a better discrimination between the two choices. In
lieu of additional refinement, the default decision is to
have the party outdoors. Based on the distributions
over ¢ki, we define
J-1 1

J-12

=
=

0.2¢12 + 0.6
0.2¢21 + 0.2¢22 + 0.342

f-ll is uniformly distributed between 0.61 and
i.e. P(J.LI\R,e) U[0.61, 0.63), while JJ2 has a
triangular distribution p(JJ21R,e) (depicted in Figure

where
0.63,

5),

=

=

{

2500(J-!2- .59)
50- 2500(JJ2- .61)
0

if .59 � J-12 � .61
if .61 � /J2 � .63
otherwise.

Figure (6) shows the region over possible values of J-11
and J-12· The EVRc8(R) is
=

!.63 50 [1.61 2500(JJ2- 0.59)J-!1dJ.l2+
.61

.59

Value of Decision-Model Refinement

179

The expected value without the conceptual refinement
on action is,
E [vJR,e] =

where
Uki =
_

Figure 5: The pdf for J.l2

jill (50-2500(J.l2
.61

-

0.61) ] J.l1dJ.l2 +

Jir6a
ll (50- 2500(J.l2- 0.61)] J.l2dJ.l2] dJ.l1

-0.620 = 0.9208-0.620 = 0.3008
. 63

.61

....,

.59 _ .....,_

__

.61

llt

2.3.2

�

i1

=

...

q,n

p(¢1, ... ,¢n lR, e) [ Tt�;,
a
k

where
Uki =

{ v(a
k,x;)
1/Ji

� p(xde)uki]

(17)

if k = 1,2
if k=3.

ifk=1,2

if k=3.

The EVRCA(R) for refining action A to A' is then the
difference between (17) and (18). We can relate these
results to the general formulation of Equation (5) by
defining the variable
(19)

for each action ak. Note that J.l1 and J.l2 are determin­
istic, while the probability distribution p(J.laJR,e) can
be derived from the distributions p( ¢; IR, e) .
Let us consider the refinement of the example prob­
lem with the addition of a third action which-to hold
the party on the porch (aa). To complete the refine­
ment, we must assess the utility values ¢1 (porch and
downpour), ¢2 (porch and drizzle), and ¢a (porch and
sunny). For simplicity, we will assume that the deci­
sion maker is certain about the value of ¢a, which is
0.81. His uncertainty over ¢1 and ¢2 are
U(0.17,0.27]
U(0.37,0.47]

The operative utilities are as follows:

Action Refinement

Similar to extending the conversation about the def­
inition of states, the set of decision alternatives may
be increased with continuing modeling effort. Con­
sider the conceptual refinement of action A={ a , a2 }
by the addition of action aa. Let A' = { a1, a2,aa}.
Unlike state variable refinement, the set of refined ac­
tions need not be mutually exclusive. Indeed, they
need not even be mutually exhaustive as some alterna­
tives can be ruled out immediately, based on common
sense knowledge or dominance relationships (Wellman,
1988). As the result of action refinement we need to
assess the utilities v(aa,x;) for all x; E X. Let ¢; de­
notes the utility v(aa,x;) for each i, and let p(¢;JR, e)
be the uncertainty over each assessment. The expected
value offered by the refined model is E[vlR(�)]

.
a

{ ¢;UJtt=v(ak,Xi)

.63

Figure 6: The region of values for J.l1 and J.l2 where
J.l* =maxk J.lk

(18)

max L:p(x;Je)uki

k= 1, 2, a

Outdoor
Indoor
Porch

Rain .4)
Down- Dnzzle Sunny
pour (.2)
(.2)
(.6)
0
.72
.22

.10
.62
.42

1
.57
.81

EV
.620
.610
.614

The optimal action without further refinement is to
hold the party outdoors, with an expected utility of
0.62.

J.l1
J.l2
J.la

0.62
0.61
0.2¢t + 0.2¢2 + 0.6¢a

There is no uncertainty on J.lt and J.l2· However, as dis­
played in Figure 7, J.la is a linear sum of two uniformly
distributed variable and has a triangular distribution
of the form, p(J.laJR,e),
=

{�

400(J.la -.564)
0- 400(J.la .614)
-

if .564 � J.la � .614
if .614 � J.l2 � .664
otherwise

and has an expected value of 0.614.

180

Poh and Horvitz

a parameter f3y, such that the numerical approxima­
tion p(Y IR(e)) � f{3y (Y) is within satisfactory limits.
We let Px1Y represent the parameter for the distri­
bution p(XIY,R(e)). Let the distributions p(,By IR,e)
and p(,BYIX IR,e) represent the decision maker's un­
certainty about the parameters ,By and Pxw, respec­
tively. The expected value that results from the struc­
tural refinement via the addition ofY as a conditioning
variable for X, is E[v iR(e)

Figure 7: The ·pdf for J..L2

=

Figure (8) shows the optimal value J.L* = max,�: J.lk as
a function of J..La. The expected value of conceptual
refinement via addition of the third alternative is

1.614
.1.564620 400(J..La - 0.564)(0.62)dJ..La
.1.614664[20- 400(J..La- 0.614)](0.62)dJ..La
400(J..La- 0.614)]J..LadJ..La - 0.62
.0.62568620 [20-0.62
= 0.00568
+

[ 1

�

.620

Figure 8: The optimal value J.L* as a function of J..La
2.4

Expected Value of Structural Refinement

Figure 9: Structural refinement on node X
Finally, we consider the value of structural refine­
ment, EVR8(R), the value of increasing the number
of conditioning variables. Figure 9 depicts an exten­
sion of conversation based on structural refinement
of the state variable X of our simple decision prob­
lem by the addition of Y as a conditioning event for
X. For example, in the party problem, we may iden­
tify "wind speed" as a conditioning variable on the
forthcoming weather. We are interested in analyzing
the additional value that is gained by the addition
of Y as a conditioning variable for X. This struc­
tural refinement requires the assessment of the prob­
ability distributions p(YIR(e)) and p(XjY, R(e)). As
before, we assume a functional form f where, for ev­
ery reasonable distribution for p(YIR(e)) , there exists

1

]

The expected value without structural refinement is

f f� Y (y) Jxf f�XIY(x)v(a.�:,x), (21)
E [ v lR, e] =max
k }y

+

.664

f p(,ByjR,e) f P(PYixlR,e)
Jf3y
Jf3YIX
m;x /{1y(y) /(1xiY(x)v(a,�:,x) . (20)

where /Jy and Px!Y are the parameters for the opera­
tive distributions
fi(YIR,e)

=

f (jy (Y)p(pyIR,e) � ��y (Y)
J(jy /

and

f fr3y(X)p(Pxw IR,e)� f�XIY(X)
jf3xiY
respectively. The EVR5(R) for the variable X, with
respect to adding a new conditioning event Y , is just
the difference between (20) and (21). The case where
X and Y are discrete variables is treated in (Poh &
Horvitz, 1992).
A special form of structural refinement is the famil­
iar expected value of information (EVI). Within the
influence diagram representation, we can view the ob­
servation of evidence as the addition of arcs between
chance nodes and decisions. We describe the relation­
ship of EVI and other dimensions of model refinement
in (Poh & Horvitz, 1992).
p(XjY, R,e)=

3

Control of Refinement

Measures of EVR, computed from a knowledge base of
probabilistic expertise about the progress of model re­
finement, hold promise for providing guidance in con­
trolling decision modeling in consultation settings, as
well as within automated decision systems. In this sec­
tion, consider control techniques for making decisions
about the refinement of decision models.
3.1

Net Expected Value of Refinement

So far, we have considered only the value of alternative
forms of effort to expending effort to refine a model.
To consider the use of EVR measures, we must balance
the expected benefits of model refinement with (1) the
cost of the assessment effort, and (2) the increased

Value of Decision-Model Refinement

computational cost of solving more refined, and po­
tentially more complex, decision models. We define
the the net expected value of refinement, NEVR, as
the difference between the EVR and the cost of mak­
ing a refinement and increase in the cost of solving the
refined model. That is NEVR(R, t)
(22)
EVR[(R(t)), <J- Ca(ta) - Cc(b.(tc ))
where R(t) is a refinement parameterized by the time
expended on a particular refinement procedure, Ca is a
function converting assessment time, ta to cost, and Cc
is a function converting changes in the expected com­
putational time, required to solve the decision prob­
lem, b.(tc), to cost. In offline, consultation settings,
we can typically assume that changes in computational
costs, associated with the solving decision models of in­
creasingly complexity, are insignificant compared with
the costs of assessment. We can introduce uncertainty
into the costs functions with ease.
=

3.2

Decisions about Alternative Refinements

Let us assume that we wish to identify the best refine­
ment procedure to extend a decision model. For now,
let us assume that we have deterministic knowledge
about the cost of refinements. We shall assume that
the cost is a deterministic function of time3 and that
computational changes with refinement are insignifi­
cant.
We can control model building with a strategic op­
timization (Horvitz, 1990) that seeks to identify the
best refinement procedure and the amount of effort to
allocate to that procedure, i.e.,
(23)
arg maxEVR
[( R(t)), <J- C(t)
R,t
Given appropriate knowledge about decision model re­
finement, we solve such a maximization problem by
computing the ideal amount of effort to expend for
each available refinement methodology, choose the pro­
cedure R* with the greatest NEVR, and apply it for
the ideal amount of time, t* computed from the max­
imization. We halt refinement when all procedures
have NEVR(R, t) < 0 for all times t.
However, we need not be limited to considering single
procedures. In a more general analysis, we allow for
the interleaving of arbitrary sequences of refinement
procedures, where each refinement procedure can be
allocated an arbitrary amount of effort, and to con­
sider sequences of refinements with the greatest ex­
pected value. As any refinement changes a model, and,
thus, changes the value of refinement for future model­
ing efforts, the identification of a theoretically optimal
sequence requires a combinatorial search through all
possibilities. Let us consider several approximations

181

A practical approach to dodging the combinatorial
control problem is to consider predefined quantities of
effort, and to employ a myopic or greedy EVR control
procedure. With a greedy assumption, we simplify our
analysis of control strategies by making the typically
invalid assumption that we will halt modeling, solve
the decision model, and take an action following a sin­
gle expenditure of modeling effort. We can further sim­
plify such a myopic analysis by assuming a predefined,
constant amount of effort to employ in NEVR anal­
yses. We compute the EVR(R(T)) for all available
refinement procedures R, where T is some constant
amount of time, or a quantity of time TR T(R), a
constant amount of time keyed to specific procedures.
At each cycle, we compute the NEVR for all proce­
dures, and implement the refinement procedure with
the greatest NEVR. We iteratively repeat this greedy
analysis until the cost of all procedures is greater than
the benefit, at which time we solve the decision prob­
lem and take the recommended action. Figure (10)
shows a fragment of the graph of possible model re­
finement steps.
=

�
\

I

'

,

...

...

\

: quantitative

�-"'

\structural

J ..... .
,

\

; quantitative

�-"'

/
I'

structural

,

\

\

conceptual

�

� ...
-,
9 , ... ,
o--.o �_) o--.o �_)
,

1

,

� �

quantitative

1 ,

��

...

quantitative

Figure 10: Greedy control of model refinement with
iterative application of NEVR analyses
We can relax the myopia of the greedy analysis by al­
lowing varying amounts of lookahead. For example,
we can consider the NEVR of two refinement steps.
Such lookahead can be invoked when single steps yield
a negative NEVR for all refinement methods. We can
also make use of theoretical dominance results. For ex­
ample, we have shown in a more comprehensive paper
that the expected value of perfect information (EVPI)
is the upper bound on the value of any structural re­
finement (Poh & Horvitz, 1992).

to such an exhaustive search.

31n practice, a decision consultant may wish to consider
such multiattribute cost models as the cost in time, dollars,
and frustration associated with pursuit of different kinds
of assessments and refinements.

4

Discussion and Related Work

The value of the EVR methods hinges on the avail­
ability of probability distributions that describe the

182

Poh and Horvitz

outcomes of extending models in different ways. We
suspect that expert analysts rely on such probabilis­
tic modeling metaknowledge, and that relatively stable
probability distributions can be assessed for prototyp­
ical contexts and states of model completeness. We do
not necessarily have to rely on assessing an expert deci­
sion analyst's probability distributions about alterna­
tive outcomes of modeling. In an automated decision
support setting, we can collect statistics about model­
ing and modeling outcomes. Such data collection can
be especially useful for the application of EVR-based
control strategies to automated reasoning systems that
construct models dynamically (Breese, 1987; Goldman
& Breese, 1992).
We are not the first to explore the value of modeling
in decision analysis. The value of modeling was first
addressed by Watson and Brown (1978) and Nickerson
and Boyd (1980). The notion of reasoning about the
value of probability assessment with an explicit consid­
eration of how second-order distributions change with
assessment effort has been explored rigously by Lo­
gan (1985). Chang and Fung (1990) have considered
the problem of dynamically refining and coarsening of
state variables in Bayesian networks. They specified
a set of constraints that must be satisfied to ·ensure
that the coarsening and weakening operations do not
affect variables that are not involved. In particular,
the joint distribution of the Markov blanket excluding
the state variable itself must be preserved. However,
the value and cost of performing such operations were
not addressed. The form of refinement that we re­
fer to as structural refinement has also been examined
by Heckerman and Jimison (1987) in their work on
attention focusing in knowledge acquisition. Finally,
related work on control of reasoning and rational deci­
sion making under resource constraints, using analyses
of the expected value of computation and considering
decisions about the use of alternative strategies and al­
locations of effort, has been explored by Horvitz (1987,
1990) and Russell and Wefald (1989).
5

Summary and Conclusions

We introduced and distinguished the expected value of
quantitative, conceptual, and structural refinement of
decision models. We believe that the analyses of the
value of model refinement hold promise for controlling
the attention of decisions makers, and of automated
reasoning systems, on the best means of extending a
decision model. Such methods can also be employed
to determine when it is best to halt refinement proce­
dures and instead to solve a decision model to identify
a best action. We look forward to assessing expert
knowledge about the value of decision-model refine­
ment and testing these ideas in real decision analyses.
We are striving to automate the assessment of knowl­
edge about model refinement, as well as the iterative
cycle of EVR computation. We are implementing key
ideas described in this paper within the IDEAL influ-

ence diagram environment (Srinivas & Breese, 1990).
Reference

Breese, J. S. (1987).

Knowledge Representation and
Inference in Intelligent Decision Systems. Ph.D.

thesis, Department of EES, Stanford University.
Chang, K.-C., & Fung, R. (1990). Refinement and
coarsening of bayesian networks. In Proceedings
of the Sixth Conference on Uncertainty in Arti­
ficial Intelligence, pp. 475-482.

Goldman, R. P., & Breese, J. S. (1992). Integrating
model constrution and evaluation. In Proceed­
ings of the Eighth Conference on Uncertainty in
Artificial Intelligence, pp. 104-111.

Heckerman, D. E., & Jimison, H. (1987). A perspective
on confidence and its use in focusing attention
during knowledge acquistion. In Proceedings of
the Third Workshop on Uncertainty in Artificial
Intelligence, pp. 123-131.

Horvitz, E. J. (1987). Reasoning about beliefs and ac­
tions under computational resource constraints.
In Proceedings of the Third Workshop on Uncer­
tainty in Artificial Intelligence, pp. 429-439.
Horvitz, E. J. (1990). Computation and action under
bounded resources. Ph.D. thesis, Depts of Com­
puter Science and Medicine, Stanford University.
Logan, D. M. (1985). The Value of Probability Assess­
ment. Ph.D. thesis, Department of Engineering­
Economic Systems, Stanford University.
Nickerson, R. C., & Boyd, D. W. (1980). The use and
value of models in decision analysis. Operations
Research, 28(1), 139-155.
Poh, K. L., & Horvitz, E. J. (1992). Probing the
value of decision-model refinement. Technical
Report 85, Palo Alto Laboratory, Rockwell In­
ternational Science Center, Palo Alto, CA.
Russell, S., & Wefald, E. (1989). Principles of metar­
easoning. In Brachman, R. J., Levesque, H. J.,
& Reiter, R. (Eds. ) , KR'89, Proceedings of the
F irst International Conference on Principles of
Knowledge Representation and Reasoning, pp.

400-411 Morgan Kaufmann.
Srinivas, S., & Breese, J. (1990). IDEAL: A software
package for analysis of influence diagrams. In
Proceedings of the Sixth Conference on Uncer­
tainty in Artificial Intelligence.

Tani, S. N. (1978). A perspective on modeling in de­
cision analysis. Mangt Sci, 24(14), 1500-1506.
Watson, S. R., & Brown, R. V. (1978). The valua­
tion of decision analysis. Journal of the Royal
Statistical Society, 141 (Part 1) , 69-78.
Wellman, M. P. (1988). Formulation of tradeoffs in
planning under uncertainty. Ph.D. thesis, De­
partment of EECS, MIT.


