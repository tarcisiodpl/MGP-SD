. The machine learning community has recently devoted much
attention to the problem of inferring causal relationships from statistical
data. Most of this work has focused on uncovering connections among
scalar random variables. We generalize existing methods to apply to collections of multi-dimensional random vectors, focusing on techniques applicable to linear models. The performance of the resulting algorithms
is evaluated and compared in simulations, which show that our methods
can, in many cases, provide useful information on causal relationships
even for relatively small sample sizes.

1

Introduction

Many techniques have recently been developed for inferring causal relationships
from data over a set of random variables [1,2,3,4,5,6,7,8,9]. While most of this
work has focused on uncovering connections among scalar random variables,
in many actual cases each of the variables of interest may consist of multiple
related, but distinct, measurements. For instance, in fMRI data analysis one
is often interested in the functional connectivity among brain regions, and for
each such region-of-interest one has data measured from a set of multiple voxels.
Typically, in these cases, some aggregate of each area is computed, after which
the standard approaches are directly applicable. However, it can be shown that
not only may information be lost when computing aggregates, but the outputs
of such methods may not even be correct in the large sample limit.
A simple example illustrating one of the problems inherent with working
with aggregates is the following. Consider three sets of variables with causal
connections X → Y → Z, i.e. the variables in X may influence the variables in
Y, but not directly the variables in Z, and the variables in Y may influence the
variables in Z. In this case, each variable x ∈ X is independent of each z ∈ Z
conditional on the full set of mediating variables Y. However, when replacing the
variables of each group with their respective mean value (the typical aggregate
used), denoted by x̄, ȳ, and z̄, in general we obtain x̄ ⊥⊥
/ z̄ | ȳ [1,10]. Thus,
it is important to develop methods for causal discovery that exploit the full
information available, as opposed to only aggregates of the data.
Towards this end, in this paper we extend two existing approaches [5,6] designed for causal discovery among scalar random variables to the case of random
vectors (i.e. groups of variables), both exploiting any kind of non-Gaussianity

2

Doris Entner, Patrik O. Hoyer

present in the data. We also extend a recent method [8] for inferring the causal
relationship among two arbitrarily distributed multi-dimensional variables to an
arbitrary number of such variables. After describing the resulting algorithms, we
evaluate and compare their performance in numerical simulations.

2

Model and Problem Statement

Throughout the paper, we will use the term ‘group’ to denote a set of underlying variables all belonging to the same (multi-dimensional) random vector
representing a single concept (e.g. one region in fMRI analysis). We use the term
‘variable’ to represent a single scalar random variable belonging to one of the
groups. Thus, for g = 1, . . . , G, let Xg denote group g, and let the random vector
(g)
(g)
xg = (x1 , . . . , xng )T collect the ng random variables belonging to group g. We
assume that the groups Xg can be arranged in a causal order K = (k1 , . . . , kG ),
such that the causal relationships among the groups can be represented by a
directed acyclic graph. The data generating process is assumed to be a set of
linear equations, given by
X
xki =
Bki ,kj xkj + eki , i = 1, . . . , G,
(1)
j<i

with Bki ,kj arbitrary (real) matrices of dimension nki × nkj , containing the
direct effects from group Xkj to group Xki . The vectors of disturbance terms
eki are assumed to be zero mean, and mutually independent over groups, i.e.
eki ⊥
⊥ ekj , i 6= j, but are allowed to be dependent within each group. If we
arrange the groups in a causal order K and define x = (xk1 , . . . , xkG ) and e =
(ek1 , . . . , ekG ), we can rewrite Equation (1) in matrix form as x = Bx+e with B
a lower block triangular matrix. The model reduces to standard LiNGAM (Linear
Non-Gaussian Acyclic Model, [4,5]) when ∀g : ng = 1 and all disturbances e are
non-Gaussian. It also includes the model of [6] when G = 2, n1 = n2 = 1 and the
disturbances are non-Gaussian. Finally, it contains as a special case the noisy
model of [8] when G = 2 (but with no restriction on the ng and e).
We assume that all variables in x are observed, and that the grouping of these
variables is known. Given merely observations of x generated by Model (1) (i.e.
B and e are unknown), we want to estimate the unknown causal order K among
these groups. We denote the data matrix of observations over the variables x as
X = (X1 , . . . , XG )T , where each column corresponds to one observation and each
row to one variable. The observations are grouped according to the G groups,
arranged in a random order, such that the first n1 rows correspond to group X1 ,
the following n2 rows to group X2 , and so on.
We note that our model family is equivalent to that given by [7]. The main
difference between our approach and theirs is that they do not assume to know
which variable belongs to what group, which results in algorithms exponential in
the number of involved variables, whereas our algorithm explicitly builds upon
such knowledge, allowing to construct computationally and statistically more
efficient algorithms, polynomial in the number of groups.

Estimating a Causal Order among Groups of Variables in Linear Models

3

3

Method

The overall algorithm for finding a causal order among the groups follows the
approach introduced in [5]. We first search for an exogenous group (Section 3.1),
and then ‘regress out’ the effect of this group on all other groups (Section 3.2).
We iterate this process to generate a full causal order over the G groups.
3.1

Finding an Exogenous Group

We generalize three existing methods to search for an exogenous group, formally
defined as below. Note that since the connections among the groups are assumed
to be acyclic, there always exists at least one such exogenous (‘source’) group.
Definition 1 A group Xj is exogenous if for xj all matrices Bj,i of Equation (1) are zero.
GroupDirectLiNGAM As our first approach, we generalize the idea of DirectLiNGAM [5] to find an exogenous variable, to finding an exogenous group.
The following lemma, which corresponds directly to Lemma 1 in [5], states a
criterion to find an exogenous group using regressions and independence tests.
Lemma 1. Let x follow Model (1) with non-Gaussian disturbance terms e. Let
(j)
r i := xi − Cxj be the residuals when regressing xi on xj using ordinary least
(j)
squares (OLS). A group Xj is exogenous if and only if xj ⊥⊥ r i for all i 6= j.
The proof of this lemma, and the proof of Lemma 2 in Section 3.2 are left to the
online appendix at http://www.cs.helsinki.fi/u/entner/GroupCausalOrder/
To apply Lemma 1 in practice, we need to test for (in)dependence between
two vectors of random variables, and combine the results of several such independence tests. Assuming that the test returns p-values pji under the null
(j)
hypothesis of xj ⊥
⊥ r i , i 6= j, we can get a measure of how exogenous group
Xj is by combining these p-values using Fisher’s method [11]. This means that
we select as the exogenous group the one minimizing
X
µ(j) = −
log(pji ).
(2)
i6=j

To obtain the p-values pji we can test for joint dependence of the two vec(j)
tors xj and r i using the Hilbert Schmidt Independence Criterion (HSIC, [12]),
which, however, requires many samples to detect dependencies for high dimensional vectors. Alternatively, we can perform pairwise tests of each variable in
(j)
xj against each variable in r i using nonlinear correlations, and combine the
resulting nj × ni p-values appropriately. Details are left to the online appendix.
Pairwise Measure Our second approach is based on modifying the pairwise
measure [6] designed for inferring the causal relationship between two linearly
related non-Gaussian scalar random variables x and y. If the true underlying
causal direction is from x to y, the model is defined as y = ρx + ey with x ⊥⊥ ey .

4

Doris Entner, Patrik O. Hoyer

As pointed out in Section 2, this is just a special case of our more general model.
The (normalized) ratio of the log likelihoods for the two possible causal models is
given by R(x, y) = (log L(x → y) − log L(y → x)) /m, where m is the sample size
and L the likelihood of the specified direction, under some suitable assumption
on the distributions of the disturbances. If the true underlying causal direction
is x → y, then R(x, y) > 0 in the large sample limit. Symmetrically, if x ← y,
then R(x, y) < 0 in the limit.
To use the ratio R(· , ·) to find an exogenous group Xj , the naı̈ve approach
(j)
(i)
(j)
(i)
is to calculate R(xk , xl ) for each pair with xk ∈ Xj , k = 1, . . . , nj , xl ∈
Xi , l = 1, . . . , ni , i 6= j, and combine these measures. However, even if Xj is
exogenous, these pairs do not necessarily meet the model assumption because of
the dependent error terms within each group, and hence there is no guarantee
for correctness even in the large sample limit. This approach, termed the Naı̈ve
Pairwise Measure, may however have a statistical advantage for small sample
sizes (see Section 4).
To obtain a consistent method (simply termed Pairwise Measure in Sec(j)
(i)
tion 4), we replace the second variable of the pairs (xk , xl ) with a quantity
which guarantees that the model assumption is met if Xj is exogenous: We
Pnj
(i)
(j)
first estimate the regression model xl = k̃=1
b̂lk̃ xk̃ + rl,(i) . If Xj is exogenous then the regression coefficients b̂lk̃ are consistent estimators of the true
total effects (when marginalizing out any intermediate groups). Hence, defining
(i)
(i) Pnj
(j)
(j)
(j) (i)
zk,l := xl − k̃=1;
b̂ x = b̂lk xk +rl,(i) yields a pair (xk , zk,l ) meeting the
k̃6=k lk̃ k̃
(j)

(i)

model assumption of [6] if Xj is exogenous. Thus, in this case, R(xk , zk,l ) > 0,
in the limit, for all k, l, and i 6= j. On the contrary, if Xj is not exogenous the
measure can take either sign, and simulations show that it is unlikely to always
obtain a positive one. A way to combine the ratios is suggested in [6], which can
be modified for the group case as
(j)

µ

=

nj

1
P

i6=j ni

nj
ni
X
XX

(j)

(i)

min{0, R(xk , zk,l )}2 .

(3)

k=1 i6=j l=1

That is, we penalize each negative value according to its squared magnitude and
adjust for the group sizes. We select the group minimizing this measure as the
exogenous one.
Trace Method Our third method for finding an exogenous group is based on
the approach of [8,9], termed the Trace Method, designed to infer the causal order
among two groups of variables X and Y with nx and ny variables, respectively.
If the underlying true causality is given by X → Y, the model is defined as y =
Bx+e, where the connection matrix B is chosen independently of the covariance
matrix of the regressors Σ := cov(x, x), and the disturbances e are independent
of x. Note that this method is based purely on second-order statistics and does
not make any assumptions about the distribution of the error terms e, as opposed
to the previous two approaches where we needed non-Gaussianity. The measure
to infer the causal direction defined in [8] is given by

Estimating a Causal Order among Groups of Variables in Linear Models







∆X→Y := log tr(B̂Σ̂B̂T )/ny − log tr(Σ̂)/nx − log tr(B̂B̂T )/ny

5

(4)

where tr(·) denotes the trace of a matrix, Σ̂ an estimate of the covariance matrix
of x, and B̂ the OLS estimate of the connection matrix from x to y. The measure
for the backward direction ∆Y→X is calculated similarly by exchanging B̂ with
the OLS estimate of the connection matrix from y to x and Σ̂ with the estimated
covariance matrix of y. If the correct direction is given by X → Y, Janzing et
al. [8] (i) conclude that ∆X→Y ≈ 0, (ii) show for the special case of B being an
orthogonal matrix and the covariance matrix of e being λI, that ∆Y→X < 0,
and (iii) show for the noise free case that ∆Y→X ≥ 0. Hence, the underlying
direction is inferred to be the one yielding ∆ closer to zero [8]. In particular, if
|∆X→Y | / |∆Y→X | < 1, then the direction is judged to be X → Y.
We suggest using the Trace Method to find an exogenous group Xj among G
groups in the following way. For each j, we calculate the measures ∆Xj →Xi and
∆Xi →Xj , for all i 6= j, and infer as exogenous group the one minimizing
X
2
∆Xj →Xi / ∆Xi →Xj .
µ(j) =
(5)
i6=j

3.2

Estimating a Causal Order

Following the approach of [5], after finding an exogenous group we ‘regress out’
the effect of this group on all other groups. Since the resulting data set follows
again the model in Equation (1) having the same causal order as the original
groups, we can search for the next group in the causal order in this reduced data
set. This is formally stated in the following lemma, which corresponds to the
combination of Lemma 2 and Corollary 1 in [5].
Lemma 2. Let x follow Model (1), and assume that the group Xj is exogenous.
(j)
Let r i := xi − Cxj be the residuals when regressing xi on xj using OLS,
for i = 1, . . . , G, i 6= j, and denote by r (j) the column vector concatenating all
these residuals. Then r (j) = B(j) r (j) + e(j) follows Model (1). Furthermore, the
(j)
residuals in r i follow the same causal order as the original groups xi , i 6= j.
Using Lemma 2, and the methods of Section 3.1, we can formalize the approach
to find a causal order among the groups as shown in Algorithm 1.
3.3

Handling Large Variable Sets with Few Observations

The OLS estimation used in Algorithm 1 requires an estimate of the inverse
covariance matrix which can lead to unreliable results in the case of low sample
size. One approach to solving this problem is to use regularization. For the L2 regularized estimate of the connection matrix we obtain Ĉi,j = Xi XTj (XTj Xj +
λI)−1 = cov(Xi , Xj ) m (m cov(Xj , Xj ) + λI)−1 , with m the sample size and λ
the regularization parameter, see for example [13]. In particular, this provides a
regularized estimate of the covariance matrix.

6

Doris Entner, Patrik O. Hoyer

Algorithm 1 (Estimating a Causal Order among Groups)
Input: Data matrix X generated by Model (1), arranged in a random causal order
Initialize the causal order K := [ ].
repeat
Find an exogenous group Xj from X using one of the approaches in Section 3.1.
Append j to K.
Replace the data matrix X with the matrix R(j) concatenating all residuals
(j)
Ri , i 6= j, from the regressions of xi on xj using OLS:
(j)

Xi = Ci,j Xj + Ri

with Ci,j = cov(Xi , Xj ) cov(Xj , Xj )−1 .

until G − 1 group indices are appended to K
Append the remaining group index to K.

Another approach is to apply the methods of Section 3.1 for finding an exogenous group to N data sets, each of which consists of G groups formed by
taking subsets of the variables of the corresponding original groups. We then
(j)
calculate measures µn , j = 1, . . . , G, n = 1, . . . , N , as in Equations (2), (3) or
(5), for each such data set separately, and pick the group Xj ∗ which minimizes
the sum over these sets to be an exogenous one, i.e.
X
j ∗ = arg min
µ(j)
(6)
n
j

1≤n≤N

(j)

where µn is the measure of group j in the nth data set. We then can proceed
as in Algorithm 1 to find the whole causal order.
Note that the same approach can be used when multiple data sets are available, which are assumed to have the same causal order among the groups but
possibly different parameter values. An example for such a scenario is given by
fMRI data from several individuals. An equivalent of Equation (6) was suggested
in [14] for the single variable case with multiple data sets.

4

Simulations

Together, the methods of Section 3 provide a diverse toolbox for inferring the
model of Section 2. Here, we provide simulations to evaluate the performance of
the variants of Algorithm 1, and compare it to a few ad hoc methods. Matlab
code is available at http://www.cs.helsinki.fi/u/entner/GroupCausalOrder/
We generate models following Equation (1) by randomly creating the connection matrices Bki ,kj , i > j with, on average, s% of the entries being nonzero
and additionally ensure that at least one entry is nonzero, to ensure a complete
graph over the groups. To obtain the disturbance terms eki for each group, we
linearly mix random samples from various independent non-Gaussian variables
as to obtain dependent error terms within each group. Finally, we generate the
sample matrix X and randomly block-permute the rows (groups) to hide the
generating causal order from the inference algorithms.

Estimating a Causal Order among Groups of Variables in Linear Models
6 vars / group

100 vars / group

12 vars / group

0.6

0.8

0.6

GDL,nlcorr

0.3
0.2
0.1

TrMeth.
0
200PwMeas.
500 1000

0.4
0.3

ICA−L
DL,nlcorr

0.2

DL,HSIC

0.1
500

sample size

1000

0
200

GDL,HSIC

500

1000

sample size

(a) 100 models with 5 groups

1 GDL,nlcorr
GDL,nlcorr,10sets
0.5 TrMeth.,L2reg

0.6

error rate

0.4

0
200

0.5

0.5

error rate

error rate

0.5

7

TrMeth.,10sets
0
200PwMeas.,L2reg
5001000

0.4

PwMeas.,10sets
0.2

0
200

NaivePwMeas.
500

1000

sample size

(b) 50 models with 3 groups

Fig. 1. Sample size (x-axis) against error rate (y-axis) for various model sizes and
algorithms, as indicated in the legends (abbreviations: GDL = GroupDirectLiNGAM;
nlcorr, HSIC: nonlinear correlation or HSIC as independence test; TrMeth. = Trace
Method; PwMeas. = Pairwise Measure; ICA-L = modified ICA-LiNGAM approach;
DL = DirectLiNGAM on the mean-variables; 10sets = Equation (6) on N = 10 data
sets; L2reg = L2 -regularization for covariance matrix). The dashed black line indicates
the number of mistakes made when randomly guessing an order.

We compare the variants of Algorithm 1 to two ad hoc methods. The first
one is a modified ICA-based LiNGAM approach [4] where instead of searching
for a permutation yielding a lower triangular connection matrix B (i.e. finding a
causal order among the variables), we search for a block permutation yielding a
lower block triangular matrix B (i.e. finding a causal order among the groups).
Secondly, we compare our approach to DirectLiNGAM [5], when replacing each
group by the mean of all its variables.1
We measure the performance of the methods by computing the error rates
for predicting whether Xi is prior to Xj , for all pairs (i, j), i < j.
Results for simulated data of sample size 200, 500 and 1000 generated from
100 random models having 5 groups with either 6 or 12 variables each, and
s = 10%, are shown in Figure 1 (a). As expected, most methods based on
Algorithm 1 improve their performance with increasing sample size. The only
exception is the Trace Method on the smaller models; to be fair the method was
not really designed for so few dimensions. Overall, the best performing method
is the Pairwise Measure, closely followed by GroupDirectLiNGAM for the larger
sample sizes. The ad hoc methods using DirectLiNGAM on the mean perform
about as well as guessing an order (indicated by the dashed black line), whereas
the modified ICA-LiNGAM approach performs better than guessing. However, it
does not seem to converge for growing sample size, probably due to the dependent
errors within each group, which is a violation of the ICA model assumption.
We next replace each group by a subset of its variables of size m = 1, . . . , ng ,
and apply Algorithm 1 to these subgroups. As expected, the larger m is, the less
ordering mistakes are made. Details can be found in the online appendix.
1

We do not compare our results to methods such as PC [1] or GES [3], as they
cannot distinguish between Markov-equivalent graphs. Hence, in these simulations,
they cannot provide any conclusions about the ordering among the groups since we
generate complete graphs over the groups to ensure a total causal order.

8

Doris Entner, Patrik O. Hoyer

Finally, we test the strategies described in Section 3.3 for handling low sample
sizes in high dimensions on 50 models with 3 groups of 100 variables each, using
200, 500 and 1000 samples, and s = 5%. For L2 -regularization, we choose the parameter λ using 10-fold cross validation on the covariance matrix. When taking
subgroups, we use N = 10 data sets, and each subgroup containing ten variables.
The error rates are shown in Figure 1 (b) (we only show the L2 -regularized results if they were better than without regularization). Unreliable estimates of the
covariance matrix seem to affect especially the Trace Method, and the Pairwise
Measure on the smaller sample sizes. On the smallest sample, using subsets seems
to be advantageous for most methods, however, the best performing approach
is the Naı̈ve Pairwise Measure, which, however, does not seem to converge to be
consistent, where as GroupDirectLiNGAM and the Pairwise Measure are.
In general, the simulations show that the introduced method often correctly
identifies the true causal order, and clearly outperforms the simple ad hoc approaches. It is left to future work to study the performance in cases of model
violations as well as to apply the method to real world data.
Acknowledgments We thank Ali Bahramisharif and Aapo Hyvärinen for discussion. The authors were supported by Academy of Finland project #1255625.

