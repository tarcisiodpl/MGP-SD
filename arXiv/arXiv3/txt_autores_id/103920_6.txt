 This paper presents a new approach for com puting posterior probabilities in Bayesian nets  which sidesteps the triangulation prob lem  The current state of art is the clique tree propagation approach  When the underlying graph of a Bayesian net is triangulated  this approach arranges its cliques into a tree and computes posterior probabilities by appropri ately passing around messages in that tree  The computation in each clique is simply di rect marginalization  When the underlying graph is not triangulated  one has to first tri angulated it by adding edges  Referred to as the triangulation problem  the problem of finding an optimal or even a  good  trian gulation proves to be difficult  In this pa per  we propose to first decompose a Bayesian net into smaller components by making use of Tarjan s algorithm for decomposing an undirected graph at all its minimal complete separators  Then  the components are ar ranged into a tree and posterior probabili ties are computed by appropriately passing around messages in that tree  The compu tation in each component is carried out by repeating the whole procedure from the be ginning  Thus the triangulation problem is sidestepped      INTRODUCTION  There has been in recent years extensive research in computing marginal and posterior marginal probabili ties in Bayesian nets  Largely due to the work of Pearl         Lauritzen and Spiegehalter         Shafer and Shenoy         and Jensen et al          an effi cient algorithm called clique tree propagation has come into being  When the underlying graphs are triangu lated  the algorithm arranges the cliques of the un derlying graphs into join trees and obtains the wanted marginals and posteriors by passing messages around in those trees  The computation in each clique is  simply direct marginalization  When the underlying graphs are not triangulated  one needs to triangulate them first  Lauritzen and Spiegehalter        or ap peal to the technique of conditioning on loop cutsets  Pearl        A similar preprocessing step is also re quired in the goal directed approach of shacter et a           In this paper  we shall refer to the problem of finding an optimal or a  good  triangulation for an untriangulated graph as the triangulation problem  It is NP hard to find either optimal loop cutsets  Stillman       or optimal triangulations  Yannakakis        Although heuristics are available  Stillman        has  demonstrated that no heuristic algorithm      can be guaranteed to produce loop cutsets within a constant difference from optimal   We conjecture that the same thing is true for triangulation  Thus  it is in teresting to investigate the possibility of approaches that sidestep the triangulation problem  Poole and Neufeld        describes an interesting im plementation of Bayesian nets in Prolog  The imple mentation encodes conditional probabilities as Prolog goal reduction rules and it organizes reasoning by us ing Bayes  Theorem and causality relationships among the variables  The need for triangulation is avoided  In this paper  we describe a new approach for comput ing posterior probabilities in Bayesian nets  which also sidesteps the triangulation problem  What differenti ates our approach from Poole and Neufeld s approach is that the former makes full use of decompositions  while the latter does not at all  The cornerstone of our approach is Tarja n s algorithm for decomposing undirected graphs  An undirected graph may contain many complete sep arators even if it not triangulated  Tarjan        presents a O nm  time algorithm for decomposing an undirected graph at all its minimal complete separa tors  where n and m are the numbers of vertices and edges of in the graph respectively  It is easy to con ceive a procedure that decomposes a Bayesian net into as many components as possible by using Tarjan s al gorithm on the moral graph  Our approach can be viewed as an advancement of the   Sidestepping the Triangulation Problem in Bayesian Net Computations  clique tree propagation approach  Like the clique tree approach  this approach first arranges the components of a Bayesian net into a tree  and then computes poste rior probabilities by appropriately passing information around in that tree  Unlike the clique tree approach  the computation in each component is not carried out by direct marginalization  Rather it is carried by re peating the whole procedure from the beginning  The organization of the paper is as follows  Section   starts the paper by reviewing some graph theory terminologies and some basic concepts pertaining to Bayesian nets  Section   introduces the concept of de composition in terms of semi Bayesian nets  Section   shows how decomposition leads to the parallel reduc tion technique  Section   introduces serial reduction the basic means for passing messages from one com ponent to another  and section   constructs the com ponent tree   the basic means for organizing message passing  The algorithm is given in section    together with an example illustrating how it works  The paper concludes at section       PREREQUISITE  no parents are roots  The set of all the parents of a vertex v will be notated by  r v   A directed cycle is a sequence of vertices in which every vertex is a parent of the vertex after it and the last vertex is a parent of the first vertex  An acyclic directed graph is one in which there are no directed cycles  To marry all the parents of a vertex in a directed graph is to add an  undirected  edge between each pair of its parents  The moral graph m G  of a directed graph G is an undirected graph obtained from G by marrying all the parents of each vertex respectively and ignoring all the directions on the arcs  A directed graph is connected if its moral graph is  Let us now review a few concepts pertaining to Bayesian nets  A Bayesian net   is a triplet  V  A  P   where     V is a set of variables     A is a set of arcs  which together with V consti tutes a directed acyclic graph G    V A      P    P vJ r v     v E V   i e P is the set the conditional probabilities of the all variables given their respective parents     Let us begin by reviewing some graph theory termi nologies  An  undirected  graph G    V  E  consists of a set V of vertices and a set E of edges  which are  unordered  pairs of vertices  A path is a sequence of vertices in which every pair of consecutive vertices is an edge  A loop is a path where no vertex appear twice except the first one and the last one  which are the same  A chordless loop is a loop in which no pair of non consecutive vertices is an edge  A genuine loop is a chordless loop of length greater than three  A triangulated graph is one without genuine loops   Variables in a Bayesian net will also be referred to as vertices and nodes when the emphasis is on the underlying graph   A graph is connected if there is a path between any two vertices  If a graph G is disconnected  a connected component of G consists of a subset of vertices in which vertices are connected to each other but not to vertices outside the subset  and of all the edges that are com posed of vertices in the subset   Y  V be the set of variables observed and Yo be the corresponding set of values  Let X  V be the set of variables of interest  The posterior probability Px XIY  Yo  of X in a Bayesian net N  given obser vations Y   Yo is obtained by first conditioning Px on Y   Yo and marginalizing the resulting joint probabil ity onto X   The problem of concern to this paper is how to compute P XIY  Yo    A separator of a graph is a subset of its vertices whose deletion from the graph will leave it disconnected  Two vertices is separated by a separator if every path con necting them contains at least one vertex in the separa tor  A separator is minimal if none of it proper subsets are separators  A subset of vertices is complete if ev ery pair of its elements is an edge  Separators can be complete  Cliques are maximal complete subsets of vertices  When the set of all the vertices is complete  we say that the graph is complete   A directed graph G    V  A  consists of a set V of vertices and a set A of arcs  which are ordered pairs of vertices  If there is an arc from vertex v  to vertex v   then v  is a parent of v  and v  is a child of v   Vertices with no children are leaves and vertices with  The prior joint probability Px of a Bayesian net    is defined by Px V     IT P vl r v          vEV  Observations may be made about variables   Let  Developed by Lauritzen and Spiegehalter         Shafer and Shenoy         Jensen et al         the clique tree propagation approach consists of three steps      Triangulate the moral graph of G      ar range the cliques into a join tree  and     properly pass messages around in the tree  Consider the Bayesian net nett in Figure        The following prior probability and conditional probabil ities are given as part of the specification  P c   P aic   P eia   P fle   P gJJ   P bia g   P hlb   and  Note that when vis a root  r v  is empty  In such a case  the expression P vJlr v   simply stands for the prior probability of v   That is to sum out all the variables not in X              Zhang and  Poole  are not given  Thus  Bayesian nets are semi Bayesian nets whose set of unspecified roots is empty  In a semi Bayesian net N    V A  R       the R i s the set of roots of the directed graph   V  A  whose prior probabilities are not given in P  So  we call R the set  I   of unspecified roots of  V  A            U  The prior joint potential P N a semi Bayesian net  N     V  G  R  P  is defined by  Px V     IT  P vj r v            EY R     And for any subset X of V   the  marginal  potential Px X  of X in  N is obtained by marginalizing P  V  onto X  i e by summing out all the variables in V  X            Q      l rop  plioa  If N is a  Bayesian net  then its prior joint potential and marginal potentials are exactly the same as its prior joint probability and marginal probabilities defined in the previous section        Figure    Bayesian net  triangulation and message passing   P dJc  h   The moral graph of its underlying graph is shown        The dashed edges are added for tri angulation  The corresponding join tree is shown in Figure        One can obtain Pnttl  dJe  eo  by pass ing messages in the way as indicated by the arrows  See the cited papers for details about the contents of the messages and how are they actually passed in the join tree  Let X be the set of variables of interest  Let Y be the set of observed variables  and Yo be the corre sponding set of observed values  We are concerned with queries of the form P  XJY   Yo   Noticing that P  XIY  Yo  can be obtained normalizing the marginal probability P N X  Y  Yo   we shall concern ourselves with queries of the form P N X  Y   Yo  in this paper      DECOMPOSITION  In this section  we shall introduce the concept of de composition in terms semi Bayesian nets  A semi Bayesian net  N is a quadruplet  N   V  A  R       where        V is a set of variables     A is the set of arcs  which together with V con stitutes a directed acyclic graph G   V  A      R is a set of roots of G           P vj r v    v E  V  R     In words  semi Bayesian net are Bayesian nets with un specified roots  i e with roots whose prior probabilities  Let us now turn to decomposition of undirected graphs  An undirected graph G    V  E  decomposes into Gt    Vi Et  and G      V  E    or simply into Vt and V  if      Vt and V  are proper subsets of V and Vt U V    V     Ei  i         consists of all the edges in E that lie completely within v  and Et U E    E  and    Vt n V  is a complete separator of G  In such a case  we say that G is de c o m p osable  There is a one to one correspondence between com plete separators and decompositions  Given any com plete separator S of an undirected graph G  G can be decomposed at s into to Vi and v  such that VI n v    s  And if G decomposes into vl and v   then Vt n V  is a complete sep arator  Tarjan        presents an O nm  time algorithm for decomposing an undirected graph at all its minimal complete separa tors  where n and m stands for the numbers of ver tices and edges of G respectively  Tarjan s algorithm is very important to our approach for computing pos terior probabilities in Bayesian nets  A semi Bayesian net  N   V  A  R      is decomposable if the moral graph m   G  of the underlying directed graph G    V  A  is decomposable  A decomposi tion  Vt  V   of m  G  induces the decomposition of the semi Bayesian net N into  Nt  J      V   A   R         where        Vt  At  R   PI  and  P  is obtained from    by removing all the items that do not involve any variables in V    V  and                And fori  E             A  is obtained A by removing any arcs whose  two vertices do not simultaneously appear in any item of  Pi  and   Sidestepping the Triangulation Problem in Bayesian Net Computations  Px X z  Y      E Px   X S Px  S Y         S Z  Figure    R   is      The theorem follows from the definition decomposi tions of semi Bayesian nets and the distributivity of multiplication w  r  t summation  Instead of giving the detailed proof  we shall provide an illustrating exam ple   A decomposition of net     the       A    P     set  of  unspecified  roots  of  Consider the Bayesian net net  in Figure    a   The The set  a  b  is a minimal complete separator  moral graph of the directed graph decomposes into  a b c d h  and  a b e f g   This induces the de composition ofnet  into semi Bayesian nets net  and net  as shown in Figure    In net   P c   P alc   P hlb   and P dlb c  are inherited from net   There is no arc from a to b because they do not simultaneously appear in any of those  conditional  probabilities of net   The root b of net  is unspecified because there is no P  b  in net   The conditional probabilities in net  are P ela   P fle   P glf   and P bla g   The root a is unspecified because there is no P a   A semi Bayesian net is simple if it contains onl y one leaf node and all other nodes are parents of this leaf node  In any semi Bayesian net that is not simple  there is always a leaf node  say l  The set  r l  of the parents of   is a complete separator  which separates I from all the nodes not in  r l  U      So  there always a minimal complete separator S that separates l from all the nodes not in S U      Thus we have  Proposition   If a semi Bayesian net is not simple  then it is decomposable  Remark  We do not want to decompose a Bayesian net at a separator that is not complete for two rea sons  First  it increases the complexity of the problem itself  and second the number of separators that are not complete may be too large      PARALLEL REDUCTION  In this section  we shall show how the concept of de composition can be used in computing marginal po tentials in semi Bayesian nets  marginal probabilities in Bayesian nets   First of all  we have the following theorem   Theorem   Suppose a semi Bayesian net    decom poses into     and     at a minimal complete separator S  of the moral graph of its underlying directed graph   Let X be a subset of variables of J    which do not ap pear in S  let Y bet a subset of variables of J   which do not appear in S  and let Z be a subset of S  Then  Suppose we want to compute Pnan d a e  in the Bayesian net net  shown Figure        By definition  we have  Prun d  a e     L P c P alc P eia P fle P glf   c b j g h  P bla  g P h b P dic  b     L  L P c P alc P hlb P dlc  b   b  c h   L P ela P f le P ulf P bla  g       J g   L Pnatz d a b Pnat  a b e   b       Equation     is true because of the distributivity of multiplication w r t summation  and equation     im mediately follows from Lhe definition of marginal po tentials in semi Bayesian nets  Suppose we want to compute the marginal potential in a semi Bayesian net N  And sup pose     decomposes into J    and J    at a minimal com plete separator S of the moral graph of its underlying directed graph  Let X  be the set of variables in X and in J    but not in S  X  be the set of variables in X and in J    but not in S  and Xs  X n S  The sets Yt  Y  and Ys are defined from Y in the same way  Let Y     YsUYz and Y     YsUYt  Let  Y t a and  Y   o be the corresponding sets of values of Y   and  Px X Y  Yo   Y    The query induced in     by Px X Y   Yo  is Px  Xt   S  Ys Y      Y   o   and the query induced in J   is Px  Xz S  Ys Y      Y do   According to Theorem    we have  PN X Y  Yo  L  Px  Xt S   Ys Y      Y z o     Xs Ys  PN  Xz S  Ys  Y      Y t o        Px X Y   Yo   we can first com Ys Y      Y   o  and Px  X  S Ys  Y t    Y do   possibly in parallel   and then  Thus to compute pute Px   X t S              Zhang and Poole combine the results by equation      This leads to the technique of parallel reduction   gin by introducing the concept of parametric semi Bayesian nets   Given a semi Bayesian net      the following procedure computes PJI X  Y  Yo    A parametric semi Bayesian net  II    V  A  R   P  is a semi Bayesian net  except that some of its conditional probabilities contain parameters  i e variables that are not members of V  Thus  semi Bayesian nets are para metric semi Bayesian nets that do not contain any pa rameters   Parallel Reduction H  II is simple  then compute PJ   X  Y  Yo   directly by marginalization  else find a minimal complete separator S of the moral graph of the underlying directed graph of  If  decompose  If at S into      and           Repeat the procedure to compute the induced query Px   XI  S  Ys  Y       Y   o      Repeat the procedure to compute the induce query Px  X  S  Ys Y      Y do      Combine the answers to the two induced queries by using equation      The procedure is termed parallel reduction because line   and line   can be executed strictly in parallel  Because of Proposition    the procedure Parallel Re duction is able to compute marginal potentials in semi Bayesian nets without triangulating the underlying graphs  But the algorithm can be very inefficient  The main purpose of this paper is to describe another algorithm called component tree propagation  which we hope is as efficient as the clique tree propagation approach based on an optimal triangulation  The algo rithm computes posterior probabilities in a Bayesian net as follows  first the Bayesian net is decomposed into components  the components are arranged into a tree  and then posterior probabilities are obtained by appropriately passing messages around in that tree  In the next section  we shall introduce the basic means for passing messages from one component to another   the serial reduction technique  In the section af ter  we shall present the basic means for controlling message passing   component trees      SERIAL REDUCTION  Suppose a semi Bayesian net  II decomposes into two components  A query in  II reduces into two sub queries  one in each of the components  The paral lel reduction procedure first computes both of the two subqueries  possibly in parallel   and then use an ad ditional formula to combine the answers to get the answer to the original query  In contrast  the serial reduction procedure will first compute only one of two subqueries  The answer is then send to the other sub query  which is thereby updated  The answer to the updated subquery is the same as the answer to the original query  This section is devoted serial reduction  Let us be   Suppose  II is a semi Bayesian net with parameters W   And suppose we want to compute the potential of  X  Y  Yo   The answer will be a function of the parameters W as well as of X  So  we shall write the query as PJ   X Y   Yo   W   where the column mark separates parameters from variables in the semi Bayesian net  Given a query PJ   X  Y   Yo   W  in a parametric semi Bayesian net       a node is laden if it is a leaf of  II and it is in the set Y  i e it is observed    Suppose  II decompose into      and N  at a minimal complete separator S  Let the sets X   Xs  X   Y   Ys  Y   Y    and Y   are defined as before  Let W  and w  be the parameters of  Ill and      respectively  As in the case of parallel reduction  the query Q  PJ   X  Y  Yo   W  induces a subquery Ql in      and a subquery Q  in       The induced subquery Q  is P N   XI  S  Ys   Y       Y   o   WI   and the induced subquery Q  is PJ l X   S  Ys   Y       Y do   W    The answer to the subquery Q  is a function fo X   S Ys   W    We extend it to be a function f X   S  W   by setting    X   S   W      To to    fo Xz S  Ys W      append the answer  ifYs    Ys o otherwise  f X     S  W   of Q   to       is     Introduce an auxiliary variable v into       which has only two possible values   and       Draw an arc to v from each variable inS     Set P v  OI S      X  S  W    Let  II  denote the resulting semi Bayesian net  To use the query Q is to replace it by Q   PJI  X Xs Y      Y   o v      X   W   Note that the auxiliary variable v is a laden variable in the updated subquery  Also note that the the semi Bayesian net  II  contains the parameters W U X    the answer of Q  to update  Theorem   Suppose  II is semi Bayesiant net with  parameters W  which decomposes at a minimal com p l ete separator S into      and       Let all the symbols be as defined above  Then  PJI X Y  Yo  W    PJit Xt Xs  Y      L  o v        X   W        Sidesteppin g the Triangulation Problem in Bayesian Net Computations            l          Q               y       G                aotiO  Figure    A serial reduction of net    Px X  Y    Yo   W   we can first compute the induced subquery Px  X  S  Ys  Y t    Y do   W   use the answer    e   Q    lidII  The theorem says  to compute the query  to construct Jl   and then compute the updated query P w  X  Xs Y      Y       X   W   This procedure  Figure    A  the  component tree for nett   Again  instead of give the detailed proof of the theo  all the non trivial minimal complete  NMC  separators of J  and decompose Jl at them into smaller compo nents  The component tree of J  is a tree whose nodes consists of all those smaller components  The tree is  is called serial reduction   rem  we shall provide an illustrating example  Con sider computing Pnetl  d  e   in the Bayesian net net   shown in Figure        The net decomposes into net  and netS  in Figure    at the minimal separator  a  g      Ve first compute Pnets a g e     and append the result to net   This gives us net   Drawn in dotted cycle  v is an auxiliary laden variable introduced  The con ditional probability of v     give a and g is set by P v   Oja g     Pnets a g  e     Thus net  contains the parameter e  The updated query is Pnet  d v       e   To see that Pnd  d v      e    Pnen d e   we notice that net  decomposes into net  and net   and that  Pnea a g v      Pnet  d v           P v        e    L Pnet   d a  g Pnet   a g v             Pnet  d a  g Pnecs  a g e  a  g      constructed as follows  Start with an arbitrary component  While there are still components left out of the tree  choose one such component  say C  which contains an NMC separator that is also con tained in one or more components in the tre  Add C to the tree by connect it to one of those components that also contain the NMC separator      Ola g    Pnecs a g e   So  a g     algorithm  one can easily conceive a procedure to find  Pnen d e    For the Bayesian net net  in Figure    there are three NMC separators are   c  h    a b   and  a g   As shown in Figure   the resulting components are netB with prior probability P c  and conditional proba bilities P djc  h   net  with conditional probabilities  P alc  and P hlb   netlO with conditional probabil P bla g   and net   with conditional probabilities P e a   P f e   and P g f   In this example  there is  ity  only one component tree  which is denoted by treel  and is shown in Figure        COMPONENT TREES  Given a query Px X Y   Yo   W  in a parametric semi Bayesian net J    a minimal complete separator  of the moral graph of the underlying directed graph of J    is trivial if it is a subset of r l  for some laden node l and its deletion from the moral graph only re sult in no more than two connected components  Tarja n s algorithm decomposes an undirected graph at all its minimal complete separators  Based on this  The union of two semi Bayesian nets  Vt A  R  Pt  and  V   A  R        is the semi Bayesian net  Vt U V  A  U A  R   t U       where R is the set of un specified roots of  Vt U V   A  U A       U       SupposeT is a component tree of a semi Bayesian net N  and     is a leaf of T  Then     decomposes into     and       the union of all other nodes ofT  Let Q be a query in J  and let Q  be the induced query in  It can be proved that this construction does result in a tree             Zhang and Poole  N  Then we can use the answer to Q  to update the         query Q           COMPONENT TREE PROPAGATION      t    We are now ready to give the component tree propaga tion algorithm  Let N be a parametric semi Bayesian net with parameters W  Here is our algorithm for com puting the answer to the query PJI X  Y   Yo   W           Main N   X  Y  Yo       If N has NMC separators  call the pro cedure SerialReduction N   X  Y   Yo       If N does not have any NMC sep arators  call the procedure Parallel Reductionl N   Serial Reduction N    X  Y  Yo      X  Y  Yo        Decompose N at all its NMC separators  and construct a component tree T     While there is at least two nodes in T do       Pick a leaf N  of T by calling the pro cedure Pick leaf T   X  Y  Yo    Call Main to compute the induced subquery Q  in N   Append the answer of Q  to the com ponent that is the neighbor of N  in T     Remove N  from T and use the an swer to Q  to update the current query      When there is only one node left in T  call Main to compute the current query   In each pass of the while loop  the current query is updated  According to Theorem    the answer to the current query is the same as the answer to the updated query  Thus  the answer to the current query when there is only one node left in T is the same as the answer to the original query P N X  Y  Yo   W   The input of to the procedure Parallel Reductionl is parametric semi Bayesiant net N and a query P N X  Y   Y    W  such that N does not have any NMC separators  The output is the answer to the query  Parallel Reductionl N    X  Y   Yo     If N is simple  then compute the answer to the query P N X  Y   Yo   W  directly by marginalization  else  Figure    Semi Bayesian nets created in computing Pnen d  e         Pick a laden node    by calling the pro cedure Pick laden node N   X  Y    Yo       Decompose N at the set  r l  of parents of l into N  and N      Call Main to compute the induced sub squeries in N  and in N       Combine the answers to the subqueries using equation       Our primary investigations indicate that the proce dures Pick leaf and Pick laden node are important to the the performance of the algorithm  How can one define those procedures so that the al gorithm achieves its optimal performance and how does this optimal performance compare to the performance of the clique tree propagation approach are topics for future re search  To end this section  let us look at an example  Con sider computing Pnetl   d  e   in the Bayesian net net   shown in Figure    Since net  has NMC separators  the procedure Serial Reduction will be called  The procedure will first decompose net   into components  semi Bayesian nets  netS  net   net    and net      Figure     It will then arrange those components into a component tree tree   which has two leaves netS and net    If the procedure Pick leaf first returns net    then the induced query Pnem e a g  will first be computed  The answer will be appended to the neighbor net   of netll in tree   resulting in the parametric semi Bayesian net net      Figure    with auxiliary variable v  and parameter e  The answer will also be used to update the query Pnetl   d  e   to the new query Pnetu d  v        e     where net    stands for the union of netS  net   and net     After net   is removed from tree   there are again two leaves netS and net tO   The current query Pnetl   d  VJ       e  induces the query Pnetl o a b vl     e  in net    I net    is chosen by Pidc leafthis time  the induced query will be com puted  Its answer will be appended to net   resulting   Sidestepping the Triangulation Problem in Bayesian Net Computations  in net     Figure     The answer will also be used to update the current query PMn  d  v       e  to the new query Pnen   d v       e   where net   stands for the union of netS and net     If Pick  leaf now picks net      then the query Pnet     c  h  v        e  induced in net   by the cur rent query Pnen  d  v        e  will be computed  Its answer will be appended to netS  resulting in netS   And the answer will also to used to update the cur rent query Pne    d  v        e  to the new query Pnets d  V       e   On the next level  PnetlO   a  b  v        e  and Pnets d  V        e  will be computed by the pra  cedure ParallelReductionl since the parametric semi Bayesian nets net     and netS  do not have any NMC separators  On the other hand  net   andnet   do have NMC separators  So  the procedure Serial Reduction will again be called in computing both Pnetu a g e  and Pnet  c h v       e   We notice that there was no triangulation in the above process of computing Pnett d e   while triangulation is a must in the clique tree propagation approach  see section        CONCLUSIONS  In this paper  we have described a approach for com puting posterior probabilities in a Bayesian net  which sidesteps the triangulation problem  Our approach begin by decomposing the Bayesian net into smaller components by making use of Tarjan s algorithm for decomposing undirected graphs at all their minimal complete separators  Like the clique tree approach  our approach arranges those components into a tree  and then computes posterior probabilities by appropri ately passing information around in that tree  Unlike the clique tree approach  the computation in each com ponent is not carried out by direct marginalization  Rather it is carried by repeating the whole procedure from the beginning  Thus  the need for triangulation is avoided  How does the performance of our approach compare to that of the clique tree propagation approach based on an optimal triangulation  This question is yet to be answered  Our hope is that proper choices of the pro  cedure Pick leaf  and Pick laden node c ou l d ensure the performance of our approach lay close to optimal   Acknowledgement The first author gained his understanding of Bayesian nets when he was with Glenn Shafer and Prakash Shenoy at Business School  University of Kansas from October      to October      and from September      to August       The paper has benefited from comments by D Ambrosio and the reviewers for UAI     Research is supported by NSERC Grant OG   P          
 We present a technique for speeding up the convergence of value iteration for par tially observable Markov decisions processes  POMDPs   The underlying idea is similar to that behind modified policy iteration for fully observable Markov decision processes  MDPs   The technique can be easily incor porated into any existing POMDP value it eration algorithms  Experiments have been conducted on several test problems with one POMDP value iteration algorithm called in cremental pruning  We find that the tech nique can make incremental pruning run sev eral orders of magnitude faster      INTRODUCTION  POMDPs are a model for sequential decision making problems where effects of actions are nondeterministic and the state of the world is not known with certainty  They have attracted many researchers in Operations Research and AI because of their potential applica tions in a wide range of areas  Monahan       Cas sandra     b   However  there is still a significant gap between this potential and actual applications  primar ily due to the lack of effective solution methods  For this reason  much recent effort has been devoted to finding efficient algorithms for POMDPs  This paper is concerned with only exact algorithms  Most exact algorithms are value iteration algorithms  They begin with an initial value function and improve it iteratively until the Bellman residual falls below a predetermined threshold  See Cassandra      a  for excellent descriptions  analyses  and empirical compar isons of those algorithms  There are also policy iteration algorithms for POMDPs  The first one is proposed by Sondik          A simpler one is recently developed by Hansen         It is known that  in terms of number of iterations  pol icy iteration for MDP converges quadratically while value iteration converges linearly  e g  Puterman       page       Hansen has empirically shown that his pol icy iteration algorithm for POMDPs also converges much faster than one of the most efficient known value iteration algorithms  namely incremental prun ing  Zhang and Liu       Cassandra et a         Policy iteration for MDPs solves a system of linear equations at each iteration  The numbers of unknowns and equations in the system are the same as the size of the state space  Consequentially  it is computationally prohibitive to solve the system when the state space is large  Modified policy iteration  MPI   Puterman       page      alleviates the problem using a method that computes an approximate solution without actu ally solving the system  Numerical results reported in Puterman and Shin        suggest that modified pol icy iteration is more efficient than either value iteration or policy iteration in practice  Hansen        points out that the idea of MPI can also be incorporated into his POMDP policy iteration algorithm and finds that such an exercise is not very helpful  Hansen        The paper describes another way to apply the MPI idea to POMDPs  Our method is based on the view that MPI is also a variant of value iteration  van Nunen          Under this view  the basic idea is to  improve  the current value function for several steps using the current policy before feeding it to the next step of value iteration  Those improvement steps are less expensive than standard value iteration steps  Nonetheless  they do get the current value function closer to the optimal value function  MPI for MDPs improves a value function at all states    As a matter of fact  it was first proposed of value iteration by van Nunen   as  a variant   Speeding Up POMDP Value Iteration  This cannot be done for POMDPs since there are infi nite many belief states  Our method improves a value function at a finite number of selected belief states  A nice property of POMDPs is that when a value func tion is improved at one belief state  it is also improved in the neighborhood of that belief state  We call our method point based improvement for the lack of a better name  It is conceptually much simpler than Hensen s policy iteration algorithm  Nonetheless  it is as effective as  in some cases more effective than  Hansen s algorithm in reducing the number of itera tions it takes to find a policy of desired quality and hence drastically speeds up incremental pruning          POMDPs  A partially observable Markov decision process  POMDP  is a sequential decision model for an agent who acts in a stochastic environment with only partial knowledge about the state of the environment  The set of possible states of the environment is referred to as the state space and is denoted by S  At each point in time  the environment is in one of the possible states  The agent does not directly observe the state  Rather  it receives an observation about it  We denote the set of all possible observations by Z  After receiving the observation  the agent chooses an action from a set A of possible actions and execute that action  There after  the agent receives an immediate reward and the environment evolves stochastically into a next state  Mathematically  a POMDP is specified by the three sets S  Z  and A  a reward function r s a   a transi tion probability P  s  j s  a   and an observation probabil ity P zjs   a   The reward function characterizes the dependency of the immediate reward on the current state s and the current action a  The transition prob ability characterizes the dependency of the next state   s on the current state s and the current action a  The observation probability characterizes the dependency of the observation z at the next time point on the next state s  and the current action a       contained in the current observation  previous obser vations  and previous actions can be summarized by a probability distribution over the state space  The probability distribution is sometimes called a belief state and denoted by b  For any possible states  b s  is the probability that the current state is s  The set of all possible belief states is called the belief space  We denote it by B  A policy prescribes an action for each possible belief state  In other words  it is a mapping from B to A  Associated with policy  r is its value function V   For each belief state b  V   b  is the expected total dis counted reward that the agent receives by following the policy starting from b  i e   POMDP AND VALUE ITERATION  We begin with a brief review of POMDPs and value iteration   Policies and value functions  Since the current observation does not fully reveal the identity of the current state  the agent needs to con sider all previous observations and actions when choos ing an action  Information about the current state           V  b      E  b  L A rt  t O  where r  is the reward received at timet and A      A      is the discount factor  It is known that there   exists a policy  r  such that V   b   V  b  for any other policy  r and any belief state b  Such a policy is called an optimal policy  The value function of an optimal policy is called the optimal value function  We denote it by v  For any positive number E  a policy  r is E optimal if V  b    f   V  b       Vb  E B   Value iteration  To explain value iteration  we need to consider how belief state evolves over time  Let b be the current belief state  The belief state at the next point in time depends not only on the current belief state  but also on the current action a and the next observation z  We denote it by b  For any states   b s   is given by  baz          E  P s   zjs  a b s  P zjb  a          and where P z  s js  a  P zjs   a P s js a  P zjb  a   Es s  P z  s js  a b s  is the renormalization constant  As the notation suggests  the constant can also be interpreted as the probability of observing z after taking action a in belief state b  Define an operator T that takes a value function V and returns another value function TV as follows  TV b    maxa r b a   A  L P zjb  a V b  Vb E B       z  where r b  a    E  r s  a b s  is the expected imme diate reward for taking action a in belief state b  For        Zhang  Lee  and Zhang  a given value function V  a policy improving if  r b      arg maxa r b  a   A       is said to be V   L P zlb  a V b    VI      Vo         n          do      n    n      Vn    DP UPDATE Vn             while maxbiVn b   Vn   b l                    return Vn        z  for all belief states b  Value iteration is an algorithm for finding   optimal policies  It starts with an initial value function Vo and iterates using the following formula  Vn     TVn    It is known  e g  Puterman       Theorem      that Vn converges to V  as n goes to infinity  Value iteration terminates when the Bellman residual ma xb IVn b   Vn   b l falls below               When it does  a Vn improving policy is   optimal  Since there are infinite many possible belief states  value iteration cannot be carried explicitly  Fortu nately  it can be carried out implicitly  Before explain ing how  we first introduce several technical concepts and notations       Technical and notational considerations  For convenience  we call functions over the state space vectors  We use lower case Greek letters a and    to refer to vectors and script letters V and U to refer to sets of vectors  In contrast  the upper letters V and U always refer to value functions  i e  functions over the belief space B  Note that a belief state is a function over the state space and hence can be viewed as a vector  A set V of vectors induces a value function as follows  f b      maxaEva b  Vb  E  B   where a b is the inner product of a and b  i e  a b  Ls a s b s   For convenience  we also use V    to denote the value function defined above  For any belief state b  V b  stands for the quantity given at the right hand side of the above formula  A vector in a set is extraneous if its removal does not change the function that the set induces  It is useful otherwise  A set of vector is parsimonious if it contains no extraneous vectors       Implicit value iteration  A value function V is represented by a set of vectors if it equals the value function induced by the set  When a value function is representable by a finite set of vec tors  there is a unique parsimonious set of vectors that represents the function   Figure    Value iteration for POMDPs  Sondik        has shown that if a value function Vis representable by a finite set of vectors  then so is the value function TV  The process of obtaining a par simonious representation for TV from a parsimonious representation of Vis usually referred to as dynamic programming update  Let V be the parsimonious set that represents V  For convenience  we sometimes use TV to denote the parsimonious set of vectors that rep resents TV  In practice  value iteration for POMDPs is implicitly  carried in the way as shown in F igure    One be gins with a value function Vo that is representable by a finite set of vectors  In this paper  we assume the initial value function is    At each iteration  one per forma dynamic programming update on the parsimo nious set Vn   of vectors that represents the previous value function Vn   and obtains a parsimonious set of vectors Vn that represents the current value nmc tion Vn  One continues until the Bellman residual maxbJVn b   Vn    b l  which is determined by solving a sequence of linear programs  falls below a threshold     PROPERTIES OF VALUE ITERATION  This paper presents a technique for speeding up the convergence of value iteration  The technique is de signed for POMDPs with nonnegative rewards  i e  POMDPs such that r s  a   O for all s and a  In this section  we study the properties of value iteration in such POMDPs and show how a POMDP with negative rewards can be transformed into one with nonnegative rewards that is in some sense equivalent  Most proofs are omitted due to space limit  We begin with a few definitions  In a POMDP  a value function U dominates another V if U b   V b  for all belief states b  It strictly dominates V if it dominates V and U b  V b  for at least one belief state b  A value function is  strictly  suboptimal if it is  strictly  dominated by the optimal value function  A set of vectors is  strictly  dominated by a value func tion if its induced value function is  A set of vectors   Speeding Up POMDP Value Iteration  is  strictly  suboptimal if it is  strictly  dominated by the optimal value function   VI       Vo         n          do      n    n      Un    DP UPDATE Vn di     Sf  maxb Un b   Vn I b       if  s   t                   Vn f  improve Un i         while  S   e                   Return Vn  A set of vectors is  strictly  dominated by another set of vectors if it is  strictly  dominated by the value func tion induced by the that set    I  General properties of value iteration  In any POMDP  if a set of vectors V is suboptimal  then so is TV  Moreover  if V dominates another set of vectors V   then TV dominates TV    Lemma I  In any POMDP  if a set of vectors V is strictly suboptimal  then there exist at least one belief state b such that TV b  V b    Figure    A new variant of value iteration   Lemma         Unless explicitly stated otherwise  all POMDPs con sidered from now on are with nonnegative rewards   Properties of value iteration in POMOPs with nonnegative rewards     Using Lemma    one can show  Consider running VI on a POMDP with nonnegative rewards  Let Vn   and Vn be respectively the sets of vectors produced at the n lth and nth iter ation  Then  Vn   is strictly dominated by Vn  which in tum is dominated by the optimal value function   Theorem I  Note that the theorem falls short of saying that  when the reward function is nonnegative  TV strictly domi nates V for any suboptimal set of vectors V  As a mat ter of fact  this is not always the case  As a counter ex ample  assume r s  a  O for a certain state so regard less of the action  Let b  be the belief state that is   at so and   everywhere else  Further assume V   bo O and let n  be a vector such that no so  V  bo  and n    s     for any other states s  It is easy to see that if V consists of only no  then TV bo  V bo   Despite of the fact TV does not always strictly dom inate V  TV b  is strictly larger than V b  for beliefs b in most parts of the belief space when the reward function is nonnegative            POMDPs with negative rewards  A POMDP with negative rewards can always be trans formed into one with nonnegative rewards by adding a large enough constant to the reward function  It is easy to see that an t optimal policy for the transformed POMDP is also t optimal for the original POMDP and vice versa  Moreover  the value function in the original POMDP of a policy equals that in the trans formed POMDP minus C          where Cis the con stant added  In other words  the original POMDP is solved if the transformed POMDP is solved  There fore  we can restrict to POMDPs with nonnegative rewards without losing generality   S PEEDING UP VALUE ITERATION  The section develops our technique for speeding up value iteration in POMDPs with nonnegative rewards  We begin with the basic idea    I  Point based improvement  Consider a suboptimal set of vectors V  By improving V  we mean to find another suboptimal set of vectors that strictly dominates V  By improving V at a belief state b  we mean to find another suboptimal set of vectors U such that U b  V b   Value iteration starts with the singleton set      which is of course suboptimal  and improves the set itera tively using dynamic programming update  Theorem     Dynamic programming is quite expensive  espe cially when performed on large sets of vectors  To speed up value iteration  we devise a very inexpensive technique called point based improvement for improv ing a set of vectors and use it multiple times in be tween dynamic programming updates  This technique can be incorporated into value iteration as shown in Figure    Applications of the technique are encapsu lated in the subroutine improve  The Bellman residual li is used in improve to determine how many times the technique is to be used   If  for any suboptimal set of vectors U  the output of improve U  li    another set of vectors   is suboptimal and dominates U  then VIl termi nates in a finite number of steps   Theorem    Let Vn and V be respectively the sets of vec tors produced at the nth iteration of VIl and VI  From Lemma   and the condition imposed on improve  we conclude that Vn is suboptimal and dominates V   Proof         Zhang  Lee  and Zhang  Since V monotonically increases with n  Theorem    and converges to V  as n goes to infinity  Vn must also converge to V   The theorem follows  D      R a U     bEBia b a  b  la EU  a    R a U     bEBia ba  b Va EU  a     Improving a set of vectors at one belief state  For the rest of this section  we let V be a fixed sub optimal set of vectors and let U TV  We develop a method for improving U  To begin with  consider how U might be improved at a particular belief state b  According to      there exists an action a such that  U b    r b a    A  L P zlb a V b         z  For each observation z  let f z be a vector in U that has maximum inner product with b  Define a new vector by  f  s    r s a    A  L P z s  ls a f z s   z s     Vs E S       We sometimes denote this vector by backup b a  U   Theorem    For the vector f  given by      we have  b f    r b a    A  L P zlb a U b         z  As pointed out at the end of Section      U b  is of ten larger than V b  A quick comparison of     and     leads us to conclude that b f  is often larger than U b   When this is the case  we have found a set that improves U at b  namely the singleton set       The set is obviously suboptimal     There is an obvious variation to the idea presented above  Instead of using the vector backup b  a  U  for the action that satisfies      we can consider the vectors backup b  a  U  for all possible actions a  and choose the one whose inner product with b is the maximum  This should  hopefully  improve U at b even further  We tried this variation and found that the costs are almost always greater than the benefits       of the belief space B respectively given by  Improving a set of vectors at multiple belief states  It is straightforward to generalize the idea of the previ ous subsection from one belief state to multiple belief states  The question is what belief states to use  There are many possible answers  Our answer is motivated by the properties of dynamic programming update  For any vector ainU  define its witness region R a U  and closed witness region R a U  w r t U to be regions  During dynamic programming update  each vector a in U is associated with a belief state that is in the closed witness region of a  We say that the belief state is the anchoring point provided for a by dynamic programming update and denote it by point a   The vector is also associated with an action  which we de note by action a   It is the action prescribed for the belief state point a  by a V    improving policy  Be cause of those  equation     is true if b is point a  and a is action a   We choose to improve U on the anchoring points using  U       backup point a  action a  U Ia E U         According to the discussions of the previous subsec tion  the value function U      is often larger than U    at the anchoring points  When a value function is larger than another one at one belief state  it is also larger in the neighborhood of the belief state  There fore  the value function U      is often larger than U    in regions around the anchoring points  Our experi ence reveal that it is often larger in most parts of the belief space  The explanation is that the anchoring points scatter  evenly  over the belief space w r t U in the sense that there is one in the closed witness region of each vector of U  There is one optimization issue  Even that the inner product of the vector backup point a   action a  U  with the belief state point a  is often larger than that of a with the belief state  it might be smaller some times  When this is the case  we use a instead of backup point a   action a  U  so that the value at the belief state is as large as possible       Relation to modified policy iteration for MDPs  Point based improvement is closely related to MPI for MDPs  Puterman       page       It can be shown that  for each anchoring point b  ul  b      r b  r b     A  L P zlb  r b  U b b          z  where  r is a V    improving policy  This formula is very similar to formula        of Puterman         MDP modified policy iteration uses the latter formula to  improve  the value of each possible state of the state space  We cannot apply the above formula to all possible belief states since there are infinite many of   Speedi ng Up POMOP Value Iteration  improve U      Uo     U  k t         do      k     k     uk        w           for each vector a in Uk   a     backup point a  action a  Uk   U W         if a  point a   a   point a     a     a   else W     WU  a       point a   t   point a          action a      action a       uk    uk u  a         while stop Uk Uk d  false      return uk u u  Figure    The improve subroutine  them  So  we choose to use the formula to improve the values of a finite number of belief states  namely the anchoring points       Repeated improvements  We now know how we might improve U at the anchor ing points  In hope to get as much improvement as possible  we want  of course  to apply the technique on U  and try to improve it further  This can easily be done  Observe that there is a one to one correspon dence between vectors in U and U   for each vector a in U  we have backup point a  action a  U  in U   We associate the latter with the same belief state and action as the former  Then we can improve U  at the anchoring points the same way as we improve U  The process can of course be repeated for the resulting set of vectors and so on  The above discussions lead to the routine shown in Fig ure    The routine improves the input vector set at the anchoring points iteratively  Improvement takes place at line    Lines   and   guarantee that the values of the anchoring point never decrease  The improved vector a  is added to W at line   so that better improve   ments can be achieved for vectors yet to be processed  At lines   and     the belief state and action associated with a vector of the previous iteration are assigned to the corresponding vector at the current iteration  The stopping criterion we use is  where E  is a positive number smaller than    In our experiments  E  is set at      Compared with the stop ping criterion of value iteration  the stopping criterion is stricter  The reason for this is that the improvement step is computationally cheap        Finally  the union UkUU is returned instead of Uk for the following reason  While Uk b  is no smaller than U b  at the anchoring points  it might be smaller at some other belief states  In other words  Uk might not dominate v  If improve simply returns uk  the conditions of Theorem   are not met  Consequently  the union ukuu is returned       Pruning extraneous vectors  The union UkUU usually contains many extraneous vectors  They should be pruned to avoid unnecessary computations in the future  One way to doing so is to simply apply Lark s algorithm  White          Lark s algorithm solves a linear program for each input vector  It is expensive when there is a large number of vectors  We use a more efficient method  The moti vation lies in two observations  First  most vectors in Uk are not extraneous  Second  many vectors in U are componentwise dominated by vectors in uk and hence are extraneous  The method is to replace line    with the following lines      Prune from U vectors that are componentwise dominated by vectors in uk      Prunes from U vectors a such that R a UkUU  is empty      return ukuu  At line     a linear program is solved for each vector in U  Since no linear programs are solved for vectors in Uk and the set U usually becomes very small in cardinality after line     the method is much more efficient than simply applying Lark s algorithm to the union UkUU        Recursive calls to  improve  Consider the set U after line    of the algorithm seg ment given in the previous subsection  If it is not empty  then every vector a in the set is useful  This is determined by solving a linear program  In addition to determining the usefulness of a  solving the linear program also produces a belief state b that is in the closed witness region R a UkUU   The facts that a is from the input set U and that b is in R a UkUU  imply that the value at b has not been improved  To achieve as much improvement as possible  we improve the value by a recursive call to improve  To be more specific  we reset point a  to b at line    and replace line    with the following      if U  f    return improve UkUU         else return uk            Zhang  Lee  and Zhang  EMPIRICAL RESULTS  ng    Experiments have been conducted to empirically de termine the effectiveness of point based improvement in speeding up value iteration and to compare it im provement with Hansen s policy iteration algorithm  Four problems were used in the experiments for both purposes  The problems are commonly referred to as Tiger  Network  Shuttle  and Aircraft ID in the litera ture and were downloaded from Cassandra s POMDP page   Information about their sizes is summarized in the following table   IISIIIZIIIAII Tiger Network Shuttle Aircraft ID                         VI     Vl                                                       c                           The effectiveness of point based improvement is deter mined by comparing VI and VI   We borrowed an implementation of VI by Cassandra and VIl was im plemented on top of his program  Cassandra s pro gram provides a number of algorithms for dynamic programming update  For our experiments  we used a variation of incremental pruning called restricted re gion  Cassandra et a        The discount factor was set at      and experiments were conducted on an UJ traSparc II  For the purpose of comparison  we collected informa tion about the quality of the policies that VI and VIl produce as a function of the times they take  The quality of a policy is measured by the distance be tween its value function to the optimal value function  i e  the minimum  such that the policy is  optimal  The smaller the distance  the better the policy  Since we do not know the optimal value function  the dis tance cannot be exactly computed  We use an upper bound derived from the Bellman residual  One experiment was conducted for each algorithm problem combination  The experiment was terminated when either an      optimal policy was produced or the run time exceeded    hours  i e        seconds  CPU time                            CPU time In seconds Nelworl   i     f              Effectiveness of point based improvement                         V                         O o           CPU time in seconds                             CPU time In seconds                                                                 Aircraft IO              i              VI     VII                               e    O o                          CPU time In seconds         Figure    Empirical results  See text for explanations  We see that VIl was able to produce a      optimal policy for all four problems in a few iterations  On the other hand  VI took     iterations to produce a     optimal policy for Network  Within the time limit  VI completed only         and    iterations respectively for Tiger  Shuttle  and Aircraft ID  Those suggest that the technique proposed in this paper is very effective in reducing the number of iterations that is required to produce good policies   It is also clear that VIl is much faster than VI  For Network  VI took about        seconds to produce a      optimal policy  while VIl took only about     seconds  The speedup is about    times  VI was not able to produce  good  policies for Tiger  Shuttle  and Aircraft ID within the time limit  while VIl produced        optimal or better policies for them in            http   vvw cs brovn edu research ai pomdp index html and        seconds respectively   The data are summarized in the four charts in Figure    Note that both axes are in logarithmic scale  There is one chart for each problem  In each chart  there are two curves  one for VI and one for VI    On each curve  there is data point for each iteration taken    Speeding Up POMDP Value Iteration       Comparisons with Hansen s policy iteration algorithm  In his implementation  Hansen used standard in cremental pruning  instead of restricted region  for dynamic programming update  Moreover  while the round off threshold is set at w   in Cassandra s pro gram  Hansen set it at w   probably because the rou tines for solving linear equations cannot handle pre cision beyond w   For fairness of comparison  we implemented VI  on top Hansen s code  The following table shows the numbers of iterations and amounts of time VI  and Hansen s algorithm took to find      optimal policies  We see that VI  took fewer iterations than Hansen s algorithms on all prob lems  It took less on the first two problems and took roughly the same time on the last two problems   Tiger Network Shuttle Aircraft ID     VI   I Hansen                       Time VI  I Hansen                                        CONCLUS IONS  We have developed a technique  namely point based improvement  for speeding up the convergence of value iteration for POMDPs  The underlying idea is simi lar to that behind modified policy iteration for MDPs  The technique can easily be incorporated into any ex isting POMDP value iteration algorithms  Experiments have been conducted on several test prob lems  We found that the technique is very effective in reducing the number of iterations that is required to obtain policies with desired quality  Because of this  it greatly speeds up value iteration  In our experiments  orders of magnitude speedups were observed  Acknowledgement  Research is supported by Hong Kong Research Grants Council Grant HKUST        E  The authors thank Cassandra and Hansen s for sharing with us their pro grams and the anonymous reviewers for useful com ments  
  policy        The value function of an optimal policy is  usually referred to as the  denoted by v   P lanning problems where effects of actions  optimal value function  and  are non deterministic can be modeled a   MDPs have been studied extensively in the dynamic  Markov decision processes   programming literature  e g   Planning prob  lems are usually goal directed   man  This paper         BertsekaB               Puter        Dean and Wellman        ini  Howard  White  proposes several techniques for exploiting the goal directedness to accelerate value itera  Kanazawa        and Dean and tiated the use of MDPs in planning problems where  tion  a standard algorithm for solving Markov  effects of actions are not deterministic  Planning prob  decision processes   Empirical studies have  lems typically have a large number of states  Solving  shown that the techniques can bring about  MDPs with large state space has hence become a hot  significant speedups   topic in AI  e g  Dean  et al       Boutillier  e t al        A planning problem can be modeled as an MDP in such way that     there is a state designated to be the  Keywords  decision theoretic planning  Markov de cision processes  value iteration  efficiency      goal         r sa    INTRODUCTION  ing an actiona ha  two consequences  The agent re  ceives an immediate reward r  s a   which depends on the current state   of the world a  well a  the action executed  and the world probabilistically moves into  transition probability  P s ls a   The action is chosen based on the current state of the world  A  poli cy   r  prescribes an action for each possi  ble state  In other words  it is a mapping from the set  S of all possible states to A  The set of possible states is assumed to be finite in this paper  The quality of  a policy  r is mea ured by its     for any states  reward V         V   s      and     the  In a Markov de cision process  MDP   an agent must  at each time point  choose an action from a finite set A of possible actions and execute the action  Execut  another state s  according to a  and an action called  value function V    s       if     otherwise    r   re  optimalif v r  s  V    s  for any states and any other  ceives starting from an initial state s  A policy rr is  the  a delcare goal and s goal   action declare goal cannot       be executed  more than once  We call MDPs with such properties  goal directed MDPs  Value iteration is a standard algorithm for solving MDPs  This paper proposes several techniques for ac celerating value iteration in goal directed MDPs  Let us begin with a brief review of value iteration and of previous works     on  speeding up value iteration   VALUE ITERATION  A value function is a mapping from the set S of pos sible states to the real line  Given a value function V  define another value function TV by  TV s   max  r s a     Y  L P s ls a V s          s   is the expected total discounted  the agent  under the guidance of  declare goal       reward function r      a  is given by  for each state  s   where          is a discount factor   T is an mapping from the space of value functions to itself  For any function V  its norm  lVII is defined        Zhang and Zhang  IIVII   max  V s l  Tis the contraction mapping Puterman       in the sense that for any two value functions U and V   by   e g   IITU  TVII   YIIU  VII     subtracts an appropriate value function from Vn and MacQueen        proposes to subtract V   s     the value of  by Schweitzer tanon  For any positive number   we say that a value function  V  is  Vn          et al         and Bertsekas and Cas  interleave standard VI steps with aggre  gation disaggregation steps  which improve the cur rent value function by solving the optimality equation  f  contracted if  for an simpler MDP obtained from the original MDP  through state aggregation  Dean and Lin The optimal value function satisfies the  tion  optimal equa          and  decompose an MDP with a large  state space into a number of MDPs with smaller state  used to construct a solution to the original MDP  Three pieces of previous workd are of direct relevance  induces a policy through   r s    arg maxa r s  a   Y  L  P s ls   a V s           s   IT the value function is f  contracted for a small number    et al         are solved using standard VI and their solutions are  and hence is   contracted   V  Dean  spaces through state aggregation  The smaller MDPs  V    TV    A value function  so   itself at a predetermined state  The aggregation disaggregation techniques introduced  the induced policy is  good enough  in the sense  that  to this paper  The first one is the Gauss Seidel variant of standard VI proposed by Hastings           Let p be  an ordering among the possible states  Instead of the operator  T  defined in equation  variant uses another operator       T  to  the Gauss Seidel improve the cur  rent value function  For any value function V   T V s   is defined for each state s by starting from the state  IIV      V ll          y         Proof of this inequality can be found in  for instance  Puterman         It is evident the policy induced by  that comes first in the ordering p and moving back wards  The values  T V s   for earlier states are used  in defining the values for later states  Specifically   T   is given by  the optimal value function is optimal  Value iteration  VI   Bellman         starts with an ar  T V s    maxa r s a     LP s ls a V  s      bitrary value function and improves it iteratively until          the value function becomes t  contracted  Here is the pseudo code  VI      Choose an initial value function set n O      Vn l  TVn     If IIVn l  Vnii E  go to step       Else return  V   where V s   T V s   when s  comes before ordering p and V s   V s   otherwise   and  The anytime algorithm presented in Dean  in the  et al         is also closely related to the methods to be proposed in increment n by     this paper  The algorithm restricts standard VI inside  and  an envelope  a subset of possible states  that contains at least one path from the initial state to the goal state   Vn l  The envelope is gradually enlarged to get better and  T is a contraction mapping  IITVnH   Vn l II   IITVn l   TVn ll    IIVnH  Vnl l    Hence  Vn l is  Since   contracted      s  better solutions  Boyan and Moore          study value iteration in  acyclic goal directed MDPs  A goal directed MDP is acyclic if once leaving a  state  the world can never come back to that state again   PREVIOUS WORK  Boyan and Moore  point out that value iteration for goal directed acyclic  VI converges geometrically at rate  Y  MDPs can be carried out in one sweep by starting from Convergence  the goal state and working backwards  Thereby the  Various modifica  tions to standard VI have been proposed and all have  amount of computations needed to compute the opti  been theoretically or empirically shown to lead to  iteration of standard VI  The method is an extension  is slow when    is close to     faster convergence   Morton and Wecker         T in  gest that one  before applying the operator  sug step  mal value function is reduced to that needed in one of the DAG SHORTEST PATH algorithm  Carmen  al      for finding  shortest path in acyclic graphs   et   Value Iteration for Goal Directed MOPs          PARSIMONIOUS VALUE  There is no guarantee that the value function returned  ITERATION  by PVI is  contracted  However  the value function should be close to be  contracted  We suggest to use  We introduce several new variants to standard VI for goal directed MDPs  Called  tion  PVI    parsimonious value itera  the first variant relies on the following in  PVI as a preprocessing step to VI  i e   to use the  value function it returns as the initial value function of VI  This way an  contracted value function can  tuition  Suppose value iteration begins with the zero  be obtained  Since the value function return by PVI is  value function   numer of steps  In our experiments  it t erminat ed in  Then at early iterations  the value  function remains zero for states far away from the goal  At later iterations  the value function does not change much for states close to the goal  The number of states whose values change significantly from one iteration to the next can be much smaller than the total number of states   At each iteration  PVI updates the value  for a state only when the value is expected to change significantly   iteration n    n     PVI performs a test to  detect states whose values change substantially from iteration n to iteration  n     The value of a state is  updated only if it passes the test  Let  Vn    and  Vn be the value functions PVI computed  at the previous two iterations   At the current itera  s if IVn       Vn        I      for a small positive con stant   and each state    such that maxaP    j s  a  O  Since the number of states reachable from s by execut  ing one action is usually small  this test is cheap  It is usually much cheaper than calculating  TVn s    es  pecially when one maintains a list of nodes reachable from each state by executing one action   Theoretical underpinings of the test are as follows  If  the value functions  Vn  and  Vn    were the value func  tions computed by VI  one could easily show that if  IVn l  s    Vn    I    yo  In  s passes the test then  words  the value for n     other  does not change much from it  to iteration n      Here is the pseudo code for PVI   PVI    Vo s  O for  any    n O      For each states   a   The idea behind PVI is rather similar to the idea underlying Boyan and Moore s one  sweep algorithm  start from the goal and work backwards  PVI does not  assume acyclicity and hence is more general  W hen the MDP is acyclic  it is almost identical to the one  sweep  Ifn  and  Vn     Vn         Hor all    such that maxaP s ls  a  O  Vn l s    V   s    et al          in the sense that values are updated only  for some states at each iteration   The difference lies  in the fact that in PVI the states whose values are  the anytime algorithm whether the value for a state  is updated depends on whether it is in the envelop and does not change with iteration   Also the entire  value iteration process needs to be carried out for each envelop      GREEDY AND DOUBLE VALUE ITERATION  Even though the test in PVI is cheap  the fact that it has to be carried out for each state  is  somewhat  unsatisfying  Greedy value iteration  GVI  avoids the test by working  in a way similar to DAG SHORTEST  PATH  Before describing GVI  we need to introduce t he con  s  is ideally reachable in one step fr om another state s if after exe  cuting a certain action in state s the probability of the world ending up in state    is the highest  A state se is ideally reachable in k steps from another state s  if there are states       s    such that si l is ideally  cept of ideal reachability  We say a state  reachable from Bi in one step for all Oi    k    Any     state is ideally reachable from itself in   step  For any state s  let d s  be the minimum number of steps in which the goal is ideally reachable from   b  Else  shall refer to d  s  as the  Vn l s    TVn s      If IIVn l   Vn II   inc re ment n by   and go to step        Else return  PVI is also related to the anytime algorithm by Dean  updated change from iteration to iteration  while in  tion n     PVI does not update the value for a state  eration  just one iteration   algorithm   Specifically  PVI begins with the zero value function  At each  close to be co contracted  VI should terminate in a small  Vn H  distancefrom s      We  to the goal   At each iteration n  GVI only updates the values for the states from which the goal is ideally reachable in n steps  Let N be the maximum number of steps that the goal can be ideally reached from any state  Then GVI terminates in exactly N iterations  For later con         Zhang and Zhang  venience  we assume that GVI takes a value function  where  a s input and uses it a s the initial value function  Here    V s s    A  is the psuedo code for GVI      For n O  ping  toN   if d s      otherwise      Return  Hence the value function returned by DVI is  It is evident see that DVI is almost identical to the Gauss Seidel variant of standard VI  except that it pro  If d s   n  set Vn I s         TVn s    poses one particular way to order the possible states  the states are ordered according to their distances to   b  Else  the goal  By introducing DVI through GVI  we hope to provide another way of looking at the Gauss Seidel variant of standard VI in the context of goal directed MOPs   VN   When the MDP is acyclic  GVI is identical to Boyan and Moore s one sweep algorithm and hence returns the optimal value function  When the MDP is cyclic  however  the value function it returns could be of very poor quality  Using it a s a preprocessing step to VI might not help much      IMPROVING PVI  The alternative understanding of DVI can be used to improve PVI  We call the improved algorithm PVIl  The pseudo code is a s follows   PV l  On the positive side  the amount of computations GVI     Vo s  O for any s  n O   does is identical to that carried out by one iteration of     For  standard VI  Also because GVI is an approximation of the entire value iteration process  the extent to which it improves the input value function should be greater than that brought about by one iteration of standard  m O  toN    a  For each state   b   VI  Thus we can expect VI to converge faster if the  s such that d s  m lfn  and IVn s    Vn l s  l  o for ails  such that maxa P s ls a  O  Vn l s    Vn s    second line is replaced by  Vn l   GVI Vn    This leads to new algorithm called double value iteration   c  Else   DVI     Vn l s  Choose an initial value function  n O     Vn l   GVI Vn     If IIVn t     Vn      e    and go to step       Else return Vn   V      IfiiV       V   li e   and  set  increment  n by  new operator T   instead of the operator T given in Equation      to update the value function Vn For  any value function V  T V s  is defined for each state   by starting with the goal state and gradually moving away  The value T V s  for a state s is defined after the values T V s   for all the states s  closer to the     having been defined  It is given by  T V s         max  r s  a          P s ls a V s   s     s      T Vn s    increment  n by   and  go to step     As it turns out  DVI can be described directly with out the reference to GVI  At each iteration  it uses a  goal than  d s     contracted   For each state s    a   DVI     V s    It can be proved that r is also a contract map  GVI VQ       TV s       Else return Vn l As PVI  PVIl should be used a s a preprocessing step to VI      EXPERIMENTS  Preliminary experiments have been carried to compare the algorithms proposed in this paper with standard value iteration  Four office environment navigation  problems borrowed from Ca ssandra et al        were used   The problems differ in corridor layout and the total number of states  There are two sets of transition probabilities  referred to a s standard and noisy tran sition probabilities respectively  Effects of actions are less certain under noisy transition probabilities than under standard transition probabilities         Value Iteration for GoalDirected MDPs                                           VI    DVV        wr        W             Dill   Wl                l  l  l  l            o e OA                              I                    vo  no  DO                         Ul             DO                              VI   D                   Fu           I    Il   OVI     Wr  PV  f              I                                 Figure    Convergence times of the algorithms in four navigation problems   Figure    Differences in perfor manc e among the rithms as problem size increases   algo  The threshold for the Bellman residual was set at       and the discount factor at       Figure   shows that convergence times of the algorithms in the four prob lems  The Xaxis repr es ents the sizes of the probl em   while theY axis represents convergence time in CPU seconds  Data were collected using a SPARC    The curves VI and DVI display the convergence times of VI and DVI respectively  while PVI and PVIl display the convergence times for the combinations of PVI and PVIl with VI   The convergence times are shown in Figure    We see that the differences in performance among the algo rithms become larger as the problem size increases  In the smallest problem PVIl converges about three times faster than VI  while in the largest problem it  Under both standard and noisy transition probabili ties  DVI and PVI converges much faster than VI and PVIl converges even faster  DVI converges slightly faster than PVI in the smallest problem but slower in all other problems  Performances of all algorithms are slightly worse under noisy transition probabilities than under standard transition probabilities  Their differ ences are also slightly larger   We propose several techniques for exploiting the goal directedness of planning problems to speed up value it eration for their MDP models  Empirical studies have shown that the techniques can bring a bo ut significant speedups   gain an idea about how the comparisons change with problem sizes  we made copies of one environment and glu e them together to form larger environmen ts    be modeled as partially obs ervable  To  converges six times faster      CONCLUSIONS AND FUTURE DIRECTIONS  MDPs world   know  assume  perfect observation  of  the  state  of  the  In many real world problems  one does not  Such problems can MDPs  POMDPs   POMDPs are much harder to solve than MDPs  We are currently investigating the possibility of applying the true state  of the  world         Zhang and Zhang  the ideas introduced in this paper to POMDPs  Aclcnowledgements  We thank Peter Dayan  Thomas L  Dean  and Michael Littman for pointers to references and thank Wenju Liu and D  Y  Yeung for useful discussions  Re search was supported by Hong Kong Research Coun cil under grants HKUST       E and Hong Kong University of Science and Technology under grant DAG      EG   RI   
  Most exact algorithms for general par tially observable Markov decision processes  POMDPs  use a form of dynamic program ming in which a piecewise linear and con vex representation of one value function is transformed into another  We examine vari ations of the  incremental pruning  method for solving this problem and compare them to earlier algorithms from theoretical and em pirical perspectives  We find that incremen tal pruning is presently the most efficient ex act method for solving POMDPs      Littman  Computer Science Dept  Brown University Providence  RI        INTRODUCTION  Partially observable Markov decision processes  POMDPs  model decision theoretic planning problems in which an agent must make a sequence of decisions to maximize its utility given uncertainty in the effects of its actions and its current state  Cassandra  Kael bling    Littman       White        At any moment in time  the agent is in one of a finite set of possible states S and must choose one of a finite set of possible actions A  After taking action a E A from state s E S  the agent receives immediate reward ra s  E and the agent s state becomes some states  with the probabil ity given by the transition function Pr s Js a  E         The agent is not aware of its current state  and in stead only knows its information state x  which is a probability distribution over possible states  x s  is the probability that the agent is in state s   After each transition  the agent makes an observation z of its current state from a finite set of possible obser vations Z  The function Pr zis  a  E       gives the probability that observation z will be made after the agent takes action a and makes a transition to state s   This results in a new information state x defined  by a xz  s        Pr  z j s    a   EsES Pr  s   ls   a x s    Pr zjx  a        where Pr zix a      L Pr zls   a  L Pr s Js  a x s    s ES  sES  Solving a POMDP means finding a policy  r that maps each information state into an action so that the expected sum of discounted rewards is maximized          is the discount rate  which controls how much future rewards count compared to near term re wards   There are many ways to approach this prob lem based on checking which information states can be reached  Washington       Hansen        search ing for good controllers  Platzman        and using dynamic programming  Smallwood   Sondik       Cheng       Monahan       Littman  Cassandra    Kaelbling        Most exact algorithms for general POMDPs use a form of dynamic programming in which a piecewise linear and convex representation of one value func tion is transformed into another  This includes algo rithms that solve POMDPs via value iteration  Sawaki   Ichikawa       Cassandra  Kaelbling    Littman        policy iteration  Sondik        accelerated value iteration  White   Scherer        structured representations  Boutilier   Poole        and ap proximation  Zhang   Liu        Because dynamic programming updates are critical to such a wide ar ray of POMDP algorithms  identifying fast algorithms is crucial  Several algorithms for dynamic programming updates  have been proposed  such as one pass  Sondik        exhaustive  Monahan        linear support  Cheng        and witness  Littman  Cassandra    Kaelbling        Cheng        gave experimental evidence that the linear support algorithm is more efficient than the   Incremental Pruning for POMDPS  states to value and is defined in terms of relatively simple transformations of other value functions   one pass algorithm  Littman  Cassandra and Kael bling        compared the exhaustive algorithm  the linear support algorithm  and the witness algorithm and found that  except for tiny problems with approx imately   observations or   states  which all three al gorithms could solve quickly  witness was the fastest and had a number of superior theoretical properties   The transformations preserve piecewise linearity and convexity  Smallwood   Sondik       Littman  Cas sandra    Kaelbling        This means that if the function V can be expressed as V x    maxaESX a for some finite set of     vectors S  which it can in most applications   then we can express vza  a     maxaES X   a  va x    maxat s X   a  and V  x    maxo ES  X  a for some finite sets of  S  vectors s  sa  and S   for all a E A  and z E Z   These sets have a unique representation of minimum size  Littman  Cas sandra    Kaelbling        and we assume that the symbols S  sa  and S  refer to the minimum size sets   Recently  Zhang and Liu        proposed a new method for dynamic programming updates in POMDPS called incremental pruning  In this paper  we analyze the basic algorithm and a novel variation and com pare them to the witness algorithm  We find that the incremental pruning based algorithms allow us to solve problems that could not be solved within reason able time limits using the witness algorithm      Here is a brief description of the set and vector nota tion we will be using  Vector comparisons are com ponentwise  a   a  if and only if for all s E S  a  s        a  s   Vector sums are also componentwise  Vector dot products are defined by a    I   o   s      s    In vector comparisons and dot products    is a vector of all zeros and   a vector of all ones  For all s E S  the vector e  is all zeros except e   s       The cross sum of two sets of vectors is AEB B    a    a E A     E B   this extends to collections of vector sets as well  Set subtraction is defined by A B    o  E Ala B    DP UPDATES  The fundamental idea of the dynamic programming  DP  update is to define a new value function V  in terms of a given value function V  Value functions are mappings from information states to expected dis counted total reward  In value iteration algorithms  V  incorporates one additional step of reward compared to V and in infinite horizon algorithms  V  represents an improved approximation that is closer to the opti mal value function   Using this notation  we described earlier as  The function V  maps information states to values and is defined by  V  x      E   z  sES  ra s x s     Y L Pr zjx a V x  zEZ         In words  Equation   says that the value for an infor mation state x is the value of the best action that can be taken from x of the expected immediate reward for that action plus the expected discounted value of the resulting information state  x  as defined in Equa tion     We can break up the value function V  defined in Equation   into simpler combinations of other value functions   V  x  va x   maxVa x        z  vza x        aEA     z  These definitions are somewhat novel and form an im portant step in the derivation of the incremental prun ing method  described in Section    Each va and vza function is a value function mapping information      S         purge  can  characterize the  S  sets   Usa   ffi S        aEA  sa saz     purge  zEZ purge  T a a z lo       E  S          where T   a  a  z  is the lSI vector given by  r a a z  s       ZI ra s     Y L o  s   P r z s  a  Pr s  j s   a          and purge   takes a set of vectors and reduces it to its unique minimum form  Equations   and   are easily justified by Equations   and   and basic properties of piecewise linear convex functions  Equation   comes from substituting Equation   into Equation    sim plifying  and using basic properties of piecewise linear convex functions  The focus of this paper is on efficient implementations for computing sa  Equation     Equations   and   can be implemented efficiently using an efficient im plementation of the purge function  described in the next section        Cassandra  Littman  and Zhang  PURGING SETS OF VECTORS     Given a set of lSI vectors A and a vector a  define  R a  A     xlx     O   x        xa   xa  a      E A  a     it is the set of information states for which vector      a is  the clear  winner   has the largest dot product  com pared to all the other vectors in A  The set  R a  A  is  called the witness region of vector a  because for any information state  maxa EAu a  x    x  in this set maxa EA  a   a     in a sense   x  x a  i  can testify that a is  needed to represent the piecewise linear convex func tion given by  AU a    Using the definition of  R   FF  w        while F i      do E F X DOMINATE c J  W    ifx  l      then F  F        else w  argmax  EF x  W  WU w     F  F    w        return  we can define  purge A      al aE  FILTER F    W    for each s in S   do w  argmax  EF e    P   wwu w   A  R a A  i   W       it is the set of vectors in A that have non empty wit  Figure    Lark s algorithm for purging a set of vectors   ness regions and is precisely the minimum size set for representing the piecewise linear convex function given by A  Littman  Cassandra    Kaelbling Figure              gives an implementation of purge F  given  a set of vectors  F  FILTER F   returns the vectors in  F  that have non empty witness regions  thereby  purg ing  or  filtering  or  pruning  out the unnecessary vectors  The algorithm is due to Lark  White Littman  Cassandra    Kaelbling                  analyze the  algorithm and describe the way that the argmax op erators need to be implemented for the analysis to hold  ties must be broken lexicographically    DoMINATE  a  A   procedure called in line  information state  x for     The  returns an  which a gives a larger dot prod  uct than any vector in A  or  l if no such  x  DOMINATE a  A    L  LP variables x s      objective  max     for each a  in A   a    do ADDCONSTRAINT L  x a         X a     AooCoNSTRAINT L  x          AooCoNSTRAINT L  x          iflNFEASIBLE L    then return   l  else  x     SOLVELP L      ifo O then return  x     else return  l        exists       that is  it returns an information state in the region  Figure  R a A    information state in a vector s witness region   It is implemented by solving a simple linear  program  illustrated in Figure The  FILTER      algorithm plays a crucial role in the in  cremental pruning method  so it deserves some addi tional explanation  The set  w  with vectors  R w  F    W  initially  empty  is filled  e   information states   The  while  loop starting on line    J E       USING PURGE IN DP  that have non empty witness regions  they are the  winners   Lines     find those  winning vectors at the     goes through the  DOMINATE is x E R   W   If there is not  we know R   F  is empty  since x E R   F  implies x E R   J  W  since W  F  If DOMINATE finds an x E R   W   we add the winning vector  not necessarily    at x to W and continue  Each iteration removes vectors  Linear programming approach to finding an  F one by one   For each   used to see if there is an  a vector from F  and when it is empty  every vector  FILTER procedure  it is trivial to compute s  sets from S and to compute S  from the sa  sets  Equations   and     Given the  the  A straightforward computation of the sa sets from the S sets  Equation    is also easy  and amounts to an exhaustive enumeration of all possible combinations of vectors followed by filtering the resulting sets  This algorithm is not efficient because the number of com binations of vectors grows exponentially in  IZI   This  can be a large number of vectors even when the sa  from F will have been classified as either a winner or  sets are relatively small  This approach to computing  not a winner   the     This assumes that A is a true set in that it contains no duplicate vectors   so   s sets was essentially proposed         under the name of  Sondik s one  sets from the  by Monahan  pass algorithm      Incremental Pruning       INCPRUNE Sg     Sgk    W    FILTER S   EB SgJ  COMPLEXITY ANALYSIS    We seek to express the running time of algorithms in terms of the number of linear programs they solve and the size of these linear programs  We choose this metric because all of the algorithms in this paper use linear programming as a fundamental subroutine  in the form of calls to DOMINATE  a  A   and the solu tion of these linear programs is by far the most time consuming part of the algorithms  In addition  tra ditional  operation count  analyses are cumbersome and unenlightening because of the difficulty of pre cisely characterizing the number of primitive opera tions required to solve each linear program  We will express the running time of W    FILTER F  in terms of the size of the sets F and W  the number of states lSI  and m  the number of vectors in W that are found by checking the e  information states  As is evident in Figure I  each iteration of the  while  loop on line   removes one vector from F  and m vec tors are removed before the loop  This means the while loop is executed precisely IFI m times  Each iteration of the  while  loop makes a single call to DoMINATE  so there are IF I  m linear programs solved in all cases  Each of these linear programs has one variable for each state in S and one for J  The total number of con straints in any one of these linear programs will be between m     and  WI     In the best case  the total number of constraints will be IFI m        miWI    W   W         m m        and the worst case will have an additional  IF    W    W   m  constraints  W hen checking the e  information states  at least one vector in W will be found  Further  when  WI     we are guaranteed to find at least two of the vectors in W  For the remainder of this paper  we assume that  W       since the case of  WI   is trivial     The witness algorithm has been analyzed previ ously  Littman  Cassandra    Kaelbling        and we list the basic results here for easy comparison  The total number of linear programs solved by wit ness is O    z jSgl    Zi  Sa     Sa     asymptoti cally  this is e   ISa l Lz IS I   Note that this is not a worst case analysis  this many linear programs will always be required  The number of constraints in each linear program is bounded by  Sal      The total number of constraints over all the linear programs is    Sa   Lz  Sg   asymptotically       In the best case there are      Lz  S   Z       S     l  JS         Lz  S     Z  worst case there are JSa   Sa  constraints        constraints and in the           JS    Z        for POMDPS             for i      to k   do W    FILTER W EB Sg          return  W  Figure      The incremental pruning method   INCREMENTAL PRUNING  This section describes the incremental pruning method  Zhang   Liu        which computes sa effi ciently from the s sets  Recall the definition for sa in Equation     sa     purge  zEZ  here  k     s   EB      purge sl  EB  s   ffi     EB sk     Zj  Note that  purge A EBB EB C  so Equation        purge purge A EBB   EB  C    can be rewritten as  Sa  purge     purge purge S   EB S   EBS        ffi  Sk         The expression for sa in Equation    leads to a very natural solution method  called incremental pruning  illustrated in Figure    In addition to being conceptu ally simpler than the witness algorithm  we will show that it can be implemented to exhibit superior perfor mance and asymptotic complexity  The critical fact required to analyze incremental prun ing is that if A purge A  and B purge  B   neither contain extra vectors  and W   purge A ffi B   then   WI     max IAI  IB           Equation    follows from the observation that for ev ery w E W  every R w  W  region is contained within R a A  and R     B  for some a E A and j  E B  This means that the size of the W set in INCPRUNE is monotonically non decreasing  it never grows explo sively compared to its final size  Figure   illustrates a family of algorithms that we col lectively call the incremental pruning method  specific incremental pruning algorithms differ in their imple mentations of the FILTER procedure  The most basic incremental pruning algorithm is given by implement ing FILTER by Lark s algorithm  Figure     we call the resulting algorithm IP  In Section    we describe several other variations        Cassandra  Littman  and Zhang  R    D        is empty  we need to R   A EBB  is empty  We can show this by contradiction  Assume there is an x  E R  A ffi B   Since  D      A ffi B  x E R  D        But we know that R   D         is empty  so this cannot be      Sal I z jS I  linear pro    Sal  I z IS  I   constraints   In the worst  Proof   The complexity of IP is grams and  case  these bounds are identical to those of the witness         algorithm  Section  However  there are POMDPs  for which the expression for the total number of con straints is arbitrarily loose  the best case total number  To prove the second part  let  of constraints for IP is asymptotically better than for  FILTER in lNCPRUNE  Figure    FILTER A EBB   This section modifies implementation of FILTER to take advantage of All the calls to  are  of the form  the the  deal of regularity  The modification yields a family of  FILTER algorithms   some of which render incremental  appearing in Figure     is used   The change is to replace line  x D          Different choices of in Figure     with  Equation   a       for a E A and j  E B  For every a  E A and f t E B  if  at  j t  E W  then either  at    d E D  or  at       E D or  a  f t  E D  Let  D   i e   IP           and  algorithm  Using either     or    exclusively in the incremental prun  constraints  Although  O ISal L z IS I     Sai IZI   the asymptotic total number of  linear programs does not change   that satisfy the  RR actually requires  slightly more linear programs than IP in the worst case  However  empirically it appears that the savings  The following lemma shows that any such choice of allows us to use the domination check in Equation  in the total constraints usually saves more time than the extra linear programs require  An even better variation of incremental pruning selects whichever     and      vector in purge  A EBB  that has not yet been to W  note that fl  W    the  IP  algorithm  are  plicated  upper bounds are possible        The only extra work that is required is  some bookkeeping to track how vectors were created and the sizes of the various sets that we will choose from   added  IS  I L z IS I linear program s and ISai ISal      I   ISI IZI total constraints  Note that tighter  though more com  set is smallest from among Equations  This will usually yield a faster algorithm in  practice  though it makes this variation much harder  D     Lemma   If R c   D            thenR c   AEBB       If x E R  D        then x E R w  W  for some wE  A B    W   D  to analyze   to either remove  from consideration  or to find a  on   RR   plexity of the algorithm to       A ffi B    a  ffi B  U  A EB             w         a  ffi B   U  at      at       E W         A EB       U  f t   aj a        E W         bounds        in lNCPRUNE  ing algorithm will improve the total constraint com  above properties  For example   upper  filtering algorithm in  We refer to variations of the incremental pruning as the restricted region  There are a number of choices for          is equivalent to using Lark s  method using a combination of Equations     D   AffiB     Simple      filtering algorithm  W hite  is the set of winning vectors found so far       equiv  as described earlier    W  and  D set      is  lNCPRUNE  Equation  of vectors satisfying the properties below  A ffi B  result in different incremental  alent to using Monahan s  can be used and still give a correct algorithm  recall that we are filtering the set of vectors  D  pruning algorithms  In general  the smaller the the more efficient the algorithm  Equation        DOMINATE  D         D  The lemma follows   pruning more efficient than when the standard version  D D D D D  argmax I  EAffiB x     xw   xw  w EW  Let  at     t    w  for any w  E W  a  E A and     E B and let  a           for a E A and    E B  By the conditions on D  we know that either  at        E D  or  at       E D  or  a       E D  Assume  a       ED  the other two cases are similar   Sincex E R  D      x     x a f     x a        This implies that x  a   xa  Adding     to both sides gives us that x   a         x  a           xw  By the definition of w  xw  x  a       Hence xw   xw    fact that the set of vectors being processed has a great         for all  GENERALIZING IP  Any set  w  The lemma is proved if we can show that  witness      First  if  show that  In principle  it is also possible to choose aD set that is the smallest set satisfying conditions     and    This ap  pears to be closely related to the NP hard vertex cover   problem  we are investigating efficient alternatives    Incremental Pruning for POMDPS     EMPIRICAL RESULTS  Although asymptotic analyses provide useful informa tion concerning the complexity of the various algo rithms  they provide no intuition about how well algo rithms perform in practice on typical problems  An other shortcoming of these analyses is that they can hide important constant factors and operations re  Table    Total execution time  Test Problem  D maze  W itness  TTOTAL       sec     IP  RR                       x                                 x                                                                                                                              Exh   quired outside of the linear programs  To address these shortcomings  we have implemented IP and variations and have run them on a suite of test problems to gauge their effectiveness  All times given are in CPU seconds on a SPARC      Cheese Part painting Network Aircraft ID Shuttle  x  CO  found that more than     of the total execution time was spent solving lin ear programs   verifying that the linear programs are the single most important contributor to the complex ity of the algorithms   Table    Total execution time  sec   for extended tests                                                                                         We profiled the execution and  To ensure fairness in comparison  we embedded all of the algorithms in the same value iteration code and used as many common subroutines as possible  We also used a commercial linear programming package to maximize the stability and efficiency of the imple mentation  We ran IP  RR  exhaustive and linear support al gorithms on   different test problems listed in Ta ble    complete problem definitions are available at http   www cs brown edu people arc research     The  Stages  column reports the number of iterations of value iteration we ran and the  IVtl  column indicates the number of vectors in the final value function    pomdp examples  html    Table   lists the total running time for each algorithm on each of the   test problems  The results indicate that RR works extremely well on a variety of test prob lems  We do not list run times for the linear support algorithm because  in all cases  it was unable to run to completion  This is because of memory limitations  space requirements  for the linear support algorithm  increase dramatically as a function of the number of states  We terminated algorithms that failed to com plete in   hours        seconds   as a result  the ex haustive algorithm   Exh    was only able to complete three of the test problems  all of which had only two observations   On the three small test problems the    This profiling data was computed running witnes s  IP  and RR on the  x  problem for   stages    The number of stages was determined by finding the maximum number of stages that the witness algorithm was able to complete within      seconds  In some of the test problems  the witness algorithm found the optimal infinite horizon value function in under      seconds  so we picked the number of iterations required to conv   rge to within machine precision of the optimal value function   Test Problem Network Shuttle  TTQTAL  Stages  W itness  IP  RR                                                         exhaustive algorithm was able to complete  it actually out performed all the other algorithms  For all but two of the test problems  the witness al gorithm was within a factor of   of the performance of RR  To highlight the advantage of the incremental pruning based algorithms  we chose the two test prob lems for which RR was more than   times faster than witness  Network       and Shuttle          and ran for a larger number of stages  As shown in Table    the witness algorithm is unable to solve a problem in   hours that RR can solve in    minutes       seconds   Although linear programming consumes most of the running time in the algorithms we examined  there are actually three phases of the value iteration algo rithm that contribute linear programs  finding the minimum size  sg  sets  constructing the  ga sets from  the s  sets  and constructing S  by combining the sa sets  Of these  only constructing the sa sets is differ ent between witness  IP  and RR  so we have chosen to present the execution times in two ways  The first  as illustrated in Table   as TTOTAL  represents the com plete running time for all stages and all phases  The second  shown in Table   as Tsa  BUILD  is the exe cution time over all stages that was devoted to con structing the sa sets from the s  sets  As the data in Tables   and   show  IP performs better than the witness algorithm on all the test problems  These tables also show how difficult it is to analyze the exact amount of savings IP yields  the amount of savings achieved varies considerably across problems        Cassandra  Littman  and Zhang  Table    Test problem parameter sizes  Test Problem  D maze  x   x  CO  x  Cheese Part painting Network Shuttle Aircraft ID  States                         Acts                     Obs                      Stages                               Vtl                               Reference Parr   Russell        Russell   Norvig        Cassandra  Kaelbling    Littman        McCallum        Kushmerick  Hanks    Weld        Chrisman         Table    Total time  sec   spent constructing sa sets  Test Problem  D maze  x   x  Cheese Part painting Network Aircraft ID Shuttle  x  CO  Tsa BUILD  Witness                                                        IP                                                     RR                                                 For RR  the set D was defined by Equation    if  BI    A  and Equation    otherwise  in most cases  this is equivalent to using the equation that leads to the smaller size for D  Looking at the data for RR  we see that in all but one case it is faster than IP  Again  the precise amount of savings varies and is difficult to quantify in general   DP UPDATE S    for each a inA   do for each z in Z do   t  FILTER  r a  a z ia    sa t  INCPRUNE S        s        S  f   FILTER  Ua sa    returnS     DISCUSSION   CONCLUSIONS  In this paper  we examined the incremental pruning method for performing dynamic programming updates in partially observable Markov decision processes  In cremental pruning compares favorably in terms of ease of implementation to the simplest of the previous al gorithms  exhal stive   has asymptotic performance as good as or better than the most efficient of the previ ous algorithms  witness   and is empirically the fastest algorithm of its kind for solving a variety of standard POMDP problems   A complete incremental pruning algorithm  RR  is shown in Figure    There are several important outstanding issues that should be explored  The first is numerical precision   St      INCPRUNE S         Sk     W  t   RR S    S    for i t    to k   do W t  RR W  S     return W    RR A B                     E            u                        Ft AEIJB Wt     for each s inS do w f   argmaxq EF es    Wt W U  w  Ft F  w  while F       do  o     B  E F D  t    o   EBB  U  a      Bj o     B  E W  D  t   A ffi       U        a i o         E W  if IBI   IAI x  then D    D  else D t  D  f   DOMINATE a       D   if X    l then F t  F   a       else w     argmaxq EF x    W    WU  w  F t  F    w          returnW Figure    Complete RR algorithm    Incremental Pruning for POMDPS each of the algorithms we studied  witness  IP  and RR  have a precision parameter c   but the effect of varying c on the accuracy of the answer differs from algorithm to algorithm  Future work will seek to de velop an algorithm with a tunable precision parameter so that sensible approximations can be generated  From a theoretical standpoint  there is still smne work to be done developing better best case and worst case analyses for RR  This type of analysis might shed some light on whether there is yet some other variation that would be a consistent improvement over IP  In any event  even the slowest variation of the incre mental pruning method that we studied is a consistent improvement over earlier algorithms  We feel that this algorithm will make it possible to greatly expand the set of POMDP problems that can be solved efficiently   C   and Poole  D        Computing optimal policies for partially observable decision processes us ing compact representations  In Proceedings of the Boutilier   Thirteenth National Conference on Artificial Intelli             AAAI Press The MIT Press   Cassandra  A  R   Kaelbling  L  P   and Littman  M  L        Acting optimally in partially observ able stochastic domains  In Proceedings of the Twelfth National Conference on Artificial Intelligence            Cheng  H  T         on  Machine           Amherst  Massachusetts  Mor gan Kaufmann  Learning   Monahan  G  E        A survey of partially observ able Markov decision processes  Theory  models  and algorithms  Management Science             Parr  R   and Russell  S        Approximating op timal policies for partially observable stochastic do mains  In Proceedings of the International Joint Con ference on Artificial Intelligence   Platzman  L  K        A feasible computational ap proach to infinite horizon partially observed Markov decision problems  Technical report  Georgia Insti tute of Technology  Atlanta  GA  Russell  S  J   and Norvig  P  ligence  A Modern Approach          Artificial Intel Englewood Cliffs  NJ   Prentice Hall   
  To determine the value of perfect informa tion in an influence diagram  one needs first to modify the diagram to reflect the change in information availability  and then to com pute the optimal expected values of both the original diagram and the modified diagram  The value of perfect information is the dif ference between the two optimal expected values  This paper is about how to speed up the computation of the optimal expected value of the modified diagram by making use of the intermediate computation results ob tained when computing the optimal expected value of the original diagram      INTRODUCTION  The concept of the value of perfect information is very useful in gaining insights about decision problems  Matheson        has demonstrated that influence di agrams provide a more suitable paradigm to address the issue than decision trees  This paper is concerned with the problem of computing the value of perfect information in influence diagrams  An influence diagram is a graphical representation of a particular decision problem  Howard and Matheson        It is an acyclic directed graph with three types of nodes  decision nodes  random nodes and value nodes  The influence diagram in Fig    represents an extension to the well known oil wildcatter problem   Raiffa       Shachter        The decision nodes are depicted as rectangles  random nodes as ellipses  and value nodes as diamonds  The total value of the diagram is oil sales minus the sum of test cost  drill cost  and sale cost  The expectation of the total value depends on the decisions made  The optimal expected value of the diagram is de fined to be the maximum of the expected total values  There are arcs from test and test result to  drill  This means that the drill decision is to be made knowing the type of test per formed and the test result  There is no arc from market information to drill  This means that market information  at the time when the oil sale policy is to be made  is not available at the time the drill decision is to be made   To reduce risks  the oil wildcatter may choose to  before making the drill decision  hire an expert to predicate market information at the time the oil sale policy is to be made  This operation may be expensive  So  before making up his mind  the oil wildcatter may wish to determine the value of the ex pert s predications  The value of perfect information on market information serves as an upper bound for the value of the predications  Specifically  it is defined as follows  Modify the diagram by adding an arc from market information to drill  The value of perfect information on market information is the difference between the optimal expected value of the modified diagram and that of the original diagram  A straightforward method of computing the value of perfect information is to exactly follow the definition  That is to respectively compute the optimal expected values of the original influence diagram and of the modified diagram  and then figure out the difference  This paper shows that one can do better than that  Since the original and the modified diagrams differ very little  there must be computation overlaps in the processes of evaluating them  By avoiding those over laps  one can speed up computing the optimal ex   Figure    The influence diagram for an extension of the oil wildcatter problem    Computing the Value of Information  pected value of the modified diagram  This is espe cially interesting if one wants to assess the value of perfect information for a number of cases  The exposition will be carried out in the terms of stepwise decomposable influence diagrams  which is reviewed in section    Section   introduces the concept of influence diagram condensation  Section   shows how to use this concept to uncover the computation overlaps mentioned in the previous paragraph  The paper concludes at section        STEPWISE DECOMPOSABLE INFLUENCE DIAGRAMS  Stepwise decomposable influence diagrams  SDID s  were first introduced by Zhang and Poole        as a generalization to the traditional notion of influence diagrams  Better references in this regard are Zhang  Qi and Poole      b  and Zhang         This section reviews the concept of SDID s        Note  Traditionally  arcs into decision nodes are inter preted as indications of information availability  Now that the no forgetting constraint has been lifted  those arcs need to be re interpreted as indication of both information availability and dependency  More explic itly  the lack of an arc from a node c to a decision node d no longer implies that information c is not observed when making decision d  It may as well mean that d is independent of c given the parents of d  Acyclicity and the leaf node constraint together de fine a very general concept of influence diagrams  One theme of Zhang        is to identify subclasses of in fluence diagrams with various computational proper ties  One important property for an influence diagram to possess is the so called stepwise solvability  which says that the diagram can be evaluated by considering one decision node at a time  If an influence diagram is not stepwise solvable  then one needs to simulta neously consider several  even all the decision nodes  which usually tends to be computationally expensive   Acyclicity  which requires that there be no di rected loops in influence diagrams   When an influence diagram is stepwise solvable  The answer  when it is stepwise decomposable  It can be shown that a stepwise decomposable influence di agram can be evaluated not only by considering one decision node at a time  but also by considering one section of the diagram at a time  Zhang  Qi and Poole        It can also be shown that an influence di agram is stepwise solvable only when it is stepwise decomposable  Zhang            Regularity  which requires that there be a total ordering among all the decision nodes   In the rest of this section  we define stepwise decomposable influence diagrams      The no forgetting constraint  which requires that any decision node and its parents be parents to all subsequent decision nodes             THE PATH TO SDID S  Traditionally  there are five constraints imposed on in fluence diagrams         The single value node constraint  which requires that there be only one value node  and     The leaf value node constraint  which requires that the value node have no children  Influence diagrams that satisfy all the five constraints will be referred to as no forgetting influence diagrams  We propose to lift constraints     and to develop a general theory of influence diagrams starting with con straints   and   only  There are several advantages to lift constraints      For instance  by lifting the no forgetting constraint we are able to  anmong other things  represent the facts that some decision nodes are conditionally indepen dent of certain pieces of information  In the extended oil wildcatter problem  Fig      it is reasonable to as sume that the decision oil sale policy is indepen dent of information on test result given the quality and quantity of oil produced  This piece of knowl edge can not be represented if the no forgetting con straint is enforced  The reader is referred to Zhang         Zhang  Qi and Poole        for other rationale for lifting constraints       INFLUENCE DIAGRAMS  An influence diagram is an acyclic directed graph con sisting of a set of random nodes C  a set of decision nodes D  and a set of value nodes U  The value nodes have no children  A random node c represents an un certain quantity whose value is determined according to a given conditional probability distribution P cJ rc   where  rc stands for the set of the parents of c  A value node v represent one portion of the decision maker s utilities  which is characterized by a value function fv  Let I be an influence diagram  For any node x in I  let  lrx denote the set of the parents of X   Let nx denote the frame of x  i e the set of possible values of x  For any set J of nodes  let nJ   nxEJ nx  Let d         dk be all the decision nodes in I  For a decision node d   a mapping      n   d      nd  is called a decision function for d   The set f all the decision functions for d   denoted by di  is called the decision function space for d   The Cartesian product of the decision function spaces for all the decision nodes is called the policy space of I  We denote it by      Given a policy                 k  E     for I  a probabil ity P  can be defined over the random nodes and the        Zhang  Qi  and Poole  decision nodes as follows  Po C  D    k  II P cl rc  II Po  dil   d          i l  cEC  where P cl rc  is given in the specification of the influ ence diagram  while P   dil   d   is given by Oi as fol lows  if     rd     d       otherwise For any value node v        must consist of only deci sion and value nodes  since value nodes do not have children  Hence  we can talk about P   r     The ex pectation of the value node v under Po  denoted by E  v   is defined as follows         A regular influence diagram is stepwise decomposable if for any decision node d  none of the decision nodes that precede d are in the downstream of    d  The influence diagram in Fig    is a SDID  No forgetting influence diagrams are SDID s  One desirable property of SDID s is that they are stepwise solvable  As an example  consider the SDID in Fig     One can first compute an optimal policy for oil sale policy in the part of the diagram that lies to the right of oil produced with oil produced included  and then find an optimal policy for drill  and then for test  The optimal expected value of the diagram obtained as a by product of computing an optimal policy for test  See Zhang  Qi and Poole      b  for details     CONDENSING SDID S  The summation of the expectations of all the value nodes is called the value of I under the policy    We denote this denoted by E  I   The maximum of E  I  over all the possible policies   is the optimal expected value of I  An optimal policy is a policy that achieves the optimal expected value  To evaluate an influence diagram is to determine its optimal expected value and to find an optimal policy   This section presents a two stage approach for evaluat ing SDID s  In the first stage  a SDID is  condensed  into a Markov decision process  Denardo        This involves two types of operations  the operation of com puting conditional probabilities and the operation of summing up several functions  In the second stage  the condensed SDID is evaluated by the various algo rithms  Qi       Qi and Poole         An influence is regular if there exists a total ordering among all the decision nodes  Even though all our results in this paper readily generalizes to influence diagrams which are not necessarily regular  we shall limit the exposition only to regular influence diagrams for the sake of simplicity   This two stage approach is interesting because it al lows easy implementation of influence diagrams on top of a system for Bayesian network computations  Zhang        The approach is also of fundamental signifi cance to the current paper  as the reader will see in Section          STEPWISE DECOMPO SABLE INFLUENCE DIAGRAMS  To introduce stepwise decomposable influence dia grams  SDID   we need the concepts of moral graph and of m separation  Let G be a directed graph  The moral graph G is an undirected graph m G  with the same vertex set as G such that there is an edge be tween two vertices in m  G  if and only if either there is an arc between them in G or they share a common child in G  The term moral graph was chosen because two nodes with a common child are  married  into an edge  Lauritzen and Spiegehalter         This approach has been developed from a similar ap proach in terms of decision graphs  Qi       Zhang  Qi and Poole     a   In the rset of this paper  we shall concentrate on the first stage  i e  condensation  Let us begin with smoothness in SDID s       SMOO THNESS IN SDID S  An influence diagram is smooth at a decision node d if there is no arcs from the downstream of  rd to  rd  If an influence diagram is smooth at all the decision nodes  we say that the diagram is smooth   In an undirected graph  two nodes x and y are sepa rated by a set of nodes A if every path connecting them contains at least one node in A  In a directed graph G  x and y are m separated by A if they are separated by A in the moral graph m G   One implication of this definition is that A m separates every node in A from any node outside A  For any decision node d of I  the downstream of  rd is the set of nodes that are not m separated from d by  rd  The upstream of  rd is that set of nodes outside  rd that are m separated from d by  rd   Figure    The influence diagram in Fig  smoothing     after        Computing the Value of Information  upstream o f   Kct      downstream o  n dj                                                                        d i          di       du          Figure    An abstract view of a smooth regular SDID  The terminal section I dk      consists of the nodes in the  rde and the nodes in the downstream of  rdk   I   d  a   Figure    The sections of the SDID in Fig     SDID s may be not smooth  For example  the SDID in Fig    is not smooth at the decision node drill  The arc from seismic structure to test result is from the downstream of  rdrill to  rdrill Two influence diagrams are strongly equivalent if they have the same set of nodes  the same optimal policies  and the same optimal expected value  A non smooth SDID can always be transformed  by a series of arc reversals  Shachter          into a strongly equivalent smooth SDID  Zhang  Qi  and Poole     b    For ex ample  the SDID in Fig    can be transformed into a strongly equivalent SDID whose underlying graphical structure is shown in Fig     This SDID is smooth  From now on  we shall only be talking about smooth SDID s       SECTIONS IN SDID S  The concept of sections in SDID is a prerequisite for the concept of condensation  Let I be a smooth regular SDID  Let d   d        dk be the decision nodes  Since I is regular  there is a total ordering among the decision nodes  Let the total ordering be as indicated by the subscriptions of the decision nodes  As a consequence  we have that d  precedes di    and there is no other decision node d such that d  precedes d and d precedes di l For any i E                k      the section of I from  rd  to  rd  l  denoted by I d  di l   is the subnetwork of I that consists of the following nodes     the nodes in  rd  U  rd         the nodes that are in both the downstream of  rd  and in the upstream of  rd      The graphical connections among the nodes remain the same as  in I except that all the arcs among the nodes in  rd  U  di  are removed  The initial section I   dt  consists of the nodes in  rd  and the nodes in the upstream of  rd   It consists of only random and value nodes   The nodes in a section that lie outside  rd  U  d   are either random nodes or value nodes  Their conditional probabilities and value functions are the same as those in I  The nodes in  rd  U  d   are either decision nodes or random nodes  There are no conditional probabili ties are associated with these nodes  Let us temporarily denote the SDID in Fig    by I  Let us denote the variables test by t  drill by d  oil sale policy by s  drill cost by de  test result by tr  oil produced by op  and market in formation by mi  There are four sections in this SDID  I    t   I t d   I  d  s    and I s     The initial section I     t  is empty  All the other sections are shown in Fig     The concept of sections provides us with a perspec tive of viewing smooth regular SDID s  A smooth regular SDID I can be thought of as consisting of a chain sections I    d    I d   d         I dk    dk   and I dk     Two neighboring sections I di l di  and I d   di l  share the nodes in  rd   which m separate the other nodes in I d    d   from all the other nodes I d  d  l  Fig    shows this abstract view of a smooth regular SDID  In the extended oil wildcatter example as shown in Fig     the sections I t  d  and I  d  s  share the nodes test and test result  and the sections I d  s   and I s   share the nodes oil produced and market in formation       CONDITIONAL PRO BABILITIES AND LOCAL VALUES IN THE SECTIONS  In the section I d  d      there is no decision node out side  rd  U   di   The value nodes are at leaves by defini tion  So  one is able to compute the conditional prob ability PI d  d      rd     rd   d   of the nodes in  rd    given the nodes in  rd  and d   We shall refer to this probability as the conditional probability of  rd    given  rd  and d  in I  In the initial section I   d    one can compute the probability PI   d l rd    We shall refer to this prob ability as the prior probab ility  r  in I  For a value node Vj in I d   d  l   one can compute conditional probability PI d  d      rvi l rd   d    Define        Zhang  Qi  and Poole  a function  J   n  d f  r d   d        nd   n by     L       v         d  u  d     PI d  d       f v    rd   d   fv     lrv        where fv  is the value function of  Vj  Figure    The condensation of the SDID in Fig      in I   Let v           Vm be all the value nodes in the section I d   di    The local value function f   n  d X nd  n of the section I d   d  t  is defined by I  m  fa   f d   d      L          d   d                       CONDENSATIO N  Intuitively  condensing a smooth regular SDID I means to do the following in each section I d   d  l  of I      getting rid of all the random nodes that are neither in the  rd  nor in  rd        combining all the value nodes into one single value node vf  and     col lecting the nodes in  ra  into one compound variable x   This results in a Markov decision process  Now the formal definition  The condensation of I  de noted by Ic  is defined as follows      It consists of the following nodes   Random nodes x         i     k   where x  is the compound variable consists of all the nodes in  ra  when    d         When  rd      or when i      x  is a degenerated variable that has only one possible value  say  o    The same decision nodes d          i      k  as in I  and  Value nodes vf         i     k       The graphical connections among the nodes are as follows   For any i E               k   there are two arcs converging at x   one from Xi   and the other from di    For any i E               k   there is an arc from x  to d    For any i E               k   there are two arcs converging at vf  one from Xi and the other from d     The presence of the node xo makes the picture ugly  But we need it for two reasons  First  there may be value nodes in the initial sec tion  Second  we want to be able to talk about the condensation of an influence diagram that contains no decision nodes   Xo   one to     The conditional probabilities and value functions are as follows   When there are no value node in the section  then    is defined to be the constant    We can also define the local value  function  for the initial section  which is not really a function  but just a constant  We shall denote this constant by fo   There are two arcs emitting from vg and the other to x        The conditional probability pe  Xi llx  d    i E            k      is defined to be PI d  l d    rd  ll rd   d    The conditional probability pe x  xo       is defined to be PI    dl    rd    and the probabil ity pc x   is trivially defined since x  takes only one value     The value function fv for vf  i E                  k   is defined to be      Fig    depicts the condensation of the SDID shown Fig    Since test has no parent  x  is a degener ated variable  The variable x  stands for the com pound variable consisting of test and test cost  and X  stands for the compound variable consist ing of oil produced and oil market  The condi tional probability pe x lx   d    for instance  is the con ditional probability PJ d s    op  mi It  tr  d  of op and mi given t  tr  and din the section I d s   The value function fvc  for the value node vf is a representation of test cost  fv is a representa tion of drill cost  and fvc  is a representation of oil produced and sale cost  There is no value node in the initial section  So fvo is the constant    The node is kept only for uniformity  Two decision networks are equivalent if they have the same optimal value and share the same optimal poli cies  The following theorem is proved in Zhang           A smooth regular SDID is equivalent to its condensation   Theorem  To end this section  we would like to echo what we said at the beginning of the section  The process of con densing a SDID only involves two types of operations  the operation of computing conditional probabilities and the operation summing up functions  see subsec tion       The latter is straightforward  The formmer can be carried out by by any well established Bayesian network evaluation algorithm  One advantage of the concept of condensation is that it leads to a simple way of implementing influence diagrams on top of a system for Bayesian network computation    Computing the Value of Information     COMPUTING THE VALUE OF PERFECT INFORMATION  Let I be a regular SDID  Let d  and c respectively be a decision node and a random node in I  such that there is no arc from c to d  in I  Let I  be the diagram obtained from I by adding arcs from c to d  and to all the subsequent decision nodes  If there is no direct cycles in I    then I  is again a regular SDID  In such a case  the value of perfect information on c at d  in I is defined to be the difference between the optimal expected value of I  and that of I  In the following  we shall use     to denote the set of parents of d in I   To determine the value of perfect information on c at d   one needs to compute the optimal expected val ues of both I and I   To this end  we adopt the two stage approach described in the previous section  i e we first compute the condensations of I and I   and then evaluate the condensations respectively  An ad vantage of this approach is that it can make use of in formation stored in the condensation of I in comput ing the condensation of I   More explicitly  for each section I d   di    of I  the conditional probability Pr a  a      l a   l l a   d   and the local value function    are computed and stored in the condensation of I  This paper seeks to make use of this conditional prob ability and this local value function in computing the conditional probability Pl  d  d            l d  d   and the local value function ff of the corresponding sec tion I  d   d     of I   To see an example   let I be the SDID shown in Fig     Consider the value of perfect informa tion on market information at drill  In this case  I  is the same as I expect for the arc from market information to drill  The section I  s     is the same as I s        Thus  when computing the condensation of I   the conditional probability and the local value function for this section can simply be re trieved from the condensation of I  The section I  t d  is the same as I t d  except that  u    tains one extra random node market information  The node market information is isolated in I  t d   In the condensation of I   one needs P   t d  mi  trlt   This can be computed by P   t d  mi  trjt    P  t d  trjt P mi    where P mi  is given in the specification of the diagram and PJ t d  trlt  can be retrieved from the condensa tion of I  The section I  d  s  is also the same as I d  s    How ever  the decision nodes drill has one more parent   It is always the case if the influence diagram is in the so called Howard normal form  See Matheson               namely market information in I  than in I  Thus  to obtain the condensation of I   one needs the conditional proba bility P    d s   op mijt tr d mi   In the condensation of I  one has PJ  d s  op  milt tr d   The nice thing is that one can easily compute P   d s  op milt tr  d mi  from Pl  d s  op milt tr d   which is the same as PJ d s  op mijt tr d   which in turn can be retrieved from the condensation of I  To summarize  it takes very little computation to ob tain the condensation of I  from the condensation of I  The rest of this section is to show that the same can be true for many other cases  We shall do this case by case  But first  some preparations       REMOVABLE ARCS  A random node can be in more than one section  In the oil wildcatter example  market information is in both the section I d  s  and the section I  s       Let dt be the last decision node such that c is in the section I dt    dt   remember that there is a total ordering among the decision nodes     The reader is advised to pay close attention to the definition of dt and the definition of d   given at the beginning of this section    ince we shall use them fre quently in the rest of the paper  It follows from a result of Zhang and Poole        that in I  the arcs from c to the decision nodes subsequent to dt  i e to the decision nodes dt b       dk are remov able  in the sense that the removal of those arcs results in an equivalent influence diagram  As a corollary  if t   s  all those arcs in I  that are not in I are remov able  Hence  I and I  are equivalent  In other words  if c is in the upstream of       the value of perfect infor mation on c at d  is    In the extended oil wildcatter example  it is of no value to acquire perfect knowledge about seismic structure at the time one is to make the oil sale policy  From now on  we shall let I  stand for the diagram after the removal of those removable arcs       TWO ASSUMPTIONS  We assume that c is a root random node  i e it has no parents  A consequence of this assumption is that if I is smooth  so is I   W hen c is not a root  one can transform the diagram by a series of arc reversals so that c becomes a root in the resulting diagram  This is very similar to the operation of smoothing mentioned in subsection      See Zhang        for details  We also assume that dt   is a parent for every value node in the section I dt    dt  The assumption is to assure that if a value node appears in a section I d   di    of I  then it appears in the corresponding section I  d   di l  of I   This a lows us more chances in making use of the local value functions of the sec         Zhang  Qi  and Poole  tions of I in computing the local value functions  of I   as the reader will see in the following  The assumption is not restrictive because one can always pretend that the value function f  of a value node v in I dt    dt  depends on dt even thought it actually does not  Under those two assumptions  we can show that I  dj dJ t  is the same as I dj dJ    except that it may contain the extra random node c       SECTIONS BEFORE da   AND SECTIONS AFTER dt  the section I  da   d    J     Ta    da       s    rd   ds l    where fa     Td    ds    can be retrieved from the con densation of I  In the extended oil wildcatter example  the section from test to drill falls into this case  SECTIONS IN BETWEEN d  ANDdt         We need to consider four cases  Let us first discuss the easiest case  the sections before d   and sections after dt  This case occurs when i  s   or i   t  In such a case  I  d   d    is exactly the same as I d    d   l  So  the conditional probability and the local value function in I  d    d    are the same as those in I d   d  l   which can simply be retrieved from the condensation of I   This subsection considers the case when s   i   t      In this case  the section I  d  d      is t he same as I d  d   l   except that it contains one extra node c  This node is isolated in I  d   d     So  c is in dependent of all other nodes in I  d  d       Since  rd      rd  u  c  and  ra       Td    u  c   the condi tional probability pl  d  di l   rai l  ra   d    satisfies pl  d  d       rai l I Ta  d      Pl  d  d      Td  t c  rd  c d      pl  d  di l   Tdi l   Tdi  d      PI d  d  t   Td  l  rd  d     In the extended oil wildcatter example  the terminal section falls into this category       THE SECTION FROM  d a    TO  d   The section I  da    d   is the same as I d    d    ex cept that it contains one extra node c  This node is isolated in I   d    d    Thus c is independent of all the other nodes in I  d    d    Since  ra       rd    and  ra     r d  U  c   the condi   tional probability PI  d     d    ra I Ta    da    can be computed by Pl  d     d     ra l ra     d       Pl  d    d    rd  ci Td    d  l    Pl  d    d    rd   rd    da   P c     PI d   d     Td i Td    d    P c             where equation     is due to the fact that c is inde pendent of all the other nodes in I  ds    d    In equa tion      P c  is given in the specification of I and PI d    d    Td   Td    ds    can be retrieved from the condensation of I  We now turn to the local value function  For any value node v in the section I d     d    we have that c rt Tv  because c is in another section  By making use of the fact that c is independent of all the other nodes in I  d s    d   again  we get Pfl d    d    r l rd     d       Pl  d    d    rv   rd     da      PI d    d    rv  Td    da     This equation and equations        give us the following formula for computing the local value function      in            where PI d  d      Td     Td  d    can be retrieved from the condensation of I since c E  rd    We now turn to the local value function  For any value node v in the section I d  d  l   we have that c   r   By making use of the fact that c is independent of all the other nodes in I  d   di l  again  we get p   d  d  t       l rd   d     Pl  d  di t   rv  rd  d      PI d  d      rv l rd   d      This equation and equations        give us the following formula for computing the local value function ff in the section I  d   d          where     rdi  d   can be retrieved from the condensa tion of I  THE SECTION FROM       dt    TO  dt  This subsection considers the section from dt l to dt  The section I  dt    dt  is the same as I dt    d t   and  Ta      lTd    U  c  and  Ta     rd  Thus we have pl  d    d    id    id     dt       PI d    d     rd    rd     c  dt       If c  E   rd   we have  PI d    d    rd  I Td    c dt    PI d  t  d    lTd  l ra     dt     lO  L   d       c  PI dt t dt     d    rd  u dt       Computing the Value of Information       Thus  one can use the right nand side of equation      to compute PI  d    d    rd l Td    dt d whencE  rd  In the extended oil wildcatter problem  the section from drill to oil sale policy is an example of this case   I  Of fundamental importance to the method is the concept of condensation  which also leads easy imple mentations of SDID s on top of a system for Bayesian network computations   On the other hand  if c rJ   Td   there is no obvious way to make use of P  d    d    rd  l rd     dt d in com puting Pl  d    d    rd  i Td     dt d In such a case  PI  d    d    rd i Td     dt    needs to be computed from scratch   Acknowledgement  Now  the local value function  If for every value node in the section I  dt   dt   one has that  r J   Td    U  dt d  then it is easy to see that  v  ff      Tdt     dt d   ft      Tdt    dt d  Again in the extended oil wildcatter problem  the sec tion from drill to oil sale policy is an example of this case  In any other case  we see no way to make use of ft   in computing JI    One needs to compute fi   from scratch        How much computation savings   To end this section  we would like to give the reader some idea about how much computation our approach can save  There are two SDID s  the original I and the modified I   The savings are in the process of evalu ating I   There are two stages  in stage   one con denses I   and in stage   one evaluates the condensed SDID  As we have shown in this section that it takes very little computation to obtain the condensation of I  from that of I  This means a lot of savings if there are many random nodes in I that are not parents of any decision nodes  since those are the nodes that the condensation precess needs to get rid of  As we have pointed out earlier  our approach is especially useful if one wishes to evaluate the value of perfect information for a number cases  One can compute the condensa tion of I once and use it for all the cases  We can also save some computation in stage    Since I  d  di    and I di di    are the same for all i    t  we need not to re evaluate these sections at all  Furthermore  we can save more if we adopt a top down approach for evaluating the condensed diagrams  See Qi        and Zhang  Qi  and Poole       a    for details     CONCLUSIONS  The value of perfect information in an influence dia gram is defined as the difference between the optimal expected value of a properly modified influence dia gram I  and  that of the I itself  In this paper  we have described a method for computing the value of perfect information  The method is incremental in the sense that it computes the value of I  by using the in termediate computation results obtained in evaluating  This paper is partly supported by NSERC Grant OG P         
  a must in real world applications  approximation methods  e g   This  paper  is  concerned  in stochastic domains by  with  planning  means of par  tially observable Markov decision processes  POMDPs   POMDPs are difficult to solve  This paper identifies a subclass of POMDPs called region observable POMDPs  which are easier to solve and can be used to approxi mate general POMDPs to arbitrary accuracy   Most previous  Cheng       Lovejoy      b  and Parr and Russell      are value function approximation methods in the sense that they approx  imate optimal value functions of POMDPs directly  We advocate model approximation methods   Such a  method approximates a POMDP itself by another that is easier to solve and uses the solution of the latter to construct an approximate solution to the original POMDP  Model approximation can be in the form of a more informative observation model  or a more deterministic  Keywords   planning under uncertainty   action model  or an aggregation of the state space   partially observable Markov decision pro  or a combination of two or all of them   cesses  problem characteristics   investigates the first alternative   This paper  The idea of approximating a POMDP by assuming a     more informative observation model is not new  Cas  INTRODUCTION  sandra et al        have proposed to approximate  To plan is to find a policy that will lead an agent to achieve a goal with minimum cost   W hen the envi  ronment of the agent  henceforth referred to as the world  is completely observable and the effects of ac tions are deterministic  planning is reduced to finding the shortest sequence of actions that leads the agent to the goal   POMDPs by using MDPs  This paper generalizes the idea  We transform a POMDP by assuming that  in addition to the observations obtained by itself  the agent also receives a report from an oracle who knows the true state of the world  The oracle does not report the true state itself  Rather  he selects  from a list of candidate regions  a region that contains the true state and reports that region  The transformed POMDP is  In real world applications  however  the world is rarely  said to be region observable because the agent knows  completely observable and effects of actions are almost  for sure that the true state is in region reported by the  always nondeterministic  For this reason  a growing  oracle   number of researchers concern themselves with plan ning in stochastic domains  e g   Dean and Wellman        Cassandra et al        Boutillier et al      Parr  and Russell        Partially observable Markov deci  sion processes  POMDPs  can be used as a model for planning in such domains   In this model  nondeter  minism in effects of actions is encoded by transition probabilities  partial observability of the world by ob servation probabilities  and goals and criteria for good plans by reward functions  POMDPs are difficult to solve and approximation is  W hen all candidate regions are singletons  the oracle actually reports the true state of the world  In such a case  the region observable POMDP reduces to an MDP  MDPs are much easier to solve than POMDPs  One would expect the region observable POMDP to be solvable when all candidate regions are small   In terms of quality of approximation  the larger the candidate regions  the less extra information the ora cle provides and hence the more accurate the approx imation  In the extreme case when there is only one        An Approximation Scheme for Decision Theoretic Planning  candidate region and it consists of all possible states of the world  the oracle provides no extra information at  all  Hence the region observable POMDP is identical  to the original POMDP  A way to determine the quality of approximation will be described  This allows one to make the tradeoff be tween approximation quality and computational com  plexity  as  follows  start with small candidate regions  and increase their sizes gradually until the approxima  As a background example  consider path planning for a robot who acts in an office environment  Here S is the set of all location orientation pairs    is the set of possible sensor readings  and A consists of actions move forward  tum left  turn right  declare goal   and  The current observation o depends on the current state  of the world s  Due to sensor noise  this dependency is  uncertain in nature  The observation o sometimes also  tion becomes accurate enough or the region observable  depends on the action that the robot has just taken a    POMDP becomes untractable   The minus sign in the subscript indicates the previous  In many applications  the agent often has a good idea about the true state of the world  planning  as an  example   Take robot path  Observing a landmark   a  room number for instance  would imply that the robot  is at the proximity of that landmark   Observing a  feature about the world  a corridor T junction for in stance  might imply the robot is in one of several re gions  Taking history into account  the robot might be able to determine a unique region for its current location  Also  an action usually moves the true state of the world to only a few  nearby  states  Thus if the robot has a good idea about the current state of world  it should continue to have a good idea about it  in the next few steps   time point  of  o  upon  s  In the POMDP model  the dependency  and  a    is numerically characterized by  P ois  a       which observation probability   a conditional probability referred to as the  is usually It is the  observation model   In a region observable POMDP  the current observa tion also depends on the previous state of the world The observation probability for this case can be       P o s a          written  s  the world will be in after taking an ac a depends on the action and on the current state  The state tion      The plus sign in the subscript indicates the next  time point   This dependency  is  again uncertain in  nature due to uncertainty in the actuator   In the a is  W hen the agent has a good idea about the true state  POMDP model  the dependency of    upon   and  at all time  accurate approximation can be achieved  numerically characterized by a conditional probability  with small candidate regions  We shall begin with a brief review of planning under uncertainty and POMDPs  We shall then formally in  P s l   a   which is usually referred to tion probability  It is the action model   as the  transi  We will often need to consider the joint conditional  P s   o  is  a  of the next state of the world  troduce region observable POMDPs as an approxima  probability  tion to general POMDPs   and the next observation given the current state and  Thereafter  we shall de  scribe a way to determine the quality of approxima  the current action  It is given by  tion  Finally  we shall report empirical results  which suggest that when there is not much uncertainty  a  P    o l  a    P s ls a P o l   a s    POMDP can be approximated accurately by a region observable POMDP that has small candidate regions and can hence be solved exactly   The POMDP model encodes the starting state by a probability mass function  Po over S  The planning function such as the fol  goal is encoded by a reward     lowing   PLANNING UNDER UNCERTAINTY AND POMDPs  r s  a    To specify a planning problem  one needs to give a set        if  a dlcare goal and   goal   otherwise        DECISION MAKING IN POMDPs  S of possible states of the world  a set   of possible observations  and a set A of possible actions  In this     paper  all those three sets are assumed to be finite   The agent chooses and executes an action at each time  One needs also to give an observation model  which describes the relationship between an observation and  edge about the true state of the world  which is sum  the state of the world  and an action model  which  marized by a probability distribution over the set of  point  The choice is made based on the agent s knowl  describes the effects of each action  Furthermore  one  possible states and called a  needs to specify the initial state of the world and  lief state  goal state   a  and a  belief state   The initial be  b is the current belief state  is the current action  H the observation o  is is P  Suppose        Zhang and Liu  obtained at the next time point  then the next belief state b  is given by  b  s     k LP s  o ls a b s            where k    La s  P s   o ls  a b s  is the normaliza tion constant  Cassandra et al       To signify the dependence of b  upon b  a  and o   we shall some   times write it as b   lb a  o   A policy  r prescribes an action for each possible belief state  Formally it is a mapping from the set B of all possible belief states to A  For each belief state b   r b  is the action prescribed by  r for b  The value function of     is defined for all belief states b by v     b    Eb L o lrt   where    y   is the discount factor and rt is the reward received at the tth step in the future  Intuitively  it is the expected discounted reward the agent can expect to receive starting from belief state b if it behaves according to policy  r  An policy  r  is optimal if v     b      V    b  for all b and all other policies  r  The value function of an optimal policy is called the optimal value function and is usually denoted by v  Policies for POMDPs can be found through value it eration  Bellman        Value iteration begins with an arbitrary initial function  Y t b  and improves it by using the following equation  Vt b    maxa r b a    y LP o   lb a  Vt    b                 where P o  lb  a    Ls s  P s   o ls a b s   and b  is a shorthand for b   lb a  o   If V       V   is called the t step optimal value function  It is well known that when the Bellman residual maxbEBIV t  b    yt    b l becomes small  l t  is close to V  and the greedy policy based on vt    r b    arg maxa r b a        LP o lb a l t   b            is a good approximation of the optimal policy  e g  Puterman        Since there are uncountably infinite many belief states  value iteration cannot to carried out explicitly  For tunately  it can be carried out implicitly due to the piecewise linearity of the t step optimal value function  Sondik        More specifically  there exists a list Vt of function of s  usually referred to simply as vectors  such that for any belief state  vt  b    maxvEV  L V s b s            Exact methods for solving POMDPs  Monahan       Eagle       and Larke       see W hite        Sondik        Cheng      Cassandra et al       attempt to find a minimum list of vectors that satisfies the above equation  Unfortunately  even the most effi cient algorithm can only solve POMDPs with no more than twenty states and fifteen observations exactly  Littman et al      Cassandra et al       Approxi mation is a must for real world problems  Most previous approximate methods  e g  Cheng       Lovejoy     b  and Parr and Russell       attempt to find a list of vectors that satisfies equation     approxi mately  This paper proposes to approximate POMDPs themselves by others that have more informative ob servations and hence are easier to solve     PROBLEM CH ARACTERJSTICS AND APPROXIMATIONS  We make the following assumption about problem characteristics  Even though in a POMDP M the agent does not know the true state of the world  he often has a good idea about it  See the introduction for justifications of this assumption  Consider another POMDP M  which is the same as M except that in addition to the observation made by itself  the agent also receives a report from an oracle who knows the true state of the world  The oracle does not report the true state itself  Rather he selects  from a list of candidate regions  a region that contains the true state and report that region  More information is available to the agent in M  than in M  extra information is provided by the oracle  When the agent already has a good idea about the true state of the world  the oracle does not provides much extra information even when the candidate regions are small  In such a case  M  is a good approximation of M  In M   the agent knows for sure that the true state of  the world is in the region reported by the oracle  For this reason  we say that it is region obseroable  The region observable POMDP M  can be much easier to solve than M when the candidate regions are small  For example  if the oracle is allowed to report only sin gleton regions  then he actually reports the true state of the world and hence M  is an MDP  MDPs are much easier the solve than POMDPs  We now set out to make the idea more concrete  Let us begin with the concept of region systems       A  Region Systems  region is simply region system is a  a subset of states of the world  A collection of regions such that no region is a subset of other regions in the collection and        An Approximation Scheme for Decision Theoretic Planning  the union of all regions equals the set of all possible  states of the world  We shall use R to denote a region and n to denote a reg ion system  Region systems are  contain the true state of the world  one that supports the function P s  ols  a   of s to the maximum degree  Where there is more than one such regions  choose  to be used to restrict the regions that the oracle can  the one that comes first in a predetermined orde ri ng  choo se to report    among the regions   There are many p o ssi ble ways to construct a region  Here are the intuitions   system  A natural way is to create a region for each  state by in clud ing its  nearby  states  this more precise   Each action has an  Let us make  intended effect   If the previous world st at e a  were known to the agent  then his current belief state b    would be proportional to P s ols  aJ  In this case  the rule minimizes extra information in the  The intended effect of move forward  for instance  is  sense th at it supports the current belief state to the  to move one step forward  We say a sta te  maximum degree   reachable in one step from another state  inform ative enough  being a landmark for instance  to  an  action whose intended effe  t    is ideally    if there is  is  when the world  is  currently in state s   to take the world into state s   A state   k  is ideally reachable ink steps from another  state so if there are state  s          BA     such that si l is  Also if the current observation is  ensure that the world state is in regio n  a   certain region  then  chosen using the rule fully supports the current  belief st ate  In such a case  no extra information is provided   ideally reachable from Si in on e step for all    i  k    Any state is ideally reachable from itself in   step   We do not claim that the rule described above is op  For any non neg ative integer k  the radius k region  is still an open problem   centered at a state  ally reachable from  s     consists of states that are ide in k or less steps   A radius k  region system is the one obtained by creating a radius k re gion for each state and then removing  one after another  regions that are subsets of others  W hen k is      the radius k region system consists of  singlet on regi on s   On the  other hand  if  there is  timal  Finding a rule that minimize extra information The  probability P Ris  o  s   a    P Ris o s  a       a k  such t hat any st ate is ideally reachable from any other  state in k or less steps  then there is only one region in the radius k region system   which is the set of all possible states       Region Observable POMDPs  the system  This subsection discusses how t he oracle should choose regions from the system  The main issu e  minimizes the amount of extra information  little  extra information as possible  the  oracle should consider what the agent alr eady knows   However  he c annot take the entire history of past ac  tions  and observations into ac count because if h e did   M  would not be a POMDP  We suggest the foll ow ing rule   For  any  non  negative  any region R   function f s  of s and  we call t he quantity supp f  R    EseR f s f EseS f s  the degree of support of f by R  If R supp orts f to degree    we say that R fully supports f  Let s  b e the previous true state of the world       being  sER R   if R is the first region s t  and for any other region  Es eRP s  ols  a      Es ER  P s   ols   a     otherwise   The region observable POMDP M  differs from the original POMDP M o nly in terms of observation  in  addition to the observation o made by himself  the an o bser vation in M  by z and Observat ion model ofM  is given by  den ote  and t he oracle is al low ed to choose region only from  To provi de a s  R  agent also receives a report R from the oracle  We shall  To complete the definition o the region observable POMDP M   assume a region system has been given  is to  of a region  chosen under the above scheme is given by  P zis a   s           P o  Rls  a          P ols  a  P Ris  o  s   a     S olving Region Observable POMDPs  For any region R  let   R be the set of belief states that  are fully supported by R  For any region system  R  let  BR      UReR Bn   Let n be the region system underlying the region  servable POMDP M    be the  previous action  and o be the current observation  The oracle should choose  among all the regions in n that  ob  It is easy to see that no m atter  what the current belief state b is  the next belief s t at e b  must be in f n  We assume that in M  the initial  belief state is in Bn  Then all possible belief states the agent  Bn  This implies that poli  n and value restricted to the subset   n of     might have are  in  cies for M  need only be defined over  iteration for  M   can  Restricting value iteration for M  to a   write z  o R     n  implies that  the t  s tep optimal value function Ui of M  is de  fined only over  Bn and the u  l  b J   maxbEBR IU   b    Bellman residual is now   Zhang and Liu       Like value iteration  restricted value iteration can be Due to region observability  re stricted implicit value iteration in M  can be done more efficiently than implicit value iteration in M  See Zhang and Liu        for details   carried out implicitly   Implicit restrict value iteration gives us a vectors  which will be henceforth denoted by Ut  It repre sents the t step optimal value function Ui b  of M  in the sense that Ut b  maxveu  L  b s V s  for any bEBn  The greedy policy for M  based on Ut is as follows  for any beBn   r   b      arg max  r b a     LP z lb a Ui b          Z   where z  stands for observation of the next time point a od b  is a shorthand for the next belief state b   lb a z      POLICY FOR THE ORIGINAL  POMDP  Suppose we have solved the region observable POMDP M   The next step is to construct a policy  r for the original POMDP M based on the solution forM   Even though it is our assumption that in the original POMDP M the agent has a good idea about the state of the world at all time  there is no guarantee that its belief state will always be in B R  There is n o oracle in M  A policy should prescribes actions for belief states in Bn as well as for belief states outside BR   An is sue here is that the policy  r  for M  is defined only for belief states in BR   Fortunately   r  can be natu rally extended to the entire belief space by ignoring the constraint bEB R  in equation      We hence define an policy  f forM as follows  for any bEB   r b      arg m axa r b  a     L P z lb  a Ui b         Z   Let k be the radius of the region system underlying M   The policy  r for M given above will be referred to as the mdius k approximate policy for M  The en tire process of obtaining the policy  including the con struction and solving of the region observable POMDP M   will be referred to as region based approximation  It is wor t hwhile  to compare this equation with equa tion      In equation      there are two terms on the right hand side  The first term is the immediate re ward for t aking action a and the second term is the discounted future reward the agent can expect to re ceive if it behaves optimally  Their sum is the total expected reward for taking action a  The action with the highest total reward is chosen   The second term is difficult to obtain  In essence  equation     approximates the second term using the optimal expected future reward the agent can receive with the help of the oracle  which is easier to compute  It should be emphasized that the presence of the oracle is assumed only in the process of computing the radius k approximate policy  The oracle is not present when executing the policy  QUALITY OF APPROXIMATION     AND SIMULATION In general  the quality of an approximate policy  f is measured by the distance between the optimal value function V   b  and the value function V    b  of  f  This measurement does not consider what the agent might know about the initial state of the world  As such  it is not appropriate for a policy obtained through region based approximation  One cannot expect such a policy be of good quality if the agent is very uncertain about the initial state of the world because it is obtained under the assumpti on that the agent has a good idea about the state of the world at all time   This section describes a scheme for determining the quality of an approximate policy in cases where the agent knows the initial state of the world with cer tainty  The scheme can be generalized to cases where there is a small amount of uncertainty about the ini tial state  for example  cases where the initial state is known to be in some small region   The agent might need to reach the goal fro m dif ferent initial states at different times  Let P s  be the frequency it will start from state sl  The qual ity of an approximate policy  r can be measured by Ls IV  s   V  s IP s   where V  s  and V  denote the rewards the agent can expect to receive starting from state s if it behaves optimally or according to  f respectively  By definition v  s     V    s  for all s  Let u be the optimal value function of the region observable POMDP M   Since more information is available to the agent in M   U  s    V s  for all s  There fore   E  U   s    V  s  P s  is an upper bound on L  V  s   V  s  P s   Let   r  be the policy for M  given by      When the Bellman residual is small   r  is close to optimal for M   and the value function v      of  f  is close to u  Con sequently  L   V    s   V  s  P s  is an upper bound on L   V  s   v r s  P s  when the Bellman residual is small enough    This is not to be confused with the initial belief state Po         An Approximation Scheme for Decision Theoretic Planning  One  way to estimate the quantity  V r s  P s    Z   V r    s      is to conduct a large number of simula tion trials  In each trial  an initial state is randomly generated according to P s   The agent is informed of the initial state  Simulation takes place in bothM and M   In M  the agent chooses  at each step  an action using  r based on the its current belief state  The ac tion is passed to a simulator which randomly generates the next state of the world and the next observation according to the transition and observation probabili ties  The observation  but not the state  is passed to  Environment A  the agent  who updates its belief state and chooses the next action  And so on and  so  forth  The trial termi  nates when the agent chooses the action declare goal or a maximum number of steps is reached  Simulation inM   takes place in a similar manner except that the  Ccut h   bviraHnt B  observations and the observation probabilities are dif ferent and actions are chosen using  r    Figure    Synthetic Office Environments   H the goal is correctly declared at the end of a trial  the agent receives a reward of the amount  Yn  where n is the number of steps  Otherwise  the agent receive  no reward  The quantity   E   V r   s   v r s  P s   can  be estimated using the difference between the average reward received in the trials for M  and the average reward received in the trials fo rM       of region system and     where there is not much un certainty  a POMDP can be accurately approximated by a region observable POMDP that can be solved ex actly  This section reports on the experiments        Synthetic Office Environments  TRADEOFF BETWEEN  Our experiments were carried using two synthetic  QUALITY OF APPROXIMATION  office environments borrowed from Cassandra et al          AND COMPLEXITY Intuitively  the larger the radius of the region system  the less the amount of extra information the oracle pro  with some minor modifications  Layouts of the  environments are shown in Figure    where squares represent locations  Each location is represented as four states in the POMDP model  one for each ori  vides  Hence the closerM  is toM and the narrower the gap between  E  v r   s P s  and Es V r s P s    entation  The dark locations are rooms connected to  pirical results  see the next section  do suggest that Ls V    s P s  increases with the radius of the region  goal location with the correct orientation   Although we have not theoretically proved this  em  system while  Ls v r   s P s   the extreme case when there  decreases with it   is  At  one region in the re  gion system that contains all the possible states of the world  M and M  are identical and hence so are  corridors by doorways   In each environment  a robot needs to reach the  tions  move forward  tum left  tum right  and declare goal  The two sets of action models given  Action  Those discussions lead to the following scheme for making the tradeoff between complexity and quality   move forward  Start with the radius   region system and increases the radius gradually until the quantity  E     V      s     tum left  V    s  P s   becomes sufficiently small or the region ob  SIMULATION EXPERIMENTS  Simulation experiments have been carried out to show that     quality of approximation increased with radius  Standard  Noisy outcomes  outcomes  tum right declare    g oal     in  the following table were used   E  v r   s P s  and E  V r s P s    servable POMDP M  becomes untractable   At each  step  the robot can execute one of the following ac  N O ll   F        F F       N        L       L L       N        R       R R       N  l O   N       F       F F      N        L           L L O l   N        R O     R R       N LO   For the action move forward  the term  F F         means that with probability      the robot actually moves two steps forward  The other terms are to be interpreted similarly  H an outcome cannot occur in a        Zhang and Liu  certain  state of the  world  then the robot is left i n the  In each state  the robot is able to perceive  in each of t hree nominal directions  front  left  and right  whether there is a doorway  wall  open  or it is undetermined  The following two sets of observation mo dels were used  Actual  Standard observations  case wall  wall           undetermined open  doorwa                       open         doorway         undetermined        wall         open         doorway         undetermined        wall  I  l      e    Noisy observations                  g  I     e z              rO ot acle       rff      r    ncWn                     r           J                         open         doorway         undetermined       wall         open         doorway         undetermined       wall         open         doorway         undetermined                                  wall          doorway          open  Envnmon A        last state before the impossible outcome       Sleps Envin n    t B          llOO           rO oracle     n orac    r             r f                            Sleps          F igure    Experiments with standard action and noisy models  The POMDPs are accurat el y approximated by region observable POMDPs with radius zero or one   Complexity of Solving the POMDPs       One of the POMDPs have     possible states while the other has      They both have    possible ob servations and   possible actions  Since the l argest POMDPs that researchers have been able to solve exactly so far have less than    states and    observations  it is safe to say no existing en ct algorithms can solve those two POMDPs   Quality of Approximation for Standard Models  To determine the quality of the radius   and radius   approximate policies for the POMDPs with s tand ard action and observation models       simulation trials were conducted using the scheme described in Section    It was assumed that the agent is equally likely to We were be able to solve the radius O and radius   start from any state  Instead of the average reward approximations  region observable POMDPs  of the over the trials  the performance of the agent is sumtwo POMDPs on a SUN SPARC o computer  The marized by the distribution of the numbers of steps it threshold for the Bellman residual was set at       took to successfully complete the trials  i e  by a funcand the discount factor at       The amounts of time tion g n  of st eps n  where for each n  g n  is the numit took in CPU seconds are collected in the following her of trials where the goal was reached and declared table  r                                      r                    in n or less steps  The average reward over the tri t         o                                                       als can be computed by Eo In  g  n  g n            F             l                      l                         l We choose the function g n  instead of the average re ward because it is more informative than the latter   I                       L                     J                 J                           J               L    We see that the radius   approximations took much longer time to solve than the radius   approximations  Also notice that the region observable POMDPs with noisy action and observation models took more time to solve that those with the standard models  We were unable to solve the radius   approximations  Other approximation techniques need to be incorpo rated in order to solve th e approximations based on region sy stems with radius larger than or equal to     Simulation results are shown in Figure    The curves instance  represent the g functions for simulations in the radius   region observable POMDPs  i e  with the help of the oracle  using their opti mal policies  In contrast  the curves rO represent the g functions for simulations in the original POMDPs  without the help of the oracle  using radius   approx imate policies  For readability  only top portions of the g functions are shown  rO oracle  for  We see that the gap between rO oracle and rO is quite   An Approximation Scheme for Decision Theoretic Planning  small in both cases  This indicates that the radius   region observable POMDPs  MDPs  are quite accu rate approximations of the original POMDPs  The  Environ     A                    I  radius   approximate policies are close to optimal for  I  the original POMDPs  The gaps between the curve   rl oracle and rl are   l  E   z  even narrower  For environment A  there is essentially  no gap  Also n otice that the curves rl lie above rO          aoo            creases with radius of region system              ro o  c          r  onoc n                           rfl    I                          and the curves rl oracle lie below rO oracle  Those support our claim that quality of approximation in                             There is a couple other facts worth mentioning  The      BOO     BOO    e  gaps are larger in environment B than in environment A  This is because environment B is more symmet ric and consequently observations a re less effective in              disambiguating uncertainty in the agent s belief about            orac    r  oi IIC   n       rfl    z  the state of the world  There were a few failures in environment A even with  the presence of the oracle  curve rl oracle   The occurred due to uncertainties in the actions  failures  models  The agent was one step away from the goal  and had an very good idea about the state of the world  An action towards the goal was taken and afterwards the agent believed strongly that the world is in the goal state  However  the action failed to effect any  Figure models       Experiments with noisy action and noisy  The POMDPs are not accurately approxi  mated by region observable POMDPs with radius zero or one   movement and the orcale s report did point this out  So a failure        POMDPs exactly   Quality of Approximation for Noisy Models  Tracing through the trials  we learned some interesting facts  In environment B  the agent  under the guidance of the radius   approximate policy  was able to quickly  One thousand trials were also conducted for the  get to the neighborhood of the goal even when starting  POMDPs with noisy action and observation models   from far way  The fact that the environment around  Results are shown in F igure  the g oal is highly symmetric was the cause of the poor performance  Often the agent was not able to deter      We see that the gaps between rl oracle and rl is sig nificantly narrower than the gaps between rO oracle and rO  especially for environment A  The curves rl lie above the curves rO and the curves rl oracle lie below rO oracle  Again  those support our claim that quality of approximation increases with radius of re  gion system  As far as absolute quality of approximation is con cerned  the radius   POMDPs are obviously very poor approximations of the original POMDPs since the gaps gaps between the curves rO oracle and rO are very wide  For Environment A  the radius   approxima tion is fairly accurate  However  the radius   ap proximation remains poor for environment B  The ra dius of region system needs to be increased  Unfortu nately  increasing the radius beyond   renders it com putationally impossible to solve the region observable  The oracle reported a region that contains both the  goal and the actual state   mine whether it was at the goal location  room   or in the opposite room  or in the left most room  or in  the room to the right of the goal location  The perfor mance would be close to optimal if the goal location had some distinct features   In environment A  the agent  again under the guidance of the radius   approximate policy  was able to reach and declare the goal successfully once it got to the neighborhood  However  it often took many unneces sarily steps before reaching the neighborhood due t o the undesirable effects o f the turning actions   Take  the lower left corner as an example  When the agent reached the corner from above  it was facing down ward  The agent executed the action turn left  Fif teen percent of the time  it ended up facing upward instead of to the right   the desired direction  The agent then decided to move forward  thinking that it was approaching the goaL But it was actually moving upward and did not realize this until a few steps later    Zhang and       Liu  The agent would perform much better there were in formative landmarks around the corners         II  T  Cheng         Algorithms for partially ob servable Markov decision processes  PhD thesis  University of British Columbia  Vancouver  BC  Canada   CONCLUSIONS  We propose to approximate a POMDP by using a region observable POMDP  The region observable POMDP has more informative observations and hence is easier to solve  A method for determining the qual ity of approximation is also described  which allows one to make the tradeoff between quality of approxi mation and computational complexity by starting with a coarse approximation and refining it gradually  Sim ulation experiments have shown that when there is not much uncertainty in the effects of actions and obser vations are informative  a P OMD P can be accurately to approximated by a region observable POMDP that  be solved exactly  However  this becomes infeasi the degree of uncertainty increases  Other ap proximate methods need to be incorporated in order to solve region observable POMDPs whose radiuses are not small   can  ble  as  Acknowledgement  Research was supported by Hong Kong Research Council under grants HKUST       E and Hong Kong University of Science and Technology under grant DAG      EG    Rl   
  and those that exploit structures in the probability ta bles  e g               We are interested in exploiting structures in the prob  This paper explores the role of independence  ability tables induced by independence of causal influ  of causal influence  ICI  in Bayesian network  ence  inference  ICI allows one to factorize a con   ICI    Heckerman  ditional probability table into smaller pieces   The concept of ICI was first introduced by       under the name causal independence   It refers to the situation where multiple causes inde  We describe a method for exploiting the fac  pendently influence a common effect  We use the term  torization in clique tree propagation  CTP    independence of causal influence  instead of  causal    the state of the art exact inference algo  independence  because many researchers have come  rithm for Bayesian networks  We also present  to agree that it captures the essence of the situation  empirical results showing that the resulting  better than the latter   algorithm is significantly more efficient than the combination of CTP and previous tech  Knowledge engineers had been using specific models of  niques for exploiting ICI   ICI in simplifying knowledge acquisition even before the inception of the concept  Keywords   Bayesian networks   causal influence   causal  independence of  independence    inference   clique tree propagation         and Heckerman  be used to simplify the structures of Bayesian networks               made the observation that ICI  into smaller pieces and showed how the Howard and Matheson  are a knowledge representation framework widely  used by AI researchers for reasoning under uncertainty  They are directed acyclic graphs where each node rep resents a random variable and is associated with a conditional probability table of the node given its par ents  This paper is about inference in Bayesian net works           enables one to factorize a conditional probability table  INTRODUCTION  Bayesian networks  Pearl  Olesen et al                 have also shown how ICI can  so that inference can be more efficient   Zhang and Poole          There exists a rich collection of algorithms   The state of the art is an exact algorithm called clique  VE algorithm    another exact inference algorithm   can be tended to take advantage of the factorization   ex  This  paper extends CTP to exploit conditional probability table factorization  We also present empirical results showing that the extended CTP is more efficient than the combination of CTP and the network simplifica tion techniques  In comparison with Zhang and Poole        this paper presents a deeper understanding ofiCI  The theory is substantially simplified   tree propagation   CTP   Lauritzen and Spiegelhalter         Jensen et al         and Shafer and Shenoy          Unfortunately  there are applications that CTP can not deal with or where it is too slow  e g         Much recent effort has been spent on speeding up inference  The efforts can be classified into those that approxi mate  e g                                                  Also known  as  junction tree propagation   and            BAYESIAN NETWORKS  A Bayesian network  BN  is an annotated directed acyclic graph  where each node represents a random variable and is attached with  a  conditional probabil  ity of the node given its parents  In addition to the explicitly represented conditional probabilities  a BN also implicitly represents conditional independence as         Zhang and Yan  sertions  Let x   x     Xn be an enumeration of all the nodes in a BN such that each node appears before its children  and let rrz  be the set of parents of a node x   The following assertions are implicitly represented   is conditionally independent of all other all other ej s given c   and      For i l         n  x  is conditionally indepen dent of variables in  xt X  x d rrz  given variables in  l  w The conditional independence assertions and the con ditional probabilities t ogether entail a joint proba bility over all the variables  As a matter of fact  by the chain rule  we have n  P z   x           zn       IJ P z lxll x           Xi      i   n  IT P z l l z          i    where the second equation is true because of the con ditional independence assertions and the conditional probabilities P x l lr r    are given in the specification of the BN  Consequently  one can  in theory  do arbitrary probabilistic reasoning in a BN     Bayesian networks place no restriction on how a node depends on its parents  Unfortunately this means that in the most general case we need to specify an expo nential  in the number of parents  number of condi tional probabilities for each node  There are many cases where there is structure in the probability ta bles  One such case that we investigate in this paper is known as independence of causal influence  ICI   The concept of ICI was first introduced by Heckerma n      The following definition first appeared in Zhang and Poole       In one interpretation  arcs in a BN represent causal relationships  the parents c   c          Cm of a node e are viewed as causes that jointly bear on the effect e    ICI refers to the situation where the causes c o c  and Cm contribute independently to the effect e  In other words  the ways by which the c  s influence e are independent       More precisely  Ct  c    and Cm are said to influence e independently if there exist random variables            and m that have the same frame  set of possible values   as e such that     C   and  There exists a commutative and associative bi nary op erato r   over the frame of e such that e    el          m  We shall refer to ei as the co ntrib ution of e  to e  In less technical terms  causes influence their common effect independently if individual contributions from differ ent causes are independent and the total influence is a combination of the individual contributions  We call the variable e a convergent variable for it is where independent contributions from different sources are collected and combined  and for the lack of a better name   Non convergent variables will simply be called regular variables  We also call   the base com bination operator of e  Different convergent variables can have difference base combination operators  The reader is referred to      for more detailed expla nations and examples of ICI  The conditional probability table P elct         em   of a convergent variable e can be factorized into smaller pieces  To be more specific  let     e  e   be the function defined by  fi e  a  e     P ei aie     INDEPENDENCE OF CAUSAL INFLUENCE     For each i  e  probabUistically depends on      Cj  s  and  for each possible value o of e  It will be referred to as the contributing factor of e  to e  Zhang and Poole      have shown that  P elc          Cm      di e  c     where  is an operator for combining factors to be defined in the following  Assume there is a fixed list of variables  some of which are designated to be convergent and others are desig nated to be regular  We shall only consider functions of variables on the list  Let f et       ec   A  B  and g e          ec A C  be two functions that share convergent variables et         e   and a list A of regular variables  B is the list of vari ables that appear only in I  and C is the list of vari ables that appear only in g  Both B and C can contain convergent variables as well as regular variables  Sup pose  i is the base combination operator of ei Then  the combination fg off and g is a function of vari ables e     e c and of the variables in A  B  and C  It is defined by       jg et a         ec ac  A B  C   f et  au        ec akl  A  B  g et  Otz          ec acz  A  C         Independence of Causal Influence and Clique Tree Propagation  constitute       heterog e  a  neous factorization of P a  b c  e  e   e    because the  joint probability can be obtained by combining those  factors in a proper order using either multiplication or the operator    The word heterogeneous is  to signify  the fact that different factor pairs might be combined  in different ways  We shall refer to the factorization as the heterogeneous factorization represented by the  BN in Figure    The heterogeneous factorization is of finer grain than the homogeneous factorization  The purpose of this  Figure    A Bayesian network   paper is to exploit such finer grain factorizations to speed up inference   write  e   We shall sometimes   gas   e     e  A B g eb      e   A C  to make explicit the arguments of f and g      The operator  is associative and commutative  W hen  In a heterogeneous factorization  the order by which factors can be combined is rather restrictive  The con  for each possible value ai of  f and g do not share convergent variables   jg is sim  ply the multiplication fg      tributing factors of a convergent variable must be com bined with themselves before they can be multiplied  FACTORIZATION OF JOINT PROBABILITIES  A BN represents a factorization of a joint probability  For example  the Bayesian network in Figure   factor izes the joint probability  DEPUTATION  P a b c et e  ea   into the  following list of factors   P a   P  b  P  c   P e la b  c   P e la  b  c   P ealel e     with other factors  This is the main issue that we need to deal with in order to take advantage of conditional probability table factorizations induced by  To alleviate the problem  we introduce the concept of deputation  It was originally defined in term of BNs       In this paper  we define it in terms of heteroge  neous factorizations themselves   In the heterogeneous factorization represented by e is to make  BN  to depute a convergent variable copy  e   of  e and  ing factors of The joint probability can be obtained by multiply ing the factors   We say that this factorization is  multiplication homogeneous because all the factors are combined in the same way by multiplication  Now suppose the e  s are convergent variables  Then their conditional probabilities can be further factorized  as  follows   replace  e with e    e  The variable  a a  in all the contribut  e  is called  the  deputy of  e and it is designated to be convergent  After depu  tation  the original convergent variable e is no longer convergent and is called a new regular variable  In con trast  variables that are regular before deputation are  called old regular variables   After deputing all convergent variables  the heteroge neous factorization represented by the BN in Figure   becomes the following list of factors   P e la  b  c      P e  a b c      P  ealell e       where the  ICI    u  e   a  I  e   b ha el  c    h  e   a      e    b ha e   c     u ei a        e  b    Ia  e  c       e  a       e b       e  c       e e        e e    P a   P b   P c     a  ea  et  a  ea e     factor      e  a   a to e   for instance  is the con  tributing factor of  We say that the following list of factors  The rest of this section is to show that deputation renders it possible to combine the factors in arbitrary order   fu  e   a  i   e   b  fta el  c           I e  a      e  b  f a e  c   fai ea e     a  ea  e    P a  P b   and P c   replace it with the corresponding new regular variable  ELIMINATING DEPUTY VARlABLES IN FACTORS  Eliminating a deputy variable e  in a factor f means to        e   Zhang and Yan  The resulting factor will be denoted by  be more specific  for any factor a list A of other variables   f e e  A   fle e  Procedure  To  of e  e  and     If x  is a new regular variable  remove from F all the factors that involve the  fle e e a  A    f e a  e  a  A   for each possible value of  e   and a list  A of  a  of  e   deputy x of  lh  combine them by   re sulting in  say  f   Add the new factor  For any factor f e   A   flz z   other variables not containing e   e   volve x   combine them by using  re sulting in  say  g  Add the new factor g  For any factor f not  to      Suppose f involve two deputy variables e and e and we want to eliminate both of them  It is evident that the order by which the deputy variables are eliminated does not affect the resulting factor  We shall denote the resulting factor by        le  e   e  e   Return   F   Theorem   The list of factors returned by sumoutc  F  x   is a  homoogeneous factorization of P xz        Xn     E    P xt  x        xn     MODIFYING CLIQUE TREE PROPAGATION  For later convenience  we introduce the concept of  homogeneous factorization in term of joint potentials     Xn be a list of variables  A joint poten tialP xl  x           Xn   is simply a non negative function     o f the variables  Joint probabilities are special joint potentials that sum to one  Consider a joint potential P et        e    x               Xn   of new regular variables e  and old regular variables Xi    F      HOMOGENEOUS FACTORIZATIONS  Let  z     z     to F  Endif     Remove from F all the factors that in  fle e e a  A    f e  a  A    for each possible value o of invol ving e   fle e  f   sumoutc F  x    A list of factors  t       fm of the e  s  their  Theorem   allows one to exploit ICI in many inference algorithms  including VE and CTP  This paper shows how CTP can be modified to take advantage of the theorem  The modified algorithm will be referred to as CTPI  As CTP  CTPI consists of five steps  namely clique tree construction  clique tree initialization  ev idence absorption  propagation  and posterior proba bility calculation  We shall discuss the steps one by  one  Familiarity with CTP is assumed      deputies eL and the xi s is a  homogeneous factor  ization  reads circle cross homogeneous factorization    Xn   if  of P el        e   Xk l  P el          e    Xk l       CLIQUE TREE CONSTRUCTION             Xn        di le et      e e      Theorem   Let  F be the heterogeneous factorization represented by a BN and let  F  be the list of factors obtained from  F by deputing all convergent variables  Then  F  is a  homogeneous factorization of the joint probability entailed by the BN  All proofs are omitted due to space limit  Since the operator  is commetative and associative  the theo rem states that factors can be combined in arbitrary order after deputation   A clique is simply a subset of nodes   A clique tree  is a tree of cliques such that if a node appear in two different cliques then it appears in all cliques on the path between those two cliques  A clique tree for a BN is constructed in two steps  first obtain an undirected graph and then build a clique tree for the undirected graph  CTPI and CTP differ only in the first step  CTP obtains an undirected graph by marrying the parents of each node  i e  by adding edges between the parents so that they are pairwise connected  and then drop directions on all arcs  The resulting undirected graph is called a moral graph of the BN      SUMMING OUT VARJABLES  Summing out a variable from a factorization is a funda mental operation in many inference algorithms  This section shows how to sum out a variable from a  homogeneous factorization of a joint potential  Let   F be a  homogeneous factorization of a joint po  In CTPI  only the parents of regular nodes  represent ing old regular variables  are married  The parents of convergent nodes  representing new regular variables  are not married  The clique tree constructed in CTPI has the following properties      for any regular node there is a clique that contain the node as well as all its parents and     for any convergent node e and each of  tential P x   x         xn   Consider the following pro  its parents x there is a clique that contains both e and  cedure   x            Independence of Causal Influence and Clique Tree Propagation            CLIQUE TREE INITIALIZATION  EVIDENCE ABSORPTION  Suppose a variable x is observed to take value a  Let Xz  a   z   be the function that takes value   when  t  a  CTPI initializes a clique tree as follows     For each regular node  find one clique that con  tains the node as well as all it s parents and attach the conditional probability of the node to that clique   and that    otherwise  CTPI absorbs the piece of evidence x a as follows  find all factors that involve x and  multiply X   a x  to those factors  Let  Xm l   O m b          Xn  be all the observed variables and  be their observed values   Let Xt           After evidence ab  sorption  the factors associated with the cliques con   a  H there is a clique that contains the node and all its parents  regard e as a regular node and  stitute a  homogenous factorization of joint poten tialPx   t       Xm Xm l am     Xn an  ofx               Xm        b  Otherwise for each parent        an  Xm be all unobserved variables   For each convergent node e  proceed as in step       x  be the contributing factor of  of e  let  x to e   f  x  e   Find one  clique that contains both e and x  attached to that clique the factor f x e    where e  is the deputy of e        CLIQUE TREE PROPAGATION  Just as in CTP  propagation in CTPI is done in two sweeps  In the first sweep messages are passed from the leaf cliques toward a pivot clique and in the sec ond sweep messages are passed from the pivot clique  After initialization  a clique is associ ated with a list  toward the leaf cliques   Unlike in CTP where mes  sages passed between neighboring cliques are factors    possibly empty  of factors   in CTPI messages passed between neighboring cliques  A couple of notes are in order  Factorizing the condi  are lists of factors   tional probability table of a convergent variable e into  Let C and C  be two neighboring cliques  Messages can be passed from C to C  when C has received mes  smaller pieces can bring about gains in inference ef  fidency because the smaller pieces can be combined with other factors before being combined with them selves  resulting in smaller intermediate factors   H  there is a clique that contains e and all its parents   then all the smaller pieces are combined at the same time when processing the clique  In such a case  we are better off to regard  e  as a regular node  representing  Procedure  this intuition  On the other hand  if all variables that appear in one factor f in the list also appear in another factor g in the list  it does not increase complexity to combine f and g   Thus we can reduce the list by carrying out  such combinations  Thereafter  we keep the reduced list of factors and combine a factor with others only when we have to   Since  is commutative and associative  the factors associated the cliques constitute a  homogenousfac torization of the joint probability entailed by the BN   End     Red uce the list F offactors and send  the  reduced list to C    visable to do the same in CTPI because the factors in  and leads to inefficiency  Experiments have confirmed  sumoutc F  Xi         for  combined at initialization and the resulting factor still involves only those variables in the clique  It is not ad  of new regular variables in the clique  Combining them   x   send Message C  C       For i l to l   F  Second  in CTP all factors associated with a clique are  all right away can create an unnecessarily large factor      all the variables in C C   Let  F b e the list of the factors associated with C and the factors sent to C from all o ther neighbors of C  Messages are passed from C t o C  by using the following subroutine   an old regular variable    volve not only variables in the clique but also deputies  Xt   sages from all the other neighbors  Suppose are       POSTERIOR PROBABILITIES  a clique and let x     x  be all unobserved variables in C  Then the factors asso  Theorem   Let C be       ciated with C and the factors sent to C from all its neighbors constitute a  homogeneous factor ization of  P Xt        Xl  Xm l   Om           Xn an    Because of Theorem    the posterior probability of any  unobserved variable x can be obtained as follows  Procedure  getProb x      Find a clique C that contains x   Letx         x  be all other variables in C  Let F be the list of the  actors associated with        Zhang and Yan  a from  summing out variable  the list h of factors  It  is the following list of one factor    P H  e e     where    l     eL e     Ea J  a fu ei  a ht  e a   Messages from cliques   and   to   are similar  To figure out the message from clique    and sent to clique  Figure      cliques       and   is   from the list of factors  Summing out     for  f  ea  Hence the message is the following list of factors   be the   J t     e e  J ts     e e   tJI et e      resulting factor      If x is a new regular flz f E r    l r   r     Else return f   E r  f   results in  tJI et e    L J  ealet e  Xe  a  et   sumoutc  F  x    End  Combine all factors in  F  Let  e   a new factor  neighbors       For i    to l   F  variable return  where the first two factors are combined due to factor list reduction  Messages from clique    are    to  cliques    and  similar   Consider computing the posterior probability of  AN EXAMPLE  ea    T he only clique where we can do this computation is clique  A clique tree for the BN in Figure   is shown in Figure        from  The message is obtained by summing out the variable  C and the factors sent to C from all its        we   J  ealet  e  X    a  et           e e   f l      e e     A clique tree for the BN in Figure     e         to clique  notice that the list of factors associated with clique      The list of factors associated with clique    and factors sent to clique    from all its  neighbors is  After initialization  the lists of factors associated   J  ealet e  Xe  a  et            eJ   e           e  e     with the cliques are as follows   lt   l    la   l      J  a ftt e a    t e  a     J  b  t  e b      e  b     J  c fta e c    a e  c     J  ea eh e      There are two variables to sum out  namely Assume  tion because they do not share convergent variables  and all its parents appear in clique      its conditional probability is not factorized  It is hence regarded as an old regular variable   ing the piece of ev idence changes the list of factors    to  Then  e   And then  e      e   is eliminated  yielding a new factor  e      chosen to be the pivot  Then mes   sages are first propagated from cliques clique     and then from clique  The message from clique               to cliques     to clique    is  and     to     and     obtained by  Finally   cf   e   e  ea le  w  is summed out  yielding a new factor    ea     is  yielding a new  l  P ealel e  Xe  a  et t et e    a e  ea   the following  And then  Suppose clique  e  and e    The first step  itself is summed out  yielding a new factor  Since  J  eslell e   is the only factor that involves et  absorb associated with clique  is to eliminate  e    e i     e  e      mul    e  e mu     e e  mua    ei e   le e     e  e  ea   Suppose e  is observed to t ake value a   e   factor  tion and combination of factors reduces to multiplica  e   is summed out before  in summing out  Several factors are combined due to factor list reduc  Also because  e       L a e  ea   e         Independence of Causal Influence and Clique Tree Propagation     EMPIRICAL COMPARISONS       WITH OTHER METHODS  This section empirically compares CTPI with CTP  We also compare CTPI with PD CTP  the combina tion of the parent divorcing transformation      and CTP  and with TT  CTP  the combination temporal transformation     and CTP    CTP          NN                  NN   number of nodes  average number of parents  average number of possible values of a node   AN PN  AN PVN   AN PN                      AN PVN                    Since clique tree construction and initialization need to be carried out only once for each network  we shall not compare in detail the complexities of algorithms in those two steps  except saying that they do not dif fer significantly  Computing posterior probabilities af ter propagation requires very little resources compared to propagation  We shall concentrate on propagation time  In standard CTP  incoming messages of a clique are  combined in the propagation module after message passing  In CTPI   on the other hand  incoming mes sages are not combined in the propagation module  For fairness of comparison  the version of CTP we implemented postpones the combination of incoming messages to the module for computing posterior prob abilities  Let us define a case to consist of a list of observed vari ables and their observed values  Propagation time and memory consumption varies from case to case  In the first three networks  the algorithms were tested using     randomly generated cases consisting of        or    observed variables  In the fourth network  only    cases were used due to time constraints  Propagation times and maximum memory consumptions across the cases were averaged  The statistics are in Figure    where the Y axises are in logscale  All data were col lected using a SPARC         i  l  The CPCS networks      are used in the comparisons  They are a good testbed for algorithms that exploits ICI since all non root nodes are convergent  The net works vary in the number of nodes  and the average number of parents of a node  and the average number of possible values of a node  variable   Their specifi cations are given in the following table  Networks Network   Network   Network   Network    K    POCTP      TTCTP       CTPI                    e           f              Networlls  CTP      PDCTP   ra   TTCTP        CTPI       e     t      e                          Networl s  Figure    Average space and time complexities of CTP  PD CTP  TT CTP  and CTPI on the CPCS net works  We see that CTPI is faster than all other algorithms and it uses much less memory  In network    for in stance  CTPI is about   faster than CTP    times faster than TT CTP  a nd     times faster than PD CTP  On average it requires  MB memory  while CTP re quires   MB  TT CTP requires   MB  and PD CTP require   MB  The networks used in our experiments are quite sim ple in the sense that the nodes have a average number of less than     parents  As a consequence  gains due to exploitation of ICI and the differences among the different ways of exploiting ICI are not very signifi cant  Zhang and Poole      have reported experiments on more complex versions of the CPCS networks with combinations of the VE algorithm and methods for ex ploiting ICI  Gains due to exploitation of ICI and the differences among the different ways of exploiting ICI are much larger  Unfortunately  none of the combina tions of CTP and methods for exploiting ICI was able to deal with those more complex network  they all ran out memory when initializing clique trees  The method of exploiting ICI described in this paper is more efficient than previous method because it di         Zhang and Yan  rectly takes advantage of the fact that ICI implies con       F   Jensen and S  K  Andersen         Approxima tions in Bayesian belief universes for knowledge based systems  in Proceedings of the Sixth Conference on Un certainty in Artificial Intelligence  Cambridge  MA  pp                  F  V  Jensen  K  G  Olesen  and K  Anderson         An algebra of Bayesian belief universes for knowledge based systems  Networks      pp                    U  Kjrerulff         Reduction of computational com plexity in Bayesian networks through removal of weak dependences  in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence  pp            ditional probability factorization  while previous meth ods make use of implications of the fact       CONCLUSIONS  We have proposed to method for  exploiting  ICI  in  CTP  The method has been empirically shown to  be more efficient than the combination of CTP and the network simplification methods for exploiting ICI  Theoretical underpinnings for the method have their roots in Zhang and Poole        and are significantly  simplified due a deeper u nderstanding of ICI         ACKNOWLEDGEMENT This  paper  has  David Poole   Kong Research and  Sino  benefited  from  discussions  with        K  G  Olesen  U  Kjrerulff  F   Jensen  B  Falck  S  Andreassen  and S  K  Andersen         A MUNIN network for the median nerve   a case study on loops  Applied Artificial Intelligence    pp                  K  G  Olesen and S  Andreassen         Specifica tion of models in large expert systems based on causal probabilistic networks  Artificial Intelligence in Medic ine    pp                  J  Pearl         Evidential reasoning using stochastic simulation of causal models  Artificial Intelligence      pp                  J  Pearl         Probabilistic Reasoning in Intelligence Systems  Networks of Plausible Inference  Morgan Kaufmann Publishers  Los Altos  CA          D  Poole  Research was supported by Hong Council under grant  HKUST      E grant  Software Research Center under  SSRC       EG    
  It is well known that conditional indepen dence can be used to factorize a joint prob ability into a multiplication of conditional probabilities  This paper proposes a con structive definition of intercausal indepen dence  which can be used to further factorize a conditional probability  An inference algo rithm is developed  which makes use of both conditional independence and intercausal in dependence to reduce inference complexity in Bayesian networks  Key words  Bayesian networks  intercausal indepen dence  definition  representation  inference     INTRODUCTION  In one interpretation of Bayesian networks  arcs are viewed as indication of causality  the parents of a ran dom variable are considered causes that jointly influ ence the variable  Pearl        The concept intercausal independence refers to situations where the mechanism by which a cause influences a variable is independent of the mechanisms by which other causes influence that variable  The noisy OR gate and noisy adder models  Good       Pearl       are examples of intercausal independence  Special cases of intercausal independence such as the OR gate model have been utilized to reduce the complexity of knowledge acquisition  Pearl       Hen rion       as well as the complexity of inference  Kim and Pearl        Beckerman        is the first re searcher to try to formally define intercausal indepen dence  His definition is temporal in nature  Based on this definition  a graph theoretic representation of intercausal independence has been proposed   noisy  This paper attempts a constructive definition  Our definition is based on the following intuition about in tercausal independence  a number of causes contribute independently to an effect and the total contribution is a combination of the individual contributions  The  definition allows us to represent intercausal indepen dence by factorization of conditional probability  in a way similar to that conditional independence can be represented by factorization of joint probability  The advantages of our factorization of conditional probability representation of intercausal independence over Beckerman s graph theoretic representation are twofold  Firstly  the symmetric nature of intercausal independence is retained in our representation  Sec ondly and more importantly  our representation allows one to make full use of intercausal independence to re duce inference complexity  While Heckerman uses intercausal independencies to alter the topologies of Bayesian networks  we follow Pearl         section        to exploit intercausal in dependencies in inference  While Pearl only deals with the case of singly connected networks  we deal with the general case   The rest of this paper is organized as follows  A constructive definition of intercausal independence is given in Section    Section   discusses factorization of a joint probability into a multiplication of conditional probabilities  and points out intercausal independence allows one to further factorize conditional probabili ties into  even smaller  factors  The fact that those  even smaller  factors might be combined by opera tors other than multiplication leads to the concept of heterogeneous factorization  HF   After some technical preparations  Sections   and     the formal definition of HF is given in section    Section   discusses how to sum out variables from an HF  An algorithm for computing marginals from an HF is given in Section    which is illustrated through an example in Section    Related work is discussed in Section        CONSTRUCTIVE INTERCAUSAL INDEPENDENCE  This sections gives a constructive definition of inter causal independence  This definition is based on the following intuition  a number of causes c   c           Cm contribute independently to an effect e and the total        Intercausal Independence and Heterogeneous Factorization  contribution is a combination of the individual contri butions  Let us begin with an example   the noisy OR gate model  Good       Pearl       Beckerman        In this model  there is a random binary variable i in correspondence to each c   which is also binary    depends on c  and is conditionally independent of any other i given the c   s  e is   if and only if all the    s are    and is   otherwise  In formula  e   V       V m Consider the case when m   and consider the condi tional probability P eict  c    For any value     of c   i              we have  P e Oic   f t  c  fJ     P t V   ict          c         P        Oict  f   P    ic  f     and  Define ft e a bct f t  deJP   a tict f t  and de fine    e     a    c   B   dJ P   a  Jc        We can rewrite the above two equations in a more compact form as follows  P e a lcl f t  c         ft e o t  Ct f dh e a    c   B          a va   a  where o   a    and a   can be either   or motivates the following definitions       This example  Let e be a discrete variable and let   be an commutative and associative binary operator over the frame n    the set of possible values of e  In the previous example    is the logic OR operator V  Let f e  x         xr Yt       y   and g  e  Xt          Xr  Zt        Zt  be two functions  where the Yi  s are different from the Zj  s  Then  the combination   g off and g is defined as follows  for any value a  of e    i eg e     a   XI        Xr  Yt         y   Zt       Zt    dej L  f e o t XJ        Xr YI        y    where e is the  e induced combination operator  The right hand of the equation makes sense because e is commutative and associative  When c           em contribute independently to e  we call e a bastard variable  A non bastard variable is said to be nor mal  We also say that f  e  c   is  a description of  the contribution by c  to e  Intuitively  the base combination operator  e g  V  determines how contributions from different sources are combined  while the induced combination operator is the reflection of the base operator at the level of conditional probability   P e llcFf t  c  P   P   V    c   f t  c  f    P     let f t P    Jcz f     P t  OJct  f t P   lic  f     P    ict f t P   llc  f            Here is our constructive definition of intercausal in dependence  We say that c         Cm contribute in dependently to e or e receives contributions indepen dently from Ct     Cm if there exists a commutative and associative binary operator   over the frame of e and real valued non negative functions It  e  c          f m  e  Cm  such that  X  Because of equation      the noisy OR gate model is an example of constructive intercausal independence  with the logic OR Vas the base combination operator  As another example  consider the noisy adder model  Beckerman        In this model  there is a random Variable i in correspondence to each Cj j e  depends On Cj and is Conditionally independent of any other ej given the c  s  The   s are combined by the addition operator     to result in e  i e  e            em  To see that e is a bastard variable in this model  let the base combination operator  e be simply     and let the description of individual contribution    e  c   be as follows  for any value o  of e and any value f  of C         e  o   c  f   def P e  aJc   B   Then it is easy to verify  that  equation     is  satisfied   It is interesting to notice the similarity between equa tion     and the following property of conditional in dependence  if a variable x is independent of another variable z given a third variable y  then there exist non negative functions f x  y  and g y  z  such that the joint probability P x  y  z   is given by P x y  z  f x  y g y  z          Those who are familiar with clique tree propagation  We shall refer to   as the base combination operator and e as the  e induced combination operator  We would like to alert the reader that  e combines values of e  while e combines functions of e  It is easy to see that the induced operator e is also commutative and associative   may remember that the  first thing to do in constructing  a clique tree from a Bayesian network is to  marry  the     parents of each node variable             Lauritzen and Spiegehalter  As implies by the word  bastard   the parents of  a bastard node will not be married  This is because the conditional probability of a bastard node is factorized into a bunch of factors  each involving only one parent         Zhang and Poole  In     conditional independence allows us to factorize a joint probability into factors that involve less vari ables  while in     intercausal independence allows us to factorize a conditional probability into a bunch of factors that involve less variables  The only difference lies in the way the factors are combined  Conditional independence has been used to reduce in ference complexity in Bayesian networks  The rest of this paper investigates how to use intercausal indepen dence for the same purpose     FACTORIZATION OF JOINT PROBABILITIES  This section discusses factorization of joint probabili ties and introduces the concept of heterogeneous fac torization  HF   A fundamental assumption under the theory of proba bilistic reasoning is that a joint probability is adequate for capturing experts  knowledge and beliefs relevant to a reasoning under uncertainty task  Factorization and Bayesian networks come into play because joint probability is difficult  if not impossible  to directly assess  store  and reason with   Let P xb x        xn  be a joint probability over variables x   x         Xn  By the chain rule of probabilities  we have P x   X     Xn   P xi P x lxt      P xnlxl       Xn                 For any i  there might be a subset        x         r  I  such that X  is conditionally independent of all the other variables in  x         X  d given the variables in       i e P x lxb      x  I  P x l r    Equation     can hence be rewritten as n  P x  x           Xn   IT P x l r               Equation     factorizes the joint proba bility P z     x        z     into a multiplication of factors P xd Ti  While the joint probability involves all then variables  the factors usually involves less than n vari ables  This fact implies savings in assessing  storing  and reasoning with probabilities  A Bayesian network is constructed from the factoriza tion as follows  construct a directed graph with nodes x   x            r   such that there is an arc from  rj to x  if and only if Xj E       and associate the conditional probability P x l r   with the node x   P x      Xn  is said to be the joint probability of the Bayesian network so constructed  Also nodes in      are called parents of  r    The above factorization is homogeneous in the sense that all the factors are combined in the same way  i e by multiplication   Figure    A Bayesian network  where e  and e  re ceive contribution independently from their respective parents  Let x          x m  be the parents of x   If x  is a bastard variable with base combination operator  i  then the conditional probability P xd r   can be further factor ized by  where  is the    induced combination operator  The fact that   might be other than multiplication leads to the concept of heterogeneous factorization  HF   The word heterogeneous reflects the fact that differ ent factors might be combined in different manners  As an example  consider the Bayesian network in Fig ure    The network indicates that P a  b  c  e  e  ea  y  can be factorized into a multiplication of P a   P b   P c   P e la b   P e la  b  c   P   eale    e     and P ylea   Now if the e  s are bastard variables  then there exist base combination operators  i  i l        such that the conditional probabilities of the e   s can be further factorized as follows  P e la  b  P e la b c  P ealet  e    fu el  a dt  el  b      e   a   n e   b    a e   c   at ea el a a  e  el   where fu et  a   for instance  denotes the contribution by a to e   and where the i s are the combination operators respectively induced by the  i s  The factorization of P a  b  c  et  e    e   y  into the factors  P a   P b   P c   P ylea   fu el  a       b   e   a       e   b   ha e    c   fat ea  et   h    t  e  and  a   e    e   is called the HF in correspondence to the Bayesian network in Figure    We shall call the fii  s heterogeneous factors since they might be com bined by operators other than multiplication  On the other hand  we shall say that the factors P a   P b   P c   and P yle   are normal    Intercausal Independence and Heterogeneous Factorization       To prevent I e   eD from being mistaken to be the con tribution by ei to e   we shall always make it explicit that I e   e  is a normal factor  not a heterogeneous factor   COMBININ G FACTORS THAT     IN V OLVE M OR E THAN ONE BAS TARD VARIABLE Even though deputation guarantees that every hetero  Figure    The Bayesian network in Figure deputation of bastard nodes         after the  geneous factor involves only one bastard variable at the beginning  inference may give rise to factors that involve more than one bastard variable  In Figure    for instance  summing out the variable a results in a factor that involves both e  and e   This section in troduces an operator for combining such factors  Suppose e         eo are bastard variables with base opera combination tor  t         k Let f et      e   xt      xr  Yl       y   and g   et      e  xt x     zt        zt   be two func tions  where the xi s are normal variables and the yj s  DEPUTATION OF BAS TARD     NODES     Consider the heterogeneous factor h   es  e   from the previous example  It contains two bastard variables e  to e   As we shall see later  it is desirable for every heterogeneous factor to contain at most one bastard variable  The concept of deputation is introduced to guarantee this   are different from the zr s  they can be bastard as well as normal variables   Then  the combination fg of f and g is defined as follows  for any particular value a  of e     fg el  a          eo O i   Xt       Xr  Yl         y   Zt          Zt    Let e be a bastard node in a Bayesian network  The deputation of e is the following operation  make a copy e  of e  make the children of e to be children of e     make e a child of e  and set the conditional probability P e le  as follows   P e Ie            if e   e  otherwise       We shall call e  the deputy of e  We shall also call P ele   the deputing function  and rewrite it as I e  e   since P ele   ensures that e and e  be the same  The Bayesian network in Figure   becomes the one in Figure   after the deputation of aU the bastard nodes  We shall call the latter a a deputation Bayesian net  work   Proposition   Let N  be a Bayesian network  and let N  is the Bayesian network obtained from N  by the  deputation of all bastard nodes  Then the joint proba bility of N can be obtained from that of N  by summing out all the deputy variables     In Figure    we have the heterogeneous factors h  es el  and f   es e    which involves two bastard variables  This may cause confusions and is undersir able for other reasons  as we shall see soon  After dep utation  each heterogeneous factor involves only one bastard variable  As a matter of fact  fst es  et  and fs  es e   have become fst es eD and fs  es e     f et  au          eo akl  x       Xr  Yl    Ys  X g e           ek          Xt        Xr  Zt        Zt                      A few notes are in order  First  fixing a list of bas tard variables and their base combination operators  one can use the operator to combined two arbitrary functions  In the following  we shall always work im plicitly with a fixed list of bastard variables  and we shall refer to  as the general combination operator  Second  when        k       th is definition reduces to equation  Third  since the base combination operators are com mutative and associative  the operator  is also com mutative and associative  Fourth  when off and g        k         fg  is simply the multiplication  Combining all the Heterogeneous Factors in a Bayesian networks  Equipped with the general combination operator   we now consider combining all the heterogeneous fac tors of the Bayesian network in F igure    Because of the third note above  we can combine them in any order  Let us first combine fu et  a   with  t  e   b       e   a  with h  e   b  and hs e   c   and fst es  eD        Zhang and Poole   F  In the following  we shall also say that the of the function F X    We now combine the resulting conditional probabili ties  Because of the fourth note  the combination of P etia  b   P e  la   b  c    and P e lei e   is their multi plication  So  the combination of all the heterogeneous factors of the Bayesian network in Figure   is simply the multiplication ofthe conditional probabilities of all the bastard variables  This is true in general   Suppose N is a deputation Bayesian network  Sup pose  F is the HF that corresponds to N   F has two interesting properties   In a deputation Bayesian network  the multiplication of the conditional probabilities of all the bastard variables is the same as the result of com bining of all the heterogeneous factors  D  Proposition         is an  HF  with     e   e    Because of the second note  we have  H t  et a b     P eda b   P e la b  c    hth     e  a b  c  P e  e  eD  ht    e  e  eD  HF s in Correspondence to Deputation Bayesian Networks  First  according to Proposition   the combination of all the heterogeneous factors is the multiplication of the conditional probabilities of all the bastard variables  Thus  the joint of  F is simply the joint probability of  N   The joint of the HF that corresponds to a deputation Bayesian network N is the same as the joint probability of N   Proposition    Note that in Figure    since ht e  e   and h  e   e   involve two bastard variables  the combination fn et a        f   e  c   ht e  et    a  e   e   would not the same as the multiplication of the condi tional probabilities of the bastard variables   To reveal the second interesting property  let us first define the concept of tidness  An HF is tidy if for each bastard variable e  there exists at most one normal factor that involves e  Moreover  this factor  if exists  involves only one other variable in addition to e itself   This is why we need deputation  deputation allows us to combine the heterogeneous factors by a single com bination operator   which opens up the possibility of combining the heterogeneous factors in any order we choose  This flexibility turns out to be the key to the method of utilizing intercausal independence we are proposing in this paper   An HF that corresponds to a deputation Bayesian net work is tidy  For each bastard variable e  I e  e   is the only one normal factor that involves e  and this factor involves only one other variable  namely e       HETEROGENEOUS FACTORIZATION  We now formally define the concept of heterogeneous factorization  Let X be a set of discrete variables  A heterogeneous factorization  HF  F over X consists of    A list e          em of variables in X that are said to be bastard variables  Associated with each bas tard variable ei is a base combination operator  i  which is commutative and associative     A set  Fo of heterogeneous factors  and    A set  F  of normal factors  We shall write an HF as a quadruplet  F   X    e    t              em   m        Fo  Ft   Variables that are not bastard are called normal  In an HF  the combination F  of all the heterogeneous factors is given by      The joint F X  of an HF is the multiplication of Fa and all the normal factors  In formula F deJ Je F f   IT g     g         Tidy HF s do not have to be in correspondence to a deputation Bayesian network  As a matter of fact  we shall start with a tidy HF that corresponds to a dep utation Bayesian network  and then sum out variables from the HF  We shall sum out variables in such a way such that the tidness is retained  Even though the HF we start out with corresponds to a deputation Bayesian network  after summing out some variables  the resulting tidy HF might no longer correspond to any deputation Bayesian network  However  we shall continue to use the terms deputy variable and deputing function     SUMM ING OUT VARIABLES FROM TIDY HF S  Let F X  be a function  Suppose A is a subset of X  The projection F A  of F X  onto A is obtained from F X  by summing out all the variables in X  A  In formula      F A  dJ I  F X   X A  When F X  probability   is  a joint probability  F A  is a marginal  Summing variables out directly from F X  usually re quire too many additions  Suppose X contains n vari ables and suppose all variables are binary  One needs to perform  n     additions to sum out one variable    Intercausal Independence and Heterogeneous Factorization  A better idea is to sum out variables from an factoriza tion of F X  if there is one  This section investigates how to sum out variables from tidy HF s  The follow ing two lemmas are of fundamental importance  and they readily follow the definition of the general com bination operator    Both m llltiplication and   are distributive w  r  t summation  More specifically  s llppose f and g are two functions and variable x appears in f and not in g   Then  Lemma         Er fg      Er f g  and  summing out z does not affect the deputing functions  Therefore   F  remains tidy  When z is a bastard variable  summing out z will not affect the deputing functions of any other bastard vari ables  Therefore   F  also remains tidy    general  a variable can appear in more than one nor mal and heterogeneous factors  The next proposition reduces the general case to the case where the variable appear in at most two factors  one normal and one heterogeneous   In  F X   and let z be a variable in X  Let It         fm be all the heteroge neous factors that involve z and let           Un be all the normal factors that involve z  Define  Proposition   Let  F be an HF of     E    Ig   CEr f  g         f aej            The following lemma spells out two conditions under which multiplication and  are associative with each other  Lemma       h fg   hf  g   n  g aeJ IT Ui j l  Let f and g be two functions   If h is a function that involves no bastard vari ables  then        Let  F  be the HF obtained from  F by removing the fi  s and the Ui  s  and by adding a new heterogeneous factor f and a new normal factor g  Then     F  is also an HF of F X   and f and  g are the only two factors that involve z  In particular  when either m O or n O  there is only one factor in  F  that involves z      If h is a function such that all the bastard variables in  h  f and not in g  h fg     hf g   appear only in  then           We now proceed to consider the problem of summing  out variables from a tidy HF in such a way that the tid ness is retained  First of all the following proposition deals with the case when the variable to be summed out appears in only one factor    Let  F be an HF of F X  and is tidy  Suppose z is a variable that appears only in one factor   A   normal or heterogeneous  Define h  Proposition  h A    z    dJ Lf A   z  Let  F  be the HF obtained from  F by replacing  f with  h   Then   F  is a HF of F X    z     the projection of F X  onto X  z    Moreover if z is not a dep llty    variable  then  F  remains tidy   Proof        The first part of proposition follows from  Lemma     For the second part  since z is not a deputy variable  it can be either a non deputy normal variable or a bas tard variable  When z is a non deputy normal variable   The factor h is heterogeneous or normal if and only if f is       If z  is not a dep llty variable  then when  F is tidy  so is  F    Proof  The first part of the proposition follows from  the commutativity and associativity of multiplication and of the general combination operator    For the second part  since z is not a deputy variable  it can either be a non deputy normal variable or a bas tard variable  When z is a non deputy normal vari ables  the operations performed by the proposition do not affect the deputing functions  Thus   F  remains tidy  When z is a bastard variable  the deputing functions are not affect either  Because for each bastard variable e  its deputing functions is the only normal factor that involves e  So   F  also remains tidy  D  The following proposition merges a normal factor into a heterogeneous factor  Proposition   Let  F be an HF of F X  and is tidy   Suppose z is a variable that appears in only one normal factor g and only one heterogeneous factor f  Define h by  h aeJfg  Let  F  be the HF obtained from  F by removing g and f  and by adding a heterogeneous judor h  If z is not        Zhang and Poole  a deputy variable  then the joint of  F  is also F X  and  F  is tidy  Moreover  h is only one factor in  F   that involves  z   Proof  We first consider the case when z is a non deputy normal variable  Because the tidness of  F  g involves no bastard variables  According to Lemmas         the joint ofF  is also F   Since g is not a deputing function  the operation of combining f and g into one factor does not affect the deputing functions  Hence   F  remains tidy  Let us now consider the case when z is a bastard vari able  Since  F is tidy  g must be the deputing function of z  Since f is the only heterogeneous factor that in volves z  all other heterogeneous factors do not involve z  According Lemma        the joint of  F  is also F  After combining f and g into a heterogeneous factor  there is no normal factor that involve z  Also  the deputing functions of the other bastard variables are not affected  Hence   F  remains tidy  D  The  in Bayesian networks  To this end  we need only con  sider deputing functions I e  e   such that I e  e       if e   e  and I e  e       otherwise  Let us say such deputing functions are identifying  Since for any func tion f e  e   x         xn      LI e  e  f e  e    Procedure PROJECTION    F  A       AN ALGORITHM  This section presents an algorithm for computing pro jections of a function F X  by summing variables from a tidy HF of F X   Because of Proposition    the al gorithm can be used to compute marginal probabili ties  and hence posterior probabilities  in Bayesian net works  To sum out the variables in X  A  an ordering needs to be specified  Lauritzen and Spiegehalter        In the literature  such an ordering is called an elimi nation ordering  which can be found by heuristics such as the maximum cardinality search  Tarjan and Yannakakis       or the maximal intersection search  Zhang        At the end of the last section  we said that a deputy variable should be summed out only after the corre sponding bastard variable has been summed out  If e is a bastard variable in A  what should we do with its deputy variable e   The paper is concerned with intercausal independence  p   Input      F   A tidy HF of a certain func tion F X  such that all the deputing functions are identifying   a tidy HF  bastard variables and non deputy normal variables  You may ask  how about deputy variables  As it turns out  after summing out a bastard variable e  its deputy e  becomes a non deputy normal variable  So  we can also sum out deputy variables  we just have to make sure to sum out a deputy variable after the corresponding bastard variable has been summed out  variable e  needs to be summed out after the corre sponding bastard variable e  As a matter of fact  sum ming out e   before e is the inverse of the deputation of e  But we have shown at the end the Section   that deputation is necessary       f e  e  x            Xn    we can handle the deputies of bastard variables in A as follows  wait till after all the other variables outside A have been summed out and all the heterogeneous factors have been combined  then simply remove all the deputing functions  replace each occurrence of a deputy variable with the corresponding bastard vari able  This operation can be viewed as the inverse of deputation   above three propositions allow us to sum out  from  It is possible to intuitively understand why a deputy  Xt        Xn  e      A  A subset of X      p   An elimination ordering consist ing all the variables other than the variables A and their deputies  In p  a deputy variable e comes right after the corresponding bastard vari able e      Output  onto A   F A    The projection of F     If p is empty  combine all the het erogeneous factors by using the gen eral combination operator   resulting in f  remove all the deputing functions and replace each occurrence of a deputy variable with the corresponding bastard variable  multiply f together with all the normal factors  output the resulting fac tion  and exit     Remove the first variable  z  from the or  dering p      Remove from  F all the heterogeneous factors ft         fl that involve z  and set  f dej  f l k  Let B be the set of all the variables that appear in f     Remove from  F  all the normal factors            Om that involve z  and set  m  D  deJ  IT  j  j  l  Let C be the set of all the variables that appear in g    Intercausal Independence and Heterogeneous Factorization     If k O  define a function  The bastard variable e  appears in heterogeneous fac tors      e ei  and  a  e    e   and in the normal factor I  e  e    After summing out eg the factors become   h by  h C  z   def Lg C   Add h into F  as        T bt eL e e   fu el a   Fo     e  a       e   b   ha e  c    F   P a   P b   P c   P  yie   h e   e       a normal factor      Else if m O  define a function h by    h B  z   deJ L f B     t   e   b   l  et ei    where Add h into F as a heterogeneous factor      Else define a function h by  h BUC  z    de  L f B g C   Add h into F as a heterogeneous factor  Endif    Recursively call PROJECTION F   A  p   The correctness of PROJECTION is guaranteed by Propositions       and     summing out a variable re quires combinin g only the factors that involve the vari able  This is why PROJECTION allows one to ex  Note that in the algorithm  ploit intercausal independencies for efficiency gains  If one ignores intercausal independencies  to sum out one variable one needs combine all the conditional proba bilities that involve the variable  There is a gain in effi ciency by using PROJECTION because intercausal in dependence allows one to further factorize conditional probabilities into factors that involve less variables  In Figure    for instance  summing out a requires com bining P e tla  b  and P e la  b  c  when intercausal in dependencies are ignored  there are five variables in volved here  By using PROJECTION  one needs to combine f    e     a  and     e  a   there are only three variables involved in the case  Finally  we would like to remark that the algorithm is an extension to a simple algorithm for computing marginal probabilities from a homogeneous factoriza tion  Zhang and Poole         tPt e  e e   def L      ea eDfaz e   e   I  e   e       Now e is the next to sum out  e appears in the heterogeneous factor t J  and the normal factor P yle   After summing out e  the factors become      where   f   e  e  y  def L if    e  e e  P yle    e   Next  summing out     Suppose the elimination ordering pis   e  c      e   e  a  b  e    Initially  the factors are as follows    Fo     u et a    t  e  b   ht e   a         e  b  h  e  c   h  e  eD  h  e  e     F     P a   P b   P c   P yle   h el eD  l  e  e   l  ea  e     tfi  e e y   Fa   tP  e   e    h  e   b       e  c      t  e  b    F   P b   P c   I  e   ei   I  e   e     a  Then  summing out    work N shown in Figure    Since P e ly O  can be readily obtained from the marginal probability P e  y   we shall show how PROJECTION computes the latter   gives us   a    To illustrate PROJECTION  consider computing the conditional probability P e ly O   in the Bayesian net  a  where  An example      Fo N  eL e y    u et a    t  e  b       e  a       e  b   h  e   c     F    P a   P b   P c   It et  ei   I  e  e     b gives  us   Fo    tj    e   e        e  c    Ft   P c   lt et eD  I  e  e     where  if   et e     def    L P b   t  et b f   e  b   L P b  t  e  b h  e   b   b  The next variable on p is e   which appears in hetero geneous factors  tj     e  e   and  tj     e    e   and normal factor h  e   ei   After summing out e  the factors be come         Zhang and Poole  together with conditional independencies  to further reduce inference complexity   where  t Js e   e  deJ     It  et  eD t Ja e   e  t J  e   e         Due to space limit  we have to discontinue the example here  Hopefully  the following two points shoul be be clear now  F irst  in summig out one variable  PRO JECTION combines only the factors that involve the variable  Second  since  not have  e   is a bastard variable  we usually do  Acknowledgement The authors are grateful for the three anonymous re viewers for their valuable comments and suggestions  Research is supported by NSERC Grant OGP        and by a travel grant from Hong Kong University of Science and Technology   
  While influence diagrams have many ad vantages as a representation framework for Bayesian decision problems  they have a se rious drawback in handling asymmetric de cision problems  To be represented in an influence diagram  an asymmetric decision problem must be symmetrized  A consid erable amount of unnecessary computation may be involved when a symmetrized influ ence diagram is evaluated by conventional al gorithms  In this paper we present an ap proach for avoiding such unnecessary compu tation in influence diagram evaluation      INTRODUCTION  Decision trees were used as a simple tool both for prob lem modeling and optimal policy computation in the early days of decision analysis  Rai ffa        A deci sion tree explicitly depicts all scenarios of the problem and specifies the  utility  the agent can get in each sce nario  An optimal policy for a decision problem can be computed from the decision tree representation of the problem by a simple  average out and fold back  method  Though conceptually simple  decision trees have First  the depen a number of drawbacks  dency  independency relationships among the variables in a decision problem cannot be represented in a deci sion tree  Second  a decision tree specifies a particular order for the assessment on the probability distribu tions of the random variables in the decision problem  This order is in most cases not a natural assessment order  Third  the size of a decision tree for a decision problem is exponential in the number of variables of the decision problem  Finally  a decision tree is not easily adaptable to changes in a decision problem  If a slight change is made in a problem  one may have to draw a decision tree anew   Scholar of Canadian Institute for Advanced Research  David Poole   Department of Computer Science UBC Vancouver B  C  Canada V T  Z  E mail  qi cs ubc ca  Influence diagrams were proposed as an alternative to decision trees for decision analysis  Howard and Math eson        Miller et  al         As a representation framework  influence diagrams do not have the afore mentioned drawbacks of decision trees  The influence diagram representation is expressive enough to explic itly describe the dependency  independency relation ships among the variables in the decision problem  it allows a more natural assessment order on the proba bilities of the uncertain variables  it is compact  and it is easy to adapt to the changes in the problem  However  in comparison with decision trees  influence diagrams have one disadvantage in representing asym metric decision problems  Covaliu and Oliver       Fung and Shachter       Phillips       Shachter       Smith et al         Decision problems are usually asymmetric in the sense that the set of possible out comes of a random variable may vary depending on different conditioning states  and the set of legitimate alternatives of a decision variable may vary depending on different information states  To be represented as an influence diagram  an asymmetric decision problem must be  symmetrized  by adding artificial states and assuming degenerate probability distributions  Smith et al         This symmetrization results in two prob lems  First  the number of information states of de cision variables are increased  Among the informa tion states of a decision variable  many are  impos sible   having zero probability   The optimal choices for these states need not be computed at all  How ever  they are computed by conventional influence di agram evaluation algorithms  Shachter       Smith et a         Shachter and Peot       Zhang and Poole       Zhang et al         Second  for each informa tion state of a decision variable  because the legitimate alternatives may constitute only a subset of the frame of the decision variable  an optimal choice is chosen from only a subset of the frame  instead of the en tire frame  However  conventional influence diagram algorithms have to consider all alternative in order to compute an optimal choice for a decision in any of its information states  Thus  it is evident that conven tional influence diagram evaluation algorithms involve unnecessary computation         Qi  Zhang  and Poole  In this paper  we present an approach for overcom ing the aforementioned disadvantage of influence di agrams  Our approach consists of two independent components  a simple extension to influence diagrams and a top down method for influence diagram evalu ation  Our extension allows explicitly expressing the fact that some decision variables have different frames in different information states  Our method  similar to Howard and Matheson s         evaluates an influence diagram in two conceptual steps  it first maps an in fluence diagram into a decision tree  Qi       in such a way th at an optimal solution tree of the decision tree corresponds to an optimal policy of the influence diagram  Thus the problem of computing an optimal policy is reduced to the problem of searching for an optimal solution tree of a decision tree  which c an be accomplished by various algorithms  Qi        Like Howard and Matheson s method  ours avoids comput ing optimal choices for decision variables in imp ossible states  Furthermore  our method has two advantages over Howard and Matheson s  First  the size of the intermediate decision tree generated by our method is much smaller than that generated by Howard and Matheson s for the same influence diagram  Second  our method provides a clean interface between in fluence diagram evaluation and Bayesian net evalu ation so that various well established algorithms for Bayesian net evaluation can be used in influence di agram evaluation  This method works for influence diagrams with or without our extension  The rest of th is paper is organized as follows  The next section introduces influence diagrams  Section   uses an example to illustrate the disadvantage that influence diagrams and their solution algorithms have with asymmetric decision problems  In Section    we present our approach for overcoming the disadvantage  Section   gives an analysis on how much can be saved by exploiting asymmetry in decision problems  Section   discusses related work and Section   concludes the paper     INFLUENCE DIAGRAMS  The following definition for influence diagrams is bor rowed from  Zhang et a          An influence diagram I is defined as a quadruple I   X  A  P U  where    X  A  is a directed acyclic graph with node set X and arc set A  The node set X is partitioned into random node set C  decision node set D and value node set U   All the nodes in U have no child  Each decision node or random node has a set  called the frame  asso ciated with it  The frame of a node consists of all the possible outcomes of the  decision or random  variable denoted by the node  For any node x E X   we use  r  x  to de note the parent set of node x in the graph and use   lx to denote the frame of node x   For any subset  J  C U D   we use   lJ to denote the Cartesian product Ilxo   lx      Pis a set of probability d istributions P cl r c   for all c E C  For each o E   lc and s E       c   the distribution specifies the conditional probability of event c   o  given that   r c    s   U is a set  gv         v        Rlv E U  of value func for the value nodes  w here R is the set of the real   tions  For a decision node d   a m apping           d       ld  is called a decision function for d   The set of all the decision functions for d   denoted by   is called the decision function space for d   Let D  d        dn  be the set of decisi on nodes in influence diagram I  The Cartesian product  TI l  is called the policy space of I        For a decision node d   a value x E      cd   is called an information state of d   and a mapping      n    d        ld  is called a decision function for d   The set of all the decision functions for d   denoted by    is called the decision function space for d   The Cartesian prod uct of the decision function spaces for all the decision nodes is called the policy space of I  We denote it by Given a p olicy                 k  E for I  a probabil P  can be defined over the random nodes and the decision nodes as follows   ity  k  P  C  D    II P cl r c   II P   d l r d      cec       where P cl r c   is given in the specification of the in fluence diagram  while P   d l r d    is given by b  as follows   Pa  d l r d           when b   r d    otherwise     d         For any value node v   r v  must consist of only deci sion and random nodes  since value nodes do not have children  Hence  we can talk about Pc  r v    The expectation of the value node v under Pa  denoted by Eo v   is defined as follows   E  v         P   r v  fv  r v       v   The summation Eo   I veu E  v  is called the value of I under the policy    The maximum of Ea over all the possible policies   is the optimal expected value of I  An optimal policy is a policy that achieves the optimal expected value  To evaluat e an influence diagram is to   In e  this paper  for any variable set J and any element  E OJ  we use J    e  to denote the set of assignments  that assign an element of  in J   e  to the corresponding variable   Solving Asymmetric Decision Problems with Influence Diagrams  determine its optimal expected value and to find an optimal policy  An influence is regular if there e xis ts a total ordering among all the decision nodes  The results presented in this paper are applicable to regular stepwise decompos able influence diagrams  Q i       Qi and Pool e       Zhang and Poole        We shall   however  limit the exposition only to regular influence diagrams with a single value node for simplicity      WHY INFLUENCE DIAGRAMS ARE NOT GOOD FOR ASYMMETRIC DECISION PROBLEMS  In this section  we illustrate by an example the dis advantages of conventional influence diagrams with asymmetric decision problems  We use the used car buyer problem  Howard       because it is a typical asy m metric decision problem and it has bee n used by other researchers  Shenoy       S mi t h et al               THE USED CAR BUYER PROBLEM  Joe is considering to buy a used car  T h e marked price is        while a three years old c ar of this model  worths        if it has no defect   Joe is uncertain  whether the car is a  peach  or a  lemon   But Joe knows that  of the ten major subsystems in the car  a peach has a defect in only one subsystem whereas a lemon has a defect in six subsystems  Joe also knows that the probability for the used car being a peach is     and the probability for the car being a lemon is      Finally  Joe knows that it will cost him     to repair one defect and      to repair six defects             INFLUENCE DIAGRAM REPRESENTATION FOR THE USED CAR BUYER PROBLEM  An influence diagram for the used car problem is shown in Fig     The random variable CC represents the car s condition  The frame for CC has two elements  peach and lemon  The variable h as no parent in the graph  thus  we specify its prior probability distribu tion in Table    The decision variable T  represents the first test de CISion  The frame for T  has four elements  nt  st  f  e and tr  repr esent in g respectively the options of performing no test  testing the steering subsystem alone  testing the fuel and electrical subsystems  and testing the transmission subsystem with a possibility of testing the differential subsystem next   The ran dom variable R  represents the first test re sults  The frame for R  has four elements  nr  zero  one and two representing respectively the four possi ble outcomes of the first test  no result  no defect  one defect and two defects  The probability distribution of the variables  conditioned on T  and CC  is given in Table    The de c isi on variable T  represents the second test de ctswn  Th e frame for T  has two elem e nt s   nt and diff  de n ot i ng the two options of performing no test and testing the differential subsystem  The random variable R  represents the second test re sults  The frame for the random variable R  has three el ement s   nr  zero and one  representing respectively the three possible outcomes of the second test  no re sult  no defect and one defect  The probability distri bution of the vari ables conditioned on T   R   T  and cc  is gi ven in Table       The decision variable B represents the purchase deci sion  The frame for B h as three elements  b  b and g  d enoting resp ectiv ely the options of not buying the car  bu yi ng the car without the anti lemon guarantee and buying the car with the anti lemon guarantee   Observ ing Joe s concern about the pos s i bil i ty that the car may be a lemon  the dealer offers an  anti lemon guarantee  option  For an additional      the anti lemon guarantee will cover the full repair cost if the car is a lemon  and cove r half of the repair cost oth erwise  At the same time  a mechanic suggests that some  mech  examination should h p  J ee   detef  mine the car s condition  In particular  th e mechanic gives Joe three alternatives  test the steering subsys tem alone at a cost of      test the fuel and electrical subsystems at a total cost of      a two test sequence in which  the transmission subsystem will be tested at a cost of      and after knowing the test result  Joe can decide whether to test the differential subsystem at an additional cost of     All tests a re guaranteed to detect a defect if one exists in the subsystem s  being tested   Figure    An influence diagram for the used car buyer problem The used car buyer problem is asymmetric in a num b er of aspects  F irst  the set of the possible outcomes of the first test result varies  depending on the choice for the first test  If the choice for the first test is nt         Qi  Zhang  and Poole  Table    The prior probability distribution of the car s condition P cc   Table    The probability distribution of the first test result P RtiT   cc  T   nt  nt st st  cc          Rl  pro b  nr       others     nr     two     zero       st  pe ach  st  peach  one  st  lemon  zero       st  lemon  one       f e          nr  zero       f e  peach  f e  peach  one  f e  peach  two  f e  lemon  zero        f e  lemon  one        f e  lemon  two                  then there is only one possible outcome for the first test result   nr  representing no result   If the choice for the first test is st or tr  then there are two possible outcomes for the first test result  zero and one  rep resenting no defect and one defect  respectively   If the choice for the first test is fl e  then there are three possible outcomes for the first test result  zero  one and two  representing no defect  one defect and two defects  respectively   However  in the influence dia gram representation  the frame of the variable R  is a common set of outcomes for all the three cases  The impossible combinations of the test choices and the test results are characterized by assigning zero prob ability to them  as shown in Table     A similar dis cussion is applicable to the variable R   Second  from the problem statement we know that testing differen tial subsystem is possible only in the states where the first test performed is on the transmission subsystem  However  in the influence diagram representation  it appears that the second test is possible in any situa tion  while the fact that the option of testing differ ential subsystem is not available in some situations is characterized by assigning unit probability to outcome nr of the variable R  conditioned on these situations  Third  when we examine the information states of the decision variable T   we will see many combinations of test options and test results are impossible  For ex ample  if Joe first tests the transmission subsystem  it is impossible to observe nr and two  If the influence diagram is evaluated by conventional algorithms  an optimal choice for the second test will be computed for each of the information states  including many im possible states  Similar argument is applicable to the decision variable B  Because it is not necessary to com pute optimal choices of a decision variables for impos sible states  it is desirable to avoid the computation     Table    The probability distribution of the second test result P R IT   R   T   cc  Tl  nt  nt st st f e f en  Rl  T            tr  nr  tr  nr  tr  two  tr  two    tr tr tr     z ero  tr  zero zero  tr  zero  tr  one  tr  tr  on e  tr  one  tr  one     cc    R   nr  prob      others     nr       others     nr       others     nr             others           nr       others     nr          nt nt  cliff diff diff cliff diff cliff cliff diff           others     zero        zero        lemon  one        peach  zero       peach  one  peach peach lemon  one           lemon  ze ro        lemon  one        OUR SOLUTION  In this section  we present an approach for overcom ing the aforementioned disadvantage of influence di agrams  Our approach consists of two independent components  a simple extension to influence diagrams and a top down method for influence diagram evalua tion  Our extension allows explicitly expressing the fact that some decision variables have different frames in dif ferent information states  We achieve this by intro ducing a framing function for each decision variable  which characterizes the available alternatives for the decision variable in different information states  With the help of framing functions  our solution algorithm effectively ignores the unavailable alternatives when computing an optimal choice for a decision variable in any information state  Our extension is inspired by the concepts of indicator valuations and effective frames proposed by Shenoy         Conceptually  our evaluation method  similar to Howard and Matheson s method  Howard and Math    Solving Asymmetric Decision Problems with Influence Diagrams  eson        consists of two steps  in order to evaluate an influence diagram  a decision tree is generated and the evaluation is then carried out on the decision tree  The first step will be described in this section  The second step can be carried out either by the simple  average out and fold back  method  Raiffa        or by a top down search algorithm  Qi        An ad vantage of using a search algorithm is that the two steps of tree generation and optimal policy computa tion can be combined into one  and only a portion of the tree needs to be generated  due to heuristic search  Our method successfully avoids the unnecessary com putations by pruning those impossible states and ig noring those unavailable alternatives for the decision variables  In comparison with than Howard and Matheson s method  ours has two distinct advantages  F irst  for the same influence diagram  our method generates a much smaller decision tree  Second  our method pro vides a clean interface to utilizing effi cient Bayesian net algorithms  Lauritzen and Spiegelhalter       Pearl                   generated by our method for an choice node corresponds to an in formation state of a decision variable  and a chance node corresponds to an uncertain state resulting from choosing an alternative for a decision variable in an information state  Two states are consistent if the variables common to both states have the same out comes  The  In the used car problem  the framing functions for the first test decision and the purchase decision are simple  they map every information state to the correspond ing full frames  The frame function for the second test decision can follows   specified as   T   X         nt diff   nt   if ur   X   otherwise      tr  be  root  a chance node representing the is in the decision tree   Initially  the empty state     For each information state S of the first decision variable d    there is a choice node  as a child of the root in the decision tree  The arc from the root to the node is labeled with the probability P   r d      S    A choice node in the decision tree is pruned if the probability on the arc to it is zero     Let N be a choice node not pruned in the decision tree  and SN be the inforn ation state associated with N  Assume that SN is for decision variable d  Then  N has lfd SN  I children  each corre sponding to an alternative in  d  SN    These chil dren are all leaf nodes if d is the last decision vari able  Otherwise  they are chance nodes  The node corresponding to alternative a E fd SN  repre sents the state  r d    SN  d  a     Let N   fd  Similarly  we define a decision function for a decision node d  as a mapping      Orr  a      nd   In a ddi t ion al      must satisfy the following constraint  For each s E n   d       s  E fa  s    In words  the choice prescribed by a decision function for a decision variable d in an information state must be a legitima te alternative   decision tree is recursively specified as follows     We extend  The framing functions express the fact that the legiti mate alternative set for a decision variable may vary in different information states  More specifically  for a de cision variable d and an information state s E O   d     d s  is the set of the legitimate alternatives the de cision maker can choose for d in information state s   Following Shenoy         we call fd s  the effective frame of decision variable d in informa tion state s   CONSTRUCTING DECISION TREES FROM INFLUENCE DIAGRAMS  In the decision tree influence diagram  a  EXTENDING INFLUENCE DIAGRAMS  influence diagrams by introducing fram ing functions to the definition given in Section    With this extension  an influence diagram I is a tuple I   X A P U  F  where X A P U have the same meaning as before  and F is a set   Orr d          of framing functions for the decision nodes        be a chance node representing a state  r d       SN di     a  and let A be the subset of the information states of decision vari able di which are consistent with  r d       SN di     a  Node N has IAI children  each being a choice node representing an information state in A   Let S be the information state rep resented by a child of N   The arc from N to the child is labeled with the conditional probability P  r d     S  r d  t    SN di l   a      In the above specification  we effectively prune all of the impossible information states for all decision vari ables and ignore the unavailable alternatives to deci sion variables  We have not specified how to compute the probabili ties on the arcs from chance nodes nor how to compute the values associated with the leaf nodes  As illus trated in  Qi and Poole        various well established Bayesian Net algorithms can be employed for comput ing the probabilities   and computing the values as sociated with the leaf nodes  which normally involve only small portions of the influence diagram  In par ticular   in order to further exploit asymmetry  Smith s method  Smith et al        can also be used for com puting those probabilities            Qi  Zhang  and Poole  HOW WELL OUR ALGORITHM DOES FOR THE USED CAR BUYER PROBLEM  When applying our algorithm to the used car buyer problem  a decision tree shown in Fig    is generated  In the graph  the leftmost box represents the only sit uation in which the first test decision is to be made  The boxes in the middle column correspond to the in formation states i n which the second test decision is to be made  Similarly  the boxes in the right column correspond to the information states in which the pur chase decision is to be made  From the figure we see that among those nodes corresponding to the infor mation states of the second test  all but two have only one child because the effective frames of the second test in the corresponding information states have only a single element  Making use of the f ramin g function this way is equivalent to six prunings   each cutting a subtree under a node corresponding to an informa tion state of the second test  Those shadowed boxes correspond to the impossible states  Our algorithm effectively detect s those impossible states and prune them when they are created  Each of such pruning amounts to cutting a subtree under the cor responding node  Consequently  our algorithm does not compute optimal choices for a decision node for those impossi ble states  For the used car buyer pro blem   our algorithm computes optimal choices for the purchase deci sion for only    information states  and opti mal choices for the second test for only   information states  among which six can be computed trivially     These constitute the minimal information state set one has to consider in order to compute an optimal policy for the used car buyer problem  This suggests that  as far as decision making concerned  our method exploits asymmetry to the maximum extent  In contras t   whereas those al gorithms that do not exploit asymmetry will compute the optimal choices for the pu rch ase decision for       x   x   x    information sta tes and wi ll compute optimal choices for the second test for    information states     RELATED WORK ON HANDLING ASYMMETRIC DECISION PROBLEMS  Recognizing that influence diagrams are not effec tive asymmetric decision problems  several researchers have recently proposed alternative r epresent ations  Fung and Shachter        propose contingent influence diagrams for explicitly expressing asymmetry of deci sion problems  In that representation  each variable is associated with a set of contingencies  and associated with one relation for each contingence  These relations collectively specify the condi t ional distribution of the variable  Covaliu and Oliver        p rop ose a different  represen   Figure    A decision tree generated for buyer problem  the used  car  tation for representing decision problems  This repre sentati on uses a decision diagram and a formulation table to specify a decision problem   A decision dia  gram is a directed acyclic graph whose directed paths identify all possible sequences of decisions and events in a decision problem  In a sense  a decision diagram is a degenerate decision tree in which paths having a common sequence of events are collap sed into one path   Covaliu and Oliver        Numer ical data are stored in the formulation table  Shenoy        proposes a  factorization  approach for representing degenerate probability distributions  In that appro ach   a degenerate probability distribution over a set of variables is decomposed into several fac tors over subsets of the variables such that the their  product  is equivalent to the original distribution  Smith et ai         present some interesting progress towards exploiting asymmetry of decision problems  They observe that an asymmetric decision problem of ten has some degenerate probability distributions  and that the influence diagram evaluation can be sped up if these degenerate probability distributions are used properly  Their philosophy is analogous to the one behind various algorithms for sparse matrix computa tion  In t hei r propos al   a conv entional influence dia gram is used to represent a decision problem at the level of relation  In addition  they propose to use a decision tree like representation to describe the con ditional probability distributions associated with the random variables in the influence diagram  The deci    Solving Asymmetric Decision Problems with Influence Diagrams  sion tree like representation is effective for economi cally representing degenerate conditional probability distributions  T hey propose a modified version of Shachter s alg ori thm  Sha chter       for influence di agram evaluation  and show how the decision tree like representation can be used to increase the effi ciency of arc reversal  a fundamental operation used in Shachter s algorithm  However  their alg o r it hm cannot avoid computing optimal choices for decision variables with respect to impossible information states  CONCLUSIONS     analyzed a drawback of influence di a gram s with asymmetric decision problems  which in duces some unnecessary computation in solving asym metric decision problems through influence diagram evaluat i on  We presented an approach for overcoming the drawb ack  Our ap p r o ach consists of a simple ex tension to influence diag rams and a top down method for influence diagram evaluation  The exte nsion fa cilitate s expressing asymmetry in influence diagrams  The top down method effectively avoids unnecessary computation   In this paper we  Acknowledgement research reported in this paper is partially under NSERC grant OGP        and Project B  of I RIS  The authors wish to thank Craig Boutili e r  Andre w Csi ng er  Mike Horsch  Keiji  The  pported  su  Kanazawa  Jim Little  Alan Mackworth  Maurice Queyranne  Jack Snoeyink and Ying Zhang for t he i r val u able comments   
