 The Bayesian Logic  BLOG  language was recently developed for defining first order probability models over worlds with unknown numbers of objects  It handles important problems in AI  including data association and population estimation  This paper extends BLOG by adopting generative processes over function spaces  known as nonparametrics in the Bayesian literature  We introduce syntax for reasoning about arbitrary collections of objects  and their properties  in an intuitive manner  By exploiting exchangeability  distributions over unknown objects and their attributes are cast as Dirichlet processes  which resolve difficulties in model selection and inference caused by varying numbers of objects  We demonstrate these concepts with application to citation matching      Introduction  Probabilistic first order logic has played a prominent role in recent attempts to develop more expressive models in artificial intelligence                           Among these  the Bayesian logic  BLOG  approach      stands out for its ability to handle unknown numbers of objects and data association in a coherent fashion  and it does not assume unique names and domain closure  A BLOG model specifies a probability distribution over possible worlds of a typed  first order language  That is  it defines a probabilistic model over objects and their attributes  A model structure corresponds to a possible world  which is obtained by extending each object type and interpreting each function symbol  Objects can either be guaranteed  meaning the extension of a type is fixed  or they can be generated from a distribution  For example  in the aircraft tracking domain      the times and radar blips are known  and the number of unknown aircraft may vary in possible worlds  BLOG as a case study provides a strong argument for Bayesian hierarchical methodology as a basis for probabilistic first order logic   BLOG specifies a prior over the number of objects  In many domains  however  it is unreasonable for the user to suggest such a proper  data independent prior  An investigation of this issue was the seed that grew into our proposal for Nonparametric Bayesian Logic  or NP BLOG  a language which extends the original framework developed in       NP BLOG is distinguished by its ability to handle object attributes that are generated by unbounded sets of objects  It also permits arbitrary collections of attributes drawn from unbounded sets  We extend the BLOG language by adopting Bayesian nonparametrics  which are probabilistic models with infinitely many parameters      The statistics community has long stressed the need for models that avoid commiting to restrictive assumptions regarding the underlying population  Nonparametric models specify distributions over function spaces  a natural fit with Bayesian methods  since they can be incorporated as prior information and then implemented at the inference level via Bayes theorem  In this paper  we recognize that Bayesian nonparametric methods have an important role to play in first order probabilistic inference as well  We start with a simple example that introduces some concepts necessary to understanding the main points of the paper  Consider a variation of the problem explored in       You have just gone to the candy store and have bought a box of Smarties  or M Ms   and you would like to discover how many colours there are  while avoiding the temptation to eat them    Even though there is an infinite number of colours to choose from  the candies are coloured from a finite set  Due to the manufacturing process  Smarties may be slightly discoloured  You would like to discover the unknown  true  set of colours by randomly picking Smarties from the box and observing their colours  After a certain number of draws  you would like to answer questions such as  How many different colours are in the box  Do two Smarties have the same colour  What is the probability that the first candy you select from a new box is a colour you have never seen before  The graphical representation of the BLOG model is shown in Fig   a  The number of Smarties of different colours  n Smartie   is chosen from a Poisson distribution with   it is unreasonable to expect a domain expert to implement nonparametrics considering the degree of effort required to grasp these abstract notions  We show that Bayesian nonparametrics lead to sophisticated representations that can be easier to implement than their parametric counterparts  We formulate a language that allows one to specify nonparametric models in an intuitive manner  while hiding complicated implementation details from the user  Sec    formalizes our proposed language extension as a set of rules that map code to a nonparametric generative process  We emphasize that NP BLOG is an extension to the BLOG language  so it retains all the functionality specified in        Figure     a  The BLOG and  b  NP BLOG graphical models for counting Smarties  The latter implements a Dirichlet process mixture  The shaded nodes are observations  mean Smartie   A colour for each Smartie s is drawn from the distribution HColourDist   Then  for every draw d  zSmartieDrawn  d  is drawn uniformly from the set of Smarties             n Smartie    Finally  we sample the observed  noisy colour of each draw conditioned on zSmartieDrawn  d  and the true colours of the Smarties  The NP BLOG model for the same setting is shown in Fig   b  The true colours of an infinite sequence of Smarties s are sampled from HColourDist   Smartie is a distribution over the choice of coloured Smarties  and is sampled from a uniform Dirichlet distribution with parameter Smartie   Once the Smarties and their colours are generated  the true Smartie for draw d  represented by the indicator zSmartieDrawn  d    s  is sampled from the distribution of Smarties Smartie   The last step is to sample the observed colour  which remains the same as in the BLOG model  One advantage of the NP BLOG model is that it determines a posterior over the number of Smarties colours without having to specify a prior over n Smartie   This is important since this prior is difficult to specify in many domains  A more significant advantage is that NP BLOG explicitly models a distribution over the collection of Smarties  This is not an improvement in expressiveness  one can always reverse engineer a parametric model given a target nonparametric model in a specific setting  Rather  nonparametrics facilitate the resolution of queries on unbounded sets  such as the colours of Smarties  This plays a key role in making inference tractable in sophisticated models with object properties that are themselves unbounded collections of objects  This is the case with the citation matching model in Sec       in which publications have collections of authors  The skeptic might still say  despite these advantages  that  We focus on an important class of nonparametric methods  the Dirichlet process  DP   because it handles distributions over unbounded sets of objects as long as the objects themselves are infinitely exchangeable  a notion formalized in Sec       The nonparametric nature of DPs makes them suitable for solving model selection problems that arise in the face of identity uncertainty and unknown numbers of objects  Understanding the Dirichlet process is integral to understanding NP BLOG  so we devote a section to it  Sec      shows how DPs can characterize collections of objects  Models based on DPs have been shown to be capable of solving a variety of difficult tasks  such as topic document retrieval          Provided the necessary expert knowledge  our approach can attack these applications  and others  We conduct a citation matching experiment in Sec     demonstrating accurate and efficient probabilistic inference in a real world problem      Dirichlet processes  A Dirichlet process G     H  DP    H   with parameter  and base measure H  is the unique probability measure defined G on the space of all probability measures    B   where  is the sample space  satisfying  G           G K    Dirichlet H           H K       for every measurable partition             K of   The base measure H defines the expectation of each partition  and  is a precision parameter  One can consider the DP as a generalization of the Dirichlet distribution to infinite spaces  In Sec       we formalize exchangeability of unknown objects  In order to explain the connection between exchangeability and the DP  it is instructive to construct DPs with the Polya urn scheme      Consider an urn with balls of K possible colours  in which the probability of the first ball being colour k is given by Hk   We draw a ball from the urn  observe its colour     then return it to the urn along with another ball of the same colour  We then make another draw  observing its colour with probability p     k        Hk       k         After N observations  the colour of the next ball is distributed as Hk   P  N      k   N      N  PN   i   k     N  i          The marginal P    N   of this process  obtained by applying the chain rule to successive predictive distributions  can be shown to satisfy the infinite mixture representation   Z K PN Y    k  i P    N     DP H  d       k i   M   k    where the k are multinomial success rates of each colour k  This result  a manifestation of de Finettis theorem  establishes the existence and uniqueness of the DP prior for the Polya urn scheme      In the Polya urn setting  observations i are infinitely exchangeable and independently distributed given the measure G  Thus  what we have established here in a somewhat cursory fashion is the appropriateness of the DP for the case when the observations i are infinitely exchangeable  Analogously  if the urn allows for infinitely many colours  then for any measurable interval  of  we have p N        N      N   X H      i      N    N i    The first term in this expansion corresponds to prior knowledge and the second term corresponds to the empirical distribution  Larger values of  indicate more confidence in the prior H  Note that  as N increases  most of the colours will be repeated  Asymptotically  one ends up sampling colours from a possibly large but finite set of colours  achieving a clustering effect  Nonetheless  there is always some probability of generating a new cluster  DPs are essential building blocks in our formulation of nonparametric first order logic  In the literature  these blocks are used to construct more flexible models  such as DP mixtures and hierarchical or nested DPs          Since observations are provably discrete  DP mixtures add an additional layer xi  P  xi  i   in order to model continuous draws xi from discrete mixture components i   In the Polya urn scheme  G is integrated out and the i s are sampled directly from H  Most algorithms for sampling DPs are based on this scheme              In the hierarchies constructed by our language  however  we rely on an explicit representation of the measure G since it is not clear we can always integrate it out  even when the measures are conjugate  This compels us to use the stick breaking construction       which establishes that i i d  sequences wk  Beta      and k  H can be used Pto construct the equivalent empirical distribution G   k   k  k    Qk  where the stick breaking weights k   wk j       wj   and can be shown to sum to unity  We abbreviate the sampling of the weights as k  Stick    This shows that G is an infinite sum of discrete values  The DP mixture due to the stick breaking construction is i   H  H zi            Stick   xi   i   zi  p xi  zi          where zi   k indicates that sample xi belongs to component k  The Smarties model  Fig   b  is in fact an example of a DP mixture  where the unbounded set of colours is   By grounding on the support of the observations  the true number of colours K is finite  At the same time  the DP mixture is open about seeing new colours as new Smarties are drawn  In NP BLOG  the unknown objects are the mixture components  NP BLOG semantics  Sec     define arbitrary hierarchies of Dirichlet process mixtures  By the stick breaking construction      every random variable xi has a countable set of ancestors  the unknown objects   hence DP mixtures preserve the well definedness of BLOG models  To infer the hidden variables of our models  we employ the efficient blocked Gibbs sampling algorithm developed in     as one of the steps in the overall Gibbs sampler  One complication in inference stems from the fact that a product of Dirichlets is difficult to simulate  Teh et al       provide a solution using an auxiliary variable sampling scheme      Syntax and semantics  This section formalizes the NP BLOG language by specifying a procedure that takes a set of statements L in the language and returns a model   A model comprises a set of types  function symbols  and a distribution over possible worlds      We underline that our language retains all the functionality of BLOG  Unknown objects must be infinitely exchangeable  but this trivially the case in BLOG  Sec      elaborates on this  We illustrate the concepts introduced in this section with an application to citation matching  Even though our citation matching model doesnt touch upon all the interesting aspects of NP BLOG  the reader will hopefully find it instrumental in understanding the semantics      Citation matching One of the main challenges in developing an automated citation matching system is the resolution of identity uncertainty  for each citation  we would like to recover its true title and authors  For instance  the following citations from the CiteSeer database probably refer to the same paper  Kozierok  Robin  and Maes  Pattie  A Learning Interface Agent for Meeting Scheduling  Proceedings of the      International Workshop on Intelligent user Interfaces  ACM Press  NY  R  Kozierok and P  Maes  A learning interface agent for scheduling meetings  In W  D  Gray  W  E  Heey  and D  Murray  editors  Proc  of the Internation al Workshop on Intelligent User Interfaces  Orlando FL  New York        ACM Press   Even after assuming the title and author strings have been segmented into separate fields  an open research problem itself    citation matching still exhibits serious challenges  two different strings may refer to the same author  e g  J F G  de Freitas and Nando de Freitas  and  conversely  the same string may refer to different authors  e g  David Lowe in vision and David Lowe in quantum field theory        type Author  type Pub  type Citation     guaranteed Citation      Author  NumAuthorsDist        Pub  NumPubsDist       Name a   NameDist       Title p   TitleDist       NumAuthors p   NumAuthorsDist       RefAuthor p  i  if Less i  NumAuthors p   then  Uniform Author a      RefPub c   Uniform Pub p      CitedTitle c   TitleStrDist Title RefPub c        CitedName c  i  if Less i  NumAuthors RefPub c    then  NameStrDist Name RefAuthor RefPub c   i      Figure    BLOG model for citation matching          type Author  type Pub     type Citation  type AuthorMention     guaranteed Citation  guaranteed AuthorMention     Name a   NameDist       Title p   TitleDist       CitedTitle c   TitleStrDist Title RefPub c        RefAuthor u   PubAuthorsDist RefPub CitedIn u        CitedName u   NameStrDist Name RefAuthor u      Figure    NP BLOG model for citation matching  There are a number of different approaches to this problem  Pasula et al  incorporate unknown objects and identity uncertainty into a probabilistic relational model       Wellner et al  resolve identity uncertainty by computing the optimal graph partition in a conditional random field       We elaborate on the BLOG model presented in      in order to contrast it with the one we propose  The BLOG model is shown in Fig    with cosmetic modifications and the function declaration statements omitted  The BLOG model describes a generative sampling process  Line   declares the object types  and line   declares that the citations are guaranteed  hence are not generated by a number statement   Lines   and   are number statements  and lines      are dependency statements  their combination defines a generative process  The process starts by choosing a certain number of authors and publications from their respective prior distributions  Then it samples author names  publication titles and the number of authors per publication  For each author string i in a citation  we choose the referring author from the set of authors  Finally  the properties of the citation objects are chosen  For example  generating an interpretation of CitedTitle c  for citation c requires values for RefPub c  and estimates of publication titles  TitleStrDist s  can be interpreted as a measure that adds noise in the form of perturbations to input string s  The NP BLOG model in Fig    follows a similar generative approach  the key differences being that it samples collections of unknown objects from DPs  and it allows for uncertainty in the order of authors in publications  But what do we gain by implementing nonparametrics  The advan   Figure    Three representations of lines     in Fig     as an NP BLOG program  as a generative process  and as a graphical model  Darker  hatched nodes are fixed or generated from other lines and shaded nodes are observed  Note the similarity between the graphical model and Fig   b  Lines     describe a DP mixture     over the publications p  where the base measure is TitleDist   Title is the hidden distribution over publication objects  the indicators are the true publications zRefPub  c  corresponding to the citations c  and the continuous observations are the titles CitedTitle  c   tage lies in the ability to capture sophisticated models of unbounded sets of objects in a high level fashion  and the relative ease of conducting inference  since nonparametrics can deal gracefully with the problem of model selection  One can view a model such as the automatic citation matcher from three perspectives  it is a set of statements in the language that comprise a program  from a statisticians point of view  the model is a process that samples the defined random variables  and from the perspective of machine learning  it is a graphical model  Fig    interprets lines     of Fig    in three different ways  The semantics  as we will see  formally unify all three perspectives  Both BLOG and NP BLOG can answer the following queries  Is the referring publication of citation c the same as the referring publication of citation d  How many authors are there in the given citation database  What are the names of the authors of the publication referenced by citation c  How many publications contain the author a  where a is one of the authors in the publication referenced by citation c  And what are the titles of those publications  However  only NP BLOG can easily answer the following query  what group of researchers do we expect to be authors in a future  unseen publication      Objects and function symbols This section is largely devoted to defining notation so that we can properly elaborate on NP BLOG semantics in Sections     to      The notation as it appears in these sections makes the connection with both first order logic and the Dirichlet process mixture presented in Sec       The set of objects of a type  is called the extension of    and is denoted by      In BLOG  extensions associated with unknown  non guaranteed  types can vary over possible worlds   so we sometimes write       The size of     is denoted by n       Note that objects may be unknown even if there is a fixed number of them  Guaranteed objects are present in all possible worlds  We denote  to be the set of possible worlds for model   A model introduces a set of function symbols indexed by the objects  For conciseness  we treat predicates as Boolean functions and constants as zero ary functions  For example  the citation matching model  Fig     has the function symbols Name and CitedTitle  among others  so there is a Name a  for every author a and CitedTitle c  for every citation c  By assigning numbers to objects as they are generated  we can consider logical variables a and c to be indices on the set of natural numbers  Since BLOG is a typed language  the range of interpretations of a function symbol f is specified by its type signature  For example  the interpretation of RefAuthor u   for each u   AuthorMention                   n AuthorMention    takes a value on the range  Author   Likewise  Title p  ranges over the set of strings  String   Figures   and   omit function declaration statements  which specify type signatures  Nonetheless  this should not prevent the reader from deducing the type signatures of the functions via the statements that generate them  Nonparametric priors define distributions over probability measures  so we need function symbols that uniformly refer to them  Letting X and Y be object domains  e g  X    Author    we define MD  X   Y  to be the set of conditional probability densities p x  X   y  Y  following the class of parameterizations D  We can extend this logic  denoting MD  MD  X   Y    Z  to be the set of probability measures p d  D   z  Z  over the choice of parameterizations d  D  conditioned on Z  And so on  For peace of mind  we assume each class of distributions D is defined on a measurable  field and the densities are integrable over the range of the sample space  Note that Y or Z  but not X   may be a Cartesian product over sets of objects  BLOG does not allow return types that are tuples of objects  so we restrict distributions of objects accordingly  One can extend the above reasoning to accommodate distributions over multiple unknown objects by adopting slightly more general notation involving products of sets of objects  We assign symbols to all the functions defined in the language L   For instance  the range of NameDist in Fig    is M  String   for some specified parameterization class  Since NameDist is not generated in another line  it must be fixed over all possible worlds  For each publication p  the interpretation of symbol PubAuthorsDist p  is assigned a value on the space MMultinomial   Author    That is  the   Even though the DP imposes a distribution over an infinite set of unknown objects  n     is still finite since it refers to the estimated number of objects in   n    corresponds to the random variables of the DP mixture  as explained in Sec        function symbol refers to a distribution over author objects  How the model chooses the success rate parameters for this multinomial distribution  given that it is not on the left side of any generating statement  is the subject of Sec       NP BLOG integrates first order logic with Bayesian nonparametric methods  but we have left out one piece of the puzzle  how to specify distributions such as NameDist  or classes of distributions  This is an important design decision  but an implementation level detail  so we postpone it to future work  For the time being  one can think of parameterizations as object classes in a programming language such as Java that generate samples of the appropriate type  We point out that there is already an established language for constructing hierarchical Bayesian models  BUGS       The truth of any first order sentence is determined by a possible world in the corresponding language  A possible world    consists of an extension     for each type  and an interpretation for each function symbol f   Sec      details how NP BLOG specifies a distribution over        Dependency statements for known objects The dependency statement is the key ingredient in the specification of a generative process  We have already seen several examples of dependency statements  and we formalize them here  It is well explained in       but we need to extend the definition in the context of nonparametrics  In BLOG  a dependency statement looks like f  x            xL    g t            tN          where f is a function symbol  x            xL is a tuple of logical variables representing arguments to the function  g is a probability density conditioned on the arguments t            tN   which are terms or formulae in the language L in which the logical variables x            xL may appear  The dependency statement carries out a generative process  For an example  lets look at the dependency statement on line    of Fig     Following the rules of semantics       line    generates assignments for random variables CitedTitle  c   for c              n Citation   from probability density g conditioned on values for zRefPub  c  and Title  p   for all p              n Pub   As in       we use square brackets to index random variables  instead of the statistics convention of using subscripts  In NP BLOG  the probability density g is itself a function symbol  and the dependency statement is given by f  x            xL    g t            tM   tM              tM  N        where f and g are function symbols  and t            tM  N are formulae of the language as in      For this to be a valid statement  g t            tM   must be defined on the range M X   Y   where X is the range of f  x            xL   and Y is the domain of the input arguments within the curly braces  The first M terms inside the parentheses are evaluated in possible world   and their resulting values determine the   choice of measure g  The terms inside the curly braces are evaluated in  and the resulting values are passed to distribution g t            tM    When all the logical variables x            xL refer to guaranteed objects  the semantics of the dependency statement are given by       The curly brace notation is used to disambiguate the two roles of input argument variables  The arguments inside parentheses are indices to function symbols  e g  the c in RefPub c  in Fig      whereas the arguments inside curly braces serve as input to probability densities  e g  the term inside the curly braces in TitleStrDist Title RefPub c      This new notation is necessary when a distribution takes both types of arguments  We dont have such an example in citation matching  so we borrow one from an NP BLOG model in the aircraft tracking domain   State a  t  if t     then  InitState   else  StateTransDist a  State a  t        The state of the aircraft a at time t is an R Vector object which stores the aircrafts position and velocity in space  When t      the state is generated from the transition distribution of aircraft a given the state at the previous time step  StateTransDist a  corresponds to a measure on the space M  R Vector     R Vector    For example  in line   of Fig     the citation objects are guaranteed  Following the rules of semantics  line   defines a random variable CitedTitle  c  corresponding to the interpretation of function symbol CitedTitle c  for every citation c  Given assignments to TitleStrDist   zRefPub  c   we use z to be consistent with the notation of the semantics used in this paper  although it makes no difference in BLOG  and Title  p  for all p   Pub   assignments that are either observed or generated from other statements  the dependency statement defines the generative process CitedTitle  c   TitleStrDist  Title  p   s t  p   zRefPub  c   BLOG allows for contingencies in dependency statements  These can be subsumed within our formal framework by defining a new measure   c  t    P i  ci  i  ti     ti             where    is the indicator function  ci is the condition i which must be satisfied in order to sample from the density i   c and t are the complete sets of terms and conditions  and the summation is over the number of clauses  Infinite contingencies and their connection to graphical models are discussed in           Exchangeability and unknown objects Unknown objects are precisely those which are not guaranteed  In this section  we formalize some important properties of generated objects in BLOG  We adopt the notion of exchangeability     to objects in probabilistic first order logic  We start with some standard definitions     In which aircraft in flight appear as blips on a radar screen  and the objectives are to infer the number of aircraft and their flight paths and to resolve identity uncertainty  arising because a blip might not represent any aircraft or  conversely  an aircraft might produce multiple detections        Definition    The random variables x            xN are  finitely  exchangeable under probability density function p if p satisfies p x            xN     p x              x N     for all permutations  on             N        When n is finite  the concept of exchangeability is intuitive  the ordering is irrelevant since possible worlds are equally likely  The next definition extends exchangeability to unbounded sequences of random variables  Definition    The random variables x    x          are infinitely exchangeable if every finite subset is finitely exchangeable      Exchangeability is useful for reasoning about distributions over properties on sets of objects in BLOG  From Definitions   and    we have the following result  Proposition    It is possible to define g in the dependency statements     and     such that the sequence of objects x            xL is finitely exchangeable if and only if the terms t            tM  N do not contain any statements referring to a particular xl   For example  the distribution of hair colours of two people  Eric and Mike  is not exchangeable given evidence that Eric is the father of Mike  What about sequences of objects such as time  As long as we do not set the predecessor function beforehand  any sequence is legally exchangeable  In this paper  models are restricted to infinitely exchangeable unknown objects  We can interpret this presupposition this way  if we reorder a sequence of objects  then their probability remains the same  If we add another object to the sequence at some arbitrary position  both the original and new sequence with one more object are exchangeable  We can then appeal to de Finettis theorem      and hence the Dirichlet process  Therefore  the order of unknown objects is not important  and we can reason about set of objects rather than sequences  While there are many domains in which one would like to infer the presence of objects that are not infinitely exchangeable  this constraint leaves us open to modeling a wide range of interesting domains  Unknown or non guaranteed objects are assigned non rigid designators  a symbol in different possible worlds does not necessarily refer to the same object  and so it does not make sense to assign it a rigid label  This consideration imposes a constraint  we can only refer to a publication p via a guaranteed object  such as a citation c that refers to it  While we cannot form a query that addresses a specific unknown object  or a subset of unknown objects  we can pose questions about publications using existential and universal quantifiers  resolved using Skolemization  for instance   We could ask  for instance  how many publications have three or more authors      Dependency statements for unknown objects Sec      formalized the notion of type extensions and function symbols in NP BLOG programs  Sec      served up the preliminaries of syntax and semantics in dependency   statements  The remaining step to complete the full prescription of the semantics as a mapping from the language L to a distribution over possible worlds  This is accomplished by constructing a Bayesian hierarchical model over random variables    n     such that the set of random variables  is in one to one correspondence with the set of function interpretations  n refers to the sizes of the type extensions  and  is a set of auxiliary random variables such R that p   n   d   p   n   One might wonder why we dont dispense of function symbols entirely and instead describe everything using random variables  as in       The principal reason is to establish the connection with firstorder logic  Also  we want to make it clear that some random variables do not map to any individual in the domain  What follows is a procedural definition of the semantics  We now define distributions over the random variables  and their mapping to the symbols of the first order logic  In order to define the rules of semantics  we collect dependency and number statements according to their input argument types  If the collection of statements includes a number statement  then the rules of semantics are given in       Otherwise  we describe how the objects and their properties are implicitly drawn from a DP  Consider a set of K dependency statements such that the generated functions f            fK all require a single input of type   and    can vary over possible worlds   We denote x to be the logical variable that ranges over      The output types of the fk s are not important   The K dependency statements look like f   x   g   t              t  M    t  M               t  M   N                   fK  x   gK  tK          tK MK  tK MK          tK MK NK   where Mk and Nk are the number of input arguments to gk    and gk     respectively  and tk i is a formula in the language in which x may appear  As in BLOG  each fk  x  is associated with a random variable fk  x   The random variables g            gK   including all those implicated in the terms  must have been generated by other lines or are observed  Overloading the notation  we define the terms tk i to be random variables that depend deterministically on other generated or observed random variables  The set of statements     defines the generative process   Stick        fk  x   gk  tk          tk Mk   tk Mk          tk Mk Nk        for k              K  x                where  is the userdefined DP concentration parameter and  is a multinomial distribution such that each success rate parameter  x determines the probability of choosing a particular object x  NP BLOG infers a distribution  over objects of type  following the condition of infinite exchangeability  For example  applying rules       to line   of Fig     we get Author  Stick Author   Name  a   NameDist   for a                If an object type does not have any dependency or number statements  then no distribution over its extension is introduced  e g  strings in the citation matching model   The implementation of the DP brings about an important subtlety  if x takes on a possibly infinite different set of values  how do we recover the true number of objects n     The idea is to introduce a bijection from the subset of positive natural numbers that consists only of active objects to the set             n      An object is active in possible world  if and only if at least one random variable is assigned to that object in   In the above example  n Author  is the number of author objects that are mentioned in the citations  Of course  in practice we do not sample an infinite series of random variables Name  a   If we declare a function symbol f with a return type  ranging over a set of unknown objects  then there exists the default generating process zf  x             We use zf  x  instead of f  x  to show that the random variables are the indicators of the DP mixture      For example  each zRefPub  c  in line   in Fig    is independently drawn from the distribution of publications Pub   We can view line   as constructing a portion of the hierarchical model  as shown in Fig     The number of publications n Pub  is set to the number of different values assigned to zRefPub  c   NP BLOG allows for the definition of a symbol f that corresponds to a multinomial distribution over      so its range is MMultinomial        It exhibits the default prior f  x   Dirichlet f            analogous to       f is a user defined scalar  We define nf  x  to be the true number of objects associated with collection f  x   This is useful for modeling collections of objects such as the authors of a publication  Applying rules          to the statements in Fig    involving publication objects  we arrive at the generative process Pub  Stick Pub   Title  p   TitleDist   for p              n Pub  PubAuthorsDist  p   Dirichlet PubAuthorsDist Author     Most of the corresponding graphical model is shown in Fig     Only the PubAuthorsDist  p s are missing  and they are shown in Fig     The true number of authors nPubAuthorsDist  p  in publication p comes from the support of all random variables that refer to it  and n Pub  is determined by nPubAuthorsDist   While this paper focuses on the Dirichlet process  our framework allows for other classes of nonparametric distributions  One example can be found in the aircraft tracking domain from Sec       in which the generation of aircraft transition tables might be specified with the statement StateTransDist a   StateTransPrior    In both cases      and       one can override the defaults by including appropriate dependency statements for f   in   Num  citations Num  papers Phrase matching RPM MCMC CRF Seg  N      NP BLOG  Figure    The white nodes are the portion of the graphical model generated in lines   and   of Fig     See Fig    for an explanation of the darkened nodes  which case we get f  x   g   following rule      For example  lines   and   in Fig    specify the generative process for the author mention objects  zRefAuthor  u   PubAuthorsDist  p  CitedName  u   NameStrDist  Name  a     s t  p   zRefPub  c   c   CitedIn  u   a   zRefAuthor  u   Fig    shows the equivalent graphical model  The generative process       is a stick breaking construction over the unknown objects and their attributes  When the objects x range over the set of natural numbers        is equivalent to the Dirichlet process G  DP     H        H K          P where G   x    x  f   x         fK  x    and H k is the base measure over the assignments to fk   defined by gk conditioned on the terms tk             tk Mk  Nk   Since BLOG is a typed  free language  we need to allow for the null assignment to f  x  when it is implicitly drawn from  in       We permit the clause f  x   if cond then null         which defines f  x    null  cond          cond    This statement is necessary to take care of the situation when an objects source can be of different types  as in the aircraft tracking domain with false alarms       Next  we briefly describe how to extend the rules of semantics to functions with multiple input arguments  Lets consider the case of two inputs with an additional logical variable y      Handling an additional input argument associated with known  guaranteed  objects is easy  We just duplicate       for every instance of y in the guaranteed type extension  This is equivalent to adding a finite series of plates in the graphical model  Otherwise  we assume the unknown objects are drawn independently  That is            Multiple unknown objects as input does cause some superficial complications with the interpretation of       as a DP  principally because we need to define new notation for products of measures over different types   Face Reinforce  Reason  Constraint                                                                                                                  Table    Citation matching results for the Phrase Matching      RPM       CRF Seg      and NP BLOG models  Performance is measured by counting the number of publication clusters that are recovered perfectly  The NP BLOG column reports an average over      samples  The DP determines an implicit distribution of unknown  infinitely exchangeable objects according to their properties  That is  the DP distinguishes unknown objects solely by their attributes  However  this is not always desirable  for instance  despite being unable to differentiate the individual pieces  we know a chess board always has eight black pawns  This is precisely why we retain the original number statement syntax of BLOG which allows the user to specify a prior over the number of unknown objects  independent of their properties  In the future  we would like to experiment with priors that straddle these two extremes  This could possibly be accomplished by setting a prior on the Dirichlet concentration parameter    By tracing the rules of semantics  one should see that only thing the citation matching model does not generate is values for CitedIn u   Therefore  they must be observed  We can also provide observations from any number of object attributes  such as CitedTitle c  and CitedName u   which would result in unsupervised learning  By modifying the set of evidence  one can also achieve supervised or semisupervised learning  Moreover  the language can capture both generative and discriminative models  depending whether or not the observations are generated  To summarize  the rules given by            combined with the number statement       construct a distribution p   z  n    such that the set of auxiliary variables is             z  is in one to one correspondence with the interpretations of the function symbols  the n are the sizes of the      and an assignment to    z  n  completely determines the possible world     The rules of semantics assemble models that are arbitrary hierarchies of DPs      Experiment  The purpose of this experiment is to show that the NPBLOG language we have described realizes probabilistic inference on a real world problem  We simulate the citation matching model in Fig    on the CiteSeer data set      which consists of manually segmented citations from four research areas in AI  We use Markov Chain Monte Carlo  MCMC  to simulate possible worlds from the model posterior given evidence in the form of cited authors and titles  Sec    briefly describes   There is much future work on this topic  An important direction is the development of efficient  flexible and on line inference methods for hierarchies of Dirichlet processes   Figure    Estimated  solid blue  and true  dashed red line  number of publications for the Face and Reasoning data   Acknowledgements This paper wouldnt have happened without the help of Brian Milch  Special thanks to Gareth Peters and Mike Klaas for their assistance  and to the reviewers for their time and effort in providing us with constructive comments   
 First order probabilistic models combine representational power of first order logic with graphical models  There is an ongoing effort to design lifted inference algorithms for first order probabilistic models  We analyze lifted inference from the perspective of constraint processing and  through this viewpoint  we analyze and compare existing approaches and expose their advantages and limitations  Our theoretical results show that the wrong choice of constraint processing method can lead to exponential increase in computational complexity  Our empirical tests confirm the importance of constraint processing in lifted inference  This is the first theoretical and empirical study of constraint processing in lifted inference      INTRODUCTION  Representations that mix graphical models and first order logiccalled either first order or relational probabilistic modelswere proposed nearly twenty years ago  Breese        Horsch and Poole        and many more have since emerged  De Raedt et al         Getoor and Taskar         In these models  random variables are parameterized by individuals belonging to a population  Even for very simple first order models  inference at the propositional levelthat is  inference that explicitly considers every individualis intractable  The idea behind lifted inference is to carry out as much inference as possible without propositionalizing  An exact lifted inference procedure for first order probabilistic directed models was originally proposed by Poole         It was later extended to a broader range of problems by de Salvo Braz et al          Further work by Milch et al         expanded the scope of lifted inference and resulted in the C FOVE algorithm  which is currently the state of the art in exact lifted inference  First order models typically contain constraints on the pa   rameters  logical variables typed with populations   Constraints are important for capturing knowledge regarding particular individuals  In Poole         each constraint is processed only when necessary to continue probabilistic inference  We call this approach splitting as needed  Conversely  in de Salvo Braz et al         all constraints are processed at the start of the inference  this procedure is called shattering   and at every point at which a new constraint arises  Both approaches need to use constraint processing to count the number of solutions to constraint satisfaction problems that arise during the probabilistic inference  Milch et al         adopt the shattering procedure  and avoid the need to use a constraint solver by requiring that the constraints be written in normal form  The impact of constraint processing on computational efficiency of lifted inference has been largely overlooked  In this paper we address this issue and compare the approaches to constraint processing listed above  both theoretically and empirically  We show that  in the worst case  shattering may have exponentially worse space and time complexity  in the number of parameters  than splitting as needed  Moreover  writing the constraints in normal form can lead to computational costs with a complexity that is even worse than exponential  Experiments confirm our theoretical results and stress the importance of informed constraint processing in lifted inference  We introduce key concepts and notation in Section   and give an overview of constraint processing during lifted inference in Section    In Section   we discuss how a specialized  CSP solver can be used during lifted inference  Theoretical results are presented in Section    Section   contains results of experiments      PRELIMINARIES  In this section we introduce a definition of parameterized random variables  which are essential components of first order probabilistic models  We also define parfactors  Poole         which are data structures used during lifted inference             KISYNSKI   POOLE  UAI      g x    PARAMETERIZED RANDOM VARIABLES  If S is a set  we denote by  S  the size of the set S   g A   A population is a set of individuals  A population corresponds to a domain in logic  A parameter corresponds to a logical variable and is typed with a population  Given parameter X   we denote its population by D X   Given a set of constraints C   we denote a set of individuals from D X  that satisfy constraints in C by D X    C   A substitution is of the form  X   t            Xk  tk    where the Xi are distinct parameters  and each term ti is a parameter typed with a population or a constant denoting an individual from a population  A ground substitution is a substitution  where each ti is a constant  A parameterized random variable is of the form f  t           tk    where f is a functor  either a function symbol or a predicate symbol  and ti are terms  Each functor has a set of values called the range of the functor  We denote the range of the functor f by range  f    A parameterized random variable f  t           tk   represents a set of random variables  one for each possible ground substitution to all of its parameters  The range of the functor of the parameterized random variable is the domain of random variables represented by the parameterized random variable  Let v denote an assignment of values to random variables  v is a function that takes a random variable and returns its value  We extend v to also work on parameterized random variables  where we assume that free parameters are universally quantified  Example    Let A and B be parameters typed with a population D A   D B    x            xn    Let h be a functor with range  true  f alse   Then h A  B  is a parameterized random variable  It represents a set of n  random variables with domains  true  f alse   one for each ground substitution  A x    B x      A x    B x             A xn   B xn    A parameterized random variable h x    B  represents a set of n random variables with domains  true  f alse   one for each ground substitution  B x             B xn    Let v be an assignment of values to random variables  If v h x    B   equals true  each of the random variables represented by h x    B   namely h x    x             h x    xn    is assigned the value true by v       PARAMETRIC FACTORS  A factor on a set of random variables represents a function that  given an assignment of a value to each random variable from the set  returns a real number  Factors are used in the variable elimination algorithm  Zhang and Poole        to store initial conditional probabilities and intermediate results of computation during probabilistic inference in graphical models  Operations on factors include mul   g xn   h x   x    h A  B   h x   xn   B A  FIRST ORDER  h xn   x    h xn   xn   PROPOSITIONAL  Figure    A parameterized belief network and its equivalent belief network  tiplication of factors and summing out random variables from a factor  Let v be an assignment of values to random variables and let F be a factor on a set of random variables S  We extend v to factors and denote by v F  the value of the factor F given v  If v does not assign values to all of the variables in S  v F  denotes a factor on other variables  A parametric factor or parfactor is a triple hC   V  Fi where C is a set of inequality constraints on parameters  V is a set of parameterized random variables and F is a factor from the Cartesian product of ranges of parameterized random variables in V to the reals  A parfactor hC   V  Fi represents a set of factors  one for each ground substitution G to all free parameters in V that satisfies the constraints in C   Each such factor FG is a factor on the set of random variables obtained by applying a substitution G  Given an assignment v to random variables represented by V  v FG     v F   Parfactors are used to represent conditional probability distributions in directed first order models and potentials in undirected first order models as well as intermediate computation results during inference in first order models  In the next example  which extends Example    we use parameterized belief networks  PBNs   Poole        to illustrate representational power of parfactors  The PBNs are a simple first order directed probabilistic model  we could have used parameterized Markov networks instead  as did de Salvo Braz et al         and Milch et al           Our discussion of constraint processing in lifted inference is not limited to PBNs  it applies to any model for which the joint distribution can be expressed as a product of parfactors  Example    A PBN consists of a directed acyclic graph where the nodes are parameterized random variables  an assignment of a range to each functor  an assignment of a population to each parameter  and a probability distribution for each node given its parents  Consider the PBN graph presented in Figure   using plate notation  Buntine    UAI       KISYNSKI   POOLE         Let g be a functor with range  true  f alse   Assume we do not have any specific knowledge about instances of g A   but we have some specific knowledge about h A  B  for case where A   x  and for case where A    x  and A   B  The probability P g A   can be represented with a parfactor h      g A    Fg i  where Fg is a factor from range h  to the reals  The conditional probability P h A  B  g A   can be represented with a parfactor h      g x     h x    B    F  i  a parfactor h A    x      g A   h A  A    F  i  and a parfactor h A    x    A    B    g A   h A  B    F  i  where F    F    and F  are factors from range g   range h  to the reals  Let C be a set of inequality constraints on parameters and X be a parameter  We denote by EXC the excluded set for X  that is  the set of terms t such that  X    t   C   A parfactor hC   V  FF i is in normal form  Milch et al         if for each inequality  X    Y    C   where X and Y are parameters  we have EXC   Y     EYC   X   In a normal form parfactor  for all parameters X of parameterized random variables in V    D X   C       D X     EXC    Example    Consider the parfactor h A    x    A    B   g A   h A  B    F  i from Example    Let C denote a set of constraints from this parfactor  The set C contains only one inequality between parameters  namely A    B  We have EAC    x    B  and EBC    A   As EAC   B     EBC   A   the parfactor is not in normal form  Recall that D A   D B    x            xn    The size of the set D A    C depends on the parameter B  It is equal n    when B   x  and n    when B    x    Other parfactors from Example   are in normal form as they do not contain constraints between parameters  Consider a parfactor h X    Y  X    a Y    a    e X   f  X Y     Fe f i  where D X    D Y   and   D X      n  Let C   denote a set     of constraints from this parfactor  As EXC   Y     EYC   X   the parfactor is in normal form and   D X    C       n    and   D Y     C       n         LIFTED INFERENCE AND CONSTRAINT PROCESSING  In this section we give an overview of exact lifted probabilistic inference developed in  Poole          de Salvo Braz et al          and  Milch et al         in context of constraints  For more details on other aspects of lifted inference we refer the reader to the above papers  Let  be a set of parfactors  Let J    denote a factor equal to the product of all factors represented by elements of   Let U be the set of all random variables represented by parameterized random variables present in parfactors in   Let Q be a subset of U  The marginal of J    on Q  denoted JQ     is defined as JQ      U Q J     Given  and Q  the lifted inference procedure computes the marginal JQ    by summing out random variables from       Q  where possible in a lifted manner  Evidence can be handled by adding to  additional parfactors on observed random variables  Before a  ground  random variable can be summed out  a number of conditions must be satisfied  One is that a random variable can be summed out from a parfactor in  only if there are no other parfactors in  involving this random variable  To satisfy this condition  the inference procedure may need to multiply parfactors prior to summing out  Multiplication has a condition of its own  two parfactors hC    V    F  i and hC    V    F  i can be multiplied only if for each parameterized random variable from V  and for each parameterized random variable from V    the sets of random variables represented by these two parameterized random variables in respective parfactors are identical or disjoint  This condition is trivially satisfied for parameterized random variables with different functors  Example    Consider the PBN from Figure   and set  containing parfactors introduced in Example    Assume that we want to compute the marginal of J    on instances of h A  B   where A    x  and A    B  We need to sum out random variables represented by g A  from parfactor h A    x    A    B    g A   h A  B    F  i  but as they are also among random variables represented by g A  in parfactor h     g A   Fg i  we have to first multiply these two parfactors  Sets of random variables represented by g A  in these two parfactors are not disjoint and are not identical and the precondition for multiplication is not satisfied       SPLITTING  The precondition for parfactor multiplication may be satisfied through splitting parfactors on substitutions  Let  be a set of parfactors  Let p f   hC   V  FF i    Let  X t  be a substitution such that  X    t     C and term t is a constant such that t  D X   or a parameter such that D t    D X   A split of p f on  X t  results in two parfactors  p f  X t  that is a parfactor p f with all occurrences of X replaced by term t  and a parfactor p fr   hC  X    t   V  FF i  We have J      J      p f    p f  X t   p fr     We call p fr a residual parfactor  Given two parfactors that need to be multiplied  substitutions on which splitting is performed are determined by analyzing constraint sets C and sets of parameterized random variables V in these parfactors  Example    Let us continue Example    A split of h      g A    Fg i on  A x    results in h      g x      Fg i and residual h A    x      g A    Fg i  The first parfactor can be ignored because it is not relevant to the query  while the residual needs to be multiplied by a parfactor h A    x    A    B    g A   h A  B    F  i  The precondition for multiplication is now satisfied as g A  represents the same set of random variables in both parfactors             KISYNSKI   POOLE MULTIPLICATION  Once the precondition for parfactor multiplication is satisfied  multiplication can be performed in a lifted manner  This means that  although parfactors participating in a multiplication as well as their product represent multiple factors  the computational cost of parfactor multiplication is limited to the cost of multiplying two factors  The only additional requirement is that the lifted inference procedure needs to know how many factors each parfactor involved in the multiplication represents and how many factors their product will represent  These numbers can be different because the product parfactor might involve more parameters than a parfactor participating in the multiplication  In such a case  a correction to values of a factor inside appropriate parfactors participating in multiplication is necessary  Detailed description of this correction is beyond the scope of this paper  For more information see Example   below and a discussion of the fusion operation in de Salvo Braz et al          For our purpose  the key point is that the lifted inference procedure needs to compute the number of factors represented by a parfactor  Given a parfactor hC   V  Fi  the number of factors it represents is equal to the number of solutions to the constraint satisfaction problem formed by constraints in C   This counting problem is written as  CSP  Dechter         If a parfactor is in normal form  each connected component of the underlying constraint graph is fully connected  see Proposition    and it is easy to compute the number of factors represented by the considered parfactor  If a parfactor is not in normal form  a  CSP solver is necessary to compute the number of factors the parfactor represents  Example    In Example   the parfactor h A    x      g A    Fg i represents n    factors  and needs to be multiplied by a parfactor h A    x    A    B    g A   h A  B    F  i  which represents  n      factors  Their product p f   hA    x    A    B   g A   h A  B    F i  where F is a factor from range g   range h  to the reals  represents  n      factors  Let v be an assignment of values to g A  and  n    n     h A  B   We have v F     v Fg   v F     Now we can sum out g A  from p f   The result needs to be represented with a counting formula  Milch et al          which is outside of the scope of this paper  What is important for the paper is that a parfactor involving counting formulas needs to be in normal form  Since p f is not in normal form  we first split it on substitution  B x    and then sum out g A  from the two parfactors obtained through splitting       SUMMING OUT  During lifted summing out  a parameterized random variable is summed out from a parfactor hC   V  FF i  which means that a random variable is eliminated from each factor represented by the parfactor in one inference step  Lifted  UAI       inference will perform summing out only once on the factor F  If some parameters only appear in the parameterized variable that is being eliminated  the resulting parfactor will represent fewer factors than the original one  As in the case of parfactor multiplication  the inference procedure needs to compensate for this difference  It needs to compute the size of the set X    D X         D Xk      C   where X            Xk are parameters that will disappear from the parfactor  This number tells us how many times fewer factors the result of summing out represents compared to the original parfactor  If the parfactor is in normal form   X   does not depend on values of parameters remaining in the parfactor after summing out  The problem reduces to  CSP and  as we mentioned in Section      it is easy to solve  If the parfactor is not in normal form   X   may depend on values of remaining parameters and a  CSP solver is necessary to compute all the sizes of the set X conditioned on values of parameters remaining in the parfactor  Example    Assume that we want to sum out f  X Y   from a parfactor h X    Y Y    a    e X   f  X Y     Fe f i  where Fe f is a factor from range e   range  f   to the reals  Let D X    D Y   and   D X      n  The parfactor represents  n      factors  Note that the parfactor is not in normal form and   D Y      X    Y Y    a   equals n    if X   a and n    if X    a  A  CSP solver could compute these numbers for us  see Example     After f  X Y   is summed out  Y is no longer among parameters of random variables and X remains the sole parameter  To represent the result of summation we need two parfactors  h     e a   Fe  i and h X    a   e X   Fe  i  where Fe  and Fe  are factors from range e  to the reals  Let y  range e   then Fe   y     zrange  f   Fe f  y  z  n  and Fe   y     zrange  f   Fe f  y  z  n         PROPOSITIONALIZATION  During inference in first order probabilistic models it may happen that none of lifted operations  including operations that are not described in this paper  can be applied  In such a situation the inference procedure substitutes appropriate parameterized random variables with random variables represented by them  This may be achieved through splitting as we demonstrate in an example below  Afterward  inference is performed  at least partially  at the propositional level  As it has a negative impact on the efficiency of inference  propositionalization is avoided as much as possible during inference in first order models  Example    Consider a parfactor h      g A    Fg i from Example    Assume we need to propositionalize g A   Recall that D A    x            xn    Propositionalization results in a set of parfactors h      g x      Fg i  h      g x      Fg i          h      g xn      Fg i  h      g xn     Fg i  Each parameterized random variable in the above parfactors represents just one   UAI              KISYNSKI   POOLE     w    w   w  W  W w          w  W      X  w              Y  X  Z          Y      X Y                     Z      Z                     w              Figure    Comparison of w and exponential functions  random variable and each parfactor represents just one factor  The set could be produced by a sequence of splits  The above informal overview of lifted inference  together with simple examples  shows that constraint processing is an integral  important part of lifted inference       CSP SOLVER AND LIFTED INFERENCE  In Section    we showed when a  CSP solver can be used during lifted probabilistic inference  A solver that enumerates all individuals from domains of parameters that form a CSP would contradict the core idea behind lifted inference  that is  performing probabilistic inference without explicitly considering every individual  In our experiments presented in Section   we used a solver  Kisynski and Poole        that addresses the above concern  It is a lifted solver based on the variable elimination algorithm for solving  CSP of Dechter        and is optimized to handle problems that contain only inequality constraints  It is not possible to describe the algorithm in detail in this paper  but below we provide some intuition behind the solver    a    b   Figure    Constraint graph with a cycle  a  and the two cases that need to be analyzed  b   us to immediately solve the corresponding  CSP  we can assign the value to W in n ways  and are left with n    possible values for X  and n    possible values for Y and Z  Hence  there are n n      solutions to this CSP instance  Consider a set of constraints  W    X W    Y  X    Z Y    Z   where all parameters have the same population of size n  The underlying graph has a cycle  see Figure    a    which makes the corresponding  CSP more difficult to solve than in the previous example  We can assign the value to W in n ways  and are left with n    possible values for X and Y   For Z we need to consider two cases  X   Y and X    Y  see Figure    b    In the X   Y case  Z can take n    values  while in the X    Y case  Z can have n    different values  Hence  the number of solutions to this CSP instance is n n        n n     n        The first case corresponds to a partition   X Y    of the set of parameters  X Y    while the second case corresponds to a partition   X    Y    of this set   First  we need to introduce a concept of a set partition  A partition of a set S is a collection  B            Bw   S of nonempty  pairwise disjoint subsets of S such that S   wi   Bi   The sets Bi are called blocks of the partition  Set partitions are intimately connected to equality  For any consistent set of equality assertions on parameters  there is a partition in which the parameters that are equal are in the same block  and the parameters that are not equal are in different blocks  If we consider a semantic mapping from parameters to individuals in the world  the inverse of this mapping  where two parameters that map to the same individual are in the same block  forms a partition of the parameters  The number of partitions of the set of size w is equal to the w th Bell number w   Bell numbers grow faster than any exponential function  see Lovasz          but for small ws they stay much smaller than exponential functions with a moderate base  see Figure      In general  to perform this kind of reasoning  we need to triangulate the constraint graph  This can be naturally achieved with a variable elimination algorithm  Each new edge adds two cases  one in which the edge corresponds to the equality constraint and one in which it corresponds to the inequality constraint  Some cases are inconsistent and can be ignored  When we have to analyze a fully connected subgraph of w new edges  we need to consider w cases  This is because each such case corresponds to a partition of the parameters from the subgraph  those parameters in the same block of the partition are equal  and parameters in different blocks are not equal  The number of such partitions is equal to w   The lifted  CSP solver analyzes w partitions of parameters  rather than nw ground substitutions of individuals  Since we do not care about empty partitions  we will never have to consider more partitions than there are ground substitutions  As w corresponds to the induced width of a constraint graph  which we do not expect to be large  and n corresponds to the population size  potentially large   the difference between w and nw can be very big  see Figure      Consider a set of constraints  W    X  X    Y  X    Z   where all parameters have the same population of size n  The underlying constraint graph has a tree structure  which allows  In practice  parameters can be typed with different populations  from the very beginning as well as because of unary constraints   In such a situation  we can apply the above        KISYNSKI   POOLE  UAI       reasoning to any set of individuals that are indistinguishable as far as counting is concerned  For example  the intersection of all populations is a set of individuals for which we only need the size  there is no point in reasoning about each individual separately  Similarly  the elements from the population of a parameter that do not belong to the population of any other parameter can be grouped and treated together  In general  any individuals that are in the populations of the same group of parameters can be treated identically  all we need is to know how many there are   two parfactors are about to be multiplied and the precondition for multiplication is not satisfied   Example    In Example   we need to know the number   D Y      X    Y Y    a    where D X    D Y   and   D X      n  Let a  denote set  a  and a  denote set D X    a   The following factor has value   for substitutions to parameters X Y that are solutions to the above CSP and   otherwise   Shattering simplifies design and implementation of lifted inference procedures  in particular  construction of elimination ordering heuristics  Unfortunately  as we show in Theorem    it may lead to creation of large number of parfactors that would not be created by following the splitting as needed approach   Y a  a  a   X a  a  a   Partition s    X     Y      X Y      X    Y              After we eliminate Y from the above factor we obtain  X a  a   Partition s    X     X    n    n   Numbers n    and n    are obtained through analysis of partitions of X and Y present in the original factor and knowledge that a  represents   individual and a  represents n    individuals  From the second factor we can infer that   D Y      X    Y Y    a   equals n    if X   a and n    if X    a  If we assume that all populations of parameters forming a  CSP are sorted according to the same  arbitrary  ordering  sets of indistinguishable individuals can be generated through a single sweep of the populations  Each such set can be represented by listing all of its elements or by listing all elements from the corresponding population that do not belong to it  For each set we choose a more compact representation   An alternative  called shattering  was proposed by de Salvo Braz et al          They perform splitting at the beginning of the inference by doing all the splits that are required to ensure that for any two parameterized random variables present in considered parfactors the sets of random variables represented by them are either identical or disjoint   Shattering was also used in Milch et al           Theorem    Let  be a set of parfactors  Let Q be a subset of the set of all random variables represented by parameterized random variables present in parfactors in   Assume we want to compute the marginal JQ     Then   i  if neither of the algorithms performs propositionalization  then every split on substitution  X t   where t is a constant  performed by lifted inference with splitting as needed is also performed by lifted inference with shattering  subject to a renaming of parameters    ii  lifted inference with shattering might create exponentially more  in the maximum number of parameters in a parfactor  parfactors than lifted inference with splitting as needed  Proof  We present a sketch of a proof of the first statement and a constructive proof of the second statement   i  Assume that lifted inference with splitting as needed performs a split  We can track back the cause of this to the initial set of parfactors   Further analysis shows that shattering the set  would also involve this split   ii  Consider the following set of parfactors      h      gQ     g   X    X            Xk     The answer from the solver needs to be translated to sets of substitutions and constraints accompanying each computed value  Standard combinatorial enumeration algorithms can do this task      THEORETICAL RESULTS  g   X    X            Xk         gk  Xk      F  i        h      g   a  X            Xk     F  i        h      g   a  X            Xk     F  i             In this section we discuss consequences of different approaches to constraint processing in lifted inference       SPLITTING AS NEEDED VS  SHATTERING  Poole        proposed a scheme in which splitting is performed as needed through the process of inference when  h      gk   a  Xk     Fk  i    k      h      gk  a    Fk i    k   and let Q be gQ       Shattering might also be necessary in the middle of inference if propositionalization has been performed    UAI       KISYNSKI   POOLE  For i              k  a set of random variables represented by a parameterized random variable gi  Xi           Xk   in a parfactor     is a proper superset of a set of random variables represented by a parameterized random variable gi  a  Xi             Xk   in a parfactor  i   Therefore lifted inference with shattering needs to perform several splits  Since the order of splits during shattering does not matter here  assume that the first operation is a split of the parfactor     on a substitution  X   a  which creates a parfactor h      gQ     g   a  X            Xk    g   X    X            Xk            gk  Xk      F  i   k       and a residual parfactor h X     a    gQ     g   X    X            Xk    g   X    X            Xk            gk  Xk      F  i    k       In both newly created parfactors  for i              k  a set of random variables represented by a parameterized random variable gi  Xi           Xk   is a proper superset of a set of random variables represented by a parameterized random variable gi  a  Xi             Xk   in a parfactor  i  and shattering proceeds with further splits of both parfactors  Assume that in next step parfactors  k      and  k      are split on a substitution  X   a   The splits result in four new parfactors  The result of the split of the parfactor  k      on  X   a  contains a parameterized random variable g   a  a          Xk   and a parfactor     needs to be split on a substitution  X   a   The shattering process continues following a scheme described above  It terminates after  k    k    splits and results in  k      parfactors  each original parfactor  i   i              k  is shattered into  ki parfactors   Assume that lifted inference proceeds with an elimination ordering g            gk  this elimination ordering does not introduce counting formulas  other do   To compute the marginal JgQ         k lifted multiplications and  k      lifted summations are performed  Consider lifted inference with splitting as needed  Assume it follows an elimination ordering g            gk   A set of random variables represented by a parameterized random variable g   X            Xk   in a parfactor     is a proper superset of a set of random variables represented by a parameterized random variable g   a  X            Xk   in a parfactor     and the parfactor     is split on a substitution  X   a   The split results in parfactors identical to the parfactors  k      and  k      from the description of shattering above  The parfactor  k      is multiplied by the parfactor     and all instances of g   a  X            Xk   are summed out from their product while all instances of g   X    X            Xk    subject to a constraint X     a  are summed out from the parfactor  k       The summations create two parfactors  h      gQ     g   X    X            Xk            gk  Xk      FFk   i    k       h      gQ     g   X    X            Xk            gk  Xk      FFk   i     k            Instances of g  are eliminated next  Parfactors  k      and  k      are split on a substitution  X   a   the results of the splits and a parfactor     are multiplied together and the residual parfactors are multiplied together  Then  all instances of g   a  X            Xk   are summed out from the first product product while all instances of g   X            Xk    subject to a constraint X     a  are summed out from the second product  The elimination of g            gk looks the same as for g    In total   k    splits   k    lifted multiplications and  k lifted summations are performed  At any moment  the maximum number of parfactors is k      The above theorem shows that shattering approach is never better and sometimes worse than splitting as needed  It is worth pointing out that splitting as needed approach complicates the design of an elimination ordering heuristic       NORMAL FORM PARFACTORS VS   CSP SOLVER  Normal form parfactors were introduced by Milch et al         in the context of counting formulas  Counting formulas are parameterized random variables that let us compactly represent a special form of probabilistic dependencies between instances of a parameterized random variable  Milch et al         require all parfactors to be in normal form to eliminate the need to use a separate constraint solver to solve  CSP  The requirement is enforced by splitting parfactors that are not in normal form on appropriate substitutions  While parfactors that involve counting formulas must be in normal form  that is not necessary for parfactors without counting formulas  It might actually be quite expensive as we show in this section  Proposition    Let hC   V  Fi be a parfactor in normal form  Then each connected component of the constraint graph corresponding to C is fully connected  Proof  Proposition   is trivially true for components with one or two parameters  Let us consider a connected component with more than two parameters  Suppose  contrary to our claim  that there are two parameters X and Y with no edge between them  Since the component is connected  there exists a path X  Z    Z       Zm  Y   As C is in normal form  EZCi   Zi       EZCi     Zi    i              m    and EZCm   Y     EYC   Zm    We have X  EZC    and consequently X  EYC   This contradicts our assumption that there is no edge between X and Y   While the above property simplifies solving  CSP for a set of constraints from a parfactor in normal form it also has negative consequences  If a parfactor is not in normal form  conversion to normal from might require several splits  For example we need three splits to convert a parfactor with the set of constraints shown in Figure    a  to a set of four parfactors in normal form  The resulting sets of constraints        KISYNSKI   POOLE W  W W  Z    X  Y          W  Z  X Y          X      X          Y  Y                   Z  Z  Figure    Constraint graphs obtained through a conversion to normal form  are presented in Figure    If the underlying graph is sparse  conversion might be very expensive as we show in the example below  Example     Consider a parfactor h X     a  X     X            X     Xk     g   X     g   X             gn  Xk     Fi  where D X      D X           D Xk    Let C denote a set of constraints from this parfactor  We have EXC     a  X            Xk   and EXCi    X     i              k  The parfactor is not in normal form because EXC    Xi       EXCi   X     i              k  As a result the size of the set D X      C depends on other parameters in the parfactor  For instance  it differs for X    a and X     a or for X    X  and X     X    A conversion of the considered parfactor to set of parfactors in normal form involves  k    splits on substitutions of the form  Xi  a      i  k and ki   ki  i     splits on substitutions of the   form  Xi  X j       i  j  k  It creates ki   ki i parfactors in normal form  In Example    we analyze how this conversion affects parfactor multiplication compared to the use of a  CSP solver  From the above example we can clearly see that the cost of converting a parfactor to normal form can be worse than exponential  Moreover  converting parfactors to normal form may be very inefficient when analyzed in context of parfactor multiplication  see Section        or summing out a parameterized random variable from a parfactor  see Section         Our empirical tests  see Section      confirm this observation  Note that splitting as needed can be used together with a  CSP solver  Poole         or with normal form parfactors  Shattering can be used with a  CSP solver  de Salvo Braz et al         or with normal form parfactors  Milch et al          The cost of converting parfactors to normal form might be amplified if it is combined with shattering         Multiplication  In the example below we demonstrate how the normal form requirement might lead to a lot of  otherwise unnecessary  parfactor multiplications  Example     Assume we would like to multiply the parfactor from Example    by a parfactor p f   h      g   X      F  i  First  let us consider how it is done with a  CSP solver  A  CSP solver computes the num   UAI       ber of factors the parfactor from Example    represents     D X         k     Next the solver computes the number of factors represented by the parfactor p f   which is trivially   D X       A correction is applied to values of the factor F  to compensate for the difference between these two numbers  Finally the two parfactors are multiplied  The whole operation involved two calls to a  CSP solver  one correction and one parfactor multiplication  Now  let us see how it can be done without the use of  CSP solver    The first parfactor is converted to a set  of ki   ki i parfactors in normal form  as presented in Example     Some of the parfactors in  contain a parameterized random variable g   a   the rest contains a parameterized random variable g   X  and a constraint X     a  so the parfactor p f needs to be split on a substitution  X   a   The split results in a parfactor h      g   a    F  i and a residual h X     a    g   X      F  i  Next  each parfactor from  is multiplied by  either the result of the split or the residual  Thus ki   ki i parfactor multiplications need to be performed and most of these multiplication require a correction prior to the actual parfactor multiplication  There is an opportunity for some optimization  as factor components of parfactors multiplications for different corrections could be cached and reused instead of being recomputed  Still  even with such a caching mechanism  multiple parfactor multiplications would be performed compared to just one multiplication when a  CSP solver is used         Summing Out  Examples   and   demonstrate how summing out a parameterized variable from a parfactor that is not in normal form can be done with a help of a  CSP solver  In the example below we show how this operation would look if we convert the parfactor to a set of parfactors in normal form which does not require a  CSP solver  Example     Assume that we want to sum out f  X Y   from the parfactor h X    Y Y    a    e X   f  X Y     Fe f i from the Example    First  we convert it to a set of parfactors in normal form by splitting on a substitutions  X a   We obtain two parfactors in normal form  h Y    a    e a   f  a Y     Fe f i  which represents n  factors  and h X    Y  X    a Y    a    e X   f  X Y     Fe f i  which represents  n     n     factors  Next we sum out f  a Y   from the first parfactor and f  X Y   from the second parfactor  In both cases a correction will be necessary  as Y will no longer among parameters of random variables and the resulting parfactors will represent fewer factors than the original parfactors  In general  as illustrated by Examples      and     conversion to normal form and  CSP solver create the same number of parfactors  The difference is  that the first method  computes a factor component for the resulting parfactors once and then applies a different correction for each result    UAI       KISYNSKI   POOLE  ing parfactor based on the answer from the  CSP solver  The second method computes the factor component multiple times  once for each resulting parfactor  but does not use a  CSP solver  As these factor components  before applying a correction  are identical  redundant computations could be eliminated by caching  We successfully adopted a caching mechanism in our empirical test  Section       but expect it to be less effective for larger problems                                      As in the case of splitting as needed  it might be difficult to design an efficient elimination ordering heuristic that would work with a  CSP solver  This is because we do not known in advance how many parfactors will be obtained as a result of summing out  We need to run a  CSP solver to obtain this information                      EXPERIMENTS  We used Java implementations of tested lifted inference methods  Tests were performed on an Intel Core   Duo     GHz processor with  GB of memory made available to the JVM       SPLITTING AS NEEDED VS  SHATTERING  In the first experiment we checked to what extent the overhead of the shattering approach can be minimized by using intensional representations and immutable objects that are shared whenever possible  We ran tests on the following set of parfactors       h      gQ     g   a    F  i        h X    a    gQ     g   X    F  i        h      g   X   g   X    F  i        h      g   X   g   X    F  i             h      gk   X   gk  X    Fk i    k   h      gk  X    Fk   i    k         All functors had the range size    and we set Q to the instance of gQ     We computed the marginal JQ     Lifted inference with shattering first performed total of k splits  then proceeded with  k     multiplications and  k summations regardless of the elimination ordering  Lifted inference with splitting as needed performed   split  k     multiplications and k     summations  for the experiment we used the best elimination ordering  that is gk   gk            g     Figure   shows the results of the experiment where we varied k from   to      Even though lifted inference with shattering used virtually the same amount of memory as lifted inference with splitting  it was slower because it performed more arithmetic operations                        Figure    Speedup of splitting as needed over shattering               CONVNFMSUM NFMSUM  CSPSUM                                                           Figure    Summing out with and without a  CSP solver        NORMAL FORM PARFACTORS VS   CSP SOLVER  For experiment in this section we randomly generated sets of parfactors  There were up to   parameterized random variables in each parfactor with range sizes varying from   to     Constraints sets contained very few  and very often zero  constraints and formed sparse CSPs  Most of parfactors were in normal form  which allowed us to account for  CSP solver overhead  There were up to    parameters present in each parfactor  Parameters were typed with the same population  We varied the size of this population from   to      to verify how well  CSP solver scaled for larger populations  In this experiment we summed out a parameterized random variable from a parfactor  We compared summing out with a help of a  CSP solver   CSP SUM  to summing out achieved by converting a parfactor to a set of parfactors in normal form and summing out a parameterized random variable from each obtained parfactor without a  CSP solver   We cached factor components as suggested in Section         For each population size we generated     parfactors and reported a cumulative time  For the second approach  we reported time including  CONV NFMSUM  and excluding  NFM SUM  conversion to normal form  Results presented on Figure   show significant cost of conversion to normal form and advantage of  CSP solver for larger population sizes            KISYNSKI   POOLE  CONCLUSIONS AND FUTURE WORK  In this paper we analyzed the impact of constraint processing on the efficiency of lifted inference  and explained why we cannot ignore its role in lifted inference  We showed that a choice of constraint processing strategy has big impact on efficiency of lifted inference  In particular  we discovered that shattering  de Salvo Braz et al         is never betterand sometimes worsethan splitting as needed  Poole         and that the conversion of parfactors to normal form  Milch et al         is an expensive alternative to using a specialized  CSP solver  Although in this paper we focused on exact lifted inference  our results are applicable to approximate lifted inference  For example  see the recent work of  Singla and Domingos        that uses shattering  It is difficult to design an elimination ordering heuristic that works well with the splitting as needed approach and a  CSP solver  We plan to address this problem in our future research  Acknowledgments The authors wish to thank Brian Milch for discussing the CFOVE algorithm with us  Peter Carbonetto  Michael Chiang and Mark Crowley provided many helpful suggestions during the preparation of the paper  This work was supported by NSERC grant to David Poole  
