 We present a generative model for representing and reasoning about the relationships among events in continuous time  We apply the model to the domain of networked and distributed computing environments where we fit the parameters of the model from timestamp observations  and then use hypothesis testing to discover dependencies between the events and changes in behavior for monitoring and diagnosis  After introducing the model  we present an EM algorithm for fitting the parameters and then present the hypothesis testing approach for both dependence discovery and change point detection  We validate the approach for both tasks using real data from a trace of network events at Microsoft Research Cambridge  Finally  we formalize the relationship between the proposed model and the noisy or gate for cases when time can be discretized      Introduction  The research described in this paper was motivated by the following real life application in the domain of networked distributed systems  In a modern enterprise network of scale  dependencies between hosts and network services are surprisingly complex  typically undocumented  and rarely static  Even though network management and troubleshooting rely on this information  automated discovery and monitoring of these dependencies remains an unsolved probJohn is now with Dickinson College  PA  Work done while a Researcher with Microsoft Research  Alex is with the University of California  Berkeley  CA  Work done while an intern with Microsoft Research    lem  In     we described a system called Constellation in which computers on the network cooperate to make this information available to all users of the network  Constellation takes a black box approach to locally  at each computer server in the network  learn explicit dependencies between its services using little more than the timings of packet transmission and reception  The black box approach is necessary since any more processing of the incoming and outgoing communication packages would imply prohibitive amounts of overhead on the computer server  The local models of dependency can then be recursively and distributively composed to provide a view of the global dependencies  In Constellation  computers on the network cooperate to make this information available to all users in the network  Constellation and its application to system wide tasks such as characterizing a networking site service and hosts dependencies for name resolution  web browsing  email  printing  reconfiguration planning and end user diagnosis are described in      This paper focuses on the probabilistic and statistical building blocks of that system  the probabilistic model used in the local learning  the EM algorithm used to fit the parameters of the model  and the statistics of the hypothesis testing used to determine the local dependencies  The model  which we call Continuous Time Noisy Or  CT NOR   takes as input sequences of input events and output events and their time stamps  It then models the interactions between the input events and output events as Poisson processes whose intensities are modulated by a  parameterized  function taking into account the distance in time between the input and output events  Through this function the domain expert is able to explicitly encode knowledge about the domain  The paper makes the following contributions       Develops an EM algorithm for fitting all the parameters of this model and an algorithm for dependence discovery and change point detection based on statistical hypothesis testing     Evaluates the performance of the model and the inference procedures both on synthetic data and on real life data taken from a substantial trace of a large computer network     Formalizes the relationship between CT NOR and the noisy or  NOR  gate      when the time between the events can be discretized  This paper is organized as follows  Section   describes the model and Section   describes the EM algorithm for fitting the parameters  Section   is concerned with the relation to the NOR gate  The algorithms and framework for applying the model to dependency discovery and change point detection is described in Section    That section also contains validation experiments with synthetic data  Section   contains experiments on real data and results  Finally  Section   has some conclusions and future work      The CT NOR model  In this section we formally describe the CT NOR model with the objective of building the likelihood equation  First  we provide some background on Poisson Processes  and then we use them to construct the model  Eq      A Poisson Process  can be thought of as random process  samples from which take the form of a set of times at which events occurred  A Poisson Process is defined over a mean  base  measure f  t  and is characterized the property that for any interval  t    t     the number of events that occur in that interval follows the Poisson distribution with the param t eter t   f  t dt  Furthermore  the number of events that occur on two disjoint intervals are independent    j   time of the lth output event and ik the time of the kth input on channel j  Furthermore  let n denote the number of output events and n j  the number of input events on channel j  Then event k in input channel j generates a Poisson process  j  of output events with the base measure pk  t     j  w j  f  t  ik    The term w j  represents the average number of output events that we expect each input event on channel j to be responsible for  and f  t  is the distribution of the delay between an input and the output events caused by it  taking as its argument the delay between the time of the output ol and the time of  j  the input ik   The mathematical structure of the intensity makes intuitive sense  the probability that a given input event caused a given output event depends on both the expected number of events it generates and the distance in time between them  We recall that given multiple independent Poisson processes  denoted as P P   we can use the sum of their intensities to construct a global Poisson proP Pn j   j  pk  t   as the cess and write  ol    P P   j k   probability of the set of n outputs  ol       l  n  The double sum runs over all the channels and over all input events in the channels  Intuitively  and similar to the NOR gate in graphical models       the independence between the between input channels translates into a model where the events in the output channel are caused by the presence of any  a disjunction  of input events in the input channels  with some uncertainty   The formal relation with NOR is presented in Section    We now proceed to write the likelihood of the data given P  j the j model and the input events  Let    w   the total mass of the Poisson base meajn sure  The number n of outputs is distributed as a Poisson distribution  n         P oisson        Let us use channel to denote a sequence of events  The CT NOR model considers a single output channel and a set of input channels  Let ol denote the   This overview is very informal  The more general and formal measure theoretic definition can be found in        In the domain of computer networks  a channel refers to a unidirectional flow of networked packets  Thus a channel will be identified by the service  e g   HTTP  LDAP  etc  and the IP address of the source or destination  In this paper we identified the packets with events as it is only their time stamp that matters   and the location of a specific output event ol is distributed with the probability density  ol      P Pn j  j  k     P Pn j  j  k     j   pk  ol    for l           n        j   w j  f  ol  ik           The likelihood of observing a set  ol   of outputs is     L o i    n  e  n X  j   j  Y w f  ol  i   k  l   jk         Before concluding this section  we expand a bit on the function f as it is an important part of the model  This function provides us with the opportunity of encoding domain knowledge regarding the expected shape of the delay between input and output events  In our experience using CT NOR to model an enterprise network we used two specific instantiations  a mixture of a narrow uniform and a decaying exponential and a mixture of a uniform and Gaussian  The uniform distribution captures the expert knowledge that a lot of the protocols involve a response within a window of time  we call this co occurrence   The Gaussian delay distribution extends the intuitions of co occurrence within a window to also capture dependencies that can be relatively far away in time  such as with the printer   The left tail of the Gaussian corresponding to negative delays is truncated  The exponential distribution captures the intuition that the possibility of dependency decays as the events are further away in time  this is true for the HTTP protocol   We will not explicitly expand these functions in the derivations as they tend to obscure the exposition  Needless to say that the parameters of these functions are all fitted automatically using EM as described in the next section  Groups of channels may have different delay distributions  in which case the delay distribution can be indexed by the channel group and all the derivations in this paper remain the same  For example  channels can be grouped by network service  where all HTTP channels have the same delay distribution  thus allowing data from multiple channels to assist in parameter fitting   but the DNS channels are allowed a different delay distribution  All the experiments in the paper use a leak  a pseudo channel with a single event at the start of the observation period and a delay distribution that is uniform over the length of the observations  This leak captures events which are not explained by the remaining channels      Fitting a CT NOR model  We perform inference and estimation on the model through the EM algorithm  We first set the stage by finding a suitable bounding function B z  for the likelihood  The EM algorithm iteratively chooses a tight bound in the E step and then maximizes the  j  bound in the M step  Let zkl be some positive vector P  j   j  such that jk zkl     for each l  For a fixed l  zkl is the probability of the latent state indicating that packet k on channel j caused output l  Then from Eq     log L o i        n X  X  w j  f  ol  ik    log  X  zkl  l          n X  jk  l          n X   j   log   j  w  Since the Poisson Process produces unordered outputs but the events are considered to be sorted  a permutation factor of n  is required  It cancels out the n  in the Poisson density    j   f  ol  ik    j   zkl  jk  log Ez   j   w j  f  ol  ik    j   zkl  l    Now  by Jensens inequality  log L o i   B z  where   j   B z        X  Ez log  w j  f  ol  ik    j   zkl  l       E Step  For a particular choice of   the parameters of the f function  and w j    the bound above is tight when  j    j   zkl   P  w j  f  ol  ik    j     j  k  because in that case   w j    f  ol  ik    j   w  j  f  ol ik    is a constant for   j  zkl  a fixed l and E log C   log EC   log C  Therefore   j  we use these choice of zkl        M step  j   For a fixed choice of zkl   we need to maximize the bound with respect to w j  and   Optimizing with respect to w j    we notice that the derivative is X X  j    B  j    n   zkl  j  w j  w l      j   yielding  w   j       j  kl zkl n j   P  k   With respect to   we can say that X  j   j  zkl log f  ol  ik      arg max   jkl  which is simply the parts of the objective function that depend on   This can be a very easy optimization problem for a large class of distributions  as it is of the same form as maximum likelihood parameter observation given observed data points and corresponding counts  For example  for the exponential family  this simply requires moment matching        P   j   jkl   j   zkl T  ol ik   P  j  jkl zkl  where    is the mean  parameterization of the estimated parameter  and T    are the sufficient statistics for the family      Relation to Noisy Or  As an alternative model  consider binning the observed data into windows of width  and modeling the presence or absence of output events in a particular bin as a NOR       The possible explanations  parents  are the presence of input events in preceding windows  We will show that a particular  natural parameterization of the NOR model is equivalent to CT NOR in the limit  as the bin width approaches zero  This relationship is important because it provides a nontrivial extension of NOR to domains with continuous time and provides insight into the independence structure of the two models  Let Ot be an indicator of presence of output events  j  between the times t and t    and It be the indicator for input events from channel j in that same time period  We will use PNOR to denote the probability under the NOR model and PCT NOR for probability under CT NOR   of distributions  this parameterization imposes only minor constraints on the weights  but will be useful for reasoning about NOR models which model the same data but with differing bin widths  When the bin width is halved  the probability that one of the sub bins has an output event must be equal to the probability that the large bin has an output event plus a second order term  This condition is required for a coherent parameterization of a family of NOR distributions and follows from the technical conditions placed on f   We argue that as the bin width  decreases  this model becomes equivalent to a CT NOR with a suitable choice of parameters  Choose a  sufficiently small that each bin contains at most one input event per channel  and at most one output event  We will t use PNOR to denote PNOR  Ot     Input   the probability that the tth bin has no output events falling into it   t PNOR       YY     w j  f   t  s  I j  s j s t     YY  j     k        j     w j  f  t  ik     o       XX  j  k  Under a CT NOR model which uses the same w j  and the same f   the probability of not observing any outputs is very similar  We use  to denote the parameter of the Poisson random variable governing the number of outputs in the interval        XX j  PNOR  Ot      Input     YY        j    p ts  I j  s      j s t   j   The p ts  is the weight associated with the possible  j   explanation Is   To prevent the number of parameters from increasing as the bin size becomes small  reparameterize with  j   p ts    w j  f   t  s   for any distribution f that satisfies some technical conditions   Since f may be a very flexible family    It is sufficient for the density to exist and be Lips      j  w j  f  t  ik     o         t   t  k  XX j  t PCT NOR  w j  w   j    j   f  x  ik  dx  j   f  t  ik     o       k    P  Poisson          exp           o      t   PNOR   o       chitz  which means that there exists a constant C such that  f  a   f  b    C a  b  for any a  b  Any continuously differentiable function with a bounded derivative satisfies this condition  It is easy to extend this proof to any bounded density with a finite number of discontinuities which has a bounded derivative everywhere except for the discontinuities    These results can be combined to demonstrate that the probability assigned to any set of output events by the two models is equal up a factor of      o n   which converges to   as  decreases to zero  The asymptotics are in terms of bin width  decreasing to zero for a fixed set of observations  so n and T are constant   PNOR  Out In  PCT NOR  Out In    Ot    Ot T     t t Y    PNOR PNOR   t t PCT NOR    PCT NOR t          o      T  n       o   n          T    n o             no            T   n o           o    CT NOR and NOR with an increasingly small bin size assign equivalent probability to any sequence of output events  indicating that the two classes of models are closely related  and that CT NOR is the model that emerges as the limit when the NORs bin size is decreased toward zero      Dependence discovery and change point detection  With the probabilistic framework described in the previous section  we can use statistical machinery to perform inference for two applications  a  inputoutput relation discovery and b  change point detection  The next two subsections describe the algorithms in detail and also validate the main assumptions using synthetically generated data  The final subsection       describes a computationally efficient approximation to the hypothesis test procedures       Dependence discovery  For the purposes of network management  a crucial problem is dependence discovery  For each computer in the network  we are interested in automatically finding out from observations which input channels have a causal effect on an output channel  We can frame the dependency discovery task as hypothesis testing  Specifically  testing whether an input channel j causes output events corresponds to testing the hypothesis that w j       One way of testing this hypothesis is through the likelihood ratio test       We fit two models  Mfull   under which   all the parameters are unrestricted  and Mres   under which w j  is constrained to be zero  The test statistic in this case is   log      log  LMres  Data  LMfull  Data   The asymptotic distribution of this test statistic is called a   and is a mixture of   with different degrees of freedom  The weights depend on the Fisher information matrix and are difficult to compute     but the significant terms in the mixture are    and    which is a delta function at zero  The   emerges as the null distribution instead of the more familiar   because the weight parameters w   are constrained to be non negative  and when an estimated w j  is zero in the unconstrained model  imposing the constraint does not change the likelihood  If a set of true null hypotheses is known  the mixture coefficients can be trivially estimated  with the weight of    being the proportion of test statistics that are    When no ground truth is available  the proportion of null hypotheses can be estimated using the method described in      and then used to estimate the mixture proportions  To demonstrate that the model efficiently recovers the true causal channels and has the proper teststatistic distribution under the null hypothesis  we first test the model on synthetic data that is generated according to some instantiation of the model     input channels are generated  half of them have no causal impact on output events and half produce a Poisson       number of output events with the delay distribution of Exponential       Note that the causality is weak  very few input events actively produce an output  For each hour      input events per channel  the corresponding output events  and     uniformly random noise events  which are not caused by any input activity  are produced  The resulting p values are plotted in Figure    Observe that the null p values  conditioned on the test statistic being non zero  are distributed uniformly  This is evidenced by the p values following the diagonal on the quantile quantile plot  The alternative p values  without any conditioning  for channels which exhibit causality are mostly very low  with     being below      Furthermore  the specific parameter estimates  the delay distribution parameter and w j    are in line with their true values       Changepoint Detection  When the relationship between events is altered  it can be an indication of a significant change in the                                 PValues                             PValues                                          Uniform                      Uniform  Figure    Quantile quantile plot of dependency discovery p values for   hours of synthetic data  The red circles are the distribution of p values for the null hypotheses  and are uniform  The blue triangles show p values of the alternative hypotheses and are small  indicating power   Figure    Quantile quantile plot of the p values for changepoint detection on synthetic data  The red circles are null hypotheses  no changepoint   the green diamonds are a weak alternative  w j  increases from      to       and the blue triangles are a strong alternative  w j  increases from      to         system  in the case of Constellation  this is of interest to the system administrators  We describe a building block for identifying whether the parameters w j  change between two time periods and demonstrate its correct functionality  Changepoint algorithms have long been studied in machine learning and statistics  and our test for whether the behavior of a parameter is altered between two time periods can be plugged into one of many existing algorithms  Furthermore  the simple two period test described here is sufficient for many monitoring applications   the weight changes   We again use the log likelihood ratio test methodology  In order to do that  it is necessary to extend the model to allow the parameters to depend on time  The model can be written as   X X  j   j   o   P P  w  j  f  ol  ik     j  k  ik  Detecting changepoints is accomplished by testing two hypotheses  The null is that the weights do not change between two time periods  and can be written  j  as wt   w j    Under the alternative  for a particular channel of interest m and an interval of time S   j    m   j   wt   m   wt    w j    w m  if t  S  w m  otherwise   The existence of a changepoint is equivalent to rejecting the null hypothesis  Fitting the alternative model is a simple modification of the EM procedure described for the null model  for fast performance  it is possible to initialize at the null models parameter values and take a single M step  reusing the latent variable distribution estimated in the E step  The test statistic in this case will again be   log  and its null distribution will be   if the true w m      and   otherwise  Figure   shows a quantile quantile plot of the pvalues  computed using the   distribution  under the null hypothesis  computed for causal channels of the same synthetic data as in section      there are two hours of data with     input events per channel per hour  As expected  the quantile quantile plot forms a straight line  demonstrating that on the synthetic dataset  the null test statistic has a   distribution  When a strong changepoint is observed  w j  changes from      to         the p values are very low  When a weak changepoint is observed  w j  changes   from      to       the p values are lower than under the null distribution but power is significantly lower than when detecting the major changepoint   ROC for HTTP         Bounding the log likelihood ratio  Computing the log likelihood ratio requires refitting a restricted model  though only a small number of EM steps is typically required  However  it is possible to bound the log likelihood ratio for dependency discovery very efficiently   True Positive Rate                 NoisyOR with bounds NoisyOR with exact computations Unamb  coocc  Std  coocc               For the restricted model testing channel ms causality  we must compute the likelihood under the constraint that w m       Take the estimates of w of the  unrestricted model and let    w m    Instead n m  of computing the ratio with the true maximum likelihood parameters for the restricted model  we propose a set of restricted parameters  and compute the ratio using them  We produce a restricted version of parameters w   by setting w m  to zero and inflating the rest by a factor of   That simply corresponds to imposing the restriction  and redistributing the weight among the rest of the parameters  so that the expected number of output packets remains the same  In that case   LMres  Data  LMf ull  Data  P Y j  m k w j  f  ol  i j  k      log P  j   j  f  ol  ik   l jk w   P  m   m  Y f  ol  ik   kw     log    P  j  f  o  i j     l l jk w k   Y X  j      log    zml    log      log  l  k  j As a reminder  zml is the latent variable distribution estimated in the E step of EM  Since the numerator of the log likelihood ratio is a lower bound and the denominator exact  this expression is a lower bound P  j    Q   on   Intuitively  log l    k zml corresponds to the probability that channel m has exactly   output events assigned to it when causality is assigned according to the EM distribution on the latent variables   The log  term corresponds to the increase in likelihood from redistributing channel ms weight among the other channels                            FDR  Figure    ROC for CT NOR and competing algorithms on data from a real enterprise network  Both the exact an approximate CT NOR tests produce detection results superior to the alternative methods      Results  We describe the results of applying the algorithms of the previous section to a subset of a real dataset consisting of a trace comprising headers and partial payload of around    billions packets collected over a     week period in      at Microsoft Research in Cambridge  England  This site contains about     networked machines and the trace captures conversations over      off site IP addresses  Ground truth for dependence discovery and change point detection is not readily available and it has to be manually generated  We took    hours of data at the web proxy and manually extracted ground truth for the HTTP traffic at this server by deep inspection of HTTP packets  It is with this part of the data that we validate our algorithms  as it provides us with objective metrics  such as precision and recall  to assess the performance of our algorithms       Dependency Discovery  First  we are interested in assessing the performance of the dependence discovery capabilities of our model and hypothesis testing algorithm  In the application of diagnosis and monitoring of networked systems it is crucial to maintain a consistent map of all the server and services inter dependencies and their changes  Finding dependencies at the server level is the main building block used by Constellation     in building this global map  We compare our method to two other alternatives  One is a simple binomial test  for each input channel  we count the number of output packets falling within a W width window of        Changepoint Detection  Since the true presence or absence of a changepoint is unknown  we estimate it from the actual packet causes  obtained through deep inspection of HTTP packets  We collect a set of input and output channel pairs for which there is no evidence of change  We regard these as coming from the null hypothesis  A set of pairs for which the ground truth provides strong evidence of a change are collected  and considered to be from the alternative hypothesis  We apply our changepoint test to that population  and report the results in Figure    The CT NOR changepoint detection algorithm produces uniformly distributed p values for channels which come from the null hypothesis and do not exhibit a changepoint  confirming that our null hypothesis distribution is calibrated  On the other hand  the test on alterna  As sometimes an input package generates more than one output packet  we enabled our model to account for this by allowing autocorrelations to take place  Namely a packet in an output channel can depend on an input channel or on the  time wise  preceding output packet                            As can be seen on the ROC curve in Figure    CT NOR successfully captures     of the true correlations with a    false positive rate  In total  the model detects     of the true correlations at     of false positives  We want to additionally point out that some of correlations present are very subtle      of the correlations are evidenced by a single output packet  We also point out that CT NOR performs significantly better than both alternatives based on co occurrence of input packets  providing even more conclusive evidence that CT NOR is capturing nontrivial dependencies  The approximation error from using the bound of section     is minimal  while the computation savings are significant  On a relatively slow laptop  the bounds on log likelihood ratio test for a hour of traffic on a busy HTTP proxy can be computed in   seconds  exact computations take    seconds   PValues  an input packet  and determine whether that number is significantly higher than if the output packets were uniformly distributed  We call this standard co occurrence  The second alternative considers an input and output channel to be dependent only if there is a unique input packet in the immediate vicinity of an output packet  The reason we select these two alternatives is that a  they reflect  by and large  current heuristics used in the systems community     and b  they will capture essentially the easy dependencies  as our results indicate                                   Uniform  Figure    Quantile Quantile plot of changepoint pvalues  The red circles are channel pairs which  according to the ground truth  not exhibit a changepoint  The blue triangles represent channel pairs exhibiting change according to the ground truth  tive hypothesis channels produces a large proportion of very small p values  indicating confidence that a changepoint occurred      Conclusions and Future Work  We presented a generative model based on Poisson processes called CT NOR  to model the relationship between events based on the time of their occurrences  The model is induced from data only containing information about the time stamps for the events  This capability is crucial in the domain of networked systems as collecting any other type of information would entail prohibitive amounts of overhead  Specific domain knowledge about the expected shape of the distribution of the time delay between events can be incorporated to the model using a parameterized function  The EM algorithm used to fit the parameters of the model given the data also induces the parameters of this function  The combination of knowledge engineering and learning from data is clearly exemplified in the application we presented to the domain of computer systems  where we used a mixture model consisting of an exponential and a uniform distribution  In terms of applying the model we focused on providing building blocks for diagnosis and monitoring    We provided algorithms based on statistical hypothesis testing for  a  discovering the dependencies between input and output channels in computer networks  and for  b  finding changes in expected behavior  change point detection   We validated these algorithms first on synthetic data  and then on a subset  HTTP traffic  of a trace of real data from events in a corporate communication network containing     computers and servers  The relationship presented in Section   between CT NOR and the NOR gate is interesting for multiple reasons  First  as the NOR gate has been extensively studied in this community in modeling and learning environment and in causal discovery      the immediate benefits are a  increasing the applicability to continuous time  and b  augmenting its modeling capabilities using the time delay functions used in this work  Second  this correspondence provide us with another intuition on the independence assumptions behind the Poisson process  as applied to the characterization of the relationship between the events in various inputs to the events in a specific output  For the particular application of dependency discovery between channels in a computer network we explored a varied set of alternative approaches  They all failed miserably  Among these  we briefly discuss two  We cast the problem as one of classification  and tried a host of Bayesian network classifiers      The idea was to first discretize time into suitable periods  and then have as features the existence or absence of events in the input channels and as the class the existence or absence of events in the output channel  The accuracy was abysmal  The main problem with this approach is that the communication in these networks is bursty by nature with relatively large periods of quiet time  Once we started to look at Poisson as the appropriate way to quantify the distributions in these classifiers the choice of the Poisson process became clear  We also explored the use of hypothesis testing comparing the inter time between events in the input and output channels to the inter time between the input and a fictitious random channel  The accuracy in terms of false positives and true positives was worse than those based on co occurrence  The main problem here is that we are considering pairwise interactions and there are many confounder in all the other channels  With regards to related approaches  both the work on continuous time Bayesian networks      and in general about dynamic Bayesian networks  e g        are obviously very different in terms of the parameterization of the models  the assumptions  and the  intended application  The work that is closest to ours is contained in the paper by Rajaram et al      where they propose a  graphical  model for point processes in terms of Poisson Networks  The main difference between their work and ours is the tradeoff between representation capabilities and complexity in inference that the different foci of our respective papers entails  Due to the distributed nature of our application domain  we concentrate on modeling the families  local parent child relationship  and basically assume that we can reconstruct  in a distributed manner based on the local information  the topology of the network  This enables us to induce families with large numbers of parents  and with relatively complex interactions as given by the delay function f   while performing inference efficiently  In the Poisson Networks paper       the number of parents of each node are restricted  and the rate function is parameterized by a generalized linear model  Even with these  relatively benign  restrictions inference is non trivial in terms of finding the structure of the Bayesian network and indeed this is a contribution of that paper  Obviously  future work includes merging both approaches  an immediate benefit would be to decrease the vulnerability of our approach to spurious causal dependencies due to ignoring the global structure in the estimation  There are other three threads that we are currently investigating for future work  The first one involves recasting the fitting and inference procedures described in the model in the Bayesian framework  An advantage of the Bayesian approach will be on the inclusion of priors  As channels differ greatly on the number of events this can further increase the accuracy of discovery  A second direction is that of incorporating False Discovery Rates     calculations in order to accurately estimate false positives when we dont have ground truth regarding the relationship between the channels  As we are performing a large number of hypothesis tests  this becomes a necessity  In     we experimented with the basic approach described in      and we verified that the approach is very conservative in the context of the HTTP and DNS protocols where we do have ground truth  We plan to explore less conservative approaches such as the one described in      or adapt the one explored in      Finally we are in the process of getting suitable data and plan to apply this model to biological networks such as neurons that communicate with other neurons using spikes in electrical potential       Acknowledgments  We thank T  Graepel for comments on a previous version of this paper  We are also grateful for the helpful suggestions of the anonymous reviewers which we hope we have addressed to their satisfaction   
 There is an obvious need for improving the per formance and accuracy of a Bayesian network as new data is observed  Because of errors in model construction and changes in the dynamics of the domains  we cannot afford to ignore the infor mation in new data  W hile sequential update of parameters for a fixed structure can be accom plished using standard techniques  sequential up date of network structure is still an open problem  In this paper  we investigate sequential update of Bayesian networks were both parameters and structure are expected to change  We introduce a new approach that allows for the flexible ma nipulation of the tradeoff between the quality of the learned networks and the amount of informa tion that is maintained about past observations  We formally describe our approach including the necessary modifications to the scoring functions for learning Bayesian networks  evaluate its effec tiveness through and empirical study  and extend it to the case of missing data      Introduction  Recently  there has been a great deal of effort in develop ing methods for learning Bayesian networks from data for density estimation  data analysis  and pattern classification  see     for a tutorial and an overview   This body of work  which includes both theoretical and experimental results  has concentrated mostly on batch learning methods  In this setting  the total corpus of data is fully available to the learning algorithm which outputs a model after multiple inspections of the data  In this paper we study the problem of sequential update of Bayesian networks  This problem is different from batch learning in two aspects   I  the learning procedure receives the data as a  read once  stream of observations  and     the learning procedure has to output a model  based on the observations seen so far  at various time points  possibly after each observation is made   Sequential update is a crucial capability for building adapt   able systems that can overcome errors in their initial model  and that can adapt to changes in the underlying distribu tion  We are especially interested in sequential update in situations where the learning procedure cannot store all of the past observations  or a complete summary of it  in main memory  Examples of such situations include monitoring systems which collect data over extended periods of time or embedded systems with limited memory capabilities  Memory constraints also arise in data mining applications that usually involve massive amount of data that must be kept on secondary storage  In these applications repeated inspection of the data is infeasible  We claim that effective sequential update of structure in volves a tradeoff between the quality of the learned net works and the amount of information that is maintained about past observations  We consider three approaches to sequential update  Two of these approaches lie on the ex tremes of the spectrum  The third approach  which is new  allows for a flexible manipulation of the tradeoff  The naive approach stores all the previously seen data  and repeatedly invokes a batch learning procedure after each new datum is recorded  This approach can use all of the information provided so far  and thus is essentially optimal in terms of the quality of the networks it can induce  This approach  however  requires vast amount of memory to store the entire corpus of data    We can attempt to avoid the overhead of storing all of the previously seen data instances by summarizing them using the model we have learned so far  This approach essentially assumes that the set of data instances being summarized are distributed according to the probability measure described by the current model  This approach  which we call MAP  for reasons that will become clear in Section       is space efficient  Unfortunately  by using the current model as a summary of past data  we strongly bias our learning proce dure towards that model  As a result  after some number of iterations  this approach locks itself into a particular model and stops adapting to new data     f course  it suffices to keep counts of the number of times  each distinct case was observed so far  This  however  is also impractical  since the number of distinct cases is exponential in  the number of variables of interest         Friedman and Goldszmidt  The third approach  which we call  incremental   provides a  as  X  Y  Z  for variable names and lowercase letters x  y  z  middle ground between the extremes defined by the naive  to denote specific values taken by those variables  Sets of  and the MAP approaches   variables are denoted by boldface capital letters  Moreover  it allows flexible  choices in the tradeoff between space and quality of the induced networks  The incremental approach interleaves steps in a search process  to find  good  models  with the incorporation of new data  This approach focuses its re sources on keeping track of just enough information to make the next decision in the search process  The basic strategy is to maintain a set of network candidates that are  on the  frontier  of this process   This set contains all of  the networks that are deemed most promising at the cur rent time  The procedure also keeps track of the required information needed for evaluating candidates on the fron tier  As we shall see  this information can be maintained in a space efficient manner  As each new data case arrives  the procedure updates the information stored in memory  and invokes the search process to check whether one of the networks in the frontier is deemed more suitable than the current model   We always assume that the current network is on the frontier   If this is the case  it adopts the new model as current model  and updates the search frontier  As we shall see  the amount of space required by the procedure is related to the size of the search frontier  The dynamics in the recording of information about the data raises a fundamental question with respect to the scor ing functions commonly used to evaluate different models  All of the scoring functions proposed in the literature as sume that alternative candidates are evaluated with respect to the same training data  In the incremental learning pro cedure  however  we can start recording the information for different candidates at different times  We propose mod ifications to the MDL score and the Bayesian score that deal with this complication  We empirically evaluate these  extended  scoring functions in conjunction with the incre mental learning procedure   denoted by boldface lowercase lettersx  y  z  we use  Val X   in the obvious way   Finally  let P be a joint probability distribution over the variables in U  and let X  Y  Z be sub sets of U  The sets X andY are conditionally independent given  P x I A  Z  z   if for all x E Va l X   y E Val Y   z y        P x I z   whenever P y  z        E  Val Z    Bayesian network is an annotated directed acyclic graph  that encodes a joint probability distribution of a set of ran  U   dom variables is a pair  B  Formally  a Bayesian network for         G       U  The first component  namely G   is a directed acyclic graph whose vertices correspond to the random variables  x        Xn  and whose edges rep  resent direct dependencies between the variables   The  graph G encodes the following set of independence as sumptions   Xi  each variable  is independent of its non  descendants given its parents in G   The second compo  e  represents the set of param eters that quantifies the network  It contains a parameter nent of the pair  namely  Bx Jpa xi        Pn xi I pa x    for each possible value Xi of X   and pa xi  of pa X    where pa Xi  denotes the set of parents of X  in G  A Bayesian network B de fines a unique joint probability distribution over  by   Pn Xt        Xn      rr  Pn Xi I pa X      U given  The problem of learning a Bayesian network can be stated as follows  Given a training set D         u          UN  of in stances of  U  find a network B that best matches D  The  common approach to this problem is to introduce a scoring function  or a  score   that evaluates the  fitness  of net  works with respect to the training data  and then to search for the best network  according to this score    The two  main scoring functions commonly used to learn Bayesian networks are the Bayesian score      Finally  we examine how to extend these methods to deal with incomplete data in sequential update  To this end  we propose a combination of two generalizations of the  expectation maximization algorithm  incremental EM      and model selection EM      The rest of this paper is organized as follows   X  Y  Z   and assignments of values to the variables in these sets are  In Sec  the principle of      and the one based on minimal description length  MDL           These scores are asymptotically equivalent as the sample size increases  Furthermore they are both asymptotically correct  with probability equal to one the learned distribu tion converges to the underlying distribution as the number of samples increases          tion    we briefly review the current practice of learning  In this paper we use the MDL score described in      which  Bayesian networks  In Section    we describe our approach  we denote as  for incremental update and develop the necessary theoret  introduced by Heckerman et  al      which we denote as S ne Details about these scores for batch learning can be  ical foundations  In Section    we perform an empirical evaluation of the three methods described above  In Section    we introduce the extension to missing data  We conclude in Section   with a discussion of related work  a summary of the main the results of this paper  and our plans for future work      found in      SMDL and the  BDe  variant of the Bayesian       What is of interest for the purposes of this  paper is to understand what information  from the training data  is needed to compute these scores  When the data is  complete   namely  each instance assigns  values to all the variables of interest  both scores have two attractive properties  The first property is that for any fixed  Learning Bayesian Networks  The Batch Method  network structure  G   there is a closed form formula for  finding the optimal parameters that maximize the score  Moreover  these parameters can be extracted from sufficient   X            Xn  of discrete random Consider a finite set U variables where each variable X  may take on values from a finite set  denoted by Val Xi   We use capital letters  such    statistics for the structure  G  To understand the notion of  sufficient statistics it is convenient to introduce additional notation  LetN  x  be the number of instances in D where  f   Sequential Update of Bayesian Network Structure  X x  Let Nf be the vector of numbers Nf x  for all values of X  from now on  we omit the superscript and the subscript of Nf whenever they are clear from the context   We call the vector Nx the sufficient statistics of X  As it turns out  we the optimal choice of parameters Bx lpa X   is a function of Nx  pa X    see  for example       Since the selection of parameters has a closed form  we can focus on choosing the best structure G for the network  The parameters are then easily computed from these sufficient statistics     The second property is decomposability of the score as signed to a structure  This means that the score S G I D  assigned to a structure G  in the context of a dataset D  has the general form  S G I D       L S X   pa X          whereS X   pa X    is the local score evaluating how good is the choice of parents for X   as determined by G   Let a family be the set composed by X  and its parents  The local score S X   pa X    depends only on the sufficient statistics for this family  Nx  pa X   Thus  if D and D  are  D  such that N D x   pa X     N x   pa X   then the local score of this particular family will be the same  The direct benefit of this property is computational  To evaluate the effect in the score that the addition  or removal  of an arc inG will have  we need only to recompute the local score of the families affected  In conclusion  for the purposes of learning using either the MDL or the Bayesian score  all the required information about the training data D is summarized by a set of sufficient statistics  These statistics are of the form Nx   pa X   for choices of pa X   in the set of networks considered during the search     Sequential Update of Bayesian Networks  Sequential update of Bayesian networks is an on line learn ing problem  At each iteration n  procedure receives a new data instance un  and then produces the next hypothesis Bn l   This estimate is then used by to perform the re quired task  e g   prediction  classification  diagnosis  etc   on the next instance Un     which in turn is used to update the network and so on  In practice  the procedure might generate a new model after some number of k instances are collected  see Section     this however  does not change the spirit of this discussion  An update procedure is evaluated by the cumulative loss in each step  This loss might be de fined in several ways  usually depending on the particular application  Thus  for example  in classification tasks  a natural measure of this loss is the number of misclassified instances  In density estimation tasks  the intent is to mea sure how well the procedure predicts the next instance  and the log loss  which is L n log PB   un   is commonly used  We start by formally describing the procedures for sequen tial update that define both ends of a spectrum in terms of the tradeoff between storage requirements and quality of       the induced networks  We then introduce the incremental approach in Section           The Naive and MAP Approaches  The naive approach to sequential update consists of storing all of the observed data  and then repeatedly invoking a batch learning procedure on u      Un to form the estimate Bn  Clearly  a procedure based on this strategy uses all of the observed information to construct the next estimate and consequently should yield optimal results  However  as noted in the introduction  this approach has unreasonable space requirements in the long run  It needs to store either all of the instances that have been observed  or keep a count of the number of times each distinct instantiation to all the variables in U was observed  The former representation grows linearly with the number of instances collected  and will become infeasible when the network is expected to perform for long periods of time  A good example of such a network  is the  alarm  network      which is part of a system for monitoring of intensive care patients  In this example  the domain consists of    variables that can have       distinct instantiations  Clearly  we cannot store counts for each possible instantiation observed in the data      An alternative approach is motivated by Bayesian learning methodology  Recall that in Bayesian analysis we start with a prior probability over possible hypotheses  models and their quantifications   and compute the posterior given our observations  In principle  we can then treat this posterior as our prior for the next iteration in the sequential process  Thus  we maintain our belief state about the possible hy potheses after observing u     Un I  Upon receiving Un  we compute the posterior  produce the estimate Bn     and store that posterior as our current belief state  This method ology has the attractive property that in the presence of some reasonable assumptions  the belief state at time n is the same as the posterior after seeing u           Un from our initial prior belief state  If we make the assumption that the structure of the network is fixed and we use conjugate priors  we can efficiently represent the posterior and update it after each iteration using a closed form formula           This approach  however  is infeasible when we also attempt to update the structure of the network  The BDe score is based on assumptions that allow us to compactly represent a prior using a single network and an equivalent sample size      Unfortunately  the posterior cannot be compactly represented  The conjugate form for this prior essentially requires storing a complete network  which is equivalent to storing the counts for all possible assignments to U  Since we cannot realize the exact Bayesian method  we can resort to the following approximation  At each step  we find  or approximate  the maximum a posteriori probability  MAP  network candidate  That is  the candidate that is considered most probable given the data seen so far  We then approximate the posterior in the next iteration by using the MAP network as the prior network with the appropriate equivalent sample size  In other words  this procedure uses the network Bn as a summary of the first n observations  This procedure is space efficient since we we only need to        Friedman and Goldszmidt  store the new instances that have been observed since we last performed the update of the MAP  An approach similar  To see how this generalizes to larger sets that covers a con siderable subset of the search space recall that the greedy hill  in spirit was proposed by Lam and Bacchus      in the  climbing search procedure works by comparing its current  context of the MDL score   G to all its neighbors  These neighbors are the networks that are one change away  i e   arc addition  dele candidate  Unfortunately  by using the MAP model as the prior for the next iteration of learning  we are loosing information  and are strongly biasing the learning process toward the MAP model itself  To illustrate this phenomena  consider  a scenario where  U  consists of two variables  X andY   In  this case  the learning procedure has to basically choose be tween two models   Bt where X is independent of Y  and  B  where X is dependent on Y  Now suppose that X and  Y are indeed correlated  but after observing the first  say     instances  the posterior probability of the model where  X  is independent of Y is higher  This can happen when  the number of training instances is too small to determine  whether observed correlations between X and Y are gen uine correlations or artifacts of sampling noise  Since Bt is the MAP model  we use it to represent our prior for the next iteration of the learning procedure  Now suppose we observe another    instances  and reconsider both models  At this stage  however  our prior is strongly biased toward  tion or reversal  from  G   Extending the argument above   we see that we can evaluate the set of neighbors of  G   by  maintaining a bounded set of sufficient statistics  Note that if  S  consists of all the sufficient statistics for  G  and its  neighbors  Nets S  contains additional networks  including many networks that add several arcs in distinct families in  G   Also note that if X C  Ny   Nets S  simpler than G  Thus   Y  then Nx can be recovered from  also contains many networks that are  Generalizing this discussion  Our approach applies to any search procedure that can define a  search frontier   This  frontier consists of all the networks it compares in the next iteration   We use F to denote this set of networks  The  choice ofF determines which sufficient statistics are main tained in memory   That is  we set  S  to contain all the  sufficient statistics needed to evaluate the networks in F  After a new instance is received  or  in general  after some  B  Thus  even if from the     samples we could have  number of new instances are received   the procedure uses  B  if the evidence  scoring network in the frontier F  or  more generally  in  determined that X and Y are correlated  the update strategy  based on the MAP model still selects for  B  contained in the new instances is again weak  This  phenomena becomes more pronounced as the equivalence sample size assigned to the prior grows   the sufficient statistics in  Nets S     S to evaluate and  select the best  Once this choice is made  it invokes the search  procedure to determine the next frontier  and updates  S  accordingly  This process may start recording new infor mation and may also remove some sufficient statistics from       memory   The Incremental Learning Procedure  The main loop of the incremental procedure can be now  In this section  we propose an approach that explores the middle ground between the two extremes discussed in the previous section   Unlike the naive approach  it does not  keep all possible data records  or an equivalent represen tation   and unlike the MAP approach  it does not rely on a single network to represent the prior information  The basic component of this procedure is a module that main tains a set  S of sufficient statistics  records  These records  allow the update procedure to select amongst a set of pos sible networks for the update   Before explaining the ap  proach in detail we introduce some necessary notation  Let  Suffi G  to denote the set of sufficient statistics for G  that is  Suffi G    Nx   pa Xi            i       n   Similarly  given a setS of sufficient statistics records  let Nets S  to be the    set of network structures that can be evaluated using the records inS  that is   Nets S       G  Suff G   S    G  and  G    As we established in Section  Set G to be initial network  Let F initial search frontier for G  LetS  Suff G  U UB EF Suff G    Forever Read data un  Update each record in S using un  if n mod k   then Let G argmaxaeNet   S  S G  I S  Update the frontier F  using a search procedure  Set S to Suff G  U Us eF Suff G    Compute optimal parameters e for G fromS  Output  G  El         This procedure focuses its resources on keeping track of just enough information to make the next decision in the search  Suppose that we are deliberating on the choice between two structures  described as follows       in order to use the MDL or BDe score  or variants thereof   to evaluate G  we need to maintain the set Suff G   and to  space  Every  its resources in preparation for the next iteration  from  Suff G  U Suffl  G   Suff G  U  NY pa Y       where pa Y    is the parent set of Y in G   Thus  we can  of  Namely      easily keep track of both these structures by maintaining a slightly larger set of statistics   This  reallocation may involve removing some sufficient statistics  evaluate G   we need to maintain the set Suff G    Now suppose that G and G  differ only by one arc from X toY   Then there is a large overlap between Suff  G  and Suff G     k steps  the procedure performs this decision   After each such decision is made  the procedure reallocates  S  and adding new ones   When we instantiate this procedure with the greedy hill climbing procedure  the frontier consists of all the neighbors  Bn  A beam search  on the other hand  can maintain j  candidates  and set the frontier to be all the neighbors of all j candidates  Other search procedures might explore only some of the neighbors of Bn and thus would have smaller        Sequential Update of Bayesian Network Structure  s earch  frontiers   normalizing factor  which is  of course  different for D  and  D     Thus  we cannot evaluate  P M ID    and  P M I D    The only unresolved issue in the description of this proce dure is the definition of the score S G I S   As we explain  in a closed form   in the next section  we cannot directly apply the scores introduced in Section     We believe that the comparison of models evaluated with respect to different but related data is a fundamental problem       question  Intuitively  we want a score that assigns higher  Scoring functions for Sequential Update  Ideally  we would like to rely on the standard scoring func tions reported in the literature to score and evaluate the different structures in the procedure described above  Un fortunately  both these scores make the assumption that we are evaluating all candidates  with respect to the same  that requires a principled solution  and we pose it as an open  confidence to families for which we have more data   What follows is our proposal which is based on modifying the existing MDL and BDe scores for learning Bayesian networks  This proposal satisfies a basic correctness prop erty  and furthermore  as our experimental results show   dataset   performs well in practice   different families at different times  the sufficient statistics  score can be casted in information theoretic terms  This assumption does not hold in our procedure  Since we may start collecting sufficient statistics records for  for the family of  X  the parent set  This effectively translates into an evalua  might summarize a larger number of instances than the sufficient statistics for  Z   with parent set  tion over different datasets   Y  lem  where we have to compare two models  Mt  and  M   such that model M  is evaluated with respect to the training set D   while model M  is evaluated with respect to the  D    ful  we assume that D  and D  are  The MDL and the BDe scores are inappropriate for this  problem in their current form  The MDL score measures the  number of bits required to encode the training data if assume  the underlying d i stri bution has the  the description of  D   where  Hn X  I pa X     tropy of X   D   D   form specified by  is much smaller than  Dt   then  would usually shorter than that of  regardless of how good the model  M   is   The same  problem occurs with the BDe score  This score evaluates the  probability of the dataset if we assume that the underlying  distribution has the form specified by the model  Again  if  D  is much smaller than D   then the probability associated  L x   pa x    N x    N  We can  of course  reset all the counters every time we start gathering some new sufficient statistics record   This  in  effect  restarts the learning process using the last suffix of the training sequence  This  however  would discard useful  information that has been gathered about the earlier parts of the sequence  Alternatively  we can adopt the Bayesian method  and compute  P M  I D   and P M  I D     Since  both of these terms are degrees of belief  we can compare them and see which candidate is considered more likely given the available evidence for that candidate  Unfortu  nately  while we have a closed form forP   Dt and P Dz    I M  P M    I M  P M    we cannot effectively evaluate the  Note  however  that that both sufficient statistics records sum  marize a suffix of the sequence subset of the other   U             u     Thus  one is always a  empirical conditional en N x   pa x      og N   pa   x         As N grows larger  this conditional entropy converges to the true conditional entropy in the underlying distribution   H X I pa X          L  x  pa x    P x  pa x   logP x  I pa x      This latter quantity is the smallest number of bits  for this particular underlying distribution  by which we can encode  the value X  if know the value of N  we get that   I D   SMDL    N  oo  pa X     If we divide by  LH X I pa X    logN  dataset is a product of the probability of each instance given than    the probability decreases for longer sequences   is the  pa x       with it will usually be larger  since the probability of a  the previous ones  Since each such term is usually smaller  I pa X      given its parents  and is equal to    both sampled from the  true in our case   the modeL However  if  N L Hn X          as   logN        X   pa Xi     Of course  for this problem to be meaning  same underlying distribution  This assumption is clearly  that  SMDL G I D      The underlying problem is a general model selection prob  training set  Our proposal is best motivated in the MDL setting  This     N     X  pa X      i  In other words  the average encoding length per instance approximates the true encoding length that can be achieved with G  The second term in this expression embodies the redundancy over the optimal encoding  had we known the  true distribution  incurred by the particular choice of model  For larger datasets  the redundancy in the the average en coding decreases  In effect  this redundancy term captures  the amount of confidence in the learned parameters  As  N increases  we are more more confident in our models  At the same time  we are less confident about models that require more parameters   This discussion suggests that the average encoding length per instance is a measure that can be compared in data sizes of different lengths  We define the  average MDL score as  SM L X   pa X    pax        v    X   pa  X   S  MDL         L     x  pa  x   N  x            Friedman and Goldszmidt  invokes a greedy hill climbing search procedure every k instances to select a network   This score measures the average number of bits needed to  represent one instance of Xi given the values of pa X    When we compare models based on different datasets  this score normalizes each evaluation to measure the average    effective compression rate per instance   k instances   The procedure also use greedy hill climbing to select a network   of  The average MDL score is consistent with the original MDL score when we compare two networks given the same data  That is  when we compare two models using the same data  both scores make the same choices  To see this  note that    the set of networks that can be evaluated using the currently stored statistics  This procedure uses the  the score will choose the right model  We now make this precise  We define the inherent error of a model G with  p to be  D P  JPB      L u P   u  log      The datasets used in the experiments were generated from  is the cross en  tropy  or Kullback Leibler divergence  between P  and Ps  This is a standard measure of distance between proba  bilistic models in statistics and information theory  We now show that given sufficient data  the average score will prefer structures that incur smaller errors   Lemma      Let G  and G  be two network structures that  are evaluated with respect to datasets D  and D  of size N  and N   respectively  that are sampled i  i d  from an under lying distribution P   If Error G   P     Error Gz  P    then as both N  and Nz both go to infinity  SuvL GI I D       SMDL  Gz I D   with probability     We suggest a similar averaging for the Bayesian score  We define the  average BDe score as Sjw  X   pa X     a X    SBDr x  p   N x  pa x    L   pa  J     This modification is motivated by  the asymptotic equivalence between the Bayesian score and the MDL score  A a general result by Schwarz that       shows  SBoe G I D    SMoL G I D          Thus  by Lemma         the average Bayesian score is also  asymptotically correct  It is not clear to us  at this stage  whether this average score has any principled probabilistic justification      neighbor frontier set we described above   two networks  the alarm network of      and the insurance network of       The alarm networks contains    variables   Error G P     mjnD P IIP a e   where  Incremental  the procedure introduced in Section       Every k instances  the procedure uses a on greedy hill climbing search to select a network from Nets S    in this case  the average MDL score assigned to the two models is their original MDL score divided by N  As with other scores  we also want to be ensured that in the limit  respect to a reference distribution  MAP  the MAP procedure described in Section      In this procedure the best network so far is used as the prior for the update in the next iteration  consisting  Experimental Evaluation  The experiments reported in this section were designed with  and the insurance network contains of         instances  The results reported in Figures   and    are averages over the results of running the algorithms on all   datasets  For all three procedures described above we update the parameters quantifying the current network candidate after each instance is received  We set the prior probability to be a weak uniform prior   equivalent sample size of     We run k IOO      and      As  tests for the following values of expected  the performance of  Naive is mostly independent        only  We run  ofk  Thus  we report the result fork  Naive with both the MDL and BDe scores  Similarly  we run Incremental with both the modified scores introduced in Section        The evaluation of the quality of the induced networks is based on the log loss  that is Ln logPB   un  This is  a standard measure of performance in density estimation  Since we generated the training sets from existing net  works  we can measure how close the learned models are to the generating distribution  In particular  we measured  normalized loss Ln logr un   logPB  un     Ln log   S      where P     is the probability of the gen  the  erating distribution  This measure has several attractive properties  First  it relates the loss incurred by the learn ing procedure to the optimal that can  instead of the true model  Second  the normalized loss is related to the cross entropy measure of distance between probabilistic models  It is easy to see that cross entropy can where the D P  IIPB     E  Iog L l  L PB   u    average is based on P   Thus  the average normalized loss  be rewritten as  to evaluate the tradeoff between space used by the learning  is an estimate of the cross entropy   We compared the performance of the three procedures we described above  All three procedures take as a parameter the number k of instances after which the network structure is reconsidered  The procedures are    Naive  the  naive approach described in Section      This procedure stores all the instances seen so far  and  be reached  Thus  it  measures the additional penalty for using an approximation  two objectives in mind  First  to show the performance of the methods we described in the previous section  Second  procedure and the quality of the learned networks      variables  From  each network we sampled   training sets  each consisting  Figure   summarizes the performance of each method in terms of the average normalized loss  As expected  Naive does better than all the others  since it is able to use all the information observed in the data in a cumulative way  The figures do not show significant difference between the MDL and BDe scores for this procedure  As we anticipated  Section         we observe MAP s ten    Sequential Update of Bayesian Network Structure       AlannBDe       r          r                  k   lOONaive lo  IOOMAP      k    MAP   lo    MAP        j        Insurance BDc                             r n   l  j       i  l                         k  IOOMAP     AP    M   s  oo AP          M   t          k IOONaive   I  I                                     llnstantts                     Q                   j                                 j ll        l                                   IQ                          JQ     J               llnsl alll    lns     nce MDL  Alarm MDL     k    IOONaivc k       lncR mon al   k        Incremental  k       lncremenw                 tlnstu J ces   i     Q                              k IOONaive k       Incremental    k       lncremenul  k     X Incremental                   lns tnncc BOe  k IOONaivc k       lncremenlal   k       lncremen al           k       lncmnenlal              llmtanca  Alarm BDe                   ll        iii   l                                        flnstances                                         t ln   tana s  Figure    Performance results of several methods for sequential update  The left column reports results for  alarm  network  and the right column reports results for the  insurance  network  Results in the the top row use the MAP procedure with  the BDe score  the middle row use the incremental procedure using the BDe score  and the lower row use the incremental procedure with the MDL  All the graphs also display the results of the naive method with the corresponding score as a  point of reference  The horizontal axis measures the number of instances seen  The vertical axis measures the average normalized loss  where each data point is the average of a window of     instances over   datasets          Friedman and Goldszmidt  dency to lock on its current model once the prior received sufficient weight  This can be seen very clearly from the results of Figure     The qualitative behavior of MAP ap proach shows i mprovement  until a stage where the equiva lent sample size of the prior is larger than about     k  then the procedure locks on a particular structure and the perfor mance levels off  In the case of k I     the performance actually degraded after this stage   Unfortunately the per formance of this case falls off the range of the graphs   Of course  we might deal with this problem by setting the prior strength to be smaller than the number of examples seen  This  however  runs the risk of learning too simple networks   Recall that we adopt complex networks only if the d ata supports it in a sufficiently large number of sam ples   We are currently in the process of experimenting with different strategies of setting the prior s equivalent sample size     As described in Section      we expect Incremental to converge to the right distribution  albeit more slowly than N a ive  This is i ndeed the qualitative behavior we can see in Figure     These curves show that the scores are consistent and that the incremental procedure is sound  in the sense that as more data is observed  the procedure outputs networks that are closer in performance to the golden model and the best possible sequential learning exhibit by the naive pro cedure  Also as expected  runs with larger k needed more time to achieve a good level of performance  This is due to the fact that the procedure posts new sufficient statistics records only every k instances  and thus  it needs several k instances before it can start maintaining sufficiently com plex statistics  However  in the long run  the procedures with larger value of k seem more robust  Figure   summarizes the space usage for these procedures  The estimates are optimistic in that we count only the nec essary data structures and not any auxiliary ones  For Na ive and MAP  we measure the space needed to store the dif ferent instances  For Incremental  we measure the space needed to store all of the active sufficient statistics records  Again as expected  Naive requires more and more mem ory since it needs to store all the previously seen instances  On the other hand  MAP requires a constant amount of memory  since it stores at most k instances at any time  Finally  Incremental allocated more memory as it learned a more complex structure  However  once the structure es timate stabilized  the memory usage of this procedure also stabilized and remained roughly constant  We remark that the memory usage in Incremental can be minimized further when we have prior knowledge that limits the space of possible networks  e g   ordering constraints   Additionally  we can use heuristic estimates to estimate which arc additions are the most promising  using  for ex ample  pairwise mutual information   and only maintain the corresponding records  We suspect that such a scheme can reduce the space requirements by a large factor at a small penalty in performance  We are currently in the process of experimenting with these extensions      Missing Data  In the above discussion  we have made the assumption that the data is complete in the sense that each data instance Un contains values for each variable in U  Unfortunately   in many real life applications we are forced to deal with incomplete data  The source of these incompleteness may come from noisy measurements  or from domains in which some attributes are not directly observed  One source of difficulty in learning from incomplete data is computational in nature  We can no longer decompose the probability of the data  This means that the score  either MDL or Bayesian  cannot be written as the sum of local terms measuring how well we model the probability of each variable given its parents as in Equation     Moreover  in order to evaluate the optimal choice of parameters for a given candidate network structure  we must perform a non linear optimization using either Expectation Maximization EM         or gradient descent          In this paper we focus on the EM procedure  The standard use of EM is for batch learning  In addition  it is restricted to induce the parame ters under the assumption of a fixed structure  In order to adapt EM to the sequential update problem we need to re lax both these restrictions  Fortunately  two recent methods deal with each of these restrictions in turn  and as we now describe  a combination of both methods leads to an elegant learning procedure for sequential update  For reasons of space  we keep the discussion at a high level  and refer the interested reader to the relevant papers cited below  The standard EM procedure for learning pa rameters iteratively and monotonically improves its current choice of parameters for a fixed structure G using the fol lowing two steps  In the first step  the current parameters e are used for computing the expected value of all the rele vant sufficient statistics records  Sujj  G   This computation evaluates E N x  I D   Pa  L c ND   x PB  D  I D   where B  G      and w e sum over all the possible com pletions n  of D  that is  assignments to all unobserved values in D   In the second step  e is replaced by the pa rameters    estimated from those expected statistics  This second step is essentially equivalent to learning parameters from complete data  The theoretical justification for this procedure shows that by proceeding in this manner with each iteration we i ncrease the probability that the observed data was sampled from the distribution represented by the structure and the induced parameters        As described the procedure is a batch learning method in that we must retain all of the training set  Neal and Hin ton      essentially show that we also i mprove this prob ability if we use an i ncremental update of the sufficient statistics  In their approach  new incoming data cases are used to continuously recompute the sufficient statistics  The intuitive j ustification behind this approach is that any two sufficiently long sequence of of i  i d  samples are similar  Thus  instead of storing the training data  this procedure relays on new data  The second enhancement they describe justifies updates to the parameters after every instance is seen  This results in the following procedure for learning parameters    Sequential Update of Bayesian Network Structure  Alarm BDc   SOOOO         nsurance BOt          k      l OO Naivc   k       Incremental       k     lncrement aJ         k  SOO IIlCrcme k  I AP                        kMAP          MAP   l                                                                                                                                                                IUns tances                           I       Insurana MDL  Alarm MDL k  lOO NaiYe   k    I    Incremental  k          Incremental k   SOO lncrerne                                                                                                o                                f       c                                          flnso  s                   I                                                                         NlnsWlccs  Figure    Space requirements of several methods for sequential update  The upper row reports methods using the BDe score and the lower methods using the MDL score  The left column reports results for the  alarm  network and the right columns reports results for the  insurance  networks  The horizontal axis measures the number of instances seen  The vertical axis measures the average space needed to store the data retained by the procedure   Set B    G     to be initial network  For all Nx E Suff  G  N x    No  PB  x  Forever Read data instance y  For all Nx E Suff B   N x     N x    a    PB  xiy  Update parameters in e from the new sufflcient statistics  Output B In this procedure  network  and  a  No designates the confidence in the initial  is a decay parameter  which has a value less  than I   Usually we set  a  to be quite close to     e g          Using this decay parameter we gradually decrease the con tribution of old samples  which where  completed  by old parameters    This procedure is not guaranteed to make  positive progress in each step  however  on the average  it does make progress and for a sufficiently long sequence  the procedure will converge   Jar results apply to the Bayesian score as well  It follows then that we can use the expected sufficient statistics in our search procedure to evaluate new models   There is no intrinsic difficulty in casting the results of     in the incremental framework of         Combining the two  techniques we get a simple modification of our approach that deals with incomplete data   Set B to be initial network  Let F initial search frontier for B  Let S   Suff B   U UB  E F Suff B    For aii Nx E S  N x     No  Ps  x   Forever Read data instance y  For all Nx E S if n  N x     N x    a   PB  xly  mod k   then Let B arg maxB ENets S  S B  I    S  Update the frontier F Update S to Suff B  u U B   E F Suff B      Compute optimal parameters for B from S  Output B     The second restriction of the standard EM is that it deals only with learning parameters in a fixed structure  Fried man     shows that if use the expected sufficient statistics to evalu ate alternative structure s using the MDL score and choose structures that are assigned a higher score than the current model  then we are bound to improve the marginal score of network with respect to the observed data  Simi   It is well known that EM methods may reach a sub optimal         Friedman and Goldszmidt  local minima  Standard techniques to avoid these minima such as  for example  running EM several times with differ ent random starting points apply here as well  Of course  we can run parallel executions of this procedure  We are currently in the process of evaluating the effectiveness of this procedure     Discussion  Previous work on the sequential update of Bayesian net works have been mostly restricted to updating the param eters assuming a fixed structure          The two notable exceptions are the approaches by Buntine     and by Lam and Bacchus          Buntine s method assumes that a total order on the variables is given  and it maintains sufficient statistics for the possible parents of each node using lattice structures  He imposes restrictions on the size of the lattices in order to bound the amount of information that is main tained  Unfortunately  Buntine does not provide any reports on experimental evaluations of his approach which makes a rigorous comparison difficult  Lam and Bacchus propose a different approach based on a modification of the MDL score  In essence  they use the current network in each iter ation as a summary of previously seen data  This is similar  in spirit  to the MAP approach described in Section         They performed experiments with the alarm network  yet their evaluation criteria is not objective in the sense of a log loss scoring of the resulting networks  and once more  rigorous comparisons are difficult  The incremental approach introduced in this paper opens new degrees of flexibility in various dimensions  First  by using different search strategies  different search frontiers are achieved  which in turn changes the amount of infor mation maintained  Additionally  we are actively consider ing different heuristics for pruning the number of sufficient statistics stored in memory  These heuristics would rule out statistics that are unlikely to lead to an arc addition  As mentioned in the introduction  the approach we describe is useful in applications that involve large amounts of data  Such applications include data mining problems and mon itoring problems  We also note that by keeping a set of likely candidates readily available  this approach can also be useful to cases where there are real time constraints on the model selection process  and decisions have to be made in a real time fashion  There are two topics that we are currently exploring  The first is an empirical evaluation of these three methods in situations where the underlying distribution drifts in time  Our goal is to characterize the different parameters that affect the efficiency of the procedure in  tracking  these changes  The second topic involves learning in the presence of incomplete data  We established the foundations of a solution with the procedure described in Section    We are in the process of conducting experiments that will allow us to evaluate the approach and propose further refinements if needed  Finally  even though our results confirm the consistency and effectiveness of the scores introduced in Section       we would like a principled derivation based on a strictly probabilistic interpretation  As mentioned in that section  we pose this as an open problem  Acknowledgments  Parts of this work were done while Nir Friedman was at SRI International  Nir Friedman s work at Berkeley was funded by ARO through MURI DAAH       I        and by NSF through FD             
 We study the connection between kappa cal culus and probabilistic reasoning in diagnosis applications  Specifically  we abstract a prob abilistic belief network for diagnosing faults into a kappa network and compare the order ing of faults computed using both methods  We show that  at least for the example exam ined  the ordering of faults coincide as long as all the causal relations in the original prob abilistic network are taken into account   We also provide a formal analysis of some net work structures where the two methods will differ  Both kappa rankings and infinitesimal prob abilities have been used extensively to study default reasoning and belief revision  But lit tle has been done on utilizing their connec tion as outlined above   This is partly be  cause the relation between kappa and prob ability calculi assumes that probabilities are arbitrarily close to one  or zero   The exper iments in this paper investigate this relation when this assumption is not satisfied  The reported results have important implications on the use of kappa rankings to enhance the knowledge engineering of uncertainty models      Introduction  Bayesian reasoning has found widespread use in re cent years       Applications based on Bayesian net  works  for example  have spanned over diagnosis  fore casting  natural language understanding  and planning      But despite the popularity of Bayesian methods  one of their key aspects has always stood in their way to further success and wider use  namely  their com mitment to point probabilities  In particular  most Bayesian techniques cannot commence without com mitting a domain expert to a full probability distri bution  which typically requires many probabilities to be specified  Although recent advances in Bayesian networks have reduced this problem by appealing to  conditional independences  there is still a significant interest in reducing this problem even further given its impact on knowledge elicitation and model building  In recent years  a number of proposals have been ex tended for the purpose of relieving domain experts from having to specify point probabilities  Many of these proposals offer concrete methods that allow Bayesian reasoning to commence without a commit ment to a complete probability distribution  An exam ple of this is Qualitative Probabilistic Networks        which allow one to reason about probabilistic influ ences among variables in a qualitative manner that is consistent with Bayesian reasoning  A second class of proposals attempts to relief experts from providing point probabilities by requiring more abstract and in tuitive belief measures that are consistent with point probabilities  A key proposal in this camp is kappa cal culus          and its probabilistic interpretation using  semantics      In this framework  experts can pro vide beliefs in the form of if then rules that are quan tified using order of magnitude probabilities  This  quantification can be naturally embedded into a causal network  where the same set of Bayesian distributed algorithms can be applied          Both kappa calculus and its probabilistic interpreta tion have been extensively studied from the perspec tive of belief revision  nonmonotonic and defeasible reasoning                                Kappa calculus was also proposed as a qualitative version of proba bilistic reasoning in      Yet  the formal relation be tween kappa calculus and probabilistic reasoning is es tablished under the assumption that probabilities are extreme  that is  not only should they be close to one or zero but also they should be arbitrarily so  This requirement  which is never met in practice  means that kappa calculus can be viewed as an abstraction  of probability calculus under the following acceptance rule       Even though probabilities may not be arbi  trarily extreme  the agent is willing to assume and be have as if they were  thus transforming them into plain beliefs quantified by kappa rankings that can be ma nipulated using kappa calculus   The question we address in this paper is the follow         Darwiche and Goldszmidt  ing  What are the consequences of adopting the ac ceptance rule  For example  what information is lost once we are willing to take regular probabilities and abstract them into plain beliefs to be processed by kappa calculus  To answer these questions  we take an empirical approach and use a diagnostic example to test our hypothesis  Our results show that in spite of differences in absolute beliefs  when it comes to or dering the set of faults  both standard probabilities and their corresponding kappa rankings coincide most of the time  Moreover  an analysis of the differences between the two calculi led us to identify two causal structures where using probabilities or kappa rankings will yield different results  The results in this paper are important for the knowl edge engineering of uncertainty models for the follow ing reasons     Eliciting and building uncertainty models seems to be an easier task in kappa calculus than in probabilities  The kappa quantification of a net work can be performed using if then rules and ignorance can be specified by declaring that both an event and its negation are possible     Models are more robust in kappa calculus  since small changes in the uncertainty will not affect much the assignment of beliefs     It seems easier to absorb the results of a proba bilistic inference once they are displayed as order of magnitude approximations  kappa rankings  of the actual probabilities     There are indicators that algorithms based on ex treme probabilities      and kappa rankings     can be faster than those based on regular probabili ties  This paper is structured as follows  We overview kappa calculus in Section   and then elaborate on its relation with probability calculus in Section    Specifically  al though kappa calculus has been developed indepen dently of probability calculus  kappa rankings can be viewed as order of magnitude probabilities when these probabilities are arbitrarily high or low  In Sec tion    we provide a formal translation from point probabilities to kappa rankings and outline the role that this translation could play in practical systems  where probabilities are not necessarily extreme  We then report a number of experiments in Section   that are designed to evaluate the proposed translation and to assess the possible loss of information it could lead to  The experiments are conducted in the context of a diagnosis task  Some of the reported results lend them selves to formal analysis that we carry out in Section    The key outcome of this analysis is an intuitive char acterization of kappa calculus on some of the causal structures appearing in real world applications  Fi nally  Section   summarizes the main results and offers another perspective on the connection between proba bilities and kappa rankings according to which kappa  rankings are strengths of default assumptions that are extracted from probabilistic information  This connec tion is in the spirit of earlier work on extreme prob abilities and t semantics      and provides a better understanding of the connection between point prob abilities  kappa rankings and default priorities     Kappa calculus  The original motivation behind kappa calculus was to propose a non probabilistic theory of ind uctive rea soning           A non probabilistic theory was sought because inductive reasoning involves classifying propo sitions according to whether they are believed or dis believed and then changing this classification upon re ceiving further information  But classical probability theory did not support such a classification  proposi tions are only graded by their probabilities and are not classified into believed  disbelieved uncommitted  Given this motivation  the properties of kappa calculus can be justified without having to appeal to a proba bilistic interpretation  which is how the calculus was argued for in       There  a state of belief is repre sented by a ranking K that maps propositions into the class of ordinals such that    K true          K a V      min K a           A rule was also given for conditioning a state of belief K on evidence J  L    a I p       K a Ap    K J L   According to kappa calculus  a proposition a is be lieved to degree s if K    a    s  is disbelieved to degree s if K a    s  and is uncommitted if K a          a      Moreover  the streng ths of these beliefs decide which of them are retracted when accommodating a disbelieved evidence  Kappa calculus then offers a framework for reasoning with defeasible beliefs  where the kappa rankings play the role of default priorities           But the calculus is analogous to probability calculus in the sense that it provides a similar machinery  a definition of a state of belief and a definition of conditionalization for ac commodating evidence  This correspondence should not be surprising  however  given the symbolic gener alization of probability theory in      which provides definitions for abstract states of belief and abstract conditionalization that subsume both probability and kappa calculi  see      also for a generalization of belief functions that subsumes kappa rankings      Kappas and probabilities  Although Spohn has motivated kappa calculus as a theory of belief change  Spohn also noted the con nection between kappas and nonstandard probabilities   On the Relation between Kappa Calculus and Probabilistic Reasoning  Figure      The car network             The purpose was mainly to explain the sym metry between properties of kappa calculus and laws of probability theory  In particular  Spohn suggested in      a mapping from probability distributions to kappa rankings that justifies the properties of kappa calculus  He proposed mapping each probability Pr a I     into a ranking k such that Pr o  I      k is finite but not in finitesimal for an infinitesimal f  Spohn then showed that we get the following       a V      min    a                 a I        a  A             which are the basic properties of kappa rankings  This result provides an interpretation of kappa rankings as order of magnitude approximations of probabilities through the following relation   f pjfk   which is equivalent to fk l   p            fk   This connection between kappa rankings and proba bilities is of great theoretical interest  For example  its role has been explored at length in providing prob abilistic semantics to defeasible if then rules that are crucial to nonmonotonic reasoning      But the con nection between kappas and probabilities is also im portant from a purely probabilistic sense  That is  a key concern of Bayesian practitioners  for example  is to continue to enjoy the merits of Bayesian tech niques while committing as little as possible to point probabilities  The view of kappa rankings as order of magnitudes probabilities is one way to satisfy this need  That is  instead of providing point probabilities  one provides kappa rankings  In fact  the role of such a connection goes beyond the knowledge elicitation pro cess to at least two other areas     Given probabilities that result from answering a query  we can map these probabilities into kappa  rankings before we present them to experts or be fore we use them as inputs to other reasoning pro cesses such as decision making     Given a set of probabilities to be computed with  we can map these into kappas and then use kappa specific algorithms for the computation  This step is significant if kappa specific algorithms turn out to be more efficient than probabilistic ones  a hope that is being backed by recent results          One should emphasize though that the above connec tion between kappas and probabilities rests on assum ing that f is infinitesimal  Given a probability distri bution  for example  the following two computations will yield the same results when an infinitesimal f is used   C   Computing posterior probabilities using probabil ity calculus and then abstracting them into kappa rankings   C   Abstracting probabilities into kappa rankings and then computing posterior kappa rankings using kappa calculus  But unless probabilities are arbitrarily high or low  computations C  and C  will be equal in a trivial sense  For example  if all probabilities are k nown to be between     and      the mapping of Equation   will produce a zero kappa for each given probability  This means that the resulting kappa distribution will be trivial  all it says is that everything is possible and that nothing is believed or disbelieved  Therefore  we are constrained in practice to select a noninfinitesimal E to use in Equation    There will al ways be tension between how close the value of t is to zero and how close the results of computations C  and C  will be  On one extreme  f is very close to zero and the results of Cl equal those of C  but pos sibly in a trivial sense because the generated kappa        Darwiche and Goldszmidt     If p      then print  oo   grees of belief  The second line corresponds to the ordering of faults when c         and the third line to              k         p      pfc     If p      then print k otherwise k      k          Goto     Figure    A procedure for translating a probability value p into a kappa value k by finding a solution to the equation  k     p      t k   rankings may have lost most of the probabilistic infor mation  On the other extreme    is not close to zero  the resulting kappa rankings are not trivial  but the results could be different from those obtained using probability calculus  The experiments in the next sec tion will assess the discrepancies between the results of kappa calculus and those of probability calculus when   is not infinitesimal  using two different measures of discrepancy  Section   will then offer a formal analy sis of these results by identifying cases in which such discrepancies are expected      Experimental results  To empirically study the connection between kappa calculus and probabilistic reasoning in those instances were    is not infinitesimal  we conducted a set of exper iments with different values of   and different evidence  These experiments were performed on a probabilistic causal network for diagnostic reasoning about faults in a car  The network is depicted in Figure     Each experiment involved setting the value of   providing observations in the form of evidence  evaluating the probabilistic network using probability calculus  trans lating the probabilistic network into a kappa network using the procedure in Figure    and then evaluating the resulting kappa network using kappa calculus  We conducted three sets of experiments for          c           and          We report below  see Tables   and    on the most representative results of the simi larities and differences between kappa and probabilis tic inference  The observables where engine start  gas gauge  lights  and engine turn over  while the faults where alternator  battery  fuel pump  gas  plugs and starter  The value of engine start was always set to BO  To assess the discrepancies between kappa and proba bility computations  we used the following two criteria      Ordering of faults   In Table   we order the faults according to their corresponding probabilities and kappas  The table contains eight  runs   where a run is defined by an instantiation of the evidence  The first line in each run corresponds to the or dering of faults according to their probabilistic de    This network was obtained from the Bayesian group at Microsoft Research       Degrees of Belief  In Table   we compare the probabilities of faults to their kappa rankings by transforming the posterior probabilities into kappa rankings following the procedure in Fig ure   and using             The first criterion provides a practical measure of the correspondence between kappa and probabilistic infer ence when the kappa network is generated automati cally from a probabilistic one  The second criterion is intended to compare the results of computations Cl and C  in Section   when    is not infinitesimal  All of the experiments reported here were conducted using CNETS      an experimental tool for represent ing and reasoning with generalized causal networks      which include kappa and probabilistic causal networks as special instances  We have the following observations about the reported results  Ordering of the faults  W hen c          the ordering of faults according to prob abilities and kappas is the same in all the runs  pro vided we break ties in a particular manner  Ties in the kappa case are expected given that they represent an abstraction of the real probabilistic value  When c           the results are also very close  except that the most likely fault and the second most likely fault are inverted in runs     and    The discrepancies in these runs are due to the same reason  loss of in formation due to the kappa abstraction  In particular  the matrix quantifying engine starts contains four rows in which the kappa of engine starts and the kappa of its negation are both zeros  That is  there are four rows in which the matrix does not commit to whether engine starts is believed or disbelieved  But when          the matrix of engine starts com mits to whether engine starts is believed or disbe lieved in each row  Degrees of belief in faults  Note that probabilities and kappas disagree more no ticeably in belief strengths than in the ordering of faults  Kappas are generally much more committed to assign stronger beliefs to the possible existence of faults than probability  This property is illustrated in Figures       and    where kappa beliefs are sharp and linear  These figures will be discussed in more detail in Section    The disagreement of belief strength in Table   prompted the formal analysis in Section    We basically identified two causal structures  a chain and a fork   see Figure    The chain struc ture was motivated by the discrepancies on the de    On the Relation between Kappa Calculus and Probabilistic Reasoning           GAS GAUGE  LIGHTS  TURN OVER  Calcuh  NOT EMPTY  WORK  YES  Pr  Ordenn     Fuel Pump  t                 EMPTY  NOT EMPTY  EMPTY  WORK  WORK  WORK  YES  NO  NO  Plug   Fuel Pump  Alterna tor  Al toe r nator  Plugs  B a Uery  Ga  Su rter  St a rter       Gas  B atte ry  St a rte r          Fuel Pump  Plug Aherna tor  Pr  G  s  Fuel Pump  Plug         Gall   Fu el Pump  Aherna  or S t a r   er  Plu  Alternator           Fuel PumpT  Ga   Pr  Fuel Pump  S tar t er  Kt       ueJ t ump          Fuel Pump   Pr  Ga  Fuel Pump            a      Fuel Pump          Fuel Pump  Ga     Gas  B attery  Plu Alt er n ato r  Ballery BaUery  Sta rter  B a ttery Starter  Aher n a tor  Ba ttery  ta rte r  Alt e rna tor  J   a ltery  Sta rter  Alternator  B attery  Pl u g s  Plugs  Plugs  G     G as  Ga s  Sta rter  A h erna  o r  Plug  St a rEr  Plu s  B at tery  Allerna to r  Battery  Alterna tor  Battery  Sia rler  PluK      NOTEMPTY  DONT  YES  Pr  Fuel Pump   K     Fuel Pump  A h e rn a to r  Bauoery  Plug  Aherna tor  Bat t er y              EMPTY  DONT  YES  Fuel PumpT  Pr  Ga s  K        Ga s  Alternator    Eai tery   Fue l Pump Fu el Pum p  Alternator  G S  Plug  S tart e r G  s  Plug   G      Stiuter  St a rter  Batter y  Alternator  P lugs  Starter  Plugs  S tarte r  P lugs  Starter  Sta rter  G    Battery          Fuel Pump     Ga       Alternator Ba ttery     NOT EMPTY  DONT  NO  Ba  uery  Pr  Alternator            Alternator  Fuel   Pum p Fue  Pump     Plugs  Plugs  S t arte r  Plug  S U  r  er  a     Plugs  Gas  S t ar t e r  Ba llery   Ke        Altetna t or     Ga   Ba llery     Fuel Pump    EMPTY  DONT  NO  B alle r y  Pr  Bal l ery   K e       Alt e r nato r     Fuel Pu m p Fuel Pump  Plugs  Ga  Sh rler  AUerna t or  e        Ba Uery  Gas  Plugs     Alternator  Starter  Fuel Pump    Table    O rde r ing of faults according to      p oste rio r probabilities that resulted from evaluating the probabilistic car network and     posterior kappa rankings that resu l ted from eva lu ating the kappa car network  A     means belie ved and a     m eans uncommitted   GAS GAUGE NOT EMPTY EMPTY  LIGHTS WORK  TUKN OVER YES  Bat te ry ok  A he r n a t or    Pr  ok ok  ok ok  ok   r     WORK  YES  NOT EMPTY  WORK  NO  ok  ok  ok  ok   EMPTY  WORK  NO  ok  ok  NOT EMPTY  DONT  YES  EMPTY  DONT DONT  NO     ok   NOT EMPTY       ok  ok      ok    EMPTY  DONT  NO     YES  ok  ok            ok   ok       Sta rter     ok ok  ok  ok ok ok ok  ok  Pr     gao  ok  ok  ok  ok  ok  ok  ok ok  ok ok  ok  ok  ok  ok  ok  ok   Pr ok    ok     ok    ok  ok  Fu el Pump     P l u gs       r  bad                 ok  bad           ok  ok  ok   ok  bad  bad      ok ok  ok   ok ok  r  ok  ok  ok  ok ok  ok  Comparing      kappa ra nk i ngs that are ab str acted from posterior probabilities that resulted from evaluating the probabilistic car network to     p oster i or kappa rankings that resulted from e va l uati ng the kappa car network  A     indicates a difference b etween the two kappa rankings  A     indi cates that the kappa ranking of a fault and that of its negation were zeros  thus leading to ig nora nce about whether the fault is present  Table           Darwiche and Goldszmidt  y  Xl  Xn  X                    Xl  X   Xn  X    a    b   Figure     a   Chain network  and  b  Fork network   It is important to point out that even though the strength of belief between probabilities and kappas does not always coincide  the most plausible faults do agree  This suggests that the precise numbers may not be relevant if we dim them for the purposes of optimal recommendations regarding repair and actions   on how far X  is from X    In kappa calculus  however  we get a different behavior  That is  if we transform the previous probabilistic chain to a kappa chain using t        we get the quantification   i  xiJ   K xiJ      K x  lxi d     and  i  xilx   Jl      Moreover  after observing that X  is true  each following X  will be believed true but with the same strength  That is  the strength of belief is independent of how far xi is from X   contrary to the probabilistic case  Figure   shows the difference between kappa and probability calculi with respect to the previous quantification of the chain      In general though  the propagation of belief from vari able X  to variable X  in such a network is governed probabilistically by the following equation   grees of belief in the subnetwork involving the nodes alternator  charge delivered and battery power  The fork structure was motivated by the discrepan cies involving the subnetwork composed of the nodes battery power  lights  radio  engine turn over   and  gas gauge   Formal Analysis  The discrepancies we obtained in the previous experi ments prompted the characterization of network con figurations on which the use of kappa calculus leads to different results from probability calculus  In partic ular  we have identified two network structures where we can characterize such a discrepancy and analyze it intuitively  The first structure is that of a chain of variables and is discussed in Section      The sec ond structure is that of a fork and is discussed in Sec tion           Propagation in chains  Consider the chain in Figure  a  where all variables are assumed to be binary with values xi  true  and x    false   Suppose that the causal links are quantified as follows  Pr xj         Pr xt I X           and P xi I xj             Suppose further that we observe variable X  to be true  What can we conclude about the probability that a descendant X  is true  According to probability calculus  the probability that any descendant xi is true will increase after observing that X   is true  Moreover  such increase will depend  P x lxi      I  IJ P xkiXk      X        Xi l       k    In contrast  kappa calculus leads to the following equa tion   K x lxl   i     min  X       Xi    L K xklxk     k         The kappa ranking corresponding to the result of Equation   should be equal to the result of Equation   when kappa rankings are generated using an infinites imal t  In the experiments of Section    however  we used real valued t  which prompted the difference in the degrees of belief in Table         Fusion in forks  Consider the network in Figure  b  where all vari ables are also assumed to be binary  Suppose that the causal links are quantified as follows  P y           P xtly         and P xtiY          Suppose further that we observe variables X  through Xi to be true  What can we conclude about the probability that Xn is true         On the Relation between Kappa Calculus and Probabilistic Reasoning  Degree of Belief    Degree of Belief                         P YIEvid   P Xn Evid              K Xn X                                             P Xn X                                                    Number of nodes                                Number of evidence nodes  Figure    The horizontal axis represents i    the distance between variables X  and X  in Figure  a  The vertical axis represents the belief that X  is true    Figure    The horizontal axis represents i  the num   X  in Figure  b  The ber of observed effects X     vertical axis represents the probabilities of Y and Xn being true     In the probabilistic case  we expect that the previous evidence will increase the probability in Xn being true  After all  the evidence increases the belief in Y being true  which translates into an increase in the proba bility of Xn being true  Moreover  the increase in the probability of Xn depends on the number of observed variables X         X   That is  the bigger i is  the big ger the increase in the probability of Xn  Figure   supports this intuition by plotting the increase for a specific quantification of the network         is affected by both the strength of y   s causal effect on x t and by the strength of believing in y   But the strength of believing in y  will be relevant only as long as it is no stronger than the causal effect  Once the belief in y  exceeds the strength of this causal effect  its exact value does not matter   In the kappa case  however  observing the truth of ef fects X          X  changes the belief in Xn but in a dif    ferent manner as depicted in Figure    That is  choos ing the quantification  K y        K xi IY       and K x t IY       leads to the following  First  if the num ber of observations is less than five  Xn is believed to be false  In case of six observations  Xn is neither believed true nor false  But as we collect more observa tions  Xn is then believed to be true  but the strength of this belief is not affected by the number of further observations   which leads to K x    min l   K y    K y    and K x t     min K y J      K y    in the above quantifica tion  That is  if y  is unknown  then xt is unknown  if y  is believed to degree    xt is believed to degree    if y  is believed to degree    xt is believed to degree    and so on  Now  as we obtain more observations about the effects of Y  our belief in it increases  but that does not affect the belief in Xn as shown above   The reason for this behavior stems from the following observation about kappa calculus  The belief in x t  In general  the equations governing the propagation of belief both in the case of probabilities and kappas are        Darwiche and Goldszmidt  ing a kappa causal network that can be processed us ing kappa calculus  To adopt this practice  however  one must first provide answers to a number of ques tions  First  would kappa rankings keep us in the realm of probability theory  the properties of which have led to the popularity of probabilistic causal net works in the first place  Would kappa networks allow  Degree of Belief    K YIEvid      the same expressiveness that one expects from proba bilistic causal networks  W hat should be done about the large body of existing probabilistic networks  Can                   Number of evidence nodes               given below      a  L P xniY  IT P xkJy P y   k l  y       X         b   m n II   Xn IY   J       K      k I  retain strong inferences  But our study also suggests that more needs to be said about when key inferences are retained  The discrepancies obtained in inferences using proba bilistic methods and kappa calculus should not be too surprising  Kappa calculus was proposed initially as a calculus for defeasible reasoning in which kappa rank ings are interpreted as default priorities  As such  the calculus has been argued for convincingly in       has  been shown to subsume many of the proposed calculi for defeasible reasoning in         and has also con tributed to the formalization of belief revision patterns that were not accounted for in the belief revision litcalculus  therefore  seems to be very defeasible reasoning perspective and the inferences it leads to seem to be well justified  The discrepancies with probability calculus  and their relation  can then be explained as follows  Kappa calcu     erature      The Xk I Y    K Y  intuitive from a  where a and b are normalization constants  As expected  in the case of probabilities  changes in the degree of belief of Xn will be gradual and cumulative as new evidence on its sibling nodes is gathered  In contrast  the propagation of beliefs in the case of the kappa case will be abrupt and sharp      In this paper  we attempted to answer some of the above questions by     proposing a concrete mapping from probabilities to kappa rankings that does not re quire probabilities to be infinitesimal      conducting an empirical study to assess the proposed mapping and to illustrate the expressiveness of kappa models in cap turing diagnostic information      providing some for mal analysis of the connection between certain classes of probabilistic and kappa causal networks  The basic conclusion we have reached is that one may abstract a probabilistic network into a kappa network and still  Figure    The horizontal axis represents i  the number of observed effects X         X  in Figure  b  T he verti cal axis represents   z     z J for z   y and z   Xto  If   z       z   is positive  then z  is believed  if it is negative  then z  is disbelieved  otherwise  z  is unknown     xniXt   these be mapped into kappa networks using some for mal procedure  Would the resulting networks capture the information represented by the original probabilis tic networks  And so on   Discussion  The use of probabilistic causal networks in diagnosis applications has become very common in recent years  One obstacle in this process  however  is the need to quantify causal relationships using point probabilities  Most often  probabilities are hard to assess and when they are provided  they seem to be too detailed for the reasoning tasks they are used to support  One possibility for simplifying this process is to quantify causal relationships using kappa rankings  thus indue   Ius as a method for defeasible reasoning manipulates prioritized beliefs  which can be extracted from probabilistic information as suggested in Section    Yet  default priorities are less informative and capture less information than probabilities  Nevertheless  people seem to perform this kind of abstraction all the time  in spite of the possible loss of information  Most of our beliefs are probabilistic in nature but they get abstracted into default assumptions for various rea sons  such as communicating them to others  index ing them efficiently  and simplifying their assessments  Thus  although the inferences made by kappa calcu lus can be well justified from the perspective of  plain beliefs  and  defeasible reasoning    they can disagree with probabilistic inferences   The work in this paper takes the first steps towards   On the Relation between Kappa Calculus and Probabilistic Reasoning  answering questions of a bigger scope such as  When should we abstract probabilities into kappa rankings  for what purpose  and for what cost gain  In this regard  we intend to continue this project in two di rections  The first one concerns the process of de cision making  The fact that the orders of faults were very similar in both probabilities and kappas suggests that the recommendations for repair should also be very similar  We intend to conduct a similar study to compare a probabilistic and a kappa decision making approach  The second direction concerns the computational value of abstracting probabilities into kappa rankings  The behavior of kappas in chains and forks suggest that the notion of belief acceptance in kappa calculus may yield a notion of weak indepen dence where belief in a node may be enough to render other nodes independent in the network    Our hope is that this property will translate into new algorithms with definite computational gains  Acknowledgments  We wish to thank J  Breese for his initial encourage ment with this project  and M  Henrion and G  Provan for comments on a previous version of this paper      J Eugene Charniak  Bayesian networks without tears  The AI Magazine                Winter           Adnan Darwiche   A Symbo lic Generalization of  Probability Theory   sity         PhD thesis  Stanford Univer      Adnan Darwiche  CNETS  A computational en vironment for generalized causal networks  Tech nical memorandum  Rockwell International  Palo Alto Laboratory            Adnan Darwiche and Matthew L  Ginsberg  A symbolic generalization of probability theory  In Proceedings of the Tenth National Conference on Artificial Intel ligence  AAAI   pages                      Adnan Darwiche and Judea Pearl  On the logic of iterated belief revision  In Theore tical Aspects of Reasoning About Knowledge  Proceedings of the      Conference  pages       Morgan Kaufmann  Publishers  Inc   San Mateo  California            Peter Gardenfors  KNOWLEDGE IN FLUX  Modeling the Dynamics of Epistemic States  The MIT press            Hector A  Geffner  Default Reasoning  Causal and Conditional Theories  MIT Press  Cam bridge  MA         This possibility      Moises Goldszmidt  Qualitative probabilities  A normative framework for commonsense reasoning  Technical Report R      University of California at Los Angeles  Ph D  thesis            Moises Goldszmidt and Judea Pearl  Reasoning with qualitative probabilities can be tractable  In Proceedings of the  th Conference on Uncertainty in AI  pages          Stanford              Daniel Hunter  Parallel belief revision   Uncer fainty in Artificial Intelligence  R D  Shachter  T S  Levitt  L N  Kana  and J F  Lemmer  eds                          Daniel Hunter  Non monotonic reasoning and the reversibility of belief change  In Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence  pages                      Judea Pearl   Probabilistic Reasoning in Intelligent Systems  Networks of Plausible Inference  Mor  gan Kaufmann Publishers  Inc   San Mateo  Cal ifornia             David Poole  Average case analysis of a search algorithm for estimating prior and posterior probabilities in bayesian networks with extreme probabilities  In Proceedings of International Joint Conference on Artifical Intelligence  I J CAI   pages                 
  This work proposes action networks as a se mantically well founded framework for rea soning about actions and change under un certainty  Action networks add two primi tives to probabilistic causal networks  con trollable variables and persistent variables  Controllable variables allow the representa tion of actions as directly setting the value of specific events in the domain  subject to preconditions  Persistent variables provide a canonical model of persistence according to which both the state of a variable and the causal mechanism dictating its value persist over time unless intervened upon by an ac tion  or its consequences   Action networks also allow different methods for quantifying the uncertainty in causal relationships  which go beyond traditional probabilistic quantifi cation  This paper describes both recent re sults and work in progress     Introduction  The work reported in this paper is part of a project that proposes a decision support tool for plan simu lation and analysis  The objective is to assist a hu man computer planner in analyzing plan trade offs and in assessing properties such as reliability  robust ness  and ramifications under uncertain conditions  The core of this tool is a framework for reasoning about actions under uncertainty  called action networks  Ac tion networks add two primitives to probabilistic causal networks  Bayes networks        controllable variables and persistent variables  Controllable vari ables are the building blocks for the representation of actions in the domain  Persistent variables allow the modeling of time and change under uncertain condi tions  Controllable variables can be influenced directly by an agent  Thus  their value can be  set  regardless of the state and influences of actual possible causes in  the domain  In this respect action networks follow the proposal in        except for the introduction of the associated notion of a precondition for the action  Ac tions will be subject to preconditions connecting con trollable variables to other variables that establish con ditions for controllability  At the heart of reasoning about actions lies the issue of modeling persistence and change  how and under what conditions should variables in a given domain persist over time when they are not influenced by ac tions  We propose a canonical model for persistence to dictate the states of special variables called persis tent variables  Traditionally  the modeling of persis tence has been accomplished by relating the state of a variable at time t to its state at previous time points  Problems with this approach has recently prompted researchers to explicitly model the causal mechanisms between variables in a network  and furthermore to persist the state of this mechanism over time  see Sec tion       In this paper we further develop this model and propose a canonical model for the causal mech anisms  This model  called the suppressor model  is based on viewing a non deterministic causal network as a parsimonious encoding of a more elaborate de terministic one in which suppressors  exceptions  of causal influences are explicated  and where all the un certainty is in the state of the suppressors  The basic intuition here is that suppressors are believed to per sist over time    and that variables tend to persist when causal influences on them are deactivated by these sup pressors  Action networks employ quantified causal structures in the form of networks as a compact specification of a state of belief and as a formal language for specifying changes in a state of belief due to both observations and actions             The causal structure allows us to deal with some of the key obstacles in reasoning about action and change such as  the frame and the concurrency problems  and reasoning about the indi rect consequences of actions  To allow for different quantifications of the uncertainty in the causal rela The uncertainty of this persistence is determined by  the specific domain    Action Networks  tions  an action network will consist of two parts  a directed graph representing a  blueprint  of the causal relationships in the domain and a quantification of these relationships  The quantification introduces a representation of the uncertainty in the domain be cause it specifies the degree to which causes will bring about their effects  Action networks will allow un certainty to be specified at different levels of abstrac tion  point probabilities  which is the common practice in causal networks       order of magnitude probabil ities  also known as c probabilities      and symbolic arguments  which allow one to explicate logically the conditions under which causes would bring about their effects      In this paper  we will concentrate on order of magnitude probabilities as proposed in      Other quantifications are described elsewhere         This paper is organized as follows  Section   de scribes action networks  It starts with a brief review of network based representations  Section       and con tinues with a description of the models of time and persistence  Section       Section     introduces the suppressor model  The representation of actions can be found in Section      Finally  Section   summarizes the main results and describes future work     Action Networks  The specification of an action network requires three components      the causal relations in the domain with a quantification of the uncertainty in these rela tions      the set of variables that are  directly  con trollable by actions with the variables that constitute their respective preconditions  and     variables that persist over time  which we will call persistences in this paper  and variables that do not persist over time  which we will call events  Once the domain is modeled using this network based representation  including uncertainty   an action net work will be unfolded to create a more elaborate tem poral network that includes additional nodes for repre senting actions and for representing the values of vari ables at different time points  In this paper  variables will be denoted by lowercase letters e  Binary variables will be assumed to take val ues from  false  tr u e    which will be denoted by e and e   respectively  For clarity of exposition  vari ables will be assumed to be binary  unless stated oth erwise  and will be referred to as propositions  An instantiated proposition  or set of propositions  will be denoted by e  and   e denotes the  negated  value of e       Network Representations  We briefly review some of the key concepts behind causal networks in this section given the central role they play in action networks  A causal network con sists of a directed acyclic graph r and a quantification       turn key  engine running  Figure    Causal family for engine running  represent ing that turn key causally influences engine running   turn key true false  engine rtmning         engine nmning      Table    x  Quantification for the engine  running fam  ily  It represents the default causal rule  If turn  ke y then engine running   Q over r  Nodes in r correspond to domain variables  and the directed edges correspond to the causal re lations among these variables  We denote the set of parents of a node e in a belief network by  r  e    i e  will denote a state of the propositions that constitute the parent set of e  The set conformed bye and its par ents i  e  is usually referred to as the causal family of e  Figure   depicts the causal family of engine running  This network in conjunction with its quantification in terms of the   calculus depicted in Table   represents the belief that the engine will be running given that we turn the ignition key    The quantification of r over the families in the net work encodes the uncertainty in the causal influences between  r e  and e  In Bayesian networks  this uncer tainty is encoded using numerical probabilities       There are  however  other ways to encode this uncer tainty that do not require an exact and complete prob ability distribution  Two recent approaches are the   calculus where uncertainty is represented in terms of plain beliefs and degrees of surprise      and argu ment calculus where uncertainty is represented using logical sentences as arguments      These approaches are regarded as abstractions of probability theory since they retain the main properties of probability includ ing Bayesian conditioning          Appendix A reviews the main ideas behind the calculus   K        Darwiche and Goldszmidt  An important property of these networks is that a com plete and coherent state of belief can be reconstructed from the local quantifications of the families  Thus  they constitute a compact specification of a state of belief  In probabilities for example  given a network containing nodes Xt        Xn   P it        x         IT    i n  P i li x          Similar equations can be obtained for the   calculus and for argument calculus  Since  in this paper we concentrate on a quantification based on plain beliefs using kappa rankings  we provide a brief review of their main properties in Appendix A       Time and Persistence  When unfolding a persistent variable in an action net work  new variables are added to represent its values at different time points  This leads to a more elaborate causal network that spans over time  The structure of this temporal network is the focus of this section  Action networks appeal to two assumptions  the re alization of which lead to a specific proposal of how to expand an action network into a temporal causal network  The first assumption states that the causal relations between variables at a specific time point are similar to those explicated in the action network  That is  if e has causes c         Cn in the action network  it will have these causes at every time point  The sec ond assumption in action networks relates to temporal persistence  It says that the state of the system mod eled by an action network persists over time  with a certain degree of uncertainty  in the absence of exter nal intervention  In the remainder of this section we formalize a proposal that realizes these assumptions  Before we present our persistence model though  it will be illustrative to discuss two intermediate proposals that have inspired the current one  Persisting Variable States  Our first approach required that we make each persis tence at timet a direct cause of itself at timet     This was intended to represent the influence that the past state of a persistence has on its immediate future state  For example  assuming that turn key is an event and engine running is a persistence  this proposal leads to Figure    This approach is reminiscent of a number of proposals in the literature          It fails  however  to capture the notion of persistence that we are after be cause it leads to conclusions that are weaker than one would expect  For example  assume that the probabil ity of engine running  at time t given turn key  at time t and engine running  at t     is     Suppose now that we turn the key at time   but the engine does not start  We repeat the experiment at times          n    with similar results  i e   the engine does not run   In this model of persistence  the probability  of engine running  at time n is still    given turn key  at time n  Yet  intuitively  we would expect the car not to start at time n given the previous sequence of observations   Persisting Causal Mechanisms  The previous example suggests that it is not enough to persist the state of engine running  One must also persist the causal mechanism between turn key and engine running       The reason why we expect the car not to start is due to our previous observations which lead us to conclude that the causal mechanism between turn key and engine running is not behav ing normally  Moreover  we seem to assume that the state of the existing mechanism persists over time since no one intervened to change it  One way to capture these intuitions is to explicitly provide a representa tion of the causal mechanism between an event e and its causes  r   e  in the network  This solution requires that we add  at least  another parent U e  to each fam ily  which represents all possible causal mechanisms between  r e   and e  The node e will then be deter ministic since its state will functionally depend on the state of  r e  and U e   This model is intuitively ap pealing in that it encodes the causal relation of a family as a set of functions between the direct causes  r  e  and their effect e  where the state U e  selects the  active  function that specifies the current causal relation  The likelihood that any of these functions is active depends on the likelihood of the state of the variable U e    Using this approach we persist the functional mecha nism represented by the U   e   nodes in each family  as shown in Figure   with regards to the engine running family    This problem will re appear even if more refined mod els of the domain are proposed  One could  for example  add more causal parents representing the exceptions that would prevent the engine from running given that the key is turned  One such exception can be a dead battery  A l  though a step in the right direction  such refinements will not solve the problem above  since we can always repro duce the counter example by introdu ng the appropriate set of observations  e g   the battery was OK at each point in time  including time  n      tThe assumption behind this representation is that the uncertainty recorded in the quantification of each family  in a network r expresses the incompleteness of our knowl edge in the causal relation between e and its set of direct causes  r e   This incompleteness arises because e inter  acts with its environment in a complex manner  and this interaction usually involves factors which are exogenous to   r  e   Furthermore  these factors are usually unknown  un observable or too many to enumerate  Thus  we can view a  non deterministic causal family as a parsimonious repre sentation of a  more elaborate  deterministic causal family  where the quantification summarizes the influence of other factors on e   Similar  representations  were  used  by  Pearl  and  Verma      for discovering causal relationships from ob servations  and by Druzdzel and Simon     in their study about the representation of causality in Bayes networks    Action Networks  TIME            n  turn key  turn key  tum key  tum key             engine running  engine running  engine running  engine running  Figure     Simple  temporal expansion of the family encoding the relation between turn key and engine running  Proposition engine running is taken to be a persistence while proposition turn key is assumed to be an event   TIME            tum key  tum key  tum key  engine running  engine running  engine running  Figure    Temporal expansion of the family encoding the relation between turn key and engine running  and including the dynamics of their causal relation through the persistence of the node U  engine  running    The concentric circles indicate that the engine running node is deterministic  and depends functionally on the states of turn key and U  engine running          Darwiche and Goldszmidt  Unfortunately  even though the model in Figure   ex plicates and persists the causal mechanism between causes and their effects  it is too weak to capture the notion of persistence we are after  Suppose for exam ple that we turn the key at time    The system will then infer that the engine will be running at time   with probability     However  the model will not be able to conclude that the engine will continue to be running at times               and so on  In fact  from the topology of Figure    we can see that whether the engine is running at time t     is marginally indepen dent of whether the key was turned at time t  which is contrary to what we would expect from the persistence assumption  Persisting Variable States and Causal Mecha nisms  The approach we adopt  depicted in Figure    can be regarded as a combination of the temporal net works in Figures   and    The proposition en gine running is functionally determined by turn key  S engine running    and I   engine running    The vari able S  engine  running  captures all possible sup pressors of the causal influences that the proposi tion turn key has on engine running  The variable I engine running  decides the state of engine running when the the suppressors manage to deactivate the causal influence of turn key on engine running  and it is directly influenced by the past state of en    gme runnmg  In the static case  when time is not involved  the pro posal can be viewed as splitting the variable U e  into two variables  S e  and   e   Note  however  that once we expand over time the notion of a causal mecha nism has a broader scope because it has to account for the previous value of proposition e  The semantics of the variable S  e   assumes that the uncertainty in the causal relation between e and its causes  r e  is due to a set of abnormalities and exceptions that suppress this causal influence  When this influence is suppressed due to these exceptions and abnormalities the value of e is set according to its previous state  represented by the variable I e    This model of persistence makes two assumptions  First  it assumes that the state of suppressors tend to persist over time  with a degree of uncertainty determined by the specific application   Second  it assumes that the state of variable   e  is de termined by the state of e at the previous time point    This model is not only intuitive and solves the prob lems outlined above  but it allows for a modular quan tification of the network  the uncertainty in the causal relations  the uncertainty in the persistence of suppres sors  and the uncertainty in the persistence of variables An expansion similar to the one in Figure   is used by Balke and Pearl     for answering probabilistic counter  factual queries  and by H eckerman and Shachter      for capturing the notion of causal persistence   Both these assumptions can be relaxed and lead to  more elaborate models  see S ecti on       tum key  engine running  Figure    The suppressor model as a functional expan sion of causal network  can be specified independently  see Eqs    and     The following section will discuss the suppressor model in more detail       The Suppressor Model  To formally describe the suppressor model we first ex amine how it expands a  static  causal network into a functional one  where all causal relations are determin istic and all the uncertainty is about the states of root nodes  Then we show how this functional expansion of a causal network lends itself naturally for captur ing the persistence assumptions that we stated in the previous section  As a proposal for functionally expanding a causal net work  the suppressor model is based on the following intuition  The uncertainty in the causal influences be tween  r e  and e is a summary of the set of excep tions that attempt to defeat or suppress this relation  For example   a banana in the tailpipe  is a possible suppressor of the causal influence that turn key has on engine running  The expansion into the suppres sor model makes the uncertain causal relation between  r e   and e functional by adding a new parent S  e   to the family  which corresponds to the suppressors to the causal relation  In addition to S  e   another par ent   e  is added  which will set the state of e in those cases in which the suppressors manage to defeat the causal influence of     e   on e  In these cases we say that the suppressors are  active    Figure   depicts the expansion of the engine running family  Once a causal network is functionally expanded ac cording to the suppressor model  the persistence as sumption stated in the previous section can be for malized by taking the variable   e  to represent the previous state of e   see Figure    The intuition be ing that in those cases where the suppressors manage to prevent the natural causal influences on e  the state        Action Networks     TIME   turn key  turn key  I engine running   engine running  engine running Figure    The suppressor of e should simply persist  and follow its  model for the  previous state   We will now present the suppressor model formally  Let S e  take values out of the set  w  w  w            wh ere S  e    ws stands for  a sup ressor o stre gth  s is active   The function F relatmg e to  ts duect causes  r   e     the suppressors S  e   and the variable   e  is given by  F i e  w  l e            if K e  I i e     if x  e  I iT  e    otherwise     e    i  i        W here     eJ r  e    represents the strength of belief in th e causal relation between  r e  and e  Eq  says that tf the strength of the active suppressor w  is less than the causal influence of  r e  on e  then the state of e is dictated by the causal influence  Otherwise  the sup pressor is successful  the causal influence is suppressed  and the state of e is the same as the state of I  e   The translation ofF into a K matrix is given by below   engine running family   sense that all causal relations are deterministic and the  only uncertainty is regarding suppressors  Moreover  we get the following guarantee about the res u lting  functional n etwork   which says that the new network  captures all the information which the initial network was set to capture  Let K represent the quantification  of a non deterministic network and let      represent the quantification of its functional expansion  Theorem    The  prior distribution  of beliefs  on  turn ky uue  S  e  is given by  Using the suppressor model  we can take any  n  on  deterministic network quantified with kappas and au  tomatically expand it into  In  general    w   w   w         the    w       a  functional network in the  suppressor  takes  values  in  In practice  however  it suffices for  the suppressor to take values in   wk  where the ranking k  appears in the matrix of  e   uue       which reflects the intuition that suppressors are typi cally inactive  and that the stronger the suppressor is  the more unlikely that it will be active   K  e I if  e     Table   shows the automatic functional expansion of the causal relation between engine running and  r  engine running   depicted in Figure   and K qu antified in Table    reflecting Eq       rue          The proof of this theorem relies on m arginalizi ng K  CJi e   S e   f e   over all the states of S e  and I  e    uue  if e   F i e  w   l e    otherwise    K e I i e    S  engin    runnin    u     u w    false  w    v  false  w  fa l  e     v  faJe  w   I enJ ine runninr  eriiine runnans  fa Jse  true  fa he  false  fa  lse  r lae  false  fa be  true   rue  true  true  true  true  fru e      true  Table    Deterministic function relating en gine running and its parents in the suppressor model  order to complete the temporal expansion  shown Figure    we must quantify the uncertainty on the persistence arcs  Causal families are connected  across time points  through the I  e  node and the suppressor S e  node  The conditional beliefs k S et z JS et   and K f e  t Jej  wi ll formally determine the strength of persistence across time  Both conditional beliefs will encode a bias against a change of state  which captures  In in        Darwiche and Goldszmidt  the intuition that any change of state must be causally motivated  Note that this quantification is done mod ularly and independently of the quantification of the uncertainty in the causal relations  This separation is important for fast and efficient model building  The quantification of these beliefs will be of course tied directly with an actual application and a specific domain  In our experiments  and for the planning do main implemented we had intuitive results with the following model in which the strength of the persis tence assumption is proportional to the strength of the change in the state of the suppressor   DO fire qun  fired gun      The assumption of persistence for the I   e   node cor responds to the following equation  if j et d   f ei  otherwise        Since I  et   determines the state of et l when suppres sors are active  the number p can be interpreted as the degree of surprise in a non causal change of state of the proposition et ls Consider the engine running network in Figure   quantified as in Table    Assume that this network is temporally expanded using the suppressor model  see Eq    and Figure     Given the ignition key is turned at times                  n    and that the engine is not running at times                  n     the model will yield the belief that the engine will not be running at time n  given that the key is turned at time n  On the other hand  given that the key is turned at time    the system will infer that the engine will be running at time    and moreover that it will be running at times          n  Example        unable to stand  Actions and Preconditions  For the representation of actions  we essentially follow the proposal in      which treats actions as external di rect interventions that deterministically set the value of a given proposition in the domain  Actions are spec ified by indicating which nodes in the causal network are controllable and under what preconditions  Syn tactically  we introduce a new node I  denoting controllability  In Figure   for example  both fired gun and  oaded gun are controllable propositions  A suit able precondition for both nodes can be holding gun  which can be represented as just another direct cause of these nodes  The corresponding matrices will then be constructed to reflect the intuition that the action doe will be effective only if the precondition is true  otherwise  the state of a node e is decided completely by the state of its natural causes  that is  excluding doe and the preconditions of do    Let the variable doe take the same values as e in addition to the value     This value does not need to be constant  although it will assumed to be so in the remainder of the paper   Figure    Causal family containing actions and rami fications  YSP   idle   The new parent set of e after e is declared as controllable will be  r e  U  doe   The new ranking    fli e     de  is     eji e     d e        fli e       if d e  idle if d     p e if d e   e       For simplicity of exposition we have omitted possible preconditions  Their inclusion will just involve a re finement of the cases in Eq    to reflect the fact that an action is possible iff its preconditions are satisfied  The advantage of using this proposal as opposed to others  such as STRIPS  is that the approach based on direct intervention takes advantage of the network rep resentation for dealing with the indirect consequences of actions and the related frame problem  In specify ing the action  shooting    for example  the user need not worry about how this action will affect the state of other related consequences such as bang noise or alive  see Figure     Example  Consider the example in Figure   encod ing a version of the Yale Shooting Problem  YSP        The relevant piece of causal knowledge available is that if a victim is shot with a loaded gun  then she he will die  There are two possible actions  shooting and load ing unloading the gun  It is also assumed that both loaded and alive persist over time  Given this infor mation the implementation of action networks will ex pand the network in Figure   both functionally and temporally   In the first scenario  we observe at time   that the individual is alive and that the gun is loaded  and that  Thus  ife is binary  de  E   e  e   idle     Action Networks  there is a shooting action at time    The model will yield that alive at time   will be false    This scenario shows the interplay between the persistences and the causal influences in the network  In the second scenario  it is observed that at time   alive is t rue  the victim actually survived the shoot ing    The model will then conclude that  first  the gun must have been unloaded prior to the shooting  although it is not capable of asserting when   and fur thermore  the belief of an action leading to u nloading the gun increases  proportional to the degree of per sistence in loading    This scenario displays the model capabiliti es for performing abductive reasoning includ ing reasoning about the set of actions that would yield a given observation          works described in this paper  including the suppres sor model expansion  the temporal expansion  and the specifi cation of actions  are fully i mplemented on top of CNETS     U All the examples described in this paper were tested  using this implementation   Acknowledgments  We wish to thank P  Dagum for discussions on the na ture and properties of the funct ional expansion of a causal network  We also thank C  Boutilier  J  Pearl  Y  Shoham and the Nobotics group at Stanford for dis cussions on the representation of actions  D  Draper  D  Etherington  and D  H eckerman provided useful comments on a previous version of this paper  This work was partially supported by ARPA contract F         C       and by IR D f unds from Rockwell Science Center   Conclusions and Future Work  the models of time  persistence  and ac constitute the co re of action networks as a formalism for reasoning about actions and change un der uncertainty  The notion of persistence was for malized through the suppressor model which evolved from other proposals for extending causal networks over time  The suppressor model should be viewed as one canonical model for representing persistence  Re laxing the assumptions in this model will yield other possible  more complex representations  For exa mple we can make the I  e  node depend on more than one past in stance of e  This would allow the representation We described tion that  of time decaying functions for the dependence of  e  on  its past values  We are currentl y exploring and charac teri z i n g this and other alternatives with the objective  to provide a library of such models that would assist the user in encoding the dynamics of causal relations in the domain of interest   We also intend to add notions of utility and preferences on outcomes and to explore the use of action networks in the formulation of a plan  given a set of objectives   Appendix  A Review of The Kappa  A  Calculus   We provide a brief summary of the   calculus and how  it can quantify over the causal relations in a network r   Let M be a set of worlds  each worl d m E M being a truth value assignment to a finite set of atomic propo sitional variables  e   e           e      Thus any world m can be represented by the conjunction of ei              e      A  belief  ranking function   m   is  an assignment  with find assigned         are and w orlds assigned       oo are considered absolute impossibilities  A pr oposi tion e is believed iff       e       w ith degree k iff       e    k   where represents the degree  of surpris e  a world m r ealiz ed  worlds considered serious possibilities   associated  in g  The paths we are currently exploring include abduc tive metho ds for uncovering the sequence of actions  that can lead to a specific set  if beliefs  and the pos sibility of interfacing action networks as an evaluation component to a planning module   This paper has focused on the   calculus instant ia tion of action networks  Future work includes allowing other quantifications of uncertainty  such as probabili ties  and arguments  and even a mixture of these  We are also studying a prob abi lis tic  and argument based  interpretation of the suppressor model  The first steps toward the quantification of action networks with ar guments is reported in      Finally we remark that  all  the  features of action net     The reasons for this conclusion are due to the con ditional independences assumed in the causal network representation  They are formally explained in depth in      Chapter     and       of  non negative integers to the elements of M such that   m      fo r at least one m E M  Intuitively  K m         m  can be considered an order of magnitude approx imation of a probability function P m  by writing P m  as a polynomial of some small qu anti ty t and taking the most significant term of that polynomial   i e   P m   Cc   m   Treating E as an infinitesi mal quantity induces a conditional ranking function   eic  on propositions and wffs governed by p r operties derived from the  probabilistic interpretation      A causal struc ture r can be quantified using a rank ing belief function     by specifying the conditional be lief for each proposition e given every state that is    eji  e    Thus   for example  Table  of i e     shows    CNETS is an experimental environment for represent ing and reasoning with generalized causal networks  which allow the quantification of uncertainty using probabilities    degrees of belief  and logical arguments             Darwiche and Goldszmidt  the   quantification of the engine running family in Figure    Table   is called the    matrix  for the en gine running family  It represents the default causal rule   If turn key  then engine  running    The   calculus does not require commitment about the belief in e  or e   Thus  as Table   in dicates  we may be ignorant about the status of the engine running given that the ignition key is not turned  In such a case the user may specify that both   engine running jturn key         engine running lturn key       indicating that both alternatives are plausible  Once similar matrices for each one of the families in a given network r are specified  the complete ranking function can be reconstructed by requiring that        m           eili e           and queries about belief for any proposition and wff can be computed using any of the well known dis tributed algorithms for belief update          The class of ranking functions that comply with the require ments of Eq    for a given network rare called stratified rankings  These rankings present analogous properties about conditional independence to those possessed by probability distributions quantifying a network with the same structure      
 We recently described a formalism for rea soning with if then rules that are expr essed  with different levels of firmness        The formalism interprets these rules as extreme conditional probability statements  specify ing orders of magnitude of disbelief  which impose const aints over possible rankings of worlds  It was shown that  once we compute a priority function z  on the rules  the de gree to which a given query is confirmed or denied can be comp uted in O lob rl   proposi tional satisfiabi lity tests  where n is the num ber of rules in the knowledge base  In this paper  we show that computing z  requires O n  x logn  satisfiability tests  not an ex p onent i al number as was conjectured in       which reduces to polynomial complexity in the case of Horn exp ressi o n s   Vi e also show how reasoning with imprecise observations can be incorporated in om formalism and how the popular notions of belief ret ision and epistemic entrenchment are embodied natu rally and tractably      Introduction  Infinitesimal Probabilities   Rankings and  Common Sense Reasoning  The uncertainty encountered in comr  n sense reason ing fluctuates over an extremely wide range  For ex ample  the probability that the new b oo k on my desk is about astrology is less than one in a million  If how ever  I spot a Zodiac sign on page    the probability becomes close to     say        Intelligent agents are expected to reason with such r are eventualities and to produce explanations and actions whenever these oc cur  Given this wide range of uncertainty fluctuations and the fact that the majority of everyday decisions in volve relatively low payoffs  the full precision of prob ability calculus may not be necessary  and a n ordei  of magnitude approxim ation may be su ffi cient   Thus      Judea Pearl  judea cs ucla edu   Cogni tive Systems Laboratory  Compu ter Science Department  University of California  Los Angeles  CA       moises cs ucla edu     instead of measurin g probabilities on a s cale from zero t o one  we can imagine projecting probability measures onto a quantized logarithmic scale and then treating beliefs that map onto two different quanta as being of different orders of magnitude  This method of approximation gives rise to a semi qualitative calculus of uncertainty  one in which de grees of  dis belief are ranked by non negative integers  corresponding perhaps to linguistic quantifiers such as  believable    unlikely    very rare   still capable of accounting for retraction and restoration of beliefs by Bayesian conditionalization  The origin of this approx imation can be traced back to E r nest Adams      who developed a logic of conditionals based on infinitesimal probabilities  and to the Ordinal Condition Functions of Spohn       Potential applications in nonmonotonic reasoning were noted in          and further developed in                          A simple way of viewing infinitesimal probabilities is to consider an ordinary probability function P defined over a set n of possible worlds  or states of the wo rld   w and to imag ine that the probability P  w  is a poly nomial function of some infinitesimal parameter     ar bitrarily close to  yet bigger than zero  for example     c t  or t    c t   Accordingly  the p robabilities as signed t o any subset of n represented by a logical for mula t p  as well as all conditional probabilities P  li   f    will be ration al functions of    We define the ranking function     l f     as the power of the most significant e  term in the expansion of P     In other words   jt p    n iff P  ifl p  has the same order of magni tude as e     li I P  The following properties of ranking functions  left hand side below  reflect  on a logarithmic scale  the usual properties of probabili t y functions  right hand sid e     with  min  replacing addition  and addition re placing multiplication    p      min   w   wl  l   P t p  P t p     Spohn    J called this function Ordinal Condition Function            P w   wl  l   P  ot p                  a  non probabilistic         Reasoning with Qualitative Probabilities Can Be Tractable  P r J A  p      P r JI p P           Parameterizing a probability measure by c  and ex tracting the lowest exponent of c  as the measure of  dis belief is proposed as a model of the process by which p eople abstract qualitative beliefs from numer ical probabilities and accept them as tentative truths  For example  we can make the following correspon dence between linguistic quantifiers and en  o P  P  e  P  P   e   P fjJ   e   P  P   e     P and    are believable    is believed    is strongly believed    is very strongly believed       J                                given is not sufficient for defining a complete ranking function  In order to draw plausible conclusions from such fragmentary pieces of information  we require ad ditional inferential machinery that should accomplish two functions  First  it should enrich the specification of the ranking function with the needed information and  second  it should operate directly on the speci fication sentences in the knowledge base  rather than on the rankings of worlds  which are too numerous to list   Such machinery is provided by a formalism called system z       which accepts knowledge in the form of quantified if then rules  e g    birds fly  with strength      and computes the plausibility of any given query  e g    Tim  being a red bird  flies  with degree       by syntactic manipulation of these rules  To accomplish these functions  system z  incorpo rates two principles in addition to those given above   These approximations yield a probabilistically sound calculus  employing integer addition  fr r manipulating the orders of magnitude of disbelief  The resulting formalism is governed by the following principles   a non negative integer K representing the degree of surprise associated with finding such a world      Each world is ranked by      Each well formed formula  wff  is given the rank of the world with the lowest    most normal world  that satisfies that formula      Given a collection of facts  we say that    follows from  P with strength   if              or  equiv alentl y  if the K rank of     degrees above that of     u         is at least        Principle s   and   follow immediately from the scman t cs decribed bove  Principle   says that    is plau Sible g ven  Iff        J          cc   where P is the cparametried probability associated with that par tlcular ra nkmg    This abstraction of probabilities matches the notion of plain belief in   l t at it is deduc tively closed  tllC price we pay  however  is that many     small probabthtles do not accumulate into a strong ar gument  as in the lottery paradox           The asic K rankig system  as described in Spohn       r qmres the spenfication of a complete ranking func tiOn before reasoning can commence  In othe  words  the knowledge base must be sufficiently rich to define the K associated with every world w  Unfortunatelv  in practice  such specification is not readily availabl   For example  we might be given the information that  birds orm ally fly   written      Jib       and no in frmatlOn whatsoever about the fl ying habits of red buds or non birds   Ve still would like to conclude that red birds normally fly  even t hough the information  If A is believed and B is believed then AI B is believed  ate  that this deviates form the threshold conception of be lief  If both P  A  and P  B  are above a certain threshold   P  A    B  may still be below that same threshold      Each input rule  if r p then    J   w i th strength       written r p    J  is interpreted as a constraint on the ranking     forcing every world in  p       ljJ to rank at least       degrees above the most normal world in r p  that is     fl p           Out of all rankings satisfying the constraints above  we adopt the  unique  ranking     that as signs each world the lowest possible  most normal  rank   rinciple    corresponds to the notion of consistency Def       and Principle   establishes that the in formaion in the set of if  hen rules  the knowledge   base  IS completed by asstgnmg the maximum like lihood possible to each world allowed by the consis tency constraints imposed by these rules  see Def      and        The first contribution of this paper is to improve the inference process of system z  and es tablish its tractability  A key step in the procedures developed in      was the computation of a priority ranking z  on the rules in the knowledge base  which was conjectured to be intractable  In Section    after some preliminary definitions in Section     we present a pr cedure for computing z  that requires a p oly llOITIIal number of propositional satisfiability tests and hence is tractable in applications permitting restricted languages  such as Horn expressions  network theories  or acyclic databases  m  The second contribution of this paper is to equip with he capability to reason with soft ev tdenc  r  nreese observations  Section     Such a capab hty  s Important when we wish to assess the plausibility of u  using Principle   above  but the con text  is not given with absolute certainty  In other words  there is some vague testimony supporting  p but tht testimony is undisclosed  or cannot be articulated usmg the basic propositions in our language  e g   tes tunony of the senses   all that can be ascertained is a summary of that testimony saying that   P is supported to a  degre n   We propose two different strategies for computmg a new ranking    from an initial one   ystem Z             Goldszmidt and Pearl  given soft evidential report supporting a wff  J  The first strategy  named J conditionalization  is based on Jeffrey s Rule of Conditioning       It interprets the re port as specifying that  all things considered   the new degree of disbelief for   should be          n  The second strategy  named L conditionalization  is based on the virtual evidence proposal described in        It interprets the report as specifying the desired sllift in the degree of belief in   as warranted by that report alone and  nothing else considered   We show that L conditionalization has roughly the same complexity as ordinary conditionalization  and then we relate our formalism to the theory of belief revision in      Fi nally  Section   summarizes the mai     esults   rule r p            violated  given that we know r p  In prob abilistic terms  consistency guarantees that for every c      there exists a probability distribution P such  that if  Pi      i E     then P       lr p        cc     Definition      The ranking     r     Pi      i  be a consistent set    The distinguished ranking      defined below  assigns to each world the lowest possible rank permitted by the admissibility constraints of Eq     Def        Such an assignment reflects the assumption that  unless we are forced to do otherwise  each world is considered as normal  likely  as possible  Let     of rules   defined as follows   Preliminary Definitions  The         w   Ranking      We start with a set of rules          I       tp   t Ji     i  n   where r p  and      are propositional  formulas over a finite language of atomic propositions      denotes a new connective  and    is a non negative integer  A truth valuation w of the atomic propositions in the language will be called a world  The satisfaction w  of a wff r p by a world is defined as usual and denoted w w satisfies r p  Let n is a model for r p if by w F r p  stand for the set of possible worlds  Ranking functions are defined as follows   Definition       Ranking s   A ranking function  is an assignment of non negative integers to the ele ments in n  such that   w      for at least one world w E n  We extend this definit ion to   duce rankings on wffs        P        mi  w      oo         w  if t p is satisfiable    otherWise        Similarly  given two wffs  P and  j  such that r p is sat isfiable   we define the conditional ranking       lr p  as     t Jjcp         tJ  A cp    K   P    Definition      Consistency  A ranking to be admissible relative to a given   iff    cp       tPi           h  tp         ljJi      is said        equivalently       ti  jr p         for every rule r p   t    E A set  is consistent iff there exists an admissible ranking K relative to       Consistency can be decided in        satisfiability tests   and it is independent of the   values assigned to the rules in          Eq    echoes the usual interpreta  tion of defaults rules       according t J which      holds in all minimal models for r p  In our case  minimality is reflected in having t l e lowest rank  that is the high w est possible likelihood  If we say that falsifies or w  rule r p       lj  whenever F  P       lj   the pa rameter    can be interpreted as the minimal degree of surprise  or abnormality  associated with finding the  violates a  where             r  I     IS  if w does not falsify any rule in   max            p   Z  r        otherwise   z   r         is a priority ranking on rules  defined by  z  r      min    F               x   w          The recursive nature between Eqs    and   is be nign  and we present an effective procedure  Procedure Z rank  for computing z  in the next section  In       we also show that Eqs    and   define a unique admis sible ranking function     that is minimal in the fol lowing sense  Any other admissible ranking function must assign a higher ranking to at least one world and a lower ranking to none  An alternative mechanism for enriching the original specification of a ranking  probability  function in the form of a set of conditional if then rules is studied in       where Maximum Entropy principle is used to select a privileged distribution among those probabil ity distributions complying with the constraints im posed by the rules  In the language of rankings  this distribution can be represented as a set of recursive equations similar to Eqs    and        w                 does n t falsify any rule n   L w   p i    p    Z   r         otherWISe  if  w       The computation of the z priorities and the query answering procedures for the ma ximum entropy ap proach has been proven to be NP hard even for Horn clauses  see           Plausible Conclusions  Computing the z  rank  Given a set     each admissible ranking  consequence relation        cr          cr  where   h   induces a  iff    cr A      A straightforward way to declare cr as a plausible conclusion of  given   would be to re quire  J cr in all x  admissible with This leads          K       The equation for z is identical to that of  replaced with         z    with   Reasoning with Qualitative Probabilities Can Be Tractable  to an entailment relation called e  semantics        entailment       and r entailment       which is recog nized as being too conservative  The approach we take here is to select a distinguished admissible ranking  in our case     and declare u asa plausible conclusion of given  written kr  y  iff K       ff                According to Eq    n i Def       both    and I  can be computed effectively once the priority rank ing z  on rules is known  We next present a pro cedure for computing z   which is identical to the one presented in      save for the crucial computation of Eq     Step   b     Whereas in      this computa tion wasthought to require an exponential search over worlds  we now show that it can be accomplished in      x log     satisfiability tests  Some of the steps in Procedure Z rank depend upon the notion of toleration  A rule         O  is tolerated by  if the wff     O       Pi     tPi is satisfiable  where i ranges over all rules in     Procedure Z  rank Input  A consistent knowledge base     Output   z   ranking on rules     Let     nz   be the set of rules tolerated by be an empty set      and let     Pi    Jj E o  do  set Z   r     nz  u  ri      While nz       do   a  Let   be the set of rules in I     nz  tolerated by      For each rule         and nz          b  For each r        E     let r lr denote the set of models for     l  that do not violate any rule in  f      compute  Z       min  n  w       b       Wrtnr  where  n  wr                  max  Z rj lwrF Pj     j  l  rienz    c   i   RZ      Let r be a rule in fl   having the lowest nz    n z  u  r    and  ri    Pi        tPi  E  Z  set  End Procedure  Theorem     establishes the correctness of Procedure Z rank  while Lemmas     and     and Theorem     determine its  polynomial  complexity  u we are concerned with the strength fJ wi t  h which  the conclusion is endorsed  then   K       A    u    J    I iff       Au   fJ      The notion of toleration is also crucial for deciding con sistenc y   ll  is consistent iff there is a tolerated default rule in every nonempty subset ll   of ll   Theorem  Note that Eqs   Def                      and    correspond to Eqs    and     in  Theorem             The function Z computed Z rank complies with De        that is Z   z     by    Let      l i I r     p   tPi  be a consistent set where the rules are sorted in nondecreasing order according to priorities Z  ri   Let   w  be defined as in Eq      Lemma           w       ma    ifw does not falsify any rule in     ll  x    I              p   Z  r        athenmse   Then  for any wff         can be computed O log IIJ propositional satisfiability tests   m  The idea is to perform a binary search on  to find the lowest Z    such that there is a model forthat does not violate any rule r  with priority Z r       Z t   This is done by dividing  into two roughly equal sections  top half  rmid to Thigh  and bottom half  rrow to rmid    A satisfiability test on the wff a         id  Pi  J tPi decides on whether the search should continue  in a recursive fashion  on the bottom halfor on the top half   The value of Z         O    in Eq    can be computed in O log IRZ I  satisfiability tests   Lemma      Let  in Step   a  be equal to   Pi    J    the com putation of Eq    is equivalent to computing the     of the wff O       i cp      tPi where i ranges over all the rules in f      by performing the binary search on the set n z    Given a consistent  the computation of the ranking z  requires        x log     satisfia bility tests   Theorem      Computing Eq    in Step   b  can be done in O log  nz  I  satisfiability tests according to Lemma       and since it will be executed at most        times  it requires a total of      x log      Loop   is performed at most     lol times  hence the whole computation of the priorities z  onrules re quires a total of        x log I  I  satisfiability tests   Once z  is known  determining the strength fJ with which an arbitrary query O  is confirmed  given the information   requires O log     satisfiability tests     and  For reasons of space  formal proofs of Lemmas            and T heorem       can be found in the Technical  Report available by request   nz  nz    Note that we need to be sorted  nondecreasingly  with respect to the priorities Z  This requires that the initial values inserted in in Ste p   of Procedure Z rank  be sorted    O Jll oJ    data comparisons   and that the  new Z value in Step   c  be i n serte d in the right place I  data comparisons  We are assuming that the cost of each of these operations is much less than that of a satisfiability test   O i R z    The complexity of the remainding steps in the proce  dure is bounded by  O jll i  satisfiability tests              Goldszmidt and Pearl First N     u  and N         u  are computed  using a binary search as in Lemma      Then  these two val ues are compared and the difference is equated with the strength    Clearly  if the rules in  are of Horn form  computing the priority ranking z  and deciding the plausibility of queries polynomial time               f   can  b e done in  Belief Change  Soft Evidence  and     Imprecise Observations  If u was defined as a pair of Boolean   where   the context  stands for the of observations at hand and u  the target  stands  So far  a query  formulae   set   f    for the conclusion whose belief we wish to confirm  deny  or assess  A query    f  would be answered in the affirmative if  f was found to hold in all mi nim a lly ranked models of  and the degree of r  elief in  f would be given by K   u       te u       In many cases  however  the queries we wish to answer cannot be cast in such a format  because our set of observations is not precise enough to be articulated as a crisp Boolean formula  For example  assume that we are throwing a formal party and our friends Mary and Bill are invited  However  judging form their pre vious behavior  we believe that  if Mary goes to the party  Bill will stay home  with strength      written M           B  Now assu me that we have a strong hunch  with degree K  that Mary will go to the party  perhaps because she is extremely well dressed and is not consulting the movie section in the Times  and we wish to inquire whether Bill will stay home  It would be inappropriate to query the system with the pair  M    B   because the context M has not been estab lished beyond doubt  The difference could be critical if we have arguments against  Bill staying home   for example  that he was seen renting a tuxedo  A flex ible system should allow the user to assign a degree of belief to each observational proposition in the con text  and proceed with analyzing their rational con sequences  Thus  a query should consist of a tuple like  I K    K        m I m   f   where each    mea sures the degree to which the contextual proposition ifl  is supported by evidence    At first glance it might seems that such facility is auto matically provided by system z   through the use of  variable strength rules  For example  to express the  fact that Mary is believed to be going to the p ar ty    we can perhaps use a dummy rule Obs   i M  stating that if Mary meets the set of observations Obs  then Mary is believed to be going to the party  and then add the proposition Obs  to the context part of the query  to indicate that Obs  has taken place    We remark that evidence in this paper is regarded as  setting the context of a query and not as a modifier of the knowledge in   Statistical methods for  l e l at er task are explored in        This proposal has several shortcomings   however   First  the net impact of our new rule Obs   M would be sensitive to previously collected information about Mary s intentions  say she has bought a plane ticket  that we may wish to suppress  In other words  we of ten wish to express the assessment that  all things considered  Mary s going to the party is believed to degree    Second  in many systems it is convenient to treat if then rules as a stable part of our knowledge  unper turbed by observations made about a particular indi vidual or in any specific set of circumstances  This permit s us to compile rules into a structure that al lows efficient query processing  Adding query induced rules to the knowledge base will neutralize this facility  Finally  mles and observations combine differently   The latter should accumulate the former do not  For example  if we have two rules a  c and b  c and  we observe a and b  system z  would believe c to a degree max          However  if a and b provide two in dependent reasons for b eliev ing c  the two observations together should endow c with a belief that is stronger than any one component in isolation  To incorporate such cumulative pooling of evidence  we must encode the assumption that a and b are conditionally indepen dent  given which is not automatically embodied in  cj   system z      To avoid these complications  the method we propose treats imprecise observations by invoking specialized conditioning operators  unconstrained by a rule s se mantics  We distinguish between two types of eviden tial reports       Type J   All things considered   our current belief in ifJ should become J       Type L   Nothing el se considered   our cur r ent  belief in  should       shift by L   Type J  All Things Considered  Let  be the wff representing the event whose belief we wish to update so that           p    J  and  consequently  e            In order to compute K  IP  for every wff   J  we rely upon Jeffrey s Rule of Conditioning       Jeffrey s rule is based on the assumption that while the observation changed the agent s degree of belief in  and in certain other proposition  it did not change the conditional degree of belief in any propositions on the evidence  or on the evidence    p       Thus  letting P  denote the agent s probability distribution after the report on the value of P   ifJ  is incorporated    The assumptions of conditional independence among  converging rules is embodied in the formalism of Maximum Entropy           This is an immediate consequence of the semantics for ra nkings and corresponds to the normalization in proba bility theory  see Eq        Reasoning with Qualitative Probabilities Can Be Tractable  and  using P to denote the agent s probability distri bution prior to this report  we have             P   P  t         and P leads to Jeffrey s rule  which     P   t bl       Translated into the language of rankings  using Eqs       Eq     yields             n                     n      e K         min n    which offers a convenient way of computing     once we specify N         and  i   ifJ    J  Eq     assumes the an especially attractive form when computing the     of a world w        Eq     corresponds exactly to the a conditionalization proposed in Spohn        Def     p        with a  J  If                oo  this process is equivalent to ordinary Bayesian conditionalization  since k  w    k wl  if w I  and     w    oo otherwise  Note  however  that in general this conditionalization is not commutative  if  Pl and z are mutually dependent  i e       zl     f            the order in which we establish t   t   J  and           h might make a difference in our final belief state  represented by the ranking k     Type L Reports  Nothing Else Considered  L conditionalization is appropriate for evidential re ports of the type  a new evidence was obtained which  by its own merit  would support  to degree L   Un like J conditionalization  the degree L now specifies changes in the belief of   not the absolute value of the final belief in  P  As in the case of type J reports  we assume that in naming  as the ditect beneficiary of the evidence  the intent is to convey the assumption of conditional independence  as formulated in Eq      Next  we assume that the degree of support L charac terizes the likelihood ratio       ifJ  associated with some undisclosed observation Obs  as is done in the method of virtual conditionalizati on                    O   bsl  P Obsl    P            Eq     is known as the J cor dition         Tbis condition mirrors probabilistic dependence  i e   P  P i Pd  f  P      lf Spohn        p       has acknowledg  c  the desirability of commutativity in evidence pooling but has not stressed that a conditionalization commutes only in a very narrow set of circumstances  partially specified by his Theorem     These circumstances require that  successive pieces of evidence support only propositions that are relatively inde pendent   the truth of one proposition should not imply a belief in another  Shenoy   i  has corrected this deficiency by devising a commutative combiuation rule which behaves similar to   conditiouing    P   l    cP              P                                    which governs the updates via the product rule               P    tP           Translated into the language of rankings  this assump tion yields  i  ifJ   i  cP    x  cfo   K       L        and  since either    cfo  or K   P  must be zero  we ob tain max O                L   max O  K    jl           L               We see that the effect of L conditionalization is to shift the degree of disbelief difference between  and    p by the specified amount L  Once       is known  we can use Jeffrey s rule   Eq      to compute the t      for an arbitrary wff      we have that                min i                            ifJ   L        min     PI      J I P             L        min t    l          I     P          depending on whether     ifJ   i   P  is less than  greater than  or equal to L  This expression takes the following form for    w    t    w          x    wl  max O              L  w  l   max O            cfo  L                   depending on whether w      or w       As in J conditionalization  if L   oo then      w    K   wl      For the general case  we can see that the effect of L conditionalization is to shift downward the K of all worlds that are models of the supported proposition  P relaLive to the    of all worlds that are not models for   b  However  unlike J conditionalization  the net rela tive shift is constaut and is equal to L  independent of the initial value of x    It is easy to verify that L conditionalization is commutative  as is its proba bilistic counterpart  see Eq       and hence it permits a recursive implementation in the case of multiple ev idence  We can illustrate these updating schemes through the party example consisting of the single rule rm   M    B    if Mary goes to the part y  then Bill will not go    A trivial application of Procedure Z rank yields z   m       and using Eqs    and   we find x  x      for every proposition x  except x   B A fl f  for which we have x   vf A B       This means that we have no reason to believe that either Mary or Bill will go to the party  but we are pretty sure that both of them will not show up  Now suppose we see that Mary is very well dressed  and this observation makes our be lief in I   increase to    that is          Af       As a consequence  our belief in Bill staying home also in creases to   since  using either J conditionalization or L conditionalization      B       Next  suppose that someone tells us that he has a strong hunch that Bill plans to show up for the party  but he fails to tell us             Goldszmidt and Pearl why  There are two ways in which this report can influ ence our beliefs  The natural way would be to assume that our informer has not seen Ivlary s dress  and might not even be aware of Bill and Mary   relationship hence we assess the impact of his report in isolation and say that whatever the value of our current belief in Bill going  it should increase by   increments  or fj L      Following Eq            B  and K        M  will both be equal to    and we are back to the initial un certainty about Bill or Mary going to the party  except that our disbelief in both  Mary and Bill being at the party has decreased to        M     B       The second way would be to assume that our informer is omni sc ient and already has t aken into consideration all we know about Bill and Mary  He means for us to revise our rankings so that the final belief in   Bill going  will be fixed at        B       With this interpretation  we J condition       on the proposition      B and obtain        M       concluding that it is Mary who will not show up to the party after all       Complexity Analysis  From Eq      we see that x    l     can b computed from x   tf i  and           which  assuming we have z    requires a logarithmic number of propositional satisfi ability tests  see Section     L conditionalization can follow a similar route  as depicted in Eq      Special precautions must be taken when simultaneous  multiple pieces of evidence become available  First  J conditionalization is not commutative  hence we can not simply compute      by J conditioning on   and then J conditioning K  on     to get  K     We must J condition simultaneously on    and   with their re spective J levels  say h and Jz  Worse yet  an auxil iary effort must be expended to compute the J level of each combination of  s  in our case        z              etc  This is no doubt a hopeless computation when the number of observations is large    conditionalization  by virtue of it s commutativity  enjoys the benefits of recursive computations  Let e   and e  be two  undisclosed  pieces of evidence sup porting     with strength L    and   with strength L      respectively  We first   condittun     on ljJ  and calculate         and          using Eq     and Eq      respectively  Applying Eq     this time to                  we calculate       tf l     Second  we L condition     on    compute K       using Eq      and  finally  using   x    tf l      and          in Eq     obtain        iJ   Y  Note that  although each of these calculations requires only O log I I  satisfiability tests  this computation is ef fective only when we have a well designated target hypothesis tjJ to est ima te  The computation must be repeated each time we change the target hypothesis  even when the context remains unaltered  This is so because we no longer have a facility for encoding a   The generalization to more than two pieces of evidence is straightforward   complete description of x     as we had for K  using the z   function   Thus  the encoding for K    may not be as economical as that for K  the number of worlds is astro nomical   unless we manage to find dummy rules th at emulate the constraints imposed on   by the  undis closed  observation  Such dummy rules must enforce the conditional independence constraints embedded in Eq      without violating the admissibility constraints  Eq     in   see             Relation to the AGM Theory of Revision  Alchourron  Gardenfors and Makinson  AG M  have advanced a set of postulates that have become a stan dard against which proposals for belief revision are tested       The AG M approach models epistemic states as deductively closed sets of  believed  sentences and characterizes how a rational agent should change its epistemic states when new beliefs are added  sub stracted  or changed   The central result of this theory is that these postulates are equivalent to the existence of a complete preordering of all propositions according to their degree of epistemic entrenchment such that belief revisions always retain more entrenched propo sitions in preference to less entrenched ones  However  the AGM postulates do not provide a calculus with which one can realize the revision process or even spec ify the content of an epistemic state                Spohn      has shown how belief revision conforming to the AG M postulates can be embodied in the con text of ranking functions  Once we specify a single ranking function K on possible worlds  we can asso ciate the set of beliefs with those propositions tf  for which                 To incorporate a new belief j   one can raise t he x  of all models of   j  relative to those of J   until k   j   becomes  at least     at which point the newly shifted ranking defines a new set of belief      This process of belief revision  corresponding to a  conditioning  with a       was shown to obey the AGM postulates  from which it follows that revision in the probabilistic  system described in this paper also obeys those postulates under the same interpretation of beliefs  Still  neither the AGM theory nor Spoh n  s embodi ment of the theory are directly applicable to AI sys tems  the former because it does not provide a finite specification for belief sets and their entrenchment or dering  and the latter because it requires an explicit encoding of the ranking function on all possible worlds  To better model AI practice  Nebel      adapted the AGM theory so that finite sets of base propositions mediate revisions  This is exemplified in the nonmono        where the tonic systems of Brewka     and Poole   base consists of propositional implications  or expecta tions        The basic idea in these syntax based sys tems is to define a  total  priority order on the set of base propositions  and select revisions to be maximally consistent relative to that order   Nebel has shown that such a  str ategy  can satisfy almost all the AGM ax    Reasoning with Qualitative Probabilities Can Be Tractable  i oms  Boutilier     has fu rther shown that  indeed  the priority fu nction z  corresponds naturally to the epis temic entrenchment ordering of the  AGI   theory       Unfortunately  even Nebel  s theory does not com pletely su cceed at formalizing the practice of belief re vision  as it does not s pecify how the priority order on the base propositions is to be determined   Although  one can imagine  in principle  that the knowledge au thor specify this order in advance  such specification would b e impractical  since the order might  an d   as  ditional statements   Having the capability of adopting new conditionals  as rules  also provides a simple semantics for interpreting more complex sentences i nvolving conditionals   e   g       I f you wear a helmet whenever y o u r i d e a motorcycle   then you wont get hurt badly i f you fall       Both nested and negated conditionals cease to be a mystery once we permit explicit references to defaul t rules  The sentence  If  a      b  then   c      d   is interpreted as   If I add the default a   b to t   the n  c  d  will be i n the consequence relation  of the  we h ave seen  should  change whenever new rules are added to the knowledge base   resulting knowledge b ase      Thus we see that there are several computational and epistemological advantages to our system over those proposed by AG M and Spoh n   stemming fmm the fact t h at our revision process revolves armmd a fi nite set of conditional rules  not around the beliefs   the rank ings or t he expectations that emanate from those rules  First   since the number of propositions in one s belief set is astronomical  and so is the number of worlds  i t i s a computational necessity t o base belief revision on rules  whose number is usual l y manageable  Secon d   our system extracts both beliefs and rankings o f be l iefs automatically from the content of      no outside specification of belief orderings is required  Thir d   in order to facilitate recovery from obsolete ob servations Spohn  s framework m ust assume that all observations are defeasible  or imprecise     which corre  sponds to a conditioning with a   oo   In our system  we can accommodate both imprecise and precise obser vations  corresponding to  a   oo  using ordinary con  d i t ioning   G i ven a set of precise observations   the set of beliefs is defined as those propositions u for which         ul        Retraction of obsolete observations can  b e done by simply removing those observations from              t  U  a        b      which i s clearly a proposition that can be tested i n the language of default based ranking systems  Note the essential distinction between having a condi  tional rule a    b explicitly in t  and certifying that f o  b holds in the consequence relation o f       The  a  former is a permanent constraint that interacts with  other rules to shape the priority order z    while the latter indicates a contingent expectation   certified by the Ramsey test  that passively reflects that ordering  This distinction gets lost i n systems such as Spohn s or G iirdenfors          that do not acknowledge the cen trality of conditionals ranking beliefs       as  the basis for generating and  Conclusions     This paper proposes a belief revision system that rea sons tractably and plausibly with l inguisti c quantifica tion of both observational reports  e g     looks l ike    and domain rules  e g      typically      We h ave shown that the system is semi tractable  namely  tractable for      This flexibility is facilitated by mil      t a ining a fixed set of con ditional rules   w i th the help uf which one can always restore beliefs  and rankings  so that  they re  every sublanguage in w hich propositional satisfiabil ity is polynomial   Horn expressions  network theories   flect t he observations at han d   i ndependently of those seen in the past   edge  this is the first system that reasons with approx imate probabi l ities which offers such broad guarantees  Finally  and perhaps most significantly  our system is capable of responding not merely to empirical obser vations but also to lingu istically transmitted infonna tion such as conditional sentences   i   e     if then rules     For example  sup p ose someone tells us t ha t   I ary too tries to avoid Bill in parties  we simply ad d this in  formation to our knowledge base in the form of a new rule  B         H   reco mpute z     an d are prepared to respond to new observations or hearsay   In Spohn s  system  where revisions are limited to a conditioning  one cannot properly revise beliefs in response to con   acyclic expressions  etc        We expect these results to carry over to the theory of possibility as formulated by Dubois and Prade          which has similar features to Spohn s system except that beliefs are measured on the real  of tractability       interval            In addition we have shown that   with out loss of tractability  the system can also accommo d ate expressions of imprecise observations  thus pro    G ardenfors        attempts to devise postulates for con ditional sentences  but finds them incompatible with the Ramsey test  page           See also Boutilier analysis of Ramsey test  and the AGM axioms        The  p ro o f  in      considers the priorities z  resulting  fiat se t  of rules       d      namely one in which are    as in system Z        Bou t i l  I  abo shows     an entrenchment ordtriug ob eying the A G M frame  from a all b   s  that wo r k obtains from the Z priorities of the negations of the material counterpart of rules   To the best of our knowl  Example      for an  due to Calabrese  personal comm u nica tion     Belief revision systems proposed in the database Jit  erature          s u ffer from the same shortcoming  In th at context defaults   and conditionals  rep r esen t integrity con straints with exceptions          Whereas most tract ability results exploit the topolog ical structure of the kn o wl edge base   hypertrees  or partial hypert rees     ours are topology independent              Goldszmidt and Pearl  viding a good model for weighing the impact of ev idence and counter evidence on our beliefs  Finally  we have shown that the enterprise of belief revision  as formulated in the work presented in       can find a tractable and natural embodiment in system z    un hindered by difficulties that plagued earlier systems                         P  G ardenfors  Nonmonotonic inferences based on ex pectations  A pre li mi n ary report  In Principles of Knowledge Representation a n d Reasoning  Proceed ings of the Second Interna tional Confe rence  pages          Boston            Acknowledgements        This work was supported in part by N ational Science Foun dation grant  IRI             and State of California MI CRO             The first author was supported by an I B M graduate fellowship                        E   W   Adams   The Logic of Conditionals  D   Reidel       C   Alchourr n  P  G iirdenfors  and D   Makinson  On the logic of theory change   Partial meet contraction  and revision functions                            F   Bacchus   loumal of Symbolic Logic   R   Ben Eliyahu   Boutilier  What is a default priority   In Proceed  the Intemational Joint Conference in A  tificial Intel ligence ICA I      Detroit                 W  Buntine   M   Dalal              m alizing commonsense knowledge  gence                                 base revision  Preliminary report  In Pmccedings of the Seventh National Confe rence on A l tificial Intelli gence  pages                          W   Dowling and J  G allier  Linear time algorithms for testing the satisfiabilit y of propositional Horn fornm lae   Journal of Logic Progra mming                             J   Doyle  Rational b elief revision   preliminary report     I n Proceedings of Principles of Knowledge Rep esenta tion and Reasoning  pages           Cambridge  Mas sachisetts                D   Dubois and H   Prade   Possibility Theory   An  Appmach to Compute rized Pmcessirz J of Uncertain ty   Plenum Press  New York                  R  Fag i n J   D  Ullman  aud  M  Vardi  On t he semantics of u p d a t es in databases  In P ocecdings of the   nd  A CM SIGA CT SlGMOD  SIGA R T Sympo  sium on Principles                 of    Database Systems  pag es       A rtificial Intelli  B   Nebel  Belief revision and default reasonin g  Synt ax based app r o aches   In Proceedings of Principles of Knowledge Representation and Reason ing  pages             Cambridge  Ma ssachisetts                 J   Pearl  Probabilistic Reasoning i n Intelligent Sys tems  Netwo ks of Plausible Inference  Morgan Kauf  maun  San M ateo  C A                Los Angeles                Investigations into a theory of knowledge  What does a conditional knowledge       J  McCarthy  Applications of circumscription to for  In Proceedings of the  th Confuence on Unce  tainty in A I  pages       Some proper ties of plausible reasoning   Goldsz m id t and J   Pearl  System z    A formal ism for reasoning with variable strength defaults  In Pmceedi ngs of A me rican A ssociation for A rtificial In telligence Conference  Anaheim  CA            Torouto                  G   Brewka  Preferred su bt heories    I n ex te n d ed logi cal framework for default reasoning  In Pmceedings of  Boston          base entail  In Proceedings of Principles of f nowl edge Representation and Reas oning  pages           ings of the Canadian Confe ence on A  tificial Intelli  gence  Vancounver              M        D   Lehmann   P h   D   dissertation  University of  Toronto                  P  Morris  and J  PearL A m aximu m  tional Workshop on Nonmonotonic Reasoning  forth  Technical Report R           Boutilier  Conditional logics for default reasouing  and belief revision     C      coming   Vermont         NP complete problems in optimal  horn clauses satisfiability   Cognitive systems lab  U CL A               Gold sz m idt         M   Goldszmidt and J   Pearl  Stratified rankings for causal relations  In Proceedings of the Fourth Interna  Representing and Reasoning w i t h p ob  abilistic Knowledge  A Logical Approach to Probabili ties  The MIT Press  Cambridge                   C   M   ligence Co nference  pages         Dordrecht   Netherlands                  H   A  Geffner  Default reasoning  Causal and condi tional theories  P h   D   dissertation  U n ive rsi ty of Cal ifornia Los Angeles  Computer Science Department  Los A ngel es          Forthcoming  M IT Press  entropy approach to nonmonotonic reasoning  In Pro ceedings of A merican A ssociation for A rtificial Intel  
 The rapid growth and diversity in service offerings and the ensuing complexity of information technology ecosystems present numerous management challenges  both operational and strategic   Instrumentation and measurement technology is  by and large  keeping pace with this development and growth  However  the algorithms  tools  and technology required to transform the data into relevant information for decision making are not  The claim in this paper  and the invited talk  is that the line of research conducted in Uncertainty in Artificial Intelligence is very well suited to address the challenges and close this gap  I will support this claim and discuss open problems using recent examples in diagnosis  model discovery  and policy optimization on three real life distributed systems      Introduction  It is undeniable that services in the internet are changing the way we travel  access information  invest  bank  shop  and conduct business and research  These services are supported by an ecosystem of information technology  IT   including storage  network  and middleware applications that is growing in complexity  The complexity is due to scale  over tens of thousands of computers in some cases   because systems are getting more distributed both in terms of function and geographical location  and because their ownership is becoming federated even inside an organization  These systems present numerous management challenges both in terms of day to day operations  and in terms of strategic and long term planning  Pervasive instrumentation and query capabilities are of course necessary elements for the solution to these  problems              In fact  there are now many commercial frameworks on the market for coordinated monitoring and control of large systems  tools such as Hewlett Packards OpenView suite of products  IBMs Tivoli  and Microsofts MOM  aggregate information from a variety of source and present it graphically to operators  Yet  it is widely recognized that the complexity and demands of these deployed systems surpasses the ability of humans to diagnose and respond to problems rapidly and correctly  and to assess strategic issues such as capacity planning              Indeed  research on tools  algorithms  and technology for analyzing and interpreting the instrumentation data leading to  automation in  diagnosis  decision making  and control  has not kept pace with the demand for practical solutions in the field  This paper  and the invited talk  makes the claim that UAI researchers are in a very advantageous position to address the challenges outlined above and advance the state of the art to close the gap between measurement and analysis for decision making  Section   enumerates the reasons for this claim and also outlines a couple of statistical inference problems that are particular to this domain and impactful for UAI research  Section   reviews examples in diagnosis  model discovery  and policy optimization on three real life distributed systems  Subsections           and     respectively   These examples are drawn from my own research experience and projects  These projects are at different levels of maturity in terms of results  Yet  all of them I believe  open difficult challenges for UAI research and are illustrative of the space  As the projects relate to my own experience  this paper and the talk should not be taken as a comprehensive review of recent research in self managing systems  datamining of systems logs  or machine learning techniques applied to systems  For that the reader is encouraged to look at the proceedings of the appropriate venues  Examples are The ACM Symposium on Operating        GOLDSZMIDT  Systems Principles  SOSP    The Usenix Symposium on Operating Systems Design and Implementation  OSDI    The IEEE International Conference on Autonomic Computing  ICAC   and the various workshops including those on Hot Topics  and those relating machine learning and systems  Section   provides some final thoughts      UAIers to the rescue   Let me start by enumerating the reasons why I believe that UAIers are perfect for the job     Given the relative maturity of measurement and instrumentation tools  as well as data collection and logging mechanisms  there are large amounts of data     There is uncertainty coming from noise in the measurements  unobservable or hidden variables  missing values  and other factors     To be really effective  the models for any of the tasks  diagnosis  decision making etc   need to combine knowledge engineering and information coming from pattern recognition  and statistical learning  a definite strength of Bayesian networks        The source of knowledge comes from the area of distributed systems and from the specific design of the particular systems in operation  At the same time  because of the complexity mentioned above  there is a need for inducing information from data in order to complement and sometimes verify this knowledge     Models should work equally well in interaction with humans as well as in automated fashion  Therefore models should be both interpretable and susceptible to audits     Some of the fundamental tasks and building blocks  diagnosis  knowledge discovery and engineering  decision making are core research issues in UAI  The UAI reader will undoubtedly spot the obvious challenges for inducing models and making decisions in a large scale IT system  large amounts of data  high dimensionality  etc  These challenges are there  and they will undoubtedly require ingenious computational methods for inference  However  I would like to draw the readers attention to two issues that are necessary building blocks and to the characteristics of this particular domain that introduces new challenges  The first      www sosp org  www usenix org   issue pertains to hypothesis testing and model selection   and the second issue pertains to the diagnosis of the parameters and structures of the graphical models we intend to use in this domain  As part of our focus as a community includes the induction of models from data  our paths cross and intersect with research and applications of statistical inference  As we will see in Section    we need for example  to determine whether quantities of interest are different from one another in a significant way  This will take the form of comparing classification error in model selection tasks  diagnosis in Subsection       determining whether two probability distributions are the same for model discovery  Subsection       and in determining whether an action has an effect in the policy optimization task in Subsection      Whether these tests and decisions are embedded and therefore being taken in an automatic fashion or being taken by humans  in order to be trusted  we need to provide guarantees on false positives  and detection rates  or in terms of log odds  In discussing the first issue  I will first concentrate on a hypothesis testing approach  and then look at a Bayesian approach  Please note that in spite of the philosophical and methodological differences  statisticians focused on Bayesian approaches still devote considerable energy and effort on hypothesis testing         As a brief review of classical hypothesis testing and to introduce some useful notation  Consider the case where we observe data X coming from a distribution f  x   and that we are interested in testing H         namely  the null hypothesis H  that the samples where generated from a distribution with parameter     The statistical procedure consists of first choosing a test statistic T   t X  where large values of T reflect evidence against H    and second computing the pvalue p   P   t X   t x    rejecting H  if p is small enough  The objective is to test whether the p value warrants the rejection of the null hypothesis  note that if the p value does not reach the desired threshold all we can say is that we failed to reject the null hypothesis  but cannot say anything formal about its true state   The statistics literature is full of examples of statistics T and ways to compute p values for most of the common cases  There are also variants depending on whether we want to test equality to     whether       or whether  lies in some interval close to     We wont go into these variants  The important thing   Depending on your leaning towards frequentist or Bayesian methodology    This is really a simplified version of hypothesis testing which suffices for the purposes of this paper  The reader is encouraged to go to the relevant literature for a formal treatment of this important subject  a good introduction can be found in          GOLDSZMIDT for this paper is that there are two characteristics of the domain of application that makes the task particularly challenging  First  we must conduct a multiple number of those tests  and second  the number of samples used in the tests varies greatly from tens to hundreds of thousands  The p value in hypothesis testing provides a basis to determine the false positive rate of a single test  The problem when we are conducting m hypothesis testing is that the p value determines the proportion of the null hypothesis falsely rejected on the m tests  Yet  what we really want to know is the number of falsely rejected hypothesis amongst just the rejected tests  Thus for example  in subsection     rejecting the null hypothesis will mean that two services are correlated in a single computer  Those are the cases of interest for us  If in a server there are        tests  and we use a p value of      as a basis for rejection  this would mean that we can expect the false positive rate to be      Now  if only       of those tests reject the null hypothesis  this would mean that half of those cases are not correlated whereas we think they are  This issue has not escaped statisticians  the False Discovery Rate  FDR  approach     provides an algorithm for selecting the appropriate p value amongst those obtained in the multiple tests to guarantee a given FDR   i e  an acceptable number of falsely rejected H  in the number of rejected tests   A search in any of your favorite engines for FDR will yield a large number of hits with very recent dates  ranging from      to       evidence of the importance of this issue in statistical research  In fact in a recent talk by Brad Efron       FDR is placed as a must directive for statistical research in the   st century   He presents studies in particle physics and studies with microarrays in modern bioscience as his motivating domains  Yet  the main characteristics of these domains are very similar to the ones outlined above  I do think that UAIers should do well and pick up these techniques and adapt them to our models and techniques  and I am glad to realize that this has already started in a paper in this conference         One of the problems with large sample sizes is that we not only have to worry about whether the differences are statistically significant  i e   the test is rejected   but we should also check whether the size of the effect is too small  As an example  suppose we are trying to decide whether the difference between two means is significant  and to make it simple assume that we    The paper is based on his presidential address to the American Statistical Association    Many thanks to D  Heckerman for sharing a preprint of this paper  I am not aware of other publications in UAI on the subject  if there are  my apologies for the omission        know that the distributions have the same variance   The usual statistic T for the test in this case is the number of standard deviations that the sample mean is away from the mean in     Note that as the number of samples increases  the value of the standard deviation decreases  This can happen to the point  where even if the test is rejected  the confidence interval is so close to zero  that the difference doesnt really make practical sense  This problem has also been addressed by statisticians in various ways  and has a longer history  One of the consequences is that we may have to select a more appropriate p value  i e   increase the number of standard deviations  to make sure that the test carries practical significance  Of course  this provides little comfort in our domain  since as stated above  there is great variability in the number of samples  Berger and colleagues           make other technical points clear  including an attempt at calibrating the p values when there are large number of samples and the difference between hypothesis testing  the risks of confusing a p value with the posterior P  H   X   and on ways of computing that posterior  This is a good transition to another approach consisting of following a strict Bayesian methodology and transforming the hypothesis testing into a log odds test  Letting M  and M  be models for H  and H    in this case we compare the  log Rratio between P  X H    M        and P  X H    M      P  X H    M     P   M     The Bayesian approach is very attractive because it offers semantic clarity  it computes a posterior directly based on the observed data  and does not  in principle suffer from the multiple hypothesis testing problem  Yet  it does present us with some additional challenges  We now need to specify a model M  for H    namely the point of comparison to H    a parameterization   and priors for   This may not be trivial in some cases  and it may even be restrictive in others  In addition  we may find ourselves resorting to MCMC simulations to compute inferences  for example the integral above  which may be prohibitive for embedded tasks in our domain  Furthermore  we would also need a threshold for our log odds decision and as exposed in     for example  this again may difficult and of course  application dependant  One is tempted to speculate that a suitable synthesis should emerge  Interestingly enough  Efron      also mentions such a synthesis  and makes a and explicit and strong connection to empirical Bayes methods in his own approach to the FDR problem  The second major issue is that of diagnosis of graphical models themselves  It has been approximately    years since Cooper and Herskovits paper      on inducing Bayesian networks from data  Yet  in spite        GOLDSZMIDT  Figure    Time series of the average response time  annotated with instances of the different signatures for the metrics that present abnormal state  The signatures were clustered  cluster   refers to a recurrent problem identified by the system operators  Cluster   presents symptoms of high Database CPU utilization and low Application server utilization  Cluster   has memory and disk utilization metrics problems   of valiant efforts for example                   we still dont have a set of tests  methodology  and software to test and diagnose over parameters and assumptions  recipes for remedial actions etc  We havent agreed  or for that matter even discussed in depth  how to compute the confidence on the presence absence of an arc or  on viable sensitivity tests for the appropriateness of a specific parameterization on a family in an induced model  This is particularly important in the domain of complex IT systems  where conditions may change drastically because of changes in workloads  daily regimes  and system configuration  and where our models that are automatically induced  see for example Subsection      are fundamental in enabling decision making  Basic and fundamental research on methodology for assessing the quality of induced models and remedial actions is badly needed      Three real systems and three tasks  As explained in the introduction  the following three subsections briefly describe three projects on real systems  These should be taken as descriptive and illustrative examples and not as set of prescriptive solutions and methodologies       Diagnosis of performance problems  SLO compliance  Transactional systems are very common architectures in web e commerce and in distributed database applications  such as desktop help or entitlement services   A common way to monitor the health of these systems is by checking the Average Response Time  ART  of the transactions  There are many reasons for this   a  it is a relatively easy metric to monitor given modern servers  just log the entry and exit times of the transaction  b  it is relatively easy to relate this time to some business objective of interest  and c  it is again a relatively easy metric to agree upon when different parts of the system belong to different organizations inside the same company  the front and middle tiers may belong to the IT department and the database may belong to a business division   Usually  the ART is part of a Service Level Objective  SLO  which in turn forms part of the contract between the different departments offering and receiving the service  sometimes even in the same organization   What is important to us is that a threshold t on the ART is agreed upon whereby if the ART   t  then the SLO is said to be violated  sometimes these violations are connected to penalties under contracts   It is important to be able to diagnose these violations in order to assign responsibilities  and to identify performance problems  misconfigurations  and faults  The complete solution to this problem is still open  In        we provided one possible building block towards this solution  Operators in one transactional system were also collecting a set of low level metrics per each server and machine instance pertaining to cpu  disk  and memory utilization  processes counters etc  There was a total of approximately    metrics per server  Operators wanted help in trying to determine which metrics were correlated to each particular instance of an SLO violation  By inducing models correlating SLO violations with the statistics of the metrics  we were able to induce a signature of each violation in terms of the metrics  Using these signatures we not only were able to highlight which metrics were in abnormal states  but we enabled the identification of recurrent SLO violations  and by using these signatures as indexes we enabled the cataloging of the different types of SLO violations for search and retrieval of successful repair actions  Once the signature was induced these tasks only required well known clustering and retrieval algorithms  Figure   shows an output of the graphical interface of the analysis system we built  which included a map of the different clustered signatures overlaid on the time series of the ART  It took    pages of email and three weeks for every competent operators to identify all the instances in cluster    Our procedures did it in a matter of minutes when used as a forensic tool  The approach we followed was that of inducing a Bayesian network classifier that used the low level metrics as features and the state of the SLO as a class variable  The advantage of using a classifier is that we had a very convincing metric to judge whether the model was indeed capturing the relationship between   GOLDSZMIDT the low level metrics and the SLO violations  classification accuracy  Also  the use of a classifier  as opposed to a regression model from the metrics to the actual ART  enabled us to use relatively simple  and robust  parameterizations for the models  In order to build the signatures  we use Bayes rule and computed the log likelihood of each metric  given its parents and the class   If the metric contributed to the classification of the SLO in the violated state  then the metric was deemed abnormal  The reader is directed to        for details  In order to increase the accuracy of the models to the low to mid   s we had to resort to the use of ensembles of classifiers  feature selection  and updates on the parameters to make sure that the classifiers were able to deal with the workload changes on the system  Feature selection implied a search over the space of models where it was really important to compare two models and determine whether the difference in classification accuracy was significant  The reason was that signatures may change drastically and appear different even if the underlying cause would be identical  Equally important was the establishment of a sound criteria to determine and select the models from the ensemble that were applicable during different time periods  These are of course realted to the two main issues discussed in Section    and the definite answers are still open       Model discovery  dependencies between computers and network services  Facilities such as remote file sharing and email invariably rely and depend on multiple network services ranging from directory services to authentication  These services are identified  to a first order of approximation  by the protocols used  e g  SMB  DNS  HTTP  LDAP etc  Regular management functions such as determining provenance  planning  detecting change  and diagnosis rely on knowledge of the dependencies between the hosts comprising the network and the network services  As the system grows and evolves  it is difficult to keep track and validate these dependencies  Thus the automatic discovery and verification of these dependencies is an important unsolved problem  In     we recently approached the problem of automatically discovering and maintaining a directed graph of these dependencies  where a node will represent a computer in the network  server  desktop etc   and an arc will represent a protocol service dependency  Figure   depicts an example of a fragment of such a graph  The data used to build that graph is from real network data obtained at Microsoft comprising headers and partial payloads of around    billion packets  The traces cover almost all the traffic transmitted and re        ceived by around     computers on the local network  The approach in     consists of two phases  The first phase discovers dependencies locally on each computer server and then a distributed algorithm builds the graph using these local dependencies  The graph s  can be maintained centrally or locally on each computer server  The approach is designed to be distributed  online  and therefore was constrained to only require the inspection of the packets headers  not the payload or content   The majority of issues of interest pertain to the local test  The first test we devised was based on comparing two cumulative distributions using a variant of the Kolmogorov Smirnov test       The main idea is as follows  We abstract the input and output packets grouping them by protocol service and source destination id  Each one of these groupings is called a channel  We want to find out whether a specific output channel B  say dns and destination y  depends on input channel A  say http and source x   To do this we fit a Cumulative Distribution Function  CDF  of the time between arrival and departure of the input output  and then compare it to the CDF of the time between the same input channel and a virtual output channel R whose departure times are uniform and random  If the two CDFs are different then we declare the channels to be dependent  their behavior differs from that between the input and a random output   The initial results are very promising and certainly enough to improve the state of the art for the network management tasks outlined above  Yet  the issues for hypothesis testing outlined in Section   are present and need to be dealt with  First  each server has on the order of hundreds inputs and hundreds of outputs  Therefore we must perform multiple hypothesis tests which leads to the necessity for some kind of FDR  In addition  the DNS server contains channels with tens of samples  and also with hundreds of samples  The HTTP server contains channels with tens of thousands of samples  We are currently systematically testing  validating and comparing our initial approach to a model selection approach  log likelihood  based on Bayesian considerations   In addition we are also testing and comparing FDR and an approach based on building more complex models between the inputs and outputs including Bayesian networks  Finally  there are many issues with building and maintaining the service dependency graphs  exploring the changes over time  and learning their normal and abnormal behavior for diagnosis purposes     This Bayesian test was mainly designed by C  Bishop and J  MacCormick    LDAP DNS KERBEROS LDAP SMB  prxy          prxy     dubitgspm    GOLDSZMIDT  HTTP                                DNS SMB           desktop  HTTP       HTTP  HTTP  HTTP  LDAP                                        webisaa   SMB                                    HTTP  HTTP  SMB           NBSS  anti virus              NBSS HTTP  HTTP  maditgddsa    Figure    Small fragment of a winserel   file srv    NBSS                            file srv     NBSS HTTP        built automatically by Constellation depicting the dependencies of a single computer in an enterprise network  The machine desktop  doubly circled in the figure  is the root of the constellation  The other nodes represent servers on which prxy    the root depends  either directly or indirectly   while the anti virus  edges reflect the corresponding services that cause depenprxy    dency        DNS  KERBEROS Netlogon SMB  SMB  SQL PROBE                                 state  If      in the Healthy any watchdog reports an error      for that computerdca   it is placed in theDNS Failure state and      MOM      assigned an appropriate recovery action from the set euro dns    LDAP              DNS SMB      above  The choice of recovery action depends on the RPC tknsmmom   recent repair history of the computer and the error SMB that is reported  We call this choice DHCP a policy   KERBEROS SMB  prxy     HTTP  cor p  dc     DNS  NBSS constellation NBSS  Policy optimization  keeping the infrastructure running  Microsoft has developed an in house infrastructure for automatic data center management called Autopilot       Its design was primarily motivated by the need to keep the total cost of a data center  including operational and capital expenses as low as possible  The first version of Autopilot concentrates on the basic services needed to keep a data center operational  provisioning and deployment  monitoring  and the hardware lifecycle including repair and replacement  In this section we concentrate on the automatic repair services of the hardware lifecycle  Autopilot supports a simple model for fault detection and recovery  It was designed to be as minimal as possible while still keeping a cluster operational  The unit of failure is defined to be a computer or network switch rather than a process  and the only available actions for repair are Reboot  ReImage  Replace  and DoNothing  Faults are detected using a set of  active  sensors called watchdogs  A watchdog probes one or more computers to test some attribute  and then reports to a part of the system called the Device Manager  The Device Manager computes an error predicate for any computer using the watchdog attributes  if any watchdog reports Error  the computer is in error  if all watchdogs report either OK or Warning the computer is not in error  Once a computer is held to be in error  it may be unavailable for a substantial period  A computer that is functioning normally is marked by the Device Manager as being  DNS MOM There is a logging service that records watchdog reDNS ports LDAP Device Manager assigned state  and repair accrbc m   v    tion  There are many important questions that can be potentially answered by mining these logs  How reliable are the different watchdogs  and can we estimate tkmomdb   and thencorp dc    minimize the false positives in watchdogs  Can we capture transient errors  Can we capture corcorp dns    related faults  Can we evaluate the effectiveness of the current policy and synthesize an optimal policy  Most of the procedures and algorithms to provide sound answers to these questions will require progress on the issues outlined in Section    Also  as these data centers contain tens of thousands of computers  and this number is growing   these algorithms and procedures need to scale in order to deal with the volume   Finally  it is enticing to think that as these logs contain the set of actions taken  plus evidence of their effects  it may provide a fertile ground for testing algorithms dealing with causal discovery and modeling           Final Remarks  The examples I present above clearly illustrate that UAI based techniques  methods  and algorithms  can be useful in automatically discovering correlations and dependencies in complex networked computer systems  that ultimately lead to better models and optimal decision making  The expected consequence of this is that the services we have come to rely upon will work optimally and uninterrupted   At the same time  I trust that the exposition above makes it clear that this domain also contains a set of challenges that will definitely advance the state of the art in UAI  However  my objective with this paper and the talk was not to present a list of research topics or what I think are future must do for UAI research  Rather  my intention has been to describe a fertile ground for research and application of UAI based techniques hoping to entice the imagination of colleagues in the field     It has been my experience in retreats and systems oriented forums that there is sometimes a myopic held belief that the only usefulness of statistical learning theory  as sometimes machine learning  belief network  and statistical induction research is grouped  is in automatically detecting abnormalities    GOLDSZMIDT  Acknowledgements Many thanks to research colleagues and collaborators in            In particular to I  Cohen and J  MacCormick for numerous discussions on these topics  Thanks to M  Isard for discussions on Autopilot and for the material in Section      K  Achan has spent a lot of time with me mining Autopilot logs        N  Friedman  M  Goldszmidt  and A  Wyner  On the application of the boostrap for computing confidence measures on features of induced Bayesian networks  In Proceedings of the International Conference on Uncertainty in Artificial Intelligence             N  Friedman and D  Koller  Being Bayesian about network structure  Machine Learning                     D  Heckerman  D  Geiger  and M  Chickering  Learning Bayesian networks  The combination of knowledge and statistical data  Machine Learning             
