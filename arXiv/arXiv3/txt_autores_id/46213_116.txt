 Qualitative possibilistic networks  also known as min based possibilistic networks  are important tools for handling uncertain information in the possibility theory framework  Despite their importance  only the junction tree adaptation has been proposed for exact reasoning with such networks  This paper explores alternative algorithms using compilation techniques  We first propose possibilistic adaptations of standard compilation based probabilistic methods  Then  we develop a new  purely possibilistic  method based on the transformation of the initial network into a possibilistic base  A comparative study shows that this latter performs better than the possibilistic adaptations of probabilistic methods  This result is also confirmed by experimental results      INTRODUCTION  In possibility theory there are two different ways to define the counterpart of Bayesian networks  This is due to the existence of two definitions of possibilistic conditioning  product based and min based conditioning  Dubois and Prade         When we use the product form of conditioning  we get a possibilistic network close to the probabilistic one sharing the same features and having the same theoretical and practical results  However  this is not the case with min based networks  In this paper  we are interested in the inference problem in multiply connected networks  which is known as a hard problem  Cooper         More precisely  we propose three compilation methods for min based possibilistic networks  The compilation of Bayesian networks is always considered as an important area  Recently  researchers  Salem Benferhat CRIL CNRS University of Artois France        benferhat cril univ artois fr  Rolf Haenni RISIS Bern University Switzerland  CH      rolf haenni bfh ch  have been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques  Darwiche         Chavira and Darwiche         Wachter and Haenni         etc  Despite the importance of possibility theory  there is no compilation that has been proposed for possibilistic networks  This paper analyzes this issue by first adapting well known compilation based probabilistic inference approaches  namely the arithmetic circuit method  Darwiche        and the logical compilation of Bayesian Networks  Wachter and Haenni         Both of them are based on a networks encoding into a logical representation and a compilation into a target compilation language  namely  DNNF  From there  all possible queries are answered in polynomial time  The third method exploits results obtained on one hand in  Benferhat et al         that transforms a minbased possibilistic network into a possibilistic knowledge base  and on the other hand results obtained regarding compilation of possibilistic bases  Benferhat et al         in order to assure inference in polytime  This method that is purely possibilistic is flexible since it permits to exploit efficiently all the existing propositional compilers  The rest of this paper is organized as follows  Section   gives a briefly background on possibility theory  possibilistic logic  possibilistic networks and introduces some compilation concepts  Section   is dedicated to possibilistic adaptations of compilation based probabilistic inference methods  Section   presents a new inference method in possibilistic networks using compiled possibilistic knowledge bases  Experimental study is presented in Section            BASIC CONCEPTS POSSIBILITY THEORY  This subsection briefly recalls some elements of possibility theory  for more details we refer to  Dubois and Prade         Let V    X    X         XN   be a set of   variables  We denote by DXi    x        xn   the domain associated with the variable Xi   By xi we denote any instance of Xi    denotes the universe of discourse  which is the Cartesian product of all variable domains in V   Each element    is called a state of   The notion of possibility distribution denoted by  is a mapping from the universe of discourse to the unit interval         To this scale  two interpretations can be attributed  a quantitative one when values have a real sense and a qualitative one when values reflect only an order between the different states of the world  This paper focuses on the qualitative interpretation of possibility theory  Given a possibility distribution   we can define a mapping grading the possibility measure of an event    by      max      has a dual measure which is the necessity measure N             Conditioning consists in modifying our initial knowledge  encoded by a possibility distribution   by the arrival of a new certain piece of information     The qualitative interpretation of the scale        leads to the well known definition of min conditioning  Hisdal          Dubois and Prade                if                         otherwise      POSSIBILISTIC LOGIC  Possibilistic logic  Dubois et al         handles qualitative uncertainty in a logical setting  A possibilistic logic formula is a pair  p  a  where p is a propositional formula and a its uncertainty degree which estimates to what extent it is certain that p is true  The higher is the weight  the more certain is the formula  A possibilistic knowledge base  is made up of a finite set of weighted formulas  i e        pi   ai    i          n        where ai is the lower bound on N  pi    Each possibilistic knowledge base induces a unique possibility distribution such that     and   pi   ai          if     pi              max  ai      pi   otherwise where    is propositional logic entailment       POSSIBILISTIC NETWORKS  A min based possibilistic network over a set of variables V   denoted by Gmin is composed of    a graphical component that is a DAG  Directed  Acyclic Graph  where nodes represent variables and edges encode the links between the variables  The parent set of a node Xi is denoted by Ui    Ui    Ui         Uim    For any ui of Ui we have ui    ui    ui         uim   where m is the number of parents of Xi   In what follows  we use xi   ui   uij to denote  respectively  possible instances of Xi   Ui and Uij     a numerical component that quantifies different links  For every root node Xi  Ui      uncertainty is represented by the a priori possibility degree  xi   of each instance xi  DXi   such that maxxi  xi        For the rest of the nodes  Ui      uncertainty is represented by the conditional possibility degree  xi  ui   of each instances xi  DXi and ui  DUi   These conditional distributions satisfy the following normalization condition  maxxi  xi  ui        for any ui   The set of a priori and conditional possibility degrees in a min based possibilistic network induce a unique joint possibility distribution defined by the following chain rule  min  X        XN     min  i    N        Xi   Ui         COMPILATION CONCEPTS  A target compilation language is a class of formulas which is tractable for a set of transformations and queries  Compilation languages are compared in terms of their spatial efficiency via the succinctness criteria and also in terms of the set of logical queries and transformations they support in polynomial time  see  Darwiche and Marquis        for more details   Within the most effective target compilation languages  we cite the Decomposable Negation Normal Form  DNNF   Darwiche         This language is universal and presents a number of properties  determinism  smoothness  etc   that makes it of a great interest  It supports a rich set of polynomial time logical operations  To define DNNF  the starting point is Negation Normal Form  NNF  which is a set of propositional formulas where possible connectives are conjunctions  disjunctions and negations  A set of important properties may be imposed to NNF  such that    Decomposability  the conjuncts of any conjunction in NNF do not share variables    Determinism  two disjuncts of any disjunction in NNF are logically contradictory    Smoothness  the disjunct of any disjunction in NNF mentions the same variables  These properties lead to a number of interesting subsets of NNF  Within these subsets  the language DNNF  Darwiche        is one of the most effective target compilation languages that supports the decomposability  We can also mention  the d DNNF sat    isfying determinism  sd DNNF satisfying smoothness and determinism  etc  Each compilation language supports some queries and transformations in polynomial time  In what follows we are in particular interested by conditioning and forgetting transformations  Darwiche and Marquis           and  as max and min operators  respectively   A sentence in  sd DNNF is a sentence in  DNNF satisfying decomposability  determinism and smoothness      In  Darwiche         authors have focused on inference in compiled Bayesian networks  The main idea is based on representing the network using a polynomial and then retrieving answers to probabilistic queries by evaluating and differentiating the polynomial  This latter itself is exponential in size  so it has been represented efficiently using an arithmetic circuit that can be evaluated and differentiated in time and space linear in the circuit size  In what follows  we propose a direct adaptation of this method in the possibilistic setting  Given a min based possibilistic network  we first encode it using a possibilistic function fmin defined by two types of variables   POSSIBILISTIC ADAPTATIONS OF COMPILATION BASED PROBABILISTIC INFERENCE METHODS  There are several compilation methods which handle the inference problem in probabilistic graphical models  In this section  we first propose an adaptation of the arithmetic circuit method of  Darwiche         Then we will study one of its variants proposed in  Wachter and Haenni         namely the logical compilation of Bayesian Networks  DNNF has been introduced for propositional language  Recall that in qualitative possibility theory  we basically manipulate two main operators Max and Min  These operators fully make sense when we deal with qualitative plausibility ordering  Therefore  we propose to define concepts of  DNNF  resp   d DNNF   sd DNNF  as adaptations of the DNNF language  resp  d DNNF  sd DNNF   Darwiche        in the possibilistic setting  definition     Definition    A sentence in  DNNF is a rooted DAG where each leaf node is labeled with true  false or variables instances and each internal node is labeled with max or min operators and can have arbitrarily several children  Roughly speaking   DNNF is the same as the classical DNNF although its operators are max and min instead of  and   respectively  Example    Figure   depicts a sentence in  DNNF  Consider the Min node  root  in this figure  This node has two children  the first contains variables A  B while the second contains variables C  D  This node is decomposable since its two children do not share variables        INFERENCE USING POSSIBILISTIC CIRCUITS   Evidence indicators  for each variable Xi in the network   we have a variable xi for each instance xi  DXi    Network parameters  for each variable Xi and its parents Ui in the network  we have a variable xi  ui for each instance xi  DXi and ui  DUi   fmin   max x  min  xi  ui  x  xi xi  ui       where x represents instantiations of all network variables and ui  x denotes the compatibility relationship among ui and x  The possibilistic function fmin of a possibilistic network represents the possibility distribution and allows to compute possibility degrees of variables of interest  Namely  for any piece of evidence e which is an instantiation of some variables E in the network  we can instantiate fmin as it returns the possibility of e   e   Definition   and Proposition     Definition    The value of the possibilistic function fmin at evidence e  denoted by fmin  e   is the result of replacing each evidence indicator xi in fmin with   if xi is consistent with e  and with   otherwise  Proposition    Let Gmin be a possibilistic network representing the possibility distribution  and having the possibilistic function fmin   For any evidence e  we have fmin  e     e    Figure    A sentence in  DNNF   A sentence in  d DNNF is a sentence in  DNNF satisfying decomposability and determinism  viewing  Let figure   be the min based possibilistic network used throughout the paper  The possibilistic function of the network in figure   has   terms corresponding to the   instantiations of variables F  B  D  Two of these terms are as follows    is outlined by algorithm    Note that the suffix P F is added to signify that this method uses a possibilistic function  fmin   before ensuring the CNF encoding  Algorithm    Inference using  DNNF   DNNFP F    Figure    Example of Gmin    fmin   max min d    f    b    d   f   b    f    b     min  d    f    b    d   f   b    f    b          If the evidence e    d    b    then fmin  d    b    is obtained by applying the following substitutions to fmin   d       d       b       b       f    f       This leads to  e         The possibilistic function fmin is then encoded on a propositional theory  CNF  using xi and xi  ui   For each network variable Xi   the encoding contains the following clauses  xi  xj     xi  xj   i    j       Moreover  for each propositional variable xi  ui   the encoding contains the clause  xi  ui          uim  xi  ui       The CNF encoding  denoted by Kfmin recovers the min joint possibility distribution  proposition     Proposition    The CNF encoding Kfmin of a possibilistic network encodes the joint distribution of given network  Once the CNF encoding is accomplished  it is then compiled into a  DNNF  from which we extract the possibilistic circuit p  definition    that implements the encoded fmin   Definition    A possibilistic circuit p encoded by a  DNNF sentence  c is a DAG in which leaf nodes correspond to circuit inputs  internal nodes correspond to max and min operators  and the root corresponds to the circuit output  As in the probabilistic case  Darwiche         this circuit can be used for linear time inference  More precisely  computing the possibility degree of an event consists on evaluating p by setting each evidence indicator x to   if the event is consistent with x  to   otherwise and applying operators in a bottom up way  This possibility degree corresponds exactly to the one computed from the min joint possibility distribution  proposition     This method referred to  DNNFP F  Data  Gmin   instance of interest x  evidence e Result   x e  begin Compilation into  DNNF Encode Gmin into fmin using equation   EncodeCNF of Gmin into  using equations         Compile  into  c p  Possibilistic Circuit of  c Inference Applying Operators on p  x  e   Root Value  p    x e    e   Root Value  p   e  if  x  e    e  then  x e    x  e  else  x e     return  x e  end  Proposition    Let Gmin be a possibilistic network  Let min be a joint distribution obtained by chain rule  Then for any a  Da and e  DE   we have  A   a E   e    min  A   a E   e  where min  A   a E   e  is obtained from min using equation   and  A   a E   e  is obtained from algorithm    The key point to observe here is that this approach can handle possibilistic circuits of manageable size as in the probabilistic case since some possibility values may have some specific values  for instance  whether they are equal to   or    and whether some possibilities are equal  In this case  we can say that the network exhibit some local structure  By exploiting it  the produced circuits can be smaller  In fact  the normalization constraint relative to the initial network will mean that we will have several values equal to    Thus the idea is to make an advantage from such a local structure which has a particular behavior with the max operator in order to construct more compact possibilistic circuits w r t  standard ones as stated by the following proposition  Proposition    Let N bposs and N bproba be the number of clauses in the possibilistic and probabilistic cases  respectively  Then N bposs  N bproba   Note that for particular situations where probability values are   or    we have N bposs   N bproba   otherwise N bposs  N bproba   Example    To illustrate algorithm   we will consider the min based possibilistic network represented in figure    We are looking for  f   d    with f  as instance of interest and d  as evidence  First  we encode the network as a possibilistic function and encode it on CNF  This latter is then compiled into  DNNF from which a possibilistic circuit is extracted  The possibility degree  f   d    is computed using this circuit in polynomial time  For instance   f    d    is computed using p by just replacing   f    d    b    b      and applying possibilistic operators in a bottom up way as shown in figure    Hence   f   d       f    d          since  f    d               from a function f encoding the CNF  Then  we have min  xi        xj      xi        xj    i e  f recovers the min joint possibility distribution min   Comparing theoretically the probabilistic and the possibilistic case allows us to deduce the following proposition  Proposition    The possibilistic encoding of a possibilistic network given by K  equation     is more compact than the probabilistic encoding given in  Wachter and Haenni         In fact  the number of variables used in K is less than the one used in  Wachter and Haenni         In particular for parameters  our approach uses one variable per different weight  while in the probabilistic encoding one variable per parameter  For each clause in K there exists a clause of the same size in the probabilistic encoding  The converse is false   Figure    Inference using the possibilistic circuit  p          INFERENCE USING POSSIBILISTIC COMPILED REPRESENTATIONS  DNNF plays an interesting role in compiling propositional knowledge bases  It has been used to compile probabilistic networks  More precisely in  Wachter and Haenni         authors have been interested in performing a CNF logical encoding of the probability distribution induced by a bayesian network  then a compilation phase from CNF to d DNNF  In this section  we propose to adapt this encoding in the possibilistic setting by taking into consideration the local structure aspect  This allows to reduce the number of additional variables comparing to the probabilistic encoding  Let  be propositions linked to networks variables and let  be propositions linked to the possibility distribution entries  equal to     We start by looking at the possibility distribution encoding  The logical representation of a network variable Xi is defined by  Xi            ui       uim  xi  ui  xi     ui  xi  ui X  i   ui  By taking the conjunction of all logical representations of variables  we obtain the networks representation  as follows      Xi      Xi   The CNF encoding  denoted by K indeed recovers the min joint possibility distribution  proposition     Proposition    Let min be the joint possibility distribution obtained using the chain rule with the minimum operator and  be the possibility degree computed  Once the qualitative network is encoded by K   it is compiled into a compilation language that supports the transformations conditioning and forgetting and the query possibilistic computation  This language is  DNNF  proposition     Therefore  the CNF encoding is first compiled  and the resulting  DNNF is then used to compute efficiently  i e  in polynomial time a posteriori possibility degrees  proposition     This method referred to  DNNF is outlined by algo     Proposition     DNNF supports conditioning  forgetting and possibilistic computation  Algorithm    Inference using  DNNF Data  Gmin   instance of interest x  evidence e Result   x e  begin Compilation into  DNNF EncodeCNF of Gmin into  using equation    Compile  into pc Inference v   Explore  DNNF x  e  pc   v   Explore  DNNF e  pc   if v   v  then  x e   v  else  x e     return  x e  end  Proposition    Let Gmin be a possibilistic network  Let min be a joint distribution obtained by chain rule  Then for any a  Da and e  DE   we have  A   a E   e    min  A   a E   e  where min  A   a E   e  is obtained from min using equation   and  A   a E   e  is obtained from algorithm    Example    Let us illustrate algorithm    In fact   of the network of figure   is      F  B  D        f         b      f   b      d      f   b      d      f   b      d      f   b      d     such as           and   correspond respectively to               and      To compute  f   d     we should first compute  f    d    using algorithm    The first step is to check if we have at least   Algorithm    Explore  DNNF Data  a set of instances x  compiled representation pc Result   x  begin if  xi  x  xi  Ui is not a leaf node then  x     else y   xi     xi  Ui is a leaf node  Ui  x  c p y  Condition pc on y c pc  y  Forget  from p y c Applying Operators on p  y  x   Root Value of pc  y return  x  end  one  as a leaf node  In this example  we have d   f   b  and d   f   b  as leaf nodes  hence conditioning should be performed  Then  a computation step is required by applying in a bottom up way Min and Max operators on the forgotten  DNNF  Therefore   f   d       f    d               NEW POSSIBILISTIC INFERENCE ALGORITHM  In  Benferhat et al          authors have been interested in the transition of possibilistic networks into possibilistic logic bases  The starting point is that the possibilistic base associated to a possibilistic network is the result of the fusion of elementary bases  Definition   presents the transformation of a min based possibilistic network into a possibilistic knowledge base  Definition    A binary variable Xi of a possibilistic network can be expressed by a local possibilistic knowledge base as follows  Xi     xi  ui   i     i       xi  ui          The possibilistic knowledge base of the whole network is  min   X   X       Xn   In another angle  researchers in  Benferhat et al         have focused on the compilation of bases under the possibilistic logic policy in order to be able to process inference from it in a polynomial time  The combination of these methods allows us to propose a new alternative approach to possibilistic inference  This is justified by the fact that the possibilistic logic reasoning machinery can be applied to directed possibilistic networks  Benferhat et al          The idea is to encode the possibilistic knowledge base min into a classical propositional base  CNF   Let A    a         an   with a          an the different weights used in min   A set of additional propositional variables  denoted by Ai   which correspond exactly to the number of different weights  are incorporated and for each formula i   ai will correspond the propositional formula i Ai   Hence  the propositional  encoding of min   denoted by K is defined by  K    i  Ai    i   ai    min          The following proposition shows that the CNF encoding K recovers the min joint possibility distribution  Proposition    Let min be the joint possibility distribution obtained using the chain rule with the minimum based conditioning and let K be the propositional base associated with the possibilistic network given by equation     Let i be a propositional formula associated with a degree ai   Then            iff  A         An      K is consistent       ai iff  A         Ai      K is inconsistent and  A         Ai       K is consistent  The CNF encoding K is then compiled into a target compilation language in order to compute a posteriori possibility degrees in an efficient way  Here  we are interested in a particular query useful for possibilistic networks  namely what is the possibility degree of an event A   a given an evidence E   e  Therefore  we propose to adapt the algorithm given in  Benferhat et al         in order to respond to this query as shown by algorithm    Proposition    shows that the possibility degree computed using algorithm   and the one computed using the min based joint possibility distribution are equal  Note that this approach is qualified to be flexible since it takes advantage of existing propositional knowledge bases compilation methods  Benferhat et al          This method referred to DNNF PKB is outlined by algorithm    Algorithm    Inference using DNNF Data  Gmin   instance of interest x  evidence e Result   x e  begin Transformation into K Transform Gmin into min using definition   Transform min into K using equation    Inference c K  T arget K   c K  K StopCompute  false i   x e     while  K   Ai  e  and  i  k  and  StopCompute false  do K  condition  K  Ai   if K  x then StopCompute true  x e     degree i  else ii   return  x e  end  Proposition     Let Gmin be a possibilistic network  Let min be a joint distribution obtained by   chain rule  Then for any a  Da and e  DE   we have  A   a E   e    min  A   a E   e  where min  A   a E   e  is obtained from min using equation   and  A   a E   e  is obtained from Algo      Indeed  in  DNNFP F   we associate propositional variables not only to possibility degrees  parameters   but also to each value xi of Xi   While in DNNF PKB only m new variables are added  one variable per different degree    Example    To illustrate algorithm   we will consider  Let us now analyze these three approaches from experimental points of view  Our experimentation is performed on random possibilistic networks  More precisely  we have compared DNNF PKB and DNNFP F on     possibilistic networks having from    to    nodes  As mentioned that the approaches focus mainly on encoding the possibilistic network as a CNF then compile it into the appropriate language  hence  it should be interesting to compare the CNF parameters  the number of variables and clauses  and the DNNF parameters  the number of nodes and edges  for the two methods   the min based possibilistic network represented in figure    The CNF encoding is as follows   K    d   f   b   A       b   A       d   f   b   A       f   A       d   f   b   A       d   f   b   A    such as A         A         A        and A        are propositional variables followed by their weights under c brackets  Compiling K into DNNF results in  K     b   A       A   f      f    d    A   d          b     f    d    A   d         f    A      d    A   d         c The computation of  f   d    using K requires two iterations  Therefore   f   d         degree            Due to the compilation step  this algorithm runs in polynomial time  Moreover  the number of additional variables is low since it corresponds exactly to the number of priority levels existing in the base      COMPARATIVE AND EXPERIMENTAL STUDIES  The paper analyzes three compilation based methods  namely DNNF PKB   DNNF and  DNNFP F   The first dimension that differentiates the three approaches proposed in this paper is the CNF encoding  It consists of specifying the number of variables and clauses per approach  The CNF of DNNF PKB is based on encoding x where x is an instance of interest having a possibility degree different from    In  DNNF  we write implications relative to instances having   as possibility degree  We can notice that the local structure in both methods is exploited in semantically different ways  In DNNF PKB  the encoding uses the number of different weights as the number of additional variables while the  DNNF encoding uses the number of the non redundant possibility degrees different from   in the distributions  Regarding the number of clauses  both methods handle possibility degrees different from    This leads us to the following proposition        CNF PARAMETERS  First we propose to test the CNF encodings characterized by the number of variables and the number of clauses  Regarding DNNF PKB  the number of additional variables correspond to the number of weights which are different  While in  DNNFP F   variables are both those associated to the possibility degrees of each distribution and those to variables instances  The number of clauses for each method is related to the CNF encoding itself  Figure   shows the results of this experimentation  Each approach is characterized by a curve for the average number of variables and a curve for the average number of clauses  It is clear that the higher the number of nodes considered in the possibilistic network  the higher the number of variables and clauses  Figure   shows that DNNF PKB has the lower number of variables and clauses comparing to  DNNFP F   which confirms the theoretical results detailed above   Proposition     The CNF encodings of DNNF PKB and  DNNF have the same number of variables and clauses  The CNF encoding of  DNNFP F is different from the ones of DNNF PKB and  DNNF  Proposition    shows the difference between  DNNFP F and DNNFPKB in terms of number of variables and clauses  Proposition     The number of variables and clauses in  DNNFP F is more important than those in DNNF PKB   Figure    CNF parameters         DNNF PARAMETERS  Once we obtain the CNF encodings  it is important to compare the number of nodes and edges for each compiled base  Figure   represents the average size of the compiled bases for the two methods in terms of nodes and edges numbers  We remark that the number of nodes and edges depends deeply on CNF parameters  More precisely  the number of nodes and edges in DNNF PKB is considered narrow comparing to  DNNFP F   This can be explained by the lower number of variables and clauses on CNFs and the local structure which shrinks the sizes of compiled bases  Comparing DNNF PKB to  DNNFP F   the behavior of DNNF PKB is important    Pearl         Benferhat and Smaoui         Acknowledgements We thank the anonymous reviewers for many interesting comments and suggestions  Also  we wish to thank Mark Chavira for our valuable discussions on this subject  The third author would like to thank the project ANR Placid   
