ions  Alejandro Isaza and Csaba Szepesvari and Vadim Bulitko and Russell Greiner Department of Computing Science  University of Alberta Edmonton  Alberta  T G  E   CANADA  isaza szepesva bulitko greiner  cs ualberta ca  Abstract In this paper  we consider planning in stochastic shortest path  SSP  problems  a subclass of Markov Decision Problems  MDP   We focus on medium size problems whose state space can be fully enumerated  This problem has numerous important applications  such as navigation and planning under uncertainty  We propose a new approach for constructing a multi level hierarchy of progressively simpler abstractions of the original problem  Once computed  the hierarchy can be used to speed up planning by first finding a policy for the most abstract level and then recursively refining it into a solution to the original problem  This approach is fully automated and delivers a speed up of two orders of magnitude over a state of the art MDP solver on sample problems while returning near optimal solutions  We also prove theoretical bounds on the loss of solution optimality resulting from the use of abstractions      Introduction and Motivation  We focus on planning in stochastic shortest path problems  the problem of reaching some goal state under uncertainty  when planning time is critical  a situation that arises  for instance  in path planning for agents in commercial video games  where map congestions are modeled as uncertainty of transitions  Another example is path planning for multi link robotic manipulators  where the uncertainty comes from unmodeled dynamics as well as sensor and actuator noise  More specifically  we consider the problem of finding optimal policies in a sequence of stochastic shortest path problems  Bertsekas   Tsitsiklis         where the problems share the same dynamics and transition costs  and differ only in the location of the goal state  When the state space underlying the problems is sufficiently large  exact planning methods are unable to deliver a solution within the required time  forcing the user to resort to approximate methods in order to scale to large domains  Exploiting the fact that multiple planning  problems share the same dynamics and transition costs  we build an abstracted representation of the shared structure where planning is faster  then map the individual planning problem into the abstract space and derive a solution there  The solution is then refined back into the original space  In a related problem of path planning under real time constraints in deterministic environments  e g   Sturtevant         a particularly successful approach is implemented in the PR LRTS algorithm  Bulitko  Sturtevant  Lu    Yau         which builds an abstract state space by partitioning the set of states into cliques  i e   each state within each cluster is connected to each other state in that cluster with a single action   Each such cluster becomes a single abstract state  Two abstract states are connected by an abstract transition if there is a pair of non abstract states  one from each abstract state  connected by a single action  The resulting abstract space is smaller and simpler  yet captures some of the structure of the original search problem  Thus  an abstract solution can be used to guide and constrain the search in the original problem  yielding a significant speed up  Further speed ups can be obtained by building abstractions on top of abstractions  which creates a hierarchy of progressively smaller abstract search spaces  PR LRTS can then be tuned to meet strict real time constraints while minimizing solution suboptimality  Note that state cliques produced by PR LRTS make good abstract states because landing anywhere in such a cluster puts the agent a single action away from any other state in the clique  This also means that the costs of the resulting actions are similar  and that the cost of a single action is negligible compared with the cost of a typical path  Finally  any  optimal  path in the original problem can be closely approximated at the abstract level  as an agent following an  optimal  path has to traverse from cluster to cluster  Since all neighboring clusters are connected in the abstract problem  it is always possible to find a path in the abstract problem that is close to the original path  Given the attractive properties or PR LRTS  it is natural to ask whether the ideas underlying it can be extended to stochastic shortest path problems  with arbitrary cost structures  In a stochastic problem  the result of planning is a closed loop policy that assigns actions to states  A successful ab    straction must be suitable for approximating the execution trace of an optimal policy  Imagine that clustering has been done in some way  The idea is again to have abstract actions that connect neighboring clusters cheaply  that is  the system should not produce expensive connections  Intuitively  we want to connect one cluster to another if  from any state of the first cluster  we can reliably get to some state of the second cluster at roughly a fixed cost  the same for any state in the first cluster   This way  simulating a policy of the original problem becomes possible at a small additional cost  the meaning of simulation will become clear later   This means that a connection between clusters is implemented by a policy with a specific set of initial states that brings the agent from any state of the source cluster to some state of the target cluster  We will use options  Sutton  Precup    Singh        for such policies  and choose clusters to allow such policies for any two neighboring clusters  Thus  it is natural to look for clusters of states that allow one to reliably simulate any trajectory from any of the states to any other state  Finally  we need an extra mechanism  the goal approach  that deals with the challenge of reaching the base level goal itself from states that are close to the goal  Thus  our planner first plans in the abstract space to reach the goal cluster  After arriving at some state of the goal approach region  the planner then uses the goal approach policy that  with high probability  moves the agent to the goal state itself  These ideas form the core of our algorithm   The MDP is undiscounted if       An action a  xX A x  is called admissible in state x if a  A x   Definition   A  generic  policy is a mapping that assigns to each history  x    a    c            xt    at    ct    xt   an action admissible in the most recent state xt   In general  a mapping that maps possible histories to some set is called a history dependent mapping  Under mild conditions  it suffices to consider only stationary  deterministic policies  Bertsekas   Tsitsiklis         on which we will focus  Definition   A stationary and deterministic policy  is a mapping of states to actions such that  x   A x  holds for any state x  X  In what follows  we will use policy to mean stationary and deterministic policies  unless otherwise mentioned  The expected cost of policyP when the system starts  in state x  is v  x      E   t    t c Xt    Xt    Xt      where Xt is a Markov chain with P  Xt     y Xt   x    p y x   x    The function v is called the value function underlying policy   One solves an MDP by finding a policy that minimizes the cost from every state  simultaneously  In this paper we deal only with stochastic shortest path problems  a subclass of MDPs  In these MDPs the problem is to get to a goal state with the least cost   The three major contributions of the paper are   i  a novel theoretical analysis of option based abstractions   ii  an effective algorithm for constructing high quality option based abstractions  and  iii  experimental results demonstrating that our algorithm performs effectively over a range of problems of varying size and difficulty   Definition   A finite stochastic shortest path  SSP  problem is a finite undiscounted MDP that has a special state  called the goal state g  such that a  A g   we have p g g  a      and c g  a  g      and the immediate costs for all the other transitions are positive   Section   formally describes our problem  and provides the theoretical underpinning of our approach  Section   then presents our algorithm for automatically building options based abstractions  and Section    our planning algorithm that uses these abstractions  Section   empirically evaluates this approach  in terms of both efficiency and effectiveness  suboptimality   Finally  Section   summarizes related work   Consider a finite SSP  X  A  p  c   Let  be a stationary policy  We say that this policy is proper if it reaches the goal state g with probability one  regardless of the initial state  Let T   RX  RX be the policys evaluation operator  X  T v  x    p y x   x    c x   x   y    v y     yX     Problem Formulation and Theory  This section formally defines stochastic shortest path problems and the abstractions that we will consider  It also presents a theoretical result that characterizes the relationship between the performance of abstract policies and policies of the original problem  Definition   A Markov Decision Process  MDP  is defined by a finite state space X               n   a finite set of actions A x  for each state x  X  transition probabilities p y x  a          that correspond to the probability that the next state is y when action a is applied in state x  immediate cost c x  a  y   R for all x  y  X and all a  A x  and a discount factor            Bertsekas and Tsitsiklis        prove that T is a contraction with respect to a weighted maximum norm  kkw    with some positive weights  w  RX     where kvkw    maxx  v x   w x   In particular  w x  can be chosen to be the expected number of steps until  reaches the goal state when started from x  The contraction coefficient of T      satisfies            maxxX w x   Thus           is the maximum of the expected number of steps to reach the goal state  or in other words  the maximum expected time policy  spends in the MDP  cf  Prop     in Bertsekas   Tsitsiklis         We adopt the notion of options from Sutton et al          Definition   An option is a triple    I     where I  X is the set of initial states   is a  generic  policy that is   defined for histories that start with a state in I and  is a history dependent mapping with range         called the terminating condition  We say that the terminating condition fires when  ht        Let T     denote the random time when the terminating condition fires for the first time while following    Note that T     is not allowed   We assume that P  T           independent of the initial state when the policy  is started  i e   the option terminates in finite time with probability one   As suggested in the introduction  an abstraction is a way to group states and the abstract actions correspond to options  Definition   We say that the MDP  X   A  p  c  is an option based abstraction of  X  A  p  c   if there exists a mapping  S   X   X specifying the states S x   X that correspond to an abstract state x  X   a set  of options abstracting the actions of the MDP and a mapping    xX A x    such that for any a  A x   if  a     I      then S x   I   Henceforth we will use abstraction instead of optionbased abstraction and will call  X   A  p  c  the abstract MDP  X the set of abstract states  A the set of abstract actions  etc  Notationally  we call  X  A  p  c  the ground level MDP  and we will identify quantities related to the abstract MDP by using a tilde        For simplicity  we will identify the abstract actions with their corresponding options  In particular  we will call a both an abstract action and an option  depending on the context  In the following  we will assume that  S x    x  X   is a partition of X  we can then let x   X  X denote the  unique  abstract state that includes x  x x   X such that x  S x x    and say that  X   S  is an aggregation of the states in X  We also define S x    S x x   as the set of states in X that are in the same partition with x  The restriction on  in the above definition ensures that the execution of any policy  in the abstract MDP is well defined and proceeds as follows  Initially  there is no active option  In general  whenever there is no active option  we look up the abstract state x   x x  based on the current state x and activate the option   x    When there is an active option  the option remains active until the corresponding terminating condition fires  When an option is active  the options policy selects the actions in the ground level MDP  This way a policy  in the abstract MDP induces a policy in the ground level MDP  Our goal now is to characterize what makes an abstraction accurate  The following theoretical analysis is novel as it considers abstractions where the action set is changed  In particular  the action set can potentially be reduced and the abstract actions can be options  To our knowledge  such options based abstractions have not been analyzed previously  the closest results are probably Theorem   of Kim and Dean        and Theorem   of Dean  Givan  and Leach         The proof is rather technical and is given   X     denotes the power set of X  the set of all subsets of X   in the extended version of our paper  Isaza  Szepesvari  Bulitko    Greiner         Consider a proper policy  of the ground level MDP  We want abstractions such that one can always find a policy in the abstract MDP  X   A  p  c  that approximates  well  no matter how  was chosen  Clearly  this depends on how the action set A and the corresponding transitions and costs are defined in the abstract MDP  Quantifying this requires a few definitions  Let p  x  y  be the probability of landing in some state of S y  when following policy  until it leaves the states of S x   when the initial state is selected at random from the states of S x  based on the distribution S x    Let c  x  denote the corresponding expected immediate cost  Now pick a proper policy  of the abstract MDP  Let w be the weight vector that makes T a contraction in the abstract MDP  P Further  define p  x  y    p y x   x   and c  x    yX p  x  y c  x  y  and the mixed l   l norm kkw      kp   p  kw      max xX  X  yX   p   x  y   p   x  y    w y    w x   Let     kc  c kw    cmax kp  p kw            where cmax is the maximum of the immediate costs in the ground level MDP  Hence    measures how well the costs and the transition probabilities induced by  after state aggregation match those of   Introduce c x    as the expected total cost incurred  conditioned on that policy  starting in state x and stopping when it exits S x   Further  introduce p y x    as the probability that  given that policy  is started in state x  when it exits S x  it enters S y   y    x x    Now fix an abstract state x  X   If the costs  c x    xS x  and probabilities  p y x    xS x    y    x  have a small range then we can model closely the behavior of  locally at S x  by introducing an option with initial states in S x  which mimics the expected behavior of  as it leaves S x   assuming  say  that the initial state in S x  is selected at random according to the distribution S x    If we do so for all abstract states x  X then we can make sure that min   is small  If the above range conditions hold for all policies  of the ground level MDP and all abstract states x  X then by introducing a sufficiently large number of abstract actions it is possible to keep max min   small  Further  notice that max p y x    is zero unless there exists a transition from some state of S x  to some state of S y   in which case we say that S x  is connected to S y   Hence  no abstract action is needed between x and y  unless S x  is connected in the ground level MDP to S y   Define TP   B X   B X    T v  x     c  x    p Since  is proper in the y   x  y v y   ground level MDP  it is not difficult to show that T is a contraction with respect to an appropriately defined weighted supremum norm  The next result gives a bound on the difference of value functions of  and  in terms of       Theorem   Let  be a proper policy in the ground level MDP and let  be a proper policy in the abstract MDP  Let w  resp   w   be the weight vector that makes T  resp   T   a contraction and let the corresponding contraction factor be   resp       Let v be the value function of  and v be the value function of   Then kv  Ev kw    kAv  v kw                    where the operator E extends functions defined over X to functions defined over X in a piecewise constant manner  E   B X    B X    Ev  x    v x x    and A   B X   B X  is the aggregation operator defined by X  AV   x    S x   z V  z   zS x   and    maxxX w  x x   w  x   The factor  measures how many more steps are needed to reach the goal if the execution of policy  is modified such that  whenever the policy enters a new cluster x  the state gets perturbed  by choosing a random state according to S x    The theorem provides a bound on the difference between the value function of a ground level policy  and the value function of an abstract policy when its value function is extended to the ground level states  The bound has two terms  The first bounds the loss due to state abstraction  while the second bounds the loss due to action abstraction  When a similar range condition holds for the abstract actions  too  then it is possible to bound the difference between the value function of the policy induced in the ground level MDP by  and Ev   yielding a difference on the value functions of  and the policy induced by   Isaza et al         provides further details  If we apply this result to an optimal policy   of the ground level MDP  we immediately get a bound on the quality of the abstraction  We may conclude then that the quality of abstraction is determined by the following factors   i  whether states with different optimal values are aggregated   ii  whether the random perturbation described in the previous paragraph can increase the number of steps to the goal substantially  and  iii  whether the immediate costs c and transition probabilities p can be matched in the abstract MDP  Since we want to build abstractions that work independently of where the goal is placed  the knowledge of the optimal policy with respect to a particular goal cannot be exploited when constructing the abstractions  In order to prevent large errors due to  i  and  ii   we restrict aggregation such that only a few states are grouped together  This makes the job of creating an aggregation easier  Fortunately  we can achieve higher compression by adding additional layers of abstractions  We can address  iii  by creating a sufficiently large number of abstract actions  Here  we use the simplifying assumption that we only create abstract actions that bring the agent from some cluster of states to some neighboring cluster  These can  serve as a basis for matching any complex next state distribution over the clusters by choosing an appropriate stochastic policy in the abstract MDP  We also want to ensure that the initial state within a cluster has a small influence on the probability of transitioning to some neighboring cluster and the associated costs  We use two constants   and   to bound the amount of variation with respect to initial states  note this allows us to control the difference between the value function of a policy induced in the ground level MDP by some abstract policy  and the extension of the value function of  defined in the abstract MDP to the ground level states  Ev   This is necessary to ensure that a good policy in the abstract MDP produces a good policy in the ground level MDP  ultimately assuring that the optimal policy of the abstract MDP will give rise to a close to optimal policy in the ground level MDP  The resulting procedure is described in the next section      Abstracting an SSP  This section describes our algorithm BuildAbstraction for automatically building options based abstractions  These abstractions are goal independent and thus apply to a series of SSPs that share the state space and transition dynamics  The process consists of four main steps  Figure         Cluster proposes candidates for abstract states      GenerateLinkCandidates proposes candidates for abstract actions  or links       Repair validates and  if necessary  repairs the links in order to satisfy the so called      connectivity property  the formal definition is given later  and Prune discards excessive links  Once an abstraction is built  we use a special purpose planning procedure  described in Section    to solve specific SSPs  The rest of this section describes the four steps of our BuildAbstraction algorithm in detail  Step    Cluster  A straightforward cluster er will cluster a state with some of its immediate neighbors  Unfortunately  this approach may group states with diverging trajectories  the trajectories from one state can differ from those of the other state   By looking for the peers of a state  predecessors of its successors  line    Figure    we hope to find a peer whose trajectories are similar to the trajectories of the first state  Note that the clustering routine creates minimal clusters  This is advantageous as it means the subsequent steps  which connect clusters  is more likely to succeed  Unfortunately  it also means relatively low reduction in the number of states  Several layers of abstractions can help increase this reduction  Step    Generate Link Candidates  After forming the initial clusters  i e   the initial abstract states   BuildAbstraction generates candidates for abstract actions  One approach is simply to propose abstract actions for all pairs of abstract states  in the hope that only important ones will remain after pruning  We use a less expensive strategy and propose abstract action candidates only for nearby clusters  line     For each such pair we add two candidate links  one in the forward and another in the backward direction  this heuristic quickly generates reasonable link candidates  We typically use k      Our experiments confirm this is   BuildAbstraction k  p  M      M  ground level MDP Cluster   for each unmarked ground state x do   Find P  x   all the predecessors of successors of x   Find y  P  x  that has the most successors in common with x   Add x to X with S x     x  y    Mark states  x  y    end for GenerateLinkCandidates   repairQ     for every x  y  X  where any state in S y  is within k ground transitions of some state in S x  do   repairQ  repairQ    x  y    y  x      end for Repair    while repairQ     do     x  y   pop an element from repairQ    set up an SSP  S  with domain R  X where S x S y   R with states in S y  as goals    attempt to find an optimal policy S in S with IPS    if no policy found then    continue    else if S does not meet the      conditions then    split the cluster adding both parts to repairQ    else    add a to A x  with  a     S x   S   IS y         set c x  a  to be the expected cost of executing a from a random state of x    set p y x  a       p y   x  a      for y     y     end if    end while Prune    for each state x do    find A  x     a            am    all abstract actions that connect clusters that are neighbors in M    order A x    A  x  to create  am             an   such that c x  ai    c x  ai      i   m              n       let A x     a            ap      end for    return  X  A  p  c    Figure    The abstraction algorithm  sufficient  increasing k results in slightly better quality  but slower running times when solving the planning problems  Step    Repair  For each candidate abstract action connecting abstract states x and y  we first need to derive an option that  starting in any state in cluster x leads the agent to some state in cluster y with a minimum total expected cost  We derive this option by setting up a shortest path problem S  whose domain includes S x  and S y   We set the domain of S to be sufficiently large that a policy within this domain can reliably take the agent from any state of S x  to some state of S y   BuildAbstraction builds this domain by performing a breadth first search from S y   proceeding backwards along the transitions  stopping at depth D   m  where D is the search depth from S y  and m is the margin to leave after all states of S x  were added to the domain  If there is any state of S x  that was not included at depth D  the Repair routine reports no solution  The transitions  actions and costs of S are inherited from the MDP M   We also add a new terminating state  which is the destination   Here IS is the characteristic function of S  IS  x      iff x  S and IS  x      otherwise   of transitions leaving the region  i e   those transitions are redirected to this new terminal  with a transition cost that exceeds the maximum of the total expected costs of the ground level MDP  The high cost discourages the solutions to enter the extra terminating state  The optimal solution to S is obtained by using the Improved Prioritized Sweeping  IPS  algorithm of McMahan and Gordon          line      We selected this algorithm based on its excellent performance and known optimality properties  IPS reduces to Dijkstras method in deterministic problems   The resulting policy  is checked against      connectivity  defined as follows  we first compute the expected total cost of reaching some states in S y  for all states of S x   let the resulting costs be c x     Similarly  we compute the probabilities p S y  x    for every x  S x   Then we check if maxx x S x   c x     c x        and maxx x S x   p S y  x     p S y  x        both hold  If these constraints are met  a new abstract action is created and is added to the set of admissible actions at x and the policy is stored as the option corresponding to this new abstract action  lines        Otherwise  the cluster is split  since every cluster has two states  this is trivial  and the appropriate link candidates are added to the repair queue so that no link between potentially connected clusters is missed  Step    Prune  After step    we have an abstract SSP whose abstract states are      connected  However  our abstract action generation mechanism may produce too many actions  which may slow down the planning algorithm  see Section     We address this problem using a pruning step that leaves only the critical and cheapest abstract actions  An action is critical if it connects clusters that are connected at the ground level with a single transition  these actions are important to keep the structure of the ground level MDP  We also keep the cheapest abstract actions as they are likely to help achieve high quality solutions  The pruning parameter  p  specifies the total number of actions to keep   If p is smaller or equal than the number of ground actions  then only the critical actions are kept   BuildAbstraction runs in time linear in the size of the input MDP  as every step is restricted to some fixed size neighborhood of some state  i e   every step is local   Further  employing a suitable data structure  the memory requirements can also be kept linear in the size of the input  These properties are important when scaling up to realistic  real world problem sizes      Planning with an Abstraction  After building an abstraction  we can use it to solve particular SSP problems  When we specify a new goal  our abstraction planner  AbsPlanner  then creates a goal approach region in the abstract MDP that includes the goal and is large enough to include all states of the cluster containing the goal  AbsPlanner builds this region by starting with the ground goal and adding states and transitions in a breadth first fashion to a certain depth  proceeding backwards along the transitions  stopping only after adding all states of the goal cluster  After building the region  AbsPlanner produces an SSP  The domain of this   SSP includes the states found in the breadth first search  and also a new terminal state that becomes the destination of transitions leaving the region  i e   those transitions are redirected to this new terminal  with a high transition cost  All other costs and transitions of this SSP are inherited from the ground level MDP  AbsPlanner uses IPS to solve the local MDP  and saves the resulting goal approach policy  It then solves the abstract MDP  where the goal cluster is set as the goal  When executing the resulting policy   AbsPlanner proceeds normally until reaching a state of the goal approach region  it then switches to the goal approach policy  which it follows until reaching the goal or leaving the region  When this latter event happens and the state is x  execution switches to the option  x x    When using multiple levels of abstraction  AbsPlanners execution follows a recursive  hierarchical strategy  Note that the size of the goal approach region is independent of the size of the MDP  Thus  the planning time will depend on the size of the top level abstract MDP  For an MDP of size n  by using log n levels of hierarchy  in theory it is then possible to achieve planning times that scale with O log n   However  depending on the problem  it might be hard to guarantee high quality solutions when using many levels of abstraction  Furthermore  in practice  over the problems used in our tests   the computation time is dominated by the time needed to set up and solve the goal approach SSPs  which is required for even one layer of abstraction  This is partly because our abstractions result in deterministic shortest path problems  whose solutions can be found significantly faster than those of stochastic problems      Empirical Evaluation  This section summarizes our empirical evaluation of this approach  in terms of the quality  suboptimality  of the solutions and the solution times  Here we report the tradeoffs of using different levels of abstraction as well as the dependence on the stochasticity of the transitions   Note that stochasticity makes it difficult to build abstractions   We also tested the performance of the algorithm on more practical problems  In addition to the results presented here  we conducted extensive experiments  studying the trade off between solution quality and solution time as a function of the various parameters of our algorithm  e g   the values of p  k  or the number of abstraction levels   the scaling behavior of our algorithm in terms of its resource usage  the quality of solutions and the solution time  These results  appearing in  Isaza et al          confirm that the algorithm is robust to the choices of its parameters and scales as expected by increasing problem sizes  We run experiments over three domains  noisy gridworlds  a river and congested game maps  The gridworlds are empty and have four actions  up  down  left  and right  each with cost    The probability that an action leaded to the expected position  e g   the action up moves the agent up one cell  is      while the probability of reaching any of the other three adjacent cells is      The river is similar to the gridworld  its dimensions are w  h  but there is a current flowing from left to right  and a fork corresponding to a line connecting the points  w    h    and  w  h      The flow is represented by modifying both the cost structure and the transition probabilities of the actions  action forward costs    backward costs    diagonally up and forward and diagonally down and forward each cost    These actions are also stochastic  For the backward action  the probabilities are     for going back and     for each of the other actions  For the other three actions  the anticipated move occurs with probability     and the other moves except backwards occur each with probability      and backwards has probability    We include the river domain to determine whether our system can deal with non uniform structures and because the fork complicates the task of creating abstractions  We empirically found the time to build abstractions for the n state gridworld was close to n     seconds  and around n    for an n state river domain  The build time for the maps  using k      was between    and     seconds   The congested game maps are again similar to gridworlds  but with obstacles and with transitions probabilities that depend on the congestion  The obstacle layout comes from commercial game maps  and the stochastic dynamics simulate what happens if multiple units traverse the same map  in narrow passages  the units to become congested  which means an agent trying to traverse such a passage is likely to be blocked  We model this by modifying each action by including a probability that the action will fail and cause the agent to stay at the same position  This failure probability depends on the position on the game map  calculated by simulating many units randomly traversing the game maps and measuring the average occupation of the individual cells  then turning the occupation numbers into probabilities  The optimal policy of an agent in a congested game map will then try to avoid narrow passages  since the higher probability of traffic congestion in such regions means an agent takes much longer to get through those regions  The baseline performance measures are obtained by running the state of the art SSP solver algorithm IPS  For each study  we generate the abstraction and then use it to solve       problems  whose start and goal locations are selected uniformly at random  For each problem we measure the solution time in seconds and the total solution cost for both IPS and our method  then compute the geometric average of the individual suboptimalities and the individual solution time ratios       Abstraction level trade offs  We used a          gridworld to analyze the trade offs of different abstraction levels  with several different parameter configurations  We say a configuration is dominant if it was a Pareto optimal  i e   if no other configuration is better in both time and suboptimality  Figure   presents properties of the dominant configurations   See  Isaza et al         for more details  including relevant pictures    We ran all experiments on a  GHz AMD Opteron tm  Processor with  GB of RAM running Linux with kernel                                                                                                                                                Solution time ratio  Figure    Subobtimality versus the solution time ratio as compared to IPS for different parameter configurations  The dominant configurations are shown for different levels of abstraction  for various abstraction levels  We see that using a smaller number of abstractions required more time but produced better solutions  i e   lower suboptimality   and higher levels of abstractions required less solution time but produced inferior solutions  i e   increased suboptimality   Note that there are dominant configurations for every level of abstraction  from   to    We obtain a level   abstraction by converting the given ground level SSP to deterministic shortest path problem with the same states   Recall that our abstraction process abstracts the state space and produces a deterministic SSP  here we just used the original state space   Figure   shows that this transformation provides solutions whose quality is slightly inferior to the original problem  but it finds this solution significantly faster  e g   in       to        of the time   We also see that these level   solutions are superior to those based on higher abstraction levels  but one can obtain these level i solutions in yet less time                 Suboptimality  level   level   level   level   level   level    Suboptimality vs  speed up on a   x   gridworld             Figure   plots the suboptimality and the speed up of finding a solution using our method  as compared to IPS  for different values of P   We see that our method loses optimality as the dynamics becomes noisier  i e   when P gets smaller   This is because our abstract actions  trying to move the agent from one abstract state to the next will fail with higher probability for noisier dynamics  Note        that the advantage of our method  in terms of planning time  becomes larger with increased stochasticity  This is because our abstractions are deterministic and planning in a deterministic system is much faster than planning with a stochastic system  Figure    plotting the absolute values of cost and time for both our method and IPS  provides another insight  It shows that for increasing stochasticity both methods are slowed down  but our method can cope better with this situation  This figure also confirms that this leads to a loss in solution quality  For our method the typical parameters produce a suboptimality of around     for the river  and around      for the gridworld domain  The speed up for the gridworld is around     while for the river it is around       Sensitivity to Stochasticity of the Dynamics  As the environment becomes noisier  it becomes more difficult to construct a high quality abstraction  This section quantifies how the solution quality and construction time relate to noise in the dynamics  In general  we consider an action successful if the agent moves to the appropriate direction  our gridworld model set the success probability to P        leaving a probability of     P     to moving in each of the other three directions  Here  we vary the value of P   All of these experiments use a        gridworld with k     and p      which means we keep only the critical actions  see Section                Solution time ratio  Figure    Subobtimality versus the solution time ratio as compared to IPS for different values of P    Cost  Suboptimality  Suboptimality vs  Speed up on a    x    gridworld                                                        Cost vs  solution time trade off in a   x   gridworld          IPS     Abstraction                                                                                                                Solution time  s   Figure    Cost versus solution time for IPS and abstraction at different values of P        Congested Game Maps  To test the performance of our approach in a more practical application  we used maps modeled after game environments from commercial video games  We first created simplified gridworlds that resemble some levels from a   Figure    A congested game map  Darker redder color refers to high congestion  Dark blue regions are impassable obstacles  popular role playing and real time strategy game  We then converted the gridworlds into congested maps as described earlier  This produced maps with state space sizes of       BG          BG          BG           WC   and       WC    Figure   provides one such map  where each states color indicates the associated congestion  warmer redder colors indicates high congestion  i e   low probability of success P   while colder bluer colors indicates low congestion  i e   high value of P    Very dark blue indicates impassable obstacles  We see that many of the states in cluttered regions are highly congested and should therefore be avoided  Figure   shows the solution time and the solution suboptimalities for both our method and IPS  for two maps from WarCraft  Blizzard Entertainment        and three maps from Baldurs Gate  BioWare Corp          including Figure    using only a single layer of abstraction  We see that our approach is indeed successful in speeding up the planning process  while keeping the quality of the resulting solutions high      Related Work  Due to space constraints we review only the most relevant work  references to other related works can be found in the extensive bibliography lists of the cited works  Dean et al         introduced the notion of  homogeneous partitions and analyzed its properties  but without giving explicit loss bounds  Kim and Dean        developed some loss bounds  Their Theorem   can be strengthened with our proof method to kv   vP k  kT vP  vP k         using our notation   basically dropping the first term in their bound  Here v  is the optimal value function in the original MDP  vP is the optimal value function of the aggregated MDP extended back to the original state space in a piecewise constant manner and T is the Bellman optimality operator in the original MDP  This bound is problematic as it does not show how the quality of  partitions influences the loss  Our bound improves on this bound in this respect  and also by extending it to the case when the abstract actions correspond to options  While Asadi and Huber        also considered such optionsbased abstractions  they assume that the abstract actions  options  are given externally  possibly by specifying goal states for each of them  and they do not develop bounds  In a number of subsequent papers  the authors refined their methods  In particular  they became increasingly focussed on learning problems  For example  in the recent follow up work  Asadi and Huber        provide a method to learn an abstract hierarchical representation that uses state aggregation and options  Since they are interested in skill transfer through a series of related problems that can differ in their cost structure  they introduce a heuristic to discover subgoals based on bottleneck states  They learn options for achieving the discovered subgoals and introduce a partitioning that respects the learned options  in the clusters typically there are many states   The success of the approach relies critically on the existence of meaningful bottleneck states  This leads to a granularity issue  identifying the bottleneck states requires computing a statistic for each state visited  meaning bottlenecks will not be pronounced if resolution is increased in narrow pathways  Nevertheless  the approach has been successfully tested in a non trivial domain of        states  Hauskrecht  Meuleau  Kaelbling  Dean  and Boutilier        introduce a method that also uses options  but the abstract states correspond to boundary states of regions  The regions are assumed to be given a priori  The idea is similar to using bottleneck states  In contrast to that work  we do not assume any prior knowledge  but construct the abstractions completely autonomously  Further  we deal with undiscounted SSPs  while Hauskrecht et al         dealt with discounted MDPs  but this difference is probably not crucial       Discussion and Future Directions  In the approach presented  options serve as closed loop abstract actions  Another way to use an abstract solution would be to use the abstract value function to guide local search initiated from the current state  These ideas has proven successful in pattern database research where the cost of an optimal solution of an abstract problem is used as a powerful heuristic for the original problem  Such a procedure has the potential to improve solution quality  while keeping low the cost of the planning steps interleaved with execution  Another idea is to use the abstraction to select the amount of such local search  i e   the depth of the rollouts   these ideas has proven successful in deterministic environments  Bulitko  Bjornsson  Lustrek  Schaeffer    Sigmundarson        Bulitko  Lustrek  Schaeffer  Bjornsson    Sigmundarson         Presently  our abstractions are deterministic  This suggests two avenues for future work  First  applying advanced heuristic search methods to such abstractions may lead to performance gains  Second  in highly stochastic domains  the abstractions determinism may lead to a poor quality of solution  as the cost of ensuring arrival at an abstract   Solution time for different game maps  Solution suboptimality for different game maps       IPS Abstraction       Suboptimality  Solution time  s                                       WC    WC    BG    BG    BG    WC    Map  WC    BG    BG    BG    Map  Figure    Solution times  left  and suboptimalities  right  for several game maps  state with certainty  or very high probability  can lead to very conservative and costly paths  Thus  it would be of interest to investigate stochastic abstractions  One idea is to modify the way abstract actions are defined  When planning to connect to abstract states after a solution of the local SSP is found  with a little extra work we can compute the probabilities of reaching various neighboring abstract states under the policy found when the policy leaves the region of interest  Yet another avenue for future work would be to move from a state based problem formulation to a feature based one  assuming that the features describe the states  The challenge is to design an algorithm that can construct an abstraction without enumerating all the states  as ours currently does  Although this paper has not attempted to address this problem  we believe that the approach proposed here  i e   incremental clustering and defining options by solving local planning problems  is applicable  Finally  although the present paper dealt only with undiscounted  stochastic shortest path problems  the approach can be extended to work for discounted problems  This holds because a discounted problem can always be viewed as an undiscounted stochastic shortest path problem where every time step a transition is made to some terminal state with probability      where          is the discount factor      Conclusions  This paper has explored ways to speed up planning in SSP problems via goal independent state and action abstraction  We strengthen existing theoretical results  then provide an algorithm for building abstraction hierarchies automatically  Finally  we empirically demonstrate the advantages of this approach by showing that it works effectively on SSPs of varying size and difficulty   Acknowledgements We gratefully acknowledge the insightful comments by the reviewers  This research was funded in part by the National Science and Engineering Research Council  NSERC   iCore and the Alberta Ingenuity Fund   
