 Previous work has shown that the problem of learning the optimal structure of a Bayesian network can be formulated as a shortest path finding problem in a graph and solved using A  search  In this paper  we improve the scalability of this approach by developing a memoryefficient heuristic search algorithm for learning the structure of a Bayesian network  Instead of using A   we propose a frontier breadth first branch and bound search that leverages the layered structure of the search graph of this problem so that no more than two layers of the graph  plus solution reconstruction information  need to be stored in memory at a time  To further improve scalability  the algorithm stores most of the graph in external memory  such as hard disk  when it does not fit in RAM  Experimental results show that the resulting algorithm solves significantly larger problems than the current state of the art     INTRODUCTION Bayesian networks are a common machine learning technique used to represent relationships among variables in data sets  When these relationships are not known a priori  the structure of the network must be learned  A common learning approach entails searching for a structure which optimizes a particular scoring function  Cooper and Herskovits       Heckerman  Geiger  and Chickering        Because of the difficulty of the problem  early approaches focused on approximation techniques to learn good networks  Cooper and Herskovits       Heckerman  Geiger  and Chickering       Heckerman       Friedman  Nachman  and Peer       Tsamardinos  Brown  and Aliferis        Unfortunately  these algorithms are unable to guarantee anything about the quality of the learned networks  Exact dynamic programming algorithms have been developed to learn provably optimal Bayesian network struc   tures  Ott  Imoto  and Miyano       Koivisto and Sood       Singh and Moore       Silander and Myllymaki        These algorithms identify optimal small subnetworks and add optimal leaves to find large optimal networks until finding the optimal network including all variables  Unfortunately  all of these algorithms must store an exponential number of subnetworks and associated information in memory  Parviainen and Koivisto        recently proposed a divide and conquer algorithm in which fewer subnetworks are stored in memory at once at the expense of longer running time  Theoretical results suggest that this algorithm is slower than dynamic programming when an exponential number of processors is not available  Yuan et al         developed an A  heuristic search formulation based on the dynamic programming recurrences to learn optimal network structures  The algorithm formulates the learning problem as a shortest path finding problem in a search graph  Each path in the graph corresponds to an ordering of the variables  and each edge on the path has a cost that corresponds to the choice of an optimal parent set for one variable out of the variables that appear earlier on the path  Together  all the edges on a path encode an optimal directed acyclic graph that is consistent with the path  The solution to the shortest path finding problem then corresponds to an optimal Bayesian network structure  The A  algorithm also uses a consistent heuristic function to prune provably suboptimal solutions during the search so as to improve its efficiency  de Campos et al         proposed a systematic search algorithm to identify optimal network structures  The algorithm begins by calculating optimal parent sets for all variables  These sets are represented as a directed graph that may have cycles  Cycles are then repeatedly broken by removing one edge at a time  The algorithm terminates with an optimal Bayesian network  However  this algorithm is shown to often learn the optimal structure slower than the dynamic programming algorithm  de Campos  Zeng  and Ji        Optimal networks have also been learned using linear programming  Jaakkola et al         This technique reformu    lates the structure learning problem as a linear program  An exponential number of constraints are used to define a convex hull in which each vertex corresponds to a DAG  Coordinate descent is used to identify the vertex which corresponds to the optimal DAG structure  Furthermore  the dual of their formulation provides an upper bound which can help guide the descent algorithm  This algorithm was shown to have similar or slightly better runtime performance as dynamic programming  Jaakkola et al         This paper describes a novel frontier breadth first branch and bound algorithm using delayed duplicate detection for learning optimal Bayesian network structures  The basic idea is to formulate the learning task as a graph search problem  The search graph decomposes into natural layers and allows searching one layer at a time  This algorithm improves the scalability of learning optimal Bayesian network structures in three ways  First  the frontier search approach allows us to reduce the memory complexity by working with only a single layer of search graphs at a time during the search  In particular  we store one layer of each search graph  the scores required for that layer and information for solution reconstruction from every previous layer  Other information is deleted  In comparison  previous dynamic programming algorithms have to store an entire exponentiallysized graph in memory  Second  branch and bound techniques allow us to safely prune unpromising search nodes from the search graphs  while dynamic programming algorithms have to evaluate the whole search space  Finally  we use a delayed duplicate detection method to ensure that  given enough hard disk space  optimal network structures can be learned regardless of the amount of RAM  Previous algorithms fail if an exponential amount of RAM is not available  The remainder of this paper is structured as follows  Section   provides an overview of the task of Bayesian network learning  Section   and   introduce two formulations for solving the learning task  dynamic programming and graph search  Section   discusses the details of the externalmemory frontier breadth first branch and bound algorithm we propose in this paper  Section   compares the algorithm against several existing approaches on a set of benchmark machine learning datasets  Finally  Section   concludes the paper     BACKGROUND A Bayesian network consists of a directed acyclic graph  DAG  structure and a set of parameters  The vertices of the graph each correspond to a random variable V    X         Xn    All parents of Xi are referred to as P Ai   A variable is conditionally independent of its non descendants given its parents  The parameters of the network specify a conditional probability distribution  P  Xi  P Ai   for each Xi    Given a dataset D    D         DN    where Di is an instantiation of all the variables in V  the optimal structure is the DAG over all of the variables which best fits D  Heckerman        A scoring function measures the fit of a network structure to D  For example  the minimum description length  MDL  scoring function  Rissanen       uses one term to reward structures with low entropy and another to penalize complex structures  Optimal structures minimize the score  Let ri be the number of states of the variable Xi   let Npai be the number of data records consistent with P Ai   pai   and let Nxi  pai be the number of data records consistent with P Ai   pai and Xi   xi   The MDL score for a structure G is defined as follows  Tian         M DL G     X  M DL Xi  P Ai          i  where  M DL Xi  P Ai     H Xi  P Ai     K Xi  P Ai      log N H Xi  P Ai     K Xi  P Ai      X Nxi  pai       Nxi  pai log  Npai xi  pai Y  ri     rl   Xl P Ai  MDL is decomposable  Heckerman        so the score for a structure is simply the sum of the score for each variable  Our algorithm can be adapted to use any decomposable function  Some sets of parents cannot form an optimal parent for any variable  as described in the following theorems from Tian        and de Campos et al          Theorem    In an optimal Bayesian network based on the MDL scoring function  each variable has at most  N log  log N   parents  where N is the number of data points  Theorem    Let U  V and X    U  If BestM DL X  U    BestM DL X  V   V cannot be the optimal parent set for X     DYNAMIC PROGRAMMING Learning an optimal Bayesian network structure is NPHard  Chickering        Dynamic programming algorithms learn optimal network structures in O n n   time and memory  Ott  Imoto  and Miyano       Koivisto and Sood       Singh and Moore       Silander and Myllymaki        Because a network structure is a DAG  the optimal structure can be divided into an optimal leaf vertex and its parents as well as an optimal subnetwork for the rest of the variables  This subnetwork is also a DAG  so it can recursively be divided until the subnetwork is only a single   vertex  At that point  the optimal parents have been found for all variables in the network and the optimal structure can be constructed  It has been shown  Silander and Myllymaki       that a more efficient algorithm begins with a  variable subnetwork and exhaustively adds optimal leaves  For the MDL scoring function and variables V  this recurrence can be expressed as follows  Ott  Imoto  and Miyano         M DL V     min  M DL V    X      XV  Figure    Parent graph for variable X   BestM DL X  V    X     where BestM DL X  V    X      min  P AX V  X   M DL X P AX     As this recurrence suggests  all dynamic programming algorithms must perform three steps  First  they must calculate the score of each variable given all subsets of the other variables as parents  There are n n  of these scores  Then  BestM DL must be calculated  For a variable X and set of possible parents V  this function returns the subset of those parents which minimizes the score for X as well as that score  There are n n  of these optimal parent sets  Finally  the optimal subnetworks must be learned  These subnetworks use BestM DL to learn the optimal leaf for every possible subnetwork  including the optimal network with all of the variables  There are  n optimal subnetworks  Figure    An order graph of four variables    GRAPH SEARCH FORMULATION We first formulate each phase of the dynamic programming algorithm as a separate search problem  including calculating parent scores  identifying the optimal parent sets  and learning the optimal subnetworks  We use an AD tree like search to calculate all of the parent scores  An AD tree  Moore and Lee       is an unbalanced tree which contains AD nodes and varying nodes  The tree is used to collect count statistics from a dataset  An AD node stores the number of records consistent with the variable instantiation of the node  while a varying node assigns a value to a variable  As shown in Equation    the entropy component of a score can be calculated based on variable instantiation counts  Each AD node has an instantiation of a set of variables U and the count of records consistent with that instantiation  That count is a value of pai for all X  V   U  Furthermore  it is a value of xi   pai for all X  U with parents U    X   We can use a depthfirst traversal of the AD tree to compute the scores  Theorem   states that only small parent sets can possibly be optimal parents when using the MDL score  All nodes below the depth specified in the theorem are pruned  The scores which are not pruned are written to disk for retrieval when  identifying optimal parent sets  We call this data structure a score cache  Each entry in the score cache contains one value of M DL X P A   A parent graph is a lattice in which each node stores one value of BestM DL for different candidate sets of variables  The score cache is used to quickly look up the scores for candidate parent sets  Figure   shows the construction of the parent graph for variable X  as a lattice  All  n  subsets of all other variables are present in the graph  Each node contains one value for BestM DL of X  and the set of candidate parents shown  That is  each node stores the subset of parents from the given candidate set which minimizes the score of X    as well as that score  The lattice divides the nodes into layers  We call the first layer of the graph  the layer with the single node for    in Figure    layer    A node in layer l has l predecessors  all in layer l     and considers candidate parent sets of size l  Layer l has C n     l  nodes  where C n  k  is the binomial coefficient  Each variable has a separate parent graph  The complete set of parent graphs stores n n  parent sets  An order graph is also a lattice  Each node contains M DL V  and the associated optimal subnetwor for one   subset of variables  Figure   displays an order graph for four variables  Its lattice structure is similar to that of the parent graphs  because it contains subsets of all variables  though  the order graph has  n nodes  The topmost node in layer   containing no variables is the start node  The bottom most node containing all variables is the goal node  A directed path in the order graph from the start node to any other node induces an ordering on the variables in the path with new variables appearing later in the ordering  For example  the path traversing nodes      X     X    X      X    X    X    stands for the variable ordering X    X    X    All variables which precede a variable in the ordering are candidate parents of that variable  Each edge on the path has a cost equal to BestM DL for the new variable in the child node given the variables in the parent node as candidate parents  The parent graphs are used to quickly retrieve these costs  For example  the edge between  X    X    and  X    X    X    has a cost equal to BestM DL X     X    X      Each node contains a subset of variables  the cost of the best path from the start node to that node  a leaf variable and its optimal parent set  The shortest paths from the start node to all the other nodes correspond to the optimal subnetworks  so the shortest path to the goal node corresponds to the optimal Bayesian network  The lattice again divides the nodes into layers  Nodes in layer l contain optimal subnetworks of l variables  Layer l has C n  l  nodes     AN EXTERNAL MEMORY FRONTIER BREADTH FIRST BRANCH AND BOUND ALGORITHM Finding an optimal Bayesian network structure can be considered a search through the order graph  This formulation allows the application of any graph search algorithm  such as A   Yuan  Malone  and Wu        to find the best path from the start node to the goal node  In particular  such a formulation allows us to treat the order and parent graphs as implicit search graphs  That is  we do not have to keep the entire graphs in memory at once  Dynamic programming can be considered as a breadth first search through this graph  Malone  Yuan  and Hansen        Previous results show that the scalability of existing algorithms for learning optimal Bayesian networks is typically limited by the amount of RAM available  To eliminate the constraint of limited RAM  we introduce a frontier breadth first branch and bound algorithm with delayed duplicate detection to do the search by adapting the breadth first heuristic search algorithm proposed by Zhou and Hansen               It is also similar to the frontier search described by Korf         Breadth first heuristic search expands a search space in order of layers of increasing g cost with each layer comprising all nodes with a same g cost  As each node is generated  a heuristic function is used to calculate a lower bound for  Algorithm   A Frontier BFBnB Search Algorithm procedure EXPAND O RDER G RAPH l  isP resent  upper  lb  maxSize  for each MDLl  U   MDLl do for each X  V   U do s  MDLl  U    BMDLl  X U   lb X  if s   upper then continue isP resent U   X    true if s   MDLl    U   X   then MDLl    U   X    s MDLP l    U   X    BMDLP l  X U  end if if  MDLl       maxSize then writeTempFile MDLl     MDLP l     end if end for end for writeTempFile MDLl     MDLP l     MDLl     MDLP l    mergeTempFiles delete MDLl end procedure procedure EXPAND PARENT G RAPH l  p  isP resent  maxSize  for each BestMDLl  p U   BestMDLl  p do for each X  V   U and X    p do S  U   X  if  isP resent S  then continue if MDL p S    BMDLl    p S  then BMDLl    p S   MDL p S  BMDLP l    p S   S end if if BMDLl  p U    BMDLl    p S  then BMDLl    p S   BMDLl  p U  BMDLP l    p S   BMDLP l  p U  end if if  BMDLl    p     maxSize then writeTempFile BMDLl    p   BMDLSl    p   end if end for end for writeTempFile BMDLl    p   BMDLP l    p   BMDLl     BMDLP l    p  mergeTempFiles delete BMDLl   BMDLP l  p  end procedure procedure EXPANDADN ODE i  U  Du   d  For j   i      n do expandVaryNode j  U  Du   d  end procedure procedure EXPANDVARY N ODE i  U  Du   d  for j      ri do updateScores U   Xi    DXi  j u   if d     then expandADNode i  U   Xi    DXi  j u   d     end for end procedure procedure UPDATESCORES  U  Du   for X  V   U do if MDL X U  is null then MDL X U   K X U  MDL X U   MDL X U    Nu  log Nu end for for X  U do if M DL X U    X   is null M DL X U    X    K X U    X    MDL X U    X    MDL X U    X    Nu  log Nu end for end procedure procedure MAIN D  upper  maxSize   N maxP arents  log log N expandADNode        D  maxP arents  lb  getBestScores writeScoresToDisk isP resent     for l      n do for p      n do expandParentGraph l  p  isP resent  maxSize  end for expandOrderGraph l  isP resent  upper  lb  maxSize  end for optimalStructure  reconstructSolution end procedure   that node  If the lower bound is worse than a given upper bound on the optimal solution  the node is pruned  otherwise  the node is added to the open list for further search  After the search  a divide and conquer method is used to reconstruct the optimal solution  Algorithm   gives the pseudocode for our BFBnB search algorithm for learning optimal Bayesian networks  The algorithm is very similar to the breadth first heuristic search algorithm but has several subtle and important differences  First  the layers in our search graphs  the parent and order graphs  do not correspond to the g costs of nodes  rather  layer l corresponds to variable sets  candidate parent sets or optimal subnetworks  of size l  For the order graph  though  we can calculate both a g  and h cost for pruning  as described in Section      We also describe how to propagate this pruning from the order graph to the parent graphs  Another difference is that our search problem is a nested search of order and parent graphs  The layered parent and order graph searches have to be carefully orchestrated to ensure the correct nodes can be accessed easily at the correct time  as described in Section      This further requires the parent scores are stored in particular order  as described in Section      Yet another difference is that we use a variant of delayed duplicate detection  Korf       in which a hash table is used to detect as many duplicates in RAM as possible before resorting to external memory  as described in Section      Finally  we store a portion of each order graph node to reconstruct the optimal network structure after the search  as described in Section          BRANCH AND BOUND We need a heuristic function f  U    g U    h U  that estimates the cost of the best path from the start node to a goal node using order node U  The g cost is simply the sum of the edge costs of the best path from the start node to U  The h cost provides a lower bound on the cost from U to the goal node  We use the following heuristic function h from Yuan et al          Definition    h U     X  BestM DL X  V  X          XV U  This heuristic function relaxes the acyclic constraint on the remaining variables in V   U and allows them to choose parents from all of the variables in V  The following theorem from Yuan et al         proves that the function is consistent  Consistent heuristics are guaranteed to be admissible  Theorem    h is consistent  In order to calculate this bound  we must know BestM DL X  V  X    Fortunately  these scores are calculated during the first phase of the algorithm  Because  the score cache contains every score which could possibly be optimal for all variables  it is guaranteed to have the optimal score for all variables given any set of parents  which is BestM DL X  V    X    Thus  we can identify these scores while calculating the scores when expanding the AD tree and store them in an array for reuse  The pseudocode uses the function getBestScores to find these scores and the array lb to store them  We can apply BFBnB to prune nodes in the order graph using the lower bound function in Equation    however  pruning is not directly applicable to the parent graphs  An optimal parent score BestM DL X  U  is only necessary if a node for U is in the order graph  Consequently  if U is pruned from the order graph  then the nodes for U are also pruned from the parent graphs  The pseudocode uses isP resent to track which nodes were not pruned  We also need an upper bound score on the optimal Bayesian network for pruning  A search node U whose heuristic value f  U  is higher than the upper bound is immediately pruned  Numerous fast  approximate methods exist for learning a locally optimal Bayesian network  We use a greedy beam search algorithm based on a local search algorithm described by Heckerman        to quickly find the upper bound  A more sophisticated algorithm could be used to find a better bound and improve pruning  The input argument upper is this bound in the pseudocode      COORDINATING THE GRAPH SEARCHES The parent and order graph searches must be carefully coordinated to ensure that the parent graphs contain the necessary nodes to expand nodes in the order graph  In particular  expanding a node U in layer l in the order graph requires BestM DL X  U   which is stored in the node U of the parent graph for X  Hence  before expanding layer  U  in the order graph  that layer of the parent graphs must already exist  Therefore  the algorithm alternates between expanding layers of the parent graphs and order graph  Expanding a node U in the parent graph amounts to generating successor nodes with candidate parents U   X  for all X in V   U  For each successor S   U   X   the hash table for the next layer is first checked to see if S has already been generated  If not  the score of using all of S as parents of X is retrieved from the score cache and compared to the score of using the parents specified in U  If using all of the variables has a better score  then an entry is added to the hash table indicating that  for possible parents S  using all of them is best  Otherwise  according to Theorem    the hash table stores a mapping from S to the parents in U  Similarly  if S has already been generated  the score of the existing best parent set for S is compared to the score using the parents in U  If the score of the parents in U is better  then the hash table mapping is updated accordingly  Once a layer of the parent graph is expanded    the whole layer can be discarded as it is no longer needed  The pseudocode uses BM DLl to store the optimal scores and BM DLP l to store the optimal parents   there is no additional work required to arrange the nodes when writing them to disk   Expanding a node U in the order graph amounts to generating successor nodes U   X  for all X in V   U  To calculate the score of successor S   U   X   the score of the existing node U is added to BestM DL X  U   which is retrieved from parent graph node U for variable X  The optimal parent set out of U is also recorded  This is equivalent to trying X as the leaf and U as the subnetwork  Next  the hash table for the next layer is consulted  If it contains an entry for S  then a node for this set of variables has already been generated using another variable as the leaf  The score of that node is compared to the score for S  If the score for S is better  or the hash table did not contain an entry for S  then the mapping in the hash table is updated  Unlike the parent graph  however  a portion of each order graph node is used to reconstruct the optimal network at the end of the search  as described in Section      This information is written to disk  while the other information is deleted  The pseudocode uses M DLl to store the score for each subnetwork and M DLP l to store the associated parent information       ORDERING THE SCORES ON DISK  Additional care is needed to ensure that parent and order graph nodes for a particular layer are accessed in a regular  structured pattern  We arrange the nodes in the parent and order graphs in queues such that when node U is removed from the order graph queue  the head of each parent graph queue for all X in V   U is U  So all of the successors of U can be generated by combining it with the head of each of those parent graph queues  Once the parent graph nodes are used  they can be removed  and the queues will be ready to expand the next node in the order graph queue  Because the nodes are removed from the heads of the queues  these invariants hold throughout the expansion of the layer  Regulating such access patterns improves the scalability of the algorithm because these queues can be stored on disk and accessed sequentially to reduce the requirement of RAM  The regular accesses also reduce disk seek time  The pseudocode assumes the nodes are written to disk in this order to easily retrieve the next necessary node   Duplicate nodes are generated during the graph searches  Duplicates in the parent and order graphs correspond to nodes which consider the same sets of variables  candidate parent sets and optimal subnetworks  respectively   Because the successors of a node always consider exactly one more variable in both the parent and order graphs  the successors of a node in layer l are always in layer l      Therefore  when a node is generated  it could only be a duplicate of a node in the open list for layer l      In both the parent and order graphs  the duplicate with the best score should be kept   The lexicographic ordering  Knuth       of nodes within each layer is one possible ordering that ensures the queues remain synchronized  For example  the lexicographic ordering of   variables of size   is   X    X      X    X      X    X      X    X      X    X      X    X      The order graph queue for layer   of a dataset with   variables should be arranged in that order  The parent graph queue for variable X should have the same sequence  but without subsets containing X  In the example  the parent graph queue for variable X  should be   X    X      X    X      X    X      As described in more detail in Section      the nodes of the graphs must be sorted to detect duplicates  the lexicographic order ensures that  For large datasets  the score cache can grow quite large  We write it to disk to reduce RAM usage  Each score M DL X  U  is used once  when node U is first generated in the parent graph for X  As described in Section      the parent graph nodes are expanded in lexicographic order  however  they are not generated in that order  The score M DL X  U   U    Y        Yl   is needed when expanding node U    Yl   in the parent graph for X  Therefore  the scores must be written to disk in that order  The pseudocode uses the writeScoresT oDisk function to sort and write the scores to disk in this order  A file is created for each variable for each layer to store these sorted scores  The file for a particular layer can be deleted after expanding that layer in the appropriate parent graph      DUPLICATE DETECTION  For large datasets  it is possible that even one layer of the parent or order graph is too large to fit in RAM  We use a variant of the delayed duplicate detection  DDD   Korf       in our algorithm to utilize external memory to solve such large learning problems  In DDD  search nodes are written to a file on disk as they are generated  After expanding a layer  an external memory sorting algorithm is used to detect and remove duplicate nodes in the file  The nodes in the file are then expanded to generate the next layer of the search  Consequently  the search uses a minimal amount of RAM  however  all generated nodes are written to disk  so much work is done reading and writing duplicates  Rather than immediately writing all generated nodes to disk  we instead detect duplicates in RAM as usual with a hash table  Once the open list reaches a user defined maximum size  its contents are sorted and written to a temporary file on disk  The open list is then cleared  At the end of each layer  the remaining contents of the open list and the temporary files are sorted and merged into a single file which contains the sorted list of nodes from that layer  For rea    sons described in Section      the lexicographic ordering of nodes within a layer is used when sorting  The hash table reduces the number of nodes written to and read from disk by detecting as many duplicates as possible in RAM  The pseudocode uses maxSize as the user defined maximum size  The function writeT empF ile sorts  writes to disk and clears the open list provided as its argument  The scores and optimal parent sets are written together on disk  The function mergeT empF iles performs an external memory merge to detect duplicates in the temp files  For the parent graphs  both the scores and optimal parent sets are kept in a single file  however  as described in Section      the parent information of the order graph must be stored for the entire search  while the score information can be deleted after use  Therefore  two separate files are used to allow the information to easily be deleted      RECONSTRUCTING THE OPTIMAL NETWORK STRUCTURE In order to trace back the optimal path and reconstruct the optimal network structure  we write a portion of each node of the order graph to a disk file once it is expanded during the order graph search  For each order graph node we write the subset of variables  the leaf variable and its optimal parents  Solution reconstruction works as follows  The final leaf variable X and its optimal parent set are retrieved from the goal node  Because the goal node considers all variables  its predecessor in the optimal path is U   V    X   This predecessor is retrieved from the file for layer  U   That node has the optimal leaf and parent set for that subnetwork  Recursively  the optimal leaves and parent sets are retrieved until reconstructing the entire network structure  We use this approach instead of the standard divide and conquer solution reconstruction because  as shown in Section    it requires relatively little memory  Furthermore  divide and conquer would require regeneration of the parent graphs  which is quite expensive in terms of time and memory  The pseudocode uses the function reconstructSolution to extract this information from the M DLP l files      ADVANTAGES OF OUR ALGORITHM Our frontier breadth first branch and bound algorithm has several advantages over previous algorithms for learning optimal Bayesian networks  First  our top down search of the AD tree for calculating scores ensures we never need to calculate scores or counts of large variable sets  The AD tree method is in contrast to the bottom up method used by other algorithms  Silander and Myllymaki        Bottom up methods must always compute the scores  or at least the counts  of large parent sets in order to correctly calculate the counts required for the smaller ones  Since our algorithm neither calculates nor  stores these counts and scores  it both runs more quickly and uses less memory  Second  the layered search strategy reduces the memory requirements by working with one layer of the parent and order graphs at a time  Other information can be either discarded immediately or stored in hard disk files for later use  e g   the information needed to reconstruct the optimal network structure  Previous formulations  such as PCaches  Singh and Moore       and arrays  Silander and Myllymaki        could not take advantage of this structure  Singh and Moore propose a depth first search through the P Caches  while Silander and Myllymakis approach identifies the sets according to their lexicographic ordering   We use the lexicographic order within each layer  not over all of the variables   These approaches can identify neither optimal parent sets nor optimal subnetworks one layer at a time  Thus  they must both keep all of the optimal parent sets and subnetworks in memory  Third  we prune the order graph using an admissible heuristic function  this further reduces the memory complexity of the algorithm  Pruning unpromising nodes from the order graph not only reduces the amount of computation but also reduces the memory requirement  Furthermore  the savings in running time and memory also propagate to parent graphs  Dynamic programming algorithms always evaluate the full order graph  The duplicate detection method we use lifts the requirement that open lists fit in RAM to detect duplicates  Because our algorithm does not resort to delayed duplicate detection until RAM is full  our algorithm can still take advantage of large amounts of RAM  By writing nodes to disk  we can learn optimal Bayesian networks even when single layers of the search graphs do not fit in RAM  Our algorithm also has advantages over other learning formulations  In contrast to the A  algorithm of Yuan et al          we only keep one layer of the order graph in memory at a time  The open and closed lists of A  keep all generated nodes in memory to perform duplicate detection  Unlike the systematic search algorithm of de Campos et al   de Campos  Zeng  and Ji        we always search in the space of DAGs  which is smaller than the space of directed graphs in which that algorithm searches  The LP algorithm  Jaakkola et al        uses the same mechanism to identify optimal parent sets as DP  therefore  it cannot complete when all optimal parent sets do not fit in memory     EXPERIMENTS We compared a Java implementation of the externalmemory frontier BFBnB search with DDD  BFBnB  to an efficient version  Silander and Myllymaki       of dynamic programming which uses external memory written   Dataset dataset n wine    adult    zoo    houseVotes    letter    statlog    hepatitis    segment    meta    imports    horseColic    spect  heart     mushroom    parkinsons    sensorReadings    autos    horseColic  full     steelPlatesFaults    flag    wdbc    epigenetic     N                                                                                                       DP                                                                              OD OD  Timing Results  s  BFBnB A                                                                                           OM       OM       OM       OM        OM        OM         OM  SS     OT OT       OT OT           OT             OT OT OT OT OT OT OT OT OT OT  Space Results  bytes  DP BFBnB     E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E    OD     E    OD     E     Table    A comparison of the running time  in seconds  for Silander and Myllymakis dynamic programming implementation  DP   Yuan et al s A  algorithm  A    de Campos et al s systematic search algorithm  SS  and our external memory frontier breadth first branch and bound algorithm  BFBnB   The run times are given for all algorithms  Maximum external memory usage is given for DP and BFBnB  For reference   E    is   gigabyte  n is the number of variables  N is the number of records  OT means failure to find optimal solutions due to running for more than   hours        seconds  less than    variables  or    hours         seconds          variables  and not producing a provably optimal solution  OM means failure to find optimal solutions due to running out of RAM    GB   OD means failure to find optimal solutions due to running out of hard disk space     GB   in C downloaded from http  b course hiit fi bene  We refer to it as DP  Previous results  Silander and Myllymaki       have shown DP is more efficient than other dynamic programming implementations  We also compared to Yuan et al s A  implementation         A   and de Campos et al s branch and bound systematic search algorithm  de Campos  Zeng  and Ji        SS  downloaded from http   www ecse rpi edu  cvrl structlearning html  We did not include comparison to the DP implementation of Malone et al          MDP  because the codebase is similar  however  MDP does not incorporate pruning or delayed duplicate detection  The running times of BFBnB and MDP are similar on datasets which both complete  but  due to duplicate detection  MDP fails when an entire layer of the order graph does not fit in RAM  Benchmark datasets from the UCI repository  Frank and Asuncion       were used to test the algorithms  We also constructed a biological dataset consisting of ChIP Seq data for epigenetic features downloaded from http   dir nhlbi nih gov papers lmi epigenomes hgtcell html  and http   dir nhlbi nih gov papers lmi epigenomes  hgtcellacetylation aspx  The experimental datasets were normalized using linear regression using the IgG control dataset downloaded from http   home gwu edu wpeng Software htm  The largest datasets in the comparison have up to    variables and over        records  Continuous and discrete variables with more than four states were discretized into two states around the mean  Records with missing values were removed  DP and SS do not calculate the MDL score for a network  however  they can calculate BIC  The score uses an equivalent calculation as MDL  so the algorithms always learned equivalent networks  The experiments were performed on a      GHz Intel i  with   GB of RAM     GB of hard disk space and running Ubuntu version        On datasets with less than    variables  all algorithms were given a maximum runtime of   hours        seconds   On datasets with    to    variables  all algorithms were given a maximum runtime of    hours         seconds     We empirically evaluated the algorithms for both space and time requirements  For the algorithms which used external memory  BFBnB and DP   we compared the maximum hard disk usage  We also compared the running times of the algorithms  The results are given in Table    Previous results found that memory is the main bottleneck restricting the size of learnable networks  Parviainen and Koivisto        As the results show  algorithms which attempt to store entire parent or order graphs in RAM  such as A  and SS  are limited to smaller sets of variables  BFBnBs duplicate detection strategy allows it to write parital search layers to hard disk when the layers are too large to fit in RAM  so it can learn optimal Bayesian network structures regardless of the amount of RAM  Consequently  hard disk space is its only memory limitation  The inexpensive cost of hard disks coupled with distributed file systems can potentially erase the effect of memory on the scalability of the algorithm  For the datasets which it could solve  A  was sometimes faster than the other algorithms  This is unsurprising since it uses only RAM  however  it is unable to solve the larger datasets that cannot fit entirely in RAM  Even on many of the smaller datasets  though  A  runs more slowly than BFBnB because it has the overhead cost to keep its open list in sorted order  BFBnB not only takes an order of magnitude less external memory  but runs several times faster than the DP algorithm on most of the datasets  DP is faster on the adult  letter and meta datasets  These datasets have a small number of variables and a large number of records  The large number of records limits the pruning of the AD tree from Theorem   and increases the runtime of BFBnB  However  BFBnB runs faster on both mushroom        records  and sensorReadings        records   Therefore  as the number of variables increases  the number of records impacts the runtime less  The SS algorithm ran much more slowly than the other algorithms  It searches in the space of directed graphs rather than DAGs  These results suggest that search in the space of DAGs is more efficient than the space of directed graphs  To demonstrate that our algorithm is applicable to larger datasets  we also tested it using the wdbc dataset     variables      records  and a biological dataset     variables         records   epigenetic  We learned the optimal network for wdbc in        seconds  about    hours  and the optimal network for epigenetic in         seconds  about   days   We also attempted to use DP  but its hard disk usage exceeded the    GB of free hard disk space on the server  Figure   shows the total memory consumption of our algorithm for wdbc  Very little memory is used before layer    and after layer     the memory consumption does not change much because the layer sizes decrease  As the figure shows  both of the middle layers use nearly    giga   Figure    Hard disk usage for the wdbc dataset  bytes of disk space  Most of this space is consumed by the parent graphs  so it is is freed after each layer  Assuming that the running time and size of the middle layers double for each additional variable  which is a rough pattern from Table    our algorithm could learn a    variable network in about    days using approximately   terabytes of hard disk space and a single processor  This suggests that our method should scale to larger networks better than the method of Parviainen and Koivisto         They observe that their implementation would take   weeks on     processors to learn a    variable network  and  even with coding improvements and massive parallelization  only networks up to    variables would be possible     CONCLUSION Learning optimal Bayesian network structures has been thought of in terms of dynamic programming  however  such a formulation naively requires O n n   memory  Other formulations have been shown to have similar or slower runtimes or require other exponential resources  such as processors  This paper formulates the structure learning problem as a frontier breadth first branch and bound search  The layered search technique allows us to work with one layer of the score cache  parent and order graphs at a time  Consequently  we delete layers of the parent graphs after expanding them and store only a portion of each order graph node to hard disk files to reduce the memory complexity  The delayed duplicate detection strategy further improves the scalability of the algorithm by writing partial layers to disk rather than requiring an entire layer fit in RAM at once  Additionally  a heuristic function allows parts of the order graph to be ignored entirely  this also reduces memory complexity and improves scalability  Experimental results demonstrate that this algorithm outperforms the previous best implementation of dynamic programming for learning optimal Bayesian networks  Our algorithm not only runs faster than the existing approach  but also takes much less space  The LP formulation exhibits similar runtime behavior as DP  so our algorithm should   similarly outperform it  It also scales to more variables than A   Additionally  by searching in the space of DAGs instead of the space of directed graphs with cycles  it proves the optimality of the learned network more quickly than SS  Future work will investigate better upper bounds and heuristic functions to further increase the size of learnable optimal networks  Also  like existing methods  Parviainen and Koivisto       Silander and Myllymaki        our algorithm can benefit from parallel computing  In addition  distributed computing can scale up our algorithm to even larger learning problems  Networks learned from our algorithm could also be used as a gold standard in studying the assumptions of approximate structure learning algorithms  Acknowledgements This work was supported by NSF CAREER grant IIS         and EPSCoR grant EPS          
  nostic system  Given that so many variables are involved  even the best solution by MAP or MPE may have an ex   Most Relevant Explanation  MRE  is a method for  nding multivariate explanations for given evidence in Bayesian networks        tremely low probability  say in the order of         It is hard  to make any decision based on such hypotheses   This pa   In real world problems  it is observed that usually only a  per studies the theoretical properties of MRE  few target variables are most relevant in explaining any  and develops an algorithm for  nding multiple  given evidence  For example  there are many possible dis   top MRE solutions  Our study shows that MRE  eases in a medical domain  but a patient can have at most  relies on an implicit soft relevance measure in  a few diseases at one time  as long as he or she does not  automatically identifying the most relevant tar   delay treatments for too long  It is desirable to  nd diag   get variables and pruning less relevant variables  nostic hypotheses containing only those relevant diseases   from an explanation  The soft measure also en   Other diseases should be excluded from further tests or  ables MRE to capture the intuitive phenomenon  treatments  In a recent work  Yuan and Lu      propose  of explaining away encoded in Bayesian net   an approach called Most Relevant Explanation  MRE  to  works   Furthermore  our study shows that the  generate explanations containing only the most relevant tar   solution space of MRE has a special lattice struc   get variables for given evidence in Bayesian networks  Its  ture which yields interesting dominance relations  main idea is to traverse a trans dimensional space contain   among the solutions  A K MRE algorithm based  ing all the partial instantiations of the target variables and  on these dominance relations is developed for   nd one instantiation that maximizes a relevance measure  generating a set of top solutions that are more  called generalized Bayes factor       representative  Our empirical results show that  shown in      to be able to  nd precise and concise ex   MRE methods are promising approaches for ex   planations  This paper provides a study of the theoretical  planation in Bayesian networks   properties of MRE and offers further evidence for its valid   The approach was  ity  The study shows that MRE relies on an implicit soft relevance measure that enables the automatic identi cation     Introduction  of the most relevant target variables and pruning of less relevant variables from an explanation  Furthermore  the solu   Bayesian networks offer compact and intuitive graphical  tion space of MRE has a special lattice structure that allows  representations of uncertain relations among the random  two interesting dominance relations among the solutions to  variables of a domain and provide a foundation for many  be de ned  These dominance relations are used to design  diagnostic expert systems   and develop a K MRE algorithm for  nding a set of top  However  these systems typi   cally focus on disambiguating single fault diagnostic hy   explanations that are more representative   potheses because it is hard to generate just right multiple   results show that MRE methods are promising approaches  fault hypotheses that contain only the most relevant faults   for explanation in Bayesian networks   Maximum a Posteriori  MAP  assignment and Most Probable Explanation  MPE  are two explanation methods for Bayesian networks that  nd a complete assignment to a set of target variables as the best explanation for given evidence and can be applied to generate multiple fault hypotheses  A priori  the set of target variables is often large and can be in tens or even hundreds for a real world diag   Our empirical  The remainder of the paper is structured as follows  We  rst review methods for explanation in Bayesian networks  including Most Relevant Explanation  Then we introduce several theoretical properties of Most Relevant Explanation  We also develop a K MRE algorithm for generating multiple top explanations and evaluate it empirically         YUAN ET AL     A         Input  Output  C         networks  However  they often fail to  nd just right ex   D        planations containing the most relevant target variables    a   Many existing methods make simplifying assumptions and  Input current     noCurr     def     ok      Related Work  Many methods exist for explaining evidence in Bayesian  B        B  UAI       focus on singleton explanations         However  singleton  A def    ok      explanations may be underspeci ed and are unable to fully explain given evidence  For the running example  the pos   D  C  output of B  def     ok      noCurr      output of D  A  B  C  and D failing independently                      and       respectively  Therefore   B  is the best singleton explanation    However  B alone does not fully explain the evidence  C or D has to be interior probabilities of  output of A  def     ok      current      current     are  noCurr      output of C  current     current     noCurr      noCurr      volved  Actually  if we are not focusing on faulty states    D  Total Output          is the best singleton explanation  It is clearly  not an adequate explanation for the evidence   current    noCurr      For a domain in which target variables are interdependent    b   multivariate explanations are often more natural for exFigure     a  A probabilistic digital circuit and  b  a corresponding diagnostic Bayesian network  plaining given evidence   However  existing methods of   ten produce hypotheses that are overspeci ed  MAP  nds a con guration of a set of target variables that maximize the joint posterior probability given partial evidence on the other variables      For the running example  if we set  A  B  C and D as the target variables  MAP will  nd  ABC D  as the best explanation  However  given that B and C are faulty  A and D are somewhat redundant  A Running Example  for explaining the evidence  MPE  nds an explanation with Let us  rst introduce a running example used throughout this paper   Consider the circuit in Figure   a  adapted  from          Gates  A  B  C  and  D are defective if they are  closed  The prior probabilities that the gates close independently are                    and       respectively  Connec   tions between the gates may not work properly with certain small probabilities  The circuit can be modeled with a diagnostic Bayesian network as shown in Figure   b   Nodes  A  B  C  and  D  correspond to the gates in the circuit and  each has two states  defective and ok  Others are input  even more variables  Several other approaches use the dependence relations encoded in Bayesian networks to prune independent variables            They will  nd the same  explanation as MAP because all of the target variables are dependent on the evidence  Yet several other methods measure the quality of an explanation using the likelihood of the evidence      Unfortunately they will over t and choose   A  B  C  D   as the explanation  because the  likelihood of the evidence given that all the target variables fail is almost        or output nodes and have two states  current or noCurr   There have been efforts trying to generate more appropri   Uncertainty is introduced to the model such that an output  ate explanations  Henrion and Druzdzel     assume that a  node is in state current with a certain probability less than  system has a set of pre de ned scenarios as potential ex        if its parent gate  when exists  is defective and any  planations and  nd the scenario with the highest posterior  of its other parents is in state current  Otherwise  it is  probability  Flores et al      propose to grow an explanation  in noCurr state with probability        For example  node  output of B takes state current with probability      if parent gate B is in state defective and parent Input is in state current   Input  and  T otal Output  in the  Bayesian network are both in the state current  The task is to diagnose the system and  nd the best fault hypotheses  Based on our knowledge of the domain  we know there are three basic scenarios that most likely lead to the observa   A is defective      B B and D are defective  tion       able at each step while maintaining the probability of each explanation above certain threshold  Nielsen et al      use a different measure called causal information  ow to grow  Suppose we observe that current  ows through the circuit  which means that nodes  tree incrementally by branching the most informative vari   and  C  are defective  and      the explanation trees  Because the explanations in the trees have to branch on the same variable s   they may still contain redundant variables  Finding more concise hypotheses also have been studied in model based diagnosis      The approach focus on truth based systems and cannot be easily generalized to deal with Bayesian networks     We use a variable and its negation to stand for its ok and  defective states respectively   UAI         YUAN ET AL   Most Relevant Explanation         There are two most essential properties for a good expla   A Theoretical Study       Theoretical properties of MRE  nation  First  the explanation should be precise  meaning it should explain the presence of the evidence well  Sec   We now discuss several theoretical properties of MRE   ond  the explanation should be concise and only contain the  Since MRE relies heavily on the  most relevant variables  The above discussions show that  ating its explanations  it is not surprising that these proper   existing approaches for explaining evidence in Bayesian  ties are mostly originated from  networks often generate explanations that are either under   properties can be found in the appendix   speci ed  imprecise  or overspeci ed  inconcise    GBF  GBF    measure in gener   The proofs of these  First  we note that GBF can be expressed in a different way  To address the limitations  Yuan and Lu      propose a  using the belief update ratio   method called Most Relevant Explanation  MRE  to au   De nition    The belief update ratio of  tomatically identify the most relevant target variables for  r x  k    e   is de ned as  x  k   given  e   given evidence in Bayesian networks  First  explanation in Bayesian networks is formally de ned as follows  De nition     Given a set of target variables  Bayesian network and evidence  e  X  in a  x  k  of  X  i e   X  k  X and X  k            GBF can then be expressed as the ratio between the belief update ratios of given  MRE is then de ned as follows       De nition    Let  P  x  k  e    P  x  k    on the remaining vari   ables  an explanation for the evidence is a partial instantiation  r x  k   e    X be a set of target variables   and  x  k   and alternative explanations  x  k   e  i e    e be  GBF  x  k    e     the evidence on the remaining variables in a Bayesian net   r x  k    e    r x  k    e        work  Most Relevant Explanation is the problem of  nding an explanation Bayes Factor score  x  k that has the maximum Generalized GBF  x  k   e   i e    M RE X  e   arg maxx  k  X  k X X  k    GBF  x  k   e        where  GBF  The most important property of MRE is that it is able to weigh the relative importance of multiple variables and only include the most relevant variables in explaining the given evidence  The degree of relevance is evaluated using a measure called conditional Bayes factor  CBF  implicitly  is de ned as  GBF  x  k    e    encoded in the GBF measure and de ned as follows   P  e x  k      P  e x  k          De nition    The conditional Bayes factor of hypothesis  y  m for given evidence e conditional on x  k is de ned as  Therefore  MRE traverses the trans dimensional space containing all the partial assignments of ment that maximizes the  GBF  score   Potentially  MRE  can use any measure that provides a common ground for comparing the partial instantiations of the target variables   GBF  CBF  y  m   e x  k     X and  nds an assign   is chosen because it is shown to provide a plausible  P  e y  m   x  k     P  e y  m   x  k    Then  we have the following theorem  Theorem    Let the conditional Bayes factor of y  m given  measure for representing the degree of evidential support  x  k  in recent studies in Bayesian con rmation theory       ratio of the alternative explanations  be less than or equal to inverse of the belief update  MRE was shown to be able to generate precise and con   CBF  y  m   e x  k     cise explanations for the running example       The best explanation according to MRE is   GBF  B  C  e                  e and write GBF  B  C   explanation than both  A           For simplicity we often omit  B  C   is a better       x  k   i e       r x  k   e        the following holds  GBF  x  k  y  m   e   GBF  x  k   e         and  B  D            because its prior and posterior probabilities are both relatively high  The posterior probabilspectively                         Therefore   CBF  y  m   e x  k   provides a soft measure on  re   the relevance of a new set of variable states with regard to  Therefore  MRE seems able to automatically  an existing explanation and can be used to decide whether  ities of the explanations are  and  GBF  identify the most relevant target variables and states as the  or not to include them in an existing explanation   explanations for given evidence   also encodes a decision boundary  the inverse belief update        YUAN ET AL   ratio of alternative explanations  x  k  given  e   UAI       which pro   a  A  b  B  c  C  vides a threshold on how important the remaining variables ab  should be in order to be included in the current explanation   aB  ac  aC  Ab  AB  Ac  AC  bc  bC  Bc  BC     CBF  y  m   e x  k   is greater than or equal to r x  k  e    y  m is regarded as relevant and will be included  Otherwise  y  m will be excluded from the explanation   If  abc  abC  aBc  aBC  Abc  AbC  ABc  ABC  Figure    Solution space of Most Relevant Explanation  Theorem   has several intuitive and desirable corollaries  First  the following corollary shows that  for any explanation  x  k  with belief update ratio greater than or equal to       adding any independent variable to the explanation will decrease its GBF score        x  k be an explanation with r x  k   e        and y be a state of a variable Y such that P  y x  k   e   P  y x  k    Then  Corollary    Let  x  k be an explanation with r x  k   e        and y be a state of variable Y independent from variables in x  k and e  Then  Corollary    Let  GBF  x  k   y   e   GBF  x  k   e    GBF  x  k   y   e   GBF  x  k   e              This is again an intuitive result  a variable state whose posterior probability decreases for given evidence should not be part of an explanation for the evidence   Therefore  adding an irrelevant variable dilutes the explanative power of an existing explanation  MRE is able to automatically prune such variables  This is clearly a desir   The above theoretical results can be veri ed using the running example  For example   able property  Note that we focus on the explanations with belief update ratio greater than or equal to      We believe that an explanation whose probability decreases given the evidence     GBF  B  C  GBF  B  C  A    GBF  B  C  D      GBF  B  C  A  D     is unlikely to be a good explanation for the evidence  Corollary   requires the additional variable pendent from both  X  k  and  E   Y  to be inde   The assumption is rather  strong  The following corollary relaxes it to be that conditionally independent from  E  given  X  k  Y  is  The results suggest that GBF has the intrinsic capability to penalize higher dimensional explanations and prune less relevant variables   and shows  the same result still holds        x  k be an explanation with r x  k   e        and y be a state of a variable Y conditionally independent from variables in e given x  k   Then  Corollary    Let  GBF  x  k   y   e   GBF  x  k   e          Explaining away  One unique property of Bayesian networks is that they can model the so called explaining away phenomenon using the  V  structure  i e   a single variable with two or more parents   This structure intuitively captures the situation where an effect has multiple causes  Observing the presence of the effect and one of the causes reduces the likelihood of the presence of the other causes  It is desirable to capture this  Corollary   is a more general result than corollary   and captures the intuition that conditionally independent variables add no additional information to an explanation in explaining given evidence  even though the variable may be marginally dependent on the evidence  Also note that these properties are all relative to an existing explanation  It is possible that a variable is independent from the evidence given one explanation  but becomes dependent on the evidence given another explanation   In other words   GBF score is not monotonic  Looking at variables one by one does not guarantee to  nd the optimal solution  The above results can be further relaxed to accommodate cases where the posterior probability of than its prior  i e    y given e is smaller  phenomenon when generating explanations  MRE seems able to capture the explaining away effect using CBF  CBF provides a measure on how relevant a new variable is to an existing explanation   In an explaining   away situation  if one of the causes is already present in the current explanation  other causes typically do not receive high CBF scores   Again for the running example    B  C  and  A  are both good explanations for the evidence by themselves  The CBF of A given only e  the effect  is equal to its GBF          which is rather high    B  C   one of the causes  is also obCBF  A  e B  C  becomes rather low and is only equal to       Clearly  CBF is able to capture the exHowever  when served   plaining away phenomenon in this example    UAI           YUAN ET AL       GBF  B    C           B   C          GBF  B   C  D          GBF A   B   C  D           Dominance relations  GBF A   MRE has a solution space with an interesting lattice structure similar to the graph in Figure   for three binary target  GBF  A          GBF  A  B          GBF  A  C          GBF  B    D           variables  The graph contains all the partial assignments of the target variables  Two explanations are linked together  Table    The top solutions ranked by GBF  The solutions in  if they only have a local difference  meaning they either  boldface are the top minimal solutions   have the same set of variables with one variable in different states  or one explanation has one fewer variable than the other explanation with all the other variables being in the same states  There are two dominance relations among these potential solutions that are implied by Figure    The  rst concept is strong dominance  De nition    An explanation  x  k  other explanation y  m if and GBF  x  k    GBF  y  m    If  x  k  strongly dominates an   only if  x  k  y  m  and  y  m   x  k is clearly a better y  m   because it not only has a no worse  strongly dominates  explanation than  explanative score but also is more concise  We only need to consider  x  k  when  nding multiple top MRE explana   tions  The second concept is weak dominance  De nition    An explanation  x  k  y  m if and GBF  x  k     GBF  y  m    other explanation  In this case   x  k  the solutions  The dominance relations de ned in the last section allow us to develop a K MRE algorithm to  nd a set of top solutions that are more representative   Let us look at the  running example again to illustrate the idea   The expla   nations in Table   have the highest GBF scores  If we simply select top three explanations solely based on GBF  we will obtain these rather similar explanations   B  C    A  B  C   and  B  C  D   which are rather similar  Since  A  B  C    B  C  D   and  A  B  C  D  are strongly dominated by  B  C   we should only consider  B  C  out of those four explanations  Similarly   A  B  and  A  C  are strongly dominated by  A   These dominated explanations should be excluded from the  weakly dominates an   only if  output all the top solutions rather than selecting any one of  x  k  y  m  and  top solution set  In the end  we get the set of top explanations shown in boldface in Table    which is clearly more diverse and representative than the original set  MAP and  has a strictly larger  GBF  score than  MPE clearly do not have this nice property   but the latter is more concise  It is possible that we  Therefore  our proposed K MRE algorithm works as fol   can include them both and let the decision makers to decide  lows  Whenever we generate a new explanation  we check  whether they prefer higher score or conciseness  However   its score against the best solution pool  If it is lower than  y  m    we believe that we only need to include  x  k    because its  the worst score in the pool  reject the new explanation  If  K  higher GBF score indicates that the extra variable states are  there are fewer than  relevant to explain given evidence and should be included  new explanation is higher than the worst score in the pool   in the explanation   we consider adding the new explanation to the top pool  We  Based on the two kinds of dominance relations  we de ne the concept minimal   best solutions or if the score of the   rst check whether the new solution is strongly or weakly dominated by any of the top explanations  If so  reject the new explanation  Otherwise  we add the new explanation  De nition    An explanation is minimal if it is neither  to the top pool  However  we then need to check whether  strongly nor weakly dominated by any other explanation   there are existing top explanations that are dominated by the newly added explanation  If yes  these existing expla   In case we want to  nd multiple top explanations  we only  nations should be excluded  Otherwise we delete the top  need to consider the minimal explanations  because they  explanation with the least score   are the most representative ones         K MRE Algorithm       Empirical Results Experimental design  In many decision problems  outputting the single top solution may not be the best practice   Decision makers typ   We tested the K MRE algorithm on a set of benchmark  ically would like multiple competing options to choose  models  including Alarm  Circuit  Hepar  Munin  and  from  This is especially important when there are multi   SmallHepar  We chose these several models because we  ple solutions that are almost equally good  For the circuit  have the diagnostic versions of these networks  whose vari   example  all three basic explanations will lead to the same  ables have been annotated into three categories  target  ob   observation  However  we can only recover one explana   servation  and auxiliary  For generating the test cases  we  tion if we are satis ed with one top solution  It is better to  used the networks as generative models and sampled with         YUAN ET AL   UAI       Precision       K   F                                                                                                                                        Singleton F MAP       P MAP MRE                 Recall  K   F   K   F                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       K          F                   K          F                                                                                                       Singleton  P MAP                           F MAP  MRE                                               I rate     K F                                                                        Alarm  Circuit  Hepar  Munin  SmallHepar  Figure    Precision vs recall plots of the results by four algorithms  Marginal  P MAP  F MAP  and MRE  on a set of benchmark diagnostic Bayesian networks K shows the number of top solutions generated  F shows the least number of faulty target variables in test cases  F Score shows the F Scores of the results of the algorithms  Marginal algorithm did not appear in rows K F  and K F  because it has only one solution   out replacement from their prior probability distributions   centage of faulty states correctly identi ed among all faulty  We only kept those test cases with at least one abnormal ob   explanation variables  and recall  the percentage of faulty  servation and used the abnormal observations as evidence   states correctly identi ed among all faulty variables in test  Since Circuit and SmallHepar have   and   target variables  cases  of these algorithms in Figure     respectively  we collected as many test cases as possible   sample results on F Score  which is de ned as  Munin also has   target variables but each with many more states  Hepar and Alarm have   and    target variables re   F Score  spectively  We collected    test cases for the last three net      We also include      precision  recall     precision   recall         works  We also extracted from them the test cases which contain at least two faulty target variables for separate ex        Results and analysis  periments on multiple fault test cases  Our experiments compared MRE with MAP given their similarities  We tested two versions of the MAP algorithm  one focusing on all the target variables  F MAP  and the other only on the target variables selected by MRE  PMAP   In addition  we compared with the Marginal algorithm  which neglects the interdependence among the target variables and uses the marginal posterior probabilities to determine the most likely states of the target variables  We plot the accuracy statistics  including precision  the per   We make the following observations from these results  First  MRE is able to achieve higher precision and or recall rates in identifying the faulty target variables than the other algorithms on all the networks except Munin  An outstanding example is the SmallHepar network  Marginal  F MAP and P MAP all failed badly on this model in identifying the faulty variables  while MRE was able to achieve reasonable performance  It is clearly desirable given that one major goal of diagnosis or explanation is to identify problems  e g   faulty states   We investigated the results of Munin   UAI       YUAN ET AL   network further and found that all target variables of these         Concluding Remarks  test cases are in faulty states  Marginal and F MAP have exactly the same statistics  which suggests that the target  In this paper  we discuss several theoretical properties of  variables may have weak correlations with each other  This  Most Relevant Explanation  MRE  and develop an algo   puts MRE in disadvantage because MRE takes into account  rithm for  nding multiple top MRE solutions  Our study  such weak correlations and generate concise explanations  shows that MRE relies on an implicit soft relevance mea   with fewer target variables  On average  the explanations of  sure in automatically identifying the most relevant target      variables out of    target variables for Alarm        for Circuit      for Hepar        for Munin  and       for SmallHepar  For networks with strong correMRE identi es  variables and pruning less relevant variables from an explanation   The soft measure also enables MRE to cap   ture the intuitive phenomenon of explaining away encoded  lations among the target variables  e g  Circuit and Hepar   in Bayesian networks  Furthermore  we de ne two dom   MRE has much higher precision recall rates  The sample  inance relations among the explanations that are implied  F score results in the case of K F  further con rmed the  by the structure of the solution space of MRE  These rela   observation   tions allow us to design and develop a K MRE algorithm  Second  by comparing rows K F  vs  K F  vs   K F  and  K F   we found that using multiple top  for  nding top MRE solutions that are much more representative   solutions helps MRE signi cantly in improving the preci   Our empirical results agree quite well with the theoretical  sion recall rates than the other algorithms  With multiple  understanding of MRE  The results show that MRE is ef   solutions  we kept the results with the maximum precision  fective in identifying the most relevant target variables  es   rates  The results seem to support our claim that K MRE  pecially the true faulty target variables  Furthermore  K   was able to generate solutions that are more representative   MRE seems able to generate more representative top ex   It is somewhat surprising that the precision recall rates of  planations than K MAP methods   F MAP were not improved at all on the networks  but those  is especially suitable for systems in which target variables  of P MAP were improved  Our hypothesis is that  since the  are strong correlated with each other and can generate more  explanations by F MAP are more grained because more  precise and concise explanations for these systems   variables are involved  its top explanations tend to agree with each other on the faulty variables and differ mostly in the less important non faulty variables  Generating multiple top solutions could not really help F MAP much in improving its accuracy statistics   We believe that MRE  This research has many future works  It is desirable to understand the theoretical complexity of MRE  It has a solution space even larger than MAP and is believed to be at least as hard  Currently we rely on an exhaustive search algorithm for solving MRE and K MRE  More ef cient  Third  although P MAP gets the target variables identi ed  methods for solving MRE need be developed to make it  by MRE as input  it still failed badly on the SmallHepar  applicable to large real world problems   network in identifying faulty states of the target variables  It did not show any signi cant advantage over F MAP on  Acknowledgement  other networks either  The results suggest that relying on  National Science Foundation grant IIS           posterior probabilities may not work well in certain diag   experimental data have been obtained using SMILE  a  nostic systems   Bayesian inference engine developed at the Decision Sys   Fourth  although multiple fault cases are believed to be  tems Laboratory at University of Pittsburgh and available  more dif cult because of their low likelihood  the algo   at  This research was supported by the All  http   genie sis pitt edu   rithms in our experiments seem able to maintain the same level of accuracy rates in face of multiple fault test cases  rows K F  and K F     
 A branch and bound approach to solving influence diagrams has been previously proposed in the literature  but appears to have never been implemented and evaluated  apparently due to the difficulties of computing effective bounds for the branch and bound search  In this paper  we describe how to efficiently compute effective bounds  and we develop a practical implementation of depth first branch and bound search for influence diagram evaluation that outperforms existing methods for solving influence diagrams with multiple stages      Introduction  An influence diagram     is a compact representation of the relations among random variables  decisions  and preferences in a domain that provides a framework for decision making under uncertainty  Many algorithms have been developed to solve influence diagrams                                Most of these algorithms  whether they build a secondary structure or not  are based on the bottom up dynamic programming approach  They start by solving small low level decision problems and gradually build on these results to solve larger problems until the solution to the global level decision problem is found  The drawback of these methods is that they can waste computation in solving decision scenarios that have zero probability or that are unreachable from any initial state by following an optimal decision policy  This drawback can be overcome by adopting a branch andbound approach to solving an influence diagram that uses a search tree to represent all possible decision scenarios  This approach can use upper bounds on maximum utility to prune branches of the search tree that correspond to lowquality decisions that cannot be part of an optimal policy  it can also prune branches that have zero probability   A branch and bound approach to influence diagram evaluation appears to have been first suggested by Pearl       He proposed it as an improvement over the classic method of unfolding an influence diagram into a decision tree and solving it using the rollback method  which itself is a form of dynamic programming      In Pearls words  A hybrid method of evaluating influence diagrams naturally suggests itself  It is based on the realization that decision trees need not actually be generated and stored in their totality to produce the optimal policy  A decision tree can also be evaluated by traversing it in a depth first  backtracking manner using a meager amount of storage space  proportional to the depth of the tree   Moreover  branch and bound techniques can be employed to prune the search space and permit an evaluation without exhaustively traversing the entire tree    an influence diagram can be evaluated by sequentially instantiating the decision and observation nodes  in chronological order  while treating the remaining chance nodes as a Bayesian network that supplies the probabilistic parameters necessary for tree evaluation   p       However  neither Pearl nor anyone else appears to have followed up on this suggestion and implemented such an algorithm  The apparent reason is the difficulty of computing effective bounds to prune the search tree  Qi and Poole      proposed a similar search based method for solving influence diagrams  but with no method for computing bounds  in fact  their implementation relied on the trivial infinity upper bound to guide the search  Recently  Marinescu      proposed a related search based approach to influence diagram evaluation  But again  he proposed no method for computing bounds  his implementation relies on brute force search  Even without bounds to prune the search space  note that both Qi and Poole and Marinescu argue that a search based approach has advantages  for example  it can prune branches that have zero probability    In this paper  we describe an implemented depth first branch and bound search algorithm for influence diagram evaluation that includes efficient techniques for computing bounds to prune the search tree  To compute effective bounds  our algorithm adapts and integrates two previous contributions  First  we adapt the work of Nilsson and Hohle      on computing an upper bound on the maximum expected utility of an influence diagram  The motivation for their work was to bound the quality of strategies found by an approximation algorithm for solving limitedmemory influence diagrams  and their bounds are not in a form that can be directly used for branch and bound search  We show how to adapt their approach to branchand bound search  Second  we adapt the recent work of Yuan and Hansen      on solving the MAP problem for Bayesian networks using branch and bound search  Their work describes an incremental method for computing upper bounds based on join tree evaluation that we show allows such bounds to be computed efficiently during branch andbound search  In addition  we describe some novel methods for constructing the search tree and computing probabilities and bounds that contribute to an efficient implementation  Our experimental results show that this approach leads to an exact algorithm for solving influence diagrams that outperforms existing methods for solving multistage influence diagrams      Background  We begin with a brief review of influence diagrams and algorithms for solving them  We also introduce an example of multi stage decision making that will serve to illustrate the results of the paper       Influence Diagrams  An influence diagram is a directed acyclic graph G containing variables V of a decision domain  The variables can be classified into three groups  V   X  D  U  where X is the set of oval shaped chance variables that specify the uncertain decision environment  D is the set of square shaped decision variables that specify the possible decisions to be made in the domain  and U are the diamond shaped utility variables representing a decision makers preferences  As in a Bayesian network  each chance variable Xi  X is associated with a conditional probability distribution P  Xi  P a Xi     where P a Xi   is the set of parents of Xi in G  Each decision variable Dj  D has multiple information states  where an information state is an instantiation of the variables with arcs leading into Dj   the selected action is conditioned on the information state  Incoming arcs into a decision variable are called information arcs  variables at the origin of these arcs are assumed to be observed before the decision is made  These variables are called the information variables of the decision  No forgetting is typically  assumed for an influence diagram  which means the information variables of earlier decisions are also information variables of later decisions  We call these past information variables the history  and  for convenience  we assume that there are explicit information arcs from history information variables to decision variables  Finally  each utility node Ui  U represents a function that maps each configuration of its parents to a utility value the represents the preference of the decision maker   Utility variables typically do not have other variables as children except multi attribute utility super value variables   The decision variables in an influence diagram are typically assumed to be temporally ordered  i e   the decisions have to be made in a particular order  Suppose there are n decision variables D    D         Dn in an influence diagram  The decision variables partition the variables in X into a collection of disjoint sets I    I         In   For each k  where     k   n  Ik is the set of chance variables that must be observed between Dk and Dk     I  is the set of initial evidence variables that must be observed before D    In is the set of variables left unobserved when decision Dn is made  Therefore  a partial order  is defined on the influence diagram over X  D  as follows  I   D   I        Dn  In         A solution to the decision problem defined by an influence diagram is a series of decision rules for the decision variables  A decision rule for Dk is a mapping from each configuration of its parents to one of the actions defined by the decision variable  A decision policy  or strategy  is a series of decision rules with one decision rule for each decision variable  The goal of solving an influence diagram is to find an optimal decision policy that maximizes the expected utility  The maximum expected utility is equal to     max P  X D  Uj  P a Uj     max     I   D   In   Dn  In  j  In general  the summations and maximizations are not commutable  The methods presented in Section     differ in the various techniques they use to carry out the summations and maximizations in this order  Recent research has begun to relax the assumption of ordered decisions  In particular  Jensen proposes the framework of unconstrained influence diagrams to allow a partial ordering among the decisions      Other research relaxes the no forgetting assumption  in particular  the framework of limited memory influence diagrams       Although the approach we develop can be extended to these frameworks  we do not consider the extension in this paper       Example  To illustrate decision making using multi stage influence diagrams  consider a simple maze navigation problem              ns        x        ns        es        x                a   Figure    Two maze domains  A star represents the goal       Figure   shows four instances of the problem  The shaded tiles represent walls  the white tiles represent movable space  and the white tiles with a star represent goal states  An agent is randomly placed in a non goal state  It has five available actions that it can use to move toward the goal  it can move a single step in any of the four compass directions  or it can stay in place  The effect of a movement action is stochastic  The agent successfully moves in the intended direction with probability       It fails to move with probability        it moves sideways with probability            for each side   and it moves backward with probability        If movement in some direction would take it into a wall  that movement has probability zero  and the remaining probabilities are normalized  The agent has four sensors  one for each direction  which sense whether the neighboring tile in that direction is a wall  Each sensor is noisy  it detects the presence of a wall correctly with probability     and mistakenly senses a wall when none is present with probability       The agent chooses an action at each of a sequence of stages  If the agent is in the goal state after the final stage  it receives a utility value of      otherwise  it receives a utility value of      Figure   a  shows the influence diagram for a two stage version of the maze problem  The variables xi and yi represent the location coordinates of the agent at time i  the variables  nsi   esi   ssi   wsi   are the sensor readings in four directions at the same time point  and the variable di represents the action taken by the agent  The utility variable u assigns a value depending on whether or not the agent is in the goal state after the last action is taken  Since the same variables occur at each stage  we can make the influence diagram arbitrarily large by increasing the number of stages       Evaluation algorithms  Many approaches have been developed for solving influence diagrams  The simplest is to unfold an influence diagram into an equivalent decision tree and solve it in that form      Another approach called arc reversal solves an influence diagram directly using techniques such as arcreversal and node removal           when a decision variable is removed  we obtain the optimal decision rule for the decision  Several methods reduce influence diagrams into  y         y       ss        ss         ws        ws          b   u  d         d       y        x         es          a  R                                                  b  Figure     a  An example influence diagram and  b  its strong join tree  The numbers in both figures stand for the indices of the variables  The node with R is the strong root   Bayesian networks by converting decision nodes into random variables such that the solution of an inference problem in the Bayesian network correspond to the optimal decision policy for the influence diagram          Another method      transforms an influence diagram into a valuation network and applies variable elimination to solve the valuation network  Recent work compiles influence diagrams into decision circuits and uses the decision circuits to compute optimal policies      this approach takes advantage of local structure present in an influence diagram  such as deterministic relations  In the following  we describe a state of the art method for solving an influence diagram using a strong join tree      This method is viewed by many as the fastest general algorithm  and we use its performance as a benchmark for our branch and bound approach  A join tree is strong if it has at least one clique  R  called the strong root  such that for any pair of adjacent cliques  C  and C    with C  closer to R than C    the variables in separator S   C   C  must appear earlier in the partial order defined in Equation     than C    C    A strong join tree for the influence diagram in Figure   a  is shown in Figure   b   An influence diagram can be solved exactly by message passing on the strong join tree  Each clique C in the join tree contains two potentials  a probability potential C and a utility potential C   For clique C  to send a message to clique C    C  and C  should be updated as follows        C    C   S   C   C       S   S   where S      C    S   max C   C     C   S  C   S  information arcs from each variable in R to D is guaranteed to have a maximum expected utility that is greater than or equal to the maximum expected utility for G  We call G an upper bound influence diagram for G   In contrast to the join tree algorithm for Bayesian networks       only the collection phase of the algorithm is needed to solve an influence diagram  After the collection phase  we can obtain the maximum expected utility by carrying out the remaining summations and maximizations in the root  In addition  we can extract the optimal decision rules for the decision variables from some of the cliques that contain these variables   Use of an upper bound influence diagram to compute bounds only makes sense if the upper bound influence diagram is simpler and much easier to solve than the original influence diagram  In fact  adding information arcs to an influence diagram can simplify its evaluation by making some other information variables non requisite  An information variable Ii is said to be non requisite          for a decision node D if  Building a join tree for a Bayesian network may fail if the join tree is too large to fit in memory  This is also true for influence diagrams  In fact  the memory requirement of a strong join tree for an influence diagram is even higher because of the constrained order in Equation      Consequently  the join tree algorithm is typically infeasible for solving all but very small influence diagrams   Ii   U  de D   D   P a D     Ii         Computing bounds  To implement a branch and bound algorithm for solving influence diagrams  we need a method for computing bounds  in particular  for computing upper bounds on the utility that can be achieved beginning at a particular stage of the problem  given the history up to that stage  A trivial upper bound is the largest state dependent value of the utility node of the influence diagram  In this section  we discuss how to compute more informative bounds  There has been little previous work on this topic  Nilsson and Hohle      develop an approach to bounding the suboptimality of policies for limited memory influence diagrams that are found by an approximation algorithm  Dechter     describes an approach to computing upper bounds in an influence diagram that is based on mini bucket partitioning  Neither work considers how to use these bounds in a branch and bound search algorithm  The approach we develop in the rest of this paper is based on the work of Nilsson and Hohle       which we extend by showing how it can be used to compute bounds for branchand bound search  The general strategy is to create an influence diagram with a value that is guaranteed to be an upper bound on the value of the original influence diagram  but that is also much easier to solve  We use the fact that additional information can only increase the value of an influence diagram  Since this reflects the well known fact that information value is non negative  we omit  for space reasons  a proof of the following theorem  Theorem    Let G be an influence diagram and D a decision variable in G  Let I be Ds information variables and R another set of random variables in G that are nondescendants of D  Then the influence diagram G that results from making R into information variables by adding       where de D  are the descendants of D  A reduction of an influence diagram is obtained by deleting all the nonrequisite information arcs       Because of the no forgetting assumption  a decision variable at a late stage may have a large number of history information variables  For decision making under imperfect information  all of these information variables are typically requisite  As a result  the size of the decision rules grows exponentially as the number of decision stages increases  which is the primary reason multi stage influence diagrams are very difficult to solve  In constructing an upper bound influence diagram  we want to add information arcs that make some or all of the history information variables for a decision node non requisite  in order to simplify the influence diagram and make it easier to solve  We adopt the strategy proposed by Nilsson and Hohle       Let nd X  be the non descendant variables of variable X  let f a X    P a X    X  be the family of variable X  i e   the variable and its parents   let f a X    Xi X f a Xi   be the family of the set of variables X  i e   the variables and their parents   and let j be  D       Dj   be a sequence of decision variables from stage   to stage j  The following theorem is proved by Nilsson and Hohle       Theorem    For an influence diagram with the constrained order in Equation      if we add to each decision variable Dj the following new information variables in the order of j   n          Nj   arg minB Bj nd Dj      B  f a j     U  de Dj     B   Dj            where    UD j k  Bj   ki j    n  V  n   U  de Dj    f a Di    j k  the following holds for any Dj in the resulting influence diagram   de Dj    U   f a i    f a Dj           ns          Past state     Sufficient Information D Set  Future state  U     x        ns        es        x        ss        ss         ws        ws          a   What this theorem means is that for each decision variable Dj in an influence diagram  there is a set of information variables Nj such that the optimal policy for Dj depends only on these information variables  and is otherwise history independent  Note that the set Nj for decision variable Dj can contain both information variables from the original influence diagram and information variables created by adding new information arcs to the diagram  The set Nj of information variables can be interpreted as the current state of the decision problem  such that if the decision maker knows the current state  it does not need to know any previous history in order to select an optimal action  in this sense  the state satisfies the Markov property    b   We call Nj a sufficient information set  SIS  for Dj   The intuition behind this approach is illustrated by Figure    The shaded oval shows the SIS set ND for decision D  The past state affects the variables in ND   D   illustrated by the arc labeled    and ND   D  affects the future state  as illustrated by arc    The future state further determines the values of the utility variables  ND   D  d separates the past and future states and prevents the direct influence shown by arc    The concept of a sufficient information set is related to the concept of extremality  as defined in       and the concept of blocking  as defined in       To construct an upper bound influence diagram  we find the SIS set for each decision in the order of Dn        D  and make the variables in each SIS set information variables for the corresponding decisions  We then delete the nonrequisite information arcs  Consider the influence diagram in Figure   a  as an example  The information set for d  is originally  ns    es    ss    ws    d    ns    es    ss    ws     We find that its sufficient information  SIS  set is  x    y     We  y         y        Figure    Relations between past and future information states and the minimum sufficient information set   Consider a decision making problem in a partially observable domain  such as the maze domain of Section      The agent cannot directly observe its location in the maze and must rely on imperfect sensor readings to infer its location  In this domain  adding information arcs from the location variables to a decision variable  so that the agent know the current location at the time it chooses an action  makes both current and history sensor readings non requisite  which results in a much simpler influence diagram  which in this case serves as an upper bound influence diagram   u  d         d       y        x         es         Figure     a  the upper bound influence diagram for the diagram in Figure   a   and  b  its strong join tree   also find that the SIS set for d  is  x    y     By making  x    y    and  x    y    information variables for d  and d  respectively  and reducing the influence diagram  we obtain the much simpler influence diagram in Figure   a   The strong join tree for the new influence diagram is shown in Figure   b   which is also much smaller than the strong join tree for the original model  Since the upper bound influence diagram assumes the actual location is directly observable to the agent  it effectively transforms a partially observable decision problem into a fully observable one  The resulting influence diagram and  hence  its join tree is much easier to solve  Finding the sufficient information set  SIS  for a decision variable in an influence diagram is equivalent to finding a minimum separating set in the moralized graph of the influence diagram       Acid and de Campos     propose an algorithm based on the Max flow Min cut algorithm     for finding a minimum separating set between two sets of nodes in a Bayesian network with some of the separating variables being fixed  We use their algorithm to find the SIS sets  The two sets of nodes are f a j   and U  de Dj    The only fixed separating variable is Dj   The algorithm first introduces two dummy variables  source and sink  to the moralized graph  The source is connected to the neighboring variables of f a j    and the sink to the variables in de Dj    an U  de Dj     We then create a max flow network out of the undirected graph by assigning each edge capacity      A solution gives a minimum separating set between the sink and source that contains Dj   We briefly mention some issues that are not described by Nilsson and Hohle       but that need to be considered in an implementation  The first issue is how to define the size of an SIS set  Theorem   uses the cardinality of the SIS set as the minimization criterion  Another viable choice is to use weight  defined as the product of number of states  of the variables in an SIS set as the minimization criterion    The relation between these two criteria is similar to the relation between treewidth and weight in constructing a junction tree  While treewidth tells us how complex a Bayesian network is at the structure level  weight provides an idea on how large the potentials of the junction tree are at the quantitative level  Both methods have been used  In our implementation  we use the cardinality  A second issue is that multiple candidate SIS sets may exist for a decision variable  In that case  we need some criterion for selecting the best one  In our implementation  we select the candidate SIS set that is closest to the descendant utility variables of the decision  Note that other candidate sets are all d separated from the utility node by the closest SIS set  This choice has the advantage that the resulting influence diagram is easiest to evaluate  however  other choices may result in an influence diagram that gives a tighter bound      Branch and bound search  In this section  we describe how to use the upper bound influence diagram to compute bounds for a depth first branch and bound search algorithm that solves the original influence diagram  We begin by showing how to represent the search space as an AND OR tree  A naive approach to computing bounds requires evaluating the entire upper bound influence diagram at each OR node of the search tree  which is computationally prohibitive  To make branch and bound search feasible  we rely on an incremental approach to computing bounds proposed by Yuan and Hansen      for solving the MAP problem in Bayesian networks using branch and bound search  We show how to adapt that approach in order to solve influence diagrams efficiently       AND OR tree search  We represent the search space for influence diagram evaluation as an AND OR tree  The nodes in an AND OR tree are of two types  AND nodes and OR nodes  AND nodes correspond to chance variables  a probability is associated with each arc originating from an AND node and the probabilities of all the arcs from an AND node sum to      The OR nodes correspond to decision variables  Each of the leaf nodes of the tree has a utility value that is derived from the utility node of the influence diagram  Qi and Poole      create an AND OR tree in which each layer of AND nodes alternates with a layer of OR nodes  Each AND node in this tree corresponds to the information set of a decision variable in the influence diagram  which is a set of information variables  To compute the probability for each arc emanating from an AND node in this tree  however  it is necessary to have the joint probability distributions of all the information sets  these are often not readily available  since variables in the same informa   ns  es  ss   ws  d   es  ss   ss   ws   ws   d   d   ss   ws  d   Figure    The AND OR tree used in our approach  Ovalshaped nodes are AND nodes  and square shaped nodes are OR nodes  tion set can belong to different clusters of a join tree  For computational convenience  our AND OR tree is based on the structure of the strong join tree in Figure   b   For the maze example  we order the variables in the information set  ns    es    ss    ws    according to the order in Equation      Note that the join tree does not have a clique that contains all four variables  in fact  they are all in different cliques  So we consider the variables one by one  That means that our AND OR tree allows several layers of AND nodes to alternate with a layer of OR nodes  See Figure   for an example of the kind of AND OR tree constructed by our search algorithm  and note that the first four layers of this AND OR tree are all AND layers  Each path from the root of the AND OR tree to a leaf corresponds to a complete instantiation of the information variables and decision variables of the influence diagram  that is  a complete history  Since we use the AND OR tree to find an optimal decision policy for the original influence diagram  we have to construct an AND OR tree that is consistent with the original constrained order  For the influence diagram in Figure   a   the partial order is  ns    es    ss    ws      d      ns    es    ss    ws      d      x    y    x    y    x    y    u       and the decision variables must occur in this order along any branch  We define a valuation function for each node in an AND OR tree as follows   a  for a leaf node  the value is its utility value   b  for an AND node  the value of is the sum of the values of its child nodes weighted by the probabilities of the outgoing arcs   c  for an OR node  the value is the maximum of the summed utility values of each child node and corresponding arc  We use this valuation function to find an optimal strategy for the influence diagram  We represent a strategy for a multi stage influence diagram as a policy tree that is defined as follows  A policy tree of an AND OR tree is a subtree such that   a  it consists of the root of the AND OR tree   b  if a non terminal AND node is in the policy tree  all its children are in the policy tree  and  c  if a non terminal OR node is in the policy   tree  exactly one of its children is in the policy tree  Given an AND OR tree that represents all possible histories and strategies for an influence diagram  the influence diagram is solved by finding a policy tree with the maximum value at the root  where the value of the policy tree is computed based on the valuation function  Depth first branch andbound search can be used to find an optimal policy tree  The AND OR tree is constructed on the fly during the branch and bound search  and it is important to do so in a way that allows the probabilities and values to be computed as efficiently as possible  We use the maze problem and the AND OR tree in Figure   as an example   The upper bound influence diagram and join tree are shown in Figure     We have already pointed out that including more layers in our AND OR tree allows us to more easily use the probabilities and utilities computed by the join tree  If we start by expanding ns   where expanding a node refers to generating its child nodes in the AND OR tree   we need the probabilities of P  ns    to label the outgoing arcs  We can readily look up the probabilities from clique           after an initial full join tree evaluation  Note that we use the join tree of the upper bound influence diagram to compute the probabilities  We can do that because these probabilities are the same as those computed from the original influence diagram  This is due to the fact that the same set of actions will reduce both models into the same Bayesian networks  Adding information arcs to an influence diagram  in order to create an upper bound influence diagram  only changes the expected utilities of the decision variables  After expanding ns    we expand any of  es    ss    ws     Suppose the next variable is es    we need the conditional probabilities of es  given ns    These probabilities can be computed by setting the state of ns  as new evidence to the join tree and evaluating the join tree again  The same process is used in expanding  ss    ws     Note that we do not have to expand one variable at a time  If a clique has multiple variables in the same information set  the variables can be expanded together because a joint probability distribution over them can be easily computed  Expanding them together also saves the need to do marginalization  For example  variables x    y    x    y              are in the same information group and also reside in a same clique  In this case  we can expand them as a single layer in the AND OR tree  After  ns    es    ss    ws    are expanded  we expand d  as an OR layer  This is where the upper bounds are needed  We set the states of  ns    es    ss    ws    as evidence to the join tree and compute the expected utility values for d  by reevaluating the join tree  The expected utilities of d  are upper bounds for the same decision scenarios of the original model  We can use the upper bounds to select the most promising decision alternative to search first  The exact value will be returned once the subtree is searched  If the  value is higher than the upper bounds of the remaining decision alternatives  these alternatives are immediately pruned because they cannot be part of an optimal decision policy  We repeat the above process until a complete policy tree is found       Incremental join tree evaluation  It is clear that repeated join tree evaluation has to be performed in computing the upper bounds and conditional probabilities  A naive approach is at each time to set the states of instantiated variables as evidence and perform a full join tree evaluation  However  that is too costly  We can use an efficient incremental join tree evaluation method to compute the probabilities and upper bound utilities  based on the incremental join tree evaluation method proposed by Yuan and Hansen      for solving the MAP problem  The key idea is that we can choose a particular order of the variables that satisfies the constraints of Equation     such that an incremental join tree evaluation scheme can be used to compute the information  Given such an order  we know exactly which variables have been searched and which variable will be searched next at each search step  We only need to send messages from the parts of the join tree that contain the already searched variables to a clique with the next search variable  For example  after we search es    the only message needs to be sent to obtain P  ns   es    is the message from clique           to            There is no need to evaluate the whole join tree  If we choose the following search order for the maze problem ns    es    ss    ws    d    ns    es    ss    ws    d    x    y    x    y    x    y    we can use an incremental message passing in the direction of the dashed arc in Figure   b  to compute the probabilities and upper bounds needed in one downward pass of a depthfirst search  Both message collection and distribution are needed in this new scheme  unlike evaluating a strong join tree for the original influence diagram  The messages being propagated contain two parts  utility potentials and probability potentials  The distribution phase is typically needed to compute the conditional probabilities  For example  suppose we decide to expand es  before ns    we have to send a message from clique           to           to obtain P  ns   es     We only need to send the probability potential in message distribution  We do not need to send utility potentials because past payoffs do not count towards the expected utilities of future decisions       Efficient backtracking  We use depth first branch and bound  DFBnB  to utilize the efficient incremental bound computation  Depth first   a  b  stages                  Join tree time mem                                       time      s      s      m  s         s      s      m  s     mem                                         DFBnB policy                                                             bounds                                                              zeros                  Exhaustive search time mem   zeros             s            m  s                       s            m  s              Table    Comparison of three algorithms  join tree algorithm  DFBnB using the join tree bounds  and exhaustive search of the AND OR tree  in solving maze problems a and b for different numbers of stages  The symbol   indicates the problem could not be solved  due to memory limits in the case of the join tree algorithm  or due to a three hour time limit in the case of the search algorithms  Running time is in hours  h   minutes  m   seconds  s  and milliseconds  Memory  mem   is in megabytes and is the peak amount of memory used by the algorithm  policy is the count of nodes  both OR and AND nodes  in the part of the search tree containing the optimal policy tree  it is the same for both DFBnB and exhaustive search   bounds is the count of times a branch from an OR node was pruned based on bounds   zeros is the count of times a branch from an AND node was pruned because it had zero probability  search makes sure that the search need not jump to a different search branch before backtracking is needed  In other words  the join tree only needs to work with one search history at a time  We do need to backtrack to a previous search node once we finish a search branch or realize that a search branch is not promising and should be pruned  We need to retract all the newly set evidence since the generation of that search node and restore the join tree to a previous state  One way to achieve this is to reinitialize the join tree with correct evidence and perform a full join tree evaluation  which we have already pointed out is too costly  Instead  we cache the potentials and separators along the message propagation path before changing them by either setting evidence or overriding them with new messages  When backtracking  we simply restore the most recently cached potentials and separators in the reverse order  The join tree will be restored to the previous state with no additional computations  This backtracking method is much more efficient than reevaluating the whole join tree  For solving the MAP problem for Bayesian networks  Yuan and Hansen      show that the incremental method is more than ten times faster than full join tree evaluation in depth first branchand bound search      Empirical Evaluation  We compared the performance of our algorithm against both the join tree algorithm     and exhaustive search of the AND OR tree  i e   no bounds are used for pruning   Experiments were run on a     GHz Duo processor with   gigabytes of RAM running Windows XP  The results in Table   are for the two maze problems in Figure    which we solved for different numbers of stages  When there are only two or three stages  the join tree al   gorithm is most efficient  This is because the strong join trees for these influence diagrams are rather small and can be built successfully  Once the join trees are built  only one collection phase is necessary to solve the influence diagram  by contrast  the depth first branch and bound algorithm  DFBnB  algorithm must perform repeated message propagations to compute upper bounds and probabilities during the search  For more then three stages  however  the join tree algorithm cannot solve the maze models because the strong join trees are too large to fit in memory  Because the exhaustive search algorithm only needs to store the search tree and policy  it can solve the maze models for up to four stages  although it takes more then    minutes to do so  The DFBnB algorithm can solve the maze models for up to five stages in less time than it takes the exhaustive search algorithm to solve them for four stages  demonstrating the advantage of using bounds to prune the search tree  Table   includes the number of times a branch of the search tree is pruned based on bounds  as well as the number of times a branch with zero probability is pruned  For the maze models with their original parameter settings  every branch of the search tree has non zero probability  Previous work has argued that one of the advantages of search algorithms for influence diagram evaluation is that they can prune branches of the search tree that have zero probability  even without bounds           To test this argument  as well as to illustrate the effect of different problem characteristics on algorithm performance  we modified the maze models described in Section      First  we removed some noise from the sensors  Each of the four sensors reports a wall in the corresponding direction of the compass  In the original problem  each sensor is noisy  it detects the presence of a wall correctly with probability     and mistakenly senses a wall when none is present with probability       As a result  every sensor reading is possible in every state and there are no zero probability branches  We   stages                      a  b  a  b  stages                          Join tree time mem                                       Maze domains modified by removing some noise from sensors DFBnB Exhaustive search time mem  policy  bounds  zeros time mem   zeros                                s                                      s                          s                m  s                             m  s                  m  s                                 h m  s                                                 s                         s                s                           s                m  s                              m  s                   m  s                                  h  m  s                     Maze domains modified by removing some noise from both actions and sensors Join tree DFBnB Exhaustive search time mem  time mem  policy  bounds  zeros time mem   zeros                                                                       s               s                           s                 s                           m  s                 m  s                               m  s                      m  s                                                                                                     s               s                           s                 s                            m  s                 m  s                               m  s                      m  s                                    Table    Comparison of three algorithms  join tree algorithm  DFBnB using the join tree bounds  and exhaustive search of the AND OR tree  in solving maze problems a and b with modified parameters  for the results in the top table  some noise is removed from the sensors only  for the results in the bottom table  some noise is removed from the actions and the sensors  Removing some noise from the actions and sensors results in more zero probability branches that can be pruned  allowing the search algorithms  but not the join tree algorithm  to solve the problem for a larger number of stages  modified the model so that each sensor accurately detects whether a wall is present in its direction of the compass  With this change  the maze problem remains partially observable  but the search tree contains many zero probability branches  as can be seen from the results in Table    Since the search algorithms can prune zero probability branches  the exhaustive search algorithm can now solve the problem for up to five stages and the DFBnB algorithm can solve the problem for up to six stages  We next made an additional change to the transition probabilities for actions  In the original problem  the agent successfully moves in the intended direction with probability       as long as there is not a wall   It fails to move with probability        it moves sideways with probability            for each side   and it moves backward with probability        We modified these transition probabilities so that the agent still moves in the intended direction with probability       but otherwise  it stays in the same position with probability       The effects of the agents actions are still stochastic  but they are more predictable  and this allows the search tree to be pruned even further  As a result  the  exhaustive search algorithm can solve the problem for up to six stages and the DFBnB algorithm can solve the problem for up to seven stages  Note that changing the problem characteristics has no effect on the performance of the join tree algorithm  The join tree algorithm solves the influence diagram for all information states  including those that have zero probability and those that are unreachable from the initial state  as a result  its memory requirements explode exponentially in the number of stages and the algorithm quickly becomes infeasible  Although the policy tree that is returned by the search algorithms can also grow exponentially in the number of stages  it does so much more slowly because so many branches can be pruned  As is vividly shown by the results for the two different mazes and for different parameter settings  the performance of the search algorithms is sensitive to problem characteristics  precisely because the search algorithms exploit a form of problem structure that is not exploited by the join tree algorithm  In addition  the results show the effectiveness of bounds in scaling up the searchbased approach       Conclusion  We have described the first implementation of a depthfirst branch and bound search algorithm for influence diagram evaluation  Although the idea has been proposed before  we adapted and integrated contributions from related work and introduced a number of new ideas to make the approach computationally feasible  In particular  we described an efficient approach for using the join tree algorithm to compute upper bounds to prune the search tree  The idea is to generate an upper bound influence diagram by allowing each decision variable to be conditioned on additional information that makes the remaining history nonrequisite  thus simplifying the influence diagram  Then a join tree of the upper bound influence diagram is used to incrementally compute upper bounds for the depth first branch and bound search  We have also described a new approach to constructing the search tree based on the structure of the strong join tree of the upper bound influence diagram  Experiments show that the resulting depth first branch and bound search algorithm outperforms the stateof the art join tree algorithm in solving multistage influence diagrams  at least when there are more than three stages  We are currently considering how to extend this approach to solve limited memory influence diagrams       which typically have many more stages  We are also exploring approaches to compute more accurate bounds for pruning the search tree  Finally  we are considering approximate and bounded optimal search algorithms for solving larger influence diagrams using the same upper bounds and AND OR search tree  Acknowledgments This research was support in part by NSF grants IIS         and EPS          and by the Mississippi Space Grant Consortium and NASA EPSCoR program   
          I                                                                                                                          AB   C              ED               F G HJI K G   L M N HJIOHJI POQRHTSULWVYX Z   SUN   P   L     acb dfegd h i j kTemlna o e prqBs h tuqUlnh tnk vwiWlna x x d y a iWl j kTemlna o e z tnh y tuqBo qBi   jTbu Jh h xhB vwiTh tno qUlnd h i jTb d a i b ace  iJd  a tuegd lmk hB z d lgluegsJJtny   z d lgluegsJJtny   Jz  B   UW B   J  UJTW    g      TJa    d o h tgluqBi b a Ji b lnd h iEq e q   q b lnh tnd cqUlnd h i  d a   q JtnhT T b l hB iuUn oUoooo  Un oU  un uuo u oo ooou owo u  u y A A z   euA  A d  a i  TdfqByB  iJhWemlndfb a  df Ta i b a  q  J Td lnd h i qBx  Ta  a i  Ta i b a tna xfqUlnd h i e   d x x  iJa hB ln Ja o qBd i Jtnh sJx a o ehBTd o h tgluqBi b aenqBo   s a d iWlntnhT T b ac  qBoh iJy ln Ja UqBtndfqBsJx ace  Ah i egacIWJa iWlnx k   a Jx d iJy d i qk acegdfqBi iJa lm  h tn Te dfe tna Jtnacega iWluqUlnd h i  Th iJhBl  q a a ATJx dfb d l h tno e h t ln JaEA z   e d iYln Ja iJa lg  hBJln Ja d o h tgluqBi b aJi b lnd h i B   Jdfbu  eg Jh Jxf  df Ta       h tn Te EC a E tueml Ta tnd  a ln Ja a AJq b l h tnoh t ln JaEA z   e qBx x ks aq eb x hWegaOq e  hWenegd sJx alnh ln JaO hWemlna   B h  ln Ja h Tlnd o qBx d o h tgluqBi b aJi b lnd h i  j d i b a b qBxfb JxfqUlnd iJy tnd h t omh d iWl  Tdfemlntnd sJTlnd h i Y  k Jdfb qBx x k    a tna Jtna   n l J     a oydfe Jtuq b lndfb qBx x k acIWJd UqBx a iWl lnh a AJq b ld iTa tna i b a d iln Ja ega iWl qBi  d o h tgluqBi b a Ji b lnd h iq e q   q b lnh tnd cqU  J i   a m l     h tn Te   a  eg qBx x kh iJx kY ega ln Ja d t qBJJtnhATd o qUlnd h i e  lnd h i  d a   JtnhT T b l hB b h i  Td lnd h i qBx Jtnh s qBsJd x d lmk   C  a n t a  d a   ega  a tuqBx a ATdfemlnd iJyqBJJtnhATd o qUlnd h iemlntuqUlna y d ace luqBsJx ace A A z   euA  A A d  a i TdfqBy iJhWemlndfb a  df Ta i b a      h   t n l J   a b h i  Td lnd h i qBx h tno e qBi    h d iWl h Tl ln Ja d t x d od luqU    a  Th iJhBl   q aa ATJx dfb d l h tno e h tln Ja A z   e n l d   h   i   e     olna t qEegd oJx a qBi qBx kTegdfe hB ln Ja d iTI Ja i b a hB a W  d iln Ja iJa lm  h tn Te EC a E tueml  Ta tnd  a ln Ja a AJq b l f d T     a   i   b   a d i qk acegdfqBi iJa lm  h tn Te   aJtnh  hWegaqBiqBJJtnhATd   h tno h t ln Ja A z   e hB ln Ja h Tlnd o qBxJd o h tgluqBi b a   o U q n l d   h i emlntuqUlna y k ln  qUl ohT Td E ace ln Ja iJa lm  h tn  emlntn b lnJtna d i Ji b lnd h i Ej d i b a ln JaEb qBxfb JxfqUlnd h i  dfe   qBtu    a   h u t T     a   t n l h q b b h oohT JqUlnaln Ja ohWeml d o h tgluqBiWl q  J Td lnd h i qBx  eg qBx x kYh iJx kY ega ln Ja d t qBJJtnhATd o qUlnd h i e OC a T     a      a   i T     a   i b a tna xfqUlnd h i e d iWlntnhT T b ac  s k ln Ja a  df Ta i b a   Jt tna  d a  Oega  a tuqBxJ h JJxfqBtqBJJtnhATd o qUlnd h i emlntuqUlna     a T A      a n t d  o   a W i luqBx tnacegJx lue eg JhU  ln  qUl ln JaEiJa  CemlntuqUlna y k hBo  y d ace qBi   h d iWl h Tl ln Ja d tx d od luqUlnd h i e  q egac      a u t   e B q  i d  o oac TdfqUlna d oJtnhU a oa iWl d iln Ja IW qBx d lmk hB ln Ja h iqBiqBi qBx kTegdfe hB ln Ja d iTI Ja i b a hB a  df Ta i b a  d  o      h g t u l B q   i   b   a Ji b lnd h i    a Jtnh  hWega q oa ln JhT Eh tqBJJtnhATd o qUlnd iJy ln Ja a AJq b l h tno hB d o h tgluqBi b a Ji b lnd h i s k a ATJx dfb   d lnx k ohT Ta x d iJy ln Ja ohWeml d o h tgluqBiWl q  J Td lnd h i qBx      O T Ta N       a     O N    Ta  a i  Ta i b a tna xfqUlnd h i erd iWlntnhT T b ac  s k a  df Ta i b a   Jt a AT a tnd oa iWluqBx tnacegJx lue eg JhU Eln  qUl ln Ja iJa   C a emluqBtgl   d ln  ln Ja ln Ja h tna lndfb qBxWtnh hBlue hB d o h tgluqBi b a enqBo   qBJJtnhATd o qUlnd h i emlntuqUlna y k hBI a tue qBid ooac TdfqUlna Jx d iJy C a  ega b qBJd luqBx x a lglna tue h t UqBtndfqBsJx aceqBi   x hU  a tg  d oJtnhU a oa iWlrd i ln Ja IW qBx d lmk hBTln Ja d o h tgluqBi b a b q ega x a lglna tue h t ln Ja d temluqUlnace   h xf x a lglna tue  Ta iJhBlna ega lue Ji b lnd h i  hBUqBtndfqBsJx aceh t emluqUlnace p a   l  A   A s a q Ji b lnd h iOhB     UqBtndfqBsJx ac  e          A                hU a t ln Ja Th o qBd   i          Ah i egdf Ta t ln Ja Jtnh sJx a o hB acemlnd o qUlnd iJyln Ja o Jx lnd Jx a d iWlna   WN  TJO O O   JOmO N y tuqBx  vwo h tgluqBi b a enqBoJx d iJyB  s q egac OqBx y h tnd ln Jo e   q aEs acb h oa qBiYd o h tgluqBiWl   qBod x khBqBJJtnhATd o qUlna d iTa tna i b a oa ln T  hT Je h t  qk acegdfqBi iJa lm  h tn Te   iJahBrln Jao qBd i Jtnh sJx a o e hB d o h tgluqBi b a enqBoJx d iJy d i  qk acegdfqBiiJa lm  h tn Te dfe tna Jtna   ega iWluqUlnd h i hBln Ja d o h tgluqBi b a Ji b lnd h i  v l dfe   a x x   iJhU  i ln  qUl ln Ja IW qBx d lmkhB d o h tgluqBi b a Ji b lnd h idfe b tnd lndfb qBx lnh d o h tgluqBi b a enqBoJx d iJy qBx y h tnd ln Jo e O     Ja b x hWega t ln Ja d o    h tgluqBi b a Ji b lnd h i lnh ln Jaq b ln qBx  hWemlna tnd h t  Tdfemlntnd sJTlnd h i  ln Ja s a lglna t d lue  a tgh tno qBi b a    k Jdfb qBx x k W  a tna Jtnacega iWl qBi w UnUmU U U UcYW  auUcYWawa a UnU a a awanUmUnamU a a awUcW U U UucUnU eWawUce U U e Uce U UmUuaweW amaa UmU a  awU a iWiW a a a iW U efU UauUcYUama YBeWUceWa a a awUci           A  rA        AmA  C aq enegJoaEln  qUl ln Ja Th o qBd i hB d iWlna y tuqUlnd h iOhB  A   A dfes h Ji  Tac rd a   ln  qU  l   a ATdfemlue vwo h tgluqBi b a enqBoJx d iJy qBJJtnhWq bu Jace ln Jdfe Jtnh sJx a o s k acemlnd o qUlnd iJy           A  rA A  rA       A    rA    A A     Ja tna   A   A r   Jdfbu  dfe b qBx x ac Eln Ja o    Uu owoUn iuu    Tn i oo  Un  dfe q Jtnh s qBsJd x d lmk  Ta i egd lmk Ji b lnd h ieg bu  ln  qUl   A   A  y q b tnhWene ln Ja a iWlnd tna  Th o qBd i     iJa Jtuq b lndfb qBx tnacIWJd tna oa iWl   hB   A   A dfe ln  qUl d l eg Jh Jxf  s aacq egk lnh enqBoJx a tnh o  vwi  h tu Ta t lnh  acemlnd o qUlnaEln Ja d iWlna y tuqBx   a y a iJa tuqUlna enqBoJx ace    A                     tnh o    A   A qBi    egaln Ja y a iJa tuqUlnac  UqBx Jace d i ln Ja enqBoJx a   oacqBi h tno Jxfq              A     A        A  WA   A     A   A     Ja acemlnd o qUlnh t qBx ohWeml egJtna x k b h i  a tny ace lnhA Ji  Ta tb a tg  luqBd i   acqB  q enegJoTlnd h i eCB DFE      Ja  a tgh tno qBi b ahB ln Ja acemlnd o qUlnh t d H i GIW qUlnd h I i   b qBi s a oacq egJtnac  s k d lue UqBtndfqBi b a   A  rA A   A       PQ    J K L M B A   A EN   O    A r   A      R JsJd i emlna d i B cSE eg JhU  eln  qUl d   A   A   J ln Ja h Tlnd o qBx  d o h tgluqBi b a Ji b lnd h i dfe  A   A    A   AT   A D A  A A       vwi ln Jdfe b q ega cln JaUqBtndfqBi b a hBTln Jaacemlnd o qUlnh trdfer a tnh VU hU    a  a tccln Ja b h i b a Tl hBJln Ja h Tlnd o qBxWd o h tgluqBi b a Ji b lnd h i dfe hBtuqUln Ja t ln Ja h tna lndfb qBx egd y iJd E b qBi b a s acb qB ega E i  Td iJ  y   dfe acIWJd UqBx a iWl lnhE i  Td iJy ln Ja  hWemlna tnd h t  Tdfemlntnd sJTlnd h i J   Jdfbu  dfe ln Ja Jtnh sJx a o ln  qUl   a qBtna   q b d iJy    W a  a tgln Ja x acene  d l egJy y acemlue ln  qUld    a E i   d i emlnacq   q Ji b lnd h i ln  qUl dfeb x hWega a iJh Jy  lnh ln JaEh Tlnd o qBx d o h tgluqBi b a Ji b lnd h i    aEb qBi emlnd x x a AT acb l y h hT  b h i  a tny a i b a tuqUlnace  X  ZY   N   Ja    O    O T Ta N        O          a ab r Oma Ndc  F  eO gf    O N   JOmO N O N   N  vwo h tgluqBi b a enqBoJx d iJyb qBi  s a acq egd x kYq  JqBTlnac  lnh egh x  a qCUqBtnd a lmk hBOd iTa tna i b a Jtnh sJx a o e d iE qk acegdfqBiiJa lg    h tn Te aceg acb dfqBx x kE i  Td iJy  hWemlna tnd h to qBtny d i qBxfe h t JiT  h s ega tn ac EUqBtndfqBsJx ace    rh o qB  ad o h tgluqBi b aenqBoJx d iJy d i  qk acegdfqBiiJa lm  h tn Te Jtuq b lndfb qBx   a lmk Jdfb qBx x k iJa ac   qega   IWJa i b a hB A z   e     Jdfbu  omh d iWlnx kEeg acb d kEqBi d o h tgluqBi b a Ji b lnd h i i  h h tna h tno qBx x k  egJJ hWegk a j  A   A ohT Ta xfe ln Ja omh d iWl Jtnh s qBsJd x d lmk Tdfemlntnd sJTlnd h ihU a t qYega l hB UqBtndfqBsJx ace              k ln Ja bu  qBd i tnJx a   a b qBiE  q b lnh tnd  a SA               d l q e h x x hU  e  j A   AT lj A   A A   m          j A     on     AS                p  AA    A  WA   on h t  E    a p  Ttuq  enqBoJx ace tnh oCacq bu    rh  Ttuq Rq enqBoJx a  hB ln Ja A z  qj A     AS            A A egacIWJa iWlndfqBx x k  C a b qBi acq egd x k y a l ln Ja A z   e d i  qk acegdfqBi iJa lm  h tn Te   d ln  iJha W  df Ta i b a  s acb qB ega d    SA                     qBtna d i ln Ja lnh  h x h y dfb qBx h tu Ta t hBrln JaiJa lm  h tn     a b qBi egd oJx d k ln Ja qBs hU a acIW qU  lnd h i lnh     n   m     j  A   T A   j A   jCr A   AgA   tA sBA A   on        Ja tnauj A   jCr A   AgA qBtna a ATJx dfb d lnx k ohT Ta x ac  d i  qk acegdfqBi iJa lm  h tn Te E    Ja qBs hU a egd oJx d E b qUlnd h itna I acb lue eghb qBx x ac    v oUou w   x iuUn oUoooo  Uq n B   D E     Jdfbu RenqkTe ln  qUl q iJhT Tadfe d i  Ta  a i  Ta iWl hB d lue iJh iT w Tacenb a i  JqBiWluey d  a i h iJx k d lue  qBtna iWlue  vwo h tgluqBi b a enqBoJx d iJy Ji  Ta teg bu  b d tub Jo   emluqBi b ace dfe acq egklnh d oJx a oa iWlcy  U hU  a  a tc   Ja i  TdfqByB  iJhWemlndfb a  df Ta i b a a ATdfemlue  d lb qBi  TtuqBo qUlndfb qBx x k bu  qBiJy a ln Ja  Ta  a i  Ta i b a tna xfqUlnd h i e qBoh iJyln Ja UqBtndfqBsJx ace Rj JJ hWega ln  qUl d iq  J Td lnd h i lnhln Ja JiJh s ega tn ac OUqBtndfqBsJx acz e  E   a qBxfegh   q aqBi a  df Ta i b a ega l     S   SA                 C a   iJhU  ln  qUl ln Ja  hWemlna tnd h t  Tdfemlntnd sJTlnd h i hB ln Ja iJa lm  h tn b qBi emlnd x x s a  q b lnh tnd  ac   egd iJy ln Ja bu  qBd i tnJx a  j A    n    n m        Ab  j A A    A     j A      on     AS                p  A    A    A  WA  U hU  a  a tc ln Ja egd oJx d E b qUlnd h i o q  Tah   t GIW qUlnd h Q i s b qBi   p   a b qBiJiJhBl   om emlln JtnhU  iJh x h iJy a t s a o q  Ta  Ja tna Ws acb qB ega  q  qk ln Ja UqB  tndfqBsJx ace d i     SA             A F   jCr A   A  h iegh oa hB    Jdfbu    o qk Ta  a i   y d  a i   n ln Ja a  df Ta   i p b a E a h tna   aqBi qBx k  a  JhU  lnh egd oJx d C k j A     SA             A A U  a E tueml d iWlntnhT T b a ln Jah x x hU  d iJy  Ta E iJd lnd h i     L  I    I q o xUu nEo  oUu u y o  oUnEn u oUuowQ ooo  Tn  u y u u xUuuo xUoUu o  o u  u y      AS               oUn o uxco  o u n iuuy u o    rU  uEoUnEUuno u u oo  n Y        AS              o Ju tna x a UqBiWl  q b lnh thB    o u n Uowuuoo  b y    A   A  ofy  o Ju y u o     xUoUu o  o u  u y  o JoUo on      o giuUn n uui owuuo S  uuoUu   uu u   Uunu   oon  o Ju Uuno u u oo n  oUn o oUunu  ow   iuUn oUoooo  Un oU Un  o Ju  oUunu n o  y           o Ju u xco  o u n iu u    oUn o  o Ju U o Ju   u xUoUu o  o u  u y oo n    A    A     wv iWlnJd lnd    a x k b   A   A d i b x   Tace ln Ja q  J Td lnd h i qB  x p UqBtndfqBsJx ace ln  qUl   iJa ac   Jelnh b h i  Td lnd h i h i d i    A             A  W hBlna ln  qUl    A   Adfe eg acb d E b lnh q  qBtglndfb JxfqBt h tu Ta tnd iJy hBrln Ja UqBtndfqBsJx ac e Jv l o qk b h iWluqBd i  Td I a tna iWl UqBtndfqBsJx ace h t  Td I a tg  a iWl h tu Ta tnd iJyWe A d  a iYln JaE Ta E iJd lnd h i  GIW qUlnd h  i   b qBi iJhU s a egd oJx d E ac  lnh j A    n   AT   m       j A      n  jCr A          A          A   gA A    A WA  A U hU  a  a tc   a iJhU    q a iJh a ATJx dfb d l h tno e hB ln Ja A z   e d i ln Ja iJa lm  h tn  qBi k oh tna  v    a   qBiWl lnh b h oJTlna qBi   emlnh tna ln JaA z   e    a iJa ac   lnh sJtnacqB  ln JaEb h i emlntuqBd iWl hB ln Ja h tnd y d i qBx iJa lm  h tn Eemlntn b lnJtna qBi  q b b h oohT JqUlnaln Ja q  J Td lnd h i qBx  Ta  a i  Ta i b a qBoh iJyln Ja UqBtndfqBsJx ace   iJa egh x T  lnd h   i dfe lnh b h i emlntn b l q iJa   iJa lm  h tn  d i    Jdfbu  acq bu  iJhT T  a     q e qBtub e   b h od iJy tnh o ln Ja UqBtndfqBsJx ace d is hBln    jCr A   A qBi       A   A  C a b qBx x eg bu  iJa lm  h tnA     o i owUu o co u  u    L  I    Il o xUu nEo  oUu u y o  oUnEn u oUuowQ ooo  Tn  u y u u xUuuo xUoUu o  o u  u y      AS               oUn o uxco  o u n iuuy u o       q b lnh tnd cqBsJx an u o Uuow ofyEo n u n u oUuowo JoUo unu  unu y u n o  y  o Ju y  o   u oUofy ou o  u  Too  Un oy  o Ju Un u o Boon oU n u o Uou w oUn o Jy u  y owu u o  Uu oUofy ou o  u  Too  Uk n j  A    A iuoUnYuuu    To C u   o i     unc o   i o     V  y  Un  owUu o cuuo ow o  u   Uu uuo  i z Tn  u y u u   xUuuo xUoUu o  o u  u   oo n   iuUn oUoooo  Un oU Un ooo  y  oUunu n o   y jCr A    A     q b lnh tnd cqBsJx a emlntn b lnJtna dfe iJhBlJiJdfIWJa   iJa qBx y h tnd ln Jo lnhb h i emlntn b l q  q b lnh tnd cqBsJx a emlntn b lnJtna h t q   qk acegdfqBi iJa lm  h tn  dfe  Tacenb tnd s ac  s a x hU   a    K S  cGu   Too oUoon  o ro i owUu o co u  uA ou  i o Tunu    IV NV      oUu u y o  oUnn u oUuow  o y u o    uxco  o u n iuu xUoUu o  o u  u y    Uo n o oy u o   C Tn  u y u uxUuuo xUoUu o  o u  u y     NV  NV      o i owUu o co u  u y ou  i o Tunu      uno u u o Jun co u y oon  oon o Ju unuxUu uuy u    o Ju oou own    oB o  iuoU Uuno u u    v oUuowEo Jun co u y  o JoUo oUunu oUn iuu y owUuuy  Ouxco  o u n iuu n co u y oon     rUu o Ju Uuno u u oon oon owu    i Juui w uuo i   Tn  u y u uxUuuo n co u o     ooo ofy    oUuo w uuo     y   o o oYoUnoUuniEuuu ouuu n uuo i     oUoou      ooo  y  oUunu n o  y B o Ux u no JoUo n oUuni uo  ofy o  y uuu o  uuu n o J  u   y    i  o J  oUo o J  u Uu o  u n owoUoo  UnE     o J  u oUuni ofy u  un E   o J  u n co uEon  uuoUu oon     oUowu u oono J  uEUuno u u ow o J u uuoUu  o  u u Un u    Ju n o o oUoon  oUn oUuni uuu ouuu n o n co u y  u uo   oUn o o J uAV         o J  ui    oo ou u o c    o  iuoUooon  o Juu n ou o  u y   Uu oUo   o u unu n oy owoUowu y      o J  u  oUunu n o  v l dfe emlntuqBd y  Wlgh tn  qBtu lnh JtnhU a ln Jah x x hU  d iJy ln Ja h tna o   G L  SUL u     uUoon    WUu ooo      ow o  oUu u y o  oUn n u o Uuowk ooo   uxco  o u n iuu   oUn ok Tn  u y u uxUuuo  xUoUu o  o u  u y         AS                uUo  u  oy o    o i owUu o c o u  u y ou   i oT  unu    x y h tnd ln Jo  dfe egd od xfqBt lnh ln J  a Bunon  unuu o   i oo  Un oa ln JhT  Jtnh  hWegac  d i B  B  SE  vwi  Ta ac Th Jt JtnhTb ac TJtna   d x xd iWlntnhB   T b a ln JaenqBoa q  J Td lnd h i qBx qBtub e q e ln Ja y tuqBJ  tnac T b lnd h i oa ln JhT  d  lm  h oa ln JhT Je  ega ln JaenqBoa h tu Ta tnd iJy C  U hU    a  a tcln Ja  Td I a tna i b a dferln  qUl y tuqBJ  tnac T b lnd h i qBs egh tns e ln Ja a  df Ta i b a    Jd x a tnac T b d iJyln Ja y tuqBJ  T   Jdfbu   d i ln Ja a i   tnacegJx lue d iYqEiJa   iJa lm  h tn    d ln Jh Tl a  df Ta i b a UqBtndfqBsJx ace  vwih Jt JtnhTb ac TJtna B  a ega  qBtuqUlna y tuqBJ  tnac T b lnd h i qBi   a W  df Ta i b aqBs egh tnTlnd h i  C a E tueml b tnacqUlna q iJa   iJa lm  h tn  ln  qUl emlnd x xWtna Jtnacega iWlln JaenqBoa Tdfemlntnd sJTlnd h i q eln Jah tnd y d i qBxUh iJa  A d  a i ln Ja iJa    emlntn b lnJtna B  a b qBi   q b lnh tnd  aln Ja Jx xUomh d iWl  hWemlna tnd h t  Tdfemlntnd sJTlnd h i  egd iJy bu  qBd i tnJx a qBi   qBs egh tns a W  df Ta i b a d iWlnh acq bu A z  yega  qBtuqUlna x k   tglnd     B   EJtnacega iWlue qegd od xfqBt df Tacq h t b h i emlntn b lnd iJy qBi h Tlnd o qBx d o h tgluqBi b a Ji b lnd h i  d i   Jdfbu   Ja egJy y acemlueE tueml lntndfqBiJy JxfqUlnd iJy ln Ja  qk acegdfqBi iJa lm  h tn  qBi   o qB  d iJy d l bu Jh tu JqBxcqBi   ln Ja i b h iT  emlntn b lnd iJy ln Ja iJa   emlntn b lnJtna tnh o ln JaEbu Jh tu JqBx y tuqBJ    Jt qBJJtnhWq bu  q h df Je Jdfe d iWlna tnoac TdfqUlna emlna   vwi jWlna     hB  x y h tnd ln Jo  T  a q  J  qBtub e s a lm  a a i qBx x ln Ja  qBtna iWluehB q iJhT Ta T  U a i b a  ln Ja xfq eml  qBtna iWl   d x xy a l qBtub e b h od iJytnh o qBx x ln Ja hBln Ja t  qBtna iWlue v  ln JaA z  Eegd  a hB ln Jaxfq eml  qBtna iWldfed iJd lndfqBx x kExfqBtny a h td  ln Ja tna qBtna o qBi k  qBtna iWlue  ln Ja iJa   A z   h t ln Ja xfq eml qBtna iWl o qk sJx hU   J G  a i iJhBlcBln Ja iJa  emlntn b lnJtna   d x xJo qB  a d o h tgluqBi b a enqBoJx d iJy d iJaA b d a iWlc   rh tna oac Tk ln Ja Jtnh sJx a o    a JtnhB   hWega ega  a tuqBx  Ja Jtndfemlndfb eh t Jtna JtnhTb acenegd iJy ln Ja h tu Ta tnd iJy hB ln Ja  qBtna iWlue   x x ln Ja  Ja Jtndfemlndfb e qBtna egJsTomacb l lnh ln Ja  qBtglndfqBxb h i emlntuqBd iWlue hB ln Ja h tnd y d i qBx iJa lm  h tn  r   Jdfbu  d iT  b x   Ta qBtub e h t  Td tnacb lnac   qUln  e ln  qUl qBx tnacq  Tk a ATdfeml qBoh iJy  qBtna iWlue r    Ja E tueml oa ln JhT  dfe lnh h tu Ta t ln Ja qBtna iWlue d i ln Ja  Tacenb a i  Td iJy h tu Ta t hB ln Ja i Jo s a t hB ln Ja d t hU  i  qBtna iWlue   k   Th d iJyegh    aEqBtna lntnk d iJylnh o qB  a ln JaExfq eml  qBtna iWl   q a q e x acene d i b h od iJyEqBtub e q e hWenegd sJx a     Jdfbu  b qBitna    T b a ln Ja egd  ahBln Ja A z   e      Ja egacb h i  Eoa ln JhT  dfe lnh h tu Ta t ln Ja  qBtna iWlue d i q  Tacenb a i  Td iJy h tu Ta t hB ln Jaegd  a hB ln Ja d t A z   e  j d i b ah Jt JJtn hWegadfelnhod iJd od  a ln Ja egd  a hBA z   e  ln Ja egacb h i    Ja Jtndfemlndfb dfe oh tnaa I acb lnd  a J   Jdfbu    a  d x x  egad i qBx xln Ja a AT a tnd oa iWlue hBrln Jdfe  qB a tc  OVA O   a  JOmO N    TJa  O  A g NA OmOO        J   O       O T Ta N      O N   JOmO N  A  O  r  Jtnh oGIW qUlnd h iJ   a b qBiEega aln  qUl lnh sJJd xf  q   q b lnh tnd     qBsJx a emlntn b lnJtna    a iJa ac  lnh q  J Eo qBi k qBtub e lnh ln Ja iJa   emlntn b lnJtnaE   Ja iO  a   q aE TdfqBy iJhWemlndfb a  df Ta i b a      Jacega a A lntuq qBtub eo qB  a q iJa lm  h tn  oh tna b h oJx a A  qBi  Yo qB  a ln Jab qBxfb JxfqUlnd h i hB ln Ja A z   e o  bu  oh tna  Td A b Jx lc  x   ln Jh Jy     a Jtnh  hWega egh oa  Ja Jtndfemlndfb eh t od iJd od  d iJy ln Ja egd  a hB A z   e  ln Jacega b qBiemlnd x xy tnhU  lnh h xfqBtny a  vwi   q b lc ln Jdfe JtnhTb acene dfe Jtuq b lndfb qBx x k acIWJd UqBx a iWl lnh a AJq b l d iTa tna i b a d i ln Ja iJa lm  h tn  r    Ja tna h tna   a  eg qBx x kh iJx k  ega qBJJtnhA   d o qUlnd h i e hB ln Ja Jx x h tno e   U a tna   a   d x xtna  d a   ega  a tuqBx qBJJtnhATd o qUlnd h i emlntuqUlna y d ace  egac  s k ln Ja a ATdfemlnd iJy d o h tg  luqBi b a enqBoJx d iJyqBx y h tnd ln Jo e h t  qk acegdfqBi iJa lm  h tn Te  C a   d x x  ega q tnJiJiJd iJy a AJqBoJx alnh d x x  emlntuqUlnaln Jacega emlntuqUlna   y d ace  H Q    L A Eegd oJx a qk acegdfqBi iJa lm  h tn   d ln  ln Jtna a sJd       g i qBtnkYUqBtndfqBsJx aced   i  d y Jtna   qBtuqBoa lna tnd  ac   q eh x x hU  e O K  K j AE  b  n  J  J    C K  r  oE A  J C   J J  EE  II IO I    N   K   J JC  J    O II O O IO I  C  J s J    JC   J     J C J   II IO I   d y Jtna O  egd oJx a  qk acegdfqBi iJa lm  h tn      qBtndfqBsJx a E dfe h s ega tn ac  d i emluqUlna EE  C a b qBi acq egd x k b qBx   b JxfqUlnaln Ja  hWemlna tnd h t omh d iWl  Tdfemlntnd sJTlnd h i hU a t R r qBi   E  n j A r  oE EE A   K  K J  W D J W  B  J c   J FWD    C C    S    K   I HJ  A F U  UoU gH UcL P  Qb SS cHJ I Y  L rN  I YF   I O z tnh s qBsJd x dfemlndfb x h y dfb enqBoJx d iJyHB SE q enegJoace ln  qUl ln Ja d o      h tgluqBi b a Ji b lnd h i   q e ln Ja enqBoa A z   eh t qBx x ln JaUqBtnd   qBsJx ace  p d   a x d  Jh hT    a d y  Wlnd iJ  y B  J   SE y h ace qemlna  Jtgln Ja t qBi   q enegJoace ln  qUlln Ja d o h tgluqBi b a Ji b lnd h i   q e ln Ja h x   x hU  d iJy h tno  j A    n   m        AT   j A      n  A     jCr A       A        HajCr A   AgA  C  K  J  D J    C   K  J B J   D    FAU  UoU HgUcL P   Qb SScHJIY L rN IYF   I Oj a  a tuqBx qBx y hB   tnd ln Jo eiJhBlndfb a ln Jax d od luqUlnd h i e hBln Jad o h tgluqBi b a Ji b   lnd h i egac s kx d   a x d  Jh hT    a d y  Wlnd iJy qBi  Jtnh  hWega q  Td o  a tna iWl h tno hB d o h tgluqBi b a Ji b lnd h i      Ja k emlnd x x q enegJoa ln Ja enqBoa emlntn b lnJtna h t ln Ja d o h tgluqBi b a Ji b lnd h i q e ln Ja h tnd y d i qBx  qk acegdfqBi iJa lm  h tn   sJTlrln Ja k tnacqBx d  a ln  qUl ln Jaa W  df Ta i b a   q e d iTI Ja i b ah i ln Ja A z   ehB qBx x ln JaiJhT Tace qBi   Jtnh  hWega ln Jah x x hU  d iJy h tno hB d o h tgluqBi b a Ji b lnd h i  j A     n  n   Ab    m         A  j A     on  jCr A       A       A    j A r  oE A C C  AmcWA  v l  egace ln Ja enqBoa A z   e q e ln Ja h tnd y d i qBx Tdfemlntnd sJTlnd h i h t iJhT Tace   d ln  iJha  df Ta i b a qBtna iWlue   ln Ja tn  dfega  d l eg Jtnd iJ Te A z   e h t ln JhWega iJhT Tace   d ln a  df Ta i b a  qBtna iWlue   s  d   h  egx k  ln JdfeqBJJtnhATd o qUlnd h i h iJx k luqB  ace d iWlnh q b b h JiWl ln Ja Jtnd od lnd  a d iTI Ja i b a hB ln Ja a  df Ta i b a Uln Ja d iTI Ja i b a hB ln Ja a  df Ta i b aiJhT Tace h i ln Ja A z   e hBrln Ja d t bu Jd xf Ttna i  H Q    L A  tnd y d i qBx A z     s q egac  vwo h tgluqBi b a JJi b lnd h i     g h t ln Ja tnJiJiJd iJya AJqBoJx a  j A r  oE A  qBi  E  tnJc T a z x B BS E  Ta tnd  a ln Ja h tno Jxfq  hBb qBxfb JxfqUlnd iJy ln Ja vnA z   e  egd iJy s a x d a  Jtnh  qByWqUlnd h i oacenenqBy aceCB cT  D E      Ja d o h tgluqBi b a Ji b lnd h id H i GIW qUlnd h i  hBI a tue q sJd y d oJtnhU a oa iWlhU a t ln Ja tna Jtnacega iWluqUlnd h iEhB  GIW qUlnd h iYcJ U hU  a  a tcrln Jdfe tna Jtnacega iWluqUlnd h iemlnd x x h iJx k luqB  ace d iWlnh q b   b h JiWl  qBtglndfqBx d iTI Ja i b aEhB ln JaEa  df Ta i b a vwi b q ega ln Ja a  df Ta i b a   TtuqBo qUlndfb qBx x kEbu  qBiJy ace  Ta  a i  Ta i b a  tna xfqUlnd h i e qBoh iJy ln Ja UqBtndfqBsJx ace  ln Jdfe qBJJtnhATd o qUlnd h iE  d x x s a egJsT  h Tlnd o qBx q e   a x x H Q    L A vnA z     s q egac  vwo h tgluqBi b C     g a JJi b lnd h i h t ln Ja tnJiJiJd iJya AJqBoJx a   Am A  Gq bu  j A   jCr A   A        A dfeEb qBx x ac  qBi o    Uu owoUn iuu V   AvnA z   A  qb h i b a Tl E tueml Jtnh  hWegac d iyB E aU hU     a  a tcTln Ja tna qBtna q b ln qBx x k o qBi k qBx y h tnd ln Jo e ln  qUl  ega ln Ja qBs hU a d o h tgluqBi b a Ji b lnd h i  d iegJd lna hB ln Ja   q b l ln  qUl ln Ja k  Td I a t d i ln Jaoa ln JhT Je hB acemlnd o qUlnd iJyYln Ja q b ln qBx luqBsJx ace  j a  a tuqBx Tk i qBodfb d o h tgluqBi b a enqBoJx d iJyqBx y hB  tnd ln Jo e  d i b x   Td iJyE vgjW w WaB  E ega x o  d o h tgluqBi b a  enqBo   Jx d iJ y B   SE  qBi   q  JqBTlnd  a d o h tgluqBi b a enqBoJx d iJ y B    SE W ega  Td I a tna iWlx acqBtniJd iJy oa ln JhT Je lnh x acqBtni ln Ja vnA z   e  a  qBi  K  J       J J  DW   K  J sU  s J   sU     Tk i qBodfb d o h tgluqBi b aenqBoJx d iJy qBx y h tnd ln JoC egd iJy ln Ja qBs hU a h tnoyo qk x acqBtni q  Td I a tna iWl d o h tgluqBi b a Ji b lnd h i   Ta  a i  Td iJy h iE    qUl TdfemluqBi b a oacq egJtna d l lntnd ace lnh od iJd   od  a  Jh t ln JatnJiJiJd iJy a AJqBoJx a   aiJa ac  lm  h  qBtuqBo   a lna tue lnh  qBtuqBoa lna tnd  a ln Ja d o h tgluqBi b a Ji b lnd h i v    a  eg a a  wp  Td  a tny a i b aq e ln Ja  TdfemluqBi b aoacq egJtna   ay a l ln Ja enqBoa egh x Tlnd h i e q e qBs hU a  v   a od iJd od  a ln Ja UqBtndfqBi b a hB ln Ja d o h tgluqBi b a enqBoJx d iJy acemlnd o qUlnh tcBln Ja x acqBtniJac d o    h tgluqBi b a Ji b lnd h iY  q e ln Ja h x x hU  d iJy omh d iWlJtnh s qBsJd x d lmk  Tdfemlntnd sJTlnd h i  H Q    L A vwo h tgluqBi b a JJi b lnd h i x acqBtniJac s kod iJd od        g d iJy ln Ja UqBtndfqBi b a hB ln Ja d o h tgluqBi b a enqBoJx d iJy acemlnd o qUlnh t h t ln Ja tnJiJiJd iJya AJqBoJx a  j A r  oE A C C  K  J F J J     s    K  J FD   s J c F D   C aEb qBiega a ln  qUl   a b qBiJiJhBl q bu Jd a  a ln Ja h Tlnd o qBx d o    h tgluqBi b aJi b lnd h i s k x acqBtniJd iJy     Ja tnacq egh i dfe ln  qUl ln Ja q b ln qBxr hWemlna tnd h t  Tdfemlntnd sJTlnd h i lmk Jdfb qBx x k iJa ac Je oh tna  qU  tuqBoa lna tue lnh qBtuqBoa lna tnd  a ln  qBi  ln Ja d o h tgluqBi b a Ji b   lnd h i   Jh t ln Ja tnJiJiJd iJya AJqBoJx a  ln Ja q b ln qBx  hWemlna tnd h t  Tdfemlntnd sJTlnd h i iJa ac Je ln Jtna a  qBtuqBoa lna tue   s  d h  egx k rd l dfe d i y a iJa tuqBx d o hWenegd sJx alnh a tgacb lnx k EJlq ln Jtna a    qBtuqBoa lna t  Tdfemlntnd sJTlnd h i   d ln  q lm  hB   qBtuqBoa lna t  Tdfemlntnd sJTlnd h i  H    I UoU gH UcL P  Qb SS cHJ I Y L rN  I YU a HT S   H U    L       H  I O    I b O U a tni qBi  Ta a l qBxV  B  SErJtnh  hWega lnh  egaln Ja UqBtndfqBsJx a a x d od i qUlnd h i B T E qBx y h tnd ln Jo lnh a x d od i qUlna ln Ja UqBtndfqBsJx ace h iJa s k h iJa d ih tu Ta t lnhy a l ln Ja A z   e rv  ln Ja b qBxfb JxfqUlnd h i b qBi s a b qBtntnd ac  h Tl a AJq b lnx k ln Ja k   d x xTy a l ln Ja a AJq b l h tno hB ln Ja d o h tgluqBi b a Ji b lnd h i q e d   i GIW qUlnd h   i JE  U hU  a  a tc UqBtndfqBsJx a a x d od i qUlnd h i dfe d iTacq egd sJx a h t xfqBtny a b h oJx a A iJa lg    h tn Te      Ja tna h tna  ln Ja kega l q ln Jtnaceg Jh xf  lnh ln Ja A z   egd  a    CO Ja iJa  a t ln Ja egd  aEhB q  A z   y a iJa tuqUlnac    Ja iOa x d od   i qUlnd iJyq UqBtndfqBsJx a a AJb a ac Je ln Ja ln Jtnaceg Jh xf Wln Ja k y a iJa tuqUlna o Jx lnd Jx a ego qBx x a t luqBsJx ace lnh qBJJtnhATd o qUlna ln Ja egd iJy x a sJd y luqBsJx a Tegh ln Ja tna dfeiJh a ATJx dfb d lh tno h tln Ja d t d o h tgluqBi b a Ji b lnd h i  Jh t ln Ja egd oJx a tnJiJiJd iJy a AJqBoJx a  egd i b a UqBtndfqBsJx a a x d od   i qUlnd h i b qBis a b qBtntnd ac  h Tl a AJq b lnx k   U a tni qBi  Ta  a l qBx  e oa ln JhT  dfe qBsJx a lnhy a iJa tuqUlna ln Ja a AJq b l A z   e  H Q    L A  qBtndfqBsJx H     g a G x d od i qUlnd h iT  s q egac Evwo h tgluqBi b a JJi b lnd h iEh t ln Ja tnJiJiJd iJy a AJqBoJx a  v   a a x d od i qUlnan E s a h tna r    a y a l b h i  Td lnd h i qBx h tno  e j  A r AqBi     j A E r A     Jdfbu  qBtna K  j A E C C  n   K r A  n  n j A e n iUA i  A e i ATn   j A i Ui A   n  K  J WF W J  sU     Am A  U a iJtnd h iQB s E   q e eg JhU  i ln  qUl h t ln Ja b qB enqBx x d iJ  d ik d yB  Jtna T ln Ja ega i egd lnd  d lmk tuqBiJy a i  A e   i A   d ln  tnaceg acb l lnh i enqUlndfemE ace ln Jah x x hU  d iJy d iJacIW qBx d lmk  n  II I oI   J c     J      s  K  i  A e    i A    N  n o      o  II Io I  Am  WA  II Io I   d y Jtna TO  b qB enqBxx d iJ    J W D  J F D      egd oJx a b qBxfb JxfqUlnd h i eg JhU  e ln  qUl ln Ja d o h tgluqBi b aJi b   lnd h iEdfeRd i  Ta ac  acIWJd UqBx a iWlRlnh ln Ja q b ln qBxY hWemlna tnd h t  Tdfemlntnd sJTlnd h i c  U hU  a  a tch txfqBtny a t ohT Ta xfe  UqBtndfqBsJx a a x d od i qUlnd h iT  s q egac d o h tgluqBi b a Ji b lnd h ihBolna i  iJa ac lnh  ega ega  a tuqBxluqBsJx ace lnhqBJJtnhATd o qUlna q egd iJy x a sJd y luqBsJx a  j d i b a ln Ja qBJJtnhATd o qUlnd h i dfe  Ttnd  a i ohWemlnx k s k luqBsJx a egd  a  ln Ja qBJJtnhATd o qUlnd h i b qBi s a qBxfeghegJsT  h Tlnd o qBx jTqBx oa tnh i u o oU VB   s ExfqUlna t Jtnh  hWega lnhd oJtnhU a ln Ja qBJJtnhATd o qUlnd h i  egd iJyJtnh s qBsJd x d lmk lntna ace lnhtna Jtnacega iWl ln Ja A z   e  e  n  hBj A e iUA  d ln  tnaceg acb l lnh j A i iUA O  Genega iWlndfqBx x k   U a iJtnd h i eg JhU  e ln  qUl ln Ja a  df Ta i b a h i q iJhT Ta    q e oh tna d iTI Ja i b a h i d lue d ooac TdfqUlna bu Jd xf Ttna i ln  qBi d lue Jtgln Ja t  Tacenb a i  JqBiWlue W hU     a a A lna i   ln Ja tnacegJx l lnh oh tna y a iJa tuqBx enb a i qBtnd hWe Q   d tuemlc x a l  e x h h  qUl ln Ja  Td   qBy iJhWemlndfb x d iJn   d k i  d y Jtnn  a  J A d  a i b h i  Td lnd h i qBx d i  Ta  a iT   Ta i b a  j  A e i T A  lj  A e i   iUA  C a   q a n  n  n  n  j A e iUA lj A e i A j A i iUAVHj A e  i A Am PIj A i iUAgA   Am D A II I oI    N W Ne O   N    e   a   rO     OVA O   a  JOmO N     J    TJa O     a   O       O T Ta N       O N   JOmO N   vwi ln JaJtna  d h  e egacb lnd h i T  aqBtny Jac ln  qUl egh oaqBJJtnhATd   o qUlnd h i e o qk iJhBls a qBsJx a lnh qBJJtnhATd o qUlna ln Ja  hWemlna tnd h t  Tdfemlntnd sJTlnd h i   a x xrC a iJhU  s a y d i lnh Tdfenb  enerh iJa qBJJtnhA   d o qUlnd h i emlntuqUlna y k ln  qUl dfe s q egac  h i ln Ja d iTI Ja i b aqBoh iJy UqBtndfqBsJx ace d i q qk acegdfqBi iJa lm  h tn    d tuemlc   a JtnhU df Ta qBiYqBi qBx kTegdfe hB ln Ja d iTI Ja i b a hB a  d    Ta i b a  C a   iJhU ln  qUl  TdfqBy iJhWemlndfb a  df Ta i b a o qB  aceln Ja qBi b acemlnh tue hB a  df Ta i b a iJhT Tace b h i  Td lnd h i qBx x k  Ta  a i  Ta iWlc C a iJa ac  lnh ohT Ta x ln Ja ohWeml d o h tgluqBiWl  Ta  a i  Ta i b a tna   xfqUlnd h i e d i h tu Ta t lnh h sTluqBd i q y h hT d o h tgluqBi b a Ji b lnd h i   iJa  ega Jx oacq egJtna lnh ohT Ta x ln Jatna xfqUlnd  a emlntna iJyBln  hB ln JaE Ta  a i  Ta i b a tna xfqUlnd h i e qBoh iJy ln Ja UqBtndfqBsJx ace dfeln Ja y u nJy ooo o xcooou unoU n Wu hB ln Ja Jtnh s qBsJd x d lmk hB qBi a  a iWl e   d ln  tnaceg acb llnhqBi a  a iW l iQB s E   h h tna h tno qBx x k TegJJ hWega ln  qUl    lidfeln Jah s ega tn ac  a  df Ta i b a    Jdfbu  od y  Wl qUn I acb l ln Ja q enegacenegoa iWl hB ln Ja Jtnh s qBsJd x d lmk hB  ir y d  d iJC y j  A i iUA  j JT   hWega ln  qU l i dfe b h i  Td lnd h i qBx x kd i  Ta  a i  Ta iWl hB   y d  a i        Ja i ln Ja ega i egd lnd  d lmk tuqBiJy a dfe  Ta E iJac  q e ln Ja  Ta tnd UqUlnd  a  n     II I oI   u  II I oI    d y JtnaC JO   TdfqBy iJhWemlndfb x d iJ   n     qB  d iJy ln Ja  Ta tnd UqUlnd  a   d ln  tnaceg acb l lnh j A i iUA    ay a lc n n i  A e   i  AT  j A e   i AEPIj A e  i A    Am A   s  d h  egx k   GIW qUlnd h iO    qBxfeghE Jh xf Je h t ln Ja  TdfqBy iJhWemlndfb x d iJ  v l eg JhU  e ln  qUl ln Jaa  df Ta i b ah iq iJhT Ta  q eoh tna d iTI Ja i b a h i d lue d ooac TdfqUlna  qBtna iWlue ln  qBi d lue Jtgln Ja t qBiT  b acemlnh tue   W hU  rx a l  e x h h   qUl q oh tna d iWlna tnacemlnd iJyEb q ega  j JJ hWega  a   q aqBi d iWlna tub qB enqBx iJa lm  h tn q erd   i  d y JtnT a D    a  q a n nu i  UAu uj A e n i A j A i   iUAV n u j A e  i A Am PI    j A i   iUAgA   AmcWA nu    qB  d iJy ln Ja  Ta tnd UqUlnd  a  d ln  tnaceg acb l lnh   j A i   iUA U  a y a lc n n i    A e     i AT    j A e   i AEPI  j A e    i A   Am sBA j A e  nu   O II O O o I I     II I uI   II I oI   u  II I oI    d y Jtna D O  i d iWlna tub qB enqBx x d iJ     yWqBd i  GIW qUlnd h i    qBxfegh  Jh xf Je h trln Jdfe b q ega      JdfertnacegJx l eg JhU  eln  qUlc qBx ln Jh Jy   qBi a  df Ta i b a iJhT Tad iWlntnhT T b ace  Ta    a i  Ta i b a qBoh iJyd lue qBi b acemlnh tue Wln Ja emlntna iJyBln  hB ln Ja  Ta    a i  Ta i b a   d x xs acb h oa  acqB  a t q eln Ja  TdfemluqBi b as a lm  a a i ln Ja qBi b acemlnh tue d i b tnacq egace    rh egJoo qBtnd  a  h Jt  Tdfenb  enegd h i acenega iWlndfqBx x k eg JhU  eln  qUlc d iEy a iJa tuqBx q e ln Ja  TdfemluqBi b a tnh o q UqBtndfqBsJx a lnh ln Ja a  d    Ta i b a s acb h oace xfqBtny a t d i q  qk acegdfqBi iJa lm  h tn   ln Jaa  d    Ta i b a  eg qBx x k   q e x acene d iTI Ja i b a h i ln Ja  hWemlna tnd h t  Tdfemlntnd   sJTlnd h i hBrln JaUqBtndfqBsJx a   xfegh  ln Ja  Ta  a i  Ta i b atna xfqUlnd h i e qBoh iJy ln Ja d ooac TdfqUlna  qBtna iWluehB qBi a  df Ta i b a iJhT TaqBtna emlntnh iJy a t ln  qBi ln JhWega qBoh iJy d lue Jtgln Ja t qBi b acemlnh tue  C a b qBi qBxfegh eg JhU ln Ja enqBoatnacegJx lue  egd iJyEqBiJhBln Ja t  Ta  a iT   Ta i b aEoacq egJtna b qBx x acy      To   oU oo n   Uu   oUoo  Un     Jdfbu O  a h od l h t sJtna  d lmk    n     Jh tacq bu  A z       j A   jCr A   A          A   AgA  d  i GIW qU  lnd h k i J jCr A   A qBtna d ooac TdfqUlna  qBtna iWlue hB    egh ln Ja d t d iTI Ja i b a qBtna  eg qBx x k emlntnh iJy    b h iWluqBd i e h s ega tn ac  UqBtnd   qBsJx ace  egh d l h iJx k tnac T b aceln Ja b h o  Jx a ATd lmk hB ln Ja A z   U hU  a  a tc ln Ja U  qBtndfqBsJx ace d   i    A   A   q a UqBtnk d iJy Tdfem  luqBi b ace tnh o     Jtnh o h Jt qBi qBx kTegdfe    a s a x d a  a ln  qUl s k ln JtnhU   d iJy q  qk ln Ja UqBtndfqBsJx ace ln  qUlqBtnaJtgln Ja t q  qk tnh o   T  a b qBi emlnd x x tnacega tn a q y h hT  qBJJtnhATd o qUlnd h i hB ln Ja h tnd y d i qBx A z       Ja tna h tna r  a Jtnh  hWega lnh qBJJtnhATd   o qUlna ln Ja Jx xWd o h tgluqBi b a Ji b lnd h i s kq  J Td iJy q  J Td lnd h i qBx qBtub e h iJx kqBoh iJy ln Ja  qBtna iWlue hB a  df Ta i b a   k ohT Ta x d iJy ln JaohWeml d o h tgluqBiWl q  J Td lnd h i qBx Ta  a i  Ta i b a qBoh iJy ln Ja UqBtndfqBsJx ace    a b qBi qBiWlndfb d  qUlna ln  qUlln Ja d o h tgluqBi b a Ji b   lnd h i b qBi s a IWJd lna b x hWega lnh ln Ja q b ln qBx  hWemlna tnd h t  Tdfemlntnd   sJTlnd h i T  aa a Jd iJy q  J Td iJy qBtub e o qB  ace ln Ja iJa lm  h tn  oh tna b h oJx a A    Jdfbu  o qk h iJx ksJtnd iJy od iJd o qBx d oJtnhU a oa iWl sJTl h iJx k o qB  ace ln Ja b h oJTluqUlnd h i oh tna b hWemlnx k  y   A      O      N  Ta Ay  r O   J    rh om emlnd k h Jt Jtnh  hWegac  qBJJtnhATd o qUlnd h i emlntuqUlna y k    a lnacemlnac  d l h i ln Ja          qBx y h tnd ln Jo  C a  a tgh tnoac  h Jt a AT a tnd oa iWlue h i ln Ja        QB SE          B SE  qBi                       B S EJiJa lm  h tn Te   Jt b h o qBtndfegh i   q e s q egac  h iEln JaYq a tuqBy   a   u o oo n Wu  u   y  oUofy owoUn iuH u B  E s a lm  a a i a A   q b l  hWemlna tnd h t o qBtny d i qBxfe hB qBx xTJiJh s ega tn ac  UqBtndfqBsJx ace qBi   enqBoJx d iJy tnacegJx lue  U a x x d iJy a  t  e TdfemluqBi b ak d a xf Je df Ta iWlndfb qBx tnacegJx lue q A e aJx x s q bu W wp a d sJx a t  Td  a tny a i b a d iohWemlb q egace   sJTlrd lue o qUomh trq  TUqBiWluqBy a dfeln  qUlrd l b qBi   qBi  Tx a  a tnh Jtnh sT  qBsJd x d lnd ace B   Jdfbu  qBtna b h ooh i d i  qk acegdfqBi iJa lm  h tn Te rC a d oJx a oa iWlnac  h JtrqBx y h tnd ln Jod i  A CqBi   a tgh tnoac h Jt lnacemlue h i q T    A U    a h i COd i  ThU    e   zb h oJTlna t   d ln   AOoa oh tnk     Z    L     L      cG L  U LWS  t S HJIY L       U    U         Ja          qBx y h tnd ln Jo dfe Jtnh  hWegac  s kaa  qBiEqBi      tnJc T a xd i B BSE     JhWega o qBd idf TacqYdfe lnh   egaega  a tuqBx emlna  e hB  cn u uuu  o   u    unn  o WoUoo  UnRA  pr z k A B cS ElnhYacemlnd   o qUlna qBi d o h tgluqBi b a Ji b lnd h i h t d o h tgluqBi b a enqBoJx d iJy  G AT a tnd oa iWluqBxrtnacegJx lue d i B BS E eg JhU Eln  qUl ln J  a          qBx y h tnd ln Jo d oJtnhU ace J h iEpr z qBi   q bu Jd a  ace q b h i egdf    a tuqBsJx a d oJtnhU a oa iWl hU a t ln Ja emluqUlna hB ln Ja qBtgl qBx y h tnd ln Jo ln Ja i rln J  a         B  E   JJtgln Ja tnoh tna  ln Ja tnacegJx lue qBxfegh eg JhU  ln  qUlrln J  a          OqBx y h tnd ln JoEqBx tnacq  Tk qBJJtnhWq bu Jace ln JaOx d od l ln  qUlYenqBoJx d iJyEqBx y h tnd ln Jo eb qBi q bu Jd a  a  h i        qBi                         s acb qB egaln Ja Jtnacb dfegd h i ln  qUl d l q bu Jd a  ace h i ln Jacega iJa lm  h tn Te dfeqBx tnacq  Tk d iln Ja enqBoa h tu Ta tq e ln JhWega hB Jtnh s qBsJd x dfemlndfb x h y dfb enqBoJx d iJy h iln Ja enqBoa iJa lm  h tn Te   d ln Jh Tla  df Ta i b a vwiln Ja xfqUlglna tb q ega  egd i b a ln Ja tna dfe iJh a  df Ta i b a d iEln Ja iJa lm  h tn Te  x h y dfb enqBo   Jx d iJyEenqBoJx ace tnh o ln Ja h Tlnd o qBx d o h tgluqBi b a Ji b lnd h i  ln JaEJtnd h t  Tdfemlntnd sJTlnd h i EC aEs a x d a  a ln  qUl Jtnacb dfegd h iOegh q bu Jd a  ac  dfe ln Ja x d od lhBenqBoJx d iJy qBx y h tnd ln Jo e   U hU  a  a tc  TJa lnh ln Ja hBlna iWlndfqBx d i emluqBsJd x d lmk hBJpr zEqBi   Ja i b a  hWem  egd sJx k h h td o h tgluqBi b a Ji b lnd h i e              b qBiemlnd x x  a tgh tno egJsT  h Tlnd o qBx x k  h t d i emluqBi b a h   i        r    Z    L UcN    U       LWSUL I  a     SUHOcL K  L U  I          S     HO   I  vwiln Jdfe a AT a tnd oa iWlc  a y a iJa tuqUlnac q lnhBluqBxBhBsBlnaceml b q egace h t ln Ja         iJa lm  h tn  y    Jacega b q egace b h i egdfemlnac OhB E  aegacIWJa i b ace hB b q egaceacq bu  Jh t acq bu  egacIWJa i b a    a tuqBi  Th ox k bu JhWega q Td I a tna iWl i Jo s a t hB a  df Ta i b aiJhT Tace O    B             W tnaceg acb lnd  a x k C a  egac Eln Jtna a  Td I a tna iWl h tno e hB d o h tgluqBi b a Ji b lnd h id i ln Jdfe a AT a tnd oa iWlue      Ja E tueml h iJa   q e tna Jtnacega iWlnac  s k vnA z   e  q e d   i GIW qUlnd h i    Jh t ln Ja egacb h i   h iJa T  a h iJx k q  J Tac  q  J Td lnd h i qBxqBtub es a   lm  a a i ln Ja  qBtna iWlue hB ln Ja a  df Ta i b a iJhT Tace E  Jh t ln Ja ln Jd tu  h iJa    a b qBtntnd ac  h Tl  x y h tnd ln Jo  Jx x kYqBi  Yq  J Tac qBx x ln Ja iJacb acenenqBtnk  qBtub e C a ln Ja i tuqB  i           h iOln Ja ln Jtna a d o h tgluqBi b a Ji b lnd h i e      Ja tnacegJx lue qBtna eg JhU  i d i  d y Jtna T  e a AT acb lnac   Td I a tna iWl tna Jtnacega iWluqUlnd h i e hB ln Ja d o h tg  luqBi b a Ji b lnd h i e k d a xf Tac   Td I a tna iWl a tntnh tue    J Td iJy qBtub e s a lm  a a iEln Ja qBtna iWlue hBa  df Ta i b a iJhT Tace sJtnd iJyWeb h i egdf    a tuqBsJx a tnac T b lnd h i d i a tntnh tc   qBd tnac h iJa  luqBd x lg lnaceml qUl    y     WF x a  a x eg JhU  e ln  qUl ln Jad oJtnhU a oa iWldfeegd yB  iJd E b qBiWlcEj d i b a ln Ja iJa   d o h tgluqBi b a Ji b lnd h i   q e a   iJa  yqBtub e d lued iTI Ja i b a h iEln Ja tnJiJiJd iJy lnd oa   q e od iT  d o qBx   J Td iJy oh tna qBtub e lnh y a lln Ja a AJq b l d o h tgluqBi b a Ji b lnd h i  h tno  Tdf   iJhBl d oJtnhU a ln Ja tnacegJx lue  sJTl h iJx k         x     x                           Hellingers distance  Hellingers distance                                                                                             EPIS  Parents Algorithm  EPIS  All             All  x      d y Jtna TOAU a x x d iJy a t  e  TdfemluqBi b a hB ln Ja              Hellingers distance  Bq x y hB  tnd ln Jo h i ln Jtna a  Td I a tna iWl h tno e hB d o h tgluqBi b a Ji b lnd h i h i        r  oUunu n o  y emluqBi  Je h t ln Ja d o h tgluqBi b a Ji b   lnd h i   d ln  q  J Td lnd h i qBxqBtub es a lm  a a i  qBtna iWluehBra  df Ta i b a   o emluqBi  Je h t ln Ja d o h tgluqBi b aEJi b lnd h iE  d ln EqBx x q      Td lnd h i qBx qBtub e  W Jo s a tue s acegdf Ta ln Ja s hATJx hBlue qBtna ln Ja oac TdfqBi a tntnh tue   Parents Algorithm                  o q  Taln Ja qBx y h tnd ln Jo x acene aA b d a iWlc z   UqBx Jad i ln Jdfe b q ega dfe    J   J     Ja tnacegJx lueb x acqBtnx k qBy tna a   d ln  h JtqBi qBx kTegdfe hB ln Ja d iTI Ja i b ahB a  df Ta i b a     BZ AC  L UcN    U       LWSUL I  a  S     HO   I    SUHOcL K  L U  I         HJI P                     vwi ln Jdfe a AT a tnd oa iWlc   a  egac Oln Ja enqBoa a AT a tnd oa iWluqBx ega lnJ qBi  O Tdf Oegh oa a AT a tnd oa iWlue h iln Ja         qBi                      iJa lm  h tn Te      JaOtnacegJx lue qBtna eg JhU  iyd i  d y Jtna J yWqBd i  q  J Td iJyqBtub e qBoh iJy ln Ja  qBtna iWluehB a  df Ta i b aiJhT Tace sJtnd iJyWe d ooac TdfqUlnad oJtnhU a oa iWlue h t              J Td iJy oh tna qBtub e lnh y a l ln Ja a AJq b l d o h tg  luqBi b aJi b lnd h ih tno  Tdf iJhBl d oJtnhU a ln Ja tnacegJx lue rsJTl h iJx k o q  Taln Ja qBx y h tnd ln Jo x acene a A b d a iWlc  D  E  O N  O  OmO N  vwiln Jdfe  qB a tc   a q  J Ttnacene q  a k Jtnh sJx a ohBd o h tgluqBi b a enqBoJx d iJy d i qk acegdfqBi iJa lm  h tn Te  ln Jatna Jtnacega iWluqUlnd h i hB ln Ja d o h tgluqBi b a Ji b lnd h i   k Jdfb qBx x k    a tna Jtnacega iWl qBi d o h tgluqBi b a Ji b lnd h i q e q   q b lnh tnd cqUlnd h i Bd a  Wq egacIWJa i b a hB b h i  Td lnd h i qBx Jtnh s qBsJd x d lmk luqBsJx aceA A z   euA  C a eg qBx x k b qBiJiJhBl qUI h tu  lnh b qBxfb JxfqUlna qBi   emlnh tna ln Ja a AJq b l h tno e hB ln JaEA z   e     Ja tna h tna   Td I a tna iWl qBJJtnhATd o qUlnd h i e   q a s a a iEluqB  a i  C a tna  d a   ac  ega  a tuqBxr h JJxfqBt qBJJtnhATd o qU  lnd h iEemlntuqUlna y d ace h t ln JaA z   e qBi    h d iWl h Tl ln Ja d t x d od   luqUlnd h i e   olna t ln  qUlc s q egac  h iEqBiEqBi qBx kTegdfe hB ln Ja d iTI T  a i b a hB a  df Ta i b ad iE qk acegdfqBi iJa lm  h tn Te    a Jtnh  hWegaqBi                                       EPIS  Parents Algorithm  All   d y Jtna JOAU a x x d iJy a t  e  TdfemluqBi b a hB ln Ja          qBx y hB  tnd ln Jo h i ln Jtna a  Td I a tna iWl h tno e hB d o h tgluqBi b a Ji b lnd h i h iA H A         qBi  A U A                      qBJJtnhATd o qUlnd h iemlntuqUlna y k ln  qUl qBd o e qUl q b b h oohT JqUlnd iJy ln Ja ohWeml d o h tgluqBiWl q  J Td lnd h i qBx  Ta  a i  Ta i b a d iWlntnhT T b ac  s kln Ja a  df Ta i b a     Ja Jtnh  hWegac  d o h tgluqBi b a Ji b lnd h i dfeacq egd a t lnhd iWlna tnJtna lc  Jt a AT a tnd oa iWluqBx tnacegJx lue qBxfegh eg JhU  ln  qUl ln Ja iJa   qBJJtnhATd o qUlnd h i  emlntuqUlna y k hBI a tueqBi d ooac TdfqUlna d oJtnhU a oa iWl hB ln Ja IW qBx d lmk hB ln Ja d o h tg  luqBi b a Ji b lnd h i  k d iWlntnhT T b d iJy oh tna  qBtuqBoa lna tue  ln Ja d oJtnhU ac  d o h tgluqBi b a Ji b lnd h i h tnoCqBxfegh sJtnd iJyWe o  bu   hBlna iWlndfqBxBh tr Tk i qBodfb enqBoJx d iJy qBx y h tnd ln Jo e q e ln Ja k b qBi x acqBtni ln Ja h tna lndfb qBx x k s a lglna t d o h tgluqBi b a Ji b lnd h i e  Ogf N OVe   rO         N  J      Jdfe tnacegacqBtubu   q e egJJ h tglnac s k ln Ja  d t Jh tub a  A b a Bh  jTb d a iWlnd E bzR acegacqBtubu y tuqBiWlue D  WBGFT  GF HFTJ   sW C a ln  qBiJ  ega  a tuqBx qBiJh i k oh  etna  d a   a tue hB ln Ja   vmW b h iT  a tna i b a h t ega  a tuqBxJd i egd y  WlgJx b h ooa iWlue ln  qUl x ac lnh d o   JtnhU a oa iWlue d i ln Ja  qB a tc  x x a AT a tnd oa iWluqBx  JqUluq   q a s a a i h sTluqBd iJac   egd iJ  y   IJ  KL Wq  qk acegdfqBid iTa tna i b a a iT    y d iJa Ta  a x h  ac EqUl ln Ja   acb dfegd h i j kTemlna o e prqBs h tuqUlnh tnk  N OPO QPQPQ uTW   W g   OPRTSJB B  qBi   qUqBd xfqBsJx a qU l M  W  y       g  N   r  VU   U  B E   A Ja iJy qBi  zh   T  tnJc T a x  W  w vgj OT i q  JqBT   lnd  a d o h tgluqBi b a enqBoJx d iJy qBx y h tnd ln JoEh t a  df Ta iWlndfqBx tnacq egh iJd iJy d i xfqBtny a  qk acegdfqBi iJa lm  h tn Te    WT   Tu n oU      u oo X i o  oU   n owu o o Wu n iu  u Y u y uuoUun i      JO    F      JUB  J   U  B SE A  Ah i qUlndT Jj JA a tglniJa tcOa  qBi p a  Ji TqBi  zh       tnJc T a x  iT  x d iJa emln  Ta iWl ohT Ta x d iJy h t b hWq bu Jac    Jtnh sJx a oegh x  d iJy egd iJy  qk acegdfqBi iJa lm  h tn Te Tvwiz un  iuuuuuoUoo n Uy      o J  u  o    o   n owu u n oUoo  Un oU   U n   u unu n iuu Un Zry u u v co u  oo n    ZVv    P   UJ qBy ace F  JH FJ  D T   d a iJi qJ W a l   a h tn      s Jj Jtnd iJy a  t  a tnxfqBy  B   E R   JJiJy qBi     a  gA UA  qBiJy BC a d y  Jd iJy qBi  d iWlna y tuqUlg  d iJy a  df Ta i b a h temlnhTbu  q emlndfb egd o JxfqUlnd h i d i  qk acegdfqBi iJa lm  h tn Te  vw   i h  U a iJtnd h i  R  j   q bu Wlna tc p  a qBi qBx qBi     U  p a ooa tcJac Td lnh tue a  Z n iuu u owoUoon ouoon  u oo X i o  oU  n owu o o Wu n iuc u bU  qBy ace B  GFJT  J  W a     a h tn    W    a       J   G xfega  d a t jTb d a i b a z JsJx dfeg Jd iJy Ah o qBi k Tvwi bB B DFEVU  A a   a   a   qk acegdfqBiEd iTa tna i b a  d iRacb h iJh oa lntndfb ohT Ta xfe  egd iJa y h h iWlna A qBtnx hYd iWlna y tuqUlnd h i e  d iuUn      u ou o  iuoU   sTA  WA O    J  sfF       JT     J B S E     U acbu  a tno qBi z tnh s qBsJd x dfemlndfb egd od xfqBtnd lmk iJa lm  h tn Te  g u o Uou wy  B A A O    sfFT    J  Jy      J B   E h   U a iJtnd h i  z tnh  qByWqUlnd iJy Ji b a tgluqBd iWlmk d i  qk acegdfqBi iJa lm  h tn Te s kJtnh s qBx dfemlndfb x h y dfb enqBoJx d iJy vwh i Z n iuu u  owoUoon ou oon  u oo X i o  oUN  n owu o o Wu n iuu    qBy ace   D GF c   J W a   a h tn     W  a         J  G xfega  d a t jTb d a i b a z JsJx dfeg T  d iJy Ah o qBi k Tvwi bB B s E h F U a iJtnd h i  j h oa Jtuq b lndfb qBx dfenegJace d i b h i emlntn b lnd iJy s a x d a  iJa lm  h tn Te  vwe i Z n iuu u owoUoon ouEoon  u oo X i o  oU   n  owu o o Wu n iuu  W qBy ace cJH F   s  J  G xfega  d a t jTb d a i b a z JsT  x dfeg Ja tue      W h tgln   U h x xfqBi       J B   E p    N  U a tni qBi  Ta   j N h h tuqBx qBi  E  jTqBx oa tnh i  h h iWlna A qBtnx h qBx y h tnd ln Jo h t Jtnh s qBsJd x dfemlndfb Jtnh  qByWqU  lnd h i d i s a x d a riJa lm  h tn Tes q egac  h i d o h tgluqBi b a enqBo   Jx d iJy qBi  EemlntuqUlnd E ac Eegd o JxfqUlnd h iElnacbu JiJdfIWJace   n  owu u n oUoo  Un oUi  WT   Tu n oU        un     o   oUowj u Y uuoy Un oo n B    JO F  GFOJ       J B  E A N  ah   h xfqB  dfe qBi  Ez N W qBiJh  h Jx hWe  qk acegdfqBi o Jx   lnd UqBtndfqUlna odfb tnhB wqBy y tna yWqUlnd h i Ji  Ta t ln Ja U a x x d iJy a  t  e  TdfemluqBi b a b tnd lna tnd h i    Y u y uuoUun i oon  l ki o  oU y owoUoofy oo  i y  D AmA O    sfF BJ B J  B c  E ag  h JtnJ  k   a JC a dfene  qBi    h  U h tu JqBi  p h h  k s a   x d a  Jtnh  qByWqUlnd h iOh t qBJJtnhATd o qUlna d iTa tna i b a OO i a oJd tndfb qBx emln  Tk  vw  i  unciuuuuuoUoo n Uy      o J  u  o  uowuuu n  o   n  n   oUA   U n   u unu n iuuUm n Z n iuu u owoUoon ouoon  u oo X i o  oU  n owu o o Wu n iu  u   Z  H   n H U  qBy ace DW  sfF DOsBT jTqBk i JtuqBiT  b dfenb h  A      JO h h tnyWqB i a qBTo qBiJi z JsJx dfeg Ja tue   B  E j  h   x o emlnac k  n unu  unu y u n ooon  oUn o y U xcoon  o u i ofy o  Un un u  u    y  z    ln Jacegdfe  jWluqBiTh tu E iJd  a tg  egd lmk    G iJy d iJa a tnd iJyB  G b h iJh odfb j kTemlna o e  a  qBtglnoa iWlc     J   B SE p   tglnd   u  uui ooon     un   o   oUowu  u  r oo   oU  i oo  UnJy oon          uo     ou  i o Tunuuo    oUoonJy  z     ln Ja    Vo  ge dfe T tnhU  i  iJd  a tuegd lmk  Ah oJTlna t jTb d a i b a   a  qBtglg  oa iWlc B J  B    E p   tglnd  qBi   p    a qBa x sJx d iJy   JqBTlnd  a d o h tgluqBi b a enqBoJx d iJyYh t acemlnd o qUlnd h iEd iEemlntn b lnJtnac E Th o qBd i e  vwk i  unciuuuuuoUoo n Uy      o J  u    o   n n   oU   U n   u unu n iuuUn Z n iuu u owoUoon ou  oon  u oo X i o  oU   n owu o o Wu n iuh u   Z  H Ppnp  U  qBy acNe D DWG F D   D  h h tnyWqBC i a qBTo qBiJi z JsJx dfeg Ja tuerjTqBi JtuqBi b dfenb h  A qBx d h tniJdfqJJB  J B   DFEVU  z acqBtnx  un uuo u oo ofy oo    i Y uuoy Un oo n Ooo  n  n owu o o Wu n o  uy owu   y   g u o Uou wy        o   y o  u    u   n   u unu n iuu   h h tg  yWqB   i a qBTo qBiJiEz JsJx dfeg Ja tue  vwi bB jTqBk i hEqUlna h  A         J B S E h  z tuq  T  qBi A  z tnhUUqBi        h df J Tx a lnh i OqBi   h   U a iJtnd h i   aiJhU  x ac Ty a a iJy d iJa a tnd iJy h t xfqBtny a s a   x d a  iJa lm  h tn Te Evwy i  unciuuuuuoUoo n Uy   E o J  u   u n  o   n  n   oU    U n   u unu n iuu Uq n Z n iuu u owoUoon ou oon  u oo X i o  oUE   n  owu o o Wu n iu  u   Z  H      U  qBy acC e D  FD F D  JrjTqBH i hEqUlna h  A      FD   h h tnyWqB i a qBTo qBiJi z JsJx dfeg Ja tue  vwi bB B c  E R    a A  R JsJd i emlna d i    o    T oUoo  Un oUn oE o J u v Un owu  oUu     v u  o JcoUr  U h  Ji COd x a   k syj h i e     J  B   s E  TjTqBx oa tnh i W JA qBiJh WqBi   j    h h tuqBx vwo h tgluqBi b a enqBoJx d iJy d i qk acegdfqBiEiJa lm  h tn Te  egd iJy Jtnh s qBsJd x d lmk lntna ace           TowoUoo  Un oU   owoUoofy oo  i yoUn i o o oUowo  n oU  u  y ofy   FD O     sfF D    JWB  J B     E R  j   q bu Wlna tc  rA     o sJtnhWegd h  qBi     Ta b x  q a tnh  j k o s h x dfbEJtnh s qBsJd x dfemlndfbEd iTa tna i b a d iC a x d a iJa lg    h tn Te  vwi    l  pUT qBy ace BG F    J      J B    E R     j   q bu Wlna t  qBi   h   z a hBlc j d o JxfqUlnd h i qBJJtnhWq bu Jace lnh y a iJa tuqBx Jtnh s qBsJd x dfemlndfb d iTa tna i b a h i s a x d a  iJa lm  h tn Te  vwq i h    U a iJtnd h i    R  j   q bu Wlna tc p  a qBi qBx qBi t   U  p a ooa tc ac Td lnh tue c  Z n iuu u owoUoon ou oon  u oo X i o  oU   n owu o o Wu n iu  u bU  qBy ace  TH FJF  J T  W a   a h tn  C  W  a        J G xfega  d a tEjTb d a i b aYz JsJx dfeg Jd iJy Ah o qBi k  vwi bB B B  E A I  a  qBi qBi      h J  U   tnJc T a x  i d o h tg  luqBi b a enqBoJx d iJy qBx y h tnd ln Jo s q egac  h i a  df Ta i b a Jtna   Jtnh  qByWqUlnd h i  vwi  unciuuuuuoUoo n Uy   Y o Ja u u B o    Un    u unu n iuu Uv n Z n iuu u owoUoon ouoon  u oo X i o  oU   n owu o o Wu n iuu   Z  H Pp   U  qBy aceW  D FT   J k  h h tnyWqBd i a qBTo qBiJi z JsJx dfeg Ja tue jTqB i JtuqBi b dfenb h  A qBx d h tniJdfqJJB    J B T E W Jp    w   qBiJyqBi      z h h x a   egd oJx a qBJJtnhWq bu  lnh  qk acegdfqBiEiJa lm  h tn Eb h oJTluqUlnd h i e vwi  unc i      o Ju   u n  o   oUn o oUo  oU  n  U n   u unu n iuu Un  u oo X i o  oUV   n owu o  o  Wu n iuu J qBy ace   s H F   s  J    FD     
