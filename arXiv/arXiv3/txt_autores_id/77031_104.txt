  We consider the problem of using a heuristic policy to improve the value approximation by the Upper Confidence Bound applied in Trees  UCT  algorithm in non adversarial settings such as planning with large state space Markov Decision Processes  Current improvements to UCT focus on either changing the action selection formula at the internal nodes or the rollout policy at the leaf nodes of the search tree  In this work  we propose to add an auxiliary arm to each of the internal nodes  and always use the heuristic policy to roll out simulations at the auxiliary arms  The method aims to get fast convergence to optimal values at states where the heuristic policy is optimal  while retaining similar approximation as the original UCT in other states  We show that bootstrapping with the proposed method in the new algorithm  UCT Aux  performs better compared to the original UCT algorithm and its variants in two benchmark experiment settings  We also examine conditions under which UCT Aux works well      Introduction  Monte Carlo Tree Search  MCTS       or more specifically Upper Confidence Bound applied in Trees  UCT       is a state of the art approach to solving large state space planning problems  Example applications of the UCT algorithm in the games domain include Go          General Game Playing      Real Time Strategy Game      etc  The algorithm estimates the value of a state by building a search tree using simulated episodes  or rollouts  via interactions with the simulator  Instead of sampling every branch equally  the goal is to focus samplings in tree branches that are more promising  In particular  UCT achieves that by choosing the action  or arm if its parent node is regarded as a multi armed bandit  with the highest estimated upper bound to simulate at every internal node  and randomly selects actions after leaving the tree to finish the rollout  Because UCT uses random sampling to discover nodes with good return  it could take a long time to achieve good performance  To address this problem  many enhancements have been used to improve the search control of the algorithm by either     tweaking the action selection formula at the internal nodes              or and     designing better informed rollout policies in place of random sampling at the leaf nodes          We consider the problem of using a heuristic function to improve the approximated value function computed by UCT  Taking the approaches above  the first method is to initialize new tree nodes with heuristic values and the second is to use the chosen heuristic to roll out simulations at the leaf nodes  As intended  these two methods could greatly influence the search control by guiding it into more promising regions that are determined by the heuristic  However  when the heuristic function does not accurately reflect the prospect of the states  it could feed the algorithm with false information  thereby leading the search into regions that should be kept unexplored otherwise  In this work  we propose a novel yet simple enhancement method  Given a heuristic in the form of an imperfect policy   the method adds an additional arm at every internal node of the search tree  This special arm is labeled by the action suggested by  and once selected  rolls out the rest of the sampling episode using   If the policy  works well at a state  we expect it to quickly give a good estimate of the value of the state without relying too much on the other arms  The method aims to get fast convergence to optimal values at states where the heuristic policy is optimal  while retaining similar approximation as the original UCT in other states  We compared this method with two aforementioned techniques in two domains  namely Obstructed Sailing  an extension of the original Sailing problem previously used to measure UCTs performance in      and Sheep Savior  a large state space MDP that characterizes a generic two player collaborative puzzle game  The results showed that UCT Aux the new algorithm  Aux for auxiliary arms  significantly outperforms its competitors when coupled with reasonable heuristics  One nice property of this method is that it does not affect the implementation of other bootstrapping techniques  No modification of the action selection formula nor the rollout policy at any leaf nodes except for the added arms is required  This allows different sources of bootstrapping knowledge to be combined into one algorithm for more performance boost  The rest of the paper is structured as follows  We first give a brief overview of MDP  UCT and its popular enhancements before presenting UCT Aux  Next  we describe two experimental setups for comparing the agents performance and analyze the results  We also identify the common properties of the heuristics used in two experimental domains and provide some insights on why UCTAux works well in those cases  Finally  we conclude the paper by discussing the possible usage of UCT Aux          Background Markov Decision Process  A Markov Decision Process characterizes a planning problem with tuple  S  A  T  R   in which  S is the set of all states     A is the set of all available actions   Ta  s  s      P  st     s   st   s  at   a  is the probability that action a  A in state s  S at time t will lead to state s   S at time t       Ra  s  s    is the immediate reward received after the state transition from s to s  triggered by action a  An action policy  is a function  possibly stochastic  that returns an action  s  for every state s  S  In infinite horizon discounted MDPs  the objective is to choose an action policy   that maximizes some cumulative of the P function t  received rewards  typically the expected discounted sum t    Ra  st   st     with         being the discount factor  An MDP can be effectively solved using different methods  one of which is the value iteration algorithm based on the Bellmans equation      The algorithm maintains a value function V  s   where s is a state  and iteratively updates the value function using the equation   X       Vt    s    max Ta  s  s   Ra  s  s     Vt  s      a  s   This value iteration algorithm is guaranteed to converge to the optimal value function V   s   which gives the optimal expected cumulative reward of running the optimal policy from state s  The optimal value function V  can be used to construct the optimal policy P by taking action a in state s such that a   argmaxa   s  Ta  s  s   V   s      The optimal Q function is constructed from V  as follows  X Ta  s  s    Ra  s  s      V   s      Q  s  a    s   Q  s  a  denotes the maximum expected long term reward of an action a when executed in state s  One key issue that hinders MDPs and Value Iteration from being widely used in real life planning tasks is the large state space size  usually exponential in the number of state variables  that is often required to model realistic problems       Upper Confidence Bound Applied to Trees  UCT   UCT     is an anytime algorithm that approximates the state action value in real time using Monte Carlo simulations  It was inspired by Sparse Sampling       the first near optimal policy whose runtime does not depend on the size of the state space  The approach is particularly suitable for solving planning problems with very large or possibly infinite state spaces  The algorithm searches forward from a given starting state  building up a tree whose nodes alternate between reachable future states and state action pairs  Figure     State nodes are called internal if their child state action pairs have been expanded and leaf otherwise  Starting with a root state  the algorithm iteratively rolls out simulations from this root node  each time an internal node is encountered  it is regarded as a multi armed bandit and UCB       is used to   a   S       S  a       a   S       a   a  S  a       S       S      a   a   S  a       S  a       S  a       S  a       S       S       S       S       Fig     A sample UCT search tree with two valid actions a  and a  at any state  Circles are state nodes and rectangles are state action nodes  solid state nodes are internal while dotted are leafs   determine the action or arm to sample  i e   the edge to traverse  In particular  at an internal node s  the algorithm selects an action according to s     log n s        U CT  s    argmax QU CT  s  a     Cp n s  a  a in which  QU CT  s  a  is the estimated value of state action pair  s  a   taken to be the weighted average of its childrens values   Cp     is a suitable hand picked constant   n s  is the total number of rollouts starting from s   n s  a  is the number of rollouts that execute a at s  At the chosen child state action node  the simulator is randomly sampled for a next state with accompanying reward  new states automatically become leaf nodes  From the leaf nodes  rollouts are continued using random sampling until a termination condition is satisfied  such as reaching terminal states or simulation length limit  Once finished  the returned reward propagates up the tree  with the value at each parent node being the weighted average of its child nodes values  suppose the rollout executes action a at state s and accumulates reward R s  a  in the end   at state action nodes  n s  i    n s  i      and QU CT  s  a    QU CT  s  a      n s a   R s  a   QU CT  s  a    at state nodes  n s    n s       Typically one leaf node is converted to internal per rollout  upon which its child state action nodes are generated  When the algorithm is terminated  the roots arm with highest QU CT  s  a  is returned       In practice  returning the arm with highest n s  a  is also a common choice    When used for infinite horizon discounted MDPs  the search can be cut off at an     horizon  Given any        with    small enough  the algorithm is proven to converge to the arm whose value is within the   vicinity of the optimal arm           Enhancement methods  In vanilla UCT  new state action nodes are initialized with uninformed default values and random sampling is used to finish the rollout when leaving the tree  Given a source of prior knowledge  Gelly and Silver     proposed two directions to bootstrap UCT     Initialize new action state nodes with n s  a    nprior  s  a  and QU CT  s  a    Qprior  s  a   and    Replace random sampling by better informed exploration guided by prior   We refer to these two algorithms as UCT I  UCT with new nodes initialized to heuristic values  and UCT S  UCT with simulations guided by prior    UCTIS is the combination of both methods  UCT I and UCT S can be further tuned using domain knowledge to mitigate the flaw of a bad heuristic and amplify the influence of a good one by adjusting the dependence of the search control on the heuristic at internal nodes  In this work  we do not investigate the effect of such tuning to ensure a fair comparison between techniques when employed as is  In the same publication      the authors proposed another bootstrapping technique  namely Rapid Action Value Estimation  RAVE   which we do not examine in this work  The technique is specifically designed for domains in which an action from a state s has similar effect regardless of when it is executed  either at s or after many moves  RAVE uses the All Moves As First  AMAF  heuristic      instead of QU CT  s  a  in Equation   to select actions  Many board games such as Go or Breakthrough     have this desired property  In our experiment domains  RAVE is not applicable  because the actions are mostly directional movements  e g    N  E  S  W    thus tied closely to the state they are performed at      UCT Aux  Algorithm  Given an added policy   we propose a new algorithm UCT Aux that follows the same search control as UCT except for two differences     At every internal node s  besides  A s   normal arms with A s  being the set of valid actions at state s  an additional arm labeled by the action  s  is created  Figure        When this arm is selected by Equation    it stops expanding the branch but rolls out a simulation using   value update is carried out from the auxiliary arm up to the root as per normal  The method aims to better manage mixed quality heuristics  If the heuristic s value estimation at a state is good  we expect the added arm to dominate   S       a  S  a       a    s    S  a       S    s             a  S  a       S      a  S  a        s    S       S    s        S      a   a    s    S  a       S  a       S    s                   S       S       S       S       Fig     Sample search tree of UCT Aux   the distribution of rollouts and quickly give a good estimate of the states value without the need to inspect other arms  Otherwise  the search control will focus rollouts in ordinary arms  thus retaining similar approximation as vanilla UCT  For stochastic heuristic policies  at every internal node  not one but  auxiliary arms are added  with  being the number of actions a such that P   s    a        As such  the number of arms at internal nodes is bounded by   A  since    A   Convergence Analysis We will show that regardless of the added policys quality  UCT Aux converges in finite horizon MDPs    The proof follows closely that of UCT analysis by treating the auxiliary arms as any other ordinary arms  As a recap  UCT convergence analysis revolves around the analysis of non stationary multi armed bandits with reward sequences satisfying some drift conditions  which is proven to be the case for UCTs internal nodes with appropriate choice of bias sequence Cp     In particular  the drift conditions imposed on the payoff sequences go as follows  Pn  The expected values of the averages X in   n  t   Xit must converge for all arms i with n being the number of pulls and Xit the payoff of pull t  Let in   E X in   and i   limn in    Cp     can be chosen such that the tail inequalities P  X i n i   i  ct n i     q ln t t  and P  X i n i   i  ct n i     t  are satisfied for ct n i     Cp n i  with n i  being the number of times arm i is pulled up to time t  Firstly  we will show that all internal nodes of UCT Aux have arms yielding rewards satisfying the drift conditions  Suppose the horizon of the MDP is D  the number of actions per state is K and the heuristic policy is deterministic               As mentioned in      for use with discounted infinite horizon MDPs  the search tree can be cut off at the effective     horizon with    being the desired accuracy at root  Empirically  Cp is often chosen to be an upper bound of the accumulated reward starting from the current state    this can be proven using induction on D  Note that i i d  payoff sequences satisfy the drift conditions trivially due to Hoeffdings inequality   D      Suppose the root has already been expanded  i e   become internal  It has K     arms  which either lead to leaf nodes  ordinary arms  or return i i d  payoffs  auxiliary arm   Since leaf nodes have i i d  payoffs  all arms satisfy drift conditions   D      Assume that all internal state nodes under the root have arms satisfying the drift conditions  e g   s  and s  in Figure    Consider any ordinary arm of the root node  the added arms payoff sequence is already i i d    for instance   s    a     Its payoff average is the weighted sum of payoff sequences in all leafs and state action nodes on the next two levels of the subtree  i e   leaf s    arms  s    a      s    a    and  s     s      all of which satisfy drift conditions due to either the inductive hypothesis or producing i i d  payoffs  Theorem   in     posits that the weighted sum of payoff sequences conforming to drift conditions also satisfies drift conditions  therefore  all arms originating from the root node satisfy drift conditions  As a result  the theorems on non stationary bandits in     hold for UCTAuxs internal nodes as well  Therefore  we can obtain similar results to Theorem   of      with the difference being statistical measures related to the auxiliary arms such as aux and aux   i e   the new algorithms probability of selecting a suboptimal arm converges to zero as the number of rollouts tends to infinity      Experiments  We compare the performance of UCT Aux against UCT  UCT I  UCT S and UCT IS in two domains  Obstructed Sailing and Sheep Savior  Obstructed Sailing extends the benchmark Sailing domain by placing random blockage in the map  the task is to quickly move a boat from one point to a destination on a map  disturbed by changing wind  while avoiding obstacles  Sheep Savior features a two player maze game in which the players need to herd a sheep into its pen while protecting it from being killed by two ghosts in the same environment      Obstructed Sailing  The Sailing domain  originally used to evaluate the performance of UCT      features a control problem in which the planner is tasked to move a boat from a starting point to a destination under certain disturbing wind conditions  In our version  there are several obstacles placed randomly in the map  see Figure  a   In this domain  the state is characterized by tuple hx  y  b  wprev   wcurr i with  x  y  being the current boat position  b the current boat posture or direction  wprev the previous wind direction and wcurr the current wind direction  Directions take values in  N  NE  E  SE  S  SW  W  NW   i e  clockwise starting from North  The controllers valid action set includes all but the directions against    a  Obstructed Sailing sample map   b  SailTowardsGoal  Fig     Obstructed Sailing domain   a  a randomized starting configuration   b  SailTowardsGoal heuristic produces near optimal estimates policies in good cases but misleads the search control in others   wcurr   out of the map or into an obstacle  After each time step  the wind has roughly equal probability to remain unchanged  switch to its left or its right       Depending on the relative angle between the action taken and wcurr   a cost from   to   minutes is incurred  Additionally  changing from a port to a starboard tack or vice versa causes a tack delay of   minutes  In total  an action can cost anywhere from   to   minutes  i e   Cmin     and Cmax           We model the problem as an infinite horizon discounted MDP with discount factor               Choice of heuristic policies  A simple heuristic for this domain is to select a valid action that is closest to the direction towards goal position regardless of the cost  thereafter referred to as SailTowardsGoal  For instance  in the top subfigure of Figure  b  at the starting state marked by S  if current wind is not SW  SailTowardsGoal will move the boat in the NE direction  otherwise  it will execute either N or E  This heuristic is used in UCT I and UCT IS by initializing new state action nodes with values nST G  s  a            d s  g    QST G  s  a   C s  a    Cmin   with C s  a  being the cost of executing action a at state s and d s    g  the minimum distance between next state s  and goal position g  The initialized   value can be seen as the minimum cost incurred when all future wind directions are favorable for desired movement  For UCT S  the random rollouts are replaced by  s    argmaxa QST G  s  a   Heuristic quality  This heuristic works particularly well for empty spaces  producing near optimal plans if there are no obstacles  However  it could be counterproductive when encountering obstacles  In the bottom subfigure of Figure  b  if a rollout from the starting position is guided by SailTowardsGoal  it could be stuck oscillating among the starred tiles  thus giving inaccurate estimation of the optimal cost       Setup and results  The trial map size is    by     with fixed starting and goal positions at respectively        and           Figure  a   We generated     randomized instances of the map  where obstacles are shuffled by giving each grid tile p       chance to be blocked     Each instance is tried five times  each of which with different starting boat postures and wind directions   Fig     Performance comparison of UCT  UCT S  UCT I  UCT IS and UCT Aux when coupled with the heuristic SailTowardsGoal  y axis is the reward average with error bars being the standard errors of the means   All UCT variants  UCT  UCT I  UCT S  UCT IS and UCT Aux  use the same Cp   Cmax              and the search horizon  is set to be      an       We tried with different values of p                             and they all yield similar results as Figure    the detailed charts are not presented due to space constraint  The search horizon is chosen to be long enough so that the cost accumulated after the horizon has small effect to the total cost    optimal path should not be very far from    steps as most actions move the boat closer to the goal  The exact optimal policy is obtained using Value Iteration  Note that the performance of Optimal agent varies because of the randomization of starting states  initial boat and wind direction  and map configurations  Given the same number of samplings  UCT Aux outperforms all competing UCT variants  despite the mixed quality of the added policy SailTowardsGoal when dealing with obstacles  Figure     Note that without parameter tuning  both UCT I and UCT S are inferior to vanilla UCT  but between UCT I and UCT S  UCT I shows faster performance improvement when the number of samplings increases  The reason is because when SailTowardsGoal produces inaccurate heuristic values  UCT I only suffers at early stage while UCT S endures misleading guidance until the search reaches states where the policy yields more accurate heuristic values  The heuristics impact is stronger in UCT S than UCTI  UCT ISs behavior is closer to UCT S than UCT I      Sheep Savior  This domain is an extension of the Collaborative Ghostbuster game introduced in      as the testbed for their assistance framework for collaborative games  The game features two players  a shepherd and a dog  whose task is to herd a sheep into its pen while avoiding it to be killed by two ghosts in a maze like environment  All non player characters  NPCs  run away from the players within a certain distance  otherwise the ghosts chase the sheep and the sheep runs away from ghosts  Since ghosts can only be shot by the Shepherd  the dogs role is strictly to gather the NPCs  Figure     Both protagonists have   movement actions  no move  N  S  E and W  while Shepherd has an additional action to inflict damage on a nearby ghost  hence a total of    compound actions  The two players are given rewards for successfully killing ghosts    points  or herding sheep into its pen     points   If the sheep is killed  the game is terminated with penalty      The discount factor in this domain is set to be            Choice of heuristic policies  The game can be seen as having three subtasks  each of which is the task of catching a single ghost or herding a single sheep  as shown in Figure    Each of these subtasks consists of only two players and one NPC  hence has manageable complexity and can be solved exactly offline using Value Iteration  A heuristic Q value can be obtained by taking the average of all individual subworlds  or subtasks  Q values  as an estimate for one state action pairs value  Specifically  at state s the policy  GoalAveraging  yields nGA  s  a     m  QGA  s  a       X Qi  si   a  m i     Fig     Task decomposition in Sheep Savior   in which si is the projection of s in subtask i  m is the number of subtasks  i e  three in this case  and Qi  si   a  are subtasks Q values  The corresponding heuristic policy can be constructed as GA  s    argmaxa QGA  s  a   Heuristic quality  GoalAveraging works well in cases when the sheep is well separated from ghosts  However  when these creatures are close to each other  the policys action estimation is no longer valid and could yield deadly results  The under performance is due to the fact that the heuristic is oblivious to the interactivity between subtasks  in this case  ghost killing sheep scenarios       Setup and results  The map shown in Figure   is tried     times  each of which with a different randomized starting configurations  We compare the means of discounted rewards produced by the following agents  Random  GoalAveraging  UCT  UCT I  UCTS  and UCT Aux  The optimal policy in this domain is not computed due to the prohibitively large state space  i e                  since each ghost has at most two health points  All UCT variants have a fixed planning depth of      In our setup  one second of planning yields roughly     rollouts on average  so we do not run simulations with higher numbers of rollouts than       due to time constraint  Moreover  in this game related domain  the region of interest is in the vicinity of     to     rollouts for practical use  As shown in Figure    UCT Aux outperforms the other variants  especially early on with small numbers of rollouts  UCT S takes advantage of GA better than UCT I  which yields even worse performance than vanilla UCT  Observing the improvement rate of UCT S we expect it to approach UCT Aux much sooner than others  although asymptotically all of them will converge to the same optimal value when enough samplings are given and the search tree is sufficiently expanded  the time taken could be prohibitively long though    Fig     Performance comparison of Random  GoalAveraging  UCT  UCT S  UCT I  and UCT Aux when coupled with Goal Averaging      Discussions  Although UCT Aux shows superior performances in the experiments above  we observe that the chosen heuristic policies share a common property that is crucial for UCT Auxs success  they show behaviors of make it or break it  In other words  at most states s  their action value estimate Q  s  a  is either near optimal or as low as that of a random policy Qrand  s  a   Specifically  in Obstructed Sailing  if following SailTowardsGoal can bring the boat from a starting state to goal position  e g   when the line of sight connecting source and destination points lies entirely in free space  the resultant course of actions does not deviate much from the optimal action sequences  However when the policy fails to reach the goal  it could be stuck fruitlessly  For instance  Figure  b depicts one such case  once the boat has reached either one of three starred tiles underneath the goal position  unless at least three to five wind directions in a row are E  SailTowardsGoal results in oscillating the boat among these starred tiles  The resultant cost is therefore very far from optimal and could be as low as the cost incurred by random movement  In contrast  an example for heuristics that are milder in nature is the policy StochasticOptimal     which issues optimal actions with probability     and random actions for the rest  This policy is also suboptimal but almost always yields better estimation than random movement  it is not as extreme as SailTowardsGoal  Figure    which charts the performance histograms of StochasticOptimal     alongside with SailTowardsGoal  shows that a majority of runs with SailTowardsGoal yield costs that are either optimal or worse than Randoms  Similarly  GoalAveraging in Sheep Savior is also extreme  By ignoring the danger of Ghosts when around Sheep  it is able to quickly herd the Sheep in the Pen or kill nearby Ghosts  good   or end the game prematurely by forcing the   Fig     Performance histograms of heuristics in Obstructed Sailing  The returned costs of a heuristic are allocated relatively into bins that equally divide the cost difference between Random and Optimal agents  x axis denotes the bin number and y axis the frequency   Sheep into Ghosts zones  bad   We hypothesize that one way to obtain extreme heuristics is by taking near optimal policies of the relaxed version of the original planning problem  in which aspects of the environment that cause negative effects to the accumulated reward are removed  For instance  SailTowardsGoal is in spirit the same as the optimal policy for maps with no obstacle  while GoalAveraging should work well if the ghosts do not attack sheep  As UCT Aux is coupled with heuristic policies with this extreme characteristic  rollouts are centralized at auxiliary arms of states where  s  is nearoptimal  and distributed to ordinary arms otherwise  Consequently  the value estimation falls back to the default random sampling where  produces inaccurate estimates instead of relying entirely on  as does UCT S       When does UCT Aux not work   Figure   charts the worst case behavior of UCT Aux when the coupled heuristics estimate is mostly better than random sampling but much worse than that of the optimal policy  e g  the heuristic StochasticOptimal     in Obstructed Sailing  The reason behind UCT Auxs flop is the same as that of UCT  i e   due to the overly optimism of UCB   as described in      At each internal node  samplings are directed into suboptimal arms that appear to perform the best so far  exponentially more than the rest  Theorem        when convergence has not started  Even though each arm is guaranteed to be sampled an infinite number of times when the number of samplings goes to infinity  Theorem         the subpolynomial rate means only a tiny fraction of samplings are spent on attempting bad looking arms  As a result  in a specially designed binary tree search case  UCT takes at least  exp exp    exp          samplings before the optimal node is discovered  the term is a composition of D    exponential functions with D being the number of actions in the optimal sequence    Fig     Bad case of UCT Aux when coupled with StochasticOptimal      Samplings UCT UCT S UCT I UCT IS UCT Aux                                                                                                                                                                                                                                                                                                                                                                                  Table    The average number of tree nodes for UCT variants in Obstructed Sailing when coupled with StochasticOptimal       UCT Aux falls into this situation when coupled with suboptimal policies whose estimates are better than random sampling  At every internal node  it artificially creates an arm that is suboptimal but produces preferable reward sequences when compared to other arms with random sampling  As a result  the auxiliary arms are sampled exponentially more often while not necessarily prescribing a good move  Table   shows some evidence of this behavior  Given the same number of samplings  UCT Aux constructs a search tree with significantly less nodes than other variants  up to    times   That means many samplings have ended up in non expansive auxiliary arms because they were preferred       Combination of UCT Aux  UCT I and UCT S  UCT Aux bootstraps UCT in an orthogonal manner to UCT I and UCT S  thus allowing combination with these common techniques for further performance boost when many heuristics are available  Figure   charts the performance of   such combinations in Obstructed Sailing  UCT Aux variants use SailTowardsGoal at the auxiliary arms while UCT I S variants use StochasticOptimal     at the ordinary arms  UCT Aux S outperforms both UCT Aux and UCT S at earlier stage  and matches the better performer among the two  i e  UCT Aux  in a long run   Fig     Combination of UCT Aux with UCT I S IS in Obstructed Sailing      Conclusion  In this work  we have introduced a novel yet simple technique to bootstrap UCT with an imperfect heuristic policy in a popular non adversarial domain  i e   planning in large state space MDPs  It is shown to be able to leverage on the well performing region while avoiding the bad regions of the policy  empirically outperforming other state of the art bootstrapping methods when coupled with the right policy  i e  the extreme kind  Our conclusion is that if such property is known before hand about a certain heuristic  UCT Aux can be expected to give a real boost over the original UCT  especially in cases with scarce computational resource  otherwise  it would be safer to employ the currently prevalent methods of bootstrapping  As such  a different mentality can be employed when designing heuristics specifically for UCT Aux  instead of safe heuristics that try to avoid as many flaws as possible  the designer should go for greedier and riskier ones  Lastly  since UCT Aux is orthogonal to other commonly known enhancements  it is a flexible tool that can be combined with others  facilitating more options   when incorporating domain knowledge into the vanilla MCTS algorithm  In the future  we plan to examine how to adapt the method to adversarial domains      Acknowledgments  This research is partially supported by a GAMBIT grant Tools for Creating Intelligent Game Agents  no  R                 from the Media Development Authority and an Academic Research Grant no  T     RES     from the Ministry of Education in Singapore   
 We apply decision theoretic techniques to construct nonplayer characters that are able to assist a human player in collaborative games  The method is based on solving Markov decision processes  which can be difficult when the game state is described by many variables  To scale to more complex games  the method allows decomposition of a game task into subtasks  each of which can be modelled by a Markov decision process  Intention recognition is used to infer the subtask that the human is currently performing  allowing the helper to assist the human in performing the correct task  Experiments show that the method can be effective  giving nearhuman level performance in helping a human in a collaborative game   Introduction Traditionally  the behaviour of Non Player Characters  NPCs  in games is hand crafted by programmers using techniques such as Hierarchical Finite State Machines  HFSMs  and Behavior Trees  Champandard        These techniques sometimes suffer from poor behavior in scenarios that have not been anticipated by the programmer during game construction  In contrast  techniques such as Hierarchical Task Networks  HTNs  or Goal Oriented Action Planner  GOAP   Orkin       specify goals for the NPCs and use planning techniques to search for appropriate actions  alleviating some of the difficulties of having to anticipate all possible scenarios  In this paper  we study the problem of creating NPCs that are able to help players play collaborative games  The main difficulties in creating NPC helpers are to understand the intention of the human player and to work out how to assist the player  Given the successes of planning approaches to simplifying game creation  we examine the application of planning techniques to the collaborative NPC creation problem  In particular  we extend a decision theoretic framework Copyright c       Association for the Advancement of Artificial Intelligence  www aaai org   All rights reserved   for assistance used in  Fern and Tadepalli       to make it appropriate for game construction  The framework in  Fern and Tadepalli       assumes that the computer agent needs to help the human complete an unknown task  where the task is modeled as a Markov decision process  MDP   Bellman        The use of MDPs provide several advantages such as the ability to model noisy human actions and stochastic environments  Furthermore  it allows the human player to be modelled as a noisy utility maximization agent where the player is more likely to select actions that has high utility for successfully completing the task  Finally  the formulation allows the use of Bayesian inference for intention recognition and expected utility maximization in order to select the best assistive action  Unfortunately  direct application of this approach to games is limited by the size of the MDP model  which grows exponentially with the number of characters in a game  To deal with this problem  we extend the framework to allow decomposition of a task into subtasks  where each subtask has manageable complexity  Instead of inferring the task that the human is trying to achieve  we use intention recognition to infer the current subtask and track the players intention as the intended subtask changes through time  For games that can be decomposed into sufficiently small subtasks  the resulting system can be run very efficiently in real time  We perform experiments on a simple collaborative game and demonstrate that the technique gives competitive performance compared to an expert human playing as the assistant   Scalable Decision Theoretic Framework We will use the following simple game as a running example  as well as for the experiments on the effectiveness of the framework  In this game  called Collaborative Ghostbuster  the assistant  illustrated as a dog  has to help the human kill several ghosts in a maze like environment  A ghost will run away from the human or assistant when they are within its vision limit  otherwise it will move randomly  Since ghosts can only be shot by the human player  the dogs   role is strictly to round them up  The game is shown in Figure    Note that collaboration is often truly required in this game   without surrounding a ghost with both players in order to cut off its escape paths  ghost capturing can be quite difficult   This algorithm is guaranteed to converge to the optimal value function V   s   which gives the expected cumulative reward of running the optimal policy from state s  The optimal value function V  can be used to construct the optimal actionsPby taking action a in state s such that a   argmaxa   s  Ta  s  s   V   s      The optimal Qfunction is constructed from V  as follows  X Q  s  a    Ta  s  s    Ra  s  s      V   s      s   The function Q  s  a  denotes the maximum expected longterm reward of an action a when executed in state s instead of just telling how valuable a state is  as does V     Figure    A typical level of Collaborative Ghostbuster  The protagonists  Shepherd and Dog in the bottom right corner  need to kill all three ghosts to pass the level   Markov Decision Processes We first describe a Markov decision process and illustrate it with a Collaborative Ghostbuster game that has a single ghost  A Markov decision process is described by a tuple  S  A  T  R  in which  S is a finite set of game states  In single ghost Collaborative Ghostbuster  the state consists of the positions of the human player  the assistant and the ghost   A is a finite set of actions available to the players  each action a  A could be a compound action of both players  If each of the human player and the assistant has   moves  north  south  east and west   A would consist of the    possible combination of both players moves   Ta  s  s      P  st     s   st   s  at   a  is the probability that action a in state s at time t will lead to state s  at time t      The human and assistant move deterministically in Collaborative Ghostbuster but the ghost may move to a random position if there are no agents near it   Ra  s  s    is the immediate reward received after the state transition from s to s  triggered by action a  In Collaborative Ghostbuster  a non zero reward is given only if the ghost is killed in that move  The aim of solving an MDP is to obtain a policy maximizes the expected cumulative reward P that t t    R st    st   st     where          is the discount factor  Value Iteration  An MDP can be effectively solved using a simple algorithm proposed by Bellman in       Bellman        The algorithm maintains a value function V  s   where s is a state  and iteratively updates the value function using the equation   X       Vt    s    max Ta  s  s   Ra  s  s     Vt  s      a  s   Intractability  One key issue that hinders MDPs from being widely used in real life planning tasks is the large state space size  usually exponential in the number of state variables  that is often required to model realistic problems  Typically in game domains  a state needs to capture all essential aspects of the current configuration and may contain a large number of state variables  For instance  in a Collaborative Ghostbuster game with a maze of size m  number of valid positions  consisting of a player  an assistant and n ghosts  the set of states is of size O mn      which grows exponentially with the number of ghosts   Subtasks To handle the exponentially large state space  we decompose a task into smaller subtasks and use intention recognition to track the current subtask that the player is trying to complete   Figure    Task decomposition in Collaborative Ghostbuster  In Collaborative Ghostbuster  each subtask is the task of catching a single ghost  as shown in Figure    The MDP for a subtask consists of only two players and a ghost and hence has manageable complexity    Human Model of Action Selection In order to assist effectively  the AI agent must know how the human is going to act  Without this knowledge  it is almost impossible for the AI to provide any help  We assume that the human is mostly rational and use the Q function to model the likely human actions  Specifically  we assume maxaAI Q i  si  ahuman  aAI    P  ahuman  wi   si      e     where  is the normalizing constant  wi represents subtask i and si is the state in subtask i  Note that we assume that the human player knows the best response from the AI sidekick and plays his part in choosing the action that matches the most valued action pair  However  the human action selection can be noisy  as modelled by Equation       Intention Recognition and Tracking We use a probabilistic state machine to model the subtasks for intention recognition and tracking  At each time instance  the player is likely to continue on the subtask that he or she is currently pursuing  However  there is a small probability that the player may decide to switch subtasks  This is illustrated in Figure    where we model a human player who tends to stick to his chosen sub goal  choosing to solve the current subtask     of the times and switching to other sub tasks     of the times  The transition probability distributions of the nodes need not be homogeneous  as the human player could be more interested in solving some specific subtask right after another subtask  For example  if the ghosts need to be captured in a particular order  this constraint can be encoded in the state machine  The model also allows the human to switch back and forth from one subtask to another during the course of the game  modelling change of mind   where T  wj  wi   is the switching probability from subtask j to subtask i  Next  we compute the posterior belief distribution using Bayesian update  after observing the human action a and subtask state si t at time t  as follows  Bt  wi  at   a  st   t       Bt  wi  t    P  at   a wi   si t       where  is a normalizing constant  Absorbing current human action a and current state into t  gives us the game history t at time t  Complexity This component is run in real time  and thus its complexity dictates how responsive our AI is  We are going to show that it is at most O k      with k being the number of subtasks  The first update step as depicted in Equation   is executed for all subtasks  thus of complexity O k      The second update step as of Equation   requires the computation of P  at   a wi   si    Equation     which takes O  A   with A being the set of compound actions  Since Equation   is applied for all subtasks  that sums up to O k A   for this second step  In total  the complexity of our real time Intention Recognition component is O k     k A    which will be dominated by the first term O k     if the action set is fixed   Decision theoretic Action Selection Given a belief distribution on the players targeted subtasks as well as knowledge to act collaboratively optimally on each of the subtasks  the agent chooses the action that maximizes its expected reward      X i  Bt  wi  t  Qi  st   a  a   argmaxa i  CAPIR  Collaborative Action Planner with Intention Recognition We implement the scalable decision theoretic framework as a toolkit for implementing collaborative games  called Collaborative Action Planner with Intention Recognition  CAPIR   Figure    A probabilistic state machine  modeling the transitions between subtasks  Belief Representation and Update The belief at time t  denoted Bt  wi  t    where t is the game history  is the conditional probability of that the human is performing subtask i  The belief update operator takes Bt   wi  t    as input and carries out two updating steps  First  we obtain the next subtask belief distribution  taking into account the probabilistic state machine model for subtask transition T  wk  wi   X Bt  wi  t      T  wj  wi  Bt   wj  t        j  CAPIRs Architecture Each game level in CAPIR is represented by a GameWorld object  which consists of two Players and multiple SubWorld objects  each of which contains only the elements required for a subtask  Figure     The game objective is typically to interact with these NPCs in such a way that gives the players the most points in the shortest given time  The players are given points in major events such as successfully killing a monster type NPC or saving a civilian type NPC  these typically form the subtasks  Each character in the game  be it the NPC or the protagonist  is defined in a class of its own  capable of executing multiple actions and possessing none or many properties  Besides movable NPCs  immobile items  such as doors or   Figure    GameWorlds components  shovels  are specified by the class SpecialLocation  GameWorld maintains and updates an internal game state that captures the properties of all objects  At the planning stage  for each SubWorld  an MDP is generated and a collaboratively optimal action policy is accordingly computed  Figure     These policies are used by the AI assistant at runtime to determine the most appropriate action to carry out  from a decision theoretic viewpoint                                                                                                                                                                                                                     Figure    CAPIRs action planning process   a  Offline subtask Planning   b  in game action selection using Intention Recognition   busters  We chose five levels  see Appendix  with roughly increasing state space size and game play complexity to assess how the technique can scale with respect to these dimensions  The participants were requested to play five levels of the game as Shepherd twice  each time with a helping Dog controlled by either AI or a member of our team  the so called human expert in playing the game  The identity of the dogs controller was randomized and hidden from the participants  After each level  the participants were asked to compare the assistants performance between two trials in terms of usefulness  without knowing who controlled the assistant at which turn  In this set of experiments  the players aim is to kill three ghosts in a maze  with the help of the assistant dog  The ghosts stochastically  run away from any protagonists if they are   steps away  At any point of time  the protagonists could move to an adjacent free grid square or shoot  however  the ghosts only take damage from the ghost buster if he is   steps away  This condition forces the players to collaborate in order to win the game  In fact  when we try the game with non collaborative dog models such as random movement  the result purely relies on chance and could go on until the time limit      steps  runs out  as the human player hopelessly chases ghosts around obstacles while the dog is doing some nonsense at a corner  Oftentimes the game ends when ghosts walk themselves into dead end corners  The twenty participants are all graduate students at our school  seven of whom rarely play games  ten once to twice a week  and three more often  When we match the answers back to respective controllers  the comparison results take on one of three possible values  being AI assistant performing better  worse or indistinguishable to the human counterpart  The AI assistant is given a score of   for a better    for an indistinguishable and    for a worse evaluation  Qualitative evaluation For simpler levels      and    our AI was rated to be better or equally good more than     the times  For level    our AI rarely got the rating of being indistinguishable  though still managed to get a fairly competitive performance  Subsequently  we realized that in this particular level  the map layout is confusing for the dog to infer the humans intention  there is a trajectory along which the human players movement could appear to aim at any one of three ghosts  In that case  the dogs initial subtask belief plays a crucial role in determining which ghost it thinks the human is targeting  Since the dogs belief is always initialized to a uniform distribution  that causes the confusion  If the human player decides to move on a different path  the AI dog is able to efficiently assist him  thus getting good ratings instead  In level    our AI gets good ratings only for less than one third of the times  but if we count indistinguishable ratings as satisfactory  the overall percentage of positive ratings exceeds       Experiment and Analysis In order to evaluate the performance of our AI system  we conducted a human experiment using Collaborative Ghost     The ghosts run away     of the times and perform some random actions in the remaining                                                                                                             Figure    Qualitative comparison between CAPIRs AI assistant and human expert  The y axis denotes the number of ratings                          AI              Human                                                             Figure    Average time  with standard error of the mean as error bars  taken to finish each level when the partner is AI or human  The y axis denotes the number of game turns  Quantitative evaluation Besides qualitative evaluation  we also recorded the time taken for participants to finish each level  Figure     Intuitively  a well cooperative pair of players should be able to complete Collaborative Ghostbusters levels in shorter time  Similar to our qualitative result  in levels      and    the AI controlled dog is able to perform at near human levels in terms of game completion time  Level    which takes the AI dog and human player more time on average and with higher fluctuation  is known to cause confusion to the AI assistants initial inference of the humans intention and it takes a number of game turns before the AI realizes the true target  whereas our human expert is quicker in closing down on the intended ghost  Level    larger and with more escape points for the ghosts but less ambiguous  takes the protagonist pair  AI  human  only      more on average completion time   Related Work Since plan recognition was identified as a problem on its own right in       Schmidt  Sridharan  and Goodson        there have been various efforts to solve its variant in different domains  In the context of modern game AI research  Bayesian based plan recognition has been inspected using  different techniques such as Input Output Hidden Markov Models  Gold        Plan Networks  Orkin and Roy        text pattern matching  Mateas and Stern        n gram and Bayesian networks  Mott  Lee  and Lester       and dynamic Bayesian networks  Albrecht  Zukerman  and Nicholson        As far as we know  our work is the first to use a combination of precomputed MDP action policies and online Bayesian belief update to solve the same problem in a collaborative game setting  Related to our work in the collaborative setting is the work reported by Fern and Tadepalli  Fern and Tadepalli       who proposed a decision theoretic framework of assistance  There are however several fundamental differences between their targeted problem and ours  Firstly  they assume the task can be finished by the main subject without any help from the AI assistant  This is not the case in our game  which presents many scenarios in which the effort from one lone player would amount to nothing and a good collaboration is necessary to close down on the enemies  Secondly  they assume a stationary human intention model  i e  the human only has one goal in mind from the start to the end of one episode  and it is the assistants task to identify this sole intention  In contrary  our engine allows for a more dynamic human intention model and does not impose a restriction on the freedom of the human player to change his mind mid way through the game  This helps ensure our AIs robustness when inferring the human partners intention  In a separate effort that also uses MDP as the game AI backbone  Tan and Cheng  Tan and Cheng       model the game experience as an abstracted MDP   POMDP couple  The MDP models the game worlds dynamics  its solution establishes the optimal action policy that is used as the AI agents base behaviors  The POMDP models the human play style  its solution provides the best abstract action policy given the human play style  The actions resulting from the two components are then merged  reinforcement learning is applied to choose an integrated action that has performed best thus far  This approach attempts to adapt to different human play styles to improve the AI agents performance  In contrast  our work introduces the multi subtask model with intention recognition to directly tackle the intractability issue of the game worlds dynamics   Conclusions We describe a scalable decision theoretic approach for constructing collaborative games  using MDPs as subtasks and intention recognition to infer the subtask that the player is targeting at any time  Experiments show that the method is effective  giving near human level performance  In the future  we also plan to evaluate the system in more familiar commercial settings  using state of the art game platforms such as UDK or Unity  These full fledged systems offer development of more realistic games but at the same time introduce game environments that are much more complex to plan  While experimenting with Collaborative Ghostbuster  we have observed that even though Value Iteration is a simple naive approach  in most cases  it suffices  converging in reasonable time  The more serious issue is the   state space size  as tabular representation of the states  reward and transition matrices takes much longer to construct  We plan to tackle this limitation in future by using function approximators in place of tabular representation   Appendix Game levels used for our experiments   Acknowledgments This work was supported in part by MDA GAMBIT grant R                and AcRF grant T     RES     in Singapore  The authors would like to thank Qiao Li  NUS   Shari Haynes and Shawn Conrad  MIT  for their valuable feedbacks in improving the CAPIR engine  and the reviewers for their constructive criticism on the paper       
