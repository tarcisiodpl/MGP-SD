  It is well known that conditional indepen dence can be used to factorize a joint prob ability into a multiplication of conditional probabilities  This paper proposes a con structive definition of intercausal indepen dence  which can be used to further factorize a conditional probability  An inference algo rithm is developed  which makes use of both conditional independence and intercausal in dependence to reduce inference complexity in Bayesian networks  Key words  Bayesian networks  intercausal indepen dence  definition  representation  inference     INTRODUCTION  In one interpretation of Bayesian networks  arcs are viewed as indication of causality  the parents of a ran dom variable are considered causes that jointly influ ence the variable  Pearl        The concept intercausal independence refers to situations where the mechanism by which a cause influences a variable is independent of the mechanisms by which other causes influence that variable  The noisy OR gate and noisy adder models  Good       Pearl       are examples of intercausal independence  Special cases of intercausal independence such as the OR gate model have been utilized to reduce the complexity of knowledge acquisition  Pearl       Hen rion       as well as the complexity of inference  Kim and Pearl        Beckerman        is the first re searcher to try to formally define intercausal indepen dence  His definition is temporal in nature  Based on this definition  a graph theoretic representation of intercausal independence has been proposed   noisy  This paper attempts a constructive definition  Our definition is based on the following intuition about in tercausal independence  a number of causes contribute independently to an effect and the total contribution is a combination of the individual contributions  The  definition allows us to represent intercausal indepen dence by factorization of conditional probability  in a way similar to that conditional independence can be represented by factorization of joint probability  The advantages of our factorization of conditional probability representation of intercausal independence over Beckerman s graph theoretic representation are twofold  Firstly  the symmetric nature of intercausal independence is retained in our representation  Sec ondly and more importantly  our representation allows one to make full use of intercausal independence to re duce inference complexity  While Heckerman uses intercausal independencies to alter the topologies of Bayesian networks  we follow Pearl         section        to exploit intercausal in dependencies in inference  While Pearl only deals with the case of singly connected networks  we deal with the general case   The rest of this paper is organized as follows  A constructive definition of intercausal independence is given in Section    Section   discusses factorization of a joint probability into a multiplication of conditional probabilities  and points out intercausal independence allows one to further factorize conditional probabili ties into  even smaller  factors  The fact that those  even smaller  factors might be combined by opera tors other than multiplication leads to the concept of heterogeneous factorization  HF   After some technical preparations  Sections   and     the formal definition of HF is given in section    Section   discusses how to sum out variables from an HF  An algorithm for computing marginals from an HF is given in Section    which is illustrated through an example in Section    Related work is discussed in Section        CONSTRUCTIVE INTERCAUSAL INDEPENDENCE  This sections gives a constructive definition of inter causal independence  This definition is based on the following intuition  a number of causes c   c           Cm contribute independently to an effect e and the total        Intercausal Independence and Heterogeneous Factorization  contribution is a combination of the individual contri butions  Let us begin with an example   the noisy OR gate model  Good       Pearl       Beckerman        In this model  there is a random binary variable i in correspondence to each c   which is also binary    depends on c  and is conditionally independent of any other i given the c   s  e is   if and only if all the    s are    and is   otherwise  In formula  e   V       V m Consider the case when m   and consider the condi tional probability P eict  c    For any value     of c   i              we have  P e Oic   f t  c  fJ     P t V   ict          c         P        Oict  f   P    ic  f     and  Define ft e a bct f t  deJP   a tict f t  and de fine    e     a    c   B   dJ P   a  Jc        We can rewrite the above two equations in a more compact form as follows  P e a lcl f t  c         ft e o t  Ct f dh e a    c   B          a va   a  where o   a    and a   can be either   or motivates the following definitions       This example  Let e be a discrete variable and let   be an commutative and associative binary operator over the frame n    the set of possible values of e  In the previous example    is the logic OR operator V  Let f e  x         xr Yt       y   and g  e  Xt          Xr  Zt        Zt  be two functions  where the Yi  s are different from the Zj  s  Then  the combination   g off and g is defined as follows  for any value a  of e    i eg e     a   XI        Xr  Yt         y   Zt       Zt    dej L  f e o t XJ        Xr YI        y    where e is the  e induced combination operator  The right hand of the equation makes sense because e is commutative and associative  When c           em contribute independently to e  we call e a bastard variable  A non bastard variable is said to be nor mal  We also say that f  e  c   is  a description of  the contribution by c  to e  Intuitively  the base combination operator  e g  V  determines how contributions from different sources are combined  while the induced combination operator is the reflection of the base operator at the level of conditional probability   P e llcFf t  c  P   P   V    c   f t  c  f    P     let f t P    Jcz f     P t  OJct  f t P   lic  f     P    ict f t P   llc  f            Here is our constructive definition of intercausal in dependence  We say that c         Cm contribute in dependently to e or e receives contributions indepen dently from Ct     Cm if there exists a commutative and associative binary operator   over the frame of e and real valued non negative functions It  e  c          f m  e  Cm  such that  X  Because of equation      the noisy OR gate model is an example of constructive intercausal independence  with the logic OR Vas the base combination operator  As another example  consider the noisy adder model  Beckerman        In this model  there is a random Variable i in correspondence to each Cj j e  depends On Cj and is Conditionally independent of any other ej given the c  s  The   s are combined by the addition operator     to result in e  i e  e            em  To see that e is a bastard variable in this model  let the base combination operator  e be simply     and let the description of individual contribution    e  c   be as follows  for any value o  of e and any value f  of C         e  o   c  f   def P e  aJc   B   Then it is easy to verify  that  equation     is  satisfied   It is interesting to notice the similarity between equa tion     and the following property of conditional in dependence  if a variable x is independent of another variable z given a third variable y  then there exist non negative functions f x  y  and g y  z  such that the joint probability P x  y  z   is given by P x y  z  f x  y g y  z          Those who are familiar with clique tree propagation  We shall refer to   as the base combination operator and e as the  e induced combination operator  We would like to alert the reader that  e combines values of e  while e combines functions of e  It is easy to see that the induced operator e is also commutative and associative   may remember that the  first thing to do in constructing  a clique tree from a Bayesian network is to  marry  the     parents of each node variable             Lauritzen and Spiegehalter  As implies by the word  bastard   the parents of  a bastard node will not be married  This is because the conditional probability of a bastard node is factorized into a bunch of factors  each involving only one parent         Zhang and Poole  In     conditional independence allows us to factorize a joint probability into factors that involve less vari ables  while in     intercausal independence allows us to factorize a conditional probability into a bunch of factors that involve less variables  The only difference lies in the way the factors are combined  Conditional independence has been used to reduce in ference complexity in Bayesian networks  The rest of this paper investigates how to use intercausal indepen dence for the same purpose     FACTORIZATION OF JOINT PROBABILITIES  This section discusses factorization of joint probabili ties and introduces the concept of heterogeneous fac torization  HF   A fundamental assumption under the theory of proba bilistic reasoning is that a joint probability is adequate for capturing experts  knowledge and beliefs relevant to a reasoning under uncertainty task  Factorization and Bayesian networks come into play because joint probability is difficult  if not impossible  to directly assess  store  and reason with   Let P xb x        xn  be a joint probability over variables x   x         Xn  By the chain rule of probabilities  we have P x   X     Xn   P xi P x lxt      P xnlxl       Xn                 For any i  there might be a subset        x         r  I  such that X  is conditionally independent of all the other variables in  x         X  d given the variables in       i e P x lxb      x  I  P x l r    Equation     can hence be rewritten as n  P x  x           Xn   IT P x l r               Equation     factorizes the joint proba bility P z     x        z     into a multiplication of factors P xd Ti  While the joint probability involves all then variables  the factors usually involves less than n vari ables  This fact implies savings in assessing  storing  and reasoning with probabilities  A Bayesian network is constructed from the factoriza tion as follows  construct a directed graph with nodes x   x            r   such that there is an arc from  rj to x  if and only if Xj E       and associate the conditional probability P x l r   with the node x   P x      Xn  is said to be the joint probability of the Bayesian network so constructed  Also nodes in      are called parents of  r    The above factorization is homogeneous in the sense that all the factors are combined in the same way  i e by multiplication   Figure    A Bayesian network  where e  and e  re ceive contribution independently from their respective parents  Let x          x m  be the parents of x   If x  is a bastard variable with base combination operator  i  then the conditional probability P xd r   can be further factor ized by  where  is the    induced combination operator  The fact that   might be other than multiplication leads to the concept of heterogeneous factorization  HF   The word heterogeneous reflects the fact that differ ent factors might be combined in different manners  As an example  consider the Bayesian network in Fig ure    The network indicates that P a  b  c  e  e  ea  y  can be factorized into a multiplication of P a   P b   P c   P e la b   P e la  b  c   P   eale    e     and P ylea   Now if the e  s are bastard variables  then there exist base combination operators  i  i l        such that the conditional probabilities of the e   s can be further factorized as follows  P e la  b  P e la b c  P ealet  e    fu el  a dt  el  b      e   a   n e   b    a e   c   at ea el a a  e  el   where fu et  a   for instance  denotes the contribution by a to e   and where the i s are the combination operators respectively induced by the  i s  The factorization of P a  b  c  et  e    e   y  into the factors  P a   P b   P c   P ylea   fu el  a       b   e   a       e   b   ha e    c   fat ea  et   h    t  e  and  a   e    e   is called the HF in correspondence to the Bayesian network in Figure    We shall call the fii  s heterogeneous factors since they might be com bined by operators other than multiplication  On the other hand  we shall say that the factors P a   P b   P c   and P yle   are normal    Intercausal Independence and Heterogeneous Factorization       To prevent I e   eD from being mistaken to be the con tribution by ei to e   we shall always make it explicit that I e   e  is a normal factor  not a heterogeneous factor   COMBININ G FACTORS THAT     IN V OLVE M OR E THAN ONE BAS TARD VARIABLE Even though deputation guarantees that every hetero  Figure    The Bayesian network in Figure deputation of bastard nodes         after the  geneous factor involves only one bastard variable at the beginning  inference may give rise to factors that involve more than one bastard variable  In Figure    for instance  summing out the variable a results in a factor that involves both e  and e   This section in troduces an operator for combining such factors  Suppose e         eo are bastard variables with base opera combination tor  t         k Let f et      e   xt      xr  Yl       y   and g   et      e  xt x     zt        zt   be two func tions  where the xi s are normal variables and the yj s  DEPUTATION OF BAS TARD     NODES     Consider the heterogeneous factor h   es  e   from the previous example  It contains two bastard variables e  to e   As we shall see later  it is desirable for every heterogeneous factor to contain at most one bastard variable  The concept of deputation is introduced to guarantee this   are different from the zr s  they can be bastard as well as normal variables   Then  the combination fg of f and g is defined as follows  for any particular value a  of e     fg el  a          eo O i   Xt       Xr  Yl         y   Zt          Zt    Let e be a bastard node in a Bayesian network  The deputation of e is the following operation  make a copy e  of e  make the children of e to be children of e     make e a child of e  and set the conditional probability P e le  as follows   P e Ie            if e   e  otherwise       We shall call e  the deputy of e  We shall also call P ele   the deputing function  and rewrite it as I e  e   since P ele   ensures that e and e  be the same  The Bayesian network in Figure   becomes the one in Figure   after the deputation of aU the bastard nodes  We shall call the latter a a deputation Bayesian net  work   Proposition   Let N  be a Bayesian network  and let N  is the Bayesian network obtained from N  by the  deputation of all bastard nodes  Then the joint proba bility of N can be obtained from that of N  by summing out all the deputy variables     In Figure    we have the heterogeneous factors h  es el  and f   es e    which involves two bastard variables  This may cause confusions and is undersir able for other reasons  as we shall see soon  After dep utation  each heterogeneous factor involves only one bastard variable  As a matter of fact  fst es  et  and fs  es e   have become fst es eD and fs  es e     f et  au          eo akl  x       Xr  Yl    Ys  X g e           ek          Xt        Xr  Zt        Zt                      A few notes are in order  First  fixing a list of bas tard variables and their base combination operators  one can use the operator to combined two arbitrary functions  In the following  we shall always work im plicitly with a fixed list of bastard variables  and we shall refer to  as the general combination operator  Second  when        k       th is definition reduces to equation  Third  since the base combination operators are com mutative and associative  the operator  is also com mutative and associative  Fourth  when off and g        k         fg  is simply the multiplication  Combining all the Heterogeneous Factors in a Bayesian networks  Equipped with the general combination operator   we now consider combining all the heterogeneous fac tors of the Bayesian network in F igure    Because of the third note above  we can combine them in any order  Let us first combine fu et  a   with  t  e   b       e   a  with h  e   b  and hs e   c   and fst es  eD        Zhang and Poole   F  In the following  we shall also say that the of the function F X    We now combine the resulting conditional probabili ties  Because of the fourth note  the combination of P etia  b   P e  la   b  c    and P e lei e   is their multi plication  So  the combination of all the heterogeneous factors of the Bayesian network in Figure   is simply the multiplication ofthe conditional probabilities of all the bastard variables  This is true in general   Suppose N is a deputation Bayesian network  Sup pose  F is the HF that corresponds to N   F has two interesting properties   In a deputation Bayesian network  the multiplication of the conditional probabilities of all the bastard variables is the same as the result of com bining of all the heterogeneous factors  D  Proposition         is an  HF  with     e   e    Because of the second note  we have  H t  et a b     P eda b   P e la b  c    hth     e  a b  c  P e  e  eD  ht    e  e  eD  HF s in Correspondence to Deputation Bayesian Networks  First  according to Proposition   the combination of all the heterogeneous factors is the multiplication of the conditional probabilities of all the bastard variables  Thus  the joint of  F is simply the joint probability of  N   The joint of the HF that corresponds to a deputation Bayesian network N is the same as the joint probability of N   Proposition    Note that in Figure    since ht e  e   and h  e   e   involve two bastard variables  the combination fn et a        f   e  c   ht e  et    a  e   e   would not the same as the multiplication of the condi tional probabilities of the bastard variables   To reveal the second interesting property  let us first define the concept of tidness  An HF is tidy if for each bastard variable e  there exists at most one normal factor that involves e  Moreover  this factor  if exists  involves only one other variable in addition to e itself   This is why we need deputation  deputation allows us to combine the heterogeneous factors by a single com bination operator   which opens up the possibility of combining the heterogeneous factors in any order we choose  This flexibility turns out to be the key to the method of utilizing intercausal independence we are proposing in this paper   An HF that corresponds to a deputation Bayesian net work is tidy  For each bastard variable e  I e  e   is the only one normal factor that involves e  and this factor involves only one other variable  namely e       HETEROGENEOUS FACTORIZATION  We now formally define the concept of heterogeneous factorization  Let X be a set of discrete variables  A heterogeneous factorization  HF  F over X consists of    A list e          em of variables in X that are said to be bastard variables  Associated with each bas tard variable ei is a base combination operator  i  which is commutative and associative     A set  Fo of heterogeneous factors  and    A set  F  of normal factors  We shall write an HF as a quadruplet  F   X    e    t              em   m        Fo  Ft   Variables that are not bastard are called normal  In an HF  the combination F  of all the heterogeneous factors is given by      The joint F X  of an HF is the multiplication of Fa and all the normal factors  In formula F deJ Je F f   IT g     g         Tidy HF s do not have to be in correspondence to a deputation Bayesian network  As a matter of fact  we shall start with a tidy HF that corresponds to a dep utation Bayesian network  and then sum out variables from the HF  We shall sum out variables in such a way such that the tidness is retained  Even though the HF we start out with corresponds to a deputation Bayesian network  after summing out some variables  the resulting tidy HF might no longer correspond to any deputation Bayesian network  However  we shall continue to use the terms deputy variable and deputing function     SUMM ING OUT VARIABLES FROM TIDY HF S  Let F X  be a function  Suppose A is a subset of X  The projection F A  of F X  onto A is obtained from F X  by summing out all the variables in X  A  In formula      F A  dJ I  F X   X A  When F X  probability   is  a joint probability  F A  is a marginal  Summing variables out directly from F X  usually re quire too many additions  Suppose X contains n vari ables and suppose all variables are binary  One needs to perform  n     additions to sum out one variable    Intercausal Independence and Heterogeneous Factorization  A better idea is to sum out variables from an factoriza tion of F X  if there is one  This section investigates how to sum out variables from tidy HF s  The follow ing two lemmas are of fundamental importance  and they readily follow the definition of the general com bination operator    Both m llltiplication and   are distributive w  r  t summation  More specifically  s llppose f and g are two functions and variable x appears in f and not in g   Then  Lemma         Er fg      Er f g  and  summing out z does not affect the deputing functions  Therefore   F  remains tidy  When z is a bastard variable  summing out z will not affect the deputing functions of any other bastard vari ables  Therefore   F  also remains tidy    general  a variable can appear in more than one nor mal and heterogeneous factors  The next proposition reduces the general case to the case where the variable appear in at most two factors  one normal and one heterogeneous   In  F X   and let z be a variable in X  Let It         fm be all the heteroge neous factors that involve z and let           Un be all the normal factors that involve z  Define  Proposition   Let  F be an HF of     E    Ig   CEr f  g         f aej            The following lemma spells out two conditions under which multiplication and  are associative with each other  Lemma       h fg   hf  g   n  g aeJ IT Ui j l  Let f and g be two functions   If h is a function that involves no bastard vari ables  then        Let  F  be the HF obtained from  F by removing the fi  s and the Ui  s  and by adding a new heterogeneous factor f and a new normal factor g  Then     F  is also an HF of F X   and f and  g are the only two factors that involve z  In particular  when either m O or n O  there is only one factor in  F  that involves z      If h is a function such that all the bastard variables in  h  f and not in g  h fg     hf g   appear only in  then           We now proceed to consider the problem of summing  out variables from a tidy HF in such a way that the tid ness is retained  First of all the following proposition deals with the case when the variable to be summed out appears in only one factor    Let  F be an HF of F X  and is tidy  Suppose z is a variable that appears only in one factor   A   normal or heterogeneous  Define h  Proposition  h A    z    dJ Lf A   z  Let  F  be the HF obtained from  F by replacing  f with  h   Then   F  is a HF of F X    z     the projection of F X  onto X  z    Moreover if z is not a dep llty    variable  then  F  remains tidy   Proof        The first part of proposition follows from  Lemma     For the second part  since z is not a deputy variable  it can be either a non deputy normal variable or a bas tard variable  When z is a non deputy normal variable   The factor h is heterogeneous or normal if and only if f is       If z  is not a dep llty variable  then when  F is tidy  so is  F    Proof  The first part of the proposition follows from  the commutativity and associativity of multiplication and of the general combination operator    For the second part  since z is not a deputy variable  it can either be a non deputy normal variable or a bas tard variable  When z is a non deputy normal vari ables  the operations performed by the proposition do not affect the deputing functions  Thus   F  remains tidy  When z is a bastard variable  the deputing functions are not affect either  Because for each bastard variable e  its deputing functions is the only normal factor that involves e  So   F  also remains tidy  D  The following proposition merges a normal factor into a heterogeneous factor  Proposition   Let  F be an HF of F X  and is tidy   Suppose z is a variable that appears in only one normal factor g and only one heterogeneous factor f  Define h by  h aeJfg  Let  F  be the HF obtained from  F by removing g and f  and by adding a heterogeneous judor h  If z is not        Zhang and Poole  a deputy variable  then the joint of  F  is also F X  and  F  is tidy  Moreover  h is only one factor in  F   that involves  z   Proof  We first consider the case when z is a non deputy normal variable  Because the tidness of  F  g involves no bastard variables  According to Lemmas         the joint ofF  is also F   Since g is not a deputing function  the operation of combining f and g into one factor does not affect the deputing functions  Hence   F  remains tidy  Let us now consider the case when z is a bastard vari able  Since  F is tidy  g must be the deputing function of z  Since f is the only heterogeneous factor that in volves z  all other heterogeneous factors do not involve z  According Lemma        the joint of  F  is also F  After combining f and g into a heterogeneous factor  there is no normal factor that involve z  Also  the deputing functions of the other bastard variables are not affected  Hence   F  remains tidy  D  The  in Bayesian networks  To this end  we need only con  sider deputing functions I e  e   such that I e  e       if e   e  and I e  e       otherwise  Let us say such deputing functions are identifying  Since for any func tion f e  e   x         xn      LI e  e  f e  e    Procedure PROJECTION    F  A       AN ALGORITHM  This section presents an algorithm for computing pro jections of a function F X  by summing variables from a tidy HF of F X   Because of Proposition    the al gorithm can be used to compute marginal probabili ties  and hence posterior probabilities  in Bayesian net works  To sum out the variables in X  A  an ordering needs to be specified  Lauritzen and Spiegehalter        In the literature  such an ordering is called an elimi nation ordering  which can be found by heuristics such as the maximum cardinality search  Tarjan and Yannakakis       or the maximal intersection search  Zhang        At the end of the last section  we said that a deputy variable should be summed out only after the corre sponding bastard variable has been summed out  If e is a bastard variable in A  what should we do with its deputy variable e   The paper is concerned with intercausal independence  p   Input      F   A tidy HF of a certain func tion F X  such that all the deputing functions are identifying   a tidy HF  bastard variables and non deputy normal variables  You may ask  how about deputy variables  As it turns out  after summing out a bastard variable e  its deputy e  becomes a non deputy normal variable  So  we can also sum out deputy variables  we just have to make sure to sum out a deputy variable after the corresponding bastard variable has been summed out  variable e  needs to be summed out after the corre sponding bastard variable e  As a matter of fact  sum ming out e   before e is the inverse of the deputation of e  But we have shown at the end the Section   that deputation is necessary       f e  e  x            Xn    we can handle the deputies of bastard variables in A as follows  wait till after all the other variables outside A have been summed out and all the heterogeneous factors have been combined  then simply remove all the deputing functions  replace each occurrence of a deputy variable with the corresponding bastard vari able  This operation can be viewed as the inverse of deputation   above three propositions allow us to sum out  from  It is possible to intuitively understand why a deputy  Xt        Xn  e      A  A subset of X      p   An elimination ordering consist ing all the variables other than the variables A and their deputies  In p  a deputy variable e comes right after the corresponding bastard vari able e      Output  onto A   F A    The projection of F     If p is empty  combine all the het erogeneous factors by using the gen eral combination operator   resulting in f  remove all the deputing functions and replace each occurrence of a deputy variable with the corresponding bastard variable  multiply f together with all the normal factors  output the resulting fac tion  and exit     Remove the first variable  z  from the or  dering p      Remove from  F all the heterogeneous factors ft         fl that involve z  and set  f dej  f l k  Let B be the set of all the variables that appear in f     Remove from  F  all the normal factors            Om that involve z  and set  m  D  deJ  IT  j  j  l  Let C be the set of all the variables that appear in g    Intercausal Independence and Heterogeneous Factorization     If k O  define a function  The bastard variable e  appears in heterogeneous fac tors      e ei  and  a  e    e   and in the normal factor I  e  e    After summing out eg the factors become   h by  h C  z   def Lg C   Add h into F  as        T bt eL e e   fu el a   Fo     e  a       e   b   ha e  c    F   P a   P b   P c   P  yie   h e   e       a normal factor      Else if m O  define a function h by    h B  z   deJ L f B     t   e   b   l  et ei    where Add h into F as a heterogeneous factor      Else define a function h by  h BUC  z    de  L f B g C   Add h into F as a heterogeneous factor  Endif    Recursively call PROJECTION F   A  p   The correctness of PROJECTION is guaranteed by Propositions       and     summing out a variable re quires combinin g only the factors that involve the vari able  This is why PROJECTION allows one to ex  Note that in the algorithm  ploit intercausal independencies for efficiency gains  If one ignores intercausal independencies  to sum out one variable one needs combine all the conditional proba bilities that involve the variable  There is a gain in effi ciency by using PROJECTION because intercausal in dependence allows one to further factorize conditional probabilities into factors that involve less variables  In Figure    for instance  summing out a requires com bining P e tla  b  and P e la  b  c  when intercausal in dependencies are ignored  there are five variables in volved here  By using PROJECTION  one needs to combine f    e     a  and     e  a   there are only three variables involved in the case  Finally  we would like to remark that the algorithm is an extension to a simple algorithm for computing marginal probabilities from a homogeneous factoriza tion  Zhang and Poole         tPt e  e e   def L      ea eDfaz e   e   I  e   e       Now e is the next to sum out  e appears in the heterogeneous factor t J  and the normal factor P yle   After summing out e  the factors become      where   f   e  e  y  def L if    e  e e  P yle    e   Next  summing out     Suppose the elimination ordering pis   e  c      e   e  a  b  e    Initially  the factors are as follows    Fo     u et a    t  e  b   ht e   a         e  b  h  e  c   h  e  eD  h  e  e     F     P a   P b   P c   P yle   h el eD  l  e  e   l  ea  e     tfi  e e y   Fa   tP  e   e    h  e   b       e  c      t  e  b    F   P b   P c   I  e   ei   I  e   e     a  Then  summing out    work N shown in Figure    Since P e ly O  can be readily obtained from the marginal probability P e  y   we shall show how PROJECTION computes the latter   gives us   a    To illustrate PROJECTION  consider computing the conditional probability P e ly O   in the Bayesian net  a  where  An example      Fo N  eL e y    u et a    t  e  b       e  a       e  b   h  e   c     F    P a   P b   P c   It et  ei   I  e  e     b gives  us   Fo    tj    e   e        e  c    Ft   P c   lt et eD  I  e  e     where  if   et e     def    L P b   t  et b f   e  b   L P b  t  e  b h  e   b   b  The next variable on p is e   which appears in hetero geneous factors  tj     e  e   and  tj     e    e   and normal factor h  e   ei   After summing out e  the factors be come         Zhang and Poole  together with conditional independencies  to further reduce inference complexity   where  t Js e   e  deJ     It  et  eD t Ja e   e  t J  e   e         Due to space limit  we have to discontinue the example here  Hopefully  the following two points shoul be be clear now  F irst  in summig out one variable  PRO JECTION combines only the factors that involve the variable  Second  since  not have  e   is a bastard variable  we usually do  Acknowledgement The authors are grateful for the three anonymous re viewers for their valuable comments and suggestions  Research is supported by NSERC Grant OGP        and by a travel grant from Hong Kong University of Science and Technology   
  There is much interest in providing proba  bilistic semantics for defaults but most ap proaches seem to suffer from one of two prob lems  either they require numbers  a problem defaults were intended to avoid  or they gen erate peculiar side effects  Rather than provide semantics to defaults  we address the original problem that defaults were intended to solve  that of reasoning un der uncertainty where numeric probability distributions are not available  We describe a non numeric formalism called an inference graph based on standard probability theory  conditional independence and sentences of confirmation  where a confirms b   coni  a  b    p a b    p a   The formalism seems to handle the exam ples from the nonmonotonic literature  Most importantly  the sentences of our system can be verified by performing an appropriate ex periment in the semantic domain     should not be thought of as having any mean ing in  the sense of  most  or  typical   they are statements the user is prepared to accept as part of an explanation as to why some thing may be true  What  then  does a default mean  Within the default logic camp  we know of no work which provides a semantics for defaults  in the sense that an experiment is described that can be performed in the semantic do main to verify the truth of a default  It is therefore compelling to view defaults as qual itative probabilistic statements where nu meric distributions are unavailable  We sur vey some of these views but note most re quire numbers  something default reasoning intended to avoid  or have side effects  Rather than  add semantics  to defaults  we construct a sound non numeric proba bilistic formalism called an inference graph  We explore its mathematical properties  then apply it to the standard examples  We con clude with a brief description of the imple mentation   Introduction  Though default rasoning involves reasoning under conditions of uncertainty  some argue it is not probabilistic reasoning  Reiter and Crisculo      distinguish the two by suggesting different interpretations for the word  most    Probabilistic reasoning gives  most  a statistical connotation  whereas default logic gives it a prototypical sense  On the other hand  Poole      claims defaults          What s in a default   Poole et al      attempt to put both default reasoning and diagnosis under a single urn brella by constructing a system containing a set of facts F known to be true  a set  of defaults  and g  a set of  possible  observa tions which are goals to be proved  Here we assume F  and g are propositional            I If D  is a subset of  FU D  I  g  and    Pearl shows p flbe      Then  from  such that  FU D  is consistent   p flb    p fleb P elb    p ll  eb p   elb   then D is an ezplanation of g  This system is based on a theorem prover and can be used in two ways  If g consists of observations known to be true  we interpret g as querying thy g   and Dis a diagnosis of g  If g is not known to be true  then g is interpreted as querying thether g   and g is a prediction of FUD  The problem default logic runs into is that there  is typically another D  such that FuD  predicts   g  this is known as the mul tiple eztension prob lem and is discussed be low   This is a very abbreviated presentation of default reasoning  for details on implemen tation and application see           AJJ pointed out in the introduction  de faults appear to have no semantics  and many researchers study the relationship be tween default reasoning and uncertainty  Rich      advocates adding certainty factors to possible hypotheses to fine tune a default reasoning systm and concludes  default rea soning is likelihood reasoning and treating it that way simplifies it    While some argue with her treatment  her conclusions seem to be widely held  Ginsberg     pursues this ap proach  At the      Workshop on Uncertainty in AI  Groaof suggested defaults are interval valued probabilities on the entire unit inter val  Default inference thus becomes closely related to Kyburg s theory of interval valued probabilities        In      McCarthy states non monotonic sentences can represent statements of in finitesimal probability  but does not go into detail  Pearl explores this in       This inter pretation has some problems  Let e   emu  b     bird  I   fly  If  it follows p  elb      But since a prior is always bounded by its conditionals on any evidence and the negation of that evidence  we can show p e      Default logic also has this  property   from no knowledge at all  we can prove   emu by cases from fly and   fly using the contra positive forms C f the defaults  This intro duces the following variant of the  lottery paradox       Suppose kangaroos  k  are exceptional because they have a marsupial birth and platypusses  p  are exceptional be cause they lay eggs but dingos  d  have no such exceptional traits  If Example      ozzie animal    e Vk V p V d  then p  dl ozzie animal  is close to one since the disjunction of the other three is close to zero  Default reasoning and circumscription      suffer the same problem  Poole        solves this by explicitly pruning the proof tree with a set of sentences called constraints  However  to do this  you need to know the right answers in advance   Besides making subclasses vanish  Pearl s  semantics suffers another problem  in gen eral  it is impossible to go out into a real problem domain and find a set of conditional probabilities infinitely close to one  Bacchus    addresses this issue of practi cality and argues for thresholding  that is  that a possible hypothesis stands for a prob ability greater than some threshold k        His system allows only a single defeasible in ference  since p bla    k and p clb    k do not in general constrain the value of p  cia  p flb      p fle   O p ble     to be greater thank  There seems to be no end to different prob  i e   there are some non bird emus  then abilistic semantics that might be added to from defaults or inference rules that might be in vented to come up with the right answers for p fle   p flbe p ble    p fl  be p   ble         I I I I I I I I I I I I I I I I I I   I I I I I I I I I I I I I I I I I I I  the various examples  We claim it is neces sary to ask again what were the original goals of formalisms such as default reasoning  in heritance hierarchies and semantic nets  We should reconsider the original objections to standard probability  and ask whether we solve the problem in a principled way  with out tlie invention of new formalisms   tic formalism based on standard probabil ity theory  conditional independence and sen rules for  Rather than give  accepting uncertain conclusions  the  inference graph allows us to make inferences about       shifts in  contains four kinds of  inference graph   f   Links with double logical links and the others probabilistic links  Each node is labelled  links             and  arrows are called are  with a name or set of names in lower case   for example   quaker or pacifist   Links are at   tached to a name or its negation at either  An inference graph is a strictly probabilis  tences of confirmation   An  Syntax  endpoint   Inference graphs          belief        Semantics  Nodes in an inference graph denote events  Generally events have two mutually exclu sive outcomes  for example fly or  Occa     fly   sionally an event may have several mutually exclusive outcomes   not all of which need be specified   for example   hawk  dove    Sentences about confirmation  are repre  Confirmation  sented by the four kinds of links in an infer  An interesting mathematical property of log  ence graph   sequent increases belief in the antecedent   a  b  means  a   b  means    a  f  b  means  ical implication is that knowledge of the con That is  a    Rosenkrantz  mation   b implies that p ajb        p a        calls this property confir  and we will see it has many of  the same useful computational properties as  other probabilistic formalisms  Confirmation describes a  shift in  belief  it  seems to be the weakest probabilistic prop erty a default  ought to have   This provides an  interesting venue to explore  rather than use knowledge of the form  birds are more likely to fly than not    we consider knowledge of the form  an individual is more likely to fly once we learn that it is a bird     Consider Nutter s example        where in  springtime it is not true that most birds fly  since most birds are flightless nestlings  Yet  the information that an individual is a bird inclines us to shift belief in favour of flying  This  also admits an interesting kind of sen  tence  If we say  Irish Canadians have red hair    we do not mean mor than half or al most all Irish Canadians have red hair  even though the stereotype is widely held   a    b means   Note     p bla    p b    p   b a    p   b         p   b a    p   b    that we insist on strict inequality   This means that links such as  sky is blue graph                  cannot appear on an inference  For the same reason  we also insist  all events are possible    The topology of the inference graph carries information about independence of events   Definition     If  p a b     a   a unconditionally independent   Definition     If  and  p a bc    p a b   a ditionally independent of c  given b    is  b  ar  e  con   Poaaibly confirmation i s too strong a term where logical implication is not involved   We use confir  mation here in the sense of partial confirmation   or relevance        p bja    p b     I If a  is a node  and b        bn are the nodes directed into a  then a is conditionally inde pendent of all the predecessors of the b  given the outcomes of the bi  Thus  an inference graph may be seen as a non numeric influence diagram      We next explore the kinds of inferences about confir mation that we can make     The confirmation relation  Definition      If  write conf a b        p ajb      p a   we also  Symmetry  Lemma      If  Proof  Rule     coni a  b   then conj b  a    Follows immediately from Bayes   This allows our system to be reversible  if we observe sneeze we can confirm has cold  Alternately  if we know that someone has a cold we can predict they will sneeze  Thus we can use the same  formalism for prediction and diagnosis       Negation  Lemma      If coni a  b   then  coni   a    b    By definition  p ajb    p a   Negat ing both sides yields p   ajb    p   a   Then p bj  a    p b  by Lemma     and negating again yields p   bl     a    p   b   Another ap plication of Lemma     yields the desired re sult    Thus  not only does bird increase belief in fly    bird increases belief in   fly  An inter esting intermediate result is that the  contra positive  form of a link yields a valid infer ence  so long as it is made from a single link  This means use of the contrapositive form of a link is valid  bu t the context of such an inference must be carefully restricted  Infer ence graphs also explain why default reason ers based on a theorem prover sometime run Proof   into difficulties when they apply the contra positive  they viola te independence assump tions       Logical Inferences  coni a  c  and conj b  d  where c and d are outcomes of the same ran dom variable  and a f  b  then conf c ab    Lemma       If  p cjab  p cja    p c   since sen tences of probability hold for logically equiv alent propositions    Default reasoners produce separate argu ments for c and d and attempt to choose among the arguments by appeal to  speci ficity    Poole      calls it preferring the most specific theory and Kirby     calls it choosing the most specific extension  W hile the default logic view seems to be to prefer the conclusion based on the most specific knowledge  we remark that there is not universal agreement on this in the prob abilist community when statistics are not good  Kyburg       suggests we make an inference based on the narrowest reference class for which we have adequate statistics  Some Bayesians suggest that data from var ious subclasses be combined       Proof      I I I I I I I I I I  b f  a  but a  b then  I  Proof   This generalizes the property of logical links to the rest of the graph     I        I  Lemma    S If  conf b a    Transitive inference  Default proofs consist of more than a single inference  part of the appeal of such reason ers is that they appear to create and argu ment by making inferences towards a goal  In general  if a   b and b   c are links on an in ference graph  we cannot conclude conj c  a   However  if c is conditionally independent of a given b  it can be shown that conj c  a   In    Thia reference  Cheeseman   was  pointed out to us by Peter  I I I I       I   I I I I I I I I I I I I I I I  Proof  p  albc    p alc    p a   from the fact  conditional independence gives us much more than transitivity  Not only can we re definition of conditional independence    verse the inference  we can perform transduc tion  inferring evidence from other evidence  Lemma      Relevance  Suppose conf a c  and conf b c  and a is conditionally indepen We can also confirm certain conjunctions  dent of b given c  Then conf ab c   Lemma      Probabilistic Resolution  If there exists c such conf a c  and conf b c  Proof  conf a b  follows from Lemma      and a is conditionally independent of b given and p alb    p a  c  from Lemma      Then c  then conf a b   p ablc    p alc p blc    p alb p b     Proof  By contradiction  Suppose p alb   p a   From the premises and an identity of     Other inferences probability it follows p alb   p al   b   Then  The following two lemmas address situations that prove to be useful in Section      The p alb  p alcb p clb    p al   cb p    clb     I   I proofs are straightforward and we omit them  I I I p a    b  p a c     b p c      b  p a   c    b p     c      b  Simplify using the conditional independence LeiDDla      If coni      a  b   coni a  c   and b  c  then coni a    be   knowledge  then subtract to obtain            p al   b   p alb     p alc   p al    c   p cl   b   p clb      Lemma      If  r  decessors of g  and  and e are the direct pre     r I  e     a I  e     r is unconditionally independent of a     conj g  e      conj   g  r     But then both terms must be positive  con tradicting the premise that conj b  c     Unsurprisingly  each such inference results in a dilution of confirmation  This lemma is needed for later results   conf a b  and conf b c  and then conf g  a   a is conditionally independent of c given b  then p alc    p alb   Lemma     If     Proof   p alc   p albc p blc    p al   bc p    blc    p alb p blc    p al   b p    blc  p alb p blc    p alb p    blc      p alb  p blc    p    blc   p alb o       Examples Birds fly        I I I I  The next two lemmas yield two ways of confirming conjunctions of events   This inference graph aims to capture a lot of information  If something is a bird  we Lemma      Irrelevance  If conf a c  and believe it is more likely that it will fly  and a is conditionally independent of b given c  if it flies  it is more likely to be airborn  If something is an emu  we are less likely to then conf a bc          I I  believe it flies  but if it is a flemu  flying emu  we again change our belief  We are inclined  to believe that birds have feathers   I  W hat  inferences can we make from this graph   Birds fty   emus don t  conj fty  bird  single  More  links  and  coni   fly  emu   containing  importantly   this  we  from the  information   cannot  conf fly emu   i e   we do not have the tiple extension   Emus don t vanish is true  we show  We  problem   Lemma     to conclude  I  We can prove  Our  quaker in creases belief in dove and republican increases belief in hawk  Since the graph contains no  prove  mul  can use  that  information about the joint distribution  we  coni   fly  b ird   emu    do not conclude either  hawk or dove if Nixon  is both   Exactly the opposite  coni  emu  bird    system concludes  However   Note that  ers  we do not accept conclusions  just increase  and  we want to  Republicans  conclude  are  political   exelusion  S     Default reasoners simply add  all the links with the result that In default reason   true given  ers based on theorem provers  there is typi cally a single proof of the default  birds  airbom from emu using fly   Poole      solves this  by accepting only what is true in every ex  tension  The semantics of extensions are not very well understood  it is trivial to gener ate probability distributions where proposi tions true in every extension are less likely than those that are  not  Thus the meaning of this conclusion is unclear  We conclude an individual is less likely to be airborn given it is an emu because  coni   airbom    fly   airbom  coni  emu    fly   and  independent of  emu given fly   Feathered things fty we can show  and  is conditionally  conj feathers  fly  using c         b ird   dove  or    dove       political  is  It is possible  to prove that an object about which nothing is known is a political non emu  We solve the problem in this formalism by making  hawk  and  dove  mutually exclu  sive but not necessarily exhaustive outcomes  Below is the historic example of  not  ing to draw an inference  H Dick is both a  show that    hawk  is confirmed by  quaker  H po quaker   we allow this  then we can confirm both  litical and  its negation given  Royal and African Elephants  This appears in           Intuitively  the graph  is suppose to show elephants are typically gray  but Royal elephants are not  H Clyde is  elephants are gray  If  phant  royal is  true  then  ele  is true and the conditional indepen  Quaker and a Republican  we do not want to  dence assumptions shield  conclude he is a hawk or dove   of       I I I  We use Lemma      to conclude African  want  I  and transitive inference is not being able to  dove  we can make the desired inference using Lemma      T he price we pay for consistency  are we to conclude about grayness   Modified Nixon Diamond  I  I  Since  conditionally independent of  both and African and a Royal elephant what       I  political is quaker given  of some random event        W ith Lemma  In  heritance sy stems cannot represent mutual  our belief in them   Emus are not airborn  Quak  I  african and  gray from the effect we conclude conf   gray  royal  I I I I I I I   I I I  african    This would not be true if the links sists of a straightforward transcription of the from royal and african to elephant were prob Lemmas in Section   into Prolog and the sys tem prints a readable proof of the probabilis abilistic  tic inferences it makes   I  Acknowledgements  I I I I  Horty et al and Sandewall disagree on this  We claim there are no  right  answers to this question and we build different graphs to model domains with different properties        Naive diagnosis  Consider the diagnostic dual to the  birds fly  problem   I I I  
  used by Recommender Systems such as Amazon com  Collaborative filtering is the process of mak ing recommendations regarding the potential preference of a user  for example shopping on  the Internet  based on the preference ratings  of the user and a number of other users for various items  This paper considers collabo rative filtering bMed on explicit multi valued ratings  To evaluate the algorithms  we con sider only pure collaborative filtering  using ratings  exclusively  and no other information  about the people or items    a book store on the web  CDNow com  a CD store on the web  and MovieFinder  com   a movie site on  the internet  Schafer  Konstan and Riedl         Collaborative filtering  CF  is the process of making  predictions whether a user will prefer a particular item   given his or her ratings on other items and given other people s ratings on various items including the one in question  CF relies on the fact that people s prefer ences are not randomly distributed  there are patterns within the preferences of a person and among simi  lar groups of people  creating correlation  The user for whom we are predicting a rating is called the ac  Our approach is to predict a user s prefer  tive user  In collaborative filtering  the main premise  ences regarding a particular item by using  is that the active user will prefer items which like  other people who rated that item and other  minded people prefer  or even that dissimilar people  items rated by the user as noisy sensors  The  don t prefer   noisy sensor model uses Bayes  theorem to  a set of ratings for various user item pairs  predict a  The problem can be formalized  given  compute the probability distribution for the  rating for a new user item pair  It is interesting that  user s rating of a new item   the abstract problem is symmetric between users and  We give two  variant models  in one  we learn a classical normal linear regression model of how users rate items  in another  we assume different users rate items the same  but the accuracy of the sensors needs to be learned  We com pare these variant models with state of the art techniques and show how they are sig  nificantly better  whether a user has rated only two items or many  We report empir ical results using the EachMovie database     of movie ratings  We also show that by con sidering items similarity along with the users similarity  the accuracy of the prediction in creases   items  Collaborative filtering has been an active area of re search in recent years   Several collaborative filter  ing algorithms have  suggested  ranging from bi  been  nary to non binary rating  implicit and explicit rating  Initial collaborative filtering algorithms were based on statistical methods using correlation between user preferences  Resnick  Iacovou  Suchak  Bergstrom and Riedl        Shardanand and Maes         These cor relation based algorithms predict the active user rat ings as a similarity weighted sum of the other users ratings   These algorithms are also referred to as  memory based algorithms  Breese  Heckerman and Kadie         Collaborative filtering is different to the standard supervised learning task because there are     Introduction  Collaborative filtering is a key technology used to build Recommender Systems on the Internet  It has been   http    research compaq com SRC eachmovie   only two attributes  each with a large domain  it is the structure within the domains that are important to the prediction  but this structure is not provided ex plicitly  Recently  some researchers have used machine learning methods  Breese et al         Ungar and Fos    SHARMA   POOLE  UAI      ter        for collaborative filtering algorithms  These methods essentially discover the hidden attributes for users and items  which explain the similarity between users and items  Breese et al   Breese et a          proposed and eval uated two probabilistic models for model based col laborative filtering algorithms  cluster models and Bayesian networks  In the cluster model  users with similar preferences are clustered together into classes  The model s parameters  the number of clusters  and the conditional probability of ratings given a class are estimated from the training data  In the Bayesian net work  nodes correspond to items in the database  The training data is used to learn the network structure and the conditional probabilities  Lawrence and Giles  collaborative filtering algorithm  Pennock et al  Pennock  Horvitz         proposed  a  called personality diagnosis  P D  and showed that PD makes better predictions than other memory and model based algorithms  This algor ithm is b ased on a probabilistic model of how people rate items  which is similar to our noisy sensor model approach   In this paper we propose and evaluate a probabilis tic approach based on a noisy sensor model  which is symmetric between users and items  Our approach is based on the idea that to predict an active user s rating for a particular item  we can use all those people who rated that item and other items rated by the active user as the noisy sensors  We view the noisy sensor model as a belief network  The conditional probabil ity table associated with each sensor reflects the noise in the sensor  To model how another user  user u  can act as a noisy sensor for the active user a s rating  we need to find a relationship between their preferences  Unfortunately  there is usually very little data  so we need to make a priori some assumptions about the relationship  Here we give two variants of the general idea for learning the noisy sensor model for explicit multi valued rat ing data  one  where we learn a classical normal lin ear regression model of how users rate items   Noisyl    and another  where we assume that the different users rate items the same and learn the accuracy of the sen sor Noisy    In  order to avoid a perfect fit wi th sparse data we add some dummy points before fitting the relationship  We use hierarchical prior to distribute the effect of dummy points over all possible rating pairs  After learning the noisy sensor model  i e  the con ditional probability table associated with each sensor node   we use Bayes  theorem to compute the proba bility distribution of the user a s rating of a new item        We evaluate both Noisyl and Noisy  on the Each Movie database of movie ratings and compare them to the state of the art techniques  We also show that symmetric collaborative filtering  which employs both user and item similarity  offers better accuracy than asymmetric collaborative filtering  Filtering Problem and     Mathematical Notation  Let N be the number of users and M be the total num ber of items in the database  S is an N x M matrix of all user s ratings for all items  Sui is the rating given by user u to item i  Let the ratings be on a cardi nal scale with m values that we denote v   v          Vm Then each rating Sui has a domain of possible values  v  v     vm    In collaborative filtering  S  the user item matrix  is ge ner ally very sparse since each user will only have rated a small percentage of the total number of items  Under this formulation  the collabo rative filtering problem becomes predicting those Sui which are not defined in S  the user  item matrix        Collaborative Filtering Using the     Noisy Sensor Model  We propose a simple probabilistic approach for sym metric collaborative filtering using the noisy sensor model for predicting the rating by user a  active user  of an item j  We use as noisy sensors    all users who rated the item j    all items rated by user  a  The sensor model is depicted as a naive Bayesian net work in Figure    The direction of the arrow shows that the prediction of user a for item j causes the sen sor u to take on a particular prediction for item j  The sensor model is the conditional probability table asso ciated with each sensor node  The noise in the sensor is reflected by the probability of incorrect prediction  that is  by the conditional probability table associated with it  To keep the model simple we use the indepen dence assumption that the prediction of any sensor for item j is independent of others  given the prediction of user a for item j  We need the following probabilities for Figure    P r  SuiiSaj    the probability of user u s prediction for item j  given the prediction of user a for item j  Pr   SakiSaj   for item  k     the probability of user a s prediction given the prediction of user a for item j    SHARMA   POOLE       UAI      to find a straight line which best fits these points  We assume that the mean of y can be expressed as a lin ear function of independent variable x  Since a model based on an independent variable cannot in general predict exactly the observed values of y  it is neces  sary to introduce the error e   For the ith observation  we have the following  y     a      e     x   We assume that unobserved errors  e   are indepen dent and normally distributed with a mean of zero  variance       and the If Yi is the linear function of e   which is normally distributed  then y  is also normally distributed  We as s ume the same variance for all the observations  T h e mean and variance of y  are given thus  Figure    Naive Bayesian Network Semantics for the Noisy Sensor Model  Pr  Saj    the prior probabil ity of active  user a s pre  diction for item j   We  the prior probability distribution of user a s rating for item j by the fraction of rating vi in the training data set  where ViE  vt V        vm     E  yi     x     a    var  y              For th e ith observation  the probability distribution function of y which is normally distributed can be writ ten thus   compute  Pr  Saj   vi    p  y where     y Jx   Ui         x    a         x   V a  exp        y    u      Given the conditional probabilities table for all sen sors  we can comp ute the probability distribution for user a s rating of an unseen item j  using the noisy sensor model as described in Figure    By applying Bayes  rule we can have the following   The joint probability distribution function   or the like lihood function denoted by LF  a      a    is the pr od uct of the individual P  y lx   over all observations   Pr  Saj I  Slj        SNj   LF  a j  a    ex   Pr  Saj   N    II  u l  Pr       Sal        SaM   M   Suj ISaj   II Pr  SakI Saj        n    IT P y Jx       k l  To use the noisy sensor model for collaborative fil tering we need the probability table for probabilities  Pr SuilSa i  and Pr Sa k Sa J  Consider first the problem of estimating Pr  Sui ISai     which is the problem of estimating user u s rating for item j given user a s rating of it  There is typically sparse data for the m x m probability table and we need to make some prior assumptions about the rela tionship  We assume that there is a linear relation ship with Gaussian error between the preferences of users and  similarly  between the ratings received by the items  Suppose the rating of user a  the indepen dent variable  is denoted by x  and t hat the rating of user u  the dependent variable  is denoted by y  Sup pose that user a and user u co  rated n it ems and their ratings over n co rated items are denoted by n pairs of observations  xl yl   x  Y          xn Yn   We want  We apply the maximum likelihood method  Gujarati        to estimate unknown parameters  a             The likelihood is maximum at the following values of the parameters  a      n LYi  f   x    After calculating the parameters a     and      we can write the expression of the probability distribution of user u s preference for item j given the user a s pref erence for it as follows  Pr   Suj    Xuj ISaj     Xaj      P   y     Xuj Jx      Xa j    UAI      SHARMA         exp v   fcr   To estimate       Xuj   a    Xaj        u  Pr  SakiSaj    We use the prior distribution of rating pairs for dis we use the same model as  case the independent variable  denotes the rating re  x  while dependent variable    In this  y denotes  And  the n pairs of  the rating received by item k    xl Yl    x  Y             xn Yn  are the rat j and k by those n users  who have co rated both items j and k  observations  ings over item  After  computing  Pr  SujiSaj  for and Pr  SakiSaj   the  probability  all users   u   for all items   k   distribution  who rated item  j   rated by user a  we  can compute the probability distribution of the user a rating for item j using Equation we need to compute and          We compute the prior distribution of  each rating pair by its frequency in the training data   Pr  Suj ISaj   j        rating pairs      described above for computing ceived by item    POOLE        u   i         In this model  free parameters   a        where u is the number of users who rated  the item j and i is the number of items rated by user  a   tributing the effect of K dummy points over all rating pairs like hierarchical  prior   be distributed over all possible rating pairs  We have experimented with parameter K  and we found that  Noisyl  performs better with K      For subsequent  experiments we  therefore  chose K     for       Noisyl   S e lec t i ng Noisy Sensors  For determining the reliability of the noisy sensors  we consider the  goodness of fit of  the fitted regression  line to a set of observations  We use the coefficient of determination  r    Gujarati         a measure that tells  how well the regression line fits the observations  This coefficient measures the proportion or percentage of the total variation in the dependent variable explained by the regression model   When the linear relationship exceeds the maximum  This  however  reduces  our ability to guarantee the ratings for K items will  r   is calculated as follows   Gujarati          value of the rating scale  we use the maximum value  when it is lower than the minimum value of the rating scale  we use the minimum value      To predict a rating  for example  to compare it with          e  L       Yi  fJ    other algorithms that predict ratings   we predict the  where y is the mean of the ratings of user u   expected value of the rating   The value of  The expected value of  the rating is defined as follows   E  Saj     is  e    Slj        SNi  I   Sal        SaM    lies b etween   and           r                 for each observation  co rated item    r      On     it means there is no linear  is horizontal line going through the mean y  We order  find a perfect linear relationship  even though the sen sor isn t perfect  If there is a perfect fit between users a and u  then the variance will be zero according to the above calculations  Therefore  the sensor u s predic  tion for item j will be perfect  or deterministic  that is   the conditional probability table associated with sen We do not want  this for our noisy sensor model because a determinis tic sensor will discount the effects of other sensors  For example  often one or two co rated items always have a perfect fit  even though such a user is not a good sensor   the user and item noisy sensors according to  r    We  use the best U user noisy sensors and best I item noisy sensors for making the predictions   The parameter  settings for U and I are explained in the next section        Variant Noisy  of Noisyl  The problem with  Noisyl is that  we must often fit lin  ear relationships with very little data  co rated items   It may be better to assume a priori the linear model and then simply learn the noise  The algorithm  Noisy   is based on the idea that different users rate items the same and  similarly  different items receive the same rating  We assume that the preferences of user a and  We hypothesize that this problem can be overcome if we add K dummy observations along with n obser vations  co rated items   We assume that user u  r      there exists a perfect linear relationship  relationship between users a and u and the best fit line  While trying to fit Jines with sparse data  we often  user     the other hand  if  K Dummy Observations  sor u will be purely deterministic   r   between the preferences of user a and user u  that      Pr  Saj  Vi IX   Vi  where X        If  a  and  user u are the same  that is  the expected value of user  u s preference of any item is equal to active user a s preference for th at item   give ratings over K dummy items  K      in  E  such a way that their ratings for K dummy items are distributed over all possible rating pairs  For m scale  rating data there are  m   possible combination of the   Yilx   x            x   We learn the variance of user u s prediction  The al gorithm  Noisy  can be  derived from algorithm  Noisyl        SHARMA   POOLE  by making the regression coefficients a     and         In this model we need to compute  u   i  free param eters  a    where u is the number of users who rated the item j and i is the number of items rated by user a  We also add the K dummy observations because the same problem  as discussed in Subsection      can arise in Noisy   We have experimented with parame ter K  and we found that Noisy  also performs better with K    For subsequent experiments we  there fore  chose K     for Noisy  also     In Noisy  we are not fitting the relationship between user a and user u  but we assume an equal relation ship  So  it doesn t make sense to use the coefficient of determination r  for finding the reliability of the noisy sensors  Rather  to find the reliability of the noisy sen sor  we use the variance  the less variance  the more reliable the noisy sensor  We use the best U user noisy sensors and best I item noisy sensors for making the predictions  The parameter settings for U and I are explained in the next section      Evaluation Framework  To evaluate the accuracy of collaborative filtering al gorithms we used the training and test set approach  In this approach  the dataset of users  and their rat ings  is divided into two  a training set and a test set  The training set is used as the collaborative filtering dataset  The test set is used to evaluate the accuracy of the collaborative filtering algorithm  We treat each user from the test set as the active user  To carry out testing  we divide the ratings by each test user into two sets  Ia and Pa  The set Ia contains ratings that we treat as observed ratings  The set Pa contains the rat ings that we attempt to predict using a CF algorithm and observed ratings  Ia  and training set  To evaluate the accuracy of the collaborative filtering algorithm we use the average absolute deviation met ric  as it is the most commonly used metric  Breese et al          The lower the average absolute deviation  the more accurate the collaborative filtering algorithm is  For all users in the test set we calculate the average absolute deviation of the predicted rating against the actual rating of items  Let the number of predicted ratings in the test set for the active user be na  then the average absolute deviation for a user is given as follows   Sa       LjEPa IPa j  ra jl   where Pai is user a s observed rating for item j and raj is user a s predicted rating for item j  These absolut e deviations are users in the test set   then averaged over all       UAI      Data and protocols  We compared both versions of our noisy sensor model to PD  Personality Diagnosis   Pennock et al         and Correlation  Pearson Correlation   Resnick et al          To compare the performance we used the same subset of the EachMovie database as used by Breese et al   Breese et al         and Pennock et al   Pennock et al          consisting of       movies          rat ings        users in the training set  and       users in the test set  In EachMovie database the ratings are elicited on a integer scale from zero to five  We also tested the algorithms on other subsets to verify that we are not finding peculiarities of the subset  We ran experiments with different amounts of ratings in set Ia to understand the effect of the amount of the observed ratings on the prediction accuracy of the collaborative filt ering algorithm  As in  Breese et al         for the AllBut  Protocol  we put a sin gle randomly selected rating for each test user in the test set Pa and the rest of the ratings in the observed set I    As in  Breese et al         for each GivenX Pro tocol  we place X ratings randomly for each test user in the observed set Ia  and the rest of the ratings in the test set Pa  We did the experiments for X        and          Selecting Noisy Sensors  After learning the noisy sensor model we determine which noisy sensors should be used in making the pre dic tion for the active user  Figure   shows the varia tion of average absolute deviation with best user noisy sensors for different numbers of best item noisy sensors for Noisy   We used the dataset as described above but the test rating and the observed ratings for each user of the test set were selected randomly  Figute   shows that the average absolute deviation de  creases with the increase in number of item sensors  There is no significant improvement in accuracy when the number of item sensors is more than twenty  It also shows that the average absolute deviation first decreases with the increase in number of user sensors and then increases as more user sensors are used for prediction  This is because the large number of user sensors results in too much noise for those user sensors that have good reliability  From the experiments  we found that both algo rithms give better performances with ten to twenty item noisy sensors and forty to seventy user noisy sen sors   For the experiments reported in the following section  we use the best fifty user noisy sensors and  We didn t use the test set for finding the number of user and item noisy sensors    UAI      SHARMA                      item nol ay sen Kit     item nol  y sensors     ilem noisy sensors    ilem no  l sensors                                               li lj O                     lf    y     il ily     so              user noisy sensors         i     il                                      agnosis match exactly with those reported in Pennock et al   Pennock et al          We took the results for Correlation directly from Pennock et al   Pennock et al             Item noisy senson   S IWI n noisy sensors        POOLE       Figure    The average absolute deviation as a function of the number of best user noisy sensors for different numbers of best item noisy sensors   Noisyl performed better than PD and Correlation for AllButl and Givenl  protocols  For GivenS and Given  protocols Noisyl performance is better than Correlation but not better than PD  We believe that Noisy  s poor performance can be explained by the fact the lines that are fitted to very small data sets are often poor fit to the actual relationship  The algo rithm Noisy   based on an equal relationship between users  doesn t suffer from the same problem  and out performed all algorithms under all protocols   Table Average absolute deviation scores on the EachMovie data for Noisy   Noisy   PD and Corre lation  note  lower scores are better        Algorithm  best twenty item noisy sensors  i e  U    and I     for both Noisyl and Noisy   The parameters U and I depend on the database  In the case of the Each Movie database  the number of users are more than the movies  also each user has rated only few movies  Due to this possibility more best user noisy sensors  U  are selected than the b est item noisy sensors  I         From Figure   we also see that the minimum aver age absolute deviation is      when we use both user and item noisy sensors  with sixty user and twenty item noisy sensors   It is      when we use only the user noisy sensors  shown by the zero item noisy sen sors case  and       when we use only the item noisy sensors  shown on the y axis for ten item noisy sen sors  This indicates that when we include the item noisy sensors along with the user noisy sensors  the quality of the prediction improves considerably  It also shows that if we use only the item noisy sensors for prediction  then the average absolute deviation be comes greater than when we use only user noisy sen sors  Therefore  symmetric collaborative filtering of fers better accuracy than asymmetric collaborative fil tering       Comparison  with Other Methods  We compared the algorithms Noisyl  Noisy   Corre lation and PD using the same training and test set as Pennock et al   Pennock et al          For each test user in the test set we use the same set of observed  Ia  and test ratings  Pa  as Pennock et al  The resultsof comparing Noisyl  Noisy   Correlation and PD are shown in Table    We re implemented Personality Diagnosis  Our results for Personality Di   Noisy  Noisyl PD Carrel  AllButl  Protocol Given   GivenS                                                                                                  Given   Shardanand and Maes  Shardanand and Maes        and Pennock et al   Pennock et al         proposed that the accuracy of a collaborative filtering algorithm should be evaluated on extreme ratings  very high or very low ratings  for items  The supposition is that  most of the time  people are interested in suggestions about items they might like or dislike  but not about items they are unsure of  Pennock et al   Pennock et al         defined the extreme ratings as those which are     above and     below the average rating in the subset  To compare the performance of algorithms with extreme ratings we computed the predicted rat ings for those test cases from the test set Pa of all protocols  whose observed rating is less than R      or greater than R        where R is the overall average rating in the subset  Table    Average absolute deviation scores on Each Movie data for Noisyl  Noisy   PD and Correlation for extreme ratings  Algorithm Noisy  Noisyl PD Carrel  AllButl                          Protocol Given   GivenS                                                     Given                             SHARMA       The results for the extreme ratings are shown in Ta ble    The results for extreme ratings show that Noisy  performs better than Noisy  for AllBut  proto col  It also performs better than PD and Correlation over Given   and GivenS protocols  Noisy  performs better than the other three algorithms over Given     GivenS and Given  protocols  Table    Significance levels of the differences in aver age absolute deviation between Noisy  and PD  and between Noisy  and PD  on EachMovie data  note  low significance levels indicate that the differences in results are unlikely to be coincidental   Protocol AllBut  AllBut   extreme  Given   Given    extreme  Given   Given   extreme  Given  Given   extreme   vs  Noisy   PD vs   Noisy   PD  vs  PD  Noisy   vs  PD  Noisy                                                                                                                                                                                                                               To determine the statistical significance of these re sults  we computed the significance levels for the dif ferences in average absolute deviation between Noisy  and PD  PD and Noisy   Noisy  and PD  and PD and Noisy for all protocols  To do this  we divided the test set for all protocols into    samples of equal size and used randomization paired sample test of differences of means  Cohen         This method calculates the sam pling distribution of the mean difference between two algorithms by repeatedly shuffling and recalculating the mean difference in        different permutations  The shuffling reverses the sign of the difference score for each sample with a probability of     The statistical significance results of the EachMovie data results are shown in Table    it shows the prob ability of achieving a difference less than or equal to the mean difference  That is  it shows the probability of incorrectly rejecting the n ull hypothesis that both algorithms  deviation scores arise from the same dis tribution     Conclusion  In this paper  we have concerned ourselves with sym metric collaborative filtering based on explicit ratings    POOLE  UAI      used for making recommendations to a user based on ratings of various items by a number of people  and the user s ratings of various items  We have described a new probabilistic approach for symmetric collaborative filtering based on a noisy sen sor model  We have shown that the noisy sensor model makes better predictions than other state of the art techniques  The results for Noisy  are highly statis tically significant  We have also shown that by includ ing the items similarity along with users similarity  the accuracy of prediction increases  This paper has only considered the accuracy of the noisy sensor model  not on the computational issues involved  It is beyond the scope of this paper to consider the trade off between off line and online computation and effective indexing to find the best matches     Acknowledgments  We thank Compaq Equipment Corporation and David M  Pennock for providing the EachMovie database and subsets used in this study  We also thank Holger Hoos for providing the valuable comments  This work was supported by Institute for Robotics and Intelli gent Systems project BOU and the Natural Sciences and Engineering R esearch Council of Canada Operat ing Grant OGP         
 The Bayesian Logic  BLOG  language was recently developed for defining first order probability models over worlds with unknown numbers of objects  It handles important problems in AI  including data association and population estimation  This paper extends BLOG by adopting generative processes over function spaces  known as nonparametrics in the Bayesian literature  We introduce syntax for reasoning about arbitrary collections of objects  and their properties  in an intuitive manner  By exploiting exchangeability  distributions over unknown objects and their attributes are cast as Dirichlet processes  which resolve difficulties in model selection and inference caused by varying numbers of objects  We demonstrate these concepts with application to citation matching      Introduction  Probabilistic first order logic has played a prominent role in recent attempts to develop more expressive models in artificial intelligence                           Among these  the Bayesian logic  BLOG  approach      stands out for its ability to handle unknown numbers of objects and data association in a coherent fashion  and it does not assume unique names and domain closure  A BLOG model specifies a probability distribution over possible worlds of a typed  first order language  That is  it defines a probabilistic model over objects and their attributes  A model structure corresponds to a possible world  which is obtained by extending each object type and interpreting each function symbol  Objects can either be guaranteed  meaning the extension of a type is fixed  or they can be generated from a distribution  For example  in the aircraft tracking domain      the times and radar blips are known  and the number of unknown aircraft may vary in possible worlds  BLOG as a case study provides a strong argument for Bayesian hierarchical methodology as a basis for probabilistic first order logic   BLOG specifies a prior over the number of objects  In many domains  however  it is unreasonable for the user to suggest such a proper  data independent prior  An investigation of this issue was the seed that grew into our proposal for Nonparametric Bayesian Logic  or NP BLOG  a language which extends the original framework developed in       NP BLOG is distinguished by its ability to handle object attributes that are generated by unbounded sets of objects  It also permits arbitrary collections of attributes drawn from unbounded sets  We extend the BLOG language by adopting Bayesian nonparametrics  which are probabilistic models with infinitely many parameters      The statistics community has long stressed the need for models that avoid commiting to restrictive assumptions regarding the underlying population  Nonparametric models specify distributions over function spaces  a natural fit with Bayesian methods  since they can be incorporated as prior information and then implemented at the inference level via Bayes theorem  In this paper  we recognize that Bayesian nonparametric methods have an important role to play in first order probabilistic inference as well  We start with a simple example that introduces some concepts necessary to understanding the main points of the paper  Consider a variation of the problem explored in       You have just gone to the candy store and have bought a box of Smarties  or M Ms   and you would like to discover how many colours there are  while avoiding the temptation to eat them    Even though there is an infinite number of colours to choose from  the candies are coloured from a finite set  Due to the manufacturing process  Smarties may be slightly discoloured  You would like to discover the unknown  true  set of colours by randomly picking Smarties from the box and observing their colours  After a certain number of draws  you would like to answer questions such as  How many different colours are in the box  Do two Smarties have the same colour  What is the probability that the first candy you select from a new box is a colour you have never seen before  The graphical representation of the BLOG model is shown in Fig   a  The number of Smarties of different colours  n Smartie   is chosen from a Poisson distribution with   it is unreasonable to expect a domain expert to implement nonparametrics considering the degree of effort required to grasp these abstract notions  We show that Bayesian nonparametrics lead to sophisticated representations that can be easier to implement than their parametric counterparts  We formulate a language that allows one to specify nonparametric models in an intuitive manner  while hiding complicated implementation details from the user  Sec    formalizes our proposed language extension as a set of rules that map code to a nonparametric generative process  We emphasize that NP BLOG is an extension to the BLOG language  so it retains all the functionality specified in        Figure     a  The BLOG and  b  NP BLOG graphical models for counting Smarties  The latter implements a Dirichlet process mixture  The shaded nodes are observations  mean Smartie   A colour for each Smartie s is drawn from the distribution HColourDist   Then  for every draw d  zSmartieDrawn  d  is drawn uniformly from the set of Smarties             n Smartie    Finally  we sample the observed  noisy colour of each draw conditioned on zSmartieDrawn  d  and the true colours of the Smarties  The NP BLOG model for the same setting is shown in Fig   b  The true colours of an infinite sequence of Smarties s are sampled from HColourDist   Smartie is a distribution over the choice of coloured Smarties  and is sampled from a uniform Dirichlet distribution with parameter Smartie   Once the Smarties and their colours are generated  the true Smartie for draw d  represented by the indicator zSmartieDrawn  d    s  is sampled from the distribution of Smarties Smartie   The last step is to sample the observed colour  which remains the same as in the BLOG model  One advantage of the NP BLOG model is that it determines a posterior over the number of Smarties colours without having to specify a prior over n Smartie   This is important since this prior is difficult to specify in many domains  A more significant advantage is that NP BLOG explicitly models a distribution over the collection of Smarties  This is not an improvement in expressiveness  one can always reverse engineer a parametric model given a target nonparametric model in a specific setting  Rather  nonparametrics facilitate the resolution of queries on unbounded sets  such as the colours of Smarties  This plays a key role in making inference tractable in sophisticated models with object properties that are themselves unbounded collections of objects  This is the case with the citation matching model in Sec       in which publications have collections of authors  The skeptic might still say  despite these advantages  that  We focus on an important class of nonparametric methods  the Dirichlet process  DP   because it handles distributions over unbounded sets of objects as long as the objects themselves are infinitely exchangeable  a notion formalized in Sec       The nonparametric nature of DPs makes them suitable for solving model selection problems that arise in the face of identity uncertainty and unknown numbers of objects  Understanding the Dirichlet process is integral to understanding NP BLOG  so we devote a section to it  Sec      shows how DPs can characterize collections of objects  Models based on DPs have been shown to be capable of solving a variety of difficult tasks  such as topic document retrieval          Provided the necessary expert knowledge  our approach can attack these applications  and others  We conduct a citation matching experiment in Sec     demonstrating accurate and efficient probabilistic inference in a real world problem      Dirichlet processes  A Dirichlet process G     H  DP    H   with parameter  and base measure H  is the unique probability measure defined G on the space of all probability measures    B   where  is the sample space  satisfying  G           G K    Dirichlet H           H K       for every measurable partition             K of   The base measure H defines the expectation of each partition  and  is a precision parameter  One can consider the DP as a generalization of the Dirichlet distribution to infinite spaces  In Sec       we formalize exchangeability of unknown objects  In order to explain the connection between exchangeability and the DP  it is instructive to construct DPs with the Polya urn scheme      Consider an urn with balls of K possible colours  in which the probability of the first ball being colour k is given by Hk   We draw a ball from the urn  observe its colour     then return it to the urn along with another ball of the same colour  We then make another draw  observing its colour with probability p     k        Hk       k         After N observations  the colour of the next ball is distributed as Hk   P  N      k   N      N  PN   i   k     N  i          The marginal P    N   of this process  obtained by applying the chain rule to successive predictive distributions  can be shown to satisfy the infinite mixture representation   Z K PN Y    k  i P    N     DP H  d       k i   M   k    where the k are multinomial success rates of each colour k  This result  a manifestation of de Finettis theorem  establishes the existence and uniqueness of the DP prior for the Polya urn scheme      In the Polya urn setting  observations i are infinitely exchangeable and independently distributed given the measure G  Thus  what we have established here in a somewhat cursory fashion is the appropriateness of the DP for the case when the observations i are infinitely exchangeable  Analogously  if the urn allows for infinitely many colours  then for any measurable interval  of  we have p N        N      N   X H      i      N    N i    The first term in this expansion corresponds to prior knowledge and the second term corresponds to the empirical distribution  Larger values of  indicate more confidence in the prior H  Note that  as N increases  most of the colours will be repeated  Asymptotically  one ends up sampling colours from a possibly large but finite set of colours  achieving a clustering effect  Nonetheless  there is always some probability of generating a new cluster  DPs are essential building blocks in our formulation of nonparametric first order logic  In the literature  these blocks are used to construct more flexible models  such as DP mixtures and hierarchical or nested DPs          Since observations are provably discrete  DP mixtures add an additional layer xi  P  xi  i   in order to model continuous draws xi from discrete mixture components i   In the Polya urn scheme  G is integrated out and the i s are sampled directly from H  Most algorithms for sampling DPs are based on this scheme              In the hierarchies constructed by our language  however  we rely on an explicit representation of the measure G since it is not clear we can always integrate it out  even when the measures are conjugate  This compels us to use the stick breaking construction       which establishes that i i d  sequences wk  Beta      and k  H can be used Pto construct the equivalent empirical distribution G   k   k  k    Qk  where the stick breaking weights k   wk j       wj   and can be shown to sum to unity  We abbreviate the sampling of the weights as k  Stick    This shows that G is an infinite sum of discrete values  The DP mixture due to the stick breaking construction is i   H  H zi            Stick   xi   i   zi  p xi  zi          where zi   k indicates that sample xi belongs to component k  The Smarties model  Fig   b  is in fact an example of a DP mixture  where the unbounded set of colours is   By grounding on the support of the observations  the true number of colours K is finite  At the same time  the DP mixture is open about seeing new colours as new Smarties are drawn  In NP BLOG  the unknown objects are the mixture components  NP BLOG semantics  Sec     define arbitrary hierarchies of Dirichlet process mixtures  By the stick breaking construction      every random variable xi has a countable set of ancestors  the unknown objects   hence DP mixtures preserve the well definedness of BLOG models  To infer the hidden variables of our models  we employ the efficient blocked Gibbs sampling algorithm developed in     as one of the steps in the overall Gibbs sampler  One complication in inference stems from the fact that a product of Dirichlets is difficult to simulate  Teh et al       provide a solution using an auxiliary variable sampling scheme      Syntax and semantics  This section formalizes the NP BLOG language by specifying a procedure that takes a set of statements L in the language and returns a model   A model comprises a set of types  function symbols  and a distribution over possible worlds      We underline that our language retains all the functionality of BLOG  Unknown objects must be infinitely exchangeable  but this trivially the case in BLOG  Sec      elaborates on this  We illustrate the concepts introduced in this section with an application to citation matching  Even though our citation matching model doesnt touch upon all the interesting aspects of NP BLOG  the reader will hopefully find it instrumental in understanding the semantics      Citation matching One of the main challenges in developing an automated citation matching system is the resolution of identity uncertainty  for each citation  we would like to recover its true title and authors  For instance  the following citations from the CiteSeer database probably refer to the same paper  Kozierok  Robin  and Maes  Pattie  A Learning Interface Agent for Meeting Scheduling  Proceedings of the      International Workshop on Intelligent user Interfaces  ACM Press  NY  R  Kozierok and P  Maes  A learning interface agent for scheduling meetings  In W  D  Gray  W  E  Heey  and D  Murray  editors  Proc  of the Internation al Workshop on Intelligent User Interfaces  Orlando FL  New York        ACM Press   Even after assuming the title and author strings have been segmented into separate fields  an open research problem itself    citation matching still exhibits serious challenges  two different strings may refer to the same author  e g  J F G  de Freitas and Nando de Freitas  and  conversely  the same string may refer to different authors  e g  David Lowe in vision and David Lowe in quantum field theory        type Author  type Pub  type Citation     guaranteed Citation      Author  NumAuthorsDist        Pub  NumPubsDist       Name a   NameDist       Title p   TitleDist       NumAuthors p   NumAuthorsDist       RefAuthor p  i  if Less i  NumAuthors p   then  Uniform Author a      RefPub c   Uniform Pub p      CitedTitle c   TitleStrDist Title RefPub c        CitedName c  i  if Less i  NumAuthors RefPub c    then  NameStrDist Name RefAuthor RefPub c   i      Figure    BLOG model for citation matching          type Author  type Pub     type Citation  type AuthorMention     guaranteed Citation  guaranteed AuthorMention     Name a   NameDist       Title p   TitleDist       CitedTitle c   TitleStrDist Title RefPub c        RefAuthor u   PubAuthorsDist RefPub CitedIn u        CitedName u   NameStrDist Name RefAuthor u      Figure    NP BLOG model for citation matching  There are a number of different approaches to this problem  Pasula et al  incorporate unknown objects and identity uncertainty into a probabilistic relational model       Wellner et al  resolve identity uncertainty by computing the optimal graph partition in a conditional random field       We elaborate on the BLOG model presented in      in order to contrast it with the one we propose  The BLOG model is shown in Fig    with cosmetic modifications and the function declaration statements omitted  The BLOG model describes a generative sampling process  Line   declares the object types  and line   declares that the citations are guaranteed  hence are not generated by a number statement   Lines   and   are number statements  and lines      are dependency statements  their combination defines a generative process  The process starts by choosing a certain number of authors and publications from their respective prior distributions  Then it samples author names  publication titles and the number of authors per publication  For each author string i in a citation  we choose the referring author from the set of authors  Finally  the properties of the citation objects are chosen  For example  generating an interpretation of CitedTitle c  for citation c requires values for RefPub c  and estimates of publication titles  TitleStrDist s  can be interpreted as a measure that adds noise in the form of perturbations to input string s  The NP BLOG model in Fig    follows a similar generative approach  the key differences being that it samples collections of unknown objects from DPs  and it allows for uncertainty in the order of authors in publications  But what do we gain by implementing nonparametrics  The advan   Figure    Three representations of lines     in Fig     as an NP BLOG program  as a generative process  and as a graphical model  Darker  hatched nodes are fixed or generated from other lines and shaded nodes are observed  Note the similarity between the graphical model and Fig   b  Lines     describe a DP mixture     over the publications p  where the base measure is TitleDist   Title is the hidden distribution over publication objects  the indicators are the true publications zRefPub  c  corresponding to the citations c  and the continuous observations are the titles CitedTitle  c   tage lies in the ability to capture sophisticated models of unbounded sets of objects in a high level fashion  and the relative ease of conducting inference  since nonparametrics can deal gracefully with the problem of model selection  One can view a model such as the automatic citation matcher from three perspectives  it is a set of statements in the language that comprise a program  from a statisticians point of view  the model is a process that samples the defined random variables  and from the perspective of machine learning  it is a graphical model  Fig    interprets lines     of Fig    in three different ways  The semantics  as we will see  formally unify all three perspectives  Both BLOG and NP BLOG can answer the following queries  Is the referring publication of citation c the same as the referring publication of citation d  How many authors are there in the given citation database  What are the names of the authors of the publication referenced by citation c  How many publications contain the author a  where a is one of the authors in the publication referenced by citation c  And what are the titles of those publications  However  only NP BLOG can easily answer the following query  what group of researchers do we expect to be authors in a future  unseen publication      Objects and function symbols This section is largely devoted to defining notation so that we can properly elaborate on NP BLOG semantics in Sections     to      The notation as it appears in these sections makes the connection with both first order logic and the Dirichlet process mixture presented in Sec       The set of objects of a type  is called the extension of    and is denoted by      In BLOG  extensions associated with unknown  non guaranteed  types can vary over possible worlds   so we sometimes write       The size of     is denoted by n       Note that objects may be unknown even if there is a fixed number of them  Guaranteed objects are present in all possible worlds  We denote  to be the set of possible worlds for model   A model introduces a set of function symbols indexed by the objects  For conciseness  we treat predicates as Boolean functions and constants as zero ary functions  For example  the citation matching model  Fig     has the function symbols Name and CitedTitle  among others  so there is a Name a  for every author a and CitedTitle c  for every citation c  By assigning numbers to objects as they are generated  we can consider logical variables a and c to be indices on the set of natural numbers  Since BLOG is a typed language  the range of interpretations of a function symbol f is specified by its type signature  For example  the interpretation of RefAuthor u   for each u   AuthorMention                   n AuthorMention    takes a value on the range  Author   Likewise  Title p  ranges over the set of strings  String   Figures   and   omit function declaration statements  which specify type signatures  Nonetheless  this should not prevent the reader from deducing the type signatures of the functions via the statements that generate them  Nonparametric priors define distributions over probability measures  so we need function symbols that uniformly refer to them  Letting X and Y be object domains  e g  X    Author    we define MD  X   Y  to be the set of conditional probability densities p x  X   y  Y  following the class of parameterizations D  We can extend this logic  denoting MD  MD  X   Y    Z  to be the set of probability measures p d  D   z  Z  over the choice of parameterizations d  D  conditioned on Z  And so on  For peace of mind  we assume each class of distributions D is defined on a measurable  field and the densities are integrable over the range of the sample space  Note that Y or Z  but not X   may be a Cartesian product over sets of objects  BLOG does not allow return types that are tuples of objects  so we restrict distributions of objects accordingly  One can extend the above reasoning to accommodate distributions over multiple unknown objects by adopting slightly more general notation involving products of sets of objects  We assign symbols to all the functions defined in the language L   For instance  the range of NameDist in Fig    is M  String   for some specified parameterization class  Since NameDist is not generated in another line  it must be fixed over all possible worlds  For each publication p  the interpretation of symbol PubAuthorsDist p  is assigned a value on the space MMultinomial   Author    That is  the   Even though the DP imposes a distribution over an infinite set of unknown objects  n     is still finite since it refers to the estimated number of objects in   n    corresponds to the random variables of the DP mixture  as explained in Sec        function symbol refers to a distribution over author objects  How the model chooses the success rate parameters for this multinomial distribution  given that it is not on the left side of any generating statement  is the subject of Sec       NP BLOG integrates first order logic with Bayesian nonparametric methods  but we have left out one piece of the puzzle  how to specify distributions such as NameDist  or classes of distributions  This is an important design decision  but an implementation level detail  so we postpone it to future work  For the time being  one can think of parameterizations as object classes in a programming language such as Java that generate samples of the appropriate type  We point out that there is already an established language for constructing hierarchical Bayesian models  BUGS       The truth of any first order sentence is determined by a possible world in the corresponding language  A possible world    consists of an extension     for each type  and an interpretation for each function symbol f   Sec      details how NP BLOG specifies a distribution over        Dependency statements for known objects The dependency statement is the key ingredient in the specification of a generative process  We have already seen several examples of dependency statements  and we formalize them here  It is well explained in       but we need to extend the definition in the context of nonparametrics  In BLOG  a dependency statement looks like f  x            xL    g t            tN          where f is a function symbol  x            xL is a tuple of logical variables representing arguments to the function  g is a probability density conditioned on the arguments t            tN   which are terms or formulae in the language L in which the logical variables x            xL may appear  The dependency statement carries out a generative process  For an example  lets look at the dependency statement on line    of Fig     Following the rules of semantics       line    generates assignments for random variables CitedTitle  c   for c              n Citation   from probability density g conditioned on values for zRefPub  c  and Title  p   for all p              n Pub   As in       we use square brackets to index random variables  instead of the statistics convention of using subscripts  In NP BLOG  the probability density g is itself a function symbol  and the dependency statement is given by f  x            xL    g t            tM   tM              tM  N        where f and g are function symbols  and t            tM  N are formulae of the language as in      For this to be a valid statement  g t            tM   must be defined on the range M X   Y   where X is the range of f  x            xL   and Y is the domain of the input arguments within the curly braces  The first M terms inside the parentheses are evaluated in possible world   and their resulting values determine the   choice of measure g  The terms inside the curly braces are evaluated in  and the resulting values are passed to distribution g t            tM    When all the logical variables x            xL refer to guaranteed objects  the semantics of the dependency statement are given by       The curly brace notation is used to disambiguate the two roles of input argument variables  The arguments inside parentheses are indices to function symbols  e g  the c in RefPub c  in Fig      whereas the arguments inside curly braces serve as input to probability densities  e g  the term inside the curly braces in TitleStrDist Title RefPub c      This new notation is necessary when a distribution takes both types of arguments  We dont have such an example in citation matching  so we borrow one from an NP BLOG model in the aircraft tracking domain   State a  t  if t     then  InitState   else  StateTransDist a  State a  t        The state of the aircraft a at time t is an R Vector object which stores the aircrafts position and velocity in space  When t      the state is generated from the transition distribution of aircraft a given the state at the previous time step  StateTransDist a  corresponds to a measure on the space M  R Vector     R Vector    For example  in line   of Fig     the citation objects are guaranteed  Following the rules of semantics  line   defines a random variable CitedTitle  c  corresponding to the interpretation of function symbol CitedTitle c  for every citation c  Given assignments to TitleStrDist   zRefPub  c   we use z to be consistent with the notation of the semantics used in this paper  although it makes no difference in BLOG  and Title  p  for all p   Pub   assignments that are either observed or generated from other statements  the dependency statement defines the generative process CitedTitle  c   TitleStrDist  Title  p   s t  p   zRefPub  c   BLOG allows for contingencies in dependency statements  These can be subsumed within our formal framework by defining a new measure   c  t    P i  ci  i  ti     ti             where    is the indicator function  ci is the condition i which must be satisfied in order to sample from the density i   c and t are the complete sets of terms and conditions  and the summation is over the number of clauses  Infinite contingencies and their connection to graphical models are discussed in           Exchangeability and unknown objects Unknown objects are precisely those which are not guaranteed  In this section  we formalize some important properties of generated objects in BLOG  We adopt the notion of exchangeability     to objects in probabilistic first order logic  We start with some standard definitions     In which aircraft in flight appear as blips on a radar screen  and the objectives are to infer the number of aircraft and their flight paths and to resolve identity uncertainty  arising because a blip might not represent any aircraft or  conversely  an aircraft might produce multiple detections        Definition    The random variables x            xN are  finitely  exchangeable under probability density function p if p satisfies p x            xN     p x              x N     for all permutations  on             N        When n is finite  the concept of exchangeability is intuitive  the ordering is irrelevant since possible worlds are equally likely  The next definition extends exchangeability to unbounded sequences of random variables  Definition    The random variables x    x          are infinitely exchangeable if every finite subset is finitely exchangeable      Exchangeability is useful for reasoning about distributions over properties on sets of objects in BLOG  From Definitions   and    we have the following result  Proposition    It is possible to define g in the dependency statements     and     such that the sequence of objects x            xL is finitely exchangeable if and only if the terms t            tM  N do not contain any statements referring to a particular xl   For example  the distribution of hair colours of two people  Eric and Mike  is not exchangeable given evidence that Eric is the father of Mike  What about sequences of objects such as time  As long as we do not set the predecessor function beforehand  any sequence is legally exchangeable  In this paper  models are restricted to infinitely exchangeable unknown objects  We can interpret this presupposition this way  if we reorder a sequence of objects  then their probability remains the same  If we add another object to the sequence at some arbitrary position  both the original and new sequence with one more object are exchangeable  We can then appeal to de Finettis theorem      and hence the Dirichlet process  Therefore  the order of unknown objects is not important  and we can reason about set of objects rather than sequences  While there are many domains in which one would like to infer the presence of objects that are not infinitely exchangeable  this constraint leaves us open to modeling a wide range of interesting domains  Unknown or non guaranteed objects are assigned non rigid designators  a symbol in different possible worlds does not necessarily refer to the same object  and so it does not make sense to assign it a rigid label  This consideration imposes a constraint  we can only refer to a publication p via a guaranteed object  such as a citation c that refers to it  While we cannot form a query that addresses a specific unknown object  or a subset of unknown objects  we can pose questions about publications using existential and universal quantifiers  resolved using Skolemization  for instance   We could ask  for instance  how many publications have three or more authors      Dependency statements for unknown objects Sec      formalized the notion of type extensions and function symbols in NP BLOG programs  Sec      served up the preliminaries of syntax and semantics in dependency   statements  The remaining step to complete the full prescription of the semantics as a mapping from the language L to a distribution over possible worlds  This is accomplished by constructing a Bayesian hierarchical model over random variables    n     such that the set of random variables  is in one to one correspondence with the set of function interpretations  n refers to the sizes of the type extensions  and  is a set of auxiliary random variables such R that p   n   d   p   n   One might wonder why we dont dispense of function symbols entirely and instead describe everything using random variables  as in       The principal reason is to establish the connection with firstorder logic  Also  we want to make it clear that some random variables do not map to any individual in the domain  What follows is a procedural definition of the semantics  We now define distributions over the random variables  and their mapping to the symbols of the first order logic  In order to define the rules of semantics  we collect dependency and number statements according to their input argument types  If the collection of statements includes a number statement  then the rules of semantics are given in       Otherwise  we describe how the objects and their properties are implicitly drawn from a DP  Consider a set of K dependency statements such that the generated functions f            fK all require a single input of type   and    can vary over possible worlds   We denote x to be the logical variable that ranges over      The output types of the fk s are not important   The K dependency statements look like f   x   g   t              t  M    t  M               t  M   N                   fK  x   gK  tK          tK MK  tK MK          tK MK NK   where Mk and Nk are the number of input arguments to gk    and gk     respectively  and tk i is a formula in the language in which x may appear  As in BLOG  each fk  x  is associated with a random variable fk  x   The random variables g            gK   including all those implicated in the terms  must have been generated by other lines or are observed  Overloading the notation  we define the terms tk i to be random variables that depend deterministically on other generated or observed random variables  The set of statements     defines the generative process   Stick        fk  x   gk  tk          tk Mk   tk Mk          tk Mk Nk        for k              K  x                where  is the userdefined DP concentration parameter and  is a multinomial distribution such that each success rate parameter  x determines the probability of choosing a particular object x  NP BLOG infers a distribution  over objects of type  following the condition of infinite exchangeability  For example  applying rules       to line   of Fig     we get Author  Stick Author   Name  a   NameDist   for a                If an object type does not have any dependency or number statements  then no distribution over its extension is introduced  e g  strings in the citation matching model   The implementation of the DP brings about an important subtlety  if x takes on a possibly infinite different set of values  how do we recover the true number of objects n     The idea is to introduce a bijection from the subset of positive natural numbers that consists only of active objects to the set             n      An object is active in possible world  if and only if at least one random variable is assigned to that object in   In the above example  n Author  is the number of author objects that are mentioned in the citations  Of course  in practice we do not sample an infinite series of random variables Name  a   If we declare a function symbol f with a return type  ranging over a set of unknown objects  then there exists the default generating process zf  x             We use zf  x  instead of f  x  to show that the random variables are the indicators of the DP mixture      For example  each zRefPub  c  in line   in Fig    is independently drawn from the distribution of publications Pub   We can view line   as constructing a portion of the hierarchical model  as shown in Fig     The number of publications n Pub  is set to the number of different values assigned to zRefPub  c   NP BLOG allows for the definition of a symbol f that corresponds to a multinomial distribution over      so its range is MMultinomial        It exhibits the default prior f  x   Dirichlet f            analogous to       f is a user defined scalar  We define nf  x  to be the true number of objects associated with collection f  x   This is useful for modeling collections of objects such as the authors of a publication  Applying rules          to the statements in Fig    involving publication objects  we arrive at the generative process Pub  Stick Pub   Title  p   TitleDist   for p              n Pub  PubAuthorsDist  p   Dirichlet PubAuthorsDist Author     Most of the corresponding graphical model is shown in Fig     Only the PubAuthorsDist  p s are missing  and they are shown in Fig     The true number of authors nPubAuthorsDist  p  in publication p comes from the support of all random variables that refer to it  and n Pub  is determined by nPubAuthorsDist   While this paper focuses on the Dirichlet process  our framework allows for other classes of nonparametric distributions  One example can be found in the aircraft tracking domain from Sec       in which the generation of aircraft transition tables might be specified with the statement StateTransDist a   StateTransPrior    In both cases      and       one can override the defaults by including appropriate dependency statements for f   in   Num  citations Num  papers Phrase matching RPM MCMC CRF Seg  N      NP BLOG  Figure    The white nodes are the portion of the graphical model generated in lines   and   of Fig     See Fig    for an explanation of the darkened nodes  which case we get f  x   g   following rule      For example  lines   and   in Fig    specify the generative process for the author mention objects  zRefAuthor  u   PubAuthorsDist  p  CitedName  u   NameStrDist  Name  a     s t  p   zRefPub  c   c   CitedIn  u   a   zRefAuthor  u   Fig    shows the equivalent graphical model  The generative process       is a stick breaking construction over the unknown objects and their attributes  When the objects x range over the set of natural numbers        is equivalent to the Dirichlet process G  DP     H        H K          P where G   x    x  f   x         fK  x    and H k is the base measure over the assignments to fk   defined by gk conditioned on the terms tk             tk Mk  Nk   Since BLOG is a typed  free language  we need to allow for the null assignment to f  x  when it is implicitly drawn from  in       We permit the clause f  x   if cond then null         which defines f  x    null  cond          cond    This statement is necessary to take care of the situation when an objects source can be of different types  as in the aircraft tracking domain with false alarms       Next  we briefly describe how to extend the rules of semantics to functions with multiple input arguments  Lets consider the case of two inputs with an additional logical variable y      Handling an additional input argument associated with known  guaranteed  objects is easy  We just duplicate       for every instance of y in the guaranteed type extension  This is equivalent to adding a finite series of plates in the graphical model  Otherwise  we assume the unknown objects are drawn independently  That is            Multiple unknown objects as input does cause some superficial complications with the interpretation of       as a DP  principally because we need to define new notation for products of measures over different types   Face Reinforce  Reason  Constraint                                                                                                                  Table    Citation matching results for the Phrase Matching      RPM       CRF Seg      and NP BLOG models  Performance is measured by counting the number of publication clusters that are recovered perfectly  The NP BLOG column reports an average over      samples  The DP determines an implicit distribution of unknown  infinitely exchangeable objects according to their properties  That is  the DP distinguishes unknown objects solely by their attributes  However  this is not always desirable  for instance  despite being unable to differentiate the individual pieces  we know a chess board always has eight black pawns  This is precisely why we retain the original number statement syntax of BLOG which allows the user to specify a prior over the number of unknown objects  independent of their properties  In the future  we would like to experiment with priors that straddle these two extremes  This could possibly be accomplished by setting a prior on the Dirichlet concentration parameter    By tracing the rules of semantics  one should see that only thing the citation matching model does not generate is values for CitedIn u   Therefore  they must be observed  We can also provide observations from any number of object attributes  such as CitedTitle c  and CitedName u   which would result in unsupervised learning  By modifying the set of evidence  one can also achieve supervised or semisupervised learning  Moreover  the language can capture both generative and discriminative models  depending whether or not the observations are generated  To summarize  the rules given by            combined with the number statement       construct a distribution p   z  n    such that the set of auxiliary variables is             z  is in one to one correspondence with the interpretations of the function symbols  the n are the sizes of the      and an assignment to    z  n  completely determines the possible world     The rules of semantics assemble models that are arbitrary hierarchies of DPs      Experiment  The purpose of this experiment is to show that the NPBLOG language we have described realizes probabilistic inference on a real world problem  We simulate the citation matching model in Fig    on the CiteSeer data set      which consists of manually segmented citations from four research areas in AI  We use Markov Chain Monte Carlo  MCMC  to simulate possible worlds from the model posterior given evidence in the form of cited authors and titles  Sec    briefly describes   There is much future work on this topic  An important direction is the development of efficient  flexible and on line inference methods for hierarchies of Dirichlet processes   Figure    Estimated  solid blue  and true  dashed red line  number of publications for the Face and Reasoning data   Acknowledgements This paper wouldnt have happened without the help of Brian Milch  Special thanks to Gareth Peters and Mike Klaas for their assistance  and to the reviewers for their time and effort in providing us with constructive comments   
 First order probabilistic models combine representational power of first order logic with graphical models  There is an ongoing effort to design lifted inference algorithms for first order probabilistic models  We analyze lifted inference from the perspective of constraint processing and  through this viewpoint  we analyze and compare existing approaches and expose their advantages and limitations  Our theoretical results show that the wrong choice of constraint processing method can lead to exponential increase in computational complexity  Our empirical tests confirm the importance of constraint processing in lifted inference  This is the first theoretical and empirical study of constraint processing in lifted inference      INTRODUCTION  Representations that mix graphical models and first order logiccalled either first order or relational probabilistic modelswere proposed nearly twenty years ago  Breese        Horsch and Poole        and many more have since emerged  De Raedt et al         Getoor and Taskar         In these models  random variables are parameterized by individuals belonging to a population  Even for very simple first order models  inference at the propositional levelthat is  inference that explicitly considers every individualis intractable  The idea behind lifted inference is to carry out as much inference as possible without propositionalizing  An exact lifted inference procedure for first order probabilistic directed models was originally proposed by Poole         It was later extended to a broader range of problems by de Salvo Braz et al          Further work by Milch et al         expanded the scope of lifted inference and resulted in the C FOVE algorithm  which is currently the state of the art in exact lifted inference  First order models typically contain constraints on the pa   rameters  logical variables typed with populations   Constraints are important for capturing knowledge regarding particular individuals  In Poole         each constraint is processed only when necessary to continue probabilistic inference  We call this approach splitting as needed  Conversely  in de Salvo Braz et al         all constraints are processed at the start of the inference  this procedure is called shattering   and at every point at which a new constraint arises  Both approaches need to use constraint processing to count the number of solutions to constraint satisfaction problems that arise during the probabilistic inference  Milch et al         adopt the shattering procedure  and avoid the need to use a constraint solver by requiring that the constraints be written in normal form  The impact of constraint processing on computational efficiency of lifted inference has been largely overlooked  In this paper we address this issue and compare the approaches to constraint processing listed above  both theoretically and empirically  We show that  in the worst case  shattering may have exponentially worse space and time complexity  in the number of parameters  than splitting as needed  Moreover  writing the constraints in normal form can lead to computational costs with a complexity that is even worse than exponential  Experiments confirm our theoretical results and stress the importance of informed constraint processing in lifted inference  We introduce key concepts and notation in Section   and give an overview of constraint processing during lifted inference in Section    In Section   we discuss how a specialized  CSP solver can be used during lifted inference  Theoretical results are presented in Section    Section   contains results of experiments      PRELIMINARIES  In this section we introduce a definition of parameterized random variables  which are essential components of first order probabilistic models  We also define parfactors  Poole         which are data structures used during lifted inference             KISYNSKI   POOLE  UAI      g x    PARAMETERIZED RANDOM VARIABLES  If S is a set  we denote by  S  the size of the set S   g A   A population is a set of individuals  A population corresponds to a domain in logic  A parameter corresponds to a logical variable and is typed with a population  Given parameter X   we denote its population by D X   Given a set of constraints C   we denote a set of individuals from D X  that satisfy constraints in C by D X    C   A substitution is of the form  X   t            Xk  tk    where the Xi are distinct parameters  and each term ti is a parameter typed with a population or a constant denoting an individual from a population  A ground substitution is a substitution  where each ti is a constant  A parameterized random variable is of the form f  t           tk    where f is a functor  either a function symbol or a predicate symbol  and ti are terms  Each functor has a set of values called the range of the functor  We denote the range of the functor f by range  f    A parameterized random variable f  t           tk   represents a set of random variables  one for each possible ground substitution to all of its parameters  The range of the functor of the parameterized random variable is the domain of random variables represented by the parameterized random variable  Let v denote an assignment of values to random variables  v is a function that takes a random variable and returns its value  We extend v to also work on parameterized random variables  where we assume that free parameters are universally quantified  Example    Let A and B be parameters typed with a population D A   D B    x            xn    Let h be a functor with range  true  f alse   Then h A  B  is a parameterized random variable  It represents a set of n  random variables with domains  true  f alse   one for each ground substitution  A x    B x      A x    B x             A xn   B xn    A parameterized random variable h x    B  represents a set of n random variables with domains  true  f alse   one for each ground substitution  B x             B xn    Let v be an assignment of values to random variables  If v h x    B   equals true  each of the random variables represented by h x    B   namely h x    x             h x    xn    is assigned the value true by v       PARAMETRIC FACTORS  A factor on a set of random variables represents a function that  given an assignment of a value to each random variable from the set  returns a real number  Factors are used in the variable elimination algorithm  Zhang and Poole        to store initial conditional probabilities and intermediate results of computation during probabilistic inference in graphical models  Operations on factors include mul   g xn   h x   x    h A  B   h x   xn   B A  FIRST ORDER  h xn   x    h xn   xn   PROPOSITIONAL  Figure    A parameterized belief network and its equivalent belief network  tiplication of factors and summing out random variables from a factor  Let v be an assignment of values to random variables and let F be a factor on a set of random variables S  We extend v to factors and denote by v F  the value of the factor F given v  If v does not assign values to all of the variables in S  v F  denotes a factor on other variables  A parametric factor or parfactor is a triple hC   V  Fi where C is a set of inequality constraints on parameters  V is a set of parameterized random variables and F is a factor from the Cartesian product of ranges of parameterized random variables in V to the reals  A parfactor hC   V  Fi represents a set of factors  one for each ground substitution G to all free parameters in V that satisfies the constraints in C   Each such factor FG is a factor on the set of random variables obtained by applying a substitution G  Given an assignment v to random variables represented by V  v FG     v F   Parfactors are used to represent conditional probability distributions in directed first order models and potentials in undirected first order models as well as intermediate computation results during inference in first order models  In the next example  which extends Example    we use parameterized belief networks  PBNs   Poole        to illustrate representational power of parfactors  The PBNs are a simple first order directed probabilistic model  we could have used parameterized Markov networks instead  as did de Salvo Braz et al         and Milch et al           Our discussion of constraint processing in lifted inference is not limited to PBNs  it applies to any model for which the joint distribution can be expressed as a product of parfactors  Example    A PBN consists of a directed acyclic graph where the nodes are parameterized random variables  an assignment of a range to each functor  an assignment of a population to each parameter  and a probability distribution for each node given its parents  Consider the PBN graph presented in Figure   using plate notation  Buntine    UAI       KISYNSKI   POOLE         Let g be a functor with range  true  f alse   Assume we do not have any specific knowledge about instances of g A   but we have some specific knowledge about h A  B  for case where A   x  and for case where A    x  and A   B  The probability P g A   can be represented with a parfactor h      g A    Fg i  where Fg is a factor from range h  to the reals  The conditional probability P h A  B  g A   can be represented with a parfactor h      g x     h x    B    F  i  a parfactor h A    x      g A   h A  A    F  i  and a parfactor h A    x    A    B    g A   h A  B    F  i  where F    F    and F  are factors from range g   range h  to the reals  Let C be a set of inequality constraints on parameters and X be a parameter  We denote by EXC the excluded set for X  that is  the set of terms t such that  X    t   C   A parfactor hC   V  FF i is in normal form  Milch et al         if for each inequality  X    Y    C   where X and Y are parameters  we have EXC   Y     EYC   X   In a normal form parfactor  for all parameters X of parameterized random variables in V    D X   C       D X     EXC    Example    Consider the parfactor h A    x    A    B   g A   h A  B    F  i from Example    Let C denote a set of constraints from this parfactor  The set C contains only one inequality between parameters  namely A    B  We have EAC    x    B  and EBC    A   As EAC   B     EBC   A   the parfactor is not in normal form  Recall that D A   D B    x            xn    The size of the set D A    C depends on the parameter B  It is equal n    when B   x  and n    when B    x    Other parfactors from Example   are in normal form as they do not contain constraints between parameters  Consider a parfactor h X    Y  X    a Y    a    e X   f  X Y     Fe f i  where D X    D Y   and   D X      n  Let C   denote a set     of constraints from this parfactor  As EXC   Y     EYC   X   the parfactor is in normal form and   D X    C       n    and   D Y     C       n         LIFTED INFERENCE AND CONSTRAINT PROCESSING  In this section we give an overview of exact lifted probabilistic inference developed in  Poole          de Salvo Braz et al          and  Milch et al         in context of constraints  For more details on other aspects of lifted inference we refer the reader to the above papers  Let  be a set of parfactors  Let J    denote a factor equal to the product of all factors represented by elements of   Let U be the set of all random variables represented by parameterized random variables present in parfactors in   Let Q be a subset of U  The marginal of J    on Q  denoted JQ     is defined as JQ      U Q J     Given  and Q  the lifted inference procedure computes the marginal JQ    by summing out random variables from       Q  where possible in a lifted manner  Evidence can be handled by adding to  additional parfactors on observed random variables  Before a  ground  random variable can be summed out  a number of conditions must be satisfied  One is that a random variable can be summed out from a parfactor in  only if there are no other parfactors in  involving this random variable  To satisfy this condition  the inference procedure may need to multiply parfactors prior to summing out  Multiplication has a condition of its own  two parfactors hC    V    F  i and hC    V    F  i can be multiplied only if for each parameterized random variable from V  and for each parameterized random variable from V    the sets of random variables represented by these two parameterized random variables in respective parfactors are identical or disjoint  This condition is trivially satisfied for parameterized random variables with different functors  Example    Consider the PBN from Figure   and set  containing parfactors introduced in Example    Assume that we want to compute the marginal of J    on instances of h A  B   where A    x  and A    B  We need to sum out random variables represented by g A  from parfactor h A    x    A    B    g A   h A  B    F  i  but as they are also among random variables represented by g A  in parfactor h     g A   Fg i  we have to first multiply these two parfactors  Sets of random variables represented by g A  in these two parfactors are not disjoint and are not identical and the precondition for multiplication is not satisfied       SPLITTING  The precondition for parfactor multiplication may be satisfied through splitting parfactors on substitutions  Let  be a set of parfactors  Let p f   hC   V  FF i    Let  X t  be a substitution such that  X    t     C and term t is a constant such that t  D X   or a parameter such that D t    D X   A split of p f on  X t  results in two parfactors  p f  X t  that is a parfactor p f with all occurrences of X replaced by term t  and a parfactor p fr   hC  X    t   V  FF i  We have J      J      p f    p f  X t   p fr     We call p fr a residual parfactor  Given two parfactors that need to be multiplied  substitutions on which splitting is performed are determined by analyzing constraint sets C and sets of parameterized random variables V in these parfactors  Example    Let us continue Example    A split of h      g A    Fg i on  A x    results in h      g x      Fg i and residual h A    x      g A    Fg i  The first parfactor can be ignored because it is not relevant to the query  while the residual needs to be multiplied by a parfactor h A    x    A    B    g A   h A  B    F  i  The precondition for multiplication is now satisfied as g A  represents the same set of random variables in both parfactors             KISYNSKI   POOLE MULTIPLICATION  Once the precondition for parfactor multiplication is satisfied  multiplication can be performed in a lifted manner  This means that  although parfactors participating in a multiplication as well as their product represent multiple factors  the computational cost of parfactor multiplication is limited to the cost of multiplying two factors  The only additional requirement is that the lifted inference procedure needs to know how many factors each parfactor involved in the multiplication represents and how many factors their product will represent  These numbers can be different because the product parfactor might involve more parameters than a parfactor participating in the multiplication  In such a case  a correction to values of a factor inside appropriate parfactors participating in multiplication is necessary  Detailed description of this correction is beyond the scope of this paper  For more information see Example   below and a discussion of the fusion operation in de Salvo Braz et al          For our purpose  the key point is that the lifted inference procedure needs to compute the number of factors represented by a parfactor  Given a parfactor hC   V  Fi  the number of factors it represents is equal to the number of solutions to the constraint satisfaction problem formed by constraints in C   This counting problem is written as  CSP  Dechter         If a parfactor is in normal form  each connected component of the underlying constraint graph is fully connected  see Proposition    and it is easy to compute the number of factors represented by the considered parfactor  If a parfactor is not in normal form  a  CSP solver is necessary to compute the number of factors the parfactor represents  Example    In Example   the parfactor h A    x      g A    Fg i represents n    factors  and needs to be multiplied by a parfactor h A    x    A    B    g A   h A  B    F  i  which represents  n      factors  Their product p f   hA    x    A    B   g A   h A  B    F i  where F is a factor from range g   range h  to the reals  represents  n      factors  Let v be an assignment of values to g A  and  n    n     h A  B   We have v F     v Fg   v F     Now we can sum out g A  from p f   The result needs to be represented with a counting formula  Milch et al          which is outside of the scope of this paper  What is important for the paper is that a parfactor involving counting formulas needs to be in normal form  Since p f is not in normal form  we first split it on substitution  B x    and then sum out g A  from the two parfactors obtained through splitting       SUMMING OUT  During lifted summing out  a parameterized random variable is summed out from a parfactor hC   V  FF i  which means that a random variable is eliminated from each factor represented by the parfactor in one inference step  Lifted  UAI       inference will perform summing out only once on the factor F  If some parameters only appear in the parameterized variable that is being eliminated  the resulting parfactor will represent fewer factors than the original one  As in the case of parfactor multiplication  the inference procedure needs to compensate for this difference  It needs to compute the size of the set X    D X         D Xk      C   where X            Xk are parameters that will disappear from the parfactor  This number tells us how many times fewer factors the result of summing out represents compared to the original parfactor  If the parfactor is in normal form   X   does not depend on values of parameters remaining in the parfactor after summing out  The problem reduces to  CSP and  as we mentioned in Section      it is easy to solve  If the parfactor is not in normal form   X   may depend on values of remaining parameters and a  CSP solver is necessary to compute all the sizes of the set X conditioned on values of parameters remaining in the parfactor  Example    Assume that we want to sum out f  X Y   from a parfactor h X    Y Y    a    e X   f  X Y     Fe f i  where Fe f is a factor from range e   range  f   to the reals  Let D X    D Y   and   D X      n  The parfactor represents  n      factors  Note that the parfactor is not in normal form and   D Y      X    Y Y    a   equals n    if X   a and n    if X    a  A  CSP solver could compute these numbers for us  see Example     After f  X Y   is summed out  Y is no longer among parameters of random variables and X remains the sole parameter  To represent the result of summation we need two parfactors  h     e a   Fe  i and h X    a   e X   Fe  i  where Fe  and Fe  are factors from range e  to the reals  Let y  range e   then Fe   y     zrange  f   Fe f  y  z  n  and Fe   y     zrange  f   Fe f  y  z  n         PROPOSITIONALIZATION  During inference in first order probabilistic models it may happen that none of lifted operations  including operations that are not described in this paper  can be applied  In such a situation the inference procedure substitutes appropriate parameterized random variables with random variables represented by them  This may be achieved through splitting as we demonstrate in an example below  Afterward  inference is performed  at least partially  at the propositional level  As it has a negative impact on the efficiency of inference  propositionalization is avoided as much as possible during inference in first order models  Example    Consider a parfactor h      g A    Fg i from Example    Assume we need to propositionalize g A   Recall that D A    x            xn    Propositionalization results in a set of parfactors h      g x      Fg i  h      g x      Fg i          h      g xn      Fg i  h      g xn     Fg i  Each parameterized random variable in the above parfactors represents just one   UAI              KISYNSKI   POOLE     w    w   w  W  W w          w  W      X  w              Y  X  Z          Y      X Y                     Z      Z                     w              Figure    Comparison of w and exponential functions  random variable and each parfactor represents just one factor  The set could be produced by a sequence of splits  The above informal overview of lifted inference  together with simple examples  shows that constraint processing is an integral  important part of lifted inference       CSP SOLVER AND LIFTED INFERENCE  In Section    we showed when a  CSP solver can be used during lifted probabilistic inference  A solver that enumerates all individuals from domains of parameters that form a CSP would contradict the core idea behind lifted inference  that is  performing probabilistic inference without explicitly considering every individual  In our experiments presented in Section   we used a solver  Kisynski and Poole        that addresses the above concern  It is a lifted solver based on the variable elimination algorithm for solving  CSP of Dechter        and is optimized to handle problems that contain only inequality constraints  It is not possible to describe the algorithm in detail in this paper  but below we provide some intuition behind the solver    a    b   Figure    Constraint graph with a cycle  a  and the two cases that need to be analyzed  b   us to immediately solve the corresponding  CSP  we can assign the value to W in n ways  and are left with n    possible values for X  and n    possible values for Y and Z  Hence  there are n n      solutions to this CSP instance  Consider a set of constraints  W    X W    Y  X    Z Y    Z   where all parameters have the same population of size n  The underlying graph has a cycle  see Figure    a    which makes the corresponding  CSP more difficult to solve than in the previous example  We can assign the value to W in n ways  and are left with n    possible values for X and Y   For Z we need to consider two cases  X   Y and X    Y  see Figure    b    In the X   Y case  Z can take n    values  while in the X    Y case  Z can have n    different values  Hence  the number of solutions to this CSP instance is n n        n n     n        The first case corresponds to a partition   X Y    of the set of parameters  X Y    while the second case corresponds to a partition   X    Y    of this set   First  we need to introduce a concept of a set partition  A partition of a set S is a collection  B            Bw   S of nonempty  pairwise disjoint subsets of S such that S   wi   Bi   The sets Bi are called blocks of the partition  Set partitions are intimately connected to equality  For any consistent set of equality assertions on parameters  there is a partition in which the parameters that are equal are in the same block  and the parameters that are not equal are in different blocks  If we consider a semantic mapping from parameters to individuals in the world  the inverse of this mapping  where two parameters that map to the same individual are in the same block  forms a partition of the parameters  The number of partitions of the set of size w is equal to the w th Bell number w   Bell numbers grow faster than any exponential function  see Lovasz          but for small ws they stay much smaller than exponential functions with a moderate base  see Figure      In general  to perform this kind of reasoning  we need to triangulate the constraint graph  This can be naturally achieved with a variable elimination algorithm  Each new edge adds two cases  one in which the edge corresponds to the equality constraint and one in which it corresponds to the inequality constraint  Some cases are inconsistent and can be ignored  When we have to analyze a fully connected subgraph of w new edges  we need to consider w cases  This is because each such case corresponds to a partition of the parameters from the subgraph  those parameters in the same block of the partition are equal  and parameters in different blocks are not equal  The number of such partitions is equal to w   The lifted  CSP solver analyzes w partitions of parameters  rather than nw ground substitutions of individuals  Since we do not care about empty partitions  we will never have to consider more partitions than there are ground substitutions  As w corresponds to the induced width of a constraint graph  which we do not expect to be large  and n corresponds to the population size  potentially large   the difference between w and nw can be very big  see Figure      Consider a set of constraints  W    X  X    Y  X    Z   where all parameters have the same population of size n  The underlying constraint graph has a tree structure  which allows  In practice  parameters can be typed with different populations  from the very beginning as well as because of unary constraints   In such a situation  we can apply the above        KISYNSKI   POOLE  UAI       reasoning to any set of individuals that are indistinguishable as far as counting is concerned  For example  the intersection of all populations is a set of individuals for which we only need the size  there is no point in reasoning about each individual separately  Similarly  the elements from the population of a parameter that do not belong to the population of any other parameter can be grouped and treated together  In general  any individuals that are in the populations of the same group of parameters can be treated identically  all we need is to know how many there are   two parfactors are about to be multiplied and the precondition for multiplication is not satisfied   Example    In Example   we need to know the number   D Y      X    Y Y    a    where D X    D Y   and   D X      n  Let a  denote set  a  and a  denote set D X    a   The following factor has value   for substitutions to parameters X Y that are solutions to the above CSP and   otherwise   Shattering simplifies design and implementation of lifted inference procedures  in particular  construction of elimination ordering heuristics  Unfortunately  as we show in Theorem    it may lead to creation of large number of parfactors that would not be created by following the splitting as needed approach   Y a  a  a   X a  a  a   Partition s    X     Y      X Y      X    Y              After we eliminate Y from the above factor we obtain  X a  a   Partition s    X     X    n    n   Numbers n    and n    are obtained through analysis of partitions of X and Y present in the original factor and knowledge that a  represents   individual and a  represents n    individuals  From the second factor we can infer that   D Y      X    Y Y    a   equals n    if X   a and n    if X    a  If we assume that all populations of parameters forming a  CSP are sorted according to the same  arbitrary  ordering  sets of indistinguishable individuals can be generated through a single sweep of the populations  Each such set can be represented by listing all of its elements or by listing all elements from the corresponding population that do not belong to it  For each set we choose a more compact representation   An alternative  called shattering  was proposed by de Salvo Braz et al          They perform splitting at the beginning of the inference by doing all the splits that are required to ensure that for any two parameterized random variables present in considered parfactors the sets of random variables represented by them are either identical or disjoint   Shattering was also used in Milch et al           Theorem    Let  be a set of parfactors  Let Q be a subset of the set of all random variables represented by parameterized random variables present in parfactors in   Assume we want to compute the marginal JQ     Then   i  if neither of the algorithms performs propositionalization  then every split on substitution  X t   where t is a constant  performed by lifted inference with splitting as needed is also performed by lifted inference with shattering  subject to a renaming of parameters    ii  lifted inference with shattering might create exponentially more  in the maximum number of parameters in a parfactor  parfactors than lifted inference with splitting as needed  Proof  We present a sketch of a proof of the first statement and a constructive proof of the second statement   i  Assume that lifted inference with splitting as needed performs a split  We can track back the cause of this to the initial set of parfactors   Further analysis shows that shattering the set  would also involve this split   ii  Consider the following set of parfactors      h      gQ     g   X    X            Xk     The answer from the solver needs to be translated to sets of substitutions and constraints accompanying each computed value  Standard combinatorial enumeration algorithms can do this task      THEORETICAL RESULTS  g   X    X            Xk         gk  Xk      F  i        h      g   a  X            Xk     F  i        h      g   a  X            Xk     F  i             In this section we discuss consequences of different approaches to constraint processing in lifted inference       SPLITTING AS NEEDED VS  SHATTERING  Poole        proposed a scheme in which splitting is performed as needed through the process of inference when  h      gk   a  Xk     Fk  i    k      h      gk  a    Fk i    k   and let Q be gQ       Shattering might also be necessary in the middle of inference if propositionalization has been performed    UAI       KISYNSKI   POOLE  For i              k  a set of random variables represented by a parameterized random variable gi  Xi           Xk   in a parfactor     is a proper superset of a set of random variables represented by a parameterized random variable gi  a  Xi             Xk   in a parfactor  i   Therefore lifted inference with shattering needs to perform several splits  Since the order of splits during shattering does not matter here  assume that the first operation is a split of the parfactor     on a substitution  X   a  which creates a parfactor h      gQ     g   a  X            Xk    g   X    X            Xk            gk  Xk      F  i   k       and a residual parfactor h X     a    gQ     g   X    X            Xk    g   X    X            Xk            gk  Xk      F  i    k       In both newly created parfactors  for i              k  a set of random variables represented by a parameterized random variable gi  Xi           Xk   is a proper superset of a set of random variables represented by a parameterized random variable gi  a  Xi             Xk   in a parfactor  i  and shattering proceeds with further splits of both parfactors  Assume that in next step parfactors  k      and  k      are split on a substitution  X   a   The splits result in four new parfactors  The result of the split of the parfactor  k      on  X   a  contains a parameterized random variable g   a  a          Xk   and a parfactor     needs to be split on a substitution  X   a   The shattering process continues following a scheme described above  It terminates after  k    k    splits and results in  k      parfactors  each original parfactor  i   i              k  is shattered into  ki parfactors   Assume that lifted inference proceeds with an elimination ordering g            gk  this elimination ordering does not introduce counting formulas  other do   To compute the marginal JgQ         k lifted multiplications and  k      lifted summations are performed  Consider lifted inference with splitting as needed  Assume it follows an elimination ordering g            gk   A set of random variables represented by a parameterized random variable g   X            Xk   in a parfactor     is a proper superset of a set of random variables represented by a parameterized random variable g   a  X            Xk   in a parfactor     and the parfactor     is split on a substitution  X   a   The split results in parfactors identical to the parfactors  k      and  k      from the description of shattering above  The parfactor  k      is multiplied by the parfactor     and all instances of g   a  X            Xk   are summed out from their product while all instances of g   X    X            Xk    subject to a constraint X     a  are summed out from the parfactor  k       The summations create two parfactors  h      gQ     g   X    X            Xk            gk  Xk      FFk   i    k       h      gQ     g   X    X            Xk            gk  Xk      FFk   i     k            Instances of g  are eliminated next  Parfactors  k      and  k      are split on a substitution  X   a   the results of the splits and a parfactor     are multiplied together and the residual parfactors are multiplied together  Then  all instances of g   a  X            Xk   are summed out from the first product product while all instances of g   X            Xk    subject to a constraint X     a  are summed out from the second product  The elimination of g            gk looks the same as for g    In total   k    splits   k    lifted multiplications and  k lifted summations are performed  At any moment  the maximum number of parfactors is k      The above theorem shows that shattering approach is never better and sometimes worse than splitting as needed  It is worth pointing out that splitting as needed approach complicates the design of an elimination ordering heuristic       NORMAL FORM PARFACTORS VS   CSP SOLVER  Normal form parfactors were introduced by Milch et al         in the context of counting formulas  Counting formulas are parameterized random variables that let us compactly represent a special form of probabilistic dependencies between instances of a parameterized random variable  Milch et al         require all parfactors to be in normal form to eliminate the need to use a separate constraint solver to solve  CSP  The requirement is enforced by splitting parfactors that are not in normal form on appropriate substitutions  While parfactors that involve counting formulas must be in normal form  that is not necessary for parfactors without counting formulas  It might actually be quite expensive as we show in this section  Proposition    Let hC   V  Fi be a parfactor in normal form  Then each connected component of the constraint graph corresponding to C is fully connected  Proof  Proposition   is trivially true for components with one or two parameters  Let us consider a connected component with more than two parameters  Suppose  contrary to our claim  that there are two parameters X and Y with no edge between them  Since the component is connected  there exists a path X  Z    Z       Zm  Y   As C is in normal form  EZCi   Zi       EZCi     Zi    i              m    and EZCm   Y     EYC   Zm    We have X  EZC    and consequently X  EYC   This contradicts our assumption that there is no edge between X and Y   While the above property simplifies solving  CSP for a set of constraints from a parfactor in normal form it also has negative consequences  If a parfactor is not in normal form  conversion to normal from might require several splits  For example we need three splits to convert a parfactor with the set of constraints shown in Figure    a  to a set of four parfactors in normal form  The resulting sets of constraints        KISYNSKI   POOLE W  W W  Z    X  Y          W  Z  X Y          X      X          Y  Y                   Z  Z  Figure    Constraint graphs obtained through a conversion to normal form  are presented in Figure    If the underlying graph is sparse  conversion might be very expensive as we show in the example below  Example     Consider a parfactor h X     a  X     X            X     Xk     g   X     g   X             gn  Xk     Fi  where D X      D X           D Xk    Let C denote a set of constraints from this parfactor  We have EXC     a  X            Xk   and EXCi    X     i              k  The parfactor is not in normal form because EXC    Xi       EXCi   X     i              k  As a result the size of the set D X      C depends on other parameters in the parfactor  For instance  it differs for X    a and X     a or for X    X  and X     X    A conversion of the considered parfactor to set of parfactors in normal form involves  k    splits on substitutions of the form  Xi  a      i  k and ki   ki  i     splits on substitutions of the   form  Xi  X j       i  j  k  It creates ki   ki i parfactors in normal form  In Example    we analyze how this conversion affects parfactor multiplication compared to the use of a  CSP solver  From the above example we can clearly see that the cost of converting a parfactor to normal form can be worse than exponential  Moreover  converting parfactors to normal form may be very inefficient when analyzed in context of parfactor multiplication  see Section        or summing out a parameterized random variable from a parfactor  see Section         Our empirical tests  see Section      confirm this observation  Note that splitting as needed can be used together with a  CSP solver  Poole         or with normal form parfactors  Shattering can be used with a  CSP solver  de Salvo Braz et al         or with normal form parfactors  Milch et al          The cost of converting parfactors to normal form might be amplified if it is combined with shattering         Multiplication  In the example below we demonstrate how the normal form requirement might lead to a lot of  otherwise unnecessary  parfactor multiplications  Example     Assume we would like to multiply the parfactor from Example    by a parfactor p f   h      g   X      F  i  First  let us consider how it is done with a  CSP solver  A  CSP solver computes the num   UAI       ber of factors the parfactor from Example    represents     D X         k     Next the solver computes the number of factors represented by the parfactor p f   which is trivially   D X       A correction is applied to values of the factor F  to compensate for the difference between these two numbers  Finally the two parfactors are multiplied  The whole operation involved two calls to a  CSP solver  one correction and one parfactor multiplication  Now  let us see how it can be done without the use of  CSP solver    The first parfactor is converted to a set  of ki   ki i parfactors in normal form  as presented in Example     Some of the parfactors in  contain a parameterized random variable g   a   the rest contains a parameterized random variable g   X  and a constraint X     a  so the parfactor p f needs to be split on a substitution  X   a   The split results in a parfactor h      g   a    F  i and a residual h X     a    g   X      F  i  Next  each parfactor from  is multiplied by  either the result of the split or the residual  Thus ki   ki i parfactor multiplications need to be performed and most of these multiplication require a correction prior to the actual parfactor multiplication  There is an opportunity for some optimization  as factor components of parfactors multiplications for different corrections could be cached and reused instead of being recomputed  Still  even with such a caching mechanism  multiple parfactor multiplications would be performed compared to just one multiplication when a  CSP solver is used         Summing Out  Examples   and   demonstrate how summing out a parameterized variable from a parfactor that is not in normal form can be done with a help of a  CSP solver  In the example below we show how this operation would look if we convert the parfactor to a set of parfactors in normal form which does not require a  CSP solver  Example     Assume that we want to sum out f  X Y   from the parfactor h X    Y Y    a    e X   f  X Y     Fe f i from the Example    First  we convert it to a set of parfactors in normal form by splitting on a substitutions  X a   We obtain two parfactors in normal form  h Y    a    e a   f  a Y     Fe f i  which represents n  factors  and h X    Y  X    a Y    a    e X   f  X Y     Fe f i  which represents  n     n     factors  Next we sum out f  a Y   from the first parfactor and f  X Y   from the second parfactor  In both cases a correction will be necessary  as Y will no longer among parameters of random variables and the resulting parfactors will represent fewer factors than the original parfactors  In general  as illustrated by Examples      and     conversion to normal form and  CSP solver create the same number of parfactors  The difference is  that the first method  computes a factor component for the resulting parfactors once and then applies a different correction for each result    UAI       KISYNSKI   POOLE  ing parfactor based on the answer from the  CSP solver  The second method computes the factor component multiple times  once for each resulting parfactor  but does not use a  CSP solver  As these factor components  before applying a correction  are identical  redundant computations could be eliminated by caching  We successfully adopted a caching mechanism in our empirical test  Section       but expect it to be less effective for larger problems                                      As in the case of splitting as needed  it might be difficult to design an efficient elimination ordering heuristic that would work with a  CSP solver  This is because we do not known in advance how many parfactors will be obtained as a result of summing out  We need to run a  CSP solver to obtain this information                      EXPERIMENTS  We used Java implementations of tested lifted inference methods  Tests were performed on an Intel Core   Duo     GHz processor with  GB of memory made available to the JVM       SPLITTING AS NEEDED VS  SHATTERING  In the first experiment we checked to what extent the overhead of the shattering approach can be minimized by using intensional representations and immutable objects that are shared whenever possible  We ran tests on the following set of parfactors       h      gQ     g   a    F  i        h X    a    gQ     g   X    F  i        h      g   X   g   X    F  i        h      g   X   g   X    F  i             h      gk   X   gk  X    Fk i    k   h      gk  X    Fk   i    k         All functors had the range size    and we set Q to the instance of gQ     We computed the marginal JQ     Lifted inference with shattering first performed total of k splits  then proceeded with  k     multiplications and  k summations regardless of the elimination ordering  Lifted inference with splitting as needed performed   split  k     multiplications and k     summations  for the experiment we used the best elimination ordering  that is gk   gk            g     Figure   shows the results of the experiment where we varied k from   to      Even though lifted inference with shattering used virtually the same amount of memory as lifted inference with splitting  it was slower because it performed more arithmetic operations                        Figure    Speedup of splitting as needed over shattering               CONVNFMSUM NFMSUM  CSPSUM                                                           Figure    Summing out with and without a  CSP solver        NORMAL FORM PARFACTORS VS   CSP SOLVER  For experiment in this section we randomly generated sets of parfactors  There were up to   parameterized random variables in each parfactor with range sizes varying from   to     Constraints sets contained very few  and very often zero  constraints and formed sparse CSPs  Most of parfactors were in normal form  which allowed us to account for  CSP solver overhead  There were up to    parameters present in each parfactor  Parameters were typed with the same population  We varied the size of this population from   to      to verify how well  CSP solver scaled for larger populations  In this experiment we summed out a parameterized random variable from a parfactor  We compared summing out with a help of a  CSP solver   CSP SUM  to summing out achieved by converting a parfactor to a set of parfactors in normal form and summing out a parameterized random variable from each obtained parfactor without a  CSP solver   We cached factor components as suggested in Section         For each population size we generated     parfactors and reported a cumulative time  For the second approach  we reported time including  CONV NFMSUM  and excluding  NFM SUM  conversion to normal form  Results presented on Figure   show significant cost of conversion to normal form and advantage of  CSP solver for larger population sizes            KISYNSKI   POOLE  CONCLUSIONS AND FUTURE WORK  In this paper we analyzed the impact of constraint processing on the efficiency of lifted inference  and explained why we cannot ignore its role in lifted inference  We showed that a choice of constraint processing strategy has big impact on efficiency of lifted inference  In particular  we discovered that shattering  de Salvo Braz et al         is never betterand sometimes worsethan splitting as needed  Poole         and that the conversion of parfactors to normal form  Milch et al         is an expensive alternative to using a specialized  CSP solver  Although in this paper we focused on exact lifted inference  our results are applicable to approximate lifted inference  For example  see the recent work of  Singla and Domingos        that uses shattering  It is difficult to design an elimination ordering heuristic that works well with the splitting as needed approach and a  CSP solver  We plan to address this problem in our future research  Acknowledgments The authors wish to thank Brian Milch for discussing the CFOVE algorithm with us  Peter Carbonetto  Michael Chiang and Mark Crowley provided many helpful suggestions during the preparation of the paper  This work was supported by NSERC grant to David Poole  
 We introduce a challenging real world planning problem where actions must be taken at each location in a spatial area at each point in time  We use forestry planning as the motivating application  In Large Scale Spatial Temporal  LSST  planning problems  the state and action spaces are defined as the cross products of many local state and action spaces spread over a large spatial area such as a city or forest  These problems possess state uncertainty  have complex utility functions involving spatial constraints and we generally must rely on simulations rather than an explicit transition model  We define LSST problems as reinforcement learning problems and present a solution using policy gradients  We compare two different policy formulations  an explicit policy that identifies each location in space and the action to take there  and an abstract policy that defines the proportion of actions to take across all locations in space  We show that the abstract policy is more robust and achieves higher rewards with far fewer parameters than the elementary policy  This abstract policy is also a better fit to the properties that practitioners in LSST problem domains require for such methods to be widely useful      INTRODUCTION  In some real world planning problems there are many actions to be taken in parallel over a spatial area  This is the case  for example  in urban planning when zoning different areas of a city for different uses  In infectious disease control  decisions need to be made about allocating medicine to thousands or millions of people spread across space based on need  cost  transportation or any number of other variables  In forestry planning  decisions come down to whether to cut each tree or not  or to perform some other  David Poole Computer Science Department University of British Columbia cs ubc ca poole  activity at every point in the forest  We call problems of this form Large Scale Spatial Temporal  LSST  planning problems  After further motivating the problem with details from the example of forestry planning we introduce a general definition of LSST planning as a reinforcement learning  Sutton and Barto        problem and discuss the properties a solution needs to possess  We demonstrate how policy gradients  Williams        can satisfy many of these properties for LSST problems  We compare two policy formulations  an explicit policy that identifies each location in space with parameters for controlling the actions taken and an abstract policy that represents the proportion of actions that will be taken across the entire space  We show that this abstract policy produces better results with far fewer parameters and we argue that the level of abstraction it uses more closely matches the level needed by human planners in forestry or other LSST domains that would utilize this method to aid in planning      FORESTRY BACKGROUND  Forestry planning as it is practiced in British Columbia will be used as our motivating example throughout this paper  Forestry is a very important industry in British Columbia generating     of the provinces GDP and employing around         people  Large regions of forest of up to several hundred thousand hectares are licensed by the government to forestry companies to cut trees and sell lumber  The government places many constraints on management activities  setting a maximum annual allowable cut  specifying areas that are off limits  specifying spatial constraints to avoid a high density of cut areas and to protect wildlife migration routes and habitats  Violations of these constraints are enforced with large fines and possible revocation of licence  Suppose you are the head forester in charge of planning for one of these companies  Your interest is to maximize your return and minimize your fines incurred  This can be achieved by providing a steady supply of logs over the long   UAI       CROWLEY ET AL   term and maintaining a healthy forest  The forest is divided up into many small spatial regions we will call cells and you must decide for every cell whether to clear cut the whole cell  cut a portion of the trees or do nothing in that cell this year  One major impact on forest health is insect infestation such as the Mountain Pine Beetle  MPB  Eng et al          MPB are tiny beetles that burrow under the bark of pine trees laying eggs  cutting off nutrients and leaving a deadly blue fungus that kills the tree  MPB are an endemic species however  in recent decades  a lack of cold winters and the large number of older trees resulting from years of forest fire suppression have provided the MPB population with the conditions they need for an explosive epidemic  Cutting down trees before a brood spread can kill the beetles but the rice sized beetles are hard to detect until a year or two after an attack  That is when the thousands of killed trees are easily spotted by their distinctive red color  The infestation is devastating the forests of British Columbia  wiping out over     of the harvestable pine in the past    years and shows no sign of stopping at the provincial or national borders       SOLUTION CONSIDERATIONS  In a complex domain such as forestry there are many researchers who have developed sophisticated simulation models for different elements of forestry planning from tree growth to MPB growth  The explicit transition models underlying these simulations are too complex and varied to be used as conditional probabilities so we must rely on the simulations themselves as black boxes that provide a future forest state given some proposed set of actions and the current state  Generating simulations is expensive  so we need to treat all simulation data as a precious resource to be used as effectively as possible  In many LSST planning domains a distinction is made between strategic  tactical and operational planning  Operational planning refers to the immediate implementation of a low level plan  eg  which particular trees to cut in which order   Tactical planning covers mid sized regions over medium timescales of less than    years  Tactical plans take into account local conditions  eg  where to build roads to access the forest  assigning workers  scheduling cutting in different areas   Strategic planning takes place at the highest levels  focussing on properties of the entire landscape  spatial constraints and total rewards into the long term future over decades or centuries  Some strategic considerations in forestry are   maintaining the proportion of trees within an age class  balancing employment between regions  satisfying spatial constraints  reducing overall pest levels  The total number of cells in a landscape can range from              In this paper we focus on strategic planning  as this is the level where effective use of large amounts of data can have the greatest impact and it is an important area of research in Forestry planning        The result of strategic planning is a strategic policy which is concerned primarily with the proportions of actions taken across the landscape and their impact on long term value  The strategic policy does not express which actions to take in particular cells in the landscape  This is due to the fact that over the level of the entire landscape there are many states that the reward function does not distinguish between  Consider a reward function based purely on the number of trees cut and constrained by a maximum allowable cut  There are many ways to achieve the maximum value that involve different assignments of actions to particular cells  These distinctions are not relevant as long as a policy can be defined to achieve the maximum value  In practice  the person designing the strategic policy often does not even have the authority to specify the lowest level action choices  eg  Clear cut cell       as these choices are made by experts on the ground based on their local context in accordance with the strategic policy  We thus have two major requirements for a good LSST planning solution     the method can efficiently find a high value policy without having an explicit transition model for the simulation    we want a strategic policy that does not commit to more detail than necessary in order to maximize reward In the following sections we give a general definition of LSST planning problems and show how to use policy gradients to achieve these requirements       CURRENT SOLUTIONS IN FORESTRY  Many existing planning solutions in forestry have relied heavily on assuming spatial independence between cells in a forest  The deterministic optimization models often used  such as linear programming  break down when faced with uncertainty about the state of the forest and cannot use any information about spatial relations between cells  This is a problem in forestry  J P Kimmins et al         since MPB breaks the assumption of spatial independence and adds uncertainty to the problem  MPB can fly between nearby cells in the forest  so the immediate neighbourhood is always relevant when planning which trees to cut in order to reduce the spread of the pests and quickly salvage trees killed by them  Other solution methods commonly used in forestry planning are simulation modelling and meta heuristics  Simulation modelling is an interactive approach where the user specifies maps  constraints and preferences for various actions and the software carries out a simulation while choosing actions consistent with the constraints and preferences  Some example simulation tools are ATLAS  http   www forestry ubc ca atlas simfor  and SELES  Fall        CROWLEY ET AL   et al          The results from these simulation planners are often then fed through other tools for analysis after which the user can alter the parameters of the simulation and run it once again  These simulations will be a useful black box to be used by higher level RL planning techniques  The final set of methods in common use are stochastic local search methods such as tabu search  genetic algorithms and simulated annealing  Pukkala and Kurttila         The general approach is to predefine fixed plans that could be applied to a cell over the entire time horizon  The search proceeds to assign one of these plans to each cell  evaluate the outcome and make local improvement steps  Simulated annealing has offerred the best hope for integrating spatial relations of these methods but uncertainty is generally not dealt with in a significant way  Uncertainty is also introduced to the otherwise relatively predictable growth of trees by the fact that the exact location and severity of the MPB infestation is unknown until a year or two after an attack  Increasingly  there are efforts in forestry planning to improve modelling of uncertainty and complex  dynamic processes in the forest  such as fire or pest infestations  Baskent and Keles         This work contributes to those efforts by translating this specific domain into a general planning problem that can be approached with recent advances from the artificial intelligence community      LSST DEFINITIONS  A landscape is partitioned spatially into a set of cells C  We assume these cells are disjoint and completely cover the area of the landscape  Cell partitioning remains constant over the planning horizon T   Each cell  c  C  at each timestep t      T    has a state  s  S  Each cell state is a column vector of real numbers s f   for all cell features f  F   These features describe different aspects of the cell such as elevation  the number of trees in the cell  the number of trees per age class and the number of MPB present in the cell  It is also possible to model spatial features in each cell which take into account attributes of neighbouring cells  Such spatial features provide a simple way to include some relevant spatial information in local features of a cell  One such feature we will use in our experiments is an aggregated count of the number of MPB that will be invading the cell from all neighbouring cells   UAI       A which in this simplified forestry problem consists of  clear cut  thin trees and do nothing  Similarly to states  the landscape action is a function  a   C  A  representing the combined actions in all cells in the landscape and referred to for a particular timestep t and cell c as at  c   An LSST planning problem is defined as an MDP  S  A  r  P   where S and A are the sets of all possible landscape states and actions  r is a reward function and P is the state transition model  The transition model P  st    st   at   will generally not be available in LSST problems in any explicit form  Instead  we assume there is access to simulations of the domain developed by domain experts  These external simulators return a new state when given the current landscape state and landscape action  By making a series of calls to the simulator we can construct a trajectory  k  of states and actions across all timesteps    k    sk    ak    sk    ak             The reward function  r st   at   st     returns a real number representing the reward received for the actions taken across the landscape st   The reward may contain local cell components  eg  the value of cut trees   spatial components  eg  constraints on the number of contiguously cut cells  and even landscape wide components  eg  a penalty for the total number of MPB present   Other important constraints in forestry are the annual allowable cut  AAC   as well as upper and lower bounds on the proportion of the forest in a different age classes  eg  no more than     of the forest is less than    years old     The total discounted reward of a trajectory is R k    t  t r skt   akt   skt     with a constant discount factor           A partially observable MDP  POMDP  is defined as above except that the states are now hidden and there is an additional set  O  of observations about the states  The probability distribution P  ot    at   st     models how likely the observation is given the most recent action and the state that resulted from that action  Although LSST planning problems are partially observable in general  for this paper we focus on the fully observed problem       POLICY DEFINITION  A cell policy    is a distribution over actions for a given cell state  The probability of taking action a in a cell that is in state s is given by  s  a     The policy parameters    are an A  F matrix of real numbers used to define the policy as a Gibbs distribution of weighted state features   The landscape state  s  represents the combined state  of all of the cells in the landscape as a function   s   C  S   The landscape state at a particular timestep t and cell c is denoted st  c    e a s  s  a        e b s  Each cell has an action  a  taken from the set of cell actions  Where  a  is a vector of feature weights combined as a dot product with the cell state feature vector  s     Variables or functions refering to the entire landscape of cells will be set in bold     When it is not relevant  the trajectory k will be dropped from state and actions names        bA   UAI           CROWLEY ET AL   LANDSCAPE POLICY  A general landscape policy   s  a     can be defined in terms of  as the joint probability of choosing all of the cell actions in a given a set of cell states  s  The landscapeparameters  define parameters for each local cell policy  The landscape policy is computed as the product of the probabilities for all cells given by the appropriate cellpolicies  We will provide two parameterizations for the landscape policy   C and      shown below  The first formulation   C   defines an explicit policy by maintaining separate parameters   C   C    for each cell and each time step   C  s  a   C          s c   a c    C  c         cC  The second formulation  shown in      describes an abstract policy where a single set of parameters       is used for all cells in the landscape at that timestep       s  a                 s c   a c       As stated earlier  in LSST planning problems we will generally be given a black box simulator rather than the transition model P   However  it turns out that computing the gradient of the value function with respect to the policy parameters   V    does not require knowing V  or P  Riedmiller et al         Sutton et al                V    p k  R k dk    p k  R k dk     p k   log p k  R k dk      Policy gradient  PG  methods seek to find optimal policies by following the gradient with respect to the policy parameters of a function describing the value of the current policy  PG researchers have recently achieved significant gains in the types of reinforcement learning problems that can be solved  Kersting and Driessens        Riedmiller et al         Sutton et al         Williams         PG methods require stochastic  parameterized policies and work well when the state space is very large and the transition dynamics are not available  These properties match well with LSST planning problems so PG methods seem a promising place to start looking for solutions  Policy gradient algorithms are founded on the observation that the expected value of a stochastic policy can be computed using previously sampled trajectories by weighting the rewards actually received during each trajectory by the probability of that trajectory given the current policy  Sutton et al          Riedmiller et al                  V   E p k  R k    p k  R k dk        Where the probability of a trajectory k is    t    t  POLICY GRADIENTS  P  skt  skt    akt    skt   akt    t       t  log  skt   akt    t    t   log  skt   akt    t         If we choose to use  C as the landscape policy then the final gradient of the policy value becomes        V    log  C  skt   akt    C t  R k   K  t k           log  skt  c   akt  c    C t  c  R k   K  t c k          log  skt  c   akt  c    C t  c  R k   K  t c k       The basic policy gradient algorithm then involves two main steps  generating samples and updating the policy  First  a new sample trajectory  k  is generated using the current policy parameters    and this trajectory is used to compute  V    Then  the policy parameters are updated by following the gradient of the policy value           V        Where  is a learning rate that controls the size of the policy update steps  This learning rate is notoriously difficult to choose as it needs to scale with the magnitude of the derivative  Riedmiller et al         describe a few techniques called optimal base lining and Rprop to counteract these difficulties which we use  We describe these methods briefly here                 kK           These two formulations will be used within a general policy gradient algorithm and compared   p k     p s          log p k  R k   K   Where K is the finite set of sampled trajectories  The log trajectory likelihood   log p k    needed in     can be computed using     to be     log p k      log p s       log P  skt  skt    akt     cC          REDUCING THE VARIANCE OF  V   The varying magnitude of R k  can lead to high variance in the estimate of  V    which will impede learning  Part of        CROWLEY ET AL   UAI       the variance can be removed by subtracting a constant baseline b from each occurrence   of R k  in equations     and      This is valid since  p k  dk           Riedmiller et al          The optimal baseline for our problem  shown here for  C  is computed for each policy parameter    f   for every   A and f  F as follows        bt    f             Algorithm  LSST PG s    initialize  randomly         K    repeat maxSamples times    Sample new trajectory  s  a  R    generateTrajectory s      K   K   s  a  R     Update policy update b as in          k k  V     K  k t  R k   bt   log  st   at    t   update  using  V  as in sec            return      C k k R k  k t c f log  st  c   at  c    t  c             C k k k t c f log  st  c   at  c    t  c          Another technique used to improve policy gradient performance is called Rprop  Riedmiller et al         which replaces scaled updating using the full gradient as in eq     with an update value     which has the same direction as  V  but a magnitude that is unrelated to the gradient  The magnitude of  is similar to the magnitude of the parameters in  and is updated incrementally based on the progress of the policy search  To update the policy we compute          and then increment the value of  in the appropriate direction based on the gradient  See  Riedmiller et al         for more details       SOLVING LSST PROBLEMS USING POLICY GRADIENTS  The formulation for  V  in     requires us to know  log  s  a     Our choice of policy parameterization allows us to express this analytically  We compute the partial derivative f V  with respect to parameter    f   for every   A and f  F       e a s f log  s  a      f log    b s bA e     f log e a s  f log e b s b     b s f e   f  a s    b b s e b    b s e f  b s   f  a s  b    b s be  This partial derivative will be different depending on whether the action for this cell  a  matches the action associated with parameter being differentiated    Since all the policy parameters are independent we know that for any action a  A and feature g  F       if    a and f   g f  a  g           otherwise This allows us to simplify f log  s  a    to  s f       s       s f   s         if    a   if     a             LSST POLICY GRADIENT ALGORITHM  Combining all of these elements together we arrive at the policy gradient algorithm that iteratively generates new trajectories and updates the policy based on the current set of trajectories   generateTrajectory s      R   for t     to T do  at   rt   st       runSim st    t   R   R    t rt return  s  a  R  runSim st    t   foreach c in C do    sample action distribution at  c    st  c    C  or  st  c     t    t  c   st     externalSimulator st   at   rt   reward st   at   st     return  at   rt   st     We set the initial value of the gradient update value   to a value of     which has been found to be reasonable for many problems  Riedmiller et al          The main loop repeats until maxSamples is reached which is simply an upper bound on the number of trajectories to sample  Some other condition could easily be used such as a measure of the current convergence of the gradient  Note also that the sample and update steps are independent and could be run varying numbers of times or in parallel      TWO ALGORITHM VARIANTS  In section      we outlined two major requirements of a good LSST planning solution  dealing with the lack of an explicit transition model and defining a strategic policy that does not overcommit to too much low level detail  Policy gradients provide a way to satisfy the first requirement as they do not require a model and only follow the   UAI       CROWLEY ET AL   gradient of the policy value  One fairly obvious approach is to define a landscape policy using  C which maintains parameters for every aspect of the state space with  C t  c  defined for each and every cell  We will call this algorithm LSST PGC and it is simply the LSST PG algorithm shown above where  C fulfills the function of  in the code       ABSTRACT ACTIONS  The algorithm LSST PGC has two major problems  First  it gives us an enormous number of parameters to search over with  A  F  T  C  dimensions  As we mentioned earlier   C  could be on the order of         whereas  A    F   and T are generally less than      This enormous space makes convergence to an optimal policy very difficult  The second problem is that LSST PGC does not give us a strategic policy but instead a very low level operational policy  The optimal strategic policy should not distinguish between particular cells  The policy should treat cells interchangeably and define a pattern of actions that gives the proportion of cells each action will be applied to across the landscape  We do not want to require a commitment to particular actions for particular cells in our strategic policy  The policy  C is  in essence  too focussed on the trees to ever see the forest and find these patterns of actions  To achieve a strategic policy we instead propose to use a single  stochastic action for the entire landscape and a single set of parameters   t for each step in time  This is the policy    shown in equation      We define a second algorithm  LSST PG    where the role of  in LSST PG is fulfilled by    instead of  C   We also need to alter the sampling line in the third method runSim to at  c    st  c     t    This shift to one set of parameters seems minor  but its impact is profound  By optimizing    we will be learning how to act on abstract cells that could occur anywhere in the landscape  While the policy is still defined for each cell  it now does not distinguish between cells based on their identity  All cells are treated equally based on their state features  Recall that cell features can also take into account information from neighbouring cells such as MPB spread  One way to think about the difference between  C and    is by an analogy to time  A policy can be stationary or nonstationary with respect to time  A stationary policy defines one set of parameters for all timesteps  Similarly     is stationary with respect to space  This spatially stationary policy defines a distribution over actions based on cell features that apply to any cell in the landscape  Note that this spatially stationary policy is well defined for any number of cells and thus has arbitrary scale much as a stationary policy has arbitrary scale in terms of planning horizons  A spatially stationary policy would have many advantages during planning  allowing us to easily change scale or apply a learned policy to different subregions of the landscape without modification           EXPERIMENTS  The goal of our research is to develop a planning algorithm that can utilize existing simulations from LSST domains in a scalable way to find high value strategic policies  There are a great variety of simulators in forestry that each require extensive expertise to set up and integrate with  We decided for this stage of our research to develop our own simple forest simulator to evaluate the performance of gradient descent on this problem  Our simulator includes state features for the distribution of tree species and age classes  the level of MPB in a cell and its neighbouring cells  The dynamics include tree birth  growth and death  replanting of young trees after clearcutting  killing of trees by MPB and the spread of MPB to nearby cells year to year  We implemented the two algorithms  LSST PG  and LSSTPGC   in Matlab and ran all tests on a dual processor Pentium      GHz PC with  GB RAM running Windows XP  The initial landscape states were varied randomly around representative values for state features based on common distributions present in data for BC forests for tree species  tree age  MPB presence and other features  The reward function assigns value to individual trees cut and penalizes various properties of the landscape state  such as  a quadratic penalty on the deviation from a desired tree density for the entire landscape  linear penalties for overcutting and for the number of trees killed by MPB  and base costs for maintaining the forest  salaries  license fees  etc   to inhibit a strategy of no cutting at all  Note that the reward function is not equally well defined at all points  The Do Nothing policy  DoN othing        ClearCut        T hin        is a bad policy to follow in our model  as it is all cost and no revenue  but it is actually much worse than the reward indicates  Modellers are not willing to even assign a utility to situations where the entire industry ceases to exist or  similarly  where all of the trees are cut down and the ecosystem is totally destroyed  Rewards can be defined accurately within reasonable regions of policy space  but they must still be defined at all points to serve as a signal to be used during policy search      RESULTS  Figure   shows a typical result for the total reward received by the two algorithms  The reward is shown for each trajectory sample and is averaged over    trials for a small problem with   cells and   timesteps  Each trial sampled     trajectories and updated the policy after every   samples using all trajectories sampled up to that point  The initial policy for each trial was specified by uniform weights across all state features combined with an initial action distribution for the action components of the parameters   DoN othing        ClearCut        T hin         Ini         CROWLEY ET AL     rithm LSST PG  does not have this problem since there are fewer deterministic policies in which to get stuck and they all have very low reward  all Do Nothing  all Cut etc     Total Reward Received for Each Sample  x           Figure   shows the gradients of the combined parameters for each action in a policy for the same trial as in figure    After the LSST PGC policy converges to a narrow range of rewards the gradient begins converging  For LSST PG    the variation in the gradient drops significantly once a good policy is found       Total Reward R k       LST PGTC LST PGT                                                      Number of Sampled Trajectories k  UAI                      Figure    Total reward received average over    trials for LSST PGC and LSST PG  on   cells with   timesteps after     samples with policy updates every five samples  Initial policy was  DoN othing        ClearCut        T hin         tially all timesteps  and cells  will have the same action distribution before they begin diverging  LSST PG  consistently finds higher value policies than LSST PGC   The abstract policy of LSST PG  is more robust across multiple trials whereas LSST PGC fixes onto a deterministic set of action assignments to particular cells that is tuned to the start state for each trial  Figure   shows the initial and final policies for the two algorithms on a single trial of a    cell planning problem  The initial policy in this trial was set to  DoN othing       ClearCut        T hin        for both algorithms  The actions that are actually taken are not exactly the same even with identical initial parameters because of the different policy structures and stochastic choices being made but the final policies are very different  LSST PG  has found a policy that does even less cutting than the initial policy but that cuts more in timesteps seven and eight to achieve a higher value  The LSST PGC algorithm has found a policy with a much higher proportion of cutting  This policy is deterministic  each cell at each timestep always has the same action taken over many different trajectory samples  LSST PGC cannot break out of this policy  even though it is incurring major penalties for overcutting  because the value of a policy is based on weighting rewards by the liklihood of past trajectories under the current policy  This makes a deterministic policy that doesnt change between samples very attractive  Once a deterministic policy is found  diverging on some cell will only lower the expected value of the policy  Algo   The LSST PG  algorithm runs about three times faster than LSST PGC on the same number of trials and trajectories  This is not surprising since both algorithms sample actions for every cell and timestep while LSST PGC uses more memory and time managing the large number of parameters  The actual runtimes for LSST PG  range from   minutes for a   cell    timestep problem  up to     minutes for a    timestep     cell problem  With the current implementation we estimate that solving a problem with a couple thousand cells would take about two days  Our implementation has a lot of room for efficiency improvements but other advances will be needed to improve speeds even further  For realistic problem sizes of hundreds of thousands of cells  forestry planners currently expect runtimes of tens of minutes  for linear problems  or up to several hours  for meta heuristic solutions       RELATED AND FUTURE WORK  We have used our own forest simulator here to keep implementation simple  To improve realism it would be best to switch to a simulator in use by forestry planners  The tools used for simulation planning such as those discussed in section     could be used for this  We are working with researchers in forestry to integrate our algorithm with more of their own data and simulations to experiment at the larger scales needed for results to be useful for Forestry planning experts  Our work builds upon research on model free reinforcement learning  RL  Sutton and Barto        and policy gradient methods  Riedmiller et al          Sutton et al         described the basis for using policy gradients within an RL framework  The algorithm presented here includes some extensions to basic PG such as reward baselining and Rprop  More advanced PG techniques are available such as natural gradients  which have been shown by Riedmiller et al         to significantly improve performance by computing the reward baseline using the Fisher information matrix of the gradient  Another obvious next step is to fully model the uncertainty about the current state and use policy gradients to solve a POMDP version of the LSST problem  Also  the independence of the update policy and generate trajectory steps would make parallelization of the algorithm straightforward  Outside of policy gradient methods there is Least Square   UAI       CROWLEY ET AL        Number of Cells Assigned that are Each Action in Initial and Final Policies  Number of Cells  Initial Policy for     Initial Policy for  C  Final Policy for     Final Policy for  C                                                                          Timestep                  Timestep      Do Nothing        Clear Cut      Timestep                     Timestep      Thin Trees  Figure    In each cell the available actions are  DoN othing      ClearCut     T hinT rees      The number of cells assigned each of these actions are shown as areas  The initial and final policies after optimization for both LSST PG  and LSST PGC are shown  This trial used     sampled trajectories using     timesteps and    cells  The initial policy was set to  DoN othing       ClearCut        T hin        for both algorithms  Policy Iteration  LSPI   Lagoudakis and Parr         another RL approach that uses a parameterized policy and learns without a transition model by using a stored history of sampled trajectories  As the policy changes the value of the policy can be recomputed based on these trajectories  LSST problems also have many similarities to multi agent planning problems as each cell can be viewed as an agent while we are seeking to optimize a joint reward based on the actions of all agents  Guestrin et al         applied LSPI to multi agent problems to find an initial estimate of the value function and specify a policy in proportion to the values of each action  New research into decentralized  PO MDPs  Seuken and Zilberstein        brings together various threads in coordinated multi agent planning into one language  LSST planning problems could be a rich problem domain for this new field  However  many multi agent methods assume that the value function is a linear combination of the local value functions of individual agents  In LSST problems this assumption does not always hold as there are nonlocal constraints on actions across the landscape such as landscape cut quotas   Spatial relations between cells are a major component of LSST problems which we have addressed here with simple aggregation features from neighbouring cells  This method could be expanded with more complex relational aggregators or by adding new variables modelling relations between groups of cells and represented in the reward and policy functions  Recently  Kersting and Driessens        introduced a non parametric policy gradient approach that might be useful for LSST planning  Their method uses a gradient tree boosting approach for learning policies in relational domains  The long timescales used in LSST problems ensure that long term plans will not be followed blindly for any length of time  Thus  planning over time periods of varying lengths  such as is done in the SMDP literature  Barto and Mahadevan         could be useful  There may be enormous gains available to be made by dynamically allowing greater abstraction of policies  models and time granularity as time progresses      CONCLUSIONS  C  The explicit policy     is overly detailed and unwieldy for real world planning  The abstract policy       is at a more appropriate  strategic level of abstraction but it is merely the other extreme end of a spectrum of policies  In between are varying levels of abstraction that could be defined by using more than one set of policy parameters applied to groups of cells  These groups could be learned from clustering of features or by hierarchical decomposition of cell state space by iteratively adding new features to define groups of similar cells  Another idea along these lines to explore is using multiple weighted policies where the weights determine which policies are applied to which cells and these weights are part of the learning process   In this paper we have introduced Large Scale SpatialTemporal planning problems  which are very challenging instances of general planning where states and actions are spatially divided into components  We have described how to apply RL techniques to these problems and demonstrated one way to use policy gradient methods to find good policies in a simulated forestry planning problem  We showed that use of a spatially stationary policy formulation greatly reduces the parameter space to be searched and improves the value of the resulting policy  We hope that raising awareness about this particular set of problems will benefit both the UAI research community        CROWLEY ET AL   Total    V  for LSST PGC per action     x                              x                          Do Nothing Clear cut Thin                           Number of Policy Updates      Total    V  for LSST PG  per action           V      V      UAI           Do Nothing Clear cut Thin             a  Algorithm LSST PGC                        Number of Policy Updates               b  Algorithm LSST PG   Figure    Policy gradients for parameters relating to each action  summed across all cells and timesteps for one trial with     sampled trajectories     cells and    timesteps  and the many researchers and planners in real world planning domains with LSST structures who are looking for a way to make their very complex problems more manageable  Acknowledgements We would like to thank Matt Hoffman for his help with PG methods and feedback received from Peter Carbonetto  Michael Chiang  Albert Jiang  Jacek Kisynski and the very helpful advice from the anonymous reviewers   
