 The paper describes aHUGIN  a tool for cre ating adaptive systems  aHUGIN is an exten sion of the HUG IN shell  and is based on t he methods repo r ted by Spiegelhalter and Lau r itzen      a   The adaptive systems resu lt ing from aHUGIN are able to adj u s t the con ditional probabi lities in the modeL A short analysis of the adap tation task is gi ven and the fe atures of aHUGIN are des cribed  Fi nally a sess i on with experiments is reported and the results are discussed      Introduction  With the revival of Bayesian methods in decision sup  port systems   Shachter       Pearl        Shafer and Pearl       Andreassen ei al      b  mainly due to  the construction of e ffi cien t methods for belief re vi  sion in causal probabilistic networks  Pearl       L au ritzen and Spiegelhalter       Andersen d a          et al        Shenoy and Shafer        the process of knowledge acq uisit ion under the Bayesi an Jensen  paradigm has become increasingly important  Wh en construc ti ng causal probabilistic network models  var ious sources may be used  ranging from ignorance over experts  subjective assessments to well established sci entific theories and statistical models based on large database s   Very often a mod el is a mixtu re of contri butions from sources of different epistemological char acter   Sometimes these contributions do not coin ci de   and  the model is a mediation between them  sometimes the  ignorance has  for ex ample  forced crude  guesses  on certain distributions   sometimes the model must vary with contexts which cannot be specified beforehand  sometimes the domain is drifting ov er time   requiring the mo del to drift  r esu lt in g model is incomplete  along with i t  and sometimes the model quite s i mply does not reflect the real world pr op erly  All the proble ms listed above call for pr ocedures which enable the sy stem to mod ify the mo d el t h ro ugh ex p eri   Finn V    Tens en  ence  We call such an a ctiv ity adaptation  and sy stems  p erfo rm in g automatic adaptation tems  we  Note that  we  call adaptwe  sys  have chosen t o distinguish adaptation  which we usc to describe the activity or creat in g models by hatch pmccssing of large data  from training       D    bases  In Spiegelhalter el rd  called  leaming and they  arc  both activities are  dist inguished as sequent i a l  learning and batch learning   W h en using adaptat ion  we  He us i ng the analogy to tlw n ot i on  of adaptive reg u lato rs in control t h eo r y   llopefully  t h is abundance of te rm i n o logy will not  confuse the reader completely  The present paper descri hes all UG IN  a tool for cre ating adaptive systems  Tlw system  which is an ex tension of  IJUGIN   Andersen  ct a        J   is based  on methods reported in Spiegellta lt er  Hid Lau ntzen  l  l  lOa   see also Spiegelhaltcr and Lamit zcn      b    and th e adapt ive s y s te ms result in g from aliUGIN arc able to adjust the con d i t i on a l probabilities in mo d el    sented  the  In a l l UGIN the mo d el is compactly repre  by  a  contingency  table of i m a gi nary counts   the adaptation p roced u re is counts in this table   a  and  proccs of    odifying  the  In se ction   we give a short analysis of the adaptation discuss various simple adapt ation methods lead ing up to a dcscri ption of the one used in all U GIN   task and  In section    we  d cscri h e the f caturcs ofalllJCJN  a  d  in section    a session with txpcrillwnts the  results  ar    disciiSSt d   i rportrd  and       JU    rnd Cowell   I J J   clt  difl erent ma in difl ercnre bctii   CII t  h is sys  Spiegclhalt cr and Cc    cll  scribe  a similar systt lll a nd rt sult s of s l ig h tly  experiments   The  tem and t he i r system  is that  we allow an extra facilit y   ailed fadwy  that  makf s the systcrn forget  t h   past at  an ex p onenti a l rate  t herel y    aking them       e prone  to adapt in changing cnvir onmcnt s      Analysis of adaptation  CPN m o dels have both a quantit at ive a n d a  qualita tive aspect  Througl  t hc eli reeled aTcs  the networ k reflects the only ways in which variables mny have im         Olesen  Lauritzen  and Jensen pact on each other  The st rengt h of the impact is mod e ll ed through conditional probability tables  We shall here describe how the probabil ity tables are modifi ed in the adaptation process  So  consider a causal probabilistic network into which information on the state of the variables can be en t ered If the state of only some of the variables is known  then the probability distributions for the re maining variables are calculated  Each time this is done  we have described a case  Now  a large number of cases is at hand  an d we want to improve the model    by adjusting the conditional probability tables to the set of cases       Direct modelling of table uncertRinty  In Fig u re     a     the state of the variable A is influenced direct ly by the states of B and C  and the strength  of this influence is modelled by P AIB  C   If the this may  for exam stren gth is subject to doubt ple  be due to d ifferent estimates from experts  or it may be due to a context influence not mod elled  like soil quality of corn fields or ge ne t i c disposition for a disease  then this doubt may be modelled directly by introducing an extra parent  T  for A   F igu re     b     This variable can be c onsi d e r ed as a ty pe vari able modelling  for example  types of context or differ ent experts  assessments  To reflect credibility of the experts or frequencies of the co ntext types  a prior dis tribution forT could be given  When a case is entered       ma y be neces  ary  These  lrffirenl  t ypes rnay  w lw n ily interdependent  for example  assessments from Vilr ious intersecting sets of experts  and it  may he neces sar y to construct a COirlplcx t ype nel work with t he risk  A simplifying assump ti o n would be global independence  the context depen dence for the conditional probabilities are mutually in dependent  In that case  each variable can he given its own parent of ty p es and retrieval and dissemination are completely lo c al  perfonned as above   However  the pro ce d u re is still v u l nera b l e t o combinatorial PX plosion  Take for instance the variahlc A in Figure I  For each parent configurat ion a type dcpcnclcncc on the distribu tion for A shall be described  These de pe nden c ies may vary a lot with tile p a rticu la r parent con figu rations and all kinds of intcr dcpcndrncics rnay  of a combinatorial explosion      be prcsellt   Tlw d or    WI     orc d     Y lw  in  l          to  the p mbabil i ty tables by a  factor which is the prodtrct of the number of states in l  c parents  So  a further  simplification wo uld he lorn  independence  tlw ron text dependence for the various p HCII  coni gtrratinns ar e mutually independent   Indirect moddliug of table    H  rtaint y       If nothing is known  a p ossibl e       t  w structure of llw  inadequacy of the rnodello  the  GlliSIS  case  for  set  the  uncertainty can not he represented directly through a  network of discr e t e types   and we rnust  leave  roorn  for all kinds of types witlt  dl kinds of distrdartions   The learning process h e re is  as  everywhere else in t he  B ayesi a n p ar a d igm   Specif y a prior distribution  of the  ty pes and calculate the posteri or given the case ob served   It remai ns to find a natural way of spccifyi  g    and  such a p robabil i ty model  Sricgelh a ltc r   Hl  a   give  a  Lauritun  range of pos  ihi ities  incl  ding normal  l og istic models   The simplest probability n odel which is convenient  f or cue I   a   case    b   Figure    Ad apt ation th ro ugh a type variable  to the CPN  the calculation of updated probabilities will yield a new distribution on T  an d we may say that the change of these probabi litie s reflects what we have learnt from the case  This process is called retrieval of experience  The new dist ri bution may now be used as prior probabilities for the next case and its impact on the conditional probabilities found by summing out the type variable  This process is called dzssemination of experience  The technique has  for example  been used in Andreassen et al      la   where the system con tain s a model for metab ol i sm in patients suffering from diabetes  Through a type variable  the syst em adapts to the characteristics of the individual patient  Several conditional probabilities in the CPN may be context dependent  and a whole set of type variables  th is  situation assu rncs that each set of entries i    the  conditional probability tahlcs for  configuration follows     Johnson  and Kotz  distribution has   sit y  f pt Pk ll for  p        I  atl   The simplicity is p ro p er    a  so called  a  part icular parent   Jmchlr  rhslnhulnm  l li    A k  dilll  nsiOJd Dirichl  t paJ It  cVrs   n           rq    ttd d  ll   X     k J      LJil    k     I  k    II            I        k  I L    p   I  in  the  distribution with  itt  crprcl a tion tri       for all  lf  i  the  im  is  consid  specified may be interpreted a  representing past exper ie nce as a coni ingency iable  cl J         nk  of counl  of pas  casrs   ered  a  noninformativc prior  the distribution  s   L  rt  i   thcreforc refetred to as   ic equivalent sample size  The updating p roced ure con  The quanti ty  sists of modifying the cou nt s as new cases observed   We shall  not give details   hut  just  state  arc  that  bci   g  the frac    aHUGIN  A System Creating Adaptive Causal Probabilistic Networks  tion a   s   m  is t he mean for the ith outcome  and for each i the variance of the probability for the ith outcome is  m  l  m    v                     s  l  Hence v  is a measure of the uncertainty of the prob ability m   Using this interpretation we also have a tool to model expert opinion s of the type  the probability is some where between p and q  but I believe it is about      In the case of two states a and b  consider  for example  the statement that the probability of a is between     an d     and that it is about      If we  as in Spiegel halter et al           interpret the statement so that the mean is      and the standard deviation is       then it can be modelled by a   dimensional Dirichlet distribution  which is called a Beta distribution   We then have to determine two counts na an d Ob whi ch satisfy the equations Oa        aa ab       and                aa ab l                  which we solve to get Oa        and Ob         This can be an attractive alternative to modelling second order u n certainty by intervals of lower and upper prob abilities  learning  Let  m           mn  and a sample size s be a given specification of the conditional probabil ity table P Aib  c    We can then act as if we had a contingency table of counts  sm          smn  lf we ge t a case in the configuration  b  c   and a   then retrieval qu ite simply consist in adding   to the count for a   and dissemination is just to calculate the new frequen cies  If global and local independence can be assumed the scheme is applicable to all tables  Back to  The scheme only works if both the states of A an d its parents are known  In general we may anticipate that the provided evidence  E  may leave uncertainty on both the states of A and of its parents  A nai ve approach in the general case could be to add a count of P  a   b  cl E  to the counts for a   This scheme is known as fractional updating  Titterington        However  the scheme has several drawbacks  For ex ample  if P A I b  c    P A I E  then the scheme may give unjustified counts yielding a false accuracy  If  for example  E    b  c   then nothing can be learned on the distribution of A  but nevertheless the sample size will be increased by one  See further discussion of this issue in Spiegelhalter and Lauritzen      a  as well as in Spiegelhalter and Cowell         A mathematically correct updating of the distribu tions under our interpretation results in a mixture of Dirichlet distributions rather than in a si ngl e one  a mixture is a linear combination with non negative co efficients summing to     This complicates the calcu lations intractably   in particular when adapting from the next case where mixtures of Dirichlet distributions are to be updated  Eventually the process will yet  agam result in a combinatorial explosion  Instead  the correct distribution is approximated by a single Dirichlet distribution  keeping the approach of modi fying counts   First of all  we want the approximated distribution to have the correct means  and the new set of probabilities  mj        m   is set to be the means of the correct distribution  Secondly  it would be prefer able also to give the distrihu t ion t he c orrect varian cPs  However  this is not possible since only one free parant eter is left  namely the equivalent sam ple size  Instead  the equivalent sample size is give n a value such t h  tt  t he   average v riancc   v    L         v   i l        ts correct  The r csu lt i ng chcrnc  wh ich is used i   aHUGIN  is t hc followittg      in t  the ltwans art  chang  d as if a full count W    oht aitwd   mi     m s  P a   b c II      w   I  P b  c I         s l  The last term   rl   be undcrst oocl so that it clistr ibut  cs that  fJ and C arc not in st ates  b  c  according to their present p ro babili t ies   may  the probability  over the  a    s  Next  the sam pie size is determined  s       L l rn        mi        k     L    i l  ll   V     i   where v  is the variance of l i in t he mixture  the for mulae may be found in Spiegelhalt er and Lauritzen       a    The new r ounts iii C s mi     Features of aHUGIN  The program a ii U G IN   wltich is currently     der int plementation  is an ex t cnsion of II U GIN  A nders  n et at         JIUGIN is a shell which allows the user to edit CPNs over finit e st at c variahlcs  and wh en t ll   CPN is specified  TIUG N creates     runtime sysl  llt for entering findings and ttpd t inr  prohahilit i s of tlw variables in the network  In aiJUGIN each variable lllilY be declared t o be in adaptation mode  If  for cx mplc  the variab le A with states a           an has parents B          C  th e n the con ditiona  probability table P  A I B         C  is modified by declaring A of ndapt at ion t y p e   The t ahle is i n  terpreted as a contingency table such that for ead  parent configuration b         c  the set P Ajb        c  is interpreted as a set of frequencies based on a sampl e of cases  Therefore the user will for each p arent config uration be prompted t o specify F QUIVALENT SAMPLE SIZE  The l arger the ESS  t hc more conservative t hc adaptation will be  The default val l     of ESS is Gk  where k is the number of states in      Alternatively t h e ttscr will he M ked t o specify an in terval for cac   of the prohahililics in t  conditional             Olesen  Lauritzen  and Jensen  probability tables  These in terva ls will th en be trans lated to sample sizes using the equivalent of      The ESS used for the given parent configur at io n will now be chosen as the minimum of the t r anslate d sam p le sizes for the individual entries        Fading  Variables in ada p tati on mode have an extra feature  fading  which makes them tend to ignore th in gs they have learnt a long time ago  considering them as less relevant  Each time a new case is taken into account  the equivalent sample si ze is discounted by a fading factor q  which is a re al number less than one but typ ically close to one  From the expression      for the Dirichlet density  it is seen that the fading scheme es sentially corresponds to flattening the density by r ais ing it to the power q  known as power steady d ynamic modelling  Smith       Smith         SIZE  In the case o f a change from accumulating to fading the EQUIVALENT SAMPLE SlZE is kept but the MAXIMAL SAMPLE SIZE provided by the user  ually claim its influence      Experiments with aHUGIN  To  in v es tigate  the  strengths  and  will grad  limitations  of  se r ies of experiments were carried out   The investigation was designed as a complete fa  lo rial simulation experilllent     t IH  now classical  Chest  aiiUGIN  a  clinic  example  Figure    originat ing frorn  and Spiegelhalt er            Laurit n  Each experiment simulates  If s is the initial ESS  then the maximal ESS after adaptation from a case is qs      Running n cases will result in a maximal ESS of    q  q s           q  This gives that        q  is the maximal sam ple size in the long run  Therefore the user is given the choice between ACCU MULATING  fading factor    and FADING  If fa din g is selected  the user is prompted for MAXIMAL SAMPLE SIZE  MSS  and the fading factor is then computed as  MSS     MSS  Defau lt value is lOOm  w h e re m is the number of entries in the table  Note  The result of fad in g is not only that the sample  size is reduced  Consider namely an entry with count a  and with samp le size s  an d suppose that ret r ie va l of a case results in an increase of the sample size by   and o f the count by x  Without fading the ratio between counts from present and past is x   a   but with fading the ratio is xjqa   This tells us that with fading the  present counts are given more weight  This can also be seen by assuming that the entry wilt never receive more counts  Without fadi n g the p roba bili ty will vanis h at the speed of a f s   n   while wi th fadi ng  the speed of vanishing is in the order of a f s   q          Runtime mode  Figure            cases   The   Ciwst clinic   X llllpk   and fo ur factors  dc   I l d ll     P  are considered  Tine r ndom  ated from  Rl  Probabilities  close to the  R   Probabilities v ry  san pks  R   o rigi n a l  are  and  L   enn  ones   difk renL from the migina  o ws   R   Probabilities  drifting over time   starting as the original ones   To control difTerences due t o chance va riat ions   the samples are reused   Thus  for example  all experimcTit s with probabilities as in Rl are based on identical dat a  Two different observational schemes      are investi gated  the first one is lll lillly included for control pur poses       Complete observations   The ad a pta tion starts with the CPN in the initial con figuration  Findings are entered  and wh en all infor mation on the case has been entered   the adaptation takes place changing the tables for the variables of      Data observed only on the variables  Visit  t o Asia     Smoker     Positive X   r ay     and   Dys  adaptation type   The P factor des c ribes difl erent weights on the prior distributions  expressed as v a ryi n g eq  ivalcnt  sR rnpk sizes  Two cases are considrcd  At any time between two cases the user can choose to change the adaptation type of any variable  When the adaptation type of a variable has been chan ge d   the user is prompt ed for p ossibl e mi s s i n g information on EQUIVALENT SAMPLE SIZE and MAXIMAL SAMPLE  pnoea     PI  Low precision  P   ll igh pncision   ESS ESS            I Oll   aHUGIN  A System Creating Adaptive Causal Probabilistic Networks  Finally  three different learning schemes  L  are inves  the random s amp le    tigated      All variables except ac c u mu latin g mode     Th ber c ulos i  s or cancer  in  Experiment      a      As    for the first      cases  then t he mod e is  postet ior probability intervals or p bls                                     A s     b ut with short memory  MSS          Tuberculosis or can ce r   is always in fixed mode as it is a pure logical tran sit ion   As can be seen  the whole in ves tigati on consists of   x   x   x        experiments   For each experiment a plot is generate d   sho w in g the current value of th e conditional probabil ities after each case has been processed  t ogether with ap proximate     p oste ri o r probability intervals   Results in accumulating mode           change d to fading  with long memory  MSS                                              d           ooo  Experiment       b           sooo  I             posterior probability intervals tor p bls   These experiments are very similar to those performed  by Spie gelhalter and Cowell         However  we al low uncertainty on all conditional probability tables  In general our results show the same pattern  For com  plete data the correct values are obtained quite fast  and the influence of the initial specifications vanishes after a f e w hundred cases  Figures    a  and    a  show an interesting phe  nomenon w h en learning from incomplete data        In these experiments  it can only be observed from t he given data that a maj ori ty of smokers suffer f rom dys pnoea  shortness of breath   It can not be inferred f rom the data whether this correlation is due to the presence or absence of bronchitis  In the fi rst exper iment  where all variables are in accumulating mode  th e frequency of bronchitis is overestimated  Figure    a    To compensate for th is   the condition al pr oba bility for dy spn oe a given bronchitis and none of the other diseases  is underestimated   F igure   a    Thus t he correlation b etw een what can a ct u ally be observed in the data is determined correctly  but the intermedi ate expl an ation is slightly incorrect  From these experiments we conclude  not su rpri singly   that the method has difficulties learning about con  ce pts on which dat a are indirect  In such situations the system rel ies str ongly on p ri or k nowledge   This con clusion was also reached by Spiegelhalter and Cowell                Results in fading mode  Figures    b   c  and    b   c  dis p l ay the results for the same exp er i m ent s as in F ig ures    a  and    a   but with the variab l es ch anged to fading with l ong memory after the first      cases  The same effect on esti mat ing intermediate variables can be observed  Note also  that the two curves vary syn ch rono u sly  Most proba bly this is a r e s u l t of variations in freq uenc ies due to           r                 Experimenl I      c        poslerior probabilily  intervals tor p bjs           i                       r                         j              Figure    Exp erimen ts with in c ompl ete data  The con diti on al probability of btonchitis given the p a t ient  is a smo k e r is learnt in  a   lccurnulating mode   b  fading m ode with long memory   c  fading mode with short  memory  ln the  third experiment   Figures     c  and     c    the  maximal samplesizes are reduced to      ThisexJwri ment reveals the mode  Figure    limit of the applicability of the frtdin     c  shows t hat t lw dat     He lwst  l X ass u ming t hat    II pa tients wi t  h bronchit is  plained by suffer from dyspnoea  To  naint ain t he consistP ncy with the d ata   the frequency of s nokcrs sufT ering from b ronchitis is  underestimated acc o r dingly  This pat t ern          H  Olesen   Lauritzen  and  Jensen  is general for fading with short memory for high and low probabilities  We conclude that special attention must be directed towards systematically m issi ng data and the choice of MSS if such variables are fading  Figure   shows a series of experiments with a declin  Experiment       a    ing probability of being a smoker  The first      cases are identical for the three plots  the variable being in         posterior probability intervals tor p s   accumulating mode  In Figure    a  the variable re  mains in this mode and it is seen how the probability is becomin g increasingly conservative as the ESS m creases   Experiment       a        posterior probabili y  ntervals for p djnot e b         b  Experiment           o posterJor probabilrty interva s for p s   c  Exp rimen              g poslerior probability lnervals tor p s   eooo  Experiment           posterio r probability intervals lor p djnot e b   b  I                r     gl  Q                          c  Experiment                    posterior  probabili y intervals for p dlnot e b          Figure    Learning about  bei ng a smoker       J    a  declining probability of       I    I                            Figure    The same experiment as in Figure   but for  Dyspnoea  given the patient has bronchitis but none of the other diseases   In Fig u re    b  the variable is changed to fading with lo ng memory  MSS   lOOO  after t he first      caes  This i n creases the dynami   behaviour of the system an d an almost correct adaptation is obtained  De  creasing the MSS to      Figure    c   increases the dynamic behaviour further  re  ult ing in stronger fluc tuations around the correct value  The general expe r i e nc e is that the  V SS shonld not he sd too low  nnd  that the experiments confirm th    of aiiUGIN   expected behaviour   aHUGIN  A System Creating Adaptive Causal Probabilistic Networks  To s u mmarize  aH U G I N seems to be able to adapt to chang i n g environments  thereby extending H U G I N with a valuable fun ctional i ty   Howeve r   special atten tion must be directed to the choice of M SS and to var i ables with systematically missing d ata   Andersen  S   K     O lesen   K  G     Jensen   F   V     and J ensen  F            H U G I N   A she l l for building Bayesian belief u ni verses for expert systems  In Proceedings of t h e     t h int ern ational joint confe r ence o n artificial intellig e n ce   p p    reprinted i n S hafer and Pearl                     Also  Andreassen   S   Benn   J   J     Hovorka  R     Olesen   K   G     and Carso n   E   R           a     A probabilis tic approach to glucose prediction and i nsulin dose adj us t ment   Techn i c al report   Inst i t u te for Elec tronic Sys tems   A a l b org U n i versity    A n d reassen  S     J ensen  F  V     and O lesen   K   G          b    Medical expert systems b ased on causal probabilistic networks  In ternational Journ al of Bi omedical Computation                  Cowell  R  G            BAlES   a probab i l i s t i c ex pert system shell with qual i tative and quantita tive learning   In Bayesian st a tistics      ed   J   M  Bern ardo   J      Berger  A   P  D awi d   and A   F  M   Smith    p   i n press  Clarendon Press  Oxford   UK   Jensen  F   V     Lauri tzen   S   L     and Olese n   K   G             Bayesian up dat i n g i n causal probabil istic networks by local c o mpu tations   Co m p u t a t ion a l St atistics Q u a rt e rly      Johnson   N   L   and Kotz            S             Distri b u t ions i n  statistics  Co ntinu ous multivariate dis t ri b u ti o n s    J oh n Wiley and Sons  New York   Lauritzen  S   L   and S p iegel h a l t er   D  J            Lo cal computations with probabilities on graphical st r u ctures and their appli cation to expert systems   with discussi on     Journ al of the Royal Sta tistical Society  Series B                               Probab ilis t i c inference  m  i n t ellig e n t  syste ms  Morgan Kaufmann   San Mateo   Shachter  R  D           Eval u at i n g influence d i agrams  Opera t i o ns Research                           Sh afer   G   R  and Pearl   J    ed      Rea dmg s  in u n certain reas on ing  Morgan Kaufm an n   San  M ateo   Califor n i a  Shenoy  P  P  and Shafer   G   R              Axioms for probab i l i ty and belief fu nction propagation   In Uncert a inty in artificial intelligence I V    ed   R  D   Shachter  T   S   Le v i t t   L  N   K an a    an d J   F   Lem mer    pp            North   Hol l and   Amsterdam   Smith  J   Q             A general ization of the Bayesian steady forecasting mo d el   Journal of t h e Royal Statistical So ci e t y  Series B         Smith  J   model    Q                          The m u l ti parameter st ead y  Journal of t h e Ro yal St atistical Society   Series B         Spiegel halter  D   and Lau r i tzen   S   L               Tech n i ques fo r Bayesi a n a n alysis i n e x p e r t  syste m s   A n n als of  
  We present a new approach to the solu tion of decision problems formulated as in fluence diagrams  The approach converts the influence diagram into a simpler structure  the Limited Memory Influence Diagram  LIMID   where only the requisite informa tion for the computation of optimal policies is depicted  Because the requisite information is explicitly represented in the diagram  the evaluation procedure can take advantage of it  In this paper we show how to convert an influence diagram to a LIMID and describe the procedure for finding an optimal strategy  Our approach can yield significant savings of memory and computational time when com pared to traditional methods      INTRODUCTION  Influence Diagrams  IDs  were introduced by Howard and Matheson        as a compact representation of decision problems  Since then  various authors have attempted to formalize their approach and develop al gorithms for evaluating IDs  Olmsted        and Shachter        initiated research in this direction  Their methods operate directly on the ID and consist of eliminating nodes from the di agram through a series of value preserving transfor mations  During the transformations the policies for the decisions are computed  Later Shachter and Ndi likilikesha        and Ndilikilikesha        proposed a similar  but more efficient approach  Other algorithms evaluate IDs by converting them into different structures  Cooper        described an ap proach where the evaluation of IDs is transformed into inference problems for Bayesian networks  Several im provements of this method were later proposed by  Shachter and Peot        and Zhang         Shenoy        presented a method where the ID is converted into a valuation network  and the optimal strategy is computed through the removal of nodes from this dia gram by fusing the valuations bearing on the node to be removed  Jensen et al         compiled the ID into a secondary structure  the strong junction tree  and solved the decision problem by the passage of messages towards the root of the tree  Our work relies on a property that has already been stressed by Shachter               and Nielsen and Jensen         Namely that in decision problems rep resented as IDs there may be information which is not requisite for computing the policies  Going further  we transform the ID into a similar  but simpler  structure termed Limited Memory Influence Diagram  LIMID  where the requisite information is explicitly depicted  and present a simple algorithm for finding the opti mal strategy using this reduced structure  This can result in significant gains in efficiency compared to tra ditional methods for solving IDs  Section   gives a basic description of LIMIDs as de veloped in Lauritzen and Nilsson         For proofs not given in the present paper  the reader is referred to this source     LIMIDS  LIMIDs are represented by directed acyclic graphs  DAGs  with three types of nodes  Chance nodes  shown as circles  represent random variables  D ecision nodes  shown as squares  represent choices or actions available to the decision maker  Finally  value nodes  shown as diamonds  represent local utility functions  The arcs in a LIMID have a different meaning based on their target  Arcs pointing to utility or chance nodes represent probabilistic or functional dependence  Arcs into decision nodes indicate which variables are known to the decision maker at the time of decision  Thus they in particular imply time precedence         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       In contrast with traditional IDs  the LIMID can repre sent decision problems that violates the assumption of no forgetting saying that variables known at the time of one decision must also be known when all later de cisions are made  The following fictitious decision problem borrowed from Lauritzen and Nilsson        illustrates a typical decision situation which is well described by a LIMID  A pig breeder is growing pigs for a pe riod of four months and subsequently selling them  During this period the pig may or may not develop a certain disease  If the pig has the disease at the time when it must be sold  the pig must be sold for slaughtering  On the other hand  if it is disease free  its expected market price as a breeding animal is higher  Once a month  a veterinary doctor sees the pig and makes a test for presence of the dis ease  The test result is not fully reliable and will only reveal the true condition  hi  of the pig with a certain probability  Based on the test result  ti   the doctor decides whether treating the pig for the disease  di    Associated with every chance node r  connoting random variable  is a non negative function Pr on Xr X Xpa r  such that     where the sum is over Xr  The term Pr does not in general correspond to a true conditional distribution but rather a family of probability distributions for r parametrized by the states of pa r   Each value node u E Y is associated with a real func tion Uu defined on Xpa u        POLICIES A N D STRATEGIES  A policy for decision node d can be regarded as a pre scription of alternatives in xd for each possible obser vation in Xpa d   To allow for the possibility of ran domizing between alternatives  we formally define a policy as follows  A policy Jd for d is a non negative function on xd X Xpa d  which indicates a probabil ity distribution over alternative choices for each pos sible value of pa  d   They must also satisfy the rela tion     as above  A strategy is a collection of policies  Jd   d E      one for each decision  A strategy q    Jd   d E     determines a joint distri bution of all the variables in V as Jq     Pr    Jd  rEI  dEtl           and Pr and Jd are indeed true conditional distributions w r t  fq The expected utility of the strategy q is given by The diagram above represents the LIMID correspond ing to the situation where the pig breeder does not keep individual records for his pigs and has to make his decision knowing only the given test result  The memory has been limited to the extreme of only re membering the present  In the LIMID  the util ity nodes u   u   u  represent the potential treatment costs  whereas u  is the  expected  market price of the pig as determined by its health at the fourth month       SPECIFICATION OF LIMIDS  Suppose we are given a LIMID C with decision nodes    and chance nodes r  We let V      U r  The set of value nodes is denoted Y  For a node n we let pa n  denote its parents  Each node n E V is associated with a variable which we likewise denote by n  that takes a value in a finite set Xn  For W  V we write Xw   XnEWXn  Typical elements in Xw are denoted by lower case values such as xw  abbreviating xv to x   EU q     L Jq x U x   X  where U   uEY Uu is the total utility  We are searching for an optimal strategy ij satisfying EU ij     EU q  for all strategies q  Such an optimal strategy is termed a global maximum strategy in Lauritzen and Nilsson              SOLUBLE LIMIDS  The complexity of finding optimal strategies within LIMIDs is in general prohibitive  This task  however  becomes feasible for LIMIDs that have a certain struc ture  For that reason they are termed soluble  In this section we formally define soluble LIMIDs and present a simple and efficient algorithm for evaluating them  For a strategy q     Jd d E     and any do E    we let Q do   q    Jdo           UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       be the partially specified strategy obtained by retract ing the policy at do  A local maximum policy for a strategy q at d   is a policy J  which satisfies  So  J  is a local maximum policy for q at do if and only if the expected utility does not increase by changing the policy J  given the other policies are as in q  The following lemma gives a method to find a local maxi mum policy  Here  f q d is defined through     and the partial strategy obtained from q by retracting Jd  Let ting the fa mily of n be defined by fa n   pa n  U   n  we now have        Lemma   A policy Jd is a local maximum policy for a strategy q at d if and only if for all Xfa d  with Jd xd I Xpa d       we have  Xd     Zd  fq d xv d zd U xv d zd    L  arg max  XV fa d   As we shall see in Theorem    an important instance of Lemma   is when the strategy q is the unifor m strategy  Here the uniform strategy ij is defined as the strategy ij  Jd  d E    where    Jd Xd I Xpa d   Letting f          IXdl  IT Pr  rEr  we now have the following special case of Lemma           policy Jd is a local maximum policy for the unifor m strategy at d if an d only if for all Xfa d  with Jd xd I Xpa d       we have    arg max  Zd  L  For a node n we let de n  denote the descendants of n  We say that a decision node do is extremal in the LIMID  if u l c     U    fa d     d  E     do        l fa do    for every utility node u E de d    Theorem   establishes the connection between opti mum policies and extremal decision nodes  Theorem    If decision node d is extremal in the L IM ID   then     d has an optimum policy  any local maximum policy for the uniform strategy at d is an optimum policy for d   Suppose decision node d is extremal in the LIMID   Then Theorem   ensures that d has an optimum pol icy Jd  We can now implement Jd by converting d into a chance node with Jd as the associated conditional probability distribution to obtain a new LIMID    It is easily seen that every optimal strategy q  for   then generates an optimal strategy for  as q q U   Jd  Thus  if   again has an extremal decision node  we can yet again find an optimum policy and convert   as above  If the process can continue until all deci sion nodes have become chance nodes  we have clearly obtained an optimal strategy for      Corollary   A  xd  denote that A and Bare d separated bySin the DAG formed by all the nodes in the LIMID   i e  including the utility nodes   f xv d zd U xv d zd    Xv fa d   P roof  For the uniform strategy ij we have from     and     that f il d ex f  Now the corollary follows from  Lemma          We thus define an exact solution ordering d        dk of the decision nodes in  as an ordering with the prop erty that for all i  di is extremal in the LIMID where di l        dk have been converted into chance nodes  A LIMID  is said to be soluble if it admits an exact solution ordering  Accordingly  computing an optimal strategy for a sol uble LIMID  can be done using the following routine  A lgorithm SINGLE POLICY UPDATING  A soluble LIMID  with exact solution order ing d          dk   Input   An optimum policy for do in the LIMID  is a policy which is a local maximum policy at d  for all strategies q in   Evidently some decision nodes may not have an optimum policy  However  in the following we present a method for  graphically  identifying decision nodes that have an optimum policy  For this purpose we let the symbolic expression  For  i     k           do      Compute an optimum policy Jd  for di     Convert di into a chance node with Jd  as its associated conditional probability function  Return   The policies    Jdk       Jd           UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Note that the policy Jd  computed in step   is only optimum for di in the LIMID where decision nodes di         dk are converted into chance nodes  The al gorithm is well defined since  as described above  the solubility of  guarantees that it is always possible to compute an optimum policy for di in step    Thus the collection   Jdk        Jd   constitutes an optimal strat egy for      EVALUATING INFLUENCE DIAGRAMS USING LIMIDS  Suppose we are given a decision problem represented by an ID and wish to evaluate it using the algorithm SINGLE POLICY UPDATING  Then one first needs to transform the ID into an  equivalent  LIMID  This is an easy task  The ID requires a linear temporal order on the decision nodes and  in addition it assumes  no forgetting   i e  all variables known at the time of one decision are assumed to be known when subsequent decisions are made  Thus  for an ID with decision nodes d        dk  where their index indicate the order of the decisions   the no forgetting assumption can be made explicit by drawing arcs from fa  dj  into di for all i and for all j   i  We call the diagram produced in this way the L IM ID version of the ID  In Fig       an ID and its LIMID version are shown  Now we have Theorem    The L IM ID version of an ID is soluble   P roof  Suppose we are given an ID with decision nodes d        dk  For the LIMID version  of the ID we have  for all i  so di is clearly extremal after making di         dk into chance nodes  Thus  is soluble with  exact solution ordering d        dk        REDUCING SOLUBLE LIMIDS  Starting from a soluble LIMID  we now present a method for identifying parents of decision nodes that are non requisite for the computation of optimum poli cies  Similar methods for IDs have been produced by Nielsen and Jensen        and Shachter        and when a LIMID is representing an ID their mehod iden tifies the same requisite parents as ours  but the sub sequent use of SINGLE POLICY UPDATING exploits this reduction to obtain lower complexity of the computa tions  As for IDs the key to simplification of computational problems for LIMIDs is the notion of irrelevance as expressed through the notion of d separation  Pearl         We say that a node n E pa  d   in  is non requisite for d if ul cn I  fa d     n     for every utility node u E de  d   If the above condition is not satisfied  then n is said to be requisite for d  A reduction of  is a LIMID obtained by successive removals of arcs from non requisite parents of deci sion nodes  It can be shown that any LIMID  has a unique minimal reduction  denoted Lmin  obtained by reducing  as much as possible  Thus in Lmin all parents of decision nodes are requisite  cf  Theorem   in Lauritzen and Nilsson          Reducing a soluble LIMID to its minimal reduction can be done by applying the following routine  Note that the algorithm runs in time O k graph size    A lgorithm Reducing Soluble LIMIDs  A soluble LIMID with exact solution ordering d l          dk   Input   For  i k           do  Remove arcs from non requisite parents of decision node di     Note that in the above algorithm the decision nodes are visited in the reverse order starting from dk  This ordering is important  If we chose some other order ing there is no guarantee that the reduced LIMID is minimal  For a discussion of this issue the reader is referred to Lauritzen and Nilsson         Fortunately  the maximum expected utility is pre served under reduction  i e  if c  is a reduction of   then the optimal strategy in c  and the optimal strategy in  have the same expected utility  In addi tion  solubility is preserved under reduction  i e  any reduction of a soluble LIMID  is itself soluble  The reader interested in the details and proofs is referred the above source  here we shall use the following the orem  Theorem    If the L IM ID  is soluble  then      its minimal reduction Lmin is soluble       any optimal st rategy for Lmin is an optimal strat egy fo r    Example   Regard the ID in Fig     and its LIMID version depicted in Fig    The latter diagram is the starting point for reducing the decision problem using Procedure     First one notes that u  and u  are the only utility nodes that are descendants of d   Furthermore   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            Figure    An influence diagram   Figure    The moralized graph of the LIMID in Fig     Figure    The LIMID version of the ID in Fig      Figure    The triangulated graph of the moral graph in Fig     The elimination order used in the triangulation process was d   r   d   r   d   r   r   d    so d  and d  are non requisite parents of d   So the arcs from d  and d  into d  are removed and we let   denote the reduced LIMID  Now one notes that in    u  is the only utility node that is a descendant of d  and since  while not affecting the correctness of the algorithm  the arcs from non requisite parents introduce unnec essary computations   d  is non requisite for d  and the arc from d  into d  is removed  In the reduced LIMID it can be seen that d   which is the only parent of d   is requisite for d   Finally  d  has no parents so no further reduction is possible and therefore the reduced LIMID  shown in Fig     is minimal       CONSTRUCTION OF JUNCTION TREES  As we shall see  computing optimum policies for the decisions during SINGLE POLICY UPDATING can be done by message passing in a so called junction tree  In the present section we describe how to compile a soluble LIMID into the junction tree  Clearly it is al ways advantageous to start with a minimal LIMID   Figure    The minimal reduction of the LIMID in Fig     The transformation from a LIMID C to a junction tree starts by adding undirected edges between all nodes with a common child  including children that are de cision nodes   Then we drop the directions on all arcs and remove all value nodes to obtain the moral graph  Next  edges are added to the moral graph to form a triangulated graph and the cliques are subsequently organized into a junction tree  This can be done in a number of ways  we refer to Cowell et al         for de tails  It is important to note that  in contrast with the local computation method described by Jensen et al         the triangulation does not need to respect any specific partial ordering of the nodes  but the trian gulation can simply be chosen to minimize the com putational costs  for example as described in Kjrerulff           Example   Fig    shows the moral graph of the min imal LIMID in Fig     and Fig    displays the triangu lation of the moral graph  The elimination order used in the triangulation process is chosen to minimize the  Figure    The junction tree of the triangulated graph in Fig            UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Here we have used the convention that     Two potentials  rw  PW  u v  and  r are considered equal  and we write  rW xw we have       Figure    The strong junction tree of the original ID represented in Fig     The rightmost clique is the strong root      size of the cliques  in particular the ordering does not respect the partial ordering of the nodes in the minimal LIMID  The junction tree for the triangulated graph is given in Fig     For comparison we have shown the strong junction tree in Fig     Even though the latter has fewer cliques than our junction tree  the largest clique in the strong junction tree contains six variables whereas the largest clique in our junction tree only contains four variables  This is important since the largest clique in the junc tion tree mainly determines the complexity of message passing in the junction tree       LIMID P OTENTIALS  In our junction tree we represent the quantitative el ements of a LIMID through entities called L IM ID potentials  or just potentials for short  Let W  V    pw uw  where  A potential on W is a pair    W     pw is a non negative real function defined on Xw     uw is a real function defined on Xw    p u   r if for all    p v xw  p xw  and uw xw  u xw  whenever p v xw             This identification of two potentials is needed to prove that marginalization and combination satisfy the ax ioms of Shenoy and Shafer         cf  Lemma     in Lauritzen and Nilsson          This in turn estab lishes the correctness of the message passing scheme presented in Section           INITIA LIZATION  To initialize the junction tree T  one assigns a vacuous potential to each clique C E C  Then for each chance node r in the LIMID C one multiplies the conditional probability function Pr onto the probability part of any clique containing fa r   When this has been done  one takes each value node u  and adds the local utility function Uu to the utility part of the potential of any clique containing pa u   The moralization process has ensured the existence of such cliques  Let  rc  pc  uc  be the potential on clique C after initialization  The joint potential  rv on T is equal to the combination of all potentials and satisfies   fv  So a potential consists of two parts where the first part pw is called the p robability part and the second part uw is called the utility part  A potential is called vacuous if its probability part is equal to unity and its utility part is equal to zero  To evaluate the de cision problem in terms of potentials we define two basic operations of combination and marginalization  This notion of operations is similar to what is used in Shenoy         Jensen et al          and Cowell et al          The combination of two potentials  rw  and  rw   pw   uw    denoted  rW    potential on wl u w  given by      Pw   uw    rw  is the         fW    fW       pw PW   uw    uw     The marginalization of the potential  rw  pw  uw  onto wl  w  denoted  fw  is the potential on wl given by                     l W   fw          w  LW W  Pwuw L    J P    LJW WtPW W Wt            cEC fC      rr  rEr  Pr   L  uEY  uu        J   U         where f is defined in     and U is the total utility       PA SSAGE OF MESSAGES  Let    rc   C E C  be a collection of potentials on the junction tree T  and let  rv    rc   C E C  be the joint potential on T  Suppose we wish to find the marginal  rtR for some clique R E C  To achieve our purpose we present a propagation scheme where mes sages are passed via a mailbox placed on each edge of the junction tree  If the edge connects cl and c   the mailbox can hold messages in the form of potentials on C  n C   So when a message is passed from C  to C  or vice versa  the message is inserted into the mailbox     Imagine for the moment that we direct all the edges in T towards the  root clique  R  Then each clique passes a message to its child after having received messages from all its other neighbours  The structure of a mes sage  fC  tC  from clique C  to its neighbour C  is given by   fCt  C      C    rc     cEne CI   C   fC tC    J      UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            where ne C   are the neighbours of C  in T     In words  the message which C  sends to its neighbour C  is the combination of all the messages that C  re ceives from its other neighbours together with its own potential  suitably marginalized         messages towards a   root clique  R as described above  When R has received a message from each of its neigh bours ne R   the combination of all messages with its own potential is equal to the ma rginalization of    V onto R           Contract       r   tfa d  Compute  r fa d R  Compute the contraction        Cfa d   of  Optimize  Define Jd Xpa d   for all Xpa d  as the distribution degenerate at a point x  t satisfying  cf  Corollary     Suppose we start with a joint potential     V on a junction tree T with cliques C  and pass     tR      f fa d    The following result follows from the fact that the two mappings  combination   Q   and marginalization       obey the Shafer Shenoy axioms  Theorem    Marginalize    CEC rC tR        R Q   cEne R  lf C tR    COMPUTING OPTIMUM POLICIES  Note that all the computations apart from the second step are local in the root clique R  Recall that  in SINGLE POLICY UPDATING  when an optimum policy Jd for d has been computed  d is con verted into a chance node with Jd as its associated probability function  To make an equivalent conver sion in our junction tree  we simply multiply Jd onto the probability part of any clique containing fa d        BY MESSAGE PA SSING  COMPUTING THE OPTIMAL STRATEGY B Y PARTIA L COLLECT  This section is concerned with showing how to find op timum policies for extremal decision nodes by message passing in the junction tree T  Let     W  Pw  uw  be a potential  The contraction of  lf W  denoted cont    W   is defined as the real function on Xw given by    cont  rw      pwuw   Accordingly  for the joint potential    V defined by     we have     cont  rv  f x U x      It is easily shown that for a potential  lf W on W and W   W we have cont  rt         L       cont  rw    W Wt  To compute an optimum policy for an extremal deci sion node d  one first note that by     and      L  f x U x      cont  ri  a d     XV fa d   Consequently  an optimum policy for d can be found as follows  First one identifies a clique  say R  that con tains fa d   The compilation of a LIMID  to a junc tion tree T guarantees the existence of such a clique  Then the following steps are carried out  cf  Corol lary   and Theorem          Collect to R to obtain    R Theorem    Collect         tR  PROPAGATIONS  Suppose we have transformed a soluble LIMID  with exact solution ordering d           dk into a junction tree T  The propagation scheme presented here can be used to compute the optimum policies during SINGLE POLICY UPDATING   As an initial step messages are collected towards any root clique Rk which contains fa dk  Then we com pute an optimum policy for dk  as described in the previous subsection  and the obtained policy is multi plied onto the probability part of Rk  In a a similar manner the policy for dk   can be com puted  First  we identify a new root clique Rk   which contains fa dk d Then we could collect messages to Rk   as above  however  this usually involves a great deal of duplication  Instead we only need to pass mes sages along the  unique  path from the old root clique Rk to Rk   This is done by first emptying the mail boxes on the path and then passing the messages  Note that after this  partial  collection of messages  Rk   has received messages from all its neighbours  Now  an optimum policy for dk I can be computed and the potential on Rk I is changed appropriately  Proceeding in this way by successively collecting mes sages to cliques containing the families of the decisions we eventually compute all the optimum policies and thus the optimal strategy  Example    as in  Fig    shows how the propagation scheme works on our junction tree  For simplicity of exposition we have omitted the mailboxes in the junction tree         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Using     we have cont   J fa d     rR   ns         L  cont nR   ns   L  PRPs uR   us   R fa d  R fa d   and Figure    Passage of messages in the junction tree  The number attached to the arcs indicate the order that the messages are passed  In our propagation scheme we successively collect messages towards cliques that contain the variables fa d       fa d   respectively  So we begin by collect ing messages to clique  r  d  d  r   since it contains fa d    d  r  d    Then we compute an optimum policy for d  and modify the probability part on the clique by multiplying it with the obtained policy for d   Now we partial collect messages towards clique  d  r  d   because it contains fa d    After comput ing an optimum policy for d  and modifying the poten tial appropriately we partial collect messages towards clique  d  r  d    Note that this clique not only con tains fa d   but it also contains fa dl   and thus we need not pass any more messages        REFINEMENT OF THE ALGORITHM  Suppose  at some stage in the algorithm  that the pol icy for decision d is to be computed  and let R be any clique containing fa d   In order to compute an optimum policy for d we collect messages towards R  The following theorem states a condition for when a message from a neighbour of R is superfluous  Theorem    Let C be a neighbour of clique R  Then  whenever S   C n R  pa d   the optimum policy for d can be computed without the message from C   Let  rR  pR  uR  be the potential on R after combining it with the messages from all its neighbours except C  Further  suppose S R n C  pa d  and let ns  ps  us  be the message from C  We need to show that for computation of the optimum policy for d as described in Section      the message ns is not needed               j fa d  nR                PRUR   R fa d   Clearly  asS pa d   fa d  we have that  L  R fa d   where co    Ps  PRPSUR  co     cont  n   i a dl                depends on Xpa d  only   Since f ex fq  where f and  q are given in     and     we have  L  R fa d   PRPSUS  us  L  V fa d   cus      f  V fa d   fij   where cis a constant  Because  is constant for fixed  Xpa d    L  Because multiple collect operations are performed in T  we may pass many messages in the course of the evaluation of all the decisions  In the present section we give a condition for certain collect operations being unnecessary   P roof   cont  R fa d   where c  depends on  this yields  PRPSUS  Xpa d           C    only   Combining     and     now yields for fixed  Xpa d   where c         i e  for each fixed Xpa d   the quantities to be optimized with and without the message from R are linearly and positively related  This completes  the proof  The following example shows an application of Theo rem    Example   The ID displayed in Fig    was intro duced by Jensen et al          Fig     shows the min imal reduction of the LIMID version of the ID and Fig     shows the junction tree of the minimal reduc tion   In order to compute the optimum policy for d  we collect flows towards clique c  since c  contains fa d     UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            Figure     The junction tree for the LIMID in Fig      Figure    An ID    t    t  Figure     The minimal reduction of the LIMID ver sion of the ID in Fig     However  an application of Theorem   gives that the message from c  is unneccessary  c  n c  is a subset of pa d   in the minimal reduction in Fig      Thus  we will only need the message from C   Furthermore  to compute the optimum policy for d  we collect flows towards clique C  since it contains fa d    But the flow from C  to C  is unnecessary because Cs n Cg is contained in pa d    Thus  only the flow from Cw to C  is needed because it has not been computed earlier  Continuing in this way  it turns out that only one flow along every edge in T is needed for the evaluation of the decision problem  see Fig       So  by applying Theorem   we only need to pass    flows which is half the flows we would have passed using the partial prop agation scheme presented earlier     DISCUSSION  The method presented here transforms decision prob lems formulated as IDs into simpler structures  termed minimal LIMIDs  having the property that all requi site information for the computation of the optimal strategy is explicitly represented  It uses recursion to solve the decision problem by exploiting that the en tire decision problem can be partitioned into a number of smaller decision problems each of which having one decision node only  A one off process of compilation  Figure     Flow of messages for the computation of the optimum policies using Theorem    The number attached to the arcs indicate the order that the mes sages are passed   is then performed on the LIMID to produce a higher level graphical structure  the junction tree  that is par ticular well suited for efficient evaluation of each of the small decision problems  The use of recursion is inspired by the well known trick of Cooper        and differs from methods in e g  Shenoy         and Jensen et al          By using recursion we do not require the storage of potentially large tables of intermediate results  see for instance Example     As a consequence  our junction tree can always be made as small as the strong junction tree  Jensen et al         and in some cases our method can result in considerable reduction of evaluation time and mem ory  This reduction happens at two levels  At the first level  we obtain a smaller junction tree because we work in the reduced structure that only includes requisite information  At the second level  we obtain a smaller junction tree because we can triangulate the structure obtained without obeying order constraints  On the other hand  our method typically passes more messages than the strong junction tree method  partly because our junction tree have more  and smaller  cliques  partly because we perform several collect prop agations  As the size of the maximal clique most often is crucial for the efficiency of local computation algo rithms  our algorithm should generally be fast com         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       pared to traditional algorithms   Shachter  R          Evaluating influence diagrams  Oper  There are many opportunities to refine and extend this research  In particular it should be possible to reduce the number of messages that are passed in the junc tion tree  We have presented one such condition for a message being redundant  but a deeper insight in the partial propagation algorithm may reveal other redun dant computations  and improve the efficiency of the algorithm  The new method presented here also opens the possibility of evaluating large and computationally prohibitive decision problems by approximating them with soluble LIMIDs  Work regarding this issue is de scribed in Lauritzen and Nilsson        and is still in progress  Acknowledgements  This research was supported by DINA  Danish Infor matics Network in the Agricultural Sciences   funded by the Danish Research Councils through their PIFT programme   Shachter  R           Bayes ball   The rational pasttime   for determining irrelevance and requisite information in belief networks and influence diagrams   In Proceed ings of the Fourteenth Annual Conference on Uncer tainty in Artificial Intelligence  UAI g    pp          Morgan Kaufmann Publishers  San Francisco  CA  Shachter  R           Efficient value of information com putation  In Proceedings of the   th Annual Con ference on Uncertainty in Artificial Intelli gence   ed  K  Laskey and H  Prade   pp           Morgan Kauf mann Publishers  San Francisco  CA  Shachter  R  and Ndilikilikesha  P          Using influence diagrams for probabilistic inference and decision mak ing  In Proceedings of the Ninth Conference on Uncer tainty in Artificial Intelli gence   ed  D  Heckermann and A  Mamdani   pp          Morgan Kaufmann  Stanford  California  Shachter  R  and Peot  M  A          Decision making using probabilistic inference methods  In Proceedings of the Eighth Annual Conference on Uncertainty in Artificial Intelligence  UAI      pp          Morgan Kaufmann Publishers  San Francisco  CA  Shenoy  P  P          Valuation based sy stems for Bayesian decision analysis  Operati ons Research               
 We describe an expert system  Maies  under development for analysing forensic identification problems involving DNA mixture traces using quantitative peak area information  Peak area information is represented by conditional Gaussian distributions  and inference based on exact junction tree propagation ascertains whether individuals  whose profiles have been measured  have contributed to the mixture  The system can also be used to predict DNA profiles of unknown contributors by separating the mixture into its individual components  The use of the system is illustrated with an application to a real world example  The system implements a novel MAP  maximum a posteriori  search algorithm that is briefly described      Introduction  Probabilistic expert systems  PES  for evaluating DNA evidence were introduced in      This paper is concerned with describing the current status of a computer software system called Maies  Mixture Analysis in Expert Systems  that analyses mixed traces where several individuals may have contributed to a DNA sample left at a scene of crime  In     it was shown how to construct a PES using information about which alleles were present in the mixture  and we refer to this article for a general description of the problem and for genetic background information   A brief summary to genetic terminology is given in Appendix A   The results of a DNA analysis are usually represented as an electropherogram  EPG  measuring responses in relative fluorescence units  RFU  and the alleles in the mixture correspond to peaks with a given height and area around each allele  see Figure    The band intensity around each allele in the relative fluorescence  Julia Mortera Dipartimento di Economia Universita Roma Tre Via Ostiense            Roma  Italy   units represented  for example  through their peak areas  contains important information about the composition of the mixture   Figure    An electropherogram  EPG  of marker VWA from a mixture  Peaks represent alleles at        and    and the areas and height of the peaks express the quantities of each  Since the peak around allelic position    is the highest this indicates that the    allele is likely to be a homozygote or a shared allele between two heterozygotes  This image is supplied courtesy of LGC Limited        The main focus of the present paper is to describe the current status of a computer package called Maies  which automatically builds Bayesian network models for mixture traces based on conditional Gaussian distributions     for the peak areas  given the composition of the true DNA mixtures  Currently the program only considers a DNA mixture from exactly two contributors  which seems to be the most common scenario in forensic casework      and the program ignores other important complications such as stutter  dropout alleles  etc    We distinguish two types of calculations that Maies can perform  One type is evidential calculation  in which a suspect with known genotype is held and we want to determine the likelihood ratio for the hypothesis that the suspect has contributed to the mixture vs  the hypothesis that the contributor is a randomly chosen individual  We distinguish two cases  the other contributor could be a victim with a known genotype or a contaminator with an unknown genotype  possibly without a direct relation to the crime  This could be a laboratory contamination or any other source of contamination from an unknown contributor  The other type calculation that Maies can perform is the separation of profiles  i e  identifying the genotype of each of the possibly unknown contributors to the mixture  the evidential calculation playing a secondary role  Both types of calculation are illustrated in      of DNA in a mixture sample  The model is idealized in that it ignores complicating artefacts such as stutter  drop out alleles and so on  and assumes that the mixture is made up of DNA from two people  who we refer to as p  and p   Typically  prior to amplification in a laboratory  a DNA mixture sample will contain an unknown number of cells from p  and a further unknown number of cells from p   Hence there is an unknown common fraction  or proportion  across the markers of the amount of DNA from p   that we denote by   In an ideal amplification apparatus  during each amplification cycle the proportion of alleles of each allelic type would be preserved without error  We model departures from this ideal as random variation using the Gaussian distributions  and we introduce an additional variance term to represent other measurement error  represented by       Previous related work includes that of     and     who respectively developed numerical methods known as Linear Mixture Analysis  LMA  and Least Square Deconvolution  LSD  for separating mixture profiles using peak area information  Both methods are based on least squares heuristics that assume the mixture proportion of the contributors DNA in the sample is constant across markers  A computer program has been written     for estimating the proportion of the individual contributions in two person mixtures and to rank the genotype combinations based on minimizing a residual sum of squares  More recently      describes PENDULUM  a computer package to automate guidelines in     and      None of the methods described above utilizing peak area information are probabilistic in nature  nor do they use information about allele frequency  In contrast  the methodology proposed in      combines a model using the gene frequencies with a model describing variability in scaled peak areas to calculate likelihood ratios and study their sensitivity to assumptions about the mixture proportions   The post amplification proportions of alleles for each marker are represented in the peak area information  which we include in the analysis through the relative peak weight  The  absolute  peak weight wa of an allele with repeat number a is defined by scaling the peak area with the repeat number as wa   aa   where a is the peak area around allele a  Multiplying the area with the repeat number is a crude way of correcting for the fact that alleles with a high repeat number tend to be less amplified than alleles with a low repeat number  For issues concerning heterozygous imbalance see        The plan of the rest of the paper is as follows  In the following section we describe the mathematical assumptions underlying the Bayesian networks that Maies generates for analysing two person DNA mixtures  We then describe the components that Maies uses to build up the networks  This is followed by a description of a simple MAP search algorithm  implemented in Maies for separation of profiles  We then illustrate the use of Maies on a real life example  and then summarize future work required to make Maies into a tool for routine casework   To avoid the arbitrariness in scaling used to measure the areas  we consider the observed relative peak weight ra   obtained by scaling with the total peak weight as X ra   wa  w    w    wa       The mathematical model  Our PES is a probabilistic model for relating the preamplification and post amplification relative amounts  We further assume that  The peak weight for an allele is approximately proportional to the amount of DNA of that allelic type   The peak weight for an allele possessed by both contributors is the sum of the corresponding weights for the two contributors   a  so that then  P  a ra        For the relative peak weight  denoted by the random variable Ra   we assume a Gaussian error distribution Ra  N  a   a          a    n    a        na           where  is the proportion  or fraction  of DNA in the  i  mixture originating from the first contributor  na is the number of alleles with repeat number a possessed  i  by person i  Note that na            and hence a            We assume an error variance for a  of the form a           a     a               for U S  Caucasians for the analysis in    of data taken from          where  and  are variance factors for the contributions to the variation from the amplification and measurement processes   Note that if a     then a       and Ra  N           The interpretation of this is that if there are no alleles of type a in the mixture prior to amplification  then any detected post amplification can be ascribed to measurement error  Similarly if a       which means that for the given marker all alleles are of type a in the mixture before amplification  then Ra  N           that is post amplification all alleles for the given marker are of type a  up to measurement error  However the Bayesian networks that Maies constructs uses the variance structure a       a              The reason is that we need to consider the correlation between weights due to the fact that they must add up to unity  It turns out  perhaps surprisingly  that the P likelihood obtained using     when conditioned on a ra     has precisely the form as would be obtained using the likelihood based on     used in our model if we ignore measurement error by setting         and   the likelihoods are P very close numerically for small    This constraint a ra     is imposed when we enter the complete set of observed peak weights as evidence in our networks  The proof  too long for this paper  may be found in       In our example in    we used           and             corresponding approximately to a standard deviation for the observed relative weight of about p                       for a       substituted into      These parameter values imply that when amplifying DNA from one heterozygous individual  for which a         an ra value at two standard deviations from the mean would give a value of                  for the ratio of the minor to the major peak area  this is about the limit of variability in peak imbalance that has been reported in the literature       and suggests that our chosen parameter values are perhaps conservative  In general the variance factors may depend on the marker and on the amount of DNA analysed  but for simplicity we use the values above   Our PES model is robust to small changes in these parameter estimates   Finally  we assume known gene frequencies of single STR alleles  in particular we use those reported in          Maies  The basic form of our Bayesian network models is fairly straightforward  but the networks can grow large when modelling the ten or so markers typical in a mixture problem   In the example in    the network has     nodes   One way to manage this complexity is to use object oriented Bayesian network software  as we describe in detail in       Here we describe Maies  a purpose built program that  after entering peak area information and available genetic information  if available  about the potential contributors  automatically constructs a single conditionalGaussian Bayesian network on which the probability calculations are performed  Maies implements the local propagation scheme of       Peak areas are automatically converted to normalized weights and entered as evidence in the relevant nodes by the program  An example of a network generated for a single marker with two alleles observed in the mixture is shown in Figure    The figure illustrates the repetitive modular structure that makes it possible for Maies to create the much larger Bayesian networks required to analyse mixtures on several markers  We now describe these various structures and how they interrelate  working from top to bottom in Figure    u mg  u pg  smg  u gt  spg  vmg  sgt  vpg  u mg  vgt p    s   p gt  u pg  u gt  p    v   target  p gt  jointgt p     p     p  x  p       inmix    p     p  x  x inmix     inmix      weight    weight  x weight    weightobs  x weightobs  p  frac   weightobs  sym  Figure    The structure of a Bayesian network generated by MAIES for a single marker  in which two allele peaks    and    were observed      The first term in the variance structure in     can be seen as a second order approximation to a more sophisticated model based on gamma distributions for the absolute scaled peak weights to be discussed elsewhere     This dataset has an observed allele    of the marker D    As none of the     subjects in      had this allele  we chose to use               as its frequency         Top level people  Maies currently models mixtures only for DNA from two individuals  Thus it sets up nodes for four individuals who are paired up  prefixed by s  for suspect   v  for victim   and u  and u  representing two unspecified persons from the population  Corresponding to each of these individuals is a triple of nodes representing their genotype  gt  on the marker  and the individuals paternal  pg  and maternal  mg  genes  The probability tables associated with the maternal and paternal genes contain the allele frequencies of the observed alleles  whilst the conditional probability table associated with the genotype node is the logical combination of the maternal and paternal gene            Repeat number nodes  On the level below the allele counting nodes are the repeat number nodes  labelled   inmix     inmix  and x inmix   These are  yes no  binary valued nodes representing whether or not the particular alleles are present in the mixture  thus for example allele   is present in the mixture if either of the allele counting nodes p    or p    takes a non zero value  For the node x inmix  the x refers to all of the alleles in the marker that are not observed  When using repeat number information as evidence the repeat number nodes present in the mixture will be given the value yes and x inmix  will be given the value no   Actual contributors to the mixture  The genotypes on the marker of the two individuals p  and p  whose DNA is in the mixture are the nodes labelled p gt and p gt  Node p gt has incoming arrows from nodes u gt  sgt and a  yes no  valued binary node labelled p    s   The function of this latter node is to set the genotype of node p gt to be that of sgt if p    s  takes the value yes  otherwise to set the genotype of node p gt to be that of u gt  An equivalent relationship holds between the genotype nodes p gt  vgt  u gt and p    v   Uniform priors are placed on the nodes p    s  and p    v   The node labelled target represents the four possible combinations of values of the two nodes p    s  and p    v   with a conditional probability table of zeros and ones representing the logical identities  The marginal posterior distribution of this node is used to calculate likelihood ratios in evidential calculations  The network also has a node representing the joint genotypes of individuals p  and p   which is labelled jointgt  with incoming arrows from p gt and p gt  the  quite big  conditional probability table associated with this node has entries that are either of zero or one  The most likely configuration of the marginal distribution in all joint genotype nodes across all markers is required for separating the mixture  and is found using the MAP search algorithm described in           i   These nodes model the na variables introduced in       Allele counting nodes  On the level below the genotype nodes for p  and p  is a set of nodes representing the number of alleles  taking the value of      or    of a certain type in each individual  Thus  for example  the node p    counts the number of alleles of repeat number   in the genotype of individual p  for the given marker  this value only depends upon the genotype of the individual p  and hence there is an arrow from p gt to p           True and observed weight nodes  These nodes are represented by the elliptical shapes  The nodes   weight    weight and x weight represent the true relative peak weights r    r  and rx respectively of the alleles      and x in the amplified DNA sample  Each true weight node is given a conditionalGaussian distribution as in      where the fraction  of DNA from p  in the mixture is modelled in the network by a discrete distribution in the node labelled p  frac  The variance is taken to be    a   The nodes   weightobs    weightobs and x weightobs represent the measured weights  The observed weight is given a conditional Gaussian distribution with mean the true weight  and measurement variance      hence leading to the variance      When using peak area information as evidence the nodes representing the observed weights will have their values set to the relative peak weights  The sym node is only used for separating a mixture of two unknown contributors  to break the symmetry between p  and p   see             Networks with more than one marker  The network displayed in Figure   generated by Maies is for a single marker  for mixture problems involving several markers the structure is similar but more complex because the number of nodes grows with the number of markers  In such a network the nodes shaded in Figure   occur only once  The unshaded nodes are replicated once for each marker  with each node having text in their labels to identify the marker that the allele or genotype nodes refer to  There will also be extra repeat number  allele counting and allele weight nodes in each marker having more than two observed alleles in the mixture  extending the pattern for the one marker network in the obvious manner       A simple MAP search algorithm  It is well known that the most likely configuration of a set of discrete variables is not necessarily the same as found by picking the most likely states in the individual marginals of the variables  see for example        The basis of the MAP search algorithm in Maies is to assume that this is close  Specifically  after entering and propagating evidence  one finds the individual marginals of the set of MAP variables M of interest  There are then two variants of the MAP algorithm  batch and sequential  In the batch variant  one finds for some reasonable number n  say n         the top n most likely configurations of the joint probability given by the product of the individual marginals of MAP variables  This is done by constructing a disconnected graph in the MAP variables  and using the efficient algorithm of       These configurations are stored in an list  c    c            cn   ordered by decreasing probability according to the independence graph  Now returning to the original Bayesian network  one propagates the available evidence E and finds the normalization constant P  E  and stores this in rp  say  short for remainder probability   One then processes the  c    c            cn   configurations as additional evidence in the original Bayesian network and finds from the normalization constant each of their probabilities P  ci   E   After processing each configuration  one keeps track of the highest probability configuration found and its probability  bp  One also subtracts p ci   E  from rp  so that if it ever happens bp   rp then the MAP has been found  If one stores all of the probabilities p ci   E   i              k for all of the configurations that have been processed  then perhaps the second  third etc  most likely configurations may Pk be identified if their probabilities exceed P  E   i   P  ci   E   The sequential variant proceeds similarly  the difference is that the candidates c    c           are generated one at a time as required  The following is pseudo code for the sequential variant for finding the MAP   the second  third  fourth etc   most likely configurations      A criminal case example  Our example is taken from Appendix B of     and illustrates the use of the amelogenin marker in the analysis of DNA mixtures when the individual contributors are of opposite sex  Peak area analysis of the amelogenin marker in DNA recovered from a condom used in a rape attack indicated an approximate     ratio for the amount of female to male DNA contributing to the mixture  Peak area information was available on six other markers  the information is shown in Table    we shall refer to this as the Clayton data   Further examples are illustrated in      and        Table    Clayton data of     showing mixture composition  peak areas and relative weights together with the DNA profiles of both victim  v  and suspect  s   For the marker D   the allele designation in brackets is as given in     using the labelling convention of      Marker  Alleles  Amelogenin  X Y                                                                 D  D    D    FGA THO VWA   Initialize  i   j      bp      and rp   P  E    While bp   rp do   Find ci and P  ci   E    If p ci   E    bp set bp   P  ci   E  and j   i   Set rp    rp  P  ci   E  and i    i       cj is the MAP configuration  For purely discrete networks  this algorithm does not appear to be as efficient as that described in       However it is neither clear that the latter can be applied to finding the MAP of a set of discrete variables in a conditional Gaussian network  nor that it could identify                      Peak area                                                                                                    Relative weight                                                                                                                                                     s  v  X Y  X                                                                  Evidential calculation  One possible use of the system to the Clayton data would be to compare the two hypotheses   H    the suspect and victim both contributed to the mixture  H    the victim and an unknown contributor contributed to the mixture   In a courtroom setting  the null hypothesis  H    would be a prosecutions case  whilst H  would represent the defences case   It is standard procedure in court for likelihood ratios of these hypotheses to be reported    of genotypes of the two contributors to the mixture  but other less likely but also plausible combinations  Maies achieves this with the MAP search algorithm described in      To do this calculation in Maies  evidence is entered in the observed relative peak area nodes  the repeat number nodes  and information on the suspect and victim genotypes  After propagating the evidence the marginal on the target is examined  This has the following values  taken from Maies    For separating a mixture  we may or may not have genetic information about one of the contributors  For our rape example  suppose that we have the genotype of the victim  Then using Maies we may enter as evidence the victims genotype  the relative peak areas and the repeat number information  We also select the value yes in the p    v  node  We then select the set of joint genotype nodes  and perform the MAP search  Maies returns two configuration  the most likely having posterior probability           with the genotype p  matching our suspect profile in Table    The second most likely combination has a posterior probability of             and differs from the true profile in the marker FGA where a homozygous          genotype is predicted  All remaining possible genotype combinations have a total probability mass of less than           u    u  v and u s and u s and v                                                                                   From this the likelihood ratio of H  to H  is calculated to be P  s and v   E  P  s and u   E                 where E denotes the complete set of evidence   Note that because we have placed uniform priors on the nodes p    s  and p    v   then P  E   s and v  P  E   s and u    P  s and v   E  P  s and u   E    It may be that only DNA from a suspect is available  but not from a victim  In such a situation we could use Maies to compare the following two hypotheses   H    the suspect and an unknown contributor contributed to the mixture  H    two unknown contributors contributed to the mixture Again in a courtroom setting these could represent prosecution and defence cases respectively  The calculation proceeds as before  but with the victim profile omitted from the evidence  This time the marginal on the target node is given by u    u  v and u s and u s and v                                                                                 and the likelihood ratio of H  vs  H  is given by P  s and u   E  P  u  and u    E                     Mixture separation calculations  The other type of calculation that may be performed with Maies is that of separating a two person mixture into genotypes of the contributors  The output from such a decomposition could be used to find a match in a DNA database search  For such a search it is useful to have not just the most likely combination  The second possibility is that no genotypic information is available on either contributor to the mixture  To do this calculation  evidence on the observed relative peak areas and the repeat number information is entered  To overcome the symmetry in the network between p  and p   we enter evidence on the sym node that p  frac is       Then  selecting the joint genotype nodes as before  we perform a MAP search  The result is shown in Table    All markers are correctly identified  Note in particular that the genotypes for the marker THO are identified correctly  In     this was only possible to do so after the victims profile was taken into account  Table    Most likely genotype combination of both contributors for Clayton data  The victim  here p   and male suspect  p   is correctly identified on every marker  The final column indicates the marginal probabilities for the genotype pairs on individual markers  with the figure in parenthesis the product of these marginals  Marker Amelogenin D  D   D   FGA THO VWA joint  Genotype Genotype p  p  XX XY                                                                               Posterior probability                                                                            For this example  the Maies MAP search algorithm identifies the next three most likely combinations         Posterior density              sumed correct  Such methods could also be useful for calibrating the variance parameters    and      We are pursuing ways that this could be accomplished using an EM estimation algorithm   Bayesian methods for estimating the variance parameters could also be developed   Nevertheless  despite these many issues  we feel that the present framework provides a sound foundation in which these and other matters can be be addressed and incorporated into Maies         Acknowledgements                                           Proportion of DNA from the major contributor   Figure    Posterior distribution of mixture proportion from Clayton data using no genotypic information  of genotypes  these have probabilities of                     and           respectively   The network has     discrete nodes and    continuous nodes  The total state space of the   joint genotype nodes is approximately             Identifying the    most likely combinations of these nodes took approximately    seconds on a    GHz laptop with    Kb memory   Finally for this example  Figure   shows the posterior distribution of the mixture proportion  the peak at around      corresponds to a mixture ratio of         in line with the approximate     estimated in          Discussion  We have described a software system  Maies  for analysing DNA mixtures using peak area information  yielding a coherent way of predicting genotypes of unknown contributors and assessing evidence for particular individuals having contributed to the mixture  and applied it to a real life example  A simple MAP search algorithm allows a set of most plausible genotypes to be generated  perhaps for use in a DNA database search for a suspect  There are a number of issues that would need addressing before the system could be used in routine analysis of casework  for example  complications such as more than two potential contributors  multiple traces  indirect genotypic evidence  stutter  etc  In addition  preliminary investigations seem to indicate that the variance factor depends critically on the total amount of DNA available for analysis  As this necessarily is varying from case to case  a calibration study should be performed to take this properly into account  Methods for diagnostic checking and validation of the model should be developed based upon comparing observed weights to those predicted when genotypes are as   This research was supported by a Research Interchange Grant from the Leverhulme Trust  We are indebted to participants in the above grant and to Sue Pope and Niels Morling for constructive discussions  We thank Caryn Saunders for supplying the EPG image used in Figure     
