 We consider how to use the Bellman residual of the dynamic programming operator to compute suboptimality bounds for solutions to stochastic shortest path problems  Such bounds have been previously established only in the special case that all policies are proper  in which case the dynamic programming operator is known to be a contraction  and have been shown to be easily computable only in the more limited special case of discounting  Under the condition that transition costs are positive  we show that suboptimality bounds can be easily computed even when not all policies are proper  In the general case when there are no restrictions on transition costs  the analysis is more complex  But we present preliminary results that show such bounds are possible      Introduction  A stochastic shortest path problem is a Markov decision process  MDP  where the objective is to find a minimumcost policy that reaches a goal or terminal state with probability           It is an elegant model for many problems of planning under uncertainty  especially for goal oriented decision theoretic planning problems where policy execution terminates once a goal condition is achieved  Standard solution methods rely on dynamic programming or linear programming  The model is also used in the development and analysis of reinforcement learning and heuristic search algorithms for MDPs             There are extensions of the stochastic shortest path problem for planning under partial observability       multi agent planning          and risksensitive planning       For the stochastic shortest path problem  the expected total cost of policy execution is bounded  without discounting  Thus it is an important alternative to the discounted infinitehorizon MDP as a model for decision theoretic planning   Although use of a discount factor sometimes has an economic justification  discounting is not well motivated for many AI planning problems and has potential drawbacks  It can skew the relative values of policies and change the optimal policy  Discounting can also make it impossible to guarantee that dynamic programming finds a policy that reaches a goal state with probability    With discounting  a policy that cycles forever without reaching the goal state still has finite total cost  which could be less than the cost of the best policy that is guaranteed to reach the goal state  Despite potential drawbacks  the discounted infinitehorizon model is widely used  One reason for its appeal is that discounting is a simple way to ensure that algorithms for solving infinite horizon MDPs have the desired convergence properties  since the dynamic programming operator is a contraction operator in this case  with the contraction rate equal to the discount factor  Although the convergence of value iteration and policy iteration for stochastic shortest path problems is well established  it is not based on a contraction property  except in the special case that all policies are proper   As a result  there is no guarantee that the convergence rate is geometric and suboptimality bounds are generally not available for solutions found by dynamic programming  in contrast to the discounted case  In this paper  we show that even though the dynamic programming operator for stochastic shortest path problems is not a contraction operator in general  it behaves like a contraction operator if the dynamic programming algorithm is started with the value function of a proper policy  which is a policy that achieves the goal state with probability one  Under the additional condition that transition costs are positive  we show how to use the Bellman residual of the dynamic programming operator to compute suboptimality bounds  In the general case where there are no restrictions on action costs  the analysis is more complex  But we establish some preliminary results that support a general approach to computing bounds  Our results apply to both completely observable and partially observable stochastic shortest path problems       Background  A stochastic shortest path problem is solved by finding an optimal policy  satisfying  We begin with a review of the stochastic shortest path problem as formulated by Bertsekas and Tsitsiklis           and extended to the partially observable case by Patek       We also review previous work on computing suboptimality bounds for solutions found by dynamic programming       Stochastic shortest path problem  Like any discrete time Markov decision process  MDP   a stochastic shortest path problem includes a set of states  S  and a set of control actions  U   which we assume are both finite  a set of transition probabilities  where pij  u  denotes the probability that the system moves to state j  S after action u  U is taken in state i  S  and a set of real valued costs  where g i  u  j  denotes the cost incurred when action u taken in state i results in a transition to state j  The expected  cost of taking action u in state i is denoted g i  u    jS pij  u g i  u  j   In addition  a stochastic shortest path problem is characterized by a set of assumptions from which it derives its special properties  The first is the following  Assumption    The state set includes a special target or terminal state  t  S  which is zero cost and absorbing  which means that ptt  u      and g t  u  t       u  U   The objective is to find a stationary policy     S  U   that reaches the terminal state while minimizing the total expected cost  We are especially interested in stationary policies that reach the terminal state with probability   from any initial state  called proper policies  Definition    Bertsekas and Tsitsiklis   A stationary policy  is said to be proper if there exists a finite positive integer m such that there is a positive probability of reaching the terminal state after at most m stages when following this policy  regardless of the initial state  that is     min P  xm   t x    i         iS       J   i    J  i   J  i   i  S    M   where J  denotes the optimal value function  Two additional assumptions of the stochastic shortest path problem ensure that an optimal policy exists and that it is proper  Assumption    There exists at least one proper policy  Assumption    For every improper policy   the corresponding cost J  i  is infinite for at least one state i  Assumption   is equivalent to the assumption that the expected cost J  i  of state i under policy  is infinite if a process started in state i and following policy  does not reach the terminal state with probability    Two special cases of the stochastic shortest path problem play an important role in the analysis of Bertsekas and Tsitsiklis            The first is the stochastic shortest path problem when all policies are proper  Analysis of this special case is easier  although the assumption that all policies are proper is often unrealistic  Another special case is the discounted infinite horizon MDP  By a well known reduction  any discounted infinite horizon MDP can be reduced to an equivalent stochastic shortest path problem in which  for every state and action pair  there is a probability       of making a transition to the terminal state  where      is the discount factor  with the other transition probabilities normalized  For the stochastic shortest path problem created by this reduction  all policies are proper  As we will see  the well known convergence properties and suboptimality bounds for the discounted infinite horizon MDP are a special case of those for the stochastic shortest path problem       Dynamic programming  The stochastic shortest path problem can be solved using dynamic programming  where the dynamic programming operator is defined as follows   T J i    min pij  u   g i  u  j    J j    i  S      uU  where xk denotes the state of the process at stage k  A stationary policy that is not proper is said to be improper  Following a policy that is proper according to this definition  a process reaches the terminal state with probability    regardless of the initial state      The value  or cost to go  function J of a stationary policy  gives the expected total cost of following the policy starting from an initial state i  defined as  N      J  i    lim E g xk    xk    x    i   i  S      N   k    For a proper policy   the expected cost is bounded above for each state  that is  J  i      i  S        jS  and the related policy evaluation operator T is defined as   T J i    pij   i    g i   i   j    J j    i  S      jS  These operators are the key steps in two dynamic programming algorithms  value iteration and policy iteration  Analysis of the convergence of value and policy iteration turns on two properties of the dynamic programming and policy evaluation operators  the monotonicity property and the contraction property  The monotonicity property is defined as follows  if J  J    then T J  T J  and T J  T J     Both operators satisfy the monotonicity property     As shorthand  we let J  J  denote J i   J   i   i  S    An operator T   such as the dynamic programming operator  is said to be a contraction operator if  for all bounded value functions J and J      T J  T J        J  J            where   with          is the contraction rate and       is some norm  For discounted infinite horizon MDPs  it is well known that both the dynamic programming operator T and the policy evaluation operator T are contraction operators  with the contraction rate  equal to the discount factor  If the state space is finite  they are contraction operators in the maximum norm  defined as   x     maxiS x i   If the state space is continuous  they are contraction operators in the supremum norm  defined as   x     supiS x i   For MDPs  the maximum or supremum norm    T J  J    is called the Bellman residual  For the stochastic shortest path problem  there is no discount factor  or equivalently  the discount factor is     In this case  a related concept of contraction operator is useful  An operator T is said to be an m stage contraction operator if T m   which is the composition of T with itself m times  is a contraction operator  that is  if there is some   with          and some norm        such that   T m J  T m J         J  J            for all bounded value functions J and J    For the stochastic shortest path problem  the dynamic programming operator is an m stage contraction operator in the maximum  or supremum  norm if there is some minimum positive probability m             that the process reaches the terminal state within m steps  beginning from any state  Bertsekas and Tsitsiklis           show that the dynamic programming operator for stochastic shortest path problems is an m stage contraction operator in the special case that all policies are proper   As for the policy evaluation operator  it is an m stage contraction if it evaluates a proper policy  The significance of showing that the dynamic programming and policy evaluation operators satisfy the contraction property  under the condition that all policies are proper  is that the convergence of both value iteration and policy iteration follows from Banachs Fixed Point Theorem  it also follows that the convergence rate is geometric  Bertsekas and Tsitsiklis consider this special case first because   The minimum probability m that the terminal state will be reached in m stages can be computed by solving a finite horizon MDP with the same transition probabilities as the stochastic shortest path problem but with a cost of   for any transition to the terminal state  and a cost of   for all other transitions  Finding a policy that minimizes the m horizon cost corresponds to finding a policy that minimizes the probability of reaching the terminal state within m stages  By Definition    the minimum probability of reaching the terminal state within m stages is greater than zero  for some positive integer m  if all policies are proper   the dynamic programming operator is not a contraction operator in the general case  In the words of Bertsekas and Tsitsiklis      our assumptions do not imply that the corresponding dynamic programming mapping is a contraction  unlike the situation in discounted problems   unless all policies are proper  They give an example to show that the contraction property does not hold in general  It is a stochastic shortest path problem with two states  state   is terminal and state   is nonterminal  Two actions are possible in the nonterminal state  One causes a deterministic transition to the terminal state with a cost of    the other causes a deterministic self transition with a cost of    Thus there are two stationary policies  one proper and the other improper  Consider two value functions J and J  for which J      J           J         and J           Under these conditions   T J     T J           J           J               J     J        which shows that  in general  T is not a contraction with respect to any norm  The principal contribution of Bertsekas and Tsitsiklis to the theory of stochastic shortest path problems is their proofs of the convergence of value iteration and policy iteration in the general case  where the dynamic programming operator is not a contraction  Given only the assumptions of the stochastic shortest path problem  they prove the following   The optimal value function J  is the unique solution of the Bellman equation J    T J    and value iteration converges to it  which means that for any initial value function J  limt T t J   J     The value function J of a proper policy  is the unique solution of the linear system of equations  J   T J   and policy evaluation converges to it  which means limt Tt J   J   for any initial J   Starting with a proper policy     policy iteration generates a sequence of policies                for which Jk    Jk   k  by alternating a policy evaluation step that computes Jk   with a policy improvement step that computes an improved policy k   using the equation Tk   Jk   T Jk   or equivalently  i  S  k   i    arg min uU        pij  u  g i u j  Jk j          jS  and the sequence converges to an optimal policy  However  their convergence results are weaker than those based on the contraction property and Banachs FixedPoint Theorem because they establish pointwise convergence  not uniform convergence  As a result  they provide no guarantee that the rate of convergence is geometric and no way to bound the number of iterations until convergence to an  optimal policy  They also do not allow easy use of the Bellman residual to compute suboptimality bounds         Suboptimality bounds  We next review how to use the Bellman residual of the dynamic programming operator to bound the suboptimality of solutions found by dynamic programming  For stochastic shortest path problems  Bertsekas     pp          gives the following result  Theorem    Bertsekas   For any stochastic shortest path problem  any value function J  a greedy policy  with respect to J  and for all i  S  the following bounds hold  J i    cN   i   J   i   J  i   J i    cN  i       where c   miniS  T J i   J i    N   i  is an upper bound on the expected number of steps needed to reach the terminal state t beginning from state i and following an optimal policy  c   maxiS  T J i   J i    and N  i  is an upper bound on the expected number of steps needed to reach t beginning from state i and following policy   Some simplifications can help bring this result into focus  Leaving out the inequalities involving J and subtracting J i   we have  cN   i   J   i   J i   cN  i   Note that c    and c     Note also that the Bellman residual is    T J  J     max c  c   Thus we have   J   i   J i      T J  J    max N   i   N  i          Bertsekas and Tsitsiklis     pp        describe how to bound the expected number of stages until termination for any stochastic shortest path problem for which all policies are proper  although it requires solving an MDP of the same size as the original stochastic shortest path problem  Given a stochastic shortest path problem  consider a related infinite horizon MDP where the transition probabilities are the same but there is a cost of   for any transition to the terminal state and all other transitions incur a cost of    For this MDP  finding a policy that minimizes the expected infinite horizon cost corresponds to finding a policy that maximizes the expected number of stages it takes to reach the terminal state  The values computed by solving this MDP bound the number of stages until termination for any policy  Obviously  these bounds are finite  and the MDP is well defined  if and only if all policies are proper  If a greedy policy  with respect to value function J is not proper  then N  i  is not finite for at least one state i and the bounds of Equations    and    are not finite  We address the challenge of how to compute bounds when not all policies are proper beginning in Section         Partial observability  In addition to this bound on the suboptimality of a value function J  the inequalities J  i   J i  cN  i  in Equation   let us bound the suboptimality of a greedy policy  with respect to J  It follows that we can compute suboptimality bounds if we can compute the bounds N   i  and N  i  on the expected number of stages until termination  Unfortunately  writes Bertsekas     pp           these bounds are easily computed or approximated only in the presence of special problem structure   Patek      extends the framework of stochastic shortest path problems to the partially observable case  A partially observable MDP  POMDP  includes the same states  actions  transition probabilities and costs defined earlier  plus a finite set of observation symbols  Z  and a set of observation probabilities  where pz  j  u  denotes the probability that symbol z  Z is observed after action u  U results in a transition to state j  S  In addition to the three assumptions of a stochastic shortest path problem given in Section      a partially observable stochastic shortest path problem includes an assumption that ensures that termination of the process is perfectly recognized  Assumption    The set of observation symbols includes a special symbol  zt  Z  which is unique to transitions to the terminal state t  That is  pzt  t  u      and pzt  j  u       u  U  j  S   Bertsekas mentions just one special case in which these bounds can be easily computed  the discounted infinitehorizon case  As already pointed out  any discounted infinite horizon MDP  with discount factor   can be reduced to an equivalent stochastic shortest path problem in which  for every state and action pair  there is a probability       of making a transition to a terminal state  with the other transition probabilities normalized accordingly  It follows that the expected number  of steps until termination  from any starting state  is t    t            Letting N   i    N  i             i  S  the well known bound on the suboptimality of a value function J  which is   J   J      T J  J          is seen to be a special case of the suboptimality bound given by Equation      As is well known  a POMDP can be solved by dynamic programming if it is first transformed into an equivalent completely observable MDP over belief states  where a belief state is an  S  dimensional vector of probabilities maintained by Bayesian conditioning  Given the assumptions of the partially observable stochastic shortest path problem  Patek      shows that value iteration and policy iteration have the same convergence properties established by Bertsekas and Tsitsiklis        in the completely observable case  His analysis follows the same outline  In the special case that all policies are proper  he shows that the dynamic programming operator is an m stage contraction operator  In the general case when not all policies are proper  he proves convergence without using a contraction property   Since by definition    J   J     maxiS  J   i   J i    we finally have    J   J      T J  J    max max N   i   N  i         iS      Uniform improvability and proper policies  The convergence proofs for value iteration and policy iteration given by Bertsekas and Tsitsiklis        and Patek      for the general case when not all policies are proper are significant because they do not depend on the contraction property  But without a contraction property  they do not ensure a geometric rate of convergence or provide a way to use the Bellman residual to compute suboptimality bounds  In the rest of the paper  we show a way around this  First  we establish a condition under which a policy found by dynamic programming is guaranteed to be proper  Theorem    For any stochastic shortest path problem and any value function J for which T J  J  a greedy policy  with respect to J  defined as   i    argmin pij  u  g i  u  j  J j     i  S  uU  jS       is a proper policy and J  J  Proof  The key observation is that T J   T J  which means that application of the dynamic programming operator T can be viewed as the first application of the policy evaluation operator T in evaluating a greedy policy  with respect to J  When T J  J  we have T J  J  By the monotonicity property  it follows that every successive iteration of T monotonically improves the value function  and thus J   limn Tn J  T J   T J  J  Since J is bounded above for every state   must be proper  We call a value function J for which T J  J a uniformly improvable value function  a term used by others           Consider the subspace of uniformly improvable value functions  J    J T J  J   By the monotonicity property  this subspace is closed under the dynamic programming operator  It follows that if value iteration is started with a value function J  J   a greedy policy with respect to this value function  and a greedy policy with respect to any subsequent value function found after any number of iterations of value iteration  must be proper  We already have the same guarantee for policy iteration  The convergence proof for policy iteration given by Bertsekas and Tsitsiklis  and by Patek in the partially observable case  requires the initial policy to be proper  otherwise  the value function computed by policy evaluation is not bounded  Given an initial proper policy  it follows from the policy improvement theorem that any policy found after any number of iterations of policy iteration must be proper  since the cost of an improved policy cannot increase for any state  Indeed  this guarantee holds precisely because the value function of an initial proper policy  is uniformly improvable  note that T J   J implies T J  J   It follows that whether we use policy iteration or value iteration  we can guarantee uniform improvability by finding an initial proper policy   Different algorithms can be used to find an initial proper policy  Since the better the initial policy and value function  the sooner policy iteration or value iteration converges  extra computational effort spent trying to find an initial proper policy that is of high quality could be well spent  However  it is not difficult to find some initial proper policy  Theorem    For any stochastic shortest path problem  the uniform random policy is proper   Proof  By Assumption    there exists some proper policy   By Definition    there is a positive integer m such that after m stages  there is a probability      that following policy  leads to the terminal state  Since a uniform random policy selects an action at random based on a uniform probability distribution  it executes the same action as policy  for m consecutive stages with probability     U   m      It follows that the probability of reaching the terminal state within m stages by following the uniform random policy is greater than or equal to     U   m     which is positive  and thus the uniform random policy is proper  When the value function is uniformly improvable  we can simplify the bounds given in Theorem   and Equations    and     Note that when T J  J  we have c      From Equation    it follows that J   i   J  i   J i   where  is a greedy policy with respect to J  Thus  must be proper   This is the same result proved in Theorem     Setting aside the bounds involving J   consider the remaining bounds  J i    cN   i   J   i   J i   Subtracting J i   we have  cN   i   J   i   J i      Thus  J   i   J i    cN   i   By assumption  T J  J  and so the Bellman residual is    T J J     c  Thus we have   J   i   J i      T J  J    N   i          where N   i   as defined in Theorem    is an upper bound on the expected number of steps needed to reach the terminal state beginning from state i and following an optimal policy  By definition    J   J     maxiS  J   i   J i    and so    J   J      T J  J    N         where N    maxiS N   i  is an upper bound on the expected number of steps needed to reach the terminal state from any other state by following an optimal policy  Note that Equation    is a simplification of Equation    and Equation    is a simplification of Equation     where both simplications are possible because the value function J is uniformly improvable  The condition that J is uniformly improvable also allows the following simplification  Theorem    If value function J is uniformly improvable  then any bound on its suboptimality is also a bound on the suboptimality of a greedy policy  with respect to J  Proof  Immediate since J  i   J i  if T J  J i     I am grateful to Bruno Scherrer for the observation expressed in this theorem and the idea of the proof       Positive transition costs  In this section  we consider a special case of the stochastic shortest path problem where all transition costs incur a positive cost  except possibly for transitions into the terminal state  By Assumption    a transition from the terminal state to itself has a cost of zero  Because a transition from a nonterminal state to the terminal state occurs only once  we do not need to place any restriction on its cost  except  of course  that it is bounded   Under this condition on transition costs  we show how to compute upper bounds  denoted N  i   i  S  on the expected number of stages until termination of any policy  for which J  J  where J is uniformly improvable  We use the notation N  i  instead of N   i  because these bounds on the expected number of stages until termination apply to any policy  for which J  J  not just an optimal policy  Since J   J  J  we have N  i   N   i   i  S  and so we can use N  i  in place of N   i  in Equations    and    to bound the suboptimality of solutions found by dynamic programming  The proof strategy we adopt to establish these bounds is to show that any policy  that does not terminate within N  i  stages on average  beginning from state i  must have an expected cost J  i  greater than J i   which contradicts the assumption that J  J  The significance of this strategy is that it does not require all policies to be proper  It simply requires a uniformly improvable value function J  Theorem    For any stochastic shortest path problem for which g i  u  j      for all i  S  u  U  j  S t  and for any policy  with value function J  J  an upper bound on the mean number of steps until the terminal state is reached beginning from any state i is J i   a N  i              b where a   min g i  u  t    i  S  u  U   denotes the minimum cost of any transition into the terminal state and b   min g i  u  j    i  S  u  U  j   t  denotes the minimum cost of any other transition  Proof  For any policy   let n  i  denote the expected number of steps until the terminal state is reached beginning from state i and following policy   Because a is the minimum cost of any transition into the terminal state and b is the minimum cost of any other transition  a   b n  i      is the minimum cost of any sequence of n  i  transitions that ends in the terminal state  therefore  J  i   a   b n  i       Now if N  i  is not an upper bound on the expected number of stages until termination for some state i  there must be some policy  for which both n  i    N  i  and J  i   J i   But since N  i     J i   a  b      then n  i    N  i  implies that J  i    J i   which contradicts the assumption that J  J  It follows that n  i   N  i   i  S   Figure    Gridworld navigation example          Note that the    in Equation    counts the transition into the terminal state  This approach to computing suboptimality bounds works best if transition costs are uniform as well as positive  If transition costs are positive but non uniform  the bounds are still valid  but potentially looser  In this case  the bounds could be improved by considering the minimum expected transition cost after an action  instead of simply the minimum transition cost  Example For illustration  consider the small gridworld navigation problem shown in Figure    Russell and Norvig      describe a completely observable version of this gridworld and Parr and Russell     describe a partially observable version  To allow reference to individual states  we number each cell of the grid from   to     Figure   shows all of the numbers except two  the    state is numbered   and the   state is numbered    Any action taken in either the    or the   state results in a deterministic transition to a terminal state  which is not shown  and a reward of    or   respectively  In all other states  any of the four possible navigation actions  with one corresponding to each direction of the compass  incurs a negative reward of       which is equivalent to a positive cost   For convenience  we keep the reward maximization framework used by Russell and Norvig      and Parr and Russell      Note that it is easily transformed to the cost minimization framework of a stochastic shortest path problem   We use the same transition and observation probabilities given by Russell and Norvig      and Parr and Russell      For this example  it is not the case that all policies are proper  One reason for adopting this simple example is that it is the same example used by Russell and Norvig      to illustrate how to compute suboptimality bounds for solutions found by dynamic programming for discounted infinite horizon MDPs  However  this example is most naturally formalized in an undiscounted reward maximization framework that is equivalent to a stochastic shortest path problem  as noted by both Russell and Norvig      and Parr and Russell      Although this gridworld example is very simple  it helps to illustrate several aspects of our approach  We implemented our approach to computing suboptimality bounds in exact value iteration and policy iteration algorithms for completely observable and partially observable stochastic shortest path problems  Table   shows the results for the first    iterations of the algorithms  starting from the   Completely observable                                J                                                                                      Value iteration m resid                                                                                                                                                        error                                                                               J                                  Partially observable Policy iteration m resid                                                        error                            J                                                                                       Value iteration m resid                                                                                                                                                        error                                                                                J                                                                                  Policy iteration m resid                                                                                                                                                        error                                                                            Table    Error bounds and related statistics for solutions found by exact value iteration and policy iteration in solving the gridworld problem in both its completely and partially observable forms  Only results for the first    iterations are shown  In the completely observable case  policy iteration converges after   iterations  For each iteration  J is the smallest value of any state or belief state  that is  J   minsS J s  in the completely observable case  and J   inf b J b  in the partially observable case  m       J           is an upper bound on the expected number of steps until termination  beginning from any state  resid  is   T J  J    the Bellman residual  and error   m resid  is the suboptimality bound  Iteration   is for the value function of the uniform random policy              J    N                                                    J    N                                                    J    N                                                    J    N                                                  J    N                                                    J    N                                                    J                                   N                              J    N                                                    J    N                                                     J                               N                              J     N                                                          Table    State values J i  and upper bounds N  i  on the expected number of steps until termination for each state i of the completely observable gridworld when solved by policy iteration  beginning from the uniform random policy   value function of the uniform random policy  The bounds depend not only on the size of the Bellman residual  but on the upper bound m   maxiS N  i  on the expected number of stages before termination  which in turn depends on the current value function  In the first couple iterations  the error bounds are loose because the value function is still rather poor and m is relatively large  As the value function improves over successive iterations  the bounds improve due to a reduction in the expected number of stages until termination  as well as a decrease in the Bellman residual  The bounds shown in Table   are based on Equation     which means that they are independent of the starting state  But Equation     upon which Equation    is based  lets us compute a separate suboptimality bound for each state  These bounds are proportional to the expected number of transitions needed to reach the goal from each state  In this respect  they are more realistic than the well known suboptimality bound for discounted infinite horizon MDPs  which is   J  J      T J J        The latter assumes that the expected number of transitions until termination is          regardless of the starting state and its distance from the goal  A second well known disadvantage of the suboptimality bound for discounted infinite horizon MDPs is that it is looser  and converges more slowly  the closer the discount factor is to    Interestingly  there is something analogous for stochastic shortest path problems  The suboptimality bound of Equation    is looser  and converges more slowly  the greater the expected number of transitions until termination starting from the farthest state from the  goal  that is  the greater maxiS N  i   An advantage of using Equation    to compute a separate suboptimality bound for each state is that many or most bounds can be relatively tight and converge quickly  even if there is some state that is very far from the goal state with a suboptimality bound that converges more slowly  For the gridworld example in its completely observable form  Table   shows the value J i  and upper bound N  i  for each state i  for each iteration of policy iteration  The smaller N  i  is  the tighter the suboptimality bound  Because the values of states close to the goal tend to converge faster than the values of states farther from the goal  as illustrated in Table    their suboptimality bounds also converge faster  Table   does show one anomaly  The upper bound N      which is for the   state  is unrealistically high because it ignores the fact that taking any action in this state causes an immediate transition to the terminal state  Taking into account the transition probabilities for this problem  we could set N     equal to    Thus we ignore it when computing the bound of Equation     The partially observable version of this problem does not have a simple solution  After    iterations  the number of vectors used to represent the value function is more than          This underscores that  in the partially observable case  suboptimality bounds are especially useful  The complexity of each iteration of policy or value iteration can grow exponentially in the number of iterations  and it is usually not possible to compute an optimal solution       General case  We next consider the general case in which there are no restrictions on transition costs and not all policies are proper  In this case  it is no longer possible to use the minimum cost of a transition to bound the average number of stages until termination of a policy  for which J  J  since the minimum cost could be zero or negative  The analysis needs to be more complex  Although we are not yet able to describe a good approach to computing suboptimality bounds in the general case  we present some preliminary results in this direction  First we show that the dynamic programming operator for stochastic shortest path problems behaves like an m stage contraction operator  even in the general case  We also show that it is possible to compute suboptimality bounds in the general case  although the bounds we describe are much too loose to be practical  Finally  we discuss some implications of these results       Contraction property  We begin by showing that the dynamic programming operator behaves like an m stage contraction when applied to a value function J that is uniformly improvable  This result is related to Theorem    which shows that a greedy policy with respect to a uniformly improvable value function is a proper policy  For any proper policy  there exists  by Definition    a finite positive integer m such that there is a positive probability of reaching the terminal state after at most m stages when following this policy  regardless of the start state  In the special case of positive transition costs  Theorem   shows how to compute such an m  in that case  m   maxiS N  i   We now show how to compute such an m for the general case in which there are no restrictions on transition costs  Consider a finite horizon MDP that has the same state set  action set  transition probabilities and transition costs as the original stochastic shortest path problem  but where the objective is to compute the minimum k stage cost  Jk  i   for each starting state i  of any policy that does not reach the terminal state within k stages  In solving this finite horizon MDP  we must partition the state set into two subsets that need to be treated separately at each stage k  For one set of states  denoted Tk   termination within k stages is inevitable under any policy  For the remaining states  S Tk   there is some policy under which the probability of termination within k stages is zero  It is only for the second set of states that we solve the finite horizon MDP for k stages  Note that Tk is a proper subset of S for all k  unless all policies are proper  Figure   gives the pseudocode for an algorithm that solves this finite horizon MDP while distinguishing between these two sets of states  The horizon is determined dynamically                                  k         k is stage    i  S   Jk  i        a i         initialization    Tk     t     termination within k stages is inevitable    while  i  S Tk such that Jk  i   J i   k    k     Tk    Tk  for each   i  S Tk    a i      u  U   jTk pij  u          a i  is set of actions that dont lead to a state in Tk    if  a i      then    all actions lead to termination    Tk    Tk   i  else    compute minimal k stage cost for this state     Jk  i     minua i  j pij  u  g i  u  j    Jk   j    Figure    Algorithm for solving finite horizon MDP in order to compute a bound m on the number of stages before termination with some positive probability of any policy  for which J  J   It is the smallest stage k for which Jk  i    a   J i   i  S Tk   where a   min g i  u  t    i  S  u  U   denotes the minimum cost of any transition to the terminal state and J is a uniformly improvable value function for the original stochastic shortest path problem  We know the horizon must be finite because a and J i  are finite  and  assuming there exists a policy under which termination can be delayed indefinitely beginning from state i   Jk  i  goes to infinity as k goes to infinity  by Assumption    Since Jk  i  is the minimum cost of any policy that does not reach the terminal state within k stages beginning from state i  Jk  i  a is a lower bound on the cost of any policy that reaches the terminal state in k     stages beginning from state i  The key observation is that for any policy  that does not reach the terminal state within k     stages with probability greater than zero beginning from state i  we have J  i   Jk  i    a   J i   which contradicts the assumption that J  J  Therefore  we can set m equal to one plus the smallest k for which Jk  i    a   J i   i  S Tk   and we have the following result  Theorem    For any stochastic shortest path problem and for any initial value function J for which T J  J  the dynamic programming operator T behaves like an m stage contraction operator  We say that the dynamic programming operator behaves like an m stage contraction operator  not that it is one  because Bertsekas and Tsitsiklis        give an example that shows that the dynamic programming operator for stochastic shortest path problems is not a contraction with respect to any norm  unless all policies are proper   The example is briefly reviewed in Section       The distinction between being an m stage contraction and behaving like one is necessary if we adopt the definition that an operator is an m stage contraction if and only if it satisfies the condition expressed by Equation   for all bounded value functions      But Theorem   only claims that the dynamic programming operator satisfies the m stage contraction property expressed by Equation   for the closed subspace of uniformly improvable value functions  Thus we could also   say that the dynamic programming operator for stochastic shortest path problems is an m stage contraction operator in the subspace of uniformly improvable value functions  but not in the space of all bounded value functions  The question of whether the dynamic programming operator is  or behaves like  a contraction operator is closely related to the possibility of bounding the suboptimality of solutions found by dynamic programming  We can use Equations    and    to compute suboptimality bounds only if we can bound the average number of stages until the terminal state is reached  and we can bound the average number of stages until the terminal state is reached only if there is some finite m such that policy execution terminates within m stages with positive probability  Conversely  the existence of some finite m such that policy execution terminates within m stages with positive probability implies that we can bound the average number of stages until termination of any policy  But a bound on the number of stages of policy execution required to reach the terminal state with positive probability is not also a bound on the average number of stages it takes to reach the terminal state  The second bound is at least as great as the first  but it is usually greater  Given that policy execution terminates within m stages with positive probability  where m is computed as above  we can describe a simple approach to bounding the average number of stages it takes to reach the terminal state  Let pt   min pit  u        i  S  u  U   denote the smallest non zero probability of a transition into the terminal state t and let pn   min pij  u        i  S  j  S t  u  U   denote the smallest non zero probability of any other transition  It follows that m   pm   pt     is a lower bound n on the probability of process termination within m stages  Thus an upper bound on the expected  number of stages until termination is given by m t       m  t   m m   If we set N  i    m m   i  S  we can use Equations    and    to compute suboptimality bounds  But almost surely  these bounds will be much too loose to be of any practical value  Their derivation does show that it is possible to use the Bellman residual to compute suboptimality bounds  however  and future work may lead to a more sophisticated approach that computes tighter bounds       Implications  Regardless of whether Theorem   supports a practical approach to computing suboptimality bounds in the general case  it has some important theoretical implications  Among them  it points to a stronger convergence proof for policy iteration than the proofs given by Bertsekas and Tsitsiklis        in the completely observable case  and by Patek      in the partially observable case  Because policy iteration must start with an initial proper policy  the dynamic programming operator used in the policy improvement step behaves like an m stage contraction  and thus  standard contraction theory can be invoked to establish uniform convergence  The m stage contraction behavior also establishes that policy iteration converges at a geometric rate  In addition  it establishes that value iteration converges at a geometric rate when given an initial value function that is uniformly improvable  By contrast  the convergence proofs of Bertakas and Tsitsiklis     and Patek      do not establish that policy iteration and value iteration converge at a geometric rate  unless all policies are proper  The significance of ensuring a geometric rate of convergence is that for any       it is possible to bound the number of iterations of value  or policy  iteration needed to find an  optimal policy  In the partially observable case  this result is especially noteworthy  The problem of finding an  optimal policy for a discounted infinite horizon POMDP is well known to be decidable  by the contraction property of the dynamic programming operator in the case of discounting  But for undiscounted infinite horizon POMDPs  the problem of finding an  optimal policy has been shown to be undecidable  in general      However  our results imply that for an important special case of undiscounted infinite horizon POMDPs  the partially observable stochastic shortest path problem  the problem of finding an  optimal policy is decidable  To help make this result seem more plausible  note that the undecidability of  approximation for undiscounted infinite horizon POMDPs is proved by reduction from the problem of maximizing the probability of reaching a goal state  where there is a reward of   for reaching the goal state  a reward of   for not reaching the goal state  and the goal state cannot be reached with probability        Obviously  this problem cannot be reduced to a partially observable stochastic shortest path problem  On the other hand  the optimization problem for discounted infinite horizon POMDPs  which is also undecidable      can be reduced to a partially observable stochastic shortest path problem  But it does not imply the undecidability of  approximation  since  approximation is decidable for discounted infinitehorizon POMDPs  Two key assumptions of the stochastic shortest path problem play a role in making  approximation decidable in the partially observable case   i  a proper policy exists  and  ii  the expected cost of policy execution beginning from any state from which the terminal state is not reached with probability   is infinite  Combined with our observation that J  J for any policy  found by dynamic programming when the initial value function J is uniformly improvable  we have been able to establish the m stage contraction behavior of the dynamic programming operator in the space of uniformly improvable value functions  In turn  it allows use of the Bellman residual of the dynamic programming operator to compute suboptimality bounds       Conclusion  For stochastic shortest path problems  we have shown that under the condition that the initial value function is uniformly improvable  a greedy policy with respect to any value function found by value iteration is proper  We have also shown how to bound the expected number of stages before the terminal state is reached when following a proper policy found by either value iteration or policy iteration  which in turn lets us use the Bellman residual of the dynamic programming operator to compute suboptimality bounds  The key formula used to compute suboptimality bounds is due to Bertsekas      But it has not been clear how it could be applied to the case where not all policies are proper  Our contribution is to show that it can be used to compute suboptimality bounds even when not all policies are proper  as long as the initial value function is uniformly improvable  In the special case of positive transition costs  especially when the transition costs are uniform or nearly uniform  we showed that useful suboptimality bounds can be easily computed  In the general case in which transition costs can be zero or negative  we showed that suboptimality bounds are possible  but without describing a practical approach to computing bounds that are tight enough to be useful  We leave this problem for future work       O  Madani  S  Hanks  and A  Condon  On the undecidability of probabilistic planning and related stochastic optimization problems  Artificial Intelligence                       M  Maskery  V  Krishnamurthy  and C  ORegan  Decentralized algorithms for netcentric force protection against antiship missiles  IEEE Transactions on Aerospace and Electronic Systems                              R  Parr and S  Russell  Approximating optimal policies for partially observable stochastic domains  In Proc  of the   th Int  Joint Conf  on Artificial Intelligence  IJCAI      pages                      S  Patek  On terminating Markov decision processes with a risk averse objective function  Automatica                             S  Patek  Partially observed stochastic shortest path problems with approximate solution by neurodynamic programming  IEEE Transactions on Systems  Man  and Cybernetics  Part A  Systems and Humans                           S  Patek and D  Bertsekas  Stochastic shortest path games  SIAM Journal of Control and Optimization                           S  Russell and P  Norvig  Artificial Intelligence  A Modern Approach  Prentice Hall    edition         
 Coordination of distributed agents is required for problems arising in many areas  including multi robot systems  networking and e commerce  As a formal framework for such problems  we use the decentralized partially observable Markov decision process  DECPOMDP   Though much work has been done on optimal dynamic programming algorithms for the single agent version of the problem  optimal algorithms for the multiagent case have been elusive  The main contribution of this paper is an optimal policy iteration algorithm for solving DEC POMDPs  The algorithm uses stochastic finite state controllers to represent policies  The solution can include a correlation device  which allows agents to correlate their actions without communicating  This approach alternates between expanding the controller and performing value preserving transformations  which modify the controller without sacrificing value  We present two efficient value preserving transformations  one can reduce the size of the controller and the other can improve its value while keeping the size fixed  Empirical results demonstrate the usefulness of value preserving transformations in increasing value while keeping controller size to a minimum  To broaden the applicability of the approach  we also present a heuristic version of the policy iteration algorithm  which sacrifices convergence to optimality  This algorithm further reduces the size of the controllers at each step by assuming that probability distributions over the other agents actions are known  While this assumption may not hold in general  it helps produce higher quality solutions in our test problems      Introduction Markov decision processes  MDPs  provide a useful framework for solving problems of sequential decision making under uncertainty  In some settings  agents must base their decisions on partial information about the system state  In that case  it is often better to use the more general framework of partially observable Markov decision processes  POMDPs   Even more general are problems in which a team of decision makers  each with its own c      AI Access Foundation and Morgan Kaufmann Publishers  All rights reserved    Bernstein  Amato  Hansen    Zilberstein  local observations  must act together  Domains in which these types of problems arise include networking  multi robot coordination  e commerce  and space exploration systems  The decentralized partially observable Markov decision process  DEC POMDP  provides an effective framework to model such problems  Though this model has been recognized for decades  Witsenhausen         there has been little work on provably optimal algorithms for it  On the other hand  POMDPs have been studied extensively over the past few decades  Smallwood   Sondik        Simmons   Koenig        Cassandra  Littman    Zhang        Hansen      a  Bonet   Geffner        Poupart   Boutilier        Feng   Zilberstein        Smith   Simmons        Smith  Thompson    Wettergreen         It is well known that a POMDP can be reformulated as an equivalent belief state MDP  A belief state MDP cannot be solved in a straightforward way using MDP methods because it has a continuous state space  However  Smallwood and Sondik showed how to implement value iteration by exploiting the piecewise linearity and convexity of the value function  This work opened the door for many algorithms  including approximate approaches and policy iteration algorithms in which the policy is represented using a finite state controller  Extending dynamic programming for POMDPs to the multiagent case is not straightforward  For one thing  it is not clear how to define a belief state and consequently form a belief state MDP  With multiple agents  each agent has uncertainty about the observations and beliefs of the other agents  Furthermore  the finite horizon DEC POMDP problem with just two agents is complete for a higher complexity class than the single agent version  Bernstein  Givan  Immerman    Zilberstein         indicating that these are fundamentally different problems  In this paper  we describe an extension of the policy iteration algorithm for single agent POMDPs to the multiagent case  As in the single agent case  our algorithm converges in the limit  and thus serves as the first nontrivial optimal algorithm for infinite horizon DEC POMDPs  A few optimal approaches  Hansen  Bernstein    Zilberstein        Szer  Charpillet    Zilberstein        and several approximate algorithms have been developed for finite horizon DEC POMDPs  Peshkin  Kim  Meuleau    Kaelbling        Nair  Pynadath  Yokoo  Tambe    Marsella        Emery Montemerlo  Gordon  Schnieder    Thrun        Seuken   Zilberstein         but only locally optimal algorithms have been proposed for the infinite horizon case  Bernstein  Hansen    Zilberstein        Szer   Charpillet        Amato  Bernstein    Zilberstein         In our algorithmic framework  policies are represented using stochastic finite state controllers  A simple way to implement this is to give each agent its own local controller  In this case  the agents policies are all independent  A more general class of policies includes those which allow agents to share a common source of randomness without sharing observations  We define this class formally  using a shared source of randomness called a correlation device  The use of correlated stochastic policies in the DEC POMDP context is novel  The importance of correlation has been recognized in the game theory community  Aumann         but there has been little work on algorithms for finding correlated policies  Each iteration of the algorithm consists of two phases  These are exhaustive backups  which add nodes to the controller  and value preserving transformations  which change the controller without sacrificing value  We first provide a novel exposition of existing single     Policy Iteration for DEC POMDPs  agent algorithms using this two phase view  and then we go on to describe the multiagent extension  There are many possibilities for value preserving transformations  In this paper  we describe two different types  both of which can be performed efficiently using linear programming  The first type allows us to remove nodes from the controller  and the second allows us to improve the value of the controller while keeping its size fixed  Our empirical results demonstrate the usefulness of value preserving transformations in obtaining high values while keeping controller size to a minimum  We note that this work serves to unify and generalize previous work on dynamic programming for DEC POMDPs  The first algorithm for the finite horizon case  Hansen et al         can be extended to the infinite horizon case and viewed as interleaving exhaustive backups and controller reductions  The bounded policy iteration algorithm for DEC POMDPs  Bernstein et al          which extends a POMDP algorithm proposed by Poupart and Boutilier         can be viewed through the lens of our framework as repeated application of a specific value preserving transformation  Because the optimal algorithm will not usually be able to return an optimal solution in practice  we also introduce a heuristic version of the policy iteration algorithm  This approach makes use of initial state information to focus policy search and further reduces controller size at each step  To accomplish this  a forward search from the initial state distribution is used to construct a set of belief points an agent would visit assuming the other agents use given fixed policies  This search is conducted for each agent and then policy iteration takes place while using the belief points to guide the removal of controller nodes  The assumption that other agents use fixed policies causes the algorithm to no longer be optimal  but it performs well in practice  We show that more concise and higher valued solutions can be produced compared to the optimal method before resources are exhausted  The remainder of the paper is organized as follows  Section   introduces the formal models of sequential decision making  Section   contains a novel presentation of existing dynamic programming algorithms for POMDPs  In section    we present an extension of policy iteration for POMDPs to the DEC POMDP case  along with a convergence proof  We discuss the heuristic version of policy iteration in section    followed by experiments using policy iteration and heuristic policy iteration in section    Finally  section   contains the conclusion and a discussion of possible future work      Formal Model of Distributed Decision Making We begin with a description of the formal framework upon which our work is based  This framework extends the well known Markov decision process to allow for distributed policy execution  We also define an optimal solution for this model and discuss two different representations for these solutions      Decentralized POMDPs A decentralized partially observable Markov decision process  DEC POMDP  is defined for  T  R      Oi  where mally as a tuple hI  S  A   I is a finite set of agents       Bernstein  Amato  Hansen    Zilberstein  Agent Agent a  s  r System  a   Agent a  System  o  r System  o   r  a   o   r Agent   a    b    c   Figure     a  Markov decision process   b  Partially observable Markov decision process   c  Decentralized partially observable Markov decision process with two agents    S is a finite set of states  with distinguished initial state s        iI Ai is a set of joint actions  where Ai is the set of actions for agent i   A    S is the state transition function  defining the distributions of states  T   SA that result from starting in a given state and each agent performing an action       is the reward function for the set of agents for each set of joint actions  R   S A and each state      iI i is a set of joint observations  where i contains observations for agent i       S     is an observation function  defining the distributions of observations  O A for the set of agents that result from each agent performing an action and ending in a given state  The special case of a DEC POMDP in which there is only one agent is called a partially observable Markov decision process  POMDP   In this paper  we consider the case in which the process unfolds over an infinite sequence of stages  At each stage  all agents simultaneously select an action  and each receives the global reward based on the reward function and a local observation based on the observation function  Thus  the transitions  rewards and observations depend on the actions of all agents  but each agent must act based only on local observations  This is illustrated in Figure    The objective of the agents is to maximize the expected discounted sum of rewards that are received  thus it is a cooperative framework  We denote the discount factor  and require that          In a DEC POMDP  the decisions of each agent affect all the agents in the domain  but due to the decentralized nature of the model each agent must choose actions based solely on local information  Because each agent receives a separate observation that does not usually provide sufficient information to efficiently reason about the other agents  solving a DECPOMDP optimally becomes very difficult  For example  each agent may receive a different      Policy Iteration for DEC POMDPs   a    b   Figure    A set of horizon three policy trees  a  and two node stochastic controllers  b  for a two agent DEC POMDP   piece of information that does not allow a common state estimate or any estimate of the other agents decisions to be calculated  These single estimates are crucial in single agent problems  as they allow the agents history to be summarize concisely  but they are not generally available in DEC POMDPs  This is seen in the complexity of the finite horizon problem with at least two agents  which is NEXP complete  Bernstein et al         and thus in practice may require double exponential time  Like the infinite horizon POMDP  optimally solving an infinite horizon DEC POMDP is undecidable as it may require infinite resources  but our method is able to provide a solution within   of the optimal with finite time and memory  Nevertheless  introducing multiple decentralized agents causes a DECPOMDP to be significantly more difficult than a single agent POMDP      Solution Representations A local policy for an agent is a mapping from local action and observation histories to actions while a joint policy is a set of policies  one for each agent in the problem  As mentioned above  an optimal solution for a DEC POMDP is the joint policy that maximizes the expected sum of rewards that are received over the finite or infinite steps of the problem  In infinite horizon problems  the rewards are discounted to maintain a finite sum  Thus  an optimal solution is a joint policy that provides the highest value starting at the given initial state of the problem  For finite horizon problems  local policies can be represented using a policy tree as seen in Figure  a  Actions are represented by the arrows or stop figures  where each agent can move in the given direction or stay where it is  and observations are labeled wl and wr for seeing a wall on the left or the right respectively  Using this representation  an agent takes the action defined at the root node and then after seeing an observation  chooses the next action that is defined by the respective branch  This continues until the action at a leaf node is executed  For example  agent   would first move left and then if a wall is seen on the right  the agent would move left again  If a wall is now seen on the left  the agent does not move on the final step  A policy tree is a record of the the entire local history for an agent up to some fixed horizon and because each tree is independent of the others it can      Bernstein  Amato  Hansen    Zilberstein  be executed in a decentralized manner  While this representation is useful for finite horizon problems  infinite horizon problems would require trees of infinite height  Another option used in this paper is to condition action selection on some internal memory state  These solutions can be represented as a set of local finite state controllers  seen in Figure  b   The controllers operate in a very similar way to the policy trees in that there is a designated initial node and following the action selection at that node  the controller transitions to the next node depending on the observation seen  This continues for the infinite steps of the problem  Throughout this paper  controller states will be referred to as nodes to help distinguish them from system states  An infinite number of nodes may be required to define an optimal infinite horizon DECPOMDP policy  but we will discuss a way to produce solutions within   of the optimal with a fixed number of nodes  While deterministic action selection and node transitions are sufficient to define this   optimal policy  when memory is limited stochastic action selection and node transition may be beneficial  A simple example illustrating this for POMDPs is given by Singh         which can be easily extended to DEC POMDPs  Intuitively  randomness can help an agent to break out of costly loops that result from forgetfulness  A formal description of stochastic controllers for POMDPs and DEC POMDPs is given in sections       and       respectively  but an example can be seen in Figure  b  Agent   begins at node   and moves up with probability      and stays in place with probability       If the agent stayed in place and a wall was then seen on the left  observation wl   on the next step  the controller would transition to node   and the agent would use the same distribution of actions again  If a wall was seen on the right instead  observation wr   there is a      probability that the controller will transition back to node   and a      probability that the controller will transition to node   for the next step  The finite state controller allows an infinite horizon policy to be represented compactly by remembering some aspects of the agents history without representing the entire local history      Centralized Dynamic Programming In this section  we cover the main concepts involved in dynamic programming for the single agent case  This will provide a foundation for the multiagent dynamic programming algorithm described in the following section      Value Iteration for POMDPs Value iteration can be used to solve POMDPs optimally  This algorithm is more complicated than its MDP counterpart  and does not have efficiency guarantees  However  in practice it can provide significant leverage in solving POMDPs  We begin by explaining how every POMDP has an equivalent MDP with a continuous state space  Next  we describe how the value functions for this MDP have special structure that can be exploited  These ideas are central to the value iteration algorithm        Belief State MDPs A convenient way to summarize the observation history of an agent in a POMDP is through a belief state  which is a distribution over system states  As it receives observations  the      Policy Iteration for DEC POMDPs  agent can update its belief state and then remove its observations from memory  Let b denote a belief state  and let b s  represent the probability assigned to state s by b  If an agent chooses action a from belief state b and subsequently observes o  each component of the successor belief state obeys the equation P P  o a  s    sS P  s   s  a b s      b  s       P  o b  a  where    P  o b  a     X       P  o a  s    s  S  X     P  s  s  a b s     sS  Note that this is a simple application of Bayes rule  It was shown by Astrom        that a belief state constitutes a sufficient statistic for the agents observation history  and it is possible to define an MDP over belief states as follows  A belief state MDP is a tuple h  A  T  Ri  where   is the set of distributions over S   A is the set of actions  same as before    T  b  a  b    is the transition function  defined as X T  b  a  b      P  b   b  a  o P  o b  a   oO   R b  a  is a reward function  defined as R b  a     X  b s R s  a    sS  When combined with belief state updating  an optimal solution to this MDP can be used as an optimal solution to the POMDP from which it was constructed  However  since the belief state MDP has a continuous   S  dimensional state space  traditional MDP techniques are not immediately applicable  Fortunately  dynamic programming can be used to find a solution to the belief state MDP  The key result in making dynamic programming practical was proved by Smallwood and Sondik         who showed that the Bellman operator preserves piecewise linearity and convexity of a value function  Starting with a piecewise linear and convex representation of V t   the value function V t   is piecewise linear and convex  and can be computed in finite time  To represent a piecewise linear and convex value function  one need only store the value of each facet for each system state  Denoting the set of facets   we can store     S dimensional vectors of real values PFor any single vector      we can define its value at the belief state b with V  b      sS b s  s   Thus  to go from a set of vectors to the value of a belief state  we use the equation X V  b    max b s  s     sS       Bernstein  Amato  Hansen    Zilberstein  s   s   s   a   s   b   Figure    A piecewise linear and convex value function for a POMDP with two states  a  and a non minimal representation of a piecewise linear and convex value function for a POMDP  b    Figure  a shows a piecewise linear and convex value function for a POMDP with two states  Smallwood and Sondik proved that the optimal value function for a finite horizon POMDP is piecewise linear and convex  The optimal value function for an infinite horizon POMDP is convex  but may not be piecewise linear  However  it can be approximated arbitrarily closely by a piecewise linear and convex value function  and the value iteration algorithm constructs closer and closer approximations  as we shall see        Pruning Vectors Every piecewise linear and convex value function has a minimal set of vectors  that represents it  Of course  it is possible to use a non minimal set to represent the same function  This is illustrated in Figure  b  Note that the removal of certain vectors does not change the value of any belief state  Vectors such as these are not necessary to keep in memory  Formally  we say that a vector  is dominated if for all belief states b  there is a vector       such that V  b     V  b     Because dominated vectors are not necessary  it would be useful to have a method for removing them  This task is often called pruning  and has an efficient algorithm based on linear programming  For a given vector   the linear program in Table   determines whether  is dominated  If variables can be found to make   positive  then adding  to the set improves the value function at some belief state  If not  then  is dominated  This gives rise to a simple algorithm for pruning a set of vectors  to obtain a minimal set   The algorithm loops through   removes each vector     and solves the linear program using  and      If  is not dominated  then it is returned to   It turns out that there is an equivalent way to characterize dominance that can be useful  Recall that for a vector to be dominated  there does not have to be a single vector that has value at least as high for all states  It is sufficient for there to exist a set of vectors such that for all belief states  one of the vectors in the set has value at least as high as the vector in question       Policy Iteration for DEC POMDPs  Variables     b s  Objective  Maximize    Improvement constraints  X    b s  s        s  X  b s  s   s  Probability constraints  X  s  b s        b s      s  Table    The linear program for testing whether a vector  is dominated        convex combination    s   s   Figure    The dual interpretation of dominance  Vector   is dominated at all belief states by either   or     This is equivalent to the existence of a convex combination of   and   which dominates   for all belief states   It can be shown that such a set exists if and only if there is some convex combination of vectors that has value at least as high as the vector in question for all states  This is shown graphically in Figure    If we take the dual of the linear program for dominance given in the previous section  we get a linear program for which the solution is a vector of probabilities for the convex combination  This dual view of dominance was first used in a POMDP context by Poupart and Boutilier         and is useful for policy iteration  as will be explained later        Dynamic Programming Update In this section  we describe how to implement a dynamic programming update to go from a value function Vt to a value function Vt     In terms of implementation  our aim is to take a minimal set of vectors t that represents Vt and produce a minimal set of vectors t   that represents Vt          Bernstein  Amato  Hansen    Zilberstein  Each vector that could potentially be included in t   represents the value of an action a and assignment of vectors in t to observations  A combination of an action and transition rule will hereafter be called a one step policy  The value vector for a one step policy can be determined by considering the action taken  the resulting state transitioned to and observation seen and the value of the assigned vector at step t  This is given via the equation X it    s    R s   i      P  s   s   i  P  o  i   s   t  i o   s     s   o  where i is the index of the vector   i  is its action  and   i  o  is the index of the vector in t to which to transition upon receiving observation o and  is the discount factor  More details on the derivation and use of this formula are provided by Zhang and Zhang         There are  A  t     possible one step policies  A simple way to construct t   is to evaluate all possible one step policies and then apply a pruning algorithm such as Larks method  Lark III         Evaluating the entire set of one step policies will hereafter be called performing an exhaustive backup  It turns out that there are ways to perform a dynamic programming update without first performing an exhaustive backup  Below we describe two approaches to doing this  The first approach uses the fact that it is simple to find the optimal vector for any particular belief state  For a belief state b  an optimal action can be determined via the equation     X P  o b  a V t  T  b a  o        argmaxaA R b  a     o  For each observation o  there is a subsequent belief state  which can be computed using Bayes rule  To get an optimal transition rule    o   we take the optimal vector for the belief state corresponding to o  Since the backed up value function has finitely many vectors  there must be a finite set of belief states for which backups must be performed  Algorithms which identify these belief states include Smallwood and Sondiks one pass algorithm         Chengs linear support and relaxed region algorithms  Cheng         and Kaelbling  Cassandra and Littmans Witness algorithm         The second approach is based on generating and pruning sets of vectors  Instead of generating all vectors and then pruning  these techniques attempt to prune during the generation phase  The first algorithm along these lines was the incremental pruning algorithm  Cassandra et al          Recently  improvements have been made to this approach  Zhang   Lee        Feng   Zilberstein               It should be noted that there are theoretical complexity barriers for DP updates  Littman et al         showed that under certain widely believed complexity theoretic assumptions  there is no algorithm for performing a DP update that is worst case polynomial in all the quantities involved  Despite this fact  dynamic programming updates have been successfully implemented as part of the value iteration and policy iteration algorithms  which will be described in the subsequent sections       Policy Iteration for DEC POMDPs        Value Iteration To implement value iteration  we simply start with an arbitrary piecewise linear and convex value function  and proceed to perform DP updates  This corresponds to value iteration in the equivalent belief state MDP  and thus converges to an   optimal value function after a finite number of iterations  Value iteration returns a value function  but a policy is needed for execution  As in the MDP case  we can use one step lookahead  using the equation     X X  b    argmaxaA R s  a b s     P  o b  a V    b  o  a     sS  o  where   b  o  a  is the belief state resulting from starting in belief state b  taking action a  and receiving observation o  We note that a state estimator must be used as well to track the belief state  Using the fact that each vector corresponds to a one step policy  we can extract a policy from the value of the vectors    X  b     argmaxk b s k  s  s  While the size of the resulting set of dominant vectors may remain exponential  in many cases it is much smaller  This can significantly simplify computation  As in the completely observable case  the Bellman residual provides a bound on the distance to optimality  Recall that the Bellman residual is the maximum distance across all belief states between the value functions of successive iterations  It is possible to find the maximum distance between two piecewise linear and convex functions in polynomial time with an algorithm that uses linear programming  Littman et al              Policy Iteration for POMDPs With value iteration  a POMDP is viewed as a belief state MDP  and a policy is a mapping from belief states to actions  An early policy iteration algorithm developed by Sondik used this policy representation  Sondik         but it was very complicated and did not meet with success in practice  We shall describe a different approach that has performed better on test problems  With this approach  a policy is represented as a finite state controller        Finite State Controllers Using a finite state controller  an agent has a finite number of internal states  Its actions are based only on its internal state  and transitions between internal states occur when observations are received  Internal states provide agents with a kind of memory  which can be crucial for difficult POMDPs  Of course  an agents memory is limited by the number of internal states it possesses  In general  an agent cannot remember its entire history of observations  as this would require infinitely many internal states  An example of a finitestate controller can be seen by considering only one agents controller in Figure  b  The operation of a single controller is the same as that for each agent in the decentralized case  We formally define a controller as a tuple hQ    A    i  where      Bernstein  Amato  Hansen    Zilberstein   Q is a finite set of controller nodes    is a set of inputs  taken to be the observations of the POMDP   A is a set of outputs  taken to be the actions of the POMDP      Q  A is an action selection function  defining the distribution of actions selected at each node      Q  A    Q is a transition function  defining the distribution of resulting nodes for each initial node and action taken  For each state and starting node of the controller  there is an expected discounted sum of rewards over the infinite horizon  It can be computed using the following system of linear equations  one for each s  S and q  Q    X X V  s  q    P  a q  R s  a     P  o  s   s  a P  q    q  a  o V  s    q       s   o q    a  Where P  a q  is the probability action a will be taken in node q and P  q    q  a  o  is the probability the controller will transition to node q   from node q after action a was taken and o was observed  We sometimes refer to the value of the controller at a belief state  For a belief state b  this is defined as X V  b    max b s V  s  q   q  s  Thus  it is assumed that  given an initial state distribution  the controller is started in the node which maximizes value from that distribution  Once execution has begun  however  there is no belief state updating  In fact  it is possible for the agent to encounter the same belief state twice and be in a different internal state each time        Algorithmic Framework We will describe the policy iteration algorithm in abstract terms  focusing on the key components necessary for convergence  In subsequent sections  we present different possibilities for implementation  Policy iteration takes as input an arbitrary finite state controller  The first phase of an iteration consists of evaluating the controller  as described above  Recall that value iteration was initialized with an arbitrary piecewise linear and convex value function  represented by a set of vectors  In policy iteration  the piecewise linear and convex value function arises out of evaluation of the controller  Each controller node has a value when paired with each state  Thus  each node has a corresponding vector and thus a linear value function over belief state space  Choosing the best node for each belief state yields a piecewise linear and convex value function  The second phase of an iteration is the dynamic programming update  In value iteration  an update produces an improved set of vectors  where each vector corresponds to a deterministic one step policy  The same set of vectors is produced in this case  but the       Policy Iteration for DEC POMDPs  Input  A finite state controller  and a parameter       Evaluate the finite state controller by solving a system of linear equations     Perform a dynamic programming update to add a set of deterministic nodes to the controller     Perform value preserving transformations on the controller     Calculate the Bellman residual  If it is less than           then terminate  Otherwise  go to step    Output  An   optimal finite state controller  Table    Policy Iteration for POMDPs  actions and transition rules for the one step policy cannot be removed from memory  Each new vector is actually a node that gets added to the controller  All of the probability distributions for the added nodes are deterministic  That is  a exhaustive backup in this context creates a new node for each possible action and possible combinations of observations and deterministic transitions into the current controller  This results in the same one step policies being considered as in the dynamic programming update described above  As there are  A  t     possible one step polices  this number also defines the number of new nodes added to the controller after an exhaustive backup  Finally  additional operations are performed on the controller  There are many such operations  and we describe two possibilities in the following section  The only restriction placed on these operations is that they do not decrease the value for any belief state  Such an operation is denoted a value preserving transformation  The complete algorithm is outlined in Table    It is guaranteed to converge to a finitestate controller that is   optimal for all belief states within a finite number of steps  Furthermore  the Bellman residual can be used to obtain a bound on the distance to optimality  as with value iteration        Controller Reductions In performing a DP update  potential nodes that are dominated do not get added to the controller  However  after the update is performed  some of the old nodes may have become dominated  These nodes cannot simply be removed  however  as other nodes may transition into them  This is where the dual view of dominance is useful  Recall that if a node is dominated  then there is a convex combination of other nodes with value at least as high from all states  Thus  we can remove the dominated node and merge it into the dominating convex combination by changing transition probabilities accordingly  This operation was proposed by Poupart and Boutilier        and built upon earlier work by Hansen      b   Formally  a controller reduction attempts to replace a node q  Q with a distribution P  q  over nodes q  Q   q such that for all s  S  X V  s  q   P  q V  s  q   qQ q        Bernstein  Amato  Hansen    Zilberstein  Variables     x   Objective  Maximize   Improvement constraints  s  V  s          X  x  V  s       Probability constraints  X    x         x         Table    The dual linear program for testing dominance for the vector   The variable x   represents P      This can be achieved by solving the linear program in Table    As nodes are used rather than vectors  we replace x   with x q  in the dual formulation which provides a probability distribution of nodes which dominate node q  Rather than transitioning into q  this distribution can then be used instead  It can be shown that if such a distribution is found and used for merging  the resulting controller is a value preserving transformation of the original one        Bounded Backups In the previous section  we described a way to reduce the size of a controller without sacrificing value  The method described in this section attempts to increase the value of the controller while keeping its size fixed  It focuses on one node at a time  and attempts to change the parameters of the node such that the value of the controller is at least as high for all belief states  The idea for this approach originated with Platzman         and was made efficient by Poupart and Boutilier         In this method  a node q is chosen  and parameters for the conditional distribution P  a  q    q  o  are to be determined  Determining these parameters works as follows  We assume that the original controller will be used from the second step on  and try to replace the parameters for q with better ones for just the first step  In other words  we look for parameters which satisfy the following inequality    X X V  s  q   P  a q  R s  a     P  q    q  a  o P  o  s   s  a V  s    q     a  s   o q    for all s  S  Note that the inequality is always satisfied by the original parameters  However  it is often possible to get an improvement  The new parameters can be found by solving a linear program  as shown in Table    Note that the size of the linear program is polynomial in the sizes of the POMDP and the controller  We call this process a bounded backup because it acts like a dynamic programming       Policy Iteration for DEC POMDPs  Variables     x a   x a  o  q     Objective  Maximize   Improvement constraints   s  V  s  q        X    x a R s  a      x a        a  o  X  x a  o  q       x a   q   a  a  x a  o  q    P  o  s   s  a V  s    q      s   o q    a  Probability constraints  X  X  x a       a  o  q    x a  o  q         Table    The linear program to be solved for a bounded backup  The variable x a  represents P  a q   and the variable x a  o  q     represents P  a  q    q  o    backup with memory constraints  To see this  consider the set of nodes generated by a DP backup  These nodes dominate the original nodes across all belief states  so for every original node  there must be a convex combination of the nodes in this set that dominate the original node for all states  A bounded backup finds such a convex combination  It can be shown that a bounded backup yields a value preserving transformation  Repeated application of bounded backups can lead to a local optimum  at which none of the nodes can be improved any further  Poupart and Boutilier        showed that a local optimum has been reached when each nodes value function is touching the value function produced by performing a full DP backup  This is illustrated in Figure        Decentralized Dynamic Programming In the previous section  we presented dynamic programming for POMDPs  A key part of POMDP theory is the fact that every POMDP has an equivalent belief state MDP  No such result is known for DEC POMDPs  making it difficult to generalize value iteration to the multiagent case  This lack of a shared belief state requires a new set of tools to be developed for solving DEC POMDP  As a step in this direction  we were able to develop an optimal policy iteration algorithm for DEC POMDPs that includes the POMDP version as a special case  This algorithm is the focus of the section  We first show how to extend the definition of a stochastic controller to the multiagent case  Multiagent controllers include a correlation device  which is a source of randomness shared by all the agents  This shared randomness increases solution quality while minimally increasing representation size without adding communication  As in the single agent case  policy iteration alternates between exhaustive backups and value preserving transforma      Bernstein  Amato  Hansen    Zilberstein  value function after DP update value function for controller  s   s   Figure    A local optimum for bounded backups  The solid line is the value function for the controller  and the dotted line is the value function for the controller that results from a full DP update   tions  A convergence proof is given  along with efficient transformations that extend those presented in the previous section      Correlated Finite State Controllers The joint policy for the agents is represented using a stochastic finite state controller for each agent  In this section  we first define a type of controller in which the agents act independently  We then provide an example demonstrating the utility of correlation  and show how to extend the definition of a controller to allow for correlation among agents        Local Finite State Controllers In a local controller  the agents node is based on the local observations received  and the agents action is based on the current node  These local controllers are defined in the same way as the POMDP controllers above  with each agent possessing its own controller that operates independently of the others  As before  stochastic transitions and action selection are allowed  We formally define a local controller for agent i as a tuple hQi   i   Ai   i   i i  where  Qi is a finite set of controller nodes   i is a set of inputs  taken to be the local observations for agent i   Ai is a set of outputs  taken to be the actions for agent i   i   Qi  Ai is an action selection function for agent i  defining the distribution of actions selected at each node of that agents controller   i   Qi  Ai  i  Qi is a transition function for agent i  defining the distribution of resulting nodes for each initial node and action taken of that agents controller  The functions i and i parameterize the conditional distribution P  ai   qi   qi   oi   which represents the combined action selection and node transition probability for agent i  When       Policy Iteration for DEC POMDPs  AB BA BB  AA  AA AB BA   R  R  R  s   s    R  BB  Figure    A DEC POMDP for which a correlated joint policy yields more reward than the optimal independent joint policy   taken together  the agents controllers determine the conditional distribution P   a   q     q   o   This is denoted an independent joint controller  In the following subsection  we show that independence can be limiting        The Utility of Correlation The joint controllers described above do not allow the agents to correlate their behavior via a shared source of randomness  We will use a simple example to illustrate the utility of correlation in partially observable domains where agents have limited memory  This example generalizes the one given by Singh        to illustrate the utility of stochastic policies in partially observable settings containing a single agent  Consider the DEC POMDP shown in Figure    This problem has two states  two agents  and two actions per agent  A and B   The agents each have only one observation  and thus cannot distinguish between the two states  For this example  we will consider only memoryless policies  Suppose that the agents can independently randomize their behavior using distributions P  a    and P  a     If the agents each choose either A or B according to a uniform distribution  then they receive an expected reward of  R  per time step  and thus an expected long term R reward of        It is straightforward to show that no independent policy yields higher reward than this one for all states  Next  let us consider the even larger class of policies in which the agents may act in a correlated fashion  In other words  we consider all joint distributions P  a    a     Consider the policy that assigns probability    to the pair AA and probability    to the pair BB  This yields an average reward of   at each time step and thus an expected long term reward of    The difference between the rewards obtained by the independent and correlated policies can be made arbitrarily large by increasing R        Bernstein  Amato  Hansen    Zilberstein        Correlated Joint Controllers In the previous subsection  we established that correlation can be useful in the face of limited memory  In this subsection  we extend our definition of a joint controller to allow for correlation among the agents  To do this  we introduce an additional finite state machine  called a correlation device  that provides extra signals to the agents at each time step  The device operates independently of the DEC POMDP process  and thus does not provide agents with information about the other agents observations  In fact  the random numbers necessary for its operation could be determined prior to execution time and made available to all agents  Formally  a correlation device is a tuple hQc   c i  where Qc is a set of nodes and c   Qc  Qc is a state transition function  At each step  the device undergoes a transition  and each agent observes its state  We must modify the definition of a local controller to take the state of the correlation device as input  Now  a local controller for agent i is a conditional distribution of the form P  ai   qi   qc   qi   oi    The correlation device together with the local controllers form a joint conditional distribution P   a   q     q   o   where  q   hqc   q            qn i  We will refer to this as a correlated joint controller  Note that a correlated joint controller with  Qc       is effectively an independent joint controller  Figure   contains a graphical representation of the probabilistic dependencies in a correlated joint controller  The value function for a correlated joint controller can be computed by solving the   following system of linear equations  one for each s  S and  q  Q   V  s   q     X    P   a  q  R s   a      X  P  s     o s   a P   q     q   a   o V  s     q        s     o   q    a  We sometimes refer to the value of the controller for an initial state distribution  For a distribution b  this is defined as V  b    max q   X  b s V  s   q    s  It is assumed that  given an initial state distribution  the controller is started in the joint node which maximizes value from that distribution  It is worth noting that correlation can increase the value of a set of fixed size controllers  but this same value can be achieved by a larger set of uncorrelated controllers  Thus  correlation is a way to make better use of limited representation size  but is not required to produce a set of optimal controllers  This is formalized by the following theorem  which is proved in Appendix A  The theorem asserts the existence of uncorrelated controllers  determining how much extra memory is needed to replace a correlation device remains an open problem  Theorem   Given an initial state and a correlated joint controller  there always exists some finite size joint controller without a correlation device that produces at least the same value for the initial state        Policy Iteration for DEC POMDPs  a   q   q   a   q   o   qc o   q   Figure    A graphical representation of the probabilistic dependencies in a correlated joint controller for two agents   In the example above  higher value can be achieved with two node uncorrelated controllers for each agent  If the problem starts in s    the first node for each agent would choose A and transition to the second node which would choose B  The second node would then transition back to the first node  The resulting policy consists of the agents alternating R between choosing AA and BB  producing an expected long term reward of   which is higher than the correlated one node policy value of    Thus  doubling memory for each agent in this problem is sufficient to remove the correlation device      Policy Iteration In this section  we describe the policy iteration algorithm  We first extend the definitions of exhaustive backup and value preserving transformation to the multiagent case  Following that  we provide a description of the complete algorithm  along with a convergence proof        Exhaustive Backups We introduced exhaustive backups in the section on dynamic programming for POMDPs  We stated that one way to implement a DP update was to perform an exhaustive backup  and then prune dominated nodes that were created  More efficient implementations were described thereafter  These implementations involved interleaving pruning with node generation  For the multiagent case  it is an open problem whether pruning can be interleaved with node generation  Nodes can be removed  as we will show in a later subsection  but for convergence we require exhaustive backups  We do not define DP updates for the multiagent case  and instead make exhaustive backups a central component of our algorithm  An exhaustive backup adds nodes to the local controllers for all agents at once  and leaves the correlation device unchanged  For each agent i   Ai   Qi   i   nodes are added to the Q local controller  one for each one step policy  Thus  the joint controller grows by  Qc   i  Ai   Qi   Oi   joint nodes  Note that repeated application of exhaustive backups amounts to a brute force search in the space of deterministic policies  This converges to optimality  but is obviously quite inefficient  As in the single agent case  we must modify the joint controller in between       Bernstein  Amato  Hansen    Zilberstein  adding new nodes  For convergence  these modifications must preserve value in a sense that will be made formal in the following section        Value Preserving Transformations We now extend the definition of a value preserving transformation to the multiagent case  In the following subsection  we show how this definition allows for convergence to optimality as the number of iterations grows  The dual interpretation of dominance is helpful in understanding multiagent valuepreserving transformations  Recall that for a POMDP  we say that a node is dominated if there is a convex combination of other nodes with value at least as high for all states  Though we defined a value preserving transformation in terms of the value function across belief states  we could have equivalently defined it so that every node in the original controller has a dominating convex combination in the new controller  For the multiagent case  we do not have the concept of a belief state MDP  so we take the second approach mentioned above  In particular  we require that dominating convex combinations exist for nodes of all the local controllers and the correlation device  A transformation of a controller C to a controller D qualifies as a value preserving transformation if C  D  where  is defined below    and R    respectively  We Consider correlated joint controllers C and D with node sets Q say that C  D if there exist mappings fi   Qi  Ri for each agent i and fc   Qc  Rc such that X V  s   q   P   r  q V  s   r    r    Note that this relation is transitive as further value preserving for all s  S and  q  Q  transformations of D will also be value preserving transformations of C     R    Examples We sometimes describe the fi and fc as a single mapping f   Q of efficient value preserving transformations are given in a later section  In the following subsection  we show that alternating between exhaustive backups and value preserving transformations yields convergence to optimality        Algorithmic Framework The policy iteration algorithm is initialized with an arbitrary correlated joint controller  In the first part of an iteration  the controller is evaluated via the solution of a system of linear equations  Next  an exhaustive backup is performed to add nodes to the local controllers  Finally  value preserving transformations are performed  In contrast to the single agent case  there is no Bellman residual for testing convergence to   optimality  We resort to a simpler test for   optimality based on the discount rate and the number of iterations so far  Let  Rmax   be the largest absolute value of an immediate reward possible in the DEC POMDP  Our algorithm terminates after iteration t    R max   t if        At this point  due to discounting  the value of any policy after step t is less than    Justification for this test is provided in the convergence proof  The complete algorithm is sketched in Table    Before proving convergence  we state a key lemma regarding the ordering of exhaustive backups and value preserving transformations  Its proof is deferred to the Appendix        Policy Iteration for DEC POMDPs  Input  A correlated joint controller  and a parameter       Evaluate the correlated joint controller by solving a system of linear equations     Perform an exhaustive backup to add deterministic nodes to the local controllers     Perform value preserving transformations on the controller  t     Rmax      If        where t is the number of iterations so far  then terminate  Else go to step     Output  A correlated joint controller that is   optimal for all states  Table    Policy Iteration for DEC POMDPs  Lemma   Let C and D be correlated joint controllers  and let C and D be the results of performing exhaustive backups on C and D  respectively  Then C  D if C  D  Thus  if there is a value preserving transformation mapping controller C to D and both are exhaustively backed up  then there is a value preserving transformation mapping controller C to D  This allows value preserving transformations to be performed before exhaustive backups  while ensuring that value is not lost after the backup  We can now state and prove the main convergence theorem for policy iteration  Theorem   For any    policy iteration returns a correlated joint controller that is   optimal for all initial states in a finite number of iterations  Proof  Repeated application of exhaustive backups amounts to a brute force search in the space of deterministic joint policies  Thus  after t exhaustive backups  the resulting controller is optimal for t steps from any initial state  Let t be an integer large enough that  t    Rmax       Then any possible discounted sum of rewards after t time steps is small   enough that optimality over t time steps implies   optimality over the infinite horizon  Now recall the above lemma  which states that performing value preserving transformations before a backup provides at least as much value as just performing a backup  By an inductive argument  performing t steps of policy iteration is a value preserving transformation of the result of t exhaustive backups  We have argued that for large enough t  the value of the controller resulting from t exhaustive backups is within   of optimal for all states  Thus  the result of t steps of policy iteration is also within   of optimal for all states        Efficient Value Preserving Transformations In this section  we describe how to extend controller reductions and bounded backups to the multiagent case  We will show that both of these operations are value preserving transformations        Controller Reductions Recall that in the single agent case  a node can be removed if for all belief states  there is another node with value at least as high  The equivalent dual interpretation is that a node       Bernstein  Amato  Hansen    Zilberstein  can be removed is there exists a convex combination of other nodes with value at least as high across the entire state space  Using the dual interpretation  we can extend this to a rule for removing nodes in the multiagent case  The rule applies to removing nodes either from a local controller or from the correlation device  Intuitively  in considering the removal of a node from a local controller or the correlation device  we consider the nodes of the other controllers to be part of the hidden state  More precisely  suppose we are considering removing node qi from agent is local controller  To do this  we need to find a distribution P  qi   over nodes qi  Qi   qi such that for all s  S  qi  Qi   and qc  Qc   V  s  qi   qi   qc     X  P  qi  V  s  qi   qi   qc     qi  where Qi represents the set of nodes for the other agents  Finding such a distribution can be formulated as a linear program  as shown in Table  a  In this case  success is finding parameters such that       The linear program is polynomial in the sizes of the DEC POMDP and controllers  but exponential in the number of agents  If we are successful in finding parameters that make       then we can merge the dominated node into the convex combination of other nodes by changing all incoming links to the dominated controller node to be redirected based on the distribution P  qi    At this point  there is no chance of ever transitioning into qi   and thus it can be removed  The rule for the correlation device is very similar  Suppose that we are considering the removal of node qc   In this case  we need to find a distribution P  qc   over nodes qc  Qc   qc   such that for all s  S and  q  Q  V  s   q  qc     X  P  qc  V  s   q  qc     qc    for the set of tuples of local controller nodes  Note that we abuse notation here and use Q excluding the nodes for the correlation device  As in the previous case  finding parameters can done using linear programming  This is shown in Table  b  This linear program is also polynomial in the the sizes of the DEC POMDP and controllers  but exponential in the number of agents  We have the following theorem  which states that controller reductions are value preserving transformations  Theorem   Any controller reduction applied to either a local node or a node of the correlation device is a value preserving transformation  Proof  Suppose that we have replaced an agent i node qi with a distribution over nodes in Qi   qi   Let us take fi to be the identity map for all nodes except qi   which will map to the new distribution  We take fc to be the identity map  and we take fj to be the identity map for all j    i  This yields a complete mapping f   We must now show that f satisfies the condition given in the definition of a value preserving transformation        Policy Iteration for DEC POMDPs   a  Variables     x qi   Objective  Maximize   Improvement constraints  s  qi   qc  V  s  qi   qi   qc         X  x qi  V  s  qi   qi   qc    qi  Probability constraints  X  qi  x qi         x qi       qi   b  Variables     x qc   Objective  Maximize   Improvement constraints  s   q V  s   q  qc         X  x qc  V  s   q  qc    qc  Probability constraints  X  qc  x qc         x qc       qc  Table     a  The linear program to be solved to find a replacement for agent is node qi   The variable x qi   represents P  qi     b  The linear program to be solved to find a replacement for the correlation node qc   The variable x qc   represents P  qc     Let Vo be the value function for the original controller  and let Vn be the value function for the controller with qi removed  A controller reduction requires that Vo  s   q    X  P   r  q Vo  s   r     r    Thus  we have for all s  S and  q  Q   Vo  s   q     X  P   a  q  R s  a       X  P   q     q   a   o P  s     o s   a Vo  s     q      s     o   q    a     X  a  P   a  q  R s  a       X  P   q     q   a   o P  s     o s   a   s     o   q        X   r   P   r  q Vo  s   r       Bernstein  Amato  Hansen    Zilberstein      X  P   a  q  R s  a       X  P   q     q   a   o P  s     o s   a P   r  q Vo  s   r      s     o   q      r    a    Notice that the formula on the right is the Bellman operator for all s  S and  q  Q  for the new controller  applied to the old value function  Denoting this operator Tn   the system of inequalities implies that Tn Vo  Vo   By monotonicity  we have that for all k     Tnk    Vo    Tnk  Vo    Since Vn   limk Tnk  Vo    we have that Vn  Vo   This is sufficient for f to satisfy the condition in the definition of value preserving transformation  The argument for removing a node of the correlation device is almost identical to the one given above          Bounded Dynamic Programming Updates In the previous section  we described a way to reduce the size of a controller without sacrificing value  Recall that in the single agent case  we could also use bounded backups to increase the value of the controller while keeping its size fixed  This technique can be extended to the multiagent case  As in the previous section  the extension relies on improving a single local controller or the correlation device  while viewing the nodes of the other controllers as part of the hidden state  We first describe in detail how to improve a local controller  To do this  we choose an agent i  along with a node qi   Then  for each oi  i   we search for new parameters for the conditional distribution P  ai   qi   qi   oi    The search for new parameters works as follows  We assume that the original controller will be used from the second step on  and try to replace the parameters for qi with better ones for just the first step  In other words  we look for parameters satisfying the following inequality    X X V  s   q   P   a  q  R s  a     P   q     q   a   o P  s     o s   a V  s     q      a  s     o   q   for all s  S  qi  Qi   and qc  Qc   The search for new parameters can be formulated as a linear program  as shown in Table  a  Its size is polynomial in the sizes of the DEC POMDP and the joint controller  but exponential in the number of agents  The procedure for improving the correlation device is very similar to the procedure for improving a local controller  We first choose a device node qc   and consider changing its parameters for just the first step  We look for parameters satisfying the following inequality    X X P   a  q  R s  a     P   q     q   a   o P  s     o s   a V  s     q     V  s   q    a  s     o   q     for all s  S and  q  Q  As in the previous case  the search for parameters can be formulated as a linear program  This is shown in Table  b  This linear program is also polynomial in the sizes of the DECPOMDP and joint controller  but exponential in the number of agents  The following theorem states that bounded backups preserve value        Policy Iteration for DEC POMDPs   a  Variables     x qc   ai    x qc   ai   oi   qi    Objective  Maximize   Improvement constraints  s  qi   qc  X  V  s   q  qc         P  ai  qc   qi   x qc   ai  R s   a      a    X    x c  ai   oi   qi   P  qi  qc   qi   ai   oi    s     o   q    qc    P   o  s   s   a P  qc   qc  V  s     q     qc      Probability constraints  X qc x qc   ai         qc   ai   oi  x qc   ai   oi   qi      x qc   ai    qi   ai  qc   ai  X  x qc   ai        qc   ai   oi   qi   x qc   ai   oi   qi         b  Variables     x qc    Objective  Maximize   Improvement constraints  s   q V  s   q  qc         X  P   a qc    q  R s   a      X  P   q    qc    q   a   o   s     o   q    qc    a     P  s    o s   a x qc   V  s     q     qc      Probability constraints  qc   X  x qc          qc   x qc        qc   Table     a  The linear program used to find new parameters for agent is node qi   The variable x qc   ai   represents P  ai  qi   qc    and the variable x qc   ai   oi   qi    represents P  ai   qi   qc   qi   oi     b  The linear program used to find new parameters for the correlation device node qc   The variable x qc    represents P  qc   qc           Bernstein  Amato  Hansen    Zilberstein  Theorem   Performing a bounded backup on a local controller or the correlation device produces a new correlated joint controller which is a value preserving transformation of the original  Proof  Consider the case in which some node qi of agent is local controller is changed  We define f to be a deterministic mapping from nodes in the original controller to the corresponding nodes in the new controller  Let Vo be the value function for the original controller  and let Vn be the value function for the new controller  Recall that the new parameters for P  ai   qi   qc   qi   oi   must satisfy the following inequality for all s  S  qi  Qi   and qc  Qc     X X Vo  s   q   P   a  q  R s  a     P   q     q   a   o P  s     o s   a Vo  s     q        a  s     o   q   Notice that the formula on the right is the Bellman operator for the new controller  applied to the old value function  Denoting this operator Tn   the system of inequalities implies that Tn Vo  Vo   By monotonicity  we have that for all k     Tnk    Vo    Tnk  Vo    Since Vn   limk Tnk  Vo    we have that Vn  Vo   Thus  the new controller is a value preserving transformation of the original one  The argument for changing nodes of the correlation device is almost identical to the one given above        Open Issues We noted at the beginning of the section that there is no known way to convert a DECPOMDP into an equivalent belief state MDP  Despite this fact  we were able to develop a provably convergent policy iteration algorithm  However  the policy iteration algorithm for POMDPs has other desirable properties besides convergence  and we have not yet been able to extend these to the multiagent case  Two such properties are described below        Error Bounds The first property is the existence of a Bellman residual  In the single agent case  it is possible to compute a bound on the distance to optimality using two successive value functions  In the multiagent case  policy iteration produces a sequence of controllers  each of which has a value function  However  we do not have a way to obtain an error bound from these value functions  For now  to bound the distance to optimality  we must consider the discount rate and the number of iterations completed        Avoiding Exhaustive Backups In performing a DP update for POMDPs  it is possible to remove certain nodes from consideration without first generating them  In Section    we gave a high level description of a few different approaches to doing this  For DEC POMDPs  however  we did not define a DP update and instead used exhaustive backups as the way to expand a controller  Since exhaustive backups are expensive  it would be useful to extend the more sophisticated pruning methods for POMDPs to the multiagent case        Policy Iteration for DEC POMDPs  Input  A joint controller  the desired number of centralized belief points k  initial state b  and fixed policy for each agent i      Starting from b    sample a set of k belief points for each agent assuming the other agents use their fixed policy     Evaluate the joint controller by solving a system of linear equations     Perform an exhaustive backup to add deterministic nodes to the local controllers     Retain nodes that contribute the highest value at each of the belief points     For each agent  replace nodes that have lower value than some combination of other nodes at each belief point     If controller sizes and parameters do not change then terminate  Else go to step    Output  A new joint controller based on the sampled centralized belief points  Table    Heuristic Policy Iteration for DEC POMDPs   Unfortunately  in the case of POMDPs  the proofs of correctness for these methods all use the fact that there exists a Bellman equation  Roughly speaking  this equation allows us to determine whether a potential node is dominated by just analyzing the nodes that would be its successors  Because we do not currently have an analog of the Bellman equation for DEC POMDPs  we have not been able to generalize these results  There is one exception to the above statement  however  When an exhaustive backup has been performed for all agents except one  then a type of belief state space can be constructed for the agent in question using the system states and the nodes for the other agents  The POMDP node generation methods can then be applied to just that agent  In general  though  it seems difficult to rule out a node for one agent before generating all the nodes for the other agents      Heuristic Policy Iteration While the optimal policy iteration method shows how a set of controllers with value arbitrarily close to optimal can be found  the resulting controllers may be very large and many unnecessary nodes may be generated along the way  This is exacerbated by the fact that the algorithm cannot take advantage of an initial state distribution and must attempt to improve the controller for any initial state  As a way to combat these disadvantages  we have developed a heuristic version of policy iteration that removes nodes based on their value only at a given set of centralized belief points  We call these centralized belief points because they are distributions over the system state that in general could only be known by full observability of the problem  As a result  the algorithm will no longer be optimal  but it can often produce more concise controllers with higher solution quality for a given initial state distribution        Bernstein  Amato  Hansen    Zilberstein      Directed Pruning Our heuristic policy iteration algorithm uses sets of belief points to direct the pruning process of our algorithm  There are two main advantages of this approach  it allows simultaneous pruning for all agents and it focuses the controller on certain areas of the belief space  We first discuss the benefits of simultaneous pruning and then mention the advantages of focusing on small areas of the belief space  As mentioned above  the pruning method used by the optimal algorithm will not always remove all nodes that could be removed from all the agents controllers without losing value  Because pruning requires each agent to consider the controllers of other agents  after nodes are removed for one agent  the other agents may be able to prune other nodes  Thus pruning must cycle through the agents and ceases when no agent can remove any further nodes  This is both time consuming and causes the controller to be much larger than it needs to be  Like the game theoretic concept of incredible threats    a set of suboptimal policies for an agent may be useful only because other agents may employ similarly suboptimal policies  That is  because pruning is conducted for each agent while holding the other agents policies fixed  polices that are useful for any set of other agent policies are retained  no matter the quality of these other agent policies  Some of an agents policies may only be retained because they have the highest value when used in conjunction with other suboptimal policies of the other agents  In these cases  only by removing the set of suboptimal policies simultaneously can controller size be reduced while at least maintaining value  This simultaneous pruning could further reduce controller sizes and thus increase scalability and solution quality  While it may be possible to define a value preserving transformation for these problems  finding a nontrivial automated way to do so while maintaining the optimality of the algorithm remains an open question  The advantage of considering a smaller part of the state space has already been shown to produce drastic performance increases in POMDPs  Ji  Parr  Li  Liao    Carin        Pineau  Gordon    Thrun        and finite horizon DEC POMDPs  Seuken   Zilberstein        Szer   Charpillet         For POMDPs  a problem with many states has a belief space with large dimensionality  but many parts may never be visited by an optimal policy  Focusing on a subset of belief states can allow a large part of the state space to be ignored without significant loss of solution quality  The problem of having a large state space is compounded in the DEC POMDP case  Not only is there uncertainty about the state  but also about the policies of the other agents  As a consequence  the generalized belief space which includes all possible distributions over states of the system and current policies of the other agents must be considered to guarantee optimality  This results in a huge space which contains many unlikely states and policies  The uncertainty about which policies other agents may utilize does not allow belief updates to normally be calculated for DEC POMDPs  but as we showed above  it can be done by assuming a probability distribution over actions of the other agents  This limits the number of policies that need to be considered by all agents and if the distributions are chosen well  may permit a high valued solution to be found     An incredible threat is an irrational strategy that the agent knows it will receive a lower value by choosing it  While it is possible the agent will choose the incredible threat strategy  it is irrational to do so         Policy Iteration for DEC POMDPs  Variables     x qi   and for each belief point b Objective  Maximize   Improvement constraints   b  qi  X   X   b s  x qi  V  qi   qi   s   V   q  s      s  X  Probability constraints   qi  x qi       and qi x qi       qi  Table    The linear program used to determine if a node q for agent i is dominated at each point b and all initial nodes of the other agents controllers  As node q may be dominated by a distribution of nodes  variable x qi   represents P  qi    the probability of starting in node q for agent i       Belief Set Generation As mentioned above  our heuristic policy iteration algorithm constructs sets of belief points for each agent which are later used to evaluate the joint controller and remove dominated nodes  To generate the belief point set  we start at the initial state and by making assumptions about the other agents  we can calculate the resulting belief state for each action and observation pair of an agent  By fixing the policies for the other agents  this belief state update can be calculated in a way very similar to that described for POMDPs in section        This procedure can be repeated from each resulting belief state until a desired number of points is generated or no new points are visited  More formally  we assume the other agents have a fixed distribution of action choice for each system state  That is  if we know P   ai  s  then we can determine the probability any state results given a belief point and an agents action and observation  The derivation of the likelihood of state s    given the belief state b  and agent is action ai and observation oi is shown below   P  s   ai   oi   b     X  P  s     ai    oi   s ai   oi   b    ai    oi  s  o s  b   a  s   P  s    s   a  b   ai    oi  s P     P    P  oi   ai   b  o s   a  s   P  s   s   a  b P   a  s  b   ai    oi  s P     P    P  oi   ai   b  o s   a  s   P  s   s   a P   ai  a  s  b P   a  s  b   ai    oi  s P     P    P  oi   ai   b     P  s   s    P    o  s    a   s a P   ai  ai   s  b P  s ai   b P  ai   b   ai    oi  s  P    P  oi   ai   b        Bernstein  Amato  Hansen    Zilberstein  P    o s   a  s  ai    oi  s P        P  s   s    a P   ai  s b s   P  oi  ai   b   where X  P  oi  ai   b     P   o s   a  s   P  s   s   a P   ai  s b s   ai  oi  s s   Thus  given the action probabilities for the other agents  i  and the transition and observation models of the system  a belief state update can be calculated      Algorithmic Framework We provide a formal description of our approach in Table    Given the desired number of belief points  k  and random action and observation selection for each agent  the sets of points are generated as described above  The search begins at the initial state of the problem and continues until the given number of points is obtained  If no new points are found  this process can be repeated to ensure a diverse set is produced  The arbitrary initial controller is evaluated and the value at each state and for each initial node of any agents controller is retained  The exhaustive backup procedure is exactly the same as the one used in the optimal algorithm  but updating the controller takes place in two steps  First  for each of the k belief points  the highest valued set of initial nodes is found  To accomplish this  the value of beginning at each combination of nodes for all agents is calculated for each of these k points and the best combination is kept  This allows nodes that do not contribute to any of these values to be simultaneously pruned  Next  each node of each agent is pruned using the linear program shown in Table    If a distribution of nodes for the given agent has higher value at each of the belief points for any initial nodes of the other agents controllers  it is pruned and replaced with that distribution  The new controllers are then evaluated and the value is compared with the value of the previous controller  This process of backing up and pruning continues while the controller parameters continue to change  Similar to how bounded policy updates can be used in conjunction with pruning in the optimal policy iteration algorithm  a nonlinear programming approach  Amato et al         can be used to improve solution quality for the heuristic case  To accomplish this  instead of optimizing the controller for just the initial belief state of the problem  all the belief points being considered are used  A simple way to achieve this is to maximize over the sum of the values of the initial nodes of the controllers weighted by the probabilities given for each point  This approach can be used after each pruning step and may further improve value of the controllers      Dynamic Programming Experiments This section describes the results of experiments performed using policy iteration  Because of the flexibility of the algorithm  it is impossible to explore all possible ways of implementing it  However  we did experiment with a few different implementation strategies to gain an idea of how the algorithm works in practice  All of these experiments were run on a     GHz Intel Pentium   with  GB of memory  Three main sets of experiments were performed on a single set of test problems        Policy Iteration for DEC POMDPs  Our first set of experiments focused on exhaustive backups and controller reductions  The results confirm that value improvement can be obtained through iterated application of these two operations  Further improvement is demonstrated by also incorporating bounded updates  However  because exhaustive backups are expensive  the algorithm was unable to complete more than a few iterations on any of our test problems  In the second set of experiments  we addressed the complexity issues by using only bounded backups  and no exhaustive backups  With bounded backups  we were able to obtain higher valued controllers while keeping memory requirements fixed  We examined how the sizes of the initial local controllers and the correlation device affected the value of the final solution  The third set of experiments examined the complexity issues caused by exhaustive backups by using the point based heuristic  This allowed our heuristic policy iteration algorithm to complete more iterations than the optimal algorithm and in doing so  increased solution quality of the largest solvable controllers  By incorporating Amato et al s NLP approach  the heuristic algorithm becomes slightly less scalable than with heuristic pruning alone  but the amount of value improvement per step increases  This causes the resulting controllers in each domain to have the highest value of any approach      Test Domains In this section  we describe three test domains  ordered by the size of the problem representation  For each problem  the transition function  observation function  and reward functions are described  In addition  an initial state is specified  Although policy iteration does not require an initial state as input  one is commonly assumed and is used by the heuristic version of the algorithm  A few different initial states were tried for each problem  and qualitatively similar results were obtained  In all domains  a discount factor of     was utilized  As a very loose upper bound  the centralized policy was calculated for each problem in which all agents share their observations with a central agent and decisions for all agents are made by the central agent  This results in a POMDP with the same number of states  but the action and observation sets are Cartesian products of the agents action and observation sets  The value of this POMDP policy is provided below  but because DEC POMDP policies are more constrained  the optimal value may be much lower  Two Agent Tiger Problem The two agent tiger problem consists of   states    actions and   observations  Nair et al          The domain includes two doors  one of which leads to a tiger and the other to a large treasure  Each agent may open one of the doors or listen  If either agent opens the door with the tiger behind it  a large penalty is given  If the door with the treasure behind it is opened and the tiger door is not  a reward is given  If both agents choose the same action  i e   both opening the same door  a larger positive reward or a smaller penalty is given to reward cooperation  If an agent listens  a small penalty is given and an observation is seen that is a noisy indication of which door the tiger is behind  While listening does not change the location of the tiger  opening a door causes the tiger to be placed behind one of the       Bernstein  Amato  Hansen    Zilberstein  door with equal probability  The problem begins with the tiger equally likely to be located behind either door  The optimal centralized policy for this problem has value         Meeting on a Grid In this problem  with    states    actions and   observations  two robots must navigate on a two by two grid  Each robot can only sense whether there are walls to its left or right  and their goal is to spend as much time as possible on the same square as the other agent  The actions are to move up  down  left  or right  or to stay on the same square  When a robot attempts to move to an open square  it only goes in the intended direction with probability      otherwise it either goes in another direction or stays in the same square  Any move into a wall results in staying in the same square  The robots do not interfere with each other and cannot sense each other  The reward is   when the agents share a square  and   otherwise  The initial state places the robots diagonally across from each other and the optimal centralized policy for this problem has value        Box Pushing Problem This problem  with     states    actions and   observations consists of two agents that get rewarded by pushing different boxes  Seuken   Zilberstein         The agents begin facing each other in the bottom corners of a four by three grid with the available actions of turning right  turning left  moving forward or staying in place  There is a     probability that the agent will succeed in moving and otherwise will stay in place  but the two agents can never occupy the same square  The middle row of the grid contains one large box in the middle of two small boxes  The small boxes can be moved by a single agent  but the large box can only be moved by both agents pushing at the same time  The upper row of the grid is considered the goal row  which the boxes are pushed into  The possible deterministic observations for each agent consist of seeing an empty space  a wall  the other agent  a small box or the large box  A reward of     is given if both agents push the large box to the goal row and    is given for each small box that is moved to the goal row  A penalty of    is given for each agent that cannot move and      is given for each time step  Once a box is moved to the goal row  the environment resets to the original start state  The optimal centralized policy for this problem has value              Exhaustive Backups and Controller Reductions In this section  we present the results of using exhaustive backups together with controller reductions  For each domain  the initial controllers for each agent contained a single node with a self loop  and there was no correlation device  For each problem  the first action of the problem description was used  This resulted in the repeated actions of opening the left door in the two agent tiger problem  moving up in the meeting on a grid problem and turning left in the box pushing problem  The reason for starting with the smallest possible controllers was to see how many iterations we could complete before running out of memory  On each iteration  we performed an exhaustive backup  and then alternated between agents  performing controller reductions until no more nodes could be removed  For bounded dynamic programming results  after the reductions were completed bounded updates were also performed for all agents  For these experiments  we attempted to improve the nodes of       Policy Iteration for DEC POMDPs  Iteration          Two Agent Tiger   S        Ai         i       Exhaustive Sizes Controller Reductions Bounded Updates                  in  s            in  s                   in  s           in   s                          in  s              in   s                               in     s                 in     s   Iteration        Meeting on a Grid   S         Ai         i       Exhaustive Sizes Controller Reductions Bounded Updates                 in  s           in  s                  in  s           in    s                          in    s                 in     s   Iteration        Box Pushing   S          Ai         i       Exhaustive Sizes Controller Reductions Bounded Updates                in  s          in   s                 in    s           in    s                         in    s               in    s   Table     Results of applying exhaustive backups  controller reductions and bounded updates to our test problems  The second column contains the sizes of the controllers if only exhaustive backups had been performed  The third column contains the resulting value  sizes of the controllers  and time required for controller reductions to be performed on each iteration  The fourth column displays these same quantities with bounded updates also being used  The   denotes that a backup and pruning were performed  but bounded updates exhausted the given resources   each agent in turn until value could not be improved for any node of any agent  For each iteration  we recorded the sizes of the controllers produced  and noted what the sizes would be if no controller reductions had been performed  In addition  we recorded the value from the initial state and the total time taken to reach the given result  The results are shown in Table     Because exhaustive backups add many nodes  we were unable to complete many iterations without exceeding memory limits  As expected  the smallest problem led to the largest number of iterations being completed  Although we could not complete many iterations before running out of memory  the use of controller reductions led to significantly smaller controllers compared to the approach of just applying exhaustive backups  Incorporating bounded updates requires some extra time  but is able to improve the value produced at each step  causing substantial improvement in some cases  It is also interesting to notice that the controller sizes when using bounded updates are not always the same as when only controller reductions are completed  This can be seen after two iterations in both the meeting on a grid and box pushing problems  This can occur because the bounded updates change node value and thus change the number and location of the nodes that are pruned  In the box pushing problem  the two agents also       Bernstein  Amato  Hansen    Zilberstein  have different size controllers after two steps  This can occur  even in symmetric problems  when a set of actions is only necessary for a single agent      Bounded Dynamic Programming Updates As we saw from the previous experiments  exhaustive backups can fill up memory very quickly  This leads naturally to the question of how much improvement is possible without exhaustive backups  In this section  we describe an experiment in which we repeatedly applied bounded backups  which left the size of the controller fixed  We experimented with different starting sizes for the local controllers and the correlation device  We define a trial run of the algorithm as follows  At the start of a trial run  a size is chosen for each of the local controllers and the correlation device  The action selection and transition functions are initialized to be deterministic  with the outcomes drawn according to a uniform distribution  A step consists of choosing a node uniformly at random from the correlation device or one of the local controllers  and performing a bounded backup on that node  After     steps  the run is considered over  In practice  we found that values often stabilized in fewer steps  We varied the sizes of the local controllers while maintaining the same number of nodes for each agent  and we varied the size of the correlation device from   to    For each domain  we increased number of nodes until the required number of steps could not be completed in under four hours  In general  runs required significantly less time to terminate  For each combination of sizes  we performed    trial runs and recorded the best value over all runs  For each of the three problems  we were able to obtain solutions with higher value than with exhaustive backups  Thus  we see that even though repeated application of bounded backups does not have an optimality guarantee  it can be competitive with an algorithm that does  However  it should be noted that we have not performed an exhaustive comparison  We could have made different design decisions for both approaches concerning the starting controllers  the order in which nodes are considered  and other factors  Besides comparing to the exhaustive backup approach  we wanted to examine the effect of the sizes of the local controllers and the correlation device on value  Figure   shows a graph of best values plotted against controller size  We found that  for the most part  the value increases when we increase the size of the correlation device from one node to two nodes  essentially moving from independent to correlated   It is worth noting that the solution quality had somewhat high variance in each problem  showing that setting good initial parameters is important for high valued solutions  For small controllers  the best value tends to increase with controller size  However  for very large controllers  this not always the case  This can be explained by considering how a bounded backup works  For new node parameters to be acceptable  they must not decrease the value for any combination of states  nodes for the other controllers  and nodes for the correlation device  This becomes more difficult as the numbers of nodes increase  and thus it is easier to get stuck in a local optimum  This can be readily seen in the two agent tiger problem and to some extent the meeting on a grid problem  Memory was exhausted before this phenomenon takes place in the box pushing problem        Policy Iteration for DEC POMDPs   a    b    c  Figure    Best value per trial run plotted against the size of the local controllers  for  a  the two agent tiger problem   b  the meeting in a grid problem and  c  the box pushing problem  The solid line represents independent controllers  a correlation device with one node   and the dotted line represents a joint controller including a two node correlation device  Times ranged from under  s for one node controllers without correlation to four hours for the largest controller found with correlation in each problem       Heuristic Dynamic Programming Updates As observed above  the optimal dynamic programming approach can only complete a small number of backups before resources are exhausted  Similarly  using bounded updates with fixed size controllers can generate high value solutions  but it can be difficult to pick the correct controller size and initial parameters  As an alternative to the other approaches  we also present experiments using our heuristic dynamic programming algorithm  Like the optimal policy iteration experiments  we initialized single node controllers for each agent with self loops and no correlation device  The same first actions were used as above and backups were performed until memory was exhausted  The set of belief points for each problem was generated given the initial state distribution and a distribution of actions for the other agents  For the meeting on a grid and box pushing problems  it was       Bernstein  Amato  Hansen    Zilberstein  assumed that all agents chose any action with equal probability regardless of state  For the two agent tiger problem  it was assumed that for any state agents listen with probability     and open each door with probability      This simple heuristic policy was chosen to allow more of the state space to be sampled by our search  The number of belief points used for the two agent tiger and meeting on a grid problems was ten and twenty points were used for the box pushing problem  For each iteration  we performed an exhaustive backup and then pruned controllers as described in steps four and five of Table    All the nodes that contributed to the highest value for each belief point were retained and then each node was examined using the linear program in Table    For results with the NLP approach  we also improved the set of controllers after heuristic pruning by optimizing a nonlinear program whose objective was the sum of the values of the initial nodes weighted by the belief point probabilities  We report the value produced by the optimal and heuristic approaches for each iteration that could be completed in under four hours and with the memory limits of the machine used  The nonlinear optimization was performed on the NEOS server  which provides a set of machines with varying CPU speeds and memory limitations  The values for each iteration of each problem are given in Figure    We see the heuristic policy iteration  HPI  methods are able to complete more iterations than the optimal methods and as a consequence produce higher values  In fact  the results from HPI are almost always exactly the same as those for the optimal policy iteration algorithm without bounded updates for all iterations that can be completed by the optimal approach  Thus  improvement occurs primarily due to the larger number of backups that can be performed  We also see that while incorporating bounded updates improves value for the optimal algorithm  incorporating the NLP approach into the heuristic approach produces even higher value  Optimizing the NLP requires a small time overhead  but substantially increases value on each iteration  This results in the highest controller value in each problem  Using the NLP also allows our heuristic policy iteration to converge to a six node controller for each agent in the two agent tiger problem  Unfortunately  this solution is known to be suboptimal  As an heuristic algorithm  this is not unexpected  and it should be noted that even suboptimal solutions by the heuristic approach outperform all other methods in all our test problems      Discussion We have demonstrated how policy iteration can be used to improve both correlated and independent joint controllers  We showed that using controller reductions together with exhaustive backups is more efficient in terms of memory than using exhaustive backups alone  However  due to the complexity of exhaustive backups  even that approach could only complete a few iterations on each of our test problems  Using bounded backups alone provided a good way to deal with the complexity issues  With bounded backups  we were able to find higher valued policies than with the previous approach  Through our experiments  we were able to understand how the sizes of the local controllers and correlation device affect the final values obtained  With our heuristic policy iteration algorithm  we demonstrated further improvement by dealing with some of the complexity issues  The heuristic approach is often able to continue       Policy Iteration for DEC POMDPs   a    b    c  Figure    Comparison of the dynamic programming algorithms on  a  the two agent tiger problem   b  the meeting in a grid problem and  c  the box pushing problem  The value produced by policy iteration with and without bounded backups as well as our heuristic policy iteration with and without optimizing the NLP were compared on each iteration until the time or memory limit was reached   improving solution quality past the point where the optimal algorithm exhausts resources  More efficient use of this limited representation size is achieved by incorporating the NLP approach as well  In fact  the heuristic algorithm with NLP improvements at each step provided results that are at least equal to the highest value obtained in each problem and sometimes were markedly higher than the other approaches  Furthermore  as far as we know  these results are the highest published values for all three of the test domains      Conclusion We present a policy iteration algorithm for DEC POMDPs  The algorithm uses a novel policy representation consisting of stochastic finite state controllers for each agent along with a correlation device  We define value preserving transformations and show that alternating between exhaustive backups and value preserving transformations leads to convergence to       Bernstein  Amato  Hansen    Zilberstein  optimality  We also extend controller reductions and bounded backups from the single agent case to the multiagent case  Both of these operations are value preserving transformations and are provably efficient  Finally  we introduced a heuristic version of our algorithm which is more scalable and produces higher values on our test problems  Our algorithm serves as the first nontrivial exact algorithm for DEC POMDPs  and provides a bridge to the large body of work on dynamic programming for POMDPs  Our work provides a solid foundation for solving DEC POMDPs  but much work remains in addressing more challenging problem instances  We focused on solving general DECPOMDPs  but the efficiency of our approaches could be improved by using structure found in certain problems  This would allow specialized representations and solution techniques to be incorporated  Below we describe some key challenges of our general approach  along with some preliminary algorithmic ideas to extend our work on policy iteration  Approximation with Error Bounds Often  strict optimality requirements cause computational difficulties  A good compromise is to search for policies that are within some bound of optimal  Our framework is easily generalized to allow for this  Instead of a value preserving transformation  we could define an   value preserving transformation  which insures that the value at all states decreases by at most    We can perform such transformations with no modifications to any of our linear programs  We simply need to relax the requirement on the value for   that is returned  It is easily shown that using an   value preserving transformation at each step leads to convergence to a policy that is   within   of optimal for all states  For controller reductions  relaxing the tolerance may lead to smaller controllers because some value can be sacrificed  For bounded backups  it may help in escaping from local optima  Though relaxing the tolerance for a bounded backup could lead to a decrease in value for some states  a small downward step could lead to higher value overall in the long run  We are currently working on testing these hypotheses empirically  General Sum Games In a general sum game  there is a set of agents  each with its own set of strategies  and a strategy profile is defined to be a tuple of strategies for all agents  Each agent assigns a payoff to each strategy profile  The agents may be noncooperative  so the same strategy profile may be assigned different values for each agent  The DEC POMDP model can be extended to a general sum game by allowing each agent to have its own reward function  In this case  the strategies are the local policies  and a strategy profile is a joint policy  This model is often called a partially observable stochastic game  POSG   Hansen et al         presented a dynamic programming algorithm for finitehorizon POSGs  The algorithm was shown to perform iterated elimination of dominated strategies in the game  Roughly speaking  it eliminates strategies that are not useful for an agent  regardless of the strategies of the other agents  Work remains to be done on extending the notion of a value preserving transformation to the noncooperative case  One possibility is to redefine value preserving transformations so that value is preserved for all agents  This is closely related to the idea of Pareto optimality  In a general sum game  a strategy profile is said to be Pareto optimal if there does not exist another strategy profile that yields higher payoff for all agents  It seems that policy iteration using the revised definition of value preserving transformation would tend to move the controller in the direction of the Pareto optimal set  Another possibility is       Policy Iteration for DEC POMDPs  to define value preserving transformations with respect to specific agents  As each agent transforms its own controller  the joint controller should move towards a Nash equilibrium  Handling Large Numbers of Agents The general DEC POMDP representation presented in this paper grows exponentially with the number of agents  as seen in the growth of the set of joint actions and observations as well as the transition  reward and observation functions  Thus this representation is not feasible for large numbers of agents  However  a compact representation is possible if each agent interacts directly with just a few other agents  We can have a separate state space for each agent  factored transition probabilities  and a reward function that is the sum of local reward functions for clusters of agents  In this case  the problem size is exponential only in the maximum number of agents interacting directly  This idea is closely related to recent work on graphical games  La Mura        Koller   Milch         Once we have a compact representation  the next question to answer is whether we can adapt policy iteration to work efficiently with the representation  This indeed seems possible  With the value preserving transformations we presented  the nodes of the other agents are considered part of the hidden state of the agent under consideration  These techniques modify the controller of the agent to get value improvement for all possible hidden states  When an agents state transitions and rewards do not depend on some other agent  it should not need to consider that agents nodes as part of its hidden state  A specific compact representation along with extensions of different algorithms was proposed by Nair et al           Acknowledgments We thank Martin Allen  Marek Petrik and Siddharth Srivastava for helpful discussions of this work  Marek and Siddharth  in particular  helped formalize and prove Theorem    The anonymous reviewers provided valuable feedback and suggestions  Support for this work was provided in part by the National Science Foundation under grants IIS         and IIS          by NASA under cooperative agreement NCC         and by the Air Force Office of Scientific Research under grants F                and FA                 Appendix A  Proof of Theorem   A correlation device produces a sequence of values that all the agents can observe  Let X be the set of all possible infinite sequences that can be generated by a correlation device  Let Vx   q    s    be the value of the correlated joint controller with respect to some correlation sequence x  X  initial nodes  q  of the agent controllers  and initial state s  of the problem  We will refer to Vx   q    s    simply as Vx  the value of some sequence x  given the controllers for the agents  We define a regular sequence as a sequence that can be generated by a regular expression  Before we prove Theorem    we establish the following property  Lemma   The value of any sequence  whether regular or non regular  can be approximated within any   by some other sequence  Proof  The property holds thanks to the discount factor used in infinite horizon DECPOMDPs  Given a sequence x with value Vx   we can determine another sequence x  such       Bernstein  Amato  Hansen    Zilberstein  that  Vx   Vx        The sequence x  is constructed by choosing the first k elements of x  and then choosing an arbitrary regular or non regular sequence for the remaining elements  kR max As long as k is chosen such that          then  Vx   Vx          Theorem   Given an initial state and a correlated joint controller  there always exists some finite size joint controller without a correlation device that produces at least the same value for the initial state  Proof  Let E represent the expected value of the joint controller with the correlation device  Let V    Vx   x  X  be the set of values produced by all the possible correlation device sequences  Let inf and sup represent the infimum and supremum of V respectively  We break the proof into two cases  depending on the relation of the expectation versus the supremum  We show in each case that a regular sequence can be found that produces at least the same value as E  Once such a regular sequence is found  then that sequence can be generated by a finite state controller that can be embedded within each agent  Thus  a finite number of nodes can be added to the agents controllers to provide equal or greater value  without using a correlation device  Case     inf  E   sup Based on Lemma    there is some regular sequence x that can approximate the supremum within    If we choose     sup E  then Vx  sup     E  Case     E   sup If there is a regular sequence  x  for which Vx   E  we can choose that sequence  If no such regular sequence exists  we will show that E    sup  We give a somewhat informal argument  but this can be more formally proven using cylinder sets as discussed by Parker         We begin by first choosing some regular sequence  We can construct a neighborhood around this sequence  as described in Lemma    by choosing a fixed length prefix of A prefixP of length k has a well defined probability that is defined as P the sequence  P       q           k   q k    where P  q     is the probability distribution P  q P  q c c c c c qc  qc  qck  P  qc of initial node of the correlation device and P  qci  qci    represents the probability of transitioning to correlation device node qci from node qci    The set of sequences that possess this prefix has probability equal to that of the prefix  Because we assumed there exists some regular sequence which has value less than the supremum  we can always choose a prefix and length such that the values of the sequences in the set are less than the supremum  Because the probability of this set is nonzero and the value of these sequences is less than the supremum  then E    sup  which is a contradiction  Therefore  some regular sequence can be found that provides at least the same value as the expected value of the correlated joint controller  This allows some uncorrelated joint controller to produce at least the same value as a given correlated one     Appendix B  Proof of Lemma   For ease of exposition  we prove the lemma under the assumption that there is no correlation device  Including a correlation device is straightforward but unnecessarily tedious        Policy Iteration for DEC POMDPs  Lemma   Let C and D be correlated joint controllers  and let C and D be the results of performing exhaustive backups on C and D  respectively  Then C  D if C  D  Proof  Suppose we are given controllers C and D  where C  D  Call the sets of joint   and R    respectively  It follows that there exists a function nodes for these controllers Q   fi   Qi  Ri for each agent i such that for all s  S and  q  Q V  s   q    X  P   r  q V  s   r      r  We now define functions fi to map between the two controllers C and D  For the old nodes  we define fi to produce the same output as fi   It remains to specify the results of fi applied to the nodes added by the exhaustive backup  New nodes of C will be mapped to distributions involving only new nodes of D  To describe the mapping formally  we need to introduce some new notation  Recall that the new nodes are all deterministic  For each new node  r in controller D  the nodes action is denoted  a  r   and its transition rule is denoted  r     r   o   Now  the mappings fi are defined such that P   r  q    P   a  r   q   YX  P   q     q   a  r    o P   r     r   o   q      q       o  for all  q in controller C and  r in controller D  We must now show that the mapping f satisfies the inequality given in the definition of a value preserving transformation  For the nodes that were not added by the exhaustive backup  this is straightforward  For the new nodes  q of the controller C  we have for all s  S   V  s   q     X  P   a  q  R s   a      X  P  s     o s   a P   q     q   a   o V  s     q        o s     q    a       X  P   a  q  R s   a     X  P  s     o s   a P   q     q   a   o     o s     q    a  X  P   r     q    V  s     r        r       X  P   a  q  R s   a      X  P  s     o s   a P   q     q   a   o P   r     q    V  s     r        o s     q      r    a      X     X  P   r  q  R s   a  r       X  P  s     o s   a  r  V  s     r     r   o      o s     r  P   r  q V  s   r      r          Bernstein  Amato  Hansen    Zilberstein  
 We consider the problem of optimal planning in stochastic domains with resource constraints  where the resources are continuous and the choice of action at each step depends on resource availability  We introduce the HAO  algorithm  a generalization of the AO  algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables  where the continuous variables represent monotonic resources  Like other heuristic search algorithms  HAO  leverages knowledge of the start state and an admissible heuristic to focus computational effort on those parts of the state space that could be reached from the start state by following an optimal policy  We show that this approach is especially effective when resource constraints limit how much of the state space is reachable  Experimental results demonstrate its effectiveness in the domain that motivates our research  automated planning for planetary exploration rovers      Introduction Many NASA planetary exploration missions rely on rovers  mobile robots that carry a suite of scientific instruments for use in characterizing planetary surfaces and transmitting information back to Earth  Because of difficulties in communicating with devices on distant planets  direct human control of rovers by tele operation is infeasible  and rovers must be able to act autonomously for substantial periods of time  For example  the Mars Exploration Rovers  MER   aka  Spirit and Opportunity  are designed to communicate with the ground only twice per Martian day  Autonomous control of planetary exploration rovers presents many challenges for research in automated planning  Progress has been made in meeting some of these challenges  For example  the planning software developed for the Mars Sojourner and MER rovers has contributed significantly  c      AI Access Foundation  All rights reserved    Meuleau  Benazera  Brafman  Hansen   Mausam  to the success of these missions  Bresina  Jonsson  Morris    Rajan         But many important challenges must still be addressed to achieve the more ambitious goals of future missions  Bresina  Dearden  Meuleau  Ramakrishnan  Smith    Washington         Among these challenges is the problem of plan execution in uncertain environments  On planetary surfaces such as Mars  there is uncertainty about the terrain  meteorological conditions  and the state of the rover itself  position  battery charge  solar panels  component wear  etc   In turn  this leads to uncertainty about the outcome of the rovers actions  Much of this uncertainty is about resource consumption  For example  factors such as slope and terrain affect speed of movement and rate of power consumption  making it difficult to predict with certainty how long it will take for a rover to travel between two points  or how much power it will consume in doing so  Because of limits on critical resources such as time and battery power  rover plans are currently very conservative and based on worst case estimates of time and resource usage  In addition  instructions sent to planetary rovers are in the form of a sequential plan for attaining a single goal  e g   photographing an interesting rock   If an action has an unintended outcome that causes a plan to fail  the rover stops and waits for further instructions  it makes no attempt to recover or achieve an alternative goal  This can result in under utilized resources and missed science opportunities  Over the past decade  there has been a great deal of research on how to generate conditional plans in domains with uncertain action outcomes  Much of this work is formalized in the framework of Markov decision processes  Puterman        Boutilier  Dean    Hanks         However  as Bresina et al         point out  important aspects of the rover planning problem are not adequately handled by traditional planning algorithms  including algorithms for Markov decision processes  In particular  most traditional planners assume a discrete state space and a small discrete number of action outcomes  But in automated planning for planetary exploration rovers  critical resources such as time and battery power are continuous  and most of the uncertainty in the domain results from the effect of actions on these variables  This requires a conditional planner that can branch not only on discrete action outcomes  but on the availability of continuous resources  and such a planner must be able to reason about continuous as well as discrete state variables  Closely related to the challenges of uncertain plan execution and continuous resources is the challenge of over subscription planning  The rovers of future missions will have much improved capabilities  Whereas the current MER rovers require an average of three days to visit a single rock  progress in areas such as automatic instrument placement will allow rovers to visit multiple rocks and perform a large number of scientific observations in a single communication cycle  Pedersen  Smith  Deans  Sargent  Kunz  Lees    Rajagopalan         Moreover  communication cycles will lengthen substantially in more distant missions to the moons of Jupiter and Saturn  requiring longer periods of autonomous behavior  As a result  space scientists of future missions are expected to specify a large number of science goals at once  and often this will present what is known as an oversubscription planning problem  This refers to a problem in which it is infeasible to achieve all goals  and the objective is to achieve the best subset of goals within resource constraints  Smith         In the case of the rover  there will be multiple locations the rover could reach  and many experiments the rover could conduct  most combinations of which are infeasible due to resource constraints  The planner must select a feasible subset of these that maximizes expected science return  When action outcomes  including resource consumption  are stochastic  a plan that maximizes expected science return will be a conditional plan that prescribes different courses of action based on the results of previous actions  including resource availability  In this paper  we present an implemented planning algorithm that handles all of these problems together  uncertain action outcomes  limited continuous resources  and over subscription planning  We formalize the rover planning problem as a hybrid state Markov decision process  that is  a Markov decision process  MDP  with both discrete and continuous state variables  and we use the continuous variables to represent resources  The planning algorithm we introduce is a heuristic search algorithm called HAO   for Hybrid state AO   It is a generalization of the classic AO  heuristic search algorithm  Nilsson        Pearl         Whereas AO  searches in discrete state spaces  HAO  solves       HAO   planning problems in hybrid domains with both discrete and continuous state variables  To handle hybrid domains  HAO  builds on earlier work on dynamic programming algorithms for continuous and hybrid state MDPs  in particular  the work of Feng et al          Generalizing AND OR graph search for hybrid state spaces poses a complex challenge  and we only consider a special case of the problem  In particular  continuous variables are used to represent monotonic resources  The search is for the best conditional plan that allows branching not only on the values of the discrete variables  but on the availability of these resources  and does not violate a resource constraint  It is well known that heuristic search can be more efficient than dynamic programming because it uses reachability analysis guided by a heuristic to focus computation on the relevant parts of the state space  We show that for problems with resource constraints  including over subscription planning problems  heuristic search is especially effective because resource constraints can significantly limit reachability  Unlike dynamic programming  a systematic forward search algorithm such as AO  keeps track of the trajectory from the start state to each reachable state  and thus it can check whether the trajectory is feasible or violates a resource constraint  By pruning infeasible trajectories  a heuristic search algorithm can dramatically reduce the number of states that must be considered to find an optimal policy  This is particularly important in our domain where the discrete state space is huge  exponential in the number of goals   and yet the portion reachable from any initial state is relatively small  due to resource constraints      Problem Formulation and Background We start with a formal definition of the planning problem we are tackling  It is a special case of a hybrid state Markov decision process  and so we first define this model  Then we discuss how to include resource constraints and formalize over subscription planning in this model  Finally we review a class of dynamic programming algorithms for solving hybrid state MDPs  since some of these algorithmic techniques will be incorporated in the heuristic search algorithm we develop in Section        Hybrid State Markov Decision Process A hybrid state Markov decision process  or hybrid state MDP  is a factored Markov decision process that has both discrete and continuous state variables  We define it as a tuple  N  X  A  P  R   where N is a discrete state variable  X    X    X         Xd   is a set of continuous state variables  A is a set of actions  P is a stochastic state transition model  and R is a reward function  We describe these elements in more detail below  A hybrid state MDP is sometimes referred to as simply a hybrid MDP  The term hybrid does not refer to the dynamics of the model  which are discrete  Another term for a hybrid state MDP  which originates in the Markov chain literature  is a general state MDP  Although a hybrid state MDP can have multiple discrete variables  this plays no role in the algorithms described in this paper  and so  for notational convenience  we model the discrete component of the state space as a single variable N   Our focus is on the continuous component  We assume N the domain of each continuous variable Xi  X is a closed interval of the real line  and so X   i Xi is the hypercube over which the continuous variables are defined  The state set S of a hybrid state MDP is the set of all possible assignments of values to the state variables  In particular  a hybrid state s  S is a pair  n  x  where n  N is the value of the discrete variable  and x    xi   is a vector of values of the continuous variables  State transitions occur as a result of actions  and the process evolves according to Markovian state transition probabilities Pr s    s  a   where s    n  x  denotes the state before action a and s     n    x    denotes the state after action a  also called the arrival state  These probabilities can be decomposed into        Meuleau  Benazera  Brafman  Hansen   Mausam   the discrete marginals Pr n   n  x  a   For all  n  x  a    Pr n   n  x  a       R  the continuous conditionals Pr x   n  x  a  n     For all  n  x  a  n     x  X Pr x   n  x  a  n   dx       P  n  N  We assume the reward associated with a transition is a function of the arrival state only  and let Rn  x  denote the reward associated with a transition to state  n  x   More complex dependencies are possible  but this is sufficient for the goal based domain models we consider in this paper      Resource Constraints and Over Subscription Planning To model the rover planning problem  we consider a special type of MDP in which the objective is to optimize expected cumulative reward subject to resource constraints  We make the following assumptions   there is an initial allocation of one or more non replenishable resources   each action has some minimum positive consumption of at least one resource  and  once resources are exhausted  no further action can be taken  One way to model an MDP with resource constraints is to formulate it as a constrained MDP  a model that has been widely studied in the operations research community  Altman         In this model  each action a incurs a transition dependent resource cost  Cai  s  s     for each resource i  Given an initial allocation of resources and an initial state  linear programming is used to find the best feasible policy  which may be a randomized policy  Although a constrained MDP models resource consumption  it does not include resources in the state space  As a result  a policy cannot be conditioned upon resource availability  This is not a problem if resource consumption is either deterministic or unobservable  But it is not a good fit for the rover domain  in which resource consumption is stochastic and observable  and the rover should take different actions depending on current resource availability  We adopt a different approach to modeling resource constraints in which resources are included in the state description  Although this increases the size of the state space  it allows decisions to be made based on resource availability  and it allows a stochastic model of resource consumption  Since resources in the rover domain are continuous  we use the continuous variables of a hybrid state MDP to represent resources  Note that the duration of actions is one of the biggest sources of uncertainty in our rover problems  and we model time as one of the continuous resources  Resource constraints are represented in the form of executability constraints on actions  where An  x  denotes the set of actions executable in state  n  x   An action cannot be executed in a state that does not satisfy its minimum resource requirements  Having discussed how to incorporate resource consumption and resource constraints in a hybridstate MDP  we next discuss how to formalize over subscription planning  In our rover planning problem  scientists provide the planner with a set of goals they would like the rover to achieve  where each goal corresponds to a scientific task such as taking a picture of a rock or performing an analysis of a soil sample  The scientists also specify a utility or reward for each goal  Usually only a subset of these goals is feasible under resource constraints  and the problem is to find a feasible plan that maximizes expected utility  Over subscription planning for planetary exploration rovers has been considered by Smith        and van den Briel et al         for deterministic domains  We consider over subscription planning in stochastic domains  especially domains with stochastic resource consumption  This requires construction of conditional plans in which the selection of goals to achieve can change depending on resource availability  In over subscription planning  the utility associated with each goal can be achieved only once  no additional utility is achieved for repeating the task  Therefore  the discrete state must include a set of Boolean variables to keep track of the set of goals achieved so far by the rover  with one Boolean      HAO   variable for each goal  Keeping track of already achieved goals ensures a Markovian reward structure  since achievement of a goal is rewarded only if it was not achieved in the past  However  it also significantly increases the size of the discrete state space  Maintaining history information to ensure a Markovian reward structure is a simple example of planning with non Markovian rewards  Thiebaux  Gretton  Slaney  Price    Kabanza             Optimality Equation The rover planning problem we consider is a special case of a finite horizon hybrid state MDP in which termination occurs after an indefinite number of steps  The Bellman optimality equation for this problem takes the following form  Vn  x      Vn  x        when  n  x  is a terminal state  otherwise      Z X max Pr n    n  x  a  Pr x    n  x  a  n     Rn   x      Vn   x     dx     aAn  x   n  N       x   We define a terminal state as a state in which no actions are eligible to execute  that is  An  x      We use terminal states to model various conditions for plan termination  This includes the situation in which all goals have been achieved  the situation in which resources have been exhausted  and the situation in which an action results in some error condition that requires executing a safe sequence by the rover and terminating plan execution  In addition to terminal states  we assume an explicit initial state denoted  n    x     Assuming that resources are limited and non replenishable  and that every action consumes some resource  and the amount consumed is greater than or equal to some positive quantity c   plan execution will terminate after a finite number of steps  The maximum number of steps is bounded by the initial resource allocation divided by c  the minimal resource consumption per step  The actual number of steps is usually much less and indefinite  because resource consumption is stochastic and because the choice of action influences resource consumption  Because the number of steps it takes for a plan to terminate is bounded but indefinite  we call this a bounded horizon MDP in contrast to a finite horizon MDP  However  we note that any bounded horizon MDP can be converted to a finite horizon MDP by specifying a horizon that is equal to the maximum number of plan steps  and introducing a no op action that is taken in any terminal state  Note that there is usually a difference between the number of plan steps and the time a plan takes to execute  Since we model time as one of the continuous resources  the time it takes to execute a plan step is both state and action dependent  and stochastic  Given a hybrid state MDP with a set of terminal states and an initial state  n    x     the objective is to find a policy      N  X   A  that maximizes expected cumulative reward  specifically  an optimal policy has a value function that satisfies the optimality equation given by Equation      In our rover domain  cumulative reward is equal to the sum of rewards for the goals achieved before reaching a terminal state and there is no direct incentive to save resources  an optimal solution saves resources only if this allows achieving more goals  However  our framework is general enough to allow reasoning about both the cost and the availability of resources  For example  an incentive for conserving resources could be modeled by specifying a reward that is proportional to the amount of resources left unused upon entering the terminal state  Note that our framework allows reasoning about both the cost and availability of resources without needing to formulate this as a problem of multi objective optimization  and we stay in a standard decision theoretic framework      Dynamic Programming for Continuous State and Hybrid State MDPs Because the planning problem we consider is a finite horizon hybrid state MDP  it can be solved by any algorithm for solving finite horizon hybrid state MDPs  Most algorithms for solving hybridstate  and continuous state  MDPs rely on some form of approximation  A widely used approach is      Meuleau  Benazera  Brafman  Hansen   Mausam  Figure    Value function in the initial state of a simple rover problem  optimal expected return as a function of two continuous variables  time and energy remaining    to discretize the continuous state space into a finite number of grid points and solve the resulting finite state MDP using dynamic programming and interpolation  Rust        Munos   Moore         Another approach is parametric function approximation  a function associated with the dynamic programming problem  such as the value function or policy function  is approximated by a smooth function of k unknown parameters  In general  parametric function approximation is faster than grid based approximation  but has the drawback that it may fail to converge  or may converge to an incorrect solution  Parametric function approximation is used by other algorithms for solving continuous state MDPs besides dynamic programming  Reinforcement learning algorithms use artificial neural networks as function approximators  Bertsekas   Tsitsiklis         An approach to solving MDPs called approximate linear programming has been extended to allow continuous as well as discrete state variables  Kveton  Hauskrecht    Guestrin         We review another approach to solving hybrid state  or continuous state  MDPs that assumes the problem has special structure that can be exploited by the dynamic programming algorithm  R The structure assumed by this approach ensures that the convolution x  Pr x    n  x  a  n    Rn   x     Vn   x    dx  in Equation     can be computed exactly in finite time  and the value function computed by dynamic programming is piecewise constant or piecewise linear  The initial idea for this approach is due to the work of Boyan and Littman         who describe a class of MDPs called time dependent MDPs  in which transitions take place along a single  irreversible continuous dimension  They describe a dynamic programming algorithm for computing an exact piecewise linear value function when the transition probabilities are discrete and rewards are piecewise linear  Feng et al         extend this approach to continuous state spaces of more than one dimension  and consider MDPs with discrete transition probabilities and two types of reward models  piecewise constant and piecewise linear  Li and Littman        further extend the approach to allow transition probabilities that are piecewise constant  instead of discrete  although this extension requires some approximation in the dynamic programming algorithm  The problem structure exploited by these algorithms is characteristic of the Mars rover domain and other over subscription planning problems  Figure   shows the optimal value functions from the initial state of a typical Mars rover problem as a function of two continuous variables  the time and energy remaining  Bresina et al          The value functions feature a set of humps and plateaus  each of them representing a region of the state space where similar goals are pursued by the optimal policy  The sharpness of a hump or plateau reflects uncertainty about achieving the goal s   Constraints that impose minimal resource levels before attempting some actions introduce       HAO   sharp cuts in the regions  Plateau regions where the expected reward is nearly constant represent regions of the state space where the optimal policy is the same  and the probability distribution over future histories induced by this optimal policy is nearly constant  The structure in such a value function can be exploited by partitioning the continuous state space into a finite number of hyper rectangular regions   A region is a hyper rectangle if it is the Cartesian product of intervals at each dimension   In each hyper rectangle  the value function is either constant  for a piecewise constant function  or linear  for a piecewise linear function   The resolution of the hyper rectangular partitioning is adjusted to fit the value function  Large hyperrectangles are used to represent large plateaus  Small hyper rectangles are used to represent regions of the state space where a finer discretization of the value function is useful  such as the edges of plateaus and the curved hump where there is more time and energy available  A natural choice of data structures for rectangular partitioning of a continuous space is kd trees  Friedman  Bentley    Finkel         although other choices are possible  Figures   and    in Section     show value functions for the initial state of a simple rover planning problem  created by a piecewise constant partitioning of the continuous state space  The continuous state domains of the transition and reward functions are similarly partitioned into hyper rectangles  The reward function of each action has the same piecewise constant  or piecewiselinear  representation as the value function  The transition function partitions the state space into regions for which the set of outcomes of an action and the probability distribution over the set of outcomes are identical  Following Boyan and Littman         both relative and absolute transitions are supported  A relative outcome can be viewed as shifting a region by a constant   That is  for any two states x and y in the same region  the transition probabilitiesP r x   x  a  and P r y    y  a  are defined in term of the probability of   such that     x   x     y    y   An absolute outcome maps all states in a region to a single state  That is  for any two states x and y in the same region  P r x   x  a    P r x   y  a   We can view a relative outcome as a pair    p   where p is the probability of that outcome  and we can view an absolute outcome as a pair  x    p   This assumes there is only a finite number of non zero probabilities  i e   the probability distribution is discretized  which means that for any state and action  a finite set of states can be reached with non zero probability  This representation guarantees that a dynamic programming update of a piecewise constant value function results in another piecewise constant value function  Feng et al         show that for such transition functions and for any finite horizon  there exists a partition of the continuous space into hyper rectangles over which the optimal value function is piecewise constant or linear  The restriction to discrete transition functions is a strong one  and often means the transition function must be approximated  For example  rover power consumption is normally distributed  and thus must be discretized   Since the amount of power available must be non negative  our implementation truncates any negative part of the normal distribution and renormalizes   Any continuous transition function can be approximated by an appropriately fine discretization  and Feng et al         argue that this provides an attractive alternative to function approximation approaches in that it approximates the model but then solves the approximate model exactly  rather than finding an approximate value function for the original model   For this reason  we will sometimes refer to finding optimal policies and value functions  even when the model has been approximated   To avoid discretizing the transition function  Li and Littman        describe an algorithm that allows piecewise constant transition functions  in exchange for some approximation in the dynamic programming algorithm  Marecki et al        describe a different approach to this class of problems in which probability distributions over resource consumptions are represented with phase type distributions and a dynamic programming algorithm exploits this representation  Although we use the work of Feng et al         in our implementation  the heuristic search algorithm we develop in the next section could use any of these or some other approach to representing and computing value functions and policies for a hybrid state MDP        Meuleau  Benazera  Brafman  Hansen   Mausam     Heuristic Search in a Hybrid State Space In this section  we present the primary contribution of this paper  an approach to solving a special class of hybrid state MDPs using a novel generalization of the heuristic search algorithm AO   In particular  we describe a generalization of this algorithm for solving hybrid state MDPs in which the continuous variables represent monotonic and constrained resources and the acyclic plan found by the search algorithm allows branching on the availability of these resources  The motivation for using heuristic search is the potentially huge size of the state space  which makes dynamic programming infeasible  One reason for this size is the existence of continuous variables  But even if we only consider the discrete component of the state space  the size of the state space is exponential in the number of discrete variables  As is well known  AO  can be very effective in solving planning problems that have a large state space because it only considers states that are reachable from an initial state  and it uses an informative heuristic function to focus on states that are reachable in the course of executing a good plan  As a result  AO  can often find an optimal plan by exploring a small fraction of the entire state space  We begin this section with a review of the standard AO  algorithm  Then we consider how to generalize AO  to search in a hybrid state space and discuss the properties of the generalized algorithm  as well as its most efficient implementations      AO  Recall that AO  is an algorithm for AND OR graph search problems  Nilsson        Pearl         Such graphs arise in problems where there are choices  the OR components   and each choice can have multiple consequences  the AND component   as is the case in planning under uncertainty  Hansen and Zilberstein        show how AND OR graph search techniques can be used in solving MDPs  Following Nilsson        and Hansen and Zilberstein         we define an AND OR graph as a hypergraph  Instead of arcs that connect pairs of nodes as in an ordinary graph  a hypergraph has hyperarcs  or k connectors  that connect a node to a set of k successor nodes  When an MDP is represented by a hypergraph  each node corresponds to a state  the root node corresponds to the start state  and the leaf nodes correspond to terminal states  Thus we often use the word state to refer to the corresponding node in the hypergraph representing an MDP  A k connector corresponds to an action that transforms a state into one of k possible successor states  with a probability attached to each successor such that the probabilities sum to one  In this paper  we assume the AND OR graph is acyclic  which is consistent with our assumption that the underlying MDP has a bounded horizon  In AND OR graph search  a solution takes the form of an acyclic subgraph called a solution graph  which is defined as follows   the start node belongs to a solution graph   for every non terminal node in a solution graph  exactly one outgoing k connector  corresponding to an action  is part of the solution graph and each of its successor nodes also belongs to the solution graph   every directed path in the solution graph terminates at a terminal node  A solution graph that maximizes expected cumulative reward is found by solving the following system of equations      if s is a terminal state  otherwise   P   V   s               maxaA s  P   s S r s  s  a   R s     V  s      where V   s  denotes the expected value of an optimal solution for state s  and V  is called the optimal evaluation function  or value function in MDP terminology   Note that this is identical to      HAO   the optimality equation for hybrid state MDPs defined in Equation      if the latter is restricted to a discrete state space  In keeping with the convention in the literature on MDPs  we treat this as a value maximization problem even though AO  is usually formalized as solving a cost minimization problem  For state space search problems that are formalized as AND OR graphs  an optimal solution graph can be found using the heuristic search algorithm AO   Nilsson        Pearl         Like other heuristic search algorithms  the advantage of AO  over dynamic programming is that it can find an optimal solution for a particular starting state without evaluating all problem states  Therefore  a graph is not usually supplied explicitly to the search algorithm  An implicit graph  G  is specified implicitly by a start node or start state s and a successor function that generates the successors states for any state action pair  The search algorithm constructs an explicit graph  G    that initially consists only of the start state  A tip or leaf state of the explicit graph is said to be terminal if it is a goal state  or some other state in which no action can be taken   otherwise  it is said to be nonterminal  A nonterminal tip state can be expanded by adding to the explicit graph its outgoing k connectors  one for each action  and any successor states not already in the explicit graph  AO  solves a state space search problem by gradually building a solution graph  beginning from the start state  A partial solution graph is defined similarly to a solution graph  with the difference that tip states of a partial solution graph may be nonterminal states of the implicit AND OR graph  A partial solution graph is defined as follows   the start state belongs to a partial solution graph   for every non tip state in a partial solution graph  exactly one outgoing k connector  corresponding to an action  is part of the partial solution graph and each of its successor states also belongs to the partial solution graph   every directed path in a partial solution graph terminates at a tip state of the explicit graph  The value of a partial solution graph is defined similarly to the value of a solution graph  The difference is that if a tip state of a partial solution graph is nonterminal  it does not have a value that can be propagated backwards  Instead  we assume there is an admissible heuristic estimate H s  of the maximal value solution graph for state s  A heuristic evaluation function H is said to be admissible if H s   V   s  for every state s  We can recursively calculate an admissible heuristic estimate V  s  of the optimal value of any state s in the explicit graph as follows      if s is a terminal state  V  s    a nonterminal tip state     H s  if s is P       maxaA s  s  S P r s  s  a   R s     V  s    otherwise        The best partial solution graph can be determined at any time by propagating heuristic estimates from the tip states of the explicit graph to the start state  If we mark the action that maximizes the value of each state  the best partial solution graph can be determined by starting at the root of the graph and selecting the best  i e   marked  action for each reachable state  Table   outlines the AO  algorithm for finding an optimal solution graph in an acyclic AND OR graph  It interleaves forward expansion of the best partial solution with a value update step that updates estimated state values and the best partial solution  In the simplest version of AO   the values of the expanded state and all of its ancestor states in the explicit graph are updated  But in fact  the only ancestor states that need to be re evaluated are those from which the expanded state can be reached by taking marked actions  i e   by choosing the best action for each state   Thus  the parenthetical remark in step   b i of Table   indicates that a parent s  of state s is not added to Z unless both the estimated value of state s has changed and state s can be reached from state s  by choosing the best action for state s    AO  terminates when the policy expansion step does not       Meuleau  Benazera  Brafman  Hansen   Mausam     The explicit graph G  initially consists of the start state s       While the best solution graph has some nonterminal tip state   a  Expand best partial solution  Expand some nonterminal tip state s of the best partial solution graph and add any new successor states to G    For each new state s  added to G  by expanding s  if s  is a terminal state then V  s          else V  s       H s      b  Update state values and mark best actions  i  Create a set Z that contains the expanded state and all of its ancestors in the explicit graph along marked action arcs   I e   only include ancestor states from which the expanded state can be reached by following the current best solution   ii  Repeat the following steps until Z is empty  A  Remove from Z a state s such that no descendant of s in G  occurs in Z  P B  Set V  s     maxaA s  s  P r s   s  a   R s      V  s     and mark the best action for s   When determining the best action resolve ties arbitrarily  but give preference to the currently marked action    c  Identify the best solution graph and all nonterminal states on its fringe    Return an optimal solution graph  Table    AO  algorithm  find any nonterminal states on the fringe of the best solution graph  At this point  the best solution graph is an optimal solution  Following the literature on AND OR graph search  we have so far referred to the solution found by AO  as a solution graph  But in the following  when AO  is used to solve an MDP  we sometimes follow the literature on MDPs in referring to a solution as a policy  We also sometimes refer to it as a policy graph  to indicate that a policy is represented in the form of a graph      Hybrid State AO  We now consider how to generalize AO  to solve a bounded horizon hybrid state MDP  The challenge we face in applying AO  to this problem is the challenge of performing state space search in a hybrid state space  The solution we adopt is to search in an aggregate state space that is represented by an AND OR graph in which there is a node for each distinct value of the discrete component of the state  In other words  each node of the AND OR graph represents a region of the continuous state space in which the discrete value is the same  Given this partition of the continuous state space  we use AND OR graph search techniques to solve the MDP for those parts of the state space that are reachable from the start state under the best policy  However  AND OR graph search techniques must be modified in important ways to allow search in a hybrid state space that is represented in this way  In particular  there is no longer a correspondence between the nodes of the AND OR graph and individual states  Each node now corresponds to a continuous region of the state space  and different actions may be optimal for different hybrid states associated with the same search node  In the case of rover planning  for example  the best action is likely to depend on how much energy or time is remaining  and energy and time are continuous state variables  To address this problem and still find an optimal solution  we attach to each search node a set of functions  of the continuous variables  that make it possible to associate different values  heuristics  and actions with different hybrid states that map to the same search node  As before  the explicit       HAO   search graph consists of all nodes and edges of the AND OR graph that have been generated so far  and describes all the states that have been considered so far by the search algorithm  The difference is that we use a more complex state representation in which a set of continuous functions allows representation and reasoning about the continuous part of the state space associated with a search node  We begin by describing this more complex node data structure  and then we describe the HAO  algorithm        Data Structures Each node n of the explicit AND OR graph G  consists of the following   The value of the discrete state variable   Pointers to its parents and children in the explicit graph and the policy graph   Openn             the Open list  For each x  X  Openn  x  indicates whether  n  x  is on the frontier of the explicit graph  i e   generated but not yet expanded   Closedn             the Closed list  For each x  X  Closedn  x  indicates whether  n  x  is in the interior of the explicit graph  i e   already expanded  Note that  for all  n  x   Openn  x   Closedn  x       A state cannot be both open and closed   There can be parts of the continuous state space associated with a node that are neither open nor closed  Until the explicit graph contains a trajectory from the start state to a particular hybrid state  that hybrid state is not considered generated  even if the search node to which it corresponds has been generated  such states are neither open nor closed  In addition  only non terminal states can be open or closed  Note that we do not refer to open or closed nodes  instead  we refer to the hybrid states associated with nodes as being open or closed   Hn     the heuristic function  For each x  X  Hn  x  is a heuristic estimate of the optimal expected cumulative reward from state  n  x    Vn     the value function  For any open state  n  x   Vn  x    Hn  x   For any closed state  n  x   Vn  x  is obtained by backing up the values of its successor states  as in Equation       n     A  the policy  Note that it is defined for closed states only   Reachablen             For each x  X  Reachablen  x  indicates whether  n  x  is reachable by executing the current best policy beginning from the start state  n    x     We assume that these various continuous functions  which represent information about the hybrid states associated with a search node  partition the state space associated with a node into a discrete number of regions  and associate a distinct value or action with each region  Given such a partitioning  the HAO  algorithm expands and evaluates these regions of the hybrid state space  instead of individual hybrid states  The finiteness of the partition is important in order to ensure that the search frontier can be extended by a finite number of expansions  and to ensure that HAO  can terminate after a finite number of steps  In our implementation of HAO   described in Section    we use the piecewise constant partitioning of a continuous state space proposed by Feng et al          However  any method of discrete partitioning could be used  provided that the condition above holds  for example  Li and Littman        describe an alternative method of partitioning  Note that two forms of state space partitioning are used in our algorithm  First  the hybrid state space is partitioned into a finite number of regions  one for each discrete state  where each of these       Meuleau  Benazera  Brafman  Hansen   Mausam  regions corresponds to a node of the AND OR graph  Second  the continuous state space associated with a particular node is further partitioned into smaller regions based on a piecewise constant representation of a continuous function  such as the one used by Feng et al          In addition to this more complex representation of the nodes of an AND OR graph  our algorithm requires a more complex definition of the the best  partial  solution  In standard AO   the oneto one correspondence between nodes and individual states means that a solution or policy can be represented entirely by a graph  called the  partial  solution graph  in which a single action is associated with each node  In the HAO  algorithm  a continuum of states is associated with each node  and different actions may be optimal for different regions of the state space associated with a particular node  For the HAO  algorithm  a  partial  solution graph is a sub graph of the explicit graph that is defined as follows   the start node belongs to a solution graph   for every non tip node in a solution graph  one or more outgoing k connectors are part of the solution graph  one for each action that is optimal for some hybrid state associated with the node  and each of their successor nodes also belongs to the solution graph   every directed path in the solution graph terminates at a tip node of the explicit graph  The key difference in this definition is that there may be more than one optimal action associated with a node  since different actions may be optimal for different hybrid states associated with the node  A policy is represented not only by a solution graph  but by the continuous functions n     and Reachablen      In particular  a  partial  policy  specifies an action for each reachable region of the continuous state space  The best  partial  policy is the one that satisfies the following optimality equation  Vn  x      Vn  x     Hn  x  when  n  x  is a nonterminal open state      Z X   max Pr n    n  x  a  Pr x    n  x  a  n     Rn   x      Vn   x     dx     Vn  x     when  n  x  is a terminal state   aAn  x   n  N       x   Note that this optimality equation is only satisfied for regions of the state space that are reachable from the start state   n    x    by following an optimal policy        Algorithm Table   gives a high level summary of the HAO  algorithm  In outline  it is the same as the AO  algorithm  and consists of iteration of the same three steps  solution  or policy  expansion  use of dynamic programming to update the current value function and policy  and analysis of reachability to identify the frontier of the solution that is eligible for expansion  In detail  it is modified in several important ways to allow search of a hybrid state space  In the following  we discuss the modifications to each of these three steps  Policy expansion All nodes of the current solution graph are identified and one or more open regions associated with these nodes are selected for expansion  That is  one or more regions of the hybrid state space in the intersection of Open and Reachable is chosen for expansion  All actions applicable to the states in these open regions are simulated  and the results of these actions are added to the explicit graph  In some cases  this means adding a new node to the AND OR graph  In other cases  it simply involves marking one or more regions of the continuous state space associated with an existing node as open  More specifically  when an action leads to a new node  this node is added to the explicit graph  and all states corresponding to this node that are reachable from the expanded region s  after the action under consideration are marked as open  When an action leads to an      HAO      The explicit graph G  initially consists of the start node and corresponding start state  n    x     marked as open and reachable     While Reachablen  x   Openn  x  is non empty for some  n  x    a  Expand best partial solution  Expand one or more region s  of open states on the frontier of the explicit state space that is reachable by following the best partial policy  Add new successor states to G    In some cases  this requires adding a new node to the AND OR graph  In other cases  it simply involves marking one or more regions of the continuous state space associated with an existing node as open  States in the expanded region s  are marked as closed   b  Update state values and mark best actions  i  Create a set Z that contains the node s  associated with the just expanded regions of states and all ancestor nodes in the explicit graph along marked action arcs  ii  Decompose the part of the explicit AND OR graph that consists of nodes in Z into strongly connected components  iii  Repeat the following steps until Z is empty  A  Remove from Z a set of nodes such that     they all belong to the same connected component  and     no descendant of these nodes occurs in Z  B  For every node n in this connected component and for all states  n  x  in any expanded region of node n  set Vn  x       max aAn  x   X  Pr n    n  x  a   Z    Pr x    n  x  a  n     Rn   x      Vn   x     dx     x   n  N  and mark the best action   When determining the best action resolve ties arbitrarily  but give preference to the currently marked action   Repeat until there is no longer a change of value for any of these nodes   c  Identify the best solution graph and all nonterminal states on its frontier  This step updates Reachablen  x      Return an optimal policy  Table    HAO  algorithm  existing node  any region s  of Markov states in this node that is both reachable from the expanded region s  and not marked as closed  is marked open  Expanded regions of the state space are marked as closed  Thus  different regions associated with the same node can be opened and expanded at different times  This process is illustrated in Figure    In this figure  nodes corresponding to a distinct value for the discrete state are represented as rectangles  and circular connectors represent actions  For each node  we see how many distinct continuous regions exist  For each such region we see whether it is closed  C  or open  O   and whether it is reachable from the initial state  R  when executing the current best policy  OPT   For instance  in Figure   a   node At Start  has a single region marked closed and reachable  and node Lost has two regions  the smallest  open and reachable  and the largest  closed and unreachable  Dynamic programming As in standard AO   the value of any newly expanded node n must be updated by computing a Bellman backup based on the value functions of the children of n       Meuleau  Benazera  Brafman  Hansen   Mausam  At Start   At Loc   O  At Start  C  C  R  R  Navigate  Start  Loc    At Loc    OPT  C  R  Navigate  Start  Loc    OPT  R  Navigate  Loc   Loc    Lost O  C  Lost O  R  O  C  C  R  At Loc    Panoramic Camera  O   a  Before expansion  Panoramic Camera   b  After expansion  Figure    Expanding a region of the state space   a  Before expansion  The nodes At Start   At Loc   and Lost have been previously created  The unique region in At Loc   is the next region to be expanded   b  After expansion  The action Navigate Loc   Loc   that can be applied in the expanded region has been added to the graph  This action can lead either to the preexisting node Lost  or to the new node At Loc    The expanded region  in At Loc     as well as the continuous regions reachable from there  in Lost and At Loc     are highlighted in a dotted framed  Following expansion  the expanded region is closed  Discrete state At Loc   has been added to the graph and all its reachable regions are open  Additionally  new open regions have been added to node Lost   in the explicit graph  For each expanded region of the state space associated with node n  each action is evaluated  the best action is selected  and the corresponding continuous value function is associated with the region  The continuous state value function is computed by evaluating the continuous integral in Equation      We can use any method for computing this integral  In our implementation  we use the dynamic programming algorithm of Feng et al          As reviewed in Section      they show that the continuous integral over x  can be computed exactly  as long as the transition and reward functions satisfy certain conditions  Note that  with some hybrid state dynamic programming techniques such as Feng et al          dynamic programming backups may increase the number of pieces of the value function attached to the updated regions  Figure   a    Once the expanded regions of the continuous state space associated with a node n are reevaluated  the new values must be propagated backward in the explicit graph  The backward propagation stops at nodes where the value function is not modified  or at the root node  The standard AO  algorithm  summarized in Figure    assumes that the AND OR graph in which it searches is acyclic  There are extensions of AO  for searching in AND OR graphs that contain cycles  One line of research is concerned with how to find acyclic solutions in AND OR graphs that contain cycles  Jimenez   Torras         Another generalization of AO   called LAO   allows solutions to contain cycles or loops in order to specify policies for infinite horizon MDPs  Hansen   Zilberstein               HAO   At Start   At Loc   C C C  At Start  C  C  R  R  Navigate  Start  Loc    At Loc    OPT  C C C  R R R  Navigate  Loc   Loc    Lost O  O  C  OPT  C  R  At Loc   O  OPT  R R R  Navigate  Loc   Loc    OPT  Navigate  Start  Loc    At Loc    Panoramic Camera  O R   a  Dynamic programming  Lost O  O  C  R  R  R  C  Panoramic Camera   b  Reachability analysis  Figure    Dynamic programming and reachability analysis  Figure   continued    a  Dynamic programming  The optimal policy has been reevaluated and Navigate Loc   Loc   appears optimal in some continuous states of At Loc    Node At Loc   is represented with a finer partition of the continuous state space to illustrate the fact that the backup increased the number of pieces of the value function associated with the expanded region   b  Reachability analysis  The newly created region of At Loc   becomes reachable  as well as the regions of Lost that can be reached through Navigate Loc   Loc     Given our assumption that every action has positive resource consumption  there can be no loops in the state space of our problem because the resources available decrease at each step  But surprisingly  there can be loops in the AND OR graph  This is possible because the AND OR graph represents a projection of the state space onto a smaller space that consists of only the discrete component of the state  For example  it is possible for the rover to return to the same site it has visited before  The rover is not actually in the same state  since it has fewer resources available  But the AND OR graph represents a projection of the state space that does not include the continuous aspects of the state  such as resources  and this means the rover can visit a state that projects to the same node of the AND OR graph as a state it visited earlier  as shown in Figure    As a result  there can be loops in the AND OR graph  and even loops in the part of the AND OR graph that corresponds to a solution  But in a sense  these are phantom loops that can only appear in the projected state space  and not in the real state space  Nevertheless we must modify the dynamic programming  DP  algorithm to deal with these loops  Because there are no loops in the real state space  we know that the exact value function can be updated by a finite number of backups performed in the correct order  with one backup performed for any state that can be visited along a path from the start state to the expanded node s   But because multiple states can map to the same AND OR graph node  the continuous region of the state space associated with a particular node may need to be evaluated more than once  To identify the AND OR graph nodes that need to be evaluated more than once  we use the following two step algorithm        Meuleau  Benazera  Brafman  Hansen   Mausam  At Start   At Location   energy       At Location   energy       At Location    At Start  energy       At Location   energy       At Location   energy       At Location    Figure    Phantom loops in HAO   solid boxes represent Markov states  Dashed boxes represent search nodes  that is  the projection of Markov states on the discrete components  Arrows represent possible state transition  Bold arrows show an instance of phantom loop in the search space   First  we consider the part of the AND OR graph that consists of ancestor nodes of the just expanded node s   This is the set Z of nodes identified at the beginning of the DP step  We decompose this part of the graph into strongly connected components  The graph of strongly connected components is acyclic and can be used to prescribe the order of backups in almost the same way as in the standard AO  algorithm  In particular  the nodes in a particular component are not backed up until all nodes in its descendant components have been backed up  Note that in the case of an acyclic graph  every strongly connected component has a single node  It is only possible for a connected component to have more than one node if there are loops in the AND OR graph  If there are loops in the AND OR graph  the primary change in the DP step of the algorithm occurs when it is time to perform backups on the nodes in a connected component with more than one node  In this case  all nodes in the connected component are evaluated  Then  they are repeatedly re evaluated until the value functions of these nodes converge  that is  until there is no change in the values of any of the nodes  Because there are no loops in the real state space  convergence is guaranteed to occur after a finite number of steps  Typically  it occurs after a very small number of steps  An advantage of decomposing the AND OR graph into connected components is that it identifies loops and localizes their effect to a small number of nodes  In experiments in our test domain  most nodes of the graph need to be evaluated just once during the DP step  and only a small number of nodes  and often none  need to be evaluated more than once  Note that decomposition of the nodes in Z into connected components is a method for improving the efficiency of the dynamic programming step  and is not required for its correctness  The alternative of repeatedly updating all nodes in Z until all their values converge is also correct  although it is likely to result in many useless updates of already converged nodes  Analysis of reachability Change in the value function can lead to change in the optimal policy  and  thus  to change in which states are visited by the best policy  This  in turn  can affect which open regions of the state space are eligible to be expanded  In this final step  HAO  identifies the best  partial  policy and recomputes Reachablen for all nodes and states in the explicit graph  as follows  see Figure   b    For each node n in the best  partial  solution graph  consider each of its parents n  in the solution graph  and all the actions a that can lead from one of the parents to n  Then Reachablen  x  is the support of Pn  x   where X Z Pn  x    Reachablen   x    Pr n   n    x    a  Pr x   n    x    a  n dx         n   a n  X       HAO   that is  Reachablen  x     x  X   Pn  x        In Equation      n is the set of pairs  n    a  where a is the best action in n  for some reachable resource level  n     n    a   N  A   x  X  Pn   x       n   x    a  Pr n   n    x  a         It is clear that we can restrict our attention to state action pairs in n   only  By performing this reachability analysis  HAO  identifies the frontier of the state space that is eligible for expansion  HAO  terminates when this frontier is empty  that is  when it does not find any hybrid states in the intersection of Reachable and Open      Convergence and Error Bounds We next consider some of the theoretical properties of HAO   First  under reasonable assumptions  we prove that HAO  converges to an optimal policy after a finite number of steps  Then we discuss how to use HAO  to find sub optimal policies with error bounds  The proof of convergence after a finite number of steps depends  among other things  on the assumption that a hybrid state MDP has a finite branching factor  In our implementation  this means that for any region of the state space that can be represented by a hyper rectangle  the set of successor regions after an action can be represented by a finite set of hyper rectangles  From this assumption and the assumption that the number of actions is finite  it follows that for every assignment n to the discrete variables  the set  x  n  x is reachable from the initial state using some fixed sequence of actions  is the union of a finite number of open or closed hyper rectangles  This assumption can be viewed as a generalization of the assumption of a finite branching factor in a discrete AND OR graph upon which the finite convergence proof of AO  depends  Theorem   If the heuristic functions Hn are admissible  optimistic   all actions have positive resource consumptions  both continuous backups and action application are computable exactly in finite time  and the branching factor is finite  then     At each step of HAO   Vn  x  is an upper bound on the optimal expected return in  n  x   for all  n  x  expanded by HAO      HAO  terminates after a finite number of steps     After termination  Vn  x  is equal to the optimal expected return in  n  x   for all  n  x  reachable under an optimal policy  i e   Reachablen  x       Proof      The proof is by induction  Every state  n  x  is assigned an initial heuristic estimate  and Vn  x    Hn  x   Vn  x  by the admissibility of the heuristic evaluation function  We make the inductive hypothesis that at some point in the algorithm  Vn  x   Vn  x  for every state  n  x   If a backup is performed for any state  n  x       Z X Vn  x    max Pr n    n  x  a  Pr x    n  x  a  n     Rn   x      Vn   x     dx  aAn  x   x   n  N      max aAn  x   X n  N     Z  Pr n   n  x  a            Pr x   n  x  a  n    Rn   x     x     Vn  x    where the last equality restates the Bellman optimality equation       Vn   x     dx       Meuleau  Benazera  Brafman  Hansen   Mausam      Because each action has positive  bounded from below  resource consumption  and resources are finite and non replenishable  the complete implicit AND OR graph must be finite  For the same reason  this graph can be turned into a finite graph without loops  Along any directed loop in this graph  the amount of maximal available resources must decrease by some   which is a positive lower bound on the amount of resources consumed by an action  Each node in this graph may be expanded a number of times that is bounded by the number of its ancestor   Each time a new ancestor is discovered  it may lead to an update in the set of reachable regions for this node   Moreover  finite branching factor implies that the number of regions considered within each node is bounded  because there are finite ways of reaching this node  each of which contributes a finite number of hyper rectangles   Thus  overall  the number of regions considered is finite  and the processing required for each region expansion is finite  because action application and backups are computed in finite time   This leads to the desired conclusion      The search algorithm terminates when the policy for the start state  n    x    is complete  that is  when it does not lead to any unexpanded states  For every state  n  x  that is reachable by following this policy  it is contradictory to suppose Vn  x    Vn  x  since that implies a complete policy that is better than optimal  By the Bellman optimality equation of Equation      we know that Vn  x   Vn  x  for every state in this complete policy  Therefore  Vn  x    Vn  x     HAO  not only converges to an optimal solution  stopping the algorithm early allows a flexible trade off between solution quality and computation time  If we assume that  in each state  there is a done action that terminates execution with zero reward  in a rover problem  we would then start a safe sequence   then we can evaluate the current policy at each step of the algorithm by assuming that execution ends each time we reach a leaf of the policy graph  Under this assumption  the error of the current policy at each step of the algorithm can be bounded  We show this by using a decomposition of the value function described by Chakrabarti et al        and Hansen and Zilberstein         We note that at any point in the algorithm  the value function can be decomposed into two parts  gn  x  and hn  x   such that gn  x      gn  x        when  n  x  is an open state  on the fringe of the greedy policy  otherwise  Z X    Pr n   n  x  a   Pr x    n  x  a   n     Rn  x    gn   x     dx          x   n  N  and hn  x     Hn  x  when  n  x  is an open state  on the fringe of the greedy policy  otherwise  Z X hn  x    Pr n    n  x  a   Pr x    n  x  a   n    hn   x   dx        n  N  x   where a is the action that maximizes the right hand side of Equation      Note that Vn  x    gn  x    hn  x   We use this decomposition of the value function to bound the error of the best policy found so far  as follows  Theorem   At each step of the HAO  algorithm  the error of the current best policy is bounded by hn   x     Proof  For any state  n  x  in the explicit search space  a lower bound on its optimal value is given by gn  x   which is the value that can be achieved by the current policy when the done action is executed at all fringe states  and an upper bound is given by Vn  x    gn  x    hn  x   as established in Theorem    It follows that hn   x    bounds the difference between the optimal value and the current admissible value of any state  n  x   including the initial state  n    x      Note that the error bound for the initial state is hn   x      Hn   x    at the start of the algorithm  it decreases with the progress of the algorithm  and hn   x        when HAO  converges to an optimal solution       HAO       Heuristic Function The heuristic function Hn focuses the search on reachable states that are most likely to be useful  The more informative the heuristic  the more scalable the search algorithm  In our implementation of HAO  for the rover planning problem  which is described in detail in the next section  we used the simple admissible heuristic function which assigns to each node the sum of all rewards associated with goals that have not been achieved so far  Note that this heuristic function only depends on the discrete component of the state  and not on the continuous variables  that is  the function Hn  x  is constant over all values of x  It is obvious that this heuristic is admissible  since it represents the maximum additional reward that could be achieved by continuing plan execution  Although it is not obvious that a heuristic this simple could be useful  the experimental results we present in Section   show that it is  We considered an additional  more informed heuristic function that solved a relaxed  suitably discretized  version of the planning problem  However  taking into account the time required to compute this heuristic estimate  the simpler heuristic performed better      Expansion Policy HAO  works correctly and converges to an optimal solution no matter which continuous region s  of which node s  are expanded in each iteration  step   a   But the quality of the solution may improve more quickly by using some heuristics to choose which region s  on the fringe to expand next  One simple strategy is to select a node and expand all continuous regions of this node that are open and reachable  In a preliminary implementation  we expanded  the open regions of  the node that is most likely to be reached using the current policy  Changes in the value of these states will have the greatest effect on the value of earlier nodes  Implementing this strategy requires performing the additional work involved in maintaining the probability associated with each state  If such probabilities are available  one could also focus on expanding the most promising node  that is  the node where the integral of Hn  x  times the probability over all values of x is the highest  as described by Mausam  Benazera  Brafman  Meuleau  and Hansen         Hansen and Zilberstein        observed that  in the case of LAO   the algorithm is more efficient if we expand several nodes in the fringe before performing dynamic programming in the explicit graph  This is because the cost of performing the update of a node largely dominates the cost of expanding a node  If we expand only one node of the fringe at each iteration  we might have to perform more DP backups than if we expand several nodes with common ancestors before proceeding to DP  In the limit  we might want to expand all nodes of the fringe at each algorithm iteration  Indeed  this variant of LAO  proved the most efficient  Hansen   Zilberstein         In the case of LAO   updates are expensive because of the loops in the implicit graph  In HAO   the update of a region induces a call to the hybrid dynamic programming module for each open region of the node  Therefore  the same technique is likely to produce the same benefit  Pursuing this idea  we allowed our algorithm to expand all nodes in the fringe and all their descendants up to a fixed depth at each iteration  We defined a parameter  called the expansion horizon and denoted k  to represent  loosely speaking  the number of times the whole fringe is expanded at each iteration  When k      HAO  expands all open and reachable regions of all nodes in the fringe before recomputing the optimal policy  When k      it expands all regions in the fringe and all their children before updating the policy  At k     it also consider the grandchildren of regions in the fringe  and so on  When k tends to infinity  the algorithm essentially performs an exhaustive search  it first expands the graph of all reachable nodes  then performs one pass of  hybrid  dynamic programming in this graph to determine the optimal policy  By balancing node expansion and update  the expansion horizon allows tuning the algorithm behavior from an exhaustive search to a more traditional heuristic search  Our experiments showed that a value of k between   and    is optimal to solve our hardest benchmark problems  see section           Meuleau  Benazera  Brafman  Hansen   Mausam  Start  ObsPt   Unsafe  C  Obs Pt   Featureless C   W  W   W   Obs Pt   ObsPt   Audience  Demo  label    Waypoint Name    Rock   IP   CHAMP  ObsPt  Far    Science Cam   Figure    The K  rover  top left  was developed at the Jet Propulsion Laboratory and NASA Ames Research Center as a prototype for the MER rovers  It is used to test advanced rover software  including automated planners of the rovers activities  Right  topological map of the      IS demo problem  Arrows labeled IP   CHAMP represent the opportunity to deploy the arm against a rock  instrument placement  and take a picture of it with the CHAMP Camera  Arrows labeled Science Cam represent the opportunity to take a remote picture of a rock with the Science Camera       Updating Multiple Regions The expansion policies described above are based on expanding all open regions of one or several nodes simultaneously  They allow leveraging hybrid state dynamic programming techniques such as those of Feng et al         and Li and Littman         These techniques may compute in a single iteration piecewise constant and linear value functions that cover a large range of continuous states  possibly the whole space of possible values  In particular  they can back up in one iteration all continuous states included between given bounds  Therefore  when several open regions of the same node are expanded at the same iteration of HAO   we can update all of them simultaneously by backing up a subset of continuous states that includes all these regions  For instance  one may record lower bounds and upper bounds on each continuous variable over the expanded regions  and then compute a value function that covers the hyper rectangle between these bounds  This modification of the algorithm does not impact convergence  As long as the value of all expanded regions is computed  the convergence proof holds  However  execution time may be adversely affected if the expanded regions are a proper subset of the region of continuous states that is       HAO    a  Value function Vn     for the initial node  The first plateau corresponds to analyzing R   the second plateau to analyzing R   and the third plateau to analyzing both R  and R     b  The policy n     for the starting node shows the partitions of the resource space where different actions are optimal  Dark  no action  Grey  navigation to R   Light  analysis of R    Figure     a  Optimal value function for the initial state of the simple rover problem over all possible values for the continuous resources  time and energy remaining   The value function is partitioned into      pieces   b  Optimal policy for the same set of states   backed up  In that case  the values of states that are not open or not reachable is uselessly computed  which deviates from a pure heuristic search algorithm  However  this modification may also be beneficial because it avoids some redundant computation  Hybrid state dynamic programming techniques manipulate pieces of value functions  Thus  if several expanded regions are included in the same piece of the value function  their value is computed only once  In practice  this benefit may outweigh the cost of evaluating useless regions  Moreover  cost is further reduced by storing the value functions associated with each node of the graph  so that computed values of irrelevant regions are saved in case these regions become eligible for expansion  i e   open and reachable  later  Thus  this variant of HAO  fully exploits hybrid state dynamic programming techniques      Experimental Evaluation In this section  we describe the performance of HAO  in solving planning problems for a simulated planetary exploration rover with two monotonic and continuous valued resources  time and battery power  Section     uses a simple toy example of this problem to illustrate the basic steps of the HAO  algorithm  Section     tests the performance of the algorithm using a realistic  real size NASA simulation of a rover and analyzes the results of the experiments  The simulation uses a model of the K  rover  see Figure    developed for the Intelligent Systems  IS  demo at NASA Ames Research Center in October       Pedersen et al          This is a complex real size model of the K  rover that uses command names understandable by the rovers execution language  so that the plans produced by our algorithm can be directly executed by the rover  For the experiments reported in Section      we did not simplify this NASA simulation model in any way        Meuleau  Benazera  Brafman  Hansen   Mausam  Figure    First iteration of HAO  on the toy problem  The explicit graph is marked by dim edges and the solution graph is marked by thick edges  Tip nodes         and   are shown with constant heuristic functions and expanded nodes      and   are shown with backed up value functions   In the planning problem we consider  an autonomous rover must navigate in a planar graph representing its surroundings and the authorized navigation paths  and schedule observations to be performed on different rocks situated at different locations  Only a subset of its observational goals can be achieved in a single run due to limited resources  Therefore  this is an oversubscribed planning problem  It is also a problem of planning under uncertainty since each action has uncertain positive resource consumptions and a probability of failing  A significant amount of uncertainty in the domain comes from the tracking mechanism used by the rover  Tracking is the process by which the rover recognizes a rock based on certain features in its camera image that are associated with the rock  During mission operations  a problem instance containing a fixed set of locations  paths  and rocks is built from the last panoramic camera image sent by the rover  Each logical rock in this problem instance corresponds to a real rock  and the rover must associate the two on the basis of features that can be detected by its instruments  including its camera  As the rover moves and its camera image changes  the rover must keep track of how those features of the image evolve  This process is uncertain and subject to faults that result in losing track of a rock  In practice  tracking is modeled in the following way   In order to perform a measurement on a rock  the rover must be tracking this rock   To navigate along a path  it must be tracking one of the rocks that enables following this path  The set of rocks that enable each path is part of the problem definition given to the planner   The decision to start tracking a rock must be made before the rover begins to move  Once the rover starts moving  it may keep track of a rock already being tracked or voluntarily stop tracking it  but it cannot acquire a new rock that was not tracked initially        HAO   Figure    Second iteration of HAO  on the toy problem   The rover may randomly lose track of some rocks while navigating along a path  The probability of losing track of a rock depends on the rock and the path followed  it is part of the problem definition given to the planner   There is no way to reacquire a rock whose track has been lost  intentionally or by accident   The number of rocks tracked strongly influences the duration and resource consumption of navigate actions  The higher the number of rocks tracked  the more costly it is to navigate along a path  This is because the rover has to stop regularly to check and record the aspect of each rock being tracked  This creates an incentive to limit the number of rocks tracked by the rover given the set of goals it has chosen and the path it intends to follow  So  the rover initially selects a set of rocks to track and tries to keep this set as small as possible given its goals  Once it starts moving  it may lose track of some rocks  and this may cause it to reconsider the set of goals it will pursue and the route to get to the corresponding rocks  It can also purposely stop tracking a rock when this is no longer necessary given the goals that are left to achieve  Our implementation of HAO  uses the dynamic programming algorithm developed by Feng et al         and summarized in Section     in order to perform backups in a hybrid state space  and partitions the continuous state space associated with a node into piecewise constant regions  It uses multiple region updates as described in Section      an upper bound on the each resource over all expanded regions is computed  and all states included between these bounds and the minimal possible resource levels are updated  In our experiments  we use the variant of the HAO  algorithm described in Section      where a parameter k sets the number of times the whole fringe is expanded at each iteration of HAO   this allows the behavior of the algorithm to be tuned from an exhaustive search to a heuristic search  We used an expansion horizon of k     for the simple example in Section     and a default expansion horizon of k     for the larger examples in Section      Section       describes experiments with different expansion horizons        Meuleau  Benazera  Brafman  Hansen   Mausam  Figure    Third iteration of HAO  on the toy problem  Our implementation of HAO  uses the simple heuristic described in Section      augmented with a small amount of domain knowledge  The value Hn  x  of a state  n  x  is essentially equal to the sum of the utilities of all goals not yet achieved in n  However  if the rover has already moved and a certain rock is not being tracked in state n  then all goals requiring this rock to be tracked are not included in the sum  This reflects the fact that  once the rover has moved  it cannot start tracking a rock any more  and thus all goals that require this rock to be tracked are unreachable  The resulting heuristic is admissible  i e   it never underestimates the value of a state   and it is straightforward to compute  Note that it does not depend on the current resource levels  so that the functions Hn  x  are constant over all values of x      Example We begin with a very simple example of the rover planning problem in order to illustrate the steps of the algorithm  We solve this example using the same implementation of HAO  that we use to solve the more realistic examples considered in Section      In this example  the targets are two rocks  R  and R   positioned at locations L  and L   respectively  The rovers initial location is L   and there is a direct path between L  and L   Analyzing rock R  yields a reward of    and analyzing rock R  yields a reward of     The rovers action set is simplified  Notably  it features a single action Pic Rx  to represents all the steps of analyzing rock Rx  and the stop tracking actions have been removed  Figure   shows the optimal value function and the optimal policy found by HAO  for the starting discrete state  and resources ranging over the whole space of possible values  Figures      and   show the step by step process by which HAO  solves this problem  Using an expansion horizon of k      HAO  solves this problem in three iterations  as follows   Iteration    As shown in Figure    HAO  expands nodes      and   and computes a heuristic function for the new tip nodes         and    The backup step yields value function estimates for nodes      and    HAO  then identifies the best solution graph and a new fringe node          HAO    a       pieces    b      pieces    c      pieces   Figure     Optimal value functions for the initial state of the simple rover problem with increasing initial resource levels  from left to right   The optimal return appears as a three dimensional function carved into the reachable space of the heuristic function  problem name Rover  Rover  Rover  Rover   rover locations           paths  goals  fluents  actions                                                  discrete states  approx                                       reachable discrete states                       explicit graph  optimal policy  longest branch                                                Table    Size of benchmark rover problems   Iteration    As shown in Figure    HAO  expands nodes         and     starting with previous fringe node    and computes heuristic functions for the new tip nodes        and     The heuristic value for node    is zero because  in this state  the rover has lost track of R  and has already analyzed R   The backup step improves the accuracy of the value function in several nodes  Node    is the only new fringe node since    is a terminal node   Iteration    As shown in Figure    HAO  expands node    and node     The search ends after this iteration because there is no more open node in the optimal solution graph  For comparison  Figure    shows how the value function found by HAO  varies with different initial resource levels  In these figures  unreachable states are assigned a large constant heuristic value  so that the value function for reachable states appears as carved in the plateau of the heuristic      Performance Now  we describe HAO s performance in solving four much larger rover planning problems using the NASA simulation model  The characteristics of these problems are displayed in Tables    Columns two to six show the size of the problems in terms of rover locations  paths  and goals  They also show the total number of fluents  Boolean state variables  and actions in each problem  Columns seven to ten report on the size of the discrete state space  The total number of discrete states is two raised to the power of the number of fluents  Although this is a huge state space  only a limited number of states can be reached from the start state  depending on the initial resource levels  The eighth column in Table   shows the number of reachable discrete states if the initial time and energy levels are set to their maximum value   The maximum initial resource levels are based on the scenario of the      IS demo and represent several hours of rover activity   It shows that simple reachability      Meuleau  Benazera  Brafman  Hansen   Mausam                              reachable created expanded in optimal policy      Number of discrete states      Number of discrete states       reachable created expanded in optimal policy                                                Initial energy                                        Initial time                                                       a  Rover        reachable created expanded in optimal policy        Number of discrete states  Number of discrete states                                                        Initial energy                                         reachable created expanded in optimal policy                           Initial time   b  Rover         reachable created expanded in optimal policy         Number of discrete states  Number of discrete states                                                      Initial energy                                            reachable created expanded in optimal policy                     Initial time   c  Rover         reachable created expanded in optimal policy         Number of discrete states  Number of discrete states                                                      Initial energy                                            reachable created expanded in optimal policy                     Initial time   d  Rover  Figure     Number of nodes created and expanded by HAO  vs  number of reachable discrete states  The graphs in the left column are obtained by fixing the initial time to its maximum value and varying the initial energy  The graphs in the right column are obtained by fixing the initial energy to its maximum value and varying the initial time  Results obtained with k           HAO   analysis based on resource availability makes a huge difference  This is partly due to the fact that our planning domain  which is very close to the K  execution language  does not allow many fluents to be true simultaneously  Columns nine and ten show the number of discrete states in the explicit graph and in the optimal policy  More precisely  the former is the number of nodes created by HAO   that is  a subset of the reachable discrete states  The number of reachable discrete states  and thus the size of the graph to explore  may seem small compared to other discrete combinatorial problems solved by AI techniques  But each iteration  a continuous approximation of the two dimensional backup is necessary to evaluate the hybrid state space associated with the graph  Finally  the last column of Table   shows the length of the longest branch in the optimal policy when the initial resource levels are set to their maximum value  The largest of the four instances  that is  Rover   is exactly the problem of the October      IS demo  This is considered a very large rover problem  For example  it is much larger than the problems faced by the MER rovers that never visit more than one rock in a single planning cycle        Efficiency of Pruning In a first set of simulations  we try to evaluate the efficiency of heuristic pruning in HAO   that is  the portion of the discrete search space that is spared from exploration through the use of admissible heuristics  For this purpose  we compare the number of discrete states that are reachable for a given resource level with the number of nodes created and expanded by HAO   We also consider the number of nodes in the optimal policy found by the algorithm  Results for the four benchmark problems are presented in Figure     These curves are obtained by fixing one resource to its maximum possible value and varying the other from   to its maximum  Therefore  they represent problems where mostly one resource is constraining  These result show  notably  that a single resource is enough to constrain the reachability of the state space significantly  Not surprisingly  problems become larger as the initial resources increase  because more discrete states become reachable  Despite the simplicity of the heuristic used  HAO  is able to by pass a significant part of the search space  Moreover  the bigger the problem  the more leverage the algorithm can take from the simple heuristic  These results are quite encouraging  but the number of nodes created and expanded does not always reflect search time  Therefore  we examine the time it takes for HAO  to produce solutions        Search Time Figure    shows HAO  search time for the same set of experiments  These curves do not exhibit the same monotonicity and  instead  appear to show a significant amount of noise  It is surprising that search time does not always increase with an increase in the initial levels of resource  although the search space is bigger  This shows that search complexity does not depend on the size of the search space alone  Other factors must explain complexity peaks as observed in Figure     Because the number of nodes created and expanded by the algorithm does not contain such noise  the reason for the peaks of computation time must be the time spent in dynamic programming backups  Moreover  search time appears closely related to the complexity of the optimal policy  Figure    shows the number of nodes and branches in the policy found by the algorithm  as well as the number of goals pursued by this policy  It shows that   i  in some cases  increasing the initial resource level eliminates the need for branching and reduces the size of the optimal solution   ii  the size of the optimal policy and  secondarily  its number of branches  explains most of the peaks in the search time curves  Therefore  the question is  why does a large solution graph induce a long time spent in backups  There are two possible answers to this question  because the backups take longer and or because more backups are performed  The first explanation is pretty intuitive  When the policy graph contains many branches leading to different combinations of goals  the value functions contain many humps and plateaus  and therefore many pieces  which impacts the complexity of dynamic programming backups  However  we do not have at this time any empirical evidence to                                      Search time  s   Search time  s   Meuleau  Benazera  Brafman  Hansen   Mausam                                                          Initial energy                                        Initial time                         Initial time                                                  Search time  s   Search time  s    a  Rover                                                                         Initial energy                                                  Search time  s   Search time  s    b  Rover                                                                        Initial energy                                        Initial time                                  Initial time                                          Search time  s   Search time  s    c  Rover                                                         Initial energy                      d  Rover  Figure     HAO  search time  The graphs in the left column are obtained by fixing the initial time to its maximum value  and the graphs in the right column are obtained by fixing the initial energy to its maximum  Results obtained with k            HAO   confirm this hypothesis  Conversely  we observe that the peak of Figure    comes with an increase of the number of backups  More work is required to explain this        Expansion Horizon The results of Section       show that HAO  can leverage even a simple admissible heuristic to prune a large portion of the search space  But it does not necessarily follow that HAO  can outperform an exhaustive search algorithm that creates a graph of all reachable states  and then executes one pass of dynamic programming in this graph to find the optimal policy  Although HAO  expands a smaller graph than such an exhaustive search  it must evaluate the graph more often  In Section      we introduced a parameter k for expansion horizon in order to allow adjustment of a trade off between the time spent expanding nodes and the time spent evaluating nodes  We now study the influence of this parameter on the algorithm  Figure    shows the number of nodes created and expanded by HAO  as a function of the expansion horizon for the four benchmark problem instances  Not surprisingly  the algorithm creates and expands more nodes as the expansion horizon increases  Essentially  it behaves more like an exhaustive search as k is increased  For the two smallest problem instances  and for large enough values of k  the number of visited states levels off when the total number of reachable states is reached  For the two largest problem instances  we had to interrupt the experiments once k reached    because search time became too long  Figure    shows the effect of the expansion horizon on the search time of HAO   For the smallest problem instance  Rover    HAO  does not have a clear advantage over an exhaustive search  with k        even though it explores fewer nodes  But for the three larger problem instances  HAO  has a clear advantage  For the Rover  problem instance  the search time of HAO  levels off after k       indicating the limit of reachable states has been reached  However  the duration of such an exhaustive search is several times longer than for HAO  with smaller settings of k  The benefits of HAO  are clearer for the two largest problem instances  As k is increased  the algorithm is quickly overwhelmed by the combinatorial explosion in the size of the search space  and simulations eventually need to be interrupted because search time becomes too long  For these same problem instances and smaller settings of k  HAO  is able to efficiently find optimal solutions  Overall  our results show that there is a clear benefit to using admissible heuristics to prune the search space  although the expansion horizon must be adjusted appropriately in order for HAO  to achieve a favorable trade off between node expansion time and node evaluation time      Conclusion We introduced a heuristic search approach to finding optimal conditional plans in domains characterized by continuous state variables that represent limited  consumable resources  The HAO  algorithm is a variant of the AO  algorithm that  to the best of our knowledge  is the first algorithm to deal with all of the following  limited continuous resources  uncertain action outcomes  and over subscription planning  We tested HAO  in a realistic NASA simulation of a planetary rover  a complex domain of practical importance  and our results demonstrate its effectiveness in solving problems that are too large to be solved by the straightforward application of dynamic programming  It is effective because heuristic search can exploit resource constraints  as well as an admissible heuristic  in order to limit the reachable state space  In our implementation  the HAO  algorithm is integrated with the dynamic programming algorithm of Feng et al          However HAO  can be integrated with other dynamic programming algorithms for solving hybrid state MDPs  The Feng et al  algorithm finds optimal policies under the limiting assumptions that transition probabilities are discrete  and rewards are either piecewiseconstant or piecewise linear  More recently developed dynamic programming algorithms for hybridstate MDPs make less restrictive assumptions  and also have the potential to improve computational       Meuleau  Benazera  Brafman  Hansen   Mausam                                                   Initial energy                           Nodes Branches Goals                                                Initial time        Number of branches and goals         Number of nodes     Number of nodes     Nodes Branches Goals  Number of branches and goals                a  Rover                                                    Initial energy                           Nodes Branches Goals                                                Initial time        Number of branches and goals         Number of nodes  Nodes Branches Goals     Number of nodes    Number of branches and goals                b  Rover                                                    Initial energy                           Nodes Branches Goals                                                Initial time        Number of branches and goals         Number of nodes  Nodes Branches Goals     Number of nodes    Number of branches and goals                c  Rover                                                    Initial energy                           Nodes Branches Goals                                                Initial time        Number of branches and goals          Number of nodes  Nodes Branches Goals     Number of nodes    Number of branches and goals                 d  Rover  Figure     Complexity of the optimal policy  number of nodes  branches  and goals in the optimal policy in the same setting as Figure           HAO        Number of discrete states      Number of discrete states        created expanded                                        Expansion horizon                             created expanded                   a  Rover         Number of discrete states  Number of discrete states      created expanded                                           b  Rover         created expanded               Expansion horizon                                               Expansion horizon          c  Rover               Expansion horizon       d  Rover   Figure     Influence of the expansion horizon on the number of nodes visited by the algorithm  efficiency  Li   Littman        Marecki et al          Integrating HAO  with one of these algorithms could improve performance further  There are several other interesting directions in which this work could be extended  In developing HAO   we made the assumptions that every action consumes some resource and resources are non replenishable  Without these assumptions  the same state could be revisited and an optimal plan could have loops as well as branches  Generalizing our approach to allow plans with loops  which seems necessary to handle replenishable resources  requires generalizing the heuristic search algorithm LAO  to solve hybrid MDPs  Hansen   Zilberstein         Another possible extension is to allow continuous action variables in addition to continuous state variables  Finally  our heuristic search approach could be combined with other approaches to improving scalability  such as hierarchical decomposition  Meuleau   Brafman         This would allow it to handle the even larger state spaces that result when the number of goals in an over subscription planning problem is increased  Acknowledgments This work was funded by the NASA Intelligent Systems program  grant NRA         Eric Hansen was supported in part by a NASA Summer Faculty Fellowship and by funding from the Mississippi Space Grant Consortium  This work was performed while Emmanuel Benazera was working at NASA Ames Research Center and Ronen Brafman was visiting NASA Ames Research Center  both as consultants for the Research Institute for Advanced Computer Science  Ronen Brafman was supported in part by the Lynn and William Frankel Center for Computer Science  the Paul Ivanier Center for Robotics and Production Management  and ISF grant          Nicolas Meuleau is a consultant of Carnegie Mellon University at NASA Ames Research Center        Meuleau  Benazera  Brafman  Hansen   Mausam                          Search time  s   Search time  s                                                               Expansion horizon                    a  Rover                   Search time  s         Search time  s        b  Rover                                    Expansion horizon                                      Expansion horizon          c  Rover               Expansion horizon       d  Rover   Figure     Influence of the expansion horizon on overall search time   
 Previous work has shown that the problem of learning the optimal structure of a Bayesian network can be formulated as a shortest path finding problem in a graph and solved using A  search  In this paper  we improve the scalability of this approach by developing a memoryefficient heuristic search algorithm for learning the structure of a Bayesian network  Instead of using A   we propose a frontier breadth first branch and bound search that leverages the layered structure of the search graph of this problem so that no more than two layers of the graph  plus solution reconstruction information  need to be stored in memory at a time  To further improve scalability  the algorithm stores most of the graph in external memory  such as hard disk  when it does not fit in RAM  Experimental results show that the resulting algorithm solves significantly larger problems than the current state of the art     INTRODUCTION Bayesian networks are a common machine learning technique used to represent relationships among variables in data sets  When these relationships are not known a priori  the structure of the network must be learned  A common learning approach entails searching for a structure which optimizes a particular scoring function  Cooper and Herskovits       Heckerman  Geiger  and Chickering        Because of the difficulty of the problem  early approaches focused on approximation techniques to learn good networks  Cooper and Herskovits       Heckerman  Geiger  and Chickering       Heckerman       Friedman  Nachman  and Peer       Tsamardinos  Brown  and Aliferis        Unfortunately  these algorithms are unable to guarantee anything about the quality of the learned networks  Exact dynamic programming algorithms have been developed to learn provably optimal Bayesian network struc   tures  Ott  Imoto  and Miyano       Koivisto and Sood       Singh and Moore       Silander and Myllymaki        These algorithms identify optimal small subnetworks and add optimal leaves to find large optimal networks until finding the optimal network including all variables  Unfortunately  all of these algorithms must store an exponential number of subnetworks and associated information in memory  Parviainen and Koivisto        recently proposed a divide and conquer algorithm in which fewer subnetworks are stored in memory at once at the expense of longer running time  Theoretical results suggest that this algorithm is slower than dynamic programming when an exponential number of processors is not available  Yuan et al         developed an A  heuristic search formulation based on the dynamic programming recurrences to learn optimal network structures  The algorithm formulates the learning problem as a shortest path finding problem in a search graph  Each path in the graph corresponds to an ordering of the variables  and each edge on the path has a cost that corresponds to the choice of an optimal parent set for one variable out of the variables that appear earlier on the path  Together  all the edges on a path encode an optimal directed acyclic graph that is consistent with the path  The solution to the shortest path finding problem then corresponds to an optimal Bayesian network structure  The A  algorithm also uses a consistent heuristic function to prune provably suboptimal solutions during the search so as to improve its efficiency  de Campos et al         proposed a systematic search algorithm to identify optimal network structures  The algorithm begins by calculating optimal parent sets for all variables  These sets are represented as a directed graph that may have cycles  Cycles are then repeatedly broken by removing one edge at a time  The algorithm terminates with an optimal Bayesian network  However  this algorithm is shown to often learn the optimal structure slower than the dynamic programming algorithm  de Campos  Zeng  and Ji        Optimal networks have also been learned using linear programming  Jaakkola et al         This technique reformu    lates the structure learning problem as a linear program  An exponential number of constraints are used to define a convex hull in which each vertex corresponds to a DAG  Coordinate descent is used to identify the vertex which corresponds to the optimal DAG structure  Furthermore  the dual of their formulation provides an upper bound which can help guide the descent algorithm  This algorithm was shown to have similar or slightly better runtime performance as dynamic programming  Jaakkola et al         This paper describes a novel frontier breadth first branch and bound algorithm using delayed duplicate detection for learning optimal Bayesian network structures  The basic idea is to formulate the learning task as a graph search problem  The search graph decomposes into natural layers and allows searching one layer at a time  This algorithm improves the scalability of learning optimal Bayesian network structures in three ways  First  the frontier search approach allows us to reduce the memory complexity by working with only a single layer of search graphs at a time during the search  In particular  we store one layer of each search graph  the scores required for that layer and information for solution reconstruction from every previous layer  Other information is deleted  In comparison  previous dynamic programming algorithms have to store an entire exponentiallysized graph in memory  Second  branch and bound techniques allow us to safely prune unpromising search nodes from the search graphs  while dynamic programming algorithms have to evaluate the whole search space  Finally  we use a delayed duplicate detection method to ensure that  given enough hard disk space  optimal network structures can be learned regardless of the amount of RAM  Previous algorithms fail if an exponential amount of RAM is not available  The remainder of this paper is structured as follows  Section   provides an overview of the task of Bayesian network learning  Section   and   introduce two formulations for solving the learning task  dynamic programming and graph search  Section   discusses the details of the externalmemory frontier breadth first branch and bound algorithm we propose in this paper  Section   compares the algorithm against several existing approaches on a set of benchmark machine learning datasets  Finally  Section   concludes the paper     BACKGROUND A Bayesian network consists of a directed acyclic graph  DAG  structure and a set of parameters  The vertices of the graph each correspond to a random variable V    X         Xn    All parents of Xi are referred to as P Ai   A variable is conditionally independent of its non descendants given its parents  The parameters of the network specify a conditional probability distribution  P  Xi  P Ai   for each Xi    Given a dataset D    D         DN    where Di is an instantiation of all the variables in V  the optimal structure is the DAG over all of the variables which best fits D  Heckerman        A scoring function measures the fit of a network structure to D  For example  the minimum description length  MDL  scoring function  Rissanen       uses one term to reward structures with low entropy and another to penalize complex structures  Optimal structures minimize the score  Let ri be the number of states of the variable Xi   let Npai be the number of data records consistent with P Ai   pai   and let Nxi  pai be the number of data records consistent with P Ai   pai and Xi   xi   The MDL score for a structure G is defined as follows  Tian         M DL G     X  M DL Xi  P Ai          i  where  M DL Xi  P Ai     H Xi  P Ai     K Xi  P Ai      log N H Xi  P Ai     K Xi  P Ai      X Nxi  pai       Nxi  pai log  Npai xi  pai Y  ri     rl   Xl P Ai  MDL is decomposable  Heckerman        so the score for a structure is simply the sum of the score for each variable  Our algorithm can be adapted to use any decomposable function  Some sets of parents cannot form an optimal parent for any variable  as described in the following theorems from Tian        and de Campos et al          Theorem    In an optimal Bayesian network based on the MDL scoring function  each variable has at most  N log  log N   parents  where N is the number of data points  Theorem    Let U  V and X    U  If BestM DL X  U    BestM DL X  V   V cannot be the optimal parent set for X     DYNAMIC PROGRAMMING Learning an optimal Bayesian network structure is NPHard  Chickering        Dynamic programming algorithms learn optimal network structures in O n n   time and memory  Ott  Imoto  and Miyano       Koivisto and Sood       Singh and Moore       Silander and Myllymaki        Because a network structure is a DAG  the optimal structure can be divided into an optimal leaf vertex and its parents as well as an optimal subnetwork for the rest of the variables  This subnetwork is also a DAG  so it can recursively be divided until the subnetwork is only a single   vertex  At that point  the optimal parents have been found for all variables in the network and the optimal structure can be constructed  It has been shown  Silander and Myllymaki       that a more efficient algorithm begins with a  variable subnetwork and exhaustively adds optimal leaves  For the MDL scoring function and variables V  this recurrence can be expressed as follows  Ott  Imoto  and Miyano         M DL V     min  M DL V    X      XV  Figure    Parent graph for variable X   BestM DL X  V    X     where BestM DL X  V    X      min  P AX V  X   M DL X P AX     As this recurrence suggests  all dynamic programming algorithms must perform three steps  First  they must calculate the score of each variable given all subsets of the other variables as parents  There are n n  of these scores  Then  BestM DL must be calculated  For a variable X and set of possible parents V  this function returns the subset of those parents which minimizes the score for X as well as that score  There are n n  of these optimal parent sets  Finally  the optimal subnetworks must be learned  These subnetworks use BestM DL to learn the optimal leaf for every possible subnetwork  including the optimal network with all of the variables  There are  n optimal subnetworks  Figure    An order graph of four variables    GRAPH SEARCH FORMULATION We first formulate each phase of the dynamic programming algorithm as a separate search problem  including calculating parent scores  identifying the optimal parent sets  and learning the optimal subnetworks  We use an AD tree like search to calculate all of the parent scores  An AD tree  Moore and Lee       is an unbalanced tree which contains AD nodes and varying nodes  The tree is used to collect count statistics from a dataset  An AD node stores the number of records consistent with the variable instantiation of the node  while a varying node assigns a value to a variable  As shown in Equation    the entropy component of a score can be calculated based on variable instantiation counts  Each AD node has an instantiation of a set of variables U and the count of records consistent with that instantiation  That count is a value of pai for all X  V   U  Furthermore  it is a value of xi   pai for all X  U with parents U    X   We can use a depthfirst traversal of the AD tree to compute the scores  Theorem   states that only small parent sets can possibly be optimal parents when using the MDL score  All nodes below the depth specified in the theorem are pruned  The scores which are not pruned are written to disk for retrieval when  identifying optimal parent sets  We call this data structure a score cache  Each entry in the score cache contains one value of M DL X P A   A parent graph is a lattice in which each node stores one value of BestM DL for different candidate sets of variables  The score cache is used to quickly look up the scores for candidate parent sets  Figure   shows the construction of the parent graph for variable X  as a lattice  All  n  subsets of all other variables are present in the graph  Each node contains one value for BestM DL of X  and the set of candidate parents shown  That is  each node stores the subset of parents from the given candidate set which minimizes the score of X    as well as that score  The lattice divides the nodes into layers  We call the first layer of the graph  the layer with the single node for    in Figure    layer    A node in layer l has l predecessors  all in layer l     and considers candidate parent sets of size l  Layer l has C n     l  nodes  where C n  k  is the binomial coefficient  Each variable has a separate parent graph  The complete set of parent graphs stores n n  parent sets  An order graph is also a lattice  Each node contains M DL V  and the associated optimal subnetwor for one   subset of variables  Figure   displays an order graph for four variables  Its lattice structure is similar to that of the parent graphs  because it contains subsets of all variables  though  the order graph has  n nodes  The topmost node in layer   containing no variables is the start node  The bottom most node containing all variables is the goal node  A directed path in the order graph from the start node to any other node induces an ordering on the variables in the path with new variables appearing later in the ordering  For example  the path traversing nodes      X     X    X      X    X    X    stands for the variable ordering X    X    X    All variables which precede a variable in the ordering are candidate parents of that variable  Each edge on the path has a cost equal to BestM DL for the new variable in the child node given the variables in the parent node as candidate parents  The parent graphs are used to quickly retrieve these costs  For example  the edge between  X    X    and  X    X    X    has a cost equal to BestM DL X     X    X      Each node contains a subset of variables  the cost of the best path from the start node to that node  a leaf variable and its optimal parent set  The shortest paths from the start node to all the other nodes correspond to the optimal subnetworks  so the shortest path to the goal node corresponds to the optimal Bayesian network  The lattice again divides the nodes into layers  Nodes in layer l contain optimal subnetworks of l variables  Layer l has C n  l  nodes     AN EXTERNAL MEMORY FRONTIER BREADTH FIRST BRANCH AND BOUND ALGORITHM Finding an optimal Bayesian network structure can be considered a search through the order graph  This formulation allows the application of any graph search algorithm  such as A   Yuan  Malone  and Wu        to find the best path from the start node to the goal node  In particular  such a formulation allows us to treat the order and parent graphs as implicit search graphs  That is  we do not have to keep the entire graphs in memory at once  Dynamic programming can be considered as a breadth first search through this graph  Malone  Yuan  and Hansen        Previous results show that the scalability of existing algorithms for learning optimal Bayesian networks is typically limited by the amount of RAM available  To eliminate the constraint of limited RAM  we introduce a frontier breadth first branch and bound algorithm with delayed duplicate detection to do the search by adapting the breadth first heuristic search algorithm proposed by Zhou and Hansen               It is also similar to the frontier search described by Korf         Breadth first heuristic search expands a search space in order of layers of increasing g cost with each layer comprising all nodes with a same g cost  As each node is generated  a heuristic function is used to calculate a lower bound for  Algorithm   A Frontier BFBnB Search Algorithm procedure EXPAND O RDER G RAPH l  isP resent  upper  lb  maxSize  for each MDLl  U   MDLl do for each X  V   U do s  MDLl  U    BMDLl  X U   lb X  if s   upper then continue isP resent U   X    true if s   MDLl    U   X   then MDLl    U   X    s MDLP l    U   X    BMDLP l  X U  end if if  MDLl       maxSize then writeTempFile MDLl     MDLP l     end if end for end for writeTempFile MDLl     MDLP l     MDLl     MDLP l    mergeTempFiles delete MDLl end procedure procedure EXPAND PARENT G RAPH l  p  isP resent  maxSize  for each BestMDLl  p U   BestMDLl  p do for each X  V   U and X    p do S  U   X  if  isP resent S  then continue if MDL p S    BMDLl    p S  then BMDLl    p S   MDL p S  BMDLP l    p S   S end if if BMDLl  p U    BMDLl    p S  then BMDLl    p S   BMDLl  p U  BMDLP l    p S   BMDLP l  p U  end if if  BMDLl    p     maxSize then writeTempFile BMDLl    p   BMDLSl    p   end if end for end for writeTempFile BMDLl    p   BMDLP l    p   BMDLl     BMDLP l    p  mergeTempFiles delete BMDLl   BMDLP l  p  end procedure procedure EXPANDADN ODE i  U  Du   d  For j   i      n do expandVaryNode j  U  Du   d  end procedure procedure EXPANDVARY N ODE i  U  Du   d  for j      ri do updateScores U   Xi    DXi  j u   if d     then expandADNode i  U   Xi    DXi  j u   d     end for end procedure procedure UPDATESCORES  U  Du   for X  V   U do if MDL X U  is null then MDL X U   K X U  MDL X U   MDL X U    Nu  log Nu end for for X  U do if M DL X U    X   is null M DL X U    X    K X U    X    MDL X U    X    MDL X U    X    Nu  log Nu end for end procedure procedure MAIN D  upper  maxSize   N maxP arents  log log N expandADNode        D  maxP arents  lb  getBestScores writeScoresToDisk isP resent     for l      n do for p      n do expandParentGraph l  p  isP resent  maxSize  end for expandOrderGraph l  isP resent  upper  lb  maxSize  end for optimalStructure  reconstructSolution end procedure   that node  If the lower bound is worse than a given upper bound on the optimal solution  the node is pruned  otherwise  the node is added to the open list for further search  After the search  a divide and conquer method is used to reconstruct the optimal solution  Algorithm   gives the pseudocode for our BFBnB search algorithm for learning optimal Bayesian networks  The algorithm is very similar to the breadth first heuristic search algorithm but has several subtle and important differences  First  the layers in our search graphs  the parent and order graphs  do not correspond to the g costs of nodes  rather  layer l corresponds to variable sets  candidate parent sets or optimal subnetworks  of size l  For the order graph  though  we can calculate both a g  and h cost for pruning  as described in Section      We also describe how to propagate this pruning from the order graph to the parent graphs  Another difference is that our search problem is a nested search of order and parent graphs  The layered parent and order graph searches have to be carefully orchestrated to ensure the correct nodes can be accessed easily at the correct time  as described in Section      This further requires the parent scores are stored in particular order  as described in Section      Yet another difference is that we use a variant of delayed duplicate detection  Korf       in which a hash table is used to detect as many duplicates in RAM as possible before resorting to external memory  as described in Section      Finally  we store a portion of each order graph node to reconstruct the optimal network structure after the search  as described in Section          BRANCH AND BOUND We need a heuristic function f  U    g U    h U  that estimates the cost of the best path from the start node to a goal node using order node U  The g cost is simply the sum of the edge costs of the best path from the start node to U  The h cost provides a lower bound on the cost from U to the goal node  We use the following heuristic function h from Yuan et al          Definition    h U     X  BestM DL X  V  X          XV U  This heuristic function relaxes the acyclic constraint on the remaining variables in V   U and allows them to choose parents from all of the variables in V  The following theorem from Yuan et al         proves that the function is consistent  Consistent heuristics are guaranteed to be admissible  Theorem    h is consistent  In order to calculate this bound  we must know BestM DL X  V  X    Fortunately  these scores are calculated during the first phase of the algorithm  Because  the score cache contains every score which could possibly be optimal for all variables  it is guaranteed to have the optimal score for all variables given any set of parents  which is BestM DL X  V    X    Thus  we can identify these scores while calculating the scores when expanding the AD tree and store them in an array for reuse  The pseudocode uses the function getBestScores to find these scores and the array lb to store them  We can apply BFBnB to prune nodes in the order graph using the lower bound function in Equation    however  pruning is not directly applicable to the parent graphs  An optimal parent score BestM DL X  U  is only necessary if a node for U is in the order graph  Consequently  if U is pruned from the order graph  then the nodes for U are also pruned from the parent graphs  The pseudocode uses isP resent to track which nodes were not pruned  We also need an upper bound score on the optimal Bayesian network for pruning  A search node U whose heuristic value f  U  is higher than the upper bound is immediately pruned  Numerous fast  approximate methods exist for learning a locally optimal Bayesian network  We use a greedy beam search algorithm based on a local search algorithm described by Heckerman        to quickly find the upper bound  A more sophisticated algorithm could be used to find a better bound and improve pruning  The input argument upper is this bound in the pseudocode      COORDINATING THE GRAPH SEARCHES The parent and order graph searches must be carefully coordinated to ensure that the parent graphs contain the necessary nodes to expand nodes in the order graph  In particular  expanding a node U in layer l in the order graph requires BestM DL X  U   which is stored in the node U of the parent graph for X  Hence  before expanding layer  U  in the order graph  that layer of the parent graphs must already exist  Therefore  the algorithm alternates between expanding layers of the parent graphs and order graph  Expanding a node U in the parent graph amounts to generating successor nodes with candidate parents U   X  for all X in V   U  For each successor S   U   X   the hash table for the next layer is first checked to see if S has already been generated  If not  the score of using all of S as parents of X is retrieved from the score cache and compared to the score of using the parents specified in U  If using all of the variables has a better score  then an entry is added to the hash table indicating that  for possible parents S  using all of them is best  Otherwise  according to Theorem    the hash table stores a mapping from S to the parents in U  Similarly  if S has already been generated  the score of the existing best parent set for S is compared to the score using the parents in U  If the score of the parents in U is better  then the hash table mapping is updated accordingly  Once a layer of the parent graph is expanded    the whole layer can be discarded as it is no longer needed  The pseudocode uses BM DLl to store the optimal scores and BM DLP l to store the optimal parents   there is no additional work required to arrange the nodes when writing them to disk   Expanding a node U in the order graph amounts to generating successor nodes U   X  for all X in V   U  To calculate the score of successor S   U   X   the score of the existing node U is added to BestM DL X  U   which is retrieved from parent graph node U for variable X  The optimal parent set out of U is also recorded  This is equivalent to trying X as the leaf and U as the subnetwork  Next  the hash table for the next layer is consulted  If it contains an entry for S  then a node for this set of variables has already been generated using another variable as the leaf  The score of that node is compared to the score for S  If the score for S is better  or the hash table did not contain an entry for S  then the mapping in the hash table is updated  Unlike the parent graph  however  a portion of each order graph node is used to reconstruct the optimal network at the end of the search  as described in Section      This information is written to disk  while the other information is deleted  The pseudocode uses M DLl to store the score for each subnetwork and M DLP l to store the associated parent information       ORDERING THE SCORES ON DISK  Additional care is needed to ensure that parent and order graph nodes for a particular layer are accessed in a regular  structured pattern  We arrange the nodes in the parent and order graphs in queues such that when node U is removed from the order graph queue  the head of each parent graph queue for all X in V   U is U  So all of the successors of U can be generated by combining it with the head of each of those parent graph queues  Once the parent graph nodes are used  they can be removed  and the queues will be ready to expand the next node in the order graph queue  Because the nodes are removed from the heads of the queues  these invariants hold throughout the expansion of the layer  Regulating such access patterns improves the scalability of the algorithm because these queues can be stored on disk and accessed sequentially to reduce the requirement of RAM  The regular accesses also reduce disk seek time  The pseudocode assumes the nodes are written to disk in this order to easily retrieve the next necessary node   Duplicate nodes are generated during the graph searches  Duplicates in the parent and order graphs correspond to nodes which consider the same sets of variables  candidate parent sets and optimal subnetworks  respectively   Because the successors of a node always consider exactly one more variable in both the parent and order graphs  the successors of a node in layer l are always in layer l      Therefore  when a node is generated  it could only be a duplicate of a node in the open list for layer l      In both the parent and order graphs  the duplicate with the best score should be kept   The lexicographic ordering  Knuth       of nodes within each layer is one possible ordering that ensures the queues remain synchronized  For example  the lexicographic ordering of   variables of size   is   X    X      X    X      X    X      X    X      X    X      X    X      The order graph queue for layer   of a dataset with   variables should be arranged in that order  The parent graph queue for variable X should have the same sequence  but without subsets containing X  In the example  the parent graph queue for variable X  should be   X    X      X    X      X    X      As described in more detail in Section      the nodes of the graphs must be sorted to detect duplicates  the lexicographic order ensures that  For large datasets  the score cache can grow quite large  We write it to disk to reduce RAM usage  Each score M DL X  U  is used once  when node U is first generated in the parent graph for X  As described in Section      the parent graph nodes are expanded in lexicographic order  however  they are not generated in that order  The score M DL X  U   U    Y        Yl   is needed when expanding node U    Yl   in the parent graph for X  Therefore  the scores must be written to disk in that order  The pseudocode uses the writeScoresT oDisk function to sort and write the scores to disk in this order  A file is created for each variable for each layer to store these sorted scores  The file for a particular layer can be deleted after expanding that layer in the appropriate parent graph      DUPLICATE DETECTION  For large datasets  it is possible that even one layer of the parent or order graph is too large to fit in RAM  We use a variant of the delayed duplicate detection  DDD   Korf       in our algorithm to utilize external memory to solve such large learning problems  In DDD  search nodes are written to a file on disk as they are generated  After expanding a layer  an external memory sorting algorithm is used to detect and remove duplicate nodes in the file  The nodes in the file are then expanded to generate the next layer of the search  Consequently  the search uses a minimal amount of RAM  however  all generated nodes are written to disk  so much work is done reading and writing duplicates  Rather than immediately writing all generated nodes to disk  we instead detect duplicates in RAM as usual with a hash table  Once the open list reaches a user defined maximum size  its contents are sorted and written to a temporary file on disk  The open list is then cleared  At the end of each layer  the remaining contents of the open list and the temporary files are sorted and merged into a single file which contains the sorted list of nodes from that layer  For rea    sons described in Section      the lexicographic ordering of nodes within a layer is used when sorting  The hash table reduces the number of nodes written to and read from disk by detecting as many duplicates as possible in RAM  The pseudocode uses maxSize as the user defined maximum size  The function writeT empF ile sorts  writes to disk and clears the open list provided as its argument  The scores and optimal parent sets are written together on disk  The function mergeT empF iles performs an external memory merge to detect duplicates in the temp files  For the parent graphs  both the scores and optimal parent sets are kept in a single file  however  as described in Section      the parent information of the order graph must be stored for the entire search  while the score information can be deleted after use  Therefore  two separate files are used to allow the information to easily be deleted      RECONSTRUCTING THE OPTIMAL NETWORK STRUCTURE In order to trace back the optimal path and reconstruct the optimal network structure  we write a portion of each node of the order graph to a disk file once it is expanded during the order graph search  For each order graph node we write the subset of variables  the leaf variable and its optimal parents  Solution reconstruction works as follows  The final leaf variable X and its optimal parent set are retrieved from the goal node  Because the goal node considers all variables  its predecessor in the optimal path is U   V    X   This predecessor is retrieved from the file for layer  U   That node has the optimal leaf and parent set for that subnetwork  Recursively  the optimal leaves and parent sets are retrieved until reconstructing the entire network structure  We use this approach instead of the standard divide and conquer solution reconstruction because  as shown in Section    it requires relatively little memory  Furthermore  divide and conquer would require regeneration of the parent graphs  which is quite expensive in terms of time and memory  The pseudocode uses the function reconstructSolution to extract this information from the M DLP l files      ADVANTAGES OF OUR ALGORITHM Our frontier breadth first branch and bound algorithm has several advantages over previous algorithms for learning optimal Bayesian networks  First  our top down search of the AD tree for calculating scores ensures we never need to calculate scores or counts of large variable sets  The AD tree method is in contrast to the bottom up method used by other algorithms  Silander and Myllymaki        Bottom up methods must always compute the scores  or at least the counts  of large parent sets in order to correctly calculate the counts required for the smaller ones  Since our algorithm neither calculates nor  stores these counts and scores  it both runs more quickly and uses less memory  Second  the layered search strategy reduces the memory requirements by working with one layer of the parent and order graphs at a time  Other information can be either discarded immediately or stored in hard disk files for later use  e g   the information needed to reconstruct the optimal network structure  Previous formulations  such as PCaches  Singh and Moore       and arrays  Silander and Myllymaki        could not take advantage of this structure  Singh and Moore propose a depth first search through the P Caches  while Silander and Myllymakis approach identifies the sets according to their lexicographic ordering   We use the lexicographic order within each layer  not over all of the variables   These approaches can identify neither optimal parent sets nor optimal subnetworks one layer at a time  Thus  they must both keep all of the optimal parent sets and subnetworks in memory  Third  we prune the order graph using an admissible heuristic function  this further reduces the memory complexity of the algorithm  Pruning unpromising nodes from the order graph not only reduces the amount of computation but also reduces the memory requirement  Furthermore  the savings in running time and memory also propagate to parent graphs  Dynamic programming algorithms always evaluate the full order graph  The duplicate detection method we use lifts the requirement that open lists fit in RAM to detect duplicates  Because our algorithm does not resort to delayed duplicate detection until RAM is full  our algorithm can still take advantage of large amounts of RAM  By writing nodes to disk  we can learn optimal Bayesian networks even when single layers of the search graphs do not fit in RAM  Our algorithm also has advantages over other learning formulations  In contrast to the A  algorithm of Yuan et al          we only keep one layer of the order graph in memory at a time  The open and closed lists of A  keep all generated nodes in memory to perform duplicate detection  Unlike the systematic search algorithm of de Campos et al   de Campos  Zeng  and Ji        we always search in the space of DAGs  which is smaller than the space of directed graphs in which that algorithm searches  The LP algorithm  Jaakkola et al        uses the same mechanism to identify optimal parent sets as DP  therefore  it cannot complete when all optimal parent sets do not fit in memory     EXPERIMENTS We compared a Java implementation of the externalmemory frontier BFBnB search with DDD  BFBnB  to an efficient version  Silander and Myllymaki       of dynamic programming which uses external memory written   Dataset dataset n wine    adult    zoo    houseVotes    letter    statlog    hepatitis    segment    meta    imports    horseColic    spect  heart     mushroom    parkinsons    sensorReadings    autos    horseColic  full     steelPlatesFaults    flag    wdbc    epigenetic     N                                                                                                       DP                                                                              OD OD  Timing Results  s  BFBnB A                                                                                           OM       OM       OM       OM        OM        OM         OM  SS     OT OT       OT OT           OT             OT OT OT OT OT OT OT OT OT OT  Space Results  bytes  DP BFBnB     E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E        E    OD     E    OD     E     Table    A comparison of the running time  in seconds  for Silander and Myllymakis dynamic programming implementation  DP   Yuan et al s A  algorithm  A    de Campos et al s systematic search algorithm  SS  and our external memory frontier breadth first branch and bound algorithm  BFBnB   The run times are given for all algorithms  Maximum external memory usage is given for DP and BFBnB  For reference   E    is   gigabyte  n is the number of variables  N is the number of records  OT means failure to find optimal solutions due to running for more than   hours        seconds  less than    variables  or    hours         seconds          variables  and not producing a provably optimal solution  OM means failure to find optimal solutions due to running out of RAM    GB   OD means failure to find optimal solutions due to running out of hard disk space     GB   in C downloaded from http  b course hiit fi bene  We refer to it as DP  Previous results  Silander and Myllymaki       have shown DP is more efficient than other dynamic programming implementations  We also compared to Yuan et al s A  implementation         A   and de Campos et al s branch and bound systematic search algorithm  de Campos  Zeng  and Ji        SS  downloaded from http   www ecse rpi edu  cvrl structlearning html  We did not include comparison to the DP implementation of Malone et al          MDP  because the codebase is similar  however  MDP does not incorporate pruning or delayed duplicate detection  The running times of BFBnB and MDP are similar on datasets which both complete  but  due to duplicate detection  MDP fails when an entire layer of the order graph does not fit in RAM  Benchmark datasets from the UCI repository  Frank and Asuncion       were used to test the algorithms  We also constructed a biological dataset consisting of ChIP Seq data for epigenetic features downloaded from http   dir nhlbi nih gov papers lmi epigenomes hgtcell html  and http   dir nhlbi nih gov papers lmi epigenomes  hgtcellacetylation aspx  The experimental datasets were normalized using linear regression using the IgG control dataset downloaded from http   home gwu edu wpeng Software htm  The largest datasets in the comparison have up to    variables and over        records  Continuous and discrete variables with more than four states were discretized into two states around the mean  Records with missing values were removed  DP and SS do not calculate the MDL score for a network  however  they can calculate BIC  The score uses an equivalent calculation as MDL  so the algorithms always learned equivalent networks  The experiments were performed on a      GHz Intel i  with   GB of RAM     GB of hard disk space and running Ubuntu version        On datasets with less than    variables  all algorithms were given a maximum runtime of   hours        seconds   On datasets with    to    variables  all algorithms were given a maximum runtime of    hours         seconds     We empirically evaluated the algorithms for both space and time requirements  For the algorithms which used external memory  BFBnB and DP   we compared the maximum hard disk usage  We also compared the running times of the algorithms  The results are given in Table    Previous results found that memory is the main bottleneck restricting the size of learnable networks  Parviainen and Koivisto        As the results show  algorithms which attempt to store entire parent or order graphs in RAM  such as A  and SS  are limited to smaller sets of variables  BFBnBs duplicate detection strategy allows it to write parital search layers to hard disk when the layers are too large to fit in RAM  so it can learn optimal Bayesian network structures regardless of the amount of RAM  Consequently  hard disk space is its only memory limitation  The inexpensive cost of hard disks coupled with distributed file systems can potentially erase the effect of memory on the scalability of the algorithm  For the datasets which it could solve  A  was sometimes faster than the other algorithms  This is unsurprising since it uses only RAM  however  it is unable to solve the larger datasets that cannot fit entirely in RAM  Even on many of the smaller datasets  though  A  runs more slowly than BFBnB because it has the overhead cost to keep its open list in sorted order  BFBnB not only takes an order of magnitude less external memory  but runs several times faster than the DP algorithm on most of the datasets  DP is faster on the adult  letter and meta datasets  These datasets have a small number of variables and a large number of records  The large number of records limits the pruning of the AD tree from Theorem   and increases the runtime of BFBnB  However  BFBnB runs faster on both mushroom        records  and sensorReadings        records   Therefore  as the number of variables increases  the number of records impacts the runtime less  The SS algorithm ran much more slowly than the other algorithms  It searches in the space of directed graphs rather than DAGs  These results suggest that search in the space of DAGs is more efficient than the space of directed graphs  To demonstrate that our algorithm is applicable to larger datasets  we also tested it using the wdbc dataset     variables      records  and a biological dataset     variables         records   epigenetic  We learned the optimal network for wdbc in        seconds  about    hours  and the optimal network for epigenetic in         seconds  about   days   We also attempted to use DP  but its hard disk usage exceeded the    GB of free hard disk space on the server  Figure   shows the total memory consumption of our algorithm for wdbc  Very little memory is used before layer    and after layer     the memory consumption does not change much because the layer sizes decrease  As the figure shows  both of the middle layers use nearly    giga   Figure    Hard disk usage for the wdbc dataset  bytes of disk space  Most of this space is consumed by the parent graphs  so it is is freed after each layer  Assuming that the running time and size of the middle layers double for each additional variable  which is a rough pattern from Table    our algorithm could learn a    variable network in about    days using approximately   terabytes of hard disk space and a single processor  This suggests that our method should scale to larger networks better than the method of Parviainen and Koivisto         They observe that their implementation would take   weeks on     processors to learn a    variable network  and  even with coding improvements and massive parallelization  only networks up to    variables would be possible     CONCLUSION Learning optimal Bayesian network structures has been thought of in terms of dynamic programming  however  such a formulation naively requires O n n   memory  Other formulations have been shown to have similar or slower runtimes or require other exponential resources  such as processors  This paper formulates the structure learning problem as a frontier breadth first branch and bound search  The layered search technique allows us to work with one layer of the score cache  parent and order graphs at a time  Consequently  we delete layers of the parent graphs after expanding them and store only a portion of each order graph node to hard disk files to reduce the memory complexity  The delayed duplicate detection strategy further improves the scalability of the algorithm by writing partial layers to disk rather than requiring an entire layer fit in RAM at once  Additionally  a heuristic function allows parts of the order graph to be ignored entirely  this also reduces memory complexity and improves scalability  Experimental results demonstrate that this algorithm outperforms the previous best implementation of dynamic programming for learning optimal Bayesian networks  Our algorithm not only runs faster than the existing approach  but also takes much less space  The LP formulation exhibits similar runtime behavior as DP  so our algorithm should   similarly outperform it  It also scales to more variables than A   Additionally  by searching in the space of DAGs instead of the space of directed graphs with cycles  it proves the optimality of the learned network more quickly than SS  Future work will investigate better upper bounds and heuristic functions to further increase the size of learnable optimal networks  Also  like existing methods  Parviainen and Koivisto       Silander and Myllymaki        our algorithm can benefit from parallel computing  In addition  distributed computing can scale up our algorithm to even larger learning problems  Networks learned from our algorithm could also be used as a gold standard in studying the assumptions of approximate structure learning algorithms  Acknowledgements This work was supported by NSF CAREER grant IIS         and EPSCoR grant EPS          
 A branch and bound approach to solving influence diagrams has been previously proposed in the literature  but appears to have never been implemented and evaluated  apparently due to the difficulties of computing effective bounds for the branch and bound search  In this paper  we describe how to efficiently compute effective bounds  and we develop a practical implementation of depth first branch and bound search for influence diagram evaluation that outperforms existing methods for solving influence diagrams with multiple stages      Introduction  An influence diagram     is a compact representation of the relations among random variables  decisions  and preferences in a domain that provides a framework for decision making under uncertainty  Many algorithms have been developed to solve influence diagrams                                Most of these algorithms  whether they build a secondary structure or not  are based on the bottom up dynamic programming approach  They start by solving small low level decision problems and gradually build on these results to solve larger problems until the solution to the global level decision problem is found  The drawback of these methods is that they can waste computation in solving decision scenarios that have zero probability or that are unreachable from any initial state by following an optimal decision policy  This drawback can be overcome by adopting a branch andbound approach to solving an influence diagram that uses a search tree to represent all possible decision scenarios  This approach can use upper bounds on maximum utility to prune branches of the search tree that correspond to lowquality decisions that cannot be part of an optimal policy  it can also prune branches that have zero probability   A branch and bound approach to influence diagram evaluation appears to have been first suggested by Pearl       He proposed it as an improvement over the classic method of unfolding an influence diagram into a decision tree and solving it using the rollback method  which itself is a form of dynamic programming      In Pearls words  A hybrid method of evaluating influence diagrams naturally suggests itself  It is based on the realization that decision trees need not actually be generated and stored in their totality to produce the optimal policy  A decision tree can also be evaluated by traversing it in a depth first  backtracking manner using a meager amount of storage space  proportional to the depth of the tree   Moreover  branch and bound techniques can be employed to prune the search space and permit an evaluation without exhaustively traversing the entire tree    an influence diagram can be evaluated by sequentially instantiating the decision and observation nodes  in chronological order  while treating the remaining chance nodes as a Bayesian network that supplies the probabilistic parameters necessary for tree evaluation   p       However  neither Pearl nor anyone else appears to have followed up on this suggestion and implemented such an algorithm  The apparent reason is the difficulty of computing effective bounds to prune the search tree  Qi and Poole      proposed a similar search based method for solving influence diagrams  but with no method for computing bounds  in fact  their implementation relied on the trivial infinity upper bound to guide the search  Recently  Marinescu      proposed a related search based approach to influence diagram evaluation  But again  he proposed no method for computing bounds  his implementation relies on brute force search  Even without bounds to prune the search space  note that both Qi and Poole and Marinescu argue that a search based approach has advantages  for example  it can prune branches that have zero probability    In this paper  we describe an implemented depth first branch and bound search algorithm for influence diagram evaluation that includes efficient techniques for computing bounds to prune the search tree  To compute effective bounds  our algorithm adapts and integrates two previous contributions  First  we adapt the work of Nilsson and Hohle      on computing an upper bound on the maximum expected utility of an influence diagram  The motivation for their work was to bound the quality of strategies found by an approximation algorithm for solving limitedmemory influence diagrams  and their bounds are not in a form that can be directly used for branch and bound search  We show how to adapt their approach to branchand bound search  Second  we adapt the recent work of Yuan and Hansen      on solving the MAP problem for Bayesian networks using branch and bound search  Their work describes an incremental method for computing upper bounds based on join tree evaluation that we show allows such bounds to be computed efficiently during branch andbound search  In addition  we describe some novel methods for constructing the search tree and computing probabilities and bounds that contribute to an efficient implementation  Our experimental results show that this approach leads to an exact algorithm for solving influence diagrams that outperforms existing methods for solving multistage influence diagrams      Background  We begin with a brief review of influence diagrams and algorithms for solving them  We also introduce an example of multi stage decision making that will serve to illustrate the results of the paper       Influence Diagrams  An influence diagram is a directed acyclic graph G containing variables V of a decision domain  The variables can be classified into three groups  V   X  D  U  where X is the set of oval shaped chance variables that specify the uncertain decision environment  D is the set of square shaped decision variables that specify the possible decisions to be made in the domain  and U are the diamond shaped utility variables representing a decision makers preferences  As in a Bayesian network  each chance variable Xi  X is associated with a conditional probability distribution P  Xi  P a Xi     where P a Xi   is the set of parents of Xi in G  Each decision variable Dj  D has multiple information states  where an information state is an instantiation of the variables with arcs leading into Dj   the selected action is conditioned on the information state  Incoming arcs into a decision variable are called information arcs  variables at the origin of these arcs are assumed to be observed before the decision is made  These variables are called the information variables of the decision  No forgetting is typically  assumed for an influence diagram  which means the information variables of earlier decisions are also information variables of later decisions  We call these past information variables the history  and  for convenience  we assume that there are explicit information arcs from history information variables to decision variables  Finally  each utility node Ui  U represents a function that maps each configuration of its parents to a utility value the represents the preference of the decision maker   Utility variables typically do not have other variables as children except multi attribute utility super value variables   The decision variables in an influence diagram are typically assumed to be temporally ordered  i e   the decisions have to be made in a particular order  Suppose there are n decision variables D    D         Dn in an influence diagram  The decision variables partition the variables in X into a collection of disjoint sets I    I         In   For each k  where     k   n  Ik is the set of chance variables that must be observed between Dk and Dk     I  is the set of initial evidence variables that must be observed before D    In is the set of variables left unobserved when decision Dn is made  Therefore  a partial order  is defined on the influence diagram over X  D  as follows  I   D   I        Dn  In         A solution to the decision problem defined by an influence diagram is a series of decision rules for the decision variables  A decision rule for Dk is a mapping from each configuration of its parents to one of the actions defined by the decision variable  A decision policy  or strategy  is a series of decision rules with one decision rule for each decision variable  The goal of solving an influence diagram is to find an optimal decision policy that maximizes the expected utility  The maximum expected utility is equal to     max P  X D  Uj  P a Uj     max     I   D   In   Dn  In  j  In general  the summations and maximizations are not commutable  The methods presented in Section     differ in the various techniques they use to carry out the summations and maximizations in this order  Recent research has begun to relax the assumption of ordered decisions  In particular  Jensen proposes the framework of unconstrained influence diagrams to allow a partial ordering among the decisions      Other research relaxes the no forgetting assumption  in particular  the framework of limited memory influence diagrams       Although the approach we develop can be extended to these frameworks  we do not consider the extension in this paper       Example  To illustrate decision making using multi stage influence diagrams  consider a simple maze navigation problem              ns        x        ns        es        x                a   Figure    Two maze domains  A star represents the goal       Figure   shows four instances of the problem  The shaded tiles represent walls  the white tiles represent movable space  and the white tiles with a star represent goal states  An agent is randomly placed in a non goal state  It has five available actions that it can use to move toward the goal  it can move a single step in any of the four compass directions  or it can stay in place  The effect of a movement action is stochastic  The agent successfully moves in the intended direction with probability       It fails to move with probability        it moves sideways with probability            for each side   and it moves backward with probability        If movement in some direction would take it into a wall  that movement has probability zero  and the remaining probabilities are normalized  The agent has four sensors  one for each direction  which sense whether the neighboring tile in that direction is a wall  Each sensor is noisy  it detects the presence of a wall correctly with probability     and mistakenly senses a wall when none is present with probability       The agent chooses an action at each of a sequence of stages  If the agent is in the goal state after the final stage  it receives a utility value of      otherwise  it receives a utility value of      Figure   a  shows the influence diagram for a two stage version of the maze problem  The variables xi and yi represent the location coordinates of the agent at time i  the variables  nsi   esi   ssi   wsi   are the sensor readings in four directions at the same time point  and the variable di represents the action taken by the agent  The utility variable u assigns a value depending on whether or not the agent is in the goal state after the last action is taken  Since the same variables occur at each stage  we can make the influence diagram arbitrarily large by increasing the number of stages       Evaluation algorithms  Many approaches have been developed for solving influence diagrams  The simplest is to unfold an influence diagram into an equivalent decision tree and solve it in that form      Another approach called arc reversal solves an influence diagram directly using techniques such as arcreversal and node removal           when a decision variable is removed  we obtain the optimal decision rule for the decision  Several methods reduce influence diagrams into  y         y       ss        ss         ws        ws          b   u  d         d       y        x         es          a  R                                                  b  Figure     a  An example influence diagram and  b  its strong join tree  The numbers in both figures stand for the indices of the variables  The node with R is the strong root   Bayesian networks by converting decision nodes into random variables such that the solution of an inference problem in the Bayesian network correspond to the optimal decision policy for the influence diagram          Another method      transforms an influence diagram into a valuation network and applies variable elimination to solve the valuation network  Recent work compiles influence diagrams into decision circuits and uses the decision circuits to compute optimal policies      this approach takes advantage of local structure present in an influence diagram  such as deterministic relations  In the following  we describe a state of the art method for solving an influence diagram using a strong join tree      This method is viewed by many as the fastest general algorithm  and we use its performance as a benchmark for our branch and bound approach  A join tree is strong if it has at least one clique  R  called the strong root  such that for any pair of adjacent cliques  C  and C    with C  closer to R than C    the variables in separator S   C   C  must appear earlier in the partial order defined in Equation     than C    C    A strong join tree for the influence diagram in Figure   a  is shown in Figure   b   An influence diagram can be solved exactly by message passing on the strong join tree  Each clique C in the join tree contains two potentials  a probability potential C and a utility potential C   For clique C  to send a message to clique C    C  and C  should be updated as follows        C    C   S   C   C       S   S   where S      C    S   max C   C     C   S  C   S  information arcs from each variable in R to D is guaranteed to have a maximum expected utility that is greater than or equal to the maximum expected utility for G  We call G an upper bound influence diagram for G   In contrast to the join tree algorithm for Bayesian networks       only the collection phase of the algorithm is needed to solve an influence diagram  After the collection phase  we can obtain the maximum expected utility by carrying out the remaining summations and maximizations in the root  In addition  we can extract the optimal decision rules for the decision variables from some of the cliques that contain these variables   Use of an upper bound influence diagram to compute bounds only makes sense if the upper bound influence diagram is simpler and much easier to solve than the original influence diagram  In fact  adding information arcs to an influence diagram can simplify its evaluation by making some other information variables non requisite  An information variable Ii is said to be non requisite          for a decision node D if  Building a join tree for a Bayesian network may fail if the join tree is too large to fit in memory  This is also true for influence diagrams  In fact  the memory requirement of a strong join tree for an influence diagram is even higher because of the constrained order in Equation      Consequently  the join tree algorithm is typically infeasible for solving all but very small influence diagrams   Ii   U  de D   D   P a D     Ii         Computing bounds  To implement a branch and bound algorithm for solving influence diagrams  we need a method for computing bounds  in particular  for computing upper bounds on the utility that can be achieved beginning at a particular stage of the problem  given the history up to that stage  A trivial upper bound is the largest state dependent value of the utility node of the influence diagram  In this section  we discuss how to compute more informative bounds  There has been little previous work on this topic  Nilsson and Hohle      develop an approach to bounding the suboptimality of policies for limited memory influence diagrams that are found by an approximation algorithm  Dechter     describes an approach to computing upper bounds in an influence diagram that is based on mini bucket partitioning  Neither work considers how to use these bounds in a branch and bound search algorithm  The approach we develop in the rest of this paper is based on the work of Nilsson and Hohle       which we extend by showing how it can be used to compute bounds for branchand bound search  The general strategy is to create an influence diagram with a value that is guaranteed to be an upper bound on the value of the original influence diagram  but that is also much easier to solve  We use the fact that additional information can only increase the value of an influence diagram  Since this reflects the well known fact that information value is non negative  we omit  for space reasons  a proof of the following theorem  Theorem    Let G be an influence diagram and D a decision variable in G  Let I be Ds information variables and R another set of random variables in G that are nondescendants of D  Then the influence diagram G that results from making R into information variables by adding       where de D  are the descendants of D  A reduction of an influence diagram is obtained by deleting all the nonrequisite information arcs       Because of the no forgetting assumption  a decision variable at a late stage may have a large number of history information variables  For decision making under imperfect information  all of these information variables are typically requisite  As a result  the size of the decision rules grows exponentially as the number of decision stages increases  which is the primary reason multi stage influence diagrams are very difficult to solve  In constructing an upper bound influence diagram  we want to add information arcs that make some or all of the history information variables for a decision node non requisite  in order to simplify the influence diagram and make it easier to solve  We adopt the strategy proposed by Nilsson and Hohle       Let nd X  be the non descendant variables of variable X  let f a X    P a X    X  be the family of variable X  i e   the variable and its parents   let f a X    Xi X f a Xi   be the family of the set of variables X  i e   the variables and their parents   and let j be  D       Dj   be a sequence of decision variables from stage   to stage j  The following theorem is proved by Nilsson and Hohle       Theorem    For an influence diagram with the constrained order in Equation      if we add to each decision variable Dj the following new information variables in the order of j   n          Nj   arg minB Bj nd Dj      B  f a j     U  de Dj     B   Dj            where    UD j k  Bj   ki j    n  V  n   U  de Dj    f a Di    j k  the following holds for any Dj in the resulting influence diagram   de Dj    U   f a i    f a Dj           ns          Past state     Sufficient Information D Set  Future state  U     x        ns        es        x        ss        ss         ws        ws          a   What this theorem means is that for each decision variable Dj in an influence diagram  there is a set of information variables Nj such that the optimal policy for Dj depends only on these information variables  and is otherwise history independent  Note that the set Nj for decision variable Dj can contain both information variables from the original influence diagram and information variables created by adding new information arcs to the diagram  The set Nj of information variables can be interpreted as the current state of the decision problem  such that if the decision maker knows the current state  it does not need to know any previous history in order to select an optimal action  in this sense  the state satisfies the Markov property    b   We call Nj a sufficient information set  SIS  for Dj   The intuition behind this approach is illustrated by Figure    The shaded oval shows the SIS set ND for decision D  The past state affects the variables in ND   D   illustrated by the arc labeled    and ND   D  affects the future state  as illustrated by arc    The future state further determines the values of the utility variables  ND   D  d separates the past and future states and prevents the direct influence shown by arc    The concept of a sufficient information set is related to the concept of extremality  as defined in       and the concept of blocking  as defined in       To construct an upper bound influence diagram  we find the SIS set for each decision in the order of Dn        D  and make the variables in each SIS set information variables for the corresponding decisions  We then delete the nonrequisite information arcs  Consider the influence diagram in Figure   a  as an example  The information set for d  is originally  ns    es    ss    ws    d    ns    es    ss    ws     We find that its sufficient information  SIS  set is  x    y     We  y         y        Figure    Relations between past and future information states and the minimum sufficient information set   Consider a decision making problem in a partially observable domain  such as the maze domain of Section      The agent cannot directly observe its location in the maze and must rely on imperfect sensor readings to infer its location  In this domain  adding information arcs from the location variables to a decision variable  so that the agent know the current location at the time it chooses an action  makes both current and history sensor readings non requisite  which results in a much simpler influence diagram  which in this case serves as an upper bound influence diagram   u  d         d       y        x         es         Figure     a  the upper bound influence diagram for the diagram in Figure   a   and  b  its strong join tree   also find that the SIS set for d  is  x    y     By making  x    y    and  x    y    information variables for d  and d  respectively  and reducing the influence diagram  we obtain the much simpler influence diagram in Figure   a   The strong join tree for the new influence diagram is shown in Figure   b   which is also much smaller than the strong join tree for the original model  Since the upper bound influence diagram assumes the actual location is directly observable to the agent  it effectively transforms a partially observable decision problem into a fully observable one  The resulting influence diagram and  hence  its join tree is much easier to solve  Finding the sufficient information set  SIS  for a decision variable in an influence diagram is equivalent to finding a minimum separating set in the moralized graph of the influence diagram       Acid and de Campos     propose an algorithm based on the Max flow Min cut algorithm     for finding a minimum separating set between two sets of nodes in a Bayesian network with some of the separating variables being fixed  We use their algorithm to find the SIS sets  The two sets of nodes are f a j   and U  de Dj    The only fixed separating variable is Dj   The algorithm first introduces two dummy variables  source and sink  to the moralized graph  The source is connected to the neighboring variables of f a j    and the sink to the variables in de Dj    an U  de Dj     We then create a max flow network out of the undirected graph by assigning each edge capacity      A solution gives a minimum separating set between the sink and source that contains Dj   We briefly mention some issues that are not described by Nilsson and Hohle       but that need to be considered in an implementation  The first issue is how to define the size of an SIS set  Theorem   uses the cardinality of the SIS set as the minimization criterion  Another viable choice is to use weight  defined as the product of number of states  of the variables in an SIS set as the minimization criterion    The relation between these two criteria is similar to the relation between treewidth and weight in constructing a junction tree  While treewidth tells us how complex a Bayesian network is at the structure level  weight provides an idea on how large the potentials of the junction tree are at the quantitative level  Both methods have been used  In our implementation  we use the cardinality  A second issue is that multiple candidate SIS sets may exist for a decision variable  In that case  we need some criterion for selecting the best one  In our implementation  we select the candidate SIS set that is closest to the descendant utility variables of the decision  Note that other candidate sets are all d separated from the utility node by the closest SIS set  This choice has the advantage that the resulting influence diagram is easiest to evaluate  however  other choices may result in an influence diagram that gives a tighter bound      Branch and bound search  In this section  we describe how to use the upper bound influence diagram to compute bounds for a depth first branch and bound search algorithm that solves the original influence diagram  We begin by showing how to represent the search space as an AND OR tree  A naive approach to computing bounds requires evaluating the entire upper bound influence diagram at each OR node of the search tree  which is computationally prohibitive  To make branch and bound search feasible  we rely on an incremental approach to computing bounds proposed by Yuan and Hansen      for solving the MAP problem in Bayesian networks using branch and bound search  We show how to adapt that approach in order to solve influence diagrams efficiently       AND OR tree search  We represent the search space for influence diagram evaluation as an AND OR tree  The nodes in an AND OR tree are of two types  AND nodes and OR nodes  AND nodes correspond to chance variables  a probability is associated with each arc originating from an AND node and the probabilities of all the arcs from an AND node sum to      The OR nodes correspond to decision variables  Each of the leaf nodes of the tree has a utility value that is derived from the utility node of the influence diagram  Qi and Poole      create an AND OR tree in which each layer of AND nodes alternates with a layer of OR nodes  Each AND node in this tree corresponds to the information set of a decision variable in the influence diagram  which is a set of information variables  To compute the probability for each arc emanating from an AND node in this tree  however  it is necessary to have the joint probability distributions of all the information sets  these are often not readily available  since variables in the same informa   ns  es  ss   ws  d   es  ss   ss   ws   ws   d   d   ss   ws  d   Figure    The AND OR tree used in our approach  Ovalshaped nodes are AND nodes  and square shaped nodes are OR nodes  tion set can belong to different clusters of a join tree  For computational convenience  our AND OR tree is based on the structure of the strong join tree in Figure   b   For the maze example  we order the variables in the information set  ns    es    ss    ws    according to the order in Equation      Note that the join tree does not have a clique that contains all four variables  in fact  they are all in different cliques  So we consider the variables one by one  That means that our AND OR tree allows several layers of AND nodes to alternate with a layer of OR nodes  See Figure   for an example of the kind of AND OR tree constructed by our search algorithm  and note that the first four layers of this AND OR tree are all AND layers  Each path from the root of the AND OR tree to a leaf corresponds to a complete instantiation of the information variables and decision variables of the influence diagram  that is  a complete history  Since we use the AND OR tree to find an optimal decision policy for the original influence diagram  we have to construct an AND OR tree that is consistent with the original constrained order  For the influence diagram in Figure   a   the partial order is  ns    es    ss    ws      d      ns    es    ss    ws      d      x    y    x    y    x    y    u       and the decision variables must occur in this order along any branch  We define a valuation function for each node in an AND OR tree as follows   a  for a leaf node  the value is its utility value   b  for an AND node  the value of is the sum of the values of its child nodes weighted by the probabilities of the outgoing arcs   c  for an OR node  the value is the maximum of the summed utility values of each child node and corresponding arc  We use this valuation function to find an optimal strategy for the influence diagram  We represent a strategy for a multi stage influence diagram as a policy tree that is defined as follows  A policy tree of an AND OR tree is a subtree such that   a  it consists of the root of the AND OR tree   b  if a non terminal AND node is in the policy tree  all its children are in the policy tree  and  c  if a non terminal OR node is in the policy   tree  exactly one of its children is in the policy tree  Given an AND OR tree that represents all possible histories and strategies for an influence diagram  the influence diagram is solved by finding a policy tree with the maximum value at the root  where the value of the policy tree is computed based on the valuation function  Depth first branch andbound search can be used to find an optimal policy tree  The AND OR tree is constructed on the fly during the branch and bound search  and it is important to do so in a way that allows the probabilities and values to be computed as efficiently as possible  We use the maze problem and the AND OR tree in Figure   as an example   The upper bound influence diagram and join tree are shown in Figure     We have already pointed out that including more layers in our AND OR tree allows us to more easily use the probabilities and utilities computed by the join tree  If we start by expanding ns   where expanding a node refers to generating its child nodes in the AND OR tree   we need the probabilities of P  ns    to label the outgoing arcs  We can readily look up the probabilities from clique           after an initial full join tree evaluation  Note that we use the join tree of the upper bound influence diagram to compute the probabilities  We can do that because these probabilities are the same as those computed from the original influence diagram  This is due to the fact that the same set of actions will reduce both models into the same Bayesian networks  Adding information arcs to an influence diagram  in order to create an upper bound influence diagram  only changes the expected utilities of the decision variables  After expanding ns    we expand any of  es    ss    ws     Suppose the next variable is es    we need the conditional probabilities of es  given ns    These probabilities can be computed by setting the state of ns  as new evidence to the join tree and evaluating the join tree again  The same process is used in expanding  ss    ws     Note that we do not have to expand one variable at a time  If a clique has multiple variables in the same information set  the variables can be expanded together because a joint probability distribution over them can be easily computed  Expanding them together also saves the need to do marginalization  For example  variables x    y    x    y              are in the same information group and also reside in a same clique  In this case  we can expand them as a single layer in the AND OR tree  After  ns    es    ss    ws    are expanded  we expand d  as an OR layer  This is where the upper bounds are needed  We set the states of  ns    es    ss    ws    as evidence to the join tree and compute the expected utility values for d  by reevaluating the join tree  The expected utilities of d  are upper bounds for the same decision scenarios of the original model  We can use the upper bounds to select the most promising decision alternative to search first  The exact value will be returned once the subtree is searched  If the  value is higher than the upper bounds of the remaining decision alternatives  these alternatives are immediately pruned because they cannot be part of an optimal decision policy  We repeat the above process until a complete policy tree is found       Incremental join tree evaluation  It is clear that repeated join tree evaluation has to be performed in computing the upper bounds and conditional probabilities  A naive approach is at each time to set the states of instantiated variables as evidence and perform a full join tree evaluation  However  that is too costly  We can use an efficient incremental join tree evaluation method to compute the probabilities and upper bound utilities  based on the incremental join tree evaluation method proposed by Yuan and Hansen      for solving the MAP problem  The key idea is that we can choose a particular order of the variables that satisfies the constraints of Equation     such that an incremental join tree evaluation scheme can be used to compute the information  Given such an order  we know exactly which variables have been searched and which variable will be searched next at each search step  We only need to send messages from the parts of the join tree that contain the already searched variables to a clique with the next search variable  For example  after we search es    the only message needs to be sent to obtain P  ns   es    is the message from clique           to            There is no need to evaluate the whole join tree  If we choose the following search order for the maze problem ns    es    ss    ws    d    ns    es    ss    ws    d    x    y    x    y    x    y    we can use an incremental message passing in the direction of the dashed arc in Figure   b  to compute the probabilities and upper bounds needed in one downward pass of a depthfirst search  Both message collection and distribution are needed in this new scheme  unlike evaluating a strong join tree for the original influence diagram  The messages being propagated contain two parts  utility potentials and probability potentials  The distribution phase is typically needed to compute the conditional probabilities  For example  suppose we decide to expand es  before ns    we have to send a message from clique           to           to obtain P  ns   es     We only need to send the probability potential in message distribution  We do not need to send utility potentials because past payoffs do not count towards the expected utilities of future decisions       Efficient backtracking  We use depth first branch and bound  DFBnB  to utilize the efficient incremental bound computation  Depth first   a  b  stages                  Join tree time mem                                       time      s      s      m  s         s      s      m  s     mem                                         DFBnB policy                                                             bounds                                                              zeros                  Exhaustive search time mem   zeros             s            m  s                       s            m  s              Table    Comparison of three algorithms  join tree algorithm  DFBnB using the join tree bounds  and exhaustive search of the AND OR tree  in solving maze problems a and b for different numbers of stages  The symbol   indicates the problem could not be solved  due to memory limits in the case of the join tree algorithm  or due to a three hour time limit in the case of the search algorithms  Running time is in hours  h   minutes  m   seconds  s  and milliseconds  Memory  mem   is in megabytes and is the peak amount of memory used by the algorithm  policy is the count of nodes  both OR and AND nodes  in the part of the search tree containing the optimal policy tree  it is the same for both DFBnB and exhaustive search   bounds is the count of times a branch from an OR node was pruned based on bounds   zeros is the count of times a branch from an AND node was pruned because it had zero probability  search makes sure that the search need not jump to a different search branch before backtracking is needed  In other words  the join tree only needs to work with one search history at a time  We do need to backtrack to a previous search node once we finish a search branch or realize that a search branch is not promising and should be pruned  We need to retract all the newly set evidence since the generation of that search node and restore the join tree to a previous state  One way to achieve this is to reinitialize the join tree with correct evidence and perform a full join tree evaluation  which we have already pointed out is too costly  Instead  we cache the potentials and separators along the message propagation path before changing them by either setting evidence or overriding them with new messages  When backtracking  we simply restore the most recently cached potentials and separators in the reverse order  The join tree will be restored to the previous state with no additional computations  This backtracking method is much more efficient than reevaluating the whole join tree  For solving the MAP problem for Bayesian networks  Yuan and Hansen      show that the incremental method is more than ten times faster than full join tree evaluation in depth first branchand bound search      Empirical Evaluation  We compared the performance of our algorithm against both the join tree algorithm     and exhaustive search of the AND OR tree  i e   no bounds are used for pruning   Experiments were run on a     GHz Duo processor with   gigabytes of RAM running Windows XP  The results in Table   are for the two maze problems in Figure    which we solved for different numbers of stages  When there are only two or three stages  the join tree al   gorithm is most efficient  This is because the strong join trees for these influence diagrams are rather small and can be built successfully  Once the join trees are built  only one collection phase is necessary to solve the influence diagram  by contrast  the depth first branch and bound algorithm  DFBnB  algorithm must perform repeated message propagations to compute upper bounds and probabilities during the search  For more then three stages  however  the join tree algorithm cannot solve the maze models because the strong join trees are too large to fit in memory  Because the exhaustive search algorithm only needs to store the search tree and policy  it can solve the maze models for up to four stages  although it takes more then    minutes to do so  The DFBnB algorithm can solve the maze models for up to five stages in less time than it takes the exhaustive search algorithm to solve them for four stages  demonstrating the advantage of using bounds to prune the search tree  Table   includes the number of times a branch of the search tree is pruned based on bounds  as well as the number of times a branch with zero probability is pruned  For the maze models with their original parameter settings  every branch of the search tree has non zero probability  Previous work has argued that one of the advantages of search algorithms for influence diagram evaluation is that they can prune branches of the search tree that have zero probability  even without bounds           To test this argument  as well as to illustrate the effect of different problem characteristics on algorithm performance  we modified the maze models described in Section      First  we removed some noise from the sensors  Each of the four sensors reports a wall in the corresponding direction of the compass  In the original problem  each sensor is noisy  it detects the presence of a wall correctly with probability     and mistakenly senses a wall when none is present with probability       As a result  every sensor reading is possible in every state and there are no zero probability branches  We   stages                      a  b  a  b  stages                          Join tree time mem                                       Maze domains modified by removing some noise from sensors DFBnB Exhaustive search time mem  policy  bounds  zeros time mem   zeros                                s                                      s                          s                m  s                             m  s                  m  s                                 h m  s                                                 s                         s                s                           s                m  s                              m  s                   m  s                                  h  m  s                     Maze domains modified by removing some noise from both actions and sensors Join tree DFBnB Exhaustive search time mem  time mem  policy  bounds  zeros time mem   zeros                                                                       s               s                           s                 s                           m  s                 m  s                               m  s                      m  s                                                                                                     s               s                           s                 s                            m  s                 m  s                               m  s                      m  s                                    Table    Comparison of three algorithms  join tree algorithm  DFBnB using the join tree bounds  and exhaustive search of the AND OR tree  in solving maze problems a and b with modified parameters  for the results in the top table  some noise is removed from the sensors only  for the results in the bottom table  some noise is removed from the actions and the sensors  Removing some noise from the actions and sensors results in more zero probability branches that can be pruned  allowing the search algorithms  but not the join tree algorithm  to solve the problem for a larger number of stages  modified the model so that each sensor accurately detects whether a wall is present in its direction of the compass  With this change  the maze problem remains partially observable  but the search tree contains many zero probability branches  as can be seen from the results in Table    Since the search algorithms can prune zero probability branches  the exhaustive search algorithm can now solve the problem for up to five stages and the DFBnB algorithm can solve the problem for up to six stages  We next made an additional change to the transition probabilities for actions  In the original problem  the agent successfully moves in the intended direction with probability       as long as there is not a wall   It fails to move with probability        it moves sideways with probability            for each side   and it moves backward with probability        We modified these transition probabilities so that the agent still moves in the intended direction with probability       but otherwise  it stays in the same position with probability       The effects of the agents actions are still stochastic  but they are more predictable  and this allows the search tree to be pruned even further  As a result  the  exhaustive search algorithm can solve the problem for up to six stages and the DFBnB algorithm can solve the problem for up to seven stages  Note that changing the problem characteristics has no effect on the performance of the join tree algorithm  The join tree algorithm solves the influence diagram for all information states  including those that have zero probability and those that are unreachable from the initial state  as a result  its memory requirements explode exponentially in the number of stages and the algorithm quickly becomes infeasible  Although the policy tree that is returned by the search algorithms can also grow exponentially in the number of stages  it does so much more slowly because so many branches can be pruned  As is vividly shown by the results for the two different mazes and for different parameter settings  the performance of the search algorithms is sensitive to problem characteristics  precisely because the search algorithms exploit a form of problem structure that is not exploited by the join tree algorithm  In addition  the results show the effectiveness of bounds in scaling up the searchbased approach       Conclusion  We have described the first implementation of a depthfirst branch and bound search algorithm for influence diagram evaluation  Although the idea has been proposed before  we adapted and integrated contributions from related work and introduced a number of new ideas to make the approach computationally feasible  In particular  we described an efficient approach for using the join tree algorithm to compute upper bounds to prune the search tree  The idea is to generate an upper bound influence diagram by allowing each decision variable to be conditioned on additional information that makes the remaining history nonrequisite  thus simplifying the influence diagram  Then a join tree of the upper bound influence diagram is used to incrementally compute upper bounds for the depth first branch and bound search  We have also described a new approach to constructing the search tree based on the structure of the strong join tree of the upper bound influence diagram  Experiments show that the resulting depth first branch and bound search algorithm outperforms the stateof the art join tree algorithm in solving multistage influence diagrams  at least when there are more than three stages  We are currently considering how to extend this approach to solve limited memory influence diagrams       which typically have many more stages  We are also exploring approaches to compute more accurate bounds for pruning the search tree  Finally  we are considering approximate and bounded optimal search algorithms for solving larger influence diagrams using the same upper bounds and AND OR search tree  Acknowledgments This research was support in part by NSF grants IIS         and EPS          and by the Mississippi Space Grant Consortium and NASA EPSCoR program   
 Bounded policy iteration is an approach to solving infinite horizon POMDPs that represents policies as stochastic finite state controllers and iteratively improves a controller by adjusting the parameters of each node using linear programming  In the original algorithm  the size of the linear programs  and thus the complexity of policy improvement  depends on the number of parameters of each node  which grows with the size of the controller  But in practice  the number of parameters of a node with non zero values is often very small  and does not grow with the size of the controller  Based on this observation  we develop a version of bounded policy iteration that leverages the sparse structure of a stochastic finite state controller  In each iteration  it improves a policy by the same amount as the original algorithm  but with much better scalability     Introduction Partially observable Markov decision processes  POMDPs  provide a framework for decision theoretic planning problems where actions need to be taken based on imperfect state information  Many researchers have shown that a policy for an infinite horizon POMDP can be represented by a finite state controller  In some cases  this is a deterministic controller in which a single action is associated with each node  and an observation results in a deterministic transition to a successor node  Kaelbling  Littman    Cassandra        Hansen        Meuleau  Kim  Kaelbling    Cassandra      a   In other cases  it is a stochastic controller in which actions are selected based on a probability distribution associated with each node  and an observation results in a probabilistic transition to a successor node  Platzman        Meuleau  Peshkin  Kim    Kaelbling      b  Baxter   Bartlett        Poupart   Boutilier        Amato  Bernstein    Zilberstein          Bounded policy iteration  BPI  is an approach to solving infinite horizon POMDPs that represents policies as stochastic finite state controllers and iteratively improves a controller by adjusting the parameters of each node using linear programming  where the parameters specify the action selection and node transition probabilities of the node  Poupart   Boutilier         BPI is related to an exact policy iteration algorithm for POMDPs due to Hansen         but differs from it by providing an elegant and effective approach to approximation in which bounding the size of the controller allows a tradeoff between planning time and plan quality  Originally developed as an approach to solving single agent POMDPs  BPI has also been generalized for use in solving decentralized POMDPs  Bernstein  Hansen    Zilberstein         In BPI  the complexity of policy improvement depends on the size of the linear programs used to adjust the parameters of each node of the controller  In turn  this depends on the number of parameters of each node  as well as the size of the state space   In the original algorithm  each node of the controller has  A   A  Z  N   parameters  where  A  in the number of actions   Z  is the number of observations  and  N   is the number of nodes of the controller  This assumes a fully connected stochastic controller  In practice  however  most of these parameters have zero probabilities  and the number of parameters of a node with non zero probabilities remains relatively constant as the number of nodes of the controller increases  Based on this observation  we propose a modified version of BPI that leverages a sparse representation of a stochastic finite state controller  In each iteration  it improves the controller by the same amount as the original algorithm  But it does so by solving much smaller linear programs  where the number of variables in each linear program depends on the number of parameters of a node with non zero values  Because the number of parameters of a node with non zero values tends to remain relatively constant as the size of the controller grows  the complexity of each iteration of the modified version of BPI tends to grow only linearly with the number of nodes of a controller  which is a dramatic improvement in scalability compared to the original algorithm      Background We consider a discrete time infinite horizon POMDP with a finite set of states  S  a finite set of actions  A  and a finite set of observations  Z  Each time period  the environment is in some state s  S  the agent takes an action a  A  the environment makes a transition to state s   S with probability P  s   s  a   and the agent observes z  Z with probability P  z s    a   In addition  the agent receives an immediate reward with expected value R s  a      We assume the objective is to maximize expected total discounted reward  where          is the discount factor  Since the state of the environment cannot be directly observed  we let b denote an  S  dimensional vector of state probabilities  called a belief state  where b s  denotes the probability that the system is in state s  If action a is taken and followed by observation z  the successor belief state  denoted baz   is determined using Bayes rule      Policy representation and evaluation A policy for a POMDP can be represented by a finite state controller  FSC   A stochastic finite state controller is a tuple   N        where N is a finite set of nodes   is an action selection function that specifies the probability  n  a    P  a n   of selecting action a  A when the FSC is in node n  N   and  is a node transition function that specifies the probability  n  a  z  n      P  n   n  a  z   that the FSC will make a transition from node n  N to node n   N after taking action a  A and receiving z  Z  The value function of a policy represented by a FSC is piecewise linear and convex  and can be computed exactly by solving the following system of linear equations  with one equation for each pair of node n  N and state s  S  X Vn  s    n  a R s  a        aA    X  n  a  z  n   P  s   s  a P  z s    a Vn   s      a z s n   In this representation of the value function  there is one  S  dimensional vector Vn for each node n  N of the controller  The value of any belief state b is determined as follows  X V  b    max b s Vn  s       nN  sS  and the controller is assumed to start in the node that maximizes the value of the initial belief state  The value function of an optimal policy satisfies the optimality equation      X a P  z b  a V  bz         V  b    max R b  a     aA  zZ  P  where R b P a    sS b s R s  a  and P  z b  a    P   b s  P  s  s  a P  z s    a     sS s S  Variables     n  a   a  n  a  z  n     a  z  n  Objective  Maximize   Improvement P constraints  Vn  s       a n  a R s  a    P  a z n  n  a  z  n   P  s   s  a P  z s    a Vn   s     s Probability constraints  P  a      Pa   a  z  n       a   a  z   n n  a      a n  a  z  n        a  z  n  Table    Linear program for improving a node n of a stochastic finite state controller       Bounded policy iteration Policy iteration algorithms iteratively improve a policy by alternating between two steps  policy evaluation and policy improvement  Hansen        proposed a policy iteration algorithm for POMDPs that represents a policy as a deterministic finite state controller  In the policy improvement step  it uses the dynamic programming update for POMDPs to add  merge and prune nodes of the controller  The algorithm is guaranteed to converge to an   optimal controller and can detect convergence to an optimal policy  A potential problem is that the number of nodes added in the policy improvement step can be large  and the controller can grow substantially in size from one iteration to the next  Because the complexity of the policy improvement step increases with the size of the controller  allowing the size of the controller to grow too fast can slow the rate of further improvement and limit scalability  The same problem occurs in value iteration  where the number of vectors in the piecewise linear and convex representation of the value function can grow too fast from one iteration to the next  Feng and Hansen        describe a method for performing approximate dynamic programming updates that has the effect of reducing the number of vectors added to a value function in value iteration  or the number of nodes added to a controller in policy iteration  For policy iteration  Poupart and Boutilier        propose another approach that also has the benefit of avoiding an expensive dynamic programming update  Their bounded policy iteration algorithm controls growth in the size of the controller in two ways  First  a policy is represented as a stochastic finite state controller that can be improved without increasing its size  by adjusting the action and nodetransition probabilities of each node of the controller using linear programming  Second  when the controller cannot be improved further in this way  k nodes are added to the controller  where k is some small number greater than or equal to one  In the rest of this background section  we review each of these two steps in further detail    Algorithm   Bounded policy iteration repeat repeat  Policy evaluation  Solve the linear system given by Equation      Policy improvement without adding nodes  for each node n of the controller do solve the linear program in Table   if       then update parameters and value vector of node end if end for until no improvement of controller  Policy improvement by adding nodes  find belief states reachable from tangent points create k new nodes that improve their value until no new node is created  Improving nodes The first step attempts to improve a controller while keeping its size fixed  For each node n of the controller  the linear program in Table   is solved  The linear program searches for action probabilities and node transition probabilities for the node that improve the value vector Vn associated with the node by some amount   for each state  where   is the objective maximized by the linear program  If an improvement is found  the parameters of the node are updated accordingly   The value vector may also be updated  and will be further improved during the policy evaluation step   Poupart and Boutilier        show that the linear program can be interpreted as follows  it implicitly considers the vectors of the backed up value function that would be created by performing a dynamic programming update  In particular  the linear program searches for a convex combination of these backed up vectors that pointwise dominates the value vector currently associated with the node  If an improvement is found  the parameters of the convex combination become the new parameters of the node  This interpretation is illustrated in Figure    which is adapted from a similar figure from Pouparts dissertation         As Figure   shows  the new value vector is parallel to the old vector  i e   the value of each component is improved by same amount  and it is tangent to the backed up value function  Adding nodes Eventually  no node can be improved further by solving its corresponding linear program  At this point  BPI is at a local optimum  It can escape such a local optimum by adding one or more nodes to the controller  The key insight is that when a local optimum is reached  the value vector of each node of the controller is tangent to the backed up value function at one or more belief states  Moreover  the solution of the linear program that is the dual of the linear program in Table   is a belief state that is tan   Figure    BPI can improve the value vector Vn by an amount   to obtain the improved value vector Vn    which is tangent to the backed up value function   gent to the backed up value function  and so is called the tangent belief state   Since most linear program solvers return both the primal and dual solutions when they solve a linear program  we assume that we get the tangent belief state when we solve the linear program in Table    in addition to the value of   and the action and node transition probabilities   To escape a local optimum  it is necessary to improve the value of the tangent belief state  This leads to a method for adding nodes to a controller  Given a tangent belief state b  the algorithm considers every belief state b  that can be reached from it in one step  i e   by taking some action a followed by some observation z   For each reachable belief state  a backup is performed  as defined by Equation     If the backed up value is better than the value of the belief state based on the current value function  a deterministic node is added to the controller that has the same action  and  for each observation  the same successor node  that created the improved backed up value  Often  it is only necessary to add a single node to the controller to escape a local optimum  But because a value vector may be tangent to the backed up value function for a linear portion of belief space  it may be necessary to add more than one node to escape the local optimum  As we will see  this method for adding nodes to escape local optima is related to the approach that we develop in this paper  Two sources of complexity Most of the computation time of BPI is spent in solving the linear programs that are used to adjust the parameters that specify the action selection probabilities and node transition probabilities of each node of a controller  The size of the linear programs  and thus the complexity of BPI  depends on two things  the size of the controller  which determines the number of variables in the linear program  and the size of the state space  which determines the number of constraints     Test problem Slotted Aloha  S         A        Z       Tiger grid  S         A        Z        Hallway  S         A        Z        Hallway   S         A        Z        Statistic total parameters per node min non zero parameters avg non zero parameters max non zero parameters total parameters per node min non zero parameters avg non zero parameters max non zero parameters total parameters per node min non zero parameters avg non zero parameters max non zero parameters total parameters per node min non zero parameters avg non zero parameters max non zero parameters                                                                   Number of nodes of controller                                                                                                                                                                                                                                                                                                                                                                 Table    Total number of parameters per node of stochastic finite state controllers found by bounded policy iteration  and minimum  average  and maximum number of parameters with non zero values  as a function of the size of the controller   The average is rounded up to the nearest integer   The four test POMDPs are from  Cassandra         In this paper  we focus on the first source of complexity  that is  we focus on improving the complexity of BPI with respect to controller size   By controller size  we mean not only the number of nodes of the controller  but also the number of actions and observations  since these together determine the number of parameters of a node   Coping with POMDPs with large state spaces is an orthogonal research issue  and several approaches have been developed that can be combined with BPI  For example  Poupart and Boutilier        describe a technique called value directed compression and report that it allows BPI to solve POMDPs with millions of states  Because the test problems used in this paper have small state spaces  it is important to keep in mind that the techniques developed in the next section for improving the scalability of BPI with respect to controller size can be combined with techniques for coping with large state spaces     Sparse bounded policy iteration In this section  we describe a modified version of BPI that we call Sparse BPI  In each iteration  it improves a FSC by the same amount as the original algorithm  but with much improved scalability  To motivate our approach  we begin with a discussion of the sparse structure of stochastic finitestate controllers found by BPI      Sparse stochastic finite state controllers As we have seen  each iteration of BPI solves  N   linear programs  and each linear program has  A     A  Z  N    variables and  S     A  Z  constraints  in addition to the constraints that the variables have non negative values   Even for small FSCs  the number of variables in the linear programs can be very large  and the fact that the number of variables grows with the number of nodes in the controller significantly limits the scalability of BPI  If we look at the controllers produced by BPI  however  we find that most of the parameters of each node  i e   most of the variables in the solutions of the linear program  have values of zero  Table   illustrates this vividly for four benchmark POMDPs from  Cassandra         The overwhelming majority of each nodes parameters have zero probabilities  This is despite the fact that all of the nodes of these controllers are stochastic  Note that a deterministic node has      Z  non zero probabilities  one for a choice of action  and  Z  to specify the successor node for each observation  Table   shows that the minimum number of non zero parameters for any node is always greater than this  which indicates that all of the nodes are stochastic  For these problems and many others  BPI is very effective in leveraging the possibility of stochastic nodes to improve a controller without increasing its size  Nevertheless  the actual number of parameters with non zero probabilities is a very small fraction of the total number of parameters  Besides the sparsity of the stochastic finite state controllers found by BPI  an equally important observation about the results in Table   is that the number of parameters with non zero probabilities tends to remain the same as the controller grows in size  whereas the total number of parameters grows significantly  In the following  we develop a modified version of BPI that exploits this sparse structure    Figure    In the left panel  the value vector is tangent to the partial backed up value function along a linear segment  The center panel shows the result of adding parameters to the sparse linear program that improve the partial backed up value function at tangent belief state t    Although this does not improve the value vector  it creates a new tangent belief state t    When parameters are added to the sparse linear program that improve its backed up value  the result is an improved value vector with a new tangent belief state t    as shown in the right panel      Sparse policy improvement algorithm We begin by describing the main idea of the algorithm  We have seen that the number of parameters of a node with non zero probabilities in the solution of the linear program in Table   is very small relative to the total number of parameters  Let us call these the useful parameters  If we could somehow identify the useful parameters of a node  we could improve a node by solving a linear program that only includes these variables  We call a linear program that only includes some of the parameters of a node a reduced linear program  Our approach will be to solve a series of reduced linear programs  where the last one is guaranteed to include all of the useful parameters of the node  This approach will achieve the same result as solving the full linear program of Table    but will be more efficient if the reduced linear programs are much smaller  We next describe an iterative method for identifying a small set of parameters that includes all of the useful parameters  Our method starts with the set of parameters of the node that currently have non zero probabilities  This guarantees a solution that is at least as good as the current policy for the node  Then we add parameters using a technique that is inspired by the technique BPI uses to add nodes to a controller in order to escape local optima  As shown in Figure    the value vector that is created by solving the linear program in Table   is tangent to the backed up value function  and the solution of the dual linear program is a tangent belief state  In the case of a reduced linear program  however  we have a partial backed up value function that does not include vectors corresponding to parameters that are not included in the reduced linear program   For example  if only one action parameter is included in the reduced linear program  the partial backed up value function does not include any vectors created by taking a different action    If the reduced linear program is missing some useful parameters  it must be possible to improve the value of the tangent belief state b by considering the vectors of the full backed up value function  So  to identify potentially useful parameters  we perform a backup for the tangent belief state b  If the backed up value  which is defined as   max R b  a     aA  X zZ  A P  z b  a  max nN  X     baz  s Vn  s   sS  is greater than the value of the tangent belief state based on the current value vector Vn    which is defined as P   sS b s Vn  s   we add to our set of parameters the parameters used to create the backed up value  the best action a  and  for each observation z  the best successor node n  Then we solve another linear program that includes these parameters in addition to the parameters contained in the previous reduced linear program  If the value vector is tangent to the partial backed up value function at a single belief state  adding the parameters that improve the backed up value of this tangent belief state improves the value vector for this node  But since the value vector may be tangent to the partial backed up value function along a linear segment  adding parameters does not guarantee improvement of the node  In this case  however  it does change the result of the new reduced linear program in an important way  Because adding these parameters to the reduced linear program improves the partial backed up value function that is searched by the linear program  the value vector is no longer tangent to the partial backed up value function at the same belief state  In other words  even if the solution of the reduced linear program is not changed by adding these parameters  the solution of its dual linear program changes in one important way  there must be a new tangent belief state  This is illustrated in Figure         This points to an iterative algorithm for improving a node  At each step  the algorithm solves a reduced linear program  and then performs a backup for the tangent belief state  If the backed up value of the tangent belief state is better than its value based on the current value vector  the parameters that produced the improved backed up value are added to the reduced linear program  Since the backedup value of the tangent belief state is improved  it must be the case that at least some of these parameters are not in the current linear program  if they were  the current value vector would not be tangent to the partial backed up value function at this belief state  The condition for terminating this iterative process of adding parameters to a reduced linear program is based on the following lemma  Lemma   The difference between the backed up value of a tangent belief state and its value based on the current value vector bounds the amount by which the linear program in Table   can improve the value vector of a node  Proof  The linear program in Table   implicitly uses the backed up value function to improve the value vector associated with a node by an amount   for each state value of the vector  in turn  this improves by   the value of any belief state based on the value vector  So  if the difference between the backed up value of any belief state and its value based on the current value vector is   then      It follows that if the backed up value of any belief state is not an improvement of its value based on the current value vector  the linear program cannot improve the value vector  Based on this lemma  we can prove the following  Theorem   This procedure of improving a stochastic node by solving a sequence of reduced linear programs is guaranteed to terminate and the last reduced linear program in the sequence produces the same result as solving the full linear program in Table    Proof  The procedure is guaranteed to terminate because whenever the backed up value of the tangent belief state is greater than its value based on the value vector created by solving the current reduced linear program  at least one  and no more than       Z   parameters will be added to the linear program  and the total number of parameters is finite  To see that the last reduced linear program solved in this sequence has the same result as solving the full linear program in Table    note that the procedure terminates when the difference between the backed up value of the tangent belief state  for the last reduced linear program  and the value of the tangent belief state based on the best value vector found so far for this node is zero  From Lemma    it follows that no further improvement is possible  We call this iterative approach to improving the nodes of a stochastic FSC sparse policy improvement  The high level  Algorithm   Sparse policy improvement for each node of controller do Create initial reduced linear program  its variables correspond to the parameters of the node that currently have non zero probabilities  threshold    repeat Solve the reduced linear program if     threshold then Use solution of LP to update parameters of node threshold    end if Do backup for tangent belief state if backed up value    current value      then Add variables to linear program that correspond to parameters that produced backed up value end if until no new variables are added to linear program increase value vector of node by   end for  pseudocode is shown in Algorithm    Although it can solve multiple linear programs for each linear program solved by the original BPI algorithm  it has an advantage if the number of variables in each of these reduced linear programs is very small compared to the number of variables in the full linear program of Table    This is the case whenever the FSCs found by BPI are very sparse  As mentioned before  this algorithm is inspired by the way the original BPI algorithm adds nodes to a controller  In both cases  the technique for breaking a local optimum at a tangent belief state is to improve the backed up value function of the tangent belief state  In the original BPI algorithm  the value of the tangent belief state is improved by adding nodes to the controller  In sparse policy improvement  it is improved by adding parameters to a node  There is also an interesting relationship between this algorithm and column generation methods in linear programming  Bertsekas   Tsitsiklis         Column generation is a useful strategy for solving linear programs where the number of variables is very large  the number of constraints is relatively small  and most variables have a value of zero in the optimal solution  It begins by considering a small subset of the variables  i e   the columns  of a problem  and solves a reduced linear program with only these variables  Then it identifies unused variables that can be added to the linear program to improve the result  Use of this strategy requires some way of determining if the current solution is optimal  and if it is not  some way of generating one or more unused variables that can improve the solution  Our algorithm can be viewed as an application of this general strategy to the problem of solving POMDPs using BPI    Test problem Slotted Aloha  S         A        Z      Tiger grid  S         A        Z       Hallway  S         A        Z       Hallway   S         A        Z        Algorithm BPI Sparse BPI BPI Sparse BPI BPI Sparse BPI BPI Sparse BPI                                               Number of nodes of controller                                                                                                                                                                                                                                                Table    Average time  in CPU milliseconds  for improving a single node of a finite state controller  as a function of the size of the controller  for four benchmark POMDPs     Experimental results We implemented sparse bounded policy iteration and tested it on several benchmark POMDPs  Table   shows results for the four POMDPs considered earlier in Table    The experiments ran on a     GHz processor using a linear program solver in CPLEX version      We solved each POMDP beginning with an initial controller with  A  nodes  and adding up to five new nodes each time the policy improvement step reached a local optimum  Table   reports the average running time to improve a single node because this dominates overall computation time  and also because it measures performance independently of design decisions such as how and when to add nodes to a controller  Table   shows that Sparse BPI is much more efficient than the original BPI algorithm in improving sparse stochastic FSCs  Even for relatively small controllers of     nodes  Sparse BPI runs between    and    times faster than BPI in solving these particular POMDPs  More importantly  its relative advantage grows with the size of the controller  The size of the reduced linear programs solved by Sparse BPI remains about the same as the controller grows in size  as a result  the time it takes to improve a single node of the controller remains relatively constant as the size of the controller grows  in contrast to BPI  An interesting difference between Sparse BPI and BPI is that the running time of an iteration of Sparse BPI depends on how much improvement of the controller is possible  If the nodes of a controller are already at a local optimum  Sparse BPI often needs to solve only a couple reduced linear programs per node in order to determine that further improvement is not possible  In this case  an iteration of Sparse BPI terminates relatively quickly  But if much improvement of the FSC is possible  Sparse BPI often needs to solve ten or twenty or even more reduced linear programs for some nodes in order to add all of the parameters that are needed to maximize improvement of the node  To obtain reliable running times for Sparse BPI  the results in Table   are averaged over several iterations of Sparse BPI   Table   shows that a larger state space slows both algorithms  but it slows the sparse algorithm more  This is because the sparse algorithm solves multiple linear programs for each linear program solved by the original algorithm  and more constraints makes these more difficult to solve  The running time of Sparse BPI could be reduced further by finding a way to reduce the number of iterations it takes to improve a node  For the hallway  problem  for example  the sparse algorithm can take ten or more iterations to improve a node  occasionally  as many as thirty or forty  Each iteration requires solving a reduced linear program  But in fact  it is only necessary to solve one linear program to change the parameters of a node  The other linear programs in the sequence of reduced linear programs solved by the sparse algorithm are used to identify a sequence of tangent belief states  for each tangent belief state  a backup is performed in order to identify parameters to add to the stochastic node  It seems possible to identify a set of belief states for which to perform backups without needing to solve a linear program to identify each one  Point based methods solve POMDPs by performing a backup for each of a finite set of belief states  Pineau  Gordon    Thrun         Recent work shows that this approach can be combined with policy iteration algorithms that represent a policy as a finite state controller  Ji  Parr  Li  Liao    Carin         An interesting direction of research is to use point based backups to identify parameters that can be added to nodes  after many backups are performed  a reduced linear program could be solved for each node in order to improve its action selection and node transition probabilities  integrating point based methods with policy iteration for stochastic finite state controllers  Another possible way to reduce the number of iterations is early termination  Instead of continuing to add parameters until the difference between the backed up value of the tangent belief state and its value based on the current value vector is zero  Sparse BPI could stop adding parameters as soon as the difference is small enough to demonstrate that only a small amount of further improvement is possible      Conclusion We have presented a modified bounded policy iteration algorithm for POMDPs called sparse bounded policy iteration  The new algorithm exploits the sparse structure of stochastic finite state controllers found by bounded policy iteration  Each iteration of the algorithm produces the identical improvement of a controller that an iteration of the original bounded policy iteration algorithm produces  but with improved scalability  Whereas the time it takes for the original algorithm to improve a single node grows with the size of the controller  the time it takes for the new algorithm to improve a single node is typically independent of the size of the controller  This makes it practical to use bounded policy iteration to find larger controllers for POMDPs  Acknowledgements This work was supported in part by the Air Force Office of Scientific Research under Award No           A     
