 We consider the problem belief state monitoring for the purposes of implementing a policy for a partially observable Markov decision process  POMDP   specifically how one might approxi mate the belief state  Other schemes for belief state approximation  e g   based on minimizing a measure such as KL divergence between the true and estimated state  are not necessarily appropri ate for POMDPs  Instead we propose a frame work for analyzing value directed approximation schemes  where approximation quality is deter mined by the expected error in utility rather than by the error in the belief state itself  We propose heuristic methods for finding good projection schemes for belief state estimation exhibiting anytime characteristics given a POMDP value function  We also describe several algorithms for constructing bounds on the error in decision qual ity  expected utility  associated with acting in ac cordance with a given belief state approximation      Introduction  Considerable attention has been devoted to partially observable Markov decision processes  POMDPs           as a model for decision theoretic planning  Their general ity allows one to seamlessly model sensor and action uncer tainty  uncertainty in the state of knowledge  and multiple objectives           Despite their attractiveness as a concep tual model  POMDPs are intractable and have found prac tical applicability in only limited special cases  Much research in AI has been directed at exploiting cer tain types of problem structure to enable value functions for POMDPs to be computed more effectively  These primar ily consist of methods that use the basic  explicit state based representation of planning problems      There has  how ever  been work on the use of factored representations that resemble classical AI representations  and algorithms for solving POMDPs that exploit this structure         Repre sentations such as dynamic Bayes nets  DBNs      are used to represent actions and structured representations of value functions are produced  Such models are important because  they allow one to deal  potentially  with problems involving a large number of states  exponential in the number of vari ables  without explicitly manipulating states  instead rea soning directly with the factored representation  Unfortunately  such representations do not automatically translate into effective policy implementation  given a POMDP value function  one must still maintain a belief state  or distribution over system states  online in order to implement the policy implicit in the value function  Belief state maintenance  in the worst case  has complexity equal to the size of the state space  exponential in the number of variables   as well  This is typically the case even when the system dynamics can be represented compactly using a DBN  as demonstrated convincingly by Boyen and Koller      Because of this  Boyen and Koller develop an approx imation scheme for monitoring dynamical systems  as op posed to POMDP policy implementation   intuitively  they show that one can decompose a process along lines sug gested by the DBN representation and maintain bounded er ror in the estimated belief state  Specifically  they approx imate the belief state by projection  breaking the joint dis tribution into smaller pieces by marginalization over sub sets of variables  effectively discounting certain dependen cies among variables  In this paper  we consider approximate belief state moni toring for POMDPs  We assume that a POMDP has been solved and that a value function has been provided to us in a factored form  as we explain below   Our goal is to de termine a projection scheme  or decomposition  so that ap proximating the belief state using this scheme hinders the ability to implement the optimal policy as little as possible  Our scheme will be quite different from Boyen and Koller s since our aim is not to keep the approximate belief state as  close  to the true belief state as possible  as measured by KL divergence   Rather we want to ensure that decision quality is sacrificed as little as possible  In many circumstances  this means that small correlations need to be accounted for  while large correlations can be ignored completely  As an example  one might imagine a process in which two parts are stamped from the same ma chine  If the machine has a certain fault  both parts have a high probability of being faulty  Yet if the decisions for subsequent processing of the parts are independent  the fact   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            that the fault probabilities for the parts are dependent is ir relevant  We can thus project our belief state into two inde pendent subprocesses with no loss in decision quality  As suming the faults are independent causes a large  error  in the belief state  but this has no impact on subsequent deci sions or even expected utility assessment  Thus we need not concern ourselves with this  error   In contrast  very small dependencies  when marginalized  may lead to very small  error  in the belief state  yet this small error can have se vere consequences on decision quality  Because of this  while Boyen and Koller s notion of pro jection offers a very useful tool for belief state approxima tion  the model and analysis they provide cannot be applied usefully to POMDPs  For example  in        this model is integrated with a  sampling based  search tree approach to solving POMDPs  Because the error in decision quality is determined as a function of the worst case decision quality with respect to actual belief state approximation error  the bounds are unlikely to be useful in practice  We strongly believe estimates of decision quality error should be based on direct information about the value function  In this paper we provide a theoretical framework for the analysis of value directed belief state approximation  VDA  in POMDPs  The framework provides a novel view of ap proximation and the errors it induces in decision quality  We use the value function itself to determine which cor relations can be  safely  ignored when monitoring one s belief state  Our framework offers methods for bounding  reasonably tightly  the error associated with a given pro jection scheme  W hile these methods are computationally intensive requiring in the worst case a quadratic increase in the solution time of a POMDP we argue that this of fline effort is worthwhile to enable fast online implemen tation of a policy with bounded loss in decision quality  We also suggest a heuristic method for choosing good pro jection schemes given the value function associated with a POMDP  Finally  we discuss how our techniques can also be applied to approximation methods other than projection  e g   aggregation using density trees                  POMDPs and Belief State Monitoring Solving POMDPs  A partially observable Markov decision process  POMDP  is a general model for decision making under uncertainty  Formally  we require the following components  a finite state spaceS  a finite action space A  a finite observation space Z  a transition function T   S x A     S   an observation function     S x A     Z   and a reward function R   S    R   Intuitively  the transition function T s  a  determines a distribution over next states when an agent takes action a in states we write Pr s  a  t  to de note the probability that state t is reached  This captures un certainty in action effects  The observation function reflects the fact that an agent cannot generally determine the true system state with certainty  e g   due to sensor noise  we write Pr s  a  z  to denote the probability that observation z      X  denotes the set of distributions over finite set X     Optimal Value Function  Belief Space  Figure I  Geometric View of Value Function is made at state s when action a is performed  Finally R s  denotes the immediate reward associated with s   The rewards obtained over time by an agent adopting a spe cific course of action can be viewed as random variables R t    Our aim is to construct apolicythat maximizes the ex pected sum of discounted rewards E CLo   R  t    where  Y is a discount factor less than one   It is well known that an optimal course of action can be determined by consid ering the fully observable belief state MDP  where belief states  distributions overS  form states  and a policy rr    S     A maps belief states into action choices  In prin ciple  dynamic programming algorithms for MDPs can be used to solve this problem  but a practical difficulty emerges when one considers that the belief space  S  is an S I I  dimensional continuous space  A key result of Sondik   I   showed that the value function V for a finite horizon prob lem is piecewise linear and convex and can be represented as a finite collection of a vectors   Specifically  one can generate a collection N of a vectors  each of dimension S l I  such that V b    ma xaEN ba  Figure I illustrates a collec tion of a vectors with the upper surface corresponding to V  Furthermore  each a E N has a specific action associ ated with it  so given belief state b  the agent should choose the action associated with the maximizing a vector  Insight into the nature of POMDP value functions  which will prove critical in the methods we consider in the next section  can be gained by examining Monahan s      method for solving POMDPs  Monahan s algorithm pro ceeds by producing a sequence of k stage to go value func tions Vk  each represented by a set of a vectors Nk  Each a E Nk denotes the value  as a function of the belief state  of executing a k step conditional plan  More precisely  let the k step observation strategies be the set oS  of map pings u   Z    Nk l  Then each a vector in Nk corre sponds to the value of executing some action a followed by implementing some u E OSk  that is  it is the value of do ing a  and executing the k    step plan associated with the a vector u z  if z is observed  Using CP a  to denote this plan  we have that CP a     a  ifz   CP u z    v z    We informally write this as  a  u   We write a   a  u   to de note the a vector reflecting the value of this plan  Given Nk  Nk l is produced in two phases  First  the set of vectors corresponding to all action observation policies   Action  costs are ignored to keep the presentation simple   For infinite horizon problems  a finite collection may not be sufficient        but will generally offer a good approximation    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       is constructed  i e   for each a E A and  J  E osk l  the vector a denoting the value of plan  a  CP  J  z       is added to k l    Second  this set is pruned by removing all domi nated vectors  This means that those vectors a such that ba is not maximal for any belief state b are removed from k      In Figure     a  is dominated  playing no useful role in the representation of V  and can be pruned  Pruning is imple mented by a series of linear programs  Refinements of this approach are possible that eliminate  or reduce  the need for pruning by directly identifying only a vectors that are non dominated             Other algorithms  such as incremen tal pruning      are similar in spirit to Monahan s approach  but cleverly avoid enumerating all observation policies  A finite k stage POMDP can be solved optimally this way and a finite representation of its value function is assured  For infinite horizon problems  a k stage solution can be used to approximate the true value function  error bounds can eas ily be derived based on the differences between successive value functions   One difficulty with these classical approaches is the fact that the a vectors may be difficult to manipulate  A sys tem characterized by n random variables has a state space size that is exponential in n  Thus manipulating a single a vector may be intractable for complex systems   Fortu nately  it is often the case that an MDP or POMDP can be specified very compactly by exploiting structure  such as conditional independence among variables  in the system dynamics and reward function        Representations such as dynamic Bayes nets  DBNs       can be used to great effect  and schemes have been proposed whereby the a vectors are computed directly in a factored form by exploiting this rep resentation  Boutilier and Poole      for example  represent a vectors as decision trees in implementing Monahan s algorithm  Hansen and Feng     use algebraic decision diagrams  ADDs  as their representation in their version of incre mental pruning   The empirical results in     suggest that such methods can make reasonably large problems solv able  Furthermore  factored representations will likely fa cilitate good approximation schemes  There is no reason in principle that the other algorithms mentioned cannot be adapted to factored representations as well        Belief State Monitoring  Even if the value function can be constructed in a compact way  the implementation of the optimal policy requires that the agent maintains its belief state over time  The monitor ing problem itself is not generally tractable  since each be lief state is a vector of size jSj  Given a compact represen tation of system dynamics and sensors in the form of DBN  one might expect that monitoring may become tractable us ing standard belief net inference schemes  Unfortunately  this is generally not the case  Though variables may be ini  The number of a vectors can grow exponentially in the worst case  as well  but for many problems the number remains manage able  and approximation schemes that simply bound their number have been proposed        ADDs  commonly used in verification  have been applied very effectively to the solution of fully observable MDPs            tially independent  thus admitting a compact representation of a distribution   and though at each time step only a small number of variables become correlated  over time these cor relations  bleed through  the DBN  rendering most  if not all  variables dependent after a time  Thus compact repre sentation of belief state is typically impossible  Boyen and Koller     have devised a clever approximation scheme for alleviating the computational burden of moni toring  In this work  no POMDP is used  but rather a sta tionary process  represented in a factored manner  e g   us ing a DBN   is assumed  This might  for example  be the process induced by adopting a fixed policy  Intuitively  they consider projection schemes whereby the joint distribution is approximated by projecting it onto a set of subsets of vari ables  It is assumed that these subsets partition the variable set  For each subset  its marginal is computed  the approx imate belief state is formed by assuming the subsets are in dependent  Thus only variables within the same subset can remain correlated in the approximate belief state  For in stance  if there are   variables A  B  C and D  the projection scheme   AB  CD  will compute the marginal distributions for AB and CD  The resulting approximate belief state  P ABCD    P AB P CD   has a compact  factored representation given by the distribution of each marginal  Formally  we say a projection scheme S is a set of subsets of the set of state variables such that each state variable is in some subset  This allows marginals with overlapping sub sets of variables  e g    ABC  BCD    We view strict par titioning as a special type of projection  Some schemes with overlapping subsets may not be computationally useful in practice because it may not be possible to easily generate a joint distribution from them by building a clique tree  We therefore classify as practical those projection schemes for which a joint distribution is easily obtained  Assuming that belief state monitoring is performed using the DBN repre senting the system dynamics  see             for details on in ference with DBNs   we obtain belief state bt l from bt us ing the following steps   a  construct a clique tree encod ing the variable dependencies of the system dynamics  for a specific action and observation  and the correlations that have been preserved by the marginals representing bt   b  initialize the clique tree with the transition probabilities  the observation probabilities and the  approximate  factored  joint distribution bt   c  query the tree to obtain the distribu tion b at the next time step  and  d  project b according to some practical projection scheme S to obtain the collec tion of marginals representing bt l   S b    The com plexity of belief state updating is now exponential only in the size of the largest clique rather than the total number of variables  Boyen and Koller show how to compute a bound on the KL divergence of the true and approximate belief states  exploiting the contraction properties of Markov processes  under certain assumptions   But direct translation of these bounds into decision quality error for POMDPs generally yields weak bounds          Furthermore  the suggestions made by Boyen and Koller for choosing good projection schemes are designed to minimize KL divergence  not to minimize error in expected value for a POMDP  For this rea    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            S b   li                    Figure    Relevant belief states at stagek  Figure    The Switch Set Swk a   of a   son we are interested in new methods for choosing projec tions that are directly influenced by considerations of value and decision quality  Other belief state approximation schemes can be used for belief state monitoring  For example  aggregation using density trees can provide a means of representing a belief state with many fewer parameters than the full joint  Our model can be applied to such schemes as well   with b  b and S b  are a  a  and aa respectively  see Fig ure     The approximation at stagek mistakenly induces the choice of the action associated with a  instead of a  at b  this incurs an error in decision quality of b  a    b  aa  While the optimal choice is in fact a   the unaccounted er ror b  a    b  a  induced by the prior approximations will be viewed as caused by the earlier approximations  our goal at this point is simply to consider the error induced by the current approximation  In order to derive an error bound we must identify for each a E k the set of vectors Swk a  that the agent can switch to by approximating its current belief state b given that b identifies a as optimal  Formally we define     Error Bounds on Approximation Schemes  In this section  we assume that a POMDP has been solved and that its value function has been provided to us  We also assume that some structured technique has been used so that a vectors representing the value function are struc tured         We begin by assuming that we have been given an approximation scheme S for belief state monitoring in a POMDP and derive error bounds associated with acting according to that approximation scheme  We focus primar ily on projection  but we will mention how other types of approximation can be fit into our model  We present two techniques for bounding the error for a given approximation scheme and show that the complexity of these algorithms is similar to that of solving the POMDP  with a  multiplica tive  overhead factor of         Plan Switching  Implementing the policy for an infinite horizonPOMDP re quires that one maintains a belief state  plugging this into the value function at each step and executing the action as sociated with the maximizing a vector  When the belief state b is approximated using an approximation scheme S  a suboptimal policy may be implemented since the maxi mizing vector for S b  will be chosen rather than the max imizing vector for b  Furthermore this mistaken choice of vectors  hence actions  can be compounded with each fur ther approximation at later stages of the process  To bound such error  we first define the notion of plan switching  We phrase our definitions in terms of finite horizon value func tions introducing the minor variations needed for infinite horizon problems later  Suppose withk stages to go  the true belief state  had we monitored accurately to that point  is b  However  due to previous belief state approximations we take our current be lief state to be b  Now imagine our approximation scheme has been aplied at timek to obtain S b   Given k  rep resenting V   suppose the maximizing vectors associated  Swk a     a  E k   b v a ba    ba S b a     S b a   Intuitively this is the set of vectors we could choose as max imizing  thus implementing the corresponding conditional plan  due to belief state approximation  In Figure    we see that Swk aa     a   a   a    The set Swk a   can be identified readily by solving a series of O lk I  optimiza tion problems each testing the possibility of switching to a specific vector aj E k  formulated as the following  pos sibly nonlinear  program  max  s  t   d b    a    at     d  v l    j   i S b    ai  at     d  v l    j   j Ls b s       v s b s        The solution to this program has a positive objective func tion value whenever there is a belief state b such that a  is optimal at b  and aj is optimal at S b   Note  in fact  that we need only find a positive feasible solution not an opti mal one  to identify aj as an element of Swk a    There are I k I switch sets to construct so    I k    optimization problems need to be solved to determine all switch sets  For linear approximation schemes  i e   those in which the constraints on S b  are linear in the variables b     these problems are easily solvable linear programs  LPs   We re turn to linear schemes in Section    Unfortunately projec tion schemes are nonlinear  making optimization  or iden tification of feasible solutions  more difficult  On the other hand  a projection scheme determines a set of linear con straints on the approximate belief state S b   For instance  consider the projection scheme S  CD DE  for        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       a POMDP with   binary variables  This projection im poses one linear constraint on S b  for each subset of the marginals in the projection    b C  b      b E  b  D  b  CD  b DE   b    b D  b CD   b  C  b  E  b  DE   Here b  denotes S b  and b XY  denotes the cumulative probability  according to belief state b  of all states where X and Y are true  These constraints define an LP that can be used to construct a superset Swk a   of Swk a    Given scheme S    M          Mn   we define the following LP  max d  s t   b  a    at      d b   aj  at      d b  M    b M  Ls b s      Vs b s        Vs b   s          When a feasible positive solution exists aj is added to the set Swk a    though in fact  it may not properly be a member of Swk a    If no positive solution exists we know a j is not in Swk a   and it is not added to Swk a    This superset of the switch set can be used to derive an upper bound on error  While the number of constraints of the type b M    b   M  is exponential in the size of the largest marginal we expect that the number of variables in each marginal for a useful projection scheme will be bounded by a small constant  In this way  the number of constraints can be viewed as con stant  i e   independent of state space size   Though the above LPs  for both linear approximations and projection schemes  look complex  they are in fact very similar in size to the LPs used for dominance testing in Monahan s pruning algorithm and the Witness algorithm  involving O ISI  variables and O INkI  constraints  The number of LP variables is exponential in the number of state variables  however the factored representation of a vectors allows LPs to be structured in such a way that the state space need not be enumerated  i e  the variables represent ing the state probabilities can be clustered   Precisely the same structuring is suggested in     and implemented in      Thus solving an LP to test if the agent can switch from a  to aj has the same complexity as a dominance test in the prun   ing phase ofPOMDP solving  However there are O INk     pairs of a vectors to test for plan switching whereas the pruning phase may require as few as INkI dominance tests if no vector is pruned  Hence  in the worst case  switch set generation may increase the running time for solving the POMDP by a factor of   INkI  at each stagek  For ak stage  finite horizon POMDP  we can now bound the error in decision quality due to approximationS  Define the bound on the maximum error introduced at each stage j    These equations can be generalized for POMDPs with non binary variables  though giving more than one equation per subset   when a is viewed as optimal as     B  a    max b  max b  a   a    J a ESW  a   Since error at a belief state is simply the expectation of the error at its component states  B  a  can be determined by comparing the vectors in S Vj a  with a component  wise  with the maximum difference being B  a    L et B    maxaENi B  a  be the greatest error introduced by a single approximation S at stage j   Then the total er ror fork successive approximations is bounded by ug             i B   For an infinite horizon POMDP  assume we have been given the infinite horizon value function N   i e    no stages are involved   Then we only need to compute the switch sets Sw  a  for this single N set  and the max imum one shot switching error B S  The upper bound on the loss incurred by applying s indefinitely is simply u    B S           Computing the error u  is roughly equivalent to performing    IN  I  dynamic programming backups on N   The LP formulation used to construct switch sets is com putationally intensive  Other methods can be used how ever to construct these switch sets  We have  for example  implemented a scheme whereby belief states are treated as vectors in        and projection schemes are viewed as dis placing these vectors  The displacement vectors  vectors which when added to a belief state b giveS b   induced by a scheme S can be computed easily and can be used to deter mine the direction in which belief state approximation shifts the true belief state  This is tum can be used to construct overestimates of switch sets  While giving rise to looser er ror bounds  this method is much more efficient in practice  Our emphasis  however  is on the analysis of error due to approximation  so we do not dwell on this scheme in this paper  see       for details        Alternative Plans  The cumulative error induced by switching plans at cur rent and future stages can be bounded in a tighter way  The idea is to generate the set of alternative plans that may be executed as a result of both current and future approxima tions  Suppose that an agent due to approximation at stage k changes its belief state from b to S b   This can induce a change in the choice of optimal a vector in Nk say from a  to a   However even though the agent has switched and chosen the first action associated with a   it has not nec essarily committed to implementing the entire conditional plan CP  a   associated with a   This is because further ap proximation at stagek     may cause it to switch from the continuation of CP  a    Suppose for instance that CP a      a       where  j   z     a  E Nk    If z is observed and the agent updates its  ap proximate  belief state S b  accurately to obtain S b   then  We use  S   j instead of Sw  to emphasize the fact that we use the approximate switch set generated for a projection scheme  however  all definitions apply equally well to exact switch sets if they are available    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            the maximizing vector at the next stage is necessarily a   But given that S b   will be approximated before the max imizing vector is chosen  the agent may adopt some other continuation of the plan if a  does not maximize value for the  second  approximated belief state S S b     In fact  the agent may implement CP a   at stage k     for any a  ESwk      a    Notice that the value of the plan actually implemented doing the first action of a   followed by the first action of a   and so on may not be represented by any a vector in k  We can actually construct the values of such plans  and thus obtain much tighter error bounds  while we perform dy namic programming  We recursively define the set of al ternative sets  orA t sets for each vector at each stage   We first define That is  if a is optimal at stage     then any vector in its switch set can have its plan executed  The future alterna tive set for any a E k  where CP a     a  a    is   FA t  a       a   a  o         v z  o    z  EAlt    o   z     If a is in fact chosen to be executed at stagek  true expected value may in fact be given by any vector in FAit  a   this is due to future switching of policies at stages followingk  Finally  define  Alt  a      U FA t a    a  E Swk  a    If a is in fact optimal at stagek for a given belief state b  but b is approximated currently and at every future stage  then expected value might be reflected by any vector in Alt  a   These vectors correspond to every possible course of ac tion that could be adopted because of approximation  if we switch vectors at stage k  we could begin to execute  the plan associated with  any a  ESwk  a   and if we begin ex ecuting a   we could end up executing  the plan associated with  any a  E FAit  a    Given these Alt sets  the error associated with belief state approximation can be given by the maximum difference in value between any a and one of itsA t vectors  These FAit and A t sets can be computed by dynamic programming while a POMDP is being solved  The complexity of this al gorithm is virtually identical to that of generating k from k    with the proviso that there are lkl Alt sets  How ever  these sets grow exponentially much like the sets k would if left unpruned  However  these sets can be pruned in exactly the same way as  sets  with the exception that since we want to produce a worst case bound on error  we want to construct a lower surface for theAlt sets rather than an upper surface  Given any Alt set  we denote by Alt the collection of vectors that are anti dominating in Alt  For example  if the collec tion of vectors in Figure    form the set Alt  a   then the vectors a  and a   making up the lower surface of this set     This definition can be more concisely specified  but this for mat makes the computational implications clear    lower surface of   a  az a      Figure    L ower surface   k  k formAlt  a   FAit  a  is defined similarly  The set of antidominated vectors can be pruned in exactly the same way that dominated a vectors are pruned from a value function  The same structuring techniques can be used to prevent ex cit state enumeration as well  This pruning can keep the AIt sets very manageable in size  Assuming we have an approximation Altk a  of Alt  a  for every a E k  we con  k   struct Alt  a  as follows   a  swk    a  is constructed  k l for each a E k     b  FAit   a   is constructed using  k Alt  a     and is then pruned to retain only anti dominating  k l vectors  and  c  Alt  a  is defined as the union of the  k l FAit  a   sets for those a  E Swk    a   and is then pruned  The following quantity bounds the error associated with ap proximating belief state using scheme S over the course of a k stage POMDP  when a represents optimal expected value for the initial belief state   k Ek   a    maxmax b  a   aI    a  E Alt  a   b  This error can be computed using simple pointwise com parison of a with each such a   It can also be restricted to that region of belief space where a is optimal  maximizing the difference only over belief states in that area to obtain a tighter bound  Approximation error can be bounded glob ally using  E     max E a   a E k   Furthermore  E      U since alternate vectors provide a much tighter way to measure cumulative error  For an infinite horizon problem  we can compute switch sets once as in the computation of Us  To compute a tighter bound E    we can constructk stages of Azt sets  backing up from   The bound E is computed as above  and we set  E       E    Us  In this way  we can obtain fairly tight bounds on the error induced by belief state approximation     Value Directed Approximations  The bounds Bk   a   and Ek described above can be used in several ways to determine a good projection scheme  In or der to compute error bounds to guide our search for a good   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Figure    L attice of Projection Schemes projection scheme  our  generic algorithm  will have to de termine the error associated with a different projectionS ap plied to each a vector  Because of this  we will consider the use of dijferent projection schemes So  for each a vector  at each stage if we have a finite horizon problem   D espite he fact that we previously derived bounds on error assurrung a uniform projection scheme  our algorithms work equally well  i e   provide legitimate bounds  if different projections are used with each vector  The projection So  adopted for vector a simply influences its switch set  Since the agent knows which vector it is  implementing  at any point in time we can record and easily apply the projection scheme So  fr that vector  This allows the agent to tailor its belief state approximation to provide good results for its currently anticipated course of action  This in tum will lead to much better performance than using a uniform scheme       Lattice of Projection Schemes  We can structure the search for a projection scheme by con sidering the lattice of projection schemes dfined by ub set inclusion  Specifically  we say St contams S   wntten loosely S   St  if every subset of S  is contaied ithi     some subset of S   This means that S  ts a finer parttt  n than S   The lattice of projections for three binary variables is illustrated in Figure    Each node represents the set of marginals defining some projection S  Above each node  the subsets corresponding to its constraining equations are listed  we refer to each such subset as a constraint   The finest projections  which are the  most approximate  sice they assume more independence  are at the top of the latttce  Edges are labeled with the subset of variables correspond ing to the single constraining equation tht ust be a ded to the parent s constraints to obtain the chtld s constramts  It should be clear that if S   St  then St offers  not neces sarily strictly  tighter bounds on error when used instead f S  at any point  To see this  imagine that various arproxt mation schemes are used for different a vectors at dtfferent stages  and that S  is used whenever a E Ni is chosen  If we keep everything fixed but replace S  with S  at a  we first observe that Sw   a   Sw   a   This ensures that B   a       Bt  a  and Bt      B    If all other projection         operators are the same  then obvwusly U k       uks   strru   lar remarks apply to the infinite horizon case  Furthermore  given the definition of Alt sets  reducing the switch set for a at stagek by using S  instead of S  ensures that the Alt sets at all preceding stages are no larger  and may well be smaller  than they would be if S  were used  For this rea son  we have that E       E   and similarly E        E     Consequently  as we move down the lattice  the bound on approximation error gets smaller  i e   our approximations improve  at least in the worst case   Of course  the com putational effort of monitoring increases as well  The pre cise computational effort of monitoring will depend on the structure of the D BN for the POMDP dynamics and its in teraction with the marginals given by the chosen projec tion scheme  however  the complexity of inference  i e   the dominant factors in the corresponding clique tree   can be easily determined for any node in the lattice       Search for a Good Projection Scheme  In a POMDP setting  the agent may have a bounded amount of time to make an online decision at each time step  For this reason  efficient belief state monitoring is crucial  However  just as solving the POMDP is viewed as an offline operation  so is the search for a good projection schme  Thus it will generally pay to expend some computattonal effort to search for a good projection scheme that makes the appropriate tradeoff between decision quality and the complexity of belief state maintenance  For instance  if any scheme S with at most c constraints offers acceptable on line performance  then the agent need only search the row of the lattice containing those projection schemes with c con straints  However  the size of this row is factorial in c  So instead we use the structure of the lattice to direct our atten tion toward reasonable projections  We describe here a generic  greedy  anytime algorithm for finding a suitable projection scheme  We start with the root  and evaluate each of its children  The child that looks most  promising  is chosen as our current projection scheme  Its children are then evaluated  and so on  this continues un til an approximation is found that incurs no err r  specifi cally  each switch set is a singleton  as we descnbe below   or a bound on the size of the projection is reached  We as sume for simplicity that at most c constraints will be al lowed  The search proceeds to depth c  n in the lattice and at each node  at most n   c   n  children are evaluated  so a total of     nc    en    nodes are examined  Since c must be greater than n the root node itself has n constraints  we assume     nc    complexity  The structure of the lattice ensures that decision quality  as measured by error bounds  cannot decrease at any step  We note that practical and non practical projections are included in the lattice  In figure    the only non practical scheme is S     AB  AC  B     During the search  it doesn t matter if a node correspondmg to a non practical scheme is traversed  as long as the final node is practical  If it is not practical  then the best prc tical sibling of that node is picked or we backtrack nt tl a   practical scheme is found  We also note that smce tIs s a greedy approach  we may not discover the bes roJectwn with a fixed number of constraints  However  tt ts a well         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       structured search space and other search methods for navi gating the lattice could be used  We first describe one instantiation of this algorithm  the finite horizon U bound search  for ak stage  finite horizon POMDP  Given the collections of a vectors N i          Nk  we run the following search independently for each vector a E N  for each i      k  The order does not matter  we will end up with a projection scheme S for each a vector  which is pplied whenever that a vector is chosen as optimal at stage z  We essentially minimize  over S  each term B  a  in he bound Uk independently  For a given vector a at stage z  the search proceeds from the root in a greedy fashion  Each hild S of the current node is evaluated by comput ing B   a   which basically requires that we compute the switch set Sw   a   which in turn requires the solution of IN  I LPs  Once the projection schemes Sa for each a are fond  the error bound Uk is given by the sum of the bounds B  as described in the previous section  At each stage i  the number of LPs that must be solved is O nc IN      since there are O IN  I  a vectors and for each a vector  the lattice search traverses    nc   nodes  each requiring the solution of    IN  I  LPs  Since the solution of the original POMDP requires the solution of at least IN I LPs  the overhead in curred is at most a factor of nc  N   The method above can be streamlined considerably  When comparing two nodes  it is not always necessary to gener ate the entire switch set to determine which node has the lowest bound W  a   Each vector a  in a s switch set intro duces an error of at most maJQ  b a  a     Since Bi a    maxa ESw  a   ma Q  b a   a     we can test vectors a  in decreasing order of contributed error until one vector is found to be in the switch set at one node but not the other  The node that does not include this vector in its switch set as the lowest bound B  a   where S is that node s projec twn scheme   Instead of solving IN  I pairs of LPs  generally only a few pairs of LPs will be solved  When testing whether two different schemes S  and S  allow switching to some a vector  the LPs to be solved for each scheme are similar  differing only in the con straints dictated by each projection scheme  This similarity can be exploited computationally by using techniques that take advantage of the numerous common constraints if we solv similar LPs  concurrently   for instance  by solving a stnpped down LP that has only the common constraints and using the dual simplex method to account for the extra constraints   Though details are beyond the scope of this paper  these techniques are faster in practice than solving each LP from scratch  The greedy search can take full ad vantage of these speed ups  each child has only one addi tional constraint  compared to its parent   so not only can structure be shared across children  but the parent s solu tion can be exploited as well  We reiterate that these LPs can also be structured  so state space enumeration is not re quired  Taken together  these computational tricks don t re duce the worst case running time of O nc  IN I   LPs  how ever in practice it is possible that only D nciNI  LPs need be solved  in which case  when integrated with the algorithm to solve the POMDP  the overhead incurred would be a factor  proportional to nc  A thorough experimentation remains to be done  There are three variations of the algorithm above  The infinite horizon U bound algorithm is much like the finite horizon version  However  we only have one set of a vetors  W  rather thank sets  Thus we compute far fewer switch sets  and calculate the final bound using the equation for U   The finite horizon E bound algorithm is similar to the above algorithm as well  The difference is that we compute A    sts  or rather approximations to them  Alt k a   to obtam tighter bounds on error  To do this requires that we compute the projection schemes for the various stages in order  from the last stage back to the first  Once a good scheme has been found for the elements of Ni  the OOt sets can be computed for stage j     without difficulty  this in volves simple DP backups   Then switch sets are computed exactly as above  from whichAzt sets  and error bounds  are generated  Finally  the infinite horizon E bound algorithm roceeds by computing the switch sets for a given projec tion only once for each vector inN   but additional DP back ups to compute Alt sets  as described in the previous sec tion  are needed to derive tight error bounds     Illustrative Example  We describe a very simple POMDP to illustrate the benefits of value directed approximation  with the aim of demon stratig that minimizing belief state error is not always ap propnate when approximate monitoring is used to imple ment an optimal policy  The process involves only seven stages with only one or two actions per stage  thus at some stages no choice needs to be made   and no observations are involved  Yet even such a simple system shows the benefits of allowing the value function to influence the choice of ap proximation scheme  We suppose there is a seven stage manufacturing process whereby four parts are produced using three machines  M  Ml  and M   Parts PI  P   P   and P  are each stamped in turn by machine M  Once stamped  parts P   and P  are processed separately  in turn  on machine Ml  while parts P  and P  are processed together on M   Machine M may be faulty  FM   with prior probability P r FM   When the parts are stamped by M  parts P   and P  may be come faulty  F    F    with higher probability of fault if FM hols  arts P  and P  may also become faulty  F   F    agam With higher probability if FM  but F  and F  are both less sensitive to FM than Fl and F   e g   P r FIIFM    P r F IFM    P r  F IFM    P r  F IFM     If PI or P  are processed on machine M  when faulty  a cost is incurred  f processed when OK  a gain is had  if not processed  re Jected   n cost or gain is had  When P  and P  are pro   cessed  jomtly on M   a greater gain is had if both parts are OK  a lesser gain is had when one part is OK  and a drastic ost is incurred if both parts are faulty  e g   machine M  iS destroyed   The specific problem parameters are given in Table    Figure   shows the dependencies between variables for the   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Stages to go     StampPI  Actions StampPI  Transitions  Rewards  only affects Fl  no reward  Correlation FI F  F  F   if FM at previous step     StampP   Stamp     StampP   StampP      SlampP   P   thenPr FI  only affects  I  Process Reject P P      F   no reward  if FM at previous step          only affects F    if FM at previous step then Pr F         elsePr F              else Pr F      if Fl then    for every state  Process  P   Process P  P   all variables are persistant  if F  then   else  all variables are persistant    for every state  all variables are persistant  if F    F  then  RejectP  P   all variables are persistant     D     A  F  AA           if  F     F  then otherwise  c    else    all variables are persistant all variables  Reject P            Table    Comparison of different distance measures  no reward        Process PI  persistant  Loss  no reward  RejectPI  are  KL                                                   only affects F    if FM at previous step thenPr F      Process RejectP       else Pr F I         then Pr F         elsePr F    StampP      Process Reject PI                     E       A           for every state              Figure    An Example Density Tree  Table    POMDP specifications for the factory example  nal stage  and the former  Pr FM F    is needed to accu rately assess Pr F  F   at the subsequent stage  Thus we maintain an approximate belief state with marginals involv ing no more than two variables  yet we are assured of acting optimally   Figure    DBN for the factory example  seven stage DBN of the example   It is clear with three stages to go  all the variables are correlated  If approximate belief state monitoring is required for execution of the op timal policy  admittedly unlikely for such a simple prob lem    a suitable projection scheme could be used  Notice that the decisions to process P  and P  at stages to go   and   are independent  they depend only on Pr Fl  and Pr F    respectively  but not on the correlation be tween the two variables  Thus  though these become quite strongly correlated with five stages to go  this correlation can be ignored without any impact on the decision one would make at those points  Conversely F  andF  become much more weakly correlated with three stages to go  but the optimal decision at the final stage does depend on their joint probability  Were we to ignore this weak correlation  we run the risk of acting suboptimally  We ran the greedy search algorithm of Section     and  as expected  it suggested projection schemes that break all cor relations except forFM andF  with four stages to go  and F  andF  with three  two  and one stage s  to go  The lat ter  Pr F  F    is clearly needed  at least for certain prior probabilities onFM  to make the correct decision at the fi  We have imposed certain constraints on actions to keep the problem simple  with the addition of several variables  the prob lem could easily be formulated as a  true  DBN with identical dy namics and action choices at each time slice   In contrast  if one chooses a projection scheme for this problem by minimizing KL divergence  L  distance  or Lz distance  different correlations will generally be pre served  For instance  assuming a uniform prior overFM  i e   machine M is faulty with probability       Table   shows the approximation error that is incurred according to each such measure when only the correlation between F  andF  is maintained or when only the correlation be tweenF  andF  is maintained  All of these  direct  mea sures of belief state error prefer the former  However  the loss in expected value due to the former belief state approx imation is       whereas no loss is incurred using the lat ter  To test this further  we also compared the approxima tion preferred using these measures over        uniformly  randomly generated prior distributions  If only theF  jF correlation is preserved at the first stage  then in     in stances a non optimal action is executed with an average loss of         This clearly demonstrates the advantage of using a value directed method to choose good approxima tion schemes      Framework Extensions  The methods described above provide means to analyze value directed approximations  Though we focused above on projection schemes  approximate monitoring can be ef fected by other means  Our framework allows for the anal ysis of error of any linear approximation scheme S  In fact  our analysis is better suited to linear approximations  the constraints on the approximate belief state S  b   if linear  allow us to construct exact switch sets Sw  a   rather than ap proximations  providing still tighter bounds  One linear approximation scheme involves the use of den sity trees       A density tree represents a distribution by aggregation  the tree splits on variables  and probabilities labeling the leaves denote the probability of every state con sistent with the corresponding branch  For instance  the        UNCERTAINTY IN ARTI FICIAL I NTELLIGENCE PROCEEDINGS       tree in Figure   dnotes a dist   ibution over four variables in which states cdef and cdef both have probability         A tree that i s polynomially sized i n the number of variables offers an exponential reduction in the number of parameters required to represent a distribution  A belief state can be ap proximated by forcing it to fit within a tree of a bounded size  or satisfying other constraints   This approximation can be reconstructed at each stage  just like projection  It is clear that a density tree approximation is linear  Furthermore  the number of constraints and required variables in the LP for computing a switch set is small  We also hope to extend this framework to analyze sampling methods                     While such schemes are generally an alyzed from the point of view of belief state error  we would like to consider the impact of sampling on decision quality and develop value directed sampling techniques that mini mize this impact      Concluding Remarks  The value directed approximation analysis we have pre sented takes a rather different view of belief state approxi mation than that adopted in previous work  Rather than try ing to ensure that the approximate belief state is as close as possible to the true belief state  we try to make the approx imate belief state induce decisions that are as close as pos sible to optimal  given constraints on  say  the size of the belief state clusters we wish to maintain  Our approach re mains tractable by exploiting recent results on factored rep resentations of value functions  There are a number of directions in which this research must be taken to verify its practicality  We are currently ex perimenting with the four bounding algorithms described in section      Ultimately  although these algorithms pro vide worst case bounds on the expected error  it is of in terest to gain some insight regarding the average error in curred in practice  We are also experimenting with other heuristics  such as the the vector space method mentioned in Section     I   that may provide a tradeoff between the qual ity of the error bounds and the efficiency of their compu tation  Other directions include the development of online  dynamic choice of projection schemes for use in search tree approaches to POMDPs  see  e g            as well as solving POMDPs in a bounded optimal way that takes into account the fact that belief state monitoring will be approximate  Acknowledgements Poupart was supported by NSERC and carried our this research while visiting the University of Toronto  Boutilier was supported by NSERC Research Grant OGP            and IRIS Phase   Project BAC   
 Planning can often be simplified by decomposing the task into smaller tasks arranged hierarchically  Charlin et al      recently showed that the hierarchy discovery problem can be framed as a non convex optimization problem  However  the inherent computational difficulty of solving such an optimization problem makes it hard to scale to realworld problems  In another line of research  Toussaint et al       developed a method to solve planning problems by maximumlikelihood estimation  In this paper  we show how the hierarchy discovery problem in partially observable domains can be tackled using a similar maximum likelihood approach  Our technique first transforms the problem into a dynamic Bayesian network through which a hierarchical structure can naturally be discovered while optimizing the policy  Experimental results demonstrate that this approach scales better than previous techniques based on non convex optimization      Introduction  Planning in partially observable domains is notoriously difficult  However  many planning tasks naturally decompose into subtasks that may be arranged hierarchically  For instance  the design of a soccer playing robot is often decomposed into low level skills such as intercepting the ball  controlling the ball  passing the ball  etc        Similarly  prompting systems that assist older adults with activities of daily living  e g   handwashing      can be naturally decomposed into subtasks for each step of an activity  When a decomposition or hierarchy is known a priori  several approaches have demonstrated that planning can be simplified and performed faster          However  the hierarchy is  Pascal Poupart Computer Science University of Waterloo Waterloo  Ontario  Canada ppoupart cs uwaterloo ca  not always known or easy to specify  and the optimal policy may only decompose approximately  To that effect  Charlin et al      showed how a hierarchy can be discovered automatically by formulating the planning problem as a non convex quartically constrained optimization problem with variables corresponding to the parameters of the policy  including its hierarchical structure  Unfortunately  the inherent computational difficulty of solving this optimization problem prevents the approach from scaling to real world problems  Furthermore  it is not clear that automated hierarchy discovery simplifies planning since the space of policies remains the same  We propose an alternative approach that demonstrates that hierarchy discovery  i  can be done efficiently and  ii  performs a policy search with a different bias than non hierarchical approaches that is advantageous when there exists good hierarchical policies  The approach combines Murphy and Paskins      factored encoding of hierarchical structures  see also       into a dynamic Bayesian network  DBN  with Toussaint et al s      maximum likelihood estimation technique for policy optimization  More precisely  we encode POMDPs with hierarchical controllers into a DBN in such a way that the policy and hierarchy parameters are entries of some conditional probability tables  We also consider factored policies that are more general than hierarchical controllers  The policy and hierarchy parameters are optimized with the expectationmaximization  EM  algorithm      Since each iteration of EM essentially consists of inference queries  the approach scales easily  Sect    briefly introduces partially observable Markov decision processes  controllers and policy optimization by maximum likelihood estimation  Sect    reviews previous work on hierarchical modeling and how to use a dynamic Bayesian network to encode a hierarchical structure  Sect    describes our proposed approach  which combines a dynamic Bayesian network encoding with maximum likelihood estimation to simultane    ously optimize a hierarchy and the controller  Sect    demonstrates the scalability of the proposed approach on benchmark problems  Finally  Sect    summarizes the paper and discusses future work      Background  Throughout the paper we denote random variables by upper case letters  e g   X   values of random variables by their corresponding lower case letters  e g   x  dom X   and sets of values by upper case letters with math calligraphy  e g   X    x    x    x      We now review POMDPs  Sect        how to represent policies as finite state controllers  Sect       and how to optimize bounded controllers  Sect             POMDPs  Partially observable Markov decision processes  POMDPs  provide a natural and principled framework for planning  A POMDP can be formally defined by a tuple hS  A  O  ps   ps   as   po   s  a   ras i where S is the set of states s  A is the set of actions a  O is the set of observations o  ps   Pr S    s  is the initial state distribution  a k a  initial belief   ps   as   Pr St     s    At   a  St   s  is the transition distribution  po   s  a   Pr Ot     o    St     s    At   a  is the observation distribution and ras   R At   a  St   s  is the reward function  Throughout the paper  it is assumed that S  A and O are finite and discrete  The goal is to select actions to maximize the rewards  At any point in time  the information available to select the next action consists of the history of past actions and observations  Hence a policy  is defined as a mapping from histories to actions  However  since histories grow with time  it is common practice to summarize histories with a fixed length sufficient statistic such as the belief distribution bs   Pr S   s   which corresponds to the state distribution  conditioned on the history of past actions and observations   The belief distribution b can be updated at each time step  based on the   action a taken and the observation Po made according ao  to Bayes theorem  bs    k s bs ps   as po   s  a  k is a normalization constant   Policies can then be defined as mappings from beliefs to actions  e g    b    a   The value V   b  of a policy  starting in belief b is measured byPthe discounted sum of  t expected P rewards  V  b    t  Ebt   r bt  bt   where rab   s bs ras   An optimal policy  is a policy with the highest value V  for all beliefs  V   b   V    b  The optimal value function also satisfies Bellmans P  ao     ab V  b equation  V   b    max r   p     a ab o o P where po   ab   ss  bs ps   as po   s  a         Finite State Controllers  A convenient representation for an important class of policies consists of finite state controllers      Instead of using beliefs as sufficient statistics of histories  the idea is to use a finite internal memory to retain relevant bits of information from histories  Each configuration of this memory can be thought of as a node in a finite state controller  where nodes select actions to be executed and edges indicate how to update nodes based on the observations received  A controller with a finite set N of nodes n can encode a stochastic policy  with three distributions  Pr N    n    pn  initial node distribution   Pr At   a   Nt   n    pa n  action selection distribution  and Pr Nt     n    Nt   n  Ot     o      pn   no   successor node distribution   Such a policy can be executed by starting in a node n sampled from pn   executing an action a sampled from pa n   receiving observation o    transitioning to node n  sampled from pn   no  and so on  The value of a controller Pcan be computed by solving a linear system  V   ns a pa n  ras   P     ns  The value at a  s  o  n  ps   as po   s  a pn   no  Vn  sP P given belief b is then V   b    n s bs pn Vns        Policy Optimization  Several techniques have been proposed to optimize controllers of a given size  including gradient ascent      stochastic local search      bounded policy iteration       non convex quadratically constrained optimization     and likelihood maximization       We briefly describe the last technique since we will use it in Sect     Toussaint et al       recently proposed to convert POMDPs into equivalent dynamic Bayesian networks  DBNs  by normalizing the rewards and to optimize a policy by maximizing the likelihood of the normalized rewards  Let R be a binary variable corresponding to normalized rewards  The reward function ras is then replaced by a reward distribution pr sat   Pr R   r   At   a  St   s  T   t  that assigns probability ras   rmax  rmin   to R     and    ras   rmax  rmin   to R      rmin   minas ras and rmax   maxas ras    An additional time variable T is introduced to simulate the discount factor and the summation of rewards  Since a reward is normally discounted by a factor  t when earned t time steps in the future  the prior pt   Pr T   t P is set to  t     where  the factor     ensures that t   pt      The resulting dynamic Bayesian network is illustrated in Fig     It can be thought of as a mixture of finite processes of length t with a     reward R earned at the end of the process  The nodes Nt encode the internal memory of the controller  Given the controller distributions pn   pa n and pn   no    it is possible to evaluate the controller   N   T     N   T     A   S   R  N   A   S   O   A   R  S       N   T   tmax  N   A   O   Ntmax  A        Otmax  we will empirically compare our approach to the nonconvex optimization techniques used to optimize recursive controllers  In another line of research  Murphy and Paskin      proposed to model hierarchical hidden Markov models  HMMs  with a dynamic Bayesian network  DBN   Theocharous et al       also used DBNs to model hierarchical POMDPs  We briefly review this DBN encoding in Sect      since we will use it in our approach to model factored controllers   Atmax      S   S   Stmax  Figure    POMDP represented as a mixture of finite DBNs  For an infinite horizon  a large enough tmax can be selected at runtime to ensure that the approximation error is small  by computing the likelihood of R      More precisely  V   ps      Pr R       rmin     rmax  rmin          Optimizing the policy can be framed as maximizing the likelihood of R     by varying the distributions pn   pa n and pn   no  encoding the policy  Toussaint et al  use the expectation maximization  EM  algorithm  Since EM is guaranteed to increase the likelihood at each iteration  the controllers value increases monotonically  However  EM is not guaranteed to converge to a global optimum  An important advantage of the EM algorithm is its simplicity both conceptually and computationally  In particular  the computation consists of inference queries that can be computed using a variety of exact and approximate algorithms      Recursive Controllers  R  Hierarchical Modeling  While optimizing a bounded controller allows an effective search in the space of bounded policies  such an approach is clearly suboptimal since the optimal controller of many problems grows doubly exponentially with the planning horizon and may be infinite for infinite horizons  Alternatively  hierarchical representations permit the representation of structured policies with exponentially fewer parameters  Several approaches were recently explored to model and learn hierarchical structures in POMDPs  Pineau et al       sped up planning by exploiting a user specified action hierarchy  Hansen et al      proposed hierarchical controllers and an alternative planning technique that also exploits a user specified hierarchy  Charlin et al      proposed recursive controllers  which subsume hierarchical controllers  and an approach that discovers the hierarchy while optimizing a controller  We briefly review recursive controllers in Sect      since  A recursive controller     consists of a recursive automaton with concrete nodes n and abstract nodes n  Abstract nodes call a subcontroller before selecting an action  A controller is said to be recursive when it can call itself  essentially encoding an infinite hierarchy  Formally  a recursive controller is parametrized by an action selection distribution for each node  e g   pa n and pa n    a successor node distributions for each node  e g   pn   no  and pn   no    and a child node distribution for each abstract node  e g   pn   n      Execution of a recursive controller is performed by executing the action selected by each node visited and continuing to the successor node selected by the observation made  However  when an abstract node is visited  before executing the action selected  its subcontroller is called and started in the child node selected by the child node distribution  A subcontroller returns control to its parent node when a special end node is reached  Charlin et al      show that optimizing a recursive controller with a fixed number of concrete and abstract nodes can be framed as a non convex quartically constrained optimization problem  The hierarchical structure is discovered as the controller is optimized since the variables of the optimization problem include the child node distributions which implicitly encode the hierarchy  Three techniques based on a general non linear solver  a mixed integer non linear approximation and a form of bounded hierarchical policy iteration are experimented with  but do not scale beyond toy problems  Furthermore  Charlin et al  do not demonstrate whether searching in the space of hierarchical controllers can speed up planning  Although it is clear that planning is simplified when a hierarchy is given a priori since the policy space is reduced  it is not clear that hierarchy discovery is beneficial since the policy space remains the same while the parameter space changes  In Sect     we demonstrate that hierarchy discovery can be beneficial when a simple hierarchical policy of high value exists     The pa n and pn   no  distributions are combined in one distribution pn  a no  in        S    S    S    E        E   the tuple hpa nl   pnl   nl   pn l  nl o  i l  The conditional probability distributions of the mixture of DBNs  denoted by p  are   transition distribution  ps   as   ps   as  Figure    HMM   S    S    S    O   O   O   DBN encoding of a   level hierarchical   observation distribution  po   s  a   po   s  a  reward distribution  pr as    ras  rmin    rmax  rmin    mixture distribution  pt         t  action distribution  pa n    pa n        Hierarchical HMMs  Murphy and Paskin      proposed to model hierarchical hidden Markov models  HMMs  as dynamic Bayesian networks  DBNs   The idea is to convert a hierarchical HMM of L levels into a dynamic Bayesian network of L state variables  where each variable encodes abstract states at the corresponding level  Here  abstract states can only call sub HMMs at the previous level  Fig    illustrates a two level hierarchical HMMs encoded as a DBN  The state variables Stl are indexed by the time step t and the level l  The Et variables indicate when a base level sub HMM has ended  returning its control to the top level HMM  The toplevel abstract state transitions according to the top HMM  but only when the exit variable Et indicates that the base level concrete state is an exit state  The base level concrete state transitions according to the base level HMM  When an exit state is reached  the next base level state is determined by the next toplevel abstract state  Factored HMMs subsume hierarchical HMMs in the sense that there exists an equivalent factored HMM for every hierarchical HMM  In Sect       we will use a similar technique to convert hierarchical controllers into factored controllers      Factored Controllers  We propose to combine the DBN encoding techniques of Murphy et al       and Toussaint et al       to convert a POMDP with a hierarchical controller into a mixture of DBNs  The hierarchy and the controller are simultaneously optimized by maximizing the reward likelihood of the DBN  We also consider factored controllers which subsume hierarchical controllers       DBN Encoding  Fig   a illustrates two consecutive slices of one DBN in the mixture  rewards are omitted  for a three level hierarchical controller  Consider a POMDP defined by the tuple hS  A  O  ps   ps   as   po   s  a   ras i and a threelevel hierarchical  non recursive  controller defined by   base pn    n  n   o  e    level node distribution    if e   exit pn    n     pn    o  n  otherwise  middle pn    n  n   o  e  e   level node distribution  if e    exit  pn    n   p        if e    exit and e     exit    n  o n n   n  otherwise  top level node distribution  pn    o  n  e  pn    o  n  if e    exit   n   n  otherwise  base level exit distribution  pe   n      if n  is an end node     otherwise  middle level exit distribution  pe   n  e      if e    exit and n  is an end node     otherwise While the Etl variables help clarify when the end of a sub controller is reached  they are not necessary  Eliminating them yields a simpler DBN illustrated in Fig   b  The conditional probability distributions of the Ntl variables become   base pn    n  n   o    level node distribution  pn    n   if n  is an end node   pn    o  n  otherwise  middle pn    n  n   o   level node distribution      if n and n are end nodes  pn    n   pn    o  n  if n  is an end node  but not n     n   n  otherwise  top level node distribution  pn    n  o  e  pn    n  o  if n  and n  are end nodes   n   n  otherwise Note that ignoring the above constraints in the conditional distributions yields a factored controller that is more flexible than a hierarchical controller since the conditional probability distributions of the Ntl variables do not have to follow the structure imposed by a hierarchy    a   N     N    b   N   N            W l o g  we initialize the start node N top of the top layer to be the first node  i e   Pr N top            The node conditional distributions pn l   n l   are initialized randomly as a mixture of three distributions   E  N     N   N     N     N   N     O   O  S  S   S  N   N     N  E  N   O   c   A  A  O   S    d  N   N   N   SS   N   N   N   S   N   N     Parameter initialization  pn l   n l    c    c  Un l  n l     c  n l nl The mixture components are a uniform distribution  a random distribution U n l    an array of uniform random numbers in          and a term enforcing nl to stay unchanged  For the node distributions at the base level we choose c       c       c      and for all other levels we choose c       c       c        Similarly we initialize the action probabilities as  N   N    N   N   S    pa nbase  c    c  Uanbase   c  a nbase  a   N    N   N   S    N   N    N    N   N    N   S   N    N    N   S   N    N    N   N    S    S  S   Figure     a  Two slices of the DBN encoding the hierarchical POMDP controller   b  A version where exit variables are eliminated   c  Variables O and A are eliminated   d  The corresponding junction tree  or rather chain  for inference        Maximum Likelihood Estimation  Following Toussaint et al s technique       we optimize a factored controller by maximizing the reward likelihood  Since the policy parameters are conditional probability distributions of the DBN  the EM algorithm can be used to optimize them  Computation alternates between the E and M steps below  We denote by ntop and nbase the top and base nodes in a given time slice  We also denote by  V   and  v  the parents of V and a configuration of the parents of V   E step  expected frequency of the hidden variables top top    Entop   Pr N P     n  R  base Eanbase   t Pr At   a  Nt   nbase  R      En l  n l     P l  l l l t Pr Nt     n    Nt        nt     R      l M step  relativeP frequency computation pntop   Entop   ntop PEntop pa nbase   Eanbase   a P Eanbase pn l   n l     En l  n l     n l En l  n l   l  with c       c       c         where the last term enforces each node nbase   i to be associated with action a   i a         E step  To speed up the computation of the inference queries in the E step  we compute intermediate terms using a forward backward procedure  Let tmax be the largest value of T   then a simple scheme that answers each query separately takes O t max   time since there are O tmax   queries and each query takes O tmax   time to run over the entire network  However  since part of the computation is duplicated in several queries  it is possible to compute intermediate terms  and  in O tmax   time from which each expectation can be computed in constant time  w r t  tmax    To simplify notation  N and n denote all the nodes and their joint configuration in a given time slice  t Forward term  ns   Pr Nt   n  St   s    ns   pnP ps t  nt   s    n s ns pn  s   ns  Backward term  ns   Pr R     Nt   n  St   s  T   t  P   ns   a pa n ras P  ns   n   s  pn  s   ns n     s   To fully take advantage of the structure of the DBN  we first marginalize the DBN w r t  the observations and actions to get the DBN in Fig   c  This   slice DBN corresponds to the joint transition distribution pn  s   ns used in the above equations  Then we compile this   slice DBN into the junction tree  actually junction chain  given in Fig   d  P P  Let ns    Pr T     ns and ns   t Pr T   t t ns   then the last two expectations of the E step   can be computed as follows     P Eanbase  s n nbase   ns pa nbase ras     P      s P  o   n  ps   as po   s  a pn   o  n n s En l  n l    s s   a n n l   n l ns pa nbase ps   as     po   s  a pn   o  n ras   n  s  l        M step  The standard M step adjusts each parameter pv  v  by normalizing the expectations computed in the Estep  i e   pnew v  v   Ev v    To speed up convergence  we instead use a variant that performs a soften greedy M step  In the greedy M step  each parameter pnew v  v  is greedily set to   when v   argmaxv fv v  and   otherwise  where fv v    Ev v   pold v  v    The greedy M step can be thought of as the limit of an infinite sequence of alternating partial E step and standard M step where the partial E step keeps f fixed  The combination of a standard M step with this specific partial E step updates pv  v  by a multiplicative factor proportional to fv v    In the limit  the largest fv v  ends up giving all the probability to the corresponding pv  v    EM variants with certain types of partial E steps ensure monotonic improvement of the likelihood when the hidden variables are independent       This is not the case here  however by softening the greedy M step we can still obtain monotonic improvement most of the time while speeding up convergence  We update pv  v  as follows  v    argmax fv v  pnew v  v     v old pv  v   vv    parameters flat  O  N       A  N   fact    O  N         A  N      forward backward complexity flat O tmax   N   S      N     S    fact  O tmax   N   S      N       S    expectation complexity flat O  N   A   S      S  O      N     S  O   fact  O  N   A   S      S  O      N       O  S     N     O    level have fewer parameters and a smaller complexity  but also a smaller policy space due to the structure imposed by the hierarchy factorization  While there is a tradeoff between policy space and complexity  hierarchical and factored controllers are often advantageous in practice since they can find more quickly a good hierarchical factored policy when there exists one  A   level factored controller with  N      nodes at each level has   O  N      parameters for pn top  o  nbase ntop and pn base  n top o  nbase   and  A  N      parameters for The complexity of the forward  backpa nbase   ward  procedure is O tmax   N   S      N       S    and the complexity of computing the expectations is O  N   A   S      S  O      N       O  S     N     O    A   level hierarchical controller is further restricted and therefore has fewer parameters  but the same time complexity     c         For c     and       this is the greedy M step  We use c     which softens  shortens  the step and improves convergence  Furthermore  adding small Gaussian noise    N           helps to escape local minima         Table    Number of parameters and computational complexity for the flat controller with  N   nodes and a   layer factored controller with  N top      N base      N      nodes   Complexity  For a flat controller  the number of parameters  neglecting normalization  is  O  N    for pn   o  n and  A  N   for pa n   The complexity of the forward  backward  procedure is O tmax   N   S      N     S    where the two terms correspond to the size of the two cliques for inference in the   slice DBN after O and A are eliminated  The complexity of computing the expectations from  and  is O  N   A   S      S  O      N     S  O    which corresponds to the clique sizes of the   slice DBN including O and A  In comparison    level hierarchical and factored controllers with  N top      N base      N      nodes at each   The first expectation of the E step does not need to be computed since Pr N top               Experiments  We first compared the performance of the maximum likelihood  ML  approach to previous optimizationbased approaches from      Table   summarizes the results for   layer controllers with certain combinations of  N base   and  N top    The problems include paint  shuttle and  x  maze  previously used in      and three additional problems  chain of chains  described below   hand washing  reduced version from      and cheese taxi  variant from        On the first three problems  ML reaches the same values as the previous optimization based approaches  but with larger controllers  We attribute this to EMs weaker ability to avoid local optima than the optimization based approaches  However  the optimization based approaches run out of memory on the last three problems  memory needs exceed   Gb of RAM   while ML scales gracefully  as analyzed in Sect          ML approach demonstrates that hierarchy discovery can be tackled with tractable algorithms  We also report the values reached with a state of the art point based value   Table    V  denotes optimal values  with truncated trajectories      except for handwashing and cheese taxi where we show the optimal value of the equivalent fully observable problem  HSVI  found a solution in less than  s for every problem except handwashing where the algorithm was halted after    hours of computation  The ML approach optimizes a factored controller for     EM iterations with a planning horizon of tmax              nodes means  N base       and  N top        For cheese taxi  we get a maximum value of       N A indicates that the solver did not complete successfully  All tests are done on a dual core x   processor     GHz  Problem paint shuttle  x  maze chain of chains handwashing cheese taxi   S    A    O   V                                                                                         HSVI  V                                 N A          Best results from     nodes t s  V                                                        N A N A  iteration method  HSVI         The next question is whether there are computational savings when automatically discovering a hierarchy  Recall that previous work has shown that policy optimization is simplified when a hierarchy is known a priori since the space of policies is restricted  The next experiment demonstrates that policy optimization while discovering a hierarchy can be done faster and or yield higher value when there exists good hierarchical policies  Table   compares the performance when optimizing flat  hierarchical and factored controllers on chain of chains  hand washing and cheesetaxi  Here  the factored and hierarchical controllers have two levels and correspond respectively to the DBNs in Fig    a  and   b    The x axis is the number of nodes for flat controllers and the product of the number of nodes at each level for hierarchical and factored controllers  Taking the product is justified by the fact that the equivalent flat controllers of some hierarchical factored controllers require that many nodes  The graphs in the right column of Table   demonstrate that hierarchical and factored controllers can be optimized faster  confirming the analysis done in Sect         There is no difference in computational complexity between the strictly hierarchical and unconstrained factored architectures  Recall however that the efficiency gains of the hierarchical and factored controllers are obtained at the cost of a restricted policy space  Nevertheless  the graphs in the left column of Table   suggest that hierarchical factored controllers can still find equally good policies when there exist one  Factored controllers are generally the most robust  With a sufficient number of nodes  they find the best policies on all three problems  Note that factored and hierarchical controllers need at least a number of nodes equal to the number of actions in the base layer in order to represent a policy that uses all actions    Factored controllers are hierarchical controllers where the restrictions imposed by the Et variables are removed   Level    ML approach  avg  over    runs  nodes t s  V                                                                 e                                                                           Level          A       B    D  C  Figure    Hierarchical controller learnt for the chainof chains  The diamond indicates an exit node  for which pe   n        This explains why hierarchical and factored controllers with less than   base nodes  for chain of chains  and   base nodes  for hand washing and cheese taxi  do poorly  The optimization of flat controllers tend to get stuck in local optima if too many nodes are used  Comparing the unconstrained factored architecture versus hierarchical  we find that the additional constraints in the hierarchical controller make the optimization problem harder although there are less parameters to optimize  As a result  EM gets stuck more often in local optima  We also examine whether learnt hierarchies make intuitive sense  Good policies for the cheese taxi and handwashing problems can often be represented hierarchically  however the hierarchical policies found didnt match hierarchies expected by the authors  Since these are non trivial problems for which there may be many ways to represent good policies in a hierarchical fashion that is not intuitive  we designed the chain ofchains problem  which is much simpler to analyze  The optimal policy of this problem consists of executing n times the same chain of n actions followed by a submit action to earn the only reward  The optimal policy requires n      nodes for flat controllers and n     nodes at each level for hierarchical controllers  For n      ML found a hierarchical controller of   nodes at each level  illustrated in Fig     The controller starts in node    Nodes at level   are abstract and descend into concrete nodes at level   by following the dashed                                                                                                                                                                                                                                       flat        factored hierarchical                                                                                                                                                                                                       flat factored hierarchical                            flat factored hierarchical                                                                                                                                                                                                                                                                                         cheese taxi  time  seconds               flat factored hierarchical                                 cheese taxi  best value                                                    flat factored hierarchical                                                                                                          handwashing  value           handwashing  time  seconds   chain of chains  value       chain of chains  time  seconds   Table    Left  The reached values depending on the number of nodes in the controller  For the factored and hierarchical controller we indicate the number of nodes in both layers  e g         means  N base       and  N top        and plot the data point at  N base   N top   on the x axis  For instance  in the case of handwashing we see how the performance depends critically on  N base    Right  The optimization time  In all cases      EM iterations are performed with a planning horizon of tmax        The results for each controller are the average of    runs with error bars of   standard deviation                        flat factored hierarchical                                                                                         nodes  edges  Control is returned to level   when an end node  denoted by a diamond  is reached  Here  the optimal policy is to do A B C three times followed by D  Hence a natural hierarchy would abstract A B C and D into separate subcontrollers  While the controller in Fig    is not completely optimal  the vertical transition from abstract node   should have probability   of reaching node A   it found an equivalent  but less intuitive abstraction by having subcontrollers that do A B C and D A B C  This suggests that for real world problems there will be many valid abstractions that are not easily interpretable by humans and the odds that an automated procedure finds an intuitive hierarchy without any additional guidance are slim           nodes     Conclusion  The key advantage of maximum likelihood is that it can exploit the factored structure in a controller architecture  This facilitates hierarchy discovery when the hierarchical structure of the controller is encoded into a corresponding dynamic Bayesian network  DBN   Our complexity analysis and the empirical run time analysis confirm the favorable scaling  In particular  we solved problems like handwashing and cheese taxi that could not be solved with the previous approaches in      Compared to flat controllers  factored controllers are faster to optimize and less sensitive to local optima when they have many nodes  Our current implementation does not exploit any factored structure   in the state  action and observation space  however we envision that a factored implementation would naturally scale to large factored POMDPs  For the chain of chains problem  maximum likelihood finds a valid hierarchy  For other problems like handwashing  there might be many hierarchies and the one found by our algorithm is usually hard to interpret  We cannot expect our method to find a hierarchy that is human readable  Interestingly  although the strictly hierarchical architectures have less parameters to optimize  they seem to be more susceptible to local optima as compared to a factored but otherwise unconstrained controller  Future work will investigate various heuristics to escape local optima during optimization  In this paper we made explicit assumptions about the structure  we prefixed the structure of the DBN to mimic a strict hierarchy or a level wise factorization and we fixed the number of nodes in each level  However  the DBN framework allows us to build on existing methods for structure learning of graphical models  A promising extension would be to use such structure learning techniques to optimize the factored structure of the controller  Since the computational complexity for evaluating  training  a single structure is reasonable  techniques like MCMC could sample and evaluate a variety of structures  This variety might also help to circumvent local optima  which currently define the most dominant limit of our approach  Acknowledgments Part of this work was completed while Charlin was at the University of Waterloo  Toussaint acknowledges support by the German Research Foundation  DFG   Emmy Noether fellowship TO          Poupart and Charlin were supported by grants from the Natural Sciences and Engineering Research Council of Canada  the Canada Foundation for Innovation and the Ontario Innovation Trust   
 Rollating walkers are popular mobility aids used by older adults to improve balance control  There is a need to automatically recognize the activities performed by walker users to better understand activity patterns  mobility issues and the context in which falls are more likely to happen  We design and compare several techniques to recognize walker related activities  A comprehensive evaluation with control subjects and walker users from a retirement community is presented      not only time consuming but may not be accurate due to synchronization issues between various sensors  An automated activity recognition system would enable clinicians to gather statistics about the activity patterns of users  their level of mobility and the context in which falls are more likely to occur  This will also be useful for the development of smart walkers that can assist users with navigation and braking while taking into account their intended activity  In this paper we describe a comparative analysis of activity recognition techniques based on hidden Markov models  HMMs  and conditional random fields  CRFs  trained by supervised and unsupervised learning for rollating walkers instrumented with various sensors  Our contributions are   Introduction  Improving the quality of life of the ever increasing elderly population is one of the key concerns for health care provision  Limitations to independent mobility for these individuals have a significant impact on the quality of their life  Devices such as rollating walkers are often used to improve the independence and mobility of older adults  Our long term goal is to improve the utility of these devices  by enabling them to perceive their environment and actively provide assistance to their users  We are collaborating with a multidisciplinary group that is studying the usage of rollating walkers  We have access to a walker     that has been instrumented with various sensors and cameras to monitor the user  We are developing automated techniques to recognize the activities performed by users with respect to their walker  e g   walking  standing  turning  etc   based on the non video sensors  This problem is significant for Kinesiologists who are studying the usage of walkers by elderly people  Currently they have to hand label the data by looking at video feeds of the user  which is  Allan Caine is currently at Research in Motion  Waterloo   the first fully automated system to automatically recognize activities performed by walker users   design and training  supervised and unsupervised  of probabilistic models  HMMs and CRFs  tailored to activity recognition with instrumented walkers   comprehensive analysis of these techniques with real data collected with control subjects and regular walker users living in a retirement community   comprehensive analysis of the ease difficulty to recognize common walker user activities with existing algorithms  The paper is organized as follows  Section   summarizes related work  Section   describes the walker  the experimental setup and our hypotheses regarding the ease difficulty of recognizing common user activities  Section   describes the recognition models  HMMs and CRFs  and their training procedures  supervised and unsupervised   Sections   and   present the results of the experiments and analyze each approach  Finally Section   concludes and discusses future work       Related Work  In      Alwan et al  describe a method that assesses basic walker assisted gait characteristics  including heel strikes  toe off events  as well as stride time  double support and right and left single support phases  These statistics are based on the measurement of weight transfer between the user and the walker by two load cells in the handles of the walker  A simple thresholding approach is used to detect peaks and valleys in the load measurements  which are assumed to be indicative of certain events in the gait cycle  This work focuses on low level gait statistics where as we are interested to recognize complex high level behaviours  Hirata et al      instrumented a walker with sensors and actuators  They recognize three user states  walking  stopped and emergency  including falling   These states are inferred based on the distance between the user and the walker  measured by a laser range finder  and the velocity of the walker  This work is limited to the three states mentioned above and would not be able to differentiate between activities that exhibit roughly the same velocity and distance measurements  e g   walking  turning  going up a ramp   A significant amount of work has been done on activity recognition in other contexts  In particular Liao et al      use a Hierarchical Markov Model to learn and infer a users daily movements through an urban community  The model uses multiple levels of abstraction in order to bridge the gap between raw GPS sensor measurements and high level information such as a users destination and mode of transportation  They use Rao Blackwellized particle filters for state estimation  In      they also recognize activities and places from GPS traces by Hierarchical CRFs  This work assumes that the high level goals and routines of the user as well as the map of the area are known and stable      Experimental Setup  We have access to a walker developed by Tung et al       A picture of the walker is shown in Fig     The walker is equipped with various sensors including a   D accelerometer in the seat  a load cell in each leg and a wheel encoder  which measures the wheels displacement  The sensor readings vary between   and         The data is channeled via blue tooth to a PDA for acquisition at    Hz  In addition to these sensors  there are two cameras on the walker  One is facing backwards and provides the video feed of the users legs  The other is looking forwards and provides the video feed of the environment  The video frame rate is approximately    frames second  In order to collect data for our models  we designed and conducted two exper   Figure    Course used for healthy young subjects Table    Behaviours in Experiment   Not Touching the Walker  NTW  Stop Standing  ST  Walking Forward  WF  Turn Left  TL  Turn Right  TR  Walking Backwards  WB  Transfers  Sit to Stand Stand to Sit   TRS   iments that are described below       Experiment       healthy young subjects  age between    and     were asked to go through the course shown in Figure   twice with the walker  The behaviours exhibited by the participants are shown in Table         Experiment    In a second experiment  we asked   regular walker users  age    to     to follow the course shown in Figure    This experiment was conducted at the Village of Winston Park  retirement community in Kitchener  Ontario  and the participants were residents of that facility  We also asked    adults  age    to     who do not live in a retirement community and are not regular walker users to follow the same course  The behaviours exhibited during this experiment include those of Tables   and         Recognizing Behaviours  Our goal is to perform behaviour recognition based on the non video sensors   Since the accelerometers  load cells and wheel encoder only measure indirectly the activities of the person  it is not obvious a priori which activities can easily recognized  We formulated the following hypotheses     The use of video data is subject to future work    range of strategies used by people to lift or lower the walker   Figure    Smart Walker  Transfers  sit to stand or stand to sit  are also expected to be difficult to recognize due to a wide range of strategies  In theory  the load of the walker should decrease as the person goes from standing to sitting and increase for sit to stand  However  some people leave the walker to hold other supports such as the arms of a chair  Some people also engage the walkers brakes as they do a transfer while others move the walker  Another difficult behaviour is the reaching task  It involves behaviours such as opening a door or picking something from ground  The wheel encoder is not useful as peoples habits of engaging the brakes during these behaviours are variable  In reaching tasks  however  the person usually keeps one hand on the walker while he she uses the other hand to reach for the object  This can be captured by the load cells as there will be more weight on one side  In the next section  we discuss various probabilistic models to recognize activities as accurately as possible despite the wide range of strategies for some behaviours   Figure    Course for older walker users    Not Touching Walker  NTW  should be easy to predict as there is no load on the walker and the walker is not moving  Standing  ST  should be differentiable from NTW based on load cell readings as load fluctuations are expected when the person touches the walker  Walking forward  WF  and walking backward  WB  should be differentiable from ST based on the wheel encoder measurements  which increase with forward movements and decrease with backward movements  Sitting on walker  SW  should also be easily distinguishable since the value of the load sensors is much higher than any other behaviour  We expect turns to be more difficult to predict  We hypothesize that the speed of the person is lower when he she is turning  We also expect a higher load on the side of the turn and some mild acceleration in the opposite direction of the turn  However this will likely vary with each person  Similarly  going up or down ramps and curbs are not expected to be easy to detect  We expect to see some fluctuations in the vertical acceleration when there is an immediate change of elevation and a sustained mild acceleration corresponding to gravity in the forward or backward direction depending on the inclination of ramps  We hypothesize that both these behaviours will be noticeable  However  they may be difficult to distinguish from WF in general  Furthermore  going up and down curbs may be particularly difficult to recognize due to the wide  Activity Recognition Models  We assume that the set of all possible behaviours is B whose cardinality is m  The behaviour at time t is represented by the random variable Bt   The reading on sensor k at time t is given by the random variable Stk where k              n   For notational convenience  we denote the sequence of behaviours from time i to j by Bi j and the observation sequence on sensor k k by Si j   The readings on all sensors observed between   n time i and j is denoted by Si j and the actual observed sequence is denoted by s  n i j   The total length of a sequence is T   The actual behaviour at time t is denoted by bt and the predicted behaviour at time t is bt   Lower case letters denote the assignment of a value to a random variable        Hidden Markov Model  We construct a hidden Markov model in which the behaviour is the hidden variable and the sensor readings are the observations  Fig      The parameters include b   b  Pr Bt   b   Bt    b   probability that the behaviour at time t is b  given that the behaviour at time t  is b   s k b  Pr Stk   s Bt   b   probability that the value measured by the k th sensor at time t is s given that the behaviour is b  and b  Pr B    b   probability that the initial behaviour is b     Table    Additional Behaviours in Experiment   Going up Ramp  GUR  Going down Ramp  GDR  Sitting on Walker  SW  Reaching Task  RT  Going up Curb  GUC  Going down Curb  GDC   Figure    HMM for behaviour recognition         Maximum Likelihood  ML  Learning  For supervised learning  we manually label the data based on the video feed  We learn b   b by counting the number of times behaviour b is followed by behaviour b  in the labeled data  PT   Bt  b   Bt   b  PT b   b   t   b  b   B  B  b  t    t   Here  x      if x is true and   otherwise  However  in some of our experiments  to avoid the bias introduced by using a fixed course  we use a simple transition model that reflects the fact that behaviours are  times more likely to persist than to change      m       if b   b  b  b   B b   b     m       otherwise We can learn the prior i from data using PT b   t     Bt   b  T Again  in some of our experiments  to avoid the bias introduced by using a fixed course  we consider all behaviours to be equally likely initially  Hence b     m b  B We can model s k b with a parametric density function such as a Gaussian  However  on close analysis of the data  we found that the distribution of s i j does not follow a Gaussian  Therefore  we discretize the sensor readings by dividing the range into D discrete intervals  We learn s k b using PT  s k b      Bt  b Sti  s  PT t    Bt  b   t    b  B  s              D  k              n   ML with EM algorithm For unsupervised learning  we use Expectation Maximization  EM      to learn the parameters  The algorithm alternates between com   puting the expectations E   b    Pr B    b s  n   t          PT     E b  b     t   Pr Bt   b  Bt     b   s  n   t          PT Ek  b  s    t   Pr Bt   b  Stk   s s  n     t       and updating the parameters b   E   b   P   b   b   E b  b     P b  E b  b    s k b   Ek  b  s   s E b  s   Prediction  Note that since the sensor readings at any given time are conditionally independent given   n the Q behaviour at that time  therefore  Pr st  Bt     i I st  i b   We use maximum a posteriori filtering to infer the most likely behaviour   given past observations  bt   maxbB Pr Bt   b s  n   t   This computation can be performed online as only past observations are used         Bayesian Learning  Bayesian learning is an alternative to maximum likelihood learning  We start from a prior distribution and update it using Bayes rule to obtain the full posterior distribution over the variables of interest  By considering the full posterior over the parameters  we hope to avoid the over fitting issues  often experienced by the EM algorithm  Unfortunately  in most cases the posterior does not have a tractable form  so we cannot sample from it directly  Consequently  we resort to sampling techniques based on simulating a Markov chain whose stationary distribution is the posterior distribution of interest  A convenient choice of a prior distribution over the parameters is a product of Dirichlet distributions of the form Dir             k   c            ck     P Q c  c  P   Q ci             kck where i i      Each Dirich ci   let is a prior for the corresponding multinomial distribution of transitions or emissions from some state  The values ci can be understood as counts indicating how many times a particular transition emission has been observed  The Dirichlet distribution is a conjugate prior for the multinomial distribution  Therefore  if the hidden states are known  the posterior distribution of the parameters will also be a product of Dirichlets with the counts updated by the number of observed transitions or emissions  If the hidden states are not known  the posterior becomes a mixture of exponentially many products of Dirichlets  each product corresponding to one possible state path  We use Gibbs sampling to estimate the posterior over hidden states and parameters  We repeatedly sample each variable from the conditional distribution given all the other variables  It can be shown that the resulting Markov chain converges to the joint distribution of   Table    HMM results for Experiment   using center of pressure  COP  features  Behaviour persistence parameter           Window size is     Observations are accelerometer measurements  speed  frontal and sagittal center of pressure and total weight  Overall accuracy is      NTW  ST  WF  TL  TR  WB  RT  SW  GUR  GDR  GUC  GDC  NTW                                                      ST                                                                          WF                                                                           TL                                                                      TR                                                                     WB                                                       RT                                                                      SW                                                            GUR                                                          GDR                                                            GUC                                                     GDC                                                         all variables  Here  we use the collapsed Gibbs sampler     which samples the hidden states and integrates out the parameters to speed up the convergence  Since we use a Dirichlet prior  the posterior probability Pr B  T   s  n   T   can be computed analytically  Pr B  T   s  n       T      Pr B  T   s  n       T        Pr      ddd  Q Y Q    bb    Y Q k  bsk   B   B    s bP P      c P   B  B      b  bb      sk bsk         Conditional Random Field  Conditional Random Fields  CRFs  are probabilistic models for segmenting and labeling sequential data      We consider the special case of linear chain CRFs  A CRF specifies the distribution of a sequence of labels  B  t   conditioned on a sequence of observations   n  S  t    In our experiments  the labels are the behaviour of the user  and the observations are the sensor measurements at each time  The probability of   n B  t   b  t conditioned on S  t   s  n   t is given by   n   n P  B  t   b  t   S  t   s  t     b k  b  T  Y  where s  s and s are transition  emission and initial state counts  By taking Pr Bt  B  t    Bt   T   s  n   T     Pr Bt   B  t    Bt   T   s  n   T     P  Bt  Pr Bt   B  t    Bt   T   s  n   T    and simplifying  we get  Pr Bt  B  t    Bt   T   s  n   T     Accuracy    B B P t  t b Bt  bt t    P  Bt Bt   Bt bt    bt      Qn  k    B sk P t t s Bt s  By repeatedly sampling each hidden state according to the distribution above  we are guaranteed to converge to the posterior distribution Pr B  T  s  n   T    Although we integrate out the parameters analytically  we can sample efficiently from the distribution over the parameters  For any assignment to B  T   the conditional distribution Pr      B  T   s  n   T   is a product of Dirichlet distributions  Since the Gibbs sampler provides us with a way to sample from Pr B  T  s  n   T    sampling from Pr      s  n   T   can be done efficiently  Gibbs sampling can be used both for learning and prediction  Here  we only use it for learning to simplify the comparison to other methods     n  ef  st  t     bt         T  Y     Z s  n   t           eg bt   bt    t      n where Z s  t   is a normalizing constant  f  s  n t   bt   is a state feature function  possibly vector valued  with the corresponding weights   and g bt    bt    is a transition feature function with the corresponding weights   Intuitively  the feature functions allow to incorporate which observations and labels are likely to occur together  Usually  the feature functions are kept fixed and the weights are learned from training data  Given a sequence of labeled training data  the most common approach to find the model weights is minimizing the negative log likelihood  Writing  for the stacked weights       the objective function is given by  L         T X t    T X     f s  n   b    g  bt    bt   t t t      T    n   log Z s  T             where the last term on the right hand side is a shrinkage prior to penalize large weights  In our experiments   Table    CRF results for Experiment   using center of pressure  COP  features  Window size is     Observations are accelerometer measurements  speed  frontal and sagittal center of pressure and total weight  Overall accuracy is      NTW  ST  WF  TL  TR  WB  RT  SW  GUR  GDR  GUC  GDC  Accuracy    NTW                                                 ST                                                           WF                                                         TL                                                        TR                                                          WB                                                  RT                                                        SW                                                   GUR                                                     GDR                                                    GUC                                                    GDC                                                  we chose         However  we found that scaling    by factors up to    and       does not yield big differences in the accuracy of the resulting models  Since the objective function is convex  its unique minimum can be found using gradient based search  The term Z s  n   t    which also depends on   can be efficiently evaluated using dynamic programming      We use conjugate gradients to minimize the negative log likelihood and stop training after     iterations  To predict behaviours  we consider the speed of the walker and the acceleration in x  y and z direction  Instead of using the raw data of the load sensors  we consider the following measurements  the total load  which is just the sum of the loads on each wheel   the frontal plane center of pressure  which is the difference between the loads on the left and on the right side divided by the total load  and the sagittal plane center of pressure  the difference between the loads on the rear and front wheels divided by the total load   Our state feature functions are based on thresholding  for each pair of behaviours b  b  and each sensor k  we compare the actual observation to a fixed threshold value  If the value is exceeded  we add some model  e   n  weight bb  k   otherwise  we add some weight bb  k   We choose the threshold values manually by a visual inspection of the data  If the labels b  b  cannot be well discriminated by looking at the data from sensor k  the threshold is chosen as the average value from sensor k  later on  such irrelevant thresholds will be given very low weights in the training of the model  We use a very simple transition model to avoid a bias towards certain transitions due to the design of the walker course  In particular  the transition feature function is given by g bt    bt      bt    bt    hence the corresponding weight  is a scalar   Given the model parameters and an observation sequence  the predicted behaviour sequence b  T maximizes the a posteriori probability of B  T   b  T    n   arg max P  B  T   b  T   S  T   s  n   T   b  T  where the maximization is over all label sequences of length T   Note that b  T can be computed efficiently similar to the Viterbi algorithm for HMMs      Results  We present results for the HMM and CRF for both experiments  We use leave one out cross validation for each round of training and prediction  Specifically  if we want to predict the behaviour sequence for a certain participant in Experiment    we learn the parameters from all other participants in Experiment    For Experiment    each person goes through the course twice  We included one instance of the course in the training data along with data from other participants while testing for the same person  Since the range of sensor readings is too large  all integers from   to          we divide the range into    intervals and set their length in such a way that the same number of readings fall into each interval  Since the participants weight varies  we also normalize the load cell readings as follows  normalizedV alue    value  min   max  min   Ground truth is established by hand labeling the data based on the video  This process is not perfect since the labeler can make mistakes while identifying behaviour transitions  For example  the labeler may interpret that a left turn started at time t  while the turn may actually start some time before t  but it only becomes evident in the video at time t  Therefore  in order to calculate our error  we introduce the concept   Table    Experiment   percentage accuracy for each behaviour  NL means the normalized load values are used  COP implies that center of pressure feature is used instead of normalized load values  LOD means observation model is learnt from data and transition model uses  for behaviour persistence  LOTD means the observation  transition and prior probabilities are learned from data  Window size is     Learning  Accuracy NTW  ST  WF  TL  TR  WB  TRS  Total  Supervised HMM NL                                  Supervised HMM COP                                  Supervised HMM LOTD NL                                  Supervised CRF                                  Unsupervised HMM EM                                  Unsupervised HMM Gibbs                                   Table    Experiment   percentage accuracy for each behaviour  NL means that normalized load values are used  COP implies that center of pressure feature is used instead of normalized load values  LOD means that the observation model is learnt from data and the transition model uses  for behaviour persistence  LOTD means the observation  transition and prior probabilities have been learned from data  Window size is    Learning  Accuracy NTW  ST  WF  TL  TR  WB  RT  SW  GUR  GDR  GUC  GDC  Total  Supervised HMM NL                                                      Supervised HMM COP                                                      Supervised HMM LOTD NL                                                      Supervised CRF                                                    Unsupervised HMM EM                                                     Unsupervised HMM Gibbs                                                     of window size  If the window size is x  then for the behaviour at time t  if we find the same behaviour in the window between time t  x and time t   x in the predicted sequence  we count it as a correct prediction  We vary our window size from   to    in intervals of    If we make a correct prediction within a window width of     then we are only off by half a second  Since older people perform behaviours at a rate that is much slower than half a second  this may still be considered accurate  Tables   and   show the results in the form of confusion matrices for Experiment   when performing supervised learning with an HMM and a CRF  In both cases  accelerometer measurements  speed  frontal and sagittal center of pressure  COP  and total weight are used as observations instead of the raw measurements  see Sect      for more details   Each entry at row i and column j indicates how many times behaviour i was confused as behaviour j  assuming a window of size     Due to a lack of space  we did not include the confusion matrices for Experiment   and for the unsupervised learning algorithms  however Tables   and   summarize the recognition accuracy of all the algorithms   for each  experiment  We define accuracy as PT t    bt   bt  T   Note that random predictions  would yield an accuracy of           in Experiment   and           in Experiment    In some situations  identifying transitions from some behaviour to another is what really matters  Hence  Fig    shows the precision and recall of behaviour transitions  in addition to recognition accuracy  as a function of the window size  We calculate the  number   of actual transitions  i e   PT AT   t    bt    bt     the number of pre  P T     dicted transitions  i e   P T   t    bt    bt    and the number of correctly predicted transitions  i e     PT CP T    b     b   b   b   b   b    t t  t t t  t  t   Then precision   CP T  AT and recall   CP T  P T       Discussion  In this section  we analyze the results presented in the previous section  Experiment   vs  Experiment    It is obvious from Tables   and   that the overall accuracy is much higher for Experiment    Difficult behaviours such as TR  TL and TRS are also predicted accurately  In fact  the accuracy for TRS is much higher than expected  One reason for the high accuracy may be that in Experiment    each person executes the course twice   Figure    Accuracy  Precision and Recall for various algorithms in each experiment  and one instance of the course execution is included in the training data while testing for the same person  Therefore  the training data may include information that is more specific to the particular person e g   how people put load on the walker for different behaviours  Secondly  for Experiment    behaviours such as NTW  ST  WF and WB are easily distinguishable from each other  Confusion is only likely between WF  TL and TR and between ST and TRS  There is a larger number of behaviours in Experiment   that are difficult to distinguish from each other based on sensor information e g  WF  TL  TR  GUR  GDR  GUC and GDC  Similarly it is difficult to distinguish between RT and ST  Center of Pressure  COP  vs  Normalized Load Values  In addition to the accelerometer values and the walker speed  we considered two alternative sets of features for prediction  One set includes the normalized load cell measurements while the other includes the frontal plane COP  the sagittal plane COP and the total load on the walker  It is evident from Tables   and   that both sets of features predict different behaviours well  When the COP features are used  we note that the prediction accuracy is lower for TR  TL  GUR  GDR  GUC and GDC and higher for the other behaviours  CRF vs  HMM  Tables   and   show that the total accuracy of the CRF is much higher than that of the HMM  This is largely due to the fact that the CRF models Pr Behaviours Observations  directly and optimizes the parameters of this distribution  On the other hand  the HMM models Pr Observations Behaviours  as well as Pr Behaviours   and then uses Bayes rule to calculate  Pr Behaviours Observations   Therefore  the HMM model is more complex and the number of parameters that we have to learn is larger  Also  the HMM makes an explicit assumption about the conditional independence of sensor measurements over time  The CRF avoids this  problematic  assumption since it does not model any distribution over the observations  However  techniques for unsupervised learning are better established for HMMs than CRFs  This becomes an important advantage since we do not need to label data in unsupervised learning  We were surprised to see that the HMM was able to predict certain behaviours better than the CRF  In general  the HMM seems to favor behaviours that occur infrequently  For example  in Table    we can see that the prediction accuracy of GUR and GUC is higher than that of WF  We expected that it would be difficult to accurately predict infrequent behaviours such as GUR and GUC  Note also that WF is often predicted as GUR and GUC  We suspect that this is due to the assumption of conditional independence between different sensors  Learning Transition Model from Data  As discussed in Sect     the experiments include a pre defined walking course that biases the behaviour transitions  This is why we considered two scenarios when learning an HMM  i  fixed transition model where each behaviour is  times more likely to persist than to transition to some other behaviour with uniform probability  denoted by LOD in Tables   and    and ii  learned transition model  denoted by LOTD   Naturally  the accuracy is higher when the transition model is learned from data  In future work  we plan to collect data with participants in their daily activities instead of a   scripted walking course  which will allow us to learn realistic and personalized transition models  ML vs  Bayesian Learning  As explained previously  manually labeling the data is a time consuming and error prone  Unsupervised learning algorithms avoid this problem  However  they take longer to converge and the solutions are usually approximations to the optimal parameters  It is interesting to note from Table   that for Experiment    the Gibbs sampling accuracy for NTW and ST is actually higher than other algorithms  One important difficulty with unsupervised learning is state matching  On one hand  unsupervised learning algorithms may pick sub behaviours of composite behaviours as a state  For example  the ramp transition can be broken up into getting on the ramp  corresponding to a blip in the vertical acceleration   and walking on the ramp  The algorithm may find a state that matches either one or both instead of the behaviours going up down ramp  On the other hand  if two behaviours are similar  the algorithm might treat them as the same state  We observe that in Table    TRS is almost never predicted correctly  This may be due to the fact that TRS is very similar to ST and hence they are merged into one state  For EM and Gibbs sampling  once a model is learned  we manually associate each latent state with the behaviour that is the most frequent  In general we used a number of latent states equal to the number of behaviours  except for Gibbs sampling in Experiment    Table    where we used more latent states      than behaviours      As a result  the accuracy improved  In future work  we would like to investigate more thoroughly whether using a larger number of latent states generally improves the accuracy  Since EM often gets stuck in local optima  we did    random restarts and showed the results of the model with the highest likelihood  In contrast  Gibbs sampling does not suffer from this problem  It is evident from Tables   and   as well as Fig    that Gibbs sampling performs better than EM      Conclusion and Future Work  This paper presented a novel and significant application of activity recognition in the context of instrumented walkers  We designed several algorithms based on HMMs and CRFs  and tested them with real users at the Village of Winston Park  retirement community in Kitchener  Ontario   A comprehensive analysis of the results showed that behaviours associated with walker usage tend to induce load  speed and acceleration patterns that are sufficient to distinguish them with reasonable accuracy  In the future  we would like to further improve the recognition accuracy  by using  the video data and exploring various feature extraction techniques  We are also working on improving the battery life and memory capacity of the walker to collect data over long periods of time  In particular  this will allow us to loan the walker to users  record their daily usage and learn realistic and personalized behaviour transition models  Finally  we hope to turn this work into a clinical tool that can be used to assess the mobility patterns of walker users and the contexts in which they are more likely to fall  Acknowledgments This work was supported by funds from CIHR  NSERC  the Ontario Ministry of Research and Innovation  Pascal Pouparts ERA  and the Canadian government  Mathieu Sinns postdoctoral fellowship   We also thank the UW Schlegel Research Institute for Aging and the Village of Winston Park as well as all the volunteers who participated in the experiments   
 We consider the problem of approximate belief state monitoring using particle filtering for the purposes of implementing a policy for a partially observable Markov decision process  POMDP   While particle fil tering has become a widely used tool in AI for monitor ing dynamical systems  rather scant attention has been paid to their use in the context of decision making  As suming the existence of a value function  we derive er ror bounds on decision quality associated with filtering using importance sampling  We also describe an adap tive procedure that can be used to dynamically deter mine the number of samples required to meet specific error bounds  Empirical evidence is offered supporting this technique as a profitable means of directing sam pling effort where it is needed to distinguish policies      Ortiz  Computer Science Department  Introduction  Considerable attention has been devoted to partially observ able Markov decision processes  POMDPs       as a model for decision theoretic planning  Their generality allows one to seamlessly model sensor and action uncertainty  uncer tainty in the state of knowledge  and multiple objectives          Despite their attractiveness as a conceptual model  POMDPs are intractable and have found practical applica bility in only limited special cases   The predominant approach to the solution of POMDPs in  volves generating an optimal or approximate value func tion via dynamic programming  this value function maps beliefstates  or distributions over system states  into opti  University ofToronto Toronto  ON M S  H  cebly cs  toronto  edu  are considerably more pressing   One important family of approximate belief state monitor  ing methods is the particle filtering or sequential Monte Carlo approach          A belief state is represented by a random sample of system states  drawn from the true state distribution   This set of particles is propagated through the  system dynamics and observation models to reflect the sys tem evolution  Such methods have proven quite effective  and have been applied in many areas of AI such as vision       and robotics       While playing a large role in AI  the application of particle filters to decision processes has been limited  While Thrun       and McAllester and Singh      have considered the use of sampling methods to solve POMDPs  we are unaware of studies using particle filters in the implementation of a POMDP policy  In this paper we examine just this  focus ing on the use of fairly standard importance sampling tech niques  Assuming a POMDP has been solved  i e   a value function constructed   we derive bounds on the error in de cision quality associated with particle filtering with a given  number of samples  These bounds can be used a priori to determine an appropriate sample size  as well as forming  the basis of a post hoc error analysis  We also devise an adaptive scheme for dynamic determination of sample size  based on the probability of making an  approximately  op timal action choice given the current set of samples at any  stage of the process  We note that similar notions have been applied to the problem of influence diagram evaluation by Ortiz and Kaelbling      with good results our approach draws much from this work  though with an emphasis on the sequential nature of the decision problem  A key motivation for taking a value directed approach to sampling lies in the fact that monitoring is an online pro  mal expected value  and implicitly into an optimal choice of action  Constructing such value functions is computa tionally intractable and much effort has been devoted to de  cess that must be effected quickly  One might argue that if the state space of a POMDP is large enough to require  veloping approximation methods or algorithms that exploit specific problem structure  Potentially more troublesome is  sampling for monitoring  then its state space is too large to hope to solve the POMDP  To counter this claim  we note  the problem of  first that recent algorithms        based on factored repre sentations  such as dynamic Bayes nets  DBNs   can of ten solve POMDPs without explicit state space enumeration and produce reasonably compact value function representa  beliefstate monitoring maintaining a be  lief state over time as actions and observations occur so that  the optimal action choice can be made  This too is gen erally intractable  since a distribution must be maintained over the set of system states  which has size exponential in the number of system variables  While value function con struction is an offline problem  belief state monitoring must be effected in real time  hence its computational demands  tions  Unfortunately  such representations do not generally  While techniques exist for generating finite state controllers for POMDPs  there are still reasons for wanting to use value function based approaches               POUPART ET AL   UAI           ptimat Value Function  translate into effective  exact  belief monitoring schemes      Even in cases where a POMDP must be solved in a traditional  fiat  fashion  we typically have the luxury of compiling a value function offiine  Thus  even for large POMOPs  we might reasonably expect to have value func tion information  either exact or approximate  available to direct the monitoring process  The fact that one is able to produce a value function ojfiine does not imply the ability  to monitor the process exactly in a timely  B lief Spocc  online fashion   We overview PO MOPs  structured solution techniques  and  Figure    Geometric View of Value Function  We also describe a dynamic sample generation scheme that relies on ideas from group sequential sampling  We exam  observation  and rr  is itself a conditional plan  Intuitively   monitoring in Section    Section   describes a basic par ticle filtering scheme for POMDPs and analyzes its error   ine this model empirically in Section    and conclude with  a discussion of future directions      POMDPs and Belief State Monitoring       Solving POMDPs  A partially observable Markov decision process  POMDP   is a general model for decision making under uncertainty  Formally  we require the following components  a finite state space S  a finite action space A  a finite observation  space Z  a transition function T     S  x  A   t   S     an  observation function     S x A    l   Z   and a reward function   S  t R  Intuitively  the transition function  T s  a    R  determines a distribution over next states when an agent takes action a in states  This captures uncertainty in action effects  The observation function reflects the fact that  an agent cannot generally determine the true system state  with certainty  e g   due to sensor noise   Finally notes the immediate reward associated with s   R s   de  The rewards obtained over time by an agent adopting a spe cific course of action can be viewed as random variables  R t   Our aim is to construct apolicy that maximizes the ex E Lo pt R t    pected sum of discounted rewards  where  J is a discount factor less than one   It is well known that an optimal course of action can be determined by consid ering the fully observable belief state MDP  where belief  states  l    S    distributions over  t  S   form states  and a policy  rr     A maps belief states into action choices  In prin  ciple  dynamic progranuning algorithms for MDPs can be used to solve this problem  A key result of Sondik       showed that the value function V for a finite horizon prob  em is piecewise linear and convex and can be represented as a finite collection of a vectors   Specifically  one can  generate a collection N of a vectors  each of dimension lSI   V b   such that   maXaeN ba  Figure   illustrates a collec tion of a vectors with the upper surface corresponding to We define ma  b    arg max  e  to be the maximizing a vector for belief state  b   ba  V   Each a E  corresponds to the expected value of executing an implicit conditional plan at a given be lief state  This conditional plan  rr  a   has the form  a  Ot        oz  rrz    On   rn   where a is an action  Oi is an  fl  X  denotes the set of distributions over finite set X     For infinite horizon problems  a finite collection may not al ways be sufficient  but will generally offer a good approximation   a plan of this form denotes the performance of action a fol lowed by execution of the remaining plan rr  in response to observation oi  We denote by A a  the  first  action a of  r a   Given belief state b  the agent should execute  the action with the maximizing a vector  A ma b    In deed  if one has access to the entire plan  IT ma b     this plan should be executed to termination  We note  however  that the plans  IT  cr   are rarely recorded explicitly   One difficulty with these classical approaches is the fact that the a vectors may be difficult to manipulate  A sys  tem characterized by  n  random variables has a state space  size that is exponential in  Thus manipulating a single  n   a vector may be intractable for complex systems   Fortu  nately  it is often the case that an MOP or POMDP can be  specified very compactly by exploiting structure  such as conditional independence among variables  in the system dynamics and reward function   I    Representations such as  dynamic Bayes nets  DBNs  can be used  and schemes have been proposed whereby the a vectors are computed directly in a factored form by exploiting this representation  Boutilier and Poole        for example  represent a vectors  as decision trees in implementing Monahan s algorithm  Hansen and Feng     use algebraic decision diagrams  ADDs  as their representation in their version of incre mental pruning  The empirical results in       suggest that  such methods can make reasonably sized problems solv able  Furthermore  factored representations will likely fa cilitate good approximation schemes        Belief State Monitoring  Given a value function represented using a collection N of a vectors  implementation of an optimal policy requires that one maintain a belief state over time in order to ap  ply it to N  Given belief state bt at timet  we determine at   A ma bt    execute at  make a subsequent obser vation otf l   then update our belief state to obtain bt l  The process is then repeated  Belief state monitoring is ef t fected by computing bt l   Pr SW  a   ot l   which in volves straightforward Bayesian updating  We denote by     T b  a  o   the update of any belief state  observation  o   We inductively define  T b a        an  on   T T  T b  a                                   b by  action  a  and            an    On l an  on   The number of a vectors can grow exponentially in the worst case  but can often be approximated    UAI       Even if the value function can be constructed in a com pact way  the monitoring problem itself is not generally tractable  since each belief state is a vector of size IS    Un fortunately  even using DBNs does not alleviate the diffi culty  since correlations tend to  bleed through  the DBN  rendering most  if not all  variables dependent after a time      Thus compact representation of the exact belief state is typically impossible  Belief state approximation is there fore often required  At any point in time we have an ap proximation   t of the true belief state bt  and must make our decisions based on this approximate belief state  While sev eral methods for belief state approximation can be used  in cluding projection  aggregation  and variational methods   and important class of techniques for dynamic problems is sampling or simulation methods     Particle Filtering for POMDPs  In this section we examine the impact of particle filtering on decision quality in POMDPs  We first describe a typical sequential importance sampling algorithm  and discuss the use of partial evidence integration  EI  in the DBN to help keep samples on track  We then analyze the error induced by one stage of belief state approximation and show how partial EI allows this analysis to be carried through multiple stages  in a way that is not possible otherwise        A       POUPART ET AL   Basic Filtering Method for POMDPs  Assume we have been provided with the value function for a specific POMDP M  This value function is represented by a finite collection of a vectors  We assume an infinite horizon model so that we have a single set   We also assume that N is of a manageable size  and that the vec tors themselves are represented compactly  using ADDs  decision trees  linear combinations of basis functions  or some other representation   We emphasize  however  that even if the value function is represented in standard state form  approximate monitoring is often needed  We note that our methods can be applied to approximate value functions  though our analysis assumes an exact setImplementation of the policy induced by this value function requires that a belief state bt be maintained over all times t  At any point in time we assume an approximation bt of the true belief state bt  and make our decisions based on this approximate belief state  The basic procedure we consider is the use of a particle filter for monitoring  with the approximate belief states so gener ated used for action selection in the POMDP  At any timet  we have a collection bt ofnt weighted particles  or system states  approximating the true distribution bt  Each particle is a pair  s i   w      We often simply write s    to refer to  the it    particle   i  nt   The total weight of the particle set bt is wt     L  w i  The particle set b  represents the following distribution  which we also refer to as bt      L  w i   s i     s  t b s     wt Given this approximation bt of b   action selection will take                        b    a   Figure    Partial Evidence Integration place in the POMDP as if b  were the true distribution  Thus  we let at   A ma b     execute action a   and make observation ot l  Our new approximate belief state t t l is generated by repeating the following steps until nt I is greater than some desired threshold      Draw a state s  from the distribution b   a state s    from the distribution Pr st  ls   a       Compute w    Pr o    st  at  st l     Add sample  s i   w i         st l w  to bt l and add w to total weight w     Draw     This sequential importance sampling procedure induces a consistent  though biased  estimate t t l of bt    and will converge to the true distribution according to the usual con vergence results  The significance of this method lies in the fact that  for a great many systems  it is easy to sample suc cessor states according to the system dynamics  i e   sample from the conditional distribution in Step     and to evaluate the observation probabilities for given states  i e   compute the weights in Step     In contrast  direct computation of Pr S  llb   at  o     is generally intractable       Evidence Integration  One difficulty with the filtering algorithm above is that the samples generated at time t     are not influenced by ob servation o     which often allows particles to drift from the true belief state  Since we assume a DBN representa tion of dynamics  partial evidence integration  El  or arc reversal     can be used to partially alleviate this problem       The generic structure of a DBN  assuming a fixed ac tion  is shown in Figure   a   reversing the arc from St l to ot I results in a network shown in Figure   b   With this structure  given a particle s  and observation ot l  a par   q  ticles    can be drawn directly  Of course  the reweighting  given ot l must now be applied to the particles in b   This gives rise to the following particle fli tering procedure used throughout the remainder of the paper   a  Given particle set b   select action A ma bt    and observe ot     b  Reweight  samples  s i   at  according  to  Pr o    st  a   and normalize to produce i     c  Draw some number of particles s   i  according to  l          POUPART ET AL    d  Sample particles  s     where R   is the range of values that can be taken by a  i e    given drawn prior particles  b  l and a    to produce t     s      Ra     is an approxi   o     in contrast to b   a     ol                 Note that the reweighted distribution mation of Pr  st I a   which represents Pr S        at    o    ia             UAI             maxs a s    mins a s      Given a particular confidence threshold of size  n  t  the accuracy of our estimate  When the DBN is factored  the arc reversal process can of  tage of the structure in CPTs represented as  say  decision trees or ADD  In this way  the usual exponential increase in  v        co    ten be fairly expensive  since it increases the connectivity of  the network  However  the reversal process can take advan  c  and a sample set  we can produce a  one sided  error bound c     on  The required sample size given error tolerance c and confi  dence threshold  table size with the number of added parents is often circum    for the estimation of  v   is   vented      We use structured arc reversal techniques in our       experiments        We can also bound the simultaneous confidence that each of  One Stage Analysis  As a precursor to bounding the error in decision quality associated with particle filtering  we consider the error in  duced by one stage of approximation only  and acting using  exact inference at all other stages   We first note the follow  our es ti mates of each a W      J   has  one sided  precision with     in Eq    and maximiz ing over all a  we obtain the sample size Nt c      probability  Decreasing J to  N t  t o    r  ea  N c  l  l    ing important fact regarding POMDPs        Let bt  ll be two belief states s t  maW    ma b    For any sequence of k observations and actions  T b  a  o    a  k l at k  and let bt k   t k T l    at   ot l       at k     ot k    Then maW k    ma bHk    with  arbitrary  nonoptimal behavior is bounded by h  then  way that b has the same maximizinga vector as b   then we  Theorem  Fact    This implies that  if we approximate     b   at timet in such a  will   a  choose the correct action at state t  and  b  choose  the optimal action at all subsequent stages if we monitor the process exactly  w r t   bt  at all subsequent stages   Now  assume we have been able to exactly compute have selected and executed action  at l  bt l   we can sample directly from the distribution  T bt l  a     o   an un bi as ed  b   using the  arc reversed  DBN to obtain  estimate b  of bt   We analyze the error associ  ated with selecting ana vector that has maximum expected value w r t   tion  is made with probability at least    o  if the error associated the one step approximation error is given by the following   particles  with exact monitoring used at all other stages of the process  then the error E  i e   difference in expected value of the policy implemented and the optimalpolicy  is bounded by  Here the error incurred is discounted by cess  Note that the error  easily bounded  b t and executing its conditional plan to comple  that point on    s i   be a collection ofnt state samples drawn fromb    The value of any  a  E  N applied to true belief state b   W J  a  b    Eb a s     v  is     h   s  denotes the value of a at state  s  such   each term  a  s i     i e   the s h  bt  is V   As  v  v   is a sample of this random variable  and the average of these is an unbiased estimate  of  We can apply  one sided  Hoeffding bounds to determine the accuracy of this estimate  Specifically   Pr V     V  c  Pr V       V  c        maxmaxa        f  m in     R   s        f   though simple domain analysis will generally yield much tighter bounds on  h   post hoc analysis on the choice of optimal choi ce has been made  a vector to determine if an  component of a  and Eb  denotes expectation with respect to distribution b  Thus the value of a can be viewed as a random variable whose expectation  w r t   h on nonoptimal behavior can be   rather loosely  using  One can also perform a  a where a    f t l to reflect the  fact that the approximation error occurs at stage t of the pro   or equivalently  acting using exact monitoring from  Let    If beliefstate I  is approximated with Nt   E  o      and made ob  at  Furthermore  assume that we can com Pr st  la     d  exactly  With these assumptions   servation  pute  Choosing the maximizing a vector using an approximate l   with sample size N    c  J  ensures that a  t optimal choice      e  n   n        e  n    n    with high probability  Assuming n   samples have been gen erated  let be the error level determined by Eq    using n t   this is generally tighter than the c used to determine sample size in Eq    since we are looking at a specific vector   Corollary     Let  btat     at   ma bt  and suppose that    r       bta t   Va EN    at   Then with probability at least    oar optimal policy will be executed  and our error is bounded by    UAI       POUPART ET AL   The parameter T represents the degree to which the value of the second best a vector may exceed the value of the best at b  in the worst case  Note that this relationship must hold for some T         If the relationship holds forT      i e   there is   separation between the maximizing vector and all other vectors at belief state b   then we are executing the optimal policy with probability at least     o and our error is bounded by j    oh         the probability with which no mistake is made before stage is at least       oj   t  Assuming a worst case bound of h on the performance of an incorrect choice  w r t  the opti mal policy  at any stage  which is thus independent of any further mistakes being made   we have expected error E on the sampling strategy where N  o    samples are generated at each stage  E is bounded as follows   t  Theorem     Multi stage Analysis  The analysis above assumes that once an a vector is cho sen  the plan corresponding to that vector will be imple mented over the problem s horizon  In fact  once the first action A  a  is taken  the next action will be dictated by re peating the procedure on the subsequent approximate belief state  Due to further sampling error  the next action cho sen may not be the  correct  continuation of the plan rr  a   Thus we have no assurances that the   optimal policy will be implemented with high probability  In what follows  we assume that our sample size and approximate belief state ll are such that T     at every point in time  i e   our approx imate beliefs always give at least   separation for the op timal vector   We discuss this assumption further below  We make some preliminary observations and definitions be fore analyzing the accumulated error         We first note that b     is an unbiased estimate of the distribution T bt  at  o     Though particle filtering does not ensure that b    is unbiased with respect to the true belief state b      our evidence integration pro cedure and reweighting scheme produce  locally  un biased estimates  To see this  notice that the distribu tion   obtained by reweighting b  w r t  o    corre sponds to exact inference assuming the distribution b  is correct for St   This exact computation is tractable precisely because of the sparse nature of this approxi mate  prior  on      Thus  the procedure for generat ing samples of st l using b  is a simple forward prop agation without reweighting  and thus provides an un biased sample of T  bt  at  o        Let us say that a mistake is made at stage t if ma b     is not optimal w r t  T b   a   o        In other words  due to sampling error  the approximate belief state i    differed from the  true  belief state one would have generated using exact inference w r t  b  in such a way as to preclude an optimal policy choice   We can now analyze the error in decision quality associated with acting under the assumption that T      Let stage t be the first stage at which a mistake is made  If this is the case  we have that ma   b         ma  T   b    a   o      for all k   t  By Fact I   this means that ma b     ma b   for all k   t  where b  is the true stage k belief state one would obtain by exact monitoring   Thus  if stage t is the first stage at which a mistake is made  we have acted ex actly as we would have using exact monitoring for the first t stages of the process  Since our sampling process produces an unbiased estimate b    ofT b   a   a     at each stage   The above reasoning assumes that T reaches zero at each stage of the process  a fact which cannot be assumed a pri ori  since it depends crucially on the particular  approxi mate  belief states that emerge during the monitoring of the process  Unfortunately  strong a priori bounds  as a simple function of and J  are not possible if T     at more than one stage  The main reason for this is that the conditional plans that one executes generally do not correspond to a vectors that make up the optimal value function  Specifi cally  when one chooses aT optimal vector  for some     T    e  at a specific stage  a  worst case  error ofT is intro duced should this be the only stage at which a suboptimal vector is chosen  If a T optimal vector is chosen at some later stage  T       the corresponding policy is r optimal with respect to a vector that is itself only approximately op timal  Unfortunately  after this second  switch  to a subop timal vector  the error with respect to the original optimal vector cannot be  usefully  bounded using the information at hand   However  even without these a priori guarantees on deci sion quality  we expect that in practice  the following ap proximate error bound will work quite well  specifically as a guide to determining appropriate sample complexity  as discussed below   E                     e hj J                      J         Intuitively  at each stage of the process a  e  optimal vec tor will be chosen with high probability  Though we cannot ensure this  in practice we expect that the cumulative error over those stages where mistakes are not made can be use fully estimated by the first term  The second term accounts for the possibility of mistakes  as in Theorem    Here a mis take refers to the probability   J event of choosing a vector at a specific stage that is not   optimal  We also note that a post hoc analysis like that described for one stage analysis can be used to bound error  Proposition   Let t be the first stage of the process at which T      and t   k be the second such stage  Then  hj J   t t e  t k l h p             f    f    E   The first term in this bound denotes the error associated with mistakes  The second term reflects the  e  bound on er In particular  it is not the case that the error is bounded by  r           POUPART ET AL        ror associated with the first switch to an approximately op timal vector at stage t  while the third reflects the second switch  The main weakness in the bound again lies in this last term and its reliance on h to bound error after a second switch  One way in which these bounds can be strengthened is through the use of switch set analysis  a technique de scribed in       The set of constraints imposed by the sam pling scheme on the true belief state are linear and a priori error bounds can be computed by  dynamic programming   Details are beyond the scope of this paper        Dynamic Sample Generation  The analysis above allows us to determine a priori the sam ple complexity required to achieve a certain error with a specified probability  Our objective is ultimately to be rea sonably sure we choose the correct  maximizing  cr vector at each stage of the process  The method above ensures this by requiring that is estimated reasonably precisely for each cr  The post hoc analysis of value separation suggests that great precision is not needed if the vectors are widely separated at the true belief state  specifically  if the best vec tor has value much greater than the second best  Draw ing on ideas from the literature on group sequential meth ods       and multiple comparisons with the best  MCB       that analyze decision making from this perspective  we describe a method that at each stage generates samples dynamically  using a sampling plan whose termination de pends on results at earlier stages of the plan  The method is inherently simple  we will take samples in batches until we can select an cr vector satisfying certain requirements  Our method recalls the application of MCB results and group se quential methods by Ortiz and Kaelbling to influence dia grams  see      for details and further references    V  UAI      to Eq    using        IXI as the individual confidence pa rameter and nt   m  as the number of samples  Defining r  as above  and combining a lower bound for o i with an upper bound for all the others  we have  Pr V         max V  a ta    r                 If r   r  is nonpositive  cri is the optimal vector with prob ability at least     o   In general  if we stop immediately after processing the first batch and select cri  the error in curred will be at most max O  r         C t     max   c        If we are unsatisfied with the precision r achieved  we gen erate a second batch of m  samples  and propose that  Pr V        amax V a  t     r                     This bound holds if we insist beforehand that we will gen erate the second batch  but it ignores that fact that we gen erate this batch only after realizing our stopping condition was not satisfied using the first batch  This dependence on the bound resulting from the first batch since these bounds are random variables  this means we do not know a priori whether we will generate a second batch requires that we correct for multiple looks at the data  We do this by insist ing that both bounds hold jointly  conjoining the bounds ob tained after two batches using the Bonferroni inequality and letting T   mi n   j IJ         a  a     Tj      V   r  Pr V   aa raJ            a    Hence  if we stop after processing at most   batches  then our error in selecting cr  will be at most max   O   r   with probability at least           o    Applyi ng this argument up to k batches  we obtain  Suppose we are trying to select the maximizing cr vector  at stage t  using belief state  bt  The basic structure of our dynamic approach requires that we generate samples from  T  t  at  at    in batches  each of some predetermined size   To generate the jth batch    a  we determine a suitable confidence parameter  j  b  we generate the jth batch of mj samples from  T bt  at  ot l    c  we compute estimates v  j  for all vectors cr based on the samples in all j batches  correspond ing precisions E     j   and let be the vector with  greatest value  v  j   crj   d  we compute threshold        maXa ta  V  j  J  V   j    Ea  j  J  J    E     j   and terminate if Tj  reaches a certain stopping criterion We now elaborate on this procedure  We use MCB results to obtain confidence lower bounds  or one sided confidence intervals  on the difference in true value between that of the vector with largest value estimate with respect to all the samples in the batches so far and the best of the other vectors  Suppose m  samples are gener ated in the first batch  Given simultaneous confidence pa rametero   we obtain the one sided boundsc a j  according       Pr V    v    r  k  where  r    k       L oi  j l  min jljl  aj a   Tj   The method  as described above will stop at  the first batch          at this point we are assured of select ing the optimal vector with high probability  If we insist that we force  r to zero  the number of batches k cannot be  l such that Tj  bounded  thus  we must set the sequence of confidence pa rameters oj such that   OJ      o  For example  we might   Lj   set  J   of j j       and the individual confidence pa rameters as Oj  llX   If there is separation between the value of the optimal vector and the second best  the process will stop after a finite number of batches  Hence  we can con tinue the process until  r   o     However  since the error in the individual estimates decreases only proportionally to     ln j  j  termination might take longer than we wish  de pending on the amount of separation and the vector value variance  This problem is exacerbated by our use of loose ranges in the computation of the precisions e  a  j    J  If we impose a limit B on the number of batches  and want to make sure that our assessment of r holds with proba bility at least   o  we need to set the sequence of con      fidence parameters  OJ  such that   Lf   OJ        o   The easi  est way to accomplish our global confidence requirements   UAI       POUPART ET AL   o  B   o  Furthermore  if we want to be sure is to set i   that the method selects a vector with true value that is no less than  t  from the optimal with the same confidence   Problem  j  to the  r maxa R   Be    In  BII o l   State Space Size  o ee Widget Pavement  then one alternative is to set the number of samples mj in each batch       If  we do not impose specific requirements then the setting of             Size ofN maximum average                       mj is arbitrary  but needs to be fixed in advance  This is  because for our analysis to hold  mj cannot depend on the outcomes from the samples themselves  Although arbitrary   in general  the setting of mi should take into consideration  a trade off between reducing the expected total number of samples before the method stop versus reducing the varia tion on the total number of samples  In general  we expect this method to be effective when there  is sufficiently large gap between the best vector and the rest  and or the ranges in vector values are sufficiently small rel ative to the value separation and the error tolerance  By us  ing loose upper bounds on the variances and accuracy pa rameters  the theoretical bounds can become very loose  and  hence do not reflect the potential gains we expect  The ver sion presented in this paper is very simple  Many variations  on the same idea are possible to try to bring the theoret ical bounds more in accordance with our belief about the  expected behavior of the method  for instance  using infor mation about range of differences in value between vector pairs  allocating some samples to estimate variance  etc     but this is beyond the scope of this paper   As before  unless we push the error tolerance  r  to zero at  each stage of the monitoring process  we cannot obtain tight bounds on error after that point  However  we can assert   Suppose beliefs are monitored according to the dynamic procedure described above using global con fidence parameter o  Furthermore  suppose that r     at each stage  Then error E satisfies Theorem    E   hf o       f    fJo  However  as noted above  the computational demands of  insisting that r     can be severe if the belief state at some time t is such that little separation exists between the  best vector and the second best  that is  if  l   lies close to a   edge  of the value function  where two optimal a vectors intersect   If r s   at all stages up to timet  then the bound described in Proposition   holds for this dynamic scheme      Empirical Evaluation  Three test problems were used to carry out experiments test ing the efficacy of our sampling procedures  we refer to        for the full specification of those problems  see also      for  a summary   Each of the three problems was solved using Hansen and Feng s     ADD implementation of incremental pruning  IP  to produce a set N of a vectors using a compact  ADD representation   In the following experiments  we report on the use of sam  pling for approximate belief state monitoring on three test problems  The goal of the experiments are twofold  to evaluate  i  the impact on decision quality induced by sam pling techniques and  ii  the sample complexity necessary  Table    Statistics for the three test problems  T he maxi mum and average size of  are taken over a    stage pro cess   to guarantee some level of decision quality  Note that the experiments do not evaluate the running time of sampling methods since that is not the focus of this paper and the ef  ficiency gains of such methods have already been clearly  demonstrated  II       In theory  exact monitoring has time complexity on the order ofO ISI   whereas sampling has a  time complexity in the order ofO m log lSI   m is the num ber of samples   T hus  a sampling strategy provides time savings when m    ISI  log lSI   The reader should also  be warned that the scope of the empirical evaluation was limited to test problems for which a set of a vectors cor responding to an optimal value function can be computed  Hence  as shown in Table    lSI and INI are fairly small  and consequently the following experiments should be consid ered preliminary   The first experiment compares the expected loss incurred by sampling methods to that of a random monitoring ap  proach  More precisely       initial belief states are picked uniformly at random and for each initial belief state  the op  timal expected total reward is compared to the cumulative  rewards earned by an agent that approximately monitors its belief state over    stages  The difference between the opti  mal expected total return and the actual return is the loss due to approximate monitoring  Table   shows the average loss due to a single approximation at the first stage  assuming exact monitoring for the remaining    stages   whereas Ta  ble   shows the average cumulative loss due to approximate  monitoring at each of the    stages  When doing random  monitoring  the agent picks a belief state at random  uni formly  and executes the optimal action for this random be  lief state  This random method can be viewed as a naive strategy that any other approximation method should be  able to beat  The sampling methods implemented are basic particle filtering  with partial evidence integration  where a  fixed number of particles             or      are sampled for each approximate belief state  The column  worst  re ports the worst possible expected loss that can be achieved by consistently choosing the worst actions   The worst ex pected loss is included to give some idea of the scale of po tential losses due to approximate monitoring   As expected  the experiments show a gradual decrease in average expected loss as the number of samples increases   When compared to the random strategy  and considering the range of values obtainable across the set of possible be haviors   sampling methods perform quite well  In Table       This worst strategy can be computed by minimizing  in stead of maximizing  the expected total reward while solving the POMDP         POUPART ET AL   P rob  Rand  Average Single Error Sampling                       Table    Comparison of the average error due to a single approximation at the first stage of a    stage process  exact monitoring being performed for the remaining    stages   Prob  CofiWidg Pav  Rand                       Average Cumulative Error Sampling                                                                                           Prob  Worst  Worst                    Table     Comparison ofthe average cumulative error due to approximate monitoring at each stage of a I S stage process   the first row of each problem indicates the actual error in curred and the second row indicates the upper bound  c pre dicted by the theory  for              This bound is loose when compared to the actual error due to the worst case nature of the analysis  The bounds may still provide some guidance regarding the amount of sampling desired to reduce the av erage expected loss to some suitable level  assuming a more or less constant ratio between the bounds and the actual er ror   In a second experiment  we evaluate the benefits of dynam ically determining the amount of sampling  For given o and    we evaluate the total number of samples necessary to guarantee that the one stage sampling error is bounded by  c with confidence     o  Table   shows how this total num ber   f samples varies as we increase the maximum number of batches  Once again       random initial belief states are chosen and the average number of samples required to decrease r below  c is reported  The column for   batch corresponds to the standard non dynamic sampling proce dure  Table   reveals that for the widget and pavement prob lems  a dynamic sampling procedure can reduce the sam pling complexity quite dramatically for a well chosen max imum number of batches  Unfortunately  the dynamic ap proach does not appear to have offered any savings in the coffee problem  Further investigation is necessary to assess the optimal  maximum  number of batches in general  In a related paper         Poupart and Boutilier also tackle the belief state monitoring problem  but using a vector space method that exploits conditional independence  The idea is to repeatedly approximate belief states using projections as initially proposed by Boyen and Koller      Projec tion schemes and sampling approaches differ in many as pects including the properties of POMDPs for which they  UA                                                               Table    Comparison of the average number of samples re  quired for adaptive sampling at the first stage of a I S stage process  J         and E     for coffee and pavement  J         and c       for widget    are most suitable  Sampling methods exploit the sparsity of belief distribution whereas projection schemes exploit conditional independence  Given that the coffee  widget and pavement problems are factored POMDPs  the vector space methods tend to perform better than sampling with respect to decision quality  For instance  average losses due to single stage approximation using the max VS search method are respectively                        for the cof fee  widget and pavement problems  similarly  the average cumulative losses over     stages are respectively                  and           However  the computational overhead associated with sampling is minimal while the overhead as sociated with choosing good projection schemes is nontriv ial  We expect the two approaches can be combined in fruit ful ways  as we discuss below      Concluding Remarks  Our value directed sampling technique can be seen as ap plying methods from the MCB and group sequential sam pling fields to the problem of particle filtering for POMDPs  We are able to derive  worst case  error bounds on such an approach  and use these bounds to suggest methods to direct sampling in such a way as to choose optimal actions rather than  necessarily  accurately estimate their values  Our ini tial empirical results are encouraging  though clearly much more substantial testing is needed  a task in which we are currently engaged  This research can be extended in a number of ways in a number of very interesting ways  One important challenge is to provide a stronger analysis of error when the precision parameter r      One strategy to circumvent this diffi culty builds on the idea of constructing the set of alternative conditional plans that may be executed when r              Another challenge is to provide an analysis i n the absence of partial EI  which locally removes bias   one idea is to use information from the DBN parameters to compute a pri ori error bounds  another is to use absolute approximation bounds similar to those used in this paper or optimal rela tive approximation methods to obtain a posteriori bounds on the error tolerance r  We are very interested in adapting these techniques to other value function representations  e g   grid based value func tions  and providing an error analysis of this method when the value function is itself an approximation of the true value function  Finally  previous work using value directed projection schemes          has been used successful ly to ex    POUPART ET AL   UAI       ploit the conditional independence present  in certain fac  tored POMDPs to speed up belief monitoring  The sam pling approach described in this work does not exploit this type of structure  however  one could sample the variables defining the factored state space in a  stratified  fashion  or apply Rao Blackwellisation methods         Acknowledgements    Boutilier and Poupart were sup  ported by the Natural Sciences and Engineering Research Council and the Institute for Robotics and Intelligent Sys tems   Ortiz was supported by NSF IGERT award SBR           
  We propose a new approach to value directed be lief state approximation for POMDPs  The value directed model allows one to choose approxima tion methods for belief state monitoring that have a small impact on decision quality  Using a vec tor space analysis of the problem  we devise two new search procedures for selecting an approxi mation scheme that have much better computa tional properties than existing methods  Though these provide looser error bounds  we show em pirically that they have a similar impact on deci sion quality in practice  and run up to two orders of magnitude more quickly     Introduction  Partially observable Markov decision processes  POMDPs  have attracted considerable attention as a model for decision theoretic planning  Their generality allows one to seamlessly model sensor and action uncertainty  uncer tainty in the state of knowledge  and multiple objectives         Their computational intractability has  however  limited their practical applicability            An important  approach to POMDPs involves constructing a value function for a beliefstate MDP offline  and maintain ing a belief state  or distribution over system states  online  which is used to implement an optimal policy       Anum ber of approaches attacking the offline computational prob lems have been studied  including improved algorithms      the use of factored representations         as well as numer ous approximation schemes      Little work has focused on the online belief state monitoring problem  Because plan ning state spaces grow exponentially with the number of variables  maintaining an explicit distribution over states is generally impractical  Even when concise representa tions such as dynamic Bayes nets  DBNs  are used  moni toring is generally intractable  since the independencies ex ploited by DBNs vanish over time  Boyen and Koller     proposed projection schemes for approximate monitoring   cebly cs toronto edu  essentially breaking weaker correlations among variables to ensure tractability  Poupart and Boutilier      proposed value directed methods for approximation  allowing the an ticipated loss in expected utility guide the choice of approx imation scheme  In this paper we pursue the value directed approach since its emphasis on minimizing impact on decision quality is a critical factor in devising useful approximations  We use the value function itself to determine which correlations can be  safely  ignored when monitoring one s belief state  We propose an alternative approach to choosing approximation schemes for monitoring in POMDPs that overcomes many of the computational bottlenecks of       We introduce a vector space formulation of the approximation problem that allows one to construct approximation schemes with looser error bounds  but much more quickly  Despite the looser bounds  we show empirically that decision quality is rarely worse than that obtained using the more intensive ap proaches  Our methods work in time roughly on order of the time taken to solve a POMDP  and since they run of fline  they can be used with any POMDP technique that can currently be applied  Furthermore  these methods take ad vantage of the factored  DBN  representations to avoid state enumeration  The offline cost allows much faster  approxi mate  online policy implementation  Even in cases where a POMDP must be solved in a traditional  flat  fashion  we typically have the luxury of compiling a value function offline  Thus  even for large POMDPs  we might reason ably expect to have value function information  either exact or approximate  available to direct the monitoring process  The fact that one is able to produce a value function offline does not imply the ability to monitor the process exactly in a timely online fashion   Finally  our model offers a novel view of the approximation problem for belief state monitor ing for POMDPs  We briefly overview POMDPs and value directed approx imation in Section    We present our vector space formu lation in Section   and provide some suggestive empirical    While techniques exist for generating finite state controllers for POMDPs  there are still reasons for wanting to use value function based approaches              POUPART   BOUTILIER  UAI      results in Section        POMDPs and Belief State Monitoring  The key components of a POMDP are  a finite state space  S  a finite action space A  a finite observation space Z  and a reward function  R     S    R  Actions induce stochastic  state transitions with specified probabilities  and an agent is provided with noisy observations of the system state  with specified probabilities   A reward is received at each state  and an agent s objective is to control the system through ju  dicious choice of action to maximize the expected reward obtained over some horizon of interest       Figure  The rewards obtained over time by an agent adopting a spe  The Switch Set  Sw o     of a   cific course of action can be viewed as random variables  R t l   Our aim is to construct a policy that maximizes the ex  pected sum of discounted rewards  E l  o  l R t    where    is a discount factor less than one   An optimal course  of action can be determined by considering the fully ob  servable beliefstate MDP  where beliefstates  distributions  over S  form states  and a policy   r     B      A maps  belief states into action choices  A key result of Sondik      showed that the value function V for a finite horizon problem is piecewise linear and convex and can be rep resented as a finite collection of a vectors  for infinite  lief states can be maintained by standardBayesian methods  but when lSI is large  the cost is prohibitive  This is espe cially true when S is determined by a set of variables X  and lSI           IXI     In such cases  DBNs can be used to rep  resent the dynamics ofPOMDPs and DBN inference tech niques that exploit conditional independence among vari ables can be applied to make monitoring more efficient  Un fortunately  as shown by Boyen and Koller      in many problems most if not all variables of DBNs tend to become  horizon problems  a finite collection generally offers a good  correlated over time so DBNs offer no significant savings   approximation  Specifically  one can generate a collection  Boyen and Koller introduced projection schemes as a  N  of a vectors  each of dimension lSI  such that  maxaEI    b a   V b           In Figure   the value function is given by  method to approximate belief states   Given variables X  defining S  a projection is a setS of subsets of X with each  the upper surface of the five vectors shown  Each vector  variable in at least one subset  Correlations among vari  is associated with a specific  course of  action  For finite  ables within a subset are preserved while the subsets are as  horizon POMOPs  a set N  sumed to be independent  For instance  if X         k is generated for each stage k of   A  B  C     AB  C  approximates the exact be Pr A B C  with b  Pr AB Pr C    the process  Algorithms exist that construct efficient repre  then projection S  sentations of a vectors  such as decision trees or algebraic  lief state  decision diagrams  ADDs   when the POMDP is specified  The assumed independence allows more efficient monitor  b                     concisely using DBNs          ing using DBNs  at most  one maintains marginals over  Insight into the nature of POMOP value functions can be  each subset inS   gained by examining Monahan s      method for solving  The choice of projection scheme  or any other approx  POMDPs  Monahan s algorithm proceeds by producing a  imation  can have a dramatic impact on decision qual  sequence of k stage to go value functions Vk  each repre sented by a set of a vectors Nk  Each  a  E  Nk denotes the  ity in a POMOP  since the approximate belief  b   can lead  to the choice of a suboptimal course of action  Poupart  value  as a function of the belief state  of executing a k step  andBoutilier      propose a value directed approximation  conditional plan  More precisely  let the k step observation strategies be the set Oft of mappings u   Z    Nk    Then each a vector in Nk corresponds to the value of ex  framework allowing computation of bounds on the loss in  ecuting some action  OS    a followed by implementing some u  that is  it is the value of doing  a   E  and executing the  u z  if z is CP a  to denote this plan  we have that CP a     a ifz  CP u z    rlz    We informally write this as  a  u   We write a    a  u   to denote the a vector re  expected utility for projection schemes  and search methods for choosing projections that tradeoff decision quality with monitoring efficiency  The techniques are computationally intensive  potentially requiring time quadratic in the solu  k     step plan associated with the a vector  tion time of thePOMOP   but this offline computation pro  observed  Using  duces a projection scheme that improves online monitoring efficiency with minimal sacrifice in decision quality  We briefly outline this model   flecting the value of this plan   Assume a POMOP has been solved giving the set  of a  The implementation of a policy requires that one monitor  vectors with a  belief state b over time so that it may be  plugged  into the  value function  or N  to make a suitable action choice  Be   E N   Let  R  a  be the optimal region for b such that o  is maximal for  a  i e   the set of belief states  b    Given a projection schemeS  the switch set Sw a  is   UAI      POUPART   BOUTILIER           A B C   the set of c  such thatS b  E R a   for some b E R a   Thus S could induce one to believe a  has maximum value at the current belief state instead of a  thereby erroneously  switching to  the plan corresponding to a  from a by using S  Figure   illustrates a switch set Sw a     a   a   a      Switch sets can be computed by solving a nonlinear pro gram for each a EN  Linear programs  LPs  can be used to more effectively produce a superset of the switch set          Given the switch sets  or supersets thereof   one can com pute an upper bound B on the loss in expected value for a single approximation using S at k stages to go   B     rnaxaeN  maX    maxa ES a  b  a  a    W hen multistage approximations are applied  one can de vise an alternative set which is similar in spirit to the switch set  The alternative set Alt   a  is the set of all a vectors cor responding to alternative plans that may be executed as a re sult of repeatedly approximating the belief state at all future time steps  see      for a precise definition   Alt   a   is con structed with a dynamic programming procedure similar to incremental pruning      One can define an upper bound E on the loss in expected value due to successive belief state approximations usingS for k stages to go   E     rnaxaeN  max   maxa EAlt a  b  a  a    These bounds can be extended to infinite horizon problems  Given the bounds B and E  one can search for an  opti mal  projection scheme by looking for the projection that minimizes one of those bounds  The space of projection schemes is very large  factorial in the number of variables   but exhibits a nice lattice structure  Figure   illustrates the lattice of projection schemes when the state space is defined by the joint instantiation of variables A  B and C  Each point denotes a projection scheme  with  descendents  of any projection corresponding to more coarse grained pro jections  As we move down the lattice  accuracy increases since the number of correlations among the variables pre served in our belief state is increased  hence  error bounds B and E monotonically decrease   but monitoring effi ciency decreases as we move downward for the same rea son  A number of search procedures can be used to traverse the lattice  using the error bounds to guide the search  For example  a simple  and incremental  greedy scheme is pro posed in       The search is stopped when a suitable accu racy efficiency tradeoff has been reached     Vector Space Analysis  We now provide a vector space analysis of belief state ap proximation by projection  showing in Section    I that pro jections allow movement of belief state only in certain di rections  defining a subspace   This allows us to view a vectors as determining gradients of value in different direc tions  approximations whose directions give similar value gradients are less likely to cause switching  hence minimiz ing error   In Section     we use this to design faster switch  Figure    Lattice of Projection Schemes test algorithms than those described above  though yield ing looser bounds  In Section     we devise a new vector space search algorithm to find projections without directly trying to minimize these error bounds  instead relying on value gradient similarity       Vector space formulation  Given a projectionS over X  let b and b    S b  be points in belief space  Define d   b    b to be the displacement vector from b to b   Projection S determines a set of lin ear equations constraining b in terms of b   For example  if X    X  Y  and S    X  Y   i e   S treats X  Y as independent   we have   d xy    d xy    d xy    d xy  d xy    d xy  d xy    d xy          Geometrically  we interpret each equation as a hyperplane  and their intersection  or solution space  is a line through the origin representing a one dimensional  in this example  subspace  This subspace captures the set of all displace ment vectors resulting from the application of S  w r t  b    Since all possible displacement vectors lie on the same line  they must all have the same direction  vectors with opposite orientation are assumed to have the same direction   To illustrate  let b x        and b y         The approxi mate belief state using S above gives   b  xy  b  xy  b  xy  b  xfi   b x b y  b x b Y  b x b y  b x b ii                        Figure   shows a three dimensional belief space for belief states xy  xy  xy and xy   All belief states b with b x       We omit dimension  b xy  as probabilities sum to      POUPART   BOUTILIER       UAI      b xy  max  x  s t   b xy  b xy       b a  a   x b   ai  a   x b  m     b m    L b s   b sj            E S   Is  Is  b  s        b xyJ  Vm   m  Vm  Table    Linear VS switch test for projection schemes  This LP has a strictly positive objective value iff there is some  b E R a   Figure    Solution space of possible exact belief states     lie in a hyperplane  and similarly for intersection is the set   b  b         b y      b       T heir  and all displace  S  b     ment vectors forb  have the same direction   For marginals other than     and      the hyperplanes and their intersec tion shift  but remain parallel   Let  Ds be the displacement subspace spanned by the set  of all displacement vectors induced by  S   it is completely  characterized by its marginals  elements  and it describes  Ds  the directions of all displacements  In general  is a     XI   c  dimensional subspace  where c is the number  of constraints  since it is the solution space of  c  linearly  independent equations  each corresponding to a constraint  d m          c is the number of subsets of variables con  tained in some subset  m E S   as above   This is obvious  when we re  rrite the constraints as  Vm   d       where Vm  is a boolean lSI vector with l at states with all XE and   at states with some  m true  false   In our example   XE m  we have  V   vx Vy  xy           xfj xy                         xy  D be te subspace spanned by the vectors mE S  D  IS  the null space of Ds  i e   the set of vectors perpendicular to each vector in Ds   vm   the space       We will see below that the subspaces  Ds and D  allow  a nice characterization of a new switch test  We first con sider a simple relaxation of the switch test of         Recall  from Section   that approximation S could induce an agent  to switch from optimal vector ai to suboptimal vector aj if  S b  E R aj  for some bE R o i    The idea behind the re  laxed vector space  VS  switch test is to simply apply the same technique ignoring the presence of other a vectors   The VS switch test asks whether there is some belief state b  The  bajyetS b ai  generalization  straightforward   to  b m       b  m    for  O j is in the VS switch set of a   This is equivalent to ask frj E Sw a   when all vectors except these two are  removed from l   Note that the VS switch set is a superset of the true switch set  Since the constraints relating bandS  b  are nonlinear  VS switch sets can be computed using nonlinear programs  We can define a simpler linear VS switch test as in Table    which produces a superset of the VS switch set  This LP is a relaxation of the LP switch test  Now define frij           a    aj to be a vector representing the difference in expected value for executing aj instead of a   We can show that the VS switch test for a  and O j is pos itive iff a j Consider a j as a gradient that mea   D    sures the error induced by an approximation when it causes  a switch from a  to aj  After an approximation  if this dif  ference changes considerably  the agent is likely to choose the wrong maximizing o  vector  Define the relative error  O j  of this change in the relative assessment of a  with re  O j as   Here  a j  b a   aj   S b  a   O j  d  a j  can be viewed as a gradient since approxima  tions corresponding to displacement vectors  a i  maximize the magnitude of  gle between  d  O ij   d parallel  to  In general  the an  dand a j is a good indicator of approximation  error  In particular  if they are perpendicular  their dot prod  Vector space switch test  for whichba     such that  ing if  O j  Let  R aj    s   spect to         andb  E  any subset m  of variables contained in some marginal mE     S b aj   nonboolean  If so we say  variables  is  uct is zero and the relative assessment of a  and aj remains unchanged  preventing any switch  By definition  the sub space  Df is the set of vectors perpendicular to all displace  ment vectors possibly induced by S  so when aij is a mem  ber of  D   all possible displacement vectors are perpendic  ular to  a j and consequently there cannot be a switch from  a  to a j  Thus a j   D iff the VS switch test is positive   This fact provides for a much more efficient method to com  ute switch sets than the LP of Table     We decompose O ij m two orthogonal vectors corresponding to the projections of a j onto  D and Ds   a j       proj  a j  D     proj  O ij   Ds    UAI      POUPART   BOUTILIER  proJ a  D  stands for the projection of a onto a J E D  then pro   a i  D  a J and  consequently  proj a j  D s  is the zero vector  otherwise  proj a j Ds  is nonzero  We can thus determine ifa j E D  by measuring the length of proj O ij  Ds   We have   when a i E D  and that IJproj a j  Ds ll  a     when O ij rf  D  In particular  the p    j  Ds   ll ll roj  squared length of proj o  j  Ds  can be computed by the  where  D    If     following equation               tJE Df  D    Vf because of its factored representation  For problems involving binary variables  every vector in Vf consists of a sequence of    s and     s  before normalization   The un normalized basis vector iim associated with subset m has a   in every component corresponding to a state with an even number of true variables in m and    in every component corresponding to a state with an odd number of true vari ables in m  For instance  projection S     XY  Y Z  has six marginals     X   Y  Z  XY andY Z   yielding the fol lowing basis vectors           iiy          iiz      vxy                          that using the original LP test  As in Section    these bounds  can be used to search the lattice of projection schemes for making appropriate time decision quality tradeoffs        Vector space search      based on the relative error expression                                t v    I JiSi   I J   I JiSi   I JiSi   I JiSi  With this orthonormal basis  we can implement VS switch tests very effectively  without recourse to the LP in Table     We must simply compute Eq    which requires O c  dot  products  If unstructured  each dot product requires    ISI  elementary operations  for a total time ofO ciSI   The use of factored representations such as ADDs considerably im proves this rwming time  Each basis vector has only two distinct values  and yields a very compact ADD representa tion  Assuming that the POMDP has been solved to pro duce ADD representations of the a vectors  then the  a j  will have compact representations  and the dot products will be computed very efficiently  often a small constant inde  pendent of the size of the state space  Hence  for sufficiently structured POMDPs  the effective rwming time of a VS switch test is O c   By comparison  solving the linear program of an LP switch test       is polynomial in the number of constraints c and  the size of the state space  Furthermore  ADDs do not pro  vide as useful a speed up for LPs since the effective state  This definition can be generalized to non binary variables   O j  We do not com  pute switch sets at all  nor attempt to minimize worst case  vector space  VS  search  process instead seeks a projection S which defines a dis placement subspace  Ds that is as perpendicular as possible  to all gradients O ij  This is motivated by the observation  that the more perpendicular the direction of an approxima  tion with respect to  a j  the smaller the magnitude of J j  and  consequently  the less likely a switch will occur  Tech  nically  this is done by minimizing the squared length of the  a j on Ds  as in Eq      The length of proj a i  Ds  has a special interpretation  it projection of each gradient  corresponds to the greatest  absolute   relative error rate for  an approximation in some direction d E  Ds   The relative  error rate corresponding to displacement vector dis the rel  ative error induced by a unit displacement in the direction  ofd          vyz    puted using the VS switch test will generally be looser than  error bounds as above  This new  Here v  is some orthonormal basis spanning The spanning set of vectors Vm above can be used to generate several orthonormal bases using the Gram Schmidt orthog onalization process and normalizing  We consider a spe cific orthonormal basis in particular which we refer to as  iie  space is the intersection of the abstract state space of all the  constraints  The price paid is that the B and E bounds com  In this section we describe an alternative search method  llproj a j  Ds ll a j O ij  L  a j vf  vx       d lldll    Ojj  Hence  by choosing a projectionS that minimizes Eq     we are minimizing the  squared  worst relative error rate that may result from projection S  When ignoring the distance between the exact and approximate belief states  the rela tive error rate permits us to quantify how bad an approxi mation in some direction is likely to be  Each projection S  constrains approximations to directions within the subspace  Ds   The directiond E  Ds with the highest  absolute  rel  ative error rate has this worst relative error rate  which also happens to be  llproj O ij  Ds  ll  Thus  it is desirable to try  to minimize Expression      Ideally we should choose an S that simultaneously mini  mizes Eq    for every gradient  a J  J i  i   In the absence of  any prior information about the relative importance of each gradient  we suggest two simple schemes   a  minimize the sum of squared lengths of each projection  or  b  minimize the squared length of the greatest projection   L llproj a j  Ds ll ji   L  a j  D jj        i  J    Jr   tJEDt  llproj a j  Ds ll  v    a j         POUPART   BOUTILIER       We refer to these schemes as the sum and the max error es timators  respectively  for projection schemes  Of course  many other schemes could be proposed   Given a vector o   E   VS search uses eitherEq    or Eq    above to find a good projection  S as follows  Starting at  the root  we traverse the lattice of projection schemes  Fig  ure    downward in a greedy manner  At each node  we pick the most promising child by minimizing Eq    or Eq    The  computational complexity of a VS search is fairly low as it avoids LPs  Its running time is  O nc JI ISI    O nc   nodes in the lattice are tra versed  each requiring the evaluation ofEq    orEq    which both take O ci XIISI  elementary operations  For each region   The VS search can also be streamlined  The constraints of a node S are essentially the same as the constraints of its par  ent node S  with one extra constraint corresponding to the marginal  that labels the edge connecting the two nodes   m  Problem  Since there is one basis vector per constraint  the following equation holds   Coffee  W idget  Pavement    and Eq    can be computed in  crementally as the lattice is traversed downward         jt i    and Weld                            The third POMDP is inspired from the pave          Since the analysis of the experiments doesn t require any specific domain knowledge  the reader is referred to       in  which the full specification of those problems is given  Each of the three problems was solved using Hansen and Feng s       ADD implementation of incremental pruning   IP  to produce a set   of a vectors using a compact ADD  Each problem is run to    stages  dis counted   Table   shows  for each problem  its full state  representation   lSI   and its  effective size   the largest intersec  tion of abstract  ADD  states encountered during solution  specifically  the LP dominance test in IP   The effective size is more relevant to solution time than  jSj   We also   X over the fifteen stages and the maximum  cally  six algorithms are tested  the B bound and E bound  O nc JI JSI   since only  This running time is significantly  greedy search with LP switch tests used in         As for  the B bound or E bound greedy search with VS switch  O nc JIJSI   is comparable  The  VS search has an extra I I factor  but one less  practice  I  I is usually larger than  c   c  factor  In  so the VS search is ac  tually slower  Again  the upper bounds on running times are given in terms of lSI  but in practice  factored represen tations can drastically reduce the size of the effective state space for structured POMDPs   which computes switch sets using an LP  bounds  the VS analogs of these procedures which com putes weaker VS switch sets using the algebraic formula tion of Section       and the VS search methods  sum and  max  of Section      which ignore these bounds  but instead try to minimize Eq      or Eq     All search algorithms per  form a lattice search within the set of projection schemes that partition variables in disjoint subsets  Furthermore  as suming that marginals of at most two variables provide a  suitable efficiency accuracy tradeoff  the lattice is traversed  until all children of  a node correspond to projections with a  marginal with   variables  This last node is the projection scheme returned by the search  We compare the time required to find a good projection us ing the different search procedures in Table     As expected   the running time is much less when using VS switch tests  compared to LP switch tests   since VS switch tests do not  Empirical Evaluation  require the solution of LPs  As for VS search algorithms   Three test problems were used to carry out the experiments  The first POMDP is essentially the coffee problem intro               and chooses a projection using either the B or E error  smaller thanO nc  kiIISik  for the B bound or E bound  duced by Boutilier and Poole             ment maintenance problem described by Puterman  search of  one dot product needs to be computed instead of one for                 each POMDP by minimizing different error bounds and or  This incremental computation scheme for traversing the lat  tests  the running time      using different switch tests  as described above  Specifi  J    proj a j  Ds  Jj  vm O  j  c constraints   time  s              Once solved  we searched for a good projection scheme for  o  j  Jr    each of the  aver   size set   L jjproj aij  Ds  ll  Vm  tice reduces the running time to  max  size of the sets  Ji  jjproj a j  Ds IJ J r  l  Solution  effective  show the solution time  in seconds  along with the average  I  Jlproj o  j  Ds ll  j i  Size of  X  full  Table    Solution statistics for the three test problems  space size  This means that both Eq   State Space Size  since one  good projection must be found for each of the I I regions  R o     UAI      The second POMDP is a  variation of the widget problem described by Draper  Hanks  whether we minimize the sum of the relative error rates or their maximum  the running time is roughly the same and it is significantly faster than B bound and E bound search algorithms that use LP switch tests  but a bit slower  if VS   UAI      POUPART   BOUTILIER  Problem  Solut   Coffee  LP  vs                                                                      widget Pavement  E bd se arch LP vs  B bd search  time                 VS search sum  max                              Error  Single  B bd search  E bd search  LP  LP  vs  vs      Error  stronger dependence on the number of a vectors  compared  to VS switch tes ts   The time to search for good projections  be much worse than that of solving POMDPs  though  this offline cost translates into online gains   In fact  only search procedures that avoid solving LPs scale effectively to larger problems  In some cases  these offer a decrease of up to two orders of magnitude  T he running time ofVS pro cedures is roughly of the same order of magnitude as that of the POMDP solution procedures  We also compare the actual average error  as well as the for mal B and  E error bounds  obtained when applying the pro  jection schemes found by various search algorithms  Tables       and     T he average error is the average loss incurred for      random initial belief states generated from a uni form distribution  We see that the average error is essen  tially the same whether the VS search procedure is used or some error bound is minimized  As a result  the dramatic computational savings associated with the VS procedures has effectively no impact on solution quality  Note that the  E bounds are much larger than the average error  observed because the bounds are concerned with the worst case scenario and  furthermore  they are not tight  supersets of the switch sets are really computed       Concluding Remarks  We have proposed a new approach to value directed belief state approximation for POMDPs  Our vector space approach using either VS switch tests or direct VS search offers significant computational benefits over the value directed methods proposed by Poupart and Boutilier         vs  E bd search LP vs  VS search max  sum  Single  Aver                                            Approx B bd                                           Several Aver                                            Approx E bd                                                Pavement problem  error comparisons  tests are used forB bound search  This is because   pared to LP switch tests   but on the other hand  it has a  B and  VS search max  Coffee problem  error comparisons  on the one hand  the VS search does not solve LPs  com  can  B bd search LP  Table switch  vs  Table    Widget problem  error comparisons  Aver                                             Table  LP  vs  Several Aver                                            Approx E bd                                            VS search sum  Several Aver                                                  Approx E bd                                            E bd search  LP  sum Aver                                              Approx B bd                                            max  Approx B bd                                            B bd search  Single  Table    Search running time in seconds  Error       W hile the error bounds are looser  we have seen in  practice that our new schemes perform as well as the others  with respect to solution quality  thus the computational savings are achieved with little impact on decision quality  Furthermore  the vector space model provides new insights into the belief state approximation problem and how approximation impacts decision quality   This novel view also gives us access to numerous tools from linear algebra to design approximation methods that could potentially offer better tradeoff s between decision quality and monitoring efficiency   For instance  it would be in  teresting to investigate linear projectors since they allow the design of linear approximation methods by specifying  among other things  a displacement subspace Ds which could be made as perpendicular as possible to the gradi ent vectors aij  Linear projectors are well studied approx imation methods with numerous properties and therefore they provide a promising alternative for improving value directed approximate belief state monitoring  The success and scalability of our methods strongly de pends on the structure and compactness of the a vectors  Therefore  one could also analyze the dependency between the a vector structure and the conditional independence structure of the transition and observation functions  From a linear algebra perspective  the a vectors can be viewed as a discounted sum of reward vectors multiplied by transition and observation matrices  T hus compact and structured a vectors could arise when the reward vectors fall into a small invariant subspace of the transition and observation matri ces   A  possible direction of research would then be to re  late the conditional independence structure of the transition and observation functions with their eigenvalue and eigen vector properties since they define the invariant subspaces  This would allow us to better characterize the situations in which our approach is suitable  We are currently extending this approach  and its analysis         POUPART   BOUTILIER  in a number of different directions  First  we motivated this  UAI          A  R  Cassandra  M  L   Littman  and N  L  Zhang  In  work by focusing on infinite horizon POMDPs  though our  cremental pruning  A simple  fast  exact method for  algorithms and analysis assume a finite set of a vectors  Of  POMDPs  In Proceedings of the Thirteenth Confer  ten one is forced to approximate the value function  e g   by  ence on Uncertainty in Artificial Intelligence  pages          Providence  RI          producing a finite set of vectors where an infinite set is re quired  or simply by reducing the number of vectors to keep it manageable in size   Our algorithms can be applied di       ning with information gathering and contingent exe  rectly to approximate value functions  and we expect that  In Proceedings of the Second International Conference on AI Planning Systems  pages          cution   the analysis can be extended with suitable modifications as well  We are also interested in applying the idea of value directed monitoring to other representations of value func tions and other forms of approximate monitoring  The use of grid based value functions      Chicago                      provides a very at  for which approximate monitoring will generally be neces tions can be used profitably to direct the choice of projection  E  A  Hansen and Z  Feng  Dynamic programming for  POMDPs using a factored state representation  In Pro   ceedings of the Fifth International Conference on AI Planning Systems  Breckenridge  CO                   tractive method for producing approximate value functions sary  We expect that information in grid based value func  D  Draper  S  Hanks  and D  Weld  Probabilistic plan      M  Hauskrecht   Val ue function approximations  for partially observable Markov decision processes    or other approximation  schemes  The use of value infor  Journal ofArtificial Intelligence Researr h              mation to guide other belief state approximation methods is         also of tremendous interest  we have recently developed a sampling  particle filtering  algorithm that is influenced by value function information                  partially observed Markov decision processes  Annals  Finally  if it is taken for  granted that some form ofbeliefstate approximation will be  used  one might attempt to solve the POMDP to account for  of Operations Researr h                             Madani  S  Hanks  and A  Condon  On the undecid ability of probabilistic planning and infinite horizon  this fact  that is  can we construct policies that are optimal  partially observable Markov decision problems   subject to the resource constraints placed on the monitoring  Natural Sciences and Engineering Research Council and         G               processes                       Portland   OR          In Proceedings ofthe Four teenth Conference on Uncertainty in Artificial Intelli gence  pages        Madison  WI               R  I  Brafman              Value  certainty in Artificial Intelligence  Seattle        This volume                   Providence          A  R  Cassandra  L  P  Kaelbling  and M  L  Littman   P  Poupart  L  E  Ortiz  and C  Boutilier   In Proceedings ofthe Seventeenth Conference on Un  A heuristic variable grid solution  teenth National Conference on Artificial Intelligence   P  Poupart and C  Boutilier  Value directed belief state  directed sampling methods for monitoring POMDPs   method for POMDPs  In Proceedings of the Four pages         In Proceedings of the Sixteenth Conference on Uncertainty in Artificial In telligence  pages          Stanford         X  Boyen and D  Koller  Tractable inference for com  plex stochastic processes   Master s thesis  University of British  approximation for POMDPs   teenth National Conference on Artificial Intelligence  pages  Approximate value directed belief state  Columbia  Vancouver   cies for partially observable decision processes using       P  Poupart   monitoring for partially observable Markov decision  C  Boutilier and D  Poole  Computing optimal poli compact representations  In Proceedings of the Thir  C  H  Papadimitriou and J  N  Tsitsiklis  The complex  erations Researr h                           search                                              ity of Markov decision processes  Mathematics of Op  C  Boutilier  T  Dean  and S  Hanks  Decision theo tional leverage  Journal of Artificial Intelligence Re  A survey of partially observable  rithms  Management Science         retic planning  Structural assumptions and computa  E  Monahan   Markov decision processes  Theory  models and algo  the Institute for Robotics and Intelligent Systems   
