  Structure     IDEAL  Influence Diagram Evaluation and Analysis in Lisp  is a software environment for creation and evaluation of belief networks and influence diagrams  IDEAL is primarily a research tool and provides an implementation of many of the latest developments in belief network and influence diagram evaluation in a unified framework   This paper describes IDEAL  and some lessons learned during its development   IDEAL is written in Common Lisp  Lisp was chosen as the implementation language since it is most suited to exploratory programming and quick development  In addition  the software is portable across a wide variety of platforms  IDEAL is a library of Lisp functions that pro vides the following features    Data structures for representing influence dia grams and belief networks        grams and belief networks   Introduction  Over the last few years influence diagrams lief networks      and be    tation schema for domains where uncertainty plays    process them  as  well              as  as  the semantics of these  on efficient algorithms to    lief networks    systems  IDEAL is a software package that was de veloped  as  a platform for research in belief networks  and influence diagrams  IDEAL also can be used to    Algorithms for p erforming inference in influence diagrams and inference and belief propagation in belief networks   library of functions that provides the belief network and influence diagram methodology for embedded use  Routines that perform some basic transforma tions of influence diagrams   create intermediate sized run time systems and as a  by other applications   Utilities that provide many useful services like consistency checking and creation of random be  This work has now matured to the point where these techniques are finding their way into production  Utilities that are of use in coding influence dia gram manipulation algorithms etc   an important role  There has been a wealth of work representations  Facilities for copying  saving  to file  and loading influence diagrams and belief networks        have emerged as attractive represen  on both basic issues such  Facilities for creating and editing influence dia    Influence diagram evaluation algorithms   IDEAL incorporates  in a unified framework   These functions can be used interactively by a  many of the latest developments in algorithms for  user typing to a Lisp interpreter or embedded in  evaluation of belief networks and influence diagrams   code by other applications  To preserve portability   In addition   it provides a complete environment for  IDEAL has only a simple character terminal based  creating  editing and saving belief networks and influ  user interface  However  it provides hooks  or easy  ence diagrams  In the rest of the paper any reference  development of a graphic interface layered over it on  to  diagrams  can be taken to refer to influence dia  any specific platform  A graphic interface has been  grams and belief networks unless stated otherwise   developed for the Symbolics environment         I I         Facilities in I DEAL Data structures  IDEAL provides abstract data structures for rep resenting influence diagrams and belief networks  These data structures and a tool kit of associated functions provide all the basic low level functionali ties required for the creation of belief networks and influence diagrams  This includes creation of directed acyclic graph topologies  creation of probability ma trices and other matrices and vectors that are indexed and sized by the states of the nodes in the graph  ac cessing these matrices and vectors  manipulation of the graph topology  control constructs that allow easy traversal of these node matrices  etc  These are low level features that can be used by programmers to develop functionalities that are not available directly in IDEAL  A user who does not need any additional functionalities can interact with IDEAL with higher level functions described below       Creating and Editing diagrams  The functions used to create and edit diagrams are at a higher level than the functions that manipulate the low level data structures  These functions expect fully specified diagrams as input and return consis tent diagrams after they are done  Some of these functions require interactive input from the user  Functions to do the following are available  Cre ation of complete diagrams  adding arcs  deleting arcs  adding nodes  deleting nodes  adding states to a node  deleting states from a node and editing node distributions  These functions make suitable assump tions that guarantee consistency of the diagram after they are done  For example  adding an arc between two nodes extends the distribution of the child node  This extension of the distribution is done such that the child node is independent of the new parent  i e  the child node has the same distribution given its predecessors regardless of the state of the new node  Most of these functions can be used embedded in code to create diagrams on the fly  These functions provide the right hooks into IDEAL for a user who is interested primarily in the existing functionality and does not need to go into the low level implementation details       Copying and Saving Diagrams  The copy function in IDEAL makes a complete copy of a fully specified diagram  This is frequently useful when one wants to make some transformation that might destructively modify the diagram  The copy   ing mechanism provides a means of keeping an un modified original in the Lisp environment  IDEAL also has functions that allow the user to save a diagram to file and to reload diagrams from these saved files  IDEAL saves the diagram in text files and so they can easily be exchanged between users at remote sites or on different platforms by elec tronic mail or other means  The saving function can be made to recognize any extensions that the user may make to the abstract diagram data structures  Thus  any custom information that a user may want to associate with the diagram can also be saved and retrieved       Utility functions  IDEAL provides a wide variety of utility functions that are of use in conjunction with belief networks and influence diagrams  Consistency checking func tions for the following are available  To check whether a diagram is consistent  i e   it is acyclic  the proba bility distributions sum to    etc   to check whether a diagram is acyclic  a lower level function   to check whether a diagram has a strictly positive distribution and to check whether a diagram is a belief network  User interface utilities are available for display ing a description of the diagram in text format  for easily accessing nodes in the diagram and for describ ing the contents of particular nodes of a diagram  A set of utility functions is available for creat ing  random  belief networks  This set of functions is useful for creating examples for testing of belief net work algorithms and for quickly creating test belief networks that satisfy certain user defined criteria  for example  see       In addition to these there are miscellaneous util ity functions  Some examples  a function for sort ing the nodes in the diagram by graph order and a function that modifies the distributions of a non strictly positive diagram slightly  as specified by an argument  to make the distribution strictly positive       Diagram transformations  This is a set of functions  each of which take a con sistent diagram as input and return a consistent di agram  These transformations are used in reduction style algorithms          They can also be used to make changes in diagrams or to preprocess them be fore passing them to an inference scheme  Some of the transformation functions are  Re moval of a particular barren node from a diagram  Removal of all barren nodes from a diagram  absorb ing a chance node in a diagram  reversing an arc   I I I I I I I I I I I I I I I I I        I I I I I I I I I I I I I I I I I I I  reducing a deterministic node etc  The transforma tion functions  as implemented  change the input di agram destructively to yield the result  Details of these transformations can be found in              Graphic Interface and documen tation  As mentioned before  IDEAL is designed to be a portable tool and so it does not include any imple mentation specific graphics features  On the other hand  hooks are available for in IDEAL for easily lay ering a graphics interface over it  Such an interface has been developed for IDEAL on Symbolics machines  In addition to standard graphic manipulation commands this interface pro vides most of the functionalities described above ei ther through mouse driven graph manipulation  for eg  reversing an arc  or through convenient menu driven commands  The interface allows convenient access to the Lisp environment in a separate window and can be a very effective programming tool when developing applications based on IDEAL  IDEAL and the Symbolics interface to IDEAL are documented in detail in        same for all algorithms of the latter three classes  So  if need be  the actual algorithm used can be a deci  sion that is transparent to the end user or any calling function which needs an inference mechanism whose details are irrelevant       Reduction algorithms  Influence diagram evaluation algorithms as described by Shachter      and Rege and Agogino      are avail able  Inference algorithms applicable to both influ ence diagrams and belief networks are also available as described in the same sources  These algorithms operate by making a series of transformations  see above  to the input diagram  The input diagram is destructively modified       Message passing algorithms  Message passing algorithms model each node as a pro cessor that communicate by means of messages  A distributed algorithm from Pearl that applies to poly trees      is implemented in IDEAL  This implemen tation also utilizes work by Peat and Shachter       A conditioning algorithm that works for all belief net works is also available  The conditioning algorithm calculates cutset weights as described by Suermondt and Cooper       A variation of the conditioning a l Algorithms in IDEAL   gorithm from Peot and Shachter      is also available  The conditioning algorithms find cutsets as described IDEAL provides many different evaluation and in by Suermondt and Cooper       ference algorithms  The implementation emphasis is on clarity rather than speed  Each of the algorithms make extensive input checks and also explicitly de     Clustering algorithms tects error conditions such as impossible evidence  see Clustering algorithms aggregate the nodes in a belief Sec       network into a join tree of  meta  nodes and then run The algorithms implemented in IDEAL fa   into an update scheme on this tree  The updated beliefs four classes  reduction algorithms          message for each of the belief network nodes is then calculated passing algorithms          clustering algorithms       from the  meta  nodes  and simulation algorithms       The algorithms in IDEAL implements two variations of the ba each class are closely related to each other but differ sic clustering algorithm described by Lauritzen and in complexity or are applicable to only specific kinds Spiegelhalter      The first considers the join tree as a of belief networks  Reduction algorithms are used for  meta  belief network and runs a variation of the poly influence diagram evaluation  i e   solving an influ tree algorithm      on it  The second variation uses ence diagram for the optimal decision strategy  and an update scheme that operates on clique potentials for inference  When used for inference they answer as described by Jensen et al      specific queries  i e  they give the updated belief of Two methods are available for making the fill in a specific target node given a set of evidence nodes  for use in construction of the join tree   Maximum The algorithms in the latter three classes   as imple Cardinality Search      and a heuristic elimination mented  can be used only for inference in belief net ordering heuristic from Jensen et  al          works  They give updated beliefs for all the nodes in the network given evidence  The data structures Simulation Algorithms     for declaring evidence before an algorithm is called and the data structures where the updated beliefs are IDEAL implements a simulation algorithm from found after the algorithm has finished running are the Pearl       This implementation can only operate on        I I  We have calibrated the estimates yielded by these functions against actual time measurements of how long it takes to solve the corresponding problems  The correlations have been strong  see Sec        belief networks with strictly positive distributions       Estimator functions  IDEAL provides run time estimator functions for some of the algorithms implemented in it  Given an algorithm and a particular belief network with a par ticular state of evidence  the estimator function gives a quick estimate of the complexity of the update pro cess  In general  belief net inference algorithms con sist of two kinds of operations  The first kind are graph operations that are polynomial in the number of nodes in the graph  eg  triangulating a graph for clustering  conversion of a multiply connected net work into a singly connected network by instanti  atmg a cutset      The other class of operations are the actual numerical calculations that are carried out over the probability and potential matrices associ ated with the graphs  We will refer to this as the update process  The overall exponential complexity algorithm derives from the fact that these matrix op erations carried out during the update process take exponential time  The estimator functions in IDEAL give a quick estimate of the complexity of these ma trix operations  The complexity count that is returned is a count of the number of steps the algorithm will spend in spanning the state spaces of the nodes or cliques in volved  For example  if a binary node A has a lone binary node B as a predecessor then the complexity count of setting the probability distribution of A is four since one has to cover a state space of   x   states  The complexity of normalizing the belief vector of A is again   since one has to cover the state space of the node A twice  once for summing the beliefs and once for normalizing them  An estimator function for a particular algorithm takes an inference problem as input  i e  a belief net work and associated evidence  The estimator per forms the polynomial time graph manipulations that are necessary for initialization before the actual up date process can begin  It then applies embedded knowledge of the update process to give an exact count of the number of steps that the update pro cess will take  A step is defined as explained in the previous paragraph  This estimate is made in linear time  So overall  the estimator function runs in time polynomial in the size of the input    Here  we refer to the actual graph algorithm implemented  as against the algorithm which would give optimal results  For example  the algorithm implemented in IDEAL for finding a  loop cutset for conditioning  runs  in polynomial time while the  p roblem of finding the minimal loop cutset is NP hard   see  for both results              Discussion  IDEAL has been a success from the experimental point of view  It has been used both for in house ap plications and research both within and outside Rock well  Some examples of the uses of IDEAL include a decision aiding model for pilots that helps to sort the vast flow of information that comes to the cockpit from the sensors on the plane  a life cycle costs anal ysis system for Rocket engines  embedded use in a natural language system for story understanding     and an implementation of interval influence diagrams      One of the lessons we learned in the process of implementing IDEAL was that many of the algorithm papers do not describe the algorithms in standard al gorithmic style  In addition they leave many details incompletely specified  From an engineering point of view  it would be very useful if we had both a more complete description of algorithms and in a more con ventional style  IDEAL s emphasis on code readabil ity and explicitness were of great help in detecting and correcting any problems that came up       Estimator functions  As explained in the previous section  the estimator functions carry out the polynomial time graph ma nipulations that precede the update process and then give an estimate of the complexity of the update pro cess  The results of the graph manipulation are re quired to make the estimate  The actual estimate is the result of applying a formula to the results of the graph manipulation  These formulae were derived by analysis of the update process of each algorithm  The estimator functions in IDEAL apply only to exact al gorithms  as opposed to approximation algorithms   As an example of an estimator function consider the estimatr for the Jensen method     of clustering    G vn a behef network the complexity of initializing   the JOm tree by the Jensen method if given by   L      N U  S U  UEJ  where U represents a Bayesian belief universe J is the join tree made up of Bayesian belief univers N U  is the number of neighbors of U in the joi tree and S U  is the size of the joint state space of the belief network nodes that are members of U   I I I I I I I I I I I I I I I I I        I I I I I I I I I I I I I I I I I I I  Update phase of Jensen algorithm                  This formula is easily derived as follows   For  each belief universe the potential distribution has to be set up by multiplying the distributions of the com ponent belief network nodes   This has complexity  S U   When a belief universe absorbs from its neigh bors the complexity of the operation is  S U    When  it updates a neighboring sepset  again the complex  ity of the operation is S U   During the collect evidence operation  each universe absorbs from its  child  neighbor sepsets and then updates its  parent  neighbor sepsets  Thus  for each universe the com plexity of the operation is  Iii   C c   u     Cll      Cll   c    iii   I             S U    During the dist ribute evidence operation  each universe first absorbs from its  parent  sepset neigh The complexity of the operation is  N U S U           e    bor and then updates all the  child  sepset neighbors   Complexity Estimate  steps   for  each universe U  Summing the terms for initializa  Figure     tion of the join tree  the collect evidence operation  example  Performance of estimator functions    e    An  and the distribute evidence operation gives the com plexity formula above  An approximate formula that gives the complex  once  This could be the case  for example  in a sys  ity of the update process in the Jensen algorithm is   tem that constructs belief networks dynamically and  L      N U  S U    L  S Us    S i   iEB  UEJ  where  U   is the smallest universe  in terms of  state space size  that contains node i of the network  The update process consists of one collect evidence operation   one distribute evidence opera  tion and a marginalization operation for setting the belief vectors of the belief network nodes  These fac tors add up to the formula above  The formula does not take into account the fact that some optimiza tion can be made based on the position of evidence in the join tree  It also does not include the operations needed to declare e vidence in the join tree  However   uses each network only once  When the same network is used repeatedly with different evidence pieces  the clustering algorithms are superior  The construction of the join tree can be considered as a compilation step of the belief network that needs to be carried  out only once      Though IDEAL is an experimental tool it gives reasonable response times for medium size problems  As an example  a      node network developed as part  of a decision aid system for aircraft pilots takes about     seconds to solve on a Symbolics       IDEAL s speed is limited both by the choice of implementation language and its implementation style  where explicit code rather than speed has been the top priority   leaving out these terms does not introduce significant error  We have obtained excellent correlations between the complexity estimates given by the estimator func       Handling determinacy and inconsistency  tions for various algorithms and the actual run time   In all the algorithms  gains can be made by explicitly  F ig   demonstrates the correlation for the update phase of the Jensen algorithm  The data in the graph  done as a pre processing step      in which case the  was collected by running tests on randomly created  network topology itself is modified  or  more gener  belief networks   ally  in the propagation phase of the algorithm   detecting determinacy in the network  This can be  As expected  particular algorithms suit particu  When the joint probability distribution of a be  lar types of problems well  When choosing what algo  lief net  i e  the joint distribution of all the variables  rithm to use  in addition to the type or size of prob  in the belief net  is not strictly positive it means that  lem  one needs to consider whether the belief network  some particular configuration of the belief net is im  involved needs to be solved just once or solved mul  possible   tiple times with different evidence sets  Conditioning  of nodes of the belief net have non strictly positive  algorithms are competitive  though not necessarily  joint distributions  i e   the unconditional probability  faster  when the problems needs to be solved only  of some joit state of the subset is zero  The actual  This in turn implies that some subset s         I I  makeup of these subsets depends on the conditional  conditional distribution in which the condition  independencies in the network  Let the network I  or some subset of nodes of  ing node set consists of some belief net nodes  the network  have an impossible state I  i  Then  obviously  any conditional probability distribution  P X I     i  where X  is another subset of nodes of  the network cannot be assigned meaningfully  If an implementation of any probabilistic inference algo rithm does not account for such circumstances  this leads to a divide by zero error if the implementa  P X  I   i   P X I   i  as  tion tries to calculate the distribution This occurs either when calculating  P X  I  i    P I  i  or when normalizing the repre sentation of P X  I  i   say R X I  i  for all states x of X where each R X   x I   i  has been found to be zero  Note that the representation is inconsis tent and cannot represent a conditional probability distribution that sums to      An impossible state can occur due to two things     Inconsistent Evidence  The evidence that the user has declared may be inconsistent with the  Reduction algorithms         In reduction algorithms a divide by zero error can occur when we try and find new conditional distri butions  This happens only during arc reversal and node absorption   In inference algorithms node ab  sorption is just a special case of arc reversal and so we need to look only at arc reversal  When performing arc reversal to find a new dis tribution and  B  P A B      b  where  A  is a single node  is a set of nodes the basic method is to  marginalize  P A  B   b   and then normalize it us  ing the marginal  We hit a divide by zero error if the marginal  a case IDEAL tribution   b  happens to be zero   In such makes P A B   b  a uniform dis  P  B      This is justified because any subsequent  manipulation of the distribution P A  B   b  by a re duction algorithm always involves multiplying it into  P B  b  first  We know that P B  b  is zero and P A B  b  can be anything  The advantage of  belief net  Let us say that the probabilities en coded in the belief net are such that for a subset  so  of nodes A of the belief net P A   a  is zero where a is some joint state of the nodes A  If  consistent  i e   the numbers still constitute a valid  the evidence we declare happens to be exactly  probability distribution  even after the tr insforma  a  or some superset of it  i e  some nodes outside  a  plus evidence for  A  then obviously we will hit  a divide by zero error when performing inference to find some distribution P B A  a  where      which are not evidence nodes   B is  this uniform assignment is that the diagram remains  tion   The disadvantage is that if the user s query  to the system was  P A  B  b   and  P B  b   hap  pens to be zero for some state of B then the user will not realize it and may ascribe some meaning to  some other set of nodes in the belief net  This is  the distribution  because the distribution we are seeking is hypo  meaning  Note that this effectively amounts to out  thetical  unassignable or meaningless  depending  putting garbage when the evidence is impossible  the  on how we look at the problem   evidence being that particular state b of  Nature of algorithm  An impossible state may also be caused by the nature of the inference al gorithm   Consider the conditioning algorithm   for example  It performs whatever inference we are interested in conditioned on every possible         P A  B   b   even though it has no  B    Message passing algoritluns  The polytree algorithm   as  implemented in IDEAL   cannot hit the divide by zero error during the prop agation phase since it calculates only joint probabil  joint state of a set of cutset nodes which make  ities  However  when normalizing the beliefs of each  the belief net singly connected  The results ob  belief network node after the propagation is done  it  tained from each of these conditionings are then  weighted  to get the results  Thus if the cutset  is possible to find that the marginal is zero  This di rectly implies that the evidence declared before the  is  propagation is impossible  i e    A  and the evidence is  node s  is  B then  we find  E   e and the target P B A  a E  e  for  a of A and then weight these If P A  a E  e  is zero for some state  the marginal is nothing but  P E   e    O  since P E   e   IDEAL de  all possible states  tects this situation explicitly and tells the user that  results   the evidence is impossible   a of A it is easy to see that we have an impossible  This conditioning algorithm makes the belief net  state which would lead to a divide by zero error in general  an algorithm can hit an impossible  a poly tree by clamping the states of a cycle cutset of nodes S  The evidence is propagated as by the polytree algorithm for each of the evidence pieces and  situation  which cannot be attributed to incon  then the result is weighted to get the beliefs of each  sistent evidence  if the algorithm calculates any  node given the evidence alone   when calculating  P B A   a E   e    Thus   I I I I I I I I I I I I I I I I I        I I I I I I I I I I I I I I I I I I I  IDEAL supports two conditioning implementa  described in the original paper  After propagation  if  The first calculates cutset weights explic  a zero marginal is encountered when normalizing the  In other words  for every node A we calcu P A S   s  E   e  and then use that to cal culate P  A E   e  as the marginal of the prod uct P A S   s  E   e P S   sfE   e   where P S   s E   e  is a  mixing  probability  We will hit the divide by zero error when P S   s E  e  is zero and we try and calculate P A S  s  E  e    beliefs this implies that the evidence was impossible   tions  itly  late  In this implementation  a cutset conditioning case  s  for which  P S   s  E   e       contribute to the overall belief   does not  So to avoid an er  ror the cutset algorithm checks for the occurrence of  P S   s  E   e       process that determines  during the recursive update  P S   s E  e    If the con  dition occurs then that cutset conditioning case  s  is  IDEAL signals the fact explicitly in both clustering implementations          Simulation Algorithms  The simulation algorithm coded in IDEAL cannot handle non strictly positive belief networks  If such a belief network is given as input the algorithm breaks with an appropriate warning   skipped  Other than being a graceful technique to de tect an impossible situation  this step  in conjunction with Suermondt and Cooper s        technique for cal  culating cutset weights  can lead to substantial com plexity gains since whole classes of impossible cutset cases can be detected and skipped with very little  Further developments     effort  For example  if the cutset consists of three bi nary nodes A  B and C  in graph order  A  B  C    then knowing that P A   t      immediately elimi nates   cutset cases  one for each state combination of  B  and  C  in conjunction with  A  t   In the second conditioning implementation        no conditional probabilities are calculated during the propagation phase and so no divide by zero errors are possible  However  it is possible that when marginal izing the belief vectors of the nodes after the propa gation  the marginals are zero  This implies that the evidence that has been propagated is impossible  see previous subsection    IDEAL detects this situation  explicitly in both conditioning implementations   We foresee more work on developing efficient estima tor functions   Each estimator function may be ex  panded into a class of functions where one may trade off the accuracy of the estimate with the time re quired to make the estimate  It may be p ossible to use these estimator functions to help choose between competing algorithms for a given problem or to use them as a search function to search through a space of competing alternative solutions  IDEAL  has incorporated almost all the pub lished work to date on exact belief network and influ ence diagram algorithms  We will probably include any promising new methods that come up  for exam ple  nested dissection               so that we can choose the  best possible method for the applications we have in  Clustering Algorithms  IDEAL supports two clustering algorithm implemen tations  The first implementation creates a join tree of cliques and calculates the conditional probabili Consider a clique A with a B  We hit the divide by zero er ror when P B   b  is   and we try and calculate P  A  B   b   When creating the join tree we assign P A   afB   b       we could assign anything  in fact  for all states a of A when P B  b      After  mind  We will also be including some approximation algorithms such as Likelihood weighting         ties in the join tree   parent clique     Acknowledgements  the join tree is created the clustering algorithm uses a variant of the polytree algorithm for evidence propa  We would like to thank Robert Goldman for being  gation and so the divide by zero problem cannot come  an invaluable source of suggestions  bug reports and  up  The second implementation from       handles a divide by zero condition during the propagat ion as  enhancements   We would also like to thank Bruce  D ambrosio  Keiji Kanazawa  Mark Peot and other users of IDEL for their suggestions and help         I I  
  To date  most probabilistic reasoning sys tems have relied on a fixed belief network constructed at design time  The network is used by an application program as a rep resentation of  in  dependencies in the do main  Probabilistic inference algorithms op erate over the network to answer queries  Recognizing the inflexibility of fixed models has led researchers to develop automated net work construction procedures that use an ex pressive knowledge base to generate a net work that can answer a query  Although more flexible than fixed model approaches  these construction procedures separate con struction and evaluation into distinct phases  In this paper we develop an approach to com bining incremental construction and evalu ation of a partial probability model  The combined method holds promise for improved methods for control of model construction based on a trade off between fidelity of re sults and cost of construction      Introduction  Most applications of belief networks for probabilistic reasoning systems have relied on a fixed belief net work  The network is constructed by the system de signer  possibly in concert with a domain expert  and then used by the application to evaluate the probabil ity of various hypotheses g iven observations  Recent work  much of it reported at this conference  has made clear that evaluation of such predefined  static models is not sufficient in many applications                Some drawbacks of such static models are inflexibility  lack of expressive power  and an inability to model a pri ori all possible situations       One approach  which has been gaining in populaJ ity  has been to mate a declarative model construction component with a sys tem for model evaluation  We refer to this approach as Knowledge based model construction  KBMC    In most previous KBMC systems  there has been a separation between the construction and evaluation components  The system generates a network from an expressive knowledge base which is then passed to an evaluative method  In the work described here  we present an algorithm which integrates these two com ponents  The method described here uses a database  which describes a class of probabilistic models  to an swer queries of the form   What is the probability of proposition x  given evidence y   Rather than building a model and then evaluating it  our approach searches through the knowledge base of model infor mation to answer the query more directly in a deduc tive style of reasoning  The basic approach combines elements of query based probabilistic inference algo rithms  such as          with existing model construc tion approaches      The result is an approximation algorithm for probabilistic inference  based on evalu ation of a partial model at each stage of the model construction process  The primary motivation for combining model con struction with evaluation is to provide better mech anisms for control of model construction  From a decision theoretic perspective  one wishes to continue to elaborate a model only if the benefits  quality of the answer to the query  exceed the costs  in terms of com putational effort   By combining evaluation with con struction  we can build an  anyti m e  algorithm by cal culating the implications of the partially constructed model  We can then stop at any time and return par tial information about the probability of a proposi tion  This will allow us  in turn  to take a decision theoretic look at the control of model building  in a simpler way than if we have to allow our model con struction component to run to completion  and then control the model evaluation  Automated model construction techniques are most appropriate where it is not practical to construct a fixed model in advance due to changes in the nature of queries or dependencies from case to case  Sepa rating construction and evaluation is useful in a situa tion where we need to configure the belief network to answer a class of queries over some period  A com bined construction and evaluation technique is use    Integrating Model Construction and Evaluation  ful in time pressured  knowledge rich domains  where time constraints make it impossible to use a large ex tensive model  The paper is organized as follows  In the next section  we review the model description language our program uses  Then we present and discuss the algorithm  We discuss a sample use of the algorithm  We comment on a prototype implementation  We conclude with some discussion of research directions that this algorithm opens up     Review of ALTERID Language  We have adopted the language of ALTERID for our ap proach  The basic structures are described briefly here  see     for a more detailed discussion   We will illus trate the constructs using relationships from a network originally presented in      Deterministic relationships in the domain are repre sented with a set of logical formulae  A formula is atomic if it is of the form P  c   z          en   where P is a relational constant and the Zi are variables  low ercase  or object constants  uppercase   Facts  P     and rules  P  Q  are defined in the normal manner for Horn clause logic programs  To capture the notion of mutually exclusive  collec tively exhaustive  sets of outcomes for a variable  we introduce the notion of alternative outcomes  The notation P z   A  B   means that for all values of z  exactly one of P  c  A  and P  c  B  is true  We will denote this set of outcomes by n  for example  flp  z   A B      P z A  P  c B    One of these out comes will be indicated by wp  For our test domain  the set of alternative outcomes is as follows  Cancer   YES  NO  y  Serum Calcium  BAD  GOOD   y  Tumor    YES  NO  y  Coma  YES  NO  y  Headache  YES  NO  y  A probabilistic dependency is an expression of the form  where P is an alternative outcome expression and each Qi is an atomic formula  possibly an alternative out come expression   and Pr is a conditional probability distribution over the alternative outcomes of P given the alternative outcomes for Q     Q       I  Qn  The dependency describes the uncertainty regarding P in the state of information where Ql    Q            Qn is true  For the cancer domain  we describe the condi tional probability of coma given its predecessors as       YES IO     y  pTumor    YES lfO     y    BAD GOOD    y          Pr wcoma WTumor WCalc     Coma    Serum Calci  Other conditional probability relationships are repre sented in a similar manner  In Section   we illustrate the construction procedure on this example  Algorithm MCE     The MCE  Model Construction Evaluation  algo rithm constructs and evaluates a model for a condi tional probability query of the form P HJE   where H is a alternative outcome statement  and E is an evi dence set of the form E    WE  i  E    WEl i En   WE   l  All evidence relevant to the hypothesis H is in cluded in the set E  The output of the algorithm is a matrix describing the probability distribution for ran dom variable H  given evidence in E and probability model information given in the database  In addition  at any time during the operation of the algorithm  a search state can be queried to generate an approxima tion to the query  This bound is based on evaluation of partial probabilistic model         MCE is an agenda based search algorithm  In the sec tion below we describe the search states  and the oper ators which can be used to yield successors of a search state       Search state s  Search states will contain information about the goal of the search  information about the unifications that have been done in the search  a graph which repre sents an expression  possibly partially evaluated  for the target probability distribution  as it is known so far  and some control information  Formally  we de scribe a search state as a tuple  S    P   e  G  M    We address each of the components of the search state in turn     Sub goals  p  These are formulae  which may represent random variables or categorical facts to be retrieved from the database  They may have associated out edges when added by the algorithm below  This is because we add a node to the query graph  see below  only when we have found all of its parents  causal influences      A substitution  most general unifier   e  Since we will be retrieving modeling information from a de ductive database  information about the binding of logical variables must be maintained     A graph which represents the current form of the expression for the queried probability  G    V  E   Associated with every vertex v E V is a distribu tion  Note that a vertex v may correspond to a set of random variables whose distributions have been             Goldman and Breese  multiplied together  There is an index function from formulae random variables to graph nodes     A set of formulae whose probability has been marginalized out of the above expression  M  This information is used to detect when a ran dom variable has been prematurely marginalized out of the conditional probability expression  The algorithm is invoked initially with a state of   H E              The goal is construction of the graph G that can be used to correctly answer the query       Search actions  Search actions fall into two broad classes  those that serve to construct the current model and those that partially evaluate the model  Broadly speaking  the model construction extension search actions take a sub goal  a random variable   and add a correspond ing node to the graph  In the process  new sub goals may be generated  since causal influences on the cur rent sub goal must be found  The model construction actions are all based on the  Causal Belief Net Algo rithm   in      The alternative to expanding the model is to partially evaluate the probability expression  The two actions used to evaluate the graph are    combination of nodes  and    marginalizing out a random variable  The for mer corresponds to clustering      and the latter to node absorption       Our treatment of the evaluation actions follows conventions introduced in the Symbolic Probabilistic Inference algorithm       Marginalization is necessary to find a numerical answer to the query  Marginalizing early also may reduce the cost of eval uating the expression  by eliminating some multipli cations  However  it also has the potential of wasting effort  if the marginalization is done too soon  if a node is marginalized out before all direct influences on it are found    Note that the search must be controlled so that find prob dependency is never applied to a sub goal P in S to which the find in graph action may be applied  see below    prove goal P  S  P an atomic formula  For every    E prove P      create a new search state as fol lows  S       r   P       G  M   In the new search state the subgoal P has been re moved because it has been proven   prove F     is a standard Prolog style horn clause de duction system  It returns a set of substitutions every one of whose elements is some    such that FB  fol lows from the contents of the database  and    is an extension of the previous substitution  e  find in graph P  S  there is a node N SUbstitution e   extension Of e SUCh that  E  V  and a  Ne  Pe  create a new search state as follows   S     r   P     G  M  G  differs from G only in the addition of out edges from N to children of P  In this case we have found a new path from some child node to a random variable that has already been in cluded in the model   detect marg error P  S  there is a node N E M  and   SUbstitution      extension Of e SUCh that NS    Pe  then Fail  This search state is invalid because some node has been marginalized out before all of its chil dren have been included in the model   We now describe each search action  starting with the model construction operators  and then the evaluation                multiply N  N   S  For N  N  E V  The multiply action merges together two graph nodes  and in par allel  multiplies together their distributions to give a new distribution  possibly with a larger state space    operators   Model Construction Operators  find prob dependency P  S  For P a sub goal of the search state S and for each probabilistic de pendency statement of the form  AlB  with B     Ql    Qn  for which there is a substitution  e  ex tending e such that Ae   create  a  new search state    as  Pe  follows     P     P  U B         G   M  G  is formed by adding P to G  A node for Pis added to V and all edges from P to nodes it causally influ ences are added to E   Evaluation Operators  The result of the multiply action is a new search state as follows  s     P    e  G   M   It is exactly as the previous search state  but We replace N and N  with an altered G  with a new node NNN   We replace all edges  z  N     z  N     N  z     N    x   with new edges   z  NNN   and  NNN   x     We multiply the proba bility distributions of N and N  to give the matrix for NNN   whose dimension is the union of the dimen sions of N a nd N     Integrating Model Construction and Evaluation  margin F  S  For F a formula whose state is referred to in exactly one N E V  Marginalize out the random variable F to give a new search state as follows   S       P e G  M u   F     The new graph  G   is the same as G  but the values of random variable F have been marginalized out of the node which is indexed under F  We mark the state to indicate that F has been marginalized out  This is used to detect errors in action detect marg error       Evaluating Partial Models  The benefits from a combined constructor evaluator arise from the ability to monitor the progress of the construction algorithm in terms of its progress towards answering the query  For example  in time pressured domains or with extremely large knowledge bases  it may be necessary to cease model construction activity before all causal and diagnostic links have been ex plored  We need to be able to access each search state and use the currently constructed model to provide partial information  e g  probability bounds  about the query  In the model construction algorithm described here  each search state contains a network Gs    V  E  representing a partial model  that if successfully com pleted  will be capable of answering the query exactly  Let GJ    V  E   be a complete and consistent net work constructed through a successful termination of the MCE algorithm  Let  s be the set of all such possible successful completions    s      GJIGJ E Descendants S    where Descendants S  are the search states accessi ble from state S  Evaluation of the partial model Gs consists of making probability statements about dis tributions consistent with all networks in  s  Obvi ously  we do not have the completion set  s to work with when we do the partial evaluation  but we can make some statements about its characteristics based on the the current state of the search alogorithm and the query  In general  this involves making some as sumptions about how the model construction sequence will terminate  We have identified three modes for evaluation of par tial models during a construction sequence         Correct Scoring  In this alternative  we make no conclusions regarding the ultimate distributions without a conclusive proof of correctness  Thus  probability statements must be consistent with all possible completions  Unfor tunately  for most construction algorithms  including ours  a highly evocative diagnostic link may be added to a model at any time rendering proper bounds non informative          Default Scoring  Here we assume that any completely specified frag ment of the network currently being constructed will constitute the finally constructed network  We look for all nodes whose immediate predecessors and indirect predecesso rs are fully specified in Gs  Any nodes in Gs that rely on subgoals still in p are not included  We apply an exact algorithm to this subnet  This pro cedure is equivalent to assuming that all remaining subgoals in p will fail  Another way of viewing this assumption is that at the time we ask for the answer  the currently constructed model is all that is avail able and should therefore be used  Obviously  as new subgoals are proven the structure and results of the query will change  This nonmonotonic behavior of the partial evaluation reflects the same concerns identified in previous defeasible probabilistic reasoning schemes                  Interval Scoring  In this method  we treat the partially constructed model as an interval based network  In this method  we treat nodes that have been identified but whose parameters are as yet unspecified as having probabil ities in         We process the resulting intervals on the query using node absorption and arc reversal pro cedures developed for interval probabilities         In contrast to default scoring  we assume that the sub goals in p will succeed  but the complete specification of the parameters of the model is incomplete  This techniques also exhibits nonmonotonic behavior  We are currently experimenting with the behavior of these alternatives as mechanisms for partial evalua tion  We are continuing to further refine partial evalu ation methods applicable to particular types of search       Search Control  Two issues dominate the control of search for MCE  The first is management of the search for a model  As soon as possible  we would like to discard partial models which do not fit the query  We argue that this issue is similar to the issue of discarding inappropriate proof trees in automated deduction  and will have little to say about this here  The second search issue is the scheduling of margin actions relative to other search actions  We are in debted to Schachter  et  al       for this perspective on model evaluation  Two search decisions must be made in choosing to employ the margin operation  The first is when the operation is likely to be correctly applica ble  The search algorithm should have heuristic infor mation which prevents it from prematurely marginal izing out nodes  Assuming that one avoids incorrectly removing nodes the second decision addresses select ing the order of marginalization to minimize the cost of evaluating the query expression              Goldman and  Breese  We are just beginning to explore the issues of search control for the MCE algorithm  We return to the sub ject of search control in the section on future work     Example  To give a flavor for the use of the algorithm  we will describe one way of answering a query from the sim ple medical diagnosis domain introduced in Section     We will assume that we have observed that our patient  Sam  has a headache and is in a coma  We would like to assess the probability that Sam has cancer  We create a search state with the following sub goals    cancer  a lt SAM    headache HEADACHE SAM    coma COMA SAM   The search state also has an empty digraph  G            empty substitution  e      and empty list of marginalized nodes  M       Note that we have felt free to direct the search in this example by hand  for instructional purposes  and also to keep the number and size of search states manage able  Let us assume that the algo rithm chooses the first of these subgoals  the query to be investigated  and the find prob dependency search action    First search action  In the database  there is a probabilistic dependency statement which gives a prior probability for the query  cancer  alt sam   This yields a new search state  In this new search state  the only two remaining sub goals correspond to the two pieces of evidence  The node contains a digraph which contains only one node which represents the query variable  There are a num ber of bindings in the substitution  Because the logic programming aspects of this example are simple  and in the interests of brevity  we will not further discuss the management of substitutions  We also now have a partial probability model  There is a single node for cancer in the network  Partial evaluation of this model at this point can proceed in several ways as discussed above  Under the default method the incremental answer is just the prior  We assume that additional subgoals will fail in the sense they will add no relevant dependencies to the model  Second search action At this point  with only one node in the graph  and that the query node   there are no nodes available for multiplication or marginal   ization  So the search algorithm will apply the find action again  to find a probabilis tic dependency for another sub goal  This time we search for causal influences on the headache observa tion   prob dependency  We retrieve from the database the statement which reports that headache depends on the presence or ab sence of a tumor  The conditional probabilities of headache and no headache based on the possible val ues of tWilor are also retrieved from the database  The resulting search state has three sub goals  a new goal for the predecessor of the headache variable  tWilor  and the previously existing one for the remain ing piece of evidence from the initial query   COMA COMA SAM   The two nodes in the digraph now are  one for headache and one for cancer  whose connec tion is still unknown  Partial evaluation at this point still returns the prior  There is no dependency yet uncovered by the algo rithm  Third and fourth search actions There are still no nodes available for evaluation actions  so we choose to apply find proh dependency to the tWilor sub goal  We find that tWilor depends on the outcome of cancer  Cancer is once again added to the list of sub goals  Note that the connection between cancer and tWilor nodes has not yet been found   Applying the find in graph action to the new cancer subgoal uncovers the connection  The node for the cancer variable  which we added in our first search action  is found already in the digraph  The resulting search state has only one remaining sub goal  coma  The corresponding digraph is given as Figure    Early evaluation  actions      We are now pre sented with the first opportunity to employ the eval uation actions  We may be certain by inspecting the database that the headache node will not causally in fluence any other nodes   Accordingly  we have the op portunity to multiply it into its parent  and marginal ize it out of the graph  Note that since we have ob served the value of the headache node  the effect of marginalizing it out is only to remove a number of ze ros from the combined node s matrix  The result of these two actions may be seen in Figure     At this point the partial evaluation action could also be undertaken under the default method  assuming that network illustrated in Figure   will be the final net work  Calculation of the query probability would pro ceed by calculating the joint probability of cancer and tWilor and summing out tumor    There will  in general  be ma ny sequences of actions which would produce an answer to the query   Note tha t it is necessa ry always to make sure that the  find in graph action would fa il before using find prob dependency  This is easy to achieve a nd we will let this pass without comment from now on   At this node into  Completing the diamond  actions        point  we could either multiply the  cancer   In Section   we discuss ways to use the structure of the database to control search    Integrating Model Construction and Evaluation  P  AI       cancer    headache   Figure    The search state just before the diamond is completed  P     coma  Figure    The search state after four search ops   the node for tumor  or we could apply the find prohdependency action to the remaining sub goal  coma  Assuming we choose the latter  we will find that the coma variable depends on tumor and on serum calcium   Further applying the find in graph action to the sub goal and the find prob dependency ac tion to the serum calcium sub goal  we arrive at the search state depicted in Figure    tumor  At this point we have an incomplete network  We know serum calcium is in the network  but we do not know its probabilities  We apply a combination of interval and exact transformations to the diagram to calculate a bound on the query probability  get ting a result that the probability lies in the interval              Note that this result ignores the possibility of a dependency between cancer and serum calcium  A final find in graph action will complete the dia mond   p Af      coma   headache   Figure    The search state after two evaluation oper ations   Completing the query The process of answering the query may now be completed by a series of eval uation actions  We suggest the following series  but others are also suitable     marginalize out serum calcium  This leaves a generalized distribution at coma which depends only on cancer and tumor     marginalize out tumor    multiply cancer into coma    marginalize out coma and normalize to get the distribution for cancer  P canceri Ev   r                       Goldman and Breese     Implementation  The algorithm described here has been implemented in  Sun Lucid  Common Lisp  running on Sun SPARe stations  It has been tested on the example given here and other examples like it  The program has been written in several different modules  one that manages the deductive database  one that manages the matrix operations  and one which manages the search opera tions  Influence diagram processing is performed with IDEAL       We thank Peter Norvig for allowing us to use his Prolog interpreter in Common Lisp for our deductive retrieval       The code is still in prototype version  and many oppor tunities for optimization remain  The bounding calcu lus has not been integrated into the construction cycle  We are still using only a generic agenda based search algorithm  For this algorithm to be practically usable  we will have to extend the code for agenda mainte nance to better control the search  Elsewhere in this paper we have suggested search control methods we believe will be successful for this program  We will be investigating these heuristics and their interaction with other aspects of the work   e g interval process ing   We will also be developing better implementa tions of existing elements of the system     Future Directions  The work described in this paper is continuing on a number of avenues  We will be conducting experimen tal tests to explore the behavior of the algorithm over several databases  In particular  we wish to explore the interaction of partial evaluation with various meth ods for search control  A related issue revolves around maintaining probability interval information in prod uct form for generalized distributions  To avoid premature marginalization  we are experi menting with a technique which makes use of infor mation about the structure of the knowledge base  We suggest an application of the technique of marker passing  treating the rule base as a graph  There will be nodes corresponding to alternative outcome state ments  There would be edges from alternative outcome statements to causal influences  corresponding to the probabilistic dependency statements  As is character istic of marker passing  the values of variables would be ignored   edges would be drawn everywhere there was a possible probabilistic dependency relation to oc cur  Before carrying out the search for a particular query  nodes corresponding to query variables would be marked  Marks would be propagated from children to possible parents  Marks would have limited  mem ory  to cut off cycles  Each type of node would have a counter  Nodes would not be marginalized out until a number of children equal to the number of marks had been found  or until all possible CBN operations were done   This could be an over cautious heuristic  espe cially in the case of rule bases with much recursion    but should prevent premature marginalization  This technique can  of course  be  outwitted   by poorly structured databases  ones where there are few pred icates but many propositions   but well known tech niques for improving Prolog programs will also make this heuristic more accurate  We would like to complement a technique like that discussed above with a search control method which would weigh the chance of premature marginalization against its benefits  reduction in the dimensionality of the matrices  and hence the number of multipli cations   As mentioned in the previous section  we are also interested in taking an  anytime  approach to the MCE algorithm  taking into account the trade off between further model construction and evaluation actions  and termination of search with estimated re sponses to a query  The present version of the algorithm assumes that ev idence relevant to the query is identified in the query  Previous work  I  conducted a search for such evidence in the database  We wish to investigate the tradeoff in search efficiency for these alternatives  As discussed in the introduction  the ultimate goal of this research is to provide a facility for informed control of model construction  A construction proce dure that can evaluate its progress toward answering a query is an important step in this direction  
 This paper presents an approach to the design of autonomous  real time systems operating  in uncertain environments  We address issues of problem solving and reflective control of rea  soning under uncertainty in terms of two fundamental elements     a set of decision theoretic models for selecting among alternative problem solving methods and    a general computational architecture for resource bounded problem solving  The decision theoretic models provide a set of principles for prioritizing the assignment of computational resources among multiple problem solving activities under uncertainty and with respect to various time constraints  Alternative problem solving methods are chosen based on their relative costs and benefits  where benefits are characterized in terms of the value of information provided by the output of a reasoning activity  The output may be an estimate of some uncertain quantity or a recommendation for action  The computational architecture  called Schemer     supports the interleaving of  and communication among  various problem solving subsystems that provide alternative approaches to information gathering  belief refinement  solution construction  and solution execution  We  discuss the role of decision theoretic control in an architecture such as Schemer II for scheduling problem solving elements and for critical event driven interruption of activities      Introduction  An autonomous system  operating in a complex and constantly changing environment  must for mulate and carry out plans to achieve desired behaviors or objectives  In such an environment the synthesis and use of plans will typically be severely constrained by limitations on time  infor mation  and other critical resources  In response to these dynamically changing constraints  the system must be able to judiciously manage its reasoning and other activities to make the best use of available resources  We refer to problem solving under these conditions as resource bounded problem solving  controlling and adapting actions to meet contextually determined constraints  In particular  we address the problem of selecting among a set of alternative reasoning activities  the control problem  in service of some object level problem  the primary problem   The control problem exhibits considerable uncertainty since the performance of alternative reasoning methods on the primary problem is highly uncertain  Complex tradeoff s concerning the costs of using alternative methods and directly acting in the world need to be considered   The control problem is essentially an issue of belief management  the presence of significant amounts of uncertainty in a realistic task environment forces a system to constantly face a fundamental choice between using its      I I I I I I I I I I I I I I I I I   I I I I I I I I I I I I I I I I I I I  current information to carry out its primary objectives and making efforts via reasoning activities to improve its state of information   Researchers in artificial intelligence have long been interested in the topic of problem solving control  Some investigators have focused on developing general architectures with features that support explicit reasoning about control of problem solving actions               The primary em phasis has been on mechanisms and representations by which control knowledge might be used  With few exceptions       the knowledge itself has been developed heuristically with little emphasis on developing general principles of control  In this work  we will model the control problem in terms of decision making under uncertainty as formalized in decision theory  As indicated above  control involves resource allocation under uncertainty with complex preferences and the need to reason about the cost and quality of information  Representations and tools from decision theory are a promising path for analysis of these problems from a formal basis       This work has been motivated in large part by a desire to incorporate principled control pro cedures within autonomous real time systems  In particular  we are extending Schemer II  a com putational architecture that allows embedding various problem solving elements in an autonomous system designed for operation in a complex  dynamic  environments      Although Schemer II s de sign provides a very robust computational framework for applying appropriately chosen techniques for control reasoning  this architecture does not by itself offer any such techniques for making the appropriate choices  In the latter part of the paper we indicate how the analytic techniques and results derived here are being incorporated into the architecture   Decision Theoretic Control     Our approach to selecting among alternative methods for reasoning about a particular problem is a computational version of an idea proposed by Matheson      and more recently Nickerson      and Lindley       The outputs of a problem solving method are viewed as information in a decision theoretic sense  that is  the outputs of a model are used to update the probability distribution about an event or potential action  We are concerned with two classes of action  primary actions and modeling actions  Primary actions involve the system s interface to the external world  e g  moving an item  opening a valve  or initiating communication  Modeling actions operate on the system s knowledge to produce new conclusions or recommendations  We use the term modeling to capture the set of actions regarding structuring  solving  and interpreting a model of a domain  Alternative methods may be based on different assumptions and require different amounts of data and time to run  The solutions provides may differ in their quality   J  perhaps expressed in terms of different attributes of a solution       The formulation of the control problem in these terms is illustrated in Figure    Modeling actions  m  are selected from a space of modeling alternatives M  Sequences of primary actions  d  are selected from a space of decision alternatives     The decisions are represented as square nodes in the figure  The overall utility of the control problem is a function of the primary decision  d   some uncertain state of the world  x   and the cost of using a particular method   c    The cost can be thought of as reflecting the  possibly uncertain  time  data  and processor requirements to use a particular problem solving methodology  m  The output of a model is s  It is available at the time the primary decision is made  indicated by the arrow from s to din Figure     The output of a model is uncertain  It also is probabilistically dependent on the state of the world  the information s from using method m provides information about the uncertain state of the world  In this sense  a problem solving methodology acts as a sensor for some unknown quantity  We express this measure of quality of output as the probability distribution  Pr sjx  m    where  is the background state of information  or context  where the distribution applies     Cohen     has referred to this tradeoff as balancing internal and external action      I I I I F igure    Control Problem Influence Diagram The expected utility ofthe meta level control problem is Ex c Uid m e   where          U x  d  c  Pr xls m e  Pr slm e  Pr clm e   Pr slx m e  Pr xle  r x I    m e  p     fx Pr s x m e  Pr xle   by a standard application of Bayes  rule  The optimal primary decision  d   m s  is obtained by solving  maxExc Uid  m  s e  d    The distribution over the uncertainty has been updated with the model output  The optimal model m  is obtained by solving  max Euc Uid  m  s  m e  mEM  The above formulation can be extended and operationalized in several ways  as discussed below  Resource Usage and Resource Constraints   In the previous formulation  usage of computa tional resources is captured in the cost  c  of using a particular method  Resources that are limited or that can be expended variably to modify the quality of a computational result can be expressed by conditioning the output  s  on the amount of resource available  In a multi processor system  there is a tradeoff between the amount of time available for a task  and the number of processors assigned to a computation  For example  the quality of an output may depend on time to the next interrupt  ti  and the number of processors  n  involving in running method m   Pr s x m ti  n  e   In this case n is a control decision while ti is an uncertain quantity  The form of this distribution can be used to capture behavior of methods whose outputs improve monotonically as additional time or processing power is applied  such as Monte Carlo methods   as opposed to those which require some threshold to provide any useful output  Decision Recommendations  In the previous formulation  the most natural interpretation for the output of a model is that it provides an assessment or diagnosis of some uncertain state of the world  We can modify the formulation for models and methods that provide decision recom mendations  As an example  suppose we have an autonomous vehicle that needs to navigate to some objective  The system may embody several alternative means of determining a path to the destination  It could use its logical knowledge to construct a plan while not explicitly considering uncertainty or resource usage  It could develop probability and utility models at various levels of      I I I I I I I I I I I I I I I   I I I I I I I I I I I I I I I I I I I  Figure    Control Problem Influence Diagram for Decision Recommendations detail  to be solved using exact or approximate methods  It could dispense with a  planning  stage altogether and use local obstacle avoidance and reactive planning methods to attempt to arrive at the destination      Any of these methods will produce output of the same general form  a sequence of actions to be taken  possibly conditional on observations and possibly iterated as a policy  e g  as in a reactive algorithm   We illustrate this model with Figure    Here we are assuming the existence of some  true  optimal course of action d  dependent on the state of the world  d  is the recommendation that would be obtained by maximizing Up x  d   the primary decision problem utility function ignoring the costs of reasoning  It is assessed probabilistically  reflecting the system s a priori uncertainty of the optimal action  The output of a method  as previously  provides an estimate of the uncertain optimal decision  The diagram also indicates that the output of the method will be used directly as the primary decision  and the decision model can be solved using Bayes rule and maximization of expected utility in the customary way  The burden of assessing the probability distribution Pr  s j d   m    can be eased by expressing it in terms of deviations from the optimal value  For example  let Pr s Pm the d lm   probability method m will provide an optimal result is Pm  The dispersion about the optimum can be captured in a number of ways depending on the particular method m being considered   A method is unbiased for real valued d  and s when E sjd  m e  d            Cost of an Error  A key consideration in the choice among alternative problem solving methods is the extent to which a sub optimal primary decision will reduce utility in the primary decision problem  For each world state x and alternative d we can calculate   U U x  d     U x  d   These sensitivity measures are indicative of how forgiving a domain is with respect to selection of action  Estimates of this sensitivity can be used to parameterize control strategies across domains and problems  as we would like to perform this type of reasoning without precise specification of U x d  c      We have developed an illustrative example using the control model described above for a specific numerical robot path planning problem      The robot needs to select among a feasible path method  F   a basic probabilistic model  B   and a more complex  information probabilistic model  I  which considers the possibility of collecting additional information as part of the primary decision problem  Each method is characterized with respect to its probability of providing an optimal solution under uncertainty  The results of the analysis are presented graphically in Figure    Optimal regions depend on ti  the time to an interrupt  and   the cost ofan error  W hen ti and Ce are low then the non probabilistic modeling method  F  is optimal  As these parameters increase  more complete probabilistic reasoning becomes preferred  The type of information summarized in this graph can form the basis for simple control rules  depending on contextual information  Though easy to implement and deliver  they nonetheless are developed based on defensible and clear criteria   Lindley      has used assumptions of normality to obtain a na lytic solutions in a  similar problem        I I I I I  t     I  Figure    Optimal Reasoning Policies  The Architecture     Schemer    is a computational architecture for resource bounded problem solving    It has been designed to allow for the interleaving of solution construction  solution execution  information gathering  and knowledge management activities  At a coarse level of description  Schemer II is an object based blackboard system  Various problem solving modules reside in a shared knowledge space  The invocation of these modules  or handlers  occurs in various ways and is further mediated by the operations of a top level controller  which schedules various pending activities for execution  manages communication with the external world  and handles interruption and resumption of ongoing activities  Schemer II provides some unique and important features to support flexible  reactive control of problem solving  In particular  the architecture supports a wide variety of techniques for flexible  dynamic scheduling  the ability to employ special purpose problem solving modules that can modify the system s control state  and  perhaps most importantly  true pre emptive control providing the problem solving system the ability to react promptly and re focus its attention in response to the occurrence of critical events  However  until recently  both scheduling and pre emptive control were handled with strictly domain specific techniques  In this section we use the decision analytic framework described in Section   to analyze     choices amongst alternative problem solving activities and    generation and fielding of interrupt conditions for executing plans       Scheduling diverse problem solving elements  The Schemer II architecture supports encapsulation and interleaved control of multiple  indepen dent problem solving methods  Schemer II s handlers  with their object oriented modularity  meet this requirement by providing a discipline for encapsulating each problem solving element as a distinct type of object  Each handler can encapsulate a specialzed type of problem solving skill  Handlers provide convenient data structures that support a strong distinction between the infor mation that is strictly local to a problem solving element and information that is to be shared with other elements  Handlers in Schemer II are triggered by changes in data in the system  via communication with external processes  or by direct invocation  In any of these cases  a single triggering event may cause several alternative problem solving methods to be invoked  Furthermore  at any time multiple tasks may be on the system schedule awaiting execution  The scheduling problem is selection among  See Fehling       for a detailed discussion of the architecture  Successful Schemer applications have been built for  a number of real time   process management  applications such as diagnosis or control of complex manufacturing processes and automated performance management of advanced avionics systems among others                   I I I I I I I I I I I I I   I I I I I I I I I I I I I I I I I I I  these alternatives based on computational costs  data requirements  and attributes of the solutions offered by each method  In previous implementations the scheduler has used a simple pre emptive  priority based schedul ing discipline  In this approach the carriers representing potential tasks have an initial fixed priority prescribed by the system developer as a feature of their associated handler  On each cycle of the top level controller the current priority of each task remaining on the schedule is then  aged    viz   has its priority value modified  in some simple and application dependent manner  The disadvantage with this approach is that it is essentially hardwired prior to execution time there is no general facility to adjust scheduling decisions in response to changes in environmental characteristics  As a supplement to the priority scheme currently in Schemer  we are implementing conflict detection and  resolution routines for dynamically assigning and updating task priorities  A conflict is detected if several handlers are triggered for execution simultaneously  Once a conflict is detected  the system will look for specialized control knowledge to make a selection or allocation as exemplified by tradeoffs such as in Figure    If no specialized knowledge is available or applicable  then a handler which performs decision theoretic reasoning can examine the conflict  develop a control model such as described in Section   and make a recommendation regarding which task should be undertaken  A potential problem with this approach is entailed by the computational  and other   resource requirements associated with this method of reasoning about control         The activities of scheduling are  inner loop  in Schemer II s overall computational activities  If the computation costs required to explicitly perform a full blown cost benefit analysis on each cycle are too high  they will outweigh the value of this control reasoning no matter how formally sound and general it is  Thus  it may be necessary to restrict the real time estimations performed in scheduling on each cycle in response to limitations such as time deadlines  In extreme cases  it may even be necessary to abandon such a method entirely in favor of the default prioritization scheme       Critical event driven control of reasoning  One of the most important objectives in the evolution of the Schemer II design has been to fun damentally support problem solving processes whose control is responsive to critical changes in the problem solving context  A problem solver dynamically formulating and executing solutions to problems in an uncertain environment must be able to react promptly to the asynchronous oc currence of such critical changes  In response to such changes  the problem solving system may decide that its current actions are no longer the most preferable ones  In using earlier versions of Schemer in applications that must exhibit reactive  real time performance  we found the capacity for  interrupt driven  control of pro blem solving to be of paramount importance  In Schemer II the occurrence of some critical event can initiate a response to immediately interrupt execution of the currently scheduled problem solving tasks  suspend them gracefully  and commence tasks that are more appropriate in response to the changed information about the problem solving context  This is readily accomplished by the use of special event handlers that carry out these actions in response to pre defined critical events  This aspect of Schemer II s design is a natural evolution of the mechanisms for  opportunistic control  typical of blackboard systems such as Hearsay II      The discussion in the previous sections focused entirely on the  planning  phase of a combined control and primary decision problem  A real time system both plans and executes actions  Suppose the system has solved both the control and primary planning problems  and is now executing the sequence of steps in the primary problem  which may involve a series of compute intensive low level tasks  We need to define a set of critical events that would render the current plan inappropriate or inoperable  and signal a need to replan at a higher level  Critical events are defined with respect to the modeling method used to generate a particular course of action  If the output of model is in the form of a decision recommendation  d  n  we annotate the recommendation with a set of assumptions on which the recommendation was based       I as in  d  m  The system will continually sense its knowledge base and the environment for conflicts with the set m and trigger a replanning task when this conflict occurs  The set m is a subset of the full set of assumptions  both implicit and explicit  which are embodied in a planning method  There are complex tradeoffs involved in identifying this  crit ical  subset  Clearly only those assumptions which when violated would cause a change in a recommended action should be included  One class of important assumptions relates to mutual exclusivity  If the system detects a condition that is not among a set of enumerated possibilities considered in generating a plan  the plan may be invalid  Other possible classes of critical assump tions relate to the validity of data  probabilities  or defaults used in a model  However  providing sensitivity to critical events with an interrupt structure causes an increase in system overhead and detracts from performance on other tasks  Additional analysis of this tradeoff in the context of real time reactive planning and execution is needed     Conclusion  This paper has described efforts to apply decision analysis to the control of problem solving within a computational problem solving architecture  We have addressed control of both assessment and planning methods  The formulation makes it clear that a system s ability to make well founded decisions about the control of its own problem solving activities is a problem of information man agement  and we use concepts based on value of information to perform these allocations  This research addresses a limitation of much of previous research on problem solving architec tures  Schemer II is a well tested and highly evolved computational approach to resource bounded problem solving  The architecture allows encapsulating and interruption of alternative problem solving methods so that various problem solving techniques can co exist and be scheduled for exe cution as needed  One critical limitation of previous research with Schemer was that the methods for coping with uncertainty and for reasoning about control were ad hoc and application specific  Adoption of decision theory promises to rectify this shortcoming by providing a set of well founded and rigorous principles for managing internal resources and other decisions under uncertainty  Current research efforts involve incorporation of the decision theoretic control methods de scribed in this paper into the latest implementation of Schemer H  In future work we will be characterizing various problem solving methods with respect to their quality of information  as well as analyzing time and other resource consumption issues  Additional methods for analyzing inter rupt conditions need to be developed  including development of formal justification and generation of interrupts     Acknowledgements  I I I I I I I I I I I I  We thank Eric Horvitz  Jackie Neider  and Sampath Srinivas for comments on an earlier draft of this paper   I  
 In previous work        we defined a mechanism for performing probabilistic reasoning in influence dia grams using interval rather than point valued proba bilities  In this paper we extend these procedures to incorporate decision nodes and interval valued value functions in the diagram  We derive the procedures for chance node removal   calculating expected value  and decision node removal   optimization  in influ ence diagrams where lower bounds on probabilities are stored at each chance node and interval bounds are stored on the value function associated with the diagram s value node  The output of the algorithm are a set of admissible alternatives for each decision variable and a set of bounds on expected value based on the imprecision in the input  The procedure can be viewed as an approximtion to a full n dimensional sensitivity analysis where n are the number of impre cise probability distributions in the input  We show the transformations are optimal and sound  The per formance of the algorithm on an influence diagrams is investigated and comparerd to an exact algorithm   probability and utility assessments  Exact verifica tion of the sensitivity of recommendations to all pos sible combinations of imprecise inputs is extremely costly from a computational perspective  The proce dure developed here reduces the computational cost to that of solving an influence diagram once  In Sec tion   we explore the nature of this approximation relative to an exact procedure  An influence diagram I    N  A  consists of a set of nodes N and arcs A  The set of nodes N   U U   U V   where U is a set of chance nodes     is a set of decision nodes  and V is the single value node  Associated with each node X E U is a set of conditional probability distributions relat ing X s outcomes to those of its predecessors  IIx  in the graph  Interval influence diagrams differ from the standard influence diagram formalism in that we specify lower hounds on the probability distributions associated with each chance node  The lower bounds are interpreted as follows  we admit any probability interpretation  p  for the diagram iff VX EU  b zlrx   p zlrx    where rx is an outcome of the combined set of states of the predecessors of X  Lower bounds for the prob ability of each possible value of the node given its The difficulty and expense of assessing probabilities predecessors are defined for all chance nodes in the for large models has motivated research in techniques graph  The upper bound u zlrx  on each probability for perform reasoning under uncertainty with under is implicit in the lower bounds   specified or constraints on probabilities  Our work in this area        developed a language of independent u zlrx       I  b z lrx  lower bounds on component probabilities in a belief z  z network as a means of expressing the imprecision in We have defined operations of chance node re probabilities  In this paper we extend the previous analysis to include influence diagrams which contain moval   corresponding to marginalization  and arc reversal   corresponding to an application of Bayes  decision and value nodes  This extension provides a capability for assess rule  when uncertainty is expressed in terms of lower ing the robustness of a set of decision recommenda bounds and conditional independence is captured by tions from an influence diagram given imprecision in the topology of the influence diagram         In this     Introduction        paper we define operations of chance node removal into the value node  corresponding to taking a con ditional expected value  and decision node removal  corresponding to maximizing expected value   The expected value is expressed in terms of a lower and upper bound  expressing the imprecision in value for each combination of predecessors  Processing of deci sion nodes will generate sets of decision alternatives which are admissible based on the imprecision in the input probabilities and values  in a manner analogous  The next definition generalizes the idea of a regu to include value constraints   I  is     Regular Constraint  A constraint said to be a regular constraint iff it is equivalent to its regular extension   I  Based on these definitions  one can define func tions which describe the regular constraints  For  I  lar constraint given in     Definition  probabilities we have   to the analy sis of sets of distributions consistent with a model developed previously   Definition     bc  x     Let Pu denote a probability distribution over the space of variables represented by the nodes in U  con ditioned on each possible alternate decision set ex pressible by the nodes in D  Let  Pu  denote the class  of all such distributions  Similarly  let V denote the set of all value functions for a value node V given its predecessors    Definition      Constraint Function  Probability   The function bc x  is said to be the lower bound of c at the point x iff  Definitions     inf  pEe  p x        v   llv      is the value function set associated with a value node V in I   We will drop the subscript on Vv when the value node designation is obvious   Definition    Constraint Functions  Value  If    is a regular value constraint for a diagram I then  are the upper and lower value constraint functions for     In         we found the need to refer to the concept  Definition    D compatible  Probability  A regular extension of a probability joint distribution  p  is said to be D compatible to defined in        as follows  an acyclic directed graph I    N  A  if and only if there is a labelling of nodes in U    Xt  X           with associated variables x n   such that  I  s   V is a general constraint on value  Definition    Regular Extension  Probability   The set c is the regular extension of a constraint c iff   c     p E Plp z   ipf p  x    p  Ec  Similarly  we now define the regular extension of a value constraint  including upper and lower limits explicitly       l We      Similarly  we have need of a compatibility con cept for value functions with respect to an influence diagram   value  D compatible  Value  A function for an influence diagram I is D compatible with I with a value node V if it is a function from the immediate predecessors of V to the reals  Definition   Regular Extension  Value  The  set     is the regular extension of    iff       v E VI  I I  tions in P  and        I I  functions in V  The  Definition  I  of compatibility of a probability distribution with an influence diagram   A general constraint is any subset of the sets P or V  Thus  c  P is a general constraint on distribu constraint was  I I  For value constraints  we have    Value Function Set  The set  Vv  I  inf  v EV  v   rv   S v  rv    S sup  v EV     v   rv     tue the term  value  function to refer to the expecta tion of value throughout the processing of the diagram      v ntl  We now link these concepts with that of a regular constraint   I I I I I I I        I I I I I I I I I I I  I I I I I I      Figure    Removal of a decision node predecssor to Figure    Removal of a chance node predecessor to the value node  the value node  v  Consider the single transformation on I producing  Diagram Regular  Value  We say a constraint v is diagram regular with respect to a new diagram with Y removed  Then for all  z   a diagram I iff for all v E v we have  vu  z     vu Yr z ue Yrlz    L vu y  x be Yalx  ifr    v is D compatible with I  and  Definition     v     is a regular constraint   Definition     Diagram Regular  Probability  We say a constraint c is diagram regular with respect to a diagram I iff there exists a set of regular con  straints C i  such that p  E  p      For each term in Equation     is  Transformations  We now present three theorems which provide the fundamental operations necessary for evaluating an influence diagram to obtain a policy based on max imization of expected value      A sequence of these operations  illustrated in Figures      and    are suf ficient to evaluate any diagram          VL y  x uc Yalx    L VU Yi x bc Ydx   if a  where y  and Yr depend on z and are such that V i VU Yr   Z      VU Yi x  V i  vL y   z      VL Yi x    D compatible with I  and  where be    is given by       VL  z    c iff       I I  becomes  becomes  are the least upper bound and the greatest lower bound respectively for v  x    EY v y   z  p y l z  over all p l  E c and v     E v  Theorem   provides a method of calculating new intervals for the value node given an initial set of in tervals for the value node and the chance node pre decessor to be removed  The proof to this theorem is an almost immediate consequence of Lemma   in      Theorem    Decision Node Removal  Consider a diagram I with value node V having im mediate predecessors Y E U U  D and decision node  E  D  with Y an immediate predecessor of D  See Figure     Let vu y d  and VL Y  d  be the upper and lower value constraint functions for a regular con straint v  Consider the single transformation on I Theorem    Chance Node Removal  producing a new diagram with D removed  Then for Consider a diagram I with value node V whose im ally  mediate predecessors are X E U U  D and Y E U and     vu y    max vu y d  with X an immediate predecessor of Y  See Figure dES y      Let bc YI z   be the lower bound constraint func vL y    min IIL Y d      tion for y given z for a regular constraint c and let dES y  be the upper and lower value and  y  z  VL y  z   vu   constraint functions for a diagram regular constraint with S y     d j    j vu y d     vL  Y d i        D        I I  v  I I  becomes  I Figure    Reversal of an arc in an influence diagram   Figure    Determination of new value intervals from value intervals associated with individual decision alternatives  Alternative d  dominated by d  and d   Let v be any value function in V  Let d   y  de but not da  The new interval is determined from the note the optimal decision policy for this value June bounds imposed by the non dominated alternatives    tion  Then  for ally  d   y  E S y   This corollary states that the optimal policy are the least upper bound and the greatest lower bound which would have been generated by the point valued respectively for procedure is included in the set of admissible deci sions generated by the interval valued procedure  v y    v  y  d   y   Loui in     defines two separate criteria for admis sibility with interval valued probabilities  The first is for all v  y  d  E v  where d  y  is the optimal decision as stated above  The second  paraphrased  is that d policy for value v  solving maxd v  y  d    is  E admissible  iff Theorem   provides a method of calculating new  p E c  v E v   Vdi  intervals for the value node given an initial set of in   tervals for the value node  Equation s        says that the new upper  lower  bound in value is just the       maximum  minimum  of the previous upper  lower  It is fairly straightforward to show that  in the case bounds on value  among the admissible decision alter of diagram regular constraints  these two definitions natives  Admissibility is defined in Equation    An are equivalent  Finally  we state the reversal theorem alternative d  is admissible if there does not exist an given in      generalizing it slightly to include decision alternative whose value interval strictly dominates  nodes as possible predecessors  This notion and the calculation of new value inter vals is illustrated in Figure    Theorem    Reversal  Consider a diagram I with In lieu of the single decision policy recommenda chance node X E U immediate predecessor Y E U  tion generated by a point valued influence diagram  with Vt E U U V a predecessor of Y but not X  the interval valued procedure creates the sets S y   V  E U U    a common predecessor of X and Y  which define the admissible decisions given values for and Va E U U  J a predecessor of X but not Y   See the predecessors of the decision node  We have the Figure     Given lower bound constraint functions following simple corollary with regard to the admis bc zly  v   va  and bd Y Vt  v   for all values of z sible set  S y   and y and associated regular constraint sets c and d  together with their corresponding upper constraint Corollary    Admissibility  Consider the same functions uc zly  v   va  and ud YIVt  v    Consider conditions as in Theorem    Let the single transformation to the diagram reversing the direction of the arc between X and Y  Let  The proof is straightforward  and brevity is omitted      I I I I I I I I I I I I I I        I I I I I I I I I I I I I I I I I I I  This definition says that a transformation is sound if the new value function and probability dis tribution that one would have obtained by applying the exact transformation to individual members of withy  chosen such thaty  if  y and uc zly  v  va   the original constraint sets is contained in the sets uc zly  v   va  for all Yi if  y  y   For all z and y  we produced by the operations described in Theorems      We state without proof  define b  ylz  v  v  va  as follows  bc zly v  va bd YIVt v    uc zly   v   va ud Y     l     t       uc zly  v  va bd Y Ivt v       I W f  z y vl v  va      then     Soundness  Algorithm  A   consist ing of a node removal or arc reversal as detailed in Theorems       and    is sound   Theorem  Soundness is a weak condition  We need to show that the intervals calculated by Theorems     are best    If W z y v   v  va      and there exists Yi if  in some appropriate sense  y y  such that uc zlyj V  va ud Y ilvt v        Theorem    Minimality  Let I   N  A   be an then we take by convention  influence diagmm and let CJ and     be diagmm reg ular constraints on the probabilities and value func tions for I   Let H be a single topological operation on I producing I   let  AH  ci  and  AH       be the con    Otherwise b  ylz v  v  va  is indeterminate  stroints on the distributions and value functions D When b  is determined  it is the greatest lower compatible with I  produced by H  Let H be the map bound for ping corresponding to H from distributions and value functions D compatible with I to those D compatible with I   Then  letting C  I    and  C  I   denote the set of diagram regular constroints for probability func tions and value functions with respect to the image diagmm I   and if     I  B  Soundness and Optimality  erotion on a diagrom I to produce a new diagrom I   H I   H represents either an arc reversal or a node removal  We define any interval tmnsforma tion algorithm   A  as an operotion on diagmm regular constraint CI and diagram regular value constmint III to produce corresponding diagmm regular constmints  An ci  and  An III  for I   We say  A is sound if for all p E CI and all v E III we have H p  E  An ci  and H v  E  An III   where H is the tmnsformation on distributions and value functions D compatible with I to distributions and value functions D compatible with I    An  CI  is the set of probability distributions produced by  A for I  that corresponds to the opemtion H    CJIH ci s cl   and c   M      IIH   I s  llf    In     we proved soundness and optimality proper ties for transformations to interval valued probability networks  In this section we state the analogous the orems for diagrams which include decision and value nodes and use the transformations stated in the pre vious section  Referring to Figure    we define  Definition     Soundness  Let H be a single op     and      E e  C II       C  I      we have   This theorem says that each transformation on constraints given by algorithm  A  produces a dia gram regular constraint set which is the smallest of all such sets that remain sound    S  Empirical Results  The approach described in this paper has been imple mented and tested on a variety of influence diagrams  In this section we describe some experiments on a par ticular diagram and illustrate use of the algorithm to examine robustness and sensitivity of results  For the  For brevity the proof is omitted         I I I I I  Figure    Mappings on probability distributions  H corresponds to a topological operation on a diagram I to produce a new diagram I    H I   H represents a mapping from the space of probability distributions or value functions that are D compatible with I to the corresponding spaces that are D compatible with I   An interval transformation algorithm A maps constraint sets into constraint sets  The algorithm is sound if H cr   Ae cr  and H   r   Ae  r   purposes of this discussion  we have encoded the oil wildcatter s decision model as an influence diagram   See Figure     The model has two decisions  The node labelled TEST is the choice among alternative geologic tests of the seismic structure in an area  No test  a cheap test and a perfect test are the alterna tives  The other decision is whether or not to drill  The arcs into the DRILL node indicate that the type of test and its result will be available when deciding whether or not to drill  TEST RESULTS provides in formation about SEISMIC STRUCTURE  which in turn provides information about the AMOUNT OF OIL  One way to characterize the interval influence di agram  liD  approach described here is to compare it to an exact approach to calculating the ramifica tions of interval inputs  The results labelled EXACT below refer to calculating values and decision recom mendations for all combinations of the endpoints of the input probability ranges  Tables   and   display the impact of using a lower bound inerval approach when three different levels of imprecision are added to the original diagram  Specifically  we examined probability ranges  of          and     for three nodes  AMOUNT OF OIL  SEISMIC STRUCTURE  and COST OF DRILLING    The exact procedure con sisted of solving the network for optimal decisions for each of      possible configurations  The exact ex pected value ranges and admissible decision sets were based on these runs   The r R     u z    b z          L  b       For th is  study  we selected a IJUbeet of nodes in the diagram for analysis and introduced this range into each distribution residing in the node   The primary decision is whether or not to DRILL  Recall from the influence diagram that the DRILL decision is conditioned on the type of test se lected and its result   one of  NS    OS   or  CS     Tabel   shows the admissible decisions for the the various possible information states for the DRILL de cision  using interval influence diagrams and the ex act procedure  The liD and the EXACT procedures provide identical sets of admissible decisions for this variable  indicating for this decision liD is a perfect approximation to the exact analysis in terms of deci sion recommendations  For the TEST decision liD is a less than perfect approximation  Table   shows that as soon as any imprecision is introduced  the liD procedure is un able to distinguish among the alternatives for TEST  At a     level of imprecision  the EXACT algorithm cannot distinguish between the  none  and  cheap  test options  The table also shows the intervals in ex pected value associated with each procedure at each level of imprecision  We can also use the liD procedure to explore the sensitivity of results to imprecision in various sets of chance nodes  For example  in Table   admissible de cision sets for the TEST decision have been generated far probability range     for various subsets of the nodes AMOUNT OF OIL  SEISMIC STRUCTURE  and COST OF DRILLING  The table indicates that results are least sensitive to imprecision in condi tional probabilities for COST OF DRILLING  Sen sitivity of results to SEISMIC STRUCTURE and AMOUNT OF OIL are approximately equivalent ac cording to the table   I I I I I I I I I I I I I I        I I I I I I I I I I I I I I I I I I I  Figure    The influence diagram for the oil wildcatter  Profits depend on OIL REVENUES  COST OF DRILLING  depending on whether the DRILL decision   and TEST  TEST represents a choice among geologic tests whose results are available when the DRILL decision is made   Range                  IlL                       EXACT TEST type IICJ none           none none           none cheap  IlL                      IIU                       liD TEST type none noneI cheap  perfect none cheap perfect none cheap perfect  Table    The table shows the sets of admissible decisions for the TEST decision for various input probability ranges   Nodes Analyzed AMOUNT OF OIL SEISMIC STRUCTURE COST OF DRILLING SEISMIC STRUCTURE COST OF DRILLING AMOUNT OF OIL COST OF DRILLING AMOUNT OF OIL SEISMIC STRUCTURE AMOUNT OF OIL SEISMIC STRUCTURE COST OF DRILLING  Range      TEST type  IlL  IICJ                   Range      TEST type  IlL  IICJ  none cheap perfect              none cheap perfect        none cheap perfect              none cheap              none cheap perfect              none cheap perfect                                           none cheap perfect none cheap perfect none  cheap perfect none cheap                                            none cheap perfect none cheap none cheap none  Table    The table shows the sets of admissible decisions for the DRILL decision for various input probability ranges         Range  TEST  cheap  TEST RESULT none NS OS  perfect  NS OS  none      cs  cs  none     and      cheap  none NS OS  perfect  NS OS  cs  cs  DRILL  EXACT liD yes yes yes yes yes yes yes yes no no yes yes yes yes yes yes yes no yes no yes yes yes yes yes no yes no yes yes yes yes  Table    The table shows the sets of admissible deci sions for the DRILL decision for various input prob ability ranges      Conclusions  Engineering Economic Systems  Stanford Univer sity  December           R D  Shachter  Evaluating influence diagrams  Operations Research                     I I I I I I I I I  In this paper we have extended previous results in interval values for influence diagrams to include deci sion making  While manipulation of belief has many interesting technical properties  the importance of varying the degree of precision in probabilities can only be gauged by including values and decisions into the analysis  This paper represents one step in that direction   I  
     The intelligent reformulation or restructuring of a be lief network can greatly increase the efficiency of in ference  However  time expended for reformulation is not available for performing inference  Thus  un der time pressure  there is a tradeoff between the time dedicated to reformulating the network and the time applied to the implementation of a solution  We investigate this partition of resources into time ap plied to reformulation and time used for inference  We shall describe first general principles for comput ing the ideal partition of resources under uncertainty  These principles have applicability to a wide variety of problems that can be divided into interdependent phases of problem solving  After  we shall present re sults of our empirical study of the problem of deter mining the ideal amount of time to devote to search ing for clusters in belief networks  In this work  we acquired and made use of probability distributions that characterize     the performance of alternative heuristic search methods for reformulating a network instance into a set of cliques  and     the time for executing inference procedures on various belief net works  Given a preference model describing the value of a solution as a function of the delay required for its computation  the system selects an ideal time to devote to reformulation   For a large class of AI problem solving techniques  great gains in efficiency can be achieved by expend ing effort on a preliminary meta analysis of a problem instance before directly executing a solution  Belief network algorithms highlight the necessity of refor mulating or restructuring problem instances  The re formulation of a belief network can greatly increase the efficiency of inference  Indeed  many belief network algorithms rely on some preliminary refor mulation procedure  Our analysis of reformulation is motivated by our pursuit of techniques for the dy namic construction and solution of belief networks         To date  investigators have made use of offline analyses for reformulating a small number of net works that will be solved many times  Unfortunately  straightforward offline analyses of reformulation  may not be effective in systems that must construct and solve belief network problems at run time  The com putational effort expended for reformulating a newly constructed belief network is not available for the pri mary task of performing inference with the network  Thus  in time dependent decision contexts  there is a tradeoff between the time dedicated to reformulating the network and the time applied to the implementa tion of a solution  We shall describe the metareasoning partition problem and present principles for computing the ideal partition of resources under uncertainty for sev eral prototypical classes of uncertainty and utility  In Section    we shall consider the global optimization of the apportionment of resources to precursory re   This work was su pported by Rockwell International Sci ence Center and the National Science Foundation under Grant IRI                formulation for the situation where the reformulated instance is solved once  In Section    we will discuss the inclusion of evidence about the progress of prob lem solving  in a formulation of the metareasoning partition problem centering on a myopic optimization policy  Following the presentation of theoretical re sults  we shall discuss in Section   an empirical study of the application of these principles to belief net  I I I  works  We focus  in particular  on an empirical anal ysis of the ideal amount of time to devote to search ing for clusters in belief networks  We acquire and apply probability distributions that characterize the  I  performance of alternative heuristic search methods for finding cliques and the time for executing infer ence procedures on various belief networks  Given a preference model  describing the value of a solution  I  as a function of the delay needed for its computa tion  the system determines the ideal time to devote to reformulation      Reformulating  Belief  Figure    Reformulation of  Net  works Brute force approaches to the solution of belief network inference problems are intractable  In a brute force analysis  we generate a joint distribu tion by taking the product of all assigned distribu tions  Given the joint distribution  we compute the marginal probability for any value of a variable or Boolean combination of values  by summing over the relevant dimensions of the joint distribution  The size of the joint distribution is exponential in the number of variables  Thus  although this naive approach is conceptually simple  it requires computation that is exponential in the number of variables  Although the problem of probabilistic inference with belief networks is  N P hard  methods have been developed for exploiting independence relations to avoid the explicit calculation of the joint probability distribution  A variety of exact methods has been developed to operate on specific topologies of be lief networks       Other recent methods forego exact calculation of probabilities  these approximation techniques produce partial results as distributions or bounds over probabilities of interest             Several promising exact and approximate ap proaches rely on the reformulation of multiply con nected networks  We are studying the ideal control of reformulation of belief network instances with the clique tree approach developed and refined in           method of conditioning       and with the nested dissection method of Cooper      The clique  with Pearl s  tree reformulation approach seeks to convert multiply  I    a  belief network instance  I  for the clique tree approach involves generating and Individual evaluating alternative sets of cliques  cliques are encircled  As highlighted by the graphs  nodes can be members of several cliques   I  connected belief networks into a corresponding singly connected network of cliques  A precursory reformu lation of a belief network instance works to identify cliques  defined as maximal sets of nodes that are completely interconnected  An algorithm has been developed to propagate evidence within this tree of cliques  which is somewhat analogous to the prop agation of belief in a singly connected network of  I  variables  Alternative clique tree reformulations are pictured in Figure    For the method of condition ing  reformulation seeks to break loops in a multiply connected belief network  by identifying and instan tiating a loop cutset  At solution time  each cutset node must be instantiated with each possible value  or combination of values   Each instance is solved as a  separate singly connected belief network prob lem  Reformulation methods work by generating and evaluating cutsets that minimize the number of prob lem instances that must be evaluated  Identifying the best cutset and identifying the best set of cliques are  N P hard problems  since in general they require searching all sets of subsets of the nodes in a belief network  However  we can de  velop heuristic strategies  see for example       and flexible  or anytime  search strategies that can deliver increasingly better reformulations as we increase the amount of reformulation time   I I I I I I I I I I       I  For some inference procedures it may be possi ble to determine t e precisely  given the amount of time spent searching for a solution  In this special situation  we can characterize computation time in terms of a deterministic function  te    R tr    In this case  the selection of te fully determines the time that the solution to the problem becomes available   I I  In general  we must consider the uncertainty in the relationship between the search time and the time required to compute a desired result  Under uncertainty  the computation time is char  I  acterized by probability distributions for different val  I  ues of search time   tt   t r  e  I I I I  Figure    A graph showing the value of a computed result  as a function of the total time required to gen erate a result  The total delay t is the sum of the time needed for reformulating and solving a belief network   t        tr   te   lief networks that are created at run time   I       I I I I  that maximizes the  tion of a value function and distributions forte  More  for Belief Networks  I  I  tr  expected value of the computation  given a specifica  Ideal Partition of Resources  I  I  tem  which may effect these distributions  Our ob jective is to choose a value of  formally  we seek to maximize the expected value of the result  with respect to tr as follows   We now shall outline the problem of ideally appor tioning resources to the reformulation of belief net  I  where e includes any background knowledge about the problem and solution methods  e g   problem size  hardware parameters  architecture of reasoning sys  works under conditions of uncertainty and describe the application of this problem to the solution of be  max  tr  Deadlin e Models  The class of  We refer to the problem of ideally apportioning re sources between a meta analysis and the solution of a base problem as the metareasoning partition problem       The ideal partition of resources depends on the architecture of an agent  on the availability and form of knowledge and metaknowledge about problem solving  and on the problem instance at hand  Most meta analyses for the reformulation of belief networks center on a search process  Thus  we cast reformula tion in terms of search  Let tr be the time the rea  soner spends on reformulating a problem instance  and let te be the time required to execute a reformu lated instance to generate a final solution  Thus  the total time required to solve the problem is t   tr  te  function solely of the time at which it becomes avail in Figure     V t   illustrf  ted  problems captures situations  deadline  where the cost incurred with delay for a computed re  sult is   or insignificant until a deadline a is reached  Thus  an analytical result obtained before time a has value k  If the result is not available by time a  the result is worthless  We can model a deadline situation via a stepwise value function  V t            k    t a  otherwise   Through substituting this step function utility into our general formulation value is  Et   VItr e      Let us assume that the value of a computed result is a  able  We express this relationship as       te  Details of a formal analysis of this problem ap pear in      Here we highlight the central results for prototypical models of cost        General Problem  j V tr   te p teltr e dte             we find that the expected    V tr  te P teltr e dte  a tr kp teltr e dte t   kp te a  trltr e               The last term is the probability that the time required for executing the reformulated solution is less than the time remaining after the reformulation process  Thus  for the deadline case  maximizing the expected value is equivalent to maximizing the probability of completing the computation before the deadline              Polynomial Urgency Models  Let us now explore the general model of reformula tion under uncertainty where the overall value of a computational result is a polynomial function of the time it becomes available  We consider a model of ur gency where the value function V t  is an nth degree polynomial   where the  ai     L aiti I  ai tr   te i  The class of target problems refers to situations where the value of a computed result is   unless it is available exactly at time a  That is  a result obtained before or after time a is worthless  This model is associ ated with events that must be coordinated tightly un der bounded resources  such as time dependent com  senting our value as a delta function     i l  i l  V t    o a  t    o a   tr   t     are constants that are customize the  model to particular contexts  Substituting the poly  dp a  irltr      we seek to maximize   dtr    t a  tr  t  p t ltr e dt     a  t   J  t t ip teltr e dt   dictates that  for target models  we should continue searching until the distribution over the expected ex ecution time achieves its mode exactly at time a      i l  t t   i  ai  where  m n  zly     O  The derivative of a probability distribution is zero at the mode of a unimodal distribution  This result  Et   VItr  e            i  m   j  te ltr   Incremental  Analysis  of  Metareasoning Partition We have described how we can characterize the ef ficacy of metareasoning processes for different types of belief networks by acquiring probability density    znp  cly d c  is the nth moment of  c given y   To maximize the expected value of a computed result defined by Equation    we set the derivative of this expression to zero  At the optimum we obtain  functions about the relationship between tr and t  for a large set of instances for each class  Such sam pling yields probability distributions p t ltr e  that we can use to calculate the optimal time to spend on metareasoning  e includes any background knowledge about the problem and solution methods which may effect these distributions  So far  we have assumed that we do not have additional knowledge about the problem besides these distributions  If the reasoner       Thus  for any context of urgency  that can be rep resented  or approximated  with a polynomial value function of order n  we can determine an ideal time to dwell on reformulation tr  given the moments and  The  first  expectation   moment  of a  distribution   m      is  the  I I I I I I  At a solution for this functional form  we have  nomial form into our general formulation  Equation  t   Target Model  munications and datasharing over limited bandwidth channels  We can model target problems by repre  n  n  V t   derivatives of the probability distribution  p te ltr  e  for i              n  For high order polynomials  solv ing Equation   and estimating the derivatives may be difficult  However  for linear and quadratic forms  the solution is straightforward   I  I I I I  I I I  is limited to a single step solution planning process  where a single meta analysis is applied to generate an ideal reformulation policy  we are indeed forced to  I  make use of a probability distribution that describes the relationship between tr and t  for an entire class of problems  However  we may wish to expend addi  I  tional resource on an incremental meta analysis  and make use of detailed information about the relation ship between tr and t  that is revealed over time   I I I       I  than the value of halting and solving the current  I  formulation of the belief network problem  We halt when the expected change in the value of solving the problem after another  I  t  e  V  t   t   e  EVhalt  I I I I      t  V tr  te p te tr Et  e dte  r  If we continue reformulation  there is a spectrum of evidence Figure      The incremental metareasoning partition  decision problem  incorporating the acquisition and use of information about the progress of reformula tion to control the extent of reformulation   E  which may be observed   EVeontinue      t  jEf V tr   L   tr   te   t  tr   Atr Et  t   t  e  P       p Et  t   t  Et  e dEdt   That is  we can assess probability distributions and  I  I  Expected value of halting  reformulation   In the incremental approach  we analyze recent  I  Note that we are not solv  we develop a greedy  hill climbing procedure for in  incorporate information about the progress of refor mulation as a useful class of evidence for determining  I      cremental reformulation   I  I  and  ing for the sequence of choices over time  but rather  I  I  Atr  with a new lottery  The incremental decision tree is displayed in Figure  I  I  is nonpositive  If we de  again examine a new decision to halt or to continue  I  I  Atr  cide to continue  we reformulate for another  the efficacy of future reformulation efforts   The further assumption that independent of  tr  te  is conditionally  given the evidence observed so far  yields  p te tr   Atr Et  t   t  e      p te Et  t   t   e        reformulation behavior to make a decision about the  simplifying the expression for Equation    The crite  value of continuing to p erform reformulation for an  rion for halting is  additional  prespecified increment of reformulation time   EVhalt  EVeontinue  We can make use of uncertain knowledge of  the form       The efficacy of the incremental approach relative  p t  tr Et  e   to an  a  priori  reformulation policy depends on the  where E  refers to evidence observed at time tr in the  structure of the problem and the costs of performing  progression of reformulation of the current instance   the incremental analysis  Associated w ith each time point and evidence  E  is  the actual data structure which embodies the current reformulated problem instance  In general   E   ori analysis can        In some cases  an  a  pri  prove that an incremental approach  is unnecessary  we show in       that for certain value  can  function distribution pairs  dominance relationships  include the complete sequence of evidence or infor  can determine the ideal reformulation policy in ad  mation collected during the process of reformulation  At each time  tr  we must decide  vance  W hen this type of simplification is not possi  whether to halt  ble  there are other factors to consider  If a reasoner  immediately and to begin to solve the current prob  cannot obtain access to evidence  E  about the time  lem formulation or to continue the reformulation  dependent behavior of a reformulation method  we  search for another  Atr   We can express the value  must treat the method as a mysterious  black box    priori  of continuing as a lottery over possible results of fur  and base decisions on an  ther reformulation search  we can sample a large set  summary distributions for large classes of problems   of cases to acquire probability distributions about  Finally if the cost of evaluating Equation  changes in distributions about  te  as more time is  spent on reformulation   a  consideration of     is high   then the overhead of metareasoning may overwhelm potential benefits   In fact  creation of distinctions  W ith the incremental metareasoning partition  and models for E  and its dynamics that are at once  analysis  we must continually determine if the ex  informative and concise are critical to the value of the  pected value of the lottery of continuing is greater  entire approach           Example   Clique Reformu  lation We have applied an incremental analysis of ideal par tition of resources to the example of clique reformu lation of a belief network  The fundamental cycle is construction of a belief network  formation of the clique tree  reformulation   and finally performing in ference  calculating the posterior probability of all unobserved variables   We shall present several de tails about the clique identification strategies  After  we shall describe the procedures used to collect prob ability distributions for use in the analysis  Finally  we shall review the results of using these distributions in an incremental analysis       Clique Reformulation Methods  is generated by varying the initial conditions to pro duce a large number of join tree topologies  After we generate an ordering  we determine the join tree structure  We then estimate the time required to solve that configuration with an efficient estimation procedure       In our study of ideal clique tree refor mulation  we based this time estimate on the sum of the state space sizes for the cliques in the tree  These  generate and test  procedures maintain a record of the best clique configuration found to that time and continues to search until the procedure is terminated  The strategies are flexible in that they generate so lutions that are monotonically increasing in quality  decreasing in te   and make available  at all times  the best join tree found so far  As the reformulation time  tr  is increased  the procedure searches addi tional join tree configurations   The clique formation methods we examine are based      Classes of Data about Reformula on construction of a join tree  The join tree is contion Efficacy structed by the following sequence of steps              Create a Markov network from the original net  As we discussed in Sections   and    we need to work by interconnecting the parents of each node make use of uncertain knowledge that relates the time needed for execution of a problem formulation to the and removing directionality from the arcs  time spent on generating the reformulation  We have    Calculate an ordering for the nodes  obtained distributions from a frequency analysis of    Fill in edges between predecessors of each node in the various reformulation strategies described above  the graph  using the ordering generated in Step We used IDEAL  a general influence diagram pro gramming environment  to collect this distributional    information       We directed the system to construct    Construct the join tree by identifying the cliques random belief networks of different sizes and con  subgraphs which are completely connected  in nectedness  and to apply reformulation algorithms to the filled in graph  the networks  We collected data for many networks Our analysis of reformulation strategies focuses to gnerate statistics regarding p Etp t IE   e  and on Step    the generation of an ordering  In par p te E   e   ticular  we examine a method developed by Kjrerulff       which we refer to asK search  The better known       Run Time Estimate and Reformulation procedure for ordering is maximum cardinality search   MCS        The MCS approach starts with an arbi One useful class of knowledge for making decisions trary node and assigns the next number to the node about the partition of resources focuses on estimated having the largest set of unnumbered neighbors  K run time as a function of reformulation time  As dis search generates an ordering by first finding a node cussed above  increasing the time for reformulation whose neighbors form a clique already  If no such increases the number of cliques that the program has node exists  the algorithm uses a cost metric  based explored and scored  with E defined as the best esti on the size of the state space of the neighbors of a mate encountered so far  To investigate the efficacy node  to determine which node to index next  of the randomized K search procedure  we generated a large number of networks and collected data about the trajectory of improvement in run time estimates     A Flexible Clique Reformulation as additional time was spent on reformulation  Sev Strategy eral of these trajectories are displayed in Figure    We implemented flexible versions of the MCS and The trajectories are normalized to show the propor K search reformulation strategies  The MCS and K tional decrease in estimated run time as a function search strategies are both sensitive to the initial or of time  Our analysis revealed that the incremental dering of a belief network  The search state space time used for generating new configurations has only  I I I I I I I I I I I I I I I I I I I   I I I I   tl  I I I I I I I I I I I I I               E       e               Cj                       t      C l        II  e m              p                                 l                      Proportional change in run time estimate  tr        Reformulation time   tr   I I          B  The proportional reduction in run time es  Figure    The probability of various levels of decrease  timate as a function of reformulation time for the K  in running time for K search for an increment of refor  search reformulation for a set of randomly generated networks   mulation search for tr  Figure            generated from a sample  of     randomly generated belief networks   modest impact on total execution time       For a particular network  the benefits of additional reformulation time are uncertain because of the interplay between the K search procedure and he specific interconnectedness and the state space size for individual variables in a networks  Also  the ex pected incremental reduction in runtime is a function of how long the search has progressed  For the in cremental algorithm  we therefore need to assess the        t  P  Ee   e                 probability distribution over the proportional reduction in Etr A from Et  for various levels of tr  Trajectories such as displayed in Figure     were analyzed  Execution time per unit run time estimate  to generate those probabilities  The actual data sets were collected for     and      second increments and applied in     second increments  Figure   displays one of these distributions  The graph shows the prob   Figure    Probability distribution over time per op eration for the clustering belief network algorithm   ability distribution over various levels of proportional decrease in estimated run time  The value at the left  zero  is the probability that the new reformulations searched in the next period will be no better than the current best   Positive probabilities to the right  indicate the chances for improved e   Cecution times   tal state space size  we divided actual execution time by E to get a measure of execution time per unit run time estimate used to generate distribution is shown in Figure  p telEtr      This      given additional reformulation time  We found that  The distributions do not reflect differences in  the K search technique performs so well  there are  possible configurations of evidence on the network   usually only minor gains to obtained from additional  The estimator E is based in the sum of the sizes of  reformulation with this technique   the state spaces for the cliques in the join tree for the constructed network  Since the problem we are an         Time of Execution Given Estimate  alyzing consists of network construction  clique tree reformulation and a single inference cycle  the infer  The conditional independence assumption of Equa  ence step we analyze is that of full propagation and  tion   allows us to assess a distribution over execution  initialization of the network  including the initial ev  times  given the values reported by the estimator E   idence vector  In analyzing clique tree formation for  independent of the particular value tr  We assessed  a net vork that would be applied to many possible  this distribution by generating E values for a num  evidence configurations  it would be necessary to ex  ber of random networks  and then timing the actual solution of each network   Since E is based on to   amine the impact of different classes of evidence on the execution time             Applying the Techniques  The criteria of Equation   has been implemented in a recent version of the IDEAL belief network environ ment  Given a specification of a value function and a belief network  the system uses empirical data to determine  in real time  whether or not to continue with reformulation  We performed an investigation of the value of metareasoning for optimizing the reformulation time  Because an analysis of the ideal reformulation of belief networks is sensitive to the efficiencies of the soft ware and hardware  as well as to the formulation of the metareasoning model  it is important to consider details of the software and hardware  All experiments were run with IDEAL on a Symbolics      Lisp Ma chine with   megabytes of physical memory  The following experimental procedure was un dertaken  A series of    node belief networks were constructed by a random belief network generator in IDEAL  For each network and value function pair  we applied     a default policy of halting reformulation after the first clique tree is identified by the K search heuristic and     the incremental reformulation policy presented in Section    based on searching through a series of c lique trees  After applying each technique  we executed an inference cycle  full propagation and marginalization of all nodes in the network   given evidence  The total time  tr   te  was used to score the computational value of each trial based on the value function  This procedure was applied to a se ries of random belief networks for a given value func tion to assess the longterm performance of the default or incremental strategy  Given the metalevel model and the classes of probability distributions described above  we explored the relative efficacy of the default and incremental analyses for several value functions and para rneterizations of these functions  These func tions are shown in Figure    Our analysis showed that the use of metareason ing to dynamically optimize the amount of time ex pended on reformulation frequently is more valuable than the static policy of halting reformulation after the first valid clique tree is discovered  We found that the preferred approach  in terms of higher ex pected value over a number of trials  depended on the form of the value function and its specific pa rameters  The incremental metareasoning procedure continue to search if the benefit of finding a better clique formulation is high enough to justify the delay associated with continuing another time increment of search  Since the K search heuristic provides a very good initial clique formulation   see F igures    incre mental searching does not tend to provide a great deal  I I I I I  Figure    Prototypical value functions used in the incremental analyses  of improvement in absolute terms  However when the costs of incremental delay are substantial  as in some of the quadratic value functions analyzed  the benefits of an improved solution can be substantial even when these improvements are expected with low probability  For the linear and exponential forms with a slow decay of value with time  the incremental policies tend to behave like the default policy  as they stop searching for better cliques immediately after the first time increment  In these cases  the incremental pol icy was just marginally worse than the default policy  For deadline models  we examined several vari ants by changing the severity of the deadline  We found that both policies performed equally well un der a variety of deadlines  indicating that the ability to make the deadline was more dependent on the vari ability in the time required to perform inference on different networks  due to topology and state space size  than on differences in metal eve  reasoning policy      Summary  We described the metareasoning partition problem and presented principles for calculating the ideal par tition of resources under uncertainty for several pro totypical classes of uncertainty and utility  We dis cussed the global optimization of the apportionment of resources for the case of a precursory reformula tion where the reformulated instance is solved once  After  we introduced the incremental analyses for in cluding evidence gleaned from observations about the progress of problem solving  Following the presenta   I I I I I I I I I I I I I I       I I I I I I I I I I I I I I  tion of our theoretical results  we discussed empirical     study of the performance of a clique tree reformula tion strategy with these principles  We showed how an incremental reasoner can reason about the value of apportioning additional time to a search for op timal clusters in belief networks versus halting and solving the current best formulation  We found that      the value of applying metalevel machinery to opti mize the partition of resources for metareasoning is sensitive to the preference model  describing the value of a solution as a function of the delay needed for its computation  We hope that other investigators will find use in the principles we described for the ideal partition of resources for reformulation under uncer       tainty  In particular  the techniques hold the promise for helping us to optimally control the dynamic con struction and solution of belief networks   F  V  Jensen  Lauritzen S  L   and Olesen K  G  Bayesian updating in recursive graphical mod els by local computations  Technical Report Re port R        Institute for Electronic Systems  Department of Mathematics and Computer Sci ence  University of Aalborg  Denmark        U  Kjrerulff  Triangulation in graphs  algorithms giving small total state space  Technical Re port R       Institute for Electronic Systems  Department of Mathematics and Computer Sci ence  University of Aalborg  Denmark         gent Systems  Networks of Plausible Inference       G  F  Cooper  Bayesian belief network inference Morgan Kaufmann  San Mateo  CA        using nested disection  Technical Report KSL       Stanford University  February            R D  Shachter and M  Peot  Simulation ap proaches to general probabilistic inference on be     M  Henrion  Propagation of uncertainty by prob lief networks  In Proceedings of Fifth Workshop abilistic logic sampling in Bayes  networks  In on Uncertainty in Artificial Intelligence  Wind J F  Lemmer and L N  Kanal  editors  Uncer sor  Canada  August       tainty in Artificial Intelligence    pages          North Holland              S  Srinivas and J  S  Breese  IDEAL  A software package for the analysis of belief networks  In     E J  Horvitz  Rational metareasoning and com pilation for optimizing decisions under bounded resources  In Proceedings of Computational In telligence     Association for Computing Ma       chinery  September        I      E J  Horvitz  G F  Cooper  and D E  Beckerman  Reflection and action under scarce resources  Theoretical principles and empirical study  In Proceedings of the Eleventh IJCAI  pages           AAAI  International Joint Conferences on Artificial Intelligence  August        I  American Association for Artificial Intelligence         S L  Lauritzen and D J  Spiegelhalter  Local computations with probabilities on graphical      J S  Breese  Construction of belief and decision structures and their application to expert sys networks  Technical Report Technical Memoran tems  J  Royal Statistical Society B              dum     Rockwell International Science Center        Palo Alto  California  January             J  Pearl  Fusion  propagation  and structuring in     J S  Breese and E J  Horvitz  Principles of prob belief networks  Artific ial Intelligence        lem reformulation under uncertainty  Techni            cal report  Stanford University  February       KSL             J  Pearl  Probabilistic Reasoning in Intelli  I  I  of Fifth Workshop on Uncertainty in Artificial Intelligence  W indsor  Canada  August        
 Heckerman        defined causal independence in terms of a set of temporal conditional independence statements  These statements formalized certain types of causal interaction where     the effect is independent of the order that causes are introduced and     the impact of a single cause on the effect does not depend on what other causes have previously been applied  In this paper  we introduce an equivalent atemporal characterization of causal independence based on a functional representation of the relationship between causes and the effect  In this representation  the interaction between causes and effect can be written as a nested decomposition of functions  Causal independence can be exploited by representing this decomposition in the belief network  resulting in representations that are more efficient for inference than general causal models  We present empirical results showing the benefits of a causal independence representation for belief network inference      Introduction  Belief networks are often used as a modeling tool when there is uncertainty in the interaction between a set of causes and effects  A typical interaction between several causes and a single effect can be modeled with the belief network shown in Figure    In the figure  the variable e represents an effect and the variables c            cn represent n causes of that effect  For binary discrete variables  this representation requires  n independent parameters to be specified  Consequently  the representation imposes intractable demands on both knowledge acquisition and inference  In response to the intractability of knowledge acquisition  prototypical interactions such as the noisy or model       have been developed  These models allow one to specify n parameters to generate the condition   figure multfaul eps width  in Figure    A belief network for multiple causes and a single effect  al probability table for a variable e as shown in Figure    Because the full table is used to characterize the relationship in the belief network  however  inference remains intractable  Last year at this conference  Heckerman defined causal independence in terms of temporal conditionalindependence constraints on a set of variables      These statements formalized certain types of causal interaction where     the effect is independent of the order that causes are introduced  and     the impact of a single cause on the effect does not depend on what other causes have previously been applied  The previous paper demonstrated how this definition generalizes the notion of a noisy or and noisy adder model  and indicated how a belief network representation of causal independence can be used to increase the speed of inference  In this paper  we transform the previous temporal definition into an equivalent atemporal representation  In doing so  we find that causal independence is a special case of a generalization of the noisy or developed by Srinivas         It also allows us to define several classes of interaction models in terms of expressiveness and efficiency  Finally  we present some empirical results regarding the storage and inference savings associated with application of causal independence to real world networks      Temporal Definition of Causal Independence  In the temporal definition of causal independence  we associate a set of variables indexed by time with each cause and with the effect  We use cjt to denote the variable associated with cause cj at time t  and et to   figure tempci eps width  in Figure    A temporal belief network representation of causal independence  denote the variable associated with the effect at time t  For all times t and t    we require the variables cjt and cjt  to have the same set of  possibly infinite  states  Under these conditions  we say that c            cn are causally independent with respect to e if the set of conditional independent assertions t   t    cj  et   c t           cj  t   cj   t           cnt   et   cjt   cjt    ckt   ckt  for k    j       hold  where  X  Y  Z  denotes the conditionalindependence assertion the sets of variables X and Y are independent  given Z  Note that Assertion   is somewhat unusual  in that independence is conditioned  in part  on the knowledge that the states of variables are equal  but otherwise undetermined  ckt   ckt  for k    j   Assertion   states that if cause cj makes a transition from one state to another between t and t    and if no other cause makes a transition during this time interval  then the probability distribution over the effect at time t  depends only on the state of the effect at time t and on the transition made by cj   the distribution does not depend on the other causation variables  Note that time is treated as an ordinal quantity in this definition and there is no need to have a continuous or discrete interval model of time  We can derive a belief network representation of causal independence from this definition  First  for each cause  designate some state of its associated variables to be distinguished  For most real world models  this state will be the one that has no bearing on the effect that is  the off statebut we do not require this association  Second  let  be an ordering of the variables  c            cn  we use ci to denote the ith variable in the ordering  Construct a belief network consisting of nodes c            cn   and e    e          en   as shown in Figure    In this belief network  node e  represents the effect when all causes take on their distinguished state  Node c  represents the state of cause c  after it has made a transition from its distinguished state  a transition may be the trivial transition  wherein the cause maintains its distinguished state   Node e  represents the effect after only c  has made the transition  In general  node ci represents the state of cause ci after it has made a  possibly trivial  transition from its distinguished state  Node ei represents the effect after causes c            ci have made their transitions  In particular  node en represents the effect after all causes have made transitions  Thus  node en corresponds to node e in Figure    The conditional independencies represented in the be   figure tworepa eps width  in  a  figure tworepb eps width  in  b  Figure    Two representations of causal independence that are equivalent to the representation in Figure    lief network of Figure   follow from the definition of causal independence  Conversely  given n  belief networks of the form in Figure  one network for each possible ordering of the n variables in the domainwe obtain the temporal definition of causal independence  In terms of the number of parameter assessment for models of discrete valued variables  causal independence yields a significant economy  As noted previously  the general multiplecause interaction illustrated in Figure   requires  n separate assessments for binary variables  one parameter for each combination of the states of the parents  In contrast  the causal independence interaction illustrated in Figure   requires only  n     assessments  four parameters for each node ei plus a single parameter for e    In Section   we discuss additional issues related to assessment of causal independence models      An Atemporal Representation of Causal Independence  In this section  we transform the temporal definition into an atemporal form  The transformation is based on the observation that we can represent the belief network in Figure   as the belief network shown in Figure  a  The double ovals represent deterministic nodesnodes whose values are a deterministic function of their parents  Each node i is a dummy node that encodes the uncertainty in the relationships among ei and its parents as described in Druzdel and Simon        or  alternatively  Heckerman and Shachter         We can think of the node i as representing the causal mechanism that mediates the interaction between the parents of ei and ei itself      although we do not require this interpretation here  The definition presented in this section is atemporal in that it relies only on specification of a functional decomposition of the interaction  with no explicit representation of time  Let variable e i represent e when all cj    ci take on their distinguished variables  So  for example  e     e    As we show in the following theorem  it turns out that if the relationships in Figure  a are true for all orderings   then the relationships in Figure  b are also true for all orderings  provided e  is certain  i e   a constant   Note that if e  is not a constant  we can introduce a dummy cause xl that is always instantiated to a nondistinguished value  In this case  we can express the uncertainty in e  as uncertainty in e l   leaving e  a constant in the mathematical formal    ism  Henrion        calls xl a base or leak cause for e  The following theorem establishes the existence of a set of functions fi and gi that satisfy the temporal definition of causal independence when expressed in a diagram such as Figure  b  Theorem   If e  is a constant  then a set of variables  c            cn   are causally independent with respect to effect e if and only if for all orderings   there exists function g  such that e   e     g   c              and  for i              n  there exist functions fi and gi such that ei   fi  e i   e i        e i   gi  ci   i       Proof  The  portion of the theorem follows directly by reading the conditional independence statements associated with causal independence directly from the belief networks associated with each ordering   We prove  by induction on n  the number of causes  When n      the theorem follows directly from the transformation described in Figure  a  For the induction step  let us suppose that  c            cn     are causally independent with respect to effect e  Let  be the ordering where ci   ci   i              n      Applying the theorem to the first n causes  we obtain the belief network in Figure  a  In particular  we have e   e n     h  cn     en   n           for some deterministic function h   Now  let  be the ordering where c    cn   and c i     ci   i              n  From the assumption of causal independence  we obtain the belief network in Figure  b  Specifically  we get e    e n     g   cn     n         Also  collapsing the functions between e    e n   and e n   in Figure  b  we obtain e   e n     h  e n     c            cn               n         for some deterministic function h   Combining Equations   and    we get e   h  cn     en   n       h  e n     c            cn               n         All variables ci and i   i              n      however  are logically independent  they are also probabilistically independent  but we do not need this fact   Therefore  e n   must summarize the effects of cn   and n   in the determination of e  andsimilarlyen must summarize the effects of c            cn               n in the determination of e  Consequently  there must exist some deterministic function h such that e   e n     e n     h e n     en       Identifying h in Equation   with f n   and g  in Equation   with g n     we obtain Equations   through  figure proofa eps width  in  a  figure proofb eps width  in  b  Figure    Two belief networks for the proof of Theorem    figure atemp eps width  in Figure    A belief network representation of a generalized noisy or model      Repeating this argument for every initial ordering   we complete the induction step   An immediate consequence of Theorem   is that we can write e as a nested set of two argument functions for any ordering      e   fn e n   f n  e  n          f   e     e          Collapsing these nested functions into a single function f   we obtain e   f  e             e n   We say that Equation    is a nested decomposition for f   In this formulation the sequence of functions that decompose the overall relationship between e and the e i depends on the ordering chosen  and each function fi may be different  If functions fn   i              n are equal to some function f  for all   however  it follows that the function f  is both commutative and associative  We find that causal independence relations that are useful in practice have this property  such as the noisy or and noisy adder models described in Heckerman         From our discussion  we see that causal independence is a special case of Srinivas generalization of the noisyor model  Srinivas model is equivalent to the belief network in Figure   where the function f is arbitrary  In particular  causal independence includes the assumption that f has a nested decomposition for any ordering of the causes  Indeed  there are many functions f that are not admitted by Heckermans definition  For example  suppose e is binary  Although both Srinivas model and causal independence admit a function f that is true if and only if e i is true for exactly one value of i  only Srinivas model permits the function f that is true if and only if e i is true for exactly two values of i  because this function f is not decomposable  Causal independence also imposes restrictions on the probability distributions for e i given ci and on the individual functions fi   In particular  given the definitions of e   a constant  and e i   it follows that e i   e  when ci takes on its distinguished value  That is  p e i   e   ci             where  is the distinguished value for ci   Furthermore  suppose all variables preceding ci take on their distinguished value  Then  e i   ei   In this situation  however  e i    e    by definition of e i    Therefore  from Equation    it follows that ei   fi  ei   e    That is  e  is the identity element of each function fi   Thus  for the binary discrete case  the atemporal version of causal independence requires assessment of only n parameters corresponding to p e i   e   ci      for a network with all binary variables  as well as the individual functions fi   It is worth noting that linear models are a special case of causal independence  In a Gaussian linear model we have n X e a  bi ci    i    where a and the bi are constants and  has a normal distribution with mean zero and variance v  written N     v    We can express this in terms of the atemporal model by letting e    a gi  ci   i     bi ci   i              n gn    cn     n       n   n    N     v  and by identifying each function fi with        Classes of Causal Interaction  Several types of causal interaction have been proposed in the literature and in this paper  These various classes of causal interaction appear in the following list  ordered from the more general to more specific     General multiple cause interaction  Causal interactions modeled with a belief network as shown in Figure       Independence of causal inputs  Described by Srinivas         and can be modeled in the belief network shown in Figure    There is no restriction of the form of f      Singly decomposable causal independence interaction  There exists some ordering  and set of functions fi   such that Equation    holds  as illustrated in Figure  b     Fully decomposable causal independence interaction  Equation    and Figure  b holds for any ordering      Fully decomposable causal independence with equal functions  The previous class with the added condition that functions fn   i              n are equal to some function f  for all   such as or or    This class includes the noisy or model      Linear Gaussian Models  A special case of class   with continuous valued causes and effects with a single Gaussian noisy input representing the variance  deterministic contributions for the other causes  and a single function f  as        Causal Independence and Assessment  A major motivation for these prototypical interaction models has been to ease the task of knowledge acquisition for networks where nodes may have many parents  Any formalism at least as specific as that described in class   will have economy of knowledge acquisition  since we obtain an exponential savings in parameter assessments  Each more specific class requires even fewer assessments  The appropriateness of each class depends on the particular application  For example  a digital circuit with multiple inputs and a single output can be modeled with a function f as illustrated in Figure    but the function f may not be fully or singly decomposable  Within the class of causal independent models one has a choice of using the temporal or the atemporal definition of causal independence for assessment  The preferred definition depends on the expert and the domain being modeled  For example  in an application involving the effect of drugs on white blood cell counts  the temporal version of causal independence was a more natural method for interacting with the expert      On the other hand  in a number of hardware troubleshooting applications      the atemporal version of causal independence  class    has been most effective  In these cases  one is typically modeling a device that will fail if any one of its components fail  leading naturally to a fully decomposable or functional model  In general  we find that both definitions are useful in dealing with experts and we can switch between one and the other as needed      Causal Independence and Inference  Each more specific class is associated with no worse and usually increased inference algorithm efficiency  For example  if the function f is decomposable in some orderingthat is as described in class  then we can obtain an exponential savings in storage when compared to classes   or    The effect of the decomposition is to reduce the number of predecessors of the effect node  In addition  if the interactions in a model are causally independent  as in class      or    then we can rearrange the belief network expression of the decomposition to improve inference further  As an example of rearranging the belief network  consider the multiply connected belief network in Figure  a  If we transform the belief network using the ordering  c    c    c     we obtain the belief network in Figure  b  In contrast  if we transform the belief net    figure ordera eps width  in  a  figure orderb eps width  in  b  figure orderc eps width  in  b  Figure     a  A multiply connected belief network   b c  Two equivalent transformations of the belief network in  a   The network in  c  has a smaller undirected cycle than  b   work using the ordering  c    c    c     then we obtain the belief network in Figure  c  Inference using exact belief network algorithms  e g   junction tree propagation     or arc reversal      may be less efficient in the belief network of Figure  b than in the belief network of Figure  c  because there is a larger cycle in the former network  A larger cycle is not necessarily worse for inference in all algorithms  and the desirability of different topologies depends on the specific inference technique being used  In the following sections  we quantify savings associated with causal independence due to     reducing the size of the predecessor sets and hence clique sizes and     rearranging network topologies       Clique Size Reduction  Although it is clear that there is an exponential savings in storage when using causal independence for a single node  it is less clear what the savings in inference time will be for more general belief networks  Here  we compare state space sizes for networks that use class   and class   representations of cause and effect versus the same models converted to models in class   or better  Table   shows the size of the maximum clique and the sum of the clique sizes for each network  These figures are proportional to the runtime of clustering style algorithms on belief networks      The BN  network analyzed in Table   is a hypothetical network consisting of ten causes and four effects  Each effect has four causes  and two of the causes are common causes of each effect  With binary nodes  this case shows no savings using causal independence  due to the small state spaces and the small number of parents  In the BN     network  each node was assumed to have   outcomes  Here  we obtain a factor of ten savings using causal independence  The Multi Connected network is an    node medical belief network  where most nodes have   or   states  There is one node in the standard version of the net that has    parents  This node and its parents form the largest clique with size      under the standard formulation  Using a class   model  the largest clique size becomes       and we obtain a factor of   savings in total clique size  The Singly Connected network represents a    node hardware diagnosis problem  The network has very few cycles and mostly binary nodes  and there are at most three parents for any causal node  In this case  the additional  figure graph eps width  in Figure    The frequency distribution of clique sizes for the BN     network under different orderings  nodes created in the decomposition result in cliques that were slightly larger than those obtained with the original network  Overall  these results indicate that use of causal independence can have substantial benefits in real world modeling tasks  especially when a node has many parents and when causes and effects have many outcomes  In each of these cases  we used a default ordering for the expansion of the causal independent effect nodes to determine state space size  In the next section  we examine what gains can be expected from searching for the best orderings for expansion  taking into account the overall topology of the graph       Evaluating Alternate Orderings  Our original hypothesis was that the different orderings of the causal independence expansions of effect nodes could have a large effect on inference  The primary effect of different orderings is to change the size of undirected loops in the original belief network  as illustrated in Figure    Under the presumption that large loops are worse than short loops for inference  as has been reported previously   we believed that expansion ordering could have a large effect on clique size and hence inference  It has become apparent  however  that loop size is not a critical determinant  at least for clustering style algorithms  In order to characterize the potential savings  we sampled the BN  style network for different orderings  The distribution of clique sizes for a series of random orderings of causal independence expansions is shown in Figure    Note that potential gains are relatively modest  The cliques with the smallest size is only slightly smaller than average  We have developed a search algorithm that combines the process of clipping the diagram with choosing the order of expansion of the causal independence nodes  This algorithm uses heuristics during clique formation to guide the search for good expansions  On the basis of the empirical data  it is likely that a naive ordering will do almost as well      Conclusions  In this paper we have developed an atemporal characterization of causal independence  The characterization is based on a functional representation of the interaction between causes and effects that can be written as a nested decomposition of functions  We have shown that when causal independence holds  we easi    Belief Network BN  binary  BN     Multi Connected Singly Connected  Classes   and   Largest Clique Sum of Cliques                                      Class   Largest Clique Sum                of Cliques                    Table    Clique sizes as a function of network and causal interaction model  ly can covert this decomposition into a belief network that yields efficiency gains in model assessment  storage  and inference   
