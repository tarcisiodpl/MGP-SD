 In this paper we introduce a class of Markov decision processes that arise as a natural model for many renewable resource allocation problems  Upon extending results from the inventory control literature  we prove that they admit a closed form solution and we show how to exploit this structure to speed up its computation  We consider the application of the proposed framework to several problems arising in very different domains  and as part of the ongoing effort in the emerging field of Computational Sustainability we discuss in detail its application to the Northern Pacific Halibut marine fishery  Our approach is applied to a model based on real world data  obtaining a policy with a guaranteed lower bound on the utility function that is structurally very different from the one currently employed      Introduction  The problem of devising policies to optimally allocate resources over time is a fundamental decision theoretic problem with applications arising in many different fields  In fact  such decisions may involve a variety of different resources such as time  energy  natural and financial resources  in allocation problems arising in domains as diverse as natural resources management  crowdsourcing  supply chain management  QoS and routing in networks  vaccine distribution and pollution management  A particularly interesting class of such problems involves policies for the allocation of renewable resources  A key and unique aspect of such a resource type is the fact that  by definition  its stock is constantly replenished by an intrinsic growth process  The most common example are perhaps living resources  such as fish populations or forests  that increase constantly by natural growth and reproduction  but less conventional resources such as users in a social com   Carla Gomes  Bart Selman Department of Computer Science Cornell University  gomes selman  cs cornell edu  munity or in a crowdsourcing project share the same intrinsic growth feature due to social interactions  A common feature of the growth processes presented is that they are density dependent  in the sense that the growth rate depends on the amount of resource available  This fact creates a challenging management problem when the aim of the intervention is to optimally use the resource  for instance by harvesting a fish population or by requiring some effort from a crowdsourcing community  especially when economic aspects are factored in  We face a similar challenge in vaccine distribution problems  where the growth rate of infections is again density dependent and the objective is to reduce its spreading  This study  in particular  has been motivated by the alarming consideration that many natural resources are endangered due to over exploitation and generally poorly managed  For instance  the Food and Agricultural Organization estimates in their most recent report that    of marine fish stocks are already depleted     are recovering from depletion      are fully exploited and     are overexploited        One of the most fundamental aspects of the problem seems to be the lack of an effective way to handle the uncertainty affecting the complex dynamics involved  While in most of the works in the literature        these growth processes are modeled with deterministic first order difference or differential equations  this approach often represents an oversimplification  In fact their intrinsic growth is often affected by many variables and unpredictable factors  For example  in the case of animal populations such as fisheries  both weather and climate conditions are known to affect both the growth and the mortality in the population  Other variable ecological factors such as the availability of food or the interaction with other species also influence their natural dynamics to the point that it is very difficult even to obtain reliable mathematical models to describe their dynamics  On the other hand  stochastic differential equations can easily incorporate these variable factors and therefore represent a more robust description  However  obtaining a prob    abilistic description of such systems is far from easy  In fact  even if in principle uncertainty could be reduced by collecting and analyzing more data  it is generally believed that complex and stochastic systems  such a marine environments  could never become predictable  to the point that the authors of      believe that predictability of anything as complex as marine ecosystem will forever remain a chimera   Moreover  there are situations of radical uncertainty       or ambiguity where a stochastic description is not feasible because the probabilities are not quantifiable  For instance  many fundamental environmental issues that we are facing  such as those surrounding the climate change debate  involve ambiguity in the sense of scientific controversies or irreducible beliefs that cannot be resolved  In the context of stochastic optimization  there are two main ways to deal with uncertainty  The first one involves a risk management approach  where it is assumed that the probabilities of the stochastic events are known a priori or are learned from experience through statistical data analysis  Within this framework  decisions are taken according to stochastic control methods  Using tools such as risksensitive Markov decision processes             it is also possible to encode into the problem the attitude towards risk of the decision maker by using an appropriate utility function  In particular the degree of risk aversion can be controlled by sufficiently penalizing undesirable outcomes with the utility function  When a fine grained stochastic description is not available  worst case game theoretic frameworks  that are inherently risk averse  play a fundamental role because it is often crucial to devise policies that avoid catastrophic depletion  This type of approach  where the problem of data uncertainty is addressed by guaranteeing the optimality of the solution for the worst realizations of the parameters  is also known in the literature as robust optimization           and has been successfully applied to uncertain linear  conic quadratic and semidefinite programming  In this paper  we present a class of Markov decision processes that arise as a natural model for many resource management problems  Instead of formulating the optimization problem in a traditional form as a maximization of an expected utility  we tackle the management problems in a game theoretic framework  where the optimization problem is equivalent to a dynamic game against nature  This formulation is a particular type of Markov game       sometimes called a stochastic game       where there are only two agents  the manager and nature  and they have diametrically opposed goals  As mentioned before  although this formulation is more conservative  it also eliminates the very difficult task of estimating the probabilities of the stochastic events affecting the system  In a context where the emphasis in the literature has traditionally been on the study of expected utilities   this approach represents a new perspective  Moreover  the policies thus obtained provide a lower bound on the utility that can be guaranteed to be achieved  no matter the outcomes of the stochastic events  For this class of problems  we are able to completely characterize the optimal policy with a theoretical analysis that extends results from the inventory control literature  obtaining a closed form solution for the optimal policy  As part of the new exciting research area of Computational Sustainability         where techniques from computer science and related fields are applied to solve the pressing sustainability challenges of our time  we present an application of the proposed framework to the Northern Pacific Halibut fishery  one of the largest and most lucrative fisheries of the Northwestern coast  In particular  our method suggests the use of a cyclic scheme that involves periodic closures of the fishery  a policy that is structurally different from the one usually employed  that instead tries to maintain the stock at a given size with appropriate yearly harvests  However  this framework is interesting in its own right and  as briefly mentioned before  it applies to a variety of other problems that share a similar mathematical structure and that arise in very different domains  For example  we can apply our framework to pollution problems  where a stock of pollutants is evolving over time due to human action  and the objective is to minimize the total costs deriving from the presence of a certain stock of pollutants and the costs incurred with cleanups  but also to crowdsourcing and other problems      MDP Formulation  In this section  we will formulate the optimization problem as discrete time  continuous space Markov decision process  Whenever possible  we will use a notation consistent with the one used in      Even if we will consider only a finite horizon problem  the results can be extended to the infinite horizon case with limiting arguments  To make the description concrete  the model will be mostly described having a natural resource management problem in mind  We consider a dynamical system evolving over time according to xn     f  xn  hn   wn        where xn  R denotes the stock of a renewable resource at time n  By using a discrete time model we implicitly assume that replacement or birth processes occur in regular  well defined breeding seasons  where f    is a reproduction function that maps the stock level at the end of one season to the new stock level level at the beginning of the next season  The control or decision variable at year n is the harvest level hn  occurring between two consecutive breeding seasons   that must satisfy    hn  xn   As mentioned in the introduction  the function f    cap    tures the intrinsic replenishment ability of renewable resources  that in many practical applications  such as fisheries or forestry  is density dependent  growth rate is high when the habitat is underutilized but it decreases when the stock is larger and intraspecific competition intensifies  Specific properties of reproduction functions f    will be discussed in detail later  but we will always assume that there is a finite maximum stock level denoted by m  To compensate for the higher level description of the complex biological process we are modeling  we introduce uncertainty into the model through wn   a random variable that might capture  for example  the temperature of the water  an uncontrollable factor that influences the growth of the resource  Given the worst case framework we are considering  we will never make assumptions on the probability distribution of wn but only on its support  or  in other words  on the possible outcomes   In fact in an adversarial setting it is sufficient to consider all possible scenarios  each one corresponding to an action that nature can take against the policy maker  without assigning them a weight in a probabilistic sense  Given the presence of stochasticity  it is convenient to consider closed loop optimization approaches  where decisions are made in stages and the manager is allowed to gather information about the system between stages  In particular  we assume that the state of the system xn  R is completely observable  For example  in the context of fisheries this means that we assume to know exactly the level of the stock xn when the harvest level hn is to be chosen  In this context  a policy is a sequence of rules used to select at each period a harvest level for each possible stock size  In particular  an admissible policy                 N   is a sequence of functions  each one mapping stocks sizes x to harvests h  so that for all x and for all i    i  x   x        We assume that the marginal harvesting cost g x  increases as the stock size x decreases  We include time preference into the model by considering a fixed discount factor                        where      is a discount rate  For any given horizon length N   we consider the problem of finding an admissible policy     i  i   N   that maximizes   x    CN  min w            wN wi  W  xi    N X  n  R xn    R xn  hn    K   hn     n    where xn is subject to     and hn   n  xn    with initial condition x    x and     if x         x      otherwise  This is a Max Min formulation of the optimization problem  where the goal is to optimize the utility in a worst case scenario  As opposed to the maximization of an expected utility             this formulation is inherently risk averse  An advantage of this formulation is that there is no need to characterize the probability distribution of the random variables wk explicitly  but only to determine their support  In fact  one should consider all the possible scenarios  without worrying about the probabilities of their occurrence      Main Results       Minimax Dynamic Programming   A policy  is called an optimal N  period policy if CN  x  attains its supremum over all admissible policies at  for all x  We call  CN  x    sup CN  x           Resource Economics  We now consider the economic aspects of the model  We suppose that the revenue obtained from a harvest h is proportional to h through a fixed price p  and that harvesting is costly  In particular we assume that there is  the optimal value function  where  represents the set of all admissible policies  As a consequence of the principle of optimality       the dynamic programming equation for this problem reads  C   x  Cn  x    a fixed set up cost K each time a harvest is undertaken  a marginal harvest cost g x  per unit harvested when the stock size is x It follows that the utility derived from a harvest h from an initial stock x is Z x g y dy  K   R x   R x  h   K      ph   R x    px   Z  x  g y dy        max  min R xn    R xn  hn     hn x wn W  K   hn     Cn   f  x  hn   wn    for all n      The latter equation can be rewritten in terms of the remaining stock z   x  hn  the post decision state  as  xh  where          Cn  x     max  zx   R x   R z   K   x  z    min Cn   f  z  wn      wn W        This formulation of the problem is effectively analogous to a game against nature in the context of a two person zero sum game  The objective is in fact devising the value of z that maximizes the utility  but assuming that nature is actively playing against the manager with the opposite intention  It can be shown  see      that Cn  x   the revenue function associated with an optimal policy  is the  unique  solution to equation      From equation     we see that an optimal policy  when there are n periods left and the stock level is x  undertakes a harvest if and only if there exists    z  x such that   If    is nondecreasing and concave on I and    is nondecreasing and K concave on  inf xI  x   supxI  x   then the composition    is K concave on I   Let    x           N  x  be a family of functions such that i  x  is Ki  concave  Then  x    mini i  x  is  maxi Ki   concave   If    is a continuous  K concave function on the interval     m   then there exists scalars    S  s  m such that   S    q  for all q      m    Either s   m and  S   K   m  or s   m and  S   K    s    q  for all q   s  m       is a decreasing function on  s  m    For all x  y  s   x   K   y    R x   R z   K    min Cn   f  z  wn      wn W   min Cn   f  x  wn     wn W  In fact  an action should be taken if and only if its associated benefits are sufficient to compensate the fixed cost incurred  By defining      The proof is not reported here for space reasons  but can be found in      Similar results for K convex functions are proved in       we have that an optimal policy  when there are n periods left and the stock level is x  undertakes a harvest if and only if there exists    z  x such that  In the following section we will prove by induction the Kconcavity of the functions Pn  x   n              N   This will allow us to characterize the structure of the optimal policy by using the last assertion of Lemma     Pn  x    R x     min Cn   f  x  wn     wn W  Pn  z   K   Pn  x         To examine this kind of relationship it is useful to introduce the notion of K concavity  a natural extension of the Kconvexity property originally introduced by Scarf in      to study inventory control problems       Preliminaries on K concavity  A function    is K concave if given three points x   y   z   y  exceeds the secant approximation to  y  obtained using the points  x   K and  z   Therefore for K     no slack is allowed and one recovers the standard definition of concavity  Formally Definition    A real valued function    is K concave if for all x  y  x   y  and for all b      x    y    x  y    y   b    y   K  b       We state some useful results concerning K concavity  Lemma    The following properties hold   A concave function is   concave and hence Kconcave for all K       If    q  and    q  are respectively K   concave and K   concave for constants K     and K      then a   q    b   q  is  aK    bK    concave for any scalars a     and b            On the Optimality of  S  s  policies  Suppose that we can prove that Pn  x  is continuous and strictly K concave  Then by Lemma   there exists Sn   sn with the properties proved in the last point of the Lemma  It is easy to see that condition     is satisfied only if x   s  in which case the optimal value of the remaining stock z would be precisely Sn   In conclusion  if we can prove the continuity and K concavity of the functions Pn  x   n              N   then following feedback control law  known as a nonstationary  S  s  policy  is optimal  At period n  a harvest is undertaken if and only if the current stock level is greater than sn   in that case the stock is harvested down to Sn   This policy is known in the inventory control literature as a nonstationary  S s  policy     because the levels Sn and sn are time dependent  Since it is assumed that the marginal harvest cost g x  is a non increasing function  we define x  to be the zero profit level such that g x      p  If g x    p for all x  we define x       As a consequence for all x   x  we have that R  x     so that R  defined in equation      is non decreasing  Moreover if the marginal harvest cost g x  is a non increasing function  then R is convex    For the sake of consistency  we call sn the threshold value that governs the decision  even if in our case Sn  sn     We also need to make an assumption on the concavity of R    In particular the marginal cost function g is allowed to decrease but not by too much  Let m Rbe an upper bound x on the possible values of x and G x      g t dt  then we need                G m   mg m    K   nondecreasing  consider the case    x    x   sn        Cn    x     Cn    x        min Cn  f  x    wn     min Cn  f  x    wn       wn W  min  wn W  x     The main result is the following theorem  where we show that if some assumptions are satisfied  the optimal policy is of  S  s  type  The key point of this inductive proof is to show that the K concavity property is preserved by the Dynamic Programming operator  Theorem    For any setup cost K     and any positive integer N   if f    w  is nondecreasing and concave for any w and if g is non increasing and satisfies condition      then the functions Pn  x  defined as in     are continuous and K concave for all n              N   Hence there exists a non stationary  S  s  policy that is optimal  The resulting optimal present value functions Cn  x  are continuous  nondecreasing and K concave for all n              N   Proof  From equation     we know that there exists a number k such that      The proof is by induction on N   The base case N     is trivial because C   x      for all x  and therefore it is continuous  nondecreasing and k concave  Now we assume that Cn  x  is continuous  nondecreasing and kconcave  and we show that Pn    x  is continuous and Kconcave  and that Cn    x  is continuous  nondecreasing and k concave  Since f    w  is nondecreasing and concave for all w  Cn  f  z  wn    is K concave by Lemma      By Lemma    wn W  If for all x    x       a condition that implies the   concavity of R    K        k   K      f  x    wn     min  wn W  x     f  x    wn     then Cn    x     Cn    x       because Cn  x  is nondecreasing  For the case sn     x    x  and sn    x    Cn    x     Cn    x       R x     R x         because R is nondecreasing on that interval  It must be the case that Sn     x  because harvesting below x  is not profitable and reduces the marginal growth of the stock  so given that sn    Sn    x  we conclude that Cn    x  is nondecreasing  It remains to show that Cn    x  is kconcave  and by equation     it is sufficient to show that it is  K      concave  To show that definition     holds for Cn    x   we consider several cases  When x   y  sn     according to equation      we have that Cn    x     Pn    x    R x   and therefore equation     holds by Lemma   because Pn   is K concave and R   is   concave  Similarly when sn     x   y  equation     holds because R   is   concave  When x  sn     y equation     reads Cn    y   b   Cn    y   Cn    x   Cn    y    x  y  b     R y   b   R y    K   R x   R y    x  y  b  K       because Pn    x   Pn    Sn     and R   is   concave      Consistency and Complexity  min Cn   f  z  wn     wn W  Even if Theorem   completely describes the structure of the optimal policy  in general there is no closed form solution for the values of Sn and sn   that need to be computed numerically  In order to use the standard dynamic programming approach  the state  control and disturbance spaces must be discretized  for instance using an evenly spaced grid  Since we are assuming that those spaces are bounded  we obtain in this way discretized sets with a finite number of elements  We can then write DP like equations for those    Pn    x    R x   if x  sn     points  using an interpolation of the value function for the Cn    x     Pn    Sn       R x   K  if x   sn     points that are not on the grid  The equations can be then solved recursively  obtaining the semi optimal action to be      taken for each point of the grid  that can then be extended The continuity of Cn    x  descends from the continuby interpolation to obtain an approximate solution to the ity of Pn    x  and because by definition Pn    sn       original problem  R sn       Pn    Sn       R sn      K  To show it is  is also K concave  Again using Lemma    if R x  is concave  then by equation     Pn    x  is K concave  The continuity of Pn    x  is implied by the continuity of Cn  x  and R x   Given that Pn    x  is K concave and continuous  the optimal action is to harvest down to Sn   if and only if the current stock level is greater than sn     so we have   The standard dynamic programming algorithm involves O  X  W   U   T    arithmetic operations  where  X  is the number of discretized states   W   the number of possible outcomes of the  discretized  uncontrollable events   U   the maximum number of possible discretized actions that can be taken in any given state and T is the length of the time horizon  However  the priori knowledge of the structure of the optimal policy can be used to speed up the computation  In fact it is sufficient to find s  for example by bisection  and compute the optimal control associated with any state larger than s to completely characterize the policy for a given time step  The complexity of this latter algorithm is O  W   U   T   log  X        Case Study  the Pacific Halibut  As part of the ongoing effort in the emerging field of Computational Sustainability  we consider an application of our framework to the Pacific Halibut fishery  The commercial exploitation of the Pacific halibut on the Northwestern coastline of North America dates back to the late     s  and it is today one of the regions largest and most profitable fisheries The fishery developed so quickly that by the early   th century it was starting to exhibit signs of overfishing  After the publication of scientific reports which demonstrated conclusively a sharp decline of the stocks  governments of the U S  and Canada signed a treaty creating the International Pacific Halibut Commission  IPHC  to rationally manage the resource  The IPHC commission controls the amount of fish caught annually by deciding each years total allowable catch  TAC   that is precisely the decision variable hn of our optimization problem       Management Problem Formulation  To develop a bioeconomic model of the fishery  we have extracted data   from the IPHC annual reports on estimated biomass xt   harvest ht and effort Et  measured in thousands of skate soaks  for Area  A  one of the major regulatory areas in which waters are divided  for a    years period from      to       To model the population dynamics  we    Data is available from the authors upon request       Effort H     sk  soaksL   Stock H     poundsL  As with all discretization schemes  we need to discuss the consistency of the method  In particular  we would like  uniform  convergence to the solution of the original problem in the limit as the discretization becomes finer  It is well known that in general this property does not hold  However in this case Theorem   guarantees the continuity of Cn   that in turn implies the consistency of the method  even if the policy itself is not continuous as a function of the state       Intuitively  discrepancies are possible only around the threshold sn   so that they tend to disappear as the discretization becomes finer   hist  stock      est  stock hist  effort est  effort                                                       Year  Figure    Fitted models      and      compared to historical data  in bold   consider the Beverton Holt model that uses the following reproduction function xn     f  sn         m sn    r  sn       sn  M        where sn   xn hn is the stock remaining after fishing  escapement  in year n  This model can be considered as a discretization of the continuous time logistic equation  Here  parameter m represents a natural mortality coefficient  r  can be interpreted as a reproduction rate and M  r  m  m is the carrying capacity of the environment  The  a priori  mortality coefficient we use is m         that is the current working value used by the IPHC  The values of r  and M are estimated by ordinary least square fitting to the historical data  Estimated values thus obtained are reported in table    while the fitted curve is shown in figure    Parameter q b p K c  m M r   Value                                         pounds                                skate soaks                        pounds           Table    Base case parameter set  Following       we suppose that the system is affected by stochasticity in the form of seasonal shocks wn that influence only the new recruitment part xn     f  sn   wn         m sn   wn  r  sn            sn  M   Instead of assuming an a priori probability distribution for wn or trying to learn one from data  that in our case would not be feasible given current scarce data availability   we will make use of the framework developed in the previous sections  In particular we will  a priori  assume that wn are random variables all having the same finite support that we will learn from data  but we will not make any assumption on the actual weight distribution  With our data  we obtain that wn                        Iw   For the economic part of the model  we start by modeling the relationship between a harvest ht that brings the population level from xt to xt  ht and the effort Et needed to accomplish this result  We will a priori assume that there is a marginal effort involved  so that       Optimal Policy  By using the dynamic programming approach on the problem discretized with a step size of           pounds  we compute the optimal policy for a management horizon of N      years  that is the length of our original time series  As predicted by Theorem    the optimal policy                  N   for the model we constructed for area  A is a non stationary  S  s  policy  In figure   a  we plot the function      to be used in the first year  the values of S  and s  are     and        respectively   In words  the optimal policy dictates that at period n a harvest is to be undertaken if and only if the current stock level is greater than sn   in that case the stock is harvested down to Sn   Optimal policy and escapement      Et    Z  xt xt ht    dy qy b  harvest escapement                     stock     pounds                        S            s                                  stock      pounds    a  Optimal rule for selecting harvests in the first year  Optimal state and control trajectories                     stock     pounds   for some q and b  This is inspired by the fact that less effort is required when the stock is abundant  and can also be interpreted as an integral of infinitesimal Cobb Douglas production functions  a standard economic model for productivity  where b and g are the corresponding elasticities  Estimated values obtained by least squares fitting are reported in table    while the resulting curve is compared with historical data in figure    Costs involved in the Halibut fishery are divided into two categories  fixed costs and variable costs  Fixed costs include costs that are independent of the number and the duration of the trips a vessel makes  therefore generically independent from the effort Et    For example  vessel repairs costs  license and insurance fees  mooring and dockage fees are typically considered fixed costs  We will denote with K the sum of all the fixed costs  that will be incurred if and only if a harvest is undertaken  Variable costs include all the expenses that are dependent on the effort level  Variable costs typically include fuel  maintenance  crew wages  gear repair and replacement  We assume that the total variable costs are proportional to the effort Et  measured in skate soaks  according to a constant c  Parameter c is set to           for      skate soaks       skate  as estimated in      Following the analysis of the historical variable and fixed costs for the halibut fishery carried on in       we assume K                for area  A  The unit price p for the halibut is set to                   pounds  as in      If we further assume a fixed discount rate          we obtain a formulation of management problem for the Halibut fishery in Area  A that fits into the framework described in the previous section  In particular  the problem for an N years horizon is that of finding an admissible policy     i  i   N   that maximizes the revenue  CN  x  where xRn is subject to       hn   n  xn   and x R x    px  c   qy b dy   stock harvest                                         time  years                b  Stock trajectory and corresponding optimal harvests   Figure    The optimal policy  The trajectory of the system when it is managed using the optimal policy is shown in figure    together with the corresponding optimal harvests  As we can see  the optimal policy is pulsing  in the sense that it involves periodic closures of the fishery  when no harvest should be undertaken so that   the fish stock has time to recover  Of course  this kind of policy could be acceptable in practice only in combination with some rotation scheme among the different Areas  so that a constant yearly production can be sustained   Optimal state and control trajectories with rolling horizon                  To see the advantage of the optimal  S  s  policy  we compare it with the historical harvest proportions and with a CPP policy that uses the historical average harvest rate a           Table   summarizes the discounted revenues corresponding to an initial stock size x           million pounds  that is the estimated stock size in       Policy Optimal S  s Historical rates Average CPP Rolling Horizon  Disc  revenue                                                          Loss                                             Table    Policy Comparison Compared to the historical policy or the CPP policy  revenues for the optimal  S  s  policy are about     higher  as reported in table    Notice that the comparison is done assuming a worst case realization of the stochasticity  or in other words that the nature is actively playing against the manager  Notice that the large harvest prescribed by the optimal  S  s  policy in the last year is an artifact of the finite horizon effect  caused by the fact that there is no reason not to exhaust the resource at the end of the management horizon  as long as it is profitable to harvest it   However it does not affect the comparison significantly due to the discount rate  In fact the  discounted  revenue for the entire last large harvest only accounts for less than    of the total revenue  This is confirmed by looking at the results obtained with a rolling horizon strategy that always picks the optimal action with a    years long management horizon in mind  As shown in figure    this  suboptimal  strategy is not affected by the finite horizon effect  The rolling horizon strategy still involves periodic closures of the fishery and significantly outperforms the historical policies  as reported in table    To further clarify that the pulsing nature of the optimal harvests is not an artifact of the finite horizon  it is also interesting to notice that the theoretical results on the optimality of  S  s  policies and the corresponding pulsing  stock      pounds   This scheme is very different from the Constant Proportional Policy  CPP  that has been traditionally used to manage the Halibut fishery  In fact a CPP works by choosing the yearly TAC as a fixed fraction of the current stock level xt   and is aimed at maintaining the exploited stock size  the escapement  at a given fixed level  This policy can be seen as a simplified version of an  S  s  policy where the two levels do not depend on the stage n and coincide  thus defining the target stock size   stock harvest                                         time  years               Figure    Harvests and stock trajectory with the rolling horizon strategy   harvests can be carried over to the infinite horizon case via limiting arguments  The high level argument is that the optimal value function Cn  x  converges uniformly to C x  as n    while Pn  x  converges uniformly to a function P  x  as n    Given that by Theorem   Pn  x  is continuous and K concave for all n  we have that P  x  must be also continuous and K concave  Using an argument similar to the one developed in section     and by using Lemma    one can show that there exists S and s such that the optimal stationary policy for the infinite horizon problem is an  S  s  policy      Conclusions  In this paper  we have analyzed the optimality of  Ss  polices for a fairly general class of stochastic discrete time resource allocation problems  When a non stationary  S  s  policy is used  a harvest is undertaken at period n if and only if the current stock level is greater than sn   in that case the stock is harvested down to Sn   The framework developed is quite general and can be applied to problems arising in very different domains  such as natural resource management  crowdsourcing  pollution management  When assumptions of Theorem   are met  we have shown that there exists a non stationary  S  s  policy that maximizes the utility in a worst case scenario  A fundamental advantage of the game theoretic approach is that it completely avoids the problem of evaluating the probability distributions of the random variables describing the uncertainty affecting those systems  a task that is difficult or even impossible to accomplish in many practical circumstances  Given the consensus reached by the scientific community on the importance of understanding the role of uncertainty when dealing with renewable resources  we believe that worst case scenario frameworks such as the   one described here provide new insights and will become increasingly important  To contribute to the effort of the Computational Sustainability community in tackling the fundamental sustainability challenges of our time  we consider an application of our model to a marine natural resource  This type of natural resources are in fact widely believed to be endangered due to over exploitation and generally poorly managed  Using Gulf of Alaska Pacific halibut data from the International Pacific halibut Commission  IPHC  annual reports  we formulated a real world case study problem that fits into our framework  In particular  our approach defines a policy with a guaranteed lower bound on the utility function that is structurally very different from the one currently employed  As a future direction  we plan to study the effects of partial observability on the optimal policies by moving into a POMDP framework  Moreover  we aim at extending the results presented here to the multidimensional case by extending the theory on the so called    S  policies from the inventory control literature      Acknowledgments  This research is funded by NSF Expeditions in Computing grant           
  According to the Erdos discrepancy conjecture  for any infinite   sequence  there exists a homogeneous arithmetic progression of unbounded discrepancy  In other words  for any   sequence  x    x         and a discrepancy C  P there exist integers m and d such that   m i   xid     C  This is an    year old open problem and recent development proved that this conjecture is true for discrepancies up to    Paul Erdos also conjectured that this property of unbounded discrepancy even holds for the restricted case of completely multiplicative sequences  CMSs   namely sequences  x    x         where xab   xa  xb for any a  b     The longest CMS with discrepancy   has been proven to be of size      In this paper  we prove that any completely multiplicative sequence of size          or more has discrepancy at least    proving the Erdos discrepancy conjecture for CMSs of discrepancies up to    In addition  we prove that this bound is tight and increases the size of the longest known sequence of discrepancy   from         to           Finally  we provide inductive construction rules as well as streamlining methods to improve the lower bounds for sequences of higher discrepancies   Introduction Discrepancy theory addresses the problem of distributing points uniformly over some geometric object  and studies how irregularities inevitably occur in these distributions  For example  this subfield of combinatorics aims to answer the following question  for a given set U of n elements  and a finite family S    S    S            Sm   of subsets of U   is it possible to color the elements of U in red or blue  such that the difference between the number of blue elements and red elements in any subset Si is small  Important contributions in discrepancy theory include the Beck Fiala theorem     and Spencers Theorem      The Beck Fiala theorem guarantees that if each element appears at most t times in the sets of S  the elements can be colored so that the imbalance  or discrepancy  is no more than  t p    According to the Spencers theorem  the discrepancy of S grows at most as   n log  m n    Nevertheless  some important questions remain open  According to Paul Erdos himself  two of his oldest conjectures relate to the discrepancy of homogeneous arithmetic progressions  HAPs       Namely  a HAP of length k and of common difference d corresponds to the sequence  d   d          kd   The first conjecture can be formulated as follows   Submitted on April          to the   th International Conference on Principles and Practice of Constraint Programming       Conjecture    Let  x    x         be an arbitrary   sequence  The discrepancy of x w r t  HAPs mustPbe unbounded  i e  for any integer C there is an integer m and an integer d m such that   i   xid     C   This problem has been open for over eighty years  as is the weaker form according to which one can restrict oneself to completely multiplicative functions  Namely  f is a completely multiplicative function if f  a  b    f  a   f  b  for any a  b  The second conjecture translates to  Conjecture    Let  x    x         be an arbitrary completely multiplicative   sequence  The discrepancy of xP w r t  HAPs must be unbounded  i e  for any integer C there is a m m and a d such that   i   xid     C   Hereinafter  when non ambiguous  we refer to the discrepancy of a sequence as its discrepancy with respect P to homogeneous arithmetic progressions  Formally  we denote disc x    maxm d   m i   xid    We denote E   C  the length for which any sequence has discrepancy at least C      or equivalently  one plus the maximum length of a sequence of discrepancy C  Similarly  we define E   C  the length for which any completely multiplicative sequence has discrepancy at least C        A proof or disproof of these conjectures would constitute a major advancement in combinatorial number theory      To date  both conjectures have been proven to hold for the case C     The values of E       E       and E      have been long proven to be         and     respectively  while recent development proved E                  Konev and Lisitsa     also provide a new lower bound for E       After   days of computation  a SAT solver was able to find a satisfying assignment for a sequence of length          Yet  it would fail to find a solution of size         in over   weeks of computation  They also report a solution of length          the longest known sequence of discrepancy    In this paper  we substantially increase the size of the longest sequence of discrepancy    from         to           In addition  we claim that E                  making this bound tight  as Plingeling was able to prove unsat and Lingeling generated an UNSAT proof in DRUP format      This paper is organized as follows  The next section formally defines the Erdos discrepancy problems  for the general case and the multiplicative case  and presents SAT encodings for both problems  We then investigate streamlined search techniques to boost the search for lower bounds of these two problems  and to characterize additional structures that appear in a subset of the solutions  Furthermore  in a subsequent section  we provide construction rules that are based on these streamliners and allow to generate larger sequences of limited discrepancy from smaller ones  The last section presents the results of these approaches   Problem Formulation In this section  we first formally define the two conjectures as decision problems and then propose encodings for these problems     Note that  if Conjecture    resp  Conjecture    were to be rejected  E   C   resp  E   C    would correspond to infinity       Definition    EDP     Given Pm two integers n and C  does there exist a   sequence  x            xn   such that   i   xid    C for any    d  n  m  n d   Konev and Lisitsa     provide a SAT encoding for this problem that uses an automaton accepting any sub sequence of discrepancy exceeding C  A state sj of the automaton corresponds to the sum of the input sequence  while the accepting state sB captures whether the sequence has exceeded the discrepancy C  A proposition Pm   m d  is true whenever the automaton is in state i   xid after reading the sequence sj  xd           x m  d    Let pi be the proposition corresponding to xi       A proposition that tracks the state of the automaton for an input sequence  xd   x d           xn dd   can be formulated as  n d   n  C  d        d  s        m     m   d      m d    pid  sj     m d    pid  sj       sj     sj  Cj C     m   d     C jC     m d      pid  sB       m d  sC  pid  sB sC       In addition  we need to encode that the automaton is in exactly one state at any point in time  Formally  we define this proposition as    n  C         dn C  mn d         i d   sj       CjC   i d   sj   Cj   j  C      i d      sj        Finally  we can encode the Erdos Discrepancy Problem as follows  EDP   n  C    sB   n  C    n     n  C  d        d    m d   of the automaton do not Furthermore  as the authors of      the actual states sj require  C     binary variables to represent the  C     values of the states  Instead  one can modify this formulation and use log    C      binary variables to encode the automaton states  For the completely multiplicative case  we introduce additional constraints to capture the multiplicative property of any element of the sequence  i e  xid   xi xd for any    d  n     i  n d  With respect to the boolean variables pi   pd and pid   such a constraint acts as XNOR gate of input pi and pd and of output pid   Formally  we denote this proposition M i  d  and define  M i  d     pi  pd  pid     pi  pd  pid     pi  pd  pid     pi  pd  pid            Importantly  for completely multiplicative sequences  the discrepancy of the subsequence  xd        xmd   of length m and common difference d will bePthe same as m the discrepancy of the P subsequence  x Pm P m       xm    Indeed   we have   i   xid     m   i   xi xd     P  xd      i   xi       i   xi    Therefore  one needs only check that m the partial sums i   xi      m  n never exceed C nor go below C  Furthermore  note that a completely multiplicative sequence is entirely characterized by the values it takes at prime positions  i e   xp  p is prime   In addition  if there exists a completely multiplicative sequence sequence  x         xp    of discrepancy C with p prime  then  Pm the sequence  x         xp        i   xi     will also be a CMS of discrepancy C  As a result  E    C  cannot be a prime number  Overall  for the completely multiplicative case  we obtain    M i  d      EDP   n  C    sB   n  C    n  C      dn  in d  Streamlined Search The encoding of EDP  given in the previous section has successfully led to prove a tight bound for the case C          On an Intel Core i      K CPU  it takes about     seconds for Plingeling     to find a satisfying assignment for EDP            and less than   hours for Glucose     to generate a proof of E                Nevertheless  for the case C      it requires more than   days of computation for Plingeling to find a sequence of size n            and fails to find a sequence of size         in over two weeks of computation  In this section  in order to improve this lower bound and acquire a better understanding of the solution space  we explore streamlining techniques that identifies additional structure occurring in a subset of the solutions  Among the solutions of a combinatorial problem  there might be solutions that possess regularities beyond the structure of the combinatorial problem itself  Streamlining     is an effective combinatorial search strategy that exploits these additional regularities  By intentionally imposing additional structure to a combinatorial problem  it focuses the search on a highly structured subspace and triggers increased constraint reasoning and propagation  This search technique is sometimes referred to as tunneling       In other words  a streamlined search consists in adding specific desired or observed regularities  such as a partial pattern that appears in a solution  to the combinatorial solver  These additional regularities boost the solver that may find more effectively larger solutions that contain these regularities  If no solution is found  the observed regularities were likely accidental  Otherwise  one can analyze these new solutions and suggests new regularities  This methodology has been successfully applied to find efficient constructions for different combinatorial objects  such as spatially balanced Latin squares       or graceful double wheel graphs       When analyzing solutions of EDP   n     for n             there is a feature that visually stands out of the solutions  When looking at a solution as a  D matrix with entries in       and changing the dimensions of the matrix  there seems to be clear preferred matrix dimensions  say m by p  such that the m rows are mostly identical for      the columns   to p     suggesting that xi   xi mod p for    i  p     We denote period x  p  t  the streamliner that enforces this observation and define  period x  p  t    xi   xi mod p    i  t  i     mod p       First  while this observation by itself did not allow to improve the current best lower bound for E       it led to the formulation of the construction of the next section  Second  it also led to the re discovery of the so called improved Walters sequence       defined as follows    if i is   mod          i           if i is   mod        i     otherwise   In the following  we denote walters x  w  the streamliner imposing that the first w elements of a sequence x follow the improved Walters sequence  i e   walters x  w    xi      i     i  w       One can easily see that the improved Walters sequence is a special case of the periodic sequence defined previously  Namely  for any sequence x where walters x  w  holds true  then we have period x     w   Finally  another striking feature of the solutions of EDP   n     is that they tend to follow a multiplicative sequence  Interesting  EDP  restricts EDP  to the special case of multiplicative functions and we observe for the case C     that this restriction substantially impacts the value of the best bound possible  i e  E               whereas E              Nevertheless  the solutions of EDP   n     exhibit a partial multiplicative property and we define  mult x  m  l    xid   xi xd    d  m     i  n d  i  l       In the experimental section  we show the speed ups that are triggered using these streamliners  and how the best lower bound for EDP   n     gets greatly improved   Construction Rule In this section  we show how we used insights from the period x  p  t  streamliner in order to generate an inductive construction rule for sequences of discrepancy C from sequences of lower discrepancy  Consider a sequence x that is periodic of period p  as defined in the previous section  i e  period x  p   x   holds true  and is of length n   p  k  Then  the sequence x can be written as  x    y    y            yp    yp    z  y    y            yp    yp    z      y    y            yp    yp    zk              Let C be discrepancy Pthe Pmof z    z    z         zk   and C the discrepancy of  y         yp     m Given that i   xip   i   zi for any    m  k  we have disc x   C  Note that if x was completely multiplicative  then it would hold disc x    C  We study the general case where x is not necessarily multiplicative  and investigate the conditions under which disc x  is guaranteed to be less or equal to C   C    For a given common difference d and length m  we consider the subsequence p   Given the definition    of x  the subsequence  xd   x d        xmd    Let q   gcd d p   xd   x d        xmd   corresponds to    yd mod p   y d mod p        y q  d mod p   zq   yd mod p   y d mod p        y q  d mod p   z q   yd mod p                         Note that if p divides d or d divides p  this subsequence becomes  zq   z q        zqm   and is of discrepancy at most C  As a result  a sufficient condition for x to be of discrepancy at most C   C  is to have yd mod p   y d mod p        y q  d mod p of discrepancy C  and summing to    We say that such a sequence has a discrepancy modp of C    Formally  we define the problem of finding such sequences as follows  Definition    Discrepancy mod p   Given two integers p and C    does there exist a   sequence  y            yp    such that     m X  yid mod p    C       d  n  m    i    p gcd d  p         p   gcd d p   X  yid mod p         d  n        i    Notice that  given the equation     p should be odd for such a sequence to exist  We encode this problem as a Constraint Satisfaction Problem  CSP  in a natural way from the problem definition  We provide the experimental results in the next section   Results All experiments were run on a Linux  version         cluster where each node has an Intel Xeon Processor X      with dual CPU  hex core      GHz    M Cache    GB RAM  Unless otherwise noted  the results were obtained using the parallel SAT solver Plingeling  version ats  for the SAT encodings  and using IBM ILOG CPLEX CP Optimizer  release        for the CP encodings  First  we evaluate the proposed streamliners for the two problems  Table   reports the length of the sequences that were successfully generated  as well as the computation time  The first clear observation is that  for EDP    the streamlined search based on the partial multiplicative property significantly boosts the search and allows to generate solutions that appear to be out of reach of the standard search approach  For example  while it takes about    days to find a solution of length         without streamliners  the      streamlined search generates a substantially large satisfying assignment of size         in about    hours  Next  we study streamliners that were used for EDP    i e  partially imposing the walters sequence  The results clearly show the speed up triggered by the combination of the new encoding for EDP  with the walters streamliners  Interestingly  the longest walters sequence of discrepancy   is of size      Nevertheless  one can successfully impose the first     elements of the walters sequence and still expand it to a sequence of length           Furthermore  when imposing walters       it takes less than   hour and an half to find a satisfying assignment for a sequence of size           Moreover  without additional streamliners  it takes about    hours to prove unsat for the case          and allows us to claim that this bound is tight  Nevertheless  the solver generates a DRUP proof of size    GB  which lies beyond the reach of traditional checkers       Encoding  EDP   EDP   Streamliners  Size of sequence Runtime  in sec   mult           mult           mult           mult            mult                                                                                                                                walters      walters      walters      walters                                                                Table    Solution runtimes of searches with and without streamliners  The streamlined search leads to new lower bounds for the   EDP problems   In terms of the inductive construction described in the previous section  we can generate sequences whose discrepancy modp is    for p in             and    while it also generates sequences of discrepancy modp equal to   for p in                             and     Overall  this proves that one can take any sequence x of length  x  and discrepancy C and generate one of length   x  and of discrepancy C      or of length    x  and of discrepancy C      As a result  this provides a new bound for the case of discrepancy    and proves E                                 Interestingly  such a long sequence suggests that the proof of the Erdos conjecture for C     may require additional insights and analytical proof  beyond the approach proposed in this work   Conclusions In this paper  we address the Erdos discrepancy problem for general sequences as well as for completely multiplicative sequences  We adapt a SAT encoding previously pro       posed and investigate streamlining methods to speed up the solving time and understand additional structures that occur in some solutions  Overall  we substantially improve the best known lower bound for discrepancy   from         to           In addition  we claim that this bound is tight  as suggested by the unsat proof generated by Lingeling  Finally  we propose construction rules to inductively generate longer sequences of limited discrepancy   Acknowledgments This work was supported by the National Science Foundation  NSF IIS award  grant           The experiments were run on an infrastructure supported by the NSF Computing research infrastructure for Computational Sustainability grant  grant            
 UCT has recently emerged as an exciting new adversarial reasoning technique based on cleverly balancing exploration and exploitation in a Monte Carlo sampling setting  It has been particularly successful in the game of Go but the reasons for its success are not well understood and attempts to replicate its success in other domains such as Chess have failed  We provide an in depth analysis of the potential of UCT in domain independent settings  in cases where heuristic values are available  and the effect of enhancing random playouts to more informed playouts between two weak minimax players  To provide further insights  we develop synthetic game tree instances and discuss interesting properties of UCT  both empirically and analytically      INTRODUCTION  The recent introduction of the Upper Confidence bounds applied to Trees  UCT  method for adversarial game playing significantly improved the standard of computer Go programs  Gelly and Silver               In fact  it now appears that we may reach human level performance in Go within the next decade  which is substantially sooner than anyone had predicted just a few years ago  The current developments are especially surprising given that the traditional minimax game tree search  which has yielded world class play in Chess and many other games  does not scale to the domain of Go  Two issues hamper the application of minimax search to Go  a very high branching factor and the lack of a high quality board evaluation function  A good board evaluation function is key in game tree search when one cannot reach terminal states in the game tree  UCT provides an effective way to address both these issues   The UCT algorithm  Kocsis and Szepesvari        is derived from a highly effective approach to solving the multi armed bandit problem called UCB   Auer et al          The UCT sampling strategy strikes a provably optimal balance between exploration of new game states and exploitation  where lines of play that appear promising are repeatedly searched to deeper levels  This novel approach means that UCT can reach regions of the search space that are much deeper than the conventional iterative deepening minimax search  which has been the gold standard for Chess and other games  When UCT encounters a non terminal leaf node  a random  or weakly informed  playout is typically used to provide some indication of the value of the state  As the designers of UCT for Go have observed  it is somewhat counter intuitive that there is any useful information to be gained from having two weak players play out the game to completion from some intermediate state  After all  any real game between competent players will follow a very different overall trajectory than one between weak players  In Go  these properties of UCT have been very useful and clearly alleviate some of the difficulties of doing a standard minimax search  the more focused search can go much deeper than any kind of iterative deepening minimax search given the high branching factor of Go  and the playouts provide useful board evaluation information  given that a good general board evaluation function for Go is not known  The success of UCT in Go raises the natural question of whether UCT is also effective in other adversarial reasoning domains  We address this question by studying UCT in the context of Chess as well as synthetic instances designed to highlight the key aspects of UCT  We chose Chess as one of our evaluation domains mainly because standard minimax search works so well for it  We can therefore study the behavior of UCT and its two key new search concepts in detail by comparing its performance with traditional minimax results as the gold standard  As we will see    UCT per se is not competitive in Chess  However  there are promising aspects of UCT that may be used to complement more traditional search  We will also identify what causes difficulties for UCT in Chess style domains  Our results are applicable to any adversarial reasoning domain that has the characteristics we identify  After discussing the basics of UCT in Section    we will empirically show in Section   that in domainindependent settings  UCT can easily outperform minimax search with a comparable amount of computational power  We will then describe in Section   how the performance of UCT can be significantly boosted when heuristic information is available and is used in place of random playouts  The way we use heuristics is much more direct than the bootstrapping approach often used to initialize UCT leaf values  However  even with a high quality heuristic  UCT does not perform well on Chess compared to a shallow minimax search  This suggests that the different success rates of UCT on Chess and Go is perhaps explained not so much by the lack of good heuristics as by the intricate properties of the two underlying search spaces   tify active nodes at the next critical level  and so on  We study the averaging backup strategy employed by UCT and show how it can make recovering from early poor choices very tricky and expensive  This suggests that other backup strategies may work better  but designing one needs further study  We also allude to differences between single agent search as in UCB   Auer et al          which has been the motivation for the multi agent UCT algorithm  and multiagent scenarios  For example  while single agent sampling based search can easily break ties between several good moves and freeze to one such good move  in two player minimax settings  the opponent constantly keeps switching in the hope of finding a better move  thus preventing the search from freezing onto a single principal variation  This results in exponential scaling of UCT in two player games that would not occur in single agent search      BACKGROUND  Monte Carlo sampling techniques have been successfully applied in the past to produce expert level play in games of incomplete information such as Bridge  Ginsberg        and Scrabble  Sheppard         However  they have seldom outperformed traditional adversarial planning techniques such as the minimax algorithm in deterministic   player game settings such as Chess  This changed recently with the emergence of UCT  which was used to produce the first program capable of master level play in  x  Go  Gelly and Silver               a domain which had thus far proven to be challenging for minimax presumably due to a large branching factor and lack of good heuristics  UCT has also proved promising in new domains such as Kriegspiel that were beyond the scope of any traditional planning techniques  Ciancarini and Favini         and in general game playing  Finnsson and Bjornsson          We will then return to playouts and demonstrate that playouts between slightly more informed players than random players can lead to discovering information that is available only to a much deeper minimax search  Not being able to discover such information can lead to UCT falling into what we call soft traps  we will show that soft traps are pervasive even in grandmaster games of Chess and will provide a concrete example  Finally  we will turn our attention to the case of synthetic instances designed to provide insights into the behavior of UCT in practice  complementing known theoretical results about bandit based sampling and UCT that provide worst case exponential time convergence guarantees in the limit  We focus  in contrast to existing analysis  e g   Auer et al         Gelly and Silver        Coquelin and Munos         on simple cases such as binary trees with implanted winning strategies of low complexity  i e   few critical moves  where UCT does work in practice  and provide a methodology to analyze the behavior of UCT on such trees   Selection  The algorithm selects an action a that maximizes an upper confidence bound q on the  action   n s  where value   s    arg maxa Q s  a    c log n s a   This part of the paper highlights and formalizes several subtle aspects of UCT  For example  we show  both empirically and analytically  that the time to convergence scales exponentially with the depth of the critical choice points in a winning strategy  In fact  we provide an expression capturing the fact that the runtime of UCT can be decomposed additively into the time it spends identifying certain active nodes at the first critical level  then the time it needs to explore the sub trees from these active nodes to iden   Q s  a  is the current estimate of the value of taking action a at state s  n s  is the total number of visits to state s over past iterations and n s  a  is the number of times action a was selected in past visits to s  If n s  a      for any action a  it is selected before any other actions are re sampled  The opposing player symmetrically selects an action that minimizes a lower confidence bound  The constant c determines how the agent trades off exploiting known good moves and exploring under sampled ones  in our experiments with Chess  this constant was fixed at     which produced  For two player games  a single iteration of UCT starting at a state s comprises the following steps    a good balance between the two strategies  Estimation  The selection operator is repeatedly applied until a previously unvisited state is reached  If this state is non terminal  a default policy is typically used to play out the game from the current position to a terminal state with reward R  R could alternately be a heuristic board evaluation  and the new state is added to the tree  Thus  on each iteration  the size of the tree grows by   node  In our experiments  the default policy selects uniformly at random from the available actions  unless noted otherwise   Value Backup  The reward R from the current UCT episode is used to update the values of all state action pairs on the path from the root to the fringe of the tree by incrementing both n s  and n s  a  by    and incrementing Q s  a  by  R  Q s  a   n s  a   This update assigns to each state action pair the average reward accrued from every episode that passed through it      DOMAIN INDEPENDENT SETTINGS  We begin by exploring the extent to which UCT style search methods can compete with minimax search in a fully domain independent setting  This situation arises  for instance  in reasoning about quantified Boolean formulas  QBF  where all we have as input is a formula  without any information about the semantics of the variables or the specifics of the problem domain the formula is encapsulating  This also happens in the general game playing setting  Finnsson and Bjornsson         For our empirical exploration of the behavior of UCT and minimax  we use the setting of Chess but modify minimax to avoid using any Chess specific heuristic information  pretending that the domain is unknown  Specifically  for k     MM k R denotes the minimax player that performs a minimax search of depth k  uses   values at a leaf if it corresponds to a terminal state  and uses the outcome of a single random rollout if the leaf corresponds to a non terminal state  This produces a player that is aware of winning  losing  positions within its search horizon  but otherwise has the same rollout style information as is available to UCT  Experimental Setup  The results are reported in Table    which gives the success rate of the column player against the row player  The success rate  throughout this paper  is computed by assigning a score of   to each game lost    to each game won  and     to each game that resulted in a draw  Note that if m games are played between two players  the sum of the success rates of the two players will be precisely m  Further  if each of players A and A wins      Table    UCT and a purely Random player compared against minimax without domain knowledge  Table reports the success rate of the column player against the row  minimax  player  Minimax depth  nodes MM   R       MM   R        MM   R          UCT  Random                         of the non drawn games against B but A draws fewer games  then the success rate of A will be higher than A  a desirable property  In this and all experiments  unless otherwise stated  we report the average success rate over a total of     games played from the default starting position of Chess  with    played as White and    as Black  The variation amongst the games is induced by the stochastic nature of  at least one of  the players  The players used for comparison are UCT with random playouts  UCT  and the random player that simply selects a legal move uniformly at random  The UCT player is given roughly the same amount of computation power  measured using the number of nodes explored  rather than runtime  in order to discount any implementation differences   as the minimax player it is competing against  We observe that even though MM k R acts without much information in many situations  it is far from a trivial player as evidenced by its clear success against the random player  Also  searching deeper improves the performance of MM k R  not only is the success rate of MM   R against the random player higher than that of MM   R  in a direct playoff  not shown in the table   MM   R has a success rate of     against MM   R  Finally  UCT significantly outperforms MM k R  demonstrating the potential of UCT in completely knowledge free settings      BOOSTING UCT WITH HEURISTIC INFORMATION  We now consider the setting where we do have prior domain knowledge  We are interested in the extent to which this can be exploited to enhance UCT  Heuristics have already provided promising results for Go  Typically the heuristic value is used to initialize the value of leaf nodes to bias the selection process in the early iterations of the search  Nonetheless  since current heuristics in Go are not very strong  UCT is set up to fairly quickly override the heuristic value with playout values once the node has been visited sufficiently many  typically a few dozen  times  In contrast  for Chess  we have heuristics that are much more pow    erful  and we explore how much they can boost the performance of UCT  To evaluate this  we consider the player UCT H that uses the board evaluation heuristic of gnuchess at the leaves visited by UCT  rather than the            values obtained from random playouts  in other words  we fully replace playouts with heuristic evaluations  This still preserves the convergence properties of UCT  i e   with sufficiently many iterations  UCT H will converge to the true minimax value of each node  We carefully rescaled the heuristic value to fall in the range         by resetting the default checkmate valuation of gnuchess to the observed maximum heuristic value of a non terminal node           Out of other candidate rescaling schemes including sigmoidal functions  this simple scheme worked the best  Against a UCT player  with random playouts  that was given         iterations for convergence  we found that UCT H had a success rate of               and       with only          and        iterations  respectively  Thus  not only is UCT H significantly faster then UCT per iteration  because it does not do playouts and thus avoids relatively expensive repeated move generation   it needs drastically fewer iterations to be competitive with UCT  A natural question to ask  then  is how well does UCTH actually compete as a player against minimax  Unfortunately  for games such as Chess where minimax is the successful strategy  even UCT H doesnt fare too well  We found that even with        iterations  UCT H is only about as powerful as MM    a   level minimax search with the gnuchess heuristic  This suggests that the difference in the performance of UCT in Go vs  Chess is not only due to the quality of the heuristic but perhaps more importantly  due to the different nature of the two underlying search spaces and how winning is defined in the two settings  Any successful sampling based player for Chess must therefore take these aspects into account      ENHANCING RANDOM PLAYOUTS  We now focus our attention on one of the two key aspects of UCT  random playouts  and ask whether such playouts can provide useful information in domains such as Chess where we already have well designed state evaluation heuristics  An interesting question in the context of playouts is  is it at all possible to obtain useful information about a strong player by doing several playouts between two weak players  We find that random playouts tend not to provide any more information than Chess heuristics themselves  but a  slightly more powerful playoutnamely a playout between two MM   playerscan  surprisingly  reveal information that is often visible only to a significantly deeper minimax player such as MM    We quantify this in terms of a strong correlation between move rankings obtained by the two players  Such information  visible only to relatively deep and systematic minimax searches  can take the form of traps as recently studied by us  Ramanujan et al          where making the wrong move leads to a state from which the opponent has a relatively simple winning strategy  such traps  even at surprisingly shallow depths  were found to be abundant even in grandmaster games of Chess  More generally  we consider here the notion of soft traps  where a wrong move takes one to a game state from which the opponent has a guaranteed strategy for gaining significant advantage in the game  This advantage could be measured in terms of an evaluation function h for the states  In our analysis of    complete grandmaster games  we discovered that     of them had at least one occurrence of a soft trap  i e   a position where an MM   and MM   search had a significant disagreement over the valuation of the best move  We now make the notion of soft traps precise  As a generalization of k move winning strategies  Ramanujan et al          consider a heuristic state evaluation function h and a parameter   Define a k move  h    advantage strategy starting from the current state s as a length k action sequence that results in a board state s  such that h s     h s      Note that when  is sufficiently large  this becomes a k move winning strategy  Definition    Let G be a   player game with a heuristic evaluation function h  and      be a constant  The current player p at state s of G is said to be at risk of falling into a soft trap if there exists a move m from state s such that after executing m  the opponent of p has k move  h    advantage strategy  The state of the game after executing m is referred to as a soft level k search trap for p  In Figure    we explore how good heuristics and various kinds of playouts are in obtaining information that is visible to a strong player  such as a deep MM k player  for experimental purposes  we use MM   as the gold standard   For this evaluation  we consider boards taken from grandmaster games and compute the ranking from best to worst    being the best  of the possible moves as given by an MM   evaluation of each resulting state  Note that during actual gameplay  only the relative ordering of moves matters  it is for this reason that we choose to study the correlation of the move rankings rather than their raw estimated values  This also helps circumvent the problem of com    Move Rank Correlation     MM H Rank                                 MM  Playout Rank          Move Rank Correlation Gnuchess Heuristic   k Random Playouts  k Heuristic Playouts      MM  Rank      Figure    A board where playouts with MM   players are able to discover a soft trap visible at depth   while complete MM   search misses it                              Estimated Rank          Figure    Correlation of move rankings of various players  x axis  against MM   rankings  y axis   Top  playouts using MM    Bottom  gnuchess heuristic  random playouts  heuristic playouts  paring leaf value estimation methods whose outputs do not map to the same range of values  For each kind of estimation method  we apply smoothing by considering estimates within some   of each other as ties and assigning them the same rank  Figure   shows the results for a typical grandmaster board    moves     plys  deep into the play  In the top panel  we compare for each child  its MM   ranking  y axis  against the ranking obtained based on playouts using two MM   players  x axis   The points being almost on the diagonal shows that the two rankings are very well correlated  especially in the region of most interestthe bottom left region  representing moves that are considered very good by both players  In contrast  the lower pane of the figure shows that the rankings obtained using the gnuchess heuristic  random playouts  or playouts between heuristic players  x axis  are much more loosely correlated with MM   rankings  y axis   For example  points in the top left corner represent moves that MM   thinks are very poor but the other player thinks are quite good indicative of traps or soft traps missed by the weaker player  Similarly  points in the bottom right corner indicate good moves  as identified by MM    that are dismissed as bad moves by the weaker player  Overall  this demonstrates that playouts between slightly informed players  namely MM   players in this case  can have a strong correlation with information  that is usually visible only to a much stronger player  namely MM   in this case  A natural question to ask at this stage  how does the ranking induced by an MM   search itself compare to that induced by a playout between two MM   players  We have discovered that there are in fact situations in which a playout of two MM   players uncovers information that an MM   search does not  Example   describes a concrete occurrence of this phenomenon  Example    Consider the Chess board shown in Figure    We will follow the standard algebraic chess notation in our discussion  where rows  ranks  are labeled     and columns  files  are labeled a h  with a  being the bottom left corner  In the given state  the Black king is in check with Black on move and an MM   search recommends that the king be moved to h   However  this allows White a devastating countermove  moving its pawn on file f to f  and thereby trapping Blacks rook  Black can stall for two moves by using its bishop to place the White king in check  and subsequently freeing its rook to escape up file e  In this case  White simply moves its own rook to the same rank as the Black rook  This sets up a situation where Black is at minimum forced to trade its queen and rook for the White queen  Sub optimal sequences of play result in much costlier piece exchanges for Black  The correct move in the original position is for Black to move its pawn on file g to g   thereby nullifying Whites pawn threatthis is the move prescribed by a complete MM   search  as well as an MM   playout      INSIGHTS INTO UCT  SYNTHETIC SEARCH SPACES  While UCT is easy to describe  it has a rich and complex behavior on adversarial search spaces such as those of Chess and Go  In order to better understand its behavior  we consider synthetic adversarial search spaces where we vary  in a controlled manner  key properties that affect the performance of UCT                                     Depth of Critical Node    d                         Depth of Critical Node    d    Depth of Critical Node    d        Depth of Critical Node    d          Iterations to Convergence  logscale          Iterations to Convergence    Iterations to Convergence  x       Convergence of UCT vs  Strategy Complexity                                            Depth of Critical Node    d                                            Depth of Critical Node    d        Figure    UCT convergence time as a function of the depths of the critical nodes  left    D contour  middle  slice with a fixed depth of critical node    in logscale  right  slice with a fixed depth of critical node    in logscale  We study game trees with implanted winning strategies for the max player  denoted Max  who is on move at the root node  The winning strategies are parametrized by the number of critical decision nodes and their depths  If Max makes the correct action choice at every critical node  then regardless of the actions chosen by either player at all other nodes  the payoff at the end of the game is     If Max chooses an incorrect action at any of the critical nodes  then the payoff at the end of the game is drawn uniformly from             This simple model captures the notion of winning plans that exist in many tactical games like Chess  where from a given state  a player can force a win by executing a sequence of a few clever moves  In these experiments  we are interested in the time UCT takes to discover the winning strategy for Max  which we define in terms of the utility assigned by UCT to the root node  Once UCT has settled on a winning sequence of moves for Max  i e   a principal variation   it will exploit it on subsequent iterations and this will force the utility of the root node to approach     A subtle point is that the min player  denoted Min  might keep forcing Max to different principal variations  nonetheless  the paths will be equally good for Max and the value of the root will still approach     Formally  let v t  be the utility assigned to the root node of the search tree after t iterations of UCT  For a single UCT search  we define the   convergence point t as the smallest t such that v t    for all t  t   We say that UCT has   converged if the current iteration number is at least t   Unless otherwise specified  we will simply use the term converged to imply   convergence at the root with              EMPIRICAL OBSERVATIONS  Figure   illustrates how the time UCT takes to converge in the presence of   step winning strategies  i e    strategies with   critical nodes  in a    level binary tree varies as a function of the depths of the two critical nodes  hereafter referred to as d  and d    with d    d     Note that in the mesh plot  the area of interest lies beyond the d    d  line  towards the back of the plot  The middle and right most panels depict slices of this surface obtained by fixing d  and d    resp  As seen in the middle panel  for a fixed d    the convergence time of UCT is essentially exponential in d    The dependence of the convergence time on d  is more intriguingwith a fixed d    UCT appears to perform best when d  is slightly more than half of d    This dip in the curve is captured by the following expression for the runtime of UCT  which we explain below  U CT  d    d      a  C d       b   d      C  d  d            where     C      empirically       and a  b     are small constants  This expression fits the mesh plot in Figure   very closely and highlights a key property of UCT in the presence of multi step winning strategies  The runtime of UCT can be decomposed additively into the time spent between consecutive critical levels  Specifically  UCT first explores roughly  d     active nodes at level d  in time O C d         then explores each of the roughly  d     subtrees below these active nodes at level d  in time O C  d  d        each to identify roughly   d  d      active nodes at level d  in each subtree  and so on down to other critical decision levels  The quantity  d      in general    di di      per subtree  representing active nodes is nothing but the minimum number of nodes that Min can continually force Max to explore until Max has figured out a winning sequence from all of these nodes  In general  we can extend this reasoning to k critical decision levels  suggesting that the runtime of UCT is captured by  U CT  d    d            dk     a  C d       b   d      U CT  d            dk         Note that Max takes C d     iterations  and not  d       to   x  A  B y   x  y    step winning strategy for Max    step winning strategy for Min  Figure    Synthetic binary trees with implanted winning strategies for both Max and Min  identify the  d     active nodes at level d  that Min can force it to  This is because  although Max ideally has the choice to freeze to any one of its equally good children  the exploration constant forces Max to explore to some extent the other child as well  especially during the initial few visits to that node  Nevertheless  the overall time is much less than the size of the full search tree till this level  which is  d  or  d       In our second experiment  we study a more complex scenario where both Max and Min have implanted strategies and a few initial samples provide incorrect guidance at the root  see Figure     In particular  we study binary trees of depth    where Max has critical nodes at depths  x    x    where x          x       and x  is even  and Min has critical nodes at  y    y    where    y    y      and y    y  are odd  In order to win  Max must move left at the root and again at level x    if Max goes right at the root  then Min can force a win by going left at levels y  and y   i e   the right child of the root is a trap state for Max   Let A and B be the subtrees rooted at the left and right children of the root node respectively  We bias the values of the leaves that are not on a winning path for either player such that the average of the values of the leaves in A is    while the average of the values of the leaves in B is      Thus  the B subtree  though ultimately a losing proposition for Max  assuming optimal play by Min  will look more promising with limited sampling  We now ask the question  how do the depths of the strategies for the two players influence UCTs convergence time  Table   presents our findings based on an average of     UCT runs on a fixed tree  On its first few iterations  UCT receives extremely noisy estimates of the utilities of its two children at the root  In the best or favorable case  these initial estimates correlate correctly with the true utilities of the children and Max chooses to explore subtree A first  In the unfavorable case  the child rankings are reversed and Max chooses to explore subtree B first  Note that any ties will eventually resolve one way or the other  and at that point   Table    Effects of the depths of Max and Mins strategies on UCTs convergence time  F and U denote instances with favorable and unfavorable initial estimates  respectively  Maxs Strategy Depth Shallow Mid level Deep  Mins Strategy Shallow Mid level F U F U                                    k   k   k   k  Depth Deep F U                 k   k   k  Table    Effects of the depths of Max and Mins strategies on the distribution of visits to the right subtree  Maxs Strategy Depth Shallow Mid level Deep  Mins Shallow F U                          Strategy Depth Mid level Deep F U F U                                                  we fall back on one of these two cases  There are a number of interesting trends in Table    First  when estimates are unfavorable at the root  the time to convergence is greater as UCT initially wastes time in subtree B until it  at least partially  uncovers Mins winning strategy  Second  this gap in convergence time is most pronounced when either Max has a shallow winning strategy or Min has a deep winning strategy  This too makes sense  in the former case  UCT can uncover Maxs strategy very quickly if given the chance  and hence the time wasted in subtree B counts relatively much more  in the latter case  UCT simply needs to work harder to uncover Mins winning strategy and switch to subtree A  Finally  we note that increasing the depth of Mins strategy slows down UCTs convergence even in the favorable instances  The data in Table    which shows the average percentage of time UCT spends in subtree B during the runs presented in Table    helps explain this phenomenon  As Mins strategy is implanted deeper down in the tree  UCT spends more time exploring subtree B  A by product of this repeated sampling from B is that the estimated utility of the root node is now heavily biased by the samples drawn from B  when UCT eventually switches to subtree A and discovers Maxs winning strategy  it needs to work extra hard to overcome this bias and reinforce the true utility of the root  we will formalize this in Section      equation       This is illustrated by the fact that when both Max and Min have shallow strategies  when UCT converges  the root node of subtree A   T  has a typical utility estimate of        when both have deep strategies  the root node of subtree A needs to reach a much higher target value of        This highlights an important shortcoming of UCT  namely that it can be overly optimistic in its estimates of state utilities  that lead it on wild goose chases  By the time it discovers that an action it has been exploring is sub optimal  nodes higher up the tree have been reinforced with so many samples that it faces an uphill task in changing these estimates  In the face of computational constraints  for example  in a timed gameplaying setting such as Blitz Chess   this is particularly troublesome for it means that UCT could easily have spent its time exploring sub optimal moves and thus faces a very real risk of falling into a trap state       ANALYTICAL INSIGHTS  While a few attempts have been made to analyze bandit based sampling methods in general and UCT in particular  e g   Auer et al         Gelly and Silver        Coquelin and Munos         these analyses are based on the worst case scenario and  in essence  boil down to showing that an exponential  or even superexponential  Coquelin and Munos         number of iterations are necessary and sufficient for UCT to converge to true minimax values  These exponential time convergence results  while intricate and interesting  do not explain the success of UCT in practice in domains such as Go  with a practically limited number of iterations available during game play  In contrast  our goal in this section is to provide a methodology for analyzing some simple scenarios where UCT does work  and obtain insights into its runtime behavior  Specifically  we will consider   step winning strategies implanted in binary trees  We highlight three take away messages  some of which have previously been observed empirically and are derived here analytically   a  the averaging backups of UCT can make recovering from poor early choices very costly   b  UCT in two player settings scales exponentially with the depth of the critical choice points  whereas in single player settings  all that matters is the number of critical choice points  not their depth  and  c  the tension between exploration and exploitation as controlled by the exploration constant  In order to make the analysis easier while still retaining the key aspects of UCT  we work with a modified version of the algorithm in this section  Instead of implementing the UCB  exploration exploitation strategy  we will use an   greedy version of the algorithm  where           is a constant determining how often subobtimal moves are explored  Specifically  when exploring a node for the first few times  UCT simply visits  T                      T       R                                                                  TL  all     p fraction   s  a                                                                                                                                                                                                                                                                                                                                      q fraction   s      b   Figure    Synthetic binary trees with implanted winning strategies for Max   a    step winning strategy   b    step winning strategy  all children once  a round   as usual  However  after this round  it selects an optimal branch  breaking ties at random  with probability      and a sub optimal branch  breaking ties at random  with probability    Auer et al         showed that this simpler variant of UCB  also has similar good convergence properties  in the limit   as long as   decreases linearly with the number of times the node is visited  We make one further modification  where instead of dealing with tie breaking  we assume that rounds similar to the first round are repeated  i e   all children explored in each round  until ties are broken  For binary trees  which will be the main focus of this section  this modification does not make a significant difference         Scenario A  For ease of illustration  we start with the simplest case and build upon it  Consider a binary game tree T with Max on play at the top node  Let T L and T R denote the left and right subtrees  respectively  of T   Suppose that all leaves of T L are labeled     i e   Max has a sure win if he makes the left move  Suppose also that a p fraction of the leaves  where p          of T R are labeled    and the rest are labeled    This tree is depicted in Figure   a   with bold edges corresponding to winning strategy moves  How long does it take for UCT to identify the left branch as the winning move  In a given round at the root node of T   a playout from the left child always leads to    while a playout from the right child leads to    with probability p  Therefore  we have a tie with probability p and it follows that the expected number of rounds needed to break the tie is       p   Hence  the total number of visits needed to the root node of T in expectation equals       p   as there are   visits per round  plus the time it takes for UCT to converge at the left child after the tie is broken  Note that the only way for the tie to be broken in this tree is to have all    playouts   on the left and exactly one   playout on the right  implying that the left move will necessarily be identified as the optimal move when the tie is broken   This will not be the case in general  as we discuss later   From this point on  T L will be visited a      fraction of the times the root node of T is visited  Let C   value  iter   denote the number of visits needed to the winning strategy node  in this case the root node of T L   for UCT to   converge at the root node of T   where value denotes the current value of the node and iter denotes the number of visits already made to the node  due to the averaging backups of UCT  the current state of the node significantly affects the time to convergence even after a winning strategy has been identified  and we will quantify this shortly  The number of visits needed to the root of T is therefore roughly       p    C   value  iter           How do we determine C   value  iter    In the unlikely case that the current value  value  is already at least as good as   i e   value   for Max   this quantity is    Otherwise  assuming subsequent visits explore the identified winning strategy  resulting in all    playout values  the averaging nature of backups dictates that  C   value  iter          iter  value    C   value  iter     iter      value C   value  iter     iter          In our case  iter        p  and value        iter as all but the very last round should result in playout values of     Plugging these values in  the number of visits to the root node of T till convergence is roughly           p      p           p        Remark    Equation     points out an interesting limitation of UCT that we have already encountered near the end of Section      namely  that the averaging backups of UCT can make recovering from poor early choices very expensive  In particular  if iter is high and value is too low  for Max   then UCT will take a long time to make up for its mistakes before it reaches    This suggests there might be other backup strategies  although finding an effective alternative backup strategy requires further study because natural choices such as simple minimaxing tend to be very brittle         Scenario B  We now explore the tension between having a small value for the exploration constant     and a large value  This example will also illustrate that the depth of the critical nodes of a winning strategy exponentially influences the number of iterations needed for convergence   This is in stark contrast to k step winning strategies in single player settings  where it is easy to argue that the depth of the critical choice points is immaterial  and all that matters is the number of critical choice points  Intuitively  the difference between the single player and two players settings is that in the former case  since all choices look equally good  or bad  at non critical points  the player can arbitrarily freeze on one of them and keep exploiting it  while in the two player setting  the opponent prevents this freezing by continually forcing the winning player to different areas of the search space in the hope of avoiding defeat  For example  for a depth d winning strategy  the losing player can force the other player to explore precisely  d   paths  Suppose that T is modified so that the strategy embodied by T L in Scenario A is actually hidden deeper and that Max needs to make one good move to get to this strategy  Specifically  we now have a   step winning strategy for Max  with critical moves at levels   and    with the subtrees at level   being identical to the ones in Scenario A  Also  let us suppose that the right subtree of the root node has a fraction q of    leaves  which will affect tie breaking at the root  This is depicted in Figure   b   Given the expression derived above for Figure   a  for the number of times we need to visit each of these subtrees at level   in order to identify the winning strategy from there on  how many times do we need to visit the root node of the tree to achieve this  First  consider a node X one level above a winning strategy at level    Min is on move at X  which means that as soon as Max begins to identify the winning strategy on the left branch of X  Min has an incentive to switch to the right branch of X  i e   whats good for Max is bad for Min   In other words  Min will keep switching between the two choices until Max has figured out the winning strategy under both choices of Min  This means that the number of visits to X that we need is twice the number of visits to each of T L   in general  when Maxs winning strategy is at depth d  the number of visits needed will be  d times the number of visits to any single winning subtree at level d hence the exponential scaling with the depth of the winning strategy  Further  the tie at the root node of T may now be broken in favor of the right child as well  as there are leaves labeled   on both sides  If the tie breaks in favor of the left child  the favorable case   then the number of iterations needed after breaking the tie is        C   value  iter        D favorable       p    the latter part of which is similar to Scenario A  mul    tiplied by   for twice the work that needs to be done due to Mins choice at level    and divided by        since in the favorable case we will visit the left subtree of T this fraction of the times we visit the root node of T   More interestingly  when the tie at the root is incorrectly broken in favor of the right hand side child at the root  the unfavorable case   the left subtree is visited only an   fraction of the time  implying that many more visits to the root node are needed in order to achieve the same number of visits as before to the strategy nodes at level    Specifically  the number of iterations needed after breaking the tie is    D unfavorable             C   value  iter      p      f x    num iterations in unlucky case                                                          epsilon  Figure    The effect of varying   on convergence time  Additionally  we must consider the time to break the tie at the root node  which is slightly more complex than in Scenario A  The fraction of    labeled leaves on the right is q and on the left is p     p    Therefore  the probability of a tie is p  q  when both playouts yield     plus   p     q   when both playouts yield     giving p   q p  q  Thus  the expected number of visits before the tie is broken is    p    q   p  q   Further  when this happens  the tie is broken in favor of the left subtree with probability p      q  and in favor of the right subtree with probability     p   q  Putting all this together  we have the following expression for the rough number of visits needed to the root node  p      q   D favorable      p    q   p  q p    q   p  q     p   q  D unfavorable    p    q   p  q  CONCLUSION  This work provides insights into the behavior of UCT and extends its analysis to complement known worst case  super  exponential convergence results  We studied UCT in domains such as Chess where traditional minimax search is very effective  Our results demonstrate that UCT consistently beats minimax in domain independent settings  that it can be significantly boosted by incorporating a state evaluation function  and that more informed playouts can enhance performance  Finally  our results on synthetic instances with implanted strategies revealed an interesting pattern in the convergence behavior of UCT      This illustrates  in a concrete fashion  the tension between small and large values of    when the goal is to minimize the number of visits to the root node to achieve convergence  see Figure   for an illustration where p       and the C value is taken to be             Acknowledgments Supported by NSF  Expeditions in Computing award for Computational Sustainability           IIS grant          and IISI  Cornell Univ   AFOSR grant FA                  
 Many tasks in human environments require performing a sequence of navigation and manipulation steps involving objects  In unstructured human environments  the location and configuration of the objects involved often change in unpredictable ways  This requires a high level planning strategy that is robust and flexible in an uncertain environment  We propose a novel dynamic planning strategy  which can be trained from a set of example sequences  High level tasks are expressed as a sequence of primitive actions or controllers  with appropriate parameters   Our score function  based on Markov Random Field  MRF   captures the relations between environment  controllers  and their arguments  By expressing the environment using sets of attributes  the approach generalizes well to unseen scenarios  We train the parameters of our MRF using a maximum margin learning method  We provide a detailed empirical validation of our overall framework demonstrating successful plan strategies for a variety of tasks    I  I NTRODUCTION When interacting with a robot  users often under specify the tasks to be performed  For example in Figure    when asked to pour something  the robot has to infer which cup to pour into and a complete sequence of the navigation and manipulation stepsmoving close  grasping  placing  and so on  This sequence not only changes with the task  but also with the perceived state of the environment  As an example  consider the task of a robot fetching a magazine from a desk  The method to perform this task varies depending on several properties of the environment  for example  the robots relative distance from the magazine  the robots relative orientation  the thickness of the magazine  and the presence or the absence of other items on top of the magazine  If the magazine is very thin  the robot may have to slide the magazine to the side of the table to pick it up  If there is a mug sitting on top of the magazine  it would have to be moved prior to the magazine being picked up  Thus  especially when the details of the manipulation task are under specified  the success of executing the task depends on the ability to detect the object and on the ability to sequence the set of primitives  navigation and manipulation controllers  in various ways in response to the environment  In recent years  there have been significant developments in building low level controllers for robots      as well as in perceptual tasks such as object detection from sensor data               In this work  our goal is to  given the environment and the task  enable robots to sequence the navigation Jaeyong Sung  Bart Selman and Ashutosh Saxena are with the Department of Computer Science  Cornell University  Ithaca  NY  Email    jysung selman asaxena  cs cornell edu   A preliminary version of this work was presented at ICML workshop on Prediction with Sequential Models              Fig     Figure showing our Kodiak PR  in a kitchen with different objects labeled with attributes  To accomplish the under defined task of pour obj     it has to first find the mug  obj    and carry it to the table  obj    since it is dangerous to pour liquid in a tight shelf  Once the mug is on the table  it has to bring the liquid by the container  obj    and then finally pour it into the mug   and manipulation primitives  Manually sequencing instructions is not scalable because of the large variety of tasks and situations that can arise in unstructured environments  In this work  we take an attribute based representation of the environment  where each object is represented with a set of attributes  such as their size  shape related information  presence of handles  and so forth  For a given task  there are often multiple objects with similar functions that can be used to accomplish the task  and humans can naturally reason and choose the most suitable object for the given task       Our model  based on attribute representation of objects  is similarly capable of choosing the most suitable object for the given task among many objects in the environment  We take a dynamic planning approach to the problem of synthesizing  in the right order  the suitable primitive controllers  The best primitive to execute at each discrete time step is based on a score function that represents the appropriateness of a particular primitive for the current state of the environment  Conceptually  a dynamic plan consists of a loop containing a sequence of conditional statements each with an associated primitive controller or action  If the current environment matches the conditions of one of   the conditional statements  the corresponding primitive controller is executed  bringing the robot one step closer to completing the overall task  example in Section III   We will show how to generalize sequencing of primitives to make them more flexible and robust  by switching to an attribute based representation  We then show how to unroll the loop into a graph based representation  isomorphic to a Markov Random Field  We then train the parameters of the model by maximum margin learning method using a dataset comprising many examples of sequences  We evaluated our model on     controller sequences for five under specified manipulation tasks generated from    environments using   primitives  We show that our model can predict suitable primitives to be executed with the correct arguments in most settings  Furthermore  we show that  for five high level tasks  our algorithm was able to correctly sequence     of the sequences in different environments  The main contributions of this paper are      using an attribute based representation of the environment for task planning  inferring the sequence of steps where the goals are under specified and have to be inferred from the context  a graph based representation of a dynamic plan by unrolling the loop into a Markov Random Field  II  R ELATED W ORK  There is a large body of work in task planning across various communities  We describe some of them in the following categories  Manual Controller Sequencing  Many works manually sequence different types of controllers to accomplish specific types of tasks  Bollini et al      develop an end to end system which can find ingredients on a tabletop and mix them uniformly to bake cookies  Others used pre programmed sequences for tea serving and carrying humans in healthcare robotics           These approaches however cannot scale to large number of tasks when each task requires its own complicated rules for sequencing controllers and assumes a controlled environment  which is very different from actual human households  where objects of interest can appear anywhere in the environment with a variety of similar objects  Beetz et al      retrieve a sequence for making a pancake from online websites but assumes an environment with correct labels and a single choice of object for the task  Human experts can generate finite state machines for robots but this again requires explicit labels  e g  AR tags        Our work addresses these problems by representing each object in the environment as a set of attributes which is more robust than labeling the individual object             In our recent work       we learn a sequence given a natural language instruction and object labels  where the focus is to learn the grounding of the natural language into the environment  Learning Activities from Videos  In the area of computer vision  several works                  consider modeling the sequence of activities that humans perform  These works are complementary to ours because our problem is to infer the sequence of controllers and not to label the videos   Symbolic Planning  Planning problems often rely on symbolic representation of entities as well as their relations  This has often been formalized as a deduction     or satisfiability problem       A plan can also be generated hierarchically by first planning abstractly  and then generating a detailed plan recursively       Such approaches can generate a sequence of controllers that can be proven to be correct          Symbolic planners however require encoding every precondition and effect of each operation  which will not scale in human environments where there are large variations  Such planners also require domain description for each planning domain including the types of each object  e g   pallet crate   surface  hoist surface   locatable  as well as any relations  e g   on x crate y surface  available x hoist   The preconditions and effects can be learned directly from examples of recorded plans          but this method suffers when there is noise in the data       and also suffers from the difficulty of modeling real world situations with the PDDL representation       Such STRIPS style representation also restricts the environment to be represented with explicit labels  Though there is a substantial body of work on labeling human environments           it still remains a challenging task  A more reliable way of representing an environment is representing through attributes         An attribute based representation even allows classification of object classes that are not present in the training data       Similarly  in our work  we represent the environment as a set of attributes  allowing the robot to search for objects with the most suitable attributes rather than looking for a specific object label  Predicting Sequences  Predicting sequences has mostly been studied in a Markov Decision Process framework  which finds an optimal policy given the reward for each state  Because the reward function cannot be easily specified in many applications  inverse reinforcement learning  IRL  learns the reward function from an experts policy       IRL is extended to Apprenticeship Learning based on the assumption that the expert tries to optimize an unknown reward function      Most similar to our work  the MaxMargin Planning frames imitation learning as a structured max margin learning problem       However  this has only been applied to problems such as  D path planning  grasp prediction and footstep prediction       which have much smaller and clearer sets of states and actions compared to our problem of sequencing different controllers  Co Active Learning for manipulation path planning       where user preferences are learned from weak incremental feedback  does not directly apply to sequencing different controllers  Both the model based and model free methods evaluate state action pairs  When it is not possible to have knowledge about all possible or subsequent states  full backup   they can rely on sample backup which still requires sufficient sample to be drawn from the state space      However  when lots of robot object interactions are involved  highly accurate and reliable physics based robotic simulation is required along with reliable implementation of each manipulation controllers  Note that each of the manipulation primitives such as grasping are still not fully solved problems  For example  consider the scenario where the robot is grasping   the edge of the table and was given the instruction of follow traj pour table shelf   It is unclear what should occur in the environment and becomes challenging to have reliable simulation of actions  Thus  in the context of reinforcement learning  we take a maximum margin based approach to learning the weight for wT  s  a  such that it maximizes the number of states where the expert outperforms other policies  and chooses the action that maximizes wT  s  a  at each time step  The key in our work is representing task planning as a graph based model and designing a score function that uses attribute based representation of environment for under specified tasks  III  O UR A PPROACH We refer to a sequence of primitives  low level navigation and manipulation controllers  as a program  To model the sequencing of primitives  we first represent each object in the environment with a set of attributes as described in Section IV B  In order to make programs generalizable  primitives should have the following two properties  First  each primitive should specialize in an atomic operation such as moving close  pulling  grasping  and releasing  Second  a primitive should not be specific to a single high level task  By limiting the role of each primitive and keeping it general  many different manipulation tasks can be accomplished with the same small set of primitives  and our approach becomes easily adaptable to different robots by providing implementation of primitives on the new robot  For illustration  we write a program for throw garbage away in Program    Most tasks could be written in such a format  where there are many if statements inside the loop  However  even for a simple throw garbage away task  the program is quite complex  Writing down all the rules that can account for the many different scenarios that can arise in a human environment would be quite challenging   more robust alternative  At each step  the current state of the environment is considered and the next appropriate action is selected by one of the conditional statements in the main loop  A well constructed dynamic plan will identify the next step required to bring the robot closer to the overall goal in any possible world state  In complex domains  dynamic plans may become too complicated  However  we are considering basic human activities  such as following a recipe  where dynamic plans are generally quite compact and can effectively lead the robot to the goal state  Moreover  as we will demonstrate  we can learn the dynamic plan from observing a series of action sequences in related environments  In order to make our approach more general  we introduce a feature based representation for the conditions of if statements  We can extract some features from both the environment and the action that will be executed in the body of if statement  With extracted features  and some weight vector w for each if statement  the same conditional statements can be written as wT   since the environment will always contain the rationale for executing certain primitive  Such a feature based approach allows us to re write Program   in the form of Program    Program   throw garbage away  Input  environment e  trash a  gc   f ind garbage can e  repeat et   current environment if w T  et  release a         then release a    else if w T  et  move close gc       then move close gc        else if wnT  et  move close a         then move close a    end if until a  inside gc  Program   throw garbage away  Input  environment e  trash a  gc   f ind garbage can e  repeat if a  is in hand   gc is close then release a    else if a  is in hand   far from gc then move close gc  else if a  is close   a  not in hand   nothing on top of a  then grasp a         else if a  is far then move close a    end if until a  inside gc  Program   is an example of what is commonly referred to as reactive or dynamic planning           In traditional deliberative planning  a planning algorithm synthesizes a sequence of steps that starts from the given state and reaches the given goal state  Although current symbolic planners can find optimal plan sequences consisting of hundreds of steps  such long sequences often break down because of unexpected events during the execution  A dynamic plan provides a much  Now all the if statements have the same form  where the same primitive along with same arguments are used in both the condition as well as the body of the if statement  We can therefore reduce all if statements inside the loop further down to a simple line which depends only on a single weight vector and a single joint feature map  as shown in Program    for finding the most suitable pair of primitive pt and its arguments  a  t   a  t    Program   throw garbage away  Input  environment e  trash ga  repeat et   current environment  pt   a  t   a  t      arg max  wT  et   pt  a  t   a  t     pt P a  t  a  t E  execute pt  a  t   a  t   until pt   done  The approach taken in Program   also allowed removing the function f ind garbage can e   Both Program   and Program   require f ind garbage can e  which depends on semantic labeling of each object in the environment  The attributes of objects will allow the program to infer which object is a garbage can without explicit encoding    Program   provides a generic representation of a dynamic plan  We will now discuss an approach to learning a set of weights  To do so  we will employ a graph like representation obtained by unrolling the loop representing discrete time steps by different layers  We will obtain a representation that is isomorphic to a Markov Random Field  MRF  and will use a maximum margin based approach to training the weight vector  Our MRF encodes the relations between the environment  primitive and its arguments  Our empirical results show that such a framework is effectively trainable with a relatively small set of example sequences  Our feature based dynamic plan formulation therefore offers an effective and general representation to learn and generalize from action sequences  accomplishing high level tasks in a dynamic environment  IV  M ODEL F ORMULATION We are given a set of possible primitives P  navigation and manipulation controllers  to work with  see Section V  and an environment E represented by a set of attributes  Using these primitives  the robot has to accomplish a manipulation task g  T   The manipulation task g is followed by the arguments ga    ga   E which give a specification of the task  For example  the program throw garbage away would have a single argument which would be the object id of the object that needs to be thrown away  At each time step t  i e   at each iteration of the loop in Program     our environment et will dynamically change  and its relations with the primitive is represented with a joint set of features  These features include information about the physical and semantic properties of the objects as well as information about their locations in the environment  Now our goal is to predict the best primitive pt  P to execute at each discrete time step  along with its arguments  pt  a  t   a  t    We will do so by designing a score function S   that represents the correctness of executing a primitive in the current environment for a task  S g ga    ga     et  pt  a  t   a  t      wT  g ga    ga     et   pt  a  t   a  t    In order to have a parsimonious representation  we decompose our score function using a model isomorphic to a Markov Random Field  MRF   shown in Figure    This allows us to capture the dependency between primitives  their arguments  and environments which are represented by set of attributes  In the figure  the top node represents the given task and its arguments  g  ga    ga     The second layer from the top represents the sequence of primitives  and the layer below represents the arguments associated with each primitive  And  the bottom node represents the environment which is represented with set of attributes  Note that we also take into account the previous two primitives in the past  together with their arguments  pt   a  t    a  t    and pt   a  t    a  t     Now the decomposed score function is  prim task  prim args env  prim args args prev  env  z    z    z      S   Sae   Spt   Saet   Spae   Sppt   Spaae   z    z    z  args env  args env task  prim prim prev  task  Fig     Markov Random Field representation of our model at discrete time step t  The top node represents the given task g  ga    ga    The second layer from the top represents the sequence of primitives  and the layer below represents the arguments associated with each primitive  And  the bottom node represents the environment represented with set of attributes   The terms associated with an edge in the graph are defined as a linear function of its respective features  and weights w  Sae   wae  T ae  a  t   et     wae  T ae  a  t   et   Spt   wpt T pt  pt   g   Similarly  the terms associated with a clique in the graph are defined as a linear function of respective features  and weights w  Saet   waet  T aet  a  t   et   g    waet  T aet  a  t   et   g  Spae   wpae  T pae  pt   a  t   et     wpae  T pae  pt   a  t   et   Sppt   wppt  T ppt  pt    pt   g    wppt  T ptt  pt    pt   t  X Spaae   wpaaeijk T paae  pt   ai k   aj t   et   i j      k t  t    Using these edge and clique terms  our score function S can be simply written in the following form  which we have seen in Program   with an extra term g for the task  S g ga    ga     et   pt  a  t   a  t      wT  g ga    ga     et   pt  a  t   a  t     A  Features In this section  we describe our features    for the different terms in the previous section  Arguments environment  ae    The robot should be aware of its location and the current level of its interaction with objects  e g   grasped   which are given as possible primitive arguments a  t   a  t   Therefore  we add two binary features which indicate whether each primitive argument is already grasped and two features for the centroid distance from the robot to each primitive arguments  For capturing spatial relation between two objects a  t and a  t   we add one binary feature indicating whether primitive arguments a  t   a  t are currently in collision with each other  Arguments environment task  aet    To capture relations between the objects of interest  task arguments  and objects of possible interest  primitive arguments   we build a binary vector of length    First four represents the indicator values of whether the objects of interest are identical as the objects of possible interest  and the last four represents spatial relation of whether they overlap from top view    It is important to realize the type of object that is below the objects of interests  and the desired property  e g   bowllike object or table like object  may differ depending on the situation  We create two feature vectors  each of length l  If the robot is holding the object  we store its extracted attributes in the first vector  Otherwise  we store them in the second vector  If the primitive has two arguments  we use the first primitive argument since it often has higher level of interaction with the robot compared to the second argument  Finally  to capture correlation between the high level task and the types of object in primitive argument  we take a tensor product of two vectors  an attribute vector of length  l for two objects and a binary occurrence vector of length  T    The matrix of size  l   T   is flattened to a vector  Primitive task  pt    The set of primitives that are useful may differ depending on the type of the task  We create a  T   P  binary co occurrence matrix between the task g and the primitive pt that has a single non zero entry in the current tasks  g th   row and current primitives  pt th   column  Primitive arguments environment  pae    Some primitives such as hold above require one of the objects in arguments to be grasped or not to be grasped to execute correctly  We create a  P     matrix where the row for the current primitive  pt th row  contains two binary values indicating whether each primitive argument is in the manipulator  Primitive primitive previous  task  ppt    The robot makes different transitions between primitives for different tasks  Thus  a binary co occurrence matrix of size  T     P   represents transition occurrence between the primitives for each task  In this matrix  we encode two transitions for the current task g  from t    to t and from t    to t  Primitive arguments arguments previous  environment  paae    For a certain primitive in certain situations  the arguments may not change between time steps  For example  pour A B  would often be preceded by hold above  A B   Thus  the matrix of size  P     is created  with the pt th row containing   binary values representing whether the two primitive arguments at time t are the same as the two arguments at t    or the two arguments at t     B  Attributes  Every object in the environment including tables and the floor is represented using the following set of attributes  height h  max width w  length l    min w  l   volume w  l  h   min w  l  h  over max w  l  h   median w  l  h  overmax w  l  h   cylinder shape  box shape  liquid  container  handle  movable  large horizontal surface  and multiplelarge horizontal surface  Attributes such as cylinder shape  box shape  container  handle  and large horizontal surface can be reliably extracted from RGB or RGBD images  and were shown to be useful in several different applications                 We study the effects of attribute detection errors on our model in Section V  C  Learning We use a max margin approach to train a single model for all tasks  This maximum margin approach fits our formulation  since it assumes that the discriminant function is a  Fig     Figure showing two of our    environments in our evaluation dataset using    objects along with PR  robot   linear function of a weight vector w and a joint feature map  g ga    ga     et   pt  a  t   a  t     and it has time complexity linear with the number of training examples when solved using the cutting plane method       We formalize our problem as a   slack structural SVM optimization problem  i  n l   C XX i min wT w   t w     l i   t    s t  T  w  for    i  n  for each time step t    p  P  a    a   E    i  i i i i   g  ga    ga     eit   pit  ai  t   ai  t    g i  ga    ga     eit   p a    a      i i i i    pt   a  t   a  t     p  a    a      t  where n is the number of example sequences  li is the length of the ith sequence  and l is the total length combining all sequences  The loss function is defined as    p  a    a      p  a    a         p    p    a     a      a     a     With a learned w  we choose the next action in sequence by selecting a pair of primitive and arguments that gives the largest discriminant value  arg max  wT  g ga    ga     et   pt  a  t   a  t     pt P a  t  a  t E  V  E XPERIMENTS Dataset  We considered seven primitives  low level controllers   move close  A   grasp  A   release  A   place above  A B   hold above  A B   follow traj circle  A  and follow traj pour  A B   Depending on the environment and the task  these primitives could be instantiated with different arguments  For example  consider an environment that contains a bottle  obj    containing liquid  obj    and an empty cup  obj    placed on top of the shelf  among other objects  If  say from a recipe  our task is to pour the liquid  then our program should figure out the correct sequence of primitives with correct arguments  based on the objects attributes  etc     pour obj     env     move close obj     grasp obj     move close obj     place above obj   obj     release obj     grasp obj     hold above obj   obj     follow traj pour obj   obj      Note that the actual sequence does not directly interact with the liquid  obj   the only object specified by the task but rather with a container of liquid  obj     an empty cup  obj     and a table  obj     while none of these objects are specified in the task arguments  As seen in this example  the input for our planning problem is under specified  For evaluation  we prepared a dataset where the goal was to produce correct sequences for the following tasks in different environments   stir A   Given a liquid A  the robot has to identify a stirrer of ideal size  from several  and stir with it  The   TABLE I R ESULT OF BASELINES   OUR MODEL WITH VARIATIONS OF FEATURE SETS   AND OUR FULL MODEL ON OUR DATASET CONSISTING OF     SEQUENCES   T HE  PRIM  COLUMNS REPRESENT PERCENTAGE OF PRIMITIVES CORRECTLY CHOSEN REGARDLESS OF ARGUMENTS   AND ARGS  COLUMNS REPRESENT PERCENTAGE OF A CORRECT PAIR OF PRIMITIVE AND ARGUMENTS   T HE LAST COLUMN SHOWS AVERAGE PERCENTAGE OF SEQUENCES CORRECT OVER THE FIVE PROGRAMS EVALUATED    chance multiclass symb plan svm symb plan manual Only edge features Only clique features Ours   full          move prim                                     close arg                              grasp prim arg                                                                  release prim arg                                                                  place above prim arg                                                                liquid may be located on a tight shelf where it would be dangerous to stir the liquid  and the robot should always stir it on top of an open surface  like a table  The robot should always only interact with the container of the liquid  rather than the liquid itself  whenever liquid needs to be carried or poured  Our learning algorithm should learn such properties  pick and place A B   The robot has to place A on top of B  If A is under some other object C  the object C must first be moved before interacting with object A  pour A   The robot has to identify a bowl like object without object labels and pour liquid A into it  Note again that liquid A cannot be directly interacted with  and it should not be poured on top of a shelf  pour to A B   The liquid A has to be poured into the container B   A variant of the previous task where the container B is specified but the model should be able to distinguish two different tasks   throw away A   The robot has to locate a garbage can in the environment and throw out object A   In order to learn these programs  we collected     sequences for     unique scenarios by presenting participants the environment in simulation and the task to be done  We considered a single armed mobile manipulator robot for these tasks  In order to extract information about the environment at each time frame of every sequence  we implemented each primitive using OpenRAVE simulator      Though most of the scenarios had a single optimal sequence  multiple sequences were introduced when there were other acceptable variations  The length of each sequence varies from   steps to    steps  providing a total of     instances of primitives  To ensure variety in sequences  sequences were generated based on the    different environments shown in Figure    using    objects each with unique attributes  Baseline Algorithms  We compared our model against following baseline algorithms       chance  At each time step  a primitive and its arguments are selected at random  multiclass  A multiclass SVM      was trained to predict primitives without arguments  since the set of possible arguments changes depending on the environment  symbolic plan svm  A PDDL based symbolic planner          requires a domain and a problem definition  Each scenario was translated to symbolic entities and relations  However  the pre conditions and effects of  hold above prim arg                                                                   traj circle prim arg                                                                       traj pour prim arg                                                                   Average prim arg                                                                  Sequence prim arg                                               each action in domain definition were hand coded  and each object was labeled with attributes using predicates  Unlike our model that works on an under specified problem  each symbolic planning problem requires an explicit goal state  In order to define these goal states  we have trained ranking SVMs      in order to detect a stirrer  an object to pour into and a garbage can for stir  pour  and throw away  respectively  Each symbolic planning instance was then solved by reducing to a satisfiability problem           symbolic plan manual  Based on the same method as symbolic plan svm  instead of training ranking SVMs  we provided ground truth goal states  Even after providing lots of hand coded rules  it is still missing some rules due to the difficulty of representation using PDDL           These missing rules include the fact that liquid needs to be handled through its container and that objects should not be manipulated on top of the shelf   Evaluation and Results  We evaluated our algorithm through   fold cross validation  computing accuracies over primitives  over primitives with arguments  and over the full sequences  Figure   a  shows the confusion matrix for prediction of our seven primitives  We see that our model is quite robust for most primitives  With our dataset  our model was able to correctly predict pairs of primitives and arguments       of the time and full sequences       of the time  Table I   Considering only the primitives without arguments  it was able to predict primitive       of the time and full sequence       of the time  The last column of Table I shows the performance with respect to whether the complete sequence was correct or not  For example  for pouring  our model has learned not only to bring a cup over to the table  but also to pick out the cup when there are multiple other objects like a pot  a bowl  or a can that may have similar properties  How do baselines perform for our under specified planning problem  The results of various baseline algorithms are shown in Table I  If the primitive and arguments pairs are predicted at random  none of the sequences would be correct because of the large search space of arguments  Multiclass predicted well for some of the primitives but suffered greatly on primitives like place above  hold above and follow traj pour  which drastically impacts constructing overall sequences  even with correct arguments selected  The symbolic planner based approaches  symbolic plan     a  Confusion matrix for the seven primitives in our dataset  Our dataset consist of     instances of seven primitives in     sequences on five manipulation tasks    b  Percentage of programs correct  Without  c  Percentage of programs correct for    any feedback in completely autonomous mode  high level tasks such as making sweet tea  In the accuracy is        With feedback  number of completely autonomous mode  the accuracy is feedbacks on x axis   the performance increases       With feedback  number of feedbacks on This is on full     sequence dataset  x axis   the performance increases  Fig     Results with cross validation   a  On predicting the correct primitive individually   b  On predicting programs  with and without user intervention   c  On performing different tasks with the predicted sequences   svm and symbolic plan manual  suffered greatly from underspecified nature of the problem  The planners predicted correctly       and       of the times  respectively  compared to our models performance of        Even though both planners made use of heavily hand coded domain definitions of the problem  due to the nature of the language used by symbolic planners  rules such as that liquid should not be handled on top of shelves were not able to be encoded  Even if the language were capable of encoding these rules  it would require a human expert in planning language to carefully encode every single rule the expert can come up with  Also  by varying the set of features  it is evident that without very robust primitive level accuracies  the models are unable to construct a single correct sequence  How important is attribute representation of objects  For     unique scenarios in our dataset  we have randomly flipped binary attributes and observed the effects of detection errors on correctness for the full sequence  Figure     When there is no error in detecting attributes  our model performs at        With     detection error  it performs at        and with     detection errors  it performs at        Since the attribute detection is more reliable than the object detection             our model will perform better than planners based on explicit object labels  How can the robot utilize learned programs  These learned programs can form higher level tasks such as making a recipe found online  For example  serving sweet tea would require the following steps  pouring tea into a cup  pouring sugar into a cup  and stirring it  Figure     We have tested each of the four tasks  serve sweet tea  serve coffeewith milk  empty container and throw away  and serve andstore  in three environments  Each of the four tasks can be sequenced in following manner by programs respectively  pour  pour to  stir  pour to  pour to  pour  throw away  and pour  pick and place  Out of total    scenarios  our model was able to successfully complete the task for   scenarios  Does the robot need a human observer  In an assistive robotics setting  a robot will be accompanied by a human observer  With help from the human  performance can be greatly improved  Instead of choosing a primitive and argument pair that maximizes the discriminant function  the robot  Fig     Effect of attribute perception error  Figure showing percentage of programs correct with attribute labeling errors for binary attributes  For     unique scenarios  binary attributes were randomly flipped   can present the top   or   primitive and argument pairs to the observer  who can simply give feedback on the best option among those choices  At the initial time step of the sequence  with only a single piece of feedback  given   or   choices  performance improves to       and       respectively from        Figure   b    If feedback was provided through whole sequence with the top   or   choices  it further improves to       and        Furthermore  the four higher level tasks  recipes  considered earlier also shows that with a single feedback at the initial time step of each program  the results improve from     to       Figure   c    Robotic Experiments  Finally  we demonstrate that our inferred programs can be successfully executed on our Kodiak PR  robot for a given task in an environment  Using our implementation of the primitives discussed in Section V  we show our robot performing the task of serving sweet tea  It comprises executing three programs in series  pour  pour to and stir  which in total required sequence of    primitives with correct arguments  Each of these programs  i e   the sequence of primitives and arguments  is inferred for this environment  Figure   shows a few snapshots and the full video is available at  http   pr cs cornell edu learningtasksequences  VI  C ONCLUSION In this paper  we considered the problem of learning sequences of controllers for robots in unstructured human environments  In an unstructured environment  even a simple task such as pouring can take variety of different sequences of controllers depending on the configuration of the environment  We took a dynamic planning approach  where we   Fig     Few snapshots of learned sequences forming the higher level task of serving sweet tea  which takes the sequence of pouring tea into a cup  pouring sugar into a cup  and then stirring it   represent the current state of the environment using a set of attributes  To ensure that our dynamic plans are as general and flexible as possible  we designed a score function that captures relations between task  environment  primitives  and their arguments  and we trained a set of parameters weighting the various attributes from example sequences  By unrolling the program  we can obtain a Markov Random Field style representation  and use a maximum margin learning strategy  We demonstrated on a series of example sequences that our approach can effectively learn dynamic plans for various complex high level tasks  ACKNOWLEDGEMENTS This work was supported in part by ONR Grant N                and Microsoft Faculty Fellowship and NSF Career award to Saxena  
  We describe research and results centering on the construction and use of Bayesian mod els that can predict the run time of problem solvers  Our efforts are motivated by observa tions of high variance in the time required to solve instances for several challenging prob lems  The methods have application to the decision theoretic control of hard search and reasoning algorithms  We illustrate the ap proach with a focus on the task of predict ing run time for general and domain specific solvers on a hard class of structured con straint satisfaction problems  We review the use of learned models to predict the ultimate  length of a trial  based on observing the be havior of the search algorithm during an early phase of a problem session  Finally  we dis cuss how we can employ the models to inform dynamic run time decisions     Introduction  The design of procedures for solving difficult problems relies on a combination of insight  observation  and it erative refinements that take into consideration the be havior of algorithms on problem instances  Complex  impenetrable relationships often arise in the process of problem solving  and such complexity le ads to uncer tainty about the basis for observed efficiencies and in effi ciences associated with specific problem instances  We believe that recent advances in Bayesian methods for learning predictive models from data offer valuable tools for designing  controlling  and understanding au tomated reasoning methods  We focus on using machine learning to characterize variation in the run time of instances observed in in herently exponential search and reasoning problems  Predictive models for run time in this domain could  Design  real rime control   World  Context  j  Contex tual  evidence  insights  Run time  Structural evidence  Ex ecution evidence  GQlliJ  Feature refinement  insights  Figure    Bayesian approach to problem solver design and optimization  We seek to learn predictive mod els to refine and control computational procedures as well as to gain insights about problem structure and hardness   provide the basis for more optimal decision making at the microstructure of algorithmic activity as well as inform higher level policies that guide the allocation of resources  Our overall methodology is highlighted in Fig     We seek to develop models for predicting execution time by considering dependencies between execution time and one or more classes of observations  Such classes include evidence about the nat ure of the generator that has provided instances  about the structural properties of instances noted before problem solving  and about the run time behaviors of solvers as they struggle to solve the instances  The research is fundamentally iterative in nature  We exploit learning methods to identify and continue to  refine observational variables and models  balancing the predictive power of multiple observations with the cost of the real time evaluation of such evidential dis    HORVITZ ET AL        tinctions  We seek ultimately to harness the learned models to optimize the performance of automated rea soning procedures  Beyond this direct goal  the overall exploratory process promises to be useful for providing new insights about problem hardness  We first provide background on the problem solving domains we have been focusing on  Then  we describe our efforts to instrument problem solvers and to learn predictive models for run time  We describe the for mulation of variables we used in data collection and model construction and review the accuracy of the in ferred models  Finally  we discuss opportunities for exploiting the models  We focus on the sample appli cation of generating context sensitive restart policies in randomized search algorithms     Hard Search Problems  UAI      distinct symbols in which some cells may be empty but no row or column contains the same element twice  The Quasigroup Completion Problem  QCP  can be stated as follows  Given a partial quasigroup of order n can it be completed to a quasigroup of the same order   n  Figure    Graphical representation of the quasigroup problem  Left  A quasigroup instance with its comple tion  Right  A balanced instance with two holes per row column   We have focused on applying learning methods to char  acterize run times observed in backtracking search pro cedures for solving NP complete problems encoded as constraint satisfaction  CSP  and Boolean satisfiabil ity  SAT   For these problems  it has proven extremely difficult to predict the particular sensitivities of run time to changes in instances  initialization settings  and solution policies  Numerous studies have demon strated that the probability distribution over run times exhibit so called heavy tails         Restart strategies have been used in an attempt to find settings for an instance that allow it to be solved rapidly  by avoiding costly journeys into a long tail of run time  Restarts are introduced by way of a parameter that terminates the run and restarts the search from the root with a new random seed after some specified amount of time passes  measured in choices or backtracks  Progress on the design and study of algorithms for SAT and CSP has been aided by the recent devel opment of new methods for generating hard random problem instances  Pure random instances  such as k Sat  have played a key role in the development of al gorithms for propositional deduction and satisfiability testing  However  they lack the structure that char acterizes real world domains  Gomes and Selman     introduced a new benchmark domain based on Quasi groups  the Quasigroup Completion Problem  QCP    QCP captures the structure that occurs in a variety of real world problems such as timetabling  routing  and statistical experimental design  A quasigroup is a discrete structure whose multipli cation table corresponds to a Latin Square  A Latin Square of order n is an n x n array in which n dis tinct symbols are arranged so that each symbol occurs once in each row and column  A partial quaisgroup  or Latin Square  of order n is an n x n array based on  QCP is an NP complete problem     and random in stances have been found to exhibit a peak in prob lem hardness as a function of the ratio of the number of uncolored cells to the total number of cells  The peak occurs over a particular range of values of this parameter  referred to as a region of phase transition         A variant of the QCP problem  Quasigroup with Holes  QWH         includes only satisfiable instances  The QWH instance generation procedure essentially inverts the completion task  it begins with a randomly generated completed Latin square  and then erases col ors or  pokes holes   Completing QWH is NP Hard      A structural property that affects hardness of in stances significantly is the pattern of the holes in row and columns  Balancing the number holes in each row and column of instances has been found to significantly increase the hardness of the problems         Experiments with Problem Solvers  We performed a number of experiments with Bayesian learning methods to elucidate previously hidden dis tinctions and relationships in SAT and CSP reason ers  We experimented with both a randomized SAT algorithm running on Boolean encodings of the QWH and a randomized CSP solver for QWH  The SAT al gorithm was Satz Rand       a randomized version of the Satz system of Li and Anbulagan       Satz is the fastest known complete SAT algorithm for hard ran dom   SAT problems  and is well suited to many inter esting classes of structured satisfiability problems  in cluding SAT encodings of quasigroup completion prob lems      and planning problems       The solver is a version of the classic Davis Putnam  DPLL  algorithm     augmented with one step lookahead and a sophisti    UAI      cated variable  HORVITZ ET AL   choice heuristic  The lookahead opera            Formulating Evidential Variables  tion is invoked at most choice points and finds any  choices that would immediately lead contradiction after unit propagation  for these  the opposite variable assignment can be immediately made  The variable ch oice heuristic is based on picking a variable that if set would cause the greatest number of ternary clauses to be reduced to binary clauses  The variable choice set was enlarged by a noise parameter of      and value selection was performed determin istically by always branching on  true  first   variable value to a  The second backtrack search algorithm we studied is randomized version of a specialized CSP solver for quasigroup completion problem s  written using the ILOG solver constraint programming library  The backtrack search algorithm uses as a variable choice heuristic a variant of the Brelaz heuristic  Further more  it uses a sophisticated propagation method to enforce the constraints that assert that all the colors in a row  column must be different  We refer to such a constraint as alldiff  The propagation of the alldiff constraint corresponds to solving a matching problem on a bipartite graph using a network flow algorithm              a  learned predictive models for run time  motivated two different classes of target problems  For the first class of problem  we assume that a solver is chal lenged by a n instance and must solve that specific problem as quickly as possible  We term this the Sin gle Instance problem  In a second class of problem  we draw cases from a distribution of instances and are required to solve any instance as soon as possible  or as many instances as possible for any amount of time allocated  We call these challenges Multiple Instance problems  and the subproblems as the Any Instance and Max Instances problems   respectively  We  by  We collected evidence and built models for CSP and Satz solvers applied to the QWH problem for both the Single In st an ce and Multiple Instances challenge  We shall refer to the four problem solving experiments as CSP QWH Single  CSP QWH Multi  Satz  QW H  Single  and S atz Q WH  Multi  Building predictive Bayesian models for the CSP  Q WH S ingle and Satz QWH Single problems centered on gathering data on the probabilistic relationships between observational variables and run time for single instances with ran domized restarts  Experiments for the CSP QWH Multi and S atz  Q WH  Multi problems centered on per forming single runs on multiple instances drawn from the same instance generator   We worked to define variables that we believed could provide information on problem solving progress for a period of observation in an early phase of runs that we refer to as th e observation horizon  The defin iti on of variables was initially guided by intuition  However  results from our early experiments helped us to refine sets of variables and to propose additional candidates  We initially explored a large number of variables  in cluding those that were difficult to compute  Although we planned ultimately to avoid the use of costly ob servations in real time forecasting settings  we were interested in probing the predictive power and inter dependencies among features regardless of cost  Un der st andin g such informational dependencies promised to be useful in understanding the potential losses in predictive power with the removal of costly features  or substitution of expensive evidence with less expen sive  approximate observations  We eventually limited the features explored to those that could be computed with low  constant  overhead  We sought to collect information about base values as well as several variants and combinations of these val ues  For example  we formulated features that could capture higher l evel patterns and dynamics of the state of a prob l em solver that could serve as useful probes of solution progress  Beyond exploring base observa tions about the program state at particular points in a case  we defined new families of observations such as first and second derivatives of the base variables  and summaries of the status of variables over time   Rather than include a separate variable in the model for each feature at each choice point which would have led to an explosion in the number of variables and severely limited generalization features and their dynamics were represented by variables for their sum mary statistics over the observation horizon  The sum mary statistics included initial  final  average  mini mum  and maximum values of the features during the observation period  For example  at each choice point  the SAT solver recorded the current number of binary clauses  The training data would thus included a vari able for the average first derivative of t he number of binary clauses during the observation period  Finally  for several of the features  we also computed a sum mary statistic that measured the number of times the sign of the feature changed from negative to positive or vice versa  We developed distinct sets of observational var iables for the CSP and Satz solvers  The features for the CSP solver included some that were generic to any constraint satisfaction problem  such as the number of backtracks  the depth of the search tree  and the   HORVITZ ET AL        average domain size of the unbound CSP variables  Other features  such as the variance in the distribution of unbound CSP variables between different columns of the square  were specific to Latin squares  As we will see below  the inclusion of such domain specific features was important in learning strongly predictive models  The CSP solver recorded    basic features at each choice point which were summarized by a to tal of     variables  The variables that turned out to be most informative for prediction are described in Sec      below  The features recorded by Satz Rand were largely generic to SAT  We included a feature for the num ber of Boolean variables that had been set positively  this feature is problem specific in the sense that under the SAT encoding we used  only a positive Boolean variable corresponds to a bound CSP variable  i e  a colored squared   Some features measured the current problem size  e g  the number of unbound variables   others the size of the search tree  and still others the effectiveness of unit propagation and lookahead  We also calculated two other features of special note  One was the logarithm of the total number of possible truth assignments  models  that had been ruled out at any point in the search  this quantity can be effi ciently calculated by examining the stack of assumed and proven Boolean variable managed by the DPLL algorithm  The other is a quantity from the theory of random graphs called     that measures the degree of interaction between the binary clauses of the formula       In all Satz recorded    basic features that were summarized in     variables      Collecting Run Time Data  For all experiments  observational variables were col lected over an observational horizon of      solver choice points  Choice points are states in search pnr cedures where the algorithm assigns a value to vari ables heuristically  per the policies implemented in the problem solver  Such points do not include the cases where variable assignment is forced via propagation of previous set values  as occurs with unit propagation  backtracking  lookahead  and forward checking  For the studies described  we represented run time as a binary variable with discrete states short versus long  We defined short runs as cases completed before the median of the run times for all cases in each data set  Instances with run times shorter than the observation horizon were not considered in the analyses   Models and Results  We employed Bayesian structure learning to infer pre dictive models from data and to identify key variables from the larger set of observations we collected  Over the last decade  there has been steady progress on methods for inferring Bayesian networks from data                  Given a dataset  the methods typically perform heuristic search over a space of dependency models and employ a Bayesian score to identify mod els with the greatest ability to predict the data  The Bayesian score estimates p modelldata  by approxi mating p  data lmodel p  model   Chickering  Hecker man and Meek     show how to evaluate the Bayesian score for models in which the conditional distributions are decision trees  This Bayesian score requires a prior distribution over both the parameters and the struc ture of the model  In our experiments  we used a uni form parameter prior  Chickering et al  suggest using a structure prior of the form  p model  r  fP  where                and fp is the number of free parameters in the model  Intuitively  smaller values of r   make large trees unlikely a priori  and thus     can be used to help avoid overfitting  We used this prior  and tuned r   as described below     We employed the methods of Chickering et a   to infer models and to build decision trees for run time from the data collected in experiments with CSP and Satz problem solvers applied to QWH problem instances  We shall describe sample results from the data col lection and four learning experiments  focusing on the CSP QWH Single case in detail            UAI      CSP QWH Single Problem  For a sample CSP QWH Single problem  we built a training set by selecting nonbalanced QWH problem instance of order    with     unassigned variables  We solved this instance      times for the training set and      times for the test data set  initiating each run with a random seed  We collected run time data and the states of multiple variables for each case over an observational horizon of      choice points  We also created a marginal model  capturing the overall run time statistics for the training set  We optimized the r   parameter used in the structure prior of the Bayesian score by splitting the training set       into training and holdout data sets  respectively  We selected a kappa value by identifying a soft peak in the Bayesian score  This value was used to build a dependency model and decision tree for run time from the full training set  We then tested the abilities of the marginal model and the learned decision tree to pre dict the outcomes in the test data set  We computed a classification accuracy for the learned and marginal   UAI      HORVITZ ET AL   models to characterize the power of these models  The classification accuracy is the likelihood that the classi fier will correctly identify the run time of cases in the test set  We also computed an average log score for the models  Fig    displays the learned Bayesian network for this dataset  The figure highlights key dependencies and variables discovered for the data set  Fig    shows the decision tree for run time  The classification accuracy for the learned model is       in contrast with a classification accuracy of       for the marginal model  The average log score of the learned model is        a nd the average log score of the marginal model was         Because this was both the strongest and most com pact model we learned  we will discuss the features it involves in more detail  Following Fig    from left to right  these are  VarRowColumn measures the variance in the number of uncolored cells in the QWH instance across rows and across columns  A low variance indicates the open cells are evenly balanced throughout the square  As noted earlier  balanced instances are harder to solve than unbalanced ones      A rather complex summary statistic of this quantity appears at the root of the de cision tree  namely the minimum of the first derivative of this quantity during the observation period  In fu ture work we will be examining this feature carefully in order to determine why this particular statistic was most relevant  AvgColumn measures the ratio of the number of uncol ored cells and the number of columns or rows  A low value for this feature indicates that the quasigroup is nearly complete  The decision tree shows that a run is likely to be fast if the min i mum value of this quantity over the entire observation period is small  MinDepth is the minimum depth of all leaves of the search tree  and the summary statistic is simply the fi nal value of this quantity  The third and fourth nodes of the decision tree show that short runs are associ ated with high minimum depth and long runs with low minimum depth  This may be interpreted as in dicating the search trees for the shorter runs have a more regular shape  AvgDepth is the average depth of a node in the search tree  The model discovers that short runs are associ ated with a high frequency in the change of the sign of the first derivative of the average depth  In other words  frequent fluctuations up and down in the aver age depth indicate a short run  We do not yet have an intuitive explanation for this phenomena        VarRowColumn appears again as the last node in the decision tree  Here we see that if the maximum vari ance of the number of uncolored cells in the QWH instance across rows and columns is low  i e   the prob lem remains balanced  then the run is long  as might be expected       CSP QWH Multi Problem  For a CSP QWH Multi problem  we built training and test sets by selecting instances of nonbalanced QWH problems of order    with     unassigned variables  We collected data on      instances for the training set and      instances for the test set  As we were running instances of potentially different fundamental hardnesses  we normalized the feature measurements by the size of the instance  measured in CSP variables  after the instances were initially sim plified by forward checking  That is  although all the instances originally had the same number of uncolored cells  polynomial time preprocessing fills in some of the cells  thus revealing the true size of the instance  We collected run time data for each instance over an observational horizon of      choice points  The learned model was found to have a classification accu racy of       in comparison to the marginal model ac curacy of        The average log score for the learned model was found to be        and the average log score for the marginal model was              Satz QWH Single Problem  We performed analogous studies with the Satz solver  In a study of the Satz QWH Single problem  we stud ied a single QWH instance  bqwh             We found that the learned model had a classification ac curacy of        in comparison to a classification accu racy of       for the marginal model  The average log score of the learned model was found to be        and the log score of the marginal model was         The predictive power of the SAT model was less than that of the corresponding CSP model  This is reason able since the CSP model had access to features that more precisely captured special features of quasigroup problems  such as balance   The decision tree was still relatively small  containing    nodes that referred to    different summary variables  Observations that turned out to be most relevant for the SAT model included    The maximum number of variables set to  true  during the observation period  As noted earlier  this corresponds to the number of CSP variables that would be bound in the direct CSP encoding    HORVITZ ET AL        UAl       Figure    The learned Bayesian network for a sample CSP QWH Single problem  Key dependencies and variables are highlighted   I Y  I              Nat                Nat             Not     S                                           Nat                                S        Not                                       Figure    The decision tree inferred for run time from data gathered in a CSP QWH Single experiment  The probability of a short run is captured by the light component of the bargraphs displayed at the leaves    UAI      HORVITZ ET AL     The number of models ruled out     The number of unit propagations performed            The number of variables eliminated by Satz s lookahead component  that is  the effectiveness o f lookahead  The quantity     described in Sec      above  a mea sure of the constrainedness of the binary clause subproblem  Satz QWH Multi Problem  For the experiment with the Satz QWH Multi prob lem  we executed single runs of QWH instances with the same parameters as the instance studied in the Satz QWH Single Problem  bqwh         for the training and test sets  Run time and observational variables were normalized in the same manner as for the CSP QWH Multi problem  The classification ac curacy of the learned model was found to be         The classification accuracy of the marginal model was found to be        The average log score for the model was        and the average log score for the marginal model was              Toward Larger Studies  For broad application in guiding computational prob lem solving  it is important to develop an understand ing of how results for sample instances  such as the problems described in Sections     through      gener alize to new instances within and across distinct classes of problems  We have been working to build insights about generalizability by exploring the statistics of the performance of classifiers on sets of problem instances  The work on studies with larger numbers of data sets has been limited by the amount of time required to generate data sets for the hard problems being stud ied  With our computing platforms  several days of computational effort were typically required to pro duce each data set  As an example of our work on generalization  we re view the statistics of model quality and classification accuracy  and the regularity of discriminatory features for additional data sets of instances in the CSP QWH Single problem class  We defined ten additional nonbalanced QWH problem instances  parameterized in the same manner as the CSP problem described in Section      order    with     unassigned variables   We employed the same data generation and analysis procedures as before  building and testing ten separate models  Generating data for these analyses using the ILOG libary executed on an       Intel Pentium III  running at     Mhz  required ap proximately twenty four hours per      runs  Thus  each CSP dataset required approximately five days of computation  In summary  we found significant boosts in classi fication accuracy for all of the instances  For the ten datasets  the mean classification accuracy for the learned models was       with a standard deviation of        The average log score for the models was        with a standard deviation of        The predictive power of the learned models stands in contrast to the classification accuracy of using background statistics  the mean classification accuracy of the marginal mod els was       with a standard deviation of        The average log score for the marginal models was        with a standard deviation of        Thus  we observed relatively consistent predictive power of the methods across the new instances  We observed variation in the tree structure and dis criminatory features across the ten learned models  Nevertheless  several features appeared as valuable discriminators in multiple models  including statistics based on measures of VarRowColumn  AvgColumn  AvgDepth  and MinDepth  Some of the evidential fea tures recurred for different problems  showing signifi cant predictive value across models with greater fre quency than others  For example  measures of the maximum variation in the number of uncolored cells in the QWH instance across rows and columns  Max VarRowColumn  appeared as being an important dis criminator in many of the models     Generalizing Observation Policies  For the experiments described in Sections   and    we employed a policy of gathering evidence over an obser vation horizon of the initial      choice points  This observational policy can be generalized in several ways  For example  in addition to harvesting evidence within the observation horizon  we can consider the amount of time expended so far during a run as an explicit observation  Also  evidence gathering can be general ized to consider the status of variables and statistics of variables at progressively later times during a run  Beyond experimenting with different observational policies  we believe that there is potential for harness ing value of information analyses to optimize the gath ering of information  For example  there is opportu nity for employing affine analysis and optimization to generate tractable real time observation policies that dictate which evidence to evaluate at different times during a run  conditioned on evidence that has already been observed during that run              HORVITZ ET AL   Time Expended  as  Evidence  In the process of exploring alternate observation policies  we investigated the value of extending the bounded horizon policy described in Section    with a consideration of the status of time expended so far during a run  To probe potential boosts with inclusion of time expended  we divided several of the data sets explored in Section     into subsets based on whether runs with the data set had exceeded specific run time boundaries  Then  we built distinct run time specific models and tested the predictive power of these models on test sets containing instances of appropriate mini mal length  Such time specific models could be used in practice as a cascade of models  depending on the amount of time that had already been expended on a run  We typically found boosts in the predictive power of models built with such temporal decompositions  As we had expected  the boosts are greatest for models conditioned on the largest amounts of expended time  As an example  let us consider one of the data sets generated for the study in Section      The model that had been built previously with all of the data had a classification accuracy of         The median time for the runs represented in the set was nearly        choice points  We created three separate sub sets of the complete set of runs  the set of runs that exceeded       choice points  the set that exceeded       choice points  and the set that had exceeded        choice points  We created distinct predictive models for each training set and tested these mod els with cases drawn from test sets containing runs of appropriate minimal length  The classification accu racies of the models for the low  medium  and high time expenditure were               and       respec tively  We shall be continuing to study the use of time allocated as a predictive variable     Application  Dynamic Restart Policies  A predictive model can be used in several ways to control a solver  For example  the variable selection heuristic used to decompose the problem instance can be designed to minimize the expected solution time of the subproblems  Another application centers on building distinct models to predict the run time as sociated with different global strategies  As an ex ample  we can learn to predict the relative perfor mance of ordinary chronological backtrack search and dependency directed backtracking with clause learn ing       Such a predictive model could be used to decide whether the overhead of clause learning would be worthwhile for a particular instance   UA       Problem and instance specific predictions of run time can also be used to drive dynamic cutoff decisions on when to suspend a current case and restart with a new random seed or new problem instance  depending on the class of problem  For example  consider a greedy analysis  where we deliberate about the value of ceas ing a run that is in progress and performing a restart on that instance or another instance  given predictions about run time  The predictive models described in this paper can provide the expected time remaining until completion of a current run  Initiating a new run will have an expected run time provided by the statistics of the marginal model  From the perspec tive of a single step analysis  when the expected time remaining for the current instance is greater than the expected time of the next instance  as defined by the background marginal model  it is better to cease ac tivity and perform a restart  More generally  we can construct richer multistep analyses that provide the fastest solutions to a particular instance or the highest rate of completed solutions with computational effort  We can also use the predictive models to perform com parative analyses with previous policies  Luby et al       have shown that the optimal restart policy  as suming full knowledge of the distribution  is one with a fixed cutoff  They also provide a universal strat egy   using gradually increasing cutoffs  for minimizing the expected cost of randomized procedures  assum ing no prior knowledge of the probability distribution  They show that the universal strategy is within a log factor of optimal  These results essential settle the distribution free case  Consider now the following dynamic policy  Observe a run for   steps  If a solution is not found  then predict whether the run will complete within a total of L steps  If the prediction is negative  then immediately restart  otherwise continue to run for up to a total of L steps before restarting if no solution is found  An upper bound on the expected run of this policy can be calculated in terms of the model accuracy A and the probability Pi of a single run successfully ending in i or fewer steps  For simplicity of exposition we assume that the model s accuracy in predicting long or short runs is identical  The expected number of runs until a solution is found is E N     A PL  Po   Po   An upper bound on the expected number of steps in a single run can be calculated by assuming that runs that end within   steps take exactly   steps  and that runs that end in      to L steps take exactly L steps  The probability that the policy continues a run past   steps  i e   the prediction was positive  is APL     A       PL   An upper bound on the expected length of a single run is Eub R       L  O  APL       A    PL    Thus  an upper bound on the expected time to        UAI      solve a  HORVITZ ET AL   proble m  E N Eub R    using the policy is  It is important to note that the expected time depends on both the accuracy of the model and the prediction point L  in general  one would want to vary L in or der to optimize the solution time   Furthermore  in  general  it would be better to design more sophisti cated dynamic policies that made use of all informa tion gathered over a run  rather than just during the first   steps  But even a non optimized policy based directly on the models discussed in this paper can out perform the optimal fixed policy  For example  in the CSP QWH single problem case  the optimal fixed pol icy has an expected solution  time of        steps  while  the dynamic policy has an expected solution time of only          steps  Optimizing the choice of L should  provide about an order of magnitude further improve ment        c s ons about the partition of resources  formulation and inference  and Klein        between  re  In other work  Horvitz  constructed Bayesian models consid  ering the time expended so far in theorem proving  They monitored the progress of search in a proposi tional theorem prover and used measures of progress in updating the probability of truth or falsity of as  sertions  A Bayesian model was harnessed to update belief about different outcomes as a function of the amount of time that problem solving continued with out halting  Stepping back to view the larger body of work on the decision theoretic control of computation  measures of  expected value of computation                 employed to guide problem solving  rely on forecasts of the refinements of partial results with future com putation  More generally  representations of problem solving progress have been central in research on flex ible or anytime methods procedures that exhibit a  While it may not be surprising that a dynamic policy  relatively smooth surface of performance  can outperform the optimal fixed policy  it is interest  location of computational resources   with the al  ing to note that this can occur when the observation time   is  greater  than the fixed cutoff   That is  for  proper values of L and A  it may be worthwhile to ob serve each run for       steps  even if the optimal fixed  strategy is to cutoff after     st eps  These and other  issues concerning applications of prediction models to restart policies are examined in detail in a forthcoming paper      Future Work and Directions  This work represents a vector in a space of ongoing re search  We are pursuing several lines of research with the goals of enhancing the power and generalizing the applicability of the predictive methods  We are explor ing the modeling of run time at a finer grain through the use of continuous variables and prototypical named     distributions  We are also exploring the value of de  Related Work  composing the learning problem into models that pre  Learning methods have been employed in previous re search in a attempt to enhance the performance opti mize reasoning systems  In work on  speed up learn ing   investigators have attempted to increase plan ning efficiency by learn i ng goal specific preferences for  plan operators             Khardon and Roth explored  the offline reformulation of representations based on experiences with problem solving in an environment to enhance run time efficiency         Our work on using  probabilistic models to learn about algorithmic perfor m ance and to guide problem solving is most c losely re  lated to research on flexible computation and decision theoretic control  Related work in this arena focused on the use of predictive models to control computa tion  Breese and Horvitz     collected data about the  dict the average execution times seen with multiple runs and models that predict how well a particular in stance will do relative to the overall hardness of the problem   In other extensions  we are exploring the  feasibility of inferring the likelihood that an instance is solvable versus unsolvable and building models that forecast the overall expected run time to completion by conditioning on each situation  We are also inter ested in pursuing more general  dynamic observational policies and in harnessing the value of information to identify a set of conditional decisions about the pattern and timing of monitoring  F inally  we are continuing to investigate the formulation and testing of ideal poli cies for harnessing the predictive models to optimize restart policies   progress of search for graph cliquing and of cutset anal ysis for use in minimizing the time of probabilistic in  ference with Bayesian networks      Summary  The work was mo  tivated by the challenge of identifying the ideal time  We presented a methodology for characterizing the run  for preprocessing graphical models for faster inference before initiating inference  trading off reformulation  time of problem instances for randomized backtrack style search algorithms that have been developed to  time for inference time   Trajectories of progress as  solve a hard class of structured constraint satisfaction  a function of  of Bayesian network prob  parameter s  lem instances were learned for use in dynamic de   problems  The methods are motivated  by recent suc  cesses with using fixed restart policies to address the   HORVITZ ET AL        UAI       high variance in running time typically exhibited by        backtracking search algorithms  We described two dis tinct formulations of problem solving goals and b uilt  D  Beckerman   J  Breese  and K   Rommelse  Decision theoretic troubleshooting  CA CM                           D   Beckerman  D   M   Chickering  C  Meek  R  Roun thwaite  and C  Kadie  Dependency networks for den sity estimation  collaborative filtering  and data visu alization  In Proceedings of UA I       Stanford  CA  pages                     E  Horvitz and A  Klein  Reasoning  metareasoning  and mathematical truth  Studies of theorem proving under limited resources  In Proceedings of UA I      pages          Montreal  Canada  August       Mor gan Kaufmann  San Francisco         E   J   Horvitz  Reasoning under varying and uncer tain resource constraints  In Proceedings of A A A I     pages             Morgan Kaufmann  San Mateo  CA  August        butions and feedback         
 Stochastic algorithms are among the best for solving computationally hard search and rea soning problems  The runtime of such pro cedures is characterized by a random vari able  Different algorithms give rise to differ ent probability distributions  One can take advantage of such differences by combining several algorithms into a portfolio  and run ning them in parallel or interleaving them on a single processor  We provide a de tailed evaluation of the portfolio approach on distributions of hard combinatorial search problems  We show under what conditions the portfolio approach can have a dramatic computational advantage over the best tra ditional methods      Introduction  Randomized algorithms are among the best current algorithms for solving computationally hard problem  Most local search methods for solving combinatorial optimization problems have a stochastic component  both to generate an initial candidate solution  as well as to choose among good local improvements during the search  Complete backtrack style search methods often also use an element of randomness in their value and variable selection in case of ties  The runtime of these algorithms varies per run on the same problem instance  and therefore can be characterized by a prob ability distribution  The performance of algorithms can also vary dramatically among different problem instances  In this case  we want to consider the per formance profile of the algorithm over a spectrum of problem instances  Carla P  Gomes works for Rome Laboratory search Associate   as  a Re  Given the diversity in performance profiles among algorithms  various approaches have been developed to combine different algorithms to take into account the computational resource constraints and to opti mize the overall performance  These considerations led to the development of anytime algorithms  Dean and Boddy        decision theoretic metareasoning and related approaches  Horvitz and Zilberstein       Russell and Norvig        and algorithm portfolio de sign  Huberman et al         Despite the numer ous results obtained in these areas  so far they have not been exploited much by the traditional commu nities that study hard computational problems  such as operations research  OR   constraint satisfaction  CSP   theorem proving  and the experimental algo rithms community  In order to bridge this gap  we study the possibility of combining algorithms in the context of the recent re sults concerning the inherent complexity of computa tionally hard search and reasoning problems  We will provide a rigorous empirical study of the performance profiles of several of the state of the art search meth ods on a distribution of hard search problems  Our search problems are based on the so called quasigroup completion task  defined below  For this particular combinatorial search problem  we can vary the compu tational difficulty and the amount of inherent problem structure in a controlled manner  This enables us to study different aspects of the algorithm performance profiles  Our studies reveal that in many cases the performance of a single algorithm dominates all others  on the prob lem class under consideration  This may be due to the fact that heuristics are often highly tuned for par ticular problem domains  Having a single algorithm that dominates over the whole spectrum of problem in stances prevents any possible payoff of combining dif ferent algorithms  However  we also identify several in teresting problem classes where no single method dom inates  We will show that on those problem classes    Algorithm Portfolio Design  Theory vs  Practice       designing a portfolio of several algorithms gives a dra  to the original problem of finding an arbitrary latin  matic improvement in terms of overall performance   square   In addition  we also show that a good strategy for de  values is as a set of additional problem constraints to  signing a portfolio is to combine many short runs of  the basic structure of the quasigroup   the same algorithm   The effectiveness of such port  folios explains the common practice of  restarts  for stochastic procedures  where the same algorithm is run repeatedly with different initial seeds for the random number generator   For related work on the effective ness of restarts  see e g   Aldous and Vazirani       Ertel       Selman and Kirkpatrick         Another way to look at these pre assigned  There is a natural formulation of the problem as a Constraint Satisfaction Problem  We have a variable  for each of the N  entries in the multiplication table of the quasigroup  and we use constraints to capture the requirement of having no repeated values in any row or column  All variables have the same domain  namely the set of elements Q of the quasigroup  Pre assigned  Our results suggest that the various ideas on flexible  values are captured by fixing the value of some of the  computation can indeed play a significant role in al  variables   gorithm design  complementing the more traditional methods for computationally hard search and reason ing problems   Colbourn        showed the quasigroup completion problem to be NP complete   In previous work  we  identified a clear phase transition phenomenon for the  The paper is organized as follows   In the next sec  quasigroup completion problem  Gomes and Selman  tion  we introduce our benchmark problem domain           the quasigroup completion problem  We also discuss  observe that the costs peak roughly around the same  See Figures   and     From the figures  we  the theoretical complexity of the problem  In section  ratio  approximately     pre  assignment  for differ     we give the performance distribution profiles for sev  ent values of N   Each data point is generated using  eral complete stochastic search methods on our prob        problem instances  The pre assigned values were  lem domain  Section    we design and evaluate various  randomly generated   This phase transition with the  algorithm portfolios  In section    we summarize our  corresponding cost profile allows us to tune the diffi  results and discuss future directions   culty of our problem class by varying the percentage of pre assigned values      A Structured Hard Search Problem  An interesting application area of latin squares is the design of statistical experiments  The purpose of latin squares is to eliminate the effect of certain system  In order to study the performance profile of differ  atic dependency among the data  Denes and Keedwell  ent search strategies  we derive generic distributions          Another interesting application is in scheduling  of hard combinatorial search problems from the do  and timetabling  For example  latin squares are useful  main of finite algerbra  In particular  we consider the  in determining intricate schedules involving pairwise  quasigroup domain  A quasigroup is an ordered pair  meetings among the members of a group  Anderson    Q           where Q is a set and      on Q such that the equations    a  is a binary operation   x    b and  y    a      b  a  b in Q  The order N of the quasigroup is the cardinality of the set Q  The best way to understand the structure of a quasigroup is to consider the N by N multipli  are uniquely solvable for every pair of elements  cation table as defined by its binary operation  The constraints on  a  quasigroup are such that its multipli  cation table defines a Latin square  This means that in each row of the table  each element of the set Q occurs          The natural perturbation of this problem is  the problem of completing a schedule given a set pre assigned meetings  The quasigroup domain has also been extensively used in the area of automated theorem proving   In this  community  the main interest in this domain has been driven by questions regarding the existence and nonex istence of quasigroups with additional mathematical properties  Fujita et al        Lam et al          exactly once  similarly  in each column  each element occurs exactly once  Denes and Keedwell        An incomplete  or  partial latin square     Computational Cost Profiles  P is a partially  filled N by N table such that no symbol occurs twice  We will now consider the computational cost of solv  in a row or a column   ing the completion problem for different search strate  The Quasigroup Completion  Problem is the problem of determining whether the  gies  As our basic search procedure  we use a complete  remaining entries of the table can be filled in such a  backtrack style search method   way that we obtain a complete latin square  that is  a  such procedures can vary dramatically depending on  full multiplication table of a quasigroup  We view the  the way one selects the next variable to branch on  the  pre assigned values of the latin square as a perturbation   variable selection strategy   and in what order the  The performance of        Gomes and Selman  crdu       order        order lJ  B order U      order lS         lOGO              f r action  of prEo asaigned eluents  Figure                The Complexity of Quasigroup Completion  oL          L   j             lS           JS     tl l Wbollr of backtra cks for first solution                              r       Log Scale       order      order  LJ      order H B order    IC               i                       o L         L            nuab lt r of ba ektra ek  fQt  fit    t soll ltiQt            Figure    Finding quasigroups of order     no pre                              faction of proe assiqned el    ents                    assigned values            Figure    Phase Transition for the Completion Prob  called the Brelaz heuristics  Brelaz  lem  laz heuristic was originally introduced for graph color  The Bre  ing procedures  It provides one of the most powerful possible values are assigned to a variable  the  value selection strategy    There is a large body of work in  graph coloring and general CSP heuristics  Trick and Johnson          both the CSP and OR communities exploring different  The Brelaz heuristic specifies a way for breaking ties in  search strategies   the First fail rule  If two variables have equally small  One of the most effective strategies is the so called First Fail heuristic    In the First Fail heuristic  the  next variable to branch on is the one with the small est remaining domain  i e   in choosing a value for the variable during the backtrack search  the search pro cedure has the fewest possible options left to explore   leading to the smallest branching factor   We con sider a popular extension of the First Fail heuristic   It s really a prerequisit for any reasonable bactrack In theorem proving and Boolean style search method  satisfiability  the rule corresponds to the powerful unit propagation heuristic   remaining domains  the Brelaz heuristic proposes to select the variable that shares constraints with the largest number of the remaining unassigned variables  A natural variation on this tie breaking rule is what we call the  reverse Berlaz  heuristic  in which preference is given to the variable that shares constraints with the smallest number of unassigned variables  Any re maining ties after the  reverse  Brelaz rule are resolved randomly  One final issue left to specify in our search procedure is the order in which the values are assigned to a variable  In the standard Brelaz  value assignment is done in lexicographical order   i e    systematic   In  our experiments  we consider four stragies         Algorithm Portfolio Design  Theory vs  Practice          o        o s                            yrr  brel tzs         brela z t rbrelus rbrel zr        EJ                              I             S   JS  to nwWer of be c hrac k s for Ur t solution              QO numbe r of ba ck tra  ks fQr  SO            first  I       solution        o J             number of cktuclcs for hrst sohotic n               L       Hl       U          nll JUlll r  lf bckuac ks for first solutiQ n  F igure    Find ing quasigroups of order    with      Fi gure    Finding quasigroups of order    with      pre assigned values   pre assigned values     Berlaz S    Berlaz with systematic value selec  tion   part of the p rofile      Berlaz R   Berlaz with random value selection     R Berlaz S    the overall profile  the bottom part gives the initial    Reverse Berlaz with systematic  First  we note that that R Brelaz R dominates R Brelaz S over the full profile  In other words  the cu mulative relative frequency curve for R Brelaz R lies  value selection  and  above that of R Brelaz S at every point along the x  R berlaz R  Reverse Brelaz with random value  As we will see below  we often encounter such pat  selecti on    terns  where one strategy simply consistently outper  axis  R Berlaz S  in turn  strictly dominates Brelaz R   forms strategies  Figure    shows the performance profile of our four  strategies for the problem of finding a quasigroup of order     no pre assigned values    Each curve gives  the cumulative distribution obtained for each strat  egy by solving the problem        times   The cost   horizontal axis  is measured in number of backtracks  which is directly proportional to the total runtime of our strategies  For exampl e  the figure shows that R Berlaz R  finished roughly     of the        runs in    b ackt racks or less  The top panel of the figure shows  Unfortunately  this leaves no room for combining strategies  one simply picks the best strategy   This may explain why some of the ideas  about combining algorithms has not received as much attention in the traditional communities that deal with hard computational problems   From the perspective of combining algorithms  what is most interesting  however  is that in the initial part of  There is still the issue of multiple runs with the same method  We ll retum to this below         Gomes and Selman  showing the inconsistency of a quasigroup comple tion problem  The instance in question has     pre assigned values  Here we again obeserve that Brelaz S  br elus         brelaz  r     rbrel Z S El rbroel    zr  M        o u  is somewhat better at finding inconsistencies quickly but again R Brelaz R dominates for most of the pro file  Again  the good initial performance of Brelaz S         can be exploited by combining many short runs  as we       will see below   o                          brel         r brelazr  t    Q g       t                      j                      Q l l numbe r of N   k tr     cks for tint olution                   g         b tli h n  brl laH rbrehc s rbrehzr                            O                          nuaber ot backtrae ks                                   t     n aber of bcktncks for first solution                      Figure    Finding quasigroups of order    at the phase transition                             the profile  see bottom panel  Figure     Brelaz S dom inates the R Brelaz R  Intuitively  Brelaz S is better than R Berlaz R at finding solutions quickly   How  ever  in the latter part of the cumulative distribution  ntmber of backt rac ks   for more than five backtracks   R Brelaz R dominates Brelaz S  In a sense  R Brelaz R gets relatively better when the search gets harder  As we will see in the next section  we can exploit this in our algorithm portfolio  Figure    Showing inconsistency of quasigroups com pletion  order    with     preassigned values    design  Figure    shows the performance profiles for quasi groups with     pre assigned values   We see essen     Portfolio Design  A  portfolio of algorithms is  tially the same pattern as in Figure    but the re gion where Brelaz S dominates is relatively smaller  When we increase the percentage of pre assigned val ues      pre assigned  Figure      we see that R Brelaz  a collection of different al  gorithms and or different copies of the same algorithm running on different processors   Here we consider the  R completely dominates the other strategies over the  case of independent runs without interprocess commu  whole problem spectrum  This pattern continues for  nication   the higher numbers of pre assigned values  Figure    at the phase transition with roughly     pre assigned   Finally  Figure   gives the performance profile for    ne can also consider the somewhat more general case of interleaving the execution of algorithms on one or more processors         Algorithm Portfolio Design  Theory vs  Practice Portfolio for   processors                               T                        bnlzs    l  Portfolio for   l     processors                                   rbu la r        brlazs      r       zr               O JS      lODO                     O J  O lS       iOO  O i btelazs     rbrel zr        L    L                                j         o  t   lns   o       rbnlaz r  ru    l     standard deviation  risk   o u  Figure    Portfolio for two processors combining Bre  Figure  laz and R Brelaz R   Brelaz and R Brelaz R  Portfolio for      l t   o u    tn dll rd d vition  processors   o s  o s  o s   o s s  o ss  lriskJ      Portfolio for twenty processors combining  within a set that is the best  both in terms of expected brltJ u   r br a z r  value and risk  This set of portfolios corresponds to the efficient set or efficient frontier  following terminology      used in the theory of mathematical finance   Within  this set  in order to minimize the risk  one has to dete riorate the expected value or  in order to improve the expected value of the portfolio  one has to increase the risk  In this context  where we characterize a portfolio in   l  btlaz    i  terms of its mean and variance  combining different  rbrdll zr  algorithms into a portfolio only makes sense if they exhibit different probability profiles and none of them    brel  zs  J rbr elatr I L             SO  lOti  l i D  JC D       standar d deviation        risk l  J SO        t iO  Figure    Portfolio for five processors combining Bre laz and R Brelaz R   dominates the others over the whole spectrum of prob lem instances   As noted earlier  algorithm  bution of algorithm  We are considering Las Vegas type algorithms  i e    A domi  nates algorithm B if the cumulative frequency distri  A lies above the cumulative fre  quency distribution of algorithm B for all points    stochastic algorithms that always return a model sat  Let us consider a set of two algorithms  algorithm    isfying the constraints of the search problem or demon  and algorithm    Let us associate a random variable  strate that no such model exists  Motwani and Ragha  with each algorithm  AI  the number of backtracks  van        The computational cost of the portfolio is  that algorithm   takes to find the first solution or to  therefore a random variable  The expected computa  prove that a solution does not exist  A   the number  tional cost of the portfolio is simply the expected value  of backtracks that algorithm   takes to find the first  of the random variable associated with the portfolio  solution or to prove that a solution does not exist   and its standard deviation is a measure of the  disper sion  of the computational cost obtained when using the portfolio of algorithms   In this sense  the stan  dard deviation is a measure of the risk inherent to the portfolio   Let us assume that we have  N processors and that we  design a portfolio using n  processors with algorithm   and n  processors with algorithm    So   N     nl    n   Let us define the random variable associted with this portfolio  X   the number of backtracks that the  The main motivation to combine different algorithms  portfolio takes to find the first solution or to prove that  into a portfolio is to improve on the performance of the  a solution does not exist   component algorithms  mainly in terms of expected computational cost but also in terms of the overall risk  As we will show  some portfolios are strictly preferrable to others  in the sense that they provide a lower risk and also a lower expected computational cost   How  ever  in some cases  we cannot identify any portfolio  The probability distribution of X is a  weighted  prob ability distribution of the probability distributions of algorithm   and algorithm      Another  More precisely  the  criterion for combining algorithms into a port  folio is given by the algorithm covariance         Gomes and Selman  probability that X   x is given by the probability that one processor takes exactly x backtracks and all the other ones take x or more backtracks to find a solution or to prove that a solution does not exist  Let us assume that we have N processors and our port folio consists of N copies of algorithm    In this case  P X x  is given by the probability that one proces sor take exactly x backtracks and the other N   take more than x backtracks  plus the probability that two processors take exactly x backtracks and the other     N    one takes more than x backtracks  etc   plus the probability that all the processors take exactly x back tracks to find a solution or to prove that a solution does not exist  The following expression gives the probabil ity function for such a portfolio   N and n   Given N processors  and let nl P X x  is given by          P Al N     x i P Al         x  N i   To consider two algorithms  we have to generalize the above expression  considering that X   x can occur just within the processors that use algorithm    or just within the processors that use algorithm   or within both  As a result  the probability function for a port folio with two algorithms  is given by the following expressiOn  Given N processors  n  such that      nl    N   and n  N   nl  P X x  is given by      P Al EE     N  nl         P A      xfP Al   x  nl i  x     xf P A    x  n  i  j  The value of i   is given by i     i i   and the term in the summation is   whenever i       or i     n      In the case of a portfolio involving two algorithms the probability distribution of the portfolio is a summation of a product of two expressions  each one correspond ing to one algorithm  In the case of a portfolio com prising M different algorithms  this probability func tion can be easily generalized  by having a summation of a product of M expressions  each corresponding to an algorithm  Once we derive the probability distribution for the ran dom variable associated with the portfolio  the calcu lation of the its expected valt e and standard deviation is straightforward        Empirical results for portfolio design  We now design different portfolios based on our perfor mance profiles from Section    We focus on the case of finding a quasigroup of order    with no preassigned values  The performance profiles are given in Figure    Note that this is an interesting case from the port folio design perspective because Brelaz S dominates in the initial part of the distribution  whereas R Brelaz R dominates in the latter part  Figures       and    give the expected values and the standard deviations of portfolios for       and    pro cessors  respectively   Results derived using the for mula given above   We see that for   processors  Fig ure     the portfolio consisting of two copies of the R Brelaz R has the best expected value and the low est standard deviation  This portfolio dominates the two other   processor portfolios  When we increase the number of processors  we ob serve an interesting shift in the optimal portfolio mix  For example  for   processors  using   Brelaz S gives a better expected value at only a slight increase in the risk  standard deviation  compared to zero Brelaz S  In this case  the efficient set comprises three portfo  lios  One with   R Brelaz R  one with   Brelaz S and   R Brelaz R  and one with   Brelaz S and   R Brelaz R  The situation changes even more dramatically if we go to yet more processors  In particular  with    processors  Figure      the best portfolio corresponds to using all processors to run the Brelaz S strategy  the lowest expected value and the lowest standard deviation   The intuitive explanantion for this is that by running many copies of Brelaz S  we have a good chance that at least one of them will find a solution quickly  This result is consistent with the common use of  random restarts  in stochastic search methods in practical applications  Our portfolio analysis also gives the somewhat counter intuitive result that  even when given two stochastic algorithms  where neither strictly dominates the other  running multiple copies of a single algorithm is preferrable to a mix of algo rithms  Figure   and Figure          Conclusions and Future Work  We have provided concrete empirical results showing the computational advantage of a portfolio approach for dealing with hard combinatorial search and rea soning problems as compared to the best more tra ditional single algorithm methods  Our analysis also showed what properties of the problem instance distri butions lead to the largest payoff for using a portfolio approach in practice  Finally  we saw how the use of random restarts of a good stochastic method is often   Algorithm Portfolio Design  Theory vs  Practice  the optimal strategy  These results suggest that ideas developed in the flexible computation community can play a significant role in practical algorithm design  Acknowledgments  We would like to thank Karen Alguire for developing an exciting tool for experimenting with the quasigroup completion problem  We also would like to thank Nort Fowler for many useful suggestions and discussions  and Neal Glassman for suggesting the domain of com binatorial design as a potential benchmark domain  The first author is a research associate with Rome Laboratory and is funded by the Air Force Office of Scientific Research  under the New World Vistas Ini tiative  F         C      and AFOSR NWV project       LIRL   RL   N      Horvitz  E  and Klein  A         Reasoning  metareason ing  and mathematical truth  studies of theorem prov ing under limited resources  Proc  of the Eleventh Conference on Uncertainty in Artificial Intelligence  UAI      August       Horvitz  E  and Z ilberstein S           Eds    Proceedings of Flexible Computation  AAAI Fall Symposium  Cam bridge  MA        Huberman  B A   Lukose  R M   and Hogg  T          An economics approach to hard computational problems  Science              Hogg  T   Huberman  B A   and W illiams   C P   Eds           Phase Transitions and Complexity  Artificial Intelligence      Spec  Issue        Kirkpatrick  S  and Selman  B         Critical Behavior in the Satisfiability of Random Boolean Expressions  Science       May                  Lam  C   Thiel  L   and Swiercz  S         Can  J  Math   Vol  XLI                       
 Survey propagation  SP  is an exciting new technique that has been remarkably successful at solving very large hard combinatorial problems  such as determining the satisfiability of Boolean formulas  In a promising attempt at understanding the success of SP  it was recently shown that SP can be viewed as a form of belief propagation  computing marginal probabilities over certain objects called covers of a formula  This explanation was  however  shortly dismissed by experiments suggesting that non trivial covers simply do not exist for large formulas  In this paper  we show that these experiments were misleading  not only do covers exist for large hard random formulas  SP is surprisingly accurate at computing marginals over these covers despite the existence of many cycles in the formulas  This re opens a potentially simpler line of reasoning for understanding SP  in contrast to some alternative lines of explanation that have been proposed assuming covers do not exist      INTRODUCTION  Survey Propagation  SP  is a new exciting algorithm for solving hard combinatorial problems  It was discovered by Mezard  Parisi  and Zecchina         and is so far the only known method successful at solving random Boolean satisfiability  SAT  problems with   million variables and beyond in near linear time in the hardest region  The SP method is quite radical in that it tries to approximate certain marginal probabilities related to the set of satisfying assignments  It then iteratively assigns values to variables with the most   Research supported by Intelligent Info  Systems Instt   IISI   Cornell Univ   AFOSR grant FA                 extreme probabilities  In effect  the algorithm behaves like the usual backtrack search methods for SAT  DPLL based   which also assign variable values incrementally in an attempt to find a satisfying assignment  However  quite surprisingly  SP almost never has to backtrack  In other words  the heuristic guidance from SP is almost always correct  Note that  interestingly  computing marginals on satisfying assignments is actually believed to be much harder than finding a single satisfying assignment   P complete vs  NPcomplete   Nonetheless  SP is able to efficiently approximate certain marginals and uses this information to successfully find a satisfying assignment  SP was derived from rather complex statistical physics methods  specifically  the so called cavity method developed for the study of spin glasses  Close connections to belief propagation  BP  methods were subsequently discovered  In particular  it was discovered by Braunstein and Zecchina         later extended by Maneva  Mossel  and Wainwright         that SP equations are equivalent to BP equations for obtaining marginals over a special class of combinatorial objects  called covers  Intuitively  a cover provides a representative generalization of a cluster of satisfying assignments  The discovery of a close connection between SP and BP via the use of covers laid an exciting foundation for explaining the success of SP  Unfortunately  subsequent experimental evidence suggested that hard random   SAT formulas have  with high probability  only one  trivial  cover  Maneva et al          This would leave all variables effectively in an undecided state  and would mean that marginals on covers cannot provide any useful information on how to set variables  Since SP clearly sets variables in a non trivial manner  it was conjectured that there must be another explanation for the good behavior of SP  in particular  one that is not based on the use of marginal probabilities of variables in the covers  In this paper  we revisit the claim that hard random  SAT formulas do not have interesting non trivial cov         KROC ET AL   ers  In fact  we show that such formulas have large numbers of non trivial covers  The main contribution of the paper is the first clear empirical evidence showing that in random   SAT problems near the satisfiability and hardness threshold      a significant number of non trivial covers exist      SP is remarkably good at computing variable marginals based on covers  and     these cover marginals closely relate to solution marginals at least in the extreme values  where it matters the most for survey inspired decimation  As a consequence  we strongly suspect that explaining SP in terms of covers may be the correct path after all  Note that     above is quite surprising for random   SAT formulas because such formulas have many loops  The known formal proof that SP computes cover marginals only applies to tree structured formulas  which in fact have only a single  trivial  cover  Further  its amazing that while SP computes such marginals in a fraction of a second  the next best methods of computing these marginals that we know of  via exact enumeration  or sampling followed by peeling  require over     CPU hours  Our experiments also indicate that cover marginals are more conservative than solution marginals in the sense that variables that are extreme with respect to cover marginals are almost certainly also extreme with respect to solution marginals  but not vice versa  This sheds light on why it is safe to set variables with extreme cover marginals in an iterative manner  as is done in the survey inspired decimation process for finding a solution using the marginals computed by SP  In addition to these empirical results  we also revisit the derivation of the SP equations themselves  with the goal of presenting the derivation in an insightful form purely within the realm of combinatorial constraint satisfaction problems  CSPs   We describe how one can reformulate in a natural step by step manner the problem of finding a satisfying assignment into one of finding a cover  by considering related factor graphs on larger state spaces  The BP equations for this reformulated problem are exactly the SP equations for the original problem  as shown in the Appendix      COVERS OF CNF FORMULAS  We start by introducing the notation and the basic concepts that we use throughout the paper  We are concerned with Boolean formulas in Conjunctive Normal Form or CNF  that is  formulas of the form F   l           l k             lm          lmkm    where each lik  called a literal   is a Boolean variable xj or its negation xj   Each conjunct of F   which itself is a disjunction of literals  is called a clause  In   CNF or   SAT formulas  every clause has   literals  Ran   dom   SAT formulas over n variables are generated by uniformly randomly choosing a pre specified number of clauses over these n variables  The Boolean satisfiability problem is the following  Given a CNF formula F over n variables  find a truth assignment  for the variables such that every clause in F evaluates to true   is called a satisfying assignment or a solution of F   We identify true with   and false with    A truth assignment to n variables can be viewed as a string of length n over the alphabet         and extending this alphabet to include a third letter  leads to a generalized assignment  A variable with the value  can be interpreted as being undecided  while variables with values   or   can be interpreted as being decided on what they want to be  We will be interested in certain generalized assignments called covers  Our formal definition of covers follows the one given by Achlioptas and Ricci Tersenghi         Let variable x be called a supported variable under a generalized assignment  if there is a clause C such that x is the only variable that satisfies C and all other literals of C are false  Otherwise  x is called unsupported  Definition    A generalized assignment           n is a cover of a CNF formula F iff    every clause of F has at least one satisfying literal or at least two literals with value  under   and     has no unsupported variables assigned   or    The first condition ensures that each clause of F is either already satisfied by  or has enough undecided variables to not cause any undecided variable to be forced to decide on a value  no unit propagation   The second condition says that each variable that is assigned   or   is set that way for a reason  there exists a clause that relies on this setting in order to be satisfied  For example  consider the formula F   x  y  z    x  y  z    x  y  z   F has exactly two covers      and     This can be verified by observing that whenever some variable is   or   then all non  variables are unsupported  Notice that the string of all s always satisfies the conditions in Definition    we refer to this string as the trivial cover  Covers were introduced by Maneva et al         as a useful concept to analyze the behavior of SP  but their combinatorial properties are much less known than those of solutions  A cover can be thought of as a partial assignment to variables  where the variables assigned  are considered unspecified  In this sense  each cover is a representative of a potentially large set of complete truth assignments  satisfying as well as not satisfying  This motivates further differentiation  Definition    A cover           n of F is a true cover iff there exists a satisfying assignment         n of F such that  and  agree on all values where   KROC ET AL   is not a   i e   i              n  i       i   i    Otherwise   is a false cover  A true cover thus generalizes at least one satisfying assignment  True covers are interesting to study when trying to satisfy a formula  because if there exists a true cover with variable x assigned   or    then there must also exist a satisfying assignment with the same setting of x  One can construct a true cover           n of F by starting with any satisfying assignment         n of F and generalizing it using a simple procedure called  propagation   The procedure starts by initially setting       It then repeatedly chooses an arbitrary variable unsupported under  and turns it into a   until there are no more unsupported variables  The resulting string  is a true cover  which can be verified as follows  The satisfying assignment  already satisfies the first condition in Definition    and  propagation does not destroy this property  In particular  a variable on which some clause relies is never turned into a   The second condition in Definition   is also clearly satisfied when  propagation halts  so that  must be a cover  Moreover  since  generalizes    it is a true cover  Note that  propagation can  in principle  be applied to an arbitrary generalized assignment  However  unless we start with one that satisfies the first condition in the cover definition   propagation may not lead to a cover  We end with a discussion of two insightful properties of covers  The first relates to self reducibility and the second to covers for tree structured formulas  No self reducibility  Consider the relation between the decision and search versions of the problem of finding a solution of a CNF formula F   In the decision version  one needs an algorithm that determines whether or not F has a solution  while in the search version  one needs an algorithm that explicitly finds a solution  The problem of finding a solution for F is selfreducible  i e   given an oracle for the decision version  one can efficiently solve the search version by iteratively fixing variables to   or    testing whether there is still a solution  and continuing in this way  Somewhat surprisingly  this strategy does not work for the problem of finding a cover  In other words  an oracle for the decision version of this problem does not immediately provide an efficient algorithm for finding a cover   The lack of self reducibility makes it very hard to find covers as we will see below   As a concrete example  consider the formula F described right after Definition    To construct a cover of F   we could ask   This was introduced under different names as the peeling procedure or coarsening  e g   by Maneva et al                whether there exists a cover with x set to    Since     is a cover  yet unknown to us   the decision oracle would say yes  We could then fix x to    simplify the formula to  y  z    y  z   and ask whether there is a cover with y set to    This residual formula indeed has    as a cover  and the oracle would say yes  With one more query  we will end up with     as the values of x  y  z  which is in fact not a cover of F   Tree structured formulas  For tree structured formulas without unit clauses  i e   formulas whose factor graph does not have a cycle  the only cover is the trivial all  cover  We argue this using the connection between covers and SP shown by Braunstein and Zecchina         which says that when generalized assignments have a uniform prior  SP on a tree formula F provably computes probability marginals of variables being       and  in covers of F   Moreover  it can be verified from the iterative equations for SP that with no unit clauses  zero marginals for any variable being   or    and full marginals for any variable being a  is a fixed point of SP  Since SP provably has exactly one fixed point on tree formulas  it follows that the only cover of such formulas is the trivial all  cover      PROBLEM REFORMULATION  FROM SOLUTIONS TO COVERS  We now show that the concept of covers can be quite naturally arrived at when trying to find solutions of a CNF formula  thus motivating the study of covers from a purely generative perspective  Starting with a CNF formula F   we describe how F is transformed step by step into the problem of finding covers of F   motivating each step  Although our discussion applies to any CNF formula F   we will be using the following example formula with   variables and   clauses to illustrate the steps   x  y  z    x  y    y  z    x  z     z      z      z      z   a  b  c  d  Let N denote the number of variables  M the number of clauses  and L the number of literals of F   Original problem  The problem is to find an asN signment in the space        that satisfies F   The factor graph for F has N variable nodes and M function nodes  corresponding directly to the variables x  y        and clauses a  b        in F  see e g  Kschischang et al           The factor graph for the example formula is depicted below  Here factors Fa   Fb         represent predicates ensuring that the corresponding clause has at least one satisfying literal         KROC ET AL  x  Fa  y  z  Fb  The solutions to this modified problem do not necessarily correspond directly to solutions of the original one  In particular  if there are no unit clauses and all variables are set to   the problem is already solved without providing any useful information   Fc  Fd  Variable occurrences  The first step in the transformation is to start treating every variable occurrence xa   xb   ya   yb         in F as a separate unit that can be either   or    This allows for more flexibility in the process of finding a solution  since a variable can decide what value to assume in each clause separately  Of course  we need to add constraints to ensure that the occurrence values are eventually consistent  for every variable x in F   we add a constraint Fx that all occurrences of x have the same value  Now the search space is       L  and the corresponding factor graph contains L variable nodes and M   N function nodes  the original clause factors Fa   Fb         and the new constraints Fx   Fy           xa  Fx  xb  Fa  xd  ya  Fb  yb  Fy  yc  za  Fc  zc  Fd  zd  Fz  At this point  we have not relaxed solutions to the original problem F   solutions to the modified problem correspond precisely to the original solutions  because variable occurrences are forced to be consistent  However  we moved this consistency check from the syntactic level  variables could not be inconsistent simply by the problem definition  to the semantic level  we have special constraints to guarantee consistency   Relaxing assignments  The next step is to relax the problem by allowing variable nodes to assume the special value   The semantics of  is undecided  meaning that the variable node is set neither to   L nor to    The new search space is            and we must specify how our constraints handle the value   Variable constraints Fx         have the same meaning as before  namely  all variable nodes xa   xb         have the same value for every variable x  Clause constraints Fa         now have a modified meaning  a clause is satisfied if it contains at least one satisfying literal or at least two literals with the value   The motivation here is to either satisfy a clause or leave enough freedom in the form of at least two undecided variables   A single undecided variable would be forced to take on a particular value if all other literals in the clause were falsified   With this transformation  the factor graph remains structurally the same  while the set of possible values for variable nodes changes   Reducing freedom of choice  To distinguish variables that could assume the value  from those that truly need to be fixed to either   or    we require that every non  variable has a clause that needs the variable to be   or   in order to be satisfied  The search space does not change  but we need to add constraints to implement the reduction in the freedom of choice  Notice that this requirement is equivalent to no unsupported variables in the definition of a cover  and that the first requirement in that definition is fulfilled by the clause constraints  Therefore  we are now searching for covers of F   A natural way to represent the no unsupported variable constraint in the factor graph is to add for each variable x a new function node Fx    connected to the variable nodes for x as well as for all other variables sharing a clause with x  This  of course  creates many new links and introduces additional short cycles  even if the original factor graph was acyclic  The following transformation step alleviates this issue  Reinterpreting variable nodes  As the final step  we change the semantics of the variable nodes values and of the constraints so that the no unsupported variable condition can be enforced without additional function nodes  The reasoning is that the simple          domain creates a bottleneck for how much information can be communicated between nodes in the factor graph  By altering the semantics of the variable nodes values  we can improve on this  The new value of a variable node xa will be a pair  rax   wxa                              so that the size of the search space is still  L   We interpret the value rax as a request from clause a to variable x with the meaning that a relies on x to satisfy it  and the value wxa as a warning from variable x to clause a that x is set such that it does not satisfy a  The values   and   indicate presence and absence  resp   of the request or warning  We can recover the original          values from these new values as follows  if rax     for some a  then x is set to satisfy clause a  if there is no request from any clause where x appears  then x is undecided  a value of  in the previous interpretation   The variable constraints Fx         not only ensure consistency of the values of xa   xb         as before  but also ensure the second cover condition as described below  The clause constraints Fa         remain unchanged  The variable constraint Fx is a predicate ensuring that   KROC ET AL  the following two conditions are met     if rax     for any clause a where x appears  then wxb     for all clauses b where x appears with the same sign as in a  and wxb     for all b where x appears with the opposite sign  Since x must be set to satisfy a  this ensures that clauses that are unsatisfied by x do receive a warning     if rax     for all clauses a where x appears  then wxa     for all of them  i e   no clause receives a warning from x  To evaluate Fx   values  rax   wxa   are needed only for clauses a in which x appears  which is exactly the set of variable nodes the factor Fx is connected to  Notice that the case  rax   wxa            cannot happen due to condition   above  The conditions also imply that the variable occurrences of x are consistent  and in particular that two clauses where x appears with opposite signs  say a and b  cannot simultaneously request to be satisfied by x  This is because either rax     or rbx     must hold due to condition    The clause constraint Fa is a predicate stating that clause a issues a request to its variable x if and only if it receives warnings from all its other variables  rax     iff wya     for all variables y    x in a  Again  Fa can be evaluated using exactly values from the variable nodes it is connected to  When clause a issues a request to variable x  i e   rax       x must be set to satisfy a  thus providing a satisfying literal for a  If a does not issue any request  then according to the condition of Fa   at least two of as variables  say x and y  must not have sent a warning  In this case  Fx and Fy state that each of x and y is either undecided or satisfies a  Thus the first condition in the cover definition holds in any solution of this new constraint satisfaction problem  The second condition also holds  because every variable x that is not undecided must have received a request from some clause a  so that x is the only literal in a that is not false  Therefore x is supported  Let us denote this final constraint satisfaction problem by P  F     It is a function of the original formula F    Notice that the factor graph of P  F   has the same topology as the factor graph of F   In particular  if F has a tree factor graph  so does P  F    Further  by the construction of P  F   described above  its solutions correspond precisely to the covers of F        INFERENCE OVER COVERS  This section discusses an approach for solving the problem P  F   with probabilistic inference using belief propagation  BP   It arrives at the survey propagation equations for F by applying BP equations to P  F          Since the factor graph of P  F   can be easily viewed as a Bayesian Network  cf  Pearl         one can compute marginal probabilities over the set of satisfying assignments of the problem  defined as Pr xa   v   all constraints of P  F   are satisfied  for each variable node xa and v                            The probability space here is over all assignments to variable nodes with uniform prior  Once these solution marginals are known  we know which variables are most likely to assume a particular value  and setting these variables simplifies the problem  A new set of marginals can be computed on this simplified formula  and the whole process repeated  This method of searching for a satisfying assignment is called the decimation procedure  The problem  of course  is to compute the marginals  which  in general  is much harder than finding a satisfying assignment   One possibility for computing marginals is to use the belief propagation algorithm  cf  Pearl         Although provably correct essentially only for formulas with a tree factor graph  BP provides a good approximation of the true marginals in many problem domains in practice  Murphy et al          Moreover  as shown by Maneva et al          applying the BP algorithm to the problem of searching for covers of F results in the SP algorithm  Thus  on formulas with a tree factor graph  the SP algorithm provably computes marginal probabilities over covers of F   which are equivalent to marginals over satisfying assignments of P  F    When the formula contains loops  SP computes a loopy approximation to the cover marginals  Specific details of the derivation of SP equations from the problem P  F   are deferred to the Appendix      EXPERIMENTAL RESULTS  This section presents our main contributions  We begin by demonstrating that non trivial covers do exist in large numbers in random   SAT formula  and then explore connections between SP  BP  and variable marginals computed from covers as well as solutions  showing in particular that SP approximates cover marginals surprisingly well       EXISTENCE OF COVERS  Motivated by theoretical results connecting SP to covers of formulas  Maneva et al         suggested an experimental study to test whether non trivial covers even exist in random   SAT formulas  They proposed a seemingly good way to do this  the peeling experiment   namely  start with a uniformly random satisfying assignment of a formula F and  while it has unsupported variables   propagate the assignment  When                            Solutions leading to the trivial cover Solutions leading to nontrivial covers     Number of unsupported variables                       Number of stars              Figure    The peeling experiment  showing the evolution of the number of stars as  propagation is performed  To understand this issue better  we ran the same peeling experiment on a      variable random   SAT formula at clause to variable ratio      which is close to the hardness threshold for random   SAT problems   but used SampleSat  Wei et al         to obtain samples  which is expected to produce fairly uniform samples  Figure   shows the evolution of the number of unsupported variables at each stage as  propagation is performed starting from a solution  Here  the x axis shows the number of stars  which monotonically increases by  propagation  The y axis shows the number of unsupported variables present at each stage  As one moves from left to right following the  propagation process  one hits a cover if the number of unsupported variables drops to zero  so that  propagation terminates   The two curves in the plot correspond to solutions that  propagated to the trivial cover and those that did not  In our experiment  out of     satisfying assignments used  nearly     led to the trivial cover  their average is represented by the top curve  The remaining     of the sampled solutions actually led to non trivial covers  their average is represented by the bottom curve  Thus  when solutions are sampled near uniformly  a substantial fraction of them lead to non trivial covers     That this was not observed by Maneva et al         can be attributed to the fact that SP was used to find satisfying assignments  Mossel         resulting in highly non uniform samples                      vars    vars    vars                          P nontrivial cover      vars    vars    vars       the process terminates  one obtains a  true  cover of F   Unfortunately  what they observed is that this process repeatedly hits the trivial all  cover  from which they concluded that non trivial covers most likely do not exist for such formulas  However  it is known that near uniformly sampling solutions of such formulas to start with is a hard problem in itself and that most sampling methods obtain solutions in a highly nonuniform manner  Wei et al          Consequently  one must be careful in drawing conclusions from relatively few and possibly biased samples   Number of nontrivial covers  KROC ET AL                                      Clausetovariable ratio                                Clausetovariable ratio       Figure    Non trivial covers in random formulas  Left  existence probability  Right  average number  An alternative method of finding covers is to create a new Boolean formula G whose solutions correspond go the covers of F   It turned out to be extremely hard to solve G to find any non trivial cover using state ofthe art SAT solvers for number of variables as low as      So we confined our experiments to small formulas  with        and    variables  We found all covers for such formulas with varying clause to variable ratios   The results are shown in Figure    where each data point corresponds to statistics obtained from     formulas  The left pane shows the probability that a random formula  for a given clause to variable ratio  has at least one non trivial cover  either true or false   The figure shows a nice phase transition where covers appear  at around         which is surprisingly sharp given the small formula sizes  Also  the region where covers surely exist is widening on both sides as the number of variables increases  supporting the claim that non trivial covers exist even in large formulas  The right pane of Figure   shows the actual number of non trivial covers  with a clear trend that the number increases with the size of the formula  for all values of the clause to variable ratio  It is worth noting that the number of covers is very small compared to the number of satisfying assignments  e g  for    variables and         the expected number of satisfying assignments is           while there are only   covers on average  Somewhat surprisingly  the number of false covers is almost negligible  around   at the peak  and does not seem to be growing nearly as fast as the total number of covers  This might explain why SP  although approximating marginals over all covers  is successful in finding satisfying assignments  We also consider how the number of solutions that lead to non trivial covers changes for larger formulas  as the number of variables N increases from     to       The left pane of Figure   shows that this number  in fact  grows exponentially with N   The number is computed by averaging over        sampled solutions from     formulas at ratio     for each N   estimating the fraction p N   of these that lead to a non trivial cover  and scaling it up by the expected number of solutions    KROC ET AL                    P non trivial cover              e      e     e     E  sols w  nontr  cover   log scale        which is               N        N    The number of solutions for such formulas at ratio     is known to be highly concentrated around its expectation   The resulting number  p N          N   is plotted on the yaxis of the left pane  with N on the x axis  The right pane of Figure   shows the data used to estimate the fraction p N   along with its fit on the y axis  with N on the x axis again                         Number of Vars   log scale                     Number of Variables        Figure    Left  Expected number of solutions leading to non trivial covers  log log scale   Right  Probability of a solution leading to a non trivial cover  Notice that the left pane is in log scale for both axes  and clearly increases faster than a linear function  This shows that the expected number of solutions that lead to non trivial covers grows super polynomially  In fact  performing a best fit for this curve suggests that this number grows exponentially  roughly as       N   This number is indeed a vanishingly small fraction of the expected number of solutions        N   as observed by Maneva et al          but nonetheless exponentially increasing  The existence of covers for random   SAT also aligns with what Achlioptas and Ricci Tersenghi        recently proved for k SAT with k          SP  BP  AND MARGINALS  We now study the behavior of SP and BP on a random formula in relation to solutions and covers of that formula  While theoretical work has shown that SP  viewed as BP on a related combinatorial problem  provably computes cover marginals on tree structured formulas  we demonstrate that even on random   SAT instances  which are far from tree like  SP approximates cover marginals surprisingly well  We also show that cover marginals  especially in the extreme range  are closely related to solution marginals in an intriguing conservative fashion  The combination of these two effects  we believe  plays a crucial role in the success of SP  Our experiments also reveal that BP performs poorly at computing any marginals of interest  Given marginal probabilities  we define the magnetization of a variable to be the difference between the marginals of the variable being positive and it being       negative  For the rest of our experiments  we start with a random   SAT formula F with      variables and       clauses  clause to variable ratio of       and plot the magnetization of the variables of F in the range           The marginals for magnetization are obtained from four different sources  which are compared and contrasted against each other      by running SP on F till the iterations converge      by running BP on F but terminating it after        iterations because the equations do not converge      by sampling solutions of F using SampleSat and computing an estimate of the positive and negative marginals from the sampled solutions  the solution marginals   and     by sampling solutions of F using SampleSat   propagating them to covers  and computing an estimate of the positive and negative marginals from these covers  the cover marginals   Note that in      we are sampling true covers and obtaining an estimate  An alternative approach is to use SP itself on F to try to sample covers of F   but the issue here is that the problem of finding  non trivial  covers is not self reducible to the decision problem of whether covers exist  as shown in Section    Therefore  it is not clear whether SP can be used to actually find a cover  despite it approximating the cover marginals very well  Recall that the SP based decimation process works by identifying variables with extreme magnetization  fixing them  and iterating  We will therefore be interested mostly in what happens in the extreme magnetization regions in these plots  namely  the lower left corner        and the upper right corner           In the left pane of Figure   we plot the magnetization computed by SP on the x axis and the magnetization obtained from cover marginals on the y axis  The scatter plot has exactly      data points  with one point for each variable of the formula F   If the magnetizations on the two axes matched perfectly  all points would fall on a single diagonal line from the bottomleft corner to the top right corner  The plot shows that SP is highly accurate at computing cover marginals  especially in the extreme regions at the bottom left and top right  The middle pane of Figure   compares the magnetization based on cover marginals with the magnetization based on solutions marginals  This will provide an intuition for why it might be better to follow cover marginals rather than solution marginals when looking for a satisfying assignment   We see an interesting s   For clarity  the plots show magnetizations for one such formula  although the trend is generic    Of course  if solution marginals could be computed perfectly  this would not be an issue  In practice  however  the best we can hope is to approximately estimate marginals                   SP Magnetization                     Solution Magnetization                         Solution Magnetization                         Cover Magnetization            KROC ET AL                                 Cover Magnetization                                BP Magnetization  Figure    Magnetization plots  Left  SP vs  covers  Middle  covers vs  solutions  Right  BP vs  solutions  shape in this plot  which can be interpreted as follows  fixing variables with extreme cover magnetizations is more conservative compared to fixing variables with extreme solution magnetizations  Which means that variables that are extreme w r t  cover based magnetization are also extreme w r t  solution based magnetization  but not necessarily vice versa   Recall that the extreme region is exactly where decimation based algorithms  that often fix a small set of extreme variables per iteration  need to be correct  Thus  etimates of cover marginals provide a safer heuristic for fixing variables than estimates of solution marginals  As a comparison with BP  the right pane of Figure   shows BP magnetization vs  magnetization based on solution marginals for the same      variable        clause formula  Since BP almost never converges on such formulas  we terminated BP after        iterations  SP took roughly    iterations to converge  and used the partially converged marginals obtained so far for computing magnetization  The plot shows that BP provides very poor estimates for the magnetizations based on solution marginals   The points are equally scattered when BP magnetization is plotted against cover magnetization   In fact  BP appears to identify as extreme many variables that have the opposite solution magnetization  Thus  when magnetization obtained from BP is used as a heuristic for identifying variables to fix  mistakes are often made that eventually lead to a contradiction  i e  unsatisfiable reduced formula      DISCUSSION  A comparison between left and right panes of Figure   suggests that approximating statistics over covers  as done by SP  is much more accurate than approximating statistics over solutions  as done by BP   This appears to be because covers are much more coarse grained than solutions  indeed  even an exponentially large cluster of solutions will have only a single cover  as its representative  This cover still captures critical properties of the cluster necessary for finding solutions  such as backbone variables  which is what SP appears to exploit  We also saw that the extreme magnetization based on cover marginals is more conservative than that based on solution marginals  as seen in the s shape of the plot in the middle pane of Figure     This suggests that while SP  based on approximating cover marginals  may miss some variables with extreme magnetization  when it does find a variable to have extreme magnetization  it is quite likely to be correct  This provides an intuitive explanation of why the decimation process based on extreme SP magnetization succeeds with high probability on random   SAT problems without having to backtrack  while the decimation process based on BP magnetizations more often fails to find a satisfying assignment in practice  We also note that BP and SP have been proven to compute exact marginals on solutions and covers  respectively  only for tree structured formulas  with some simple exceptional cases like formulas with a single cycle   For BP  solution marginals on tree formulas are already non trivial  and it is reasonable to expect it to compute a fair approximation of marginals on loopy networks  formulas   However  for SP  cover marginals on tree formulas are trivial  the only cover here is the all  cover  Cover marginals become interesting only when one goes to loopy formulas  such as random  SAT  In this case  as seen in our experiments  it is remarkable that the SP computes a good approximation of non trivial cover marginals for non tree formulas  We hope that our results have convincingly demonstrated that the study of the covers of formulas is very fruitful and may well lead to a correct explanation of the success of SP    KROC ET AL   
