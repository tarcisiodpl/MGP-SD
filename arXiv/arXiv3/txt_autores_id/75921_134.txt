  eas include distributed robot control  networking and e commerce   We present a memory bounded optimization approach for solving infinite horizon decentralized POMDPs  Policies for each agent are represented by stochastic finite state controllers  We formulate the problem of optimizing these policies as a nonlinear program  leveraging powerful existing nonlinear optimization techniques for solving the problem  While existing solvers only guarantee locally optimal solutions  we show that our formulation produces higher quality controllers than the state of the art approach  We also incorporate a shared source of randomness in the form of a correlation device to further increase solution quality with only a limited increase in space and time  Our experimental results show that nonlinear optimization can be used to provide high quality  concise solutions to decentralized decision problems under uncertainty   Although there has been some recent work on exact and approximate algorithms for DEC POMDPs  Nair et al         Bernstein et al         Hansen et al         Szer et al         Szer and Charpillet        Seuken and Zilberstein         only two algorithms  Bernstein et al         Szer and Charpillet        are able to find solutions for the infinite horizon case  Such domains as networking and robot control problems  where the agents are in continuous use are more appropriately modeled as infinite horizon problems  Exact algorithms require an intractable amount of space for all but the smallest problems  This may occur even if an optimal or near optimal solution is concise  DECPOMDP approximation algorithms can operate with a limited amount of memory  but as a consequence may provide poor results   Introduction  Markov decision processes  MDPs  have been widely used to study single agent sequential decision making with full observability  Partially observable Markov decision processes  POMDPs  have had success modeling the more general situation in which the agent has only partial information about the state of the system  The decentralized partially observable Markov decision processes  DEC POMDP  is an even more general framework which extends the POMDP model to mutiagent settings  In a DEC POMDP each agent must make decisions based on uncertainty about the other agents as well as imperfect information of the system state  The agents seek to maximize a shared total reward using solely local information in order to act  Some examples of DEC POMDP application ar   In this paper  we propose a new approach that addresses the space requirement of DEC POMDP algorithms while maintaining a principled method based on the optimal solution  This approach formulates the optimal memory bounded solution for the DECPOMDP as a nonlinear program  NLP   thus allowing a wide range of powerful nonlinear optimization techniques to be applied  This is done by optimizing the parameters of fixed size independent controllers for each agent  which when combined  produce the policy for the DEC POMDP  While no existing NLP solver guarantees finding an optimal solution  our new formulation facilitates a more efficient search of the solution space and produces high quality controllers of a given size  We also discuss the benefits of adding a shared source of randomness to increase the solution quality of our memory bounded approach  This allows a set of independent controllers to be correlated in order to produce higher values  without sharing any local information  Correlation adds another mechanism in efforts to gain the most possible value with a fixed amount of      AMATO ET AL   space  This has been shown to be useful in order to increase value of fixed size controllers  Bernstein et al         and we show that is also useful when combined with our NLP approach  The rest of the paper is organized as follows  We first present some background on the DEC POMDP model and the finite state controller representation of their solution  We then describe the current infinite horizon algorithms and describe some of their flaws  As an alternative  we present a nonlinear program that represents the optimal fixed size solution  We also incorporate correlation into the NLP method and discuss its benefits  Lastly  experimental results are provided comparing the nonlinear optimization methods with and without correlation and the current state of theart DEC POMDP approximation algorithm  This is done by using an off the shelf  locally optimal nonlinear optimization method to solve the NLPs  but more sophisticated methods are also possible  For a range of domains and controller sizes  higher valued controllers are found with the NLP and using correlation further increases solution quality  This suggests that high quality  concise controllers can be found in many diverse DEC POMDP domains      DEC POMDP model and solutions  We first review the decentralized partially observable Markov decision process  DEC POMDP  model  For clarity  we present the model for two agents as it is straightforward to extend it to n agents  A two agent DEC POMDP can be defined with the tuple  M   hS  A    A    P  R          O  T i  S  the finite set of states  A  and A    the finite sets of actions for each agent  P   the set of state transition probabilities  P  s   s  a    a     the probability of transitioning from state s to s  when actions a  and a  are taken by agents   and   respectively  R  the reward function  R s  a    a     the immediate reward for being in state s and agent   taking action a  and agent   taking action a     and     the finite sets of observations for each agent  O  the set of observation probabilities  O o    o   s    a    a     the probability of agents   and   seeing observations o  and o  respectively given agent   has taken action a  and agent   has taken action a  and this results in state s  Since we are considering the infinite horizon DECPOMDP  the decision making process unfolds over an infinite sequence of stages  At each step  every agent  chooses an action based on their local observation histories  resulting in an immediate reward and an observation for each agent  Note that because the state is not directly observed  it may be beneficial for the agent to remember the observation history  A local policy for an agent is a mapping from local observation histories to actions while a joint policy is a set of policies  one for each agent in the problem  The goal is to maximize the infinite horizon total cumulative reward  beginning at some initial distribution over states called a belief state  In order to maintain a finite sum over the infinite horizon  we employ a discount factor           As a way to model DEC POMDP policies with finite memory  finite state controllers provide an appealing solution  Each agents policy can be represented as a local controller and the resulting set of controllers supply the joint policy  called the joint controller  Each finite state controller can formally be defined by the tuple hQ    i  where Q is the finite set of controller nodes     Q  A is the action selection model for each node  and    Q  A  O  Q represents the node transition model for each node given an action was taken and an observation seen  For n agents  the value for starting in agent  s nodes  q and at state s is given by   n XY X P  s    a  s  V   q  s    P  ai  qi   R s   a       i s   a   n X XY         O  o s    a  P  qi  qi   ai   oi  V  q   s     o  q    i  This is also referred to as the Bellman equation  Note that the values can be calculated offline in order to determine controllers for each agent that can then be executed online for distributed control      Previous work  As mentioned above  the only other algorithms that we know of that can solve infinite horizon DECPOMDPs are those of Bernstein et al         and Szer and Charpillet         Bernstein et al s approach  called bounded policy iteration for decentralized POMDPs  DEC BPI   is an approximate algorithm that also uses stochastic finite state controllers  Szer and Charpillets approach is also an approximate algorithm  but it uses deterministic controllers  DEC BPI improves a set of fixed size controllers by using linear programming  This is done by iterating through the nodes of each agents controller and attempting to find an improvement  A linear program searches for a probability distribution over actions and transitions into the agents current controller that increases the value of the controller for any initial state   AMATO ET AL  For variables  x  q   a   y  q   a   o  q     and z  q  s  Maximize X     b   s z q     s   s  Given the Bellman constraints   q  s z  q  s     X      X X X               y  q   a   o  q  z q   s   O  o s    a  P  s  s   a  x  q   a  R s   a     s     a    o  q    For each agent i and set of agents  i  apart from i  Independence constraints  ai    q  X  X  x  q   a     ai   a   q   o  qi   X  y  q   a   o  q          qi  f    a  x qi   qi  ai  X  f y qi   qi   ai   afi   oi   ofi   q        qi  And probability constraints  X X f f qi x qi   qi    a      and qi   oi   ai y qi   qi   ai   aci   oi   ofi   q           a  q     q   a x  q   a     and  q   o   a y  q   a   o  q        Table    The nonlinear program representing the optimal fixed size controller  Variable x  q   a  represents P   a  q   variable y  q   a   o  q     represents P  q     q   a   o   variable z  q  s  represents V   q  s   q   represents the initial controller f node for each agent  Superscripted f s such as qi represent arbitrary fixed values  and any initial node of the other agents controllers  If the improvement is discovered  the node is updated based on the probability distributions found  Each node for each agent is examined in turn and the algorithm terminates when no agent can make any further improvements  This algorithm allows memory to remain fixed  but provides only a locally optimal solution  This is due to the linear program considering the old controller values from the second step on and the fact that improvement must be over all possible states and initial nodes for the controllers of the other agents  As the number of agents or size of controllers grows  this later drawback is likely to severely hinder improvement  Szer and Charpillet have developed a best first search algorithm that finds deterministic finite state controllers of a fixed size  This is done by calculating a heuristic for the controller given the known deterministic parameters and filling in the remaining parameters one at a time in a best first fashion  They prove that this technique will find the optimal deterministic finite state controller of a given size  but its use remains limited  This approach is very time and memory intensive and is restricted to deterministic controllers      Nonlinear optimization approach  Due to the high space complexity of finding an optimal solution for a DEC POMDP  fixed size solutions are very appealing  Fixing memory balances optimality and computational concerns and should allow high quality solutions to be found for many problems  Using Bernstein et al s DEC BPI method reduces problem complexity by fixing controller size  but solution quality is limited by a linear program that requires improvement across all states and initial nodes of the other agents  Also  each agents controller is improved separately without consideration for the knowledge of the initial problem state  thus reducing solution quality  Both of these limitations can be eliminated by modeling a set of optimal controllers as a nonlinear program  By setting the value as a variable and using constraints to maintain validity  the parameters can be updated in order to represent the globally optimal solution over the infinite horizon of the problem  Rather than the the iterative process of DEC BPI  the NLP improves and evaluates the controllers of all agents at once for a given initial state in order to make the best possible use of the controller size  Compared with other DEC POMDP algorithms  the NLP approach makes more efficient use of memory      AMATO ET AL   than the exact methods  and using locally optimal NLP algorithms provides an approximation technique with a search based on the optimal solution of the problem  Rather than adding nodes and then attempting to remove those that will not improve the controller  as a dynamic programming approach might do  we search for the best controllers of a fixed size  The NLP is also able to take advantage of the start distribution  thus making better use of its size  The NLP approach has already shown promise in the POMDP case  In a previous paper  Amato et al          we have modeled the optimal fixed size controller for a given POMDP as an NLP and with locally optimal solution techniques produced consistently higher quality controllers than a current stateof the art method  The success of the NLP in the single agent case suggested that an extension to DECPOMDPs could also be successful  To construct this NLP  extra constraints are needed to guarantee independent controllers for each agent  while still maximizing the value       Nonlinear problem model  The nonlinear program seeks to optimize the value of fixed size controllers given a initial state distribution and the DEC POMDP model  The parameters of this problem in vector notation are the joint action selection probabilities at each node of the controllers P   a  q   the joint node transition probabilities P  q     q   a   o  and the values of each node in each state  V   q  s   This approach differs from Bernstein et al s approach in that it explicitly represents the node values as variables  To ensure that the values are correct given the action and node transition probabilities  nonlinear constraints must be added to the optimization  These constraints are the Bellman equations given the policy determined by the action and transition probabilities  Constraints are also added to ensure distributed action selection and node transitions for each agent  We must also ensure that all probabilities are valid numbers between   and    Table   describes the nonlinear program that defines the optimal controller for an arbitrary number of agents  The value of designated initial local nodes is maximized given the initial state distribution and the necessary constraints  The independence constraints guarantee that action selection and transition probabilities can be summed out for each agent by ensuring that they do not depend on any information that is not local  Theorem   An optimal solution of the NLP results in optimal stochastic controllers for the given size and initial state distribution   Proof sketch  The optimality of the controllers follows from the NLP constraints and maximization of given initial nodes at the initial state distribution  The Bellman equation constraints restrict the value variables to valid amounts based on the chosen probabilities  the independence constraints guarantee distributed control and the maximum value is found for the initial nodes and state  Hence  this represents optimal controllers       Nonlinear solution techniques  There are many efficient algorithms for solving large NLPs  When the problem is non convex  as in our case  there are multiple local maxima and no NLP solver guarantees finding the optimal solution  Nevertheless  existing techniques proved useful in finding high quality results for large problems  For this paper  we used a freely available nonlinearly constrained optimization solver called filter  Fletcher et al         on the NEOS server  http   wwwneos mcs anl gov neos    Filter finds solutions by a method of successive approximations called sequential quadratic programming  SQP   SQP uses quadratic approximations which are then more efficiently solved with quadratic programming  QP  until a solution to the more general problem is found  A QP is typically easier to solve  but must have a quadratic objective function and linear constraints  Filter adds a filter which tests the current objective and constraint violations against those of previous steps in order to promote convergence and avoid certain locally optimal solutions  The DEC POMDP and nonlinear optimization models were described using a standard optimization language AMPL      Incorporating correlation  Bernstein et al  also allow each agents controller to be correlated by using a shared source of randomness in the form of a correlation device  As an example of one such device  imagine that before each action is taken  a coin is flipped and both agents have access to the outcome  Each agent can then use that new information to affect their choice of action  Along with stochasticity  correlation is another means of increasing value when memory is limited  A correlation device provides extra signals to the agents and operates independently of the DECPOMDP  That is  the correlation device is a tuple hC  i  where C is a set of states and    C  C is a stochastic transition function that we will represent as P  c   c   At each step of the problem  the device transitions and each agent can observe its state    AMATO ET AL      For variables  w c  c     x  q   a  c   y  q   a   o  q     c  and z  q  s  c  Maximize X b   s z q     s  s  Given the Bellman constraints      X X X X X                 w c  c  z q   s   c  y  q   a   o  q   c  O  o s    a  P  s  s   a  x  q   a  c  R s   a      q  s z  q  s  c      a  s     o  q    c   Table    The nonlinear program representing the optimal fixed size controller including a correlation device  Variable x  q   a  c  represents P   a  q  c   variable y  q   a   o  q     c  represents P  q     q   a   o  c   variable z  q  s  c  represents V   q  s  c   q   represents the initial controller node for each agent and w c  c    represents P  c   c   The other constraints are similar to those above with the addition of a sum to one constraint for the correlation device  size DEC BPI DEC BPI corr NLO NLO corr The independent local controllers defined above can be modified to make use of the correlation device  This                       is done by making the parameters dependent on the                       signal from the correlation device  For agent i  ac                      tion selection is then P  ai  qi   c  and node transition is                       P  qi   qi   ai   oi   c   For n agents  the value of the correTable    Broadcast problem values using NLP methlated joint controller beginning in nodes  q  state s and ods and DEC BPI with and without a   node correlacorrelation device state c is defined as V   q  s  c      n tion device X X XY P  s    a  s  O  o s     a  P  ai  qi   c  R s   a     i s    o   a size DEC BPI DEC BPI corr NLO NLO corr   n XY X        s    s  s  s P  qi  qi   ai   oi   c  P  c  c V  q     s    c           s  s  s  s i c   q     s  s    s     s Our NLP can be extended to include a correlation    s   s     s      s device  This optimization problem  the first part of which is shown in Table    is very similar to the preTable    Broadcast problem mean optimization times vious NLP  A new variable is added for the transiusing NLP methods and DEC BPI with and without tion function of the correlation device and the other a   node correlation device variables now include the signal from the device  The Bellman equation incorporates the new correlation device signal at each step  but the other constraints reEach NLP and DEC BPI algorithm was run until conmain the same  A new probability constraint is also vergence was achieved with ten different random deadded to ensure that the transition probabilities for terministic initial controllers  and the mean values and each state of the correlation device sum to one  times are reported  The times reported for each NLP     Experimental results  We tested our nonlinear programming approach in three DEC POMDP domains  In each experiment  we compare Bernstein et al s DEC BPI with NLP solutions using filter for a range of controller sizes  We also implemented each of these approaches with a correlation device of size two  We do not compare with Szer and Charpillets algorithm because the problems presented in that work are slightly different than those used by Bernstein et al  Nevertheless  on the problems that we tested  our approach can and does achieve higher values than Szer and Charpillets algorithm for all of the controller sizes for which that the best first search is able to find a solution   method can only be considered estimates due to running each algorithm on external machines with uncontrollable load levels  but we expect that they vary by only a small constant  Note that our goal in these experiments is to demonstrate the benefits of our formulation when used in conjunction with an off the shelf solver such as filter  The formulation is very general and many other solvers may be applied  Throughout this section we will refer to our nonlinear optimization as NLO and the optimization with the correlation device with two states as NLO corr       Broadcast problem  A DEC POMDP used by Bernstein et al  was a simplified two agent networking example  This problem      AMATO ET AL   Figure    Recycling robots values using NLP methods and DEC BPI with and without a   node correlation device has   states    actions and   observations  At each time step  each agent must choose whether or not to send a message  If both agents send  there is a collision and neither gets through  A reward of   is given for every step a message is successfully sent over the channel and all other actions receive no reward  Agent   has a     probability of having a message in its queue on each step and agent   has only a     probability  The domain is initialized with only agent   possessing a message and a discount factor of     was used  Table   shows the values produced by DEC BPI and our nonlinear programming approach with and without a correlation device for several controller sizes  Both nonlinear techniques produce the same value      for each controller size  In all cases this is a higher value than that produced by Bernstein et al s independent and correlated approaches  As     is the maximum value that any approach that we tested receives for the given controller sizes  it is likely that it is optimal for these sizes  The time used by each algorithm is shown in Table    As expected  the nonlinear optimization methods require more time to find a solution than the DECBPI methods  As noted above  solution quality is also higher using nonlinear optimization  Either NLP approach can produce a higher valued one node controller in an amount of time similar to or less than each DECBPI method  Therefore  for this problem  the NLP methods are able to find higher valued  more concise solutions given a fixed amount of space or time       Recycling robots  As another comparison  we have extended the Recycling Robot problem  Sutton and Barto        to the multiagent case  The robots have the task of picking up cans in an office building  They have sensors to  Figure    Recycling robots graphs for value vs time for the NLP and DEC BPI methods with and without the correlation device  find a can and motors to move around the office in order to look for cans  The robots are able to control a gripper arm to grasp each can and then place it in an on board receptacle  Each robot has three high level actions      search for a small can      search for a large can or     recharge the battery  In our two agent version  the larger can is only retrievable if both robots pick it up at the same time  Each agent can decide to independently search for a small can or to attempt to cooperate in order to receive a larger reward  If only one agent chooses to retreive the large can  no reward is given  For each agent that picks up a small can  a reward   is given and if both agents cooperate to pick the large can  a reward of   is given  The robots have the same battery states of high and low  with an increased likelihood of transitioning to a low state or exhausting the battery after attempting to pick up the large can  Each robots battery power depends only on its own actions and each agent can fully observe its own level  but not that of the other agent  If the robot exhausts the battery  it is picked up and plugged into the charger and then continues to act on the next step with a high battery level  The two robot version used in this paper has   states    actions and   observations  A discount factor of     was used  We can see in Figure   that in this domain higher quality controllers are produced by using nonlinear optimization  Both NLP methods permit higher mean values than either DEC BPI approach for all controller sizes  Also  correlation is helpful for both the NLP and DEC BPI approaches  but becomes less so for larger controller sizes  For the nonlinear optimization cases  both approaches converge to within a small amount of the maximum value that was found for any controller size tested  As controller size grows  the NLP methods are able to reliably find this solution and correlation is no longer useful    AMATO ET AL      The running times of each algorithm follow the same trend as above in which the nonlinear optimization approaches required much more time as controller size increases  The ability for the NLP techniques to produce smaller  higher valued controllers with similar or lesser running time also follows the same trend  Figure   shows the values that can be attained for each method based on the mean time necessary for convergence  Results are included for NLP techniques up to four nodes with the correlation device and five nodes without it while DEC BPI values are given for fourteen nodes with the correlation device and eighteen without it  This graph demonstrates that even if we allow controller size to continue to grow and examine only the amount of time that is necessary to achieve a solution  the NLP methods continue to provide higher values  Although the values of the controllers produced by the DEC BPI methods are somewhat close to those of the NLP techniques as controller size grows  our approaches produce that value with a fraction of the controller size       Figure    Multiagent Tiger problem values using NLP methods and DEC BPI with and without a   node correlation device   Multiagent tiger problem  Another domain with   states    actions and   observations called the multiagent tiger problem was introduced by Nair et al   Nair et al          In this problem  there are two doors  Behind one door is a tiger and behind the other is a large treasure  Each agent may open one of the doors or listen  If either agent opens the door with the tiger behind it  a large penalty is given  If the door with the treasure behind it is opened and the tiger door is not  a reward is given  If both agents choose the same action  i e   both opening the same door  a larger positive reward or a smaller penalty is given to reward this cooperation  If an agent listens  a small penalty is given and an observation is seen that is a noisy indication of which door the tiger is behind  While listening does not change the location of the tiger  opening a door causes the tiger to be placed behind one of the door with equal probability  A discount factor of     was used  Figure   shows the values attained by each NLP and DEC BPI method for the given controller sizes  Figure   shows the values of just the two NLP methods  These graphs show that not only do the NLP methods significantly outperform the DEC BPI approaches  but correlation greatly increases the value attained by the nonlinear optimization  The individual results for this problem suggest the DEC BPI approach is more dependent on the initial controller and the large penalties in this problem result in several results that are very low  This outweighs the few times that more reasonable value is attained  Nevertheless  the max value attained by DEC BPI for all cases is still less than the  Figure    Multiagent Tiger problem values using just the NLP methods with and without a   node correlation device   Figure    Multiagent Tiger problem graphs for value vs  time for the NLP methods with and without the correlation device  mean value attained by the NLP methods  Again for this problem  more time is needed for the NLP approaches  but one node controllers are produced with higher value than any controller size for the DEC BPI      AMATO ET AL   methods and require very little time  The usefulness of the correlation device is illustrated in Figure    For given amounts of time  the nonlinear optimization that includes the correlation device produces much higher values  The DEC BPI methods are not included in this graph as they were unable to produce mean values greater than     for any controller size up to    for which mean time to convergence was over      seconds  This shows the importance of correlation in this problem and the ability of our NLP technique to take advantage of it      Conclusion  We introduced a novel approach to solving decentralized POMDPs by using a nonlinear program formulation  This memory bounded stochastic controller formulation allows a wide range of powerful nonlinear programming algorithms to be applied to solve DECPOMDPs  The approach is easy to implement as it mostly involves reformulating the problem and feeding it into an NLP solver  We showed that by using an off the shelf locally optimal NLP solver  we were able to produce higher valued controllers than the current state of the art technique for an assortment of DEC POMDP problems  Our experiments also demonstrate that incorporating a correlation device as a shared source of randomness for the agents can further increase solution quality  While the time taken to find a solution to the NLP can be higher  the fact that higher values can be found with smaller controllers by using the NLP suggests adopting more powerful optimization techniques for smaller controllers can be more productive in a given amount of time  The combination of start state knowledge and more advanced optimization allows us to make efficient use of the limited space of the controllers  These results show that this method can allow compact optimal or near optimal controllers to be found for various DEC POMDPs  In the future  we plan to conduct a more exhaustive analysis of the NLP representation and explore more specialized algorithms that can be tailored for this optimization problem  While the performance we get using a standard nonlinear optimization algorithm is very good  specialized solvers might be able to further increase solution quality and scalability  We also plan to characterize the circumstances under which introducing a correlation device is cost effective  Acknowledgements An earlier version of this paper without improvements such as incorporating a correlation device was pre   sented at the AAMAS    Workshop on Multi Agent Sequential Decision Making in Uncertain Domains  This work was supported in part by the Air Force Office of Scientific Research  Grant No  FA               and by the National Science Foundation  Grant No            Any opinions  findings  conclusions or recommendations expressed in this manuscript are those of the authors and do not reflect the views of the US government  
 This article presents the state of the art in optimal solution methods for decentralized partially observable Markov decision processes  Dec POMDPs   which are general models for collaborative multiagent planning under uncertainty  Building off the generalized multiagent A    GMAA   algorithm  which reduces the problem to a tree of one shot collaborative Bayesian games  CBGs   we describe several advances that greatly expand the range of DecPOMDPs that can be solved optimally  First  we introduce lossless incremental clustering of the CBGs solved by GMAA   which achieves exponential speedups without sacrificing optimality  Second  we introduce incremental expansion of nodes in the GMAA  search tree  which avoids the need to expand all children  the number of which is in the worst case doubly exponential in the nodes depth  This is particularly beneficial when little clustering is possible  In addition  we introduce new hybrid heuristic representations that are more compact and thereby enable the solution of larger Dec POMDPs  We provide theoretical guarantees that  when a suitable heuristic is used  both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent  Finally  we present extensive empirical results demonstrating that GMAA  ICE  an algorithm that synthesizes these advances  can optimally solve Dec POMDPs of unprecedented size      Introduction A key goal of artificial intelligence is the development of intelligent agents that interact with their environment in order to solve problems  achieve goals  and maximize utility  While such agents sometimes act alone  researchers are increasingly interested in collaborative multiagent systems  in which teams of agents work together to perform all manner of tasks  Multiagent systems are appealing  not only because they can tackle inherently distributed problems  but because they facilitate the decomposition of problems too complex to be tackled by a single c      AI Access Foundation  All rights reserved    Oliehoek  Spaan  Amato    Whiteson  agent  Huhns        Sycara        Panait   Luke        Vlassis        Busoniu  Babuska    De Schutter         One of the primary challenges of multiagent systems is the presence of uncertainty  Even in single agent systems  the outcome of an action may be uncertain  e g   the action may fail with some probability  Furthermore  in many problems the state of the environment may be uncertain due to limited or noisy sensors  However  in multiagent settings these problems are often greatly exacerbated  Since agents have access only to their own sensors  typically a small fraction of those of the complete system  their ability to predict how other agents will act is limited  complicating cooperation  If such uncertainties are not properly addressed  arbitrarily bad performance may result  In principle  agents can use communication to synchronize their beliefs and coordinate their actions  However  due to bandwidth constraints  it is typically infeasible for all agents to broadcast the necessary information to all other agents  In addition  in many realistic scenarios  communication may be unreliable  precluding the possibility of eliminating all uncertainty about other agents actions  Especially in recent years  much research has focused on approaches to  collaborative  multiagent systems that deal with uncertainty in a principled way  yielding a wide variety of models and solution methods  Pynadath   Tambe        Goldman   Zilberstein        Seuken   Zilberstein         This article focuses on the decentralized partially observable Markov decision process  Dec POMDP   a general model for collaborative multiagent planning under uncertainty  Unfortunately  solving a Dec POMDP  i e   computing an optimal plan  is generally intractable  NEXP complete   Bernstein  Givan  Immerman    Zilberstein        and in fact even computing solutions with absolutely bounded error  i e   o approximate solutions  is also NEXP complete  Rabinovich  Goldman    Rosenschein         In particular  the number of joint policies grows exponentially with the number of agents and observations and doubly exponentially with respect to the horizon of the problem   Though these complexity results preclude methods that are efficient on all problems  developing better optimal solution methods for Dec POMDPs is nonetheless an important goal  for several reasons  First  since the complexity results describe only the worst case  there is still great potential to improve the performance of optimal methods in practice  In fact  there is evidence that many problems can be solved much faster than the worst case complexity bound indicates  Allen   Zilberstein         In this article  we present experiments that clearly demonstrate this point  on many problems  the methods we propose scale vastly beyond what would be expected for a doubly exponential dependence on the horizon  Second  as computer speed and memory capacity increase  a growing set of small and medium sized problems can be solved optimally  Some of these problems arise naturally while others result from the decomposition of larger problems  For instance  it may be possible to extrapolate optimal solutions of problems with shorter planning horizons  using them as the starting point of policy search for longer horizon problems as in the work of Eker and Akn         or to use such shorter horizon  no communication solutions inside problems with communication  Nair  Roth    Yohoo        Goldman   Zilberstein         More generally  optimal policies of smaller problems can potentially be used to find good solutions for larger problems  For instance  transfer planning  Oliehoek        Oliehoek  Whiteson    Spaan     Surprisingly  the number of states in a Dec POMDP is less important  e g   brute force search depends on the number of states only via its policy evaluation routine  which scales linearly in the number of states         Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs        employs optimal solutions to problems with few agents to better solve problems with many agents  By performing  approximate  influence based abstraction and influence search  Witwicki        Oliehoek  Witwicki    Kaelbling         optimal solutions of component problems can potentially be used to find  near  optimal solutions of larger problems  Third  optimal methods offer important insights into the nature of specific Dec POMDP problems and their solutions  For instance  the methods introduced in this article enabled the discovery of certain properties of the BroadcastChannel benchmark problem that make it much easier to solve  Fourth  optimal methods provide critical inspiration for principled approximation methods  In fact  almost all successful approximate Dec POMDP methods are based on optimal ones  see  e g   Seuken   Zilberstein      b      a  Dibangoye  Mouaddib    Chai draa        Amato  Dibangoye    Zilberstein        Wu  Zilberstein    Chen      a  Oliehoek        or locally optimal ones  Velagapudi  Varakantham  Scerri    Sycara           and the clustering technique presented in this article forms the basis of a recently introduced approximate clustering technique  Wu  Zilberstein    Chen         Finally  optimal methods are essential for benchmarking approximate methods  In recent years  there have been huge advances in the approximate solution of Dec POMDPs  leading to the development of solution methods that can deal with large horizons  hundreds of agents and many states  e g   Seuken   Zilberstein      b  Amato et al         Wu et al       a  Oliehoek        Velagapudi et al          However  since computing even o approximate solutions is NEXP complete  any method whose complexity is not doubly exponential cannot have any guarantees on the absolute error of the solution  assuming EXP  NEXP   As such  existing effective approximate methods have no quality guarantees   Consequently  it is difficult to meaningfully interpret their empirical performance without the upper bounds optimal methods supply  While approximate methods can also be benchmarked against lower bounds  e g   other approximate methods   such comparisons cannot detect when a method fails to find good solutions  Doing so requires benchmarking against upper bounds and  unfortunately  upper bounds that are easier to compute  such as QMDP and QPOMDP  are too loose to be helpful  Oliehoek  Spaan    Vlassis         As such  benchmarking with respect to optimal solutions is an important part of the verification of any approximate algorithm  Since existing optimal methods can only tackle very small problems  scaling optimal solutions to larger problems is a critical goal      Contributions This article presents the state of the art in optimal solution methods for Dec POMDPs  In particular  it describes several advances that greatly expand the horizon to which many DecPOMDPs can be solved optimally  In addition  it proposes and evaluates a complete algorithm that synthesizes these advances  Our approach is based on the generalized multiagent A   GMAA   algorithm  Oliehoek  Spaan    Vlassis         which makes it possible to reduce the problem to a tree of one shot collaborative Bayesian games  CBGs   The appeal of this    The method by Velagapudi et al         repeatedly computes best responses in a way similar to DP JESP  Nair  Tambe  Yokoo  Pynadath    Marsella         The best response computation  however  exploits sparsity of interactions     Note that we refer to methods without quality guarantees as approximate rather than heuristic to avoid confusion with heuristic search  which is used throughout this article and is exact         Oliehoek  Spaan  Amato    Whiteson  approach is the abstraction layer it introduces  which has led to various insights into DecPOMDPs and  in turn  to the improved solution methods we describe  The specific contributions of this article are      We introduce lossless clustering of CBGs  a technique to reduce the size of the CBGs for which GMAA  enumerates all possible solutions  while preserving optimality  This can exponentially reduce the number of child nodes in the GMAA  search tree  leading to huge increases in efficiency  In addition  by applying incremental clustering  IC  to GMAA   our GMAA  IC method can avoid clustering exponentially sized CBGs     We introduce incremental expansion  IE  of nodes in the GMAA  search tree  Although clustering may reduce the number of children of a search node  this number is in the worst case still doubly exponential in the nodes depth  GMAA  ICE  which applies IE to GMAA  IC  addresses this problem by creating a next child node only when it is a candidate for further expansion     We provide theoretical guarantees for both GMAA  IC and GMAA  ICE  In particular  we show that  when using a suitable heuristic  both algorithms are both complete and search equivalent     We introduce an improved heuristic representation  Tight heuristics like those based on the underlying POMDP solution  QPOMDP   or the value function resulting from assuming   step delayed communication  QBG   are essential for heuristic search methods like GMAA   Oliehoek  Spaan    Vlassis         However  the space needed to store these heuristics grows exponentially with the problem horizon  We introduce hybrid representations that are more compact and thereby enable the solution of larger problems     We present extensive empirical results that show substantial improvements over the current state of the art  Whereas Seuken and Zilberstein        argued that GMAA  can at best optimally solve Dec POMDPs only one horizon further than brute force search  our results demonstrate that GMAA  ICE can do much better  In addition  we provide a comparative overview of the results of competitive optimal solution methods from the literature  The primary aim of the techniques introduced in this article is to improve scalability with respect to the horizon  Our empirical results confirm that these techniques are highly successful in this regard  As an added bonus  our experiments also demonstrate improvement in scalability with respect to the number of agents  In particular  we present the first optimal results on general  non special case  Dec POMDPs with more than three agents  Extensions of our techniques to achieve further improvements with respect to the number of agents  as well as promising ways to combine the ideas behind our methods with state of the art approximate approaches  are discussed under future work in Section       This article synthesizes and extends research that was already reported in two conference papers  Oliehoek  Whiteson    Spaan        Spaan  Oliehoek    Amato                Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs      Organization The article is organized as follows  Section   provides background on the Dec POMDP model  the GMAA  heuristic search solution method  as well as suitable heuristics  In Section    we introduce lossless clustering of the CBGs and its integration into GMAA   Section   introduces the incremental expansion of search nodes  The empirical evaluation of the proposed techniques is reported in Section    We give a treatment of related work in Section    Future work is discussed in Section   and conclusions are drawn in Section        Background In a Dec POMDP  multiple agents must collaborate to maximize the sum of the common rewards they receive over multiple timesteps  Their actions affect not only their immediate rewards but also the state to which they transition  While the current state is not known to the agents  at each timestep each agent receives a private observation correlated with that state  Definition    A Dec POMDP is a tuple D  S  A  T  O  O  R  b    h   where  D              n  is the finite set of agents     S   s           s S  is the finite set of states    A   i Ai is the set of joint actions a   ha            an i  where Ai is the finite set of actions available to agent i   T is a transition function specifying the state transition probabilities Pr s  s a    O   i Oi is the finite set of joint observations  At every stage one joint observation o   ho       on i is received  Each agent i observes only its own component oi    O is the observation function  which specifies observation probabilities Pr o a s     R s a  is the immediate reward function mapping  s a  pairs to real numbers   b    S  is the initial state distribution at time t      where  S  denotes the infinite set of probability distributions over the finite set S   h is the horizon  i e   the number of stages  We consider the case where h is finite  At each stage t           h     each agent takes an individual action and receives an individual observation  Example    Recycling Robots   To illustrate the Dec POMDP model  consider a team of robots tasked with removing trash from an office building  depicted in Fig     The robots have sensors to find marked trash cans  motors to move around in order to look for cans  as well as gripper arms to grasp and carry a can  Small trash cans are light and compact enough for a single robot to carry  but large trash cans require multiple robots to carry them out together  Because more people use them  the larger trash cans fill up more quickly  Each robot must also ensure that its battery remains charged by moving to a charging station before it expires  The battery level for a robot degrades due to the distance the robot travels and the weight of the item being carried  Each robot knows its own battery level but not that of the other robots and only the location of other robots within sensor range  The goal of this problem is to remove as much trash as possible in a given time period  This problem can be represented as a Dec POMDP in a natural way  The states  S  consist of the different locations of each robot  their battery levels and the different amounts of trash in the cans  The actions  Ai   for each robot consist of movements in different directions as well as decisions       Oliehoek  Spaan  Amato    Whiteson  Figure    Illustration of the Recycling Robots example  in which two robots have to remove trash in an office environment with three small  blue  trash cans and two large  yellow  ones  In this situation  the left robot might observe that the large trash can next to it is full  and the other robot that the small trash can is empty  However  none of them is sure of the trash cans state due to limited sensing capabilities  nor do they see the state of trash cans further away  In particular  one robot has no knowledge regarding the observations of the other robot  to pick up a trash can or recharge the battery  when in range of a can or a charging station   The observations  Oi   of each robot consist of its own battery level  its own location  the locations of other robots in sensor range and the amount of trash in cans within range  The rewards  R  could consist of a large positive value for a pair of robots emptying a large  full  trash can  a small positive value for a single robot emptying a small trash can and negative values for a robot depleting its battery or a trash can overflowing  An optimal solution is a joint policy that leads to the expected behavior  given that the rewards are properly specified   That is  it ensures that the robots cooperate to empty the large trash cans when appropriate and the small ones individually while considering battery usage   For explanatory purposes  we also consider a much simpler problem  the so called decentralized tiger problem  Nair et al          Example    Dec Tiger   The Dec Tiger problem concerns two agents that find themselves in a hallway with two doors  Behind one door  there is a treasure and behind the other is a tiger  The state describes which door the tiger is behindleft  sl   or right  sr  each occurring with     probability  i e   the initial state distribution b  is uniform   Each agent can perform three actions  open the left door  aOL    open the right door  aOR   or listen  aLi    Clearly  opening the door to the treasure will yield a reward  but opening the door to the tiger will result in a severe penalty  A greater reward is given for both agents opening the correct door at the same time  As such  a good strategy will probably involve listening first  The listen actions  however  also have a minor cost  negative reward   At every stage the agents get an observation  The agents can either hear the tiger behind the left  oHL   or right  oHR   door  but each agent has a     chance of hearing it incorrectly  getting the wrong observation   Moreover  the observation is informative only if both agents listen  if either agent opens a door  both agents receive an uninformative  uniformly drawn  observation and the problem resets to sl or sr with equal probability  At this point the problem just continues  such that the agents may be able to open the door to the treasure multiple times  Also note that  since the only two observations the agents can get are oHL   oHR   the agents have no way of detecting that the problem has been reset  if one agent opens the door while the other listens  the other agent will not be able to tell that the door was opened  For a complete specification  see the discussion by Nair et al           Given a Dec POMDP  the agents common goal is to maximize the expected cumulative reward or return  The planning task entails finding a joint policy    h           n i from the space of joint policies   that specifies an individual policy i for each agent i  Such an       Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  individual policy in general specifies an individual action for each action observation history  AOH   it    a i  o i          ait   oti    e g   i   it     ati   However  it is possible to restrict our attention to deterministic or pure policies  in which case i maps each observation history   t to an action  e g   i   o t     at   The number of such policies is  OH   o i          oti      oit  O i i i h      O       O   i  Ai   i and the number of joint policies is therefore   n O  h     O  A    O            where A and O denote the largest individual action and observation sets  The quality of a particular joint policy is expressed by the expected cumulative reward it induces  also referred to as its value  Definition    The value V    of a joint policy  is V      E  h  hX t    i R st  at    b            where the expectation is over sequences of states  actions and observations  The planning problem for a Dec POMDP is to find an optimal joint policy     i e   a joint policy that maximizes the value      arg max V     Because an individual policy i depends only on the local information  oi available to an agent  the on line execution phase is truly decentralized  no communication takes place other than that modeled via actions and observations  The planning itself however  may take place in an off line phase and be centralized  This is the scenario that we consider in this article  For a more detailed introduction to Dec POMDPs see  e g   the work of Seuken and Zilberstein        and Oliehoek             Heuristic Search Methods In recent years  numerous Dec POMDP solution methods have been proposed  Most of these methods fall into one of two categories  dynamic programming and heuristic search methods  Dynamic programming methods take a backwards or bottom up perspective by first considering policies for the last time step t   h    and using them to construct policies for stage t   h     etc  In contrast  heuristic search methods take a forward or top down perspective by first constructing plans for t     and extending them to later stages  In this article  we focus on the heuristic search approach that has shown state of the art results  As we make clear in this section  this method can be interpreted as searching over a tree of collaborative Bayesian games  CBGs   These CBGs provide a convenient abstraction layer that facilitates the explanation of the techniques introduced in this article  This section provides some concise background on heuristic search methods  For a more detailed description  see the work of Oliehoek  Spaan  and Vlassis         For a further description of dynamic programming methods and their relationship to heuristic search methods  see the work of Oliehoek               Multiagent A  Szer  Charpillet  and Zilberstein        introduced a heuristically guided policy search method called multiagent A   MAA    It performs an A  search over partially specified joint policies        Oliehoek  Spaan  Amato    Whiteson  t    t    t    i   aLi i      i  oHR  oHL aOL  i   aOL  oHL  oHR  oHL  oHR  aLi  aLi  aOL  aLi  i   i    Figure    An arbitrary policy for the Dec Tiger problem  The figure illustrates the different types of partial policies used in this paper  The shown past policy  i consists of two decision rules i    i    Also shown are two sub tree policies i      i     introduced in Section         pruning joint policies that are guaranteed to be worse than the best  fully specified  joint policy found so far  Oliehoek  Spaan  and Vlassis        generalized the algorithm by making explicit the expand and selection operators performed in the heuristic search  The resulting algorithm  generalized MAA   GMAA   offers a unified perspective of MAA  and the forward sweep policy computation method  Emery Montemerlo         which differ in how they implement GMAA s expand operator  forward sweep policy computation solves  i e   finds the best policy for  collaborative Bayesian games  while MAA  finds all policies for those collaborative Bayesian games  as we describe in Section        The GMAA  algorithm considers joint policies that are partially specified with respect to time  These partially specified policies can be formalized as follows  Definition    A decision rule it for agent is decision for stage t is a mapping from action  t  Ai   observation histories for stage t to actions it    i In this article  we consider only deterministic policies  Since such policies need to condition their actions only on observation histories  they are made up of decision rules that map length  t  Ai   A joint decision rule  t   h t          nt i specifies t observation histories to actions  it   O   i a decision rule for each agent  Fig    illustrates this concept  as well as that of a past policy  which we introduce shortly  As discussed below  decision rules allow partial policies to be defined and play a crucial role in GMAA  and the algorithms developed in this article  Definition    A partial or past policy for stage t  ti   specifies the part of agent is policy that relates to stages t   t  That is  it specifies the decision rules for the first t stages  ti    i   i           it     A past policy for stage h is just a regular  or fully specified  policy hi   i   A past joint policy t                     t    specifies joint decision rules for the first t stages  GMAA  performs a heuristic search over such partial joint policies t by constructing a search tree as illustrated in Fig   a  Each node q   ht   vi in the search tree specifies a past joint policy t and heuristic value v  This heuristic value v of the node represents an optimistic estimate of the past joint policy Vb  t    which can be computed via Vb  t     V     t   t     H t   h   t                 Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  B                                            B                     B      B          a  The MAA  perspective        B               B       b  The CBG perspective   Figure    Generalized MAA   Associated with every node is a heuristic value  The search trees for the two perspectives shown are equivalent under certain assumptions on the heuristic  as explained in Section      where H t   h  is a heuristic value for the remaining h  t stages and V     t   t   is the actual expected reward t achieves over the first t stages  for its definition  see Appendix A     Clearly  when H t   h  is an admissible heuristica guaranteed overestimationso is Vb  t     Algorithm   illustrates GMAA   It starts by creating a node q   for a completely unspecified joint policy   and placing it in an open list L  Then  it selects nodes  Algorithm    and expands them  Algorithm     repeating this process until it is certain that it has found the optimal joint policy  The Select operator returns the highest ranked node  as defined by the following comparison operator  Definition    The node comparison operator   is defined for two nodes q   ht  vi  q    ht  v  i as follows       if v    v  v   v q   q    depth q    depth q      otherwise if depth q     depth q            t t      otherwise  That is  the comparison operator first compares the heuristic values  If those are equal  it compares the depth of the nodes  Finally  if nodes have equal value and equal depth  it lexically compares the past joint policies  This ranking leads to A  behavior  i e   selecting the node from the open list with the highest heuristic value  of GMAA   as well as guaranteeing the same selection order in our incremental expansion technique  introduced in Section     Ranking nodes with greater depth higher in case of equal heuristic value helps find tight lower bounds early by first expanding deeper nodes  Szer et al         and is also useful in incremental expansion     More formally  H should not underestimate the value  Note that  unlike classical A  applications such as path planningin which an admissible heuristic should not overestimatein our setting we maximize reward  rather than minimize cost         Oliehoek  Spaan  Amato    Whiteson  Algorithm   Generalized multiagent A   Input  a Dec POMDP  an admissible heuristic H  an empty open list L Output  an optimal joint policy      vGM AA      q    h        v    i    L insert q        repeat    q  Select L     QExpand  Expand q  H     if depth q    h    then      QExpand contains fully specified joint policies  we only are interested in the best one      h  vi  BestJointPolicyAndValue QExpand       if v   vGM AA then         found a new best joint policy      vGM AA  v     L Prune vGM AA     optionally  prune the open list      end if     else    add expanded children to open list      L insert  q   QExpand   q   v   vGM AA       end if     PostProcessNode q  L      until L is empty     return    Algorithm   Select L   Return the highest ranked node from the open list  Input  open list L  total order on nodes   Output  the highest ranked node q     q   q  L s t  q L  q     q   q    q     return q   The Expand operator constructs QExpand   the set of all child nodes  That is  given a node that contains partial joint policy t                     t     it constructs t     the set of all t                       t    t    by appending all possible joint decision rules  t for the next time step t  For all these t     a heuristic value is computed and a node is constructed  After expansion  the algorithm checks  line    if the expansion resulted in fully specified joint policies  If not  all children with sufficient heuristic value are placed in the open list Algorithm   Expand q  H   The expand operator of plain MAA   Input  q   ht   vi the search node to expand  H the admissible heuristic  Output  QExpand the set containing all expanded child nodes     QExpand        t     t     t      t    t       for t    t   do    Vb  t      V     t  t       H t        q   ht     Vb  t    i    QExpand  Insert q       end for    return QExpand        create child node    Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  Algorithm   PostProcessNode q L  Input  q the expanded parent node  L the open list  Output  the expanded node is removed     L Pop q    line      If the children are fully specified  BestJointPolicyAndValue returns only the best joint policy  and its value  from QExpand  see Algorithm    in Appendix A   for details of BestJointPolicyAndValue   GMAA  also maintains a lower bound vGM AA which corresponds to the actual value of the best fully specified joint policy found so far  If the newly found joint policy has a higher value this lower bound is updated  lines    and      Also  any nodes for partial joint policies t   with an upper bound that is lower than the best solution so far  Vb  t       vGM AA   can be pruned  line      This pruning takes additional time  but can save memory  Finally  PostProcessNode simply removes the parent node from the open list  this procedure is augmented for incremental expansion in Section     The search ends when the list becomes empty  at which point an optimal joint policy has been found  GMAA  is complete  i e   it will search until it finds a solution  Therefore  in theory  GMAA  is guaranteed to eventually produce an optimal joint policy  Szer et al          However  in practice  this is often infeasible for larger problems  A major source of complexity is the full expansion of a search node  The number of joint decision rules for stage t that can form the children of a node at depth t in the search tree  is     t O  A  n  O             which is doubly exponential in t  Comparing       with        we see that the worst case complexity of expanding a node for the deepest level in the tree t   h    is comparable to that of brute force search for the entire Dec POMDP  Consequently  Seuken and Zilberstein        conclude that MAA  can at best solve problems whose horizon is only   greater than those that can already be solved by nave brute force search        The Bayesian Game Perspective GMAA  makes it possible to interpret MAA  as the solution of a collection of collaborative Bayesian games  CBGs   We employ this approach throughout this article  as it facilitates the improvements to GMAA  that we introduce  each of which results in significant advances in the state of the art in Dec POMDP solutions  A Bayesian game  BG  models a one shot interaction between a number of agents  It is an extension of the well known strategic game  also known as a normal form game  in which each agent holds some private information  Osborne   Rubinstein         A CBG is a BG in which the agents receive identical payoffs  In the Bayesian game perspective  each node q in the GMAA  search tree  along with its corresponding partial joint policy t   defines a CBG  Oliehoek  Spaan    Vlassis         That is  given state distribution b    for each t   it is possible to construct a CBG B b   t   that represents the decision making problem for stage t given that t was followed for the first t stages starting from b    When it is clear what b  is  we simply write B t       We follow the convention that the root has depth           Oliehoek  Spaan  Amato    Whiteson  Definition    A collaborative Bayesian game  CBG  B b   t     hD  A    Pr    ui modeling stage t of a Dec POMDP  given initial state distribution b  and past joint policy t   consists of   D  the set of agents          n    A  the set of joint actions     the set of their joint types  each of which specifies a type for each agent    h           n i   Pr    a probability distribution over joint types   u  a  heuristic  payoff function mapping joint type and action to a real number  u  a   In any Bayesian game  the type i of an agent i represents the private information it holds  For instance  in a Bayesian game modeling of a job recruitment scenario  the type of an agent may indicate whether that agent is a hard worker  In a CBG for a Dec POMDP  an agents private information is its individual AOH  Therefore  the type i of an agent i corresponds to  it   its history of actions and observations  i   it   Similarly  a joint type corresponds to a joint AOH      t   Consequently  u should provide a  heuristic  estimate for the long term payoff of each    t  a  pair  In other words  the payoff function corresponds to a heuristic Q value  u  a   b   t  a   We discuss how to compute such heuristics in Section      Given t   b    and the Q  correspondence of joint types and AOHs  the probability distribution over joint types is  Pr     Pr   t  b   t            where the latter probability is the marginal of Pr s   t  b   t   as defined by  A    used in the computation of the value of a partial joint policy V     t   t   in Appendix A    Note that due to the correspondence between types and AOHs  the size of a CBG B b   t   for a stage t is exponential in t  In a CBG  each agent uses a Bayesian game policy i that maps individual types to actions  i  i     ai   Because of the correspondence between types and AOHs  a  joint  policy for the CBG  corresponds to a  joint  decision rule     t   In the remainder of this article  we assume deterministic past joint policies t   which implies that only one   t will have non zero probability given the observation history  o t   Thus   effectively maps observation histories to actions  The number of such  for B b   t   is given by        The value of a joint CBG policy  for a CBG B b   t   is  X b   t     t     Pr   t  b   t  Q        Vb        t  where  t    t     hi   it  ii     n denotes the joint action that results from application of the individual CBG policies to the individual AOH  it specified by   t   Example    Consider a CBG for Dec Tiger given the past joint policy   that specifies to listen at the first two stages  At stage t      each agent has four possible observation histories          oHL  oHL     oHL  oHR     oHR  oHL     oHR  oHR    that correspond directly to its possible types  The O i probabilities of these joint types given   are listed in Fig   a  Since the joint OHs together with   determine the joint AOHs  they also correspond to so called joint beliefs  probability distributions over states  introduced formally in Section       Fig   b shows these joint beliefs  which can serve as the basis for the heuristic payoff function  as further discussed in Section             Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs   o    oHL  oHL    oHL  oHR    oHR  oHL    oHR  oHR     o    oHL  oHL                             oHL  oHR                             oHR  oHL                             oHR  oHR                             a  The joint type probabilities    o    oHL  oHL    oHL  oHR    oHR  oHL    oHR  oHR     o    oHL  oHL                           oHL  oHR                         oHR  oHL                         oHR  oHR                           b  The induced joint beliefs  Listed is the probability Pr sl       b    of the tiger being behind the left door   Figure    Illustration for the Dec Tiger problem with a past joint policy   that specifies only listen actions for the first two stages   Algorithm   Expand CBG q  H   The expand operator of GMAA  that makes use of CBGs  Input  q   ht   vi the search node to expand  b   a   Input  H the admissible heuristic that is of the form Q  Output  QExpand the set containing all expanded child nodes  b    B b   t    ConstructBG b   t   Q     QExpand  GenerateAllChildrenForCBG B b   t       return QExpand   as explained in Section         A solution to the CBG is a  that maximizes        A CBG is equivalent to a team decision process and finding a solution is NP complete  Tsitsiklis   Athans         However  in the Bayesian game perspective of GMAA   illustrated in Fig   b  the issue of solving a CBG  i e   finding the highest payoff   is not so relevant because we need to expand all   That is  the Expand operator enumerates all  and appends them to t to form the set of extended joint policies   t      t        is a joint CBG policy of B b   t   and uses this set to construct QExpand   the set of child nodes  The heuristic value of such a child node q  QExpand that specifies t      t     is given by Vb  t       V     t   t     Vb             The Expand operator that makes use of CBGs is summarized in Algorithm    which uses the GenerateAllChildrenForCBG subroutine  Algorithm    in Appendix A     Fig   b illustrates the Bayesian game perspective of GMAA         Oliehoek  Spaan  Amato    Whiteson      Heuristics To perform heuristic search  GMAA  defines the heuristic value Vb  t   using        In contrast  the Bayesian game perspective uses        These two formulations are equivalent when b faithfully represents the expected immediate reward  Oliehoek  Spaan    Vlasthe heuristic Q sis         The consequence is that GMAA  via CBGs is complete  and thus finds optimal solutions  as stated by the following theorem  Theorem    When using a heuristic of the form b   t  a    Est  R st  a      t     E  t    Vb    t         t   a   Q           where Vb    t      Q    t          t      is an overestimation of the value of an optimal joint policy     GMAA  via CBGs is complete  Proof  See appendix  In this theorem  Q    t  a  is the Q value  i e   the expected future cumulative reward of performing a from   t under joint policy   Oliehoek  Spaan P   Vlassis         The expectation t   of the immediate reward will also be written as R   a    sS R s a  Pr s   t  b     It can be computed using Pr s   t  b     a quantity we refer to as the joint belief resulting from   t and that we also denote as b  The joint belief itself can be computed via repeated application of Bayes rule  Kaelbling  Littman    Cassandra         or as the conditional of  A     The rest of this subsection reviews several heuristics that have been used for GMAA         QMDP b   a  is to solve the underlying MDP  i e   to One way to obtain an admissible heuristic Q  assume the joint action is chosen by a single puppeteer agent that can observe the true state  This approach  known as QMDP  Littman  Cassandra    Kaelbling         uses the t MDP value function Qt  M  s  a   which can be computed using standard dynamic programming t b  t techniques  Puterman         In order to transform the Qt  M  s  a  values to QM    a  values  we compute  X t  b M    t  a           Q Q  s a  Pr s   t  b     M  sS  Solving the underlying MDP has time complexity that is linear in h  which makes it  especially compared to the Dec POMDP  easy to compute  In addition  it is only necessary to store a value for each  s a  pair  for each stage t  However  the bound it provides on the optimal Dec POMDP Q  value function is loose  Oliehoek   Vlassis               QPOMDP Similar to the underlying MDP  one can define the underlying POMDP of a Dec POMDP  i e   assuming the joint action is chosen by a single agent with access to the joint observation      Alternatively one can view this POMDP as a multiagent POMDP in which the agents can instantaneously broadcast their private observations         Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  Tree  Vector  t   t   t   t   Figure    Visual comparison of tree and vector based Q representations   The resulting solution can be used as a heuristic  called QPOMDP  Szer et al         Roth  Simmons    Veloso         The optimal QPOMDP value function satisfies   QP  bt   a    R bt  a     X  P  ot    bt  a  max QP  bt    at      at    ot   O          P where bt is the joint belief  R bt  a    sS R s a bt  s  is the immediate reward  and bt   is the joint belief resulting from bt by action a and joint observation ot     To use QPOMDP   for b P    t  a    Qt  b t  a   each   t   we can directly use the value for the induced joint belief  Q P  There are two approaches to computing QPOMDP   One is to construct the belief MDP tree of all joint beliefs  illustrated in Fig     left   Starting with b   corresponding to the empty joint AOH        for each a and o we compute the resulting     and corresponding    belief b and continue recursively  Given this tree  it is possible to compute values for all the nodes by standard dynamic programming  Another possibility is to apply vector based POMDP techniques  see Fig     right    The Q value function for a stage QtP  b a  can be represented using a set of vectors for each joint t    Kaelbling et al          Qt  b a  is then defined as the maximum action V t    V t          V A  P inner product  QtP  b a    max b  vat   t V t va a  Given V h    the vector representation of the last stage  we can compute V h    etc  In order to limit the growth of the number of vectors  dominated vectors can be pruned  Since QMDP is an upper bound on the POMDP value function  Hauskrecht         QPOMDP provides a tighter upper bound to Q than QMDP   However  it is also more costly to compute and store  both the tree based and the vector based approach may need to store a number of values exponential in h        Oliehoek  Spaan  Amato    Whiteson        QBG A third heuristic  called QBG   assumes that each agent in the team has access only to its individual observation but it can communicate with a   step delay   We define QBG as X QB    t  a    R   t  a    max Pr ot      t  a QB    t     ot                ot   O  t   where    h   ot           n  on  i is a tuple of individual policies i   Oi  Ai for the CBG t constructed for    a  Like QPOMDP   QBG can also be represented using vectors  Varaiya   Walrand        Hsu   Marcus        Oliehoek  Spaan    Vlassis        and the same two manners of computation  tree and vector based  apply  It yields a tighter heuristic than QPOMDP   but its computation has an additional exponential dependence on the maximum number of individual observations  Oliehoek  Spaan    Vlassis         which is particularly troubling for the vector based computation  since it precludes effective application of incremental pruning  A  Cassandra  Littman    Zhang         To overcome this problem  Oliehoek and Spaan        introduce novel tree based pruning methods      Clustering GMAA  solves Dec POMDPs by repeatedly constructing CBGs and expanding all the joint BG policies  for them  However  the number of such  is equal to the number of regular MAA  child nodes given by       and thus grows doubly exponentially with the horizon h  In this section  we propose a new approach for improving scalability with respect to h by clustering individual AOHs  This reduces the number of  and therefore the number of constructed child nodes in the GMAA  search tree   Previous research has also investigated such clustering  Emery Montemerlo  Gordon  Schneider  and Thrun        propose clustering types based on the profiles of the payoff functions of the CBGs  However  the resulting method is ad hoc  Even given bounds on the error of clustering two types in a CBG  no guarantees can be made about the quality of the Dec POMDP solution  as the bound is with respect to a heuristic payoff function  In contrast  we propose to cluster histories based on the probability these histories induce over histories of the other agents and over states  The critical advantage of this criterion  which we call probabilistic equivalence  PE   is that the resulting clustering is lossless  the solution for the clustered CBG can be used to construct the solution for the original CBG and the values of the two CBGs are identical  Thus  the criterion allows for clustering of AOHs in CBGs that represent Dec POMDPs while preserving optimality    In Section      we describe how histories in Dec POMDPs can be clustered using the notions of probabilistic and best response equivalence  This allows histories to be clustered    The name QBG stems from the fact that such a   step delayed communication scenario can be modeled as a CBG  Note  however  that the CBGs used to compute QBG are of a different form than the B b   t   discussed in Section        in the latter  types correspond to length t  action   observation histories  in the former  types correspond to length   observation histories     While CBGs are not essential for clustering  they provide a convenient level of abstraction that simplifies exposition of our techniques  Moreover  this level of abstraction makes it possible to employ our results concerning CBGs outside the context of Dec POMDPs      The probabilistic equivalence criterion and lossless clustering were introduced by Oliehoek et al          This article presents a new  simpler proof of the optimality of clustering based on PE         Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  when it is rational to always choose the same action  In Section      we describe the application of these results to GMAA   Section     introduces improved heuristic representations that allow for the computation over longer horizons      Lossless Clustering in Dec POMDPs In this section  we discuss lossless clustering based on the notion of probabilistic equivalence  We show that this clustering is lossless by demonstrating that probabilistic equivalence implies best response equivalence  which describes the conditions that a rational agent will select the same action for two of its types  To prove this implication  we show that the best response depends only on the multiagent belief  i e   the probability distribution over states and policies of the other agents   which is the same for two probabilistically equivalent histories  Relations to other equivalence notions are discussed in Section          Probabilistic Equivalence Criterion We first introduce the probabilistic equivalence criterion  which can be used to decide whether two individual histories  ia   ib can be clustered without loss in value  Criterion    Probabilistic Equivalence   Two AOHs  ia   ib for agent i are probabilistically equivalent  PE   written P E  ia   ib    when the following holds     i s  Pr s      i   ia     Pr s      i   ib            These probabilities can be computed as the conditional of Pr s   t  b   t    defined by  A     In subsections             we formally prove that PE is a sufficient criterion to guarantee that clustering is lossless  In the remainder of Section       we discuss some key properties of the PE criterion in order to build intuition  Note that the criterion can be decomposed into the following two criteria     i    i s  Pr      i   ia     Pr      i   ib            Pr s      i   ia     Pr s      i   ib            These criteria give a natural interpretation  the first says that the probability distribution over the other agents AOHs must be identical for both  ia and  ib   The second demands that the resulting joint beliefs are identical  The above probabilities are not well defined without the initial state distribution b  and past joint policy t   However  since we consider clustering of histories within a particular CBG  for some stage t  constructed for a particular b   t   they are implicitly specified  Therefore we drop these arguments  clarifying the notation  Example    In Example    the types  oHL  oHR   and  oHR  oHL   of each agent are PE  To see this  note that the rows  columns for the second agent  for these histories are identical in both Fig   a and Fig   b  Thus  they specify the same distribution over histories of the other agents  cf  equation        and the induced joint beliefs are the same  cf  equation          Probabilistic equivalence has a convenient property that our algorithms exploit  if it holds for a particular pair of histories  then it will also hold for all identical extensions of those histories  i e   it propagates forwards regardless of the policies of the other agents        Oliehoek  Spaan  Amato    Whiteson  Definition    Identical extensions   Given two AOHs  ia t   ib t   their respective extensions   a t        a t  ai  oi   and   b t        b t  a  o   are called identical extensions if and only if i  i  i  ai   ai and oi   oi    i  i  i  Lemma    Propagation of PE   Given  ia t   ib t that are PE  regardless of the decision rule the other agents use   t  i    identical extensions are also PE  ati ot   t st    t   i    i    i  t   t t     t     b t t t   t     i  i  ai  oi     i         Pr st         i   ia t  ati  ot   i     i     Pr s  Proof  The proof is listed in the appendix  but holds intuitively because if the probabilities described above were the same before  they will also be the same after taking the same action and seeing the same observation  Note that  while the probabilities defined in       superficially resemble beliefs used in POMDPs  they are substantially different  In a POMDP  the single agent can compute its individual belief using only its AOH  It can then use this belief to determine the value of any future policy  as it is a sufficient statistic of the history to predict the future rewards  Kaelbling et al         Bertsekas         Thus  it is trivial to show equivalence of AOHs that induce the same individual belief in a POMDP  Unfortunately  Dec POMDPs are more problematic  The next section elaborates on this issue by discussing the relation to multiagent beliefs        Sub Tree Policies  Multiagent Beliefs and Expected Future Value To describe the relationship between multiagent beliefs and probabilistic equivalence  we must first discuss the policies an agent may follow and their resulting values  We begin by introducing the concept of sub tree policies  As illustrated in Fig     on page       a  deterministic  policy i can be represented as a tree with nodes labeled using actions and edges labeled using observations  the root node corresponds to the first action taken  other nodes specify the action for the observation history encoded by the path from the root node  As such  it is possible to define sub tree policies  i   which correspond to sub trees of agent is policy i  also illustrated in Fig      In particular  we write w i   o t   i  ht       i  for the sub tree policy of i corresponding to w observation history  oit that specifies the actions for the last    h  t stages  We refer to   as the policy consumption operator  since it w consumes the part of the policy corresponding to  oit   Similarly we write i  k   o l   i  kl i  note that in        i is just a    h steps to go sub tree policy  and use similar notation     k   for joint sub tree policies  For a more extensive treatment of these different forms of policy  we refer to the discussion by Oliehoek         Given these concepts  we can define the value of a    k stages to go joint policy starting from state s  XX w Pr s  o s a V  s      k  o          V  s    k     R s a    s  o  Here  a is the joint action specified by the roots of the individual sub tree policies specified by    k for stage t   h  k        Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  From this definition  it follows directly that the probability distribution over states s and sub tree policies over other agents    i is sufficient to predict the value of a sub tree policy i   In fact  such a distribution is known as a multiagent belief bi  s    i    Hansen  Bernstein    Zilberstein         Its value is given by XX V  bi     max bi  s    i  V  s hi      i i         i  s     i  and we refer to the maximizing i as agent is best response for bi   This illustrates that a multiagent belief is a sufficient statistic  it contains sufficient information to predict the value of any sub tree policy i   It is possible to connect action observation histories to multiagent beliefs by fixing the policies of the other agents  Given that the other agents will act according to a profile of policies    i   agent i has a multiagent belief at the first stage of the Dec POMDP  bi  s    i     b   s   Moreover  agent i can maintain such a multiagent belief during execution  As such  given    i   each history  i induces a multiagent belief  which we will write as bi  s    i   i      i   to make the dependence on  i      i explicit  The multiagent belief for a history is defined as bi  s    i   i      i     Pr s    i   i   b       i            and induces a best response via        BR  i     i     arg max i  XX s  bi  s    i   i      i  V  s    i  i               i  From this we can conclude that two AOHs  ia   ib can be clustered together if they induce the same multiagent belief  However  this notion of multiagent belief is clearly quite different from the distributions used in our notion of PE  In particular  to establish whether two AOHs induce the same multiagent belief  we need a full specification of    i   Nevertheless  we show that two AOHs that are PE are also best response equivalent and that we can therefore cluster them  The crux is that we can show that  if Criterion   is satisfied  the AOHs will always induce the same multiagent beliefs for any    i  consistent with the current past joint policy   i          Best Response Equivalence Allows Lossless Clustering of Histories We can now relate probabilistic equivalence and the multiagent belief as follows  Lemma    PE implies multiagent belief equivalence   For any    i   probabilistic equivalence implies multiagent belief equivalence      P E  ia   ib    s   i bi  s    i   ia      i     bi  s    i   ib      i          Proof  See appendix  This lemma shows that if two AOHs are PE  they produce the same multiagent belief  Intuitively  this gives us a justification to cluster such AOHs together  since a multiagent belief is a sufficient statistic we should act the same when we have the same multiagent belief  but since Lemma   shows that  ia   ib induces the same multiagent beliefs for any    i when they are PE  we can conclude that we will always act the same in those histories  Formally  we prove that  ia   ib are best response equivalent if they are PE        Oliehoek  Spaan  Amato    Whiteson  Theorem    PE implies best response equivalence   Probabilistic equivalence implies bestresponse equivalence  That is     P E  ia   ib      i BR  ia     i     BR  ib     i   Proof  Assume any arbitrary    i   then BR  ia     i     arg max  XX  bi  s    i   ia  V  s    i  i      arg max  XX  bi  s    i   ib  V  s    i  i     BR  ib     i     i  i  s  s     i     i  where Lemma   is employed to assert the equality of bi    ia   and bi    ib    This theorem is key because it demonstrates that when two AOHs  ia   ib of an agent are PE  then that agent need not discriminate between them now or in the future  Thus  when searching the space of joint policies  we can restrict our search to those that assign the same sub tree policy i to  ia and  ib   As such  it directly provides intuition as to why lossless clustering is possible  Formally  we define the clustered joint policy space as follows  Definition    Clustered joint policy space   Let C   be the subset of joint policies that is clustered  i e   each i that is part of a   C assigns the same sub tree policy to action observation histories that are probabilistically equivalent  Corollary    Existence of an optimal clustered joint policy   There exists an optimal joint policy in the clustered joint policy space  max V      max V     C            Proof  It is clear that the left hand side of        is upper bounded by the right hand side  since C    Now suppose that     arg max V    has strictly higher value than the best clustered joint policy  For at least one agent i and one pair of PE histories  ia    ib     must assign different sub tree policies ia    ib  otherwise   would be clustered   Without loss of generality we assume that there is only one such pair  It follows directly from Theorem   that from this policy we can construct a clustered policy  C  C  by assigning either ia or ib to both  ia    ib   that is guaranteed to have value no less than     thereby contradicting the assumption that   has strictly higher value than the best clustered joint policy  This formally proves that we can restrict our search to C   the space of clustered joint policies  without sacrificing optimality        Clustering with Commitment in CBGs Though it is now clear that two AOHs that are PE can be clustered  making this result operational requires an additional step  To this end  we use the abstraction layer provided by Bayesian games  Recall that in the CBG for a stage  the AOHs correspond to types        Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  Therefore  we want to cluster these types in the CBG  To accomplish the clustering of two types ia  ib   we introduce a new type ic to replace them  by defining    i Pr ic     i     Pr ia     i     Pr ib     i   j a  u hic     i i  a     Pr ia     i  u hia     i i  a    Pr ib     i  u  ib     i  a    Pr ia     i     Pr ib     i                   Theorem    Reduction through commitment   Given that agent i in collaborative Bayesian game B is committed to selecting a policy that assigns the same action for two of its types ia  ib   i e   to selecting a policy i such that i  ia     i  ib    the CBG can be reduced without loss in value for any agents  That is  the result is a new CBG B  in which agent i employs a policy i that reflects the clustering and whose expected payoff is the same as in the original  CBG  V B  i     i     V B  i     i    Proof  See appendix  This theorem shows that  given that agent i is committed to taking the same action for its types ia  ib   we can reduce the collaborative Bayesian game B to a smaller one B  and  translate the joint CBG policy   found   for B back to a joint CBG policy  in B  This does not necessarily mean that    i     i is also a solution for B  because the best response of agent i against    i may not select the same action for ia  ib   Rather i is the best response against    i given that the same action needs to be taken for ia  ib     Even though Theorem   only gives a conditional statement that depends on an agent being committed to select the same action for two of its types  the previous subsection discussed when a rational agent can make such a commitment  Combining these results gives the following corollary  Corollary    Lossless Clustering with PE   Probabilistically equivalent histories  ia   ib can be clustered without loss in heuristic value by merging them into a single type in a CBG  Proof  Theorem   shows that  given that an agent i is committed to take the same action for two of its types  those types can be clustered without loss in value  Since  ia   ib are PE  they are best response equivalent  which means that the agent is committed to use the same sub tree policy i and hence the same action ai   Therefore we can directly apply clustering without loss in expected payoff  which in a CBG for a stage of a Dec POMDP means no loss in expected heuristic value as given by        Intuitively  the maximizing action is the same for  ia and  ib regardless of what  future  joint policies    i the other agents will use and hence we can cluster them without loss in heuristic value  Note that this does not depend on which heuristic is used and hence also holds for an optimal heuristic  i e   when using an optimal Q value function that gives the true value   This directly relates probabilistic equivalence with equivalence in optimal value        Although we focus on CBGs  these results generalize to BGs with individual payoff functions  Thus  they could potentially be exploited by algorithms for general payoff BGs  Developing methods that do so is an interesting avenue for future work      The proof originally provided by Oliehoek et al         is based on showing that histories that are PE will induce identical Q values         Oliehoek  Spaan  Amato    Whiteson  Algorithm   ClusterCBG B  Input  CBG B Output  Losslessly clustered CBG B    for each agent i do    for each individual type i  B i do    if Pr i       then    B i  B i  i    continue    end if    for each individual type i  B i do    isProbabilisticallyEquivalent  true    for all hs    i i do     if Pr s    i  i      Pr s    i  i   then     isProbabilisticallyEquivalent  false     break     end if     end for     if isProbabilisticallyEquivalent then     B i  B i  i     for each a  A do     for all    i do     u i     i  a   min u i     i  a  u i     i  a       Pr i     i    Pr i     i     Pr i     i       Pr i     i          end for     end for     end if     end for     end for     end for     return B   Prune i from B     Prune i from B     take the lowest upper bound    Note that this result establishes a sufficient  but not necessary condition for lossless clustering  In particular  given policies for the other agents  many types are best response equivalent and can be clustered  However  as far as we know  the criterion must hold in order to guarantee that two histories have the same best response against any policy of the other agents      GMAA  with Incremental Clustering Knowing which individual histories can be clustered together without loss of value has the potential to speed up many Dec POMDP methods  In this article  we focus on its application within the GMAA  framework  Emery Montemerlo et al         showed how clustering can be incorporated at every stage in their algorithm  when the CBG for a stage t is constructed  a clustering of the individual histories  types  is performed first and only afterwards is the  reduced  CBG solved  The same approach can be employed within GMAA  by modifying the Expand procedure  Algorithm    to cluster the CBG before calling GenerateAllChildrenForCBG  Algorithm   shows the clustering algorithm  It takes as input a CBG and returns the clustered CBG  It performs clustering by performing pairwise comparison of all types of each       Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  b Algorithm   ConstructExtendedBG B  t    Q   Input  A CBG B for stage t     and the joint BG policy followed  t    b   a   Input  An admissible heuristic of the form Q   Output  CBG B for stage t     B   B  make a copy of B that we subsequently alter     for each agent i do    B   i   ConstructExtendedTypeSet i   overwrite the individual type sets     end for    B     iD i  the new joint type set  does not have to be explicitly stored      for each joint type      t   at   ot    B    do    for each state st  S do    Compute Pr st     from Pr st    t    via Bayes rule      end for     Pr    Pr ot   t   at    Pr  t        for each a  A do     q     for each history   t represented by  do b   t  a   b we can take the lowest upper bound       q  min q Q    if Q  Q     end for     B   u  a   q     end for     end for     return B   agent to see if they satisfy the criterion  yielding O  i      comparisons for each agent i  Each comparison involves looping over all hs    i i  line     If there are many states  some efficiency could be gained by first checking       and then checking        Rather than taking the average as in         on line    we take the lowest payoff  which can be done if we are using upper bound heuristic values  The following theorem demonstrates that  when incorporating clustering into GMAA   the resulting algorithm is still guaranteed to find an optimal solution  Theorem    When using a heuristic of the form       and clustering the CBGs in GMAA  using the PE criterion  the resulting search method is complete  Proof  Applying clustering does not alter the computation of lower bound values  Also  heuristic values computed for the expanded nodes are admissible and in fact unaltered as guaranteed by Corollary    Therefore  the only difference with regular GMAA  is that the class of considered joint policies is restricted to C   the class of clustered joint policies  not all possible child nodes are expanded  because clustering effectively prunes away policies that would specify different actions for AOHs that are PE and thus clustered  However  Corollary   guarantees that there exists an optimal joint policy in this restricted class  The modification of the Expand proposed above is rather naive  To construct B b   t   it must first construct all  Oi  t possible AOHs for agent i  given the past policy ti    The subsequent clustering involves pairwise comparison of all these exponentially many types  Clearly  this is not tractable for later stages  However  because PE of AOHs propagates forwards  i e   identical extensions of PE histories are also PE   a more efficient approach is possible  Instead of clustering this exponentially       Oliehoek  Spaan  Amato    Whiteson  Algorithm   Expand IC q  H   The expand operator for GMAA  IC  Input  q   ht   vi the search node to expand  b   a   Input  H the admissible heuristic that is of the form Q  Output  QExpand the set containing expanded child nodes     B t     t   CBG  retrieve previous CBG  note t    t     t     t  b t t     B     ConstructExtendedBG B       Q     B t    ClusterBG B t       t  CBG  B t    store pointer to this CBG     QExpand  GenerateAllChildrenForCBG B t       return QExpand  growing set of types  we can simply extend the already clustered types of the previous stages CBG  as shown in Algorithm    That is  given i   the set of types of agent i at the previous stage t     and it  the policy agent i took at that stage  the set of types at stage t  i   can be constructed as   i   i    i  it   i   oti     i  i  oti  Oi          This means that the size of this newly constructed set is  i      i     Oi     If the type set i at the previous stage t    was much smaller than the set of all histories  i     Oi  t    then the new type set i is also much smaller   i     Oi  t   In this way  we bootstrap the clustering at each stage and spend significantly less time clustering  We refer to the algorithm that implements this type of clustering as GMAA  with Incremental Clustering  GMAA  IC   This approach is possible only because we perform an exact  value preserving clustering for which Lemma   guarantees that identical extensions will also be clustered without loss in value  When performing the same procedure in a lossy clustering scheme  e g   as in EmeryMontemerlo et al          errors might accumulate  and a better option might be to re cluster from scratch at every stage  Expansion of a GMAA  IC node takes exponential time with respect to both the number of agents and types  as there are O  A  n      joint CBG policies and thus child nodes in the GMAA  IC search tree  A is the largest action set and  is the largest type set   Clustering involves a pairwise comparison of all types of each agent and each of these comparisons needs to check O    n   S   numbers for equality to verify        The total cost of clustering can therefore be written as O n         n   S    which is only polynomial in the number of types  When clustering decreases the number of types      it can therefore significantly reduce the number of child nodes and thereby the overall time needed  However  when no clustering is possible  some overhead will be incurred      Improved Heuristic Representation Since clustering can reduce the number of types  GMAA  IC has the potential to scale to larger horizons  However  doing so has important consequences for the computation of the heuristics  Previous research has shown that the upper bound provided by QMDP is often too loose for effective heuristic search  Oliehoek  Spaan    Vlassis         However  the space needed to store tighter heuristics such as QPOMDP or QBG grows exponentially with the horizon  Recall from Section        see Fig     that there are two approaches to computing       Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  b with minimum size  Algorithm   Compute Hybrid Q                                                 Qh    R           R A    z   A    S  for t   h    to   do   t     A  y    if z   y then V  VectorBackup Qt     V   Prune V  Qt  V  z   V      S  end if if z  y then Qt  TreeBackup Qt     end if end for   vector representation of last stage   the size of the  A  vectors   size of AOH representation    From now on z  y   QPOMDP or QBG   The first constructs a tree of all joint AOHs and their heuristic values  which is simple to implement but requires storing a value for each    t   a  pair  the number of which grows exponentially with t  The second approach maintains a vector based representation  as is common for POMDPs  Though pruning can provide leverage  in the worst case  no pruning is possible and the number of maintained vectors grows doubly exponentially with h  t  the number of stages to go  Similarly  the initial belief and subsequently reachable beliefs can be used to reduce the number of vectors retained at each stage  but as the number of reachable beliefs is exponential in the horizon the exponential complexity remains  Oliehoek  Spaan  and Vlassis        used a tree based representation for the QPOMDP and QBG heuristics  Since the computational cost of solving the Dec POMDP was the bottleneck  the inefficiencies in the representation could be overlooked  However  this approach is no longer feasible for the longer horizons made possible by GMAA  IC   Hybrid  t    To mitigate this problem  we propose a hybrid represent   tation for the heuristics  as illustrated in Fig     The main insight is that the exponential growth of the two existing representations occurs in opposite directions  Therefore  we can t   use the low space complexity side of both representations  the later stages  which have fewer vectors  use a vector based representation  while the earlier stages  which have fewer histot   ries  use a history based representation  This is similar to the idea of utilizing reachable beliefs to reduce the size of the Figure    An illustration of vector representation described above but  rather than stor  the hybrid representation  ing vectors for the appropriate AOHs at each step  only the values are needed when using the tree based representation  Algorithm   shows how  under mild assumptions  a minimally sized representation can be computed  Starting from the last stage  the algorithm performs vector backups  switching to tree backups when they become the smaller option  For the last time step h     we represent       Oliehoek  Spaan  Amato    Whiteson  Qt by the set of immediate reward vectors     and variable z  initialized on line    keeps track of the number of parameters needed to represent Qt as vectors for the time step at hand  Note that z depends on how effective the vector pruning is  i e   how large the parsimonious representation of the piecewise linear and convex value function is  Since this is problem dependent  z can be updated only after pruning has actually been performed  line     By contrast y  the number of parameters in a tree representation  can be computed directly from the Dec POMDP  line     When z   y  the algorithm switches to tree backups        Incremental Expansion The clustering technique presented in the previous section has the potential to significantly speed up planning if much clustering is possible  However  if little clustering is possible  the number of children in the GMAA  search tree will still grow super exponentially  This section presents incremental expansion  a complementary technique to deal with this problem  Incremental expansion exploits recent improvements in effectively solving CBGs  First note that during the expansion of the last stage t   h    for a particular h    we are only interested in the best child  h    h      which corresponds to the optimal solution of the Bayesian game  h        As such  for this last stage  we can use new methods for solving CBGs  Kumar   Zilberstein      b  Oliehoek  Spaan  Dibangoye    Amato        that can provide speedups of multiple orders of magnitude over brute force search  enumeration     Unfortunately  the improvements to GMAA  afforded by this approach are limited  in order to guarantee optimality  it still relies on expansion of all  child nodes corresponding to all  joint CBG policies  for the intermediate stages  thus necessitating a brute force approach  However  many of the expanded child nodes may have low heuristic values Vb and may therefore never be selected for further expansion  Incremental expansion overcomes this problem because it exploits the following key observation  if we can generate the children in decreasing heuristic order using an admissible heuristic  we do not have to expand all the children  As before  an A  search is performed over partially specified policies and each new CBG is constructed by extending the CBG for the parent node  However  rather than fully expanding  i e   enumerating all the CBG policies of and thereby constructing all children for  each search node  we instantiate an incremental CBG solver for the corresponding CBG  This incremental solver returns only one joint CBG policy at a time  which is then used to construct a single child t      t      By revisiting the nodes  only the promising child nodes are expanded incrementally  Below  we describe GMAA  ICE  an algorithm that combines GMAA  IC with incremental expansion  We establish theoretical guarantees and describe the modifications to BaGaBaB  the CBG solver that GMAA  ICE employs  that are necessary to deliver the child nodes in decreasing order      Only in exceptional cases where a short horizon is combined with large state and action spaces will representing the last time step as vectors not be minimal  In such cases  the algorithm can be trivially adapted      This assumes that the vector representation will not shrink again for earlier stages  Although unlikely in practice  such cases would prevent the algorithm from computing a minimal representation      Kumar and Zilberstein      b  tackle a slightly different problem  they introduce a weighted constraint satisfaction approach to solving the point based backup in dynamic programming for Dec POMDPs  However  this point based backup can be interpreted as a collection of CBGs  Oliehoek et al                 Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs      GMAA  with Incremental Clustering and Expansion We begin by formalizing incremental expansion and incorporating it into GMAA  IC  yielding GMAA  with incremental clustering and expansion  GMAA  ICE   At the core of incremental expansion lies the following lemma  Lemma    Given two joint CBG policies     for a CBG B b   t    if Vb     Vb       then for the corresponding child nodes Vb  t      Vb  t      Proof  This holds directly by the definition of Vb  t   as given by        Vb  t       V      t    t     Vb     V      t    t     Vb        Vb  t       It follows directly that  if for B b   t   we use a CBG solver that can generate a sequence of policies             such that Vb     Vb              then  for the sequence of corresponding children  Vb  t      Vb  t               Exploiting this knowledge  we can expand only the first child t   and compute its heuristic value Vb  t     using        Since all the unexpanded siblings will have heuristic values less than or equal to that  we can modify GMAA  IC to reinsert the node q into the open list L to act as a placeholder for all its non expanded children  Definition    A placeholder is a node for which at least one child has been expanded  A placeholder has a heuristic value equal to its last expanded child  Thus  after expansion of a search node qs child  we update q v  the heuristic value of the node  to Vb  t      the value of the expanded child  i e   we set q v  Vb  t      As such  we can reinsert q into L as a placeholder  As mentioned above  this is correct because all the unexpanded siblings  for which the parent node q now is a placeholder  have heuristic values lower than or equal to Vb  t      Therefore the next sibling q  represented by the placeholder is always expanded in time  q  is always created before nodes with lower heuristic value are selected for further expansion  We keep track of whether a node is a previously expanded placeholder or not  As before  GMAA  ICE performs an A  search over partially specified policies  As in GMAA  IC  each new CBG is constructed by extending the CBG for the parent node and then applying lossless clustering  However  rather than expanding all children  GMAA  ICE requests only the next solution  of an incremental CBG solver  from which a single child t      t     is constructed  In principle GMAA  ICE can use any CBG solver that is able to incrementally deliver all  in descending order of Vb     We propose a modification of the BaGaBaB algorithm  Oliehoek et al          briefly discussed in Section      Fig    illustrates the process of incremental expansion in GMAA  ICE  with t indexed by letters  First  a CBG solver for the root node ha   i is created  and the optimal solution   is computed  with value    This results in a child hb   i  and the root is replaced by a placeholder node ha   i  As per Definition    the node comparison operator   b appears before a in the       Oliehoek  Spaan  Amato    Whiteson  Legend  t v  t  a    a    Root node  a       b    t    b     New B a   Vb    c    t   ht   vi in open list  ha   i  a      New B b   Vb     ha   i hc   i hb   i  hb   i ha   i  b    c    d     Next solution of B a   Vb       hd     i ha     i hc   i hb   i  Figure    Illustration of incremental expansion  with the nodes in the open list at the bottom  Past joint policies t are indexed by letters  Placeholder nodes are indicated by dashes  open list and hence is selected for expansion  Its best child hc   i is added and hb   i is replaced by placeholder hb   i  Now the search returns to the root node  and the second best solution   is obtained from the CBG solver  leading to child hd     i  Placeholder nodes are retained as long as they have unexpanded children  only their values are updated  When using GMAA  ICE  we can derive lower and upper bounds for the CBG solution  which can be exploited by the incremental CBG solver  The incremental CBG solver for B t   can be initialized with lower bound vCBG   vGM AA  V      t    t            where vGM AA is the value of the current best solution  and V      t    t   is the true expected value of t over the first t stages  Therefore  vCBG is the minimum value that a candidate must generate over the remaining h  t stages in order to beat the current best solution  Note that each time the incremental CBG solver is queried for a solution  vCBG is re evaluated  using         because vGM AA may have changed  When the used heuristic faitfully represents the immediate reward  i e   is of the form         then  for the last stage t   h     we can also specify an upper bound for the solution of the CBG vCBG   Vb  h     V      h    h           If this upper bound is attained  no further solutions will be required from the CBG solver  The upper bound holds since by       Vb      Vb  h    V      h    h       V  h    V      h    h     Vb  h     V      h    h      In the first step  Vb  h     V  h    because h is a fully specified policy and the heuristic value given by       equals the actual value when a heuristic that faithfully represents the expected       Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  Algorithm    Expand ICE q  H   The expand operator for GMAA  ICE  Input  q   ht   vi the search node to expand  b   a   Input  H the admissible heuristic that is of the form Q  Output  QExpand the set containing   or   expanded child nodes     if IsPlaceholder q  then    B t    t  CBG  reuse stored CBG     else    B t     t   CBG  retrieve previous CBG  note t    t     t     t  b    B t    ConstructExtendedBG B t       Q  t t    B     ClusterBG B        B t   Solver  CreateSolver B t       t  CBG  B t    store pointer to this CBG     end if  set lower bound for CBG solution      vCBG   vGM AA  V      t    t       if t   h    then     vCBG   Vb  h     V      h    h     upper bound only used for last stage CBG      else     vCBG         end if b   t  i  B t   Solver NextSolution vCBG  vCBG    compute next CBG solution      h t   V t     if  then     t     t    t    create partial joint policy  t t       t  t b b     V      V       V      compute heuristic value      q   ht     Vb  t    i  create child node      QExpand   q        else     QExpand    fully expanded  exists no solution s t  V   h     vCBG       end if     return QExpand  Algorithm    PostProcessNode ICE q  L   Post processing of a node in GMAA  ICE  Input  q the last expanded node  L the open list  Output  q is either removed or updated     L Pop q     if q is fully expanded or depth q    h    then    Cleanup q  delete the node and the associated CBG and Solver     return    else    c  last expanded child of q    q v  c v  update heuristic value of parent node     IsPlaceholder q   true  remember that q is a placeholder     L Insert q   reinsert at appropriate position      end if        Oliehoek  Spaan  Amato    Whiteson  immediate reward is used  This implies that Vb    itself is a lower bound  In the second step V  h    Vb  h     because Vb  h    is admissible  Therefore  we can stop expanding when we find a  with  lower bound  heuristic value equal to the upper bound vCBG   This applies only to the last stage because only then the first step is valid  GMAA  ICE can be implemented by replacing the Expand and the PostProcessNode procedures of Algorithms   and   by Algorithms    and     respectively  Expand ICE first determines if a placeholder is being used and either reuses the previously constructed incremental CBG solver or constructs a new one  Then  new bounds are calculated and the next CBG solution is obtained  Subsequently  only a single child node is generated  rather than expanding all children as in Algorithm      PostProcessNode ICE removes the last node that was returned by Select only when all its children have been expanded  Otherwise  it updates that nodes heuristic value and reinserts it in the open list  See Appendix A   for GMAA  ICE shown as a single algorithm      Theoretical Guarantees In this section  we prove that GMAA  IC and GMAA  ICE are search equivalent  As a direct result we establish that GMAA  ICE is complete  which means that integrating incremental expansion preserves the optimality guarantees of GMAA  IC  Definition     We call two GMAA  variants search equivalent if they select exactly the same sequence of non placeholder nodes corresponding to past joint policies to expand in the search tree using the Select operator  For GMAA  IC and GMAA  ICE we show that the set of selected nodes are the same  However  the set of expanded nodes can be different  in fact  it is precisely these differences that incremental expansion exploits  Theorem    GMAA  ICE and GMAA  IC are search equivalent  Proof  Proof is listed in Section A   of the appendix  Note that Theorem   does not imply that the computational and space requirements of GMAA  ICE and GMAA  IC are identical  On the contrary  for each expansion  GMAA  ICE generates only one child node to be stored on the open list  In contrast  GMAA  IC generates a number of child nodes that is  in the worst case  doubly exponential in the depth of the selected node    However  GMAA  ICE is not guaranteed to be more efficient than GMAA  IC  For example  in the case where all child nodes still have to be generated  GMAA  ICE will be slower due to the overhead it incurs  Corollary    When using a heuristic of the form       GMAA  ICE is complete  Proof  Under the stated conditions  GMAA  IC is complete  see Theorem     GMAA  ICE is search equivalent to GMAA  IC  it is also complete   Since      When a problem allows clustering  the number of child nodes grows less dramatically  see Section            Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs      Incremental CBG Solvers Implementing GMAA  ICE requires a CBG solver that can incrementally deliver all  in descending order of Vb     To this end  we propose to modify the Bayesian game Branch and Bound  BaGaBaB  algorithm  Oliehoek et al          BaGaBaB performs an A  search over  partially specified  CBG policies  Thus  when applied within GMAA  ICE  it performs a second  nested A  search  To expand each node in the GMAA  search tree  a nested A  search computes the next CBG solution    This section briefly summarizes the main ideas behind BaGaBaB  for more information  see Oliehoek et al         and our modifications  BaGaBaB works by creating a search tree in which the nodes correspond to partially specified joint CBG policies  In particular  it represents a  as a joint action vector  a vector h                    i of the joint actions that  specifies for each joint type  Each node g in the BaGaBaB search tree represents a partially specified vector and thus a partially specified joint CBG policy  For example  a completely unspecified vector h            i corresponds to the root node  while an internal node g at depth d  root being at depth    specifies joint actions for the first d joint types g                    d                    The value of a node V  g  is the value of the best joint CBG policy consistent with it  Since this value is not known in advance  BaGaBaB performs an A  search guided by an optimistic heuristic  In particular  we can compute an upper bound on the value achievable for any such partially specified vector by computing the maximum value of the complete information joint policy that is consistent with it  i e   a non admissible joint policy that selects the maximizing joint actions for the remaining joint types   Since this value is a guaranteed upper bound on the maximum value achievable by a consistent joint CBG policy  it is an admissible heuristic  We propose a modification to BaGaBaB to allow solutions to be incrementally delivered  The main idea is to retain the search tree after a first call of BaGaBaB on a particular CBG B t   and update it during subsequent calls  thereby saving computational effort  Standard A  search terminates when a single optimal solution has been found  This behavior is the same when incremental BaGaBaB is called for the first time on a B t    However  during standard A   nodes whose upper bound is lower than the best known lower bound can be safely deleted  as they will never lead to an optimal solution  In contrast  in an incremental setting such nodes cannot be pruned  as they could possibly result in the k th best solution and therefore might need to be expanded during subsequent calls to BaGaBaB  Only nodes returned as solutions are pruned in order to avoid returning the same solution twice  This modification requires more memory but does not affect the A  search process otherwise  When asked it for the k th solution  BaGaBaB resets its internal lower bound to the value of the next best solution that was previously found but not returned  or to vCBG as defined in       if no such solution was found   Then it starts an A  search initialized using the search tree resulting from the  k     th solution  In essence  this method is similar to searching for the best k solutions  where k can be incremented on demand  Recently it was shown that  for fixed k  such a modification preserves all the theoretical guarantees  soundness  completeness      While GMAA  ICE could also use any other incremental CGB solver  there are few that avoid enumerating all  before providing the first result and thus have the potential to work incrementally  An exception may be the method of Kumar and Zilberstein      b   which employs AND OR branch and bound search with the EDAC heuristic  and is thus limited to the two agent case   As a heuristic search method  it may be amenable to an incremental implementation though to our knowledge this has not been attempted         Oliehoek  Spaan  Amato    Whiteson  optimal efficiency  of the A  algorithm  Dechter  Flerova    Marinescu         but the results trivially transfer to the setting where k is allowed to increase      Experiments In this section  we empirically test and validate all the proposed techniques  lossless clustering of joint histories  incremental expansion of search nodes  and hybrid heuristic representations  After introducing the experimental setup  we compare the performance of GMAA  IC and GMAA  ICE to that of GMAA  on a suite of benchmark problems from the literature  Next  we compare the performance of the proposed methods with state of the art optimal and approximate Dec POMDP methods  followed by a case study of the scaling behavior with respect to the number of agents  Finally  we compare memory requirements of the hybrid heuristic representation to those of the tree and vector representations      Experimental Setup The most well known Dec POMDP benchmarks are the Dec Tiger  Nair et al         and BroadcastChannel  Hansen et al         problems  Dec Tiger was discussed extensively in Section    In BroadcastChannel  two agents have to transmit messages over a communication channel  but when both agents transmit at the same time a collision occurs that is noisily observed by the agents  The FireFighting problem models a team of n firefighters that have to extinguish fires in a row of nh houses  Oliehoek  Spaan    Vlassis         Each agent can choose to move to any of the houses to fight fires at that location  if two agents are in the same house  they will completely extinguish any fire there  The  negative  reward of the team of firefighters depends on the intensity of the fire at each house  when all fires have been extinguished  reward of zero is received  In the Hotel   problem  Spaan   Melo         travel agents need to assign customers to hotels with limited capacity  They can also send a customer to a resort but this yields lower reward  In addition  we also use the following problems  Recycling Robots  Amato  Bernstein    Zilberstein         a scaled down version of the problem described in Section    GridSmall with two observations  Amato  Bernstein    Zilberstein        and Cooperative Box Pushing  Seuken   Zilberstein      a   a larger two robot benchmark  Table   summarizes these problems numerically  listing the number of joint policies for different planning horizons  Experiments were run on an Intel Core i  CPU running Linux  and GMAA   GMAA  IC  and GMAA  ICE were implemented in the same code base using the MADP Toolbox  C     Spaan   Oliehoek         The vector based QBG representation is computed using a variation of Incremental Pruning  adapted for computing Q functions instead of regular value functions   corresponding to the NaiveIP method as described by Oliehoek and Spaan         To implement the pruning  we employ Cassandras POMDP solve software  A  R  Cassandra         For the results in Sections     and      we limited each process to  Gb RAM and a maximum CPU time of      s  Reported CPU times are averaged over    independent runs and have a resolution of     s  Timings are given only for the MAA  search processes  since       Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  problem primitives  Dec Tiger  num   for h  n   S    Ai     Oi                             e       e        e    BroadcastChannel                  e       e       e    GridSmall                    e        e         e    Cooperative Box Pushing                    e       e         e      Recycling Robots                  e       e        e    Hotel                     e       e        e      FireFighting                    e       e        e    Table    Benchmark problem sizes and number of joint policies for different horizons  computation of the heuristic is the same for both methods and can be amortized over multiple runs    All problem definitions are available via http   masplan org      Comparing GMAA   GMAA  IC  and GMAA  ICE We compared GMAA   GMAA  IC  and GMAA  ICE using the hybrid QBG representation  While all methods compute an optimal policy  we expect GMAA  IC to be more efficient than GMAA  when lossless clustering is possible  Furthermore  we expect GMAA  ICE to provide further improvements in terms of speedup and scaling to longer planning horizons  The results are shown in Table    For all entries where we report results  the QBG heuristics could be computed  thanks to the hybrid representation  Consequently  the performance of GMAA  IC is much better than all previously reported results  including those of Oliehoek et al          who were often required to resort to QMDP for larger problems and or horizons  The entries marked by  show the limits when using QMDP instead of QBG   in most of these problems we can reach longer horizons with QBG   Only for FireFighting can GMAA  ICE with QMDP compute solutions for higher h than is possible with QBG  hence the missing   and showing that GMAA  ICE is more efficient using a loose heuristic than GMAA  IC   Furthermore  the  entries indicate that the horizon to which we can solve a problem with a tree based QBG representation is often much shorter  These results clearly illustrate that GMAA  IC leads to a significant improvement in performance  In all problems  GMAA  IC was able to produce a solution more quickly and to increase the largest solvable horizon over GMAA   In some cases  GMAA  IC is able to drastically increase the solvable horizon  Furthermore  the results clearly demonstrate that incremental expansion allows for significant additional improvements  In fact  the table demonstrates that GMAA  ICE significantly outperforms GMAA  IC  especially in problems where little clustering is possible  The results in Table   also illustrate the efficacy of a hybrid representation  For problems like GridSmall  Cooperative Box Pushing  FireFighting and Hotel   neither the tree nor vector representation is able to provide a compact QBG heuristic for the longer hori    The heuristics computation time ranges from less than a second to hours  for high h in some difficult problems   Table   presents some heuristic computation time results         Oliehoek  Spaan  Amato    Whiteson  h              V  TGMAA   s  Dec Tiger                                                                    TIC  s   TICE  s   h                                                                                                   FireFighting hnh     nf    i                                                                                                                                   GridSmall                                                                                                                         Hotel                                                                                                                                                                                                                Cooperative Box Pushing                                                                                                                                                    V  TGMAA   s  TIC  s  TICE  s  Recycling Robots                                                                                                                                                                                                                                                                                                                                 BroadcastChannel                                                                                                                                                                                                                                                                                                                                                                                                                                                       Table    Experimental results comparing regular GMAA   GMAA  IC  and GMAA  ICE  Listed are the computation times of GMAA   TGMAA     GMAA  IC  TIC    and GMAA  ICE  TICE    using the hybrid QBG representation  We use the following symbols   memory limit violations   time limit overruns    heuristic computation exceeded memory or time limits   maximum planning horizon using QMDP    maximum planning horizon using tree based QBG   Bold entries indicate that only the methods proposed in this article have computed these results  zons  Apart from Dec Tiger and FireFighting  computing and storing QBG  or another tight heuristic  for longer horizons is the bottleneck to further scalability  Together  these algorithmic improvements lead to the first optimal solutions for many problem horizons  In fact  for the vast majority of problems tested  we provide results for longer horizons than any previous work  the bold entries   These improvements are quite sub      Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  h                                               BGh      cBGt   Dec Tiger                                                                                    FireFighting hnh     nf    i                                                                                                                         GridSmall                                                        Hotel                                                                                              e                                       e                                       e                                       e                                   Cooperative Box Pushing                                 h                                                                         BGh     cBGt   Recycling Robots        remaining stages         remaining stages         remaining stages          remaining stages             remaining stages     e       remaining stages     e        remaining stages     e        remaining stages     e        remaining stages      remaining stages      remaining stages      remaining stages BroadcastChannel        for all t          for all t          for all t           for all t            for all t            for all t              for all t      e        for all t      e        for all t      e        for all t       for all t       for all t       for all t       for all t                                                                Table    Experimental results detailing the effectiveness of clustering  Listed are the size of the CBGs for t   h    without clustering   BGh      and the average CBG size for all stages with clustering   cBGt     stantial  especially given that lengthening the horizon by one increases the problem difficulty exponentially  cf  Table           Analysis of Clustering Histories Table   provides additional details about the performance of GMAA  IC  by listing the number of joint types in the GMAA  IC search   cBGt    for each stage t  These are averages since the algorithm forms CBGs for different past policies  leading to clusterings of different sizes    To see the impact of clustering  the table also lists  BGh     the number of joint types in the CBGs constructed for the last stage without clustering  which is constant  In Dec Tiger  the time needed by GMAA  IC is more than   orders of magnitude less than that of GMAA  for horizon h      For h      this test problem has     e   joint policies  and no other method has been able to optimally solve it  GMAA  IC  however  is able to do so in reasonable time  In Dec Tiger  there are clear symmetries between the     Note that in some problem domains we report smaller clusterings than Oliehoek et al          Due to an implementation mistake  their clustering was overly conservative  and did not in all cases treat two histories as probabilistically equivalent  when in fact they were         Oliehoek  Spaan  Amato    Whiteson  observations that allow for clustering  as demonstrated by Fig     Another key property is that opening the door resets the problem  which may also facilitate clustering  In FireFighting  for short planning horizons no lossless clustering is possible at any stage  and as such  the clustering incurs some overhead  However  GMAA  IC is still faster than GMAA  because constructing the BGs using bootstrapping from the previous CBG takes less time than constructing a CBG from scratch  Interesting counterintuitive results occur for h      which was solved within memory limits  in contrast to h      In fact  using QMDP we could compute optimal values V  for h      and it turns out that these are equal to that for h      The reason is that the optimal joint policy is guaranteed to extinguish all fires in   stages  For subsequent stages  all the rewards will be    While this itself does not influence clustering  the further analysis of Table   reveals that the CBG instances encountered during the h     search happen to cluster much better than those in h      which is possible because the heuristics vary with the horizon  In fact    for h     sends both agents to the middle house at t      while for h      agents are dispatched to different houses  When both agents fight fires at the same house  the fire is extinguished completely  and resulting joint observations do not provide any new information  As a result  different joint types lead to the same joint belief  which means they can be clustered  If agents visit different houses  their observations do convey information  leading to different possible joint beliefs  which cannot be clustered   Hotel   allows for a large amount of clustering  and GMAA  IC outperforms GMAA  by a large margin  with the former reaching h     and the latter h      This problem is transition and observation independent  Becker  Zilberstein  Lesser    Goldman        Nair  Varakantham  Tambe    Yokoo        Varakantham  Marecki  Yabu  Tambe    Yokoo         which facilitates clustering  as we further discuss in Section      Unlike methods specifically designed to exploit transition and observation independence  GMAA  IC exploits this structure without requiring a predefined explicit representation of it  Further scalability is limited by the computation of the heuristic  For BroadcastChannel  GMAA  IC achieves an even more dramatic increase in performance  allowing the solution of up to horizon h        Analysis reveals that the CBGs constructed for all stages are fully clustered  they contain only one type for each agent  The reason is as follows  When constructing a CBG for t      there is only one joint type for the previous CBG so  given      the solution for the previous CBG  there is no uncertainty with respect to the previous joint action a    The crucial property of BroadcastChannel is that the  joint  observation reveals nothing about the new state  but only about what joint action was taken  e g   collision if both agents chose to send   As a result  the different individual histories can be clustered  In a CBG constructed for stage t      there is again only one joint type in the previous game  Therefore  given the past policy  the actions of the other agents can be perfectly predicted  Again the observation conveys no information so this process repeats  Thus  the problem has a special property which could be described as non observable given the past joint policy  GMAA  IC automatically exploits this property  Consequently  the time needed to solve each CBG does not grow with the horizon  The solution time  however  still increases super linearly because of the increased amount of backtracking  As in FireFighting  performance is not monotonic in the planning horizon  In this case however  clustering is clearly not responsible for the difference  Rather  the only explanation is that for certain horizons  there are many near optimal joint policies  leading to more backtracking and a higher search cost        Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs      Nodes at depth t      DecTiger  h    Full Exp  DecTiger  h    Inc  Exp  GridSmall  h    Full Exp  GridSmall  h    Inc  Exp  FireFighting  h    Full Exp  FireFighting  h    Inc  Exp                         t        Figure    Number of expanded partial joint policies t for intermediate stages t             h     in log scale          Analysis of Incremental Expansion In Dec Tiger for h      GMAA  ICE achieves a speedup of three orders of magnitude and can compute a solution for h      unlike GMAA  IC  For GridSmall  it achieves a large speedup for h     and fast solutions for h     and    where GMAA  IC runs out of memory  Similar positive results are obtained for FireFighting  Cooperative Box Pushing and Recycling Robots  In fact  when using QMDP   GMAA  ICE is able to compute solutions well beyond h        for the FireFighting problem  which stands in stark contrast to GMAA  IC that only computes solutions to h     with this heuristic  Note that BroadcastChannel is the only problem for which GMAA  IC is  slightly  faster than GMAA  ICE  Because this problem exhibits clustering to a single joint type  the overhead of incremental expansion does not pay off  To further analyze incremental expansion  we examined its impact on the number of nodes expanded for intermediate stages t             h     Fig    shows the number of nodes expanded in GMAA  ICE and the number that would be expanded for GMAA  IC  which can be easily computed since they are search tree equivalent   There is a clear relationship between the results from Fig    and Table    illustrating  e g   why GMAA  IC runs out of memory on GridSmall h      The plots confirm our hypothesis that  in practice  only a small number of child nodes are queried        Analysis of Hybrid Heuristic Representation Fig    illustrates the memory requirements in terms of number of parameters  i e   real numbers  for the tree  vector  and hybrid representations for QBG   where the latter is computed following Algorithm    Results for the vector representation are omitted when those representations grew beyond limits  The effectiveness of the vector pruning depends on the problem and the complexity of the value function  which can increase suddenly  as for instance happens in Fig   c  These results show that  for several benchmark Dec POMDPs  the hybrid representation allows for significant savings in memory  allowing the computation of tight heuristics for longer horizons        Oliehoek  Spaan  Amato    Whiteson  h  MILP  DP LPC  DP IPG  GMAA  QBG IC  ICE  heur  BroadcastChannel  ICE solvable to h                                                                                                                                         Dec Tiger  ICE                                                                     solvable to h                                                              FireFighting    agents    houses    firelevels   ICE                                                   GridSmall  ICE solvable to h                                                         Recycling Robots  ICE solvable to h                                                                                        Hotel                     ICE solvable to h                                                                       Cooperative Box Pushing                                                             solvable to h                                                                                                                                                                                         QPOMDP    ICE solvable to h                                                                       Table    Comparison of runtimes with other methods  Total time of the GMAA  methods is given by taking the time from the method column  IC or ICE  and adding the heuristic computation time  heur   We use the following symbols   memory limit violations    time limit overruns    heuristic computation exceeded memory or time limits         Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs      Memory required                                    Horizon             a  Dec Tiger                        Horizon             d  Recycling Robots                            Horizon          Tree Vector Hybrid                                         Horizon  Tree Vector Hybrid   c  Hotel         Tree Vector Hybrid  Memory required  Memory required                      b  FireFighting               Tree Vector Hybrid         Memory required  Memory required      Tree Vector Hybrid  Memory required          Tree Vector Hybrid                                         Horizon   e  BroadcastChannel                 Horizon         f  GridSmall   Figure    Hybrid heuristic representation  The y axis shows number of real numbers stored for different representations of QBG for several benchmark problems  in log scale       Comparing to Other Methods In this section  we compare GMAA  IC and GMAA  ICE to other methods from the literature  We begin by comparing the runtimes of our methods against the following state ofthe art optimal Dec POMDP methods  MILP    Aras   Dutech        converts the DecPOMDP to a mixed integer linear program  for which numerous solvers are available  We have used MOSEK version      DP LPC    Boularias   Chaib draa        performs dynamic programming with lossless policy compression  with CPLEX      as the LP solver  DP IPG  Amato et al         performs exact dynamic programing with incremental policy     The results reported here deviate from those reported by Aras and Dutech         For a number of problems  Aras et al  employed a solution method that solves the MILP as a series  a tree  of smaller MILPs by branching on the continuous realization weight variables for earlier stages  That is  for each past joint policy t for some stage t  they solve a different MILP involving the subset of consistent sequences  Additionally  for FireFighting and GridSmall  we use the benchmark versions standard to the literature  Oliehoek  Spaan    Vlassis        Amato et al          whereas Aras and Dutech        use non standard versions  This explains the difference between our results and the ones reported in their article  personal communication  Raghav Aras       The goal of Boularias and Chaib draa        was to find non dominated joint policies for all initial beliefs  The previously reported results concerned run time to compute the non dominated joint policies  without performing pruning on the full length joint policies  In contrast  we report the time needed to compute the actual optimal Dec POMDP policy  given b     This additionally requires the final round of pruning and subsequently computing the value for each of the remaining joint policies for the initial belief  This additional overhead explains the differences in run time between what we report here and what was previously reported  personal communication  Abdeslam Boularias          Oliehoek  Spaan  Amato    Whiteson  Problem Dec Tiger Cooperative Box Pushing GridSmall  h        m        VMBDP                  V                   Table    Comparison of optimal  V    and approximate  VMBDP   values   generation that exploits known start state and knowledge about what states are reachable in doing the DP backup  Table    which shows the results of the comparison  demonstrates that  in almost all cases  the total time of GMAA  ICE  given by the sum of heuristic computation time and the time for the GMAA  phase  is significantly less than that of any other state of the art methods  Moreover  as demonstrated in Table    GMAA  ICE can compute solutions for longer horizons for all these problems  except for Cooperative Box Pushing and Hotel      For these problems  it is not possible to compute QBG for longer horizons  Overcoming this problem could enable GMAA  ICE to scale to further horizons as well  The DP LPC algorithm proposed by Boularias and Chaib draa        also improves the efficiency of optimal solutions by a form of compression  The performance of their algorithm  however  is weaker than that of GMAA  IC  There are two main explanations for the performance difference  First  DP LPC uses compression to more compactly represent the values for sets of useful sub tree policies  by using sequence form representation  The policies themselves  however  are not compressed  they still specify actions for every possible observation history  for each policy it needs to select an exponential amount of sequences that make up that policy   Hence  it cannot compute solutions for long horizons  Second  GMAA  IC can exploit knowledge of the initial state distribution b    Overall  GMAA  ICE substantially improves the state of the art in optimally solving Dec POMDPs  Previous methods typically improved the feasible solution horizon by just one  or only provided speed ups for horizons that could already be solved   By contrast  GMAA  ICE dramatically extends the feasible solution horizon for many problems  We also consider MBDP based approaches  the leading family of approximate algorithms  Table    which reports the VMBDP values produced by PBIP IPG  Amato et al          with typical maxTrees parameter setting m   demonstrates that the optimal solutions produced by GMAA  IC or GMAA  ICE are of higher quality  PBIP IPG was chosen because all other MBDP algorithms with the same parameters achieve at most the same value  While not exhaustive  this comparison illustrates that even the best approximate Dec POMDP methods in practice provide inferior joint policies on some problems  Conducting such analysis is possible only if optimal solutions can be computed  Clearly  the more data that becomes available  the more thorough the comparisons that can be made  Therefore  scalable optimal solution methods such as GMAA  ICE are critical for improving these analyses        Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  problem primitives  num   for h  n   S    A    O                                 e       e                          e        e                        e       e        e                        e       e        e                         e       e        e     Table    FireFightingGraph  the number of joint policies for different numbers of agents and horizons  with   possible fire levels      Scaling to More Agents All of the benchmark problems in our results presented so far were limited to two agents  Here  we present a case study on FireFightingGraph  Oliehoek  Spaan  Whiteson    Vlassis         a variation of FireFighting allowing for more agents  in which each agent can only fight fires at two houses  instead of at all of them  Table   highlights the size of these problems  including the total number of joint policies for different horizons  We compared GMAA   GMAA  IC  GMAA  ICE  all using a QMDP heuristic   BruteForceSearch  and DP IPG  with a maximum run time of    hours and running on an Intel Core i  CPU  averaged over    runs  BruteForceSearch is a simple optimal algorithm that enumerates and evaluates all joint policies  and was implemented in the same codebase as the GMAA  variations  DP IPG results use the original implementation and were run on an Intel Xeon computer  Hence  while the timing results are not directly comparable  the overall trends are apparent  Also  since the DP IPG implementation is limited to   agents  no results are shown for more agents  Fig     shows the computation times for FireFightingGraph across different numbers of of agents and planning horizons  while Table   lists the optimal values obtained  As expected  the baseline BruteForceSearch performs very poorly  only scaling beyond h     for   agents  while DP IPG can only reach h      On the other hand  regular GMAA  performs relatively well  scaling to a maximum of   agents  However  GMAA  IC and GMAA  ICE improve the efficiency of GMAA  by    orders of magnitude  As such  they substantially outperform the other three methods  and scale up to   agents  The benefit of incremental expansion is clear for n        where GMAA  ICE can reach a higher horizon than GMAA  IC  Hence  although this article focuses on scalability in the horizon  these results show that the methods we propose can also improve scalability in the number of agents      Discussion Overall  the empirical results demonstrate that incremental clustering and expansion offers dramatic performance gains on a diverse set of problems  In addition  the results on Broad    In Hotel    DP IPG performs particularly well because the problem structure has limited reachability  That is  each agent can fully observe its local state  but not that of the other agent  and in all local states except one there is one action that dominates all others  As a result  DP IPG can generate a small number of possibly optimal policies         Oliehoek  Spaan  Amato    Whiteson                                                                                                               computation time  s   computation time  s   computation time  s                                                                    h   agents                                                                                        h   agents   a  GMAA  results                                  h   agents   b  GMAA  IC results       c  GMAA  ICE results                                                                                                          computation time  s      computation time  s                                                         h   agents                                          h   agents   d  BruteForceSearch results    e  DP IPG results   Figure     Comparison of GMAA   GMAA  IC  GMAA  ICE  BruteForceSearch  and DP IPG on the FireFightingGraph problem  Shown are computation time  in log scale  for various number of agents and horizons  Missing bars indicate that the method exceeded time or memory limits  However  the DP IPG implementation only supports   agents   h            n                                                 n                               n                               n             n             Table    Value V  of optimal solutions to the FireFightingGraph problem  for different horizons and numbers of agents   castChannel illustrate a key advantage of our approach  when a problem possesses a property that makes a large amount of clustering possible  our clustering method exploits this property automatically  without requiring a predefined explicit representation of it        Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  Of course  not all problems admit great reductions via clustering  One domain property that allows for clustering is when the past joint policy encountered during GMAA  makes the observations superfluous  as with BroadcastChannel and FireFighting  In Dec Tiger  we see that certain symmetries can lead to clustering  However clustering can occur even without these properties  In fact  for all problems and nearly all horizons that we tested  the size of the CBGs can be reduced  Moreover  in accordance with the analysis of Section      the improvements in planning efficiency are huge  even for modest reductions in CBG size  One class of problems where we can say something a priori about the amount of clustering that is possible is the class of Dec POMDPs with transition and observation independence  Becker et al          In such problems  the agents have local states and the transitions are independent  which for two agents can be expressed as Pr s    s   s    s    a    a      Pr s   s    a    Pr s   s    a             Similarly  the observations are assumed to be independent  which means that for each agent the observation probability depends only on its own action and local state  Pr oi  ai   si    For such problems  the probabilistic equivalence criterion       factors too  In particular  due to transition and observation independence           holds true for any  ia   ib   Moreover        factors as the product Pr s    s               Pr s        Pr s        and thus holds if Pr s     a     Pr s     b    That is  two histories can be clustered if they induce the same local belief  As such  the size of the CBGs directly corresponds to the product of the number of reachable local beliefs  Since the transition and observation independent Hotel   problem is also locally fully observable  and the local state spaces consist of four states  there are only four possible local beliefs  which is consistent with the CBG size of    from Table     Moreover  we see that this maximum size is typically only reached at the end of search  This is because good policies defer sending customers to the hotel and thus do not visit local states where the hotel is filled in the earlier stages  In more general classes of problems  even other weakly coupled models  e g   Becker  Zilberstein    Lesser        Witwicki   Durfee         the criterion       does not factor  and hence there is no direct correspondence to the number of local beliefs  As such  only by applying our clustering algorithm can we determine how well such a problem clusters  This is analogous to  e g   state aggregation in MDPs  e g   discussed in Givan  Dean    Greig        where it is not known how to predict a priori how large a minimized model will be  Fortunately  our empirical results demonstrate that  in domains that admit little or no clustering  the overhead is small  As expected  incremental expansion is most helpful for problems which do not allow for much clustering  However  the results for  e g   Dec Tiger illustrate that there is a limit to the amount of scaling that the method can currently provide  The bottleneck is the solution of the large CBGs for the later stages  the CBG solver has to solve these large CBGs when returning the first solution in order to guarantee optimality  but this takes takes a long time  We expect that further improvements to CBG solvers can directly add to the efficacy of incremental expansion  Our experiments also clearly demonstrate that the Dec POMDP complexity results  while important  are only worst case results  In fact  the scalability demonstrated in our experiments clearly show that in many problems we successfully scale dramatically beyond what would be     This assumes no external state variable s           Oliehoek  Spaan  Amato    Whiteson  expected for a doubly exponential dependence on the horizon  Even for the smallest problems  a doubly exponential scaling in the horizon implies that it is impossible to compute solutions beyond h     at all  as indicated by the following simple calculation  let n       Ai       actions   Oi        observations  then     Ai   n  Oi             Ai   n  Oi                e    Thus  even in the simplest possible case  we see an increase of a factor       e   from h     to h      Similarly  the next increment  from h     to h      increases the size of the search space by a factor       e    However  our experiments clearly indicate that in almost all cases  things are not so dire  That is  even though matters look bleak in the light of the complexity results  we are in many cases able to perform substantially better than this worst case      Related Work In this section  we discuss a number of methods that are related to those proposed in this article  Some of these methods have already been discussed in earlier sections  In Section    we indicated that our clustering method is closely related to the approach of Emery Montemerlo et al         but is also fundamentally different because our method is lossless  In Section      we discussed connections to the approach of Boularias and Chaib draa        which clusters policy values  This contrasts with our approach which clusters the histories and thus the policies themselves  leading to greater scalability  In Section        we discussed the relationship between our notion of probabilistic equivalence  PE  and the multiagent belief  However  there is yet another notion of belief  employed in the JESP solution method  Nair et al          that is superficially more similar to the PE distribution  A JESP belief for an AOH  i is a probability distribution Pr s  o  i   i   b       i   over states and observation histories of other agents given a  deterministic  full policy of all the other agents  It is a sufficient statistic  since it induces a multiagent belief  thus it also allows for the clustering of histories  The crucial difference with  and the utility of  PE lies in the fact that the PE criterion is specified over states and AOHs given only a past joint policy  That is        does not induce a multiagent belief  Our clustering approach also resembles a number of methods that employ other equivalence notions  First  several approaches exploit the notion of behavioral equivalence  Pynadath   Marsella        Zeng et al         Zeng   Doshi         They consider  from the perspective of a protagonist agent i  the possible models of another agent j  Since j affects i only through its actions  i e   its behavior  agent i can cluster together all the models of agent j that lead to the same policy j for that agent  That is  it can cluster all models of agent j that are behaviorally equivalent  In contrast  we do not cluster models of other agents j  but histories of this agent i if all the other agents  as well as the environment  are guaranteed to behave the same in expectation  thus leading to the same best response of agent i  That is  our method could be seen as clustering histories that are expected environmental behavior equivalent  The notion of utility equivalence  Pynadath   Marsella        Zeng et al         is closer to PE because it also takes into account the  value of the  best response of agent i  in particular  it clusters two models mj and mj if using BR mj  the best response against mj  achieves the same value against mj    However  it remains a form of behavior equivalence in that it clusters models of other agents  not histories of the protagonist agent        Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  There are also connections between PE and work on influence based abstraction  Becker et al         Witwicki   Durfee        Witwicki        Oliehoek et al          since the influence  or point in parameter space  Becker et al         is a compact representation of the other agents policies  Models of the other agents can be clustered if they lead to the same influence on agent i  However  though more fine grained  this is ultimately still a form of behavioral equivalence  A final relation to our equivalence notion is the work by Dekel  Fudenberg  and Morris         which constructs a distance measure and topology on the space of types with the goal of approximating the infinite universal type space  the space of all possible beliefs about beliefs about beliefs  etc   for one shot Bayesian games  Our setting  however  considers a simple finite type space where the types directly correspond to the private histories  in the form of AOHs  in a sequential problem  Thus  we do not need to approximate the universal type space  instead we want to know which histories lead to the same future dynamics from the perspective of an agent  Dekel et al s topology does not address this question  Our incremental expansion technique is related to approaches extending A to deal with large branching factors in the context of multiple sequence alignment  Ikeda   Imai        Yoshizumi  Miura    Ishida         However  our approach is different because we do not discard unpromising nodes but rather provide a mechanism to generate only the necessary ones  Also  when proposing MAA   Szer et al         developed a superficially similar approach that could be applied only to the last stage  In particular  they proposed generating the child nodes one by one  each time checking if a child is found with value equal to its parents heuristic value  Since the value of such a child specifies a full policy  its value is a lower bound and therefore expansion of any remaining child nodes can be skipped  Unfortunately  a number of issues prevent this approach from providing substantial leverage in practice  First  it cannot be applied to intermediate stages    t   h    since no lower bound values for the expanded children are available  Second  in many problems it is unlikely that such a child node exists  Third  even if it does  Szer et al  did not specify an efficient way of finding it  Incremental expansion overcomes all of these issues  yielding an approach that  as our experiments demonstrate  significantly increases the size of the Dec POMDPs that can be solved optimally  This article focuses on optimal solutions for Dec POMDPs over a finite horizon  As part of our evaluation  we compare against the MILP approach  Aras   Dutech         DPILP  Boularias   Chaib draa        and DP IPG  Amato et al          an extension of the exact dynamic programming algorithm  Hansen et al          Research on finite horizon DecPOMDPs has considered many other approaches such as bounded approximations  Amato  Carlin    Zilberstein         locally optimal solutions  Nair et al         Varakantham  Nair  Tambe    Yokoo        and approximate methods without guarantees  Seuken   Zilberstein      b      a  Carlin   Zilberstein        Eker   Akn        Oliehoek  Kooi    Vlassis        Dibangoye et al         Kumar   Zilberstein      b  Wu et al       a  Wu  Zilberstein    Chen      b   In particular  much research has considered the optimal and or approximate solution of subclasses of Dec POMDPs  One such subclass contains only Dec POMDPs in which the agents have local states that other agents cannot influence  The resulting models  such as the TOI Dec MDP  Becker et al         Dibangoye  Amato  Doniec    Charpillet        and NDPOMDP  Nair et al         Varakantham et al         Marecki  Gupta  Varakantham  Tambe    Yokoo        Kumar   Zilberstein         can be interpreted as independent  PO MDPs for       Oliehoek  Spaan  Amato    Whiteson  each agent that are coupled through the reward function  and possibly an unaffectable state feature   On the other hand  event driven interaction models  Becker et al         consider agents that have individual rewards but can influence each others transitions  More recently  models that allow for limited transition and reward dependence have been introduced  Examples are interaction driven Markov games  Spaan   Melo         DecMDPs with sparse interactions  Melo   Veloso         distributed POMDPs with coordination locales  Varakantham et al         Velagapudi et al          event driven interactions with complex rewards  EDI CR   Mostafa   Lesser         and transition decoupled Dec POMDPs  Witwicki   Durfee        Witwicki         While the methods developed for these models often exhibit better scaling behavior than methods for standard Dec  PO MDPs  they typically are not suitable when agents have extended interactions  e g   to collaborate in transporting an item  Also  there have been specialized models that consider the timing of actions whose ordering is already determined  Marecki   Tambe        Beynier   Mouaddib         Another body of work addresses infinite horizon problems  Amato  Bernstein    Zilberstein        Amato  Bonet    Zilberstein        Bernstein  Amato  Hansen    Zilberstein        Kumar   Zilberstein      a  Pajarinen   Peltonen         in which it is not possible to represent a policy as a tree  These approaches represent policies using finite state controllers that are then optimized in various ways  Also  since the infinite horizon case is undecidable  Bernstein et al          the approaches are approximate or optimal given a particular controller size  While there exists a boundedly optimal approach that can theoretically construct a controller within any o of optimal  it is only feasible for very small problems or a large o  Bernstein et al          There has also been great interest in Dec POMDPs that explicitly take into account communication  Some approaches try to optimize the meaning of communication actions without semantics  Xuan  Lesser    Zilberstein        Goldman   Zilberstein        Spaan  Gordon    Vlassis        Goldman  Allen    Zilberstein        while others use fixed semantics  e g   broadcasting the local observations   Ooi   Wornell        Pynadath   Tambe        Nair et al         Roth et al         Oliehoek  Spaan    Vlassis        Roth  Simmons    Veloso        Spaan  Oliehoek    Vlassis        Goldman   Zilberstein        Becker  Carlin  Lesser    Zilberstein        Williamson  Gerding    Jennings        Wu et al          Since models used in the first category  e g   the Dec POMDP Com  can be converted to normal Dec POMDPs  Seuken   Zilberstein         the contributions of this article are applicable to those settings  Finally  there are numerous models closely related to Dec POMDPs  such as POSGs  Hansen et al          interactive POMDPs  I POMDPs   Gmytrasiewicz   Doshi         and their graphical counterparts  Doshi  Zeng    Chen         These models are more general in the sense that they consider self interested settings where each agent has an individual reward function  I POMDPs are conjectured to also require doubly exponential time  Seuken   Zilberstein         However  for the I POMDP there have been a number of recent advances  Doshi   Gmytrasiewicz         The current paper makes a clear link between best response equivalence of histories and the notion of best response equivalence of beliefs in I POMDPs  In particular  this article demonstrates that two PE action observation histories  AOHs  induce  given only a past joint policy  a distribution over states and AOHs of other agents  and therefore will induce the same multiagent belief for any future policies of other agents  These induced multiagent beliefs  in turn  can be interpreted as special cases of I POMDP beliefs where the model of the other agents are sub intentional models in the form of a fixed policy tree  Rabinovich and Rosenschein        introduced a method that  rather than optimizing       Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  the expected value of a joint policy  selects coordinated actions under uncertainty by tracking the dynamics of an environment  This approach  however  requires a model of the ideal system dynamics as input and in many problems  such as those considered in this article  identifying such dynamics is difficult      Future Work Several avenues for future work are made possible by the research presented in this article  Perhaps the most promising is the development of new approximate Dec POMDP algorithms  While this article focused on optimal methods  GMAA  ICE can also be seen as a framework for approximate methods  Such methods could be derived by limiting the amount of backtracking  employing approximate CBG solvers  Emery Montemerlo  Gordon  Schneider    Thrun        Kumar   Zilberstein      b  Wu et al       a   integrating GMAA  methods for factored Dec POMDPs  Oliehoek  Spaan  Whiteson    Vlassis        Oliehoek        Oliehoek et al          performing lossy clustering  Emery Montemerlo        Wu et al         or using bounded approximations for the heuristics  In particular  it seems promising to combine approximate clustering with approximate factored GMAA  methods  Lossy clustering could be achieved by generalizing the probabilistic equivalence criterion  which is currently so strict that little or no clustering may be possible in many problems  An obvious approach is to cluster histories for which the distributions over states and histories of other agents are merely similar  as measured by  e g   Kullback Leibler divergence  Alternately  histories could be clustered if they induce the same individual belief over states  Pr s  i      X  Pr s      i   i                 i  While individual beliefs are not sufficient statistics for history  we hypothesize that they constitute effective metrics for approximate clustering  Since the individual belief simply marginalizes out the other agents histories from the probabilities used in the probabilistic equivalence criterion  it is an intuitive heuristic metric for approximate clustering  While this article focuses on increasing scalability with respect to the horizon  developing techniques to deal with larger number of agents is an important direction of future work  We plan to further explore performing GMAA  using factored representations  Oliehoek  Spaan  Whiteson    Vlassis         In that previous work  we could only exploit the factorization at the last stage  since earlier stages required full expansions to guarantee optimality  However  for such larger problems  the number of joint BG policies  i e   number of child nodes  is directly very large  earlier stages are more tightly coupled   therefore incremental expansion is crucial to improving the scalability of optimal solution methods with respect to the number of agents  Another avenue for future work is to further generalize GMAA  ICE  In particular  it may be possible to flatten the two nested A searches into a single A search  Doing so could lead to significant savings as it would obviate the need to solve an entire CBG before expanding the next one  In our work  we employed the plain A algorithm as a basis  but a promising direction of future work is to investigate what A enhancements from the literature  Edelkamp   Schrodl        can benefit GMAA  most  In particular  as we described in our experiments  different past joint policies can lead to CBGs of different sizes  One idea       Oliehoek  Spaan  Amato    Whiteson  is to first expand parts of the search tree that lead to small CBGs  by biasing the selection operator  but not the pruning operator  so as to maintain optimality   Yet another important direction for future work is the development of tighter heuristics  Though few researchers are addressing this topic  the results presented in this article underscore how important such heuristics are for solving larger problems  Currently  the heuristic is the bottleneck in four out of the seven problems we considered  Moreover  two of the problems where this is not the bottleneck can already be solved for long  h       horizons  Therefore  we believe that computing tight heuristics for longer horizons is the single most important research direction for further improving the scalability of optimal Dec POMDP solution methods with respect to the horizon  A different direction is to employ our theoretical results on clustering beyond the DecPOMDP setting to develop new solution methods for CBGs  For instance  a well known method for computing a local optimum is alternating maximization  AM   starting from an arbitrary joint policy  compute a best response for some agent given that other agents keep their policies fixed and then select another agents policy to improve  etc  One idea is to start with a completely clustered CBG  where all agents types are clustered together and thus a random joint CBG policy has a simple form  each agent just selects a single action  Only when improving the policy of an agent do we consider all its actual possible types to compute its best response  Subsequently  we cluster together all types for which that agent selects the same action and proceed to the next agent  In addition  since our clustering results are not restricted to the collaborative setting  it may also be possible to employ them  using a similar approach  to develop new solution methods for general payoff BGs  Finally  two of our other contributions can have a significant impact beyond the problem of optimally solving Dec POMDPs  First  the idea of incrementally expanding nodes introduced in GMAA  ICE can be applied in other A search methods  Incremental expansion is most useful when children can be generated in order of decreasing heuristic value without prohibitive computational effort  and in problems with a large branching factor such as multiple sequence alignment problems in computational biology  Carrillo   Lipman        Ikeda   Imai         Second  representing PWLC value functions as a hybrid of a tree and a set of vectors can have wider impact as well  e g   in online search for POMDPs  Ross  Pineau  Paquet    Chaib draa             Conclusions This article presented a set of methods that advance the state of the art in optimal solution methods for Dec POMDPs  In particular  we presented several advances that aim to extend the horizon over which optimal solutions can be found  These advances build off the GMAA  heuristic search approach and include lossless incremental clustering of the CBGs solved by GMAA   incremental expansion of nodes in the GMAA  search tree  and hybrid heuristic representations  We provided theoretical guarantees that  when a suitable heuristic is used  both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent  Finally  we presented extensive empirical results demonstrating that GMAA  ICE can optimally solve Dec POMDPs of unprecedented size  We significanty increase the planning horizons that can be tackledin some cases by more than an order of magnitude  Given that an increase of the horizon by one results in an exponentially larger search space  this constitutes a very large improvement  Moreover  our techniques also im      Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  prove scalability with respect to the number of agents  leading to the first ever solutions of general Dec POMDPs with more than three agents  These results also demonstrated how optimal techniques can yield new insights about particular Dec POMDPs  as incremental clustering revealed properties of BroadcastChannel that make it much easier to solve  In addition to facilitating optimal solutions  we hope these advances will inspire new principled approximation methods  as incremental clustering has already done  Wu et al          and enable them to be meaningfully benchmarked   Acknowledgments We thank Raghav Aras and Abdeslam Boularias for making their code available to us  Research supported in part by AFOSR MURI project  FA               and in part by NWO CATCH project               M S  is funded by the FP  Marie Curie Actions Individual Fellowship          FP  PEOPLE      IEF    Appendix A  Appendix A   Auxiliary algorithms Algorithm    implements the BestJointPolicyAndValue function  which prunes all child nodes that are not fully specified  Algorithm    generates all children of a particular CBG  Algorithm    BestJointPolicyAndValue QExpand    Prune fully expanded nodes from a set of nodes QExpand returning only the best one and its value  Input  QExpand a set of nodes for fully specified joint policies  Output  the best full joint policy in the input set and its value     v        for q  QExpand do    QExpand  Remove q     h  vi  q    if v   v  then    v  v          end if    end for     return h    v  i  A   Detailed GMAA  ICE algorithm The complete GMAA  ICE algorithm is shown in Algorithm     A   Computation of V     t   t   The quantity V     t   t   is defined recursively via  V     t   t     V     t   t      Est    t   R st    t     t       b    t          A      Oliehoek  Spaan  Amato    Whiteson  Algorithm    GenerateAllChildrenForCBG B t     Input  CBG B t    Output  QExpand the set containing all expanded child nodes for this CBG     QExpand        for all jointP CBG policies  for B do    Vb      Pr  u         t     t    t    create partial joint policy  t t       t  t b b    V      V       V      compute heuristic value     q   ht     Vb  t    i  create child node     QExpand  Insert q       end for    return QExpand  The expectation is taken with respect to the joint probability distribution over states and joint AOHs that is induced by t   X Pr st    t  b   t     Pr ot  at   st   Pr st  st   at    Pr at   t    t    Pr st     t   b   t    st  S   A    Here    t      t   at   ot   and Pr at   t    t    is the probability that t specifies at  for AOH   t   which is   or   in case of deterministic past joint policy t    A   Proofs Proof of Theorem   Substituting       in       yields X b   t   t    t    Vb      Vb   t     Pr   t  b   t  Q    t     X   t      Pr   t  b   t   Est  R st   t    t        t     E t    Vb    t         t    t    t       Est   t  R st   t    t     b    t     E t    Vb    t       b    t    t     Est   t  R st   t    t     b    t     E t    Q    t          t        b    t      t    t      Est   t  R st   t    t     b    t     H  t     h   t       where H  is an optimal admissible heuristic  Substituting this into       we obtain Vb  t      t    t      V     t   t     Est   t  R st   t    t     b    t     E t    Vb    t       b    t    t    V     t   t     Est   t  R st   t    t      b    t       H  t     h   t       via  A       V     t  t       H  t     h   t      which demonstrates that the heuristic value Vb  t   used by GMAA  via CBGs using heuristic of a form       is admissible  as it is lower bounded by the actual value for the first t plus an admissible heuristic  Since it performs heuristic search with this admissible heuristic  this algorithm is also complete        Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs  Algorithm    GMAA  ICE                                                                                                                                                                                     vGM AA         v    q    h    vi LIE   q     repeat q  Select LIE    q   ht   vi  IE L  pop q  if IsPlaceholder q  then B t    t  CBG  reuse stored CBG  else  Construct extended BG and solver   B t     t   CBG  note t    t     t     t  t t  B     ConstructExtendedBG B       B t    ClusterBG B t    B t   Solver  CreateSolver B t    t  CBG  B t   end if  Expand a single child   vCBG   vGM AA  V      t    t   vCBG     if last stage t   h    then vCBG   Vb  h     V      h    h    end if h t   Vb   t  i  B t   Solver NextSolution vCBG  vCBG   if not  t then  fully expanded  no solution s t  V   h     vCBG   delete q  and its CBG   solver  continue   i e  goto line     end if t     t    t   Vb  t      V     t   t     Vb   t   if last stage t   h    then  Note that    t     V      Vb  t       if V      vGM AA then vGM AA  V     found new lower bound     LIE  prune vGM AA   end if delete q  and its CBG   solver  else q   ht     Vb  t    i LIE  insert q    q  ht   Vb  t    i   Update parent node q  which now is a placeholder   LIE  insert q  end if until LIE is empty        Oliehoek  Spaan  Amato    Whiteson  Proof of Lemma   t t   t t   and   Proof  Assume an arbitrary ati  ot      i  at  i  ot      i      i      i  s   i     We have that t      a t t t Pr st         i  ot   i  i  ai     i   X t t t   t t t      i   ia t   Pr ot     Pr st    st  ati  at  i   Pr at  i       i   t  i   Pr st      i  o  i  ai  a  i  s st     X  t t t   t t t      i   ib t     Pr st    st  ati  at  i   Pr at  i       i   t  i   Pr st    Pr ot   i  o  i  ai  a  i  s  st  t     b t t t   Pr st         i  ot   i  i  ai     i   t    Because we assumed an arbitrary st         i  ot   i   we have that st     t    ot     i  i  t   t     t   t     b t t t   a t t t     i  oi  i  ai     i   Pr st         i  ot   i  i  ai     i     Pr s   A     In general we have that t    Pr s  t    t   t       i   it  ati  ot   i     i       t t t Pr st         i  ot   i  i  ai     i   Pr ot      t  at   t   i     i  i    i  t    t t t Pr st         i  ot   i  i  ai     i   P t   t   t t t t         t   Pr s   i  oi  i  ai     i   t     s     i  Now  because of  A     both the numerator and denominator are the same when substituting  ia t   ib t in this equation  Consequently  we can conclude that t   t t     t     b t t t   t     i  i  ai  oi     i   Pr st         i   ia t  ati  ot   i     i     Pr s t    t t     and   Finally  because ati   ot      i were all arbitrarily chosen  we can conclude that i      i  s       holds   Proof of Lemma   Proof  Assume an arbitrary    i  s and    i   then we have bi  s    i   ia      i     Pr s    i   ia      i  b    X   Pr s    i       i   ia     i  b         i   factoring the joint distribution      X  Pr s      i   ia     i  b    Pr    i  s      i    ia     i  b          i        Incremental Clustering and Expansion for Faster Optimal Planning in Dec POMDPs      i only depends on      i     i      X  Pr s      i   ia     i  b    Pr    i       i     i      s      i only depend on   i      X  Pr s      i   ia    i  b    Pr    i       i     i     due to PE     X  Pr s      i   ib    i  b    Pr    i       i     i         i       i       i            Pr s    i   ib     i  b      bi  s    i   ib      i   We can conclude this holds for all    i  s and    i   Proof of Theorem    Search Equivalence  To prove search equivalence  we explicitly write a node as a tuple q   ht   v  PHi  where t is the past joint policy  v the nodes heuristic value  and PH a boolean indicating whether it is a placeholder  We consider the equivalence of the maintained open lists  The open list L maintained by GMAA  IC contains only non expanded nodes q  In contrast  the open list LIE of GMAA  ICE contains both non expanded nodes q and placeholders  previously expanded nodes   q  We denote the ordered subset of LIE containing non expanded nodes with Q and that containing placeholders with Q  We treat these open lists as ordered sets of heuristic values and their associated nodes  Definition     L and LIE are equivalent  L  LIE if     Q  L     The qs have the same ordering  L remove L   Q    Q       Nodes q in L but not Q have a placeholder q that is the parent of and higher ranked than q  q ht  vq  falsei L Q   q ht   vq  trueiQ s t   t    t       q   q       There are no other placeholders  Fig     illustrates two equivalent lists in which the past joint policies are indexed with letters  Note that the placeholders in LIE are ranked higher than the nodes in L that they represent  Let us write IT IC L  and IT ICE LIE   for one iteration  i e   one loop of the main repeat in Algorithm    of the respective algorithms  Let IT ICE  denote the operation that repeats IT ICE as long as a placeholder was selected  so it ends when a q is expanded   Lemma    If L  LIE   then executing IT IC L  and IT ICE  LIE   leads to new open lists that are again equivalent  L   LIE   Proof  When IT ICE  selects a placeholder q  it generates child q  that was already present in L  due to properties   and   of Definition     and inserts it  Insertion occurs at the same relative location as IT IC because both algorithms use the same comparison operator  Definition     Together these facts guarantee that the insertion preserves properties   and        A remove B  removes the elements of B from A without changing As ordering         Oliehoek  Spaan  Amato    Whiteson  LIE  L Q Vb  t           c d e                 f g h i j  Q  Vb  t     d       f g  Vb       t a  b    placeholder for  c e j     same nodes  same position   o  placeholder for  h i  consistent ordering for equal values  Figure     Illustration of equivalent lists  Past joint policies are indexed by letters  In this example  a and b have been expanded earlier  but are not yet fully expanded in the ICE case   If there are remaining unexpanded children of q  IT ICE  reinserts q with an updated heuristic value q v  q   v that is guaranteed to be an upper bound on the value of unexpanded siblings q  since q   v   Vb  q      Vb  q       q   v  preserving properties   and     When IT ICE  finally selects a non placeholder q  it is guaranteed to be the same q as selected by IT IC  due to properties   and     Expansion in ICE generates one child q   again inserted at the same relative location as in IC  and inserts placeholder q   hq   q   v  truei for the other siblings q   again preserving properties   and     Proof of Theorem    The fact that GMAA  ICE and GMAA  IC are search equivalent follows directly from Lemma    Search equivalence means that both algorithms select the same non placeholders q to expand  Since both algorithms begin with identical  and therefore trivially equivalent  open lists  they maintain equivalent open lists throughout search  As such  property   of Definition    ensures that every time IT ICE  selects a non placeholder  IT IC selects it too   
  Online  sample based planning algorithms for POMDPs have shown great promise in scaling to problems with large state spaces  but they become intractable for large action and observation spaces  This is particularly problematic in multiagent POMDPs where the action and observation space grows exponentially with the number of agents  To combat this intractability  we propose a novel scalable approach based on samplebased planning and factored value functions that exploits structure present in many multiagent settings  This approach applies not only in the planning case  but also in the Bayesian reinforcement learning setting  Experimental results show that we are able to provide high quality solutions to large multiagent planning and learning problems      Introduction  Online planning methods for POMDPs have demonstrated impressive performance  Ross et al         on large problems by interleaving planning with action selection  The leading such method  partially observable Monte Carlo planning  POMCP   Silver and Veness         achieves performance gains by extending sample based methods based on Monte Carlo tree search  MCTS  to solve POMDPs  While online  sample based methods show promise in solving POMDPs with large state spaces  they become intractable as the number of actions or observations grow  This is particularly problematic in the case of multiagent systems  Specifically  we consider multiagent partially observable Markov decision processes  MPOMDPs   which assume all agents share the same partially observable view of the world and can coordinate their actions  Because the MPOMDP model is centralized  POMDP methods apply  but the fact that the number of joint actions and observations scales exponentially in the number of agents renders current POMDP methods intractable  To combat this intractability  we provide a novel sample based online planning algorithm that exploits multiagent structure  Our method  called factored value partially observable Monte Carlo planning  FV POMCP   is based on POMCP and is the first MCTS method that exploits locality of interaction  in many MASs  agents interact directly with a subset of other agents  This structure enables a decomposition of the value function into a set of overlapping factors  which can be used to produce high quality solutions  Guestrin  Koller  and Parr        Nair et al         Kok and Vlassis         But unlike these previous approaches  we will not assume a factored model  but only that the value function can be approximately factored  We present two variants of FVPOMCP that use different amounts of factorization of the value function to scale to large action and observation spaces  Not only is our FV POMCP approach applicable to large MPOMDPs  but it is potentially even more important for Bayesian learning where the agents have uncertainty about the underlying model as modeled by Bayes Adaptive POMDPs  BA POMDPs   Ross et al          These models translate the learning problem to a planning problem  but since the resulting planning problems have an infinite number of states  scalable sample based planning approaches are critical to their solution       We show experimentally that our approach allows both planning and learning to be significantly more efficient in multiagent POMDPs  This evaluation shows that our approach significantly outperforms regular  non factored  POMCP  indicating that FV POMCP is able to effectively exploit locality of interaction in both settings      Background  We first discuss multiagent POMDPs and previous work on Monte Carlo tree search and Bayesian reinforcement learning  BRL  for POMDPs        Multiagent POMDPs  An MPOMDP  Messias  Spaan  and Lima        is a multiagent planning model that unfolds over a number of steps  At every stage  agents take individual actions and receive individual observations  However  in an MPOMDP  all individual observations are shared via communication  allowing the team of agents to act in a centralized manner  We will restrict ourselves to the setting where such communication is free of noise  costs and delays  An MPOMDP is a tuple hI  S   Ai    T  R   Zi    O  hi with  I  a set of agents  S  a set of states with designated initial state distribution b    A   i Ai   the set of joint actions  using action sets for each agent  i    T   a set of state transition probabilities  T s as   Pr s   s   a   the probability of transitioning from state s to s  when actions  a are taken by the agents  R  a reward function  R s   a   the immediate reward for being in state s and taking actions  a  Z   i Zi   the set of joint observations  using observation sets for each agent  i  O  a set   of observation probabilities  O as  z   Pr  z  a s     the probability of seeing observations  o given actions  a were taken and resulting state s    h  the horizon  An MPOMDP can be reduced to a POMDP with a single centralized controller that takes joint actions and receives joint observations  Pynadath and Tambe         Therefore  MPOMDPs can be solved with POMDP solution methods  some of which will be described in the remainder of this section  However  such approaches do not exploit the particular structure inherent to many MASs  In Sec     we present a first online planning method that overcomes this deficiency        Monte Carlo Tree Search for  M POMDPs  Most research for  mutliagent  POMDPs has focused on planning  given a full specification of the model  determine an optimal policy    mapping past observation histories  which can be summarized by distributions b s  over states policy can be extracted from an optimal Q value function  P called beliefs  P to actions  An optimal       Q b a    s R s a    z P  z b a  maxa Q b  a    by acting greedily  Computing Q b a   however  is complicated by the fact that the space of beliefs is continuous  POMCP  Silver and Veness         is a scalable method which extends Monte Carlo tree search  MCTS  to solve POMDPs  At every stage  the algorithm performs online planning  given the current belief  by incrementally building a lookahead tree that contains  statistics that represent  Q b a   The algorithm  however  avoids expensive belief updates by creating nodes not for each belief  but simply for each action observation history h  In particular  it samples hidden states s only at the root node  called root sampling  and uses that state to sample a trajectory that first traverses the lookahead tree and then performs a  random  rollout  The return of this trajectory is used to update the statistics for all visited nodes  Because this search tree can be enormous  the search is directed topthe relevant parts by selecting actions to maximize the upper confidence bounds  U  h a    Q h  a    c log N      n  Here  N is the number of times the history has been reached and n is the number of times that action a has been taken in that history  POMCP can be shown to converge to an   optimal value function  Moreover  the method has demonstrated good performance in large domains with a limited number of simulations            Bayesian RL for  M POMDPs  Reinforcement learning  RL  considers the more realistic case where the model is not  perfectly  known in advance  Unfortunately  effective RL in POMDPs is very difficult  Ross et al         introduced a framework  called the Bayes Adaptive POMDP  BA POMDP   that reduces the learning problem to a planning problem  thus enabling advances in planning methods to be used in the learning problem  In particular  the BA POMDP utilizes Dirichlet distributions to model uncertainty over transitions and observations  Intuitively  if the agent could observe states and observations  it could maintain vectors  and  of counts for transitions and observations respectively  Let ass  be the transition count of the number times state s  resulted from taking action a in state s and sa  z be the observation count representing the number of times observation z was seen after taking action a and transitioning to state s    These counts induce a probability distribution over the possible transition and observation models  Even though the agent cannot observe the states and has uncertainty about the actual count vectors  this uncertainty can be represented using the POMDP formalism  by including the count vectors as part of the hidden state of a special POMDP  called a BA POMDP  The BA POMDP can be extended to the multiagent setting  Amato and Oliehoek         yielding the BayesAdaptive multiagent POMDP  BA MPOMDP  framework  BA MPOMDPs are POMDPs  but with an infinite state space since there can be infinitely many count vectors  While a quality bounded reduction to a finite state space is possible  Ross et al          the problem is still intractable  sample based planning is needed to provide solutions  Unfortunately  current methods  such as POMCP  do not scale well to multiple agents      Exploiting Graphical Structure  POMCP is not directly suitable for multiagent problems  in either the planning or learning setting  due to the fact that the number of joint actions and observations are exponential in the number of agents  We first elaborate on these problems  and then sketch an approach to mitigate them by exploiting locality between agents        POMCP for MPOMDPs  Bottlenecks  The large number of joint observations is problematic since it leads to a lookahead tree with very high branching factor  Even though this is theoretically not a problem in MDPs  Kearns  Mansour  and Ng         in partially observable settings that use particle filters it leads to severe problems  In particular  in order to have a good particle representation at the next time step  the actual joint observation received must be sampled often enough during planning for the previous stage  If the actual joint observation had not been sampled frequently enough  or not at all   the particle filter will be a bad approximation  or collapse   This results in sampling starting from the initial belief again  or alternatively  to fall back to acting using a separate  history independent  policy such as a random one  The issue of large numbers of joint actions is also problematic  the standard POMCP algorithm will  at each node  maintain separate statistics  and thus separate upper confidence bounds  for each of the exponentially many joint actions  Each of the exponentially many joint actions will have to be selected at least a few times to reduce their confidence bounds  i e   exploration bonus   This is a principled problem  in cases where each combination of individual actions may lead to completely different effects  it may be necessary to try all of them at least a few times  In many cases  however  the effect of a joint action is factorizable as the effects of the action of individual agents or small groups of agents  For instance  consider a team of agents that is fighting fires at a number of burning houses  as illustrated in Fig    a   The rewards depend only on the amount of water deposited on each house rather than the exact joint action taken  Oliehoek et al          While this problem lends itself to a natural factorization  other problems may also be factorized to permit approximation        Coordination Graphs  In certain multiagent settings  coordination  hyper  graphs  CGs   Guestrin  Koller  and Parr        Nair et al         have been used to compactly represents interactions between subsets of agents  In this paper we extend      a    b    c   Figure     a  Illustration of fire fighting  b  Coordination graph with   houses and   agents  c  Illustration of a sensor network problem on a grid that is used in the experiments  this approach to MPOMDPs  We first introduce the framework of CGs in the single shot setting  A CG specifies a set of payoff components E    Qe    and each component e is associated with a subset of agents  These subsets  which we also denote using e  can be interpreted as  hyper  edges in a graph where the nodes are agents  P The goal in a CG is to select a joint action that maximizes the sum of the local payoff components Q  a    e Qe   ae     A CG for the fire fighting problem is shown in Fig    b   We follow the cited literature in assuming that a suitable factorization is easily identifiable by the designer  but it may also be learnable  Even if a payoff function Q  a  does not factor exactly  it can be approximated by a CG  For the moment assuming a stateless problem  we will consider the case where histories are included in the next section   an action value function can be approximated by X Q  a   Qe   ae        e  We refer to this as the linear approximation of Q  since one can show that this corresponds to an instance of linear regression  See Sec      P Using a factored representation  the maximization max a e Qe   ae   can be performed efficiently via variable elimination  VE   Guestrin  Koller  and Parr         or max sum  Kok and Vlassis         These algorithms are not exponential in the number of agents  and therefore enable significant speed ups for larger number of agents  The VE algorithm  which we use in the experiments  is exponential in the induced width w of the coordination graph        Mixture of Experts Optimization  VE can be applied if the CG is given in advance  When we try to exploit these techniques in the context of POMCP  however  this is not Pthe case  As such  the task we consider here is to find the maximum of an estimated factored function Q  a    e Qe   ae    Note that we do not necessarily require the best approximation to the entire Q  as in      Instead  we seek an estimation Q for which the maximizing joint action  aM is close to the maximum of the actual  but unknown  Q value  Q  aM    Q  a    For this purpose  we introduce a technique called mixture of experts optimization  In contrast to methods based on linear approximation      we do not try to learn a best fit factored Q function  but directly try to estimate the maximizing joint action  The main idea is that for each local action  ae we introduce an expert that predicts the total value Q  ae     E Q  a     ae    For a joint action  these responsesone of each payoff component e P are put in a mixture with weights e and used to predict the maximization joint action  arg max a e e Q  ae    This equation is the sum of restricted scope functions  which is identical to the case of linear approximation      so VE can be used to perform this maximization effectively  In the remainder of this paper  we will integrate the weights and simply write Qe   ae     e Q  ae      Since we focus on the one shot setting here  the Q values in the remainder of this section should be interpreted as those for one specific joint history h  i e   Q  a   Q h  a         a     a     o     o     o     a    o    Figure    Factored Statistics  joint histories are maintained  for specific joint actions and observations specified by  a j and  o k    but action statistics are factored at each node  The experts themselves are implemented as maximum likelihood estimators of the total value  That is  each expert  associated with a particular  ae   keeps track of the mean payoff received when  ae was performed  which can be done very efficiently  An additional benefit of this approach is that it allows for efficient estimation of upper confidence bounds by also keeping track of how often this local action was performed  which in turns facilitates easy integration in POMCP  as we describe next      Factored Value POMCP  This section presents our main algorithmic contribution  Factored Value POMCP  which is an online planning method for POMDPs that can exploit approximate structure in the value function by applying mixture of experts optimization in the POMCP lookahead search tree  We introduce two variants of FV POMCP  The first technique  factored statistics  only addresses the complexity introduced by joint actions  The second technique  factored trees  additionally addresses the problem of many joint observations  FV POMCP is the first MCTS method to exploit structure in MASs  achieving better sample complexity by using factorization to generalize the value function over joint actions and histories  While this method was developed to scale POMCP to larger MPOMDPs in terms of number of agents  the techniques may be beneficial in other multiagent models and other factored POMDPs        Factored Statistics  We first introduce Factored Statistics which directly applies mixture of experts optimization to each node in the POMCP search tree  As shown in Fig     the tree of joint histories remains the same  but the statistics retained at for each history is now different  That is  rather than maintaining one set of statistics in each node  i e  joint history  h  for the expected value of each joint action Q  h   a   we maintain a set of statistic for each component e that estimates the values Qe   h   ae   and corresponding upper confidence bounds  Joint actions are selected according to the maximum  factored  upper confidence bound  X max Ue   h   ae      a  e  p Where Ue   h   ae     Qe   h   ae     c log N h      n ae using the Q value and the exploration bonus added for that factor  For implementation  at each node for a joint history  h  we store the count for the full history N h as well as the Q values  Qe   and the counts for actions  n ae   separately for each component e  Since this method retains fewer statistics and performs joint action selection more efficiently via VE  we expect that it will be more efficient than plain application of POMCP to the BA MPOMDP  However  the complexity due to joint observations is not directly addressed  because joint histories are used  reuse of nodes           a     o     e       a   E    a     o      o   E    o      a   E    o   E    a      a   E    o      o   E    o   E   Figure    Factored Trees  local histories for are maintained for each factor  resulting in factored history and action statistics   Actions and observations for component i are represented as  a ji and  o ki   and creation of new nodes for all possible histories  including the one that will be realized  may be limited if the number of joint observations is large        Factored Trees  The second technique  called Factored Trees  additionally tries to overcome the large number of joint observations  It further decomposes the local Qe s by splitting joint histories into local histories and distributing them over the factors  That is  in this case  we introduce an expert for each local  he    ae pair  During simulations  the agents know  h and action selection is conducted by maximizing over the sum of the upper confidence bounds  X max Ue   he    ae      a  e  q where Ue   he    ae     Qe   he    ae     c log N he      n ae   We assume that the set of agents with relevant actions and histories for component Qe are the same  but this can be generalized  This approach further reduces the number of statistics maintained and increases the reuse of nodes in MCTS and the chance that nodes in the trees will exist for observations that are seen during execution  As such  it can increase performance by increasing generalization as well as producing more robust particle filters  This type of factorization has a major effect on the implementation  rather than constructing a single tree  we now construct a number of trees in parallel  one for each factor e as shown in Fig     A node of the tree for component e now stores the required statistics  N he   the count for the local history  n ae   the counts for actions taken in the local tree and Qe for the tree  Finally  we point out that this decentralization of statistics has the potential to reduce communication since the components statistics in a decentralized fashion could be updated without knowledge of all observation histories      Theoretical Analysis  Here  we investigate the approximation quality induced by our factorization techniques   The most desirable quality bounds would express the performance relative to optimal  i e   relative to flat POMCP  which converges in probability an   optimal value function  Even for the one shot case  this is extremely difficult for any method employing factorization based on linear approximation of Q  because Equation     corresponds to aPspecial case of P linear regression  In this case  we can write     in terms of basis functions and weights as  Q    a     a   where the he  ae are the basis functions  he  ae   a      iff  a specifies  ae for ae he   ae    e e e e   ae we   component e  and   otherwise   As such  providing guarantees with respect to the optimal Q  a  value would   Proofs  can be found in Appendix B       require developing a priori bounds for the approximation quality of  a particular type of  basis functions  This is a very difficult problem for which there is no good solution  even though these methods are widely studied in machine learning  However  we do not expect our methods to perform well on arbitrary Q  Instead  we expect them to perform well when Q is nearly factored  such that     approximately holds  since then the local actions contain enough information to make good predictions  As such  we analyze the behavior of our methods when the samples of Q come from a factored function  i e   as in       contaminated with zero mean noise  In such cases  we can show the following  Theorem    The estimate Q of Q made by a mixture of experts converges in probability to the true value plus p a sample policy dependent bias term  Q  a   Q  a    B    a   The bias is given by a sum of biases induced by pairs e e    XX X     ae   e   ae  Qe    ae   e   ae  e    B    a    e e    e   a  e   e  Here   ae  e is the action of the agents that participate both in e and e   specified by  a  and  ae   e are the actions of agents in e  that are not in e  Because we observe the global reward for a given set of actions  the bias is caused by correlations in the sampling policy and the fact that we are overcounting value from other components  When there is no overlap  and the sampling policy we use is component wise      ae   e   ae         ae   e   a e         ae   e    this over counting is the same for all local actions  ae   Theorem    When value components do not overlap and a component wise sampling policy is used  mixture of experts optimization recovers the maximizing joint action  Similar reasoning can be used to establish bounds on the performance in the case of overlapping components  subject to assumptions about properties of the true value function  Let N  e  denote the neighborhood of component e  the set of other components e  which have an overlap with e  those that have at least one agent participating in them that also participates in e   Theorem    If for all overlapping components e e    and any two intersection action profiles  ae  e   a e  e for their intersection  the true value function satisfies  ae   e  Qe    ae   e   ae  e    Qe    ae   e   a e  e           e   e        ae   e    E    N  e     A         e   e   the number of intersection action profiles  then mixture of experts optimization  in the limit  will with  A return a joint action whose value lies within   of the optimal solution  The analysis shows that a sufficiently local Q function can be effectively optimized when using a sufficiently local sampling policy  Under the same assumptions  we can also derive guarantees for the sequential case  It is not directly possible to derive bounds for FV POMCP itself  since it is not possible to demonstrate that the UCB exploration policy is component wise   but it seems likely that UCB exploration leads to an effective policy that nearly satisfies this property  Moreover  since bias is introduced by the interaction between action correlations and differences in non local components  even when using a policy with correlations  the bias may be limited if the Q function is sufficiently structured  In the factored tree case  we can introduce a strong result  Because histories for other agents outside the factor are not included and we do not assume independence between factors  the approximation quality may suffer  where  h is Markov  this is not the case for the local history  he   As such  the expected return for such a local history depends on the future policy as well as the past one  via the distribution over histories of agents not included in e   This implies that convergence is no longer guaranteed  Proposition    Factored Trees FV POMCP may diverge       Proof  FT FV POMCP  with c      corresponds to a general case of Monte Carlo control  i e   SARSA     with linear function approximation that is greedy w r t  the current value function  Such settings may result in divergence  Fairbank and Alonso         Even though this is a negative result  and there is no guarantee of convergence for FT FV POMCP  in practice this need not be a problem  many reinforcement learning techniques that can diverge  e g   neural networks  can produce high quality results in practice  e g    Tesauro        Stone and Sutton         Therefore  we expect that if the problem exhibits enough locality  the factored trees approximation may allow good quality policies to be found very quickly  Finally  we analyze the computational complexity  FV POMCP is implemented by modifying POMCPs S IMULATE function  as described in Appendix A   The maximization is performed by variable elimination  which has complexity O n Amax  w   with w the induced width and  Amax   the size of the largest action set  In addition  the algorithm updates each of the  E  components  bringing the total complexity of one call of simulate to O  E    n Amax  w        Experimental Results  Here  we empirically investigate the effectiveness of our factorization methods by comparing them to nonfactored methods in the planning and learning settings  Experimental Setup  We test our methods on versions of the firefighting problem from Section   and on sensor network problems  In the firefighting problems  fires are suppressed more quickly if a larger number of agents choose that particular house  Fires also spread to neighbors houses and can start at any house with a small probability  In the sensor network problems  as shown by Fig    c    sensors were aligned along discrete intervals on two axes with rewards for tracking a target that moves in a grid  Two types of sensing could be employed by each agent  one more powerful than the other  but using more energy  or the agent could do nothing  A higher reward was given for two agents correctly sensing a target at the same time  The firefighting problems were broken up into n    overlapping factors with   agents in each  representing the agents on the two sides of a house  and the sensor grid problems were broken into n   factors with n       agents in each  representing all agents along the y axis and one agent along the x axis   For the firefighting problem with   agents   S          A       and  Z       and with    agents   S             A          and  Z          For the sensor network problems with   agents   S        A       and  Z       and with   agents   S         A         and  Z         Each experiment was run for a given number of simulations  the number of samples used at each step to choose an action  and averaged over a number of episodes  We report undiscounted return with the standard error  Experiments were run on a single core of a     GHz machine with  GB of memory  In both cases  we compare our factored representations to the flat version using POMCP  This comparison uses the same code base so it directly shows the difference when using factorization  POMCP and similar sample based planning methods have already been shown to be state of the art methods in both POMDP planning  Silver and Veness        and learning  Ross et al          MPOMDPs  We start by comparing the factored statistics  FS  and factored tree  FT  versions of FV POMCP in multiagent planning problems  Here  the agents are given the true MPOMDP model  in the form of a simulator  and use it to plan  For this setting  we compare to two baseline methods  POMCP  regular POMCP applied to the MPOMDP  and random  uniform random action selection  Note that while POMCP will converge to an   optional solution  the solution quality may be poor when using small number of simulations  The results for   agent and    agent firefighting problems with horizon    are shown in Figure   a   For the   agent problem  POMCP performs poorly with a few simulations  but as the number of simulations increases it outperforms the other methods  presumably converging to an optimal solution   FT provides a high quality solution with a very small number of simulations  but the resulting value plateaus due to approximation error  FS also provides a high quality solution with a very small number of simulations  but is then able to converge to a solution that is near POMCP  In the    agent problem  POMCP is only able to generate a solution that is slightly better than random while the FV POMCP methods are able to perform much better  In fact  FT performs                                                 POMCP  true                                                          FS  true                                                   FT  true                             Random                                            FS  true   Firefighting    Agents  FT  true                               FS  true            FT  true                                                                                                              No learning           Random                                                                                               Random                                                                                                      POMCP        err sims                   Sensor Grid    agents        Sensor Grid  Horizon           Value  Value  Value  Value                                                       POMCP  true                                   Random  Random                                      Number of Simulations POMCP  true  FS  true         FT  true   Random                                 FS        FT              Number of Simulations BA POMCP     POMCP  true        Sensor Grid  Horizon           POMCP  true   POMCP  true        Value        Value  FT  true         No learning  Firefighting  Horizon     Firefighting  Horizon            Random Number of Simulations POMCP  true  FS  true   FT  true       a  MPOMDP results  Value                                                                                                                                                                                                          Number of Simulations POMCP  true  FS  true                                                        Firefighting     Agents                                                                                               Random BA POMCP                                                                  FT                  errors                                                         FS       Number of Simulations POMCP  true   POMCP  true           POMCP  true         Random  Random               FS            Number of Simulations  FT  BA POMCP              FS                 Number of Simulations  FT  BA POMCP  Random                      FS                       Number of Simulations FT  BA POMCP   b  BA MPOMDP results  Figure    Results for  a  the planning  MPOMDP  case  log scale x axis  and  b  the learning  BA MPOMDP  case for the firefighting and sensor grid problems  very well with a small number of samples and FS continues to improve until it reaches solution that is similar to FT  Similar results are seen in the sensor grid problem  POMCP outperforms a random policy as the number of simulations grows  but FS and FT produce much higher values with  the available simulations  FT seems to converge to a low quality solution  in both planning and learning  due to the loss of information about the   targets previous position that is no longer known to local factors  In this problem  POMCP requires over    minutes for an episode of       simulations  making reducing the number of simulations crucial in problems of this size  These results clearly illustrate the benefit of FV POMCP by exploiting structure for planning in MASs  BA MPOMDPs  We also investigate the learning setting  i e   when the agents are only given the BA POMDP model   Here  at the end of each episode  both the state and count vectors are reset to their initial values  Learning in partially observable environments is extremely hard  and there may be many equivalence classes of transition and observation models that are indistinguishable when learning  Therefore  we assume a reasonably good model of the transitions  e g   because the designer may have a good idea of the dynamics   but only a poor estimate of the observation model  because the sensors may be harder to model   For the BRL setting  we compare to the following baseline methods  POMCP  regular POMCP applied to the true model using         simulations  this is the best proxy for  and we expect this to be very close to  the optimal value   and BA POMCP  regular POMCP applied to the BA POMDP  Results for a four agent instance of the fire fighting problem are shown in Fig    b   for h           In both cases  the FS and FT variants approach the POMCP value  For a small number of simulations FT learns very quickly  providing significantly better values than the flat methods and better than FS for the increased horizon  FS learns more slowly  but the value is better as the number of simulations increases  as seen in the horizon    case  due to the use of the full history  After more simulations in the horizon    problem  the performance of the flat model  BA MPOMDP  improves  but the factored methods still outperform it and this increase is less           visible for the longer horizon problem  Similar results are again seen in the four agent sensor grid problem  FT performs the best with a small number of simulations  but as the number increases  FS outperforms other methods  Again  for these problems  BA POMCP requires over    minutes for each episode for the largest number of simulations  showing the need for more efficient methods  These experiments show that even in challenging multiagent settings with state uncertainty  BRL methods can learn by effectively exploiting structure      Related Work  MCTS methods have become very popular in games  a type of multiagent setting  but no action factorization has been exploited so far  Browne et al          Progressive widening  Coulom        and double progressive widening  Couetoux et al         have had some success in games with large  or continuous  action spaces  The progressive widening methods do not use the structure of the coordination graph in order to generalize value over actions  but instead must find the correct joint action out of the exponentially many that are available  which may require many trajectories   They are also designed for fully observable scenarios  so they do not address the large observation space in MPOMDPs  The factorization of the history in FTs is not unlike the use of linear function approximation for the state components in TD Search  Silver  Sutton  and Muller         However  in contrast to that method  due to our particular factorization  we can still apply UCT to aggressively search down the most promising branches of the tree  While other methods based on Q learning  Guestrin  Lagoudakis  and Parr        Kok and Vlassis        exploit action factorization  they assume agents observe individual rewards  rather than the global reward that we consider  and it is not clear how these could be incorporated in a UCT style algorithm  Locality of interaction has also been considered previously in decentralized POMDP methods  Oliehoek        Amato et al         in the form of factored Dec POMDPs  Oliehoek  Whiteson  and Spaan        Pajarinen and Peltonen        and networked distributed POMDPs  ND POMDPs   Nair et al         Kumar and Zilberstein        Dibangoye et al          These models make strict assumptions about the information that the agents can use to choose actions  only the past history of individual actions and observations   thereby significantly lowering the resulting value  Oliehoek  Spaan  and Vlassis         ND POMDPs also impose additional assumptions on the model  transition and observation independence and a factored reward function   The MPOMDP model  in contrast  does not impose these restrictions  Instead  in MPOMDPs  each agent knows the joint action observation history  so there are not different perspectives by different agents  Therefore     factored Dec POMDP and ND POMDP methods do not apply to MPOMDPs  they specify mappings from individual histories to actions  rather than joint histories to joint actions      ND POMDP methods assume that the value function is exactly factored as the sum of local values  perfect locality of interaction  while in an MPOMDP  the value is only approximately factored  since different components can correlate due to conditioning the actions on central information   While perfect locality of interaction allows a natural factorization of the MPOMDP value function  but our method can be applied to any MPOMDP  i e   given any factorization of the value function   Furthermore  current factored Dec POMDP and ND POMDP models generate solutions given the model in an offline fashion  while we consider online methods using a simulator in this paper  Our approach builds upon coordination graphs  Guestrin  Koller  and Parr         to perform the joint action optimization efficiently  but factorization in one shot problems has been considered in other settings too  Amin et al   Amin  Kearns  and Syed        present a method to optimize graphical bandits  which relates to our optimization approach  Since their approach replaced the UCB functionality  it is not obvious how their approach could be integrated in POMCP  Moreover  their work  focuses on minimizing regret  which is not an issue in our case   and does not apply when the factorization does not hold  Oliehoek et al   Oliehoek  Whiteson  and Spaan        present an factored payoff approach that extends coordination graphs to imperfect information settings where each agent has its own knowledge  This is not relevant for our current algorithm  which assumes that joint observations will be received by a centralized decision maker  but could potentially be useful to relax this assumption           Conclusions  We presented the first method to exploit multiagent structure to produce a scalable method for Monte Carlo tree search for POMDPs  This approach formalizes a team of agents as a multiagent POMDP  allowing planning and BRL techniques from the POMDP literature to be applied  However  since the number of joint actions and observations grows exponentially with the number of agents  nave extensions of single agent methods will not scale well  To combat this problem  we introduced FV POMCP  an online planner based on POMCP  Silver and Veness        that exploits multiagent structure using two novel techniquesfactored statistics and factored trees to reduce    the number of joint actions and    the number of joint histories considered  Our empirical results demonstrate that FV POMCP greatly increases scalability of online planning for MPOMDPs  solving problems with    agents  Further investigation also shows scalability to the much more complex learning problem with four agents  Our methods could also be used to solve POMDPs and BA POMDPs with large action and observation spaces as well the recent Bayes Adaptive extension  Ng et al         of the self interested I POMDP model  Gmytrasiewicz and Doshi          Acknowledgments F O  is funded by NWO Innovational Research Incentives Scheme Veni               C A was supported by AFOSR MURI project  FA               and ONR MURI project  N               
 Coordination of distributed agents is required for problems arising in many areas  including multi robot systems  networking and e commerce  As a formal framework for such problems  we use the decentralized partially observable Markov decision process  DECPOMDP   Though much work has been done on optimal dynamic programming algorithms for the single agent version of the problem  optimal algorithms for the multiagent case have been elusive  The main contribution of this paper is an optimal policy iteration algorithm for solving DEC POMDPs  The algorithm uses stochastic finite state controllers to represent policies  The solution can include a correlation device  which allows agents to correlate their actions without communicating  This approach alternates between expanding the controller and performing value preserving transformations  which modify the controller without sacrificing value  We present two efficient value preserving transformations  one can reduce the size of the controller and the other can improve its value while keeping the size fixed  Empirical results demonstrate the usefulness of value preserving transformations in increasing value while keeping controller size to a minimum  To broaden the applicability of the approach  we also present a heuristic version of the policy iteration algorithm  which sacrifices convergence to optimality  This algorithm further reduces the size of the controllers at each step by assuming that probability distributions over the other agents actions are known  While this assumption may not hold in general  it helps produce higher quality solutions in our test problems      Introduction Markov decision processes  MDPs  provide a useful framework for solving problems of sequential decision making under uncertainty  In some settings  agents must base their decisions on partial information about the system state  In that case  it is often better to use the more general framework of partially observable Markov decision processes  POMDPs   Even more general are problems in which a team of decision makers  each with its own c      AI Access Foundation and Morgan Kaufmann Publishers  All rights reserved    Bernstein  Amato  Hansen    Zilberstein  local observations  must act together  Domains in which these types of problems arise include networking  multi robot coordination  e commerce  and space exploration systems  The decentralized partially observable Markov decision process  DEC POMDP  provides an effective framework to model such problems  Though this model has been recognized for decades  Witsenhausen         there has been little work on provably optimal algorithms for it  On the other hand  POMDPs have been studied extensively over the past few decades  Smallwood   Sondik        Simmons   Koenig        Cassandra  Littman    Zhang        Hansen      a  Bonet   Geffner        Poupart   Boutilier        Feng   Zilberstein        Smith   Simmons        Smith  Thompson    Wettergreen         It is well known that a POMDP can be reformulated as an equivalent belief state MDP  A belief state MDP cannot be solved in a straightforward way using MDP methods because it has a continuous state space  However  Smallwood and Sondik showed how to implement value iteration by exploiting the piecewise linearity and convexity of the value function  This work opened the door for many algorithms  including approximate approaches and policy iteration algorithms in which the policy is represented using a finite state controller  Extending dynamic programming for POMDPs to the multiagent case is not straightforward  For one thing  it is not clear how to define a belief state and consequently form a belief state MDP  With multiple agents  each agent has uncertainty about the observations and beliefs of the other agents  Furthermore  the finite horizon DEC POMDP problem with just two agents is complete for a higher complexity class than the single agent version  Bernstein  Givan  Immerman    Zilberstein         indicating that these are fundamentally different problems  In this paper  we describe an extension of the policy iteration algorithm for single agent POMDPs to the multiagent case  As in the single agent case  our algorithm converges in the limit  and thus serves as the first nontrivial optimal algorithm for infinite horizon DEC POMDPs  A few optimal approaches  Hansen  Bernstein    Zilberstein        Szer  Charpillet    Zilberstein        and several approximate algorithms have been developed for finite horizon DEC POMDPs  Peshkin  Kim  Meuleau    Kaelbling        Nair  Pynadath  Yokoo  Tambe    Marsella        Emery Montemerlo  Gordon  Schnieder    Thrun        Seuken   Zilberstein         but only locally optimal algorithms have been proposed for the infinite horizon case  Bernstein  Hansen    Zilberstein        Szer   Charpillet        Amato  Bernstein    Zilberstein         In our algorithmic framework  policies are represented using stochastic finite state controllers  A simple way to implement this is to give each agent its own local controller  In this case  the agents policies are all independent  A more general class of policies includes those which allow agents to share a common source of randomness without sharing observations  We define this class formally  using a shared source of randomness called a correlation device  The use of correlated stochastic policies in the DEC POMDP context is novel  The importance of correlation has been recognized in the game theory community  Aumann         but there has been little work on algorithms for finding correlated policies  Each iteration of the algorithm consists of two phases  These are exhaustive backups  which add nodes to the controller  and value preserving transformations  which change the controller without sacrificing value  We first provide a novel exposition of existing single     Policy Iteration for DEC POMDPs  agent algorithms using this two phase view  and then we go on to describe the multiagent extension  There are many possibilities for value preserving transformations  In this paper  we describe two different types  both of which can be performed efficiently using linear programming  The first type allows us to remove nodes from the controller  and the second allows us to improve the value of the controller while keeping its size fixed  Our empirical results demonstrate the usefulness of value preserving transformations in obtaining high values while keeping controller size to a minimum  We note that this work serves to unify and generalize previous work on dynamic programming for DEC POMDPs  The first algorithm for the finite horizon case  Hansen et al         can be extended to the infinite horizon case and viewed as interleaving exhaustive backups and controller reductions  The bounded policy iteration algorithm for DEC POMDPs  Bernstein et al          which extends a POMDP algorithm proposed by Poupart and Boutilier         can be viewed through the lens of our framework as repeated application of a specific value preserving transformation  Because the optimal algorithm will not usually be able to return an optimal solution in practice  we also introduce a heuristic version of the policy iteration algorithm  This approach makes use of initial state information to focus policy search and further reduces controller size at each step  To accomplish this  a forward search from the initial state distribution is used to construct a set of belief points an agent would visit assuming the other agents use given fixed policies  This search is conducted for each agent and then policy iteration takes place while using the belief points to guide the removal of controller nodes  The assumption that other agents use fixed policies causes the algorithm to no longer be optimal  but it performs well in practice  We show that more concise and higher valued solutions can be produced compared to the optimal method before resources are exhausted  The remainder of the paper is organized as follows  Section   introduces the formal models of sequential decision making  Section   contains a novel presentation of existing dynamic programming algorithms for POMDPs  In section    we present an extension of policy iteration for POMDPs to the DEC POMDP case  along with a convergence proof  We discuss the heuristic version of policy iteration in section    followed by experiments using policy iteration and heuristic policy iteration in section    Finally  section   contains the conclusion and a discussion of possible future work      Formal Model of Distributed Decision Making We begin with a description of the formal framework upon which our work is based  This framework extends the well known Markov decision process to allow for distributed policy execution  We also define an optimal solution for this model and discuss two different representations for these solutions      Decentralized POMDPs A decentralized partially observable Markov decision process  DEC POMDP  is defined for  T  R      Oi  where mally as a tuple hI  S  A   I is a finite set of agents       Bernstein  Amato  Hansen    Zilberstein  Agent Agent a  s  r System  a   Agent a  System  o  r System  o   r  a   o   r Agent   a    b    c   Figure     a  Markov decision process   b  Partially observable Markov decision process   c  Decentralized partially observable Markov decision process with two agents    S is a finite set of states  with distinguished initial state s        iI Ai is a set of joint actions  where Ai is the set of actions for agent i   A    S is the state transition function  defining the distributions of states  T   SA that result from starting in a given state and each agent performing an action       is the reward function for the set of agents for each set of joint actions  R   S A and each state      iI i is a set of joint observations  where i contains observations for agent i       S     is an observation function  defining the distributions of observations  O A for the set of agents that result from each agent performing an action and ending in a given state  The special case of a DEC POMDP in which there is only one agent is called a partially observable Markov decision process  POMDP   In this paper  we consider the case in which the process unfolds over an infinite sequence of stages  At each stage  all agents simultaneously select an action  and each receives the global reward based on the reward function and a local observation based on the observation function  Thus  the transitions  rewards and observations depend on the actions of all agents  but each agent must act based only on local observations  This is illustrated in Figure    The objective of the agents is to maximize the expected discounted sum of rewards that are received  thus it is a cooperative framework  We denote the discount factor  and require that          In a DEC POMDP  the decisions of each agent affect all the agents in the domain  but due to the decentralized nature of the model each agent must choose actions based solely on local information  Because each agent receives a separate observation that does not usually provide sufficient information to efficiently reason about the other agents  solving a DECPOMDP optimally becomes very difficult  For example  each agent may receive a different      Policy Iteration for DEC POMDPs   a    b   Figure    A set of horizon three policy trees  a  and two node stochastic controllers  b  for a two agent DEC POMDP   piece of information that does not allow a common state estimate or any estimate of the other agents decisions to be calculated  These single estimates are crucial in single agent problems  as they allow the agents history to be summarize concisely  but they are not generally available in DEC POMDPs  This is seen in the complexity of the finite horizon problem with at least two agents  which is NEXP complete  Bernstein et al         and thus in practice may require double exponential time  Like the infinite horizon POMDP  optimally solving an infinite horizon DEC POMDP is undecidable as it may require infinite resources  but our method is able to provide a solution within   of the optimal with finite time and memory  Nevertheless  introducing multiple decentralized agents causes a DECPOMDP to be significantly more difficult than a single agent POMDP      Solution Representations A local policy for an agent is a mapping from local action and observation histories to actions while a joint policy is a set of policies  one for each agent in the problem  As mentioned above  an optimal solution for a DEC POMDP is the joint policy that maximizes the expected sum of rewards that are received over the finite or infinite steps of the problem  In infinite horizon problems  the rewards are discounted to maintain a finite sum  Thus  an optimal solution is a joint policy that provides the highest value starting at the given initial state of the problem  For finite horizon problems  local policies can be represented using a policy tree as seen in Figure  a  Actions are represented by the arrows or stop figures  where each agent can move in the given direction or stay where it is  and observations are labeled wl and wr for seeing a wall on the left or the right respectively  Using this representation  an agent takes the action defined at the root node and then after seeing an observation  chooses the next action that is defined by the respective branch  This continues until the action at a leaf node is executed  For example  agent   would first move left and then if a wall is seen on the right  the agent would move left again  If a wall is now seen on the left  the agent does not move on the final step  A policy tree is a record of the the entire local history for an agent up to some fixed horizon and because each tree is independent of the others it can      Bernstein  Amato  Hansen    Zilberstein  be executed in a decentralized manner  While this representation is useful for finite horizon problems  infinite horizon problems would require trees of infinite height  Another option used in this paper is to condition action selection on some internal memory state  These solutions can be represented as a set of local finite state controllers  seen in Figure  b   The controllers operate in a very similar way to the policy trees in that there is a designated initial node and following the action selection at that node  the controller transitions to the next node depending on the observation seen  This continues for the infinite steps of the problem  Throughout this paper  controller states will be referred to as nodes to help distinguish them from system states  An infinite number of nodes may be required to define an optimal infinite horizon DECPOMDP policy  but we will discuss a way to produce solutions within   of the optimal with a fixed number of nodes  While deterministic action selection and node transitions are sufficient to define this   optimal policy  when memory is limited stochastic action selection and node transition may be beneficial  A simple example illustrating this for POMDPs is given by Singh         which can be easily extended to DEC POMDPs  Intuitively  randomness can help an agent to break out of costly loops that result from forgetfulness  A formal description of stochastic controllers for POMDPs and DEC POMDPs is given in sections       and       respectively  but an example can be seen in Figure  b  Agent   begins at node   and moves up with probability      and stays in place with probability       If the agent stayed in place and a wall was then seen on the left  observation wl   on the next step  the controller would transition to node   and the agent would use the same distribution of actions again  If a wall was seen on the right instead  observation wr   there is a      probability that the controller will transition back to node   and a      probability that the controller will transition to node   for the next step  The finite state controller allows an infinite horizon policy to be represented compactly by remembering some aspects of the agents history without representing the entire local history      Centralized Dynamic Programming In this section  we cover the main concepts involved in dynamic programming for the single agent case  This will provide a foundation for the multiagent dynamic programming algorithm described in the following section      Value Iteration for POMDPs Value iteration can be used to solve POMDPs optimally  This algorithm is more complicated than its MDP counterpart  and does not have efficiency guarantees  However  in practice it can provide significant leverage in solving POMDPs  We begin by explaining how every POMDP has an equivalent MDP with a continuous state space  Next  we describe how the value functions for this MDP have special structure that can be exploited  These ideas are central to the value iteration algorithm        Belief State MDPs A convenient way to summarize the observation history of an agent in a POMDP is through a belief state  which is a distribution over system states  As it receives observations  the      Policy Iteration for DEC POMDPs  agent can update its belief state and then remove its observations from memory  Let b denote a belief state  and let b s  represent the probability assigned to state s by b  If an agent chooses action a from belief state b and subsequently observes o  each component of the successor belief state obeys the equation P P  o a  s    sS P  s   s  a b s      b  s       P  o b  a  where    P  o b  a     X       P  o a  s    s  S  X     P  s  s  a b s     sS  Note that this is a simple application of Bayes rule  It was shown by Astrom        that a belief state constitutes a sufficient statistic for the agents observation history  and it is possible to define an MDP over belief states as follows  A belief state MDP is a tuple h  A  T  Ri  where   is the set of distributions over S   A is the set of actions  same as before    T  b  a  b    is the transition function  defined as X T  b  a  b      P  b   b  a  o P  o b  a   oO   R b  a  is a reward function  defined as R b  a     X  b s R s  a    sS  When combined with belief state updating  an optimal solution to this MDP can be used as an optimal solution to the POMDP from which it was constructed  However  since the belief state MDP has a continuous   S  dimensional state space  traditional MDP techniques are not immediately applicable  Fortunately  dynamic programming can be used to find a solution to the belief state MDP  The key result in making dynamic programming practical was proved by Smallwood and Sondik         who showed that the Bellman operator preserves piecewise linearity and convexity of a value function  Starting with a piecewise linear and convex representation of V t   the value function V t   is piecewise linear and convex  and can be computed in finite time  To represent a piecewise linear and convex value function  one need only store the value of each facet for each system state  Denoting the set of facets   we can store     S dimensional vectors of real values PFor any single vector      we can define its value at the belief state b with V  b      sS b s  s   Thus  to go from a set of vectors to the value of a belief state  we use the equation X V  b    max b s  s     sS       Bernstein  Amato  Hansen    Zilberstein  s   s   s   a   s   b   Figure    A piecewise linear and convex value function for a POMDP with two states  a  and a non minimal representation of a piecewise linear and convex value function for a POMDP  b    Figure  a shows a piecewise linear and convex value function for a POMDP with two states  Smallwood and Sondik proved that the optimal value function for a finite horizon POMDP is piecewise linear and convex  The optimal value function for an infinite horizon POMDP is convex  but may not be piecewise linear  However  it can be approximated arbitrarily closely by a piecewise linear and convex value function  and the value iteration algorithm constructs closer and closer approximations  as we shall see        Pruning Vectors Every piecewise linear and convex value function has a minimal set of vectors  that represents it  Of course  it is possible to use a non minimal set to represent the same function  This is illustrated in Figure  b  Note that the removal of certain vectors does not change the value of any belief state  Vectors such as these are not necessary to keep in memory  Formally  we say that a vector  is dominated if for all belief states b  there is a vector       such that V  b     V  b     Because dominated vectors are not necessary  it would be useful to have a method for removing them  This task is often called pruning  and has an efficient algorithm based on linear programming  For a given vector   the linear program in Table   determines whether  is dominated  If variables can be found to make   positive  then adding  to the set improves the value function at some belief state  If not  then  is dominated  This gives rise to a simple algorithm for pruning a set of vectors  to obtain a minimal set   The algorithm loops through   removes each vector     and solves the linear program using  and      If  is not dominated  then it is returned to   It turns out that there is an equivalent way to characterize dominance that can be useful  Recall that for a vector to be dominated  there does not have to be a single vector that has value at least as high for all states  It is sufficient for there to exist a set of vectors such that for all belief states  one of the vectors in the set has value at least as high as the vector in question       Policy Iteration for DEC POMDPs  Variables     b s  Objective  Maximize    Improvement constraints  X    b s  s        s  X  b s  s   s  Probability constraints  X  s  b s        b s      s  Table    The linear program for testing whether a vector  is dominated        convex combination    s   s   Figure    The dual interpretation of dominance  Vector   is dominated at all belief states by either   or     This is equivalent to the existence of a convex combination of   and   which dominates   for all belief states   It can be shown that such a set exists if and only if there is some convex combination of vectors that has value at least as high as the vector in question for all states  This is shown graphically in Figure    If we take the dual of the linear program for dominance given in the previous section  we get a linear program for which the solution is a vector of probabilities for the convex combination  This dual view of dominance was first used in a POMDP context by Poupart and Boutilier         and is useful for policy iteration  as will be explained later        Dynamic Programming Update In this section  we describe how to implement a dynamic programming update to go from a value function Vt to a value function Vt     In terms of implementation  our aim is to take a minimal set of vectors t that represents Vt and produce a minimal set of vectors t   that represents Vt          Bernstein  Amato  Hansen    Zilberstein  Each vector that could potentially be included in t   represents the value of an action a and assignment of vectors in t to observations  A combination of an action and transition rule will hereafter be called a one step policy  The value vector for a one step policy can be determined by considering the action taken  the resulting state transitioned to and observation seen and the value of the assigned vector at step t  This is given via the equation X it    s    R s   i      P  s   s   i  P  o  i   s   t  i o   s     s   o  where i is the index of the vector   i  is its action  and   i  o  is the index of the vector in t to which to transition upon receiving observation o and  is the discount factor  More details on the derivation and use of this formula are provided by Zhang and Zhang         There are  A  t     possible one step policies  A simple way to construct t   is to evaluate all possible one step policies and then apply a pruning algorithm such as Larks method  Lark III         Evaluating the entire set of one step policies will hereafter be called performing an exhaustive backup  It turns out that there are ways to perform a dynamic programming update without first performing an exhaustive backup  Below we describe two approaches to doing this  The first approach uses the fact that it is simple to find the optimal vector for any particular belief state  For a belief state b  an optimal action can be determined via the equation     X P  o b  a V t  T  b a  o        argmaxaA R b  a     o  For each observation o  there is a subsequent belief state  which can be computed using Bayes rule  To get an optimal transition rule    o   we take the optimal vector for the belief state corresponding to o  Since the backed up value function has finitely many vectors  there must be a finite set of belief states for which backups must be performed  Algorithms which identify these belief states include Smallwood and Sondiks one pass algorithm         Chengs linear support and relaxed region algorithms  Cheng         and Kaelbling  Cassandra and Littmans Witness algorithm         The second approach is based on generating and pruning sets of vectors  Instead of generating all vectors and then pruning  these techniques attempt to prune during the generation phase  The first algorithm along these lines was the incremental pruning algorithm  Cassandra et al          Recently  improvements have been made to this approach  Zhang   Lee        Feng   Zilberstein               It should be noted that there are theoretical complexity barriers for DP updates  Littman et al         showed that under certain widely believed complexity theoretic assumptions  there is no algorithm for performing a DP update that is worst case polynomial in all the quantities involved  Despite this fact  dynamic programming updates have been successfully implemented as part of the value iteration and policy iteration algorithms  which will be described in the subsequent sections       Policy Iteration for DEC POMDPs        Value Iteration To implement value iteration  we simply start with an arbitrary piecewise linear and convex value function  and proceed to perform DP updates  This corresponds to value iteration in the equivalent belief state MDP  and thus converges to an   optimal value function after a finite number of iterations  Value iteration returns a value function  but a policy is needed for execution  As in the MDP case  we can use one step lookahead  using the equation     X X  b    argmaxaA R s  a b s     P  o b  a V    b  o  a     sS  o  where   b  o  a  is the belief state resulting from starting in belief state b  taking action a  and receiving observation o  We note that a state estimator must be used as well to track the belief state  Using the fact that each vector corresponds to a one step policy  we can extract a policy from the value of the vectors    X  b     argmaxk b s k  s  s  While the size of the resulting set of dominant vectors may remain exponential  in many cases it is much smaller  This can significantly simplify computation  As in the completely observable case  the Bellman residual provides a bound on the distance to optimality  Recall that the Bellman residual is the maximum distance across all belief states between the value functions of successive iterations  It is possible to find the maximum distance between two piecewise linear and convex functions in polynomial time with an algorithm that uses linear programming  Littman et al              Policy Iteration for POMDPs With value iteration  a POMDP is viewed as a belief state MDP  and a policy is a mapping from belief states to actions  An early policy iteration algorithm developed by Sondik used this policy representation  Sondik         but it was very complicated and did not meet with success in practice  We shall describe a different approach that has performed better on test problems  With this approach  a policy is represented as a finite state controller        Finite State Controllers Using a finite state controller  an agent has a finite number of internal states  Its actions are based only on its internal state  and transitions between internal states occur when observations are received  Internal states provide agents with a kind of memory  which can be crucial for difficult POMDPs  Of course  an agents memory is limited by the number of internal states it possesses  In general  an agent cannot remember its entire history of observations  as this would require infinitely many internal states  An example of a finitestate controller can be seen by considering only one agents controller in Figure  b  The operation of a single controller is the same as that for each agent in the decentralized case  We formally define a controller as a tuple hQ    A    i  where      Bernstein  Amato  Hansen    Zilberstein   Q is a finite set of controller nodes    is a set of inputs  taken to be the observations of the POMDP   A is a set of outputs  taken to be the actions of the POMDP      Q  A is an action selection function  defining the distribution of actions selected at each node      Q  A    Q is a transition function  defining the distribution of resulting nodes for each initial node and action taken  For each state and starting node of the controller  there is an expected discounted sum of rewards over the infinite horizon  It can be computed using the following system of linear equations  one for each s  S and q  Q    X X V  s  q    P  a q  R s  a     P  o  s   s  a P  q    q  a  o V  s    q       s   o q    a  Where P  a q  is the probability action a will be taken in node q and P  q    q  a  o  is the probability the controller will transition to node q   from node q after action a was taken and o was observed  We sometimes refer to the value of the controller at a belief state  For a belief state b  this is defined as X V  b    max b s V  s  q   q  s  Thus  it is assumed that  given an initial state distribution  the controller is started in the node which maximizes value from that distribution  Once execution has begun  however  there is no belief state updating  In fact  it is possible for the agent to encounter the same belief state twice and be in a different internal state each time        Algorithmic Framework We will describe the policy iteration algorithm in abstract terms  focusing on the key components necessary for convergence  In subsequent sections  we present different possibilities for implementation  Policy iteration takes as input an arbitrary finite state controller  The first phase of an iteration consists of evaluating the controller  as described above  Recall that value iteration was initialized with an arbitrary piecewise linear and convex value function  represented by a set of vectors  In policy iteration  the piecewise linear and convex value function arises out of evaluation of the controller  Each controller node has a value when paired with each state  Thus  each node has a corresponding vector and thus a linear value function over belief state space  Choosing the best node for each belief state yields a piecewise linear and convex value function  The second phase of an iteration is the dynamic programming update  In value iteration  an update produces an improved set of vectors  where each vector corresponds to a deterministic one step policy  The same set of vectors is produced in this case  but the       Policy Iteration for DEC POMDPs  Input  A finite state controller  and a parameter       Evaluate the finite state controller by solving a system of linear equations     Perform a dynamic programming update to add a set of deterministic nodes to the controller     Perform value preserving transformations on the controller     Calculate the Bellman residual  If it is less than           then terminate  Otherwise  go to step    Output  An   optimal finite state controller  Table    Policy Iteration for POMDPs  actions and transition rules for the one step policy cannot be removed from memory  Each new vector is actually a node that gets added to the controller  All of the probability distributions for the added nodes are deterministic  That is  a exhaustive backup in this context creates a new node for each possible action and possible combinations of observations and deterministic transitions into the current controller  This results in the same one step policies being considered as in the dynamic programming update described above  As there are  A  t     possible one step polices  this number also defines the number of new nodes added to the controller after an exhaustive backup  Finally  additional operations are performed on the controller  There are many such operations  and we describe two possibilities in the following section  The only restriction placed on these operations is that they do not decrease the value for any belief state  Such an operation is denoted a value preserving transformation  The complete algorithm is outlined in Table    It is guaranteed to converge to a finitestate controller that is   optimal for all belief states within a finite number of steps  Furthermore  the Bellman residual can be used to obtain a bound on the distance to optimality  as with value iteration        Controller Reductions In performing a DP update  potential nodes that are dominated do not get added to the controller  However  after the update is performed  some of the old nodes may have become dominated  These nodes cannot simply be removed  however  as other nodes may transition into them  This is where the dual view of dominance is useful  Recall that if a node is dominated  then there is a convex combination of other nodes with value at least as high from all states  Thus  we can remove the dominated node and merge it into the dominating convex combination by changing transition probabilities accordingly  This operation was proposed by Poupart and Boutilier        and built upon earlier work by Hansen      b   Formally  a controller reduction attempts to replace a node q  Q with a distribution P  q  over nodes q  Q   q such that for all s  S  X V  s  q   P  q V  s  q   qQ q        Bernstein  Amato  Hansen    Zilberstein  Variables     x   Objective  Maximize   Improvement constraints  s  V  s          X  x  V  s       Probability constraints  X    x         x         Table    The dual linear program for testing dominance for the vector   The variable x   represents P      This can be achieved by solving the linear program in Table    As nodes are used rather than vectors  we replace x   with x q  in the dual formulation which provides a probability distribution of nodes which dominate node q  Rather than transitioning into q  this distribution can then be used instead  It can be shown that if such a distribution is found and used for merging  the resulting controller is a value preserving transformation of the original one        Bounded Backups In the previous section  we described a way to reduce the size of a controller without sacrificing value  The method described in this section attempts to increase the value of the controller while keeping its size fixed  It focuses on one node at a time  and attempts to change the parameters of the node such that the value of the controller is at least as high for all belief states  The idea for this approach originated with Platzman         and was made efficient by Poupart and Boutilier         In this method  a node q is chosen  and parameters for the conditional distribution P  a  q    q  o  are to be determined  Determining these parameters works as follows  We assume that the original controller will be used from the second step on  and try to replace the parameters for q with better ones for just the first step  In other words  we look for parameters which satisfy the following inequality    X X V  s  q   P  a q  R s  a     P  q    q  a  o P  o  s   s  a V  s    q     a  s   o q    for all s  S  Note that the inequality is always satisfied by the original parameters  However  it is often possible to get an improvement  The new parameters can be found by solving a linear program  as shown in Table    Note that the size of the linear program is polynomial in the sizes of the POMDP and the controller  We call this process a bounded backup because it acts like a dynamic programming       Policy Iteration for DEC POMDPs  Variables     x a   x a  o  q     Objective  Maximize   Improvement constraints   s  V  s  q        X    x a R s  a      x a        a  o  X  x a  o  q       x a   q   a  a  x a  o  q    P  o  s   s  a V  s    q      s   o q    a  Probability constraints  X  X  x a       a  o  q    x a  o  q         Table    The linear program to be solved for a bounded backup  The variable x a  represents P  a q   and the variable x a  o  q     represents P  a  q    q  o    backup with memory constraints  To see this  consider the set of nodes generated by a DP backup  These nodes dominate the original nodes across all belief states  so for every original node  there must be a convex combination of the nodes in this set that dominate the original node for all states  A bounded backup finds such a convex combination  It can be shown that a bounded backup yields a value preserving transformation  Repeated application of bounded backups can lead to a local optimum  at which none of the nodes can be improved any further  Poupart and Boutilier        showed that a local optimum has been reached when each nodes value function is touching the value function produced by performing a full DP backup  This is illustrated in Figure        Decentralized Dynamic Programming In the previous section  we presented dynamic programming for POMDPs  A key part of POMDP theory is the fact that every POMDP has an equivalent belief state MDP  No such result is known for DEC POMDPs  making it difficult to generalize value iteration to the multiagent case  This lack of a shared belief state requires a new set of tools to be developed for solving DEC POMDP  As a step in this direction  we were able to develop an optimal policy iteration algorithm for DEC POMDPs that includes the POMDP version as a special case  This algorithm is the focus of the section  We first show how to extend the definition of a stochastic controller to the multiagent case  Multiagent controllers include a correlation device  which is a source of randomness shared by all the agents  This shared randomness increases solution quality while minimally increasing representation size without adding communication  As in the single agent case  policy iteration alternates between exhaustive backups and value preserving transforma      Bernstein  Amato  Hansen    Zilberstein  value function after DP update value function for controller  s   s   Figure    A local optimum for bounded backups  The solid line is the value function for the controller  and the dotted line is the value function for the controller that results from a full DP update   tions  A convergence proof is given  along with efficient transformations that extend those presented in the previous section      Correlated Finite State Controllers The joint policy for the agents is represented using a stochastic finite state controller for each agent  In this section  we first define a type of controller in which the agents act independently  We then provide an example demonstrating the utility of correlation  and show how to extend the definition of a controller to allow for correlation among agents        Local Finite State Controllers In a local controller  the agents node is based on the local observations received  and the agents action is based on the current node  These local controllers are defined in the same way as the POMDP controllers above  with each agent possessing its own controller that operates independently of the others  As before  stochastic transitions and action selection are allowed  We formally define a local controller for agent i as a tuple hQi   i   Ai   i   i i  where  Qi is a finite set of controller nodes   i is a set of inputs  taken to be the local observations for agent i   Ai is a set of outputs  taken to be the actions for agent i   i   Qi  Ai is an action selection function for agent i  defining the distribution of actions selected at each node of that agents controller   i   Qi  Ai  i  Qi is a transition function for agent i  defining the distribution of resulting nodes for each initial node and action taken of that agents controller  The functions i and i parameterize the conditional distribution P  ai   qi   qi   oi   which represents the combined action selection and node transition probability for agent i  When       Policy Iteration for DEC POMDPs  AB BA BB  AA  AA AB BA   R  R  R  s   s    R  BB  Figure    A DEC POMDP for which a correlated joint policy yields more reward than the optimal independent joint policy   taken together  the agents controllers determine the conditional distribution P   a   q     q   o   This is denoted an independent joint controller  In the following subsection  we show that independence can be limiting        The Utility of Correlation The joint controllers described above do not allow the agents to correlate their behavior via a shared source of randomness  We will use a simple example to illustrate the utility of correlation in partially observable domains where agents have limited memory  This example generalizes the one given by Singh        to illustrate the utility of stochastic policies in partially observable settings containing a single agent  Consider the DEC POMDP shown in Figure    This problem has two states  two agents  and two actions per agent  A and B   The agents each have only one observation  and thus cannot distinguish between the two states  For this example  we will consider only memoryless policies  Suppose that the agents can independently randomize their behavior using distributions P  a    and P  a     If the agents each choose either A or B according to a uniform distribution  then they receive an expected reward of  R  per time step  and thus an expected long term R reward of        It is straightforward to show that no independent policy yields higher reward than this one for all states  Next  let us consider the even larger class of policies in which the agents may act in a correlated fashion  In other words  we consider all joint distributions P  a    a     Consider the policy that assigns probability    to the pair AA and probability    to the pair BB  This yields an average reward of   at each time step and thus an expected long term reward of    The difference between the rewards obtained by the independent and correlated policies can be made arbitrarily large by increasing R        Bernstein  Amato  Hansen    Zilberstein        Correlated Joint Controllers In the previous subsection  we established that correlation can be useful in the face of limited memory  In this subsection  we extend our definition of a joint controller to allow for correlation among the agents  To do this  we introduce an additional finite state machine  called a correlation device  that provides extra signals to the agents at each time step  The device operates independently of the DEC POMDP process  and thus does not provide agents with information about the other agents observations  In fact  the random numbers necessary for its operation could be determined prior to execution time and made available to all agents  Formally  a correlation device is a tuple hQc   c i  where Qc is a set of nodes and c   Qc  Qc is a state transition function  At each step  the device undergoes a transition  and each agent observes its state  We must modify the definition of a local controller to take the state of the correlation device as input  Now  a local controller for agent i is a conditional distribution of the form P  ai   qi   qc   qi   oi    The correlation device together with the local controllers form a joint conditional distribution P   a   q     q   o   where  q   hqc   q            qn i  We will refer to this as a correlated joint controller  Note that a correlated joint controller with  Qc       is effectively an independent joint controller  Figure   contains a graphical representation of the probabilistic dependencies in a correlated joint controller  The value function for a correlated joint controller can be computed by solving the   following system of linear equations  one for each s  S and  q  Q   V  s   q     X    P   a  q  R s   a      X  P  s     o s   a P   q     q   a   o V  s     q        s     o   q    a  We sometimes refer to the value of the controller for an initial state distribution  For a distribution b  this is defined as V  b    max q   X  b s V  s   q    s  It is assumed that  given an initial state distribution  the controller is started in the joint node which maximizes value from that distribution  It is worth noting that correlation can increase the value of a set of fixed size controllers  but this same value can be achieved by a larger set of uncorrelated controllers  Thus  correlation is a way to make better use of limited representation size  but is not required to produce a set of optimal controllers  This is formalized by the following theorem  which is proved in Appendix A  The theorem asserts the existence of uncorrelated controllers  determining how much extra memory is needed to replace a correlation device remains an open problem  Theorem   Given an initial state and a correlated joint controller  there always exists some finite size joint controller without a correlation device that produces at least the same value for the initial state        Policy Iteration for DEC POMDPs  a   q   q   a   q   o   qc o   q   Figure    A graphical representation of the probabilistic dependencies in a correlated joint controller for two agents   In the example above  higher value can be achieved with two node uncorrelated controllers for each agent  If the problem starts in s    the first node for each agent would choose A and transition to the second node which would choose B  The second node would then transition back to the first node  The resulting policy consists of the agents alternating R between choosing AA and BB  producing an expected long term reward of   which is higher than the correlated one node policy value of    Thus  doubling memory for each agent in this problem is sufficient to remove the correlation device      Policy Iteration In this section  we describe the policy iteration algorithm  We first extend the definitions of exhaustive backup and value preserving transformation to the multiagent case  Following that  we provide a description of the complete algorithm  along with a convergence proof        Exhaustive Backups We introduced exhaustive backups in the section on dynamic programming for POMDPs  We stated that one way to implement a DP update was to perform an exhaustive backup  and then prune dominated nodes that were created  More efficient implementations were described thereafter  These implementations involved interleaving pruning with node generation  For the multiagent case  it is an open problem whether pruning can be interleaved with node generation  Nodes can be removed  as we will show in a later subsection  but for convergence we require exhaustive backups  We do not define DP updates for the multiagent case  and instead make exhaustive backups a central component of our algorithm  An exhaustive backup adds nodes to the local controllers for all agents at once  and leaves the correlation device unchanged  For each agent i   Ai   Qi   i   nodes are added to the Q local controller  one for each one step policy  Thus  the joint controller grows by  Qc   i  Ai   Qi   Oi   joint nodes  Note that repeated application of exhaustive backups amounts to a brute force search in the space of deterministic policies  This converges to optimality  but is obviously quite inefficient  As in the single agent case  we must modify the joint controller in between       Bernstein  Amato  Hansen    Zilberstein  adding new nodes  For convergence  these modifications must preserve value in a sense that will be made formal in the following section        Value Preserving Transformations We now extend the definition of a value preserving transformation to the multiagent case  In the following subsection  we show how this definition allows for convergence to optimality as the number of iterations grows  The dual interpretation of dominance is helpful in understanding multiagent valuepreserving transformations  Recall that for a POMDP  we say that a node is dominated if there is a convex combination of other nodes with value at least as high for all states  Though we defined a value preserving transformation in terms of the value function across belief states  we could have equivalently defined it so that every node in the original controller has a dominating convex combination in the new controller  For the multiagent case  we do not have the concept of a belief state MDP  so we take the second approach mentioned above  In particular  we require that dominating convex combinations exist for nodes of all the local controllers and the correlation device  A transformation of a controller C to a controller D qualifies as a value preserving transformation if C  D  where  is defined below    and R    respectively  We Consider correlated joint controllers C and D with node sets Q say that C  D if there exist mappings fi   Qi  Ri for each agent i and fc   Qc  Rc such that X V  s   q   P   r  q V  s   r    r    Note that this relation is transitive as further value preserving for all s  S and  q  Q  transformations of D will also be value preserving transformations of C     R    Examples We sometimes describe the fi and fc as a single mapping f   Q of efficient value preserving transformations are given in a later section  In the following subsection  we show that alternating between exhaustive backups and value preserving transformations yields convergence to optimality        Algorithmic Framework The policy iteration algorithm is initialized with an arbitrary correlated joint controller  In the first part of an iteration  the controller is evaluated via the solution of a system of linear equations  Next  an exhaustive backup is performed to add nodes to the local controllers  Finally  value preserving transformations are performed  In contrast to the single agent case  there is no Bellman residual for testing convergence to   optimality  We resort to a simpler test for   optimality based on the discount rate and the number of iterations so far  Let  Rmax   be the largest absolute value of an immediate reward possible in the DEC POMDP  Our algorithm terminates after iteration t    R max   t if        At this point  due to discounting  the value of any policy after step t is less than    Justification for this test is provided in the convergence proof  The complete algorithm is sketched in Table    Before proving convergence  we state a key lemma regarding the ordering of exhaustive backups and value preserving transformations  Its proof is deferred to the Appendix        Policy Iteration for DEC POMDPs  Input  A correlated joint controller  and a parameter       Evaluate the correlated joint controller by solving a system of linear equations     Perform an exhaustive backup to add deterministic nodes to the local controllers     Perform value preserving transformations on the controller  t     Rmax      If        where t is the number of iterations so far  then terminate  Else go to step     Output  A correlated joint controller that is   optimal for all states  Table    Policy Iteration for DEC POMDPs  Lemma   Let C and D be correlated joint controllers  and let C and D be the results of performing exhaustive backups on C and D  respectively  Then C  D if C  D  Thus  if there is a value preserving transformation mapping controller C to D and both are exhaustively backed up  then there is a value preserving transformation mapping controller C to D  This allows value preserving transformations to be performed before exhaustive backups  while ensuring that value is not lost after the backup  We can now state and prove the main convergence theorem for policy iteration  Theorem   For any    policy iteration returns a correlated joint controller that is   optimal for all initial states in a finite number of iterations  Proof  Repeated application of exhaustive backups amounts to a brute force search in the space of deterministic joint policies  Thus  after t exhaustive backups  the resulting controller is optimal for t steps from any initial state  Let t be an integer large enough that  t    Rmax       Then any possible discounted sum of rewards after t time steps is small   enough that optimality over t time steps implies   optimality over the infinite horizon  Now recall the above lemma  which states that performing value preserving transformations before a backup provides at least as much value as just performing a backup  By an inductive argument  performing t steps of policy iteration is a value preserving transformation of the result of t exhaustive backups  We have argued that for large enough t  the value of the controller resulting from t exhaustive backups is within   of optimal for all states  Thus  the result of t steps of policy iteration is also within   of optimal for all states        Efficient Value Preserving Transformations In this section  we describe how to extend controller reductions and bounded backups to the multiagent case  We will show that both of these operations are value preserving transformations        Controller Reductions Recall that in the single agent case  a node can be removed if for all belief states  there is another node with value at least as high  The equivalent dual interpretation is that a node       Bernstein  Amato  Hansen    Zilberstein  can be removed is there exists a convex combination of other nodes with value at least as high across the entire state space  Using the dual interpretation  we can extend this to a rule for removing nodes in the multiagent case  The rule applies to removing nodes either from a local controller or from the correlation device  Intuitively  in considering the removal of a node from a local controller or the correlation device  we consider the nodes of the other controllers to be part of the hidden state  More precisely  suppose we are considering removing node qi from agent is local controller  To do this  we need to find a distribution P  qi   over nodes qi  Qi   qi such that for all s  S  qi  Qi   and qc  Qc   V  s  qi   qi   qc     X  P  qi  V  s  qi   qi   qc     qi  where Qi represents the set of nodes for the other agents  Finding such a distribution can be formulated as a linear program  as shown in Table  a  In this case  success is finding parameters such that       The linear program is polynomial in the sizes of the DEC POMDP and controllers  but exponential in the number of agents  If we are successful in finding parameters that make       then we can merge the dominated node into the convex combination of other nodes by changing all incoming links to the dominated controller node to be redirected based on the distribution P  qi    At this point  there is no chance of ever transitioning into qi   and thus it can be removed  The rule for the correlation device is very similar  Suppose that we are considering the removal of node qc   In this case  we need to find a distribution P  qc   over nodes qc  Qc   qc   such that for all s  S and  q  Q  V  s   q  qc     X  P  qc  V  s   q  qc     qc    for the set of tuples of local controller nodes  Note that we abuse notation here and use Q excluding the nodes for the correlation device  As in the previous case  finding parameters can done using linear programming  This is shown in Table  b  This linear program is also polynomial in the the sizes of the DEC POMDP and controllers  but exponential in the number of agents  We have the following theorem  which states that controller reductions are value preserving transformations  Theorem   Any controller reduction applied to either a local node or a node of the correlation device is a value preserving transformation  Proof  Suppose that we have replaced an agent i node qi with a distribution over nodes in Qi   qi   Let us take fi to be the identity map for all nodes except qi   which will map to the new distribution  We take fc to be the identity map  and we take fj to be the identity map for all j    i  This yields a complete mapping f   We must now show that f satisfies the condition given in the definition of a value preserving transformation        Policy Iteration for DEC POMDPs   a  Variables     x qi   Objective  Maximize   Improvement constraints  s  qi   qc  V  s  qi   qi   qc         X  x qi  V  s  qi   qi   qc    qi  Probability constraints  X  qi  x qi         x qi       qi   b  Variables     x qc   Objective  Maximize   Improvement constraints  s   q V  s   q  qc         X  x qc  V  s   q  qc    qc  Probability constraints  X  qc  x qc         x qc       qc  Table     a  The linear program to be solved to find a replacement for agent is node qi   The variable x qi   represents P  qi     b  The linear program to be solved to find a replacement for the correlation node qc   The variable x qc   represents P  qc     Let Vo be the value function for the original controller  and let Vn be the value function for the controller with qi removed  A controller reduction requires that Vo  s   q    X  P   r  q Vo  s   r     r    Thus  we have for all s  S and  q  Q   Vo  s   q     X  P   a  q  R s  a       X  P   q     q   a   o P  s     o s   a Vo  s     q      s     o   q    a     X  a  P   a  q  R s  a       X  P   q     q   a   o P  s     o s   a   s     o   q        X   r   P   r  q Vo  s   r       Bernstein  Amato  Hansen    Zilberstein      X  P   a  q  R s  a       X  P   q     q   a   o P  s     o s   a P   r  q Vo  s   r      s     o   q      r    a    Notice that the formula on the right is the Bellman operator for all s  S and  q  Q  for the new controller  applied to the old value function  Denoting this operator Tn   the system of inequalities implies that Tn Vo  Vo   By monotonicity  we have that for all k     Tnk    Vo    Tnk  Vo    Since Vn   limk Tnk  Vo    we have that Vn  Vo   This is sufficient for f to satisfy the condition in the definition of value preserving transformation  The argument for removing a node of the correlation device is almost identical to the one given above          Bounded Dynamic Programming Updates In the previous section  we described a way to reduce the size of a controller without sacrificing value  Recall that in the single agent case  we could also use bounded backups to increase the value of the controller while keeping its size fixed  This technique can be extended to the multiagent case  As in the previous section  the extension relies on improving a single local controller or the correlation device  while viewing the nodes of the other controllers as part of the hidden state  We first describe in detail how to improve a local controller  To do this  we choose an agent i  along with a node qi   Then  for each oi  i   we search for new parameters for the conditional distribution P  ai   qi   qi   oi    The search for new parameters works as follows  We assume that the original controller will be used from the second step on  and try to replace the parameters for qi with better ones for just the first step  In other words  we look for parameters satisfying the following inequality    X X V  s   q   P   a  q  R s  a     P   q     q   a   o P  s     o s   a V  s     q      a  s     o   q   for all s  S  qi  Qi   and qc  Qc   The search for new parameters can be formulated as a linear program  as shown in Table  a  Its size is polynomial in the sizes of the DEC POMDP and the joint controller  but exponential in the number of agents  The procedure for improving the correlation device is very similar to the procedure for improving a local controller  We first choose a device node qc   and consider changing its parameters for just the first step  We look for parameters satisfying the following inequality    X X P   a  q  R s  a     P   q     q   a   o P  s     o s   a V  s     q     V  s   q    a  s     o   q     for all s  S and  q  Q  As in the previous case  the search for parameters can be formulated as a linear program  This is shown in Table  b  This linear program is also polynomial in the sizes of the DECPOMDP and joint controller  but exponential in the number of agents  The following theorem states that bounded backups preserve value        Policy Iteration for DEC POMDPs   a  Variables     x qc   ai    x qc   ai   oi   qi    Objective  Maximize   Improvement constraints  s  qi   qc  X  V  s   q  qc         P  ai  qc   qi   x qc   ai  R s   a      a    X    x c  ai   oi   qi   P  qi  qc   qi   ai   oi    s     o   q    qc    P   o  s   s   a P  qc   qc  V  s     q     qc      Probability constraints  X qc x qc   ai         qc   ai   oi  x qc   ai   oi   qi      x qc   ai    qi   ai  qc   ai  X  x qc   ai        qc   ai   oi   qi   x qc   ai   oi   qi         b  Variables     x qc    Objective  Maximize   Improvement constraints  s   q V  s   q  qc         X  P   a qc    q  R s   a      X  P   q    qc    q   a   o   s     o   q    qc    a     P  s    o s   a x qc   V  s     q     qc      Probability constraints  qc   X  x qc          qc   x qc        qc   Table     a  The linear program used to find new parameters for agent is node qi   The variable x qc   ai   represents P  ai  qi   qc    and the variable x qc   ai   oi   qi    represents P  ai   qi   qc   qi   oi     b  The linear program used to find new parameters for the correlation device node qc   The variable x qc    represents P  qc   qc           Bernstein  Amato  Hansen    Zilberstein  Theorem   Performing a bounded backup on a local controller or the correlation device produces a new correlated joint controller which is a value preserving transformation of the original  Proof  Consider the case in which some node qi of agent is local controller is changed  We define f to be a deterministic mapping from nodes in the original controller to the corresponding nodes in the new controller  Let Vo be the value function for the original controller  and let Vn be the value function for the new controller  Recall that the new parameters for P  ai   qi   qc   qi   oi   must satisfy the following inequality for all s  S  qi  Qi   and qc  Qc     X X Vo  s   q   P   a  q  R s  a     P   q     q   a   o P  s     o s   a Vo  s     q        a  s     o   q   Notice that the formula on the right is the Bellman operator for the new controller  applied to the old value function  Denoting this operator Tn   the system of inequalities implies that Tn Vo  Vo   By monotonicity  we have that for all k     Tnk    Vo    Tnk  Vo    Since Vn   limk Tnk  Vo    we have that Vn  Vo   Thus  the new controller is a value preserving transformation of the original one  The argument for changing nodes of the correlation device is almost identical to the one given above        Open Issues We noted at the beginning of the section that there is no known way to convert a DECPOMDP into an equivalent belief state MDP  Despite this fact  we were able to develop a provably convergent policy iteration algorithm  However  the policy iteration algorithm for POMDPs has other desirable properties besides convergence  and we have not yet been able to extend these to the multiagent case  Two such properties are described below        Error Bounds The first property is the existence of a Bellman residual  In the single agent case  it is possible to compute a bound on the distance to optimality using two successive value functions  In the multiagent case  policy iteration produces a sequence of controllers  each of which has a value function  However  we do not have a way to obtain an error bound from these value functions  For now  to bound the distance to optimality  we must consider the discount rate and the number of iterations completed        Avoiding Exhaustive Backups In performing a DP update for POMDPs  it is possible to remove certain nodes from consideration without first generating them  In Section    we gave a high level description of a few different approaches to doing this  For DEC POMDPs  however  we did not define a DP update and instead used exhaustive backups as the way to expand a controller  Since exhaustive backups are expensive  it would be useful to extend the more sophisticated pruning methods for POMDPs to the multiagent case        Policy Iteration for DEC POMDPs  Input  A joint controller  the desired number of centralized belief points k  initial state b  and fixed policy for each agent i      Starting from b    sample a set of k belief points for each agent assuming the other agents use their fixed policy     Evaluate the joint controller by solving a system of linear equations     Perform an exhaustive backup to add deterministic nodes to the local controllers     Retain nodes that contribute the highest value at each of the belief points     For each agent  replace nodes that have lower value than some combination of other nodes at each belief point     If controller sizes and parameters do not change then terminate  Else go to step    Output  A new joint controller based on the sampled centralized belief points  Table    Heuristic Policy Iteration for DEC POMDPs   Unfortunately  in the case of POMDPs  the proofs of correctness for these methods all use the fact that there exists a Bellman equation  Roughly speaking  this equation allows us to determine whether a potential node is dominated by just analyzing the nodes that would be its successors  Because we do not currently have an analog of the Bellman equation for DEC POMDPs  we have not been able to generalize these results  There is one exception to the above statement  however  When an exhaustive backup has been performed for all agents except one  then a type of belief state space can be constructed for the agent in question using the system states and the nodes for the other agents  The POMDP node generation methods can then be applied to just that agent  In general  though  it seems difficult to rule out a node for one agent before generating all the nodes for the other agents      Heuristic Policy Iteration While the optimal policy iteration method shows how a set of controllers with value arbitrarily close to optimal can be found  the resulting controllers may be very large and many unnecessary nodes may be generated along the way  This is exacerbated by the fact that the algorithm cannot take advantage of an initial state distribution and must attempt to improve the controller for any initial state  As a way to combat these disadvantages  we have developed a heuristic version of policy iteration that removes nodes based on their value only at a given set of centralized belief points  We call these centralized belief points because they are distributions over the system state that in general could only be known by full observability of the problem  As a result  the algorithm will no longer be optimal  but it can often produce more concise controllers with higher solution quality for a given initial state distribution        Bernstein  Amato  Hansen    Zilberstein      Directed Pruning Our heuristic policy iteration algorithm uses sets of belief points to direct the pruning process of our algorithm  There are two main advantages of this approach  it allows simultaneous pruning for all agents and it focuses the controller on certain areas of the belief space  We first discuss the benefits of simultaneous pruning and then mention the advantages of focusing on small areas of the belief space  As mentioned above  the pruning method used by the optimal algorithm will not always remove all nodes that could be removed from all the agents controllers without losing value  Because pruning requires each agent to consider the controllers of other agents  after nodes are removed for one agent  the other agents may be able to prune other nodes  Thus pruning must cycle through the agents and ceases when no agent can remove any further nodes  This is both time consuming and causes the controller to be much larger than it needs to be  Like the game theoretic concept of incredible threats    a set of suboptimal policies for an agent may be useful only because other agents may employ similarly suboptimal policies  That is  because pruning is conducted for each agent while holding the other agents policies fixed  polices that are useful for any set of other agent policies are retained  no matter the quality of these other agent policies  Some of an agents policies may only be retained because they have the highest value when used in conjunction with other suboptimal policies of the other agents  In these cases  only by removing the set of suboptimal policies simultaneously can controller size be reduced while at least maintaining value  This simultaneous pruning could further reduce controller sizes and thus increase scalability and solution quality  While it may be possible to define a value preserving transformation for these problems  finding a nontrivial automated way to do so while maintaining the optimality of the algorithm remains an open question  The advantage of considering a smaller part of the state space has already been shown to produce drastic performance increases in POMDPs  Ji  Parr  Li  Liao    Carin        Pineau  Gordon    Thrun        and finite horizon DEC POMDPs  Seuken   Zilberstein        Szer   Charpillet         For POMDPs  a problem with many states has a belief space with large dimensionality  but many parts may never be visited by an optimal policy  Focusing on a subset of belief states can allow a large part of the state space to be ignored without significant loss of solution quality  The problem of having a large state space is compounded in the DEC POMDP case  Not only is there uncertainty about the state  but also about the policies of the other agents  As a consequence  the generalized belief space which includes all possible distributions over states of the system and current policies of the other agents must be considered to guarantee optimality  This results in a huge space which contains many unlikely states and policies  The uncertainty about which policies other agents may utilize does not allow belief updates to normally be calculated for DEC POMDPs  but as we showed above  it can be done by assuming a probability distribution over actions of the other agents  This limits the number of policies that need to be considered by all agents and if the distributions are chosen well  may permit a high valued solution to be found     An incredible threat is an irrational strategy that the agent knows it will receive a lower value by choosing it  While it is possible the agent will choose the incredible threat strategy  it is irrational to do so         Policy Iteration for DEC POMDPs  Variables     x qi   and for each belief point b Objective  Maximize   Improvement constraints   b  qi  X   X   b s  x qi  V  qi   qi   s   V   q  s      s  X  Probability constraints   qi  x qi       and qi x qi       qi  Table    The linear program used to determine if a node q for agent i is dominated at each point b and all initial nodes of the other agents controllers  As node q may be dominated by a distribution of nodes  variable x qi   represents P  qi    the probability of starting in node q for agent i       Belief Set Generation As mentioned above  our heuristic policy iteration algorithm constructs sets of belief points for each agent which are later used to evaluate the joint controller and remove dominated nodes  To generate the belief point set  we start at the initial state and by making assumptions about the other agents  we can calculate the resulting belief state for each action and observation pair of an agent  By fixing the policies for the other agents  this belief state update can be calculated in a way very similar to that described for POMDPs in section        This procedure can be repeated from each resulting belief state until a desired number of points is generated or no new points are visited  More formally  we assume the other agents have a fixed distribution of action choice for each system state  That is  if we know P   ai  s  then we can determine the probability any state results given a belief point and an agents action and observation  The derivation of the likelihood of state s    given the belief state b  and agent is action ai and observation oi is shown below   P  s   ai   oi   b     X  P  s     ai    oi   s ai   oi   b    ai    oi  s  o s  b   a  s   P  s    s   a  b   ai    oi  s P     P    P  oi   ai   b  o s   a  s   P  s   s   a  b P   a  s  b   ai    oi  s P     P    P  oi   ai   b  o s   a  s   P  s   s   a P   ai  a  s  b P   a  s  b   ai    oi  s P     P    P  oi   ai   b     P  s   s    P    o  s    a   s a P   ai  ai   s  b P  s ai   b P  ai   b   ai    oi  s  P    P  oi   ai   b        Bernstein  Amato  Hansen    Zilberstein  P    o s   a  s  ai    oi  s P        P  s   s    a P   ai  s b s   P  oi  ai   b   where X  P  oi  ai   b     P   o s   a  s   P  s   s   a P   ai  s b s   ai  oi  s s   Thus  given the action probabilities for the other agents  i  and the transition and observation models of the system  a belief state update can be calculated      Algorithmic Framework We provide a formal description of our approach in Table    Given the desired number of belief points  k  and random action and observation selection for each agent  the sets of points are generated as described above  The search begins at the initial state of the problem and continues until the given number of points is obtained  If no new points are found  this process can be repeated to ensure a diverse set is produced  The arbitrary initial controller is evaluated and the value at each state and for each initial node of any agents controller is retained  The exhaustive backup procedure is exactly the same as the one used in the optimal algorithm  but updating the controller takes place in two steps  First  for each of the k belief points  the highest valued set of initial nodes is found  To accomplish this  the value of beginning at each combination of nodes for all agents is calculated for each of these k points and the best combination is kept  This allows nodes that do not contribute to any of these values to be simultaneously pruned  Next  each node of each agent is pruned using the linear program shown in Table    If a distribution of nodes for the given agent has higher value at each of the belief points for any initial nodes of the other agents controllers  it is pruned and replaced with that distribution  The new controllers are then evaluated and the value is compared with the value of the previous controller  This process of backing up and pruning continues while the controller parameters continue to change  Similar to how bounded policy updates can be used in conjunction with pruning in the optimal policy iteration algorithm  a nonlinear programming approach  Amato et al         can be used to improve solution quality for the heuristic case  To accomplish this  instead of optimizing the controller for just the initial belief state of the problem  all the belief points being considered are used  A simple way to achieve this is to maximize over the sum of the values of the initial nodes of the controllers weighted by the probabilities given for each point  This approach can be used after each pruning step and may further improve value of the controllers      Dynamic Programming Experiments This section describes the results of experiments performed using policy iteration  Because of the flexibility of the algorithm  it is impossible to explore all possible ways of implementing it  However  we did experiment with a few different implementation strategies to gain an idea of how the algorithm works in practice  All of these experiments were run on a     GHz Intel Pentium   with  GB of memory  Three main sets of experiments were performed on a single set of test problems        Policy Iteration for DEC POMDPs  Our first set of experiments focused on exhaustive backups and controller reductions  The results confirm that value improvement can be obtained through iterated application of these two operations  Further improvement is demonstrated by also incorporating bounded updates  However  because exhaustive backups are expensive  the algorithm was unable to complete more than a few iterations on any of our test problems  In the second set of experiments  we addressed the complexity issues by using only bounded backups  and no exhaustive backups  With bounded backups  we were able to obtain higher valued controllers while keeping memory requirements fixed  We examined how the sizes of the initial local controllers and the correlation device affected the value of the final solution  The third set of experiments examined the complexity issues caused by exhaustive backups by using the point based heuristic  This allowed our heuristic policy iteration algorithm to complete more iterations than the optimal algorithm and in doing so  increased solution quality of the largest solvable controllers  By incorporating Amato et al s NLP approach  the heuristic algorithm becomes slightly less scalable than with heuristic pruning alone  but the amount of value improvement per step increases  This causes the resulting controllers in each domain to have the highest value of any approach      Test Domains In this section  we describe three test domains  ordered by the size of the problem representation  For each problem  the transition function  observation function  and reward functions are described  In addition  an initial state is specified  Although policy iteration does not require an initial state as input  one is commonly assumed and is used by the heuristic version of the algorithm  A few different initial states were tried for each problem  and qualitatively similar results were obtained  In all domains  a discount factor of     was utilized  As a very loose upper bound  the centralized policy was calculated for each problem in which all agents share their observations with a central agent and decisions for all agents are made by the central agent  This results in a POMDP with the same number of states  but the action and observation sets are Cartesian products of the agents action and observation sets  The value of this POMDP policy is provided below  but because DEC POMDP policies are more constrained  the optimal value may be much lower  Two Agent Tiger Problem The two agent tiger problem consists of   states    actions and   observations  Nair et al          The domain includes two doors  one of which leads to a tiger and the other to a large treasure  Each agent may open one of the doors or listen  If either agent opens the door with the tiger behind it  a large penalty is given  If the door with the treasure behind it is opened and the tiger door is not  a reward is given  If both agents choose the same action  i e   both opening the same door  a larger positive reward or a smaller penalty is given to reward cooperation  If an agent listens  a small penalty is given and an observation is seen that is a noisy indication of which door the tiger is behind  While listening does not change the location of the tiger  opening a door causes the tiger to be placed behind one of the       Bernstein  Amato  Hansen    Zilberstein  door with equal probability  The problem begins with the tiger equally likely to be located behind either door  The optimal centralized policy for this problem has value         Meeting on a Grid In this problem  with    states    actions and   observations  two robots must navigate on a two by two grid  Each robot can only sense whether there are walls to its left or right  and their goal is to spend as much time as possible on the same square as the other agent  The actions are to move up  down  left  or right  or to stay on the same square  When a robot attempts to move to an open square  it only goes in the intended direction with probability      otherwise it either goes in another direction or stays in the same square  Any move into a wall results in staying in the same square  The robots do not interfere with each other and cannot sense each other  The reward is   when the agents share a square  and   otherwise  The initial state places the robots diagonally across from each other and the optimal centralized policy for this problem has value        Box Pushing Problem This problem  with     states    actions and   observations consists of two agents that get rewarded by pushing different boxes  Seuken   Zilberstein         The agents begin facing each other in the bottom corners of a four by three grid with the available actions of turning right  turning left  moving forward or staying in place  There is a     probability that the agent will succeed in moving and otherwise will stay in place  but the two agents can never occupy the same square  The middle row of the grid contains one large box in the middle of two small boxes  The small boxes can be moved by a single agent  but the large box can only be moved by both agents pushing at the same time  The upper row of the grid is considered the goal row  which the boxes are pushed into  The possible deterministic observations for each agent consist of seeing an empty space  a wall  the other agent  a small box or the large box  A reward of     is given if both agents push the large box to the goal row and    is given for each small box that is moved to the goal row  A penalty of    is given for each agent that cannot move and      is given for each time step  Once a box is moved to the goal row  the environment resets to the original start state  The optimal centralized policy for this problem has value              Exhaustive Backups and Controller Reductions In this section  we present the results of using exhaustive backups together with controller reductions  For each domain  the initial controllers for each agent contained a single node with a self loop  and there was no correlation device  For each problem  the first action of the problem description was used  This resulted in the repeated actions of opening the left door in the two agent tiger problem  moving up in the meeting on a grid problem and turning left in the box pushing problem  The reason for starting with the smallest possible controllers was to see how many iterations we could complete before running out of memory  On each iteration  we performed an exhaustive backup  and then alternated between agents  performing controller reductions until no more nodes could be removed  For bounded dynamic programming results  after the reductions were completed bounded updates were also performed for all agents  For these experiments  we attempted to improve the nodes of       Policy Iteration for DEC POMDPs  Iteration          Two Agent Tiger   S        Ai         i       Exhaustive Sizes Controller Reductions Bounded Updates                  in  s            in  s                   in  s           in   s                          in  s              in   s                               in     s                 in     s   Iteration        Meeting on a Grid   S         Ai         i       Exhaustive Sizes Controller Reductions Bounded Updates                 in  s           in  s                  in  s           in    s                          in    s                 in     s   Iteration        Box Pushing   S          Ai         i       Exhaustive Sizes Controller Reductions Bounded Updates                in  s          in   s                 in    s           in    s                         in    s               in    s   Table     Results of applying exhaustive backups  controller reductions and bounded updates to our test problems  The second column contains the sizes of the controllers if only exhaustive backups had been performed  The third column contains the resulting value  sizes of the controllers  and time required for controller reductions to be performed on each iteration  The fourth column displays these same quantities with bounded updates also being used  The   denotes that a backup and pruning were performed  but bounded updates exhausted the given resources   each agent in turn until value could not be improved for any node of any agent  For each iteration  we recorded the sizes of the controllers produced  and noted what the sizes would be if no controller reductions had been performed  In addition  we recorded the value from the initial state and the total time taken to reach the given result  The results are shown in Table     Because exhaustive backups add many nodes  we were unable to complete many iterations without exceeding memory limits  As expected  the smallest problem led to the largest number of iterations being completed  Although we could not complete many iterations before running out of memory  the use of controller reductions led to significantly smaller controllers compared to the approach of just applying exhaustive backups  Incorporating bounded updates requires some extra time  but is able to improve the value produced at each step  causing substantial improvement in some cases  It is also interesting to notice that the controller sizes when using bounded updates are not always the same as when only controller reductions are completed  This can be seen after two iterations in both the meeting on a grid and box pushing problems  This can occur because the bounded updates change node value and thus change the number and location of the nodes that are pruned  In the box pushing problem  the two agents also       Bernstein  Amato  Hansen    Zilberstein  have different size controllers after two steps  This can occur  even in symmetric problems  when a set of actions is only necessary for a single agent      Bounded Dynamic Programming Updates As we saw from the previous experiments  exhaustive backups can fill up memory very quickly  This leads naturally to the question of how much improvement is possible without exhaustive backups  In this section  we describe an experiment in which we repeatedly applied bounded backups  which left the size of the controller fixed  We experimented with different starting sizes for the local controllers and the correlation device  We define a trial run of the algorithm as follows  At the start of a trial run  a size is chosen for each of the local controllers and the correlation device  The action selection and transition functions are initialized to be deterministic  with the outcomes drawn according to a uniform distribution  A step consists of choosing a node uniformly at random from the correlation device or one of the local controllers  and performing a bounded backup on that node  After     steps  the run is considered over  In practice  we found that values often stabilized in fewer steps  We varied the sizes of the local controllers while maintaining the same number of nodes for each agent  and we varied the size of the correlation device from   to    For each domain  we increased number of nodes until the required number of steps could not be completed in under four hours  In general  runs required significantly less time to terminate  For each combination of sizes  we performed    trial runs and recorded the best value over all runs  For each of the three problems  we were able to obtain solutions with higher value than with exhaustive backups  Thus  we see that even though repeated application of bounded backups does not have an optimality guarantee  it can be competitive with an algorithm that does  However  it should be noted that we have not performed an exhaustive comparison  We could have made different design decisions for both approaches concerning the starting controllers  the order in which nodes are considered  and other factors  Besides comparing to the exhaustive backup approach  we wanted to examine the effect of the sizes of the local controllers and the correlation device on value  Figure   shows a graph of best values plotted against controller size  We found that  for the most part  the value increases when we increase the size of the correlation device from one node to two nodes  essentially moving from independent to correlated   It is worth noting that the solution quality had somewhat high variance in each problem  showing that setting good initial parameters is important for high valued solutions  For small controllers  the best value tends to increase with controller size  However  for very large controllers  this not always the case  This can be explained by considering how a bounded backup works  For new node parameters to be acceptable  they must not decrease the value for any combination of states  nodes for the other controllers  and nodes for the correlation device  This becomes more difficult as the numbers of nodes increase  and thus it is easier to get stuck in a local optimum  This can be readily seen in the two agent tiger problem and to some extent the meeting on a grid problem  Memory was exhausted before this phenomenon takes place in the box pushing problem        Policy Iteration for DEC POMDPs   a    b    c  Figure    Best value per trial run plotted against the size of the local controllers  for  a  the two agent tiger problem   b  the meeting in a grid problem and  c  the box pushing problem  The solid line represents independent controllers  a correlation device with one node   and the dotted line represents a joint controller including a two node correlation device  Times ranged from under  s for one node controllers without correlation to four hours for the largest controller found with correlation in each problem       Heuristic Dynamic Programming Updates As observed above  the optimal dynamic programming approach can only complete a small number of backups before resources are exhausted  Similarly  using bounded updates with fixed size controllers can generate high value solutions  but it can be difficult to pick the correct controller size and initial parameters  As an alternative to the other approaches  we also present experiments using our heuristic dynamic programming algorithm  Like the optimal policy iteration experiments  we initialized single node controllers for each agent with self loops and no correlation device  The same first actions were used as above and backups were performed until memory was exhausted  The set of belief points for each problem was generated given the initial state distribution and a distribution of actions for the other agents  For the meeting on a grid and box pushing problems  it was       Bernstein  Amato  Hansen    Zilberstein  assumed that all agents chose any action with equal probability regardless of state  For the two agent tiger problem  it was assumed that for any state agents listen with probability     and open each door with probability      This simple heuristic policy was chosen to allow more of the state space to be sampled by our search  The number of belief points used for the two agent tiger and meeting on a grid problems was ten and twenty points were used for the box pushing problem  For each iteration  we performed an exhaustive backup and then pruned controllers as described in steps four and five of Table    All the nodes that contributed to the highest value for each belief point were retained and then each node was examined using the linear program in Table    For results with the NLP approach  we also improved the set of controllers after heuristic pruning by optimizing a nonlinear program whose objective was the sum of the values of the initial nodes weighted by the belief point probabilities  We report the value produced by the optimal and heuristic approaches for each iteration that could be completed in under four hours and with the memory limits of the machine used  The nonlinear optimization was performed on the NEOS server  which provides a set of machines with varying CPU speeds and memory limitations  The values for each iteration of each problem are given in Figure    We see the heuristic policy iteration  HPI  methods are able to complete more iterations than the optimal methods and as a consequence produce higher values  In fact  the results from HPI are almost always exactly the same as those for the optimal policy iteration algorithm without bounded updates for all iterations that can be completed by the optimal approach  Thus  improvement occurs primarily due to the larger number of backups that can be performed  We also see that while incorporating bounded updates improves value for the optimal algorithm  incorporating the NLP approach into the heuristic approach produces even higher value  Optimizing the NLP requires a small time overhead  but substantially increases value on each iteration  This results in the highest controller value in each problem  Using the NLP also allows our heuristic policy iteration to converge to a six node controller for each agent in the two agent tiger problem  Unfortunately  this solution is known to be suboptimal  As an heuristic algorithm  this is not unexpected  and it should be noted that even suboptimal solutions by the heuristic approach outperform all other methods in all our test problems      Discussion We have demonstrated how policy iteration can be used to improve both correlated and independent joint controllers  We showed that using controller reductions together with exhaustive backups is more efficient in terms of memory than using exhaustive backups alone  However  due to the complexity of exhaustive backups  even that approach could only complete a few iterations on each of our test problems  Using bounded backups alone provided a good way to deal with the complexity issues  With bounded backups  we were able to find higher valued policies than with the previous approach  Through our experiments  we were able to understand how the sizes of the local controllers and correlation device affect the final values obtained  With our heuristic policy iteration algorithm  we demonstrated further improvement by dealing with some of the complexity issues  The heuristic approach is often able to continue       Policy Iteration for DEC POMDPs   a    b    c  Figure    Comparison of the dynamic programming algorithms on  a  the two agent tiger problem   b  the meeting in a grid problem and  c  the box pushing problem  The value produced by policy iteration with and without bounded backups as well as our heuristic policy iteration with and without optimizing the NLP were compared on each iteration until the time or memory limit was reached   improving solution quality past the point where the optimal algorithm exhausts resources  More efficient use of this limited representation size is achieved by incorporating the NLP approach as well  In fact  the heuristic algorithm with NLP improvements at each step provided results that are at least equal to the highest value obtained in each problem and sometimes were markedly higher than the other approaches  Furthermore  as far as we know  these results are the highest published values for all three of the test domains      Conclusion We present a policy iteration algorithm for DEC POMDPs  The algorithm uses a novel policy representation consisting of stochastic finite state controllers for each agent along with a correlation device  We define value preserving transformations and show that alternating between exhaustive backups and value preserving transformations leads to convergence to       Bernstein  Amato  Hansen    Zilberstein  optimality  We also extend controller reductions and bounded backups from the single agent case to the multiagent case  Both of these operations are value preserving transformations and are provably efficient  Finally  we introduced a heuristic version of our algorithm which is more scalable and produces higher values on our test problems  Our algorithm serves as the first nontrivial exact algorithm for DEC POMDPs  and provides a bridge to the large body of work on dynamic programming for POMDPs  Our work provides a solid foundation for solving DEC POMDPs  but much work remains in addressing more challenging problem instances  We focused on solving general DECPOMDPs  but the efficiency of our approaches could be improved by using structure found in certain problems  This would allow specialized representations and solution techniques to be incorporated  Below we describe some key challenges of our general approach  along with some preliminary algorithmic ideas to extend our work on policy iteration  Approximation with Error Bounds Often  strict optimality requirements cause computational difficulties  A good compromise is to search for policies that are within some bound of optimal  Our framework is easily generalized to allow for this  Instead of a value preserving transformation  we could define an   value preserving transformation  which insures that the value at all states decreases by at most    We can perform such transformations with no modifications to any of our linear programs  We simply need to relax the requirement on the value for   that is returned  It is easily shown that using an   value preserving transformation at each step leads to convergence to a policy that is   within   of optimal for all states  For controller reductions  relaxing the tolerance may lead to smaller controllers because some value can be sacrificed  For bounded backups  it may help in escaping from local optima  Though relaxing the tolerance for a bounded backup could lead to a decrease in value for some states  a small downward step could lead to higher value overall in the long run  We are currently working on testing these hypotheses empirically  General Sum Games In a general sum game  there is a set of agents  each with its own set of strategies  and a strategy profile is defined to be a tuple of strategies for all agents  Each agent assigns a payoff to each strategy profile  The agents may be noncooperative  so the same strategy profile may be assigned different values for each agent  The DEC POMDP model can be extended to a general sum game by allowing each agent to have its own reward function  In this case  the strategies are the local policies  and a strategy profile is a joint policy  This model is often called a partially observable stochastic game  POSG   Hansen et al         presented a dynamic programming algorithm for finitehorizon POSGs  The algorithm was shown to perform iterated elimination of dominated strategies in the game  Roughly speaking  it eliminates strategies that are not useful for an agent  regardless of the strategies of the other agents  Work remains to be done on extending the notion of a value preserving transformation to the noncooperative case  One possibility is to redefine value preserving transformations so that value is preserved for all agents  This is closely related to the idea of Pareto optimality  In a general sum game  a strategy profile is said to be Pareto optimal if there does not exist another strategy profile that yields higher payoff for all agents  It seems that policy iteration using the revised definition of value preserving transformation would tend to move the controller in the direction of the Pareto optimal set  Another possibility is       Policy Iteration for DEC POMDPs  to define value preserving transformations with respect to specific agents  As each agent transforms its own controller  the joint controller should move towards a Nash equilibrium  Handling Large Numbers of Agents The general DEC POMDP representation presented in this paper grows exponentially with the number of agents  as seen in the growth of the set of joint actions and observations as well as the transition  reward and observation functions  Thus this representation is not feasible for large numbers of agents  However  a compact representation is possible if each agent interacts directly with just a few other agents  We can have a separate state space for each agent  factored transition probabilities  and a reward function that is the sum of local reward functions for clusters of agents  In this case  the problem size is exponential only in the maximum number of agents interacting directly  This idea is closely related to recent work on graphical games  La Mura        Koller   Milch         Once we have a compact representation  the next question to answer is whether we can adapt policy iteration to work efficiently with the representation  This indeed seems possible  With the value preserving transformations we presented  the nodes of the other agents are considered part of the hidden state of the agent under consideration  These techniques modify the controller of the agent to get value improvement for all possible hidden states  When an agents state transitions and rewards do not depend on some other agent  it should not need to consider that agents nodes as part of its hidden state  A specific compact representation along with extensions of different algorithms was proposed by Nair et al           Acknowledgments We thank Martin Allen  Marek Petrik and Siddharth Srivastava for helpful discussions of this work  Marek and Siddharth  in particular  helped formalize and prove Theorem    The anonymous reviewers provided valuable feedback and suggestions  Support for this work was provided in part by the National Science Foundation under grants IIS         and IIS          by NASA under cooperative agreement NCC         and by the Air Force Office of Scientific Research under grants F                and FA                 Appendix A  Proof of Theorem   A correlation device produces a sequence of values that all the agents can observe  Let X be the set of all possible infinite sequences that can be generated by a correlation device  Let Vx   q    s    be the value of the correlated joint controller with respect to some correlation sequence x  X  initial nodes  q  of the agent controllers  and initial state s  of the problem  We will refer to Vx   q    s    simply as Vx  the value of some sequence x  given the controllers for the agents  We define a regular sequence as a sequence that can be generated by a regular expression  Before we prove Theorem    we establish the following property  Lemma   The value of any sequence  whether regular or non regular  can be approximated within any   by some other sequence  Proof  The property holds thanks to the discount factor used in infinite horizon DECPOMDPs  Given a sequence x with value Vx   we can determine another sequence x  such       Bernstein  Amato  Hansen    Zilberstein  that  Vx   Vx        The sequence x  is constructed by choosing the first k elements of x  and then choosing an arbitrary regular or non regular sequence for the remaining elements  kR max As long as k is chosen such that          then  Vx   Vx          Theorem   Given an initial state and a correlated joint controller  there always exists some finite size joint controller without a correlation device that produces at least the same value for the initial state  Proof  Let E represent the expected value of the joint controller with the correlation device  Let V    Vx   x  X  be the set of values produced by all the possible correlation device sequences  Let inf and sup represent the infimum and supremum of V respectively  We break the proof into two cases  depending on the relation of the expectation versus the supremum  We show in each case that a regular sequence can be found that produces at least the same value as E  Once such a regular sequence is found  then that sequence can be generated by a finite state controller that can be embedded within each agent  Thus  a finite number of nodes can be added to the agents controllers to provide equal or greater value  without using a correlation device  Case     inf  E   sup Based on Lemma    there is some regular sequence x that can approximate the supremum within    If we choose     sup E  then Vx  sup     E  Case     E   sup If there is a regular sequence  x  for which Vx   E  we can choose that sequence  If no such regular sequence exists  we will show that E    sup  We give a somewhat informal argument  but this can be more formally proven using cylinder sets as discussed by Parker         We begin by first choosing some regular sequence  We can construct a neighborhood around this sequence  as described in Lemma    by choosing a fixed length prefix of A prefixP of length k has a well defined probability that is defined as P the sequence  P       q           k   q k    where P  q     is the probability distribution P  q P  q c c c c c qc  qc  qck  P  qc of initial node of the correlation device and P  qci  qci    represents the probability of transitioning to correlation device node qci from node qci    The set of sequences that possess this prefix has probability equal to that of the prefix  Because we assumed there exists some regular sequence which has value less than the supremum  we can always choose a prefix and length such that the values of the sequences in the set are less than the supremum  Because the probability of this set is nonzero and the value of these sequences is less than the supremum  then E    sup  which is a contradiction  Therefore  some regular sequence can be found that provides at least the same value as the expected value of the correlated joint controller  This allows some uncorrelated joint controller to produce at least the same value as a given correlated one     Appendix B  Proof of Lemma   For ease of exposition  we prove the lemma under the assumption that there is no correlation device  Including a correlation device is straightforward but unnecessarily tedious        Policy Iteration for DEC POMDPs  Lemma   Let C and D be correlated joint controllers  and let C and D be the results of performing exhaustive backups on C and D  respectively  Then C  D if C  D  Proof  Suppose we are given controllers C and D  where C  D  Call the sets of joint   and R    respectively  It follows that there exists a function nodes for these controllers Q   fi   Qi  Ri for each agent i such that for all s  S and  q  Q V  s   q    X  P   r  q V  s   r      r  We now define functions fi to map between the two controllers C and D  For the old nodes  we define fi to produce the same output as fi   It remains to specify the results of fi applied to the nodes added by the exhaustive backup  New nodes of C will be mapped to distributions involving only new nodes of D  To describe the mapping formally  we need to introduce some new notation  Recall that the new nodes are all deterministic  For each new node  r in controller D  the nodes action is denoted  a  r   and its transition rule is denoted  r     r   o   Now  the mappings fi are defined such that P   r  q    P   a  r   q   YX  P   q     q   a  r    o P   r     r   o   q      q       o  for all  q in controller C and  r in controller D  We must now show that the mapping f satisfies the inequality given in the definition of a value preserving transformation  For the nodes that were not added by the exhaustive backup  this is straightforward  For the new nodes  q of the controller C  we have for all s  S   V  s   q     X  P   a  q  R s   a      X  P  s     o s   a P   q     q   a   o V  s     q        o s     q    a       X  P   a  q  R s   a     X  P  s     o s   a P   q     q   a   o     o s     q    a  X  P   r     q    V  s     r        r       X  P   a  q  R s   a      X  P  s     o s   a P   q     q   a   o P   r     q    V  s     r        o s     q      r    a      X     X  P   r  q  R s   a  r       X  P  s     o s   a  r  V  s     r     r   o      o s     r  P   r  q V  s   r      r          Bernstein  Amato  Hansen    Zilberstein  
