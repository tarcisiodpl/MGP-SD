 Multiagent planning and coordination problems are common and known to be computationally hard  We show that a wide range of two agent problems can be formulated as bilinear programs  We present a successive approximation algorithm that significantly outperforms the coverage set algorithm  which is the state of the art method for this class of multiagent problems  Because the algorithm is formulated for bilinear programs  it is more general and simpler to implement  The new algorithm can be terminated at any time andunlike the coverage set algorithmit facilitates the derivation of a useful online performance bound  It is also much more efficient  on average reducing the computation time of the optimal solution by about four orders of magnitude  Finally  we introduce an automatic dimensionality reduction method that improves the effectiveness of the algorithm  extending its applicability to new domains and providing a new way to analyze a subclass of bilinear programs      Introduction We present a new approach for solving a range of multiagent planning and coordination problems using bilinear programming  The problems we focus on represent various extensions of the Markov decision process  MDP  to multiagent settings  The success of MDP algorithms for planning and learning under uncertainty has motivated researchers to extend the model to cooperative multiagent problems  One possibility is to assume that all the agents share all the information about the underlying state  This results in a multiagent Markov decision process  Boutilier         which is essentially an MDP with a factored action set  A more complex alternative is to allow only partial sharing of information among agents  In these settings  several agentseach having different partial information about the worldmust cooperate with each other in order to achieve some joint objective  Such problems are common in practice and can be modeled as decentralized partially observable MDPs  DEC POMDPs   Bernstein  Zilberstein    Immerman         Some refinements of this model have been studied  for example by making certain independence assumptions  Becker  Zilberstein    Lesser        or by adding explicit communication actions  Goldman   Zilberstein         DEC POMDPs are closely related to extensive games  Rubinstein         In fact  any DEC POMDP represents an exponentially larger extensive game with a common objective  Unfortunately  DEC POMDPs with just two agents are intractable in general  unlike MDPs that can be solved in polynomial time  Despite recent progress in solving DEC POMDPs  even state of the art algorithms are generally limited to very small problems  Seuken   Zilberstein         This has motivated the development of algorithms that either solve a restricted class of problems  Becker  c      AI Access Foundation  All rights reserved    Petrik   Zilberstein  Lesser    Zilberstein        Kim  Nair  Varakantham  Tambe    Yokoo        or provide only approximate solutions  Emery Montemerlo  Gordon  Schneider    Thrun        Nair  Roth  Yokoo    Tambe        Seuken   Zilberstein         In this paper  we introduce an efficient algorithm for several restricted classes  most notably decentralized MDPs with transition and observation independence  Becker et al          For the sake of simplicity  we denote this model as DEC MDP  although this is usually used to denote the model without the independence assumptions  The objective in these problems is to maximize the cumulative reward of a set of cooperative agents over some finite horizon  Each agent can be viewed as a single decision maker operating on its own local MDP  What complicates the problem is the fact that all these MDPs are linked through a common reward function that depends on their states  The coverage set algorithm  CSA  was the first optimal algorithm to solve efficiently transition and observation independent DEC MDPs  Becker  Zilberstein  Lesser    Goldman         By exploiting the fact that the interaction between the agents is limited compared to their individual local problems  CSA can solve problems that cannot be solved by the more general exact DEC POMDP algorithms  It also exhibits good anytime behavior  However  the anytime behavior is of limited applicability because solution quality is only known in hindsight  after the algorithm terminates  We develop a new approach to solve DEC MDPsas well as a range of other multiagent planning problemsby representing them as bilinear programs  We also present an efficient new algorithm for solving these kinds of separable bilinear problems  When the algorithm is applied to DEC MDPs  it improves efficiency by several orders of magnitude compared with previous state of the art algorithms  Becker        Petrik   Zilberstein      a   In addition  the algorithm provides useful runtime bounds on the approximation error  which makes it more useful as an anytime algorithm  Finally  the algorithm is formulated for general separable bilinear programs and therefore it can be easily applied to a range of other problems  The rest of the paper is organized as follows  First  in Section    we describe the basic bilinear program formulation and how a range of multiagent planning problems can be expressed within this framework  In Section    we describe a new successive approximation algorithm for bilinear programs  The performance of the algorithm depends heavily on the number of interactions between the agents  To address that  we propose in Section   a method that automatically reduces the number of interactions and provides a bound on the degradation in solution quality  Furthermore  to be able to project the computational effort required to solve a given problem instance  we develop offline approximation bounds in Section    In Section    we examine the performance of the approach on a standard benchmark problem  We conclude with a summary of the results and a discussion of future work that could further improve the performance of this approach      Formulating Multiagent Planning Problems as Bilinear Programs We begin with a formal description of bilinear programs and the different types of multiagent planning problems that can be formulated as such  In addition to multiagent planning problems  bilinear programs can be used to solve a variety of other problems such as robotic manipulation  Pang  Trinkle    Lo         bilinear separation  Bennett   Mangasarian        A Bilinear Programming Approach for Multiagent Planning         and even general linear complementarity problems  Mangasarian         We focus on multiagent planning problems where this formulation turns out to be particularly effective  Definition    A separable bilinear program in the normal form is defined as follows  maximize w x y z  T T T T f  w  x  y  z    sT   w   r  x   x Cy   r  y   s  z  subject to A  x   B  w   b        A  y   B  z   b  w  x  y  z     The size of the program is the total number of variables in w  x  y and z  The number of variables in y determines the dimensionality of the program    Unless otherwise specified  all vectors are column vectors  We use boldface   and   to denote vectors of zeros and ones respectively of the appropriate dimensions  This program specifies two linear programs that are connected only through the nonlinear objective function term xT Cy  The program contains two types of variables  The first type includes the variables x  y that appear in the bilinear term of the objective function  The second type includes the additional variables w  z that do not appear in the bilinear term  As we show later  this distinction is important because the complexity of the algorithm we propose depends mostly on the dimensionality of the problem  which is the number of variables y involved in the bilinear term  The bilinear program in Eq      is separable because the constraints on x and w are independent of the constraints on y and z  That is  the variables that participate in the bilinear term of the objective function are independently constrained  The theory of nonseparable bilinear programs is much more complicated and the corresponding algorithms are not as efficient  Horst   Tuy         Thus  we limit the discussion in this paper to separable bilinear programs and often omit the term separable  As discussed later in more detail  a separable bilinear program may be seen as a concave minimization problem with multiple local minima  It can be shown that solving this problem is NP complete  compared to polynomial time complexity of linear programs  In addition to the formulation of the bilinear program shown in Eq       we also use the following formulation  stated in terms of inequalities  maximize x y  xT Cy  subject to A  x  b   x   A  y  b   y        The latter formulation can be easily transformed into the normal form using standard transformations of linear programs  Vanderbei         In particular  we can introduce slack    It is possible to define the dimensionality in terms of x  or the minimum of dimensions of x and y  The issue is discussed in Appendix B         Petrik   Zilberstein  variables w  z to obtain the following identical bilinear program in the normal form  xT Cy  maximize w x y z  subject to A  x  w   b  A  y  z   b        w  x  y  z    We use the following matrix and block matrix notation in the paper  Matrices are denoted by square brackets  with columns separated by commas and rows separated by semicolons  Columns have precedence over rows  For example  the notation  A  B  C  D    corresponds to the matrix  A C  B   D  As we show later  the presence of the variables w  z in the objective function may prevent a crucial function from being convex  Since this has an unfavorable impact on the properties of the bilinear program  we introduce a compact form of the problem  Definition    We say that the bilinear program in Eq      is in a compact form when s  and s  are zero vectors  It is in a semi compact form if s  is a zero vector  The compactness requirement is not limiting because any bilinear program in the form shown in Eq      can be expressed in a semi compact form as follows  maximize w x y z x y  sT  w     r T x     xT  subject to A  x   B  w   b  x     y             C   y x   r T y y     A  y   B  z   b        sT  z  w  x  y  z    Clearly  feasible solutions of Eq      and Eq      have the same objective value when y is set appropriately  Notice that the dimensionality of the bilinear term in the objective function increases by   for both x and y  Hence  this transformation increases the dimensionality of the program by    The rest of this section describes several classes of multiagent planning problems that can be formulated as bilinear programs  Starting with observation and transition independent DEC MDPs  we extend the formulation to allow a different objective function  maximizing average reward over an infinite horizon   to handle interdependent observations  and to find Nash equilibria in competitive settings      DEC MDPs As mentioned previously  any transition independent and observation independent DECMDP  Becker et al         may be formulated as a bilinear program  Intuitively  a DECMDP is transition independent when no agent can influence the other agents transitions  A DEC MDP is observation independent when no agent can observe the states of other agents  These assumptions are crucial since they ensure a lower complexity of the problem  Becker       A Bilinear Programming Approach for Multiagent Planning  et al          In the remainder of the paper  we use simply the term DEC MDP to refer to transition and observation independent DEC MDP  The DEC MDP model has proved useful in several multiagent planning domains  One example that we use is the Mars rover planning problem  Bresina  Golden  Smith    Washington         first formulated as a DEC MDP by Becker et al          This domain involves two autonomous rovers that visit several sites in a given order and may decide to perform certain scientific experiments at each site  The overall activity must be completed within a given time limit  The uncertainty about the duration of each experiment is modeled by a given discrete distribution  While the rovers operate independently and receive local rewards for each completed experiment  the global reward function also depends on some experiments completed by both rovers  The interaction between the rovers is thus limited to a relatively small number of such overlapping tasks  We return to this problem and describe it in more detail in Section    A DEC MDP problem is composed of two MDPs with state sets S    S  and action sets A    A    The functions r  and r  define local rewards for action state pairs  The initial state distributions are   and     The MDPs are coupled through a global reward function defined by the matrix R  Each entry R i  j  represents the joint reward for the state action i by one agent and j by the other  Our definition of a DEC MDP is based on the work of Becker et al          with some modifications that we discuss below  Definition    A two agent transition and observation independent DEC MDP with extended reward structure is defined by a tuple hS  F    A  P  Ri   S    S    S    is the factored set of world states  F    F   S    F   S    is the factored set of terminal states               where i   Si          are the initial state distribution functions  A    A    A    is the factored set of actions  P    P    P     Pi   Si  Ai  Si          are the transition functions  Let a  Ai be an action  then Pia   Si  Si          is a stochastic transition matrix such that Pi  s  a  s      Pia  s  s    is the probability of a transition from state s  Si to state s   Si of agent i  assuming it takes action a  The transitions from the final states have   probability  that is Pi  s  a  s        if s  Fi   s   Si   and a  Ai    R    r    r    R  where ri   Si  Ai   R are the local reward functions and R    S   A      S   A      R is the global reward function  Local rewards ri are represented as vectors  and R is a matrix with  s    a    as rows and  s    a    as columns  Definition   differs from the original definition of transition and observation independent DEC MDP  Becker et al        Definition    in two ways  The modifications allow us to explicitly capture assumptions that are implicit in previous work  First  the individual MDPs in our model are formulated as stochastic shortest path problems  Bertsekas   Tsitsiklis         That is  there is no explicit time horizon  but instead some states are terminal  The process stops upon reaching a terminal state  The objective is to maximize the cumulative reward received before reaching the terminal states  The second modification of the original definition is that Definition   generalizes the reward structure of the DEC MDP formulation  using the extended reward structure  The joint rewards in the original DEC MDP are defined only for the joint states  s   S    s   S          Petrik   Zilberstein  s    s    s    s    s    s    s    t   s   s   s  t   s    t   s    Figure    An MDP and its stochastic shortest path version with time horizon    The dotted circles are terminal states   visited by both agents simultaneously  That is  if agent   visits states s     s   and agent   visits states s     s     then the reward can only be defined for joint states  s     s     and  s     s      However  our DEC MDP formulation with extended reward structure also allows the reward to depend on  s     s     and  s     s      even when they are not visited simultaneously  As a result  the global reward may depend on the history  not only on the current state  Note that this reward structure is more general than what is commonly used in DEC POMDPs  We prefer the more general definition because it has been already implicitly used in previous work  In particular  this extended reward structure arises from introducing the primitive and compound events in the work of Becker et al          This reward structure is necessary to capture the characteristics of the Mars rover benchmark  Interestingly  this extension does not complicate our proposed solution methods in any way  Note that the stochastic shortest path formulation  right side of Figure    inherently eliminates any loops because time always advances when an action is taken  Therefore  every state in that representation may be visited at most once  This property is commonly used when an MDP is formulated as a linear program  Puterman         The solution of a DEC MDP is a deterministic stationary policy              where i   Si   Ai is the standard MDP policy  Puterman        for agent i  In particular  i  si   represents the action taken by agent i in state si   To define the bilinear program  we use variables x s    a    to denote the probability that agent   visits state s  and takes action a  and y s    a    to denote the same for agent    These are the standard dual variables in MDP formulation  Given a solution in terms of x for agent    the policy is calculated for s  S  as follows  breaking ties arbitrarily     s    arg max x s  a  aA   The policy   is similarly calculated from y  The correctness of the policy calculation follows from the existence of an optimal policy that is deterministic and depends only on the local states of that agent  Becker et al          The objective in DEC MDPs in terms of x and y is then to maximize  X X X X r   s    a   x s    a      R s    a    s    a   x s    a   y s    a      r   s    a   y s    a     s  S  a  A   s  S  s  S  a  A  a  A   s  S  a  A   The stochastic shortest path representation is more general because any finite horizon MDP can be represented as such by keeping track of time as part of the state  as illustrated       A Bilinear Programming Approach for Multiagent Planning  s    s    s    r   s    s    s    s   s    s    Figure    A sample DEC MDP   in Figure    This modification allows us to apply the model directly to the Mars rover benchmark problem  Actions in the Mars rover problem may have different durations  while all actions in finite horizon MDPs take the same amount of time  A DEC MDP problem with an extended reward structure can be formulated as a bilinear mathematical program as follows  Vector variables x and y represent the state action probabilities for each agent  as used in the dual linear formulation of MDPs  Given the transition and observation independence  the feasible regions may be defined by linear equalities A  x     and x     and A  y     and y     The matrices A  and A  are the same as for the dual formulation of total expected reward MDPs  Puterman         representing the following equalities for agent i  X a  Ai  X X  x s    a      Pi  s  a  s   x s  a    i  s      sSi aAi  for every s   Si   As described above  variables x s  a  represent the probabilities of visiting the state s and taking action a by the appropriate agent during plan execution  Note that for agent    the variables are y s  a  rather than x s  a   Intuitively  these equalities ensure that the probability of entering each non terminal state  through either the initial step or from other states  is the same as the probability of leaving the state  The bilinear problem is then formulated as follows  maximize x y  r T x   xT Ry   r T y  subject to A  x      x   A  y       y        In this formulation  we treat the initial state distributions i as vectors  based on a fixed ordering of the states  The following simple example illustrates the formulation  Example    Consider a DEC MDP with two agents  depicted in Figure    The transitions in this problem are deterministic  and thus all the branches represent actions ai   ordered for each state from left to right  In some states  only one action is available  The shared reward r    denoted by a dotted line  is received when both agents visit the state  The local rewards are denoted by the numbers above next to the states  The terminal states are omitted  The       Petrik   Zilberstein  agents start in states s   and s   respectively  The bilinear formulation of this problem is  maximize subject to  x s     a     r   y s     a    x s     a        x s    a      x s    a     x s     a    x s     a     x s     a    x s     a     x s     a                  y s     a      y s     a    y s     a     y s     a    y s     a     y s     a    y s     a     y s     a    y s     a     y s     a                     While the results in this paper focus on two agent problems  our approach can be extended to DEC MDPs with more than two agents in two ways  The first approach requires that each component of the global reward depends on at most two agents  The DEC MDP then may be viewed as a graph with vertices representing agents and edges representing the immediate interactions or dependencies  To formulate the problem as a bilinear program  this graph must be bipartite  Interestingly  this class of problems has been previously formulated  Kim et al          Let G  and G  be the indices of the agents in the two partitions of the bipartite graph  Then the problem can be formulated as follows  X T maximize riT xi   xT i Rij yj   rj yj x y  iG   jG   subject to Ai xi      x i    i  G   Aj yj      yj    j  G        Here  Rij denotes the global reward for interactions between agents i and j  This program is bilinear and separable because the constraints on the variables in G  and G  are independent  The second approach to generalize the framework is to represent the DEC MDP as a multilinear program  In that case  no restrictions on the reward structure are necessary  An algorithm to solve  say a trilinear program  could be almost identical to the algorithm we propose  except that the best response would be calculated using bilinear  not linear programs  However  the scalability of this approach to more than a few agents is doubtful      Average Reward Infinite Horizon DEC MDPs The previous formulation deals with finite horizon DEC MDPs  An average reward problem may also be formulated as a bilinear program  Petrik   Zilberstein      b   This is particularly useful for infinite horizon DEC MDPs  For example  consider the infinitehorizon version of the Multiple Access Broadcast Channel  MABC   Rosberg        Ooi   Wornell         In this problem  which has been used widely in recent studies of decentralized decision making  two communication devices share a single channel  and they need to periodically transmit some data  However  the channel can transmit only a single message at a time  When both agents send messages at the same time  this leads to a collision  and the transmission fails  The memory of the devices is limited  thus they need to send the messages sooner rather than later  We adapt the model from the work of Rosberg         which is particularly suitable because it assumes no sharing of local information among the devices        A Bilinear Programming Approach for Multiagent Planning  The definition of average reward two agent transition and observation independent DECMDP is the same as Definition    with the exception of the terminal states  policy  and objective  There are no terminal states in average reward DEC MDPs  and the policy          may be stochastic  That is  i  s  a           is the probability of agent i taking an action a in state s  The objective is to find a stationary infinite horizon policy  that maximizes the average reward  or gain  defined as follows  Definition    Let             be a stochastic policy  and Xt and Yt be random variables that represent the probability distributions over the state action pairs at time t of the two agents respectively according to   The gain G of the policy  is then defined for states s   S  and s   S  as   N     X   G s   s     lim E s      s      s      s     r   Xt     R Xt   Yt     r   Yt     N  N       t    where i  si   is the distribution over the actions in state si   Note that the expectation is with respect to the initial states and action distributions  s       s       s       s      The actual gain of a policy depends on the agents initial state distributions       and may be expressed as  T G    with G represented as a matrix  Puterman         for example  provides a more detailed discussion of the definition and meaning of policy gain  To simplify the bilinear formulation of the average reward DEC MDP  we assume that r      and r       The bilinear program follows  maximize p   p   q   q     p    p    q    q      pT   Rp   subject to pX     p     s   S  p   s    a   s   S  s   S  s   S   aA X  aA X  aA X  aA   p   s    a    p   s    a   p   s    a     X    p   s  a P a s  s       sS X   aA   q   s    a    aAX    X    q   s  a P a s  s       s     sS   aA    p   s  a P a s  s   sS X   aA   q   s    a    aA   X             q   s  a P a s  s       s     sS   aA   The variables in the program come from the dual formulation of the average reward MDP linear program  Puterman         The state sets of the MDPs is divided into recurrent and transient states  The recurrent states are expected to be visited infinitely many times  while the transient states are expected to be visited finitely many times  Variables p  and p  represent the limiting distributions of each MDP  which is non zero for all recurrent states  The  possibly stochastic  policy i of agent i is defined in the recurrent states by the probability of taking action a  Ai in state s  Si   pi  s  a      a  Ai pi  s  a    i  s  a    P        Petrik   Zilberstein  a    a    b    b    b    b    a    a    a    a    a    a    a    a    r   r   r   r   r   r   r   r   Figure    A tree form of a policy for a DEC POMDP or an extensive game  The dotted ellipses denote the information sets   The variables pi are   in transient states  The policy in the transient states is calculated from variables qi as  qi  s  a  i  s  a    P     a  Ai qi  s  a   The correctness of the constraints follows from the dual formulation of optimal average reward  Puterman       Equation         Petrik and Zilberstein      b  provide further details of this formulation      General DEC POMDPs and Extensive Games The general DEC POMDP problem and extensive form games with two agents  or players  can also be formulated as bilinear programs  However  the constraints may not be separable because actions of one agent influence the other agent  The approach in this case may be similar to linear complementarity problem formulation of extensive games  Koller  Megiddo    von Stengel         and integer linear program formulation of DEC POMDPs  Aras   Charpillet         The approach we develop is closely related to event driven DECPOMDPs  Becker et al          but it is in general more efficient  Nevertheless  the size of the bilinear program is exponential in the size of the DEC POMDP  This can be expected since solving DEC POMDPs is NEXP complete  Bernstein et al          while solving bilinear programs is NP complete  Mangasarian         Because the general formulation in this case is somewhat cumbersome  we only illustrate it using the following simple example  Aras        provides the details of a similar construction  Example    Consider the problem depicted in Figure    assuming that the agents are cooperative  The actions of the other agent are not observable  as denoted by the information sets  This approach can be generalized to any problem with any observable sets as long as the perfect recall condition is satisfied  Agents satisfy the perfect recall condition when they remember the set of actions taken in the prior moves  Osborne   Rubinstein         Rewards are only collected in the leaf nodes in this case  The variables on the edges represent the probability of taking the action  Here  variables a denote the actions of one agent  and       A Bilinear Programming Approach for Multiagent Planning  variables b of the other  The total common reward received in the end is  r   a   b   a   r    a   b   a   r    a   b   a   r    a   b   a   r    a   b   a   r    a   b   a   r    a   b   a   r    a   b   a   r    The constraints in this problem are of the following form  a     a        Any DEC POMDP problem can be represented using the approach used above  It is also straightforward to extend the approach to problems with rewards in every node  However  the above formulation is clearly not bilinear  To apply our algorithm to this class of problems  we need to reformulate the problem in a bilinear form  This can be easily accomplished in a way similar to the construction of the dual linear program for an MDP  Namely  we introduce variables  c     a   c     a   c     a   a   c     a   a   and so on for every set of variables on any path to a leaf node  Then  the objective may be reformulated as follows  r   c   b   r    c   b   r    c   b   r    c   b   r    c   b   r    c   b   r    c   b   r    c   b   r    Variables bij are replaced in the same fashion  This objective function is clearly bilinear  The constraints may be reformulated as follows  The constraint a     a       can be multiplied by a   and then replaced by c     c     c     and so on  That is  the variables in each level have to sum to the variable that is their least common parent in the level above for the same agent      General Two Player Games In addition to cooperative problems  some competitive problems with   players may be formulated as bilinear programs  It is known that the problem of finding an equilibrium for a bi matrix game may be formulated as a linear complementarity problem  Cottle  Pang    Stone         It has also been shown that a linear complementarity problem may be formulated as a bilinear problem  Mangasarian         However  a direct application of these two reductions results in a complex problem with a large dimensionality  Below  we demonstrate how a general game can be directly formulated as a bilinear program  There are many ways to formulate a game  thus we take a very general approach  We simply assume that each agent optimizes a linear program  as follows  maximize x  maximize  d   x    r T x   xT C  y  subject to A  x   b   y       d   y    r T y   xT C  y  subject to A  y   b  y   x             Petrik   Zilberstein  In Eq       the variable y is considered to be a constant and similarly in Eq      the variable x is considered to be a constant  For normal form games  the constraint matrices A  and A  are simply rows of ones  and b    b       For competitive DEC MDPs  the constraint matrices A  and A  are the same as in Section      Extensive games may be formulated similarly to DEC POMDPs  as described in Section      The game specified by linear programs Eq      and Eq      may be formulated as a bilinear program as follows  First  define the reward vectors for each agent  given a policy of the other agent  q   y    r    C  y q   x    r    C T x   These values are unrelated to those of Eq       The complementary slackness values  Vanderbei        for the linear programs Eq      and Eq      are      k   x  y        q   y T  T   A  x     k   x  y        q   x T  T A     y  where   and   are the dual variables of the corresponding linear programs  For any primal feasible x and y  and dual feasible   and     we have that k   x  y         and k   x  y          The equality is attained if and only if x and y are optimal  This can be used to write the following optimization problem  in which we implicitly assume that x y      are feasible in the appropriate primal and dual linear programs             min k   x  y        k   x  y       x y       T T min  q   y T  T   A   x    q   x     A   y  x y       T T T min   r    C  y T  T   A   x     r    C  x     A   y  x y       T T min r T x   r T y   xT  C    C   y  xT AT       y A      x y       T min r T x   r T y   xT  C    C   y  bT      b       x y       Therefore  any feasible x and y that set the right hand side to   solve both linear programs in Eq      and Eq      optimally  Adding the primal and dual feasibility conditions to the above  we get the following bilinear program  minimize x y       T r T x   r T y   xT  C    C   y  bT      b     subject to A  x   b   A  y   b   r    C  y  AT        r    C T x  AT        x   y              A Bilinear Programming Approach for Multiagent Planning  Algorithm    IterativeBestResponse B  x    w   rand   i   while yi     yi or xi     xi do  yi   zi    arg maxy z f  wi    xi    y  z     xi   wi    arg maxx w f  w  x  yi   zi       ii                  return f  wi   xi   yi   zi    The optimal solution of Eq       is   and it corresponds to a Nash equilibrium  This is because both the primal variables x  y and dual variables       are feasible and the complementary slackness condition is satisfied  The open question in this example are the interpretation of an approximate result and a formulation that would select the equilibrium  It is not clear yet whether it is possible to formulate the program so that the optimal solution will be a Nash equilibrium that maximizes a certain criterion  The approximate solutions of the program probably correspond to   Nash equilibria  but this remain an open question  The algorithm in this case also relies on the number of shared rewards being small compared to the size of the problem  But even if this is not the case  it is often possible that the number of shared rewards may be automatically reduced as described in Section    In fact  it is easy to show that a zero sum normal form game is automatically reduced to two uncoupled linear programs  This follows from the dimensionality reduction procedure in Section        Solving Bilinear Programs One simple method often used for solving bilinear programs is the iterative procedure shown in Algorithm    The parameter B represents the bilinear program  While the algorithm often performs well in practice  it tends to converge to a suboptimal solution  Mangasarian         When applied to DEC MDPs  this algorithm is essentially identical to JESP  Nair  Tambe  Yokoo  Pynadath    Marsella       one of the early solution methods  In the following  we use f  w  x  y  z  to denote the objective value of Eq       The rest of this section presents a new anytime algorithm for solving bilinear programs  The goal of the algorithm to is to produce a good solution quickly and then improve the solution in the remaining time  Along with each approximate solution  the maximal approximation bound with respect to the optimal solution is provided  As we show below  our algorithm can benefit from results produced by suboptimal algorithms  such as Algorithm    to quickly determine tight approximation bounds      The Successive Approximation Algorithm We begin with an overview of a successive approximation algorithm for bilinear problems that takes advantage of a low number of interactions between the agents  It is particularly suitable when the input problem is large in comparison to its dimensionality  as defined in Section    We address the issue of dimensionality reduction in Section          Petrik   Zilberstein  We begin with a simple intuitive explanation of the algorithm  and then show how it can be formalized  The bilinear program can be seen as an optimization game played by two agents  in which the first agent sets the variables w  x and the second one sets the variables y  z  This is a general observation that applies to any bilinear program  In any practical application  the feasible sets for the two sets of variables may be too large to explore exhaustively  In fact  when this method is applied to DEC MDPs  these sets are infinite and continuous  The basic idea of the algorithm is to first identify the set of best responses of one of the agents  say agent    to some policy of the other agent  This is simple because once the variables of agent   are fixed  the program becomes linear  which is relatively easy to solve  Once the set of best response policies of agent   is identified  assuming it is of a reasonable size  it is possible to calculate the best response of agent    This general approach is also used by the coverage set algorithm  Becker et al          One distinction is that the representation used in CSA applies only to DEC MDPs  while our formulation applies to bilinear programsa more general representation  The main distinction between our algorithm and CSA is the way in which the variables y  z are chosen  In CSA  the values y  z are calculated in a way that simply guarantees termination in finite time  We  on the other hand  choose values y  z greedily so as to minimize the approximation bound on the optimal solution  This is possible because we establish bounds on the optimality of the solution throughout the calculation  As a result  our algorithm converges more rapidly and may be terminated at any time with a guaranteed performance bound  Unlike the earlier version of the algorithm  Petrik   Zilberstein      a   the version described in this paper calculates the best response using only a subset of the values of y  z  As we show  it is possible to identify regions of y  z in which it is impossible to improve the current best solution and exclude these regions from consideration  We now formalize the ideas described above  To simplify the notation  we define feasible sets as follows  X     x  w  A  x   B  w   b    Y      y  z  A  y   B  z   b      We use y  Y to denote that there exists z such that  y  z   Y   In addition  we assume that the problem is in a semi compact form  This is reasonable because any bilinear program may be converted to semi compact form with an increase in dimensionality of one  as we have shown earlier  Assumption    The sets X and Y are bounded  that is  they are contained in a ball of a finite radius  While Assumption   is limiting  coordination problems under uncertainty typically have bounded feasible sets because the variables correspond to probabilities bounded to         Assumption    The bilinear program is in a semi compact form  The main idea of the algorithm is to compute a set X  X that contains only those elements that satisfy a necessary optimality condition  The set X is formally defined as follows          X   x   w    y  z   Y f  w   x   y  z    max f  w  x  y  z     x w X        A Bilinear Programming Approach for Multiagent Planning  As described above  this set may be seen as a set of best responses of one agent to the variable settings of the other  The best responses are easy to calculate since the bilinear program in Eq      reduces to a linear program for fixed w  x or fixed y  z  In our algorithm  we assume that X is potentially a proper subset of all necessary optimality points and focus on the approximation error of the optimal solution  Given the set X  the following simplified problem is solved  maximize w x y z  f  w  x  y  z   subject to  x  w   X        A  y   B  z   b  y  z     Unlike the original continuous set X  the reduced set X is discrete and small  Thus the elements of X may be enumerated  For a fixed w and x  the bilinear program in Eq       reduces to a linear program  To help compute the approximation bound and to guide the selection of elements for X  we use the best response function g y   defined as follows  g y     max   w x z  x w X  y z Y    f  w  x  y  z     max   x w  x w X   f  w  x  y       with the second equality for semi compact programs only and feasible y  Y   Note that g y  is also defined for y    Y   in which case the choice of z is arbitrary since it does not influence the objective function  The best response function is easy to calculate using a linear program  The crucial property of the function g that we use to calculate the approximation bound is its convexity  The following proposition holds because g y    max x w  x w X  f  w  x  y     is a maximum of a finite set of linear functions  Proposition    The function g y  is convex when the program is in a semi compact form  Proposition   relies heavily on the separability of Eq       which means that the constraints on the variables on one side of the bilinear term are independent of the variables on the other side  The separability ensures that w  x are valid solutions regardless of the values of y  z  The semi compactness of the program is necessary to establish convexity  as shown in Example    in Appendix C  The example is constructed using the properties described in the appendix  which show that f  w  x  y  z  may be expressed as a sum of a convex and a concave function  We are now ready to describe Algorithm    which computes the set X for a bilinear problem B such that the approximation error is at most      The algorithm iteratively adds the best response  x  w  for a selected pivot point y into X  The pivot points are selected hierarchically  At an iteration j  the algorithm keeps a set of polyhedra S        Sj which represent the triangulation of the feasible space Y   which is possible based on Assumption    For each polyhedron Si    y        yn      the algorithm keeps a bound  i on the maximal difference between the optimal solution on the polyhedron and the best solution found so far  This error bound on a polyhedron Si is defined as   i   e Si      max   w x y  x w X ySi    f  w  x  y       max  w x y  x w X ySi         f  w  x  y        Petrik   Zilberstein  Algorithm    BestResponseApprox B       returns  w  x  y  z                                           Create the initial polyhedron S    S    y        yn      Y  S       Add best responses for vertices of S  to X X   arg max x w X f  w  x  y             arg max x w X f  w  x  yn              Calculate the error   and pivot point  of the initial polyhedron            P olyhedronError S         Section     Section        Initialize the number of polyhedra to   j      Continue until reaching a predefined precision    while maxi       j  i     do    Find the polyhedron with the largest error i  arg maxk       j  k      Select the pivot point of the polyhedron with the largest error y  i      Add the best response to the pivot point y to the set X X  X   arg max x w X f  w  x  y           Calculate errors and pivot points of the refined polyhedra for k              n     do j j        Replace the k th vertex by the pivot point y Sj   y  y        yk    yk           yn         j   j    P olyhedronError Sj        Section     Section        Take the smaller of the errors on the original and the refined polyhedron  The error may not increase with the refinement  although the bound may  j  min  i    j        Set the error of the refined polyhedron to    since the region is covered by the refinements  i        w  x  y  z   arg max w x y z    return  w  x  y  z          x w X  y z Y    f  w  x  y        where X represents the current  not final  set of best responses  Next  a point y  is selected as described below and n     new polyhedra are created by replacing one of the vertices by y  to get   y    y             y    y    y                     y            yn   y     This is depicted for a   dimensional set Y in Figure    The old polyhedron is discarded and the above procedure is then repeatedly applied to the polyhedron with the maximal approximation error  For the sake of clarity  the pseudo code of Algorithm   is simplified and does not address any efficiency issues  In practice  g yi   could be cached  and the errors  i could be stored in a prioritized heap or at least in a sorted array  In addition  a lower bound li and an upper bound ui is calculated and stored for each polyhedron Si    y        yn      The function e Si   calculates their maximal difference on the polyhedron Si and the point where it is attained  The error bound  i on the polyhedron Si may not be tight  as we describe in Remark     As a result  when the polyhedron Si is refined to n polyhedra S         Sn  with online error       A Bilinear Programming Approach for Multiagent Planning  y   y  y   y   Figure    Refinement of a polyhedron in two dimensions with a pivot y    bounds             n   it is possible that for some k    k    i   Since S         Sn   Si   the true error on Sk  is less than on Si and therefore   k may be set to  i   Conceptually  the algorithm is similar to CSA  but there are some important differences  The main difference is in the choice of the pivot point y  and the bounds on g  CSA does not keep any upper bound and it evaluates g y  on all the intersection points of planes defined by the current solutions in X  That guarantees that g y  is eventually known precisely  Becker et al          A similar approach was also taken for POMDPs  Cheng         The  X    upper bound on the number of intersection points in CSA is dim Y   The principal problem is that the bound is exponential in the dimension of Y   and experiments do not show a slower growth in typical problems  In contrast  we choose the pivot points to minimize the approximation error  This is more selective and tends to more rapidly reduce the error bound  In addition  the error at the pivot point may be used to determine the overall error bound  The following proposition states the soundness of the triangulation  proved in Appendix A  The correctness of the triangulation establishes that in each iteration the approximation error over Y is equivalent to the maximum of the approximation errors over the current polyhedra S        Sj   Proposition     In the proposed triangulation  the sub polyhedra do not overlap and they cover the whole feasible set Y   given that the pivot point is in the interior of S       Online Error Bound The selection of the pivot point plays a key role in the performance of the algorithm  in both calculating the error bound and the speed of convergence to the optimal solution  In this section we show exactly how we use the triangulation in the algorithm to calculate an error bound  To compute the approximation bound  we define the approximate best response function g y  as  g y    max f  w  x  y       x w  x w X   Notice that z is not considered in this expression  since we assume that the bilinear program is in the semi compact form  The value of the best approximate solution during the execution of the algorithm is  max  f  w  x  y       max g y   yY   w x y z  x w X yY          Petrik   Zilberstein  This value can be calculated at runtime when each new element of X is added  Then the maximal approximation error between the current solution and the optimal one may be calculated from the approximation error of the best response function g    as stated by the following proposition  Proposition     Consider a bilinear program in a semi compact form  Then let w  x  y be an optimal solution of Eq       and let w   x   y  be an optimal solution of Eq       The approximation error is then bounded by  f  w   x   y        f  w  x  y      max  g y   g y     yY  Proof  f  w   x   y        f  w  x  y       max g y   max g y   max g y   g y  yY  yY  yY  Now  the approximation error is maxyY g y   g y   which is bounded by the difference between an upper bound and a lower bound on g y   Clearly  g y  is a lower bound on g y   Given points in which g y  is the same as the best response function g y   we can use Jensens inequality to obtain the upper bound  This is summarized by the following lemma  Lemma     Let yi  Y for i              n     such that g yi     g yi    Then  P Pn   Pn   n   g i   ci yi  i   ci g yi   when i   ci     and ci    for all i  The actual implementation of the bound relies on the choice of the pivot points  Next we describe the maximal error calculation on a single polyhedron defined by S    y        yn    Let matrix T have yi as columns  and let L    x        xn     be the set of the best responses for its vertices  The matrix T is used to convert any y in absolute coordinates to a relative representation t that is a convex combination of the vertices  This is defined formally as follows        y   T t   y  y        t          T t    t where the yi s are column vectors  We can represent a lower bound l y  for g y  and an upper bound u y  for g y  as  l y    max rT x   xT Cy xL  u y     g y     g y           T t    g y     g y           T     T  T         y      The upper bound correctness follows from Lemma     Notice that u y  is a linear function  which enables us to use a linear program to determine the maximal error point        A Bilinear Programming Approach for Multiagent Planning  Algorithm    PolyhedronError B  S  P  one of Eq        or       or       or        t  the optimal solution of P      the optimal objective value of P      Coordinates t are relative to the vertices of S  convert them to absolute values in Y     Tt     return                 Remark     Notice that we use L instead of X in calculating l y   Using all of X would lead to a tighter bound  as it is easy to show in three dimensional examples  However  this also would substantially increase the computational complexity  Now  the error on a polyhedron S may be expressed as  e S   max u y   l y    max u y   max rT x   xT Cy yS  yS  T  xL  T    max min u y   r x  x Cy  yS xL  We also have     y  S  y   T t  t      T t       As a result  the point with the maximal error bound may be determined using the following linear program in terms of variables t     maximize t       subject to    u T t   rT x  xT CT t  x  L         T t     t    Here x is not a variable  The formulation is correct because all feasible solutions are bounded below the maximal error and any maximal error solution is feasible  Proposition     The optimal solution of Eq       is equivalent to maxyS  u y   l y    We thus select the next pivot point to greedily minimize the error  The maximal difference is actually achieved in points where some of the planes meet  as Becker et al         have suggested  However  checking these intersections is very similar to running the simplex algorithm  In general  the simplex algorithm is preferable to interior point methods for this program because of its small size  Vanderbei         Algorithm   shows a general way to calculate the maximal error and the pivot point on the polyhedron S  This algorithm may use the basic formulation in Eq        or the more advanced formulations in Eqs              and      defined in Section      In the following section  we describe a more refined pivot point selection method that can in some cases dramatically improve the performance        Petrik   Zilberstein          h        Yh                 Yh     Figure    The reduced set Yh that needs to be considered for pivot point selection      Advanced Pivot Point Selection As described above  the pivot points are chosen greedily to both determine the maximal error in each polyhedron and to minimize the approximation error  The basic approach described in Section     may be refined  because the goal is not to approximate the function g y  with the least error  but to find the optimal solution  Intuitively  we can ignore those regions of Y that will not guarantee any improvement of the current solution  as illustrated in Figure    As we show below  the search for the maximal error point could be limited to this region as well  We first define a set Yh  Y that we will search for the maximal error  given that the optimal solution f   h  Yh    y g y   h  y  Y    The next proposition states that the maximal error needs to be calculated only in a superset of Yh   Proposition     Let w  x  y  z be the approximate optimal solution and w   x   y    z  be the optimal solution  Also let f  w   x   y    z     h and assume some Yh  Yh   The approximation error is then bounded by  f  w   x   y    z     f  w  x  y  z   max g y   g y   yYh  Proof  First  f  w   x   y    z      g y     h and thus y   Yh   Then  f  w   x   y    z     f  w  x  y  z    max g y   max g y  yYh  yY   max g y   g y  yYh   max g y   g y  yYh  Proposition    indicates that the point with the maximal error needs to be selected only from the set Yh   The question is how to easily identify Yh   Because the set is not convex in general  a tight approximation of this set needs to be found  In particular  we use methods       A Bilinear Programming Approach for Multiagent Planning  that approximate the intersection of a superset of Yh with the polyhedron that is being refined  using the following methods     Feasibility  Eq         Require that pivot points are feasible in Y      Linear bound  Eq         Use the linear upper bound u y   h     Cutting plane  Eq         Use the linear inequalities that define YhC   where YhC   R Y     Yh is the complement of Yh   Any combination of these methods is also possible  Feasibility The first method is the simplest  but also the least constraining  The linear program to find the pivot point with the maximal error bound is as follows  maximize   t y z     subject to    u T t   rT x   xT CT t  x  L   T t     t           y   Tt A  y   B  z   b  y  z     This approach does not require that the bilinear program is in the semi compact form  Linear Bound The second method  using the linear bound  is also very simple to implement and compute  and it is more selective than just requiring feasibility  Let  Yh    y u y   h    y g y   h    Yh   This set is convex and thus does not need to be approximated  The linear program used to find the pivot point with the maximal error bound is as follows  maximize   t     subject to    u T t   rT x   xT CT t  x  L   T t     t           u T t   h The difference from Eq       is the last constraint  This approach requires that the bilinear program is in the semi compact form to ensure that u y  is a bound on the total return  Cutting Plane The third method  using the cutting plane elimination  is the most computationally intensive one  but also the most selective one  Using this approach requires additional assumptions on the other parts of the algorithm  which we discuss below  The method is based on the same principle as  extensions in concave cuts  Horst   Tuy         We start with the set YhC because it is convex and may be expressed as      T T T T max sT w   r x   y C x   r y  h            w x       A  x   B  w   b         w  x            Petrik   Zilberstein  y   f  y  y  f   Y  Yh  Figure    Approximating Yh using the cutting plane elimination method  To use these inequalities in selecting the pivot point  we need to make them linear  But there are two obstacles  Eq       contains a bilinear term and is a maximization  Both of these issues can be addressed by using the dual formulation of Eq        The corresponding linear program and its dual for fixed y  ignoring constants h and r T y  are  maximize w x  T T T sT   w   r  x   y C x  subject to A  x   B  w   b   minimize         bT    subject to AT     r    Cy  w  x           B T   s   Using the dual formulation  Eq       becomes      T T min b     r  y  h   AT     r    Cy B T   s  Now  we use that for any function  and any value  the following holds  min  x      x   x     x  Finally  this leads to the following set of inequalities  r T y  h  bT   Cy  AT     r  s   B T  The above inequalities define the convex set YhC   Because its complement Yh is not necessarily convex  we need to use its convex superset Yh on the given polyhedron  This is done by projecting YhC   or its subset  onto the edges of each polyhedron as depicted in Figure   and described in Algorithm    The algorithm returns a single constraint which cuts off part of the set YhC   Notice that only the combination of the first n points fk is       A Bilinear Programming Approach for Multiagent Planning  Algorithm    PolyhedronCut  y            yn      h  returns constraint  T y                          Find vertices of the polyhedron  y            yn     inside of YhC I   yi yi  YhC        Find vertices of the polyhedron outside of YhC O   yi yi  Yh        Find at least n points fk in which the edge of Yh intersects an edge of the polyhedron k   for i  O do for j  I do fk  yj   max    yi  yj     YhC      k k     if k  n then break    Find  and    such that  f            fn      and  T           Determine the correct orientation of the constraint to have all y in Yh feasible    if yj  O  and  T yj    then    Reverse the constraint if it points the wrong way                         return  T y    used  In general  there may be more than n points  and any subset of points fk of size n can be used to define a new cutting plane that constraints Yh   This did not lead to significant improvements in our experiments  The linear program to find the pivot point with the cutting plane option is as follows  maximize   t y     subject to    u T t   rT x   xT CT t  T t     t     x  L       y   Tt Ty   Here    and  are obtained as a result of running Algorithm    Note that this approach requires that the bilinear program is in the semi compact form to ensure that g y  is convex  The following proposition states the correctness of this procedure  Proposition     The resulting polyhedron produced by Algorithm   is a superset of the intersection of the polyhedron S with the complement of Yh   Proof  The convexity of g y  implies that YhC is also convex  Therefore  the intersection Q    y  T y      S       Petrik   Zilberstein  is also convex  It is also a convex hull of points fk  YhC   Therefore  from the convexity of YhC   we have that Q  YhC   and therefore S  Q  Yh       Dimensionality Reduction Our experiments show that the efficiency of the algorithm depends heavily on the dimensionality of the matrix C in Eq       In this section  we show the principles behind automatically determining the necessary dimensionality of a given problem  Using the proposed procedure  it is possible to identify weak interactions and eliminate them  Finally  the procedure works for arbitrary bilinear programs and is a generalization of a method we have previously introduced  Petrik   Zilberstein      a   The dimensionality is inherently part of the model  not the problem itself  There may be equivalent models of a given problem with very different dimensionality  Thus  procedures for reducing the dimensionality are not necessary when the modeler can create a model with minimal dimensionality  However  this is nontrivial in many cases  In addition  some dimensions may have little impact on the overall performance  To determine which ones can be discarded  we need a measure of their contribution that can be computed efficiently  We define these notions more formally later in this section  We assume that the feasible sets have bounded L  norms  and assume a general formulation of the bilinear program  not necessarily in the semi compact form  Given Assumption    this can be achieved by scaling the constraints when the feasible region is bounded  Assumption     For all x  X and y  Y   their norms satisfy kxk     and kyk      We discuss the implications of and problems with this assumption after presenting Theorem     Intuitively  the dimensionality reduction removes those dimensions where g y  is constant  or almost constant  Interestingly  these dimensions may be recovered based on the eigenvectors and eigenvalues of C T C  We use the eigenvectors of C T C instead of the eigenvectors of C  because our analysis is based on L  norm of x and y and thus of C  The L  norm kCk  is bounded by the largest eigenvalue of C T C  In addition  a symmetric matrix is required to ensure that the eigenvectors are perpendicular and span the whole space  Given a problem represented using Eq       let F be a matrix whose columns are all the eigenvectors of C T C with eigenvalues greater than some   Let G be a matrix with all the remaining eigenvectors as columns  Notice that together  the columns of the matrices span the whole space and are real valued  since C T C is a symmetric matrix  Assume without loss of generality that the eigenvectors are unitary  The compressed version of the bilinear program is then the following  maximize w x y   y   z  T T f  w  x  y    y    z    r T x   sT   w   x CF y    r  F  subject to A  x   B  w   b       y  A  F G   B  z   b  y  w  x  y    y    z               y  G   sT  z y         A Bilinear Programming Approach for Multiagent Planning  Notice that the program is missing the element xT CGy    which would make its optimal solutions identical to the optimal solutions of Eq       We describe a more practical approach to reducing the dimensionality in Appendix B  This approach is based on singular value decomposition and may be directly applied to any bilinear program  The following theorem quantifies the maximum error when using the compressed program  Theorem     Let f  and f  be optimal solutions of Eq      and Eq       respectively  Then  p      f   f       Moreover  this is the maximal linear dimensionality reduction possible with this error without considering the constraint structure   Proof  We first show that indeed the error is at most  and that any linearly compressed problem with the given error has at least f dimensions  Using a mapping that preserves the feasibility of both programs  the error is bounded by                   y  y    f w  x  F G   z  f  w  x  y        xT CGy    y  z Denote the feasible region of y  as Y    From the orthogonality of  F  G   we have that ky  k     as follows        y  y   F G y        T  y  F y   y  GT GT y   y  kGT yk    ky  k  Then we have       max max xT CGy   max kCGy  k  y  Y  q q p  max y T GT C T CGy   max y T Ly    y  Y  xX  y  Y   y  Y   The result follows from Cauchy Schwartz inequality  the fact that C T C is symmetric  and Assumption     The matrix L denotes a diagonal matrix of eigenvalues corresponding to eigenvectors of G  Now  let H be an arbitrary matrix that satisfies the preceding error inequality for G  Clearly  H  F     otherwise y  kyk       such that kCHyk       Therefore  we have  H   n   F     G   because  H     F      Y    Here      denotes the number of columns of the matrix  Alternatively  the bound can be proved by replacing the equality A  x   B  w   b  by kxk       The bound can then be obtained by Lagrange necessary optimality conditions  In these bounds we use L   norm  an extension to a different norm is not straightforward  Note       Petrik   Zilberstein  Y  y  kyk      Figure    Approximation of the feasible set Y according to Assumption     also that this dimensionality reduction technique ignores the constraint structure  When the constraints have some special structure  it might be possible to obtain an even tighter bound  As described in the next section  the dimensionality reduction technique generalizes the reduction that Becker et al         used implicitly  The result of Theorem    is based on an approximation of the feasible set Y by kyk      as Assumption    states  This approximation may be quite loose in some problems  which may lead to a significant multiplicative overestimation of the bound in Theorem     For example  consider the feasible set depicted in Figure    The bound may be achieved in a point y  which is far from the feasible region  In specific problems  a tighter bound could be obtained by either appropriately scaling the constraints  or using a weighted L  with a better precision  We partially address this issue by considering the structure of the constraints  To derive this  consider the following linear program and corresponding theorem  maximize x  cT x  subject to Ax   b        x   Theorem     The optimal solution of Eq       is the same as when the objective function is modified to cT  I  AT  AAT    A x  where I is the identity matrix  Proof  The objective function is  max   x Ax b  x    cT x      max   x Ax b  x    cT  I  AT  AAT    A x   cT AT  AAT    Ax    cT AT  AAT    b    max   x Ax b  x    cT  I  AT  AAT    A x   The first term may be ignored because it does not depend on the solution x        A Bilinear Programming Approach for Multiagent Planning  The following corollary shows how the above theorem can be used to strengthen the dimensionality reduction bound  For example  in zero sum games  this stronger dimensionality reduction splits the bilinear program into two linear programs  Corollary     Assume that there are no variables w and z in Eq       Let  T   Qi    I  AT i  Ai Ai   Ai      i           where Ai are defined in Eq       Let C be  C   Q  CQ    where C is the bilinear term matrix from Eq       Then the bilinear programs will have identical optimal solutions with either C or C  Proof  Using Theorem     we can modify the original objective function in Eq      to  T   T T   T f  x  y    r T x   xT  I  AT    A  A    A    C I  A   A  A    A    y   r  y   For the sake of simplicity we ignore the variables w and z  which do not influence the bilinear T   term  Because both  I  AT i  Ai Ai   Ai   for i        are orthogonal projection matrices  none of the eigenvalues in Theorem    will increase  The dimensionality reduction presented in this section is related to the idea of compound events used in CSA  Allen  Petrik  and Zilberstein      a      b  provide a detailed discussion of this issue      Offline Bound In this section we develop an approximation bound that depends only on the number of points for which g y  is evaluated and the structure of the problem  This kind of bound is useful in practice because it provides performance guarantees without actually solving the problem  In addition  the bound reveals which parameters of the problem influence the algorithms performance  The bound is derived based on the maximal slope of g y  and the maximal distance among the points  Theorem     To achieve an approximation error of at most    the number of points to be evaluated in a regular grid with k points in every dimension must satisfy     n  kCk  n n k      where n is the number of dimensions of Y   The theorem follows using basic algebraic manipulations from the following lemma  Lemma     Assume that for each y   Y there exists y   Y such that ky   y  k    and g y      g y     Then the maximal approximation error is      max g y   g y   kCk    yY        Petrik   Zilberstein  Proof  Let y  be a point where the maximal error is attained  This point is in Y   because this set is compact  Now  let y  be the closest point to y  in L  norm  Let x  and x  be the best responses for y  and y  respectively  From the definition of solution optimality we can derive  T T T r T x    r T y    xT   Cy   r  x    r  y    x  Cy   r T  x   x      x   x   T Cy    The error now can be expressed  using the fact that kx   x  k      as  T T T     r T x    r T y    xT   Cy   r  x   r  y   x  Cy     r T  x   x       x   x   T Cy    x   x   T Cy     x   x   T Cy    x   x   T C y   y     x   x   T  y   y     ky   y  k  C k x   x   k  ky   y  k   ky   y  k   max  max   x kxk      y kyk      xT Cy   kCk  The above derivation follows from Assumption     and the bound reduces to the matrix norm using Cauchy Schwartz inequality  Not surprisingly  the bound is independent of the local rewards and transition structure of the agents  Thus it in fact shows that the complexity of achieving a fixed approximation with a fixed interaction structure is linear in the problem size  However  the bounds are still exponential in the dimensionality of the space  Notice also that the bound is additive      Experimental Results We now turn to an empirical analysis of the performance of the algorithm  For this purpose we use the Mars rover problem described earlier  We compared our algorithm with the original CSA and with a mixed integer linear program  MILP   derived for Eq      as Petrik and Zilberstein      b  describe  Although Eq      can also be modeled as a linear complementarity problem  LCP   Murty        Cottle et al          we do not evaluate that option experimentally because LCPs are closely related to MILPs  Rosen         We expect these two formulations to exhibit similar performance  We also do not compare to any of the methods described by Horst and Tuy        and Bennett and Mangasarian        due to their very different nature and high complexity  and because some of these algorithms do not provide any optimality guarantees  In our experiments  we applied the algorithm to randomly generated problem instances with the same parameters that Becker et al               used  Each problem instance includes   rovers and   sites  At each site  the rovers can decide to perform an experiment or to skip the site  Performing experiments takes some time  and all the experiments must be performed in    time units  The time required to perform an experiment is drawn from a discrete normal distribution with the mean uniformly chosen from          The variance       A Bilinear Programming Approach for Multiagent Planning  Algorithm    MPBP  Multiagent Planning with Bilinear Programming     Formulate DEC MDP M as a bilinear program B       Section      B    ReduceDimensionality B  with              Section    Appendix B  Convert B   to a semi compact form       Definition    h        Presolve step  run Algorithm    times with random initialization for i             do h  max h  IterativeBestResponse B            Algorithm        BestResponseApprox B                             Algorithm     is     of the mean  The local reward for performing an experiment is selected uniformly from the interval           for each site and it is identical for both rovers  The global reward  received when both rovers perform an experiment on a shared site  is super additive and is     of the local reward  The experiments were performed with sites                 as shared sites  Typically  the performance of the algorithm degrades with the number of shared sites  Because the problem with fewer than   shared sitesas used in the original CSA paperwere too easy to solve  we only present results for problems with   shared sites  Note that CSA was used on this problem with an implicit dimensionality reduction due to the use of the compound events  In these experiments  the naive dimensionality of Y in Eq      is                 This dimensionality can be reduced to be one per each shared site using the automatic dimensionality reduction procedure  Each dimension then represents the probability that an experiment on a shared site is performed regardless of the time  Therefore  the dimension represents the sum of the individual probabilities  Becker et al         achieved the same compression using compound events  where each compound event represents the fact that an experiment is performed on some site regardless of the specific time  The complete algorithmMultiagent Planning with Bilinear Programming  MPBP is summarized in Algorithm    The automatic dimensionality reduction reduces Y to   dimensions  Then  reformulating the problem to a semi compact form increases the dimensionality to    We experimented with different configurations of the algorithm that differ in the way the refinements of the pivot point selection is performed  The different methods  described in Section      were used to create six configurations as shown in Figure    The configuration C  corresponds to an earlier version of the algorithm  Petrik   Zilberstein      a   We executed the algorithm    times with each configuration on every problem  randomly generated according to the distribution described above  The results represent the average over the random instances  The maximum number of iterations of the algorithm was      Due to rounding errors  we considered any error less than     to be    The algorithm is implemented in MATLAB release     a  The linear solver we used is MOSEK version      The hardware configuration was Intel Core   Duo     GHz Low Voltage with  GB RAM  The time to perform the dimensionality reduction is negligible and not included in the result  A direct comparison with CSA was not possible because CSA cannot solve problems with this dimensionality within a reasonable amount of time  However  in a very similar       Petrik   Zilberstein  Configuration  Feasible  Eq         C   Linear bound  Eq         Cutting plane  Eq              C     C          C          C          C   Presolve               Figure    The six algorithm configurations that were evaluated  Feasible  linear bound  and cutting plane refer to methods used to determine the optimal solution   problem setup with at most   shared sites  CSA solved only     of the problems  and the longest solution took approximately   hours  Becker et al          In contrast  MPBP solved all     problems with   shared sites optimally in less than   second on average  about       times faster  In addition  MPBP returns solutions that are guaranteed to be close to optimal in the first few iterations  While CSA also returns solutions close to optimal very rapidly  it takes a very long time to confirm that  Figure   shows the average guaranteed ratio of the optimal solution  achieved as a function of the number of iterations  that is  points for which g y  is evaluated  This figure  as all others  shows the result of the online error bound  This value is guaranteed and is not based on the optimal solution  This compares the performance of the various configurations of the algorithm  without using the presolve step  While the optimal solution was typically discovered in the first few iterations  it takes significantly longer to prove its optimality  The average of absolute errors in both linear and log scale are shown in Figure     These results indicate that the methods proposed to eliminate the dominated region in searching for the pivot point can dramatically improve performance  While requiring that the new pivot points are feasible in Y improves the performance  it is much more significant with    Fraction Optimal  C        C        C  C                                         Iteration            Figure    Guaranteed fraction of optimality according to the online bound        A Bilinear Programming Approach for Multiagent Planning            C   C   C      C            C  C   C        Absolute Error  Absolute Error                          C                       Iteration                           Iteration            Figure     Comparison of absolute error of various region elimination methods   a better approximation of Yh   As expected  the cutting plane elimination is most efficient  but also most complex  To evaluate the tradeoffs in the implementation  we also show the average time per iteration and the average total time in Figure     These figures show that the time per iteration is significantly larger when the cutting plane elimination is used  Overall  the algorithm is faster when the simpler linear bound is used                                 Total Seconds  Seconds per Iteration  This trend is most likely problem specific  In problems with higher dimensionality  the more precise cutting plane algorithm may be more efficient  Implementation issues play a significant role in this problem too  and it is likely that the implementation of Algorithm   can be further improved                                C   C   C      C   C   C   C   C   Figure     Time per iteration and the total time to solve  With configurations C  and C    the optimal value is not reached with     iterations   The figure only shows the time to compute up to     iterations         Petrik   Zilberstein         C  C     C  Absolute Error     C                               Iteration          Figure     Influence of the presolve method  Figure    shows the influence of using the presolve method  The plots of C  and C  are identical to the plots of C  and C  respectively  indicating that the presolve method does not have any significant influence  This also indicates that a solution that is very close to optimal is obtained when the values of the initial points are calculated  We also performed experiments with CPLEXa state of the art MILP solver on the direct MILP formulation of the DEC MDP  CPLEX was not able to solve any of the problems within    minutes  no matter how many of the sites were shared  The main reason for this is that it does not take any advantage of the limited interaction  Nevertheless  it is possible that some specialized MILP solvers may perform better      Conclusion and Further Work We present an algorithm that significantly improves the state of the art in solving two agent coordination problems  The algorithm takes as input a bilinear program representing the problem  and solves the problem using a new successive approximation method  It provides a useful online performance bound that can be used to decide when the approximation is good enough  The algorithm can take advantage of the limited interaction among the agents  which is translated into a small dimensionality of the bilinear program  Moreover  using our approach  it is possible to reduce the dimensionality of such problems automatically  without extensive modeling effort  This makes it easy to apply our new method in practice  When applied to DEC MDPs  the algorithm is much faster than the existing CSA method  on average reducing computation time by four orders of magnitude  We also show that a variety of other coordination problems can be treated within this framework  Besides multiagent coordination problems  bilinear programs have been previously used to solve problems in operations research and global optimization  Sherali   Shetty        White        Gabriel  Garca Bertrand  Sahakij    Conejo         Global optimization deals with finding the optimal solutions to problems with multi extremal objective function  Solution techniques often share the same idea and are based on cutting plane methods  The main idea is to iteratively restrict the set of feasible solutions  while improving the incumbent       A Bilinear Programming Approach for Multiagent Planning  solution  Horst and Tuy        provide an excellent overview of these techniques  These algorithms have different characteristics and cannot be directly compared to the algorithm we developed  Unlike these traditional algorithms  we focus on providing quickly good approximate solutions with error bounds  In addition  we exploit the small dimensionality of the best response space Y to get tight approximation bounds  Future work will address several interesting open questions with respect to the bilinear formulation as well as further improvement of the efficiency of the algorithm  With regard to the representation  it is yet to be determined whether the anytime behavior can be exploited when applied to games  That is  it is necessary to verify that an approximate solution to the bilinear program is also a meaningful approximation of the Nash equilibrium  It is also important to identify the classes of extensive games that can be efficiently formulated as bilinear programs  The algorithm we present can be made more efficient in several ways  In particular  a significant speedup could be achieved by reducing the size of the individual linear programs  The programs are solved many times with the same constraints  but a different objective function  The objective function is always from a small dimensional space  Therefore  the problems that are solved are all very similar  In the DEC MDP domain  one option would be to use a procedure similar to action elimination  In addition  the performance could be significantly improved by starting with a tight initial triangulation  In our implementation  we simply use a single large polyhedron that covers the whole feasible region  A better approach would be to start with something that approximates the feasible region more tightly  A tighter approximation of the feasible region could also improve the precision of the dimensionality reduction procedure  Instead of the naive ellipsis used in Assumption    it is possible to use one that approximates the feasible region as tightly as possible  It is however very encouraging to see that even without these improvements  the algorithm is very effective compared with existing solution techniques   Acknowledgments We thank Chris Amato  Raghav Aras  Alan Carlin  Hala Mostafa  and the anonymous reviewers for useful comments and suggestions  This work was supported in part by the Air Force Office of Scientific Research under Grants No  FA               and FA               and by the National Science Foundation under Grants No  IIS         and IIS           Appendix A  Proofs Proof of Proposition    The proposition states that in the proposed triangulation  the sub polyhedra do not overlap and they cover the whole feasible set Y   given that the pivot point is in the interior of S  Proof  We prove the theorem by induction on the number of polyhedron splits that were performed  The base case is trivial  there is only a single polyhedron  which covers the whole feasible region  For the inductive case  we show that for any polyhedron S the sub polyhedra induced by the pivot point y cover S and do not overlap  The notation we use is the following  T       Petrik   Zilberstein  denotes the original polyhedron and y   T c is the pivot point  where  T c     and c     Note that T is a matrix and c  d  y are vectors  and  is a scalar  We show that the sub polyhedra cover the original polyhedron S as follows  Take any a   T d such that  T d     and d     We show that there exists a sub polyhedron that contains a and has y as a vertex  First  let     T T    T This matrix is square and invertible  since the polyhedron is non empty  To get a representation of a that contains y  we show that there is a vector o such that for some i  o i             a   T d   T o    y   o     for some       This will ensure that a is in the sub polyhedron with y with vertex i replaced by y  The value o depends on  as follows        y   o   d   T   This can be achieved by setting     min i  d i   T   y  i      Since both d and c   T   y are non negative  This leaves us with an equation for the sub polyhedron containing the point a  Notice that the resulting polyhedron may be of a smaller dimension than n when o j      for some i    j  To show that the polyhedra do not overlap  assume there exists a point a that is common to the interior of at least two of the polyhedra  That is  assume that a is a convex combination of the vertices  a   T  c    h  y     y  a   T  c    h  y     y    where T  represents the set of points common to the two polyhedra  and y  and y  represent the disjoint points in the two polyhedra  The values h    h        and   are all scalars  while c  and c  are vectors  Notice that the sub polyhedra differ by at most one vertex  The coefficients satisfy  c      c      h      h                  T  T    c    h             c    h                A Bilinear Programming Approach for Multiagent Planning  Since the interior of the polyhedron is non empty  this convex combination is unique  First assume that h   h    h    Then we can show the following  a   T  c    hy     y    T  c    hy     y  T  c      y    T  c      y    y      y            This holds since y  and y  are independent of T  when the polyhedron is nonempty and y     y    The last equality follows from the fact that y  and y  are linearly independent  This is a contradiction  since           implies that the point a is not in the interior of two polyhedra  but at their intersection  Finally  assume WLOG that h    h    Now let y   T  c     y      y    for some scalars      and      that represent a convex combination  We get  a   T  c    h  y     y    T   c    h  c     h         y    h    y  a   T  c    h  y     y    T   c    h  c    h    y     h         y    The coefficients sum to one as shown below   T  c    h  c     h            h       T c        h    T c              T c        h       T  c    h  c         h             T c        h    T c              T c        h      Now  the convex combination is unique  and therefore the coefficients associated with each vertex for the two representations of a must be identical  In particular  equating the coefficients for y  and y  results in the following  h          h     h      h             h     h         h     h            h   h                h   h         We have that       and       from the fact that y is in the interior of the polyhedron S  Then  having      is a contradiction with a being a convex combination of the vertices of S   Appendix B  Practical Dimensionality Reduction In this section we describe an approach to dimensionality reduction that is easy to implement  Note that there are at least two possible approaches to take advantage of reduced dimensionality  First  it is possible to use the dimensionality information to limit the algorithm to work only in the significant dimensions of Y   Second  it is possible to modify the bilinear program to have a small dimensionality  While changing the algorithm may be more straightforward  it limits the use of the advanced pivot point selection methods described in Section      Here  we show how to implement the second option in a straightforward way using singular value decomposition        Petrik   Zilberstein  The dimensionality reduction is applied to the following bilinear program  maximize w x y z  T T T r T x   sT   w   x Cy   r  y   s  z  subject to A  x   B  w   b  A  y   B  z   b         w  x  y  z    Let C   SV T T be a singular value decomposition  Let T    T    T     such that the singular value of vectors ti in T  is less than the required    Then  a bilinear program with reduced dimensionality may be defined as follows  maximize w x y y z  T T T r T x   sT   w   x SV T  y   r  y   s  z  subject to T  y   y A  x   B  w   b         A  y   B  z   b  w  x  y  z    Note that y is not constrained to be non negative  One problematic aspect of reducing the dimensionality is how to define the initial polyhedron that needs to encompass all feasible solutions  One option is to make it large enough to contain the set  y kyk        but this may be too large  Often in practice  it may be more efficient to first triangulate a rough approximation of the feasible region  and then execute the algorithm on this triangulation   Appendix C  Sum of Convex and Concave Functions In this section we show that the best response function g y  may not be convex when the program is not in a semi compact form  The convexity of the best response function is crucial in bounding the approximation error and in eliminating the dominated regions  We show that when the program is not in a semi compact form  the best response function can we written as a sum of a convex function and a concave function  To show that consider the following bilinear program  maximize w x y z  T T T f   r T x   sT   w   x Cy   r  y   s  z  subject to A  x   B  w   b  A  y   B  z   b  w  x  y  z    This problem may be reformulated as  f       max  max  max  g    y    sT   z    y z  y z Y    x w  x w X   y z  y z Y    T T T r T x   sT   w   x Cy   r  y   s  z              A Bilinear Programming Approach for Multiagent Planning  where g    y     max   x w  x w X   T T r T x   sT   w   x Cy   r  y   Notice that function g    y  is convex  because it is a maximum of a set of linear functions  Since f   max y  y z Y   g y   the best response function g y  can be expressed as  g y     max   z  y z Y        g    y    sT   z   g  y     max   z  y z Y    sT  z    g  y    t y   where t y     max   z A  y B  z b    y z    sT   z   Function g    y  does not depend on z  and therefore could be taken out of the maximization  The function t y  corresponds to a linear program  and its dual using the variable q is   b   A  y T q  minimize q  subject to B T q  s         Therefore  t y      b   A  y T q   min   q B T qs     which is a concave function  because it is a minimum of a set of linear functions  The best response function can now be written as  g y    g    y    t y   which is a sum of a convex function and a concave function  also known as a d c  function  Horst   Tuy         Using this property  it is easy to construct a program such that g y  will be convex on one part of Y and concave on another part of Y   as the following example shows  Note that in semi compact bilinear programs t y       which guarantees the convexity of g y   Example     Consider the following bilinear program  maximize x y z  x   xy   z  subject to    x    yz   z  A plot of the best response function for this program is shown in Figure                  Petrik   Zilberstein     maxx f x y                                       y  Figure     A plot of a non convex best response function g for a bilinear program  which is not in a semi compact form   
 Coordination of distributed agents is required for problems arising in many areas  including multi robot systems  networking and e commerce  As a formal framework for such problems  we use the decentralized partially observable Markov decision process  DECPOMDP   Though much work has been done on optimal dynamic programming algorithms for the single agent version of the problem  optimal algorithms for the multiagent case have been elusive  The main contribution of this paper is an optimal policy iteration algorithm for solving DEC POMDPs  The algorithm uses stochastic finite state controllers to represent policies  The solution can include a correlation device  which allows agents to correlate their actions without communicating  This approach alternates between expanding the controller and performing value preserving transformations  which modify the controller without sacrificing value  We present two efficient value preserving transformations  one can reduce the size of the controller and the other can improve its value while keeping the size fixed  Empirical results demonstrate the usefulness of value preserving transformations in increasing value while keeping controller size to a minimum  To broaden the applicability of the approach  we also present a heuristic version of the policy iteration algorithm  which sacrifices convergence to optimality  This algorithm further reduces the size of the controllers at each step by assuming that probability distributions over the other agents actions are known  While this assumption may not hold in general  it helps produce higher quality solutions in our test problems      Introduction Markov decision processes  MDPs  provide a useful framework for solving problems of sequential decision making under uncertainty  In some settings  agents must base their decisions on partial information about the system state  In that case  it is often better to use the more general framework of partially observable Markov decision processes  POMDPs   Even more general are problems in which a team of decision makers  each with its own c      AI Access Foundation and Morgan Kaufmann Publishers  All rights reserved    Bernstein  Amato  Hansen    Zilberstein  local observations  must act together  Domains in which these types of problems arise include networking  multi robot coordination  e commerce  and space exploration systems  The decentralized partially observable Markov decision process  DEC POMDP  provides an effective framework to model such problems  Though this model has been recognized for decades  Witsenhausen         there has been little work on provably optimal algorithms for it  On the other hand  POMDPs have been studied extensively over the past few decades  Smallwood   Sondik        Simmons   Koenig        Cassandra  Littman    Zhang        Hansen      a  Bonet   Geffner        Poupart   Boutilier        Feng   Zilberstein        Smith   Simmons        Smith  Thompson    Wettergreen         It is well known that a POMDP can be reformulated as an equivalent belief state MDP  A belief state MDP cannot be solved in a straightforward way using MDP methods because it has a continuous state space  However  Smallwood and Sondik showed how to implement value iteration by exploiting the piecewise linearity and convexity of the value function  This work opened the door for many algorithms  including approximate approaches and policy iteration algorithms in which the policy is represented using a finite state controller  Extending dynamic programming for POMDPs to the multiagent case is not straightforward  For one thing  it is not clear how to define a belief state and consequently form a belief state MDP  With multiple agents  each agent has uncertainty about the observations and beliefs of the other agents  Furthermore  the finite horizon DEC POMDP problem with just two agents is complete for a higher complexity class than the single agent version  Bernstein  Givan  Immerman    Zilberstein         indicating that these are fundamentally different problems  In this paper  we describe an extension of the policy iteration algorithm for single agent POMDPs to the multiagent case  As in the single agent case  our algorithm converges in the limit  and thus serves as the first nontrivial optimal algorithm for infinite horizon DEC POMDPs  A few optimal approaches  Hansen  Bernstein    Zilberstein        Szer  Charpillet    Zilberstein        and several approximate algorithms have been developed for finite horizon DEC POMDPs  Peshkin  Kim  Meuleau    Kaelbling        Nair  Pynadath  Yokoo  Tambe    Marsella        Emery Montemerlo  Gordon  Schnieder    Thrun        Seuken   Zilberstein         but only locally optimal algorithms have been proposed for the infinite horizon case  Bernstein  Hansen    Zilberstein        Szer   Charpillet        Amato  Bernstein    Zilberstein         In our algorithmic framework  policies are represented using stochastic finite state controllers  A simple way to implement this is to give each agent its own local controller  In this case  the agents policies are all independent  A more general class of policies includes those which allow agents to share a common source of randomness without sharing observations  We define this class formally  using a shared source of randomness called a correlation device  The use of correlated stochastic policies in the DEC POMDP context is novel  The importance of correlation has been recognized in the game theory community  Aumann         but there has been little work on algorithms for finding correlated policies  Each iteration of the algorithm consists of two phases  These are exhaustive backups  which add nodes to the controller  and value preserving transformations  which change the controller without sacrificing value  We first provide a novel exposition of existing single     Policy Iteration for DEC POMDPs  agent algorithms using this two phase view  and then we go on to describe the multiagent extension  There are many possibilities for value preserving transformations  In this paper  we describe two different types  both of which can be performed efficiently using linear programming  The first type allows us to remove nodes from the controller  and the second allows us to improve the value of the controller while keeping its size fixed  Our empirical results demonstrate the usefulness of value preserving transformations in obtaining high values while keeping controller size to a minimum  We note that this work serves to unify and generalize previous work on dynamic programming for DEC POMDPs  The first algorithm for the finite horizon case  Hansen et al         can be extended to the infinite horizon case and viewed as interleaving exhaustive backups and controller reductions  The bounded policy iteration algorithm for DEC POMDPs  Bernstein et al          which extends a POMDP algorithm proposed by Poupart and Boutilier         can be viewed through the lens of our framework as repeated application of a specific value preserving transformation  Because the optimal algorithm will not usually be able to return an optimal solution in practice  we also introduce a heuristic version of the policy iteration algorithm  This approach makes use of initial state information to focus policy search and further reduces controller size at each step  To accomplish this  a forward search from the initial state distribution is used to construct a set of belief points an agent would visit assuming the other agents use given fixed policies  This search is conducted for each agent and then policy iteration takes place while using the belief points to guide the removal of controller nodes  The assumption that other agents use fixed policies causes the algorithm to no longer be optimal  but it performs well in practice  We show that more concise and higher valued solutions can be produced compared to the optimal method before resources are exhausted  The remainder of the paper is organized as follows  Section   introduces the formal models of sequential decision making  Section   contains a novel presentation of existing dynamic programming algorithms for POMDPs  In section    we present an extension of policy iteration for POMDPs to the DEC POMDP case  along with a convergence proof  We discuss the heuristic version of policy iteration in section    followed by experiments using policy iteration and heuristic policy iteration in section    Finally  section   contains the conclusion and a discussion of possible future work      Formal Model of Distributed Decision Making We begin with a description of the formal framework upon which our work is based  This framework extends the well known Markov decision process to allow for distributed policy execution  We also define an optimal solution for this model and discuss two different representations for these solutions      Decentralized POMDPs A decentralized partially observable Markov decision process  DEC POMDP  is defined for  T  R      Oi  where mally as a tuple hI  S  A   I is a finite set of agents       Bernstein  Amato  Hansen    Zilberstein  Agent Agent a  s  r System  a   Agent a  System  o  r System  o   r  a   o   r Agent   a    b    c   Figure     a  Markov decision process   b  Partially observable Markov decision process   c  Decentralized partially observable Markov decision process with two agents    S is a finite set of states  with distinguished initial state s        iI Ai is a set of joint actions  where Ai is the set of actions for agent i   A    S is the state transition function  defining the distributions of states  T   SA that result from starting in a given state and each agent performing an action       is the reward function for the set of agents for each set of joint actions  R   S A and each state      iI i is a set of joint observations  where i contains observations for agent i       S     is an observation function  defining the distributions of observations  O A for the set of agents that result from each agent performing an action and ending in a given state  The special case of a DEC POMDP in which there is only one agent is called a partially observable Markov decision process  POMDP   In this paper  we consider the case in which the process unfolds over an infinite sequence of stages  At each stage  all agents simultaneously select an action  and each receives the global reward based on the reward function and a local observation based on the observation function  Thus  the transitions  rewards and observations depend on the actions of all agents  but each agent must act based only on local observations  This is illustrated in Figure    The objective of the agents is to maximize the expected discounted sum of rewards that are received  thus it is a cooperative framework  We denote the discount factor  and require that          In a DEC POMDP  the decisions of each agent affect all the agents in the domain  but due to the decentralized nature of the model each agent must choose actions based solely on local information  Because each agent receives a separate observation that does not usually provide sufficient information to efficiently reason about the other agents  solving a DECPOMDP optimally becomes very difficult  For example  each agent may receive a different      Policy Iteration for DEC POMDPs   a    b   Figure    A set of horizon three policy trees  a  and two node stochastic controllers  b  for a two agent DEC POMDP   piece of information that does not allow a common state estimate or any estimate of the other agents decisions to be calculated  These single estimates are crucial in single agent problems  as they allow the agents history to be summarize concisely  but they are not generally available in DEC POMDPs  This is seen in the complexity of the finite horizon problem with at least two agents  which is NEXP complete  Bernstein et al         and thus in practice may require double exponential time  Like the infinite horizon POMDP  optimally solving an infinite horizon DEC POMDP is undecidable as it may require infinite resources  but our method is able to provide a solution within   of the optimal with finite time and memory  Nevertheless  introducing multiple decentralized agents causes a DECPOMDP to be significantly more difficult than a single agent POMDP      Solution Representations A local policy for an agent is a mapping from local action and observation histories to actions while a joint policy is a set of policies  one for each agent in the problem  As mentioned above  an optimal solution for a DEC POMDP is the joint policy that maximizes the expected sum of rewards that are received over the finite or infinite steps of the problem  In infinite horizon problems  the rewards are discounted to maintain a finite sum  Thus  an optimal solution is a joint policy that provides the highest value starting at the given initial state of the problem  For finite horizon problems  local policies can be represented using a policy tree as seen in Figure  a  Actions are represented by the arrows or stop figures  where each agent can move in the given direction or stay where it is  and observations are labeled wl and wr for seeing a wall on the left or the right respectively  Using this representation  an agent takes the action defined at the root node and then after seeing an observation  chooses the next action that is defined by the respective branch  This continues until the action at a leaf node is executed  For example  agent   would first move left and then if a wall is seen on the right  the agent would move left again  If a wall is now seen on the left  the agent does not move on the final step  A policy tree is a record of the the entire local history for an agent up to some fixed horizon and because each tree is independent of the others it can      Bernstein  Amato  Hansen    Zilberstein  be executed in a decentralized manner  While this representation is useful for finite horizon problems  infinite horizon problems would require trees of infinite height  Another option used in this paper is to condition action selection on some internal memory state  These solutions can be represented as a set of local finite state controllers  seen in Figure  b   The controllers operate in a very similar way to the policy trees in that there is a designated initial node and following the action selection at that node  the controller transitions to the next node depending on the observation seen  This continues for the infinite steps of the problem  Throughout this paper  controller states will be referred to as nodes to help distinguish them from system states  An infinite number of nodes may be required to define an optimal infinite horizon DECPOMDP policy  but we will discuss a way to produce solutions within   of the optimal with a fixed number of nodes  While deterministic action selection and node transitions are sufficient to define this   optimal policy  when memory is limited stochastic action selection and node transition may be beneficial  A simple example illustrating this for POMDPs is given by Singh         which can be easily extended to DEC POMDPs  Intuitively  randomness can help an agent to break out of costly loops that result from forgetfulness  A formal description of stochastic controllers for POMDPs and DEC POMDPs is given in sections       and       respectively  but an example can be seen in Figure  b  Agent   begins at node   and moves up with probability      and stays in place with probability       If the agent stayed in place and a wall was then seen on the left  observation wl   on the next step  the controller would transition to node   and the agent would use the same distribution of actions again  If a wall was seen on the right instead  observation wr   there is a      probability that the controller will transition back to node   and a      probability that the controller will transition to node   for the next step  The finite state controller allows an infinite horizon policy to be represented compactly by remembering some aspects of the agents history without representing the entire local history      Centralized Dynamic Programming In this section  we cover the main concepts involved in dynamic programming for the single agent case  This will provide a foundation for the multiagent dynamic programming algorithm described in the following section      Value Iteration for POMDPs Value iteration can be used to solve POMDPs optimally  This algorithm is more complicated than its MDP counterpart  and does not have efficiency guarantees  However  in practice it can provide significant leverage in solving POMDPs  We begin by explaining how every POMDP has an equivalent MDP with a continuous state space  Next  we describe how the value functions for this MDP have special structure that can be exploited  These ideas are central to the value iteration algorithm        Belief State MDPs A convenient way to summarize the observation history of an agent in a POMDP is through a belief state  which is a distribution over system states  As it receives observations  the      Policy Iteration for DEC POMDPs  agent can update its belief state and then remove its observations from memory  Let b denote a belief state  and let b s  represent the probability assigned to state s by b  If an agent chooses action a from belief state b and subsequently observes o  each component of the successor belief state obeys the equation P P  o a  s    sS P  s   s  a b s      b  s       P  o b  a  where    P  o b  a     X       P  o a  s    s  S  X     P  s  s  a b s     sS  Note that this is a simple application of Bayes rule  It was shown by Astrom        that a belief state constitutes a sufficient statistic for the agents observation history  and it is possible to define an MDP over belief states as follows  A belief state MDP is a tuple h  A  T  Ri  where   is the set of distributions over S   A is the set of actions  same as before    T  b  a  b    is the transition function  defined as X T  b  a  b      P  b   b  a  o P  o b  a   oO   R b  a  is a reward function  defined as R b  a     X  b s R s  a    sS  When combined with belief state updating  an optimal solution to this MDP can be used as an optimal solution to the POMDP from which it was constructed  However  since the belief state MDP has a continuous   S  dimensional state space  traditional MDP techniques are not immediately applicable  Fortunately  dynamic programming can be used to find a solution to the belief state MDP  The key result in making dynamic programming practical was proved by Smallwood and Sondik         who showed that the Bellman operator preserves piecewise linearity and convexity of a value function  Starting with a piecewise linear and convex representation of V t   the value function V t   is piecewise linear and convex  and can be computed in finite time  To represent a piecewise linear and convex value function  one need only store the value of each facet for each system state  Denoting the set of facets   we can store     S dimensional vectors of real values PFor any single vector      we can define its value at the belief state b with V  b      sS b s  s   Thus  to go from a set of vectors to the value of a belief state  we use the equation X V  b    max b s  s     sS       Bernstein  Amato  Hansen    Zilberstein  s   s   s   a   s   b   Figure    A piecewise linear and convex value function for a POMDP with two states  a  and a non minimal representation of a piecewise linear and convex value function for a POMDP  b    Figure  a shows a piecewise linear and convex value function for a POMDP with two states  Smallwood and Sondik proved that the optimal value function for a finite horizon POMDP is piecewise linear and convex  The optimal value function for an infinite horizon POMDP is convex  but may not be piecewise linear  However  it can be approximated arbitrarily closely by a piecewise linear and convex value function  and the value iteration algorithm constructs closer and closer approximations  as we shall see        Pruning Vectors Every piecewise linear and convex value function has a minimal set of vectors  that represents it  Of course  it is possible to use a non minimal set to represent the same function  This is illustrated in Figure  b  Note that the removal of certain vectors does not change the value of any belief state  Vectors such as these are not necessary to keep in memory  Formally  we say that a vector  is dominated if for all belief states b  there is a vector       such that V  b     V  b     Because dominated vectors are not necessary  it would be useful to have a method for removing them  This task is often called pruning  and has an efficient algorithm based on linear programming  For a given vector   the linear program in Table   determines whether  is dominated  If variables can be found to make   positive  then adding  to the set improves the value function at some belief state  If not  then  is dominated  This gives rise to a simple algorithm for pruning a set of vectors  to obtain a minimal set   The algorithm loops through   removes each vector     and solves the linear program using  and      If  is not dominated  then it is returned to   It turns out that there is an equivalent way to characterize dominance that can be useful  Recall that for a vector to be dominated  there does not have to be a single vector that has value at least as high for all states  It is sufficient for there to exist a set of vectors such that for all belief states  one of the vectors in the set has value at least as high as the vector in question       Policy Iteration for DEC POMDPs  Variables     b s  Objective  Maximize    Improvement constraints  X    b s  s        s  X  b s  s   s  Probability constraints  X  s  b s        b s      s  Table    The linear program for testing whether a vector  is dominated        convex combination    s   s   Figure    The dual interpretation of dominance  Vector   is dominated at all belief states by either   or     This is equivalent to the existence of a convex combination of   and   which dominates   for all belief states   It can be shown that such a set exists if and only if there is some convex combination of vectors that has value at least as high as the vector in question for all states  This is shown graphically in Figure    If we take the dual of the linear program for dominance given in the previous section  we get a linear program for which the solution is a vector of probabilities for the convex combination  This dual view of dominance was first used in a POMDP context by Poupart and Boutilier         and is useful for policy iteration  as will be explained later        Dynamic Programming Update In this section  we describe how to implement a dynamic programming update to go from a value function Vt to a value function Vt     In terms of implementation  our aim is to take a minimal set of vectors t that represents Vt and produce a minimal set of vectors t   that represents Vt          Bernstein  Amato  Hansen    Zilberstein  Each vector that could potentially be included in t   represents the value of an action a and assignment of vectors in t to observations  A combination of an action and transition rule will hereafter be called a one step policy  The value vector for a one step policy can be determined by considering the action taken  the resulting state transitioned to and observation seen and the value of the assigned vector at step t  This is given via the equation X it    s    R s   i      P  s   s   i  P  o  i   s   t  i o   s     s   o  where i is the index of the vector   i  is its action  and   i  o  is the index of the vector in t to which to transition upon receiving observation o and  is the discount factor  More details on the derivation and use of this formula are provided by Zhang and Zhang         There are  A  t     possible one step policies  A simple way to construct t   is to evaluate all possible one step policies and then apply a pruning algorithm such as Larks method  Lark III         Evaluating the entire set of one step policies will hereafter be called performing an exhaustive backup  It turns out that there are ways to perform a dynamic programming update without first performing an exhaustive backup  Below we describe two approaches to doing this  The first approach uses the fact that it is simple to find the optimal vector for any particular belief state  For a belief state b  an optimal action can be determined via the equation     X P  o b  a V t  T  b a  o        argmaxaA R b  a     o  For each observation o  there is a subsequent belief state  which can be computed using Bayes rule  To get an optimal transition rule    o   we take the optimal vector for the belief state corresponding to o  Since the backed up value function has finitely many vectors  there must be a finite set of belief states for which backups must be performed  Algorithms which identify these belief states include Smallwood and Sondiks one pass algorithm         Chengs linear support and relaxed region algorithms  Cheng         and Kaelbling  Cassandra and Littmans Witness algorithm         The second approach is based on generating and pruning sets of vectors  Instead of generating all vectors and then pruning  these techniques attempt to prune during the generation phase  The first algorithm along these lines was the incremental pruning algorithm  Cassandra et al          Recently  improvements have been made to this approach  Zhang   Lee        Feng   Zilberstein               It should be noted that there are theoretical complexity barriers for DP updates  Littman et al         showed that under certain widely believed complexity theoretic assumptions  there is no algorithm for performing a DP update that is worst case polynomial in all the quantities involved  Despite this fact  dynamic programming updates have been successfully implemented as part of the value iteration and policy iteration algorithms  which will be described in the subsequent sections       Policy Iteration for DEC POMDPs        Value Iteration To implement value iteration  we simply start with an arbitrary piecewise linear and convex value function  and proceed to perform DP updates  This corresponds to value iteration in the equivalent belief state MDP  and thus converges to an   optimal value function after a finite number of iterations  Value iteration returns a value function  but a policy is needed for execution  As in the MDP case  we can use one step lookahead  using the equation     X X  b    argmaxaA R s  a b s     P  o b  a V    b  o  a     sS  o  where   b  o  a  is the belief state resulting from starting in belief state b  taking action a  and receiving observation o  We note that a state estimator must be used as well to track the belief state  Using the fact that each vector corresponds to a one step policy  we can extract a policy from the value of the vectors    X  b     argmaxk b s k  s  s  While the size of the resulting set of dominant vectors may remain exponential  in many cases it is much smaller  This can significantly simplify computation  As in the completely observable case  the Bellman residual provides a bound on the distance to optimality  Recall that the Bellman residual is the maximum distance across all belief states between the value functions of successive iterations  It is possible to find the maximum distance between two piecewise linear and convex functions in polynomial time with an algorithm that uses linear programming  Littman et al              Policy Iteration for POMDPs With value iteration  a POMDP is viewed as a belief state MDP  and a policy is a mapping from belief states to actions  An early policy iteration algorithm developed by Sondik used this policy representation  Sondik         but it was very complicated and did not meet with success in practice  We shall describe a different approach that has performed better on test problems  With this approach  a policy is represented as a finite state controller        Finite State Controllers Using a finite state controller  an agent has a finite number of internal states  Its actions are based only on its internal state  and transitions between internal states occur when observations are received  Internal states provide agents with a kind of memory  which can be crucial for difficult POMDPs  Of course  an agents memory is limited by the number of internal states it possesses  In general  an agent cannot remember its entire history of observations  as this would require infinitely many internal states  An example of a finitestate controller can be seen by considering only one agents controller in Figure  b  The operation of a single controller is the same as that for each agent in the decentralized case  We formally define a controller as a tuple hQ    A    i  where      Bernstein  Amato  Hansen    Zilberstein   Q is a finite set of controller nodes    is a set of inputs  taken to be the observations of the POMDP   A is a set of outputs  taken to be the actions of the POMDP      Q  A is an action selection function  defining the distribution of actions selected at each node      Q  A    Q is a transition function  defining the distribution of resulting nodes for each initial node and action taken  For each state and starting node of the controller  there is an expected discounted sum of rewards over the infinite horizon  It can be computed using the following system of linear equations  one for each s  S and q  Q    X X V  s  q    P  a q  R s  a     P  o  s   s  a P  q    q  a  o V  s    q       s   o q    a  Where P  a q  is the probability action a will be taken in node q and P  q    q  a  o  is the probability the controller will transition to node q   from node q after action a was taken and o was observed  We sometimes refer to the value of the controller at a belief state  For a belief state b  this is defined as X V  b    max b s V  s  q   q  s  Thus  it is assumed that  given an initial state distribution  the controller is started in the node which maximizes value from that distribution  Once execution has begun  however  there is no belief state updating  In fact  it is possible for the agent to encounter the same belief state twice and be in a different internal state each time        Algorithmic Framework We will describe the policy iteration algorithm in abstract terms  focusing on the key components necessary for convergence  In subsequent sections  we present different possibilities for implementation  Policy iteration takes as input an arbitrary finite state controller  The first phase of an iteration consists of evaluating the controller  as described above  Recall that value iteration was initialized with an arbitrary piecewise linear and convex value function  represented by a set of vectors  In policy iteration  the piecewise linear and convex value function arises out of evaluation of the controller  Each controller node has a value when paired with each state  Thus  each node has a corresponding vector and thus a linear value function over belief state space  Choosing the best node for each belief state yields a piecewise linear and convex value function  The second phase of an iteration is the dynamic programming update  In value iteration  an update produces an improved set of vectors  where each vector corresponds to a deterministic one step policy  The same set of vectors is produced in this case  but the       Policy Iteration for DEC POMDPs  Input  A finite state controller  and a parameter       Evaluate the finite state controller by solving a system of linear equations     Perform a dynamic programming update to add a set of deterministic nodes to the controller     Perform value preserving transformations on the controller     Calculate the Bellman residual  If it is less than           then terminate  Otherwise  go to step    Output  An   optimal finite state controller  Table    Policy Iteration for POMDPs  actions and transition rules for the one step policy cannot be removed from memory  Each new vector is actually a node that gets added to the controller  All of the probability distributions for the added nodes are deterministic  That is  a exhaustive backup in this context creates a new node for each possible action and possible combinations of observations and deterministic transitions into the current controller  This results in the same one step policies being considered as in the dynamic programming update described above  As there are  A  t     possible one step polices  this number also defines the number of new nodes added to the controller after an exhaustive backup  Finally  additional operations are performed on the controller  There are many such operations  and we describe two possibilities in the following section  The only restriction placed on these operations is that they do not decrease the value for any belief state  Such an operation is denoted a value preserving transformation  The complete algorithm is outlined in Table    It is guaranteed to converge to a finitestate controller that is   optimal for all belief states within a finite number of steps  Furthermore  the Bellman residual can be used to obtain a bound on the distance to optimality  as with value iteration        Controller Reductions In performing a DP update  potential nodes that are dominated do not get added to the controller  However  after the update is performed  some of the old nodes may have become dominated  These nodes cannot simply be removed  however  as other nodes may transition into them  This is where the dual view of dominance is useful  Recall that if a node is dominated  then there is a convex combination of other nodes with value at least as high from all states  Thus  we can remove the dominated node and merge it into the dominating convex combination by changing transition probabilities accordingly  This operation was proposed by Poupart and Boutilier        and built upon earlier work by Hansen      b   Formally  a controller reduction attempts to replace a node q  Q with a distribution P  q  over nodes q  Q   q such that for all s  S  X V  s  q   P  q V  s  q   qQ q        Bernstein  Amato  Hansen    Zilberstein  Variables     x   Objective  Maximize   Improvement constraints  s  V  s          X  x  V  s       Probability constraints  X    x         x         Table    The dual linear program for testing dominance for the vector   The variable x   represents P      This can be achieved by solving the linear program in Table    As nodes are used rather than vectors  we replace x   with x q  in the dual formulation which provides a probability distribution of nodes which dominate node q  Rather than transitioning into q  this distribution can then be used instead  It can be shown that if such a distribution is found and used for merging  the resulting controller is a value preserving transformation of the original one        Bounded Backups In the previous section  we described a way to reduce the size of a controller without sacrificing value  The method described in this section attempts to increase the value of the controller while keeping its size fixed  It focuses on one node at a time  and attempts to change the parameters of the node such that the value of the controller is at least as high for all belief states  The idea for this approach originated with Platzman         and was made efficient by Poupart and Boutilier         In this method  a node q is chosen  and parameters for the conditional distribution P  a  q    q  o  are to be determined  Determining these parameters works as follows  We assume that the original controller will be used from the second step on  and try to replace the parameters for q with better ones for just the first step  In other words  we look for parameters which satisfy the following inequality    X X V  s  q   P  a q  R s  a     P  q    q  a  o P  o  s   s  a V  s    q     a  s   o q    for all s  S  Note that the inequality is always satisfied by the original parameters  However  it is often possible to get an improvement  The new parameters can be found by solving a linear program  as shown in Table    Note that the size of the linear program is polynomial in the sizes of the POMDP and the controller  We call this process a bounded backup because it acts like a dynamic programming       Policy Iteration for DEC POMDPs  Variables     x a   x a  o  q     Objective  Maximize   Improvement constraints   s  V  s  q        X    x a R s  a      x a        a  o  X  x a  o  q       x a   q   a  a  x a  o  q    P  o  s   s  a V  s    q      s   o q    a  Probability constraints  X  X  x a       a  o  q    x a  o  q         Table    The linear program to be solved for a bounded backup  The variable x a  represents P  a q   and the variable x a  o  q     represents P  a  q    q  o    backup with memory constraints  To see this  consider the set of nodes generated by a DP backup  These nodes dominate the original nodes across all belief states  so for every original node  there must be a convex combination of the nodes in this set that dominate the original node for all states  A bounded backup finds such a convex combination  It can be shown that a bounded backup yields a value preserving transformation  Repeated application of bounded backups can lead to a local optimum  at which none of the nodes can be improved any further  Poupart and Boutilier        showed that a local optimum has been reached when each nodes value function is touching the value function produced by performing a full DP backup  This is illustrated in Figure        Decentralized Dynamic Programming In the previous section  we presented dynamic programming for POMDPs  A key part of POMDP theory is the fact that every POMDP has an equivalent belief state MDP  No such result is known for DEC POMDPs  making it difficult to generalize value iteration to the multiagent case  This lack of a shared belief state requires a new set of tools to be developed for solving DEC POMDP  As a step in this direction  we were able to develop an optimal policy iteration algorithm for DEC POMDPs that includes the POMDP version as a special case  This algorithm is the focus of the section  We first show how to extend the definition of a stochastic controller to the multiagent case  Multiagent controllers include a correlation device  which is a source of randomness shared by all the agents  This shared randomness increases solution quality while minimally increasing representation size without adding communication  As in the single agent case  policy iteration alternates between exhaustive backups and value preserving transforma      Bernstein  Amato  Hansen    Zilberstein  value function after DP update value function for controller  s   s   Figure    A local optimum for bounded backups  The solid line is the value function for the controller  and the dotted line is the value function for the controller that results from a full DP update   tions  A convergence proof is given  along with efficient transformations that extend those presented in the previous section      Correlated Finite State Controllers The joint policy for the agents is represented using a stochastic finite state controller for each agent  In this section  we first define a type of controller in which the agents act independently  We then provide an example demonstrating the utility of correlation  and show how to extend the definition of a controller to allow for correlation among agents        Local Finite State Controllers In a local controller  the agents node is based on the local observations received  and the agents action is based on the current node  These local controllers are defined in the same way as the POMDP controllers above  with each agent possessing its own controller that operates independently of the others  As before  stochastic transitions and action selection are allowed  We formally define a local controller for agent i as a tuple hQi   i   Ai   i   i i  where  Qi is a finite set of controller nodes   i is a set of inputs  taken to be the local observations for agent i   Ai is a set of outputs  taken to be the actions for agent i   i   Qi  Ai is an action selection function for agent i  defining the distribution of actions selected at each node of that agents controller   i   Qi  Ai  i  Qi is a transition function for agent i  defining the distribution of resulting nodes for each initial node and action taken of that agents controller  The functions i and i parameterize the conditional distribution P  ai   qi   qi   oi   which represents the combined action selection and node transition probability for agent i  When       Policy Iteration for DEC POMDPs  AB BA BB  AA  AA AB BA   R  R  R  s   s    R  BB  Figure    A DEC POMDP for which a correlated joint policy yields more reward than the optimal independent joint policy   taken together  the agents controllers determine the conditional distribution P   a   q     q   o   This is denoted an independent joint controller  In the following subsection  we show that independence can be limiting        The Utility of Correlation The joint controllers described above do not allow the agents to correlate their behavior via a shared source of randomness  We will use a simple example to illustrate the utility of correlation in partially observable domains where agents have limited memory  This example generalizes the one given by Singh        to illustrate the utility of stochastic policies in partially observable settings containing a single agent  Consider the DEC POMDP shown in Figure    This problem has two states  two agents  and two actions per agent  A and B   The agents each have only one observation  and thus cannot distinguish between the two states  For this example  we will consider only memoryless policies  Suppose that the agents can independently randomize their behavior using distributions P  a    and P  a     If the agents each choose either A or B according to a uniform distribution  then they receive an expected reward of  R  per time step  and thus an expected long term R reward of        It is straightforward to show that no independent policy yields higher reward than this one for all states  Next  let us consider the even larger class of policies in which the agents may act in a correlated fashion  In other words  we consider all joint distributions P  a    a     Consider the policy that assigns probability    to the pair AA and probability    to the pair BB  This yields an average reward of   at each time step and thus an expected long term reward of    The difference between the rewards obtained by the independent and correlated policies can be made arbitrarily large by increasing R        Bernstein  Amato  Hansen    Zilberstein        Correlated Joint Controllers In the previous subsection  we established that correlation can be useful in the face of limited memory  In this subsection  we extend our definition of a joint controller to allow for correlation among the agents  To do this  we introduce an additional finite state machine  called a correlation device  that provides extra signals to the agents at each time step  The device operates independently of the DEC POMDP process  and thus does not provide agents with information about the other agents observations  In fact  the random numbers necessary for its operation could be determined prior to execution time and made available to all agents  Formally  a correlation device is a tuple hQc   c i  where Qc is a set of nodes and c   Qc  Qc is a state transition function  At each step  the device undergoes a transition  and each agent observes its state  We must modify the definition of a local controller to take the state of the correlation device as input  Now  a local controller for agent i is a conditional distribution of the form P  ai   qi   qc   qi   oi    The correlation device together with the local controllers form a joint conditional distribution P   a   q     q   o   where  q   hqc   q            qn i  We will refer to this as a correlated joint controller  Note that a correlated joint controller with  Qc       is effectively an independent joint controller  Figure   contains a graphical representation of the probabilistic dependencies in a correlated joint controller  The value function for a correlated joint controller can be computed by solving the   following system of linear equations  one for each s  S and  q  Q   V  s   q     X    P   a  q  R s   a      X  P  s     o s   a P   q     q   a   o V  s     q        s     o   q    a  We sometimes refer to the value of the controller for an initial state distribution  For a distribution b  this is defined as V  b    max q   X  b s V  s   q    s  It is assumed that  given an initial state distribution  the controller is started in the joint node which maximizes value from that distribution  It is worth noting that correlation can increase the value of a set of fixed size controllers  but this same value can be achieved by a larger set of uncorrelated controllers  Thus  correlation is a way to make better use of limited representation size  but is not required to produce a set of optimal controllers  This is formalized by the following theorem  which is proved in Appendix A  The theorem asserts the existence of uncorrelated controllers  determining how much extra memory is needed to replace a correlation device remains an open problem  Theorem   Given an initial state and a correlated joint controller  there always exists some finite size joint controller without a correlation device that produces at least the same value for the initial state        Policy Iteration for DEC POMDPs  a   q   q   a   q   o   qc o   q   Figure    A graphical representation of the probabilistic dependencies in a correlated joint controller for two agents   In the example above  higher value can be achieved with two node uncorrelated controllers for each agent  If the problem starts in s    the first node for each agent would choose A and transition to the second node which would choose B  The second node would then transition back to the first node  The resulting policy consists of the agents alternating R between choosing AA and BB  producing an expected long term reward of   which is higher than the correlated one node policy value of    Thus  doubling memory for each agent in this problem is sufficient to remove the correlation device      Policy Iteration In this section  we describe the policy iteration algorithm  We first extend the definitions of exhaustive backup and value preserving transformation to the multiagent case  Following that  we provide a description of the complete algorithm  along with a convergence proof        Exhaustive Backups We introduced exhaustive backups in the section on dynamic programming for POMDPs  We stated that one way to implement a DP update was to perform an exhaustive backup  and then prune dominated nodes that were created  More efficient implementations were described thereafter  These implementations involved interleaving pruning with node generation  For the multiagent case  it is an open problem whether pruning can be interleaved with node generation  Nodes can be removed  as we will show in a later subsection  but for convergence we require exhaustive backups  We do not define DP updates for the multiagent case  and instead make exhaustive backups a central component of our algorithm  An exhaustive backup adds nodes to the local controllers for all agents at once  and leaves the correlation device unchanged  For each agent i   Ai   Qi   i   nodes are added to the Q local controller  one for each one step policy  Thus  the joint controller grows by  Qc   i  Ai   Qi   Oi   joint nodes  Note that repeated application of exhaustive backups amounts to a brute force search in the space of deterministic policies  This converges to optimality  but is obviously quite inefficient  As in the single agent case  we must modify the joint controller in between       Bernstein  Amato  Hansen    Zilberstein  adding new nodes  For convergence  these modifications must preserve value in a sense that will be made formal in the following section        Value Preserving Transformations We now extend the definition of a value preserving transformation to the multiagent case  In the following subsection  we show how this definition allows for convergence to optimality as the number of iterations grows  The dual interpretation of dominance is helpful in understanding multiagent valuepreserving transformations  Recall that for a POMDP  we say that a node is dominated if there is a convex combination of other nodes with value at least as high for all states  Though we defined a value preserving transformation in terms of the value function across belief states  we could have equivalently defined it so that every node in the original controller has a dominating convex combination in the new controller  For the multiagent case  we do not have the concept of a belief state MDP  so we take the second approach mentioned above  In particular  we require that dominating convex combinations exist for nodes of all the local controllers and the correlation device  A transformation of a controller C to a controller D qualifies as a value preserving transformation if C  D  where  is defined below    and R    respectively  We Consider correlated joint controllers C and D with node sets Q say that C  D if there exist mappings fi   Qi  Ri for each agent i and fc   Qc  Rc such that X V  s   q   P   r  q V  s   r    r    Note that this relation is transitive as further value preserving for all s  S and  q  Q  transformations of D will also be value preserving transformations of C     R    Examples We sometimes describe the fi and fc as a single mapping f   Q of efficient value preserving transformations are given in a later section  In the following subsection  we show that alternating between exhaustive backups and value preserving transformations yields convergence to optimality        Algorithmic Framework The policy iteration algorithm is initialized with an arbitrary correlated joint controller  In the first part of an iteration  the controller is evaluated via the solution of a system of linear equations  Next  an exhaustive backup is performed to add nodes to the local controllers  Finally  value preserving transformations are performed  In contrast to the single agent case  there is no Bellman residual for testing convergence to   optimality  We resort to a simpler test for   optimality based on the discount rate and the number of iterations so far  Let  Rmax   be the largest absolute value of an immediate reward possible in the DEC POMDP  Our algorithm terminates after iteration t    R max   t if        At this point  due to discounting  the value of any policy after step t is less than    Justification for this test is provided in the convergence proof  The complete algorithm is sketched in Table    Before proving convergence  we state a key lemma regarding the ordering of exhaustive backups and value preserving transformations  Its proof is deferred to the Appendix        Policy Iteration for DEC POMDPs  Input  A correlated joint controller  and a parameter       Evaluate the correlated joint controller by solving a system of linear equations     Perform an exhaustive backup to add deterministic nodes to the local controllers     Perform value preserving transformations on the controller  t     Rmax      If        where t is the number of iterations so far  then terminate  Else go to step     Output  A correlated joint controller that is   optimal for all states  Table    Policy Iteration for DEC POMDPs  Lemma   Let C and D be correlated joint controllers  and let C and D be the results of performing exhaustive backups on C and D  respectively  Then C  D if C  D  Thus  if there is a value preserving transformation mapping controller C to D and both are exhaustively backed up  then there is a value preserving transformation mapping controller C to D  This allows value preserving transformations to be performed before exhaustive backups  while ensuring that value is not lost after the backup  We can now state and prove the main convergence theorem for policy iteration  Theorem   For any    policy iteration returns a correlated joint controller that is   optimal for all initial states in a finite number of iterations  Proof  Repeated application of exhaustive backups amounts to a brute force search in the space of deterministic joint policies  Thus  after t exhaustive backups  the resulting controller is optimal for t steps from any initial state  Let t be an integer large enough that  t    Rmax       Then any possible discounted sum of rewards after t time steps is small   enough that optimality over t time steps implies   optimality over the infinite horizon  Now recall the above lemma  which states that performing value preserving transformations before a backup provides at least as much value as just performing a backup  By an inductive argument  performing t steps of policy iteration is a value preserving transformation of the result of t exhaustive backups  We have argued that for large enough t  the value of the controller resulting from t exhaustive backups is within   of optimal for all states  Thus  the result of t steps of policy iteration is also within   of optimal for all states        Efficient Value Preserving Transformations In this section  we describe how to extend controller reductions and bounded backups to the multiagent case  We will show that both of these operations are value preserving transformations        Controller Reductions Recall that in the single agent case  a node can be removed if for all belief states  there is another node with value at least as high  The equivalent dual interpretation is that a node       Bernstein  Amato  Hansen    Zilberstein  can be removed is there exists a convex combination of other nodes with value at least as high across the entire state space  Using the dual interpretation  we can extend this to a rule for removing nodes in the multiagent case  The rule applies to removing nodes either from a local controller or from the correlation device  Intuitively  in considering the removal of a node from a local controller or the correlation device  we consider the nodes of the other controllers to be part of the hidden state  More precisely  suppose we are considering removing node qi from agent is local controller  To do this  we need to find a distribution P  qi   over nodes qi  Qi   qi such that for all s  S  qi  Qi   and qc  Qc   V  s  qi   qi   qc     X  P  qi  V  s  qi   qi   qc     qi  where Qi represents the set of nodes for the other agents  Finding such a distribution can be formulated as a linear program  as shown in Table  a  In this case  success is finding parameters such that       The linear program is polynomial in the sizes of the DEC POMDP and controllers  but exponential in the number of agents  If we are successful in finding parameters that make       then we can merge the dominated node into the convex combination of other nodes by changing all incoming links to the dominated controller node to be redirected based on the distribution P  qi    At this point  there is no chance of ever transitioning into qi   and thus it can be removed  The rule for the correlation device is very similar  Suppose that we are considering the removal of node qc   In this case  we need to find a distribution P  qc   over nodes qc  Qc   qc   such that for all s  S and  q  Q  V  s   q  qc     X  P  qc  V  s   q  qc     qc    for the set of tuples of local controller nodes  Note that we abuse notation here and use Q excluding the nodes for the correlation device  As in the previous case  finding parameters can done using linear programming  This is shown in Table  b  This linear program is also polynomial in the the sizes of the DEC POMDP and controllers  but exponential in the number of agents  We have the following theorem  which states that controller reductions are value preserving transformations  Theorem   Any controller reduction applied to either a local node or a node of the correlation device is a value preserving transformation  Proof  Suppose that we have replaced an agent i node qi with a distribution over nodes in Qi   qi   Let us take fi to be the identity map for all nodes except qi   which will map to the new distribution  We take fc to be the identity map  and we take fj to be the identity map for all j    i  This yields a complete mapping f   We must now show that f satisfies the condition given in the definition of a value preserving transformation        Policy Iteration for DEC POMDPs   a  Variables     x qi   Objective  Maximize   Improvement constraints  s  qi   qc  V  s  qi   qi   qc         X  x qi  V  s  qi   qi   qc    qi  Probability constraints  X  qi  x qi         x qi       qi   b  Variables     x qc   Objective  Maximize   Improvement constraints  s   q V  s   q  qc         X  x qc  V  s   q  qc    qc  Probability constraints  X  qc  x qc         x qc       qc  Table     a  The linear program to be solved to find a replacement for agent is node qi   The variable x qi   represents P  qi     b  The linear program to be solved to find a replacement for the correlation node qc   The variable x qc   represents P  qc     Let Vo be the value function for the original controller  and let Vn be the value function for the controller with qi removed  A controller reduction requires that Vo  s   q    X  P   r  q Vo  s   r     r    Thus  we have for all s  S and  q  Q   Vo  s   q     X  P   a  q  R s  a       X  P   q     q   a   o P  s     o s   a Vo  s     q      s     o   q    a     X  a  P   a  q  R s  a       X  P   q     q   a   o P  s     o s   a   s     o   q        X   r   P   r  q Vo  s   r       Bernstein  Amato  Hansen    Zilberstein      X  P   a  q  R s  a       X  P   q     q   a   o P  s     o s   a P   r  q Vo  s   r      s     o   q      r    a    Notice that the formula on the right is the Bellman operator for all s  S and  q  Q  for the new controller  applied to the old value function  Denoting this operator Tn   the system of inequalities implies that Tn Vo  Vo   By monotonicity  we have that for all k     Tnk    Vo    Tnk  Vo    Since Vn   limk Tnk  Vo    we have that Vn  Vo   This is sufficient for f to satisfy the condition in the definition of value preserving transformation  The argument for removing a node of the correlation device is almost identical to the one given above          Bounded Dynamic Programming Updates In the previous section  we described a way to reduce the size of a controller without sacrificing value  Recall that in the single agent case  we could also use bounded backups to increase the value of the controller while keeping its size fixed  This technique can be extended to the multiagent case  As in the previous section  the extension relies on improving a single local controller or the correlation device  while viewing the nodes of the other controllers as part of the hidden state  We first describe in detail how to improve a local controller  To do this  we choose an agent i  along with a node qi   Then  for each oi  i   we search for new parameters for the conditional distribution P  ai   qi   qi   oi    The search for new parameters works as follows  We assume that the original controller will be used from the second step on  and try to replace the parameters for qi with better ones for just the first step  In other words  we look for parameters satisfying the following inequality    X X V  s   q   P   a  q  R s  a     P   q     q   a   o P  s     o s   a V  s     q      a  s     o   q   for all s  S  qi  Qi   and qc  Qc   The search for new parameters can be formulated as a linear program  as shown in Table  a  Its size is polynomial in the sizes of the DEC POMDP and the joint controller  but exponential in the number of agents  The procedure for improving the correlation device is very similar to the procedure for improving a local controller  We first choose a device node qc   and consider changing its parameters for just the first step  We look for parameters satisfying the following inequality    X X P   a  q  R s  a     P   q     q   a   o P  s     o s   a V  s     q     V  s   q    a  s     o   q     for all s  S and  q  Q  As in the previous case  the search for parameters can be formulated as a linear program  This is shown in Table  b  This linear program is also polynomial in the sizes of the DECPOMDP and joint controller  but exponential in the number of agents  The following theorem states that bounded backups preserve value        Policy Iteration for DEC POMDPs   a  Variables     x qc   ai    x qc   ai   oi   qi    Objective  Maximize   Improvement constraints  s  qi   qc  X  V  s   q  qc         P  ai  qc   qi   x qc   ai  R s   a      a    X    x c  ai   oi   qi   P  qi  qc   qi   ai   oi    s     o   q    qc    P   o  s   s   a P  qc   qc  V  s     q     qc      Probability constraints  X qc x qc   ai         qc   ai   oi  x qc   ai   oi   qi      x qc   ai    qi   ai  qc   ai  X  x qc   ai        qc   ai   oi   qi   x qc   ai   oi   qi         b  Variables     x qc    Objective  Maximize   Improvement constraints  s   q V  s   q  qc         X  P   a qc    q  R s   a      X  P   q    qc    q   a   o   s     o   q    qc    a     P  s    o s   a x qc   V  s     q     qc      Probability constraints  qc   X  x qc          qc   x qc        qc   Table     a  The linear program used to find new parameters for agent is node qi   The variable x qc   ai   represents P  ai  qi   qc    and the variable x qc   ai   oi   qi    represents P  ai   qi   qc   qi   oi     b  The linear program used to find new parameters for the correlation device node qc   The variable x qc    represents P  qc   qc           Bernstein  Amato  Hansen    Zilberstein  Theorem   Performing a bounded backup on a local controller or the correlation device produces a new correlated joint controller which is a value preserving transformation of the original  Proof  Consider the case in which some node qi of agent is local controller is changed  We define f to be a deterministic mapping from nodes in the original controller to the corresponding nodes in the new controller  Let Vo be the value function for the original controller  and let Vn be the value function for the new controller  Recall that the new parameters for P  ai   qi   qc   qi   oi   must satisfy the following inequality for all s  S  qi  Qi   and qc  Qc     X X Vo  s   q   P   a  q  R s  a     P   q     q   a   o P  s     o s   a Vo  s     q        a  s     o   q   Notice that the formula on the right is the Bellman operator for the new controller  applied to the old value function  Denoting this operator Tn   the system of inequalities implies that Tn Vo  Vo   By monotonicity  we have that for all k     Tnk    Vo    Tnk  Vo    Since Vn   limk Tnk  Vo    we have that Vn  Vo   Thus  the new controller is a value preserving transformation of the original one  The argument for changing nodes of the correlation device is almost identical to the one given above        Open Issues We noted at the beginning of the section that there is no known way to convert a DECPOMDP into an equivalent belief state MDP  Despite this fact  we were able to develop a provably convergent policy iteration algorithm  However  the policy iteration algorithm for POMDPs has other desirable properties besides convergence  and we have not yet been able to extend these to the multiagent case  Two such properties are described below        Error Bounds The first property is the existence of a Bellman residual  In the single agent case  it is possible to compute a bound on the distance to optimality using two successive value functions  In the multiagent case  policy iteration produces a sequence of controllers  each of which has a value function  However  we do not have a way to obtain an error bound from these value functions  For now  to bound the distance to optimality  we must consider the discount rate and the number of iterations completed        Avoiding Exhaustive Backups In performing a DP update for POMDPs  it is possible to remove certain nodes from consideration without first generating them  In Section    we gave a high level description of a few different approaches to doing this  For DEC POMDPs  however  we did not define a DP update and instead used exhaustive backups as the way to expand a controller  Since exhaustive backups are expensive  it would be useful to extend the more sophisticated pruning methods for POMDPs to the multiagent case        Policy Iteration for DEC POMDPs  Input  A joint controller  the desired number of centralized belief points k  initial state b  and fixed policy for each agent i      Starting from b    sample a set of k belief points for each agent assuming the other agents use their fixed policy     Evaluate the joint controller by solving a system of linear equations     Perform an exhaustive backup to add deterministic nodes to the local controllers     Retain nodes that contribute the highest value at each of the belief points     For each agent  replace nodes that have lower value than some combination of other nodes at each belief point     If controller sizes and parameters do not change then terminate  Else go to step    Output  A new joint controller based on the sampled centralized belief points  Table    Heuristic Policy Iteration for DEC POMDPs   Unfortunately  in the case of POMDPs  the proofs of correctness for these methods all use the fact that there exists a Bellman equation  Roughly speaking  this equation allows us to determine whether a potential node is dominated by just analyzing the nodes that would be its successors  Because we do not currently have an analog of the Bellman equation for DEC POMDPs  we have not been able to generalize these results  There is one exception to the above statement  however  When an exhaustive backup has been performed for all agents except one  then a type of belief state space can be constructed for the agent in question using the system states and the nodes for the other agents  The POMDP node generation methods can then be applied to just that agent  In general  though  it seems difficult to rule out a node for one agent before generating all the nodes for the other agents      Heuristic Policy Iteration While the optimal policy iteration method shows how a set of controllers with value arbitrarily close to optimal can be found  the resulting controllers may be very large and many unnecessary nodes may be generated along the way  This is exacerbated by the fact that the algorithm cannot take advantage of an initial state distribution and must attempt to improve the controller for any initial state  As a way to combat these disadvantages  we have developed a heuristic version of policy iteration that removes nodes based on their value only at a given set of centralized belief points  We call these centralized belief points because they are distributions over the system state that in general could only be known by full observability of the problem  As a result  the algorithm will no longer be optimal  but it can often produce more concise controllers with higher solution quality for a given initial state distribution        Bernstein  Amato  Hansen    Zilberstein      Directed Pruning Our heuristic policy iteration algorithm uses sets of belief points to direct the pruning process of our algorithm  There are two main advantages of this approach  it allows simultaneous pruning for all agents and it focuses the controller on certain areas of the belief space  We first discuss the benefits of simultaneous pruning and then mention the advantages of focusing on small areas of the belief space  As mentioned above  the pruning method used by the optimal algorithm will not always remove all nodes that could be removed from all the agents controllers without losing value  Because pruning requires each agent to consider the controllers of other agents  after nodes are removed for one agent  the other agents may be able to prune other nodes  Thus pruning must cycle through the agents and ceases when no agent can remove any further nodes  This is both time consuming and causes the controller to be much larger than it needs to be  Like the game theoretic concept of incredible threats    a set of suboptimal policies for an agent may be useful only because other agents may employ similarly suboptimal policies  That is  because pruning is conducted for each agent while holding the other agents policies fixed  polices that are useful for any set of other agent policies are retained  no matter the quality of these other agent policies  Some of an agents policies may only be retained because they have the highest value when used in conjunction with other suboptimal policies of the other agents  In these cases  only by removing the set of suboptimal policies simultaneously can controller size be reduced while at least maintaining value  This simultaneous pruning could further reduce controller sizes and thus increase scalability and solution quality  While it may be possible to define a value preserving transformation for these problems  finding a nontrivial automated way to do so while maintaining the optimality of the algorithm remains an open question  The advantage of considering a smaller part of the state space has already been shown to produce drastic performance increases in POMDPs  Ji  Parr  Li  Liao    Carin        Pineau  Gordon    Thrun        and finite horizon DEC POMDPs  Seuken   Zilberstein        Szer   Charpillet         For POMDPs  a problem with many states has a belief space with large dimensionality  but many parts may never be visited by an optimal policy  Focusing on a subset of belief states can allow a large part of the state space to be ignored without significant loss of solution quality  The problem of having a large state space is compounded in the DEC POMDP case  Not only is there uncertainty about the state  but also about the policies of the other agents  As a consequence  the generalized belief space which includes all possible distributions over states of the system and current policies of the other agents must be considered to guarantee optimality  This results in a huge space which contains many unlikely states and policies  The uncertainty about which policies other agents may utilize does not allow belief updates to normally be calculated for DEC POMDPs  but as we showed above  it can be done by assuming a probability distribution over actions of the other agents  This limits the number of policies that need to be considered by all agents and if the distributions are chosen well  may permit a high valued solution to be found     An incredible threat is an irrational strategy that the agent knows it will receive a lower value by choosing it  While it is possible the agent will choose the incredible threat strategy  it is irrational to do so         Policy Iteration for DEC POMDPs  Variables     x qi   and for each belief point b Objective  Maximize   Improvement constraints   b  qi  X   X   b s  x qi  V  qi   qi   s   V   q  s      s  X  Probability constraints   qi  x qi       and qi x qi       qi  Table    The linear program used to determine if a node q for agent i is dominated at each point b and all initial nodes of the other agents controllers  As node q may be dominated by a distribution of nodes  variable x qi   represents P  qi    the probability of starting in node q for agent i       Belief Set Generation As mentioned above  our heuristic policy iteration algorithm constructs sets of belief points for each agent which are later used to evaluate the joint controller and remove dominated nodes  To generate the belief point set  we start at the initial state and by making assumptions about the other agents  we can calculate the resulting belief state for each action and observation pair of an agent  By fixing the policies for the other agents  this belief state update can be calculated in a way very similar to that described for POMDPs in section        This procedure can be repeated from each resulting belief state until a desired number of points is generated or no new points are visited  More formally  we assume the other agents have a fixed distribution of action choice for each system state  That is  if we know P   ai  s  then we can determine the probability any state results given a belief point and an agents action and observation  The derivation of the likelihood of state s    given the belief state b  and agent is action ai and observation oi is shown below   P  s   ai   oi   b     X  P  s     ai    oi   s ai   oi   b    ai    oi  s  o s  b   a  s   P  s    s   a  b   ai    oi  s P     P    P  oi   ai   b  o s   a  s   P  s   s   a  b P   a  s  b   ai    oi  s P     P    P  oi   ai   b  o s   a  s   P  s   s   a P   ai  a  s  b P   a  s  b   ai    oi  s P     P    P  oi   ai   b     P  s   s    P    o  s    a   s a P   ai  ai   s  b P  s ai   b P  ai   b   ai    oi  s  P    P  oi   ai   b        Bernstein  Amato  Hansen    Zilberstein  P    o s   a  s  ai    oi  s P        P  s   s    a P   ai  s b s   P  oi  ai   b   where X  P  oi  ai   b     P   o s   a  s   P  s   s   a P   ai  s b s   ai  oi  s s   Thus  given the action probabilities for the other agents  i  and the transition and observation models of the system  a belief state update can be calculated      Algorithmic Framework We provide a formal description of our approach in Table    Given the desired number of belief points  k  and random action and observation selection for each agent  the sets of points are generated as described above  The search begins at the initial state of the problem and continues until the given number of points is obtained  If no new points are found  this process can be repeated to ensure a diverse set is produced  The arbitrary initial controller is evaluated and the value at each state and for each initial node of any agents controller is retained  The exhaustive backup procedure is exactly the same as the one used in the optimal algorithm  but updating the controller takes place in two steps  First  for each of the k belief points  the highest valued set of initial nodes is found  To accomplish this  the value of beginning at each combination of nodes for all agents is calculated for each of these k points and the best combination is kept  This allows nodes that do not contribute to any of these values to be simultaneously pruned  Next  each node of each agent is pruned using the linear program shown in Table    If a distribution of nodes for the given agent has higher value at each of the belief points for any initial nodes of the other agents controllers  it is pruned and replaced with that distribution  The new controllers are then evaluated and the value is compared with the value of the previous controller  This process of backing up and pruning continues while the controller parameters continue to change  Similar to how bounded policy updates can be used in conjunction with pruning in the optimal policy iteration algorithm  a nonlinear programming approach  Amato et al         can be used to improve solution quality for the heuristic case  To accomplish this  instead of optimizing the controller for just the initial belief state of the problem  all the belief points being considered are used  A simple way to achieve this is to maximize over the sum of the values of the initial nodes of the controllers weighted by the probabilities given for each point  This approach can be used after each pruning step and may further improve value of the controllers      Dynamic Programming Experiments This section describes the results of experiments performed using policy iteration  Because of the flexibility of the algorithm  it is impossible to explore all possible ways of implementing it  However  we did experiment with a few different implementation strategies to gain an idea of how the algorithm works in practice  All of these experiments were run on a     GHz Intel Pentium   with  GB of memory  Three main sets of experiments were performed on a single set of test problems        Policy Iteration for DEC POMDPs  Our first set of experiments focused on exhaustive backups and controller reductions  The results confirm that value improvement can be obtained through iterated application of these two operations  Further improvement is demonstrated by also incorporating bounded updates  However  because exhaustive backups are expensive  the algorithm was unable to complete more than a few iterations on any of our test problems  In the second set of experiments  we addressed the complexity issues by using only bounded backups  and no exhaustive backups  With bounded backups  we were able to obtain higher valued controllers while keeping memory requirements fixed  We examined how the sizes of the initial local controllers and the correlation device affected the value of the final solution  The third set of experiments examined the complexity issues caused by exhaustive backups by using the point based heuristic  This allowed our heuristic policy iteration algorithm to complete more iterations than the optimal algorithm and in doing so  increased solution quality of the largest solvable controllers  By incorporating Amato et al s NLP approach  the heuristic algorithm becomes slightly less scalable than with heuristic pruning alone  but the amount of value improvement per step increases  This causes the resulting controllers in each domain to have the highest value of any approach      Test Domains In this section  we describe three test domains  ordered by the size of the problem representation  For each problem  the transition function  observation function  and reward functions are described  In addition  an initial state is specified  Although policy iteration does not require an initial state as input  one is commonly assumed and is used by the heuristic version of the algorithm  A few different initial states were tried for each problem  and qualitatively similar results were obtained  In all domains  a discount factor of     was utilized  As a very loose upper bound  the centralized policy was calculated for each problem in which all agents share their observations with a central agent and decisions for all agents are made by the central agent  This results in a POMDP with the same number of states  but the action and observation sets are Cartesian products of the agents action and observation sets  The value of this POMDP policy is provided below  but because DEC POMDP policies are more constrained  the optimal value may be much lower  Two Agent Tiger Problem The two agent tiger problem consists of   states    actions and   observations  Nair et al          The domain includes two doors  one of which leads to a tiger and the other to a large treasure  Each agent may open one of the doors or listen  If either agent opens the door with the tiger behind it  a large penalty is given  If the door with the treasure behind it is opened and the tiger door is not  a reward is given  If both agents choose the same action  i e   both opening the same door  a larger positive reward or a smaller penalty is given to reward cooperation  If an agent listens  a small penalty is given and an observation is seen that is a noisy indication of which door the tiger is behind  While listening does not change the location of the tiger  opening a door causes the tiger to be placed behind one of the       Bernstein  Amato  Hansen    Zilberstein  door with equal probability  The problem begins with the tiger equally likely to be located behind either door  The optimal centralized policy for this problem has value         Meeting on a Grid In this problem  with    states    actions and   observations  two robots must navigate on a two by two grid  Each robot can only sense whether there are walls to its left or right  and their goal is to spend as much time as possible on the same square as the other agent  The actions are to move up  down  left  or right  or to stay on the same square  When a robot attempts to move to an open square  it only goes in the intended direction with probability      otherwise it either goes in another direction or stays in the same square  Any move into a wall results in staying in the same square  The robots do not interfere with each other and cannot sense each other  The reward is   when the agents share a square  and   otherwise  The initial state places the robots diagonally across from each other and the optimal centralized policy for this problem has value        Box Pushing Problem This problem  with     states    actions and   observations consists of two agents that get rewarded by pushing different boxes  Seuken   Zilberstein         The agents begin facing each other in the bottom corners of a four by three grid with the available actions of turning right  turning left  moving forward or staying in place  There is a     probability that the agent will succeed in moving and otherwise will stay in place  but the two agents can never occupy the same square  The middle row of the grid contains one large box in the middle of two small boxes  The small boxes can be moved by a single agent  but the large box can only be moved by both agents pushing at the same time  The upper row of the grid is considered the goal row  which the boxes are pushed into  The possible deterministic observations for each agent consist of seeing an empty space  a wall  the other agent  a small box or the large box  A reward of     is given if both agents push the large box to the goal row and    is given for each small box that is moved to the goal row  A penalty of    is given for each agent that cannot move and      is given for each time step  Once a box is moved to the goal row  the environment resets to the original start state  The optimal centralized policy for this problem has value              Exhaustive Backups and Controller Reductions In this section  we present the results of using exhaustive backups together with controller reductions  For each domain  the initial controllers for each agent contained a single node with a self loop  and there was no correlation device  For each problem  the first action of the problem description was used  This resulted in the repeated actions of opening the left door in the two agent tiger problem  moving up in the meeting on a grid problem and turning left in the box pushing problem  The reason for starting with the smallest possible controllers was to see how many iterations we could complete before running out of memory  On each iteration  we performed an exhaustive backup  and then alternated between agents  performing controller reductions until no more nodes could be removed  For bounded dynamic programming results  after the reductions were completed bounded updates were also performed for all agents  For these experiments  we attempted to improve the nodes of       Policy Iteration for DEC POMDPs  Iteration          Two Agent Tiger   S        Ai         i       Exhaustive Sizes Controller Reductions Bounded Updates                  in  s            in  s                   in  s           in   s                          in  s              in   s                               in     s                 in     s   Iteration        Meeting on a Grid   S         Ai         i       Exhaustive Sizes Controller Reductions Bounded Updates                 in  s           in  s                  in  s           in    s                          in    s                 in     s   Iteration        Box Pushing   S          Ai         i       Exhaustive Sizes Controller Reductions Bounded Updates                in  s          in   s                 in    s           in    s                         in    s               in    s   Table     Results of applying exhaustive backups  controller reductions and bounded updates to our test problems  The second column contains the sizes of the controllers if only exhaustive backups had been performed  The third column contains the resulting value  sizes of the controllers  and time required for controller reductions to be performed on each iteration  The fourth column displays these same quantities with bounded updates also being used  The   denotes that a backup and pruning were performed  but bounded updates exhausted the given resources   each agent in turn until value could not be improved for any node of any agent  For each iteration  we recorded the sizes of the controllers produced  and noted what the sizes would be if no controller reductions had been performed  In addition  we recorded the value from the initial state and the total time taken to reach the given result  The results are shown in Table     Because exhaustive backups add many nodes  we were unable to complete many iterations without exceeding memory limits  As expected  the smallest problem led to the largest number of iterations being completed  Although we could not complete many iterations before running out of memory  the use of controller reductions led to significantly smaller controllers compared to the approach of just applying exhaustive backups  Incorporating bounded updates requires some extra time  but is able to improve the value produced at each step  causing substantial improvement in some cases  It is also interesting to notice that the controller sizes when using bounded updates are not always the same as when only controller reductions are completed  This can be seen after two iterations in both the meeting on a grid and box pushing problems  This can occur because the bounded updates change node value and thus change the number and location of the nodes that are pruned  In the box pushing problem  the two agents also       Bernstein  Amato  Hansen    Zilberstein  have different size controllers after two steps  This can occur  even in symmetric problems  when a set of actions is only necessary for a single agent      Bounded Dynamic Programming Updates As we saw from the previous experiments  exhaustive backups can fill up memory very quickly  This leads naturally to the question of how much improvement is possible without exhaustive backups  In this section  we describe an experiment in which we repeatedly applied bounded backups  which left the size of the controller fixed  We experimented with different starting sizes for the local controllers and the correlation device  We define a trial run of the algorithm as follows  At the start of a trial run  a size is chosen for each of the local controllers and the correlation device  The action selection and transition functions are initialized to be deterministic  with the outcomes drawn according to a uniform distribution  A step consists of choosing a node uniformly at random from the correlation device or one of the local controllers  and performing a bounded backup on that node  After     steps  the run is considered over  In practice  we found that values often stabilized in fewer steps  We varied the sizes of the local controllers while maintaining the same number of nodes for each agent  and we varied the size of the correlation device from   to    For each domain  we increased number of nodes until the required number of steps could not be completed in under four hours  In general  runs required significantly less time to terminate  For each combination of sizes  we performed    trial runs and recorded the best value over all runs  For each of the three problems  we were able to obtain solutions with higher value than with exhaustive backups  Thus  we see that even though repeated application of bounded backups does not have an optimality guarantee  it can be competitive with an algorithm that does  However  it should be noted that we have not performed an exhaustive comparison  We could have made different design decisions for both approaches concerning the starting controllers  the order in which nodes are considered  and other factors  Besides comparing to the exhaustive backup approach  we wanted to examine the effect of the sizes of the local controllers and the correlation device on value  Figure   shows a graph of best values plotted against controller size  We found that  for the most part  the value increases when we increase the size of the correlation device from one node to two nodes  essentially moving from independent to correlated   It is worth noting that the solution quality had somewhat high variance in each problem  showing that setting good initial parameters is important for high valued solutions  For small controllers  the best value tends to increase with controller size  However  for very large controllers  this not always the case  This can be explained by considering how a bounded backup works  For new node parameters to be acceptable  they must not decrease the value for any combination of states  nodes for the other controllers  and nodes for the correlation device  This becomes more difficult as the numbers of nodes increase  and thus it is easier to get stuck in a local optimum  This can be readily seen in the two agent tiger problem and to some extent the meeting on a grid problem  Memory was exhausted before this phenomenon takes place in the box pushing problem        Policy Iteration for DEC POMDPs   a    b    c  Figure    Best value per trial run plotted against the size of the local controllers  for  a  the two agent tiger problem   b  the meeting in a grid problem and  c  the box pushing problem  The solid line represents independent controllers  a correlation device with one node   and the dotted line represents a joint controller including a two node correlation device  Times ranged from under  s for one node controllers without correlation to four hours for the largest controller found with correlation in each problem       Heuristic Dynamic Programming Updates As observed above  the optimal dynamic programming approach can only complete a small number of backups before resources are exhausted  Similarly  using bounded updates with fixed size controllers can generate high value solutions  but it can be difficult to pick the correct controller size and initial parameters  As an alternative to the other approaches  we also present experiments using our heuristic dynamic programming algorithm  Like the optimal policy iteration experiments  we initialized single node controllers for each agent with self loops and no correlation device  The same first actions were used as above and backups were performed until memory was exhausted  The set of belief points for each problem was generated given the initial state distribution and a distribution of actions for the other agents  For the meeting on a grid and box pushing problems  it was       Bernstein  Amato  Hansen    Zilberstein  assumed that all agents chose any action with equal probability regardless of state  For the two agent tiger problem  it was assumed that for any state agents listen with probability     and open each door with probability      This simple heuristic policy was chosen to allow more of the state space to be sampled by our search  The number of belief points used for the two agent tiger and meeting on a grid problems was ten and twenty points were used for the box pushing problem  For each iteration  we performed an exhaustive backup and then pruned controllers as described in steps four and five of Table    All the nodes that contributed to the highest value for each belief point were retained and then each node was examined using the linear program in Table    For results with the NLP approach  we also improved the set of controllers after heuristic pruning by optimizing a nonlinear program whose objective was the sum of the values of the initial nodes weighted by the belief point probabilities  We report the value produced by the optimal and heuristic approaches for each iteration that could be completed in under four hours and with the memory limits of the machine used  The nonlinear optimization was performed on the NEOS server  which provides a set of machines with varying CPU speeds and memory limitations  The values for each iteration of each problem are given in Figure    We see the heuristic policy iteration  HPI  methods are able to complete more iterations than the optimal methods and as a consequence produce higher values  In fact  the results from HPI are almost always exactly the same as those for the optimal policy iteration algorithm without bounded updates for all iterations that can be completed by the optimal approach  Thus  improvement occurs primarily due to the larger number of backups that can be performed  We also see that while incorporating bounded updates improves value for the optimal algorithm  incorporating the NLP approach into the heuristic approach produces even higher value  Optimizing the NLP requires a small time overhead  but substantially increases value on each iteration  This results in the highest controller value in each problem  Using the NLP also allows our heuristic policy iteration to converge to a six node controller for each agent in the two agent tiger problem  Unfortunately  this solution is known to be suboptimal  As an heuristic algorithm  this is not unexpected  and it should be noted that even suboptimal solutions by the heuristic approach outperform all other methods in all our test problems      Discussion We have demonstrated how policy iteration can be used to improve both correlated and independent joint controllers  We showed that using controller reductions together with exhaustive backups is more efficient in terms of memory than using exhaustive backups alone  However  due to the complexity of exhaustive backups  even that approach could only complete a few iterations on each of our test problems  Using bounded backups alone provided a good way to deal with the complexity issues  With bounded backups  we were able to find higher valued policies than with the previous approach  Through our experiments  we were able to understand how the sizes of the local controllers and correlation device affect the final values obtained  With our heuristic policy iteration algorithm  we demonstrated further improvement by dealing with some of the complexity issues  The heuristic approach is often able to continue       Policy Iteration for DEC POMDPs   a    b    c  Figure    Comparison of the dynamic programming algorithms on  a  the two agent tiger problem   b  the meeting in a grid problem and  c  the box pushing problem  The value produced by policy iteration with and without bounded backups as well as our heuristic policy iteration with and without optimizing the NLP were compared on each iteration until the time or memory limit was reached   improving solution quality past the point where the optimal algorithm exhausts resources  More efficient use of this limited representation size is achieved by incorporating the NLP approach as well  In fact  the heuristic algorithm with NLP improvements at each step provided results that are at least equal to the highest value obtained in each problem and sometimes were markedly higher than the other approaches  Furthermore  as far as we know  these results are the highest published values for all three of the test domains      Conclusion We present a policy iteration algorithm for DEC POMDPs  The algorithm uses a novel policy representation consisting of stochastic finite state controllers for each agent along with a correlation device  We define value preserving transformations and show that alternating between exhaustive backups and value preserving transformations leads to convergence to       Bernstein  Amato  Hansen    Zilberstein  optimality  We also extend controller reductions and bounded backups from the single agent case to the multiagent case  Both of these operations are value preserving transformations and are provably efficient  Finally  we introduced a heuristic version of our algorithm which is more scalable and produces higher values on our test problems  Our algorithm serves as the first nontrivial exact algorithm for DEC POMDPs  and provides a bridge to the large body of work on dynamic programming for POMDPs  Our work provides a solid foundation for solving DEC POMDPs  but much work remains in addressing more challenging problem instances  We focused on solving general DECPOMDPs  but the efficiency of our approaches could be improved by using structure found in certain problems  This would allow specialized representations and solution techniques to be incorporated  Below we describe some key challenges of our general approach  along with some preliminary algorithmic ideas to extend our work on policy iteration  Approximation with Error Bounds Often  strict optimality requirements cause computational difficulties  A good compromise is to search for policies that are within some bound of optimal  Our framework is easily generalized to allow for this  Instead of a value preserving transformation  we could define an   value preserving transformation  which insures that the value at all states decreases by at most    We can perform such transformations with no modifications to any of our linear programs  We simply need to relax the requirement on the value for   that is returned  It is easily shown that using an   value preserving transformation at each step leads to convergence to a policy that is   within   of optimal for all states  For controller reductions  relaxing the tolerance may lead to smaller controllers because some value can be sacrificed  For bounded backups  it may help in escaping from local optima  Though relaxing the tolerance for a bounded backup could lead to a decrease in value for some states  a small downward step could lead to higher value overall in the long run  We are currently working on testing these hypotheses empirically  General Sum Games In a general sum game  there is a set of agents  each with its own set of strategies  and a strategy profile is defined to be a tuple of strategies for all agents  Each agent assigns a payoff to each strategy profile  The agents may be noncooperative  so the same strategy profile may be assigned different values for each agent  The DEC POMDP model can be extended to a general sum game by allowing each agent to have its own reward function  In this case  the strategies are the local policies  and a strategy profile is a joint policy  This model is often called a partially observable stochastic game  POSG   Hansen et al         presented a dynamic programming algorithm for finitehorizon POSGs  The algorithm was shown to perform iterated elimination of dominated strategies in the game  Roughly speaking  it eliminates strategies that are not useful for an agent  regardless of the strategies of the other agents  Work remains to be done on extending the notion of a value preserving transformation to the noncooperative case  One possibility is to redefine value preserving transformations so that value is preserved for all agents  This is closely related to the idea of Pareto optimality  In a general sum game  a strategy profile is said to be Pareto optimal if there does not exist another strategy profile that yields higher payoff for all agents  It seems that policy iteration using the revised definition of value preserving transformation would tend to move the controller in the direction of the Pareto optimal set  Another possibility is       Policy Iteration for DEC POMDPs  to define value preserving transformations with respect to specific agents  As each agent transforms its own controller  the joint controller should move towards a Nash equilibrium  Handling Large Numbers of Agents The general DEC POMDP representation presented in this paper grows exponentially with the number of agents  as seen in the growth of the set of joint actions and observations as well as the transition  reward and observation functions  Thus this representation is not feasible for large numbers of agents  However  a compact representation is possible if each agent interacts directly with just a few other agents  We can have a separate state space for each agent  factored transition probabilities  and a reward function that is the sum of local reward functions for clusters of agents  In this case  the problem size is exponential only in the maximum number of agents interacting directly  This idea is closely related to recent work on graphical games  La Mura        Koller   Milch         Once we have a compact representation  the next question to answer is whether we can adapt policy iteration to work efficiently with the representation  This indeed seems possible  With the value preserving transformations we presented  the nodes of the other agents are considered part of the hidden state of the agent under consideration  These techniques modify the controller of the agent to get value improvement for all possible hidden states  When an agents state transitions and rewards do not depend on some other agent  it should not need to consider that agents nodes as part of its hidden state  A specific compact representation along with extensions of different algorithms was proposed by Nair et al           Acknowledgments We thank Martin Allen  Marek Petrik and Siddharth Srivastava for helpful discussions of this work  Marek and Siddharth  in particular  helped formalize and prove Theorem    The anonymous reviewers provided valuable feedback and suggestions  Support for this work was provided in part by the National Science Foundation under grants IIS         and IIS          by NASA under cooperative agreement NCC         and by the Air Force Office of Scientific Research under grants F                and FA                 Appendix A  Proof of Theorem   A correlation device produces a sequence of values that all the agents can observe  Let X be the set of all possible infinite sequences that can be generated by a correlation device  Let Vx   q    s    be the value of the correlated joint controller with respect to some correlation sequence x  X  initial nodes  q  of the agent controllers  and initial state s  of the problem  We will refer to Vx   q    s    simply as Vx  the value of some sequence x  given the controllers for the agents  We define a regular sequence as a sequence that can be generated by a regular expression  Before we prove Theorem    we establish the following property  Lemma   The value of any sequence  whether regular or non regular  can be approximated within any   by some other sequence  Proof  The property holds thanks to the discount factor used in infinite horizon DECPOMDPs  Given a sequence x with value Vx   we can determine another sequence x  such       Bernstein  Amato  Hansen    Zilberstein  that  Vx   Vx        The sequence x  is constructed by choosing the first k elements of x  and then choosing an arbitrary regular or non regular sequence for the remaining elements  kR max As long as k is chosen such that          then  Vx   Vx          Theorem   Given an initial state and a correlated joint controller  there always exists some finite size joint controller without a correlation device that produces at least the same value for the initial state  Proof  Let E represent the expected value of the joint controller with the correlation device  Let V    Vx   x  X  be the set of values produced by all the possible correlation device sequences  Let inf and sup represent the infimum and supremum of V respectively  We break the proof into two cases  depending on the relation of the expectation versus the supremum  We show in each case that a regular sequence can be found that produces at least the same value as E  Once such a regular sequence is found  then that sequence can be generated by a finite state controller that can be embedded within each agent  Thus  a finite number of nodes can be added to the agents controllers to provide equal or greater value  without using a correlation device  Case     inf  E   sup Based on Lemma    there is some regular sequence x that can approximate the supremum within    If we choose     sup E  then Vx  sup     E  Case     E   sup If there is a regular sequence  x  for which Vx   E  we can choose that sequence  If no such regular sequence exists  we will show that E    sup  We give a somewhat informal argument  but this can be more formally proven using cylinder sets as discussed by Parker         We begin by first choosing some regular sequence  We can construct a neighborhood around this sequence  as described in Lemma    by choosing a fixed length prefix of A prefixP of length k has a well defined probability that is defined as P the sequence  P       q           k   q k    where P  q     is the probability distribution P  q P  q c c c c c qc  qc  qck  P  qc of initial node of the correlation device and P  qci  qci    represents the probability of transitioning to correlation device node qci from node qci    The set of sequences that possess this prefix has probability equal to that of the prefix  Because we assumed there exists some regular sequence which has value less than the supremum  we can always choose a prefix and length such that the values of the sequences in the set are less than the supremum  Because the probability of this set is nonzero and the value of these sequences is less than the supremum  then E    sup  which is a contradiction  Therefore  some regular sequence can be found that provides at least the same value as the expected value of the correlated joint controller  This allows some uncorrelated joint controller to produce at least the same value as a given correlated one     Appendix B  Proof of Lemma   For ease of exposition  we prove the lemma under the assumption that there is no correlation device  Including a correlation device is straightforward but unnecessarily tedious        Policy Iteration for DEC POMDPs  Lemma   Let C and D be correlated joint controllers  and let C and D be the results of performing exhaustive backups on C and D  respectively  Then C  D if C  D  Proof  Suppose we are given controllers C and D  where C  D  Call the sets of joint   and R    respectively  It follows that there exists a function nodes for these controllers Q   fi   Qi  Ri for each agent i such that for all s  S and  q  Q V  s   q    X  P   r  q V  s   r      r  We now define functions fi to map between the two controllers C and D  For the old nodes  we define fi to produce the same output as fi   It remains to specify the results of fi applied to the nodes added by the exhaustive backup  New nodes of C will be mapped to distributions involving only new nodes of D  To describe the mapping formally  we need to introduce some new notation  Recall that the new nodes are all deterministic  For each new node  r in controller D  the nodes action is denoted  a  r   and its transition rule is denoted  r     r   o   Now  the mappings fi are defined such that P   r  q    P   a  r   q   YX  P   q     q   a  r    o P   r     r   o   q      q       o  for all  q in controller C and  r in controller D  We must now show that the mapping f satisfies the inequality given in the definition of a value preserving transformation  For the nodes that were not added by the exhaustive backup  this is straightforward  For the new nodes  q of the controller C  we have for all s  S   V  s   q     X  P   a  q  R s   a      X  P  s     o s   a P   q     q   a   o V  s     q        o s     q    a       X  P   a  q  R s   a     X  P  s     o s   a P   q     q   a   o     o s     q    a  X  P   r     q    V  s     r        r       X  P   a  q  R s   a      X  P  s     o s   a P   q     q   a   o P   r     q    V  s     r        o s     q      r    a      X     X  P   r  q  R s   a  r       X  P  s     o s   a  r  V  s     r     r   o      o s     r  P   r  q V  s   r      r          Bernstein  Amato  Hansen    Zilberstein  
 Planning for distributed agents with partial state information is considered from a decision theoretic perspective  We describe generaliza tions of both the MDP and POMDP models that allow for decentralized control  For even a small number of agents  the finite horizon prob lems corresponding to both of our models are complete for nondeterministic exponential time  These complexity results illustrate a fundamen tal difference between centralized and decentral ized control of Markov processes  In contrast to the MDP and POMDP problems  the problems we consider provably do not admit polynomial time algorithms and most likely require doubly exponential time to solve in the worst case  We have thus provided mathematical evidence corre sponding to the intuition that decentralized plan ning problems cannot easily be reduced to cen tralized problems and solved exactly using estab lished techniques      Introduction  Among researchers in artificial intelligence  there has been growing interest in problems with multiple distributed agents working to achieve a common goal  Grosz   Kraus        Lesser        desJardins et al         Durfee        Stone   Veloso         In many of these problems  intera gent communication is costly or impossible  For instance  consider two robots cooperating to push a box  Mataric         Communication between the robots may take time that could otherwise be spent performing physical actions  Thus  it may be suboptimal for the robots to communi cate frequently  A planner is faced with the difficult task of deciding what each robot should do in between com munications  when it only has access to its own sensory information  Other problems of planning for distributed agents with limited communication include maximizing the  throughput of a multiple access broadcast channel  Ooi   Womell        and coordinating multiple spacecraft on a mission together  Estlin et al          We are interested in the question of whether these planning problems are computationally harder to solve than problems that involve planning for a single agent or multiple agents with access to the exact same information  We focus on centralized planning for distributed agents  with the Markov decision process  MDP  framework as the basis for our model of agents interacting with an envi ronment  A partially observable Markov decision process  POMDP  is a generalization of an MDP in which an agent must base its decisions on incomplete information about the state of the environment  White         We extend the POMDP model to allow for multiple distributed agents to each receive local observations and base their decisions on these observations  The state transitions and expected rewards depend on the actions of all of the agents  We call this a decentralized partially observable Markov de cision process  DEC POMDP   An interesting special case of a DEC POMDP satisfies the assumption that at any time step the state is uniquely determined from the current set of observations of the agents  This is denoted a decen tralized Markov decision process  DEC MDP   The MDP  POMDP  and DEC MDP can all be viewed as special cases of the DEC POMDP  The relationships among the models are shown in Figure    There has been some related work in AI  Boutilier        studies multi agent Markov decision processes  MMDPs   but in this model  the agents all have access to the same in formation  In the framework we describe  this assumption is not made  Peshkin et al         use essentially the DEC POMDP model  although they refer to it as a partially ob servable identical payoff stochastic game  POIPSG   and discuss algorithms for obtaining approximate solutions to the corresponding optimization problem  The models that we study also exist in the control theory literature  Ooi et al         Aicardi et al          However  the compu tational complexity inherent in these models has not been studied  One closely related piece of work is that of Tsit        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       agent given that it chose action a in state s  There are several different ways to define  long term re ward  and thus several different measures of optimality  In this paper  we focus on finite horizon optimality  for which the aim is to maximize the expected sum of rewards re ceived over T time steps  Formally  the agent should maximize Figure    The relationships among the models   siklis and Athans         in which the complexity of non sequential decentralized decision problems is studied  We discuss the computational complexity of finding opti mal policies for the finite horizon versions of these prob lems  It is known that solving an MDP is P complete and that solving a POMDP is PSPACE complete  Papadim itriou   Tsitsiklis         We show that solving a DEC POMDP with a constant number  m         of agents is com plete for the complexity class nondeterministic exponen tial time  NEXP   Furthermore  solving a DEC MDP with a constant number  m         of agents is NEXP complete  This has a few consequences  One is that these problems provably do not admit polynomial time algorithms  This trait is not shared by the MDP problems nor the POMDP problems  Another consequence is that any algorithm for solving either problem will most likely take doubly expo nential time in the worst case  In contrast  the exact al gorithms for finite horizon POMDPs take  only  exponen tial time in the worst case  Thus  our results shed light on the fundamental differences between centralized and de centralized control of Markov decision processes  We now have mathematical evidence corresponding to the intuition that decentralized planning problems are more difficult to solve than their centralized counterparts  These results can steer researchers away from trying to find easy reductions from the decentralized problems to centralized ones and to ward completely different approaches  A precise categorization of the two agent DEC MDP prob lem presents an interesting mathematical challenge  The extent of our present knowledge is that the problem is PSPACE hard and is contained in NEXP      Centralized Models  A Markov decision process  MDP  models an agent acting in a stochastic environment to maximize its long term re ward  The type of MDP that we consider contains a finite setS of states  withs  ES as the start state  For each state s E S  As is a finite set of actions available to the agent  P is the table of transition probabilities  where P s Js  a  is the probability of a transition to state s  given that the agent performed action a in states  R is the reward func tion  where R s  a  is the expected reward received by the  where r st  at  is the reward received at time step t  A  policy    for a finite horizon MDP is a mapping from each states and timet to an action    s t   This is called a non stationary policy  The decision problem corresponding to a finite horizon MDP is as follows  Given an MDP M  a positive integer T  and an integer K  is there a policy that yields total reward at least K  An MDP can be generalized so that the agent does not nec essarily observe the exact state of the environment at each time step  This is called a partially observable Markov de cision process  POMDP   A POMDP has a state setS  a start stateso E S  a table of transition probabilities P  and a reward function R  just as an MDP does  Additionally  it contains a finite set n of observations  and a table   of ob servation probabilities  where O oJa s   is the probability that o is observed  given that action a was taken and led to state s   For each observation o E     Ao is a finite set of actions available to the agent  A policy    is now a mapping from histories of observations o        Ot to actions in Ao      The decision problem for a POMDP is stated in exactly the same way as for an MDP      Decentralized Models  A decentralized partially observable Markov decision pro cess  DEC POMDP  is a generalization of a POMDP to allow for distributed control by m agents that may not be able to observe the exact state  A DEC POMDP contains a finite set S of states  with so E S as the start state  The transition probabilities P s  Is  a        am  and expected rewards R s  a        am  depend on the ac tions of all agents  ni is a finite set of observations for agent i  and   is a table of observation probabilities  where O o        omJa        am s   is the probability that o        om are observed by agents            m respectively  given that the action tuple  a        am  was taken and led to state s   Each agent i has a set of actions A for each observation oi E Oi  Notice that this model reduces to a POMDP in the one agent case  For each a        am  s   let w a     am s   denote the set of observation tuples that have a nonzero chance of occurring given that the action tuple  a        am  was taken and led to state s   To form a decentralized Markov decision process  DEC MDP   we add the requirement           UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       that for each a       am s   and each  o          om  E w a       am s   the state is uniquely determined by  o      om   In the one agent case  this model is essentially the same as an MDP  We define a local policy  oi  to be a mapping from local histories of observations of        o to actions ai E A   A joint policy  o           om   is defined to be a tu ple of local policies  We wish to find a joint policy that maximizes the total expected return over the finite hori zon  The decision problem is stated as follows  Given a DEC POMDP M  a positive integer T  and an integer K  is there a joint policy that yields total reward at least K  Let DEC POMDP and DEC MDPm denote the deci m sion problems for them agent DEC POMDP and them agent DEC MDP  respectively      Complexity Results  It is necessary to consider only problems for which T   lSI  If we place no restrictions on T  then the upper bounds do not necessarily hold  Also  we assume that each of the elements of the tables for the transition prob abilities and expected rewards can be represented with a constant number of bits  With these restrictions  it was shown in  Papadimitriou   Tsitsiklis        that the de cision problem for an MDP is P complete  In the same paper  the authors showed that the decision problem for a POMDP is PSPACE complete and thus probably does not admit a polynomial time algorithm  We prove that for all m       DEC POMDP is NEXP complete  and for all m m       DEC MDP is NEXP complete  where NEXP   m c NTIME    n    Papadimitriou         Since P   J  NEXP  we can be certain that there does not exist a polynomial time algorithm for either problem  Moreover  there probably is not even an exponential time algorithm that solves either problem  For our reduction  we use a problem called TILING  Pa padimitriou         which is described as follows  We are given a set of square tile types T    to        tk    to gether with two relations H  V  T x T  the horizontal and vertical compatibility relations  respectively   We are also given an integer n in binary  A tiling is a function f   O         n      x  O         n        t T  A tiling f is consistent if and only if  a  f O      to  and  b  for all i j  f i j  f i   j   E H  and  f i j  f i j     E V  The decision problem is to tell  given T  H  V  and n   whether a consistent tiling exists  It is known that TILING is NEXP complete  Theorem    complete   For all m        DEC POMDP  m  is  NEXP  Proof  First  we will show that the problem is in NEXP  We can guess a joint policy o and write it down in exponential  time  This is because a joint policy consists of m map pings from local histories to actions  and since T   lSI  all histories have length less than lSI  A DEC POMDP together with a joint policy can be viewed as a POMDP to gether with a policy  where the observations in the POMDP correspond to the observation tuples in the DEC POMDP  In exponential time  each of the exponentially many possi ble sequences of observations can be converted into belief states  The transition probabilities and expected rewards for the corresponding  belief MDP  can be computed in exponential time  Kaelbling et al          It is possible to use dynamic programming to determine whether the policy yields expected reward at least K in this belief MDP  This takes at most exponential time  Now we show that the problem is NEXP hard  For sim plicity  we consider only the two agent case  Clearly  the problem with more agents can be no easier  We are given an arbitrary instance of TILING  From it  we construct a DEC POMDP such that the existence of a joint policy that yields a reward of at least zero is equivalent to the existence of a consistent tiling in the original problem  Furthermore  T   lSI in the DEC POMDP that is constructed  Intu itively  a local policy in our DEC POMDP corresponds to a mapping from tile positions to tile types  i e   a tiling  and thus a joint policy corresponds to a pair of tilings  The pro cess works as follows  In the position choice phase  two tile positions are randomly  chosen  by the environment  Then  at the tile choice step  each agent sees a different position and must use its policy to determine a tile to be placed in that position  Based on information about where the two positions are in relation to each other  the environ ment checks whether the tile types placed in the two posi tions could be part of one consistent tiling  Only if the nec essary conditions hold do the agents obtain a nonnegative reward  It turns out that the agents can obtain a nonnega tive expected reward if and only if the conditions hold for all pairs of positions the environment can choose  i e   there exists a consistent tiling  We now present the construction in detail  During the posi tion choice phase  each agent only has one action available to it  and a reward of zero is obtained at each step  The states and the transition probability matrix comprise the nontrivial aspect of this phase  Recall that this phase intu itively represents the choosing of two tile positions  First  let the two tile positions be denoted  i  jt  and   i   jz   where       i   i  j  jz     n      There are  log n steps in this phase  and each step is devoted to the choosing of one bit of one of the numbers   We assume that n is a power of two  It is straightforward to modify the proof to deal with the more general case   The order in which the bits are chosen is important  and it is as follows  The bits of i  and i  are chosen from least significant up to most sig nificant  alternating between the two numbers at each step  Then j  and jz are chosen in the same way  As the bits of   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS           the numbers are being determined  information about the relationships between the numbers is being recorded in the state  How we express all of this as a Markov process is explained below      Vertically Adjacent Tile Positions This component is used to check whether the first tile posi tion is directly above the second one  Its regular expression is as follows   Each state has six components  and each component rep resents a necessary piece of information about the two tile positions being chosen  We describe how each of the com ponents changes with time  A time step in our process can be viewed as having two parts  which we refer to as the stochastic part and the deterministic part  During the stochastic part  the environment  flips a coin  to choose either the number   or the number    each with equal prob ability  After this choice is made  the change in each com ponent of the state can be described by a deterministic finite automaton that takes as input a string of O s and    s  the en vironment s coin flips   The semantics of the components  along with their associated automata  are described below                                                  Bit Chosen in the Last Step  This component of the state says whether   or   was just chosen by the environment  The corresponding automaton consists of only two states     Number of Bits Chosen So Far This component simply counts up to  logn  in order to determine when the position choice phase should end  Its automaton consists of  logn     states      Equal Tile Positions After the  logn steps  this component tells us whether the two tile positions chosen are equal or not  For this automa ton  along with the following three  we need to have a no tion of an accept state  Consider the following regular ex pression              Note that the DFA corresponding to the above expression  on an input of length  logn  ends in an accept state if and only if  i  i      iz jz      Upper Left Tile Position  This component is used to check whether the first tile posi tion is the upper left comer of the grid  Its regular expres sion is as follows               The corresponding DFA  on an input of length  logn  ends in an accept state if and only if   i  j               Horizontally Adjacent Tile Positions This component is used to check whether the first tile po sition is directly to the left of the second one  Its regular expression is as follows                                                 logn  The corresponding DFA  on an input of length  logn  ends in an accept state if and only if i  j          iz jz   So far we have described the six automata that determine how each of the six components of the state evolve based on input    or    from the environment  We can take the cross product of these six automata to get a new automaton that is only polynomially bigger and describes how the entire state evolves based on the sequence of O s and    s chosen by the environment  This automaton  along with the en vironment s  coin flips   corresponds to a Markov process  The number of states of the process is polylogarithmic inn  and hence polynomial in the size of the TILING instance  The start state s  is a tuple of the start states of the six au tomata  The table of transition probabilities for this process can be constructed in time polylogarithmic in n  We have described the states  actions  state transitions  and rewards for the position choice phase  and we now describe the observation function  In this DEC POMDP  the obser vations are uniquely determined from the state  For the states after which a bit of i  or i  has been chosen  agent one observes the first component of the state  while agent two observes a dummy observation  The reverse is true for the states after which a bit of i  or jz has been chosen  Intu itively  agent one  sees  only i   jl   and agent two  sees  only   iz  z    When the second component of the state reaches its limit  the tile positions have been chosen  and the last four com ponents of the state contain information about the tile po sitions and how they are related  Of course  the exact tile positions are not recorded in the state  as this would require exponentially many states  This marks the end of the posi tion choice phase  In the next step  which we call the tile choice step  each agent has k     actions available to it  corresponding to each of the tile types  to        tk  We de note agent one s choice t  and agent two s choice t   No matter which actions are chosen  the state transitions de terministically to some final state  The reward function for this step is the nontrivial part  After the actions are chosen  the following statements are checked for validity    i    j     then t    t     If i  jl         then t    t     If il     jl     iz jz    then  t    t      If i  i        iz jz    then  tl   t      If h jl            logn  The corresponding DFA  on an input of length  logn  ends in an accept state if and only if   i       j       iz jz     E E  H  V   If all of these are true  then a reward of   is received  Oth erwise  a reward of    is received  This reward function can be computed from the TILING instance in polynomial   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS           time  To complete the construction  the horizon T is set to  log n  exactly the number of steps it takes the process to reach the tile choice step  and fewer than the number of states lSI   Now we argue that the expected reward is zero if and only if there exists a consistent tiling  First  suppose a consis tent tiling exists  This tiling corresponds to a local policy for an agent  If each of the two agents follows this policy  then no matter which two positions are chosen by the en vironment  the agents choose tile types for those positions so that the conditions checked at the end evaluate to true  Thus  no matter what sequence of O s and   s the environ ment chooses  the agents receive a reward of zero  Hence  the expected reward for the agents is zero  For the converse  suppose the expected reward is zero  Then the reward is zero no matter what sequence of O s and   s the environment chooses  i e   no matter which two tile positions are chosen  This implies that the four condi tions mentioned above are satisfied for any two tile posi tions that are chosen  The first condition ensures that for all pairs of tile positions  if the positions are equal  then the tile types chosen are the same  This implies that the two agents  tilings are exactly the same  The last three condi tions ensure that this tiling is consistent    Theorem    For all m         DEC MDPm is NEXP  complete  Proof   Sketch  Inclusion in NEXP follows from the fact  that a DEC MDP is a special case of a DEC POMDP  For NEXP hardness  we can reduce a DEC POMDP with two agents to a DEC MDP with three agents  We simply add a third agent to the DEC POMDP and impose the following requirement  The state is uniquely determined by just the third agent s observation  but the third agent always has just one action and cannot affect the state transitions or rewards received  It is clear that the new problem qualifies as a DEC MDP and is essentially the same as the original DEC POMDP    The reduction described above can also be used to con struct a two agent DEC MDP from a POMDP and hence show that DEC MDP  is PSPACE hard  However  this technique is not powerful enough to prove the NEXP hardness of the problem  In fact  the question of whether DEC MDP  is NEXP hard remains open  Note that in the reduction in the proof of Theorem    the observa tion function is such that there are some parts of the state that are hidden from both agents  This needs to some how be avoided in order to reduce to a two agent DEC MDP  A simpler task may actually be to derive a better up per bound for the problem  For example  it may be pos sible that DEC MDP  E co NEXP  where co NEXP    LIL E NEXP     Regardless of the outcome  the problem provides an interesting mathematical challenge      Discussion  Using the tools of worst case complexity analysis  we analyzed two models of decision theoretic planning for distributed agents  Specifically  we proved that the finite horizon m agent DEC POMDP problem is NEXP complete form      and the finite horizonm agent DEC MDP problem is NEXP complete form       The results have some theoretical implications  First  un like the MDP and POMDP problems  the problems we studied provably do not admit polynomial time algorithms  since P   NEXP  Second  we have drawn a connection be tween work on Markov decision processes and the body of work in complexity theory that deals with the exponen tial jump in complexity due to decentralization  Peterson   Reif        Babai et al          Finally  the two agent DEC MDP case yields an interesting open problem  The solution of the problem may imply that the difference be tween planning for two agents and planning for more than two agents is a significant one in the case where the state is collectively observed by the agents  There are also more direct implications for researchers try ing to solve problems of planning for distributed agents  Consider the growing body of work on algorithms for ob taining exact or approximate solutions for POMDPs  e g   Jaakkola et al         Cassandra et al         Hansen         It would have been beneficial to discover that a DEC POMDP or DEC MDP is just a POMDP  in dis guise   in the sense that it can easily be converted to a POMDP and solved using established techniques  We have provided evidence to the contrary  however  The complex ity results do not answer all of the questions surrounding how these problems should be attacked  but they do sug gest that the fundamentally different structure of the de centralized problems may require fundamentally different algorithmic ideas  Finally  consider the infinite horizon versions of the afore mentioned problems  It has recently been shown that the infinite horizon POMDP problem is undecidable  Madani et al         under several different optimality criteria  Since a POMDP is a special case of a DEC POMDP  the corresponding DEC POMDP problems are also undecid able  In addition  because it is possible to reduce a POMDP to a two agent DEC MDP  the DEC MDP problems are also undecidable  Acknowledgments  The authors thank Micah Adler  Andy Barto  Dexter Kozen  Victor Lesser  Frank McSherry  Ted Perkins  and Ping Xuan for helpful discussions  This work was sup ported in part by the National Science Foundation under grants IRI          IRI          and CCR         and an NSF Graduate Fellowship to Daniel Bernstein        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Proceedings of the Sixteenth National Conference on Arti  
  eas include distributed robot control  networking and e commerce   We present a memory bounded optimization approach for solving infinite horizon decentralized POMDPs  Policies for each agent are represented by stochastic finite state controllers  We formulate the problem of optimizing these policies as a nonlinear program  leveraging powerful existing nonlinear optimization techniques for solving the problem  While existing solvers only guarantee locally optimal solutions  we show that our formulation produces higher quality controllers than the state of the art approach  We also incorporate a shared source of randomness in the form of a correlation device to further increase solution quality with only a limited increase in space and time  Our experimental results show that nonlinear optimization can be used to provide high quality  concise solutions to decentralized decision problems under uncertainty   Although there has been some recent work on exact and approximate algorithms for DEC POMDPs  Nair et al         Bernstein et al         Hansen et al         Szer et al         Szer and Charpillet        Seuken and Zilberstein         only two algorithms  Bernstein et al         Szer and Charpillet        are able to find solutions for the infinite horizon case  Such domains as networking and robot control problems  where the agents are in continuous use are more appropriately modeled as infinite horizon problems  Exact algorithms require an intractable amount of space for all but the smallest problems  This may occur even if an optimal or near optimal solution is concise  DECPOMDP approximation algorithms can operate with a limited amount of memory  but as a consequence may provide poor results   Introduction  Markov decision processes  MDPs  have been widely used to study single agent sequential decision making with full observability  Partially observable Markov decision processes  POMDPs  have had success modeling the more general situation in which the agent has only partial information about the state of the system  The decentralized partially observable Markov decision processes  DEC POMDP  is an even more general framework which extends the POMDP model to mutiagent settings  In a DEC POMDP each agent must make decisions based on uncertainty about the other agents as well as imperfect information of the system state  The agents seek to maximize a shared total reward using solely local information in order to act  Some examples of DEC POMDP application ar   In this paper  we propose a new approach that addresses the space requirement of DEC POMDP algorithms while maintaining a principled method based on the optimal solution  This approach formulates the optimal memory bounded solution for the DECPOMDP as a nonlinear program  NLP   thus allowing a wide range of powerful nonlinear optimization techniques to be applied  This is done by optimizing the parameters of fixed size independent controllers for each agent  which when combined  produce the policy for the DEC POMDP  While no existing NLP solver guarantees finding an optimal solution  our new formulation facilitates a more efficient search of the solution space and produces high quality controllers of a given size  We also discuss the benefits of adding a shared source of randomness to increase the solution quality of our memory bounded approach  This allows a set of independent controllers to be correlated in order to produce higher values  without sharing any local information  Correlation adds another mechanism in efforts to gain the most possible value with a fixed amount of      AMATO ET AL   space  This has been shown to be useful in order to increase value of fixed size controllers  Bernstein et al         and we show that is also useful when combined with our NLP approach  The rest of the paper is organized as follows  We first present some background on the DEC POMDP model and the finite state controller representation of their solution  We then describe the current infinite horizon algorithms and describe some of their flaws  As an alternative  we present a nonlinear program that represents the optimal fixed size solution  We also incorporate correlation into the NLP method and discuss its benefits  Lastly  experimental results are provided comparing the nonlinear optimization methods with and without correlation and the current state of theart DEC POMDP approximation algorithm  This is done by using an off the shelf  locally optimal nonlinear optimization method to solve the NLPs  but more sophisticated methods are also possible  For a range of domains and controller sizes  higher valued controllers are found with the NLP and using correlation further increases solution quality  This suggests that high quality  concise controllers can be found in many diverse DEC POMDP domains      DEC POMDP model and solutions  We first review the decentralized partially observable Markov decision process  DEC POMDP  model  For clarity  we present the model for two agents as it is straightforward to extend it to n agents  A two agent DEC POMDP can be defined with the tuple  M   hS  A    A    P  R          O  T i  S  the finite set of states  A  and A    the finite sets of actions for each agent  P   the set of state transition probabilities  P  s   s  a    a     the probability of transitioning from state s to s  when actions a  and a  are taken by agents   and   respectively  R  the reward function  R s  a    a     the immediate reward for being in state s and agent   taking action a  and agent   taking action a     and     the finite sets of observations for each agent  O  the set of observation probabilities  O o    o   s    a    a     the probability of agents   and   seeing observations o  and o  respectively given agent   has taken action a  and agent   has taken action a  and this results in state s  Since we are considering the infinite horizon DECPOMDP  the decision making process unfolds over an infinite sequence of stages  At each step  every agent  chooses an action based on their local observation histories  resulting in an immediate reward and an observation for each agent  Note that because the state is not directly observed  it may be beneficial for the agent to remember the observation history  A local policy for an agent is a mapping from local observation histories to actions while a joint policy is a set of policies  one for each agent in the problem  The goal is to maximize the infinite horizon total cumulative reward  beginning at some initial distribution over states called a belief state  In order to maintain a finite sum over the infinite horizon  we employ a discount factor           As a way to model DEC POMDP policies with finite memory  finite state controllers provide an appealing solution  Each agents policy can be represented as a local controller and the resulting set of controllers supply the joint policy  called the joint controller  Each finite state controller can formally be defined by the tuple hQ    i  where Q is the finite set of controller nodes     Q  A is the action selection model for each node  and    Q  A  O  Q represents the node transition model for each node given an action was taken and an observation seen  For n agents  the value for starting in agent  s nodes  q and at state s is given by   n XY X P  s    a  s  V   q  s    P  ai  qi   R s   a       i s   a   n X XY         O  o s    a  P  qi  qi   ai   oi  V  q   s     o  q    i  This is also referred to as the Bellman equation  Note that the values can be calculated offline in order to determine controllers for each agent that can then be executed online for distributed control      Previous work  As mentioned above  the only other algorithms that we know of that can solve infinite horizon DECPOMDPs are those of Bernstein et al         and Szer and Charpillet         Bernstein et al s approach  called bounded policy iteration for decentralized POMDPs  DEC BPI   is an approximate algorithm that also uses stochastic finite state controllers  Szer and Charpillets approach is also an approximate algorithm  but it uses deterministic controllers  DEC BPI improves a set of fixed size controllers by using linear programming  This is done by iterating through the nodes of each agents controller and attempting to find an improvement  A linear program searches for a probability distribution over actions and transitions into the agents current controller that increases the value of the controller for any initial state   AMATO ET AL  For variables  x  q   a   y  q   a   o  q     and z  q  s  Maximize X     b   s z q     s   s  Given the Bellman constraints   q  s z  q  s     X      X X X               y  q   a   o  q  z q   s   O  o s    a  P  s  s   a  x  q   a  R s   a     s     a    o  q    For each agent i and set of agents  i  apart from i  Independence constraints  ai    q  X  X  x  q   a     ai   a   q   o  qi   X  y  q   a   o  q          qi  f    a  x qi   qi  ai  X  f y qi   qi   ai   afi   oi   ofi   q        qi  And probability constraints  X X f f qi x qi   qi    a      and qi   oi   ai y qi   qi   ai   aci   oi   ofi   q           a  q     q   a x  q   a     and  q   o   a y  q   a   o  q        Table    The nonlinear program representing the optimal fixed size controller  Variable x  q   a  represents P   a  q   variable y  q   a   o  q     represents P  q     q   a   o   variable z  q  s  represents V   q  s   q   represents the initial controller f node for each agent  Superscripted f s such as qi represent arbitrary fixed values  and any initial node of the other agents controllers  If the improvement is discovered  the node is updated based on the probability distributions found  Each node for each agent is examined in turn and the algorithm terminates when no agent can make any further improvements  This algorithm allows memory to remain fixed  but provides only a locally optimal solution  This is due to the linear program considering the old controller values from the second step on and the fact that improvement must be over all possible states and initial nodes for the controllers of the other agents  As the number of agents or size of controllers grows  this later drawback is likely to severely hinder improvement  Szer and Charpillet have developed a best first search algorithm that finds deterministic finite state controllers of a fixed size  This is done by calculating a heuristic for the controller given the known deterministic parameters and filling in the remaining parameters one at a time in a best first fashion  They prove that this technique will find the optimal deterministic finite state controller of a given size  but its use remains limited  This approach is very time and memory intensive and is restricted to deterministic controllers      Nonlinear optimization approach  Due to the high space complexity of finding an optimal solution for a DEC POMDP  fixed size solutions are very appealing  Fixing memory balances optimality and computational concerns and should allow high quality solutions to be found for many problems  Using Bernstein et al s DEC BPI method reduces problem complexity by fixing controller size  but solution quality is limited by a linear program that requires improvement across all states and initial nodes of the other agents  Also  each agents controller is improved separately without consideration for the knowledge of the initial problem state  thus reducing solution quality  Both of these limitations can be eliminated by modeling a set of optimal controllers as a nonlinear program  By setting the value as a variable and using constraints to maintain validity  the parameters can be updated in order to represent the globally optimal solution over the infinite horizon of the problem  Rather than the the iterative process of DEC BPI  the NLP improves and evaluates the controllers of all agents at once for a given initial state in order to make the best possible use of the controller size  Compared with other DEC POMDP algorithms  the NLP approach makes more efficient use of memory      AMATO ET AL   than the exact methods  and using locally optimal NLP algorithms provides an approximation technique with a search based on the optimal solution of the problem  Rather than adding nodes and then attempting to remove those that will not improve the controller  as a dynamic programming approach might do  we search for the best controllers of a fixed size  The NLP is also able to take advantage of the start distribution  thus making better use of its size  The NLP approach has already shown promise in the POMDP case  In a previous paper  Amato et al          we have modeled the optimal fixed size controller for a given POMDP as an NLP and with locally optimal solution techniques produced consistently higher quality controllers than a current stateof the art method  The success of the NLP in the single agent case suggested that an extension to DECPOMDPs could also be successful  To construct this NLP  extra constraints are needed to guarantee independent controllers for each agent  while still maximizing the value       Nonlinear problem model  The nonlinear program seeks to optimize the value of fixed size controllers given a initial state distribution and the DEC POMDP model  The parameters of this problem in vector notation are the joint action selection probabilities at each node of the controllers P   a  q   the joint node transition probabilities P  q     q   a   o  and the values of each node in each state  V   q  s   This approach differs from Bernstein et al s approach in that it explicitly represents the node values as variables  To ensure that the values are correct given the action and node transition probabilities  nonlinear constraints must be added to the optimization  These constraints are the Bellman equations given the policy determined by the action and transition probabilities  Constraints are also added to ensure distributed action selection and node transitions for each agent  We must also ensure that all probabilities are valid numbers between   and    Table   describes the nonlinear program that defines the optimal controller for an arbitrary number of agents  The value of designated initial local nodes is maximized given the initial state distribution and the necessary constraints  The independence constraints guarantee that action selection and transition probabilities can be summed out for each agent by ensuring that they do not depend on any information that is not local  Theorem   An optimal solution of the NLP results in optimal stochastic controllers for the given size and initial state distribution   Proof sketch  The optimality of the controllers follows from the NLP constraints and maximization of given initial nodes at the initial state distribution  The Bellman equation constraints restrict the value variables to valid amounts based on the chosen probabilities  the independence constraints guarantee distributed control and the maximum value is found for the initial nodes and state  Hence  this represents optimal controllers       Nonlinear solution techniques  There are many efficient algorithms for solving large NLPs  When the problem is non convex  as in our case  there are multiple local maxima and no NLP solver guarantees finding the optimal solution  Nevertheless  existing techniques proved useful in finding high quality results for large problems  For this paper  we used a freely available nonlinearly constrained optimization solver called filter  Fletcher et al         on the NEOS server  http   wwwneos mcs anl gov neos    Filter finds solutions by a method of successive approximations called sequential quadratic programming  SQP   SQP uses quadratic approximations which are then more efficiently solved with quadratic programming  QP  until a solution to the more general problem is found  A QP is typically easier to solve  but must have a quadratic objective function and linear constraints  Filter adds a filter which tests the current objective and constraint violations against those of previous steps in order to promote convergence and avoid certain locally optimal solutions  The DEC POMDP and nonlinear optimization models were described using a standard optimization language AMPL      Incorporating correlation  Bernstein et al  also allow each agents controller to be correlated by using a shared source of randomness in the form of a correlation device  As an example of one such device  imagine that before each action is taken  a coin is flipped and both agents have access to the outcome  Each agent can then use that new information to affect their choice of action  Along with stochasticity  correlation is another means of increasing value when memory is limited  A correlation device provides extra signals to the agents and operates independently of the DECPOMDP  That is  the correlation device is a tuple hC  i  where C is a set of states and    C  C is a stochastic transition function that we will represent as P  c   c   At each step of the problem  the device transitions and each agent can observe its state    AMATO ET AL      For variables  w c  c     x  q   a  c   y  q   a   o  q     c  and z  q  s  c  Maximize X b   s z q     s  s  Given the Bellman constraints      X X X X X                 w c  c  z q   s   c  y  q   a   o  q   c  O  o s    a  P  s  s   a  x  q   a  c  R s   a      q  s z  q  s  c      a  s     o  q    c   Table    The nonlinear program representing the optimal fixed size controller including a correlation device  Variable x  q   a  c  represents P   a  q  c   variable y  q   a   o  q     c  represents P  q     q   a   o  c   variable z  q  s  c  represents V   q  s  c   q   represents the initial controller node for each agent and w c  c    represents P  c   c   The other constraints are similar to those above with the addition of a sum to one constraint for the correlation device  size DEC BPI DEC BPI corr NLO NLO corr The independent local controllers defined above can be modified to make use of the correlation device  This                       is done by making the parameters dependent on the                       signal from the correlation device  For agent i  ac                      tion selection is then P  ai  qi   c  and node transition is                       P  qi   qi   ai   oi   c   For n agents  the value of the correTable    Broadcast problem values using NLP methlated joint controller beginning in nodes  q  state s and ods and DEC BPI with and without a   node correlacorrelation device state c is defined as V   q  s  c      n tion device X X XY P  s    a  s  O  o s     a  P  ai  qi   c  R s   a     i s    o   a size DEC BPI DEC BPI corr NLO NLO corr   n XY X        s    s  s  s P  qi  qi   ai   oi   c  P  c  c V  q     s    c           s  s  s  s i c   q     s  s    s     s Our NLP can be extended to include a correlation    s   s     s      s device  This optimization problem  the first part of which is shown in Table    is very similar to the preTable    Broadcast problem mean optimization times vious NLP  A new variable is added for the transiusing NLP methods and DEC BPI with and without tion function of the correlation device and the other a   node correlation device variables now include the signal from the device  The Bellman equation incorporates the new correlation device signal at each step  but the other constraints reEach NLP and DEC BPI algorithm was run until conmain the same  A new probability constraint is also vergence was achieved with ten different random deadded to ensure that the transition probabilities for terministic initial controllers  and the mean values and each state of the correlation device sum to one  times are reported  The times reported for each NLP     Experimental results  We tested our nonlinear programming approach in three DEC POMDP domains  In each experiment  we compare Bernstein et al s DEC BPI with NLP solutions using filter for a range of controller sizes  We also implemented each of these approaches with a correlation device of size two  We do not compare with Szer and Charpillets algorithm because the problems presented in that work are slightly different than those used by Bernstein et al  Nevertheless  on the problems that we tested  our approach can and does achieve higher values than Szer and Charpillets algorithm for all of the controller sizes for which that the best first search is able to find a solution   method can only be considered estimates due to running each algorithm on external machines with uncontrollable load levels  but we expect that they vary by only a small constant  Note that our goal in these experiments is to demonstrate the benefits of our formulation when used in conjunction with an off the shelf solver such as filter  The formulation is very general and many other solvers may be applied  Throughout this section we will refer to our nonlinear optimization as NLO and the optimization with the correlation device with two states as NLO corr       Broadcast problem  A DEC POMDP used by Bernstein et al  was a simplified two agent networking example  This problem      AMATO ET AL   Figure    Recycling robots values using NLP methods and DEC BPI with and without a   node correlation device has   states    actions and   observations  At each time step  each agent must choose whether or not to send a message  If both agents send  there is a collision and neither gets through  A reward of   is given for every step a message is successfully sent over the channel and all other actions receive no reward  Agent   has a     probability of having a message in its queue on each step and agent   has only a     probability  The domain is initialized with only agent   possessing a message and a discount factor of     was used  Table   shows the values produced by DEC BPI and our nonlinear programming approach with and without a correlation device for several controller sizes  Both nonlinear techniques produce the same value      for each controller size  In all cases this is a higher value than that produced by Bernstein et al s independent and correlated approaches  As     is the maximum value that any approach that we tested receives for the given controller sizes  it is likely that it is optimal for these sizes  The time used by each algorithm is shown in Table    As expected  the nonlinear optimization methods require more time to find a solution than the DECBPI methods  As noted above  solution quality is also higher using nonlinear optimization  Either NLP approach can produce a higher valued one node controller in an amount of time similar to or less than each DECBPI method  Therefore  for this problem  the NLP methods are able to find higher valued  more concise solutions given a fixed amount of space or time       Recycling robots  As another comparison  we have extended the Recycling Robot problem  Sutton and Barto        to the multiagent case  The robots have the task of picking up cans in an office building  They have sensors to  Figure    Recycling robots graphs for value vs time for the NLP and DEC BPI methods with and without the correlation device  find a can and motors to move around the office in order to look for cans  The robots are able to control a gripper arm to grasp each can and then place it in an on board receptacle  Each robot has three high level actions      search for a small can      search for a large can or     recharge the battery  In our two agent version  the larger can is only retrievable if both robots pick it up at the same time  Each agent can decide to independently search for a small can or to attempt to cooperate in order to receive a larger reward  If only one agent chooses to retreive the large can  no reward is given  For each agent that picks up a small can  a reward   is given and if both agents cooperate to pick the large can  a reward of   is given  The robots have the same battery states of high and low  with an increased likelihood of transitioning to a low state or exhausting the battery after attempting to pick up the large can  Each robots battery power depends only on its own actions and each agent can fully observe its own level  but not that of the other agent  If the robot exhausts the battery  it is picked up and plugged into the charger and then continues to act on the next step with a high battery level  The two robot version used in this paper has   states    actions and   observations  A discount factor of     was used  We can see in Figure   that in this domain higher quality controllers are produced by using nonlinear optimization  Both NLP methods permit higher mean values than either DEC BPI approach for all controller sizes  Also  correlation is helpful for both the NLP and DEC BPI approaches  but becomes less so for larger controller sizes  For the nonlinear optimization cases  both approaches converge to within a small amount of the maximum value that was found for any controller size tested  As controller size grows  the NLP methods are able to reliably find this solution and correlation is no longer useful    AMATO ET AL      The running times of each algorithm follow the same trend as above in which the nonlinear optimization approaches required much more time as controller size increases  The ability for the NLP techniques to produce smaller  higher valued controllers with similar or lesser running time also follows the same trend  Figure   shows the values that can be attained for each method based on the mean time necessary for convergence  Results are included for NLP techniques up to four nodes with the correlation device and five nodes without it while DEC BPI values are given for fourteen nodes with the correlation device and eighteen without it  This graph demonstrates that even if we allow controller size to continue to grow and examine only the amount of time that is necessary to achieve a solution  the NLP methods continue to provide higher values  Although the values of the controllers produced by the DEC BPI methods are somewhat close to those of the NLP techniques as controller size grows  our approaches produce that value with a fraction of the controller size       Figure    Multiagent Tiger problem values using NLP methods and DEC BPI with and without a   node correlation device   Multiagent tiger problem  Another domain with   states    actions and   observations called the multiagent tiger problem was introduced by Nair et al   Nair et al          In this problem  there are two doors  Behind one door is a tiger and behind the other is a large treasure  Each agent may open one of the doors or listen  If either agent opens the door with the tiger behind it  a large penalty is given  If the door with the treasure behind it is opened and the tiger door is not  a reward is given  If both agents choose the same action  i e   both opening the same door  a larger positive reward or a smaller penalty is given to reward this cooperation  If an agent listens  a small penalty is given and an observation is seen that is a noisy indication of which door the tiger is behind  While listening does not change the location of the tiger  opening a door causes the tiger to be placed behind one of the door with equal probability  A discount factor of     was used  Figure   shows the values attained by each NLP and DEC BPI method for the given controller sizes  Figure   shows the values of just the two NLP methods  These graphs show that not only do the NLP methods significantly outperform the DEC BPI approaches  but correlation greatly increases the value attained by the nonlinear optimization  The individual results for this problem suggest the DEC BPI approach is more dependent on the initial controller and the large penalties in this problem result in several results that are very low  This outweighs the few times that more reasonable value is attained  Nevertheless  the max value attained by DEC BPI for all cases is still less than the  Figure    Multiagent Tiger problem values using just the NLP methods with and without a   node correlation device   Figure    Multiagent Tiger problem graphs for value vs  time for the NLP methods with and without the correlation device  mean value attained by the NLP methods  Again for this problem  more time is needed for the NLP approaches  but one node controllers are produced with higher value than any controller size for the DEC BPI      AMATO ET AL   methods and require very little time  The usefulness of the correlation device is illustrated in Figure    For given amounts of time  the nonlinear optimization that includes the correlation device produces much higher values  The DEC BPI methods are not included in this graph as they were unable to produce mean values greater than     for any controller size up to    for which mean time to convergence was over      seconds  This shows the importance of correlation in this problem and the ability of our NLP technique to take advantage of it      Conclusion  We introduced a novel approach to solving decentralized POMDPs by using a nonlinear program formulation  This memory bounded stochastic controller formulation allows a wide range of powerful nonlinear programming algorithms to be applied to solve DECPOMDPs  The approach is easy to implement as it mostly involves reformulating the problem and feeding it into an NLP solver  We showed that by using an off the shelf locally optimal NLP solver  we were able to produce higher valued controllers than the current state of the art technique for an assortment of DEC POMDP problems  Our experiments also demonstrate that incorporating a correlation device as a shared source of randomness for the agents can further increase solution quality  While the time taken to find a solution to the NLP can be higher  the fact that higher values can be found with smaller controllers by using the NLP suggests adopting more powerful optimization techniques for smaller controllers can be more productive in a given amount of time  The combination of start state knowledge and more advanced optimization allows us to make efficient use of the limited space of the controllers  These results show that this method can allow compact optimal or near optimal controllers to be found for various DEC POMDPs  In the future  we plan to conduct a more exhaustive analysis of the NLP representation and explore more specialized algorithms that can be tailored for this optimization problem  While the performance we get using a standard nonlinear optimization algorithm is very good  specialized solvers might be able to further increase solution quality and scalability  We also plan to characterize the circumstances under which introducing a correlation device is cost effective  Acknowledgements An earlier version of this paper without improvements such as incorporating a correlation device was pre   sented at the AAMAS    Workshop on Multi Agent Sequential Decision Making in Uncertain Domains  This work was supported in part by the Air Force Office of Scientific Research  Grant No  FA               and by the National Science Foundation  Grant No            Any opinions  findings  conclusions or recommendations expressed in this manuscript are those of the authors and do not reflect the views of the US government  
 We present decentralized rollout sampling policy iteration  DecRSPI   a new algorithm for multi agent decision problems formalized as DEC POMDPs  DecRSPI is designed to improve scalability and tackle problems that lack an explicit model  The algorithm uses MonteCarlo methods to generate a sample of reachable belief states  Then it computes a joint policy for each belief state based on the rollout estimations  A new policy representation allows us to represent solutions compactly  The key benefits of the algorithm are its linear time complexity over the number of agents  its bounded memory usage and good solution quality  It can solve larger problems that are intractable for existing planning algorithms  Experimental results confirm the effectiveness and scalability of the approach      Introduction  Planing under uncertainty in multi agent settings is a challenging computational problem  particularly when agents with imperfect sensors and actuators  such as autonomous rovers or rescue robots  must reason about a large space of possible outcomes and choose a plan based on their incomplete knowledge  The partially observable Markov decision process  POMDP  has proved useful in modeling and analyzing this type of uncertainty in single agent domains  When multiple cooperative agents are present  each agent must also reason about the decisions of the other agents and how they may affect the environment  Since each agent can only obtain partial information about the environment and sharing all the local information among the agents is often impossible  each agent must act based solely on its local information  These problems can be modeled as decentralized POMDPs  DEC POMDPs       When a complete model of the domain is available  DECPOMDPs can be solved using a wide range of optimal or  Xiaoping Chen School of Computer Science University of Sci    Tech  of China Hefei  Anhui        China xpchen ustc edu cn  approximate algorithms  particularly MBDP      and its descendants             Unfortunately  these algorithms are quite limited in terms of the size of the problems they can tackle  This is not surprising given that finite horizon DECPOMDPs are NEXP complete      Intuitively  the main reason is that it is hard to define a compact belief state and compute a value function for DEC POMDPs  as is often done for POMDPs  The state and action spaces blow up exponentially with the number of agents  Besides  it is very difficult to search over the large policy space and find the best action for every possible situation  Another key challenge is modeling the dynamics of the entire domain  which may include complex physical systems  Existing DEC POMDP algorithms assume that a complete model of the domain is known  This assumption does not hold in some real world applications such as robot soccer  Incomplete domain knowledge is often addressed by reinforcement learning algorithms       However  most cooperative multi agent reinforcement learning algorithms assume that the system state is completely observable by all the agents      Learning cooperative policies for multiagent partially observable domains is extremely challenging due to the large space of possible policies given only the local view of each agent  In reinforcement learning  a class of useful techniques such as Monte Carlo methods allows agents to choose actions based on experience       These methods require no prior knowledge of the dynamics  as long as sample trajectories can be generated online or using a simulator of the environment  Although a model is required  it must only provide enough information to generate samples  not the complete probability distributions of all possible transitions that are required by planning algorithms  In many cases it is easy to generate samples by simulating the target environment  but obtaining distributions in explicit form may be much harder  In the robot soccer domain  for example  there exist many high fidelity simulation engines  It is also possible to put a central camera on top of the field and obtain samples by running the actual robots  This paper introduces the decentralized rollout sampling   policy iteration  DecRSPI  algorithm for finite horizon DEC POMDPs  Our objective is to compute a set of cooperative policies using Monte Carlo methods  without having an explicit representation of the dynamics of the underlying system  DecRSPI first samples a set of reachable belief states based on some heuristic policies  Then it computes a joint policy for each belief state based on the rollout estimations  Similar to dynamic programming approaches  policies are constructed from the last step backwards  A new policy representation is used to bound the amount of memory  To the best of our knowledge  this is the first rollout based learning algorithm for finite horizon DEC POMDPs  DecRSPI has linear time complexity over the number of agents and it can solve much larger problems compared to existing planning algorithms  We begin with some background on the DEC POMDP model and the policy structure we use  We then describe each component of the rollout sampling algorithm and analyze its properties  Finally  we examine the performance of DecRSPI on a set of test problems  and conclude with a summary of related work and the contributions      Decentralized POMDPs  Formally  a finite horizon DEC POMDP can be defined as a tuple hI  S   Ai     i    P  O  R  b    T i  where  I is a collection of agents  identified by i           m   and T is the time horizon of the problem   S is a finite state space and b  is the initial belief state  i e   a probability distribution over states    Ai is a discrete action space for agent i  We denote by  a   ha    a         am i a joint action where ai  Ai     iI Ai is the joint action space  and A  i is a discrete observation space for agent i  Similarly  o   ho    o         om i is a joint observation     iI i is the joint obserwhere oi  i and  vation space      S  is the state transition function and  P   S A P  s   s   a  denotes the probability of the next state s  when the agents take joint action  a in state s          is an observation function and  O   SA   O  o s    a  denotes the probability of observing  o after taking joint action  a with outcome state s       R is a reward function and R s   a  is  R   SA the immediate reward after agents take  a in state s  In a DEC POMDP  each agent i  I executes an action ai based on its policy at each time step t  Thus a joint action  a of all the agents is performed  followed by a state transition of the environment and an identical joint reward obtained by the team  Then agent i receives its private observation oi from the environment and updates its policy for the next execution cycle  The goal of each agent is to choose a policy  that maximizes thePaccumulated reward of the team over T the horizon T   i e  t   E R t  b     Generally  a policy qi is a mapping from agent is observation history to an action ai and a joint policy  q   hq    q         qm i is a vector of policies  one for each agent  The value of a fixed joint policy  q at state s can be computed recursively by the Bellman equation  X P  s   s   a O  o s     a V  s     q o   V  s   q    R s   a    s     o  where  a is the joint action specified by  q  and  q o is the joint sub policy of  q after observing  o  Given a state distribution b   s   the value of a joint policy  q can be computed by X V  b   q    b s V  s   q      sS  Note that in a DEC POMDP  each agent can only receive its own local observations when executing the policy  Therefore the policy must be completely decentralized  which means the policy of an agent must be guided by its own local observation history only  It is not clear how to maintain a sufficient statistic  such as a belief state in POMDPs  based only on the local partial information of each agent  Thus  most of the works on multi agent partially observable domains are policy based and learning in DEC POMDP settings is extremely challenging  While the policy execution is decentralized  planning or learning algorithms can operate offline and thus may be centralized           The policies for finite horizon DEC POMDPs are often represented as a set of local policy trees                     Each tree is defined recursively with an action at the root and a subtree for each observation  This continues until the horizon is reached at a leaf node  A dynamic programming  DP  algorithm was developed to build the policy trees optimally from the bottom up       In this algorithm  the policies of the next iteration are enumerated by an exhaustive backup of the current trees  That is  for each action and each resulting observation  a branch to any of the current trees is considered  Unfortunately  the number of possible trees grows double exponentially over the horizon  Recently  memory bounded techniques have been introduced  These methods keep only a fixed number of trees at each iteration                 They use a fixed amount of memory and have linear complexity over the horizon  There are many possibilities for constructing policies with bounded memory  In this work we use a stochastic policy for each agent  It is quite similar to stochastic finite state controllers  FSC   used to solve infinite horizon POMDPs      and DEC POMDPs      But our stochastic policies have a layered structure  one layer for each time step  Each layer has a fixed number of decision nodes  Each node is labeled with an action and includes a node selection function  The selection function is a mapping from an observation to   start node a     o                           a  o   T  o   o          o       a  o   a   Algorithm    Rollout Sampling Policy Iteration  a   o        o   a   Figure    An agents stochastic policy with two observations and two decision nodes in each layer  a probability distribution over the nodes of the next layer  In this paper  we denote by Qti the set of decision nodes of agent i  I at time step t     T   Also  N denotes the predetermined size of Qti and  qi   oi   is the probability of selecting the node of the next layer qi  after observing oi   An example of such stochastic policies is shown in Figure    In the planning phase  a set of stochastic polices are constructed offline  one for each agent  When executing the policy  each agent executes the action in the current node and then transitions to the next node based on the received observation as well as the node selection function  We show how the stochastic node selection function can be optimized easily by our policy improvement technique  The following sections describe the algorithm in details      The Rollout Sampling Method  In this section  we propose a new rollout sampling policy iteration  DecRSPI  for DEC POMDPs that heuristically generates stochastic policies using an approximate policy improvement operator trained with Monte Carlo simulation  The approximate operator performs policy evaluation by simulation  evaluating a joint policy  q at state s by drawing K sample trajectories of  q starting at s  Then  the operator performs policy improvement by constructing a series of linear programs with parameters computed from samples and then solving the linear programs to induce a new improved approximate policy  Similar to MBDP  DecRSPI generates policies using point based dynamic programming  which builds policies according to heuristic state distributions from the bottom up  The key difference is that DecRSPI improves the policies by simulation without knowing the exact transition function P   observation function O and reward function R of the DEC POMDP model  Note that DecRSPI is performed offline in a centralized way  but the computed policies are totally decentralized  The use of simulation assumes that the state of the environment can be reset and the system information  state  reward and observations  are available after executing a joint action by the agents  In the planning phase  this information is often available  In large real world systems  modeling    given T  N generate a random joint policy Q sample a set of beliefs B for t     T  n     N for t T to   do for n   to N do   tn b  Bnt     qQ repeat foreach agent i  I do keep the other agents policies qi fixed foreach action ai  Ai do i  estimate the parameter matrix build a linear program with i i  solve the linear program i  i   hai   i i  hai   i i  arg maxi Rollout b  hai   i i  update agent is policy qi by hai   i i until no improvement in all agents policies   return the joint policy Q  the exact DEC POMDP is extremely challenging and even the representation itself is nontrivial for several reasons  First  the system may be based on some complex physical models and it may be difficult to compute the exact P   O and R  Second  the state  action and observation spaces may be very large  making it hard to store the entire transition table  Fortunately  simulators of these domains are often available and can be modified to compute the policies as needed  We activate DecRSPI by providing it with a random joint policy and a set of reachable state distributions  computed by some heuristics  The joint policy is initialized by assigning a random action and random node selection functions for each decision node from layer   to layer T   Policy iteration is performed from the last step t T backward to the first step t    At each iteration  we first choose a state distribution and an unimproved joint policy  Then we try to improve the joint policy based on the state distribution  This is done by keeping the policies of the other agents fixed and searching for the best policy of one agent at a time  We continue to alternate between the agents until no improvement is achievable for the current policies of all the agents  This process is summarized in Algorithm         Belief Sampling  In this paper  the belief state b   S  is a probability distribution over states  We use it interchangeably with the state distribution with the same meaning  Generally  given belief state bt at time t  we determine  at   execute  at and make a subsequent observation  o t     then update our belief state to obtain bt     In single agent POMDPs  this belief state is obtained via straightforward Bayesian updating  by computing bt     P r S bt    at    o t      Unfortunately  even if the transition and observation functions are available  the belief update itself is generally time consuming   Algorithm    Belief Sampling for n   to N do Bnt   for t     T h  choose a heuristic policy for k   to K do s  draw a state from b  for t   to T do t  t   btk  s    a  select a joint action based on h s   simulate the model with s   a s  s  for t   to T do bt  compute the belief by particle set t Bnt  Bnt   bt   return the belief set B  because each belief state is a vector of size  S   To approximate belief states by simulation  consider the following particle filtering procedure  At any time step t  we have a collection t of K particles  The particle set t   t     T represents the following state distribution  PK      btk  s   t   t b  s    k     s  S     K where btk  s  is the k th particle of t   As mentioned above  the significance of this method lies in the fact that  for many applications  it is easy to sample successor states according to the system dynamics  But direct computation of beliefs is generally intractable especially when the dynamics specification is unavailable  Another key question is how to choose the heuristic policies  In fact  the usefulness of the heuristics and  more importantly  the computed belief states  are highly dependent on the specific problem  Instead of just using one heuristic  a whole portfolio of heuristics can be used to compute a set of belief states  Thus  each heuristic is used to select a subset of the policies  There are a number of possible alternatives  Our first choice is the random policy  where agents select actions randomly from a uniform distribution at each time step  Another choice is the policy of the underlying MDP  That is  agents can learn an approximate MDP value function by some MDP learning algorithms and then select actions greedily based on that value function  In specific domains such as robot soccer  where learning the MDP policy is also hard  hand coded policies or policies learned by DecRSPI itself with merely random guidance are also useful as heuristics  The overall belief sampling method is detailed in Algorithm         Policy Improvement  In multi agent settings  agents with only local information must reason about all the possible choices of the others and select the optimal joint policy that maximizes the teams expected reward  One straightforward method for finding  i  oi   qi   x oi   qi    P subject to oi  qi  x oi   qi        oi q  x oi   qi         Maximize x  P  oi i  P  qi  Qt   i  i  Table    Linear program to improve agent is policy where the variable x oi   qi       qi   oi   is the node selection table  the optimal joint policy is to simply search over the entire space of possible policies  evaluate each one  and select the policy with the highest value  Unfortunately  the number T of possible joint policies is O   Ai    i        i       I     Instead of searching over the entire policy space  dynamic programming  DP  constructs policies from the last step up to the first one and eliminates dominated policies at the early stages       However  the exhaustive backup in the DP algorithm at t still generates agent is policies of the order O  Ai   Qit    i      Memory bounded techniques have been developed to combine the top down heuristics and the bottom up dynamic programming together  keeping only a bounded number of policies at each iteration       This results in linear complexity over the horizon  but the one step backup operation is still time consuming       Our algorithm is based on the MBDP algorithm       but it approximates the backup operation with an alternating maximization process  As shown in Algorithm    the basic idea is to choose each agent in turn and compute the best response policy  while keeping the policies of the other agents fixed  This process is repeated until no improvement is possible for all agents  That is  the process ends when the joint policy converges to a Nash equilibrium  This method was first introduced by Nair et al       and later refined by Bernstein et al       The differences are  Nair et al  use the method to reformulate the problem as an augmented POMDP  Bernstein et al  use it to optimize the controllers of infinite horizon DEC POMDPs  In contrast  when an agent is chosen  our algorithm approximates the best response policy that maximizes the following value  X Y V  b   q    R b   a   P r s     o b   a   qi   oi  V  s     q     s     o   q   i      P   where P r sP    o b   a    sS b s P  s   s   a O  o s     a  and R b   a    sS b s R s   a   This value function is similar to Equation    but for a stochastic joint policy  Notice that our algorithm is designed to work when an explicit form of system dynamics is not available  Our solution  as shown in Algorithm    is two fold  first we find the best node selection function i for every action ai  Ai and generate a set of stochastic policies i   then we evaluate the policy qi  i for the given belief point b  choose the best one and update the current policy of agent i with it  In order to find the best i that maximizes the value function of Equation   given ai and other agents policies qi   we use the linear program shown in Table    Note that   Algorithm    Parameter Estimation  Algorithm    Rollout Evaluation  Input  b  ai   qi ai  get actions from qi for k   to K do s  draw a state from b s     o  simulate the model with s   a oi  s    oi    oi  s    oi       normalize oi for oi  i foreach oi  i   qi   Qt   do i for k   to K do s    oi  draw a sample from oi   qi  get other agents policy   qi   oi   i  oi   qi   k  Rollout s      q    P K     i  oi   qi     K k   i  oi   qi  k return the parameter matrix i  Input  t  s   q t for k   to K do vk    while t  T do  a  get the joint action from  q t s    r   o  simulate the model with s   a vk  vk   r   q t       q t    o  s  s    t  t     PK   V  K k   vk return the average value V  R b   a  is a constant given b  ai   qi and is thus omitted  The matrix i of the linear program is defined as follows  X   i  oi   qi      P r s     o b   a  qi  oi  V  s     q       s   oi  qi  Q   where  qi  oi     k  i  qk   ok    Since the dynamics is unknown  Algorithm   is used to estimate i   It first estimates P r s     o b   a  by drawing K samples from one step simulation  Then it estimates each element of i by an  other K samples with  qi  oi    The value of V  s     q     is approximated by the rollout operation as follows       Rollout Evaluation  The rollout evaluation is a Monte Carlo method to estimate the value of a policy  q at a state s  or belief state b   without requiring an explicit representation of the value function as the DP algorithm does  A rollout for hs   qi simulates a trajectory starting from state s and choosing actions according to policy  q up to the horizon T   The observed total accumulated reward is averaged over K rollouts to estimate the value V  s   q   If a belief state b is given  it is straightforward to draw a state s from b and perform this simulation  The outline of the rollout process is given in Algorithm    The accuracy of the expected value estimate improves with the number of rollouts  Intuitively  the value starting from hs   qi can be viewed as a random variable whose expectation is V  s   q   Each rollout term vk is a sample of this random variable and the average of these V is an unbiased estimate of V  s   q   Thus  we can apply the following Hoeffding bounds to determine the accuracy of this estimate  Property    Hoeffding inequality   Let V be a random variable in  Vmin   Vmax   with V   E V    observed values PK   v    v         vK of V   and V   K k   vk   Then   P r V  V         exp  K   V    P r V  V        exp  K   V  where V   Vmax  Vmin is the range of values   Given a particular confidence threshold  and a size of samples K  we can produce a PAC style error bound  on the accuracy of our estimate V   s V  ln              K Property    If the number of rollouts K is infinitely large  the average value returned by the rollout algorithm V will converge to the expected value of the policy V   The required sample size given error tolerance  and confidence threshold  for the estimation of V is  K        V  ln                It is difficult to compute a meaningful error bound for the overall algorithm  There are several reasons      DecRSPI is an MBDP based algorithm and MBDP itself has no guarantee on the solution quality since the belief sampling method is based on domain dependent heuristics      the local search technique  which updates one agents policy at a time  could get stuck in a suboptimal Nash equilibrium  and     the error may accumulate over the horizon  because the policies of the current iteration depend on the policies of previous iterations  Thus  we demonstrate the performance and benefits of DecRSPI largely based on experimental results       Complexity Analysis  Note that the size of each agents policy is predetermined with T layers and N decision nodes in each layer  At each iteration  DecRSPI chooses an unimproved joint policy and tries to improve the policy parameters  actions and node selection functions  of each agent  Thus  the amount of space is of the order O mT N   for m agents  Several rollouts are performed in the main process of each iteration  The time per rollout grows linearly with T   Therefore  the total time with respect to the horizon is on the order of              T    T     T      i e  O T      Theorem    The DecRSPI algorithm has linear space and quadratic time complexity with respect to the horizon T                                                     a  Meeting in a    Grid                                         Horizon                                 Horizon   b  Cooperative Box Pushing   c  Stochastic Mars Rover       Box Pushing Meeting Grid Mars Rover          Value                     Box Pushing Meeting Grid Mars Rover                                                          Horizon   d  Horizon vs  Runtime                                 Number of Trials   e  Trials vs  Value  Time  s                     DecRSPI DGD PBIP IPG                                           Horizon  Time  s        DecRSPI DGD PBIP IPG Value  DecRSPI DGD PBIP IPG Value  Value                                                                       Box Pushing Meeting Grid Mars Rover                                 Number of Trials   f  Trials vs  Runtime  Figure    Experimental results for standard benchmark problems  Clearly  the amount of space grows linearly with the number of agents  At each iteration  the main loop chooses N joint policies  For each joint policy  the improvement process selects agents alternatively until no improvement is possible  In practice  we set thresholds both for the minimum improvement  e g        and the maximum repeat count  e g        The improvement process terminates when one of these bounds is reached  Theoretically  the runtime of a rollout inside the improvement process is independent of the number of agents  However in practice  systems with more agents will take significantly more time to simulate  thereby increasing the time per rollout  But this is due to the complexity of domains or simulators  not the complexity of the DecRSPI algorithm  Theorem    Ignoring system simulation time  the DecRSPI algorithm has linear time and space complexity with respect to the number of agents  I       Experiments  We performed experiments on several common benchmark problems in the DEC POMDP literature to evaluate the solution quality and runtime of DecRSPI  A larger distributed sensor network domain was used to test the scalability of DecRSPI with respect to the number of agents       Benchmark Problems  We first tested DecRSPI on several common benchmark problems for which the system dynamics  an explicit representation of the transition  observation and reward functions  is available  To run the learning algorithm  we implemented a DEC POMDP simulator based on the dynamics and learned the joint policy from the simulator  We used two types of heuristic policies to sample belief states  the  random policy that randomly chooses an action with a uniform distribution  and the MDP based policy that chooses an action according to the global state  which is known during the learning phase   For the benchmark problems  we solved the underlying MDP models and used the policies for sampling  DecRSPI selects a heuristic each time with a chance of      acting randomly and      for MDP policies  There are few work on learning policies in the general DEC POMDP setting  In the experiments  we compared our results with the distributed gradient descent  DGD       with different horizons  The DGD approach performs the gradient descent algorithm for each agent independently to adapt the parameters of each agents local policy  We also present the results of PBIP IPG      the best existing planning algorithm  for these domains  Notice that PBIP IPG computes the policy based on an explicit model of system dynamics  Thus  the values of PBIP IPG can be viewed as upper bounds for learning algorithms  Due to the randomness of Monte Carlo methods  we ran the algorithm    times per problem and reported average runtimes and values  The default number of policy nodes N is   and the number of samples K is     We experimented with three common DEC POMDP benchmark problems  which are also used by PBIPIPG      The Meeting in a    Grid problem     involves two robots that navigate in a    grid and try to stay as much time as possible in the same cell  We adopted the version used by Amato et al       which has    states    actions and   observations per robot  The results for this domain with different horizons are given in Figure   a   The Cooperative Box Pushing problem      involves two robots that cooperate with each other to push boxes to their destinations in a    grid  This domain has     states    actions and   observations per robot  The results are given in Figure   b   The Stochastic Mars Rover problem     is            DecRSPI Value DGD Value DecRSPI Time DecRSPI SIM Time              Figure    The distributed sensor network domain                       Time  s   Value                    a larger domain with   robots      states    actions and   observations per robot  The results are given in Figure   c   In all three domains  DecRSPI outperforms DGD with large margins  In the Meeting in Grid and Mars Rover domains  the learning results of DecRSPI are quite close to the planning values of PBIP IPG  Note that PBIP IPG is also an MBDP based algorithm whose values represent good upper bounds on the learning quality of DecRSPI  Being close to the value of PBIP IPG means that DecRSPI does learn good policies given the same heuristics and policy sizes  In the Cooperative Box Pushing domain  the gap between DecRSPI and PBIP IPG is a little bit larger because this problem has more complex interaction structure than the other two domains  Interestingly  in this domain  the value of DGD decreases with the horizon  We also present timing results for each domain with different horizons  T   in Figure   d   which shows the same property  quadratic time complexity  as stated in Theorem    In Figure   e   we test DecRSPI with different number of trials  K  and a fixed horizon of     The value of the Meeting in Grid and Mars Rover domains becomes stable when the number of trials is larger than     But the Box Pushing problem needs more trials  about     to get to a stable value  which is very close to the value of PBIP IPG  In Figure   f   we show that runtime grows linearly with the number of trials in all three domains  It is worthwhile to point out that in these experimental settings  DecRSPI runs much faster than PBIP IPG  For example  in the Stochastic Mars Rover domain with horizon     PBIP IPG may take      s while DecRSPI only needs     s       Distributed Sensor Network  The distributed sensor network  DSN  problem  adapted from       consists of two chains with identical number of sensors as shown in Figure    The region between the chains is split into cells and each cell is surrounded by four sensors  The two targets can move around in the place  either moving to a neighboring cell or staying in place with equal probability  Each target starts with an energy level of    A target is captured and removed when it reaches    Each sensor can take   actions   track left  track right and none  and has   observations  left and right cells are occupied or not   resulting in joint spaces of   I  actions and   I  observations  e g                joint actions and                joint observations for the    agents case   Each track action has a cost of    The energy of a target will be decreased by   if it is tracked by at least three of the                              Number of Agents          Figure    Value and runtime of DSN  T      N     K      four surrounding sensors at the same time  When a target is captured  the team gets a reward of     When all targets are captured  the DSN restarts with random target positions  This domain is designed to demonstrate the scalability of DecRSPI over the number of agents  Most of the planning algorithms for general DEC POMDPs have only been tested in domains with   agents                     It is very challenging to solve a general DEC POMDP problems with many agents because the joint action and observation spaces grow exponentially over the number of agents  Our results in the DSN domain with horizon    and random heuristics are shown in Figure    Again  DecRSPI achieves much better value than DGD  The value decreases with the growing number of agents because there are more cells for the targets to move around and greater chance of sensor miscoordination  Note that DecRSPI solves this problem without using any specific domain structure and the learned policies are totally decentralized  without any assumption of communication or global observability  The figure also shows two measures of timing results  DecRSPI Time  the runtime of DecRSPI  and DecRSPI SIM Time  the overall runtime including domain simulation time  The two measures of runtime grow with the number of agents  As stated in Theorem    DecRSPI Time grows linearly  which shows that it scales up very well with the number of agents      Related Work  Several policy search algorithms have been introduced to learn agents policies without the system dynamics  The distributed gradient descent  DGD  algorithm performs gradient based policy search independently on each agents local controller using the experience data       Zhang et al       proposed an online natural actor critic algorithm using conditional random fields  CRF   It can learn cooperative policies with CRFs  but it assumes that agents can communicate and share their local observations at every step  Melo      proposed another actor critic algorithm with natural gradient  but it only works for transition independent DEC POMDPs  In contract  our algorithm learns cooperative policies for the general DEC POMDP setting without any assumption about communication    The rollout sampling method has been introduced to learn MDP policies without explicitly representing the value function              The main idea is to produce training data through extensive simulation  rollout  of the previous policy and use a supervised learning algorithm  e g  SVM  to learn a new policy from the labeled data  The rollout technique is also widely used to perform lookahead and estimate the value of action in online methods            Our algorithm uses rollout sampling to estimate the parameters of policy improvements and select the best joint policy      Conclusion  We have presented the decentralized rollout sampling policy iteration  DecRSPI  algorithm for learning cooperative policies in partially observable multi agent domains  The main contribution is the ability to compute decentralized policies without knowing explicitly the system dynamics  In many applications  the system dynamics is either too complex to be modeled accurately or too large to be represented explicitly  DecRSPI learns policies from experience obtained by merely interacting with the environment  The learned policies are totally decentralized without any assumption about communication or global observability  Another advantage of DecRSPI is that it focuses the computation only on reachable states  As the experiments show  little sampling is needed for domains where agents have sparse interaction structures  and the solution quality calculated by a small set of samples is quite close to the best existing planning algorithms  Most importantly  DecRSPI has linear time complexity over the number of agents  Therefore DecRSPI can solve problems with up to    agents as shown in the experiments  Additionally  DecRSPI bounds memory usage as other MBDP based algorithms  In the future  we plan to further exploit the interaction structure of agents and make even better use of samples  which will be helpful for large real world domains   Acknowledgments This work was supported in part by the Air Force Office of Scientific Research under Grant No  FA                the National Science Foundation under Grant No  IIS         the Natural Science Foundations of China under Grant No            and the National Hi Tech Project of China under Grant No      AA  Z      
 Decentralized POMDPs provide an expressive framework for multi agent sequential decision making  While finite horizon DECPOMDPs have enjoyed significant success  progress remains slow for the infinite horizon case mainly due to the inherent complexity of optimizing stochastic controllers representing agent policies  We present a promising new class of algorithms for the infinite horizon case  which recasts the optimization problem as inference in a mixture of DBNs  An attractive feature of this approach is the straightforward adoption of existing inference techniques in DBNs for solving DEC POMDPs and supporting richer representations such as factored or continuous states and actions  We also derive the Expectation Maximization  EM  algorithm to optimize the joint policy represented as DBNs  Experiments on benchmark domains show that EM compares favorably against the state of the art solvers      Introduction  Decentralized partially observable MDPs  DECPOMDPs  have emerged in recent years as an important framework for modeling sequential decision making by a team of agents      Their expressive power makes it possible to tackle coordination problems in which agents must act based on different partial information about the environment and about each other to maximize a global reward function  Applications of DEC POMDPs include coordinating the operation of planetary exploration rovers      coordinating firefighting robots       broadcast channel protocols     and target tracking by a team of sensor agents       However  the rich model comes with a priceoptimally solving a finite horizon DEC POMDP  Shlomo Zilberstein Computer Science Dept  University of Massachusetts  Amherst shlomo cs umass edu  is NEXP Complete      In contrast  finite horizon POMDPs are PSPACE complete       a strictly lower complexity class that highlights the difficulty of solving DEC POMDPs  Recently  a multitude of point based approximate algorithms have been proposed for solving finite horizon DEC POMDPs              However  unlike their point based counterparts in POMDPs             they cannot be easily adopted for the infinite horizon case due to a variety of reasons  For example  POMDP algorithms represent the policy compactly as  vectors  whereas all DEC POMDP algorithms explicitly store the policy as a mapping from observation sequences to actions  making them unsuitable for the infinitehorizon case  In POMDPs  the Bellman equation forms the basis of most point based solvers  but as Bernstein et  al      highlight  no analogous equation exists for DEC POMDPs  To alleviate such problems  most infinite horizon algorithms represent agent policies as finite state controllers         So far  only two algorithms have shown promise for effectively solving infinite horizon DEC POMDPsdecentralized bounded policy iteration  DEC BPI      and a non linear programming based approach  NLP       However  both of these algorithms have significant drawbacks in terms of the representative class of problems that can be handled  For example  solving DEC POMDPs with continuous state or action spaces is not supported by either of these approaches  Scaling up to structured representations such as factored or hierarchical state space is difficult due to convergence issues in DEC BPI and a potential increase in the number of non linear constraints in the NLP solver  Further  none of the above approaches have been shown to work for more than   agents  a significant bottleneck for solving practical problems  To address these shortcomings  we present a promising new class of algorithms which amalgamates planning with probabilistic inference and opens the door   to the application of rich inference techniques to solving infinite horizon DEC POMDPs  Our technique is based on Toussaint et  al s approach of transforming the planning problem to its equivalent mixture of dynamic Bayes nets  DBNs  and using likelihood maximization in this framework to optimize the policy value           Earlier work on planning by probabilistic inference can be found in      Such approaches have been successful in solving MDPs and POMDPs       They also easily extend to factored or hierarchical structures      and can handle continuous action and state spaces thanks to advanced probabilistic inference techniques       We show how DEC POMDPs  which are much harder to solve than MDPs or POMDPs  can also be reformulated as a mixture of DBNs  We then present the Expectation Maximization algorithm  EM  to maximize the reward likelihood in this framework  The EM algorithm naturally has the desirable anytime property as it is guaranteed to improve the likelihood  and hence the policy value  with each iteration  We also discuss its extension to large multiagent systems  Our experiments on benchmark domains show that EM compares favorably against the state of the art algorithms  DEC BPI and NLP based optimization  It always produces better quality policies than DEC BPI and for some instances  it nearly doubles the solution quality of the NLP solver  Finally  we discuss potential pitfalls  which are inherent in the EM based approach   This added uncertainty about other agents in the system make DEC POMDPs NEXP complete      We are concerned with solving infinite horizon DECPOMDPs with a discount factor   We represent the stationary policy of each agent using a fixed size  stochastic finite state controller  FSC  similar to      An FSC can be described by a tuple hN      i  N denotes a finite set of controller nodes n     N  A represents the actions selection model or the probability an   P  a n      N  Y  N represents the node transition model or the probability n  ny   P  n   n  y      N  N represents the initial node distribution n   P  n   We adopt the convention that nodes of agent  s controller are denoted by p and agent  s by q  Other problem parameters such as observation function P  y  z s  a  b  are represented using subscripts as Pyzsab   The value for starting the controllers in nodes hp  qi at state s is given by  h X V  p  q  s    ap bq Rsab   i X a b X X  Ps  sab Pyzs  ab p  py q  qz V  p    q     s      s   p   q    y z  The goal is to set the parameters h    i of the agents controllers  of some given size  that maximize the expected discounted reward for the initial belief b    X V  b      p q b   s V  p  q  s  p q s     The DEC POMDP model  In this section  we introduce the DEC POMDP model for two agents      Note that finite horizon DECPOMDPs are NEXP complete even for two agents  The set S denotes the set of environment states  with a given initial state distribution b    The action set of agent   is denoted by A and agent   by B  The state transition probability P  s   s  a  b  depends upon the actions of both the agents  Upon taking the joint action ha  bi in state s  agents receive the joint reward R s  a  b   Y is the finite set of observations for agent   and Z for agent    O s  ab  yz  denotes the probability P  y  z s  a  b  of agent   observing y  Y and agent   observing z  Z when the joint action ha  bi was taken and resulted in state s  To highlight the differences between a single agent POMDP and a DEC POMDP  we note that in a POMDP an agent can maintain a belief over the environment state  However  in a DEC POMDP  an agent is not only uncertain about the environment states but also about the actions and observations of the other agent  Therefore in a DEC POMDP a belief over the states cannot be maintained during execution time      DEC POMDPs as mixture of DBNs  In this section  we describe how DEC POMDPs can be reformulated as a mixture of DBNs such that maximizing the reward likelihood  to be defined later  in this framework is equivalent to optimizing the joint policy  Our approach is based on the framework proposed in          to solve Markovian planning problems using probabilistic inference  First we informally describe the intuition behind this reformulation  for details please refer to       and then we describe in detail the steps specific to DEC POMDPs  A DEC POMDP can be described using a single DBN where the reward is emitted at each time step  However  in our approach  it is described by an infinite mixture of a special type of DBNs where reward is emitted only at the end  For example  Fig    a  describes the DBN for time t      The key intuition is that for the reward emitted at any time step T   we have a separate DBN with the general structure as in Fig    b   Further  to simulate the discounting of rewards  probability of time variable T is set as P P  T   t     t        This ensures that t   pt      In addition  the random variable r shown in Fig    a b    p   p   p  a   a   y   a   y z  s   r  s  b   q    a   pT  y   yT  y z   yT zT  p   p   s  z  q   b   pT  aT  a  s   b  q   p   sT  y   a   yT  aT  r  z   zT  q   qT   b   bT s   s   sT  r   c   Figure    a  DEC POMDP DBN for time step    b  for time step T   c  POMDP DBN for time step T  is a binary variable with its conditional distribution  for any time T   described using the normalized immediate reward as Rsab   P  r     sT   s  aT   a  bT   b     Rsab  Rmin    Rmax  Rmin    This scaling of the reward is the key to transforming the optimization problem from the realm of planning to likelihood maximization as stated below   denotes the parameters h    i for each agents controller  Theorem    Let the  CPT of binary rewards r be such that Rsab  Rsab and the discounting time prior be set   as P  T      T        Then  maximizing the likelihood L   P  r        in the mixture of DBNs is equivalent to optimizing the DEC POMDP policy  Furthermore  the joint policy value relates linearly P to the likelihood as V     Rmax  Rmin  L          T  T Rmin The proof is omitted as it is very similar to that of MDPs and POMDPs       Before detailing the EM algorithm  we describe the DBN representation of DECPOMDPsthe basis for any inference technique  The DBN for any time step T is shown in Fig    b   Every node is a random variable with subscripts indicating time  pi denotes controller nodes for agent   and qi for agent    The remaining nodes represent the states  actions  and observations  There are four kinds of dependencies induced by the DEC POMDP model that the DBN must represent   State transitions  State transitions as a result of the joint action of both agents and the previous state  shown by the DBNs middle layer   Controller node transitions     These transitions depend on the last controller state and the most recent individual observation received  They are shown in the top and bottom layers   Action probabilities     The action taken at any time step t depends on the current controller state  The links between controller nodes  pi or qi   and action nodes  ai or bi   model this   Observation probabilities  First  the probability of receiving joint observation yi zi depends on the joint action of both agents and the domain  state  This relationship is modeled by the DBN nodes labeled yi zi   Second  the individual observation each agent receives is a deterministic function of the joint observation  That is Pyy  z    P  y y   z         if y   y   else    This is modeled by a link between yi zi and the nodes yi and zi   To highlight the differences from a POMDP  Fig    c  shows the DBN for a POMDP  The sheer scale of interactions present in a DEC POMDP DBN become clear   from this comparison  also highlighting the difficulty of solving DEC POMDPs even approximately  In a POMDP  an agent receives the observation which is affected by the environment state  whereas in a DECPOMDP agents only perceive the individual part of the joint observation yi zi   Such differences in the interaction structure make the E and M steps of a DECPOMDP EM very different from that of a POMDP  despite sharing the same high level principles      EM algorithm for DEC POMDPs  This section describes the EM algorithm     for maximizing the reward likelihood in the mixture of DBNs representing DEC POMDPs  In the corresponding DBNs  only the binary reward is treated as observed  r       all other variables are latent  While maximizing the likelihood  EM yields the DEC POMDP joint policy parameters   EM also possesses the desirable anytime characteristic as the likelihood  and the policy value which is proportional to the likelihood  is guaranteed to increase per iteration until convergence  We note that EM is not guaranteed to converge to the global optima  However  in the experiments we show that EM almost always achieves similar values as the state of the art NLP based solver     and much better than DEC BPI      The main advantage of using EM lies in its ability to easily generalize to much richer representations than currently possible for DECPOMDPs such as factored or hierarchical controllers  continuous state and action spaces  Another important advantage is the ability to generalize the solver to larger multi agent systems with more than   agents    The E step we derive next is generic as any probabilistic inference technique can be used       E step  p q s  In the E step  for the fixed parameter   forward messages  and backward messages  are propagated  First  we define the following Markovian transitions on the  p  q  s  state in the DBN of Fig    b   These transitions are independent of the time t due to the stationary joint policy  We also adopt the convention that for any random variable v  v   refers to the next time slice and v refers to the previous time slice  For any group of variables v  Pt  v  v    refers to P  vt   v  vt     v     P  p   q    s   p  q  s    X p  py  q  qz  Py  z  abs  ap bq Ps  sab       aby   z       p  q  s       p q b   s  X   P  p    q     s   p  q  s t   p  q  s      t  p   q   s    p q s  Intuitively   messages compute the probability of visiting a particular  p  q  s  state in the DBN as per the current policy  The  messages are similar to computing the value of starting the controllers in nodes hp  qi at state s using dynamic programming  They are propagated backwards and are defined as Pt  r     p  q  s   However  this particular definition would require separate inference for each DBN as for T and T   step DBN  t will be different due to difference in the time to go  T  t and T    t   To circumvent this problem   messages are indexed backward in time as   p  q  s    PT   r     p  q  s  using the index  such that      denotes the time slice t   T   Hence we get  X    p  q  s    Rsab ap bq ab    p  q  s   If both  and  messages are propagated for k steps P k  and L k   T     T LT   then the message propagation can be stopped            X      p    q     s   P  p    q     s   p  q  s   p   q    s   Based on the  and  messages P we also calculate two more quantities  p  q  s    t P  T   t  p  q  s  and P  p  q  s    t P  T   t  p  q  s   which will be used in the M step  The cut off time for message propagation can either be fixed a priori or be more flexible based on the likelihood accumulation  If  messages  Complexity  Calculating the Markov transitions on the  p  q  s  chain has complexity O N   S   A  Y      where N is the maximum number of nodes for a controller  The message propagation has complexity O Tmax N   S      Techniques to effectively reduce this complexity without sacrificing accuracy will be discussed later       t is defined as Pt  p  q  s     It might appear that we need to propagate  messages for each DBN separately  but as pointed out in       only one sweep is required as the head of the DBN is shared among all the mixture components  That is    is the same for all the T step DBNs with T     We will omit using  as long as it is unambiguous      are propagated for t steps and  messages for  steps  then the likelihood for T   t    is given by X Lt    P  r     T   t          t  p  q  s   p  q  s   M step  In the DBNs of Fig    a b  every variable is hidden except the reward variable  After each M step  EM provides better estimates of these variables  improving the likelihood L and hence the policy value  For details of EM  we refer to      The parameters to estimate are h    i for each agent  For a particular DBN for time T   let L    P  Q  A  B  S  denote the latent variables  where each variable denotes a sequence of length T   That is  P   p  T   EM maximizes the following expected complete log likelihood for the DEC POMDP DBN mixture   denotes the previous parameters and   denotes new parameters  XX Q         P  r      L  T     log P  r      L  T       T  L  In the rest of the section  all the derivations refer to the general DBN structure of the DEC POMDP as in Fig    b   The joint probability of all the variables is  T      Y P  r      L  T       P  T   Rsab t T ap bq Pssab      t   Pyyz Pzyz Pyzsab ppy qqz ap bq p q b   s  t        where     brackets indicate the time slices  i e   Rsab t T   R sT   aT   bT    Taking the log  we get  log P  r      L  T                T X t    log pt pt  yt    T X t   T X  log at pt    T X  log bt qt  t    log qt qt  zt  t      log p    log q        where the missing terms represents the quantities independent of   As all the policy parameters h    i get separated out for each agent in the log above  we first derive the action updates for an agent by substituting Eq    in Q                Action updates    The update for action parameters ap for agent   can   be derived by simplifying Q      as follows   Q           X  P  T    T X X       P  r      a  p T     t log ap t   a p  T     By breaking the above summation between t   T and t     to T     we get  X  P  T    T     X    Rsab ap bq T  p  q  s  log ap    X  p  q   s  y   z    b  The above expression is maximized by setting the pa  rameter ap to be   P  T    T     apqbs T   X   X  The product P  s   a  q  s P  y   z    a  q  s    can be further simplified by marginalizing out over actions b of agent   as follows   X X X      ap log ap  p  q  s  Rsab bq       ap qs b   X X                        p   q   s  p py q qz Py z s ab bq Ps sab   X ap X   p  q  s  Rsab bq   Cp qs   b   X X  p    q     s   p  py  q  qz  Py  z  s  ab bq Ps  sab       ap      T t   p    q     s   Pt  a  p  p    q     s    log ap  t   app  q   s   p  q   s   y   z    In the above equation  we marginalized the last time slice over the variables  q  b  s   For the intermediate time slice t  we condition upon the variables  p    q     s    in the next time slice t      We now use the definition of  and move the summation over time T inside for the last time slice and further marginalize over the remaining variables  q  s  in the intermediate slice t     X    Rsab ap bq  p  q  s  log ap    b  where Cp is a normalization constant  The action pa  rameters bq of the other agent can be found similarly by the analogue of the previous equation         Controller node transition updates  The update for controller node transition parameters ppy for agent   can be found by maximizing Q       w r t  ppy as follows   a p q b s  X  P  T    T   X X    log ap           T t   p   q   s  ap  Upon further marginalizing over the joint observations y   z   and simplifying we get  X X X     ap log ap Rsab bq  p  q  s    qs  X   X  p  q   s  y   z   T     P  T    b T   X   X  P  T    T     P  p   q   s  a  p  q  s t  p  q  s   ap  Q          p  q   s  sq        t   ap  T     X              T t   p   q   s  P  s  a  q  s   t    p  py  q  qz  P  y   z    a  q  s   t  p  q  s       X T     P  T    T X X  log  ppy T t  p  q  s Pt  p  p  y  s  q T      t   ppysq  By further marginalizing over the variables  s  q  for the previous time slice of t and over the observations z of the other agent  we get    X  ppy log  ppy  ppy  p  q   s   y   z    t   ppy  By marginalizing over the variables  q  s  for the current time slice t  we get     We resolve the above time summation  as in       based P PT   on the fact that t   f  T  t    g t  can be T    P P  rewritten as t   T  t   fP  T  t    g t  P and then  setting    T  t    to get t   g t      f      Finally we get   X X X      ap log ap  p  q  s  Rsab bq       ap qs b   X                p   q   s  p  py  q  qz  P  s  a  q  s P  y z  a  q  s    T X X     P  r      p  p  y T     t log  ppy   X  P  T    T X X  T t  p  q  s qqz  t   sqsqz  T     P  yz p  q  s P  s p  q  s t   p  q  s  The above equation can be further simplified by marginalizing the product P  yz p  q  s P  s p  q  s  over actions a and b of both the agents as follows     X ppy  ppy log  ppy   X T     P  T    T X X  T t  p  q  s qqz  t   sqsqz  t   p  q  s   X ab  Pyzsab Pssab ap bq   Upon resolving the time summation as before  we get the final M step estimate   ppy    ppy X  p  q  s  p  q  s qqz Cpy sqsqz X Pyzsab Pssab ap bq  The parameters  qqz for the other agent can be found in an analogous way  Initial node distribution  The initial node distribution  for controller nodes of agent   and   can be updated as follows  We do not show the complete derivation as it is similar to that of the other parameters  p X  p  q  s q Ps b   s      p    Cp qs        Complexity and implementation issues  The complexity of updating all action parameters is O N   S   AY      Updating node transitions requires O N   S   Y     N   S   Y   A     This is relatively high when compared to the POMDP updates requiring O N   S   AY   mainly due to the scale of the interactions present in DEC POMDPs  In our experimental settings  we observed that having a relatively small sized controller  N     suffices to yield good quality solutions  The main contributor to the complexity is the factor S   as we experimented with large domains having nearly     states  The good news is that the structure of the E and M step equations provides a way to effectively reduce this complexity by significant factor without sacrificing accuracy  For a given state s  joint action ha  bi and joint observation hy  zi  the possible next states can be calculated as follows  succ s  a  b  y  z     s   P  s   s  a  b P  y  z s    a  b        For most of the problems  the size of this set is typically a constant k       Such simple reachability analysis and other techniques could speed up the EM algorithm by more than an order of magnitude for large problems  The effective complexity reduces to O N   SAY   k  for the action updates and O N   SY   k N   SY   A  k  for node transitions  Other enhancements of the EM implementation are discussed in Section        DEC BPI                          NLP                  EM                      DEC BPI    s    s  s  s  EM    s    s    s     s  Table    Broadcast channel  Policy value  execution time       ab         Size          Experiments  We experimented with several standard   agent DECPOMDP benchmarks with discount factor      Complete details of these problems can be found in          We compare our approach with the decentralized bounded policy iteration  DEC BPI  algorithm     and a non convex optimization solver  NLP       The DEC BPI algorithm iteratively improves the parameters of a node using a linear program while keeping the other nodes parameters fixed  The NLP approach recasts the policy optimization problem as a non linear program and uses an off the shelf solver  Snopt      to obtain a solution  We implemented the EM algorithm in JAVA  All our experiments were on a Mac with  GB RAM and    GHz CPU  Each data point is an average of    runs with random initial controller parameters  In terms of solution quality  EM is always better than DEC BPI and it achieves similar or higher solution quality than NLP  We note that our current implementation is mainly a proof of concept  we have not yet implemented several enhancements  discussed later  that could improve the performance of the EM approach  In contrast  the NLP solver     is an optimized package and therefore for larger problems is currently faster than the EM approach  The fact that a crude implementation of the EM approach works so well is very encouraging  Table   shows results for the broadcast channel problem  which has   states    actions per agent and   observations  This is a networking problem where agents must decide whether or not to send a message on a shared channel and must avoid collision to get a reward  We tested with different controller sizes  On this problem  all the algorithms compare reasonably well  with EM being better than DEC BPI and very close in value to NLP  The time for NLP is also   s  Fig    a  compares the solution quality of the EM approach against DEC BPI and NLP for varying controller sizes on the recycling robots problem  In this problem  two robots have the task of picking up cans in an office building  They can search for a small can  a big can or recharge the battery  The large item is only retrievable by the joint action of the two robots  Their goal is to coordinate their actions to maximize the joint reward  EM    and NLP    show the results with controller size   for both agents in Fig    a   For this problem  EM works much better than both DEC BPI and the NLP approach  EM achieves a value of     for all controller sizes  providing nearly     improvement over DEC BPI        and     improvement over NLP         Fig    b  shows the time comparisons for EM with different controller sizes  Both the NLP and DEC BPI take nearly  s to converge  EM                            EM    EM    NLP    NLP    DEC BPI    DEC BPI                     a                    Iteration                                EM    EM                        Iteration             b    S        A        Y                                    Time  sec       Policy Value         Time  sec   Policy Value      EM    EM    NLP    NLP        c                    Iteration                               S         A        Y           EM    EM                        Iteration                  d   Figure    Solution quality and runtime for recycling robots  a     b  and meeting on a grid  c     d   with controller size   has comparable performance  but as expected  EM with   node controllers takes longer as the complexity of EM is proportional to O N      Fig    c  compares the solution quality of EM on the meeting on a grid problem  In this problem  agents start diagonally across in a      grid and their goal is to take actions such that they meet each other  i e   share the same square  as much as possible  As the figure shows  EM provides much better solution quality than the NLP approach  EM achieves a value of     which nearly doubles the solution quality achieved by NLP          DEC BPI results are not plotted as it performs much worse and achieves a solution quality of    essentially unable to improve the policy at all even for large controllers  Both DEC BPI and NLP take around  s to converge  Fig    d  shows the time comparison for EM versions  EM with   node controllers is very fast and takes    s to converge     iterations   Also note that in both the cases  EM could run with much larger controller sizes        but the increase in size did not provide tangible improvement in solution quality  Fig    shows the results for the multi agent tiger problem  involving two doors with a tiger behind one door and a treasure behind the other  Agents should coordinate to open the door leading to the treasure      Fig    a  shows the quality comparisons  EM does not perform well in this case  even after increasing the controller size  it achieves a value of     NLP works better with large controller sizes  However  this experiment presents an interesting insight into the workings of EM as related to the scaling of the rewards  Recalling the relation between the likelihood and the policy value from Theorem    the equation for this problem                    Likelihood  Policy Value                EM    EM    EM     NLP    NLP    NLP                a               Iteration                                         S        A        Y             EM    EM                 Iteration             b   Figure    Solution quality  a  and likelihood  b  for tiger  is  V        L          For EM to achieve the same solution as the best NLP setting        the likelihood should be       Fig    b  shows that the likelihood EM converges to is       Therefore  from EMs perspective  it is finding a really good solution  Thus  the scaling of rewards has a significant impact  in this case  adverse  on the policy value  This is a potential drawback of the EM approach  which applies to other Markovian planning problems too when using the technique of       Incidently  DEC BPI performs much worse on this problem and gets a quality of     Fig    shows the results for the two largest DECPOMDP domainsbox pushing and Mars rovers  In the box pushing domain  agents need to coordinate and push boxes into a goal area  In the Mars rovers domain  agents need to coordinate their actions to perform experiments at multiple sites  Fig    a  shows that EM performs much better than DEC BPI for every controller size  For controller size    EM achieves better quality than NLP with comparable runtime  Fig    b       iterations   However  for the larger controller size        it achieves slightly lower quality than NLP  For the largest Mars rovers domain  Fig    c    EM achieves better solution quality         than NLP          However  EM also takes many more iterations to converge than for previous problems and hence  requires more time than NLP  EM is also much better than DEC BPI  which achieves a quality of      and takes even longer to converge  Fig    d        Conclusion and future work  We present a new approach to solve DEC POMDPs using inference in a mixture of DBNs  Even a simple implementation of the approach provides good results  Extensive experiments show that EM is always better than DEC BPI and compares favorably with the stateof the art NLP solver  The experiments also highlight two potential drawbacks of the EM approach  the adverse effect of reward scaling on solution quality and slow convergence rate for large problems  We are currently addressing the runtime issue by parallelizing the algorithm  For example   and  can be propagated in parallel  Even updating each nodes parameters can                EM    EM    NLP    NLP    DEC BPI    DEC BPI                   a                                       Iteration   S          A        Y                        EM    EM    NLP    NLP    DEC BPI                                              Iteration   b   Policy Value      Time  sec  logscale   Policy Value                      Time  sec  logscale                EM    NLP    DEC BPI               c                      Iteration               S          A        Y                             EM    NLP    DEC BPI                       Iteration               d   Figure    Solution quality and runtime for box pushing  a     b  and Mars rovers  c     d   be done in parallel for each iteration  Furthermore  the structure of EMs update equations is very amenable to Googles Map Reduce paradigm      allowing each parameter to be computed by a cluster of machines in parallel using Map Reduce  Such scalable techniques will certainly make our approach many times faster than the current serial implementation  We are also investigating how a different scaling of rewards affects the convergence properties of EM  The main benefit of the EM approach is that it opens up the possibility of using powerful probabilistic inference techniques to solve decentralized planning problems  Using a graphical DBN structure  EM can easily generalize to richer representations such as factored or hierarchical controllers  or continuous state and action spaces  Unlike the existing techniques  EM can easily extend to larger multi agent systems with more than   agents  The ND POMDP model      is a class of DECPOMDPs specifically designed to support large multiagent systems  It makes some restrictive yet realistic assumptions such as locality of interaction among agents  and transition and observation independence  EM can naturally exploit such independence structure in the DBN and scale to larger multi agent systems  something that current infinite horizon algorithms fail to achieve  Hence the approach we introduce offers great promise to overcome the shortcomings of the prevailing approaches to multi agent planning   Acknowledgments Support for this work was provided in part by the National Science Foundation Grant IIS         and by the Air Force Office of Scientific Research Grant FA                 
 Computing maximum a posteriori  MAP  estimation in graphical models is an important inference problem with many applications  We present message passing algorithms for quadratic programming  QP  formulations of MAP estimation for pairwise Markov random fields  In particular  we use the concaveconvex procedure  CCCP  to obtain a locally optimal algorithm for the non convex QP formulation  A similar technique is used to derive a globally convergent algorithm for the convex QP relaxation of MAP  We also show that a recently developed expectationmaximization  EM  algorithm for the QP formulation of MAP can be derived from the CCCP perspective  Experiments on synthetic and real world problems confirm that our new approach is competitive with maxproduct and its variations  Compared with CPLEX  we achieve more than an order ofmagnitude speedup in solving optimally the convex QP relaxation      INTRODUCTION  Probabilistic graphical models provide an effective framework for compactly representing probability distributions over high dimensional spaces and performing complex inference using simple local update procedures  In this work  we focus on the class of undirected models called Markov random fields  MRFs   Wainwright and Jordan         A common inference problem in this model is to compute the most probable assignment to variables  also called the maximum a posteriori  MAP  assignment  MAP estimation is crucial for many practical applications in computer vision and bioinformatics such as protein design  Yanover et al         Sontag et al         among others  Computing  Shlomo Zilberstein Department of Computer Science University of Massachusetts Amherst shlomo cs umass edu  MAP exactly is NP hard for general graphs  Thus approximate inference techniques are often used  Wainwright and Jordan        Sontag et al          Recently  several convergent algorithms have been developed for MAP estimation such as tree reweighted max product  Wainwright et al         Kolmogorov        and max product LP  Globerson and Jaakkola        Sontag et al          Many of these algorithms are based on the linear programming  LP  relaxation of the MAP problem  Wainwright and Jordan         A different formulation of MAP is based on quadratic programming  QP   Ravikumar and Lafferty        Kumar et al          The QP formulation is an attractive alternative because it provides a more compact representation of MAP  In a MRF with n variables  k values per variable  and  E  edges  the QP has O nk  variables whereas the LP has O  E k     variables  The large size of the LP makes off the shelf LP solvers impractical for several real world problems  Yanover et al          Another significant advantage of the QP formulation is that it is exact  However  the QP formulation is non convex  making global optimization hard  To remedy this  Ravikumar and Lafferty        developed a convex QP relaxation of the MAP problem  Our main contribution is the analysis of the QP formulations of MAP as a difference of convex functions  D C   problem  which yields efficient  graph based message passing algorithms for both the non convex and convex QP formulations  We use the concaveconvex procedure  CCCP  to develop the message passing algorithms  Yuille and Rangarajan         Motivated by geometric programming  Boyd et al          we present another QP based formulation of MAP and solve it using the CCCP technique  The resulting algorithm is shown to be equivalent to a recently developed expectation maximization  EM  algorithm that provides good performance for large MAP problems  Kumar and Zilberstein         The CCCP approach  however  is more flexible than EM and makes it easy to incorporate additional constraints that can   tighten the convex QP  Kumar et al          All the developed CCCP algorithms are guaranteed to converge to a local optimum for non convex QPs  and to the global optimum for convex QPs  All the algorithms also provide monotonic improvement in the objective   Nonetheless  for several problems  a local optimum of this QP provides a good solution as we will show empirically  This was also observed by Kumar and Zilberstein          We experiment on synthetic benchmarks and realworld protein design problems  Yanover et al          Against max product  Pearl         CCCP provides significantly better solution quality  sometimes more than     for large Ising graphs  On the real world protein design problems  CCCP achieves near optimal solution quality for most instances  and is significantly faster than the max product LP method  Sontag et al          Ravikumar and Lafferty        proposed to solve the convex QP relaxation using standard QP solvers  Our message passing algorithm for this case provides more than an order of magnitude speedup against the state of the art QP solver CPLEX           QP FORMULATION OF MAP  A pairwise Markov random field  MRF  is described using an undirected graph G    V  E   A discrete random variable xi with a finite domain is associated with each node i  V of the graph  Associated with each edge  i  j   E is a potential function ij  xi   xj    The complete assignment x has the probability   X   p x     exp ij  xi   xj   ijE  The MAP problem consists of finding the most probable assignment to all the variables under p x     This is equivalent to finding the assignment x that maxP imizes the function f  x      ijE ij  xi   xj    We assume w l o g  that each ij is nonnegative  otherwise a constant can be added to each ij without changing the optimal solution  Let pi be the marginal probability associated with each MRF node i  V   The MAP quadratic programming  QP  formulation  Ravikumar and Lafferty        is given by  X X max pi  xi  pj  xj  ij  xi   xj       p  subject to  ijE xi  xj  X  pi  xi        pi  xi      i  V  xi  The above QP is compact even for large graphical models and has simple linear constraints  O nk  variables and n normalization constraints where n    V   and k is the domain size  Ravikumar and Lafferty        also show that this formulation is exact  That is  the global optimum of the above QP will maximize the function f  x    and an integral MAP assignment can be extracted from it  However this formulation is non convex  making global optimization hard   The Concave Convex Procedure  The concave convex procedure  CCCP   Yuille and Rangarajan        is a popular approach to optimize a general non convex function expressed as a difference of two convex functions  We use this method to obtain message passing algorithms for QP formulations of MAP  We describe it here briefly  Consider the optimization problem  min g x    x          where g x    u x   v x  is an arbitrary function with u   v being real valued convex functions and  being a convex set  The CCCP method provides an iterative procedure that generates a sequence of points xl by solving the following convex program  xl     arg min u x   xT v xl     x          Each iteration of CCCP decreases the objective g x  and is guaranteed to converge to a local optimum  Sriperumbudur and Lanckriet              Solving MAP QP Using CCCP  We first show how the CCCP framework can be used to solve the QP in Eq       We adopt the convention that a MAP QP always refers to the QP in Eq       the convex variant of this QP shall be explicitly differentiated when addressed later  Consider the following functions u  v  u p     X X ij  xi   xj     p i  xi     p j  xj     ij x x       X X ij  xi   xj      pi  xi     pj  xj     ij x x       i  v p     i  j  j  The above functions are convex because the quadratic functions f  z    z   and f  y  z     y   z   are convex  and the nonnegative weighted sum of convex functions is also convex  Boyd and Vandenberghe        Ch      It can be easily verified that the QP in Eq      can be written as minp  u p   v p   with normalization and nonnegativity constraints defining the constraint set   Intuitively  we used the simple identity  xy    x    y       x   y     We also negated the objective function to convert maximization to minimization  For simplicity  we denote the gradient v p xi   by xi v  X X X X xi v   pi  xi   ij  xi   xj    ij  xi   xj  pj  xj   jNe i  xj  jNe i  xj   The first part of the above equation involves a local computation associated with an MRF node and the second part defines the messages j from neighbors j  N e i  of node i  It can be made explicit as follows  X X X  xi    ij  xi   xj    j  xi     ij  xi   xj  pj  xj   jNe i  xj  xj  X  xi v  pi  xi   xi      j  xi         jN e i   CCCP iterations  Each iteration of CCCP involves solving the convex program of Eq       First we write the Lagrangian function involving only the normalization constraints  later we address the nonnegativity inequality constraints  l v denotes the gradient from the previous iteration l  X X ij  xi   xj     p i  xi     p j  xj   L p        ij xi xj XX X X  pi  xi  lxi v   i   pi  xi           i  xi  xi  i  Solving for the first order optimality conditions p L x        and  L x         we get the solution  pl   i  xi      lxi v   i        xi    i   P       xi  xi     X xi  lxi v  xi              Nonnegativity constraints  Nonnegativity constraints in the MAP QP are inequality constraints which are harder to handle as the Karush KuhnTucker  KKT  conditions include the nonlinear complementary slackness condition  j p j  xj        Boyd and Vandenberghe         We can use interior point methods  but they lose the efficiency of graph based message passing  Fortunately  we show that for MAP QP  the KKT conditions are easily satisfied by incorporating an inner loop in the CCCP iterations  Alg    shows the complete message passing procedure to solve MAP QPs  Each outer loop corresponds to solving the CCCP iteration of Eq      and is run until the desired number of iterations is reached  The messages  are used for computing the gradient v as in Eq       The inner loop corresponds to satisfying the KKT conditions including the nonnegativity inequality constraints  Intuitively  the strategy to handle inequality constraints is as highlighted in  Bertsekas        Sec          considering all possible combinations of inequality constraints being active  pi  xi        or inactive  pi  xi        and solving the resulting KKT conditions  which is easier as they become linear equations  If the resulting solution satisfies the KKT conditions of the original problem  then we have a valid solution for the original optimization problem  Of course      Graph based message passing for MAP estimation input  Graph G    V  E  and potentials ij per edge   outer loop starts repeat foreach node i  PV do ij  xj    xi pi  xi  ij  xi   xj   Send message ij to each neighbor j  Ne i  foreach node i  V do zeros     inner loop starts repeat Set pi  xi      xi  zeros Calculate pi  xi   using Eq       xi    zeros zeros  zeros   xi   pi  xi        until all beliefs pi  xi      until stopping criterion is satisfied return  The decoded complete integral assignment  this is highly inefficient for the general case  But fortunately for the MAP QP  we show that the inner loop of Alg    recovers the correct solution and the Lagrange multipliers are computed efficiently for the convex program of Eq       We describe it below  The inner loop includes local computation to each MRF node i and does not require message passing  Intuitively  the set zeros tracks all the settings x i of the variable xi for which pi  x i   was negative in any previous inner loop iteration  It then clamps all such beliefs to   for all future iterations  Then the beliefs for the rest of the settings of xi are computed using Eq       The new Lagrange multiplier i  which corre    sponds to the condition P  L x           is calculated using the equation xi  x  pi  xi        i  Lemma    The inner loop of Alg    terminates with worst case complexity O k      and yields a feasible point for the convex program of Eq       Proof  The size of the set zeros increases with each iteration  therefore the inner loop must terminate as each variables domain is finite  With the domain size of a variable being k  the inner loop can run for at most k iterations  Computing new beliefs within each inner loop iteration also requires O k  time  Thus the worst case total complexity is O k      The inner loop can terminate only in two ways  before iteration k or at iteration k  If it terminates before iteration k  then it implies that all the beliefs pi  xi   must be nonnegative  The normalization constraints are always enforced by the Lagrange multipliers i s  If it terminates during iteration k  then it implies that k    settings of the variable xi are clamped to zero as the size of the set zeros will be exactly k     The size cannot be k because that would make all the beliefs equal to zero  making it impossible to satisfy the normalization constraint  i will not allow this  The size   cannot be smaller than k    because the set zeros grows by at least one element during each previous iteration  Therefore the only solution during iteration k is to set the single remaining setting of the variable xi to   to satisfy the normalization and nonnegativity constraints simultaneously  Therefore the inner loop always yields a feasible point upon termination  Empirically  we observed that even for large protein design problems with k        the number of required inner loop iterations is below     far below the worst case complexity  For a fixed outer loop l  let the inner loop iterations be indexed by r  Lemma    The Lagrange multiplier corresponding to the normalization constraint for a MRF variable xi always increases with each inner loop iteration  Proof  Each inner loop iteration r computes P a new Lagrange multiplier ri for the constraint i pi  xi       using Eq       We show that r     ri   For the inner i loop iteration r  some of the computed beliefs must be negative  otherwise the inner loop must have terminated  Let x i denote those settings of variable xi for which pi  x i       in iteration r  From the normalization constraint for iteration r  we get  X lx v  ri i xi   xi              We used the explicit representation of pi  xi   from Eq       Since pi  x i   are negative  we get  X lx v  ri i xi  x i   xi              The belief for all such x i will become zero for the next inner loop iteration r      From the normalization constraint for iteration r      we get  X lx v  r   i i xi  x i   xi              We used a slight simplification in the above equations as we ignored the effect of previous iterations  before iteration r  However  it will not change the conclusion as all the beliefs that were clamped to zero earlier  before iteration r  shall remain so for all future iterations  Note that xi and  do not depend on the inner loop iterations  Subtracting Eq     from Eq      X    r    ri           i  xi   xi  x  i  Since we assumed that all potential functions ij are nonnegative  we must have  r    ri        Hence i r   r i   i and the lemma is proved   Theorem    The inner loop of Alg    correctly recovers all the Lagrange multipliers for the equality and inequality constraints for the convex program of Eq       thus solving it exactly  Proof  Lemma   shows that the inner loop provides a feasible point of the convex program  We now show that this point also satisfies the KKT conditions  thus is the optimal solution  The KKT conditions for the normalization constraints are always satisfied during the belief updates  see  Eq        The main task is to show that for the inequality constraint pi  xi       the KKT conditions hold  That is  if pi  xi        then the Lagrange multiplier i  xi       and if pi  xi        then i  xi        By using the KKT condition pi  xi   L p                 we get  X X ij  xi   xj  pi  xi  lxi v   i  i  xi       jN e i  xj       The main focus of the proof is on the beliefs for elements in the set zeros  Let us focus on the end of an inner loop iteration r  when a new element x i is added to zeros because its computed belief pi  x i        Using Eq       we know that pi  x i     pi  x i    lx  vri i   x i      Because      we get  ri   lx i v        For all future iterations of the inner loop  pi  x i   will be set to zero  Therefore the KKT condition for iteration r     mandates that r    x i       Setting pi  x i       i in Eq        we get  r    x i     r    lx i v i i        We know from Lemma   that r     ri   Combining i r     this fact with Eq        we get i  xi        thereby satisfying the KKT condition  Note that the only component depending on the inner loop in the above condition is ri   x i is fixed during each inner loop  Furthermore  for all future inner loop iterations  the KKT conditions for all elements x i s in the set zeros will be met due to the increasing nature of the multiplier i   Therefore  when the inner loop terminates  we shall have correct Lagrange multipliers  satisfying     for all the elements of the set zeros  For the rest of the elements  the multiplier       satisfying all the KKT conditions  As the first order KKT conditions are both necessary and sufficient for optimality in convex programs  Bertsekas        Sec          the inner loop solves exactly the convex program in Eq              Solving Convex MAP QP Using CCCP  Because the previous QP formulation of MAP is nonconvex  global optimization is hard  To remedy this  Ravikumar and Lafferty        developed a convex QP relaxation for MAP  which performed well on their benchmarks  Recently  Kumar et al         showed that the convex QP relaxation is also equivalent to the second order cone programming  SOCP  relaxation  Ravikumar and Lafferty        proposed to solve such QP using standard QP solvers  We show using CCCP that this QP relaxation can be solved efficiently using graph based message passing  and the resulting algorithm converges to the global optimum of the relaxed QP  Experimentally  we found the resulting message passing algorithm to be highly efficient even for large graphs  outperforming CPLEX by more than an order of magnitude  The relaxed QP is described as follows  XX XX pi  xi  di  xi    pi  xi  pj  xj  ij  xi   xj   max p  ij xi  xj  xi  i    XX i  p i  xi  di  xi          xi  The relaxation is based on adding a diagonal term  di  xi    for each variable xi   Note that under the integrality assumption pi  xi     p i  xi    thus the first and last terms cancel out  resulting in the original MAP QP  The diagonal term is given by  di  xi      X X  ij  xi   xj    jN e i  xj     Consider the convex function u p  represented as  X X ij  xi   xj     X   pi  xi  di  xi   p i  xi     p j  xj       i x ij x x i  j  i  and the convex function v p  represented as  X X ij  xi   xj   ij xi xj        X pi  xi   pj  xj     pi  xi  di  xi   i xi  The above two functions are the same as the original QP formulation  except for the added diagonal terms di  xi    It can be easily verified that the relaxed QP objective can be written as minp  u p   v p   subject to normalization and nonnegativity constraints  Note that the maximization of the relaxed QP is converted to minimization by negating the objective  The gradient required by CCCP is given by  X X xi v   pi  xi   xi     ij  xi   xj  pj  xj     di  xi   jN e i  xj  Notice the close similarity with the MAP QP case in Eq       The only additional term is di  xi    which  needs to be computed only once before message passing begins  The messages for the relaxed QP case are exactly the same as the  messages for the MAP QP  The Lagrangian corresponding to the convex program of Eq      is similar to thePMAP QP case  see Eq       with an additional term i xi p i  xi  di  xi    The constraint set  includes the normalization and nonnegativity constraints as for the MAP QP case  Solving for the optimality conditions p L p        and  L p         we get the new beliefs as follows  pil    xi      lxi v  i  di  xi      xi          The Lagrange multiplier i for the normalization constraint can be calculated by using the equation P xi pi  xi        The only difference from the corresponding Eq      for the MAP QP is the additional term di  xi   in the denominator  Thanks to these strong similarities  we can show that Alg    also works for the convex MAP QP with minor modifications  First  we calculate the diagonal terms di  xi   once for each variable xi of the MRF  The message passing procedure for each outer loop iteration remains the same  The second difference lies in the inner loop that enforces the nonnegativity constraints  The inner loop now uses Eq       instead of Eq      to estimate new beliefs pi  xi    The proof is omitted being very similar to the MAP QP case  Theorem    The CCCP message passing algorithm converges to a global optimum of the convex MAP QP  The result is based on the fact that CCCP converges to a stationary point of the given constrained optimization problem that satisfies the KKT conditions  Sriperumbudur and Lanckriet         Because the KKT conditions are both necessary and sufficient for convex optimization problems with linear constraints  Bertsekas         the result follows  We also highlight that a global optimum of the convex QP may not solve the MAP problem exactly  as the convex QP is a variational approximation to MAP that may not be tight  Nonetheless  it has shown to perform well in practice  Ravikumar and Lafferty              GP Based Reformulation of MAP QP  We now present another formulation of the MAP QP problem based on geometric programming  GP   A GP is a type of mathematical program characterized by objectives and constraints that have a special form  For details  we refer to  Boyd et al          While the QP formulation of MAP in Eq      is not exactly a GP  it bears a close resemblance  This allows us to transfer some ideas from GP  which we shall describe  We start with some basic concepts of GP    Definition    Let x            xn denote real positive variables  A real valued monomial function has the form f  x    cxa    xa       xann   where c     and ai     A posynomial function is a sum of one or more monoPK mials  f  x    k   ck x a k xa   k    xannk In a GP  the objective function and all inequality constraints are posynomial functions  The MAP QP  see Eq       satisfies this requirement  the potential function ij corresponds to ck and is positive  the ij     case can be excluded for convenience   node marginals pi  xi   are real positive variables  Since we already assumed marginals pi to be positive  nonnegativity inequality constraints are not required  In a GP  the equality constraints can only be monomial functions  This is not satisfied in MAP QP as normalization constraints are posynomials  Nonetheless  we proceed as in GP by converting the original problem using certain optimality preserving transformations  The first change is to let pi  xi     eyi  xi     where yi  xi   is an unconstrained variable  This is allowed as all marginals must be positive  The second change is to take the log of the objective function  because log is a monotonic function  this will not change the optimal solution  The reformulated MAP QP is shown below   X X     min   log exp yi  xi   yj  xj   log ij  xi   xj   ij xi  xj  subject to   X  eyi  xi       i  V  xi  This nonlinear program has the same optimal solutions as the original MAP QP  As log sum exp is convex  the objective function of the above problem is concave  Note that we are minimizing a concave function that can have multiple local optima  We again solve it using CCCP  Consider the function u y      and v y  as the objective of the above program  but without the negative sign  The gradient required by CCCP is    P P jN e i  xjij  xi   xj   exp yi  xi   yj  xj   l        yi v   P P ij xi  xj ij  xi   xj   exp yi  xi   yj  xj   The Lagrangian corresponding to Eq      with constraint set including only normalization constraints is  X X X L y       yi  xi  lyi v   i   eyi  xi       i xi  i  xi  Using the first order optimality condition  we get    lyi v exp yi  xi     i        We note that the denominator of Eq       is a constant for each yi  xi    Therefore we represent it using cl   Re   substituting pi  xi     eyi  xi   and lyi v in Eq        P P jN e i  xj ij  xi   xj  pi  xi  pj  xj     pi  xi     cl i where p i  xi   is the new parameter for the current iteration  and parameters without asterisk  on the R H S   are from the previous iteration  Since cl i is also a constant  we can replace them by a normalization constant Ci to get the final update equation  P P jN e i  xj ij  xi   xj  pj  xj     pi  xi     pi  xi   Ci The message passing process for this version of CCCP is exactly the same as that for MAP QP and convex QP  This version does not require an inner loop as all the node marginals remain positive using such updates  This update process is also identical to the recently developed message passing algorithm for MAP estimation that is based on expectation maximization  EM  rather than CCCP  Kumar and Zilberstein         However  CCCP provides a more flexible framework in that it handled the nonconvex and convex QP in a similar way as shown earlier  Furthermore  the CCCP framework allows for additional constraints to be added to the convex QP to make it tighter  Sriperumbudur and Lanckriet         In sum  we have shown that the concave convex procedure provides a unifying framework for the various quadratic programming formulations of the MAP problem  Each iteration of CCCP can be easily implemented using graph based message passing  Interestingly  the messages exchanged for all the QP formulations we discussed remain exactly the same  the differences lie in how new node marginals are computed using such messages      EXPERIMENTS  We now present an empirical evaluation of the CCCP algorithms  We first report results on synthetic graphs generated using the Ising model from statistical physics  Baxter         We compare max product  MP  and the CCCP message passing algorithm for the QP formulation of MAP  We generated  D nearest neighbor grid graphs for a number of grid sizes  ranging between        to         and varying values of the coupling parameter  All the variables were binary  The node potentials were generated by sampling from the uniform distribution U              The coupling strength  dcoup   for each edge was sampled from U     following the mixed Ising model  The binary edge potential ij was defined as follows    dcoup xi   xj ij  xi   xj     dcoup xi    xj                                                                                                                                                                                                                                                                                        CCCP CCCP                         CCCP             MP MP MP                CCCP CCCP CCCP             MP                               MP MP                                                                                  a   a  Ising    a        Ising  b  Ising    b        Ising Ising      c     Ising                     b           c               Ising                     Ising                     Ising    c   a   b   c  Ising       Ising       Ising       Ising a        Ising b         c     Ising c         a  Ising  a         b  Ising  b         c  Ising                                                                                                                                                                                                                                             CCCP CCCP CCCP                                        CCCP CCCP CCCP                                                                                                      d   d  Ising    d          Ising               e  Ising    e          Ising              f  Protein Protein  f  instances design Protein instances design   e      f design              instances                          Ising             Ising                       Ising        Ising         f instances Protein design instances Ising d         Ising e          f design Protein design instances  d  Ising  d          e  Ising  e          f  Protein   d    e    f   Figure     a    e  show quality comparison between max product  MP  and CCCP for Ising graphs with varying number of nodes              The x axis denotes the coupling parameter   y axis shows solution quality   f  shows the solution quality CCCP achieves as a percentage of the optimal value  y axis  for different protein design instances  x axis    For every grid size and each setting of the coupling strength parameter   we generated    instances by sampling dcoup per edge  For each instance  we considered the best solution quality of    runs for both max product and CCCP  We then report the average quality of the    instances achieved for each parameter   Both max product and CCCP were implemented in JAVA and ran on a    GHz CPU  Max product was allowed      iterations and often did not converge  whereas CCCP converged within     iterations  Fig    ae  show solution quality comparisons between MP and CCCP  For        graphs  Fig    a    both CCCP and MP achieve similar solution quality  The gain in quality provided by CCCP increases with the size of the grid graph  For        grids  the average gain in solution quality    QCCCP  QM P   QM P    for each coupling strength parameter  is over      For         Fig    c   grids  the gain is above     for each parameter   for        grids it is     and for        grids it is      Overall  CCCP provides much better performance than max product over these Ising graphs  And unlike max product  CCCP monotonically increases solution quality and is guaranteed to converge  A detailed performance evaluation of the convex QP is provided  Ravikumar and Lafferty         As such Ising graphs have relatively small QP representation  the CCCP message passing method and CPLEX had similar runtime for the convex QP  We also experimented on the protein design benchmark  total of    instances   Yanover et al          In these problems  the task is to find a sequence of amino acids that is as stable as possible for a given backbone structure of protein  This problem can be modeled using a pairwise Markov random field  These problems are particularly hard and dense with up to     variables  each with a large domain size of up  to     values  Fig    f  shows the   of the optimal value CCCP achieves against the best upper bound provided by the LP based approach MPLP  Sontag et al          MPLP has been shown to be very effective in solving exactly the MAP problem for several real world problems  However for these protein design problems  due to the large variable domains  its reported mean running time is     hours  Sontag et al          As Fig    f  shows  CCCP achieves nearoptimal solutions  on average within       of the optimal value  A significant advantage of CCCP is its speed  it converges within      iterations for all these problems and requires      seconds for the largest instance  much faster than MPLP  The mean running time of CCCP was     seconds for this dataset  Thus CCCP can prove to be quite effective when fast  approximate solutions are desired  The main reason for this speedup is that CCCPs messages are easier to compute than MPLPs as also highlighted in  Kumar and Zilberstein         Compared to the EM approach of  Kumar and Zilberstein         CCCP provides better solution quality  EM achieved     of the optimal value on average  while CCCP achieves        The overhead of the inner loop in CCCP is small against EM which takes      seconds for the largest instance  while CCCP takes      seconds  We also tested CCCP on the protein prediction dataset  Yanover et al          The problems in this dataset are much smaller than those in the protein design dataset  and both max product and MPLP achieve good solution quality  CCCPs performance was worse in this case  partly due to the local optima present in the nonconvex QP formulation of MAP  The convex QP formulation was not tight in this case  Fig    a  shows runtime comparison of CCCP against CPLEX for the convex QP for the    largest protein               relaxation  CCCP provided more than an order ofmagnitude speedup over the state of the art QP solver CPLEX  These results offer a powerful new way for solving efficiently large MAP estimation problems                   Acknowledgments           Support for this work was provided in part by the NSF Grant IIS         and by the AFOSR Grant FA                                     a  Time comparison               b  Quality comparison  Figure     a  Time comparison of CCCP for convex QP against CPLEX for the largest    protein design instances  x axis   The y axis denotes TCCCP  TCP LEX as a percentage   b  denotes the signed quality difference QCCCP  QCP LEX   a higher value is better   design problems w r t  the number of graph edges  After trying different QP solver options available in CPLEX  we chose the barrier method which provided the best performance  As CPLEX was quite slow  we let CPLEX use   CPU cores with  GB RAM  while CCCP only used a single CPU  As this figure shows  CCCP is more than an order of magnitude faster than CPLEX even when it uses a single core  The longest CPLEX took was      seconds  whereas CCCP only took    seconds for the same instance  The mean running time of CPLEX was      seconds  for CCCP  it was    seconds  Surprisingly  CCCP converges in only    iterations to the optimal solution for all    problems  Fig    b  shows the signed quality difference between CCCP and CPLEX for the convex QP objective  CPLEX provides the optimal solution within some non zero    we used the default setting   This figure shows that even within    iterations  CCCP achieved a slightly better solution  The decoded solution quality provided by the convex QP was decent  within     of the optimal value  but not as high as the CCCP method for the nonconvex QP      CONCLUSION  We presented new message passing algorithms for various quadratic programming formulations of the MAP problem  We showed that the concave convex procedure provides a unifying framework for different QP formulations of the MAP problem represented as a difference of convex functions  The resulting algorithms were shown to be convergent  to a local optimum for the nonconvex QP and to the global optimum of the convex QP  Empirically  the CCCP algorithm was shown to work well on Ising graphs and real world protein design problems  The CCCP approach provided much better solution quality than max product for Ising graphs and converged significantly faster than max product LP for protein design problems  while providing near optimal solutions  For the convex QP  
