 In this paper  we present the DifferenceBased Causality Learner  DBCL   an algorithm for learning a class of discrete time dynamic models that represents all causation across time by means of difference equations driving change in a system  We motivate this representation with real world mechanical systems and prove DBCLs correctness for learning structure from time series data  an endeavour that is complicated by the existence of latent derivatives that have to be detected  We also prove that  under common assumptions for causal discovery  DBCL will identify the presence or absence of feedback loops  making the model more useful for predicting the effects of manipulating variables when the system is in equilibrium  We argue analytically and show empirically the advantages of DBCL over vector autoregression  VAR  and Granger causality models as well as modified forms of Bayesian and constraintbased structure discovery algorithms  Finally  we show that our algorithm can discover causal directions of alpha rhythms in human brains from EEG data      INTRODUCTION AND MOTIVATION  In the past    years in AI  the practice of learning causal models from data has received considerable attention  cf   Pearl and Verma        Cooper and Herskovits        Spirtes et al          Existing methods   Also Department of Biomedical Informatics  School of Medicine  University of Pittsburgh   Also Faculty of Computer Science  Bialystok University of Technology  Wiejska   A         Bialystok  Poland   Marek J  Druzdzel marek sis pitt edu Decision Systems Laboratory School of Information Sciences University of Pittsburgh Pittsburgh  PA         USA  are based on the formalism of structural equation models  SEMs   which originated in the econometrics literature over    years ago  cf   Strotz and Wold         and Bayesian networks  Pearl        which started the paradigm shift of graphical models in AI and machine learning    years ago  These methods have predominately focused on the learning of equilibrium  static  causal structure  and have recently gained inroads into mainstream scientific research  especially in biology  cf   Sachs et al          Despite the success of these static methods  many realworld systems are dynamic in nature and are accurately modeled by systems of simultaneous differential equations  Temporal causality  in general  has been studied extensively in econometrics over the past four decades  Granger causality and vector autoregression  VAR  methods have become very influential  cf   Granger        Engle and Granger        Sims         In AI  there has been work on learning Dynamic Bayesian Networks  DBNs   Friedman et al         and modified Granger causality  Eichler and Didelez         None of these models explicitly take into account the fact that many dynamic systems are based on differential equations  This makes their representations overly general for such systems  allowing arbitrary causal relations across time  In this paper  we show that differential equations impose strict constraints on crosstemporal causal edges  and we present a method that is capable of exploiting that fact  This paper considers Difference Based Causal Models  DBCMs   a class of discrete time dynamic models inspired by Iwasaki and Simon        that models all causation across time by means of difference equations driving change in the system  This paper presents the first method to learn DBCMs from data  the Difference Based Causality Learner  DBCL   This algorithm treats differences as latent variables and conducts an efficient search to find them in the course of constructing a DBCM  This method exploits the fact that unknown derivatives have fixed relationships to   known variables and so are easier to find than latent variables in general  We prove that DBCL correctly learns DBCMs given faithfulness and a conditional independence oracle  and show empirically that it is also robust in the sense of avoiding unnecessary calculations of higher order derivatives  thus preventing mistakes due to numerical errors  We show that compared to Granger causality and VAR models  DBCL output is much more parsimonious and informative  We also show empirically that it outperforms variants of the PC algorithm and greedy Bayesian search algorithms that have been modified to assume DBCM structure  Finally  we prove that DBCL will always identify instantaneous feedback loops when the underlying system is a DBCM  making it easier to detect when an equilibrated model will be causal  To our knowledge  no other method for causal discovery is guaranteed to identify the presence or absence of feedback loops      for the harmonic oscillator are given by Newtons  nd law describing the acceleration a of the mass under the forces  due to the weight  due to the spring  Fx   and due to viscosity  Fv   acting on the block  These forces instantaneously determine a  furthermore  they indirectly determine the values of all integrals of a  in particular the velocity v and the position x  of the block  The longer time passes  the more influence the forces have on those integrals  Writing this continuous time system as a discrete time model  v and x are approximately determined by the difference equations  v t     v t   at t and xt     xt   v t t  resulting in the cross temporal causal links in the graph of Figure   a  and  b   Thus  differential equation systems imply cross temporal arcs with a regular structure  DBCMs assume that all cross temporal arcs are of this form   DIFFERENCE BASED CAUSAL MODELS  Stated briefly  a DBCM is a discrete time model  based on SEMs  with a graphical interpretation very similar to DBNs  Contemporaneous causation is allowed  i e   like DBNs  variables can be caused by other variables in the same time slice  The defining characteristic of a DBCM is that all causation across time is due to a derivative  e g   x  causing a change in its integral  e g   x   This cross temporal restriction makes DBCMs a subset of causal models as defined by Pearl        and structural equation models similar to those discussed    years ago by Strotz and Wold         DBCM like models were discussed by Iwasaki and Simon        and Dash              to analyze causality in dynamic systems  but to date no algorithm exists to learn them from data  As an example  consider the set of equations describing the motion of a damped simple harmonic oscillator  SHO   A block of mass m is suspended from a spring in a viscous fluid and several different forces are acting on it  such as the forces resulting from the spring and that of gravity  The harmonic oscillator is an archetypal dynamic system  ubiquitous in nature  Although a linear system  it can form a good approximation to many nonlinear systems close to equilibrium  Furthermore  the F   ma relationship is a canonical example of contemporaneous causation  applying a force to cause a body to accelerate instantly  Thus  although this system is simple  it illustrates many important points  Causal interactions even in such a simple system are problematic when using standard representations for causality  as we will show shortly  Like all mechanical systems  the equations of motion   a    b    c   Figure     a  The causal graph of a DBCM for the mass spring system is always first order Markovian   b  The shorthand graph of  a  using dashed edges to indicate cross temporal causation   c  The unrolled graph with all vs and as marginalized out is infiniteorder Markovian  More formally  DBCMs are a restricted form of structural equation models  SEMs   We first review these models  and then discuss our additional constraints  We use the notation  A   B   C   to indicate that variable A is conditionally independent on B given a set of variables C   Definition    structural equation model   A SEM is a pair hV   E i  where V    V            Vn   is a set of variables  and E    E            En   is a set of equations such that each Ei  E can be written in the form  Vi    fi  W i   i   where W i  V   Vi is called the set of causes  or parents  of Vi   denoted by Pa Vi    and the i are noise terms such that  i   j    i    j  The noise terms i are intended to represent the set of causes of each variable that are not directly accounted for in the model  Historically  SEMs use linear equations with normally distributed noise terms  A SEM defines a directed graph such that each variable X  V is represented by a node and there is an edge Y  X for each Y  Pa X   In this way  SEMs can model relations between variables in a very   general way  Furthermore  SEMs can be used to represent causality in dynamic systems for a discretetime setting by defining the set of variables to be a time series  V   V    V    V            where V t    V t           Vnt   denotes the set of n variables at time t  We call SEMs that partition their variables according to time indices dynamic SEMs  DBCMs are a restricted form of dynamic SEMs  They assume that all causation across time is due to instantaneous causation of the difference of some variables  Definition    Difference variable   Let V   V    V    V          be a time series  The n th order difference variable n V t of variable V t  V t is defined recursively as  n V t   n  V t    n  V t    with   V t   V t    In particular    V t   V t    V t   which we sometimes shorten to V t   When we invert the difference equation to give the value of V t   in terms of its past value and its difference  we call it the integral equation of V t     I e   the integral equation of V t   is V t     V t   V t   Integral equations are identities and so are always deterministic  The graphs of Figure   use the standard notation from physics such that the derivative of x is velocity  v    x  and the derivative of velocity is acceleration  a    x   A DBCM is a dynamic SEM in which all causation across time is due to the presence of integral equations  Because all DBCMs are based on difference equations that do not vary from time to time  we can restrict ourselves to partitioning the variables into two time slices         where the  th time slice determines the initial conditions and  st time slice determines the transitions  Definition    Difference Based Causal Model   A DBCM M is a dynamic SEM S   hV   E i with V   V    V   and E   E    E   such that there exists a cross temporal parent of some variable Vi   V   if and only if Ei  is the integral equation for variable Vi    This definition implies that the parent set of a variable X   that has parents in the previous time slice is Pa X        X     X      If this is the case  we call X an integral variable  An integral variable X is part of a chain of causation j X    j  X           X j   We call the highest derivative  j X  of this chain the prime variable of X  which we will also denote as Prime X   In the example of Figure    variables x and v are integral variables  and a is the prime variable of x and v  Finally  any variable that is not an integral variable and is not a prime variable is called a static variable  This term does not imply that the variable is  not changing from time step to time step  because it might have a causal ancestor that is part of an integration chain  However  we use this term to emphasize that the change is not due to a dynamic process involving these variables  In Figure    m  Fv and Fx are static variables  The definition of DBCMs does not require that the contemporaneous structure be acyclic  however  in this paper we only consider acyclic DBCMs  It should be emphasized that this assumption does not restrict us to non feedback systems  rather  this assumption implies that all feedback requires time to occur and thus will only occur through an integral variable  E g   the position x of the mass in the SHO causes an instantaneous spring force Fx that results in an instantaneous acceleration a  Over time  a causes a change in x via integration  Thus we have the feedback loop  x   Fx   a   v    x    Although the instantaneous part  x   Fx   a    is acyclic  this is still a feedback system  Fb X  is the set of instantaneous descendants of X which are also ancestors of Prime X   in this example  Fb x     Fx     Another interpretation is that by rejecting instantaneous loops we assume that the observation time scale is much smaller than any time scale of the system dynamics  Since the contemporaneous structure is not changing over time  the equations in E   and E   are partially overlapping  those that correspond to contemporaneous structure are identical in both sets  but E   contains initial conditions for all integral variables  and E   contains integral equations for integral variables  The graph in Figure   b  is a compressed version of the fully unrolled DBCM  The cycle in the graph caused by the dashed links is really an acyclic structure extending across time       COMPARISON OF REPRESENTATIONS  Dynamic SEMs  like Granger causality and VAR models  allow arbitrary edges to exist across time  For many real physical systems this representation is too general  DBCMs  by contrast  assume that all causation works in the same way as in mechanical systems  This restriction represents a tradeoff between expressibility and tractability  On one hand  DBCMs are only able to represent mechanical systems that are first order Markovian  On the other hand  DBCMs are in principle easier to learn because  even when the derivatives are unobserved in the data as we will assume  e g   in the previously introduced example we do not include v and a  or any other derivative in the data set   at least we know something about these latent variables   that are required to make the system Markovian  When confronted with data that was generated by differential equations with some derivatives missing  the distinction between DBCL and the other approaches becomes glaring  Whereas  as we will show shortly  DBCL attempts to search for and identify the latent derivative variables  other approaches would try to marginalize them out  One might suspect that there is not much difference  For example  one might expect that a second order differential equation would simply result in a second order Markov model when the derivatives are marginalized out  Unfortunately that is not the case  because the causation among the derivatives forms an infinite chain into the past  Thus  any approach that tries to marginalize out the derivatives must include infinite edges in the model  for example  such as those in Figure   c   In the harmonic oscillator system with all derivatives marginalized out  all parents of a in time slice i of the DBCM are parents of x for all time slices j   i      Thus  the benefits of using the DBCM representation are not merely computational  but in fact  without learning the derivatives directly  the correct model does not have a finite representation      DBCM LEARNING  The DBCM Learning problem can be posed in the following way  Given time series data over a set of variables V   derive a DBCM over a set of variables V  V   where V contains differences of variables that are derived from the original data  In other words  DBCL does not assume all relevant derivatives or the order of those derivatives are known  Instead  it treats these missing derivatives as latent variables and tries to discover them  We assume that  aside from these derivatives  there are no other latent confounding variables present  Note  for example  that this assumption also rules out the existence of structures of the form X  X  Y   where X and X are latent and Y is observable  because the X process forms a latent chain across time that can confound Y at different times  DBCL relies on the standard assumption of faithfulness  Spirtes et al          Faithfulness is the converse of the Markov condition  and it is the critical assumption that allows structure to be uncovered from independence relations  However  when a dynamic system goes through equilibrium  by definition  faithfulness is violated  For example  if the motion of the block in the simple harmonic oscillator reaches equilibrium then  by definition  the equation a    Fx   Fv   mg  m becomes     Fx  Fv  mg  This means that the values of the forces acting on the block are no longer correlated with the value of a  even though they are direct causes  of a  Thus  by assuming faithfulness  we are implicitly assuming that no equilibrations have occurred       THE ALGORITHM  DBCL consists of two steps      detecting prime  and integral  variables  V    and     learning the contemporaneous structure  The first step is achieved by calculating numerical derivatives of all variables and then deciding which ones should be prime variables  This is based on the following theorem  which exploits the fact that only prime variables can always be made independent across time by conditioning on variables in V    Theorem    detecting prime variables   Let I be the set of conditional independence relations implied by faithfulness applied to a DBCM M   hV   E i with V   V    V   and E   E    E     Let j X   denote the j th order difference of some X    V     Then j X   is the prime variable of X   if and only if it can be d separated from itself in the future and none of the lower order differences can be d separated  i e      there exists a W  V  j   j     X    X   W    I  and  such  that     there exists no set W    V   such that  k X     k X     W      I  for all k   j  Once we have found V   the set of integral and prime variables in the model  learning contemporaneous structure over the two time slice model becomes a problem of learning a time series model from causally sufficient data  i e   there do not exist any latent common causes   Theorem   shows that we can learn the contemporaneous structure from time series data despite the fact that data from time to time is not independent  This is because we know by construction that the integral variables will d separate the time slices  so all structure between variables can be obtained by conditioning only on variables in the same time slice  Theorem    learning contemporaneous structure   Let I be the set of conditional independence relations implied by faithfulness applied to a DBCM M   hV   E i  where V   V    V     There is an edge X    Y   if and only if     Either X   or Y   is not an integral variable  and       there exists no V    V      X     Y     such that    X     Y     V      I  In addition to having discovered the latent variables in the data  and the structure between non integral   variables  we also know that there can be no contemporaneous edges between two integral variables  and integral variables can have only outgoing contemporaneous edges  We can thus restrict the search space of causal structures  Theorems   and   together with these constraints form the basis of the DBCL algorithm  Algorithm    DBCL  sketch    Input  a maximum difference parameter kmax      a time series dataset D over a set of variables V         V  V    Output  a set V of prime and integral variables    and a partially directed graph G over V   V  V      Find relevant latent derivatives  Theorem      a  Initialize k      and let V be all differences up to kmax    b  Let W be all variables plus their differences up to kth order that are in V    c  For all V  V   without a prime variable  check to see if there exists a set W    W that renders i V   independent of i V     for the i  k  If so  remove all j V     j   i    from V where i  is the lowest i for which the independence occurred   d  Let k   k      If not all prime variables have been found and k  kmax   go to Step  b     Learn the structure  Theorem      a  Learn the contemporaneous structure by using any correct causal discovery algorithm under causally sufficient data  Impose the following constraints  i  Forbid edges between all integral variables  ii  If X is an integral variable with an edge X  Y   direct the edge such that X  Y    b  Add all cross temporal links specified by the set of integral and prime variables  The output of DBCL will depend on the algorithm used in Step    Our implementation is based on the constraint based search PC algorithm  so the contemporaneous structure will be a partially directed graph that represents the statistical equivalence class in which the true directed graph belongs  One might argue that because there are deterministic relationships  the integral equations  in a DBCM  the faithfulness assumption is not valid  However  all deterministic relations involve exactly   variables  e g   X     X   and X     However two of those variables are in time slice    so DBCLs conditioning tests never involve all three variables at the same time  The hidden variable thus effectively adds noise to the deterministic relationship        IDENTIFICATION OF EMC VIOLATION  Given a model output from DBCL  one might be interested in performing causal inference  i e   prediciting the effects of manipulating components of the system  This operation is complicated by the presence of equilibrations that may have occurred in the system  Dash              shows that some dynamic systems do not obey Equilibration Manipulation Commutability  EMC   i e   the causal graph that results when an equilibrium model is manipulated can be different from the  true  graph that results when the dynamic model is manipulated and then equilibrated  Dash points out two conditions which aid in EMC identification  First  if a variable is self regulating  meaning that X  P a Prime X    then when X is equilibrated  the parent set of X and the children set of X are unchanged  Thus  with respect to manipulations on X  the EMC condition is obeyed  Second  a sufficient condition for the violation of EMC exists when the set of feedback variables of some  non self regulating  X is nonempty in the equilibrium graph  In this case  there will always exist a manipulation that violates EMC  Given a DBCM with all edges oriented  it is trivial to check these two conditions  however  since DBCL is not guaranteed to find the orientation of every edge in the DBCM structure  it is not obvious that DBCL is useful for identifying EMC violation  The following theorem shows that DBCL output will always identify self regulating variables  Theorem    Let D be a DBCM with a variable X that has a prime variable Prime X   The partially directed graph returned by Algorithm   with a perfect independence oracle will have an edge between X and Prime X  if and only if X is self regulating  It is easy to show that a feedback set of X is empty if and only if all paths from X to Prime X  have a collider  Again  since DBCL is not guaranteed to identify all edge orientations  not all colliders are necessarily identified  According to the faithfulness condition  DBCL will detect a correct equivalence class  and so will detect the correct adjacencies and the correct v structures  unshielded colliders   thus Theorem   shows that we can always identify whether or not Fb X  is empty  Theorem    Let G be the contemporaneous graph of a DBC model  Then for a variable X in G  Fb X     if and only if for each undirected path P   hP    P            Pn i between P    X and Pn   Prime X   there exists a v structure Pi  Pj  Pk in G such that Pi   P j   P k  P   Theorem   asserts that we can determine whether or   not there exists a directed path from X to Prime X   In fact  this theorem does not make use of the fact that the path terminates on a prime variable  so it actually serves as an identifiability proof for all causal descendants of any integral variable      RESULTS  For our empirical studies  we generated data from real physical systems that are representative of the type of systems found in nature  We also applied DBCL to real EEG brain data to reveal the causal propagation of alpha waves   Validation of DBCL is complicated by the fact that  as far as we know  there exist few suitable baseline methods that are even in principle able to correctly learn a DBCM when derivatives are unknown  As discussed in Section      if one tries to learn causal relations with the latent variables marginalized out  an infinite order Markov model results  Figure   c    The FCI algorithm  Spirtes et al          which attempts to take into consideration latent variables  would also result in an infinite order Markov model because it does not try to isolate and learn the latent variables and the structure between them and the observables  The structural EM algorithm  Friedman        does try to learn explicit latent variables and structure  However  applying it naively would be unfair since DBCL uses background information about the latent variables that structural EM would not be privy to  Thus  in order to provide a fair baseline  we chose to adapt some standard algorithms for discovery from causally sufficient data by providing them with known information about the latent derivatives  We used both the PC algorithm and a greedy Bayesian approach on a data set with all differences up to some maximum kmax     calculated a priori  and we applied some heuristics to interpret the output as a DBCM  While perhaps not fully satisfying  we felt that this provided the fairest comparison to a baseline  Essentially  this allows us to assess how well Step   of DBCL  the main novel component  performed on learning latent differences  Once those latent differences are found  we used the PC algorithm and the Bayesian search algorithm to recover the contemporaneous structure  but without imposing the structure of a DBCM  In PC and DBCM we used a significance level of       The Bayesian approach starts with an empty network and then first greedily adds arcs using a Bayesian score with the K  prior  Cooper and   All experiments were performed with SMILE  a Bayesian inference engine developed at the Decision Systems Laboratory and available at http   genie sis pitt edu    Herskovits         and then greedily removes arcs  For the Bayesian approach we discretized the data into five bins with approximately equal counts  It is possible to use a Bayesian approach without discretizing the data  Geiger and Heckerman         which we may explore in the future       HARMONIC OSCILLATORS  We tested DBCL on models of two physical systems  namely a SHO and the more complex coupled SHO shown in Figure   a   Although the SHO is a deter    a    b    c  Figure     a  The causal graph of the coupled SHO system   b  A typical Granger causality graph recovered with simulated data   c  The number of parents of x  over time lag recovered from a VAR model  typical results   ministic system  having noise is still realistic  e g   friction  air pressure  temperature  all of these factors are weak latent causes that add noise when determining the forces of the system  Thus all non integral equations used Gaussian error terms that were resampled at every time interval  For both systems we selected parameters of our models in such a way that they were stable  i e   produced measurements within reasonable bounds  We generated     data sets of       records for each system  We should emphasize that  as mentioned earlier  we do not include the derivatives in the data set  but only the original variables  We first computed Granger causality models and VAR models for some of the simulated data for the coupled SHO just to illustrate how uninformative these models   are when the latent derivatives are unknown  Those results are shown in Figure   b  and Figure   c   respectively  The Granger graph is more difficult to interpret than the DBCM because of the presence of multiple double headed edges indicating latent confounders  It was noted that the sole integral variables appeared in the Granger graph with reflexive edges  which might lead to an alternative algorithm for finding prime variables  However  the Granger graph does not provide enough information to perform causal reasoning  The VAR model is also difficult to interpret  as it attempts to learn structure over time of an infinite order Markov model  The graph of Figure   c  shows that variable x  has    parents spread out over time lags from   to      binned into groups of t       at significance level of       Thus while VAR models might be useful for prediction  they provide little insight into the causality of DBCMs  There were four algorithms used for quantitative baselines  two based on the PC algorithm and two based on the Bayesian algorithm  We will call them P C    P C    B    and B    respectively  For all baselines the procedure for detecting the prime variables was the same  all derivatives up to a maximum order were precalculated  and prime variables were determined to be the lowest order derivative that was not connected to itself in the future in the output graph  In the second step P C  and B  reported the contemporaneous structure that was found during the search for prime variables  For P C  and B    a separate step was made wherein we created a new dataset using only the derivatives found in step    and relearned contemporaneous structure from this reduced dataset  The results for the SHO are shown in the following table  PC  PC  B  B  DBC   low                        hi                        Edel                     Eadd                       Oerr                    The first two columns of the table show the percentage of derivatives too low and to high  respectively  The other three columns of the table show the percentage of edges that were deleted  added  and incorrectly oriented  For example  on average  DBCL added     extra edges for every     edges in the correct graph  whereas P C  added     extra edges per     original edges  The table below shows the results for the coupled SHO  P C  P C  B  B  DBC   low                            hi                    Edel                    Eadd                      Oerr                   These results show that DBCL is effective at both  learning the correct difference variables and of learning contemporaneous structure of these systems  For the SHO  the PC baselines are performing as well as DBCL for discovering prime variables  however  when the network gets more complicated  there is a clear difference  Also  in all cases the second step makes a big difference between baselines and DBCM  most likely because enforcing the DBCM structure is essential  We did try other significance levels besides       but all results showed the same trend       EEG BRAIN DATA  In our second experiment  we attempted to learn a DBCM of the causal propagation of alpha waves  an      Hz signal that typically occurs in the human brain when the subject is in a waking state with eyes closed  Subjects were asked to close their eyes and then an EEG measurement was recorded  The data consisted of    subjects and a multivariate time series of    variables was recorded for each subject     containing over         time steps at a sampling rate of     Hz  Each variable corresponds to a brain region using the standard       convention for placement of the electrodes on the human scalp  Alpha rhythms are known to operate in a specific frequency band peaking at    Hz  To focus our results more on this process  we tried learning a DBCM using just the    Hz power signal over time  We divided the data into    s segments  performed a FFT on that segment and extracted the power of the    Hz bin for each time slice  When learning the DBCM  we used the same significance level and kmax as before  The result for subject    is displayed in Figure    The circles represent the    variables that correspond to the brain regions  The top of this graph represents the front of the brain and the bottom the back  The small squares in each circle represent the derivatives that were found  The lower left is the original EEG signal  the lower right the first derivative  the top right the second derivative  and the top left the third derivative  In some regions  no derivatives were detected  so those squares have been left out  Here  and in typical subjects  there are only a few regions that required derivatives to explain their variation  The locations of those regions varied quite a bit from subject to subject  but there were some common patterns  Across all subjects     of    occipital regions had at least one derivative present  This contrasts to the frontal lobes where across all subjects only   of    frontal regions had one derivative or more  When a region had at least one derivative  rarely  if ever  did   Data available at http   www causality inf ethz ch  repository php id         Figure    Output of DBCL on data filtered for alpha wave power   it also have an incoming edge from some region that did not have a derivative  This suggests that the regions containing the dynamic processes were the primary drivers of alpha wave activity  Since most of these drivers occurred in the occipital lobes  this is consistent with the widely accepted view that alpha waves originate from the visual cortex  There were many regions that did not require any derivatives to explain their signals  The alpha wave activity in these regions is quickly       s  determined given the state of the generating regions  One hypothesis to explain this is given by Gomez Herrero et al         where they point out that conductivity of the skull can have significant impact on EEG readings by causing local signals to be a superposition of readings across the brain  Thus  if the readings of alpha waves detected in  say  the frontal region of the brain is due merely to conductivity of the skull  we would have effectively instantaneous determination of the alpha signal in those regions given the value in the regions generating the alpha waves  We should note that when DBCL is applied to the raw  unfiltered  data  the resulting DBCM is much less neat  Most regions have at least one derivative present  and connectivity among regions is much higher and more difficult to interpret  This is not surprising given the massively parallel activity occuring in the brain  and it suggests that when seeking to learn causal interactions in the brain  it may be useful to partition brain signals into different frequency bands  We hope to look more fully into different bands and possibly for causal interactions among different bands   DISCUSSION  The main contribution of this work is to present a new representation for learning models from time series data  While DBCMs have been discussed elsewhere in terms of analyzing causality of dynamic systems  there has not as yet been an algorithm to learn them from data  This paper presents such an algorithm and  in the process  makes DBCMs accessible to a wide range of practitioners in econometrics  biology and AI who currently rely on Granger causality  vector autoregression or graphical models to model dynamic systems  We have argued that DBCMs are particularly suited to learning systems which are based on differential equations  and have shown empirically that  for such systems when the relevant derivatives are unknown  DBCL will learn models accurately where existing approaches will fail  We have proven that under common assumptions DBCL will learn the correct equivalence class for a DBCM  and have shown that several important feautures of the underlying DBCM are identifiable from this equivalence class  such as the presence of feedback loops and the set of descendants of integral variables  While there exist mathematical dynamic systems that cannot be written as DBCMs  we believe that systems based on differential equations are ubiquitous in nature  and  therefore  will be well approximated by DBCMs  Furthermore  we have argued that there does not exist a representation that is capable of learning a finite model of these systems without first finding the correct latent derivative variables  This is because marginalizing out latent derivative variables results in an infinite order Markov model  Thus our method can be viewed as contributing to the very hard problem of discovering latent common causes in difference equation systems  We have also shown that DBCL can learn parsimonious representations for causal interactions of alpha waves in human brains that are consistent with previous research  We plan to apply this method to understanding causal pathways in the brain more broadly using a combination of EEG and MEG brain data  In general  we find it surprising that after nearly    years of developing theories for identification of causes in econometrics  rarely  if ever  have researchers attempted to apply these theories to even the simplest dynamic physical systems  We feel that our work thus exposes a glaring gap in causal discovery and representation  and we hope that by reversing that process applying a representation that works well on known mechanical systems to more complicated biological  econometric and AI systemswe can make new inroads to causal understanding in these disciplines    Acknowledgments Marek Druzdzel was supported in part by the National Institute of Health under grant number U  HL          We would like to the thank the anonymous reviewers for their useful feedback   
 Numerous temporal inference tasks such as fault monitoring and anomaly detection exhibit a persistence property  for example  if something breaks  it stays broken until an intervention  When modeled as a Dynamic Bayesian Network  persistence adds dependencies between adjacent time slices  often making exact inference over time intractable using standard inference algorithms  However  we show that persistence implies a regular structure that can be exploited for efficient inference  We present three successively more general classes of models  persistent causal chains  PCCs   persistent causal trees  PCTs  and persistent polytrees  PPTs   and the corresponding exact inference algorithms that exploit persistence  We show that analytic asymptotic bounds for our algorithms compare favorably to junction tree inference  and we demonstrate empirically that we can perform exact smoothing on the order of     times faster than the approximate Boyen Koller method on randomly generated instances of persistent tree models  We also show how to handle non persistent variables and how persistence can be exploited effectively for approximate filtering      Introduction  Persistence is a common trait of many real world systems  It is used to model permanent changes in state  such as when components of a system that have broken until someone intervenes to fix them  Especially interesting and useful are diagnostic models where misalignments and other process drifts may cause a cascade of other failures  all of which may also persist until the root cause is fixed  Even when such changes are not truly permanent  they are often reversed slowly  Denver H  Dash Intel Research and Department of Biomedical Informatics University of Pittsburgh Pittsburgh  PA        relative to the time scale of the model  and persistence can be a good approximation in such systems  For instance  vehicular accidents cause obstructions on the road that last much longer than the required detection time and are thus persistent for the purpose of detection       Another example is outbreak detection      where an infected population stays infected much longer than the desired detection time  There are many other examples of persistence and approximate persistence  Dynamic Bayesian Networks  DBNs      are a general formalism for modeling temporal systems under uncertainty  Many standard time series methods are special cases of DBNs  including Hidden Markov Models      and Kalman filters      Discrete DBNs in particular are a very popular formalism  but usually suffer from intractability     when dense inter temporal dependencies are present among hidden state variables  leading many to search for approximation algorithms                  Unfortunately  modeling persistence with DBNs requires the introduction of many inter temporal arcs  often making exact inference intractable with standard inference algorithms  In this paper  we define Persistent Causal DBNs  PCDBNs   a particular class of DBN models capable of modeling many real world systems that involve long chains of causal influence coupled with persistence of causal effects  We show that a linear time algorithm exists for inference  smoothing  in linear chain and tree based PC DBNs  We then generalize our results to polytree causal networks  where the algorithm remains exact  and to general networks  where it inherits properties of loopy belief propagation       Our method relies on a transformation of the original prototype network  allowing smoothing to be done efficiently  however  this method does not readily deal with the incremental filtering problem  Nonetheless  we show empirically that  if evidence is observed at every time slice  approximate filtering can be accomplished with fixed window smoothing  producing lower error than approximate Boyen Koller  BK  filtering       using a fraction of the computation time  The algorithm that we present exploits a particular type of determinism that is given by the persistence relation  There has been other work that seeks to directly or indirectly exploit general deterministic structure in Bayesian networks using compilation approaches      a generalized version belief propagation       and variable elimination with algebraic decision diagrams          These more general methods have not been tailored to the important special cases of DBNs and persistency  To our knowledge  this is the first work to investigate persistency in DBNs  The paper is organized as follows  In Section   we introduce the changepoint transformation  Section   introduces persistent causal chain DBNs and the corresponding inference algorithm  which retains all the essential properties of later models  Then  Section   will discuss the steps leading to a fully general algorithm  Experimental results are presented in Section    followed by conclusions      Notation and changepoints  Consider a Bayesian network  BN  with N binary variables Xi   we will refer to this network as the prototype  The corresponding Dynamic BN with M slices is created by replicating the prototype M times and connecting some of the variables to their copies in the next slice  In our notation  upper indices range over time slices of the DBN  lower indices range over variables in each time slice  Colon notation is used to denote sets and sequences  Thus  for instance  X   M denotes the entire temporal sequence of values of X  from time   to time M   Variables without an upper index will refer to their respective counterparts in the prototype  We say that a variable Xk is persistent if    P  Xk  U   if Xkt      P  Xkt     Xkt    U t         if Xkt          where U   P a Xk   refers to the parents of Xk in the prototype  In other words    is an absorbing state  Sometimes      a variable is called persistent if it has an arc to the next slice copy of itself  Our definition of persistence is strictly stronger  but no confusion should arise in this paper  There are  M temporal sequences of values of a binary variable Xk   If the variable is persistent  the number of configurations is reduced to M      Information about Xk  M can be summarized by looking at the time when X changed from   to    we sometimes refer to the   state as the off state and   as the on state   Thus  inference in the persistent DBN with binary variables is equivalent to inference in a network whose topology closely resembles that of the prototype and whose  variables are M    ary discrete changepoint variables  with correspondingly defined conditional probability distributions  CPDs   as shown in Figure  b  The models in Figure  a and  b are identical  one can go back and forth between them by recognizing that  X   j    X j        X j        and  X j        X   j   If the prototype is a tree  belief propagation in the transformed network yields an algorithm whose complexity is O M   N    The quadratic part of the computation comes from summing over the M     values of the single parent for each of the M     values of the child  Similarly  if the prototype is a polytree  complexity will be proportional to M Umax      where Umax is the largest in degree in the network  This transformation by itself  when all hidden state variables are persistent  allows us to perform smoothing much more efficiently than by operating on the original DBN  There is  however  additional structure in the CPDs that allows us to do better by a factor of M   and we can also adapt our algorithm to deal with the case when some hidden variables are not persistent      PCC DBN inference  To simplify the exposition  let us now focus on a specific prototype  a persistent causal chain DBN  PCCDBN   This is a chain with P a Xi      Xi     i           N and P a O    XN  thus it has N   nodes   Let us further assume that the leaves are nonpersistent and observed  while the causes  X nodes  are all persistent and hidden  The network is shown in Figure  a and its transformed version in Figure  b  Consider the problem of computing P  O   This is in general one of the most difficult inference problems  requiring one to integrate out all hidden state variables  and is implicit in most inference queries  P  O  M      X    M   M P  O  M   X  N    P  X  N           M X  N  Let  jk      jk  M   index the sequence of Xk  M in which variable Xkjk is the last  highest time  variable to be in the off state  unless jk     in which case it indexes the sequence in which all Xk are in the on state  As an example  if M      then jk                indexes the states Xk  M                         respectively  for all k  All configurations not indexed by ji have zero probability due to the persistence assumption  To simplify notation  we use jk to denote the event that Xk  M is the sequence indexed by jk   We also say that Xk fired at jk   We can decompose Equa    X    X         X M  X   X    X         X M  X   where kL contains all the terms in the sum such that Xk first fires when Xk  has not fired  X  kL                XN                  XN          jk  L  M XN       O   X N  X  kL    O O        OM  OM   a        kL contains all the terms in which Xk first fires when Xk  has also fired   Ljk  M  O   k Pbkjk Pbk  jk      k PbkL Pkjk L Pk  jk           and kL contains the final term in which Xk never fires    b   kL   PbkL PkM L  M k           Figure     a  A PCC DBN network with N     nodes per slice   b  the transformed network  We sometimes refer to X   as the temporal parent of X   and to X   as its causal parent   In order to calculate Equation   in time O M N    we need to pre compute kL   kL and kL for all values of L in O M   for each variable Xk    tion   according to the network structure as follows        P  O  M      M X j           M X  P  j     M X  P  j    j           j      P  jN   jN      P  O  M   jN         jN     Denote by Pk the probability that variable Xk will fire for the first time given that its causal parent has fired  and by Pbk the probability that Xk will fire for the first time given that its causal parent has not fired  Pk    j P  Xkj       Xkj       Xk        Pbk    j P  Xkj       Xkj       Xk         Let Pk and Pbk denote the complements    Pk and    Pbk   respectively  We can define L k recursively to denote the partial sum over jk from Equation    conditioned on jk    L  X k L P  jk   jk    L   jk       k  jk   M with boundary condition L   jN   L   N     P  O Using this notation  Equation   can be rewritten as   P  O  M      M X  P  j     j         j      Now we now need to show that one can calculate the L entire set   M   N in time O M N    Each k can be written as follows  L L L L k   k   k   k         Upward Recursion Relations  As a boundary condition for the recursion  assume we have calculated kN    for all    k  M   We show how to do this in time O M   in Section      Also  this algorithm requires the pre calculation and caching of Pbki for    k  N and    i  M   which can be done recursively in O M N   time and space  Inspecting Equation   more closely  it should be easy i to see that one can calculate N for    i  M in O M   time using the following recursion  li     li   Pbli  Pbl  ik            with boundary condition l      for all l  One can also calculate ki for    i  M with the recursion  ki     ki Pk   Pbki   Pk  i  k     Pb        k  with boundary condition kM     for all l  Finally  one i can calculate N for    i  M with the recursion  ki     ki Pk Pb        k  with boundary condition kM   PbkM  M k   for all l  i i i Once N   N and N are calculated  one can calculate i all N for    i  M in O M   time using Equations            and    After   M is calculated  we N can use Equation   to obtain   M N   in time O M    and repeat N times to get all values of   M   N   Thus the entire calculation takes O M N   time         Computing iN     To finalize the proof  we have to show how to calculate iN     the probability of the observations for a   M given configuration i of XN   for all    i  M in time O M    Recall that iN     P  O  M   jN   i   Since the parent of each Oj is given  for each i  this calculation is simply the product of the observations  P  O  M   jN   i     M Y  k P  Ok   XN   jN   i         k    Using our existing notation  we define  N     N          k P  Ok   XN       k  P  O    k XN                    iN    can be calculated for all    i  M in time O M   via the recursion relation  M Y   N       N             Note that this formulation puts no distributional assumption on P  O XN    The leaves can be distributed as multinomials  Gaussians etc  as is often done with Hidden Markov models      when they are put to their many uses   N             N     and        N      N      Downward Recurrences  The above discussion completes the description of the  pass of PCC DBN algorithm  Similar reasoning can be applied to obtain the  pass recurrences that we now give without full derivation  Analogously to   the semantics of jk is p Xk   j Ok     where Ok  is the subset of evidence reachable from Xk through its parent    jk is again a sum of three components  jk   kj   kj   kj         accounts for the terms where the parent has not yet changed  k     k        Pbk    Pbk   k  Pk        with initialization kM     for all k   accounts for the terms where the parent has already changed  k      k   Pk   Pbk     Pk      k           We only have evidence in the bottom layer in PCCDBNs  but this will come handy in the next section   with boundary condition k    Pk  k  for all k  Also  since Xk eventually changes in this scenario  kM       accounts for the terms where the node never changes  X kM        Pbki  PkM i  ik     iM  Because the upper index refers to the changepoint of Xk   only kM is non zero  We can just compute this in O M   without the need for recurrences  Initialization of the  recurrences happens at the root s  of the network  For any root r   r   Pbk and reM   b b   ir  Pbr   Finally  M Pr  Pk   currently i   r r   r      PCC DBN and belief propagation  We have just defined PCC DBN  a version of belief propagation that first collects the evidence by passing the  messages towards the root of the chain and the proceeds to distribute information towards the leaves via the  messages  After propagation is complete  we can obtain any posterior as p Xk  O    k    k          It is now useful to recall the types of potentials involved in Pearls algorithm     and how they relate to the quantities above  For each node X  there are local potentials X  x  def p X   x e  X   and    X  x  def p e  X   x   where e and e denote reX X X spectively the evidence reachable through parents and the evidence reachable from X downwards  X included  There are two types of messages in Pearls algorithm  XYi sent by X to its children and XUi sent to its parents  A closer look at PCC DBN reveals that each k is identical to Xk Xk   the message from Xk to its single parent Xk    The local potential Xk  jk   is identical to k     because there are no children other than Xk   and evidence is only observed at the bottom of the chain  k corresponds directly to Xk  jk    This is why Equation    works       Simple Generalizations and Causal Trees  While PCC DBNs are useful for demonstrating the general ideas of handling the probability distributions arising from the changepoint transformation  they form a rather restricted class of networks  and the inference query that we performed was also restricted  Here we state succinctly a set of simple alterations which allow this algorithm to be relaxed in various ways  General evidence patterns We can have observations anywhere in the network  in any time slice    Casting the inference as belief propagation gives the answer to any probabilistic query as Equation    with one caveat  An observation such as X       does not tell us with certainty the position of the changepoint  but it just provides evidence that j       thus we cannot simply set the changepoint variable to state    Rather  the potentials corresponding to such evidence must be multiplied onto the messages as prescribed by the belief propagation algorithm  see Equation      Non stationarity Stationarity of conditional probability distributions was used to simplify the formulae in the previous exposition  but is not required  All that is needed is to keep running products of respective probabilities instead of the powers in the exponents of Pk   Pbk   They need to be computed incrementally and tabulated to avoid hidden linear terms in the computation  Extension to trees The extension of PCC DBN to causal trees  PCT DBNs  is now fairly straightforward  Because each node Xk can now have multiple children Ch Xk    we must replace k in all recurrences with the true  potential for Xk   Y Xk   Xk Xk  i        iCh Xk    where Xk Xk accounts for evidence observed in Xk s temporal chain  The vector Xk Xk is zero where the evidence rules out a changepoint  before the time t of the last observed Xkt     and after the time s of first observed Xks      Everywhere else  Xk Xk  jk        Note that X  x  potential can be obtained in O M   time per node  In computation of  potentials  k on the right hand side of the recurrences is replaced by the P a Xk  Xk   which in turn include the influence of evidence under Xk s siblings  Y P a Xk  Xk   Xk  i        iCh P a Xk    Xk  Again  this preserves the O M   per node complexity  Thus  PCT DBN is linear in both N and M       Further Generalizations  In this section we describe three more important generalizations of PC DBNs  polytrees  non persistent nodes  and finally an approximate algorithm for general DAGs  These relaxations are more involved than those of Section     and thus require more elaboration       Polytrees  Belief propagation          is a powerful framework for exact inference in polytree networks  Polytrees  unlike  trees  allow multiple parents of a node  but remain acyclic in the undirected sense  In polytree changepoint networks  structure in the conditional probability table P  X   x U   can be exploited to save a multiplicative factor of M     just as we showed for tree networks  The  recurrences run over the first parent variable  while the remaining parents are summed over by brute force  Similarly  the  recurrences run over the parent that the message is addressed to  For instance  the definition of  will be replaced by   L     j   k X Y  z  Y  j      z  i b   P  P k  jk P k  Ljk  M  k  z    z L    k  k  k         and Equation    by  i   ki  kM  ki         Pk   i  Pbk       i  Y z       z  Pbk   i    Pk  i  k              where  assuming we are sending to the first parent   z   Pk  z  Pb k     P  Xk     U       I U    z       P  Xk     U       I U    z         are now functions of the joint configuration of the remaining parents U     The proportionality constant in Equation    equals the product of the remaining parents  messages  We call this PPT DBN  the persistent polytree algorithm  The worst case time complexity of PPT DBN is dominated by the cost associated with the largest familyclique  O  M     Umax    The  T BN algorithm      suffers a worst case time complexity O   N M    as all nodes in two slices may be entangled     in the clique to connect the two subsequent time slices  even though the prototype network is a polytree      section         Therefore  we expect PPT DBN will be comparatively better for shorter temporal chains of larger networks  However  PPT DBN really shines on space complexity  At most O M N   memory is consumed  compared to  TBN  where the potentials in the joint tree can grow as large as O   N    Later we show experimentally how dramatic the difference can be       Non persistent nodes  While it is convenient to assume that all non leaf variables are persistent  it does limit the modeling power at our disposal  We now show how an occasional nonpersistent variable in the network can also be handled in polynomial time  We assume the non persistent variable is isolated  that is  all of its neighbors are persistent  We make this assumption in order to avoid having to invoke an embedded general DBN inference   X   X   X   with kj defined appropriately   X        P  Y       X     P  Y     Y k       X k   kj  X k     P  Y k     Y k       X k         Y  Figure    The minimal example of network with a nonpersistent node   D  CD  C  BC   a  D   j      ki  j    k   i  B   C D      D   b   c  Figure    a  Induced cliques and separators b  Enlarged clique for generalized BP c   message flow in a network with a non persistent variable        algorithm such as  TBN to handle connected nonpersistent variables  It can be done  but quickly becomes complex and inelegant  A simple way to handle connected non persistent nodes is to combine them into a single joint node  Obviously  this solution causes exponential growth in the state space of the joined nodes  making it somewhat unappealing  A two slice approach made aware of the determinism  e g  by use of ADD compilation         could very well work better for networks with only a few persistent variables         To illustrate how an isolated non persistent node would be handled  assume first a simple structure such as in Figure    Then we can efficiently compute P  Y   j  by moving the sums inward  P P  Y   j    P  X P  Y   j X    M X   M   Q Q k P P  X k  X k    j   k   X       X M k  X X P P  X     j P  X    X     j       P  X M  X M    M j X      XM      z      z  M        This gives rise to the recurrence X P  X M   v X M     i   M M j i  j           v  ki  j     X v  P  X k   v X k    i   kj  k     v     k   j   M      The general case  Pearls belief propagation has been generalized to the clique tree propagation algorithm       With belief propagation  BP   the cliques correspond to edges of the original polytree and the separators consist of single nodes  In the process of message passing  the variables not in the separator are summed out of the clique potentials   A simple example  X   k     kj k  j   k  j    Therefore  we do not need to compute  for every j  but compute ki  M   for all k as a special case and then ki  M     for all k to start the recursion  All other values can be read off ki  M     with the appropriate indexing shift  Thus  we can obtain the entire distribution P  Y   in O M   time  Allowing non persistent variables to take on multiple values is also straightforward  we only need to allow the bottom index in ki to range over the domain of Xk    j C   C                    C M  CB  BCD  if if if if  Now  P  Y   j         Moreover  a short analysis will reveal that  B  B  k  The PCT DBN algorithm used the natural cliques induced by the transformed network  Assume we have a situation such as in Figure    Because the variable C is not persistent  the size of the induced separator C is  M   However  we can work conceptually with a larger clique BCD  Message propagation then calls for summing out all C   M   which we can do without actually instantiating the clique potential using a recurrence derived much like that of Equation     In the interest of space  we only show here the simple  recurrence  The full recurrence calls for summing over all persistent variables in the clique and the resulting complexity is O  M     B    where B is the number of persistent neighbors of the non persistent variable      Experimental evaluation  We implemented our algorithms in Matlab and compare them to the exact and approximate algorithms as implemented in the Bayesian Network Toolbox  BNT        Namely  we will compare to the Boyen Koller  BK  algorithm     in its    exact and    fully factored setting  Although BK reduces in its exact form to the incremental junction tree algorithm  we found it was faster in practice than the  TBN implementation    Speedup vs standard inference  growing N     Speedup vs standard inference  growing M            exact inference out of memory            Filtering time  sec   Filtering time  sec                     PCTDBN BK exact BK approx                PCTDBN BK exact BK approx                                    nodes in thebinary tree                      Figure    Performance scale up of PCT DBN with N   The temporal length was fixed at M       Note log scale y axis                     Number of time slices            Figure    Performance scale up of PCT DBN with M   The number of nodes was held at N       Scale up  growing N  crosses  M            PPTDBN BK exact BK approx  Therefore the  TBN algorithm is not included in the evaluation        Filtering time  sec   Matlab run time is not the ideal measure of algorithm complexity as it is arguably more sensititve to the quality of implementation compared to other languages  However  we should note that we did not make any special effort to optimize our code for Matlab  and the BNT library is a widely used and mature code base  so we expect any advantages due to code quality to fall to the competing approaches  Our Matlab code and further evaluation results can be downloaded at http   www cs pitt edu  tomas papers UAI                                                         Number of oneslice nodes               Figure    Performance scale up of PPT DBN with N    Speed of the tree algorithm  To compare inference speed  a network with the structure of a full binary tree with N nodes was generated  Among the M N possible observations      of the variables were set to a random value  subject to persistence constraints so that P  E         We measured the time to execute the query p X   E the posterior probability over the root nodefor each algorithm    This process was repeated     times for each M   N combination and the respective times added up  The results are graphed out in Figures   and    PCT DBN outperforms both the exact incremental joint tree algorithm and the approximate BK algorithm  assuming independence  by several orders of magnitude as N   the size of a slice grows  Figure     In fact  the exact algorithm soon runs out of memory  around N       and only the approximate version keeps up  Exact PCT DBN inference also performs consistently about     times faster than exact junction tree and approximate BK inference when we look at scale up with the number of slices  as shown in   The actual query is in fact irrelevant as all algorithms compute all posterior marginals simultaneously   Figure         Speed of the polytree algorithm  The asymptotic time complexity of PPT DBN  as M increases  may be less favorable than that of the incremental approaches  However  its lower memory complexity is very favorable  as documented by the following experiment  We generated a network where most non root nodes have exactly   parents and measured the time for the three inference algorithms  Quadratic scale up with M is expected for PPT DBN in such a network  Figure   shows the exact PPT DBN algorithm to be several times faster than  but scaling very similarly to  the approximate fully factorized Boyen Koller algorithm with an M      time slice inference window  Peeking ahead into Figure   suggests the time performance would be about identical at M      time slices  The junction tree algorithm does not scale beyond    nodes due to memory usage  Figure   shows clearly that asymptotically  PPT DBN   Running error with M  N      Scale up  growing M  crosses  N                   BK FF W    W    W    W                  RMS error in posterior  Filtering time  sec              BK exact BK approx                                          Number of time slices                    Figure    Performance scale up of PPT DBN with M   The number of nodes was held at N       scales with a steeper slope than both BK inference and junction tree inference  Indeed  for about N       BK eventually surpasses PPT DBN in terms of speed  However  it remains faster than junction tree incremental inference throughout the range  On a computer with   GB RAM  the exact version begins to hit memory limits around M       and N       We conclude that if exact inference is desired for persistent polytree causal networks  using the PPT DBN algorithm is a better choice for a wide range of inference window lengths  Furthermore  if approximate inference is acceptable  we show in Section     that for large enough N   fixed window smoothing using PPTDBN can outperform BK inference in terms of RMS error  while still performing many times faster  For the special case of persistent causal trees  the new algorithm dominates by orders of magnitude in all ranges that we tested versus both junction tree and BK assuming intra slice independence                  PPTDBN              Fixed window approximation  A minor disadvantage of the PPT DBN algorithm is that it cannot do online inference yet  Therefore  when monitoring a process  M grows and so does the computation time  In practice  only a fixed number of most recent observations are usually considered with older observations falling out of the window  Thus we evaluate if reasonable precision can be attained with small window sizes  where PPT DBN dominates  Figure   shows  for several time slices t  the root mean square error of computed posterior marginals v uN uX t ErrBK   t  PBK  Xit  O  t    Pex  Xit  O  t     i    incurred by the fully factored Boyen Koller method and the same error for PPT DBN which ignores all                                   Time  Figure    Accuracy of PC DBN with growing inference window M   Averages are over     different parameterizations of the network  error bars are omitted for clarity but standard deviations are the same order of magnitude as the means for all curves  evidence older than W   for different values of W   We use a binary tree prototype with all leaf variables nonpersistent and observed  All non leaf variables are persistent and hidden  The CPT probabilities are sampled uniformly at random  The observed evidence O is obtained by forward sampling the DBN and restricting it to the observables  We find that the error of our algorithm falls with growing W as expected  The results become even more favorable for PC DBN as N  the number of nodes per slice  grows  see also further results online   The error made by fixing the inference window tends to be lower than that of the Boyen Koller approximation for reasonable values of W and we can eliminate the unfavorable dependence on M at a small price of accuracy  One clear drawback to a naive implementation of the fixed window approach is that if evidence is not observed at each time slice  in the presence of persistence a piece of crucial evidence might drop off the window preventing the model from remembering that a persistent state was already acheived  This glitch could in principle be fixed by caching when persistent variables have turned on      Conclusions and future work  We presented an algorithm for PC DBNs  a way to exploit the special structure of the DBN probability distribution when many variables are persistent  Unlike forward backward approaches to DBN inference that work slice to slice  we collapse the entire temporal progression and perform inference in the original prototype network structure  For trees  the algorithm is many times faster than state of the art general purpose exact and approximate DBN inference algorithms  while having a space complexity of only   O M N    This continues to hold even in the polytree generalization with inference window lengths into the hundreds  While this method does not directly yield an incremental filtering algorithm  we show that a fixed window smoothing version of PC DBN inference can perform approximate filtering faster and with comparable or less error than BK filtering  Although we have not presented a filtering algorithm that can exploit persistence  we do believe that one is possible  The number of possible joint configurations of variables in two subsequent slices is  N with the persistence assumption as opposed to  N in the general network  This hints at the possibility of a  TBN like algorithm leveraging persistence and still remaining linear in the number of time slices       Jin H  Kim and Judea Pearl  A computational model for combined causal and diagnostic reasoning in inference systems  In Proceedings IJCAI     Karlsruhe  Germany   pages                   Daphne Koller and Nir Friedman  Bayesian Networks and Beyond  Unpublished manuscript       David Larkin and Rina Dechter  Bayesian inference in the presence of determinism  In Proceedings of Workshop on AI and Statistics  AISTAT            Kevin Murphy  The Bayes Net Toolbox for Matlab  Computing Science and Statistics                 Kevin Murphy  Dynamic Bayesian Networks  Representation  Inference and Learning  PhD thesis  EECS  University of California  Berkeley  Berkeley  CA  July        Another possible direction for this work is to allow multi resolution temporal modeling by modeling systems on very short time scales  but utilizing a persistence approximation for the slow processes  In such cases  a model with a single time scale could efficiently and accurately deal with systems that have both fast and slow processes        Kevin Murphy and Yair Weiss  The factored frontier algorithm for approximate inference in DBNs  In Proceedings of   th NIPS  volume            Also interesting is the vision of approximate inference algorithms not requiring persistence  but simply assuming that the hidden state changes at most once in the period of interest  If the change in the hidden state is relatively slow  this could be a fairly accurate approximation  Such problems are often found in bioinformatics areas such as phylogeny discovery  where time of a mutation is of interest            Mark Andrew Paskin  Exploiting Locality in Probabilistic Inference  PhD thesis  EECS  University of California  Berkeley  Berkeley  CA  December        
