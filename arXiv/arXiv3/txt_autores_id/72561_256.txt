 The effect of inaccuracies in the parameters of a dynamic Bayesian network can be investigated by subjecting the network to a sensitivity analysis  Having detailed the sensitivity functions involved in our previous work  we now study the effect of parameter inaccuracies on a recommended decision in view of a threshold decisionmaking model  We describe the effect of varying one or more parameters from a conditional probability table and present a computational procedure for establishing bounds between which assessments for these parameters can be varied without inducing a change in the recommended decision  We illustrate the various concepts by means of a real life dynamic network in the field of infectious disease     Introduction Probabilistic graphical models are often used in contexts where human decision makers have to make a decision in uncertainty  The marginal probability distributions yielded by the model then are taken as input to a decision making model  The simplest model for choosing between alternative decisions is the threshold decision making model  in which an output probability is compared against a number of fixed threshold probabilities which demarcate the boundaries for the various decisions       In our application for ICU care  for example  a clinician has to decide whether or not to start antibiotics treatment for a patient who is suspected of having ventilator associated pneumonia  VAP   based upon the probability of VAP being present  Probabilistic graphical models are typically learned from data or constructed with the help of domain experts  Due to incompleteness of data and partial knowledge of the domain under study  the numerical parameters of the model tend to be inaccurate to at least some degree  The inaccuracies may affect the output probabilities of the model as well as the decisions based upon these probabilities  The effects  of inaccuracies in the parameters of a network on its output probabilities can be studied by subjecting the network to a sensitivity analysis               In view of a decisionmaking model  however  robustness of the output of a probabilistic graphical model pertains not just to the computed output probabilities but also to the decisions based upon these probabilities  In this paper  we study this type of robustness for dynamic Bayesian networks  DBNs  in view of the threshold decision making model  Previous work on sensitivity analysis of Bayesian networks  BNs  in general showed that any posterior probability for an output variable is a quotient of two linear functions in any of the networks parameters      the posterior probability can further be expressed as a sum of such functions in all parameters from a single conditional probability table      Building upon these results  we show that in sensitivity analysis of DBNs a quotient of polynomial functions is obtained  where the order of these polynomials is linear in the time scope taken into consideration         We further show how the resulting functions can be used to study the robustness of a decision that is based upon an output probability of the network  By doing so  we focus not just on the effect of varying a single parameter  but also on the effect of varying all parameters from a given conditional probability table  In our medical application  for example  we thus provide for studying the extent to which the sensitivity and specificity rates of a particular diagnostic test can be varied without affecting the clinicians decision for treatment  We illustrate the various concepts involved by means of a sensitivity analysis of our dynamic network in the field of infectious disease      Establishing the sensitivity functions for a DBN has a time complexity similar to that of performing exact inference in such a model  Using quotients of higher order polynomials for further processing in view of the threshold decisionmaking model can also be highly demanding from a computational point of view      To handle the complexity involved  we present an approximate method for sensitivity analysis that reduces the runtime requirements involved yet incurs only a small loss in accuracy    three input processes  summarised in immunological status   three input observable variables  hospitalisation  mechanical ventilation  and previous antibiotics   one hidden input variable  aspiration   and seven output observable variables  summarised in symptoms signs   Per time step  representing a single day  the model includes    variables  Each of the interacting processes consists of seven subprocesses that are a priori independent  The transition matrices of these subprocesses are moderately stochastic  Figure   shows the dVAP network in a compact way  Figure    The dVAP model for the diagnosis of VAP for two consecutive time steps  clear nodes are hidden  shaded nodes are observable  The dashed boxes indicate the hidden processes     Sensitivity analysis revisited    Preliminaries  Having been studied extensively in the context of BNs  sensitivity analysis has received less attention in DBNs  In this section  we briefly review previous work on sensitivity analysis in BNs and extend it to a dynamic context   The simplest type of dynamic network is a hidden Markov model  HMM  H    X  Y  A  O    involving a single stochastic process       We use Xn to denote the hidden variable of the modelled process at time step n  Xn has the possible states xni   i              l  l     The transition matrix for Xn is denoted as A    ai j   with elements ai j   p Xn     xn     Xn   xni    i  j              l  for all j n  We denote the observable variables by Yn   with values yjn   j              m  m     The value of Yn is generated from the state of the hidden variable according to a timeinvariant observation matrix O    oi j   with oi j   p Yn   yjn   Xn   xni    i              l  j              m  for all n  We further denote by     i   the initial probability vector for the hidden variable  with i   p X    x i    i              l  A DBN of more general structure is an extension of an HMM  capturing a compound process that involves a collection of hidden variables  We assume that our dynamic networks are time invariant  that is  neither the topology nor the parameters of the model change across time steps  In this paper  we focus on the task of monitoring in DBNs  that is  on computing marginal distributions for the models hidden variables for some time step n given the observations that are available up to and including that time step  For this purpose  the interface algorithm is available       which basically is an extension of the junction tree algorithm for probabilistic inference in graphical models in general  The interface algorithm efficiently exploits the concept of forward interface  which is the set of variables at time step n that affect some variables at time step n     directly  in the sequel  we use F I to denote this set of variables  The computational complexity of the interface algorithm is exponential in the number of hidden variables at each time step  which is prohibitive for larger models  Throughout the paper we will use the dVAP network for illustration purposes  The dVAP network is a real life DBN for diagnosing VAP in ICU patients and is destined for use in clinical practice      The network includes two interacting hidden processes  colonisation and pneumonia        Sensitivity analysis of BNs Sensitivity analysis of a BN amounts to establishing  for each of the networks parameter probabilities  a function that expresses an output probability of interest in terms of the parameter under study             We take the posterior probability p b   e  for our probability of interest  where b is a specific value of the variable B and e denotes the available evidence  we further let    p hi     be our parameter under study  where hi is a value of the variable H and  is a specific combination of values for the parents of H  The sensitivity of the probability p b   e  to variation of the parameter  now is expressed by a sensitivity function p b   e     If the parameters p hj      hj    hi   specified for H are co varied proportionally according to     if j   i p hj           p hj       p h otherwise i    for p hi          then this function is a quotient of two linear functions in   that is  p b   e       p b  e    c      c    p e    d      d   where c    c    d  and d  are constants with respect to       Under the assumption of proportional co variation  therefore  any sensitivity function is characterised by at most three constants  Note that for parameters of which the probability of interest is algebraically independent  the function simply equals the posterior probability p b   e   Any computations can therefore be restricted to the sensitivity set for the variable of interest  which can be readily established from the networks graphical structure  An efficient scheme for sensitivity analysis is available      which requires an inward propagation in the junction tree for processing evidence and an outward propagation for establishing the constants of the sensitivity functions for all parameters per output probability        Sensitivity analysis of DBNs            n            c n r  a   c n r p xnr   a     cn  n r  a   cn  n r           cn r  are constants with respect to a dewhere pendent on n  note that the function is a polynomial of order n    in the parameter under study  For an initial parameter    i    the function is linear p xnr        c n r     c n r where c n r and c n r are constants with respect to    For an HMM in which no evidence has been entered  the observable variables do not belong to the sensitivity set of the hidden variable  Its prior probability therefore is algebraically independent of any observation parameter  We now assume that some evidence has been entered into the model  we use en to denote the cumulated evidence up to and including time step n  For the sensitivity function that expresses the posterior probability p xnr   en   in terms of a transition parameter a   ai j  A  we find that n            c n r  a   c n r p xnr   en   a   cn  n r a   n  n  p en   a   dn r a           d n r a   d n r   n    where cn  n r           cn r   dn r           dn r again are constants with respect to a   The function thus is a quotient of two polynomials of order n     For an observation parameter o   oi j   the sensitivity function is  p xnr   en   o   cbn r ob           c n r  o   c n r   n p en   o   dn r on           d n r o   d n r where b   n if r   i and b   n    otherwise  cbn r           c n r   dnn r           d n r are constants with respect to o   The order of the polynomials involved again grows linearly with n  For an initial parameter    to conclude  we have that the sensitivity function is a quotient of two linear functions  Similar results hold for probabilities of interest belonging to any possible time step no   n or no   n      The results for HMMs are readily generalised to DBNs  We consider the posterior probability of interest p bnr   en   of the state br of the hidden variable Bn given the evidence en   Then  for any variable Hn  Sens Bn   en    the sensitivity function expressing p bnr   en   in    p hni     is a quotient        p vap   e    The main difference with sensitivity analysis of BNs is that a parameter occurs multiple times in a DBN  In previous work      we derived functional forms to express the sensitivity of a probability of interest of an HMM in terms of a parameter under study  We briefly review these sensitivity functions  We begin by studying the sensitivities of an HMM in which no evidence has been entered as yet  The probability of interest is the prior probability p xnr   of some state xr of the hidden variable Xn   Let a   ai j  A be a transition parameter in the model such that Xn is algebraically dependent on a   Then                                                    Figure    The sensitivity function expressing the probability of pneumonia at day   given evidence e  for a patient  in terms of the parameter    p leucocytosis   yes   pneumonia   yes    of two polynomials of order n  if Hn  F I  or of order n otherwise  As an example sensitivity function  Figure   depicts  for the dVAP network  the effect of varying the parameter    p leucocytosis   yes   pneumonia   yes  on the probability of pneumonia at day   given evidence e  for a specific patient  The depicted function is a quotient of two polynomials of order   each  For computing the constants in the various sensitivity functions  we combined the interface algorithm with the junction tree scheme for sensitivity analysis  further details of the resulting algorithm are out of the scope of this paper     Threshold decision making BNs in general yield marginal probability distributions for their output  Often these marginal distributions are input to a decision maker who has to make a decision  The simplest model for choosing between alternative decisions is the threshold decision making model  In this section  we briefly review the threshold model for decision making and describe sensitivity analysis of BNs in view of this model  Although generally applicable  the threshold decisionmaking model is used most notably for patient management in medicine       Since our dVAP network also pertains to the field of medicine  we present the model in medical terms  With the model  a clinician decides whether or not to gather additional information from diagnostic tests and whether or not to give treatment based upon the probability of disease for a patient  The threshold model to this end builds upon various threshold probabilities of disease  The treatment threshold probability p is the probability at which the clinician is indifferent between giving treatment and withholding treatment  If  for a specific patient  the probability of disease exceeds the treatment threshold probability  then the clinician will decide to treat the patient as if the disease were known to be present with certainty  Alternatively  if the probability of disease is less than p   the clinician will basically withhold treatment  As a consequence of the uncertainty concerning the true condition   no treat p  p   test no treat  p    treat  treat  Figure    The threshold decision model  of the patient however  additional information from a diagnostic test may affect the clinicians basic management decision  The threshold model therefore includes another two threshold probabilities  The no treatment test threshold probability of disease p is the probability at which the clinician is indifferent between the decision to withhold treatment and the decision to obtain additional diagnostic information  The test treatment threshold probability p  is the probability at which the clinician is indifferent between obtaining additional information and starting treatment rightaway  Figure   illustrates the various threshold probabilities employed by the model  In view of the threshold model for decision making  sensitivity of the output of a network pertains no longer to just a probability of interest computed from the network  but also to the decision based upon it  To take the various threshold probabilities employed into consideration  the method of sensitivity analysis of BNs has been enhanced with the computation of upper and lower bounds between which a networks parameters can be varied without inducing a change in decision       The computation of these bounds builds upon the sensitivity functions relating the probability of interest to the networks parameters  By equating the function for a specific parameter to the various threshold probabilities  bounds are obtained between which the parameter can be varied  Since the sensitivity functions for a BN are either monotonically non decreasing or monotonically non increasing  a single lower bound and a single upper bound are guaranteed to exist     Sensitivity analysis of decisions with DBNs The probabilities established from a dynamic network are also often employed for decision making  The goal of the dVAP model  for example  is to monitor the onset of ventilator associated pneumonia in ICU patients and to start appropriate treatment as soon as possible  Sensitivity to parameter variation then pertains not just to the probability of VAP but also to the decision that the clinician makes based upon this probability  To provide for studying this type of sensitivity  we extend sensitivity analysis of DBNs in view of threshold decision making      Analysis of single parameters Suppose that a posterior probability p xnr   en   of interest has been computed from a DBN  based upon this probability  a particular decision has been established from the threshold decision making model  We now are interested  in the effect of variation of a parameter  on this decision  To compute upper and lower bounds between which the parameter can be varied without inducing a change in decision  the sensitivity function p xnr   en     is equated to the threshold probabilities p and p    respectively  The lower and upper bounds thus are solutions of the equations p xnr   en     p xnr   en       p and   p  p en     p en           We recall that for DBNs a sensitivity function in general is a quotient of higher order polynomials  Contrary to threshold decision making for BNs therefore  there is no guarantee that these functions are monotonically non decreasing or non increasing  The equations stated above may thus have multiple solutions instead of single ones  We begin by studying a parameter for which single solutions exist for the two threshold equations above  Suppose that the lower and upper bounds computed from the equations are  and   respectively  If p xnr   en     p   then the decision to withhold treatment remains unaltered for any value of  within the interval                If p xnr   en     p    the decision to start treatment immediately remains unaltered for any value of  within                  If p  p xnr   en    p    then the decision to gather further information will be the same for any value of  within the interval                  As an example from the dVAP network  we illustrate the bounds on variation of the parameter    p leucocytosis   yes   pneumonia   yes  in view of the management decision for a particular patient at day    Figure   shows the sensitivity function that expresses the probability of VAP for this patient in terms of   Suppose that the three threshold probabilities are fixed at p        p         and p          From the dVAP network  we have that p vap    e            and hence that p  p vap    e     p    The clinician thus decides to gather additional information for the patient  Solving the two threshold equations from      we find a lower bound on the parameter under study equal to           the upper bound is             For any value of the parameter within the interval             therefore  the decision to gather additional information will remain unaltered  Since the original value of the parameter has been assessed at      we conclude that the decision is not very robust with regard to this parameter  We now turn to parameters for which the threshold equations have multiple solutions  Suppose that from the notreatment test threshold probability p   we find the vector of solutions                      r    in which the parameter values i are given in ascending order  from the test treatment threshold probability p    we find the vector                         s     again with the i  in ascending order  We further use v i   to denote the value of the first derivative of the sensitivity function for the parameter  at i   The value of the first derivative helps in determining      treat       p        p vap  e              test           p                    no treat                                      Figure    Threshold decision making for the treatment of pneumonia when varying the parameter    p temperature   low   pneumonia   no  a drugs   yes    the intervals in which a particular decision holds  Now  if the output probability p is smaller than p   the decision to withhold treatment remains unaltered for any value of  that belongs to the compound interval                         r      r is even                            r    r   r is odd whenever v          and for any value of  belonging to                               r    r   r is even                                  r      r is odd whenever v          Similarly  if the output probability p   is larger than p    we find compound intervals    and   for the parameter  within which the decision to start treatment immediately remains unaltered  Finally  if the output probability lies between p and p    the vectors   and    are merged into the vector  m     m    m           qm    q   r   s  Now  the decision will be the same for any value of  within the interval m    m   qm   v  m            m    m    m        q  m   m   qm   v  m         m    m    m    m        q  Note that in case v  m       and the value of the sensitivity function for      is greater than p   the interval m is the same as when v  m        As an example  Figure   depicts the probability of pneumonia at day   given evidence e  in terms of the parameter    p temperature   low   pneumonia   no  a drugs   yes   the original value of  equals      giving p vap    e             The figure also shows the intersection points with the threshold probabilities  which have been set at p        and p          We compute the two lower bounds to be            and             the upper bound is              Using v          we find that the decision to withhold treatment remains unaltered for any value of  in                   We conclude that the decision is relatively robust with regard to the parameter      Analysis of full conditional probability tables In addition to single parameters  we may be interested in the effects of varying multiple parameters  for example  Figure    The sensitivity function expressing the probability of pneumonia given e  in terms of the sensitivity se and specificity sp rates of the CPT for radiological signs   from a single conditional probability table  CPT       In a medical application for instance  we may wish to study the robustness of a decision in terms of both the sensitivity and the specificity of a particular diagnostic test and not just in one of these rates  Recall that these rates express the probabilities that a test result is found to be positive  negative  in a patient who does  does not  have the disease  We now extend the previous results for single parameters to CPTs to provide for such an analysis  For BNs  any posterior probability for the output variable is a quotient of sums of linear functions in the parameters of a CPT         For a dynamic network we obtain sums of polynomial functions  For a joint probability p bnr   en   we find that  Cn   X p bnr   en     gi  i   i    where  Cn   denotes the number of parent configurations of the variable Cn and gi  i   represents a polynomial function in the parameter i for a specific parent configuration for Cn   The polynomial functions gi  i   are all of the same order and can be computed individually using the considerations of the previous section  For the joint probability p en   a similar result holds  We conclude that  for a DBN  the sensitivity function for a CPT is a quotient of sums of higher order polynomials in the parameters under study  As an example  Figure   illustrates the effect of varying the sensitivity and specificity rates of the CPT for radiological signs of pneumonia at day   given evidence e  for a specific patient  The depicted sensitivity function is a quotient of sums of two polynomial functions of order   each  Upon studying the effects of varying all parameters from a CPT in view of threshold decision making  we have to solve threshold equations similar to      For a single parameter  we identified intervals for the parameters value within which a decision remains unaltered  For a CPT  we now identify areas in higher space with the same meaning  In the remainder of this section  we consider a CPT with                  treat          p               se        test                            p              no treat                                sp                         Figure    Threshold decision making when varying the sensitivity and specificity rates se   sp of the CPT for radiological signs   sensitivity and specificity rates as in the previous example  similar results hold for more complex CPTs  We begin again by studying a CPT for which single lower   and sp rebounds exist for the two rates  denoted as se spectively  By re arranging the individual polynomial func tions included  we can express the relationship between se  and sp as   ge se     gb sp   where ge and gb are polynomials of the same order  If ge is invertible in         we have that   se   ge   b g  sp      which defines the relationship between the se and sp   The horizontal line test can be used for checking whether ge is invertible in         Establishing ge    however  is computationally expensive if not infeasible         and and sp To determine the relationship between se thereby study the robustness of a recommended decision  we use a numerical approximation procedure  We repeat    edly pick a value se         and solve for sp              From the pairs  se   sp   thus obtained  we construct a line     A and sp l representing the relationship between se   similar approach is used to find a line l that represents the     relationship between the upper bounds se and sp for the two rates  We note that our procedure requires solving just a single polynomial equation  which is feasible in general       For larger CPTs  however  the procedure becomes computationally more demanding  since a larger sample of points is required to assure good results   We now have that  if the probability of interest p xnr   en   falls below p   the decision to withhold treatment remains unaltered for any pair  se   sp   below l   If p xnr   en     p    the decision to start treatment remains unaltered for any pair  se   sp   above l    Finally  if p  p xnr   en    p    the decision will be the same for any pair  se   sp   between l and l    Figure   now illustrates the sensitivity analysis  With our approximation procedure  two lines are established that  serve to divide the unit square into three areas in which different decisions apply  For our example patient  we have that p vap    e               p    Since the original values for the sensitivity and specificity rates under study are     and      respectively  we conclude that the decision to start treatment right away is quite robust with regard to the CPT  The three bullets in the figure highlight three other interesting cases  For the bullet with sp        we observe that the decision to test is only moderately robust since a small change in sp or se can alter the decision  For the bullet with sp        the decision to treat is quite robust since only a major change in sp or se can induce another decision  Finally  for the bullet with sp         the decision to test is not robust at all since a small change in sp or se suffices to alter the decision  To conclude  we note that when the function ge  is not invertible  our sampling procedure will result in multiple solutions  The unit square for se and sp will then be divided in compound areas per decision  similar to the compound intervals in the single parameter case     An approximate scheme The number of constants in the sensitivity functions of a DBN and the number of propagations required to compute these constants grows linearly with n  Moreover  the computational burden of solving polynomials of high order can grow dramatically       For a larger time scope  therefore  sensitivity analysis in view of threshold decision making can become quite hard  To reduce the order of the polynomials in the sensitivity functions and thereby the runtime requirements  we present a method for approximate sensitivity analysis that builds on the concept of contraction of a Markov process      We discuss our method for DBNs with a single hidden process and review its extension to DBNs with multiple processes  We consider two probability distributions  and   over a variable W   Conditioning on a set of observations is known to never increase the relative entropy of these distributions  Denoting the conditioning by o    we thus have that D o  ko       D k          where D stands for the relative entropy  Now  consider the extreme case where  and   have their entire probability mass on two different states wi and wk respectively  We denote by A   the distribution that results from processing through the transition matrix A  Even though  and   do not agree on any state  processing will cause them to place some mass  on some state wj   They then  agree for a mass of min A  wj   wi     A    wj   wk    on that state wj   Based on this property  the minimal mixing rate of the matrix A is defined as X   A   min min A  wj   wi     A    wj   wk    i k  j   Given the minimal mixing rate of a transition matrix A  the following theorem now holds      D A  kA           A    D k    We say that the stochastic process with transition matrix A contracts with probability A   Combining equation     with the previous theorem gives D A o   kA o            A    D k    Performing conditioning on two different distributions and subsequently transitioning them  will thus result in two new distributions whose distance in terms of relative entropy is reduced by a factor smaller than one  Our approximate method for sensitivity analysis now builds on the contraction property reviewed above  Suppose that we are interested in the probability of some state of the hidden variable Xn at time step n  After entering the available evidence en into the model  we can compute the exact posterior distribution p Xn   en    Building on the contraction property  however  we can also compute an approximate distribution pe Xn   en   starting from time step n   with     n   n  without losing too much accuracy  We define  the backward acceptable window n   for time step n with a specified level of accuracy    to be the number of time steps we need to use from the past to compute the probability distribution of the hidden variable at time step n within an accuracy of    We now propose to perform sensitivity analysis for time step n considering only the backward ac   Note that the resulting functions ceptable window n   then include polynomials of order O n  n   rather than of order O n  compared to the true functions  For a given level of accuracy    we have to determine the maximum value of n for which D p Xn   en  ke p Xn   en     nn     A    D p Xn   en  kp X        where pe Xn   en   denotes the approximate distribution of  Xn that is computed using n     Solving for n   we find that          log   D p Xn   en  kp X     n   max    n log    A       where bc stands for the integer part  Starting from n   n and decreasing the value of n one step at a time  we can readily establish the value of n that first satisfies the equation      To this end  the interface algorithm needs to have computed and stored the exact posterior distributions p Xno   eno   for all no  n  given evidence eno   The procedure to compute the optimal value n requires at most n computations of     and thus is not very demanding from a computational point of view  We recall  how   ever  that for the computation of n   the interface algorithm needs to have established the exact posterior distributions given the available evidence  Now in a full sensitivity analysis  the effects of parameter variation are being studied for a number of evidence profiles  The above procedure may then become rather demanding since for every such profile a full propagation with the interface algorithm is required  An alternative way would then be to approximate n given   from the start and perform the entire anal ysis with the backward acceptable window n     If we assume that D p Xn  kp X     is bounded from above by a known constant M   we find that an approximate value for n would satisfy        log   M   n  max    n  log    A   Note that for a given   and A   the higher the value of M   the smaller the value of n and hence the larger the backward acceptable window  Knowledge of the domain under study can help in determining a suitable value for M   For a patient profile for example  M can be determined by inserting worst case scenario observations for the first time step and computing for that time the posterior probability distribution for the hidden variable from which M can be readily established  The complexity that our method now entails is just the complexity of computing M which is similar to performing a single propagation for a single time step  This computational burden is considerably less than the burden of performing n time steps of exact inference  which we thereby forestall in the sensitivity analysis  Note that for some patients the computation of n based upon this value M will lead to a larger backward acceptable window than the one computed directly from equation      In view of sensitivity analysis  we observe that the value of n that is established as outlined above  is based on the original values of all parameters of the model under study  We further observe that the minimal mixing rate A used in the computation of n is algebraically dependent only on  the models transition parameters  Using n   based upon n for sensitivity analysis  therefore  is guaranteed to result in approximate sensitivity functions within an accuracy of   for any non transition parameter  For transition parameters  this guarantee does not hold in general  We note  however  that for the original value of a transition parameter  the difference between the true probability of interest and the approximate one is certain to be smaller than    Since the value n changes with A in a stepwise manner only  this property holds for a range of values for the parameter  Figure   illustrates the relationship between n and A given particular values for n    and M   We observe from the figure that there is a range of values of A for which the value of n stays the same  We expect a similar property to hold for a range of values for the transition parameter a   We are currently studying this issue and hope to report results in the near future    n                 
  Qualitative probabilistic networks have been de signed for probabilistic reasoning in a qualita tive way  Due to their coarse level of represen tation detail  qualitative probabilistic networks do not provide for resolving trade offs and typ ically yield ambiguous results upon inference  We present an algorithm for computing more in sightful results for unresolved trade offs  The al gorithm builds upon the idea of using pivots to zoom in on the trade offs and identifying the in formation that would serve to resolve them     INTRODUCTION  Qualitative probabilistic networks were introduced in the early     s for probabilistic reasoning with uncertainty in a qualitative way  Wellman         A qualitative prob abilistic network encodes variables and the probabilistic relationships between them in a directed acyclic graph  The encoded relationships basically represent influences on probability distributions  Each of these influences is sum marised by a qualitative sign indicating the direction of shift in one variable s distribution occasioned by a shift in another variable s distribution  For probabilistic inference with qualitative networks  an elegant algorithm based upon the idea of propagating and combining signs is available  Druzdzel   Henrion      a    Qualitative probabilistic networks capture the relationships between their variables at a coarse level of representation detail  As a consequence  these networks do not provide for resolving trade offs  that is  for establishing the net re sult of two or more conflicting influences on a variable s probability distribution  If trade offs are represented in a qualitative network  then probabilistic inference will typi cally yield ambiguous results  Once an ambiguity arises  it will spread throughout most of the network upon infer ence  even if only a very small part of the network is truly ambiguous   The issue of dealing with trade offs in qualitative prob abilistic networks has been addressed before by several researchers  S  Parsons has introduced  for example  the concept of categorical influences  A categorical influence is either an influence that serves to increase a probability to   or an influence that decreases a probability to    re gardless of any other influences  and thereby resolves any trade off in which it is involved  Parsons          C  L  Liu and M  P  Wellman have designed a method for resolving trade offs based upon the idea of reverting to numerical probabilities whenever necessary  Liu   Wellman         S  Renooij and L C  van der Gaag have enhanced the ba sic formalism of qualitative probabilistic networks by dis tinguishing between strong and weak influences  Trade off resolution during inference is then based on the idea that strong influences dominate over conflicting weak ones  Renooij   Van der Gaag          These approaches to trade off resolution are all based on a refinement of the rep resentation used in the basic formalism  In this paper  we present a new algorithm for dealing with trade offs in qualitative probabilistic networks  Rather than resolving trade offs by providing for a finer level of rep resentation detail  our algorithm identifies the information that would serve to resolve the trade offs present in a quali tative probabilistic network  From this information  a more insightful result than ambiguity is constructed  Our algorithm for dealing with trade offs builds upon the idea of zooming in on the part of a qualitative probabilis tic network where the actual trade offs reside  After a new observation has been entered into the network  probabilis tic inference will provide the sign of the influence of this observation on the variable of interest  given previously en tered observations  If this sign is ambiguous  then there are trade offs present in the network  In fact  a trade off must reside along the reasoning chains between the observation and the variable of interest  Our algorithm isolates these reasoning chains to constitute the part of the network that is relevant for addressing the trade offs present  From this relevant part  an informative result is constructed for the variable of interest in terms of values for the variables in         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       volved and the relative strengths of the influences between them  We believe that qualitative probabilistic networks can play an important role in the construction of quantitative proba bilistic networks for real life application domains  as well as for explanation of their reasoning processes  The con struction of a probabilistic network typically sets out with the construction of the network s digraph  As the assess ment of the various probabilities required is a far harder task  it is performed only when the network s digraph is considered robust  Now  by assessing signs for the influ ences modelled in the digraph  a qualitative network is ob tained that can be exploited for studying the projected prob abilistic network s reasoning behaviour prior to the assess ment of its probabilities  For this purpose  algorithms are required that serve to derive as much information as possi ble from a qualitative probabilistic network  We look upon our algorithm as a first step to this end  The paper is organised as follows  In Section    we pro vide some preliminaries concerning qualitative probabilis tic networks  In Section    we introduce our algorithm for zooming in on trade offs informally  by means of an ex ample  The algorithm is discussed in further detail in Sec tion    The paper ends with some concluding observations in Section       PRELIMINARIES  A qualitative probabilistic network encodes the statistical variables from a domain of application and the probabilis tic relationships between them in a directed acyclic graph G  V G    A G      Each node in the set V G  repre sents a statistical variable  Each arc in the set A G  can be looked upon as expressing a causal influence from the node at the tail of the arc on the node at the arc s head  More for mally  the set of arcs captures probabilistic independence among the represented variables  We say that a chain be tween two nodes is blocked if it includes either an observed node with at least one outgoing arc or an unobserved node with two incoming arcs and no observed descendants  If all chains between two nodes are blocked  then these nodes are said to be d separated and the corresponding variables are considered conditionally independent given the entered observations  Pearl             A qualitative probabilistic network associates with its di graph G a set  of qualitative influences and synergies  Wellman          A qualitative influence between two nodes expresses how the values of one node influence the probabilities of the values of the other node  A positive qualitative influence of node A on its successor B expresses that observing higher values for A makes higher values for B more likely  regardless of any other direct influences on B  the influence is denoted S  A  B    where    is the in fluence s sign  A negative qualitative influence  denoted  S j  and a zero qualitative influence  denoted S    are de fined analogously  If the influence of node A on node B is not monotonic or unknown  we say that it is ambiguous  denoted Sb A  B    The set of influences of a qualitative probabilistic net work exhibits various properties  Wellman          The property of symmetry states that  if the network includes the influence Sb A  B    then it also includes Sb B  A    J E                The property of transitivity asserts that qualitative influences along a simple chain that specifies at most one incoming arc for each node  combine into a single influence with the  operator from Table    The property of composition asserts that multiple influences between two nodes along parallel chains combine into a single influence with the EB operator  Table    The   and EB operators                                                 EB                                          In addition to influences  a qualitative probabilistic net work includes synergies that express how the value of one node influences the probabilities of the values of another node in view of a value for a third node  Druzdzel   Henrion      b    A negative product synergy of node A on node B  and vice versa  given the value c for their common successor C  denoted X    A  B   c    ex presses that  given c  higher values for A render higher val ues for B less likely  Positive  zero  and ambiguous prod uct synergies are defined analogously  A product synergy induces a qualitative influence between the predecessors of a node upon observation of that node  the induced influence is coined an intercausal influence  In this paper  we assume that induced intercausal influences are added to a qualita tive probabilistic network s graph as undirected edges  procedure PropagateSign from to message    sign to   sign to  EB message  for each  induced  neighbour Vi of to do linksign  sign of  induced  influence between to and Vi  message  sign to    linksign  if Vi  from and Vi  Observed and sign Vi   I  sign Vi  EB message then PropagateSign to  Vi message   Figure    The Sign propagation Algorithm  For probabilistic inference with a qualitative probabilis tic network  an elegant algorithm is available from M J  Druzdzel and M  Henrion      a   this algorithm is summarised in pseudocode in Figure    The basic idea of the algorithm is to trace the effect of observing a node s value on the other nodes in a network by message passing   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS  between neighbouring nodes  For each node  a node sign is determined  indicating the direction of change in the node s probability distribution occasioned by the new ob servation given all previously observed node values  Ini tially  all node signs equal       For the newly observed node  an appropriate sign is entered  that is  either a     for the observed value true or a     for the value false  by calling PropagateSign observed node  observed node  sign   Each node receiving a message updates its sign and sub sequently sends a message to each neighbour that is not d separated from the observed node and to every node on which it exerts an induced intercausal influence  The sign of this message is the   product of the node s  new  sign and the sign of the influence it traverses  This process is re peated throughout the network  building on the properties of symmetry  transitivity  and composition of influences  The process repeatedly visits each node that needs a change of sign  Since a node can change sign at most twice  once from   to  or   and then only to    each node is visited at most twice  The process is therefore guaranteed to halt     OUTLINE OF THE ALGORITHM  If a qualitative probabilistic network models trade offs  it will typically yield ambiguous results upon inference with the sign propagation algorithm  From Table    we have that whenever two conflicting influences on a node are combined with the EEl operator  an ambiguous sign will re sult  Once an ambiguous sign is introduced  it wil l spread throughout most of the network and an ambiguous sign is likely to result for the node of interest  By zooming in on the part of the network where the actual trade offs reside and identifying the information that would serve to resolve these trade offs  a more insightful result can be constructed  We illustrate the basic idea of our algorithm to this end              Figure    The Result of Propagating     for NodeH  has been observed for the node H and that we are inter ested in its influence on the probability distribution of node A  Tracing the influence of the node sign    for nodeH  indicating its observed value  on every node s distribution by means of the sign propagation algorithm  results in the node signs shown in Figure    These signs reveal that at least one trade off must reside along the reasoning chains between the observed node H and the node of interest A  These chains together constitute the part of the network that is relevant for addressing the trade offs that have given rise   to the ambiguous result for node A  we term this part the relevant network  For the example  the relevant network is shown in Figure   below the dashed line  Our algorithm now isolates this relevant network for further investigation  To this end  it deletes from the network all nodes and arcs that are connected to  but no part of the reasoning chains fromH to A  A relevant network for addressing trade offs typically in cludes many nodes with ambiguous node signs  Often  however  only a small number of these nodes are actually involved in the trade offs that have given rise to the am biguous result for the node of interest  Figures   and    for example  reveal that  while the nodes A  B  and C have ambiguous node signs  the influences between them are not conflicting  In fact  every possible unambiguous node sign sign C  for node C would result in the unambiguous sign sign C            EB   sign C    for node A  For addressing the trade offs involved  therefore  the part of the relevant network between node C and node A can be dis regarded  Node C in fact separates the part of the relevant network that contains trade offs from the part that does not  We call node C the pivot node for the node of interest     Figure    The Example Qualitative Probabilistic Network  As our running example  we consider the qualitative proba bilistic network from Figure    Suppose that the value true  In general  the pivot node in a relevant network is a node with an ambiguous sign for which every possible unam biguous sign would uniquely determine an unambiguous sign for the node of interest  in addition  no other node hav ing this property resides on an unblocked chain from the        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Figure    The Construction of a Sign for Node C  network  the nodes from the resolution frontier exert two separate influences on the pivot node C  the influence from node I via node Don C and the influence from G on C  For the sign   of the influence of node I via node Don C and for the sign o  of the influence of G on C  we find that    Figure    The Relevant Network  below the Dashed Line  observed node to the pivot node  that is  the pivot node is the node with this property  closest  to the observed node  Note that every network includes such a node  Our algo rithm now selects from the relevant network the pivot node for the node of interest  From the definition of pivot node  it can be shown that there must be two or more different reasoning chains in the rel evant network from the observed node to the pivot node  the net influences along these reasoning chains  moreover  must be conflicting or ambiguous  To resolve the ambiguity at the pivot node  the relative strengths of the various influ ences as well as the signs of some of the nodes involved need be known  From Figures   and    for example  we have that node I lies at the basis of the ambiguous sign for the pivot node C  Note that it receives an ambiguous node sign itself as a result of two conflicting  non ambiguous  influences  An unambiguous node sign for node I would not suffice to fix an unambiguous sign for node C  Even knowledge of the relative strengths of the two conflicting influences from node I on the pivot node would not suf fice for this purpose  however  a positive node sign for node I  for example  would still cause node G  residing on one of the reasoning chains from I to C  to receive an ambiguous node sign  which in tum gives rise to an am biguous influence on C  Node G therefore also lies at the basis of the ambiguity at the pivot node  Now  every com bination of unambiguous node signs for the nodes G and I would render the separate influences on the pivot node unambiguous  Knowledge of the relative strengths of these influences would suffice to determine an unambiguous sign for the pivot node  We call a minimal set of nodes having this property the resolutionfrontier for the pivot node  In terms of signs for the nodes from the resolution frontier  our algorithm now constructs a  conditional  sign for the pivot node by comparing the relative strengths of the vari ous influences exerted on it upon inference  In the example        sign I            sign I                sign G       sign G     where Di  i          are as in Figure    For the node sign sign C  of the pivot node  the algorithm now constructs the    following result  if              then sign C         else sign C           where     denotes the strength of the sign    So  if the two influences on node C have opposite signs  then their rela tive strengths will determine the sign for node C  The sign of the node of interest A then follows directly from the node sign of C     SPLITTING UP AND CONSTRUCTING SIGNS  In this section we detail some of the issues involved in our algorithm for pivotal pruning of trade offs  In doing so  we assume that a qualitative probabilistic network does not in clude any ambiguous influences  that is  ambiguous node signs upon inference result from unresolved trade offs  We further assume that observations are entered into the net work one at a time  We also assume that sign propagation resulted in an ambiguous sign for the network s node of in terest  For ease of reference  Figure   summarises the zoom algorithm in pseudocode  procedure PivotalPruning Q    Qrel     ComputeRelevantNetwork Q   pivot    ComputePivot Qrel   ConstructResults Qrel  pivot   Figure    The Basic Algorithm  In detailing the algorithm  we focus attention on identify ing the relevant part of a qualitative probabilistic network along with its pivot node and on constructing from these an informative result for the node of interest    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       IDENTIFYING THE RELEVANT NETWORK  Our algorithm identifies from a qualitative probabilistic network the relevant part for addressing the trade offs that have resulted in an ambiguous sign for the node of inter est  We begin by formally defining the concept of relevant network  Definition   Let Q  G    be a qualitative probabilistic network as defined in Section    Let   be the set of previ ously observed nodes in Q  let E be the node for which new evidence has become available  and let I be the network s node of interest  The relevant network for E and I given   is the qualitative probabilistic network Qrel  G      such that            V G   consists of all nodes that occur on a chain from  E to I that is not blocked by     A G        V G    x  V G     n A G    and    consists of all qualitative influences and synergies from  that involve nodes from G  only   The concept of relevance has been introduced before  most notably for quantitative probabilistic networks  see for ex ample  Druzdzel   Suermondt        Shachter          In fact  for quantitative and qualitative probabilistic networks various different concepts of relevance have been distin guished  For a node of interest I  previously observed nodes    and a newly observed nodeE  we say that a node N is   structurally relevant to I  if N is not d separated from I given   U  E             computationally nor dynamically relevant to the node of interest A  The concept of dynamic relevance was introduced to de note the nodes constituting the reasoning chains between a newly observed node and a node of interest in a probabilis tic network  Druzdzel   Suermondt         The set of all nodes that are dynamically relevant to the node of interest I and the newly observed nodeE  given the previously ob served nodes    can in fact be shown to induce the relevant network forE and I given    as defined in Definition    From a qualitative probabilistic network  the set of dy namically relevant nodes can be established by first deter mining all nodes that are computationally relevant to the node of interest I and then removing the nodes that are not on any reasoning chain from the newly observed node E to I  For computing the set of all computationally rele vant nodes  the efficient Bayes Ball algorithm is available from R D  Shachter         The algorithm takes for its in put a probabilistic network  the set of all observed nodes   U  E   and the node of interest I  it returns the sets of nodes that are computationally relevant  or requisite  to I  From the set of computationally relevant nodes  all nodes that are not on any reasoning chain from the newly observed node E to the node of interest I need be iden tified  these nodes are termed nuisance nodes for E and I  An efficient algorithm is available for identifying these nodes  Lin   Druzdzel         The algorithm takes for its input a computationally relevant network  the set of previ ously observed nodes    the newly observed node E  and the node of interest I  it returns the set of nuisance nodes for E and I  The algorithm for computing the relevant part of a qualitative probabilistic network is summarised in pseudocode in Figure    function ComputeRelevantNetwork Q    computationally relevant to I  if the  conditional  probabilities for N are required for computing the posterior probability distribution for I given the ob servations for   U  E   and  Qrel  requisites     BayesBaii G    U  E   I   V G       V G   requisites  U  E   A G        V G  x V G   n A G   nuisances     ComputeNuisanceNodes G   V G      V G    nuisances  A G       V G  x V G   n A G   L        all influences and synergies from L  in G   return Qrel  G  L    dynamically relevant to I andE  if N partakes in the impact of E on I in the presence of the observations forO   In our example qualitative network  node D is structurally relevant  computationally relevant  and dynamically rele vant to the node of interest A  NodeE is structurally rele vant to node A yet neither computationally nor dynamically relevant  Node J is structurally irrelevant to the observed nodeH  as is also evidenced by its node sign     upon in ference  it is both structurally and computationally relevant to the node of interest A  yet dynamically irrelevant  The newly observed node H is d separated from A by its be ing observed  It therefore is not structurally relevant to A  it is computationally as well as dynamically relevant to A  however  Node M  to conclude  is neither structurally nor           Figure    The Algorithm for Computing the Relevant Net work       IDENTIFYING THE PIVOT NODE  After establishing the relevant part of a qualitative proba bilistic network for addressing the trade offs present  our algorithm identifies the pivot node  The pivot node serves to separate the part of the relevant network that contains the trade offs that have given rise to the ambiguous sign for the node of interest  from the part that does not con    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            tain these trade offs  The pivot node will allow for further focusing  We recall that the pivot node is a node with an ambiguous node sign  for which every possible unambigu ous sign would uniquely determine an unambiguous sign for the node of interest  We define the concept of pivot node more formally   Let Q  G     be a relevant qualitative probabilistic network  let   be the set of previously ob served nodes  let E be the newly observed node  and let I be the network s node of interest  as before  The pivot node for I and E is a node P E V  G  such that  Definition      Sb E  P     Sb  P  I        E E   with o            with              and  there does not exist a node P  with the above prop erties that resides on a chain from E to P that is not blocked by     The pivot node in a relevant qualitative probabilistic net work has various convenient properties  Before discussing these properties  we briefly review the concept of an ar ticulation node from graph theory  In a digraph  an ar ticulation node is a node that upon removal along with its incident arcs  makes the digraph fall apart into vari ous separate components  In the digraph of our example network  as shown in Figure    the articulation nodes are the nodes C  D  H  I  and L  for the relevant network  de picted in Figure    node C is the only articulation node  however  Articulation nodes are identified using a depth first search algorithm  for details  we refer the reader to  Cormen et al          Theorems   and   now state impor tant properties of a pivot node that allow for its identifica tion  Theorem    Theorem   Let Q  G     be a relevant qualitative probabilistic network  let E and I be as before  The pivot node for I and E is unique     Proof   sketch   From Definition   we have that the rele vant network consists of only nodes that reside on an un blocked chain from the newly observed node E to the node of interest I  From the definition of articulation node  we further have that every such chain must include all articula tion nodes in the relevant network  In fact  every reasoning chain from E to I visits the articulation nodes in the same order  From Definition   we have that no two pivot nodes can reside on the same unblocked chain to the node of in terest  We conclude that the pivot node is unique     From the proof of Theorem   we have that the articula tion nodes in a relevant network allow a total ordering  We number the articulation nodes  together with the node of in terest I  from I  for the node closest to the newly observed node  to m  for the node of interest  The pivot node now is the node with the lowest ordering number for which an unambiguous sign would uniquely determine an unambigu ous sign for the node of interest  To identify the pivot node  our algorithm starts with investigating the articulation node closest to the node of interest  this node is numbered m      The algorithm investigates whether an unambiguous sign for this candidate pivot node would result in an unambigu ous sign for the node of interest upon sign propagation  By propagating a     from the candidate pivot node to the node of interest I  the node sign resulting for I is the sign of the net influence of the candidate pivot node on I  If this sign is ambiguous  then the node of interest itself is the pivot node  Otherwise  the algorithm proceeds by investigating the ar ticulation node numbered m    and so on  The algorithm is summarised in pseudocode in Figure       function ComputePivot Q    candidates    I  U FindArticulationNodes G   order the nodes from candidates from   to m  return FindPivot m       Let Q  G     be a relevant qualitative probabilistic network  let E be the newly observed node and let I be the node of interest  The pivot node for I and E is either the node of interest I or an articulation node in    function FindPivot i   pivot  G   PropagateSign node i node i      if sign node i          then return node i      else FindPivot i     Proof  sketch    By definition we have that every possible unambiguous node sign for the pivot node determines an unambiguous sign for the node of interest I  It will be ev ident that node I itself satisfies this property  Either the node of interest I or another node on an unblocked chain from E to I  therefore  is the pivot node  Now  suppose that node I is not the pivot node  As a sign for the pivot node uniquely determines the sign for I  we conclude that all influences exerted upon I must traverse the pivot node  Every unblocked chain from E to I  therefore  must include the pivot node  As a consequence  removing the pivot node along with its incident arcs from the relevant network will cause the network to fall apart into separate components  We conclude that the pivot node is an articulation node     pivot        Figure    The Algorithm for Computing the Pivot Node       CONSTRUCTING RESULTS  From its definition  we have that there must be two or more different reasoning chains in the relevant network from the newly observed node to the pivot node  the net influences along these reasoning chains are conflicting or ambiguous  Our algorithm focuses on the ambiguity at the pivot node and identifies the information that would serve to resolve it  For this purpose  the algorithm zooms in on the part   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       of the relevant network between the newly observed node and the pivot node  we call this part the pruned relevant network  Note that the pruned relevant network is readily computed by exploiting the property that the pivot node is an articulation node  From the pruned relevant network  the algorithm first selects the so called candidate resolvers  Definition   Let Q  G    be a relevant qualitative probabilistic network  let E be the newly observed node and let I be the network s node of interest  Let P be the pivot node for I and E  Now  let Qpru  G      be the pruned relevant network for P  A candidate resolver for P is a node Ri E V G     Ri  f  P  such that    where j j once again is used to denote the strength of the sign    We would like to note that as  in general  the resolu tion frontier includes a small number of nodes  the number of signs to be computed for the pivot node is limited  In ad dition  we note that the process of constructing informative results can be repeated recursively for the nodes in the pivot node s resolution frontier  until the newly observed node is reached  The basic algorithm is summarised in pseudocode in Figure    procedure ConstructResults Q pivot       Ri    sign Ri      and in degree Ri J           Qpru    ComputePrunedNetwork Q pivot   candidates    ComputeCandidates Qpru pivot   output ComputeResults Qpru pivot candidates   E  or    function     From among the candidate resolvers in the pruned relevant network  our algorithm now constructs the resolution fron tier  We recall that the resolution frontier is a minimal set of nodes for which unambiguous node signs would uniquely determine the signs of the separate influences on the pivot node   function ComputeFrontier pivotfrontier  candidates   frontier for all Vi  such that  Vi pivot  or  pivot  Vi  on a reasoning chain from E do if Vi E candidates thenfrontier   frontier U  Vi  else ComputeFrontier Vi Jrontier candidates      The resolution frontier can be constructed by recursively traversing the various reasoning chains from the pivot node back to the observed node E and checking whether the nodes visited are candidate resolvers  Once the resolution frontier has been identified from the pruned relevant network  the algorithm constructs a  con ditional  sign for the pivot node in terms of signs for the nodes from the frontier  Let F be the resolution frontier for the pivot node P  For each resolver Ri E F  let s   j       denote the signs of the various different reasoning chains from Ri to the pivot node  For each combination of node signs sign Ri   Ri E F  the sign of the pivot node is computed to be if  I EB sign R  s        I EB sign R  s    then sign PJ               sign Ri   l l  s          sign Ri   l l  s   I  else sign P          I   ComputeResults Qpru pivot candidates    frontier    ComputeFrontier pivot    candidates   for all Ri E frontier do determines    j      for all R  Efrontier and sign R      do return inequality       The candidate resolvers for the pivot node are easily iden tified from the pruned relevant network   Definition   Let Q  G    be a pruned relevant quali tative probabilistic network  let E and I be as before  Let P be the pivot node for I and E  and let R be the set of candidate resolvers for P  as defined in Definition    The resolution frontier F for P is the maximal subset of R such that for each candidate resolver Ri E F there exists at least one unblocked chain from E via Ri to P such that no node Rj E R resides on the subchain from Ri to P        Figure    The Algorithm for Constructing Results  To conclude  we would like to note that for computing in formative results for a relevant network s pivot node  the pruned network can be even further restricted  To this end  a so called boundary node can be identified for the newly observed node  The boundary node is the articulation node closest to the node of interest that has an unambiguous node sign after propagation of the observation entered  Con structing results can then focus on the part of the relevant network between the pivot node and the boundary node  Moreover  if the thus pruned network includes many artic ulation nodes  it may very well be that trade offs exist be tween the articulation nodes numbered k   and k  but not between k and k      Distinguishing between these com ponents is straightforward and allows for further focusing on the actual trade offs involved in inference        CONCLUSIONS  We have presented a new algorithm for dealing with trade offs in qualitative probabilistic networks  Rather than re solve trade offs by providing for a finer level of representa tion detail  our algorithm identifies from a qualitative prob abilistic network the information that would serve to re solve the trade offs present  For this purpose  the algorithm zooms in on the part of the network where the actual trade offs reside and identifies the pivot node for the node of in    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            terest  The sign of the pivot node uniquely determines the sign of the node of interest  For the pivot node  a more informative result than ambiguity is constructed in terms of values for the node s resolvers and the relative strengths of the influences upon it  This process of constructing in formative results can be repeated recursively for the pivot node s resolvers  As we have already mentioned in our introduction  we be lieve that qualitative probabilistic networks can play an im portant role in the construction of quantitative networks for real life application domains  as well as for explanation of their reasoning processes  For the purpose of explanation  qualitative probabilistic networks have been proposed be fore  The concept of pivot node for zooming in on trade offs and constructing insightful results for a network s node of interest is a very powerful concept to enable explanation of complex reasoning processes in quantitative probabilis tic networks  Acknowledgments  This work was partially supported by the EPSRC under grant GR L      and a Ph D  studentship  
  Qualitative probabilistic networks have been introduced as qualitative abstractions of Bayesian belief networks  One of the ma jor drawbacks of these qualitative networks is their coarse level of detail  which may lead to unresolved trade offs during inference  We present an enhanced formalism for qualita tive networks with a finer level of detail  An enhanced qualitative probabilistic net work differs from a regular qualitative net work in that it distinguishes between strong and weak influences  Enhanced qualitative probabilistic networks are purely qualitative in nature  as regular qualitative networks are  yet allow for efficiently resolving trade offs during inference      INTRODUCTION  The formalism of Bayesian belief networks is generally considered an intuitively appealing and powerful for malism for capturing complex problem domains along with their uncertainties  The usually large number of probabilities required for a belief network  how ever  tends to pose a major obstacle to their applica tion  To mitigate this obstacle  qualitative probabilis tic networks have been introduced as qualitative ab stractions of Bayesian belief networks  Wellman         Like a Bayesian belief network  a qualitative proba bilistic network encodes variables and the probabilis tic interrelationships among these variables in a di rected graph  the relationships are not quantified by conditional probabilities as in a belief network  but are summarised by qualitative signs instead  For inference with a qualitative probabilistic network  an elegant al gorithm is available  based on the idea of propagating signs  Druzdzel   Henrion          One of the major drawbacks of qualitative probabilis tic networks is their coarse level of detail  As a conse quence of their high abstraction level  qualitative prob abilistic networks do not provide for modelling the intricacies involved in weighing conflicting influences and  hence  do not provide for resolving trade offs  In ference with a qualitative probabilistic network for a real life domain of application  therefore  quite often leads to ambiguous results  Ambiguous results in inference can be averted by en hancing the formalism of qualitative probabilistic net works to provide for a finer level of detail  Roughly speaking  the finer the level of detail  the more trade offs can be resolved during inference  The problem of trade off resolution within the framework of quali tative networks has been addressed before by others  S  Parsons has introduced  for example  the concept of categorical influences  A categorical influence is either an influence that serves to increase a probability to   or an influence that decreases a probability to    re gardless of any other influences  and thereby resolves any trade off in which it is involved  Parsons         C     Liu and M P  Wellman have designed two meth ods for resolving trade offs based upon the idea of re verting to numerical probabilities whenever necessary  Liu   Wellman         While only some trade offs can be resolved by the use of categorical influences  the methods of Liu and Wellman provide for resolving any trade off  Their methods  however  require a fully specified  numerical belief network  We would like to mention that various other approaches to dealing with uncertainty in a qualitative way have been proposed in the literature  These approaches are not tailored for use within the framework of qualitative probabilistic networks and therefore will not be reviewed here  To provide for trade off resolution without resorting to numerical probabilistic information  we have designed an intuitively appealing formalism of enhanced quali tative networks  An enhanced qualitative probabilis tic network differs from a regular qualitative network        Renooij and van der Gaag  in that it distinguishes between strong and weak in fluences  For inference  we have generalised the sign propagation algorithm for regular qualitative networks to deal with the strong and weak influences of an en hanced qualitative network  Trade off resolution dur ing inference is based on the idea that strong influences dominate over conflicting weak influences  The paper is organised as follows  In Section    we provide some preliminaries from the field of qualita tive networks to introduce our notational conventions  In Section    we present the formalism of enhanced qualitative probabilistic networks  In Section    we detail various properties of these enhanced networks  thereby providing for a sign propagation algorithm for inference  The paper is rounded off with some conclu sions and directions for future research in Section       PRELIMINARIES  Qualitative probabilistic networks have been intro duced as abstractions of Bayesian belief networks  Be fore addressing qualitative networks  we briefly review their quantitative counterparts  A Bayesian belief net work is a concise representation of a joint probability distribution on a set of statistical variables  It encodes  in an acyclic directed graph  the variables concerned along with their probabilistic interrelationships  Each node in the digraph represents a variable  the prob abilistic relationships between the variables are cap tured in the digraph s set of arcs  Associated with each variable is a set of conditional probability dis tributions describing the relationship of this variable with its  immediate  predecessors in the digraph  We introduce a small Bayesian belief network that will serve as our running example throughout the paper  Example     We consider the small belief network shown in Figure    The network represents a fragment Pr a           I     Pr d tJ    Pr d tf   I  iil  Pr f Pr   a  Pr t        Pr t a                       ij l  Pr d Pr d tf  I  I                                  Figure    The Antibiotics Belief Network  of fictitious and incomplete medical knowledge  per taining to the effects of administering antibiotics on a patient  Node A represents whether or not a pa tient takes antibiotics  Node T models whether or not the patient has typhoid fever and node D represents presence or absence of diarrhoea in the patient  Node  F  to conclude  describes whether or not the composi tion of the patient s bacterial flora has changed  Ty phoid fever and a change in the patient s bacterial flora are modelled as the possible causes of diarrhoea  An tibiotics can cure typhoid fever by killing the bacteria that cause the infection  However  antibiotics can also change the composition of the patient s bacterial flora  thereby increasing the risk of diarrhoea  D Qualitative probabilistic networks bear a strong re semblance to their quantitative counterparts  A qual itative probabilistic network also comprises an acyclic digraph modelling variables and probabilistic interre lationships among variables  Instead of conditional probability distributions  however  a qualitative prob abilistic network associates with its digraph qualitative influences and qualitative synergies  Wellman         A qualitative influence between two nodes expresses how the values of one node influence the probabilities of the values of the other node  A positive qualita tive influence of node A on its  immediate  successor B  denoted s  A B   expresses that observing higher values for A makes higher values for B more likely  regardless of any other direct influence on B  that is  Pr b I ax   Pr b I iix     for any combination of values x for the set  r B     A  of  immediate  predecessors of B other than A  A negative qualitative influence  denoted by s   and a zero qualitative influence  denoted by S   are defined analogously  replacing  in the above formula by     and    respectively  If the influence of node A on node B is not monotonic or unknown  we say that it is ambiguous  denoted S   A B   The set of influences of a qualitative probabilis tic network exhibits various convenient properties  Wellman         The property of symmetry guar antees that  if the network includes the influence s  A  B   then it also includes s  B  A   The prop erty of tmnsitivity asserts that qualitative influences along a trail  that specifies at most one incoming arc for each node  combine into a single influence with the  operator from Figure    The property of compo sition asserts that multiple qualitative influences be tween two nodes along parallel chains combine into a single influence with the   operator                                                  Figure    The      Ell                               and   Operators           Enhancing QPNs for Trade off Resolution  From Figure    we have that combining parallel qual itative influences with the Ell operator may yield an ambiguous result  Such an ambiguity  in fact  results whenever influences with opposite signs are combined  We say that the trade off that is reflected by the con flicting influences cannot be resolved  Note that  in contrast with the Ell operator  the  operator cannot introduce ambiguities upon combining signs of influ ences along trails   bilities of the original belief network  in real life appli cations  these relationships are elicited directly from domain experts  For reasoning with a qualitative probabilistic network  an elegant algorithm is available from M J  Druzdzel and M  Henrion         this algorithm is summarised in pseudocode in Figure    The basic idea of the algoprocedure  Propagate Sign from  to  message   sign  to       sign to  Ell message  for each  induced  neighbour V  of to do linksign      sign of  induced  influence between to and V   message      sign to   linksign  if V  f  from and V    Observed and sign V   f  sign V   Ell message then Propagate Sign to  V   message   In addition to influences  a qualitative probabilistic network includes synergies  that express how the value of one node influences the probabilities of the values of another node in view of a given value for a third node  Henrion   Druzdzel         A negative product synergy of node A on node B  and vice versa  given the value c for their common successor C  denoted x   A B  c   expresses that  given c  higher values for A render higher values for B less likely  that is  Pr c I abx   Pr c I abx   Pr c I abx   Pr c I abx   S   for any combination of values x for the set       C     A B  of predecessors of C other than A and B  A product synergy induces a qualitative influence be tween the predecessors of a node upon observation  the induced influence is coined an intercausal influ ence  Positive  zero  and ambiguous product synergies again are defined analogously  Example     We consider the qualitative abstraction of the Antibiotics belief network from Figure    From the conditional probability distributions specified for node T  we have that Pr t I a      Pr t I a   S    and therefore that s  A T   we further find that s  A F    s  T D   and s  F D   Either value for node D  in addition  induces a negative intercausal influence between the nodes T and F  The result ing qualitative probabilistic network is shown in Fig ure    D  Figure    The Qualitative Antibiotics Network   We would like to note that  although in the previous example we have computed the qualitative probabilis tic relationships among the variables from the proba        Figure    The Sign Propagation Algorithm  rithm is to trace the effect of observing a node s value on the other nodes in the network by message passing between neighbouring nodes  For each node  a sign is determined  indicating the direction of change in the node s probabilities occasioned by the new observation given all previously observed node values  Initially  all node signs equal      For the newly observed node  an appropriate sign is entered  that is  either a     for the observed value true or a     for the value false  The node updates its sign and subsequently sends a message to each neighbour and every node on which it exerts an induced intercausal influence  The sign of this message is the  product of the node s  new  sign and the sign of the influence it traverses  This process is repeated throughout the network  building on the properties of symmetry  transitivity  and composition of influences     THE ENHANCED FORMALISM  Qualitative probabilistic networks model a problem domain at a coarse level of detail  This coarseness of representation is most visible in the way relation ships among variables are captured  the relationships are summarised by qualitative influences without any indication of their strengths  As a consequence of the coarse level of detail  any trade off encountered dur ing inference will remain unresolved  To allow for re solving trade offs in a qualitative way  we enhance the formalism of qualitative probabilistic networks by as sociating a relative strength with influences  If in a trade off  for example  the positive influence is known to be stronger than the conflicting negative one  we may then conclude the combined influence to be posi tive  thereby resolving the trade off         Renooij and van der Gaag  In our formalism of enhanced qualitative probabilistic networks  we distinguish between strong and weak in fluences  We begin by focusing on the strong and weak positive influences  The basic idea is to partition the set of all positive influences into two disjoint sets of in fluences in such a way that any influence from the one subset is stronger than any influence from the other subset  To this end  a cut off value   is introduced  This value serves to partition the set of qualitative influences into a set of influences that capture a differ ence in probabilities larger than   and a set of influ ences that model a difference smaller than    An influ ence from the former subset will be termed a strongly positive influence  an influence from the latter subset will be termed a weakly positive influence  More formally  a strongly positive qualitative influence of a node A on its successor B  denoted s   A B   expresses  first and foremost  that observing higher values for A makes higher values for B more likely  regardless of any other influence on B  in addition  it expresses that Pr b I ax      Upon abstracting a Bayesian belief network to an en hanced qualitative probabilistic network  the cut off value   needs to be chosen explicitly  This cut off value will typically vary from application to applica tion  Note that it is always possible to choose a cut off value  as the value       yields a trivial partitioning of the set of influences  Example     We consider once again the Antibiotics belief network from Example      Suppose that we choose for our cut off value           For the influence of node A on node T  we now find that Pr t I a    Pr t I a      and I Pr t I a    Pr t I a  I              We therefore conclude that s   A  T   We further find that s   T  D   s  A  F   and s  F  D   The resulting enhanced qualitative probabilistic network is shown in Figure       Pr b I ax          for any combination of values x for the set  r  B     A  of predecessors of B other than A  where   is the cut off value used  A weakly positive qualitative influence of A on B  denoted s  A  B   is a positive qualitative influence such that Pr b I  ax        Figure    The Enhanced Antibiotics Network   Pr b I ax      for any combination of values x for the set  r B     A  of predecessors of B other than A  where   once again is the cut off value used  Strongly negative qualitative influences  denoted s    and weakly negative qualita tive influences  denoted s   are defined analogously  zero qualitative influences and ambiguous qualitative influences are defined as in regular qualitative proba bilistic networks  In the sequel  we will use the phrase strong influences to refer to both strongly positive and strongly negative influences  the phrase weak influ ences is meant to have an analogous meaning  We further say that a product synergy is strongly negative if it induces a strongly negative intercausal influence  Strongly positive product synergies are defined analo gously  zero product synergies and ambiguous product synergies again are defined as in regular qualitative networks  We would like to note that  in our enhanced formal ism  the meaning of the sign of an influence has slightly changed  W hile in a regular qualitative probabilistic network  the sign of an influence represents the sign of a difference in probabilities only  in an enhanced qual itative network a sign in addition captures the relative magnitude of the difference   We would like to note that  in real life applications of enhanced qualitative probabilistic networks  a cut off value need not be established explicitly  The parti tioning into strong and weak influences then is elicited directly from the domain experts involved in the con struction of the network     INFERENCE WITH AN ENHANCED NETWORK  For inference with a regular qualitative probabilistic network  an elegant algorithm is available  We recall from Section   that this algorithm builds on the idea of propagating signs throughout a network and com bining them with the    and Ell operators  We fur ther recall that the algorithm exploits the properties of symmetry  transitivity  and parallel composition of influences  To generalise the idea of sign propagation to inference with an enhanced qualitative probabilis tic network  we enhance  in the Sections     and      the    and Ell operators to provide for the properties of transitivity and parallel combination of strong and weak influences  in Section      we address the prop erty of symmetry    Enhancing QPNs for Trade off Resolution       ENHANCING THE    PERATOR  For propagating qualitative signs along trails of nodes in an enhanced qualitative probabilistic network  we enhance the   operator that is defined for regular qualitative networks  to apply to strong and weak in fluences  We recall that the   operator basically pro vides for multiplying signs of influences  In a regular qualitative probabilistic network  an influence captures a difference between two probabilities  Upon multiply ing the signs of two influences  therefore  the sign of the result of the multiplication of two such differences is computed  In our formalism of enhanced qualitative probabilistic networks  we have added an explicit no tion of relative magnitude to influences  It will be ev ident that these relative magnitudes need to be taken into consideration when multiplying signs  To address the effect of multiplying two signs in an en hanced qualitative probabilistic network  we consider the network fragment shown in Figure    The frag      Figure    A Fragment of a Network  ment includes the trail of nodes A  B  C  with two qualitative influences between them  in addition  X denotes the set of all predecessors of B other than A  and Y is the set of all predecessors of C other than B  For the qualitative influence of A on C we have that Pr c I axy   Pr c I axy     Pr c I by   Pr c I by    Pr b I ax   Pr b I ax   for any combination of values x for the set of nodes X and any combination of values y for the set Y  Suppose that both qualitative influences in the net work fragment under consideration are strongly posi tive  that is  we have that s   A  B  and s   B  C   suppose that we have used the cut off value   for dis tinguishing between strong and weak influences  From the expression stated above for the influence of node A on node C  we now find that Pr c I axy   Pr c I axy       for any combination of values xy for the set of nodes XU Y  Since          we have that     S    Upon mul tiplying the signs of two strong influences  therefore        a sign results that expresses an influence that may or may not be stronger than a single weakly positive in fluence  Now suppose that both qualitative influences in the network fragment from Figure   are weakly positive  that is  we have that s  A  B  and s  B  C   For the influence of node A on node C  we now find that Pr c I axy   Pr c I axy   S  j  for any combination of values xy for the set X U Y  While the influence resulting from the multiplication of two strong influences cannot be compared to a sin gle weak influence  the above observation shows that the resulting influence will always be at least as strong as an influence resulting from the multiplication of two weak influences  To provide for comparing qualitative influences along different trails with respect to their magnitude  as required for trade off resolution  there fore  we need to retain the length of the trail in the network over which influences have been multiplied  To provide for comparing qualitative influences along different trails  we augment every influence s sign by a superscript  called the sign s multiplication index  A strongly positive qualitative influence with multiplica tion index i of node A on node B  written s     A  B   is now taken to denote that Pr b I ax   Pr b I ax    i for every combination of values x for the set X of pre decessors of B other than A  A weakly positive quali tative influence with multiplication index i of A on B  written s    A  B   is taken to indicate that    s Pr b I ax   Pr b I ax   s  i for every combination of values x for the set X  The signs associated with the arcs of the digraph are inter preted as having a multiplication index equal to    Building on the concept of multiplication index  Fig ure   shows the table for the enhanced   operator  From the table  it is readily seen that the          and   signs combine as in a regular qualitative probabilistic network  the difference is just in the handling of the multiplication indices  In the table  there appear signs    and     we will elaborate on the meaning of these signs in Section      We like to further comment on the combination of the signs    and   i  In doing so  we consider once again the network fragment from Figure    Suppose that we have s    A  B  for the influence of node A on node B  and s     B  C  for the influence of B on C  The weakly positive influence of A on B expresses that Pr b I ax   Pr b I ax   s  i        Renooij and van der Gaag                j   j                 i   j    i  j                             i j  i             i      j  i i  j                                        i j   j  i i  j                                  i     i             j     i j                    i    i j                    F igure    The Enhanced  Operator  for every combination of values x for the set X of pre decessors of B other than A  The strongly positive qualitative influence of B on C further expresses that Pr c I by   Pr c I by     i for every combination of values y for the set Y of pre decessors of C other than B  For the influence of A on C  we now find that Pr c I axy   Pr c I axy    i  B  C  respectively  between the nodes A and C  and various qualitative influences  in addition  X denotes the set of all predecessors of B other than A  and Y is the set of all predecessors of C other than A and B  For the net qualitative influence of node A on node C along the two parallel trails  we have that Pr c I axy   Pr c I axy     Pr c I aby   Pr c I aby   Pr b I ax     Pr c I aby   Pr c I aby   Pr b I ax     Pr c I aby   Pr c I aby       for every combination of values xy for the set X U Y    We therefore conclude that s   A  C   So             i     Similar observations apply to any multiplication of a weak and a strong influence       for any combination of values x for the set of nodes X and any combination of values y for the set Y   ENHANCING THE EB OPERATOR  For combining multiple qualitative influences between two nodes along parallel trails in an enhanced quali tative network  we enhance the EB operator that is de fined for regular qualitative probabilistic networks  to apply to strong and weak influences  We recall that the EB operator basically provides for adding signs of influ ences  We further recall that  upon adding the signs of two conflicting influences in a regular qualitative net work  the represented trade off cannot be resolved and an ambiguous influence results  In our formalism of enhanced qualitative probabilistic networks  we have added an explicit notion of relative magnitude to in fluences  These relative magnitudes can now be taken into consideration when adding the signs of conflict ing influences and used to resolve trade offs  thereby forestalling ambiguous results  When addressing the enhanced  operator  in the pre vious section  we have argued that the multiplication of two influences yields an influence of possibly smaller magnitude  We will now see that the addition of two influences  in contrast  may result in an influence of larger magnitude  To address the effect of adding two signs in an enhanced qualitative probabilistic network  we consider the network fragment shown in Figure    The fragment includes the parallel trails A  C  and A   Figure    Another Network Fragment  Suppose that all qualitative influences in the network fragment under consideration are weakly positive  that is  we have that s  A  B   s  B  C   and s  A  C   suppose that we have used the cut off value   for dis tinguishing between strong and weak influences  The net influence of node A on node C equals the sum of the influence with sign    along the trail A  C  and the influence with sign    along the trail A  B  C  From the expression stated above for the net influence of A on C  we find that Pr c I axy   Pr c I axy      The minimum of this difference is attained  for exam ple  for Pr c I aby       which enforces Pr c I aby       and Pr b I ax    Pr b I ax       We further find that Pr c I axy   Pr c I axy            Enhancing QPNs for Trade off Resolution  E  i                              m                 i        i                   b           c                                i                    j     i                          a       j    d     j                 j                                    where m   min i j    a     if i      j     otherwise b        if j      i     otherwise  c     if i       j     otherwise d     if j       i     otherwise  Figure    The Enhanced E  Operator  This maximum is attained  for example  for Pr c I aby       Pr c I aby          Pr c I aby             Pr c I aby          and Pr b I ax       In computing the maximum of the difference we have used explicitly the information that all influences are weakly positive  From the maximum attained  it is readily seen that the addition of two weakly positive influences yields a result that may or may not be stronger than a weakly positive influence  In general we have that the result of adding two positive or two negative influences is at least as strong as the strongest of the influences added  From the preceding observations  we have that the qualitative influence that results from adding two weakly positive influences is either weakly positive or strongly positive  So although the resulting influence is known to be positive  its relative magnitude is un known  To capture this ambiguity we use    to denote the influence s sign  An ambiguously positive qualita tive influence of node A on node C written s   A  C   is therefore taken to indicate that        Pr c I axy   Pr c I axy            for any combination of values xy for the set X U Y  Similarly     is used to denote an ambiguously nega tive qualitative influence  The enhanced EB operator is shown in Figure    From the table  it is readily seen that the          and   signs combine as in a regular qualitative probabilistic network  the difference is just in the handling of the multiplication indices and the ambiguity subscripts  We like to further comment on the resolution of trade offs using the enhanced EB operator  In doing so  we consider once again the network fragment from Fig ure    Suppose that we have s   A  C  for the direct influence of node A on node C  and that we further have s   A B  and s  B C   The net influence of node A on node C equals the sum of the influence with sign     along the trail A  C and the influence with sign    along the trail A  B  C  From the expression for the net influence of A on C we find that Pr c I axy    Pr c I axy          The minimum for the difference is attained  for ex ample  for Pr c I aby         Pr c I aby       Pr c I aby      Pr c I aby      and Pr b I ax   Pr b I ax      In computing the minimum of the difference  we have once again exploited the information with re gard to the signs and relative magnitudes of the in fluences involved  From the minimum attained  it is readily seen that the net influence of node A on node C is positive  However as           the net influence may either be strong or weak  We conclude that the net influence of A on C is ambiguously positive  So       EB         Similar observations apply to various other trade offs         THE PROPERTY OF SYMME TRY  The sign propagation algorithm for inference with a regular qualitative network explicitly builds on the properties of symmetry transitivity and parallel com position of influences  We have so far addressed the    and E  operators and have thereby guaranteed the transitivity and parallel composition properties of in fluences  We now focus on the property of symmetry to enable the propagation of qualitative influences over a single arc in the network in both directions  In a regular qualitative probabilistic network  the property of symmetry guarantees that  if a node A exerts an influence on a node B  then node B exerts an influence of the same sign on node A  In an en hanced qualitative network  an influence and its re verse also are both positive or both negative  The symmetry property however  does not hold with re gard to the relative magnitudes of an influence and its reverse  The reverse of a strongly positive qualitative influence may be a weakly positive influence  and vice versa  As the relative magnitude of the reverse of a positive influence is unknown  the reverse is taken to be ambiguously positive  A similar observation applies to the reverse of a negative influence  To conclude  we would like to mention that an alter native way of ensuring that the property of symmetry holds in an enhanced qualitative network is to spec         Renooij and van der Gaag  ify the signs of all reversed influences explicitly  these signs will then have to be elicited from the domain experts involved in the network s construction       TRADE OFF RESOLUTION  AN EXAMPLE  In the previous sections  we have argued that the properties of symmetry  transitivity  and parallel com position of influences hold in an enhanced qualita tive probabilistic network  The sign propagation algo rithm from Section   therefore is generalised straight forwardly to apply to enhanced qualitative networks  instead of the regular  and E  operators  it just has to use the enhanced operators for propagating and com bining influences  We illustrate the application of the algorithm by means of our running example  Example     We consider once again the qualitative Antibiotics network from Figure    Suppose that we enter the sign   for node A  Node A propagates this sign towards node T  Node T thereupon receives the sign         and sends it to node D  Node D in turn receives the sign           it does not pass on any sign  Node A also sends its positive sign to node F  Node F receives the sign          and passes it on to node D  Node D then receives the additional sign          The two signs that enter node D are combined and result in the ambiguous sign   E          Now  consider the enhanced Antibiotics network from Figure    We enter the sign     for node A  this sign reflects a positive observation for A  We once again apply the sign propagation algorithm  this time using our enhanced operators  Recall that initially all influ ences  signs have a multiplication index of    Node A propagates its sign towards node T  Node T receives the sign                  and sends it to node D  Node D receives                    Node A sends its sign     also to node F  Node F there upon receives the sign              and passes it on to node D  Node D receives the additional sign              Combining the two signs that enter node D results in the sign      E          Note that  while in the regular qualitative network the rep resented trade off cannot be resolved and results in an ambiguous influence  the trade off is resolved in the enhanced qualitative probabilistic network       Conclusions and further research  One of the major drawbacks of qualitative probabilis tic networks is their coarse level of detail  Although it may suffice for some problem domains  the coarse ness of detail may lead to unresolved trade offs dur ing inference in other domains  To provide for re   solving trade offs  we have enhanced the formalism of qualitative probabilistic networks by distinguish ing between strong and weak influences  We have enhanced the multiplication and addition operators to guarantee the transitivity and parallel composition properties of influences  thereby generalising the basic sign propagation algorithm to apply to enhanced qual itative networks  We have shown that our formalism provides for resolving trade offs in a qualitative  yet efficient way  Our formalism of enhanced qualitative probabilistic networks does not provide for resolving all possible trade offs during inference  Since qualitative abstrac tions do not have the same expressiveness as numerical belief networks  it is hardly likely that any qualitative abstraction will be able to resolve all possible trade offs  We suspect  however  that in our enhanced signs more information is hidden than we currently exploit upon multiplying and adding influences  In the near future  we will therefore investigate whether still more trade offs can be resolved within the framework of our enhanced qualitative networks  In addition  we will address the non associativity of the addition operator for influences and design heuristics to forestall unnec essary ambiguous results  To conclude  we will extend our formalism to incorporate non binary variables  
 Many AI researchers argue that probability theory is only capable of dealing with uncertainty in situations where a fully specified joint probability distribution is available  and conclude that it is not suitable for application in AI systems  Probability intervals  however  constitute a means for expressing incompleteness of information  We present a method for computing probability interval  for probabilities of interest from a partial specification of a joint probability distribution   Our method improves on  erties of this graph are then exploited for computing precise intervals  In Section   we introduce the notion of a partial specification of a joint probability distribution  Fur thermore  the foundation for our method for comput ing probability intervals from such a partial specifi cation is layed  In Section   we discuss how indepen dency constraints can be taken into consideration  In Section   we briefly point out that our method can be of real help in the process of knowledge acquisition for so called belief networks   earlier approaches by allowing for independency relation ships between statistical variables to be exploited       Introduction  The adversaries of probability theory for dealing with uncertainty in AI systems often argue that it is not expressive enough to cope with the different kinds of uncertainty that are encountered in real life sit uations  More in specific  it has been argued that probability theory is not able to distinguish between uncertainty and ignorance due to incompleteness of information  The suitability of probability intervals for expressing incompleteness has been pointed out decisively by J  Pearl in  Pearl      a   In this pa per  we present a framework for computing probabil ity intervals from an incomplete set of probabilities  The general idea of our approach is to take the ini tially given probabilities as defining constraints on a yet unknown joint probability distribution  Several authors have already addressed the problem of com puting probability intervals  see for example  Cooper        and  Nilsson         Our approach differs from the mentioned ones by taking independency relation ships between the statistical variables discerned into consideration  In order to do so  we assume that the independencies in the unknown distribution are spec ified in a special type of graph  The topological prop      Computing  Probability In  tervals In this section  we concentrate ourselves on the notion of a partial specification of a joint probability distri bution and develop a method for computing proba bility intervals for probabilities of interest from such a partial specification  For the moment  we assume that no independencies between the statistical vari ables discerned exist  We begin by introducing some terminology  Let    a         a    be a free Boolean algebra gen erated by a set of atomic propositions A    al      a     n     alternatively  the Boolean al gebra   a        a    may be viewed as a sample space being  spanned  by a set of statistical variables Ai  i            n  each taking values from  ai   ai  A partial specification of a joint probability distribution  on B a         a    is a total function P   C          where C     a           a     We call such a partial specification consistent if there exists at least one joint probability distribution Pr on   such that Pr is an extension of P onto B a       a     notation  Pr lc   P   otherwise  P is said to be inconsis tent  Furthermore  we say that P  uniquely  defines Pr if Pr is the only joint probability distribution on B at      a    such that Prlc   P  Now  let Bo be        I I  the subset of B a        an  consisting of its  smallest  following inhomogeneous system of linear  elements   that is  let          equations     P   I  n Bo       Ld L   a  or     a   a  E  A  i l  We state some convenient p rop er ties of this set Bo  Let th e elements of Bo be enumera ted as b   i            n  Then  for any joint probability distribu tion Pr on B a        an  we have that   I        if j  I   if j E Ie             n  in which Ie  is the index set for Cj E Th is system of linear equations has the    unkn o wn  where d  j    Xt   I  Pr b              I         x      Now  let  right hand sides and  p   ll  denote the column  Fur   vector  the vector of unknowns   cl of  thermore  let D denote the coefficient matrix of th sys tem  We will use the matrix equation D     p t The probabilitie s Pr b   for the elements b  E    will denote the system of linear equations obtained from be called the constituent probabilities of Pr  Fur ther  a partial specification P as described We ha ve th e follo wing relation between extension more  for each element b E B a        an  there exists a uniqu e set of indic es Ib E             n  such that of a consistent partial specification of a joint probabil b   V b   this set Ib will be called the index set ity distribution and solutions to the matrix equatio i l  above   iEZ   for b  For each joint p rob ability distribution Pr on B  a        a n  we then have that Pr b     obtained from it    n  i                is a solution to D ll  i E Z b   In addition  it can easily be shown that any consistent partial specification P               defined on     uniquely defines a joint probability distribution Pr   a          an   In the sequel    an  as long as we will write   instead of   a       ambiguity cannot occur  We exploit the set  o and its properties for com puting probability intervals from an arbitrary partial specification  Suppose that we are given probabilities for a number of arbitrary elements of the Boolean al gebra     that is  we consider the case in which we are  given a consistent partial specification P of a joint probability distribution on   that is defined on an arbitrary subset C     The problem of computing probability intervals from P will now be transformed into an equivalent problem in linear algebra  The  Pr on  I l    p     Pr on B  such  that Pr I c     C    c     cm   m         be a subset of    P  C          be a consistent partial speci fication of a joint probability distribution on B  We Let      Note that although every joint probability distri  l    Dz  p corresponds with a  probabilistic  extension P  Dz   p may have solutions in which at leas one of the x   s is less than zero  It can easily be shown that the problem of find ing for a given   E B the least upper bound to the probability of b relative to a partial sp eci fic a ti on Pi   of  equivalent to the following linear programming pro lem         constituent probabilities Pr b       E     of Pr be de noted by z    i         n  and let the initially speci  m  fied probabilities P c     Pr c    e  e C  i       L CjZj   Pr b            be denoted by  p    Using       and       we    i l  subject to      i     diJZi  p    i      obtain the   ii   z   lll  I  maximize  and let  now consider an arbitrary  yet unknown  joint prob ability distribution Pr on B with Pr I c  P  Let the  Rl  bution Pr which is an extension of P corresponds uniquely with a solution to the matrix equatio Dz   p obtained from P  not every solution t  general idea is to take the initially given probabili ties as defining constraints on a yet unknown joint probability distribution   B  For any non ega ti v e solution vector  ll with com n ponents z   t                 to Dz   p  we hav that Pr       x   b  E Bo  de fin es a joint proba bility distribution  on the entire algebra    For any joint p roba bility distribution  such tha t Pr I c   P  we have that the vector of constituent probabilities x    Pr b    b  E         L Pr b    I l  fori            I Cl l            for J             n  I I  I        I  I  I  I  I  I  I I  I I  I  I  I  I I  I I  I  I     if j  Ib   and di j constitute the if j E I matrix D  A similar statement can be made for the greatest lower bound to the probability of b relative to P  Note that this linear programming approach can deal with conditional probabilities in the same way in which it handles prior ones  furthermore  the approach allows for initial specifications of bounds to probabilities instead of point estimates  It is well known that an LP problem can be solved in polynomial time  that is  polynomial in the size of the problem   Papadimitriou         The size of an LP problem is dependent  among other factors  upon the number of variables it comprises  Now note that the specific type of problem discussed in the foregoing has exponentially many variables  that is  exponential in the number of statistical variables discerned in the problem domain  Therefore  computing probability intervals requires an exponential number of steps     tionships exist and that the cliques are interrelated only through their intersections  In order to be able to exploit these properties  we further assume that all initially given probabilities are local to the cliques of G  Once more we introduce some new terminology  Let G    V G   E G   be a decomposable graph with the vertex set V G     V         Vn   n       and the clique set C   G     C         Clm   m       to be taken as a decomposable   map of an unknown joint probability distribution Pr  We take the graph from Figure l a  as our running example  Let B be the free Boolean algebra generated by  V  IV  E V G    fur thermore  for each clique C i  let B Cli     be the free Boolean algebra generated by  Vj I Vj E V C i    Now  let P be a partial specification of a joint proba bility distribution on B  recall that all initially given probabilities are local to the cliques of G   We say that P is consistent with respect to G if P can be extended in at least one way to a joint probability distribution Pr on B such that Pr is decomposable relative toG  that is  such that Pr can be expresssed   Exploiting Independency in terms of marginal distributions on the cliques of G  The initi ally given probabilities being local now Relationships allows us to apply the notions introduced in the pre In the preceding section we have presented a linear ceding section separately to marginal distributions on programming method for computing probability in th e cliques of G  We begin by taking the definition tervals from a consistent partial specification of a of a partial spec ification of a joint probability distri joint probability distribution  The initially assessed bution to apply to margin al distrbutions  a partial probabilities were viewed as defining constraints on specification of a marginal distribution on B  Cli  is a an unknown probability distribution  We assumed total function met    Ci          where Ci  B Cli   that no independency relationships existed between Note that we may now view Pas been defined by a the statistical variables discerned  In this section  set of partial specifications of marginal distributions the linear programming approach is extended with M   met  I C i E C  G    Furthermore  we take the an additional method for representing and exploiting notion of consistency to apply to partial sp ec ifi ca ti ons tions  we call such a partial spec independency relationships  Note that representing of marginal it can be extended in at least independency relationships in a straightforward man ification consistent tual marginal dist rib u tion  ner yields nonlinear constraints and therefore is not one way The analogy between the notions of a consistent suitable for our purposes  where Cj              We assume that the independency relationships be   partial specification of a joint probability distribution  tween the statistical variables have been specificed as  and a consistent partial specification of  an   map of the unknown joint probability distribu  distribution suggest that we may apply the linear  tion  programming method presented in the preceding  Pr   Informally speaking  an   map of  Pr  is an  undirected graph in which the vertices represent the  a marginal sec  tion separately to each of the partial specifications  statistical variables discerned and the missing edges  of marginal distributions associated with the  indicate the independencies holding between the vari  of G   ables  Furthermore  we assume that the fill in algo  of marginal distributions have been specified consis  rithm by R E  Tarjan and  M   Yannakakis has been  applied to yield a decomposable   map G of  Pr   An  cliques  However  even if all partial specifications  tently  they might still not give rise to a  joint prob  ability distribution that respects the independency  I  map is decomposable if it does not contain any ele  relationships shown in G  We therefore  mentary cycles of length four or more without a short  additional notions of consistency   define  some  cut  For further information  the reader is referred to   Pearl       b    We will show that we can take advan      The set M of partial specifications of  marginal if each  tage of the topology of G by observing that between  distributions is called locally consistent  the variables in a clique of G no independency rela   mcz  E M  i               m   is c o ns i ste nt          I  I  I  I  I  I  I  I I I   a   I separate systems of constraints re subsequetly   bined into one large system of constramts  thts  Figure      M is called globally consistent if there exists  a set M    J  cJ  I J  CJ       Cl         lj  of marginal distributions IJCI  on B Cl   such that for each clique Cl  E C  G   IJCI  is an ex tension of mcz  e M  and furthermore that for each pair of cliques Cl   Cli E Cl G  with V Cii n V CI      we have that J  CI  V Cl  n V Clj     IJcz  V CI   n V Cij    such a set M is called a global extension of M   It can be shown that global consistency of M is a necessary and sufficient condition for P being consis tent with respect to G  further details are provided in  Gaag         We now apply the linear programming method from the preceding section separately to each of the partial specifications of marginal distributions asso ciated with the cliques of G  For each clique Cl  E Cl  G  we now define a vector z  of constituent proba bilities of a yet unknown marginal distribution J  CI  in the manner described in Section    From the partial specification met  associated with clique Cl  we then obtain an appropriate system of linear constraints with the constituent probabilities as unknowns  This system will be denoted by D z    m   z       The  com   hnear   l  system will be denoted by Dz   m  z        To guarantee that every nonnea tive solution to the t u obtained system of constramts defines an extenston of the initially given probabilities to a joint proba bility distribution that is decomposable relative to G  we have to augment the system with some ad ditional constraints  called independency constraints  expressing that the set M of partial specifications marginal distributions has to be globally consistent  In theory we now have to obtain for each pair o cliques Cli  Cl  E C  G  with V Cli  n V  Cli       a number of constraints specifying that IJcz  V Cli  V Clj     J  Ct V CI   n V Clj    However  if we do so  we get any redundant constraints  in fact  the reader may verify that it suffices to obtain independency constraints from the clique intersections represented in a join tree of G only  Figure l b  shows a join tree Ta of our example graph  Note that the resulting independency constraints each in volve variables from two cliques only  In the sequel  the system of independency constraints for the intersection of two cliques Cli and Clj will be denoted  o nl   l         I  I  I  I  I  I  I  I  I  I  I  I  I I  I  I  I  I I  by T  ji  Tj ii      the system of independency constraints obtained from an entire clique tree of G will be denoted by T      From now on we will call D   m  T             the joint system of con straints  Analogous to our observations in Section    we have that the problem of finding for a given b e B  which is local to a clique of G   the least upper bound to the probability of b is equivalent to maximizing the probability of b subject to this joint system of constraints  Again  a similar statement can be made concerning the greatest lower bound to the probability of b  It should be evident that in the resulting probability interval the independency rela tionships shown in G have been taken into account properly  We can solve the linear programming problem dis cussed above using a traditional LP program or a de composition method like Dantzig Wolfe decomposi tion   Papadimitriou         In such a straightfor ward approach  however  the modular structure of the problem at hand is not fully exploited  We will present an algorithm for solving the problem in which the computations are restricted to local computations per clique only  First  we describe its basic idea in formally for our running example  Consider Figure   in which the join tree Ta of G has been depicted once more  this time explicitly showing the clique intersections  We view Ta as a computational architecture in which the vertices are autonomous ohjuts holding the local systems of con straints as private data  These objects are only able to communicate with their direct neighbours and only  through  the independency constraints  the edges are viewed as communication channels  The indepen dency constraints are used for relating variables from one clique to variables from another one  Now  sup pose that we are interested in the least upper bound to a probability of interest which is local to a specific clique  like the one shown in the figure  The object corresponding with the clique now sends a request for information about further constraints  if any  to its neighbours and then waits until it has received the requested information from all of them  For the mo ment  each  interior  object in the join tree just passes the request on to its other neighbours and awaits the requested information  As soon as a leaf  or the root  of the tree receives such a request for information  a second pass through the tree is started  The leaf com putes the feasible set of its local system of constraints and derives from it  by means of projection  the set of feasible values for the probabilities which are the constituent probabilities for the intersection s  with its neighbour s   This information then is passed on to these neighbours via the appropriate communica   tion channels using the independency constraints for  translation  of the variables  This results in the ad dition of extra constraints to the local system of con straints of these neighbours  These computations are performed by the interior vertices as well until the ob ject that started the computation has been reached again  The arcs in Figure   represent the flow of com putation from this second pass through the join tree  From its  extended  local system of constraints  the object that started the computation may now com pute the least upper bound to our probability of inter est  The result obtained is the same as when obtained directly from the joint system of constraints  The in tuition of this property is that when the process has again reached the object that started the computa tion  this object has been  informed  of all constraints of the entire joint system  By directing the same pro cess once more towards the root and the leaves of the tree  all objects can be brought into this state  So  in three passes through a join tree  each object locally has a kind of global knowledge concerning the joint system of constraints  It will be evident that for any probability of interest which is local to a clique we can now compute a probability interval locally  The  following algorithm describes  these  three  Without loss of generality we assume that the computation is started by the root Cl  of the clique tree Ta  It performs the following actions   passes      Send a request for information to all neighbours  and wait      If a return message  having the form of a system of constraints  has been received from all neigh bours  then add these systems of constraints to the local system of constraints D     m   z        compute the feasible set F  of the re sulting system and derive from it the  convex  set  T Jz l z  E F    for each neighbour Cli of Cl    Clj  send this information as a system of constraints to Cli us ing T J  T   i          For each such neighbouring clique  Each leaf Cli of Ta performs the following actions     Wait for a message      If a request for information is received  then com pute the feasible set F  of the local system of constraints DiZi   m   z      and derive from it the set  TiJZil Zi E Fi   for the neighbour Cl      Send this information as a system of constraints  to Cli using TaJZi  Tj  ZJ for a message         and then wait        I  I  I  I  I  maximize Pr v V         I  I  I  c ompuce  I  add  add  I  compute  I  add  compute  I  I  I l  Figure       If a system of linear constraints is received  then  add this system to the local system of constraints  D z      m   z        Cit be defined as the vertex on the path from Cl  to Cl  and let Cli be defined as the set of all other neighbours of Cl   Each interior vertex Cl  performs the following For each interior vertex  Cl    let the vertex  actions      Wait for a message          If systems of constraints have been received from all neighbours Clc E Cli  or from Ctt  respec tively   then add these additional systems of con straints to D z    m   z      compute the feasible set F  of the resulting system of constraints and derive from it the set  TiJ z  lza E Fi  for Cli  Cit  or for each Clj E Cli  respectively             For each such clique  Clj   send this information  as a system of constraints to Cli using  Tj iZj       T  jZ            I l  The correctness of the algorithm has been proven in If a request for information is received from the   Gaag         In general  the algorithm may take ex ponential time  However  if the maximal clique size  other neighbours Cli e Cli   is small compared to the number of statistical vari   neighbour Cit  then pass this message on to all        I  I I  I  I  I  I  I  I I  I  I  I  I  I  I  I  I   I  ables  the algorithm will take polynomial time  An important question for the algorithm to be of prac tical use is the question whether it is likely that the mentioned restriction will be met in practice  Con cerning this  J  Pearl argues that sparse  irregular graphs are generally appropriate in practical applica tions   Pearl      b   The probability intervals obtained after application of the algorithm may be rather wide  in fact they may be too wide for practical purposes  However  the intervals are precise and in a sense  honest   they just reflect the lack of knowledge concerning the joint probability distribution      Conclusion  intervals can guide the expert in providing further information   
  In building Bayesian belief networks  the elic itation of all probabilities required can be a major obstacle  We learned the extent of this often cited observation in the construc tion of the probabilistic part of a complex influence diagram in the field of cancer treat ment  Based upon our negative experiences with existing methods  we designed a new method for probability elicitation from do main experts  The method combines various ideas  among which are the ideas of transcrib ing probabilities and of using a scale with both numerical and verbal anchors for mark ing assessments  In the construction of the probabilistic part of our influence diagram  the method proved to allow for the elicita tion of many probabilities in little time     INTRODUCTION  As more and more Bayesian belief networks are be ing developed for complex problem domains  it is be coming increasingly apparent that the elicitation of all probabilities required is not an easy task  In fact  the elicitation of probabilities is often referred to as a major obstacle in building a Bayesian belief network  Jensen        Druzdzel   Van der Gaag         We experienced the extent to which probability elicitation can be an obstacle to advancement in the construc tion of the probabilistic part of a complex influence diagram in the field of cancer treatment  The Antoni van Leeuwenhoekhuis in the Netherlands  hosting the Netherlands Cancer Institute  is spe cialised in the treatment of cancer patients  In the hos pital  every year some hundred patients receive treat ment for oesophageal carcinoma  Patients with oe sophageal carcinoma currently are assigned to a ther apy by means of a standard protocol  involving a small  and B G  Taal The Netherlands Cancer Institute Antoni van Leeuwenhoekhuis Plesmanlaan          CX Amsterdam The Netherlands  B M P  Aleman   number of variables  Based upon this protocol      of the patients show a favourable response to the ther apy instilled  In the context of a project aimed at the development of a more fine grained protocol with a higher favourable response rate  an influence diagram is being developed for patient specific therapy selec tion for oesophageal carcinoma  The influence diagram is destined for use in the Antoni van Leeuwenhoekhuis  The oesophagus influence diagram is being hand crafted with the help of two experts in oncology from the Netherlands Cancer Institute  After carefully modeling the characteristics of an oesophageal carci noma and the possible effects of the various different therapeutic alternatives available in the graphical part of the diagram  we focused on the elicitation of the probabilities required for the diagram s quantitative part  As in many problem domains  various differ ent sources of probabilistic information appeared to be readily available for the elicitation task  Neither data collection nor a thorough literature review  how ever  yielded any usable results  The single remaining source of probabilistic information  therefore  was the knowledge and personal clinical experience of the two domain experts involved in the project  For eliciting the conditional probabilities required for the oesophagus influence diagram  we set out us ing various well known methods with our domain experts  we used a numerical scale for marking assessments and we used the concept of lotteries  Morgan   Henrion         The various problems we encountered with these methods and the amount of time these methods tended to take for the separate assessments  soon revealed that the elicitation of the large number of probabilities required was infeasible with these methods  Based upon our negative experiences with existing methods  we designed a new method for eliciting probabilities from domain experts  We tailored our method to eliciting a large number of probabilities in little time  As assessments obtained in little        van der Gaag  Renooij  Witteman  Aleman  and Taal  time can be quite inaccurate  we envisage the use of our method as the first step in an iterative proce dure of stepwise refinement of probability assessments  Coupe et al          Our method combines various different ideas  Among these are the ideas of present ing conditional probabilities as fragments of text and of providing a scale for marking assessments with both numerical and verbal anchors  Using our method in the construction of the probabilistic part of the oe sophagus influence diagram  we elicited from our do main experts the conditional probabilities required at a rate of          probabilities per hour  In an eval uation interview  the experts indicated that they had felt very comfortable with the method  The paper is organised as follows  In Section    we provide some details of the oesophagus influence di agram and  discuss our initial experiences with prob ability elicitation for the diagram  In Section    we describe the method we designed for eliciting a large number of probabilities from domain experts  In Sec tion    we evaluate the use of our method in the con struction of the probabilistic part of the oesophagus influence diagram  more specifically  we comment on the observations put forward by the domain experts using the method  The paper is rounded off with some conclusions in Section       reduction of the patient s primary tumour and an im proved passage of food through the oesophagus  The various therapeutic alternatives available differ in the extent to which these effects can be attained  Instilla tion of a therapy further is expected to be accompa nied not just by beneficial effects but also by various complications  these complications can be very serious and may even lead to death  The effects and com plications to be expected from the various therapeu tic alternatives available for a patient depend on the characteristics of his or her carcinoma  on the depth of invasion of the carcinoma into the oesophageal wall and neighbouring structures  and on the extent of the carcinoma s metastases  It will be evident that the possible effects and complications require careful bal ancing before a therapy is decided upon  The overall structure of the oesophagus influence di agram is shown in Figure    The graphical part of the diagram was handcrafted with the help of two do main experts from the Netherlands Cancer Institute  the construction of this graphical part took approxi mately two years  with one two hour interview every two or three weeks  The influence diagram currently  THE OESOPHAGUS INFLUENCE DIAGRAM  As a consequence of a lesion of the oesophageal wall  for example as a result of frequent reflux  a carci noma may develop in a patient s oesophagus  An oe sophageal carcinoma has various characteristics that influence its prospective growth  These characteristics include the location of the carcinoma in the oesopha gus  the histological type of the carcinoma  its length  and its macroscopic shape  An oesophageal carcinoma typically invades the oesophageal wall and upon fur ther growth may invade neighbouring structures such as the trachea and bronchi  In due time  the carcinoma may give rise to lymphatic metastases in distant lymph nodes and to haematogenous metastases in  for exam ple  the lungs and the liver of the patient  The depth of invasion and the extent of the metastases of the carcinoma largely influence a patient s life expectancy  While establishing the presence of an oesophageal car cinoma in a patient is relatively easy  the selection of an appropriate therapy is a far harder task  In the Antoni van Leeuwenhoekhuis  various different thera peutic alternatives are available  ranging from surgical removal of the oesophagus  to radiotherapy  and po sitioning a prosthesis in the oesophagus  The effects aimed at by instilling a therapy include removal or  complications  Figure    The Overall Structure of the Oesophagus Influence Diagram  includes one decision node and over    chance nodes  Of these     chance nodes pertain to the characteris tics of an oesophageal carcinoma  to the depth of its invasion  and to the extent of its metastases  the re maining chance nodes model the possible effects and complications of the various therapies available  For the chance nodes  a total of almost      probabilities is required  So far  we focused our elicitation efforts on the part of the diagram that pertains to the characteristics  depth of invasion  and metastases of an oesophageal carcinoma  This part constitutes a coherent and self supporting Bayesian belief network  The    nodes in    How to Elicit Many Probabilities  valved require some thousand probability assessments  The node requiring the largest number of assessments       models the staging of the carcinoma  this node is a deterministic node  classifying an oesophageal carci noma according to the depth of its invasion and the ex tent of its metastases  The non deterministic node re quiring the largest number of probability assessments is the node describing the result of an echo endoscopic examination of a patient s oesophagus with respect to the depth of invasion of the carcinoma in the oe sophageal wall  it requires    assessments  The elicitation of the probabilities required for the    nodes indicated above proved to be a major obstacle in the construction of our influence diagram  As in many problem domains  various sources of probabilis tic information were available  We collected data from historical patient records and we performed a litera ture review  Unfortunately  the Netherlands being a low incidence country for oesophageal carcinoma  we were not able to compose an up to date  large and rich enough data collection to allow for reliable assessment of the probabilities required  after due consideration  we decided instead to save the collected data for eval uation purposes  Our literature review  although very thorough  also did not result in ready made assess ments  in fact  hardly any results reported in the lit Prature turned out to be usable for our influence dia gram  The single remaining source of probabilistic in formation  therefore  was the knowledge and personal clinical experience of the two domain experts involved in the project  The problems of bias and poor cali bration that  unfortunately  are typically encountered when eliciting judgemental probabilities from experts are widely known  Kahneman et al          In our first efforts to elicit conditional probabilities for the oesophagus influence diagram from our domain ex perts  we focused on the use of a probability scale for marking assessments  on the frequency method  and on the concept of lotteries  as well known methods for probability elicitation  Morgan   Henrion        Gigerenzer   Hoffrage         These methods were dPsigned to avert to at least some extent the typ ical problems found with human probability assess ment  Before commenting on our experiences with these methods  we would like to emphasise that  prior to the construction of the oesophagus influence dia gram  the domain experts involved had little or no ac quaintance of expressing their knowledge and clinical experience into numbers  The probability scale we used in the elicitation was a horizontal line with the three anchors         and    The domain experts were asked to mark their assess ments for all conditional probabilities pertaining to a single variable given some conditioning context on       the same line  The probabilities to be assessed were presented to them in mathematical notation  Unfor tunately  the experts involved in the project had some difficulties working with the mathematical notation  In addition  they felt quite uncomfortable with the probability scale  as it gave them very little to go by  The request to mark various assessments on a single line further appeared to introduce a bias towards aes thetically distributed marks  With the frequency method  the domain experts were asked to envisage a population of one hundred patients suffering from an oesophageal carcinoma with certain characteristics  They were asked to assess the num ber of patients from among this population who would show a characteristic under study  Since oesophageal carcinoma has a low incidence in the Netherlands  vi sualising one hundred patients with certain specific characteristics turned out to be a demanding  if not impossible  task  The use of lotteries for probability elicitation  unfor tunately  also entailed various difficulties  The domain experts indicated that they often felt confronted with lotteries that were very hard to conceive because of the rare  or unethical  situations they represented  More over  the use of lotteries tended to take so much time that it soon became apparent that the elicitation of several thousands of conditional probabilities in this way was quite infeasible     THE ELICITATION M ETHOD  For the probabilistic part of the oesophagus influence diagram  several thousands of conditional probabili ties had to be assessed  As we argued in the previous section  these probabilities had to be elicited from the domain experts involved in the construction of the dia gram  Experience with well known methods for prob ability elicitation had shown that assessing all prob abilities required was no easy task  Based upon the negative experiences with these methods  we designed a new method for eliciting probabilities from domain experts that is tailored to the elicitation of a large number of conditional probabilities in little time  In this section  we present our method  its use will be commented upon in Section    Our method for probability elicitation from domain ex perts combines various different ideas  Although sev eral of these ideas were presented before by others  we combined and enhanced them to yield a novel and  as we will argue in the next section  effective elicitation method  At the heart of our method lies the idea of presenting domain experts with a separate figure for every conditional probability that needs to be assessed  Figure   shows  as an example  the figure pertaining        van der Gaag  Renooij  Witteman  Aleman  and Taal  certain  almost  probable              expected  polypoid oesophageal carci less than   em  How carcinoma invades into the muscu  Consider a patient with a  noma  the carcinoma has a length of likely is it that this  laris propria  T    fifty fifty      of the patient s oesophageal wall  but  not beyond   uncertain    improbable  almost  impossible         F igure    The Fragment of Text and Probability Scale for the Assessment of the Conditional Probability Pr Invasion    T  I Shape   polypoid  Length    em     to the conditional probability Pr   Invasion      T   Shape     Length   polypoid      em     for the oesophagus influence diagram  On the left of the figure is a fragment of text that transcribes the conditional probability to be assessed  Using a frag ment of text to denote a probability circumvents the need to use mathematical notation  The fragment is stated in terms of likelihood rather than in terms of frequency to forestall difficulties with the assessment of a conditional probability for which the conditioning context is quite rare  To facilitate the assessment of a required probability  a vertical scale is depicted to the right of the text fragment  Indicated on this scale are various different numerical and verbal anchors  We will presently comment on the specific anchors used  The figures pertaining to the various conditional prob abilities to be assessed are grouped in such a way that the probabilities from the same conditional distribu tion can be taken into consideration simultaneously  the figures are presented in groups of two or three on consecutive single sided sheets of paper  Explic itly grouping related probabilities has the advantage of reducing the number of times a mental switch of conditioning context is required of the domain experts during the elicitation  The probability scale to be used with our method spec ifies both numerical and verbal anchors  Research on human probability judgement has indicated that most people tend to feel more at ease with verbal probabil ity expressions than with numbers  Verbal expressions of probability are considered to be more natural than numerical probabilities  easier to understand and com   municate  and better suited to convey the vagueness of one s opinions  Wallsten et al          this observa tion also holds for physicians and other health workers  Merz et al          Words  however  should not self evidently be preferred to numbers  as neither should numbers to words  In fact  the two modes of commu nicating probabilistic information can both be used  Brun   Teigen         Motivated by these observa tions  we decided to search for a probability scale for marking assessments that is based on both numbers and verbal probability expressions  To develop a scale of verbal probability expressions to be used with numbers  we undertook four separate studies  In the first study  we asked subjects to pro vide a list of the verbal probability expressions they commonly use  This study yielded seven most fre quently used expressions  being  translated from the corresponding Dutch expressions   certain     proba ble     expected     fifty fifty     uncertain     improba ble    and  impossible    In the second study   other  subjects were asked to rank order these expressions  The results from this study indicated that the seven verbal probability expressions had a considerably sta ble rank ordering between subjects  To establish the relative distances among the seven expressions  in the third study  subjects were asked to compare each pair of expressions and assess the degree to which the two expressions conveyed the same probability  The dis tances yielded by this study were used to project the verbal probability expressions onto a numerical scale  The expression  certain  was calculated to be equiva lent to        probable  to be equivalent to approx imately      and  expected  approximately to       fifty fifty  was calculated to be equal to       uncer    How to Elicit Many Probabilities  tain  approximately to       improbable  to approx imately      and  impossible   to conclude  was cal culated to denote     Using this projection of verbal probability expressions onto numbers  the fourth study focused on the question whether decisions were influ enced by the mode in which probability information was presented  The results indicated that the decisions made were independent of whether the probability in formation was expressed numerically or verbally  We would like to note that the four studies included sub jects from the field of medicine  For further details of the studies  we refer the reader to an extended paper  Renooij   W itteman         The fact that the subjects in our studies interpreted the verbal probability expressions as intended  moti vated us to further elaborate on a scale with both nu merical and verbal anchors for use as an aid for prob ability elicitation  Since the verbal probability expres sions were explicitly intended as independent anchors on the scale rather than as translations for the nu merical probabilities  we decided to position the verbal probability expressions close by rather than simply be side the numerical anchors  We further decided to add the moderator   almost   to the most extreme ver bal expressions to indicate the positions of very small and very large probabilities  The resulting probability scale is the scale shown in Figure       EVA LUATION OF THE ELICITATION M ETHOD  We used our newly designed method for probability elicitation from domain experts in the construction of the probabilistic part of the oesophagus influence di agram  In this section  we evaluate the use of our method  More specifically  we comment upon the ob servations put forward by the domain experts involved  In addition  we briefly review the preliminary results from an initial evaluation of the influence diagram in the making       USING THE METHOD  The elicitation of the conditional probabilities required for the part of the oesophagus influence diagram out lined in Section    took approximately two months  with one two hour interview with the domain experts every two weeks  Each interview focused on a small coherent part of the diagram  Prior to every inter view  the elicitors spent some ten hours preparing the fragments of text to be presented with our probability scale to the experts  In the first interview  the domain experts were in formed of the basic ideas underlying the new elicitation       method  The intended use of the probability scale was detailed to them  In addition  the experts were told that their initial probability assessments would be sub jected to a sensitivity analysis to reveal the sensitivi ties of the diagram s results to the various assessments  and that we would try and refine the most influential ones later on in the project  for details of our proce dure for stepwise refinement of assessments  we refer the reader once again to  Coupe et al          The ba sic idea of sensitivity analysis was explained to the domain experts in depth to reassure them that rough assessments for the requested conditional probabilities would suffice at this stage in the construction of the influence diagram  Following the last interview  the domain experts were asked to evaluate the new method we had used with them for probability elicitation  For this purpose  a written evaluation form was designed so as to not influ ence their observations  In the evaluation  the domain experts indicated that they had felt very comfortable with the elicitation method  They found the method most effective and much easier to use than any method for probability elicitation they had used before  We recall from Section   that one of the ideas under lying our method for probability elicitation is the use of a fragment of text to indicate a conditional prob ability that needs to be assessed  The use of these fragments of text seemed to work very well  The two domain experts mentioned that they had had no dif ficulties understanding the described patient charac teristics  During the interviews  the elicitors had of ten noted that the described characteristics served to call to mind various different concrete patients  Al though the experts could not envisage a large group of patients with certain specific characteristics  their ex tensive clinical experience with cancer patients in gen eral and their knowledge of reactive growth of cancer cells  along with information recalled from literature  enabled them to provide the requested assessments  With respect to the probability scale used for marking assessments  the domain experts indicated that they had found the presence of both numerical and verbal anchors helpful  They mentioned that  upon thinking about a conditional probability to be assessed  they used words as well as numbers  Depending on how fa miliar they felt with the characteristics described in a fragment of text  they preferred using verbal or numer ical expressions  The more uncertain they were about the probability to be assessed  for example  the more they were inclined to think in terms of words  The verbal anchors on the scale then helped them to deter mine the position that they felt expressed the proba bility they had in mind         van der Gaag  Renooij  Witteman  Aleman  and Taal  The two domain experts further mentioned that they had felt comfortable with the specific verbal anchors used with the probability scale  They indicated  how ever  that the expression  impossible  is hardly ever used in oncology  Especially in their communication with patients  oncologists appear to prefer the more cautious expression  improbable  to refer to almost impossible events  As a consequence  our domain ex perts tended to interpret the expression  improbable  as a    or even smaller probability rather than as a probability of around      Since the probability scale provided both words and numbers  they had no dif ficulties indicating what they meant to express  The experts also mentioned that an extra anchor around     would have been useful  Note that these ob servations pertain to the lower half of the scale only  We would like to add to these observations that our probability scale hardly accommodates for indicating extreme probability assessments  that is  assessments very close to   or    During the various interviews  however  the domain experts never seemed to want to express such extreme assessments  When asked about extreme probabilities  they confirmed our observation  Another idea underlying our method is the idea of grouping the figures used in such a way that the prob abilities from the same conditional distribution can be taken into consideration simultaneously  During the elicitation interviews  the domain experts were ad vised first to focus on the probabilities from a condi tional distribution that were the easiest to assess  and then to distribute the remaining probability mass over the more difficult probabilities  This turned out to be a most effective heuristic for eliciting assessments for variables with more than two or three values  To conclude  we would like to point out that  during the earlier  rather unsuccessful  elicitation efforts  our domain experts had acquired some acquaintance of ex pressing their knowledge and personal clinical experi ence into numbers  As a result  they now appeared to be less daunted by the assessment task   into the oesophageal wall and the worse off the pa tient is  For the variable Invasion  various conditional probabilities had to be assessed  pertaining to differ ing shapes and varying lengths of a carcinoma  Upon assessing the conditional probabilities required for the variable Invasion  the domain experts started with the probabilities for the depth of invasion of a polypoid oe sophageal carcinoma with a length of less than   cen timeters  They subsequently indicated that patients with ulcerating tumours of this length were     worse off with regard to the depth of invasion of the carci noma than patients with equivalent polypoid tumours  They thus explicitly related two conditional probabil ity distributions for the variable Invasion to one an other  As trends appeared to be a quite natural way of expressing probabilistic information  we encouraged our domain experts to provide trends wherever appro priate  We designed a generic method for handling the trends provided by our domain experts in an intuitively ap pealing and mathematically correct way  The method is best explained in terms of the example trend stated above  Suppose that  given a polypoid oesophageal carcinoma of less than   centimeters  the probabilities for the four different values of the variable Invasion have been assessed at x   x   x   and x    x  be ing the probability assessment for the value Ti  After consultation with our domain experts  we interpreted the specified trend as follows      of the patients with a polypoid tumour of less than   centimeters with Ti for its depth of invasion would have had Ti    for  an ulcerating tumour  i            The basic idea of the interpretation of the trend is depicted in Figure    For the probability assessments Yt  Y   Y   and Y  for                T          the depth of invasion if the tumour would have been  T   T   T   THE USE OF TRENDS  Figure    Handling Trends  During the elicitation interviews with our domain ex perts  the concept of trend emerged  We use the term to denote a fixed relation between two conditional probability distributions for the same variable given different conditioning contexts  To illustrate the concept of trend  we address the vari able Invasion modelling the depth of invasion of an oesophageal carcinoma into the wall of a patient s oe sophagus  This variable can take one of the values Tl  T   T   and T   the higher the number indicated in the value  the deeper the carcinoma has invaded  the different values of the variable Invasion given an ulcerating oesophageal carcinoma of less than   cen timeters  we thus find Yt       Y        Y        Y             X        X         X  X       X         X  X  X         X  X              It is readily verified that the resulting assessments Yt  Y   Y   and Y  each lie between   and    and together   How to Elicit Many Probabilities  add up to    In addition  it will be evident that this method for handling trends can easily be generalised to variables with another number of values and to trends specifying other percentages and other directions of change       AN INITIAL EVALUATION OF THE DIAGRAM  With our new method  we elicited from the domain experts involved all conditional probabilities required for the part of the oesophagus influence diagram that pertains to the characteristics  depth of invasion  and metastases of an oesophageal carcinoma  As men tioned before in Section    this part of the diagram constitutes a coherent and self supporting Bayesian belief network  it provides for predicting the stage of a patient s oesophageal carcinoma from the results of various different diagnostic tests  To get some prelim inary insight in the quality of the influence diagram in the making  we performed an initial evaluation of the    node belief network with patient data from     patients  available from the Netherlands Cancer Insti tute  Before detailing this evaluation and its results  we would like to note that the data collection used is known to be biased  to contain inconsistencies  and to be incomplete in a non random way  For each patient from our data collection  we instan tiated  in the belief network  all nodes pertaining to diagnostic tests for which a test result was available for the patient under consideration  These diagnos tic tests range from a biopsy of the primary tumour to an echo endoscopic examination of the oesophagus  and an X ray of the patient s chest  From the thus partially instantiated belief network  we computed the most likely stage of the patient s oesophageal carci noma and compared it with the stage recorded in the data  The stage of an oesophageal carcinoma can be either I  IIA  liB  III  IVA  or IVB  For    patients from our data collection  unfortu nately  the stage of the oesophageal carcinoma was not recorded  which left us with     patients for our eval uation  In    of these     patients  the stage of the carcinoma recorded in the data matched the stage with highest probability computed from the belief network  Under the assumption that the stages recorded in the data are correct  therefore  in     of the patients the network predicted the correct stage  We would like to note that this percentage is not uncommon in evalua tions of knowledge based systems  Berner et al          Careful examination of the data of the patients for which the belief network returned an incorrect stage learned that the network s prediction deviated from the data most notably for patients with an oesophageal       carcinoma of stage IVB  For some     of the patients with a IVB staged carcinoma  another stage was pre dicted by the network  the stage IVB was quite often yielded as the second most likely stage  however  Af ter removing all patients with a IVB staged carcinoma from our data collection  the network predicted the correct stage for     of the remaining patients  To conclude our initial evaluation  we re addressed the data of patients for whose oesophageal carcinoma the belief network predicted a stage that differed from the stage recorded in the data  in doing so  we once again included the patients with a IVB staged carcinoma  Since most probability assessments for the network had been rounded off at     we investigated the effect  on the percentage of correct predictions  of consider ing certain stages as  almost  correct  To this end  we considered an oesophageal carcinoma as  almost  cor rectly staged by the network  if for the stage recorded in the data a probability was computed from the net work that differed by at most    from the probability of the most likely stage  The percentage of correct pre dictions then approached      Given that the proba bilities used are rough  initial assessments and that the patient data definitely require clearing out  the results from the initial evaluation are quite encouraging     CONCLUSIONS  We experienced the extent to which probability elici tation can be an obstacle to advancement in the con struction of the probabilistic part of the oesophagus influence diagram  Motivated by our negative ex periences with existing methods  we designed a new method for eliciting probabilities from domain experts  Our method combines various different ideas  among which are the ideas of transcribing probabilities and of using a scale with both numerical and verbal anchors  We used our new elicitation method for eliciting the probabilities required for the oesophagus influence di agram and evaluated its use with the domain experts involved  The experts indicated that they found the method much easier to use than any method for prob ability elicitation they had used before  For the construction of the oesophagus influence dia gram  our newly designed elicitation method entailed a major breakthrough  Prior to the use of our method  we had spent over a year experimenting  on and off  with other methods for probability elicitation without success  Using our elicitation method  the probabili ties for a major part of the oesophagus influence di agram were elicited in just two months  time  Our method tends to take considerable time on the part of the elicitors in preparing the various interviews with the experts  However  the ease with which probabili         van der Gaag  Renooij  Witteman  Aleman  and Taal  ties are elicited with the method makes this time cer tainly well spent  
 While known algorithms for sensitivity analysis and parameter tuning in probabilistic networks have a running time that is exponential in the size of the network  the exact computational complexity of these problems has not been established as yet  In this paper we study several variants of the tuning problem and show that these problems are NPPP  complete in general  We further show that the problems remain NP complete or PP complete  for a number of restricted variants  These complexity results provide insight in whether or not recent achievements in sensitivity analysis and tuning can be extended to more general  practicable methods      Introduction  The sensitivity of the output of a probabilistic network to small changes in the networks parameters  has been studied by various researchers                       Whether the parameter probabilities of a network are assessed by domain experts or estimated from data  they inevitably include some inaccuracies  In a sensitivity analysis of the network  the parameter probabilities are varied within a plausible range and the effect of the variation is studied on the output computed from the network  be it a posterior probability or the most likely value of an output variable  The results of a sensitivity analysis are used  for example  to establish the robustness of the networks output  The results are used also upon engineering a probabilistic network  for example to distinguish between parameters which allow some imprecision and parameters which should be determined as accurately as possible      Another use is for carefully tuning the parameter probabilities of a network to arrive at some desired model behavior       Research efforts in sensitivity analysis and parameter tuning for probabilistic networks have resulted in a variety of fundamental insights and computational methods  While the majority of these insights and methods pertain to a one way sensitivity analysis in which the effect of varying a single parameter probability on a single output probability or output value is studied  recently there also has been some pioneering work on extending these insights to higher order analyses         The currently available algorithms for sensitivity analysis and parameter tuning of probabilistic networks have a running time that is exponential in the size of a network  This observation suggests that these problems are intractable in general  The actual computational complexity of the problems has not been studied yet  however  In this paper we define several variants of the tuning problem for probabilistic networks and show that these variants are NPPP  complete in general  We further show that the tuning problem remains NP comlete  even if the topological structure of the network under study is restricted to a polytree  and PP complete  even if the number of conditional probability tables involved is bounded  Given the unfavorable complexity results obtained  even for restricted cases of the tuning problem  we have that we cannot expect to arrive at efficient  more general computational methods for sensitivity analysis and parameter tuning for probabilistic networks  Our complexity results in fact suggest that further research should concentrate on tuning a limited number of parameters  in networks where inference is tractable  The paper is organized as follows  After briefly reviewing the basic concepts involved in sensitivity analysis and parameter tuning in Section    we present some preliminaries from complexity theory in Section   and formally define several variants of the tuning problem in Section    We give a general completeness proof for these problems in Section    We further address some special  restricted cases of these problems in Section    The paper ends with our concluding observations   in Section        Sensitivity analysis and tuning  A probabilistic network B    G    includes a directed acyclic graph G    V  A   where V    V            Vn   models a set of stochastic variables and A models the  in dependences between them  and a set of parameter probabilities   capturing the strengths of the relationships between the variables  The network Qn models a joint probability distribution Pr V    i   Pr vi    Vi    over its variables  where  V   denotes the parents of V in G  We will use Pr C   c   E   e  to denote the probability of the value c of the output variable C  given an instantiation e to the set of evidence variables E  which will be abbreviated as Pr c   e   We will denote a particular set of parameter probabilities as X    and we will use X to denote a single parameter  We will use x and x to denote the combination of values of a set of parameters  respectively the value of a single parameter  In sensitivity analysis and parameter tuning  we are interested in the effect of changes in the parameter probabilities X on an output probability for a designated variable C  The sensitivity function fPr c e   X  expresses the probability of the output in terms of the parameter set X  We will omit the subscript if no ambiguity can occur  In a one way sensitivity analysis  we measure the sensitivity of an output probability of interest with respect to a single parameter  The parameter under consideration is systematically varied from   to   and the other parameters from the same CPT are co varied such that their mutual proportional relationship is kept constant       Thus  if the parameter X   Pr bi      denoting the conditional probability of the value bi of the variable B given a particular configuration  of Bs parents  is varied from   to    the other parameters P r bj     for the variable B are varied such that Pr bj     X    Pr bj        X    Pr bi      for any value bj other than bi   Under the condition of covariation  the sensitivity function f  X  is a quotient of two linear functions     and takes the form c   X   c  c   X   c  where the constants can be calculated from the other parameter probabilities in the network  f  X     A one way sensitivity analysis can be extended to measure the effect of the simultaneous variation of two parameters on the output      The sensitivity function then generalizes to f  X    X       c   X   X    c   X    c   X    c  c   X   X    c   X    c   X    c   In this function  the terms c   X   X  and c   X   X  capture the interaction effect of the parameters on the output variable  This can further be generalized to nway sensitivity analyses        where multiple parameters are varied simultaneously  While higher order analyses can reveal synergistic effects of variation  the results are often difficult to interpret       For performing a one way sensitivity analysis  efficient algorithms are available that build upon the observation that for establishing the sensitivity of an output probability it suffices to determine the constants in the associated sensitivity function  The simplest method for this purpose is to compute  from the network  the probability of interest for up to three values for the parameter under study  using the functional form of the function to be established  a system of linear equations is obtained  which is subsequently solved      For the network computations involved  any standard propagation algorithm can be used  A more efficient method determines the required constants by propagating information through a junction tree  similar to the standard junction tree propagation algorithm       This method requires a very small number of inward and outward propagations in the tree to determine either the constants of all sensitivity functions that relate the probability of interest to any one of the network parameters  or to determine the sensitivity functions for any output probability in terms of a single parameter  Both algorithms are exponential in the size of the network  yet have a polynomial running time for networks of bounded treewidth  Closely related to analyzing the effect of variation of parameters on the outputand often the next step after performing such an analysisis tuning the parameters  such that the output has the desired properties  The output may need to satisfy particular constraints  e g  Pr c   e   q  Pr c    e   Pr c    e   q or Pr c    e   Pr c    e   q  for a particular value q  There are a number of algorithms to determine the solution space for a set of parameters given such constraints      The computational complexity of these algorithms is always exponential in the treewidth w of the graph  i e   the size of the largest clique in the jointree   yet varies from O cw   for single parameter tunQk ing  to O n  i   F  Xi    cw   for tuning n parameters  where c is a constant  k is the number of CPTs that include at least one of the parameters being varied  and F  Xi   denotes the size of the i th CPT  Note that the tuning problem is related to the inference problem in so called credal networks      where each variable is associated with sets of probability measures  rather than single values as in Bayesian networks  This problem has been proven NPPP  complete      Often  we want to select a combination of values for the   parameters that satisfies the constraints on the output probability of interest  but has minimal impact on the other probabilities computed from the network  In other cases  we want the modification to be as small as possible  In other words  we want to find a tuning that not merely satisfies the constraints  but is also optimal  either with respect to the minimal amount of parameter change needed  or the minimal change in the joint probability distribution induced by the parameter change  Here we discuss two typical distance measures between joint probability distributions  namely those proposed by Kullback and Leibler       and Chan and Darwiche      The distance measure introduced by Chan and Darwiche      denoted by DCD   between two joint probability distributions Prx and Prx  is defined as  def  DCD  Prx   Prx      ln max   Prx    Prx     ln min  Prx     Prx      where  is taken to range over the joint probabilities of the variables in the network  The Kullback Leibler measure       denoted by DKL   is defined as  def  DKL  Prx   Prx       X  Prx    ln    Prx    Prx      Calculating either distance between two distributions is intractable in general  It can be proven that calculating DCD is NP complete and that calculating DKL is PP complete    The Euclidean distance is a convenient way to measure the amount of change needed in x to go from Prx to Prx    This distance  denoted by DE   is defined as  s X def  xi  x i    DE  x  x      xi x x i x   The Euclidean distance depends only on the parameters that are changed and can be calculated in O   X         Complexity theory  In the remainder  we assume that the reader is familiar with basic concepts of computational complexity theory  such as the classes P and NP  and completeness proofs  For a thorough introduction to these subjects we refer to textbooks like      and       In addition to these basic concepts  we use the complexity class PP  Probabilistic Polynomial time   This class contains languages L accepted in polynomial time by a Probabilistic Turing Machine  Such a machine augments the more traditional non deterministic Turing Machine with a probability distribution associated   These results are not yet published but will be substantiated in a forthcoming paper   with each state transition  e g  by providing the machine with a tape  randomly filled with symbols       If all choice points are binary and the probability of each transition is      then the majority of the computation paths accept a string s if and only if s  L  A typical problem in PP  in fact PP complete  is the Inference problem           given a network B  a variable V  in V  and a rational number    q     determine whetherQPr V    v     q  Recall that n Pr V            Vn     i   Pr Vi    Vi     To determine whether Pr v     q  we sum over all marginal probabilities Pr V            Vn   that are consistent with v    This can be done using a Probabilistic Turing Machine in polynomial time  The machine calculates the multiplication of conditional probabilities Pr Vi    Vi     i              n  choosing a computation path in which each variable Vi is assigned a value according to the conditional probability Pr Vi    Vi     Each computation path corresponds to a specific joint value assignment  and the probability of arriving in a particular state corresponds with the probability of that assignment  At the end of this computation path  we accept with probability         q       if the joint value assignment to V            Vn is consistent with v    and we accept with probability          if the joint value assignment is not consistent with v    The majority of the computation paths  i e           then arrives in an accepting state if and only if Pr v     q  Another concept from complexity theory that we will use in this paper is oracle access  A Turing Machine M has oracle access to languages in the class A  denoted as MA   if it can query the oracle in one state transition  i e   in O     We can regard the oracle as a black box that can answer membership queries in constant time  For example  NPPP is defined as the class of languages which are decidable in polynomial time on a non deterministic Turing Machine with access to an oracle deciding problems in PP  Informally  computational problems related to probabilistic networks that are in NPPP typically combine some sort of selecting with probabilistic inference  Not all real numbers are exactly computable in finite time  Since using real numbers may obscure the true complexity of the problems under consideration  we assume that all parameter probabilities in our network are rational numbers  thus ensuring that all calculated probabilities are rational numbers as well  This is a realistic assumption  since the probabilities are normally either assessed by domain experts or estimated by a learning algorithm from data instances  For similar reasons  we assume that ln x  is approximated within a finite precision  polynomial in the binary representation of x       Problem definitions  In the previous sections  we have encountered a number of computational problems related to sensitivity analysis and parameter tuning  To prove hardness results  we will first define decision problems related to these questions  Because of the formulation in terms of decision problems  all problems are in fact tuning problems  Parameter Tuning Instance  Let B    G    be a Bayesian network where  is composed of rational probabilities  and let Pr be its joint probability distribution  Let X   be a set of parameters in the network  let C denote the output variable  and c a particular value of C  Furthermore  let E denote a set of evidence variables with joint value assignment e  and let    q     Question  Is there a combination of values x for the parameters in X such that Prx  c   e   q  Parameter Tuning Range Instance  As in Parameter Tuning  Question  Are there combinations of values x and x  for the parameters in X such that Prx  c   e   Prx   c   e   q  Evidence Parameter Tuning Range Instance  As in Parameter Tuning  furthermore let e  and e  denote two particular joint value assignments to the set of evidence variables E  Question  Is there a combination of values x for the parameters in X such that Prx  c   e     Prx  c   e     q  Minimal Parameter Tuning Range Instance  As in Parameter Tuning  furthermore let r  Q    Question  Are there combinations of values x and x  for the parameters in X such that DE  x  x     r and such that Prx  c   e   Prx   c   e   q  Minimal Change Parameter Tuning Range Instance  As in Parameter Tuning  furthermore let s  Q    and let D denote a distance measure for two joint probability distributions as reviewed in Section     Question  Are there combinations of values x and x  for the parameters in X such that D x  x     s and Prx  c   e   Prx   c   e   q  Mode Tuning Instance  As in Parameter Tuning  furthermore let   Pr C   denote the mode of Pr C   Question  Are there combinations of values x and x  for the parameters in X such that   Prx  C   e        Prx   C   e     Furthermore  we define Evidence Mode Tuning  Minimal Parameter Mode Tuning  and Minimal Change Mode Tuning corresponding to the Parameter Tuning variants of these problems      Completeness results  We will construct a hardness proof for the Parameter Tuning Range problem  Hardness of the other problems can be derived with minimal changes to the proof construction  More specifically  we prove NPPP hardness of the Parameter Tuning Range problem by a reduction from E Majsat  this latter problem has been proven complete by Wagner      for the class NPPP   We will use a reduction technique  similar to the technique used by Park and Darwiche      to prove NPPP  hardness of the Partial Map problem  We first observe that all tuning problems from Section   are in NPPP   given x  x    q  r and s  we can verify all claims in polynomial time using a PP oracle  since inference is PP complete       For example  with the use of the oracle  we can verify in polynomial time whether Prx  c   e   Prx   c   e   q  for a given x  x    and q  Likewise  we can calculate the Euclidean distance of x and x  in polynomial time and verify that it is less than r  Determining whether a distance between two joint probability distributions is smaller than s is NP complete  for the distance DCD defined by Chan and Darwiche      or PP complete  for the distance DKL defined by Kullback and Leibler        Thus  we can non deterministically compute an assignment to X and check  using a PP oracle  that the distance is smaller than s  Therefore  all problems are in NPPP   To prove hardness  we will reduce Parameter Tuning Range from E Majsat  defined as follows  E Majsat Instance  Let  be a Boolean formula with n variables Vi        n   grouped into two disjoint sets VE   V            Vk and VM   Vk             Vn   Question  Is there an instantiation to VE such that for at least half of the instantiations to VM    is satisfied  We construct a probabilistic network B from a given Boolean formula  with n variables Vi and instantiation templates VE and VM   For all variables Vi   in the formula   we create a matching stochastic variable Vi in V for the network B   with possible values true and false with uniform distribution  These variables are roots in the network B   We denote Xi   Pr Vi   true  as the parameter of Vi   For each logical operator in   we create an additional stochastic variable in the network  whose parents are   constructed as described above  Trivially  there exists a combination of parameter values x  such that Prx   C   true       namely all assignments in which XS      In that case  at least one of the parents of C has the value false with probability   and thus Prx   C   true        C   S      V   V  V  V   VM  VE  Figure    Example of construction  the corresponding sub formulas  or single variable in case of a negation operator  and whose conditional probability table is equal to the truth table of that operator  For example  the  operator would have a conditional probability Pr    true      if and only if both its parents have the value true  and   otherwise  We denote the stochastical variable that is associated with the top level operator in  with V   Furthermore  we add a variable S with values true and false  with uniform probability distribution where XS   Pr S   true  is the parameter of S  Lastly  we have an output variable C  with parents S and V and values true and false  whose CPT is equal to the truth table of the  operator  The set of parameters X in the Parameter Tuning Range problem now is defined to be  X            Xk    XS   i e   the parameters of the variables in VE and the parameter of S  We set q        Figure   shows the graphical structure of the probabilistic network constructed for the E Majsat instance    VE   VM    where     V   V     V    VE    V    V     and VM    V     Note that this EMajsat instance is satisfiable with V    V    F   for that instantiation to VE   at least half of the possible instantiations to VM will satisfy the formula  Theorem    Parameter Tuning Range is NPPP complete  PP  Proof  Membership of NP can be proved as follows  Given x  and x  we can verify whether Prx  c   e   Prx   c   e   q in polynomial time  given an oracle that decides Inference  Since Inference is PP complete  this proves membership of NPPP   To prove hardness  we construct a transformation from the E Majsat problem  Let    VE   VM   be an instance of E Majsat  and let B be a probabilistic network  with parameters x    X            Xk    XS    On the other hand  if x includes XS      then Prx  C   true  depends on the values of X            Xk   More in particular  there exist parameter values such that Prx  C   true        if and only if    VE   VM   has a solution  We can construct a solution x by assigning   to XS     to all variables in  X            Xk   where the corresponding variable in VE is set to true  and   where it is set to false  On the other hand  if    VE   VM   is not satisfiable  then Prx  C   true  will be less than     for any parameter setting  Due to the nature of the CPTs of the operator variables which mimic the truth tables of the operators  Prx  C   true      for a value assignment to the parameters that is consistent with a satisfying truth assignment to   If there does not exist a truth assignment to the variables in VE such that the majority of the truth assignments to the variables in VM satisfies   then there cannot be a value assignment to X such that Prx  C   true        Thus  if we can decide whether there exist two sets of parameter settings x and x  such that in this network B   Prx  C   true   Prx   C   true   q  then we can answer    VE   VM   as well  This reduces E Majsat to Tuning Parameter Range  Note that the constructed proof shows  that the Parameter Tuning Range problem remains NPPP complete  even if we restrict the set of parameters to constitute only prior probabilities  if all variables are binary  if all nodes have indegree at most    if the output is a singleton variable  and if there is no evidence  We will now show completeness proofs of the other problems  Corollary    All tuning problems defined in Section   are NPPP  complete  Proof  We will show how the above construction can be adjusted to prove hardness for these problems   Parameter Tuning  From the above construct  leave out the nodes S and C  such that x    X            Xk    There is an instantiation x such that Prx  V   true        if and only if    VE   VM   has a solution   Evidence Parameter Tuning Range  From the above construct  replace S with a singleton evidence variable E with values true and false and uniform distribution  denote E   true as e  and E   false as e  and let x    X            Xk      Prx  C   true   E   e        for all possible parameter settings of x  On the other hand  Prx  V   true      and thus Prx  C   true   E   e        if and only if    VE   VM   has a solution   Minimal Parameter Tuning Range and Minimal Change Parameter Tuning Range  These problems have Tuning Parameter Range as a special case  set r  s     and thus hardness follows by restriction   Mode Tuning  Since C has two values  Pr C   false       Pr C   true   In particular    C    true if Pr C   true        and   C    false if Pr C   false        If XS     then   C    false  Pr C   true        if and only if   C    true  Evidence Mode Tuning  Minimal Parameter Mode Tuning  and Minimal Change Mode Tuning  Apply similar construct modifications as with the corresponding Parameter Tuning problems      Restricted problem variants  In the previous section  we have shown that in the general case  Parameter Tuning Range is NPPP complete  In this section  the complexity of the problem is studied for restricted classes of instances  More in particular  we will discuss tuning problems in networks with bounded topologies and tuning problems with a bounded number of CPTs containing parameters to be tuned       S   V   V   Vn  VS  S   S   Sn  C  Figure    Construction with polytrees  probability distribution  We denote the parameter of Vi as Xi as in the previous construct  We define a clause selector variable S  with values c            cm and   uniform probability  i e  Pr S    ci     m   Furthermore  we define clause satisfaction variables Si   with values c            cm   associated with each variable  Every variable Si has Vi and Si  as parents  Lastly  we define a variable VS   with values true and false  with uniform probability distribution  and parameter XS   and a variable C with values true and false  parents XS and Sn   See Figure   for the topology of this network  The CPT for Si  i     and C is given in Table    In this table  T  Vi   j  and F  Vi   j  are Boolean predicates that evaluate to   if the truth assignment to Vi satisfies  respectively does not satisfy  the j th clause   Si  c  cj  Pr Si   Vi   Si    Si   c  Si    c      T  Vi   j  F  Vi   j   Pr C   T   VS   Sn   Sn xS xS c      cj      Table    CPT for Pr Si   Vi   Si    and Pr C   Sn   VS    Bounded topologies  In this section we will show that restrictions on the topology of the network alone will not suffice to make the problem tractable  In fact  Parameter Tuning Range remains hard  even if B is a polytree  Similar results can be derived for the other problems  To prove NP completeness of Parameter Tuning Range on polytrees  we reduce Maxsat to Parameter Tuning Range on polytrees  using a slightly modified proof from       The  unweighted  Maxsat problem is defined as follows  Maxsat Instance  Let  be a Boolean formula in CNF format  let C   C        Cm denote its clauses and V   V        Vn its variables  and let    k  m  Question  Is there an assignment to the variables in   such that at least k clauses are satisfied  We will construct a polytree network B as follows  For each variable in the formula  we create a variable in the network with values true and false  with uniform  Theorem    Parameter Tuning Range remains NP complete if B is restricted to polytrees   Proof  Membership of NP is immediate  since we can decide Inference in polynomial time on polytrees  Given x  and x  we can thus verify whether Prx   C   c   Prx  C   c   q in polynomial time  To prove NP hardness  we reduce Maxsat to Parameter Tuning Range  Let    k  be an instance of Maxsat  From the clauses C and variables V   we construct B as discussed above  Similarly as in the previous proof  if XS     then Pr C   true      for any instantiation to the parameters X  to Xn   If XS     then we observe the following  For every instantiation cj to S    the probability distribution of Si is as follows  Pr Si   c    Vi       if the instantiation to V        Vi satisfies clause cj   and   otherwise  Pr Si   cj   Vi       if this instantiation does not satisfy clause cj     Pr Si   xi       Si   c    V    i satisfies cj    Si   cj   V    i satisfies cj  Si   c    V    i does not satisfy cj     Si   cj   V    i does not satisfy cj otherwise             Of course  Pr Si   is conditioned on Vi and thus depends on Xi   For Xi     or Xi      either Pr Si   c        or Pr Si   cj        for intermediate values of Xi the probability mass is shared between Pr Si   c    and Pr Si   cj    But then Pr Sn   c    is   for a particular clause selection cj in S    if and only if the parameter setting to X  to Xn satisfies that clause  Due to the conditional probability table of C and XS      Prx  C   true      if and only if the parameter setting x satisfies that clause  Summing over S  yields Prx  C   true    nk   where k is the number of clauses that is satisfied by x  Thus  a Parameter Tuning Range query with values   and nk would solve the Maxsat problem  This proves NP hardness of Parameter Tuning Range on polytrees       Bounded number of CPTs  In the previous section we have shown that a restriction on the topology of the network in itself does not suffice to make parameter tuning tractable  In this section we will show that bounding the number of CPTs containing parameters in X in itself is not sufficient either  Note that trivial solutions to the Parameter Tuning may exist for particular subsets X of the set of parameter probabilities   For example  if X constitutes all conditional probabilities Pr C   c    C    for all configurations of parents of C  then a trivial solution would set all these parameters to q  If the number of parameters in X is logarithmic in the total number of parameter probabilities  i e     X   p log       for any polynomial p  then the problem is in PPP   since we can try all combinations of parameter settings to   or   in polynomial time  using a PP oracle  If both the number of CPTs containing one or more parameters in the set X is bounded by a factor k  independent of the number of total number of parameter probabilities   and the indegree of the corresponding nodes is bounded  then Parameter Tuning is PP complete  Hardness follows immediately since Parameter Tuning has Inference as a trivial special case  for zero parameters   We will prove membership of PP for this problem for a single parameter in a root node and show that the result also holds for a k bounded number of CPTs with m parents  Similar observations can be made for the other tuning problems defined in Section    Theorem    Parameter Tuning is PP complete if the number of CPTs containing parameters and the indegree of the corresponding nodes are bounded   Proof  First let us assume k      i e   all n parameters are taken from the CPT of a single node V   Furthermore  let us assume for now that V is a root node  To solve Parameter Tuning  we need to decide whether Pr C   c   q for a particular combination of values of Pthe parameters in X  Conditioning on V gives us i Pr C   c   V   vi    Pr V   vi    P Since Pr V   vi        Pr C   c  is maximal i for Pr V   vi       for a particular vi   Thus  if we want to decide whether Pr C   c   q for a particular combination of values of the parameters  then it suffices to determine whether this is the case when we set Pr V   vi       for a particular parameter vi    Using this observation  we will construct a Probabilistic Turing Machine M by combining several machines accepting Inference instances  At its first branching step  M either accepts with probability      or   runs  with probability  n   one of n Probabilistic Turing Machines Mi  i              n   which on input B i  with Pr V   vi        and q accept if and only if Pr C   c    q  If any Mi accepts  then M accepts  The majority of computation paths of M accepts if and only if the Parameter Tuning instance is satisfiable  If V is not a root node  then we must branch over each parent configuration  For k CPTs with at most n parameters in each CPT and m incoming arcs  we need to construct a combined Probabilistic Turk ing Machine consisting of O nm   Probabilistic Turing Machines accepting instances of Inference  For bounded m and k  this is a polynomial number of machines and thus computation takes polynomial time  Thus  Parameter Tuning is in PP for a bounded number of CPTs containing parameters and a bounded indegree of the corresponding nodes m and k      Conclusion  In this paper  we have addressed the computational complexity of several variants of parameter tuning  Existing algorithms for sensitivity analysis and parameter tuning  see e g       have a running time  exponential in both the treewidth of the graph and in the number of parameters varied  We have shown that parameter tuning is indeed hard  even if the network has a restricted polytree and if the number of parameters is bounded  We conclude  that Parameter Tuning is tractable only if both constraints are met  i e   if probabilistic inference is easy and the number of parameters involved is bounded    If the number of parameters subject to tuning does not constitute all parameter probabilities in the CPT  then we need to test whether Pr C   c   q when all parameters have the value   as well    Acknowledgments This research has been  partly  supported by the Netherlands Organisation for Scientific Research  NWO   The authors wish to thank Hans Bodlaender and Gerard Tel for their insightful comments on earlier drafts of this paper  We wish to thank the anonymous reviewers for their thoughtful remarks   
 We present a method for learning the parameters of a Bayesian network with prior knowledge about the signs of influences between variables  Our method accommodates not just the standard signs  but provides for context specific signs as well  We show how the various signs translate into order constraints on the network parameters and how isotonic regression can be used to compute order constrained estimates from the available data  Our experimental results show that taking prior knowledge about the signs of influences into account leads to an improved fit of the true distribution  especially when only a small sample of data is available  Moreover  the computed estimates are guaranteed to be consistent with the specified signs  thereby resulting in a network that is more likely to be accepted by experts in its domain of application      INTRODUCTION  For constructing a Bayesian network  often knowledge is acquired from experts in its domain of application  Experience shows that domain experts can quite easily and reliably specify the graphical structure of a network      yet tend to have more problems in coming up with the probabilities for its numerical part      If data from every day problem solving in the domain is available therefore  one would like to use these data for estimating the probabilities that are required for the graphical structure to arrive at a fully specified network  For many applications  unfortunately  the available data sample is quite small  giving rise to inaccurate estimates  The inaccuracies involved may then lead to a reasoning behaviour of the resulting network that violates common domain knowledge and the network will not easily be accepted by experts in the domain  While domain experts often are found to have difficulties in coming up with probability assessments  evidence  is building up that they feel more comfortable with providing qualitative knowledge about the probabilistic influences between the variables concerned          The qualitative knowledge provided by the domain experts  moreover  tends to be more robust than their numerical assessments  We demonstrated before that expert knowledge about the signs of influences between the variables in a Bayesian network can be used to improve the probability estimates obtained from small data samples      We now extend our previous work to accommodate the wider range of contextspecific signs  and context specific independences more specifically  We argue that these signs impose order constraints on the probabilities required for the network  We then show that the problem of estimating probabilities under these order constraints is a special case of isotonic regression  Building upon this property  we present an estimator that is guaranteed to produce probability estimates that reflect the qualitative knowledge specified by the experts  The resulting network as a consequence is less likely to exhibit counterintuitive reasoning behaviour and is more likely to be accepted than a network with unconstrained estimates  The paper is organised as follows  In the next section  we briefly review qualitative influences  In section    we discuss isotonic regression and provide an algorithm for its computation  We then show in section    that the problem of learning constrained network parameters is a special case of isotonic regression  we also discuss how the different constraints that result from qualitative influences are handled  and how the order constraints can be used in a Bayesian context  In section    we report on experiments that we performed on a small artificial Bayesian network and on a real life network in the medical domain  Finally  we draw a number of conclusions from our work and indicate interesting directions for further research      QUALITATIVE INFLUENCES  From a qualitative perspective  the variables in a Bayesian network may be related in different ways  In the sequel   we assume all variables of the network to be binary  Let X    X            Xk   be the parents of a variable Y   and let  X     X      X             Xk           k consist of vectors x    x    x            xk   of values for the k variables in X  that is   X  is the set of all parent configurations of Y   Slightly abusing terminology  we sometimes say that Xi occurs or is present if it has the value one  We write Xa for the sub vector of X containing the variables Xj for j  a  where a is a subset of K               k   We further write Xa for XK a   A qualitative influence      between two variables expresses how observing a value for the one variable affects the probability distribution for the other variable  A positive influence of Xi on Y along an arc Xi  Y means that the occurrence of Xi increases the probability that Y occurs  regardless of any other direct influences on Y   that is  for all x  x    X  with xi      x i     and xi   x i   we have p y     x   p y     x     Similarly  there is a negative influence of Xi on Y along an arc Xi  Y if the occurrence of Xi decreases the probability that Y occurs  that is  for all x  x    X  with xi      x i     and xi   x i   we have p y     x   p y     x     A pos   itive influence of Xi on Y is denoted by Xi  Y and a  negative influence by Xi  Y   An influence with either a positive or negative sign is called a signed influence  If no sign is specified  we call the influence unsigned  The idea of signs of influences is readily extended to include context specific signs       A positive influence of Xi on Y within the context XC   c  C              k    i  means that whenever XC   c  the occurrence of Xi increases the probability that Y occurs  that is  for all x  x    X  with xC   x C   c  xi      x i     and xC i    x C i    we have p y     x   p y     x     A contextspecific negative influence is defined analogously  A zero influence of Xi on Y within the context XC   c models a local context specific independence  cf        that is  for all x  x    X  with xC   x C   c  xi      x i     and xC i    x C i    we have p y     x    p y     x     A signed context specific influence of Xi on Y along an s arc Xi  Y is denoted by Xi  Y  XC   c   with s            Note that ordinary signed influences are special cases of context specific influences with C     Further note that a signed influence in essence specifies a constraint on the parameters associated with a variable  We assume  throughout the paper  that a domain expert provides the signs of the qualitative influences between the variables in a network  We would like to mention that for real life applications these signs are quite readily obtained from experts by using a special purpose elicitation technique tailored to the acquisition of signs of qualitative influences           ISOTONIC REGRESSION  Our approach to obtaining parameter estimates for a Bayesian network that satisfy the signs of the influences specified by experts  is a special case of isotonic regression       In this section we review isotonic regression in general  in the next section we discuss its application to parameter estimation for Bayesian networks  Let Z    z    z            zn   be a nonempty finite set of constants and let   be a quasi order on Z  that is     for all z  Z  z   z  reflexivity   and    for all x  y  z  Z   x   y  y   z  x   z  transitivity   Any real valued function f on Z is isotonic with respect to   if  for any z  z    Z  z   z   implies f  z   f  z      We assume that each element zi of Z is associated with a real number g zi    these real numbers typically are estimates of the function values of an unknown isotonic function on Z  Furthermore  each element of Z has associated a positive weight w zi   that typically indicates the precision of this estimate  An isotonic function g  on Z now is an isotonic regression of g with respect to the weight function w and the order   if and only if it minimizes the sum n X  w zi    f  zi    g zi      i    within the class of isotonic functions f on Z  The existence of a unique g  has been proven by Brunk      Isotonic regression provides a solution to many statistical estimation problems in which we have prior knowledge about the order of the parameters to be estimated  For example  suppose that we want to estimate binomial parameters p    p z     p z             p zn    where p zi   denotes the probability of success in population zi   Let ni denote the number of observations sampled from population zi   and let the number of successes Yi in this sample be binomially distributed with Yi  B ni   p zi     Then the isotonic regression of the estimates Yi   Yi  ni with weights w zi     ni provides the maximum likelihood estimate of p given that p is isotonic on  Z      Note that this example suggests that the order constrained estimates are obtained by first computing the unconstrained estimates and then performing the isotonic regression of these basic estimates with appropriate weights  Isotonic regression problems can be solved by quadratic programming methods  Various dedicated algorithms  often restricted to a particular type of order  have been proposed as well  For Z linearly ordered  for example  the pool   adjacent violators  PAV  algorithm is well known  For our application  however  we require an algorithm that is applicable to sets of constants with arbitrary quasi orders  For this purpose we will use the minimum lower sets  MLS  algorithm proposed by Brunk      The MLS algorithm builds upon the concept of a lower set  A subset L of Z is a lower set of Z if z  L  z    Z  and z     z imply z    L  The weighted average of a function g on Z for a nonempty subset A is defined as P zA w z g z  Av A    P zA w z  The algorithm now takes for its input the set of constants Z    z    z            zn   with quasi order    With each zi  Z again is associated a weight w zi   and a real number g zi    The algorithm returns the isotonic regression g  of g with respect to w and    The MLS algorithm resolves violations of the order constraints by averaging over suitably chosen subsets of Z  For the final solution  it partitions the set Z into a number of subsets on which the isotonic regression is constant  The first subset B  in the final solution is a lower set of  Z      The second subset is a lower set of  Z   B          where    is obtained from   by removing all order relations involving elements of B    This process is continued until Z is exhausted  In each iteration the lower set with minimum weighted average is selected  in case multiple lower sets attain the same minimum  their union is taken  MinimumLowerSets Z     g z   w z   L   Collection of all lower sets of Z wrt   Repeat S B    A  L   Av A    minLL Av L   For each z  B do g   z    Av B  For each L  L do L L B Z  Z  B Until Z    Return g   The bottleneck of the algorithm from a computational point of view clearly is the generation of the lower sets  which is exponential in the size of the set of constants  We return to this observation in section          PARAMETER LEARNING  In this section we address the maximum likelihood estimation of parameters for a Bayesian network subject to the constraints imposed by the signs of influences  and show that it can be viewed as a special case of isotonic regression  We note that in the presence of signs  the parameters associated with the different parent configurations of a variable are no longer unrelated  Only those combinations of parameter values that are isotonic with respect to  the quasi order imposed by the specified signs  are feasible  The parameters associated with different variables are still unrelated however  because a sign imposes constraints on the parameters for a single variable only  We restrict our attention therefore to the parameters associated with a single variable       ISOTONIC REGRESSION FORMULATION  To cast our problem of constrained parameter estimation into the isotonic regression framework  we proceed as follows  For parents X of Y   we construct an order   on  X  in such a way that   corresponds to the order  on the parameters p y     x   x   X   that is implied by the specified signs  More specifically  for any qualitative s influence Xi  Y  XC   c   s            we impose the following order on  X   for all x  x    X  with xC   x C   c  xi      x i     and xC i    x C i     if s     then x    x  since the positive sign implies p y     x     p y     x    if s    then x   x    since the negative sign implies p y     x   p y     x      if s     then x   x  and x    x  since the zero enforces the equality p y     x      p y     x   The other ordering statements follow from the transitivity and reflexivity properties of quasi orders  The specified influences constrain the parameters p y     x  to be nondecreasing on   X       Now suppose that we have available a data set D from which we would like to estimate the parameters p y     x   The unconstrained maximum likelihood estimate of p y     x  is given by p y     x     n y      x  n x   where n y      x  denotes the number of observations in D with y     and X   x  The following observation now links isotonic regression to the problem currently considered  the isotonic regression p  y     x  of p y     x  with weights w x    n x  provides the maximum likelihood estimate of p y     x   for all x   X   subject to the constraint that these estimates must be non decreasing on   X            see also      page      To illustrate the above observation  we consider a fragment of a Bayesian network  Let X    X    X    X    be the parents of a variable Y   with qualitative influences on Y as shown in figure    The fragment expresses the prior knowledge that X  has a positive influence on Y and that  if X  is absent  X  has a negative influence on Y   it further models that if X  is present and X  is absent  then X  has no   thermore  the estimates in the second row should be equal  due to the context specific independence specified  Clearly  this constraint is violated as well  Finally  the estimates should be non increasing in the first row  due to the contextspecific negative influence of X    This constraint is satisfied   Figure    An example network fragment  influence on Y   The network does not specify any prior knowledge about the sign of the influence of X  on Y   Figure   shows the quasi order on the parent configurations that is imposed by the specified influences  where an arrow from x to x  indicates that x immediately precedes x  in the order  Because no influence of X  on Y has been specified  the parent configurations that have a different value for X  are incomparable  As a consequence  the order consists of two components  one for X      and one for X       Estimates may be computed for the two components separately  because there are no order constraints between parent configurations contained in different components  Also note that the component for X       depicted in the left part of figure    contains a cycle as a result of the context specific independence specified for X    Because of the independence the constraint p  y                p  y              must be satisfied  This is modelled by considering the two parent configurations           and           as a single element in the ordering  as shown in figure                                                                         Figure    Order corresponding to the network fragment                                             Figure    Updated order for X                Table   shows for each parent configuration with X      the counts obtained from a given sample  as well as the associated maximum likelihood estimates p y     x   We note that the estimates should be non decreasing in both columns of the table  due to the positive influence of X  on Y   This constraint is violated in the first column  Fur   The MLS algorithm resolves the identified constraint violations by averaging the unconstrained estimates p y     x  over conflicting cells from the table  It starts with the computation of the weighted average of p y     x  for all lower sets of  X   table   shows the resulting averages  The minimum average is achieved by              so the algorithm sets p  y                     This element is removed from all lower sets  and their weighted averages are recomputed  Now             has the minimum weighted average  and we get p  y                      The element           is removed from all lower sets  and                        is the only remaining lower set  Its weighted average is        so the algorithm sets p  y                p  y                       Now the component of the order with X      has been solved  Note that the two constraint violations have been resolved simultaneously by averaging the pair of violators p y              and p y               Next we consider the parent configurations with X       Table   shows for each such parent configuration the counts obtained from the available sample  and the associated maximum likelihood estimates p y     x   Note that there are no observations in the sample with the parent configuration            In such cases we put p y     x        and give the cell an arbitrary small weight  As a consequence the estimate will the be dominated by other parameter estimates as soon as it is pooled to resolve conflicts  From the specified signs  we have that the estimates should be non decreasing within each column  and non increasing in the first row  The row constraint is satisfied  but the column constraints are not  Table   gives all lower sets with X       and their weighted averages  The set                        achieves the minimum and the MLS algorithm sets p  y                p  y                     Note that the constrained estimate for the empty cell           has simply been set equal to the estimate for the conflicting cell            The elements           and           are removed from all lower sets  and the weighted averages         are recomputed  Now the minimum weighted average of              is achieved by                         so we get p  y                p  y                      In this step the violation of the order constraint in the first column is resolved by averaging the parameter estimates for the two conflicting cells  Note that the constrained joint estimate is closer to the unconstrained estimate for cell           than to the unconstrained estimate for cell           because we have more observations in the former than in the latter and the former thus has a larger associated weight    Table    Counts and ML estimates for X      X      X      X       X                              X                           Table    The weighted average of the lower sets for X      Lower set                                                                                  Weighted average                                           We would also like to note that  although the algorithm computes p  y                    for the empty cell            any value in the interval          would actually have satisfied the constraints  An alternative to the proposed procedure would be to remove the empty cells before application of the MLS algorithm  and after the estimates for the other cells have been computed  determine feasible estimates for the empty cells  Since  X  is exhausted after this step  the algorithm halts  We observe that the resulting parameter estimates indeed satisfy the constraints imposed by the qualitative influences  Also note that the parameters that have not been involved in any constraint violations have retained their original estimates       COMPLEXITY OF THE MLS ALGORITHM  We argued in section   that the number of lower sets is the dominant factor in the runtime complexity of the minimum lower sets algorithm  To determine this number  we start with the simple case where all k parents of a variable have a context independent signed influence  Without loss of generality  we assume all signs to be positive  Since all parents are binary  any parent configuration from  X  is uniquely determined by the parents that have the value    or alternatively  by a subset of                k   The partial order on  X  is therefore isomorphic to the partial order generated by set inclusion on P                k    Every lower set now corresponds uniquely to an antichain in this partial order  Hence  the number of distinct lower sets equals the number of distinct nonempty antichains of subsets of a k set  which adheres to the well known Sloane sequence  Table    Counts and ML estimates for X      X      X      X       X                             X                            Table    The weighted average of the lower sets for X      Lower set                                                                                                                                                                             Weighted average                                                       A        Writing  L k   for the number of lower sets for k parents as above  we thus find that  L              L                and  L                      We conclude that the MLS algorithm is feasible for up to five or six parents with signed influences only  From our example network fragment  we noted that unsigned influences serve to partition the set of parent configurations  X  into disjoint subsets  such that no element of the one subset is order related to any element of the other subsets  We argued that constrained estimates may be computed for these subsets separately  thereby effectively decomposing the parameter learning problem into a number of independent smaller problems  This decomposition yields a considerable improvement of the efficiency of the computations involved  In general  let k  denote the number of parents with a signed influence and let k  denote the number of parents with an unsigned influence  The number of configurations for the parents with an unsigned influence equals  k    The order graph thus consists of  k  components  The number of lower sets of the entire order is given by k   L k    k         L k              This number grows very rapidly  For k      and k       for example  the algorithm would need to compute the weighted average of                   lower sets  By treating each component in the order as a separate problem  the algorithm initially has to compute the weighted average of  L k    k        k   L k     lower sets  For k      and k       this amounts to just               lower sets  In the presence of context specific signs  the analysis of the algorithms runtime complexity becomes more complicated  We restrict ourselves to the following observations  First  the absence of signs in particular contexts can also lead to a decomposition of the order  and hence to a similar reduction of the computations involved as in the case of unsigned influences  On the other hand  the absence of signs in particular contexts can also lead to an increase of the number of lower sets  Secondly  context specific independences lead to a reduction of the number of elements in the ordering  that is of the number of parameters to be estimated  and hence to a reduction of the number of lower   sets       MC  The parameter learning method described in the previous sections does not require that an expert specifies numerical values for the parameters concerned  The expert only has to provide signs for the various influences  Should uncertain prior knowledge about the numeric values of the parameters be available in addition to knowledge about the signs of influences  then we can accommodate this information  Suppose that the expert is willing to specify a Beta prior for the parameters p y     x   x   x   We assume that he chooses the hyperparameters a x  and b x  such that his experience is equivalent to having seen the value y     a total of a x     times in h x    a x    b x     trials  h is called the prior precision  Let p   y     x  denote the modal value of p y     x   that is  p   y     x  is a priori considered the most likely value of p y     x   We now further assume that the experts values for a x  and b x  are such that the modes p   y     x     a x      h x   x   X   are isotonic with respect to the order imposed by the signs he specified  In forming the joint prior for p y     x   x   x   we assume local parameter independence  cf         except that the parameter values must be isotonic  This means that the prior density is   for non isotonic value combinations for the parameters  and proportional to the product Beta distribution for isotonic value combinations  The isotonic MAP estimates then are given by the isotonic regression of p   y     x  D     n x p y     x    h x p   y     x  n x    h x   with weights n x    h x   see       As before order constrained estimation now amounts to performing isotonic regression on basic estimates with appropriately chosen weights  The basic estimates are the unconstrained MAP estimates p   y     x  D  for the parameters  The weight is n x    h x   that is the sum of the number of actual observations for parent configuration x and the prior precision h x  specified by the expert  Note that in case of a flat prior  Beta       h       the orderconstrained maximum likelihood estimates are returned         BAYESIAN ESTIMATION  EXPERIMENTAL RESULTS  To study the behaviour of the isotonic estimator in a slightly more involved setting  we compare it to the standard maximum likelihood estimator on the well known Brain Tumour network      the network and the signs of the influences are depicted in figure    For the network  we specified probabilities consistent with the constraints to generate data samples for our experiments  Note that  even though the true distribution satisfies the constraints  this need not  SH       CT    ISC  B ISC        ISC         B        B        C  Figure    The Brain Tumour network  hold for the relative frequencies in the samples  especially not for smaller sample sizes  Our implementation first generates  for each variable  the quasi order on its parent configurations that corresponds with the specified signs  For each order it finds the separate components  all parent configurations contained in the same cycle are mapped to a single element and the order is adjusted accordingly  For each component of the order  the parameters for the parent configurations contained in that component are estimated using the MLS algorithm  We drew samples of various sizes from the network using logic sampling  for each sample size      data sets were drawn  From each data set  both the standard maximumlikelihood estimates and the constrained estimates of the network parameters were calculated  Given these parameter estimates  the joint distribution defined by the resulting network was computed  This distribution then was compared to the true joint distribution defined by the original network  For comparing the distributions  we used the well known Kullback Leibler divergence  The KullbackLeibler divergence of Pr  from Pr is defined as KL Pr  Pr       X x  Pr x  log  Pr x  Pr   x   where a term in the sum is taken to be   if Pr x       and infinity whenever Pr   x      and Pr x       c is used to The results are summarized in table    where Pr denote the joint distribution that was obtained with unconstrained maximum likelihood estimation  To illustrate the benefits of modeling context specific independences  we first estimated the various parameters without taking the   embedded zeroes into account  that is  we used B  C    and ISC  C  The resulting distribution is denoted by  Pr in the table  Finally  Pr denotes the distribution that was obtained with the isotone estimator using the embedded zeroes  The averages reported in the table were computed from those data sets for which the KL divergence was smaller than infinity for the maximum likelihood estimates  the isotone estimates always have KL divergence   Table    n                           Experimental results  the Brain Tumour network c KL Pr  Pr  KL Pr  Pr   KL Pr  Pr                                                                                                                                                       smaller than infinity in these cases as well  The number of data sets from which the averages were computed was              and     for sample sizes               and       respectively  For sample sizes         and     we used Bayesian estimation with a Beta      prior for all parameters  that is  the prior mode of all parameters was set to     with prior precision equal to    Note that by setting all parameters to the same value we never violate any order constraints  The unconstrained and isotonic MAP estimates were used as point estimates for the parameters  We used Bayesian estimation for the smallest sample sizes because otherwise the KL divergence would almost always be equal to infinity  The results reveal that the isotonic estimator consistently yields a better fit of the true distribution compared to the unconstrained maximum likelihood estimator  although the differences are small  We note that for the smaller data sets the differences are more marked than for the larger data sets  This conforms to our expectations  since for smaller data sets standard maximum likelihood estimation has a higher probability of yielding estimates that violate the specified constraints  For larger data sets  the standard estimator and the isotonic estimator are expected to often result in the same estimates  Note that using the contextspecific independences gives an additional improvement of fit  as was to be expected  To conclude  we applied the isotonic estimator to a real life Bayesian network in the field of oncology  The O ESOCA network provides for establishing the stage of a patients oesophageal cancer  based upon the results of a number of diagnostic tests  The network was constructed with the help of gastroenterologists from the Netherlands Cancer Institute  Antoni van Leeuwenhoekhuis  the experts provided the knowledge for the configuration of the networks structure and also provided probability assessments for the networks parameters  From the original O ESOCA network  we constructed a binary network for our experiment  carefully building upon knowledge of the domain  The resulting network includes    variables with a total of    parameters  From values of the the various parameters  we established the signs for the qualitative influences between the variables  The network includes    influences  figure   de   Table    Experimental results  the O ESOCA network c n KL Pr  Pr  KL Pr  Pr                                                                                                                 picts the binary O ESOCA network  The signs of the qualitative influences are shown over the corresponding arcs  for readibility  only the context independent signs are shown  where a question mark is used to denote an ambiguous influence  c again The experimental results are displayed in table    Pr denotes the joint distribution resulting from the unconstrained MAP estimates  and Pr the joint distribution resulting from the isotonic MAP estimates  The results are in line with the results obtained for the brain tumour network  the isotonic estimator is consistently better  and the difference becomes smaller as the sample size increases      CONCLUSIONS  Taking prior knowledge about the signs of influences between variables into account upon estimating the parameters of a Bayesian network  results in an improved fit of the true distribution  The improvement is relatively large for small samples  since these are more likely to give rise to maximum likelihood estimates that violate the constraints  Since the constrained parameter estimates are in accordance with prior knowledge specified by experts  the resulting network is more likely to be accepted in its domain of application  An interesting extension of our method would be to allow for non binary variables with linearly ordered discrete values  A signed influence on such a variable is defined in terms of a stochastic order on the distributions given the different parent configurations  Learning the parameters of a network subject to the resulting constraints in our opinion merits further research  Acknowledgement The authors would like to thank Kim H  Liem for writing part of the program code  and performing the experiments   
  The currently most efficient algorithm for infer ence with a probabilistic network builds upon a triangulation of a network s graph  In this paper  we show that pre processing can help in finding good triangulations for probabilistic networks  that is  triangulations with a minimal maximum clique size  We provide a set of rules for step wise reducing a graph  The reduction allows us to solve the triangulation problem on a smaller graph  From the smaller graph s triangulation  a triangulation of the original graph is obtained by reversing the reduction steps  Our experimental results show that the graphs of some well known real life probabilistic networks can be triangu lated optimally just by pre processing  for other networks  huge reductions in size are obtained      Introduction  The currently most efficient algorithm for inference with a probabilistic network is the junction tree propagation al gorithm that builds upon a triangulation of a network s moralised graph         The running time of this algorithm depends on the triangulation used  In general  it is hard to find a triangulation for which this running time is minimal  As there is a strong relationship between the running time of the algorithm and the maximum of the triangulation s clique sizes  for real life networks triangulations are sought for which this maximum is minimal  The minimum of the maximum clique size over all triangulations of a graph is a well studied notion  both by researchers in the field of probabilistic networks and by researchers in graph theory and graph algorithms  In the latter field of research  the no tion of treewidth is used to denote this minimum minus one  Unfortunately  computing the treewidth of a given graph is an NP complete problem      When solving hard combinatorial problems  pre process is often profitable  The basic idea is to reduce the size  ing   frankvde linda cs uu nl  of the problem under study  using relatively little compu tation time and without losing optimality  The smaller  and presumably easier  problem is subsequently solved  In this paper  we discuss pre processing for triangulation of probabilistic networks  We provide a set of rules for step wise reducing the problem of finding a triangulation for a network s moralised graph with minimal maximum clique size  to the same problem on a smaller graph  Various al gorithms can then be used to solve the smaller problem  Given a triangulation of the smaller graph  a triangulation of the original graph is obtained by reversing the reduction steps  Our reduction rules are guaranteed not to destroy op timality with respect to maximum clique size  Experiments with pre processing revealed that our rules can effectively reduce the problem size for various real life probabilistic networks  In fact  the graphs of some well known networks are triangulated optimally just by pre processing  In this paper  we do not address the second phase in the pre processing approach outlined above  that is  we do not ad dress actually constructing triangulations with a minimal or close to minimal maximum clique size  Recent research re sults indicate  however  that for small graphs optimal trian gulations can be feasibly computed  Building upon a vari ant of an algorithm by Amborg  Corneil  and Proskurowski      K  Shoikhet and D  Geiger performed various exper iments on randomly generated graphs      Their results indicate that this algorithm allows for computing optimal triangulations of graphs with up to I    vertices  The paper is organised as follows  In Section    we review some basic definitions  In Section    we present our pre processing rules  The computational model in which these rules are employed  is discussed in Section    In Section    we report on our experiments with well known real life probabilistic networks  The paper ends with some conclu sions and directions for further research in Section        Definitions  The currently most efficient algorithm for probabilistic in ference operates on a junction tree that is derived from a   UAI      BODLAENDER ET AL   triangulation of the moralisation of the digraph o f a proba bilistic network  We review the basic definitions involved  Let G  V  A  be a directed acy clic graph  The moral isation of G is the undirected graph M G  obtained from G by adding edges between everypair ofnon adjacent ver t i ces that have a common successor  vertices v and w have a common s ucc essor if there is a vertex x with   v  x  E A a nd  w  x  E A   and then dropping the arcs  directions     Let G  V  E  be an undirected graph  A set of vertices W  V is called a clique in G if there is an edge between every pair of disjoint vertices from W  the cardinality of W is the clique s size  For a set of vertices W  V  the subgraph induced by W is the graph G WJ    W   W x W  n E   for a single vertex v  we write G  v to denote G V    v    The graph G is triangulated if it does not contain an induced subgraph that is a simple cycle of length at least four  A triangulation of G is a triangulated graph H  G  that contains G as a sub graph  The treewidth of the triangulation H  G  of G is the maximum clique size in H  G  minus    The treewidth of G  denoted r  G   is the minimum treewidth over all triangulations of G     graph H is a minor of a graph G if H can be obtained from G by zero or more vertex deletions  edge deletions  and edge contractions  edge c ontra cti on is the operation that replaces two adjacent vertices v and w by a single ver tex that is connected to all neighbours of v and w   It is well known  see for example       that the treewidth of a minor of Gis never larger than the treewidth of G itself   A  of an undirected graph G    V  E  is a bijection V t t             jVI   For v E V and a linear or dering f of G  v  we denote by  v f  the linear ordering F of G that is obtained by addingv at the beginning off  that is f  v    and  for all w     v  F w  f w       A linear ordering f is a perfect elimination scheme for G if  for each v E V  its higher ordered neighbours form a clique  that is  if every pair of distinct verti ces in the set  wE V I  v  w  E E and f v    f w   is adjacent  It is well known  see for example       that a graph is triangu lated if and only if it allows a perfect elimination scheme   A linear ordering         of maximal cliques in G and  for each vertex v  the set     i I v E Vi  c ons titu tes a connected subtree ofT  It is well known  see for example       that a graph is trian gulated if and only if it has a junction tree   Tv     Safe reduction rules  Pre processing a probabilistic network for triangulation builds upon a set of reduction rules  Th ese rules allow for stepwise reducing a network s moralised graph to another graph with fewer vertices  The steps applied during the re duction can be reversed  thereby enabling us to compute a triangulation of the original graph from a triangulation of the smaller graph  In this section  we discuss the various rules  a discussion ofthe computational method in which these rules are employed  is deferred to Section    During a graph s reduction  we maintain a stack of elimi nated vertices and an integer low that gives a lower bound for the treewidth of the original graph  Application of a re duction rule serves to modify the current graph G to G  and to possibly update low to low   We say that the rule is safe if max r G   low  m ax r G     low    By applying safe rules  therefore  we have as an invariant that the treewidth of the original graph equals the maximum ofthe treewidth ofthe reduced graph and the value low  In the sequel  we assume that the original moralised graph has at least one edge and that low is initialised at       Our first reduction rule applies to simplicial vertices  A vertex v is simplicial in an undirected graph G if the neigh bours ofv form a clique in G  Lemma   Let  G  be an undirected graph and let v be a  simplicial vertex in  G with degree d         Then      For a graph G and a linear ordering f of G  thereis a unique minimal triangulation H  G  of G that has f forits perfect elimination scheme  This triangulation  which we term the fill in given j  can be constructed by  fori           VI  turning the set of higher numbered neighbours of f    i  into a clique  The maximum clique size minus   of this fill in is called the treewidth of f  The treewidth of a lin ear ordering of a triangulated graph equals the maximum number of higher numbered neighbours of a vertex          To conclude  a junction tree of an undirected graph G    V  E  is a tree T    I  F   where every node i E I has associated a vertex set Vi  such that the following two properties hold  the set  Vi I i E I  equals the set     r G      max d r G  v      v  f  of G of minimum G   v of treewidth at most max d  r G  v    there is a linear ordering  treewidth  where f is a linear ordering of  Proof  Since G contains a clique of size d      we have that r G       d  We further observe that r G       r G   v   because G   v is a minor of G  We therefore have that r G       max d  r G  v      Now  let f be a linear ordering of G v of treewidth k     max d  r G  v    Let H be the fill in of G  v given f  Adding vertex v and its  formerly  incident edges to H yields a graph H  that is still triangu lated  as every pair of neighbours of vis adjacent  v cannot belong to a simple  ch ordless   cycle of length at least four  The maximum clique size of H  therefore equals the maxi mum ofd  andk l  Hence r G   max d r G v    from which we conclude the first property stated in the lemma  To prove the second property  we observe that the linear ordering  v  f  is a perfect elimination scheme for H   as removal of v upon computing the fill in of H  does   not create any additional edges        UAI      BODlAENDER ET AL   Our first reduction rule  illustrated in Figure      now is   Reduction Rule    Simplicial vertex rule  G if there is a neighbour w of v such that all other neigh bours of v form a clique in G  Figure   illustrates the basic idea  As we allow other neighbours of v to be adjacent to simplicial vertices are also almost simplicial   Letv be a simplicial vertex of degree d     Removev  Set low to max  ow  d    w     From Lemma   we have that the simplicial vertex rule is safe  The second property stated in the lemma further pro vides for the rule s reversal when computing a triangulation of the original graph from one of the reduced graph                                      x                 w    edge  edge or non edge  Figure    An almost simplicial vertex                            v    Figure    The simplicial vertex rule   Because the digraph G of a probabilistic network is moralised before it is triangulated  it is likely to give rise to many simplicial vertices  We consider a vertex v with outdegree zero in G  Since all neighbours of v have an arc pointing into v  moralisation will connect every two neigh bours that are not yet adjacent  thereby effectively turning v into a simplicial vertex  The simplicial vertex rule wiii thus remove at least all vertices that have outdegree zero in the network s original digraph  As every directed acyclic graph has at least one vertex of outdegree zero  at least one reduc tion will be performed  As the reduced graph need not be the moralisation of a directed acyclic graph  it is possible that no further reductions can be applied    The digraph G of a probabilistic network may also include vertices with indegree zero and outdegree one  These ver tices will always be simplicial in the moralisation of G  We consider a vertex v with indegree zero and a single arc pointing into a vertex w  In the moralisation of G  w and its  former  predecessors constitute a clique  As all neigh bours ofv belong to this clique  v is simplicial  A special case of the simplicial vertex rule applies to ver tices of degree    it is termed the twig rule  after      Reduction Rule la  Twig rule  Letv be a vertex of degree    Removev  The twig rule is based upon the observation that vertices of degree one are always simplicial  Another special case is the islet rule that serves to remove vertices of degree zero  We would like to note that many heuristic triangulation al gorithms  such as the algorithm described in      remove simplicial vertices  Our second reduction rule applies to almost simplicial ver tices  A vertex v is almost simplicial in an undirected graph  Let G be an undirected graph and let v be an almost simplicial vertex in G with degree d     Let G  be the graph that is obtained rom G by turning the neighbours ofv into a clique and then removing v  Then   Lemma       r G        r G  and r G       max d  r G     the linear ordering  v  f  of G  with f a linear or dering ofG  oftreewidth at most max d r G     has treewidth at most max d  r G      Let w be a neighbour of v such that the other neighbours of v form a clique  As we can obtain G  from G by contracting the edge  v  w     G  is a minor of G  We therefore have that r G        r G   Now  let f be a linear ordering ofG  of treewidthk      max d  r G     Let H be the fill in ofG  given f  If we add v and its  formerly  adja cent edges to H  thenv is simplicial in the resulting graph H  Using Lemma l  we find that r H   max k  d   The D two properties stated in the lemma now follow  Proof      Our second reduction rule  illustrated in Figure    is  Reduction Rule    Almost simplicial vertex rule  Let v be an almost simplicial vertex of degree d     If low  d  then add an edge between every pair of non adjacent neighbours of v  removev  Building upon Lemma   we find that the almost simpli cial vertex rule is safe  Suppose that we have G and low before  and G  and low  after application of the rule  Then  r G        r G   r G       max d r G     and d      low   low  We conclude that max r G  low  max r G   low    Examples can be constructed  unfortu nately  that show that the rule is not safe for low   d     Figure    The almost simplicial vertex rule        BODLAENDER ET AL   UAI      A special case of the almost simplicial vertex rule applies  Lemma   Let G be an undirected graph and let v  w be  to vertices of degree two  A vertex of degree two is  by def  two vertices of degree three having the same set of neigh bours  Let G  be the graph that is obtained from G by turning the set of neighbours ofv into a clique and then removingv and w  Then   inition  almost simplicial and we can therefore replace it by an edge between its neighbours  provided that the original graph has treewidth at least two  The resulting rule  illus trated in Figure    is called the series rule  after         Reduction Rule  a  Series rule    Let v be a vertex of degree    If low     then add an edge between the neighbours of v  if they are not already adjacent          v               Now  let  in of G  given                    be a vertex of degree   such that at least                                            f    of G     From Lemma   we have safeness of the buddy rule  which is illustrated in Figure                         w  be vertices of degree   having the same  add an edge between every pair of non                           adjacent of neighbours of v  removev      removew    Figure    The triangle rule  As the series and triangle rules are special cases of the al  most simplicial vertex rule  both are safe  If the twig and islet rules cannot be applied to a non empty undirected graph  then the graph has treewidth at least two  We can then set low to max  low      Note that from this ob servation we have that the islet and twig rules suffice for re ducing any graph of treewidth one to the empty graph  The  islet  twig and series rules suffice for reducing any graph of treewidth two to the empty graph  So  if low        for a given non empty graph and the islet  twig and series rules cannot be applied  then we know that the graph has treewidth at least three  We can then set low to max low      As for treewidths one and two  there is a set of rules that                                            V                                            w  Figure    The buddy rule  The cube rule  presented schematically in Figure    is slightly more complicated  The subgraph shown on the left is replaced by the one on the right  in addition  low is set to max  low      Vertices v   w  and  x  in the subgraph may  be adjacent to other vertices in the rest of the graph  the four non labeled vertices cannot have such  outside  edges  Due to space limitations we do not provide a lemma from which safeness of the rule can be seen  the proof of safe ness  however  is similar to the proofs given above   This set of rules was first identified by S   Arnborg and A  Proskurowski      The islet  twig  series and triangle rules are among the set of six  The two other rules are of interest to us  not just because they provide for computing optimal triangulations for graphs of treewidth three  but also because they give new reduction rules           suffice for reducing any graph of treewidth three to the empty graph   their  formerly   H  then both are simplicial in the result  If low         then  removev      By  set of neighbours   adjacent neighbours of v      f  If we add v and w with  The properties stated in the lemma now follow   Letv   add an edge between every pair of non    w   Reduction Rule    Buddy rule  If low     then    and  v  therefore  equals the maximum of   and the treewidth off   two of its neighbours are adjacent     be the neighbours of  ing graph  The treewidth of the ordering  v   w   Reduction Rule  b  Triangle rule    y and z  adjacent edges to  Another special case is the triangle rule  shown in Figure     v  x   G   So  G  is a minor of G  We find that r G         r G   f be a linear ordering of G  and let H be the fill     I  Figure    The series rule   Let  Let  contracting the edges   v  x  and   w  y  in G  we obtain              the linear ordering  v   w  f    with f a linear or dering ofG  oftreewidth at most max    r G     has treewidth at most max    r G      Proof   remove v     r G         r G  and r G        max    r G      Figure    The cube rule    BODLAENDER ET AL       The sub graph in the left hand side of the  cube rule  is not  UAI         If a reduction rule can be applied to G   it is executed   very likely to occur in the moralisation of a probabilistic  modifying G  accordingly  Each vertex thus removed  network s digraph  but it is not impossible  The main rea  is pushed onto the stack S  if prescribed by the rule   son for our interest in the rule is that it is one of the six  the lower bound  rules that suffice for reducing graphs of treewidth three to  until the reduction rules are no longer applicable   low    for a given non empty islet  twig  series  triangle  buddy and cube  the empty graph  So  if graph and the  rules  cannot be applied  then we know that the graph has  treewidth at least four  We can then set To conclude this section  Figure  low to max  low       The figure depicts how application of our reduction rules serves to reduce the fragment to a single vertex  In fact  the moralised graph of the  entire  ALARM network is thus re  duced to the empty graph  which indicates that our reduc tion rules provide for constructing an optimal triangulation   This step is repeated     If no rule can be applied and low       then  low  is  increased by I  The reduction is continued at step       L et G  be the graph  that results after execution of the  previous steps  Using an exact or heuristic algorithm     shows a fragment of the  well known ALARM network  along with its moralisation   low is updated   G  is triangulated       Let H be the triangulation that results from step    For H  a perfect elimination scheme f is constructed      Until S is empty  the top element  v  and f is replaced by  v  f    is popped from S     Let f  be the linear ordering resulting from the previ   ous step  The fill in of M G  given f  is constructed  The steps I through    describe the reduction of the graph In step    the graph that re  of a probabilistic network   sults after reduction is triangulated  For this purpose  vari  ous different algorithms can be used  If the algorithm em ployed is  exact   that is  if it yields a triangulation of min  imal treewidth  then our method yields an optimal trian gulation for the original moralised graph  For many real life networks  the combination of our reduction rules with an exact algorithm results in an optimal triangulation in reasonable time  Figure    A fragment of the ALARM network and the re  duction of its moralisation      implements  We argued that application of our rules may reduce a net work s moralised graph to the empty graph  The computa tional method complements this reduction by its reversal  thereby providing for the construction of a triangulation of minimal treewidth  For networks that cannot be triangu lated optimally just by pre processing  our rules are com bined with an algorithm that serves to find an optimal or close to optimal triangulation of the reduced graph  The computational method takes for its input the directed acyclic graph G of a probabilistic network  it outputs a tri angulation of the moralisation of G  The method uses a stack S to hold the eliminated vertices in the order in which they were removed during the graph s reduction  Moreover   low  maintains a lower bound for the treewidth  of the original moralised graph  it is initialised at    The method now amounts to the following sequence of steps  I  The moralisation M  G  of G is computed and G  is  initialised at M G    triangulation algo  we will argue in the next section  however  these heuris  pre processing of probabilistic networks for triangulation   the value  heuristic  moralised graph then is not guaranteed to be optimal  As  The reduction rules described in the previous section are  computational method that  not be feasibly computed  a  rithm can be used  The treewidth yielded for the original  Computational method  employed within a  If after reduction a graph of consider  able size remains for which an optimal triangulation can  tic algorithms tend to result in better triangulations for the graphs that result from pre processing than for the original graphs  If  after executing the steps I through    the re duced graph is empty  we can construct a triangulation of minimal treewidth for the moralised graphjust by reversing the various reduction steps  and further triangulation is not necessary  This situation occurs  for example  if the origi nal graph is already triangulated or has treewidth at most    The ALARM network gives another example  its moralised graph has treewidth four and is reduced to the empty graph  In step    of our computational method  each of the reduc  tion rules is investigated to establish whether or not it can be applied to the current  reduced  graph G   As soon as an applicable rule is found  it is executed  When analysing the computational complexity of our method  it is readily seen that investigating applicability of the various reduc tion rules is the main bottleneck  as all other steps  except for the triangulation in step     take linear time       Investigating applicability of the islet   twig and series rules  takes a total amount of computation time that is linear in the number of vertices  To this end  we maintain for each   BOPLAENDER ET AL   UAI          network  vertex an integer that indicates its degree  we further main  before moralisation  tain lists of the vertices of degree zero  one  and two  The  buddy  triangle and cube rules  lVI  can also be implemented to  WILSON  take overall linear time  for example using techniques from      More straightforward implementations  however  will  ALARM  VSD  also be fast enough for moderately sized networks   OESOPHAGUS  For the simplicial vertex and almost simplicial vertex rules  efficient implementation is less straightforward  To inves  MUNIN  OESOPHAGUS   ICEA PATHFINDER  tigate whether or not a vertex is simplicial  we must verify that each pair of its neighbours are adjacent  For this pur pose  we have to use a data structure that allows for quickly checking adjacency  such as a two dimensional array  For a vertex of degree d  investigating  O d   time  In  a graph with  n  simpliciality  then takes  vertices  we may  have  to  O n  times  Each such check O ne  time  where d v  is the de   check for simplicial vertices costs  OCL v d v     gree of vertex v     and e  denotes the number of edges m the  graph  The total time spent on investigating applicability of the  simplicial vertex rule  is therefore  O n e   As  the  treewidth of the moralised graph of a real life probabilistic  network is typically bounded  we can refrain from checking simpliciality for vertices of large degree  giving a running time of  O n    in practice  For the almost  simplicial vertex  rule  similar observations apply   Table l   IAI                                               after moralisation  lVI  lEI                                                                           l         The sizes of the digraphs of the various networks  and of their moralisations   PR   PR   PR    PR l U  series  PR   U  triangle  buddy  cube  S UPR    With each of these sets of rules  the moralisations of  the networks  graphs were reduced until the rules were no longer applicable   T he table reports the sizes of the It reveals  for example  that  resulting reduced graphs  the set of rules  P R   suffices for reducing the moralised  graphs of the WILSON and OESOPHAGUS networks to the empty graph   with the additional simplicial vertex rule   the moralised graphs of the ALARM and VSD networks are also reduced to the empty graph  These four networks are     therefore triangulated optimally just by pre processing  We  Experimental results  would like to note that addition of the almost simplicial ver  T he computational method outlined in the previous section implements our method of pre processing probabilistic net  tex rule to PR   did not result in any further reductions  For the OESOPHAGUS   MUNIN  ICEA and PATHFINDER  works for triangulation  We conducted some experim ents  networks  we further studied the effect of pre processing  results of these experiments are reported in this section   algorithms  Table  with the method to study the effect of pre processing  The  The experiments were conducted on eight real life prob abilistic networks in the field of medicine  the WILSON network for the diagnosis of Wilson s liver disease  the  well known  ALARM network for  anaesthesia monitoring   the VSD network for the prognosis of ventricular septal de fect in infants  the OESOPHAGUS network for the staging of oesophageal cancer and the extended OESOPHAGUS   network for the prediction of response to treatment  the well known MUNIN network for the interpretation of elec tromyographic findings  the ICEA network for the predic tion of coverage by antibiotics of pathogens causing pneu monia and the well known PATHFINDER network for the    diagn sis of lymphatic disease  The sizes of the digraphs of these networks and of their moralisations  expressed in terms of the  number of vertices and the number of arcs and  edges  respectively  are given in Table I  The effects of employing various different sets of pre processing rules on the eight networks under study are sum marised in Table  S P R       T he sets employed are denoted    simplicial vertex   islet  twig   y  on the treewidths yielded b various heuristic triangulation    summarises the results obtained with maximum cardinality search  denoted MCS       and with  the  perfect triangulation and minimal triangulation vari breadth first search  denoted LEX p  ants of lexicographic  and LEX M  respectively        We ran the three heuris tic algorithms on the original moralisations of the four net works and on the reduced graphs after employing the sets of rules P R   through PR    The table reveals that  for the MUNIN and ICEA networks  the heuristic algorithms tend to yield a smaller treewidth from the reduced graphs than from the original moralisations   The table in addi  tion shows that the further reduced a graph  the less com putation time is spent by the algorithms  As the time spent on pre processing is negligible  these results indicate that pre processing a probabilistic network is profitable  not just with respect to the quality of the triangulation yielded but also with respect to the computation time spent  In our experiments we observed that the treewidths found by the heuristic algorithms depend to a large extent on the vertex with which the computation is started  To investigate the effect of the starting vertex  we ran the three heuristic algorithms a number of times every time starting with    different vertex  Table  a    summarises the results  indicating   BODLAENDER ET AL       network  before  pre pro  pre pro  pre pro  pre pro  withS  with PR    with PR    lVI  WILSON ALARM VSD OESOPHAGUS OESOPHAGUS  MUNIN                              ICEA PATHFINDER  UA       lEI                                 lVI                          lEI    II                       lVI  lVI  lEI  II                                                                                lEI  pre pro  pre pro  with PR    with PR    lVI                                                        lEI                            lVI  lEI                                                Table    The effects of employing different sets of reduction rules   lVI  lEI                                         MUNIN O        MUNIN                 network  CPU time spent  computed treewidth  MCS  LEX F  LEX  M  MCS  LEX p  LEX  M                                                                                                                                                                                                                                                                                                                                                                       PATHFINDER O       PATHFINDER  I            PATHFINDER                                                                     PATHFINDERA                                                          OESOPHAGUS     OESOPHAGUS     OESOPHAGUS    OESOPHAGUS   OESOPHAGUS A  MUNIN     MUNIN  ICEA O ICEA l ICEA    ICEA J  PATHFINDER                                                                 Table    The effect of pre processing on the treewidths yielded by the three heuristic triangulation algorithms   network  lVI  lEI         LEX p  MCS  rEX  M  min  average  max  min  average  max  min  average  max                                  to                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            PATHFINDER                                               PATHFINDER                                                                                                        OESOPHAGUS    OESOPHAGUS  I OESOPHAGUS      OESOPHAGUS     OESOPHAGUS A MUNIN O MUNJN l MUNIN     MUNIN  ICEA O ICEA  I ICEA    ICEA  PATHFINDER Q PATHFINDER   PATHFINDER                                     Table    Some statistics for the three heuristic triangulation algorithms    UAI      per graph the minimum and maximum treewidth found and  form better  Some of our reduction rules are safe also with  the average treewidth over all possible starting vertices  We  respect to minimum overall state space  Other rules  how  would like to note that  using integer linear programming  ever  are safe only under additional constraints on their ap  techniques on the most reduced graph  we found the exact  plication  We plan to further investigate pre processing for  treewidth of the PATHFINDER network to be     finding triangulations with minimum overall state space      Conclusions and further research  When solving hard combinatorial problems  pre processing is often profitable   Based upon this general observation   we designed a computational method that provides for pre processing of probabilistic networks for triangulation  Our method exploits a set of rules for stepwise reducing the problem of finding a triangulation of minimum treewidth for a network s moralised graph to the same problem on a  Acknowledgements  The research of the first author was partly supported by EC contract IST             Project ALCOM FT  Algorithms and Complexity   Future Technologies   The re search of the last two authors was partly supported by the Nether lands Computer Science Research F oundation with financial sup port from the Netherlands Organisation for Scientific Research   
  To investigate the robustness of the output probabilities of a Bayesian network  a sensi tivity analysis can be performed  A one way sensitivity analysis establishes  for each of the probability parameters of a network  a func tion expressing a posterior marginal proba bility of interest in terms of the parameter  Current methods for computing the coeffi cients in such a function rely on a large num ber of network evaluations  In this paper  we present a method that requires just a single outward propagation in a junction tree for es tablishing the coefficients in the functions for all possible parameters  in addition  an in ward propagation is required for processing evidence  Conversely  the method requires a single outward propagation for computing the coefficients in the functions expressing all possible posterior marginals in terms of a sin gle parameter  We extend these results to an n way sensitivity analysis in which sets of pa rameters are studied      INTRODUC TION  The robustness of the output probabilities of a Bayesian network can be investigated by performing a sensitivity analysis of the network  For mathemat ical models in general  sensitivity analysis serves to identify the effects of the inaccuracies in a model s pa rameters on its output  Morgan   Henrion        For a Bayesian network  more specifically  performing a sensitivity analysis yields insight in the relation be tween the probability parameters of the network and its posterior marginals  The simplest type of sensi tivity analysis is a one way analysis in which a single parameter is studied  a more general n way analysis serves to investigate the joint effects of inaccuracies in  a set of parameters  In the brute force approach to performing a one way sensitivity analysis of a Bayesian network  each proba bility parameter of the network is varied systematically and the effect on the output probabilities of the net work is investigated  Performing a sensitivity analysis in this way requires thousands of network evaluations  or  full  propagations  and is  therefore  much too time consuming to be of any practical use  Laskey        has been the first to address the compu tational complexity of sensitivity analysis of Bayesian networks  She has introduced a method for computing the partial derivative of a posterior marginal proba bility with respect to a parameter under study  Her method thus yields a first order approximation of the effect of varying a single probability parameter on a posterior marginal  Compared to the brute force ap proach  her method requires considerably less compu tational effort  The method  however  provides insight only in the effect of small variations of parameters  when larger variations are considered  the quality of the approximation may rapidly break down  The relation between a posterior marginal probability of interest and a parameter under study can be ex pressed through a simple mathematical function  The function expressing the posterior marginal is a quo tient of two linear functions in the parameter  as has been shown by Castillo et al          Building upon this property  it suffices to establish the coefficients in this function to determine the effect of parameter variation  Castillo et al         and Coupe   van der Gaag        have designed methods to this end  These methods require a single network evaluation for each coefficient to be established  Although these methods currently are the most efficient available  they rely on a large number of network evaluations and  as a con sequence  are infeasible for large realistic networks  In this paper  we present a new method for sensitivity analysis of Bayesian networks  Our method  like the        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       two methods mentioned above  exploits the property that a posterior marginal probability relates by a sim ple mathematical function to a parameter under study  It requires just a single outward propagation in a junc tion tree  however  to compute the coefficients in the functions for all possible parameters  in addition  it re quires an inward propagation for processing evidence  Conversely  the method requires a single outward prop agation for establishing the coefficients in the functions expressing all possible posterior marginals in terms of a single parameter  Our method can be readily ex tended to an n way sensitivity analysis in which sets of parameters are varied  In addition to a sensitivity analysis  an uncertainty analysis can be performed for investigating the robust ness of the output probabilities of a Bayesian network  In an uncertainty analysis  all parameters are varied si multaneously through sampling  it therefore provides little insight into the effects of variation of specific pa rameters  Experiments with uncertainty analysis have led to the suggestion that Bayesian networks are in sensitive to inaccuracies in their parameters  Pradhan et al        Henrion et al         In these experiments  however  a measure of model robustness was obtained by assuming a lognormal distribution for each parame ter and averaging over the probability of the true diag nosis for various diagnostic situations in a medical ap plication  Rather than in the average of the probabili ties of the true diagnosis  however  it is in the variation of these probabilities that inaccuracies in parameters are reflected  From these experimental results  there fore  no decisive conclusions can be drawn as to the sensitivity of Bayesian networks  In fact  Coupe et al         have reported high sensitivities in an emprical study in the medical domain  involving real patient data  We feel that these and emerging similar expe riences warrant further investigation into sensitivity analysis of Bayesian networks      THE BASIC P ROPERT Y  Sensitivity analysis of a Bayesian network basically amounts to establishing  for each of the network s pa rameters  a function expressing an output probabil ity in terms of the parameter under study  For out put probabilities  we shall consider posterior marginal probabilities of the form y   p a I e   where a is a value of a variable A and e denotes the evidence avail able  Each of the network s parameters is of the form x   p bi l r   where b  is a value of a variable B and  r is an arbitrary combination of values of the set of parents II  pa B  of B  We will write p ale  x  to denote the function expressing the posterior marginal p a I e  in terms of the parameter x  In the sequel  we will assume that in a sensitivity anal ysis  upon varying a parameter x p b  l r   each of the other probabilitiesp bj l r  is co varied accordingly  by scaling by the ratio between the probability masses left  More formally  let the variable B have for its do main dom B   b      bm   m       Note that the parameters p bj l r   j  f  i   are functions of x  We now assume for these functions that       if j  i otherwise  with p b  l r               With the assumption of co variation as outlined above  the function y x  yielded by a sensitivity analysis is a quotient of two linear functions in x  The following theorem reviews this important property  the associ ated proof provides the basis for the algorithms pre sented in Sections   and    Theorem   Let p be the probability function defined by a Bayesian network over a set of variables V  Let y   p a I e  and x p b  l r  be as indicated above  Then     The paper is organised as follows  Section   reviews the important basic property that a posterior marginal probability can be expressed as a quotient of two linear functions in a parameter under study  In Section    we briefly describe currently available methods for sensi tivity analysis that build upon this property  In Sec tion    we present our method for computing the co efficients in the functions for all possible parameters  using just one propagation in a junction tree  In Sec tion    we describe a similar method for computing the coefficients in the sensitivity functions relating all possible posterior marginals to a single probability pa rameter  These methods are generalised to an n way sensitivity analysis in Section    The paper ends with some concluding remarks in Section     y  where a            p a e  x  p e  x   ax  f    X           and   are constants with respect to x   Proof  The joint probability p a e  can be expressed  in terms of x as  p a e  x        V    x    where Lv a     dp V  denotes summation over the variables V   A          D  with A        DE V fixed at values a        d  respectively  The sum Lv a ep V  in the above equation can be split into n    separate sums  such that the first sum        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       includes only terms with the value b  for B and the state  r for II  the second sum includes only terms with the value b  for B and II in state  r  and so on  and the last sum includes the remaining terms  So           as described by Castillo et a           After having identified the set of relevant parameters  the sensitivity analysis can be restricted to this set  Building upon the set of n relevant parameters  x           Xn  the algorithm of Castillo et a          iden tifies sets of monomials for which the coefficients will be zero in the linear function p a  e  x           Xn    For the resulting m monomials  the algorithm constructs a system of m independent equations of the form i y p a  e  xi          x   where  for each j  xj denote arbitrary values for parameter Xj  The corresponding values yi  i          m  are obtained through m net work evaluations  The coefficients in the function are now determined by solving the set of equations thus obtained  Coupe   van der Gaag independently de scribed a similar method  also based on the idea of solving a system of independent equations  They fur ther argue that in a one way sensitivity analysis three network evaluations suffice per relevant parameter        p       p V     l v a e b      V   L L   p bl r    V a e n f    j i For the probability p e  we derive a similar expression by summing  in the above derivation  over all values of the variable A instead of keeping it fixed at a  From the resulting expressions p a  e  x  and p e  x   it is readily seen that the output y p a I e  can be written as a quotient of two functions that are linear in x  D    From Theorem   we have that the function that ex presses a posterior marginal probability y in terms of a single parameter x is characterised by at most three coefficients  The theorem is easily extended to n parameters  The function then includes the prod ucts of all possible combinations of parameters  termed monomials  in both its numerator and its denomina tor  The numerator as well as the denominator are characterised by  n coefficients  many of which may be zero     CURRENT METHODS  The most efficient methods for sensitivity analysis of Bayesian networks currently available exploit the basic property reviewed in the previous section  We briefly review these methods  Not all parameters in a Bayesian network can influ ence a posterior marginal probability of interest  The subset of parameters  possibly  influencing the poste rior marginal is dependent upon the evidence e  The set of relevant parameters is easily identified using a variation of the algorithm described by Geiger et a    The methods reviewed above have a computational complexity that is considerably less than the brute force approach of systematic variation of parameters  However  the methods can still be quite time consum ing  for a network of realistic size  it can easily require several hundreds of network evaluations to perform a one way sensitivity analysis  An more general n way sensitivity analysis to study the joint effect of simulta neous variation of n parameters can in fact be so time consuming that it is infeasible in practice     ANALYSIS OF ONE OUTPUT WRT  ALL PARAMETERS  The new methods for sensitivity analysis presented in this paper have been tailored to Bayesian networks in their junction tree representation  The methods basi cally perform a single or a few outward propagations in a junction tree and  as a result  are much less time consuming than the methods reviewed in the previous section  In this section  we present our method for computing the coefficients in the sensitivity functions expressing a posterior marginal of interest y p a I e  in terms of all possible probability parameters x  Recall that these functions are of the form presented in Theorem    Our method now builds on the idea that  in a junction tree  the expressions for p a  e  and p e  in terms of x can be derived from the potential of a clique containing both the variable and the parents to which the parameter x pertains  The following theorem details the coefficients to be computed     Theorem   Let p be the probability function defined by a Bayesian network and let T be a junction tree   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            for the network  Let y   p aie  and x   p b  l r  be as before  Suppose that  in T  an inward propagation has been performed towards a clique containing the variable of interest A  suppose that subsequently an outward propagation from this clique has been per formed with the value a for A  Now  let K be a clique in T containing both the variable B and its par ents II  pa B   let x   p K a e  be the potential of clique K after the abovementioned propagations  Then  p a e  x   ax     with  a   LK b      x    LK b      x p b l r      p b l r         J  r          L   i  LK b      x   L x     p b    r  K ll  f           Proof  The property follows directly from the proof of Theorem   by observing that p a e       K  K  D  Building upon similar observations  we have the fol lowing corollary  Corollary   Letp be the probability function defined by a Bayesian netwerk and let T be a junction tree for the network  Let x   p b l r  and K be as before  Suppose that the evidence e has been processed in T by an inward and subsequent outward propagation  Let  K   p K e  be the potential of clique K after the propagation  Then  p e  x     X    with              K      K     LK b              p b i r  j  f i K       r     Compute the coefficients a and     using the equa tions     and     from Theorem    for all relevant parameters  locally per clique  We would like to note that our method requires just one inward and two outward propagations to estab lish all sensitivity functions for a specific posterior marginal  whereas the methods reviewed in the pre vious section require three inward and outward prop agations per parameter  The method described above outlines the basic idea  The method  however  may be easier to implement in the alternative form based upon Theorems   and    Theorem   Let the junction tree T be as before  Also  let y   p aie  and x   p b l r  be as before and let K be a clique in T including both B and II   pa B   Now  let x  be the initially specified value for x and let x  denote an arbitrary other value for x  Suppose that  in T  an inward propagation has been performed towards a clique containing the vari able of interest A  suppose that subsequently an out ward propagation has been performed with the value a for A  Now  let x  p K a e  be the resulting clique potential for clique K  Let  y   p a e  x    L x  p  Bi r  y  p a e  x        K    p B i  r      Enter the evidence e into the junction tree and perform an inward and an outward propagation using an arbitrary root clique     Compute the coefficients  Y and    using the equa tions     and     from Corollary    for all relevant parameters  locally per clique     Perform an outward propagation from a clique containing the variable of interest A  with the ad ditional evidence A  a        where p B l r  and p  B l r  denote parameter vectors with x   x  and x   x   respectively  Then  y   ax     with       Theorem   and Corollary   provide the basis for our method for computing the coefficients in the func tions expressing the posterior marginal of interest y  p a I e  in terms of all possible parameters x  The method is composed of the following steps        K      Since both variable B and its parents are in cluded in clique K  we can obtain from the parameter vector  Proof   p Bi r      q  x           Qi   x     x   Qi l  x         Qn x       the parameter vector p  Bi r    q  x        q   x   x  q l  x       q x       where q and q  are parameters co varying according to equation      by multiplication of the potential x by p  Bl r jp Bl r   From Theorem    we have that y  ax      The expressions for a and    now follow D from simple mathematical manipulation         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Let the junction tree T be as before  Also  let y   p afe  and x   p bi f r  be as before  Suppose that  in T  an inward propagation has been performed towards a clique including the variable of interest A  Then  p e  x     X     with     Compute the coefficients      from Theorem     Theorem        eta   aa and    f a   f a         where aa f a and aa f a are as in equation      ob tained from outward propagations with the evidence A   a and A        a  respectively  Proof  We begin by observing that p e  x    p a e  x    p a e  x   By entering the evidence A   a in a clique H containing the variable A and propagating outwards  we obtain the potential p K  a  e  for clique K  From this potential  K p a  e  Y   LK cPK is readily computed  as de scribed in equations     from Theorem    Similarly  by entering the evidence that A does not have the value a  that is  by multiplying the clique potential for H with a vector over dom A   in which the en try corresponding to state a is zero and all other en tries equal    and propagating outwards  we obtain the potential  p K  a  e   From this poten tial  Ya   p a e    LK  is readily computed  Using equation     from Theorem    we get y and Ya Now  using equation      we find eta  aa  f a  and f a Inserting these coefficients into the expres sion p e  x   p a  e  x    p a  e  x  yields the result D stated in the theorem              Our alternative method  building upon Theorems   and    is composed of the following steps     Enter the evidence e into the junction tree and perform an inward propagation towards a clique H containing the variable of interest A     Perform an outward propagation from H with the additional evidence A a        Compute y    and y  using the equations     and     from Theorem       Compute the coefficients a   aa and f    f a  using      for all relevant parameters  locally per clique     Retract the evidence A the evidence e      a without retracting     Perform an outward propagation from H with the additional evidence A        a     Compute Ya and Ya  using the equations     and     from Theorem     and    using equation  To allow for retracting the evidence A   a in Step   of our method without retracting e  fast retraction prop agation  Cowell   Dawid       is used in Step    Comparing the computational costs of the two al ternative methods  we note that they both require one inward and two outward propagations  Consider ing the first method  we observe that Steps   and   are equally costly  The computation of the coeffi cients a and  Y costs   Jdom K J m operations  where m   fdom pa B  J  the computation of j  and   costs  fdom K  I arithmetic operations  Thus  the addi tional cost of the first method is roughly in the or der of   to   times Jdom K f  The additional costly steps in the second method are Steps   and    both costing approximately   fdom K J operations  Thus  the additional cost of the second method is roughly   fdom K f arithmetic operations  This rough com parison of the computational costs of the two methods only addresses the number of arithmetic operations in volved  The first method  however  has a much larger overhead in terms of computing indices in performing the various summations  Thus  depending on the im plementation  the two methods might very well have comparable performance       ANALYSIS OF ALL OUTPUTS WRT  ONE PARAMETER  Having identified a parameter x to which an output probability of a Bayesian network is particularly sensi tive  one might be interested in establishing sensitivity functions for all possible posterior marginals in terms of this parameter  Such an analysis amounts to com puting the coefficients in these functions  Note that while the method described in Section   provides for evaluating the overall robustness of a Bayesian net work  the method described in this section is provides for getting insight in the spread of influence from sep arate parameters  The method of Castillo et a          and the method of Coupe   van der Gaag        can be exploited for es tablishing the coefficients in the sensitivity functions for all possible output probabilities  requiring three propagations per posterior marginal  Building upon the ideas put forward in the previous section  how ever  a more efficient method is obtained  Theorem   provides the basis for our method  Let the junction tree T be as before  Also  let y p afe  and x p bl r  be as be fore  Suppose that  in T  an inward propagation has Theorem          Compute aa and f a  using        Y      UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            been performed towards a clique containing the vari able B to which the parameter x pertains  Then  p e  x     X  J with   Y   O a   O a and J  f a   f a         where O a f a and a a f a are as in equation      ob tained from two outward propagations with two dis tinct values for x  Proof  Let K be a clique containing both the variable B and its set of parents II   pa B   Let x  and x  denote two different values of the parameter x  From the outward propagation using x  from clique K  we obtain the probability vector p A e  x      y  Ya  through marginalization of the clique potential for a clique H containing A  Similarly  from the outward propagation with x   we find the vector p  A e  x       y  Ya   From     n WAY SENSITIVITY ANALYSIS  So far  we have addressed one way sensitivity analy ses only  in which the effects of separate parameters are studied  In this section  we turn our attention to more general n way analyses in which the effects of simultaneous variation of n parameters are studied  One can regardn way sensitivity analysis as involving analyses of joint effects for all subsets of size n or less of all  say m  relevant parameters  Using the method of Castillo et al          this would involve L    l        separate analyses and I   I ri probability propa gations to compute the  n coefficients  assuming r ary variables         Provided the n parameters all belong to the same clique  Theorem   below states that we only need one propagation to compute the  n coefficients  but I    I local computations involving marginaliza tions of clique potentials            we get the result stated in the theorem      Theorem   provides the basis for our method for com puting the coefficients in the functions expressing all possible output probabilities y   p a I e  in a single pa rameter x p blrr   The method is composed of the following steps        Enter the evidence e into the junction tree and perform an inward and an outward propagation using an arbitrary root clique     Compute the probability vector p A e    y  Ya  through marginalization of the clique po tential for a clique H containing A  for all vari ables of interest     Change the value of parameter x and perform an outward propagation from a clique containing both the variable B and its parents     Compute the probability vector p  A e     y  Ya  through marginalization of the potential for clique H  for all variables of interest   In essence  Theorem   states that the mathematical expression for a probability  p e  x   of a vector of in stantiations  e  as a function of a probability parame ter  x  takes the form of a linear function of x  The orem   generalizes this statement to the case with n parameters  x          Xn  and states that the resulting function is a multilinear function in x         Xn  To simplify the exposition  we shall assume that the parameters are independent  that is  for each pair of parameters  x    p bx  lrrx   and Xj   p bx  lrrx    with the associated variables Bx   IIx   Bx   and IIx   it holds true that rrx    rrx   Bx   j  IIx   and Bx   j  IIx   To generalize the theorem to cover the case of dependent parameters is fairly straightforward  Let p be the probability function for a Bayesian network  where evidence e has been prop agated in a junction tree  T  for the network  Let X   x           Xn  be a set of parameters of the net work  where  for each i             n   Theorem       X   p bx  lrrx    with the associated variables  Bx  and IIx   being members of a clique  C  in T  Then  p e  X        Compute a a  a a  f a  and f a  using the equation     from Theorem       Compute   Y  and J  using the equation        We would like to note that our method requires just one inward and two outward propagations to estab lish all sensitivity functions for a specific probability parameter   L  Yx Z  II  zx  z     zEZ  L    Jc   C ll  r  where II   IIx      IIxn    rr   rrx      rrxn    and   Yx Z         X ZI where  fx Y        X YI  L  x Y    yz   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            If  instead of summing over subsets S  X Z  we sum over the subsets of Z and takes care that the signs of the terms are preserved  we get the desired result  D where c   p C e   W   X Y  w    wk    k     W I   by   byp byiY    b    b n        b  n w    and Px  denotes the initial value of Xi           Proof  Using the same procedure as in the proof of Theorem   we get  p e  X        F  c o     L L  X    c     c        b zt   c   X   c   P b    l x   xr     p bn   l xn   xn  b Zn  Lc b zl       b  l n   f  c p b   l x      p bn   l xJ     L  C        c   L   li  L        z  p b    ru   ur       flzEZ p bz   rz   fluEU p b i ru   C      r     p b  l ru   ur   p b u  l ru u   uiUI      uEU    II     p b   ru  p b   ru   U   p bul ru     p bu l ru       b  l u   p  u I u  uEU       II x          ISI   xES sr   u   L     Inserting      in      and rearranging terms yields p e  X      L IT L z  Z   X zEZ S   U  n     p e  X        ISI  IT x  xES  L r Z  II  Z   X  z     zEZ  J         Through one way analysis involving the parameter x we obtain the constants ax and f x in    axx   f x  The  n constants in      are related to ax and f x in the way that ax equals the sum of all coefficients  r  Z   for which xE Z  and f x equals the sum of the remaining coefficients  That is  ax   f x         c            and  where U   X  Z  u       uiU I   Now  expand ing the terms p b   ru  u   uE U  using      an easy calculation yields  II     p b u   l uiui  uiUI   Lc bz b         c  L    p e  x   The multiple sum can be grouped into sums over the subsets Z  X such that b   bz for each zE Z  p e  X   Note that  since the computation of  X  X  ranges over all subsets of X  rx Z  can be computed  for each Z C X  as a sum over a subset of the terms involved in the computation of  Yx X    The result presented in Theorem   is limited in the sense that all parameters under investigation must belong to the same clique in the junction tree  We shall now present a more general method which uti lizes the results obtained by lower order analyses  Let X n   be the parameters under investi X   x    gation and write     L L  r  Z         r  Z    J         Z   X xEZ  Z   X x lZ        n      Thus  for each one way analysis of a parameter Xi  i    we obtain equations of the form      and       In addition  we obtain the equation    p e      L  Z   X  r  Z   II Zo   J         zEZ   n  where z  denote the value specified for parameter z in the Bayesian network  This gives us a total of     equations  However  we need  at least   n equations to compute the  n coefficients  Now  if for each parameter  z E Z  we assign a new value and perform a full propagation  then we obtain an additional     equations  Thus  we can obtain at least  n equations by performing   n    l n    J  full propagations with different parameter values  in addition to the initial propagation performed for the   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            one way analyses  For example  to perform   way analyses  a total of two full propagations is sufficient  This result can be generalized very easily  since each m way analysis gives rise to  m equations of the form      and       Thus  to perform n way analyses  where n   m  we need at most       additional propagations  as there are     relevant m way analyses  So  for example  if we have performed   way analyses and want to perform   way analyses  no further propagations are needed     CONC LUD ING REMARKS  We have presented methods for sensitivity analysis of Bayesian networks which are significantly more ef ficient than current methods  In the case of one way analysis  the number of probability propagations of current methods grows linearly in the number of relevant parameters  whereas the methods presented above only requires one inward and one or two out ward propagations  no matter the number of relevant parameters  To substantiate the importance of this difference  we have investigated three real world networks to get an idea of the typical number of relevant parameters in a realistic scenario  All three networks are from the medical domain  a subnetwork of Munin  Andreassen et a         containing      variables  a network mod elling the pathophysiology of ventricular septal defect  Coupe et al        containing    variables  and a net work related to disorders in the oesophagus containing    variables  The investigation were conducted using real patient data involving  respectively         and   patients  The average number of relevant parameters were found to be             and      respectively   Since no censoring of parameters representing func tional relationships were performed on parameters for the Munin network  the figure       is probably some what overestimated   Efficient methods for sensitivity analysis play an im portant role in both the knowledge acquisition and the validation phases for manually constructed Bayesian network models  Coupe et al         reports on an empirical study using sensitivity analysis to focus attention on the most in fluential parameters in the knowledge acquisition pro cess  thereby considerably reducing the time required to acquire the parameter values  The validation phase involves two aspects  fine tuning and robustness analysis  The fine tuning aspect in   volves adjustment of the parameter values to make the network respond correctly to a number of test cases  A gradient descent approach is useful for that purpose  cf  neural network type training   where the gradi ent of a posterior marginal with respect to a subset of parameters can easily be computed through a minor modification of the algorithms for sensitivity analy sis  Based on the work described in the present paper  Jensen        has suggested a method for gradient de scent training of Bayesian networks  Once a network has been fine tuned  and thus responds correctly on a selection of test cases  the robustness of the network may be investigated  This involves  in essence  determining lower and upper bounds for parameter values for which the output of the network still agrees with the test cases  A parameter value close to one of the bounds indicate a possible lack of robustness  Given analytic expressions for the outputs in terms of the parameters  derived by e g  methods described in the present paper  these bounds are easily determined  The time complexity ofn way sensitivity analysis may be fairly high for large n  even with the methods pre sented in this paper  Also  our method assumes that the variables and the parents associated with then pa rameters reside in the same clique in a junction tree  The method of Coupe et al         for n way sensitiv ity analysis is based on propagation of tables of coeffi cients in a junction tree  and  therefore  has a  poten tially very much  larger space requirement  However  their method is general in the sense that it does not put any restrictions on the location of the parameters  During the initial phase of the work  the authors received valuable comments from Finn V  Jensen  Also  he suggested the basis for the general method for n way analysis  described at the end of Section    Acknowledgements  The research has been partly funded by the Danish National Centre for IT research  Project no        
  Straightforward sensitivity analysis of a probabilistic net work is highly time consuming  The probability of interest  With the advance of efficient algorithms for  sen sitivity analysis of probabilistic networks  study  has to be computed from the network for a number of devi  ing the sensitivities revealed by real life net  eter  Even for a rather small network  this easily requires  works is becoming feasible  As the amount of data yielded by an analysis of even a moderately sized network is already overwhelming  effective methods for extracting relevant information from these data are called for  to study the  derivatives  One such method is  of the sensitivity func  tions yielded  to identify the parameters that upon variation are expected to have a large effect on a probability of interest  We further propose to build upon the concept of  admissible deviation   which captures the extent to which a parameter can be varied without inducing a change in the most likely outcome   We illustrate these con  cepts by means of a sensitivity analysis of a real life probabilistic network in the field of oncology   ations from the original assessment for every single param thousands of propagations   Recently  however  efficient  algorithms for sensitivity analysis have become available  rendering analysis of real life probabilistic networks feasi ble         These algorithms build upon the observation that the sensitivity of a probability of interest to parameter vari ation complies with a simple mathematical function  this  sensitivity function  basically expresses the probability of  interest in terms of the parameter under study  Computing the constants for these sensitivity functions requires just a limited number of propagations  Sensitivity analysis of a real life probabilistic network tends to result in a huge amount of data  For example  for a real life network in the field of oncology  comprising some lOOO parameters  we found that a single analysis yielded close to      sensitivity functions with two or three con stants each  Because the sensitivities exhibited by a prob     abilistic network typically change with evidence  such  Introduction  The numerical parameters for a probabilistic network are generally estimated from statistical data or assessed by hu man experts in the domain of application  As a result of incompleteness of data and partial knowledge of the do main  the assessments obtained inevitably are inaccurate  Since the outcome of a probabilistic network is built upon these assessments  it may be sensitive to the inaccuracies involved and  as a result  may even be unreliable  The reliability of the outcome of a probabilistic network can be evaluated by subjecting the network to a  analysis   an  analysis has to be performed a number of times  For our  sensitivity  In general  sensitivity analysis of a mathematical  model amounts to investigating the effects of inaccuracies in the model s parameters by systematically varying the values of these parameters      For a probabilistic network  sensitivity analysis amounts to varying the assessments for one or more of its numerical parameters and investigating the effects on  for example  a probability of interest       network  for example  we conducted over     analyses in volving data from real patients  For extracting relevant in formation from the data thus generated  effective methods are called for that allow for  automatically  identifying pa rameters that are quite influential upon variation   These  parameters should be selected for further investigation as  the inaccuracies in their assessments are likely to affect the network s outcome  In her work on sensitivity analysis of probabilistic net works  K  Blackmond Laskey introduced the concept of  sensitivity value      The concept builds upon the  par derivative of the sensitivity function that expresses the  tial   network s probability of interest in terms of a parameter  under study  the sensitivity value is the absolute value of this derivative at the original assessment for the parame ter  As currently available algorithms for sensitivity anal ysis of probabilistic networks yield the probability of in    UAI       VAN DER GAAG   RENOOIJ  terest explicitly as a fimction of the parameter under study  the derivative and its associated sensitivity value are read ily determined  In this paper  we study the derivatives of sensitivity functions and show how sensitivity values can be used for selecting parameters for further investigation  In many real life applications of probabilistic networks  the outcome of interest is not a probability  but rather the most likely value of a variable of interest  In a medical applica tion  for example  the outcome of interest may be the most likely diagnosis given a patient s symptoms and signs  For this type of outcome  the derivative of a sensitivity fimction and its associated sensitivity value are no longer appropri ate for establishing the effect of parameter variation  a pa rameter with a large sensitivity value may upon variation not induce a change in the most likely outcome  while a parameter with a small sensitivity value may induce such a change for just a small deviation from its original assess ment  To describe the sensitivities of this type of outcome  we introduce the concept of admissible deviation  This concept captures the extent to which a parameter can be varied without inducing a change in the most likely value for the variable of interest  We show how the concept of admissible deviation can be used for selecting parameters  The various concepts introduced in this paper will be illus trated by means of a sensitivity analysis of a moderately sized real life probabilistic network in the field of oncol ogy  We would like to note that this network exhibits con siderable sensitivity to parameter variation  This observa tion contradicts earlier suggestions that probabilistic net works are highly insensitive to inaccuracies in the assess ments for their parameters I        Our results now seem to indicate that the sensitivities exhibited by probabilistic networks may vary from application to application  The paper is organised as follows  In Section    we provide some preliminaries on sensitivity analysis of probabilistic networks  In Section    we briefly discuss the oesophagus network with which we will illustrate the concepts intro duced in the subsequent sections  In Section    we study the derivative of a sensitivity function and its associated sensitivity value  In Section    we introduce the concept of admissible deviation and discuss its use for extracting relevant information from sensitivity data  The paper ends with our conclusions and directions for further research in Section       Preliminaries  Sensitivity analysis of a probabilistic network amounts to establishing  for each of the network s numerical parame ters  the sensitivityfunction that expresses the probability of interest in terms of the parameter under study  In the sequel  we denote the probability of interest by Pr  A a I e     or Pr a I e  for short  where a is a specific value of the vari able A of interest and e denotes the available evidence  The         network s parameters are denoted by x   p bi I  r   where bi is a value of a variable B and  r is a combination of val ues for the parents of B  We write  Pr ale  x  to denote the sensitivity function that expresses the probability Pr a I e   in terms of the parameter x  for ease of exposition  we will  often omit or abbreviate the subscript for the fimction sym bol f  as long as ambiguity cannot occur   Upon varying a single parameter x p bi I  r  in a prob abilistic network  the other parameters p bi I  r   j  f   i  specified for the variable B need to be co varied  Each pa rameter p bi I  r  can thus be seen as a function gj x  of the parameter x under study  In the sequel  we assume that the parameters p bj I  r  are co varied in such a way that their mutual proportional relationship is kept constant  that is  a parameter p bi I  r   is co varied according to    if j     i  otherwise for p bi I  r       Under the assumption of co variation described above  a sensitivity function f x  is a quotient of two functions that are linear in the parameter x under study         more for mally  the function takes the form  f x   x  b cx  d   a   where the constants a  b  c  and d are built from the assess ments for the numerical parameters that are not being var ied  From this property we have that a sensitivity function is characterised by at most three constants  These constants can be feasibly determined from the network  for example by computing the probability of interest for a small num ber of values for the parameter under study and solving the resulting system of equations      An even more efficient algorithm that is closely related to junction tree propaga tion  is available from         The oesophagus network  The oesophagus network that will be used to illustrate con cepts throughout this paper  is a real life probabilistic net work for the staging of oesophageal cancer  The network was constructed and refined with the help of two experts in gastrointestinal oncology from the Netherlands Cancer Institute  Antoni van Leeuwenhoekhuis  and is destined for use in clinical practice      As a consequence of a lesion of the oesophageal wall  a tumour may develop in a patient s oesophagus  The char acteristics of the tumour  such as its location in the oe sophagus and its macroscopic shape  influence the tumour s prospective growth  The tunour typically invades the oe sophageal wall and upon further growth may affect neigh    VAN DER GMG   RENOOIJ       UAI          Gutro eirrami       F                  I           CT         Figure    The oesophagus network  bouring organs  In time  the tumour may give rise to lym phatic and haematogenous metastases  The characteristics  depth of invasion  and extent of metastasis of the cancer largely influence a patient s life expectancy and are indica tive of the effects and complications to be expected from the various therapeutic alternatives  The three factors are summarised in the stage of the cancer  which can be ei ther I  IIA  liB  III  IVA or  VB  in the order of progressive disease  To establish the stage of a patient s cancer  typi cally a number of diagnostic tests are performed  ranging from biopsies of the primary tumour to gastroscopic and endosonographic examinations of the oesophagus  The oesophagus network is depicted in Figure    It cur rently includes    variables  The number of values per vari able ranges between two and six  with an average of      The number of incoming arcs per variable ranges between zero and three with an average of      the average number of outgoing arcs is      with a minimum of zero and a maxi mum of     For the network  a total of    probabilities are specified  The variable with the largest number of proba bilities        models the stage of the cancer  this variable is deterministic  The largest number of probabilities specified for a non deterministic variable is     Building upon the concepts outlined in Section    we per formed a sensitivity analysis of the oesophagus network  We took the probability of a specific stage for the proba bility of interest  As six different stages are defined  we performed six separate analyses  each time focusing on an   other stage  Of the prior network  that is  the network with out any evidence entered  we found     of the     param eters to be influential upon variation  For these parameters  the analyses yielded a total of      sensitivity functions with two or three constants each  Because a network s sensitivities typically change with evidence  we repeated the analysis with data entered from     patients diagnosed with oesophageal cancer  The number of data entered per patient ranged between   and     with an average of        The parameters that we found to be influential upon vari ation differed between patients  The number of influential parameters also differed considerably and was found to be as high as         The derivative and its sensitivity value  For extracting relevant information from the huge amount of sensitivity data that is typically generated from a proba bilistic network  effective methods are needed  In this sec tion  we discuss a method for this purpose that is based on the derivative of a sensitivity function and its associated sensitivity value  This method provides for selecting pa rameters that upon variation have a large effect on a proba bility of interest  In the next section  we introduce another method  based on the concept of admissible deviation  that focuses on the most likely value of a variable of interest  In Section    we argued that the sensitivity of a probability of interest to variation of a parameter x can be expressed as   VAN DER GMG   RENOOIJ  UAI       a function f   x  of the form f x   effect may break down rapidly  as noted before by Laskey      For some parameters  it may be that the sensitivity val  a x   b   cx d  ues for slightly smaller assessments are very large and the sensitivity values for slightly larger assessments are quite  The first derivative r   X  of this function is  especially the sensitivity values become quite large  Find  The probability of interest is sensitive to deviations from the original assessment x  of the parameter under study  if  lf  xo l  is greater than zero   As an example  Figure   depicts a sensitivity function that we found for the oesophagus network  The function shows  p Sono cervix   yes I probability Pr Stage   III I  the effect of varying the parameter    no  on the  specific patient  The assessment for the pa          For this assessment  we find a sensitivity value of       which indicates that devia rameter under study is x   tions from the original assessment would not greatly affect the probability of interest  If the assessment would have been      rather than       however  a much larger sensitiv ity value  of        would have been found  which indicates  that even minor deviations would have had a considerable effect  For the value value of just        ample  this property holds  the sensitivity value of the orig  ing a relatively small sensitivity value for a parameter s as    Metas cervix case    for a  small  For the parameter under study in Figure    for ex inal assessment is rather small  but for smaller assessments  ad bc f  x     c x   d    the sensitivity value              on the other hand  a sensitivity  is found  If the assessment for the pa  rameter would have been       therefore  deviations would have had hardly any effect on the probability of interest  Based upon the concept of sensitivity value  we might con clude that the parameters for whose assessments the sensi tivity values are the largest  are the most likely to be quite influential upon variation and therefore should be selected for further investigation  Sensitivity values  however  pro vide insight in the effect of small deviations only from a parameter s assessment  When larger deviations are con sidered  the quality of the value  as  an approximation of the  sessment is therefore no guarantee that the probability of interest is hardly affected by variation of this parameter  From our discussion of the sensitivity function shown in Figure    we have that the quality of a sensitivity value for indicating the effect of parameter variation decreases as the parameter s original assessment lies closer to the x coordinate of the function s  Therefore  to de   shoulder    cide whether or not the probability of interest is likely to be affected by inaccuracies in the assessment for a specific parameter  we should consider not just the associated sen sitivity value but also the distance of the assessment to the x coordinate of the shoulder of the sensitivity function  We define the concept of shoulder more formally  A sensitivity function is either a linear function or a hyper bola  For  c       for example  the sensitivity function is  linear  as we then have that  f x   ax   b   ax   b cx d  d       d  x      d  If the sensitivity function is not linear  it takes the form of a hyperbola  f x       x       s    t   with  c               s    d     c  and t     a    c  For ease of reference  Figure   depicts the general shape of a hyperbola  The hyperbola has two asymptotes  parallel to the x  andy axes  in x              sand f x     t  respectively   The hyperbola further is symmetrical in the line that has       an absolute gradient of   and goes through the intersection  d     point  s  t  of the hyperbola s asymptotes  The point        II  is termed the        d      center   s  t   of the hyperbola  The point at which  the hyperbola intersects with the line in which it is symmet rical  is called the vertex of the hyperbola  this vertex is the       point referred to before as the function s shoulder  As the       symmetry line has a gradient with an absolute value of      the gradient of the hyperbola at the vertex also has an abso OL    L      L                                  xo  p Sono eTuiz   yes Meta   I CeTvix  no   Figure      Pr Stage   III I case    in terms of the pa   p Sono cervix  yes I Metas cervix   no    probability rameter x  The sensitivity function J x  expressing the  lute value ofl  Using this property we can easily determine the x coordinate x of the vertex     lf  x l     I  x  rs                   x   s   vfrT  We now return to a sensitivity function f x  expressing a        VAN DER GAAG   RENOOIJ  j x               t                                    UAI       tivity value can be used for selecting parameters that upon     variation have a large effect on a specific probability of in terest  Often  however  we are interested not in the effect of parameter variation on a probability of interest  but in the effect on the most likely value for a variable of interest  In a medical application  for example  the most likely diagno  vertex  sis given a patient s symptoms and signs may be the out come of interest  For this type of outcome  the derivative                                          center  of a sensitivity function and its associated sensitivity value are no longer appropriate for establishing a parameter s ef fect upon variation  For some parameters  deviation from their original assessment may have a considerable effect on  X  s  the probability of a specific outcome and yet not induce a Figure   Ahyperbolaf x    change in the most likely one  for other parameters  varia  network s probability of interest in terms of a parameter x  nonetheless result in a change in the most likely outcome   under study  As noted above  if this function is not linear  it  We would like to note that the idea of focusing on the most  takes the form of a hyperbola  The center of the hyperbola  likely value of a specific variable conforms to the practice  tion may have little effect on the probabilities involved and  is           Because a sensitivity function is defined on  of sensitivity analysis of decision theoretic models where        the entire probability interval        and moreover is non  the most preferred decision is the outcome of interest  negative  we have that        and           For the xc c    To provide for studying the effects of parameter variation  coordinate x of the vertex of the hyperbola  we find     x   in view of a most likely outcome  we enhance the basic method of sensitivity analysis for probabilistic networks   d  Jia d b  c l                      with the computation of an interval within which a param            c  eter can be varied without inducing a change in the most  For the sensitivity function shown in Figure    for example   the x coordinate of the vertex of the hyperbola is x We observe that the original assessment x                     for the  likely value of a variable of interest  Now  let A be the  variable of interest  Let x be the parameter under study and let xo be its assessment  The admissible    r  s   parameter under study lies quite close to this coordinate   is a pair of real numbers  We recall that the sensitivity value for x  is just       Be  between the bounds max xo  cause the assessment lies close to x  however  the sensitiv ity value cannot be considered a good approximation of the  effect of the parameter s variation  small deviations from the assessment  especially to smaller values  have a con siderable effect on the probability of interest  We conclude that this parameter should be selected for further investiga  deviation  for x   such that x  can be varied    r      and min   xo    s       without inducing a change in the most likely value for the  variable A   r  and s  moreover  are the largest numbers for  which this property holds  Note that the interval within which a parameter can be freely varied can be bounded for two reasons  either the most likely outcome changes  if the parameter is varied beyond the specified boundary   tion  regardless of its small sensitivity value  From the above observations  we have that for extracting  from the sensitivity data the parameters that deserve fur ther attention  parameters whose assessment has a sensi tivity value larger than   should be identified  In addition  parameters whose original assessment lies close to the  x  coordinate of the vertex of the associated sensitivity func tion should be selected  To conclude our discussion of sensitivity values  we would like to note that in the analysis of the oesophagus network we found rather strong sensitivities  An example is shown       in Figure    for the parameter under study  a sensitivity value of     was found      Admissible deviation  In the previous section  we focused on the derivative of a sensitivity function and showed how its associated sensi   Q UL              Figure  p X lung       MIMetas lungs  T he sensitivity function      f x            expressing the  Pr Stage   IVB I case    in terms of the pa p X lungs yes I Metas lungs   no   rameter x probability         VAN DER GAAG   RENOOIJ  UAI      or the boundary of the probability interval          has been       Metas lungs   no   on the probabilities of the various  reached  To express that a parameter can be varied as far as  different values of the variable  the boundary of the probability interval  we use the symbol  tient  From the figure  it is readily seen that the param  oo  eter s assessment has associated a small sensitivity value  in the admissible deviation associated with its assess  Stage  for a specific pa  ment   from each of the sensitivity functions  The sensitivity func  The admissible deviation for a parameter s assessment can  tion f v  x   for example  that expresses the probability Pr Stage   IVB I case      in terms of the parameter  be computed from the sensitivity functions are yielded for the various values  a    Pr a le   that  of the variable A   under study  is  f  More specifically  the admissible deviation is established  xo ai is the most likely value for      VB X  by studying the points at which two or more of these func     tions intersect  Suppose that for the original assessment  of the parameter x  the value  lf vs      I            tion  that neither of the bounds of the admissible deviation   r  s     oo   The admissible deviation for x  now is the pair  where the leftmost deviation  number for which x    r  r  is the smallest real  is the x coordinate of a point at   Pr a le  x  intersects with fPr a  e  x  with j     i  the  which the sensitivity function another sensitivity function rightmost deviation  s  We further observe that the assessment  x           for the  parameter lies relatively far from the x coordinate i  of the function s vertex   i   is defined analogously               yi       l l                          We present various examples to illustrate the difference be tween using sensitivity values and admissible deviations for          X           X           from which we find a sensitivity value of  the variable of interest  Also suppose  for ease of exposi equals        Based on the concept of sensitivity value as discussed in the  studying the effects of parameter variation  The examples  previous section  we conclude that the parameter does not  are taken from patient specific analyses of the oesophagus  deserve additional attention  Inspection of Figure   now  network  The figures used display the sensitivity functions  further reveals that the admissible deviation for the param  yielded for all the values of the variable  eter s assessment equals  Stage  that models  the stage of a patient s cancer  As we are now considering    oo  oo     which indicates that the  parameter can be varied over the entire probability interval  a variable of interest rather than a probability of interest           using the concept of sensitivity value for selecting interest  Based on the concept of admissible deviation  therefore   ing parameters amounts to examining the sensitivity values  the parameter should also be further disregarded   from all functions for the parameter under study   without inducing a change in the most likely stage   For our next example  we consider a parameter that induces  Our first example addresses a parameter that induces small  large sensitivity values  but upon variation is not expected  sensitivity values and upon variation does not result in a  to result in a change in the most likely outcome  The sensi  change in the most likely outcome  The sensitivity func  tivity functions for the parameter are depicted in Figure  tions for the parameter are depicted in Figure    the fig  the figure shows the effects of varying  ure shows the effects of varying  x   p  CT lungs   yes I  yes I Metas loco   no   x     p  CT loco        on the probabilities of the dif                 IIA                              Q         p CT Iung   yeIMetas lungs  no      The sensitivity functions expressing the prob Pr Stage I case      in terms of the parameter p CT lungs  yes I Metas lungs  no   Figure  abilities  p CT Ioco     li  IMetas loco  no      The sensitivity functions expressing the prob Pr Stage I case     in terms of the parameter p CT loco  yes I Metas loco  no   Figure  abilities        VAN DER GMG   RENOOIJ  Stage  For the parameter s       large sensitivity values  ferent values of the variable  original assessment x       are found from some of the six functions   UAI       computed to be         oo    As the leftmost deviation is  quite small compared to the assessment         we find that  For example   the parameter deserves further investigation  We would like  from the sensitivity function that expresses the probability  to note that  if the original assessment had been in the inter  Pr Stage     IIA  sitivity value of  I case     in terms of the parameter  a sen       is found   W hen studying the effects of  variation on the probabilities of interest  therefore  the pa rameter should be selected for further investigation  Now consider the effects of variation on the most likely value of the variable  Stage   For the parameter s assessment xo   we find that stage III is the most likely stage for the patient under study  The sensitivity function for this stage inter sects with the sensitivity function for stage IIA at x           Further inspection of the figure reveals that the admissible deviation for the parameter s assessment is   oo          We  observe that the rightmost deviation is relatively large com pared to the original assessment   We therefore conclude  that inaccuracies in the assessment are not reasonably ex pected to affect the most likely outcome  and the parameter should not be selected for further investigation   values but upon variation affects the most likely outcome  Figure   depicts the sensitivity functions for the parameter  x    p Wall inv   T  I Shape   polypoid         Length   lOcm   For the original assessment x        of the pa  rameter  the largest sensitivity values are found from the  lf IA Q     lf u              III   then the sensitivity values would not have  extremely small for both directions of variation  Our final example pertains to a parameter that induces large sensitivity values and upon variation is expected to result in a change in the most likely outcome   The sensitivity     the p  CT organs    functions for the parameter are displayed in Figure figure shows the effects of varying x  medias  llnv organs     none       From two of the sensitivity  functions  we find relatively large sensitivity values   lf  A       I lf n        I                   In addition  the original assessment x           for the pa  xm of the vertices of the two functions   XnA  i m                   Based upon the concept of sensitivity value  therefore  the parameter should be selected for further investigation  We now consider the effects of variation on the most likely out come  We observe that for the original assessment the most                   likely stage is IIA  The admissible deviation for the assess ment is computed to be  As the sensitivity functions are linear in the parameter under study  the computed sensitivity values describe the effects of variation exactly                  changed  but the admissible deviation would have become  rameter lies very close indeed to the x coordinates  i nA and  We now address a parameter that induces small sensitivity  functions for the stages IIA and  val  Based upon the concept of  sensitivity value  we therefore conclude that the parame ter should not be selected for further investigation  Now  the admissible deviation for the parameter s assessment is          oo    We observe that the  leftmost deviation is extremely small  indicating that an in accuracy by just        in the original assessment will lead Stage and  to a different most likely value for the variable  may in fact result in a different treatment decision for the patient under consideration  The above examples illustrate that admissible deviations                     IIA               j iiA            j ill           p Wallinv    T ISI ape  polypoid              Length       Figure    The sensitivity functions expressing the prob  Pr Stage I case      in terms of the parameter p Wall inv   T  Shape  polypoid        Length    cm   abilities  p CT organl    m edia  tllnv organ s  none      The sensitivity functions expressing the prob Pr Stage I case     in terms of the parameter p CT organs  medias   Inv organs  none   Figure  abilities   UAI       VAN DER GAAG   RENOOIJ  provide additional insight into the sensitivity of a proba bilistic network s outcome to inaccuracies in its parame ters  We would like to note that the use of admissible de viations is especially of interest when decisions are to be based on the most likely value of a variable of interest             Acknowledgements  This research has been  partly  supported by the Netherlands Computer Science Research Foundation with financial support from the Netherlands Organisation for Scien tific Research  NWO   We are most grateful to Babs Taal and Berthe Aleman from the Netherlands Cancer Institute  Antoni van Leeuwenhoekhuis  who spent much time and effort in the con  struction of the oesophagus network      Conclusions  
