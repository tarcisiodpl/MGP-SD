  We describe a graphical representation of probabilistic relationships an alternative to the Bayesian network called a dependency network  Like a Bayesian network  a depen dency network has a graph and a probabil ity component  The graph component is a  cyclic  directed graph such that a node s parents render that node independent of all other nodes in the network  The probabil ity component consists of the probability of a node given its parents for each node  as in a Bayesian network    We identify several ba sic properties of this representation  and de scribe its use in collaborative filtering  the task of predicting preferences  and the visu alization of predictive relationships  Keywords  Dependency networks  graphical models  inference  data visualization  exploratory data analy sis  collaborative filtering  Gibbs sampling    Introduction  The Bayesian network has proven to be a valuable tool for encoding  learning  and reasoning about probabilis tic relationships  In this paper  we introduce another graphical representation of such relationships called a dependency network  The representation can be thought of as a collection of regression classification models among variables in a domain that can be com bined using Gibbs sampling to define a joint distribu tion for that domain  The dependency network has several advantages and disadvantages with respect to the Bayesian network  For example  a dependency net work is not useful for encoding causal relationships and is difficult to construct using a knowledge based ap proach  Nonetheless  in our three years of experience with this representation  we have found it to be easy to  learn from data and quite useful for encoding and dis playing predictive  i e   dependence and independence  relationships  In addition  we have empirically verified that dependency networks are well suited to the task of predicting preferences a task often referred to as col laborative filtering  Finally  the representation shows promise for density estimation and probabilistic infer ence  The representation was conceived independently by Hofmann and Tresp         who used it for density es timation  and Hofmann        investigated several of its theoretical properties  In this paper  we summarize their work  further investigate theoretical properties of the representation  and examine its use for collabora tive filtering and data visualization  In Section    we define the representation and describe several of its basic properties  In Section    we de scribe algorithms for learning a dependency network from data  concentrating on the case where the local distributions of a dependency network  similar to the local distributions of a Bayesian network  are encoded using decision trees  In Section    we describe the task of collaborative filtering and present an empirical study showing that dependency networks are almost as accurate as and computationally more attractive than Bayesian networks on this task  Finally  in Sec tion    we show how dependency networks are ideally suited to the task of visualizing predictive relationships learned from data     Dependency Networks  To describe dependency networks and how we learn them  we need some notation  We denote a variable by a capitalized token  e g   X  X      Age   and the state or value of a corresponding variable by that same token in lower case  e g   x  x      age   We denote a set of variables by a bold face capitalized token  e g   X  X   Pa    We use a corresponding bold face lower case token  e g   x  x   pa   to denote an assignment of   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       state or value to each variable in a given set  We use p X xiY   y   or p xiy  as a shorthand  to denote the probability that X   x given Y   y  We also use p  x iy  to denote the probability distribution for X given Y  both mass functions and density functions   Whether p xiy  refers to a probability  a probability density  or a probability distribution will be clear from context     Consider a domain of interest having variables X    X          Xn     A dependency network for X is a pair     P  where q is a  cyclic  directed graph and P is a set of probability distributions  Each node in q corre sponds to a variable in X  We use X  to refer to both the variable and its corresponding node  The parents of node X   denoted Pa   correspond to those variables Pa  that satisfy     The distributions in P are the local probability distributions p  x  j p a     i              n  We do not require the distributions p   x  x       Xi   Xi         xn    i             n to be obtainable  via inference  from a sin gle joint distribution p x   If they are  we say that the dependency network is consistent with p  x   We shall say more about the issue of consistency later in this section   l  A Bayesian network for X defines a joint distribution for X via the product of its local distributions  A dependency network for X also defines a joint distri bution for X  but in a more complicated way via a Gibbs sampler  e g   Gilks  Richardson  and Spiegel halter         In this Gibbs sampler  we initial ize each variable to some arbitrary value  We then repeatedly cycle through each variable X          Xn  in this order  and resample each X  according to p x ix       Xi   Xi         Xn    p  x   lp a     We call this procedure an ordered Gibbs sampler  As described by the following theorem  also proved in Hofmann         this ordered Gibbs sampler defines a joint dis tribution for X  Theorem    An ordered Gibbs sampler applied to a dependency network for X  where each X  is discrete and each local distribution p   x  IPa   is positive  has a unique stationary joint distribution for X   xt  tth  Proof  Let be the sample of x after the iteration of the ordered Gibbs sampler  The sequence x   x         can be viewed as samples drawn from a homogenous Markov chain with transition matrix M having ele ments Mjli   p   xt       i    We use the termi nology of Feller         It is not difficult to see that M is the product M  Mn  where Mk is the  lo cal  transition matrix describing the resampling of Xk  jlxt                    according to the local distribution p xk IPak  The pos itivity of local distributions guarantees the positivity of M  which in turn guarantees     the irreducibility of the Markov chain and     that all of the states are ergodic  Consequently  there exists a unique joint dis tribution that is stationary with respect to M    Because the Markov chain described in the proof is irreducible and ergodic  after a sufficient number of iterations  the samples in the chain will be drawn from the stationary distribution for X  Consequently  these samples can be used to estimate this distribution  Note that the Theorem holds for both consistent and inconsistent dependency networks  Furthermore  the restriction to discrete variables can be relaxed  but will not be discussed here  In the remainder of this paper  we assume all variables are discrete and each local distribution is positive  In addition to determining a joint distribution  a de pendency network for a given domain can be used to compute any conditional distribution of interest that is  perform probabilistic inference  We discuss an algorithm for doing so  which uses Gibbs sampling  in Heckerman  Chickering  Meek  Rounthwaite  and Kadie         That Gibbs sampling is used for in ference may appear to be a disadvantage of depen dency networks with respect to Bayesian networks  When we learn a Bayesian network from data  how ever  the resulting structures are typically complex and not amenable to exact inference  In such situations  Gibbs sampling  or even more complicated Monte Carlo techniques  are used for inference in Bayesian networks  thus weakening this potential advantage  In fact  when we have data and can learn a model for X  dependency networks have an advantage over Bayesian networks  Namely  we can learn each local distribution in a dependency network independently  without regard to acyclicity constraints  Bayesian networks have one clear advantage over de pendency networks  In particular  dependency net works are not suitable for the representation of causal relationships  For example  if X causes Y  so that X and Y are dependent   the corresponding depen dency network is X     Y  that is  X is a parent of Y and vice versa  It follows that dependency networks are difficult to elicit directly from experts  Without an underlying causal interpretation  knowledge based elicitation is cumbersome at best  Another important observation about dependency net works is that  when we learn one from data as we have described learning each local distribution independently the model is likely to be inconsistent   In an extreme case  where     the true joint distribu    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            tion lies in one of the possible models      the model search procedure finds the true model  and     we have essentially an infinite amount of data  the learned model will be consistent   A simple approach to avoid this difficulty is to learn a Bayesian network and apply inference to that network to construct the dependency network  This approach  however  will eliminate the advantage associated with learning dependency net works just described  is likely to be computationally inefficient  and may produce extremely complex local distributions  When ordered Gibbs sampling is applied to an inconsistent dependency network  it is important to note that the joint distribution so defined will de pend on the order in which the Gibbs sampler visits the variables  For example  consider the inconsistent dependency network X    Y  If we draw sample pairs  x  y  that is  x and then y then the resulting sta tionary distribution will have X and Y independent  In contrast  if we draw sample pairs  y  x   then the resulting stationary distribution may have X and Y dependent  The fact that we obtain a joint distribution from any dependency network  consistent or not  is comforting  A more important question  however  is what distri bution do we get  The following theorem  proved in Heckerman et al          provides a partial answer  Theorem    If a dependency network for X is con sistent with a positive distribution then the sta tionary distribution defined in Theorem   is equal to  p x    p x    When a dependency network is inconsistent  the situa tion is even more interesting  If we start with learned local distributions that are only slight perturbations  in some sense  of the true local distributions  will Gibbs sampling produce a joint distribution that is a slight perturbation of the true joint distribution  Hof mann        argues that  for discrete dependency net works with positive local distributions  the answer to this question is yes when perturbations are measured with an L  norm  In addition  Heckerman et al         show empirically using several real datasets that the joint distributions defined by a Bayesian network and dependency network  both learned from data  are sim ilar  We close this section with several facts about consis tent dependency networks  proved in Heckerman et al          We say that a dependency network for X is bi directional if X  is a parent of Xj if and only if Xj is a parent of X    for all X  and Xj in X  We say that a distribution is consistent with a dependency net work structure if there exists a consistent dependency network with that structure that defines  p x   p x    Theorem    The set of positive distributions consis tent with a dependency network structure is equal to the set of positive distributions defined by a Markov network structure with the same adjacencies  Note that  although dependency networks and Markov networks define the same set of distributions  their rep resentations are quite different  In particular  the de pendency network includes a collection of conditional distributions  whereas the Markov network includes a collection of joint potentials  Let pa  be the lh parent of node X    A consistent de pendency network is minimal if and only if  for every node X  and for every parent pa    X  is not indepen dent of pa  given the remaining parents of X    Theorem    A minimal consistent dependency net work for a positive distribution must be bi directional   p x      Learning Dependency Networks  In this section  we mention a few important points about learning dependency networks from data  When learning a dependency network for X  each local distribution for X  is simply a regression classification model  with feature selection  for x  with X   xi  as inputs  If we assume that each local distribution has a parametric model p  x   Pa   B    and ignore the de pendencies among the parameter sets Bt         n  then we can learn each local distribution independently us ing any regression classification technique for mod els such as a generalized linear model  a neural net work  a support vector machine  or an embedded re gression classification model  Heckerman and Meek         From this perspective  the dependency network can be thought of as a mechanism for combining re gression classification models via Gibbs sampling to determine a joint distribution  In the work described in this paper  we use decision trees for the local distributions  A good discussion of methods for learning decision trees is given in Breiman  Friedman  Olshen  and Stone         We learn a deci sion tree using a simple hill climbing approach in con junction with a Bayesian score as described in Fried man and Goldszmdit        and Chickering  Hecker man  and Meek         To learn a decision tree for X    we initialize the search algorithm with a single ton root node having no children  Then  we replace each leaf node in the tree with a binary split on some variable Xj in X  X    until no such replacement in creases the score of the tree  Our binary split on Xj is        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       a decision tree node with two children  one of the chil dren corresponds to a particular value of Xj  and the other child corresponds to all other values of Xj  Our Bayesian scoring function uses a uniform prior distri bution for all decision tree parameters  and a structure prior proportional to KJ  where K     is a tunable pa rameter and f is the number of free parameters in the decision tree  In studies that predated those described in this paper  we have found that the setting K        yields accurate models over a wide variety of datasets  We use this same setting in our experiments  For comparison in these experiments  we also learn Bayesian networks with decision trees for local distri butions using the algorithm described in Chickering  Heckerman  and Meek         When learning these networks  we use the same parameter and structure priors used for dependency networks  We conclude this section by noting an interesting fact about the decision tree representation of local distri butions  Namely  there will be a split on variable X in the decision tree for Y if and only if there is an arc from X to Y in the dependency network that in cludes these variables  As we shall see in Section    this correspondence helps the visualization of data     Collaborative Filtering  In the remainder of this paper  we consider useful ap plications of dependency networks  whether they be consistent or not  The first application is collaborative filtering   CF   the task of predicting preferences  Examples of this task include predicting what movies a person will like based on his or her ratings of movies seen  predicting what new stories a person is interested in based on other stories he or she has read  and predicting what web pages a person will go to next based on his or her history on the site  Another important application in the burgeoning area of e commerce is predicting what products a person will buy based on products he or she has already purchased and or dropped into his or her shopping basket  Collaborative filtering was introduced by Resnick  la covou  Suchak  Bergstrom  and Riedl        as both the task of predicting preferences and a class of al gorithms for this task  The class of algorithms they described was based on the informal mechanisms peo ple use to understand their own preferences  For ex ample  when we want to find a good movie  we talk to other people that have similar tastes and ask them what they like that we haven t seen  The type of algo rithm introduced by Resnik et al          sometimes called a memory based algorithm  does something simi   lar  Given a user s preferences on a series of items  the algorithm finds similar users in a database of stored preferences  It then returns some weighted average of preferences among these users on items not yet rated by the original user  As done in Breese  Heckerman  and Kadie         let us concentrate on the application of collaborative filtering that is  preference prediction  In their pa per  Breese et al         describe several CF sce narios  including binary versus non binary preferences and implicit versus explicit voting  An example of explicit voting would be movie ratings provided by a user  An example of implicit voting would be know ing only whether a person has or has not purchased a product  Here  we concentrate on one scenario im portant for e commerce  implicit voting with binary preferences for example  the task of predicting what products a person will buy  knowing only what other products they have purchased  A simple approach to this task  described in Breese et al          is as follows  For each item  e g   prod uct   define a variable with two states corresponding to whether or not that item was preferred  e g   pur chased   We shall use     and     to denote not preferred and preferred  respectively  Next  use the dataset of ratings to learn a Bayesian network for the joint distribution of these variables X   X        Xn  The preferences of each user constitutes a case in the learning procedure  Once the Bayesian network is constructed  make predictions as follows  Given a new user s preferences x  use the Bayesian network to determine p Xi    lx   Xi      for each prod uct Xi not purchased  That is  infer the probability that the user would have purchased the item had we not known he did not  Then  return a list of recom mended products among those that the user did not purchase ranked by this probability  Breese et al         show that this approach out performs memory based and cluster based methods on several implicit rating datasets  Specifically  the Bayesian network approach was more accurate and yielded faster predictions than did the other methods  What is most interesting about this algorithm in the context of this paper is that only the probabilities p X     lx  X       are needed to produce the recom mendations  In particular  these probabilities may be obtained by a direct lookup in a dependency network  p Xi      lx  X          p Xi      lpa          where pai is the instance of Pai consistent with X  Thus  dependency networks are a natural model on which to base CF predictions  In the remainder of this section  we compare this approach with that based   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            Table    Number of users  items  and items per user for the datasets used in evaluating the algorithms  Users in tra mmg set Users in test set  Total items Mean items per user in training set  Dataset MS COM Nielsen                                              MSNHC                           on Bayesian networks for datasets containing binary implicit ratings  Datasets       We evaluated Bayesian networks and dependency net works on three datasets      Nielsen  which records whether or not users watched five or more minutes of network TV shows aired during a two week period in       made available courtesy of Nielsen Media Re search       MS  COM  which records whether or not users of microsoft com on one day in      visited ar eas  vroots    of the site  available on the Irvine Data Mining Repository   and     MSNBC  which records whether or not visitors to MSNBC on one day in      read stories among the most popular      stories on the site  The MSNBC dataset contains        users sampled at random from the approximate         users that visited the site that day  In a separate anal ysis on this dataset  we found that the inclusion of ad ditional users did not produce a substantial increase in accuracy  Table     provides additional information about each dataset  All datasets were partitioned into training and test sets at random           Evaluation Criteria and Experimental Procedure  We have found the following three criteria for collab orative filtering to be important      the accuracy of the recommendations      prediction time the time it takes to create a recommendation list given what is known about a user  and     the computational re sources needed to build the prediction models  We measure each of these criteria in our empirical com parison  In the remainder of this section  we describe our evaluation criterion for accuracy  Our criterion attempts to measure a user s expected utility for a list of recommendations  Of course  dif ferent users will have different utility functions  The measure we introduce provides what we believe to be a good approximation across many users  The scenario we imagine is one where a user is shown  a ranked list of items and then scans that list for pre ferred items starting from the top  At some point  the denote user will stop looking at more items  Let the probability that a user will examine the kth item on a recommendation list before stopping his or her scan  where the first position is given by k     Then  a reasonable criterion is  p k   cfaccuracyt  list    LP k  c k k       where c k is   if the item at position k is preferred and   otherwise  To make this measure concrete  we assume that p k  is an exponentially decaying function   p k   Tk a       where a is the  half life  position the position at which an item will be seen with probability      In our experiments  we use a     In one possible implementation of this approach  we could show recommendations to a series of users and ask them to rate them as  preferred  or  not preferred    We could then use the average of cfaccuarcy   list  over all users as our criterion  Be cause this method is extremely costly  we instead use an approach that uses only the data we have  In par ticular  as already described  we randomly partition a dataset into a training set and a test set  Each case in the test set is then processed as follows  First  we randomly partition the user s preferred items into in put and measurement sets  The input set is fed to the CF model  which in turn outputs a list of recommen dations  Finally  we compute our criterion as N           Lk cS k   cfaccuracy hst    K   N L    i l L k      p k  p k        where N is the number of users in the test set  K  is the number of preferred items in the measurement set for user i  and c  k is   if the kth item in the recommen dation list for user i is preferred in the measurement set and   otherwise  The denominator in Equation   is a per user normalization factor  It is the utility of a list where all preferred items are at the top  This nor malization allows us to more sensibly combine scores across measurement sets of different size  We performed several experiments reflecting differing numbers of ratings available to the CF engines  In the first protocol  we included all but one of the preferred items in the input set  We term this protocol all but    In additional experiments  we placed       and    pre ferred items in the input sets  We call these protocols given    given    and given     The all but   experiments measure the algorithms  per formance when given as much data as possible from   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       each test user  The various given experiments look at users with less data available  and examine the perfor mance of the algorithms when there is relatively little known about an active user  When running the given m protocols  if an input set for a given user had less than m preferred items  the case was eliminated from the evaluation  Thus the number of trials evaluated under each protocol varied   Table    CF accuracy for the MS COM  Nielsen  and MSNBC datasets  Higher scores indicate better per formance  Statistically significant winners are shown in boldface  MS COM  Algorithm BN DN  All experiments were performed on a     MHz Pen tium II with     MB of memory  running the NT     operating system            Given                Tables   and   compare the two methods with the remaining criteria  Here  dependency networks are a clear winner  They are significantly faster at prediction sometimes by almost an order of magnitude and require substantially less time and memory to learn   AllBut                                         Baseline                              Given   Given                AllBut          GivenS              RD                            Baseline                              Nielsen  Algorithm BN DN  Table   shows the accuracy of recommendations for Bayesian networks and dependency networks across the various protocols and three datasets  For a com parison  we also measured the accuracy of recommen dation lists produced by sorting items on their overall popularity  p X        The accuracy of this approach is shown in the row labeled  Baseline   A score in boldface corresponds to a significantly significant win ner  We use ANOVA  e g   McClave and Dietrich        with a       to test for statistical significance  When the difference between two scores in the same column exceed the value of RD  required difference   the difference is significant   The magnitudes of accuracy differences  however  are not that large  In particular  the ratio of   cfac curacy BN    cfaccuracy DN   to  cfaccuracy BN  cfaccuracy Baseline   averages    S percent across the datasets and protocols   Given                RD  Results  From the table  we see that Bayesian networks are more accurate than dependency networks  This re sult is interesting  because there are reasons to ex pect that dependency networks will be more accurate than Bayesian networks and vice versa  On the one hand  the search process that learns Bayesian net works is constrained by acyclicity  suggesting that de pendency networks may be more accurate  On the other hand  the conditional probabilities used to sort the recommendations are inferred from the Bayesian network  but learned directly in the dependency net work  Therefore  dependency networks may be less accurate  because they waste data in the process of learning what could otherwise be inferred  For this or perhaps other reasons  the Bayesian networks are more accurate   GivenS                                   MSNBC  Algorithm BN DN  Given   GivenS                          S   Given                RD                             Baseline                              AllBut             S  Overall  Bayesian networks are slightly more accurate but much less attractive from a computational per spective     Data Visualization  Bayesian networks are well known to be useful for visualizing causal relationships  In many circum stances  however  analysts are only interested in predictive that is  dependency and independency relationships  In our experience  the directed arc se mantics of Bayesian networks interfere with the visu alization of such relationships  As a simple example  consider the Bayesian network X     Y  Those familiar with the semantics of Bayesian networks immediately recognize that observ ing Y helps to predict X  Unfortunately  the untrained individual will not  In our experience  this person will interpret this network to mean that only X helps to predict Y  and not vice versa  Even people who are expert in d separation semantics will sometimes have difficulties visualizing predictive relationships using a Bayesian network  The cognitive act of identifying a node s Markov blanket seems to interfere with the vi sualization experience  Dependency networks are a natural remedy to this   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            problem  If there is no arc from X to Y in a depen dency network  we know immediately that X does not help to predict Y   Table    Number of predictions per second for the MS COM  Nielsen  and MSNBC datasets  MS COM  Algorithm BN DN  Given              Given              Algorithm BN DN  Given               Given               Algorithm BN DN  Given              Given              Given               AllBut              Nielsen  Given                AllBut               MSNBC  Given              AllBut              Table    Computational resources for model learning  MS COM  Algorithm BN DN  Memory  Meg            Algorithm BN DN  Memory  Meg           Algorithm BN DN  Memory  Meg            Learn Time  sec                Nielsen  Learn Time  sec             MSNBC  Learn Time  sec                Figure   shows a dependency network learned from a dataset obtained from Media Metrix  The dataset contains demographic and internet use data for about       individuals during the month of January       On first inspection of this network  an interest ing observation becomes apparent  there are many  predictive  dependencies among demographics  and many dependencies among frequency of use  but there are few dependencies between demographics and frequency of use  Over the last three years  we have found numerous interesting dependency relationships across a wide va riety of datasets using dependency networks for visu alization  In fact  we have given dependency networks this name because they have been so useful in this regard  The network in Figure   is displayed in DNViewer  a dependency network visualization tool developed at Microsoft Research  The tool allows a user to display both the dependency network structure and the de cision tree associated with each variable  Navigation between the views is straightforward  To view a de cision tree for a variable  a user simply double clicks on the corresponding node in the dependency network  Figure   shows the tree for Shopping Freq  An inconsistent dependency net learned from data of fers an additional advantage for visualization  If there is an arc from X to Y in such a network  we know that X is a significant predictor of Y  significant in what ever sense was used to learn the network  Under this interpretation  a uni directional link between X and Y is not confusing  but rather informative  For example  in Figure    we see that Sex is a significant predictor of Socioeconomic status  but not vice versa an in teresting observation  Of course  when making such interpretations  one must always be careful to recog nize that statements of the form  X helps to predict Y  are made in the context of the other variables in the network  In DNViewer  we enhance the ability of dependency networks to reflect strength of dependency by includ ing a slider  on the left   As a user moves the slider from bottom to top  arcs are added to the graph in the order in which arcs are added to the dependency net work during the learning process  When the slider is in its upper most position  all arcs  i e   all significant dependencies  are shown    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            Figure    A dependency network for Media Metrix data  The dataset contains demographic and internet use data for about       individuals during the month of January       The node labeled Overall Freq represents the overall frequency of use of the internet during this period  The nodes Search Freq  Edu Freq  and so on represent frequency of use for various subsets of the internet       r          J                   C l                 i  J                NoM                      ahw                 other       ott  r          Low          Figure    The decision tree for Shopping Freq obtained by double clicking that node in the dependency network  The histograms at the leaves correspond to probabilities of Shopping Freq use being zero  one  and greater than one visit per month  respectively    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            r  I I  I  I s  irko  Figure    The dependency network in Figure   with the slider set at half position  Figure   shows the dependency network for the Media Metrix data with the slider at half position  At this setting  we find the interesting observation that the dependencies between Sex and XXX Freq  frequency of hits to pornographic pages  are the strongest among all dependencies between demographics and internet use     Summary and Future Work  We have described a new representation for probabilis tic dependencies called a dependency network  We have shown that a dependency network  consistent or not  defines a joint distribution for its variables  and that models in this class are easy to learn from data  In particular  we have shown how a dependency network can be thought of as a collection of regres sion classification models among variables in a domain that can be combined using Gibbs sampling to define a joint distribution for the domain  In addition  we have shown that this representation is useful for col laborative filtering and the visualization of predictive relationships  Of course  this research is far from complete  There are many questions left to be answered  For example   what are useful models  e g   generalized linear models  neural networks  support vector machines  or embed ded regression classification models  for a dependency network s local distributions  Another example of par ticular theoretical interest is Hofmann s        result that small    norm perturbations in the local distribu tions lead to small    norm perturbations in the joint distributions defined by the dependency network  Can this result be extended to norms more appropriate for probabilities such as cross entropy  Finally  the dependency network and Bayesian net work can be viewed as two extremes of a spectrum  The dependency network is ideal for situations where the conditionals p x lx   x   are needed  In con trast  when we require the joint probabilities p x   the Bayesian network is ideal because these probabil ities may be obtained simply by multiplying condi tional probabilities found in the local distributions of the variables  In situations where we need probabili ties of the form p ylx  y   where Y is a proper subset of the domain X  we can build a network structure that enforces an acyclicity constraint among only the variables Y  In so doing  the conditional probabilities p ylx  y  can be obtained by multiplication    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Acknowledgments  We thank Reimar Hofmann for useful discussions  Datasets for this paper were generously provided by Media Metrix  Nielsen Media Research  Nielsen   Mi crosoft Corporation  MS COM   and Steven White and Microsoft Corporation  MSNBC   
