 Qualitative possibilistic networks  also known as min based possibilistic networks  are important tools for handling uncertain information in the possibility theory framework  Despite their importance  only the junction tree adaptation has been proposed for exact reasoning with such networks  This paper explores alternative algorithms using compilation techniques  We first propose possibilistic adaptations of standard compilation based probabilistic methods  Then  we develop a new  purely possibilistic  method based on the transformation of the initial network into a possibilistic base  A comparative study shows that this latter performs better than the possibilistic adaptations of probabilistic methods  This result is also confirmed by experimental results      INTRODUCTION  In possibility theory there are two different ways to define the counterpart of Bayesian networks  This is due to the existence of two definitions of possibilistic conditioning  product based and min based conditioning  Dubois and Prade         When we use the product form of conditioning  we get a possibilistic network close to the probabilistic one sharing the same features and having the same theoretical and practical results  However  this is not the case with min based networks  In this paper  we are interested in the inference problem in multiply connected networks  which is known as a hard problem  Cooper         More precisely  we propose three compilation methods for min based possibilistic networks  The compilation of Bayesian networks is always considered as an important area  Recently  researchers  Salem Benferhat CRIL CNRS University of Artois France        benferhat cril univ artois fr  Rolf Haenni RISIS Bern University Switzerland  CH      rolf haenni bfh ch  have been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques  Darwiche         Chavira and Darwiche         Wachter and Haenni         etc  Despite the importance of possibility theory  there is no compilation that has been proposed for possibilistic networks  This paper analyzes this issue by first adapting well known compilation based probabilistic inference approaches  namely the arithmetic circuit method  Darwiche        and the logical compilation of Bayesian Networks  Wachter and Haenni         Both of them are based on a networks encoding into a logical representation and a compilation into a target compilation language  namely  DNNF  From there  all possible queries are answered in polynomial time  The third method exploits results obtained on one hand in  Benferhat et al         that transforms a minbased possibilistic network into a possibilistic knowledge base  and on the other hand results obtained regarding compilation of possibilistic bases  Benferhat et al         in order to assure inference in polytime  This method that is purely possibilistic is flexible since it permits to exploit efficiently all the existing propositional compilers  The rest of this paper is organized as follows  Section   gives a briefly background on possibility theory  possibilistic logic  possibilistic networks and introduces some compilation concepts  Section   is dedicated to possibilistic adaptations of compilation based probabilistic inference methods  Section   presents a new inference method in possibilistic networks using compiled possibilistic knowledge bases  Experimental study is presented in Section            BASIC CONCEPTS POSSIBILITY THEORY  This subsection briefly recalls some elements of possibility theory  for more details we refer to  Dubois and Prade         Let V    X    X         XN   be a set of   variables  We denote by DXi    x        xn   the domain associated with the variable Xi   By xi we denote any instance of Xi    denotes the universe of discourse  which is the Cartesian product of all variable domains in V   Each element    is called a state of   The notion of possibility distribution denoted by  is a mapping from the universe of discourse to the unit interval         To this scale  two interpretations can be attributed  a quantitative one when values have a real sense and a qualitative one when values reflect only an order between the different states of the world  This paper focuses on the qualitative interpretation of possibility theory  Given a possibility distribution   we can define a mapping grading the possibility measure of an event    by      max      has a dual measure which is the necessity measure N             Conditioning consists in modifying our initial knowledge  encoded by a possibility distribution   by the arrival of a new certain piece of information     The qualitative interpretation of the scale        leads to the well known definition of min conditioning  Hisdal          Dubois and Prade                if                         otherwise      POSSIBILISTIC LOGIC  Possibilistic logic  Dubois et al         handles qualitative uncertainty in a logical setting  A possibilistic logic formula is a pair  p  a  where p is a propositional formula and a its uncertainty degree which estimates to what extent it is certain that p is true  The higher is the weight  the more certain is the formula  A possibilistic knowledge base  is made up of a finite set of weighted formulas  i e        pi   ai    i          n        where ai is the lower bound on N  pi    Each possibilistic knowledge base induces a unique possibility distribution such that     and   pi   ai          if     pi              max  ai      pi   otherwise where    is propositional logic entailment       POSSIBILISTIC NETWORKS  A min based possibilistic network over a set of variables V   denoted by Gmin is composed of    a graphical component that is a DAG  Directed  Acyclic Graph  where nodes represent variables and edges encode the links between the variables  The parent set of a node Xi is denoted by Ui    Ui    Ui         Uim    For any ui of Ui we have ui    ui    ui         uim   where m is the number of parents of Xi   In what follows  we use xi   ui   uij to denote  respectively  possible instances of Xi   Ui and Uij     a numerical component that quantifies different links  For every root node Xi  Ui      uncertainty is represented by the a priori possibility degree  xi   of each instance xi  DXi   such that maxxi  xi        For the rest of the nodes  Ui      uncertainty is represented by the conditional possibility degree  xi  ui   of each instances xi  DXi and ui  DUi   These conditional distributions satisfy the following normalization condition  maxxi  xi  ui        for any ui   The set of a priori and conditional possibility degrees in a min based possibilistic network induce a unique joint possibility distribution defined by the following chain rule  min  X        XN     min  i    N        Xi   Ui         COMPILATION CONCEPTS  A target compilation language is a class of formulas which is tractable for a set of transformations and queries  Compilation languages are compared in terms of their spatial efficiency via the succinctness criteria and also in terms of the set of logical queries and transformations they support in polynomial time  see  Darwiche and Marquis        for more details   Within the most effective target compilation languages  we cite the Decomposable Negation Normal Form  DNNF   Darwiche         This language is universal and presents a number of properties  determinism  smoothness  etc   that makes it of a great interest  It supports a rich set of polynomial time logical operations  To define DNNF  the starting point is Negation Normal Form  NNF  which is a set of propositional formulas where possible connectives are conjunctions  disjunctions and negations  A set of important properties may be imposed to NNF  such that    Decomposability  the conjuncts of any conjunction in NNF do not share variables    Determinism  two disjuncts of any disjunction in NNF are logically contradictory    Smoothness  the disjunct of any disjunction in NNF mentions the same variables  These properties lead to a number of interesting subsets of NNF  Within these subsets  the language DNNF  Darwiche        is one of the most effective target compilation languages that supports the decomposability  We can also mention  the d DNNF sat    isfying determinism  sd DNNF satisfying smoothness and determinism  etc  Each compilation language supports some queries and transformations in polynomial time  In what follows we are in particular interested by conditioning and forgetting transformations  Darwiche and Marquis           and  as max and min operators  respectively   A sentence in  sd DNNF is a sentence in  DNNF satisfying decomposability  determinism and smoothness      In  Darwiche         authors have focused on inference in compiled Bayesian networks  The main idea is based on representing the network using a polynomial and then retrieving answers to probabilistic queries by evaluating and differentiating the polynomial  This latter itself is exponential in size  so it has been represented efficiently using an arithmetic circuit that can be evaluated and differentiated in time and space linear in the circuit size  In what follows  we propose a direct adaptation of this method in the possibilistic setting  Given a min based possibilistic network  we first encode it using a possibilistic function fmin defined by two types of variables   POSSIBILISTIC ADAPTATIONS OF COMPILATION BASED PROBABILISTIC INFERENCE METHODS  There are several compilation methods which handle the inference problem in probabilistic graphical models  In this section  we first propose an adaptation of the arithmetic circuit method of  Darwiche         Then we will study one of its variants proposed in  Wachter and Haenni         namely the logical compilation of Bayesian Networks  DNNF has been introduced for propositional language  Recall that in qualitative possibility theory  we basically manipulate two main operators Max and Min  These operators fully make sense when we deal with qualitative plausibility ordering  Therefore  we propose to define concepts of  DNNF  resp   d DNNF   sd DNNF  as adaptations of the DNNF language  resp  d DNNF  sd DNNF   Darwiche        in the possibilistic setting  definition     Definition    A sentence in  DNNF is a rooted DAG where each leaf node is labeled with true  false or variables instances and each internal node is labeled with max or min operators and can have arbitrarily several children  Roughly speaking   DNNF is the same as the classical DNNF although its operators are max and min instead of  and   respectively  Example    Figure   depicts a sentence in  DNNF  Consider the Min node  root  in this figure  This node has two children  the first contains variables A  B while the second contains variables C  D  This node is decomposable since its two children do not share variables        INFERENCE USING POSSIBILISTIC CIRCUITS   Evidence indicators  for each variable Xi in the network   we have a variable xi for each instance xi  DXi    Network parameters  for each variable Xi and its parents Ui in the network  we have a variable xi  ui for each instance xi  DXi and ui  DUi   fmin   max x  min  xi  ui  x  xi xi  ui       where x represents instantiations of all network variables and ui  x denotes the compatibility relationship among ui and x  The possibilistic function fmin of a possibilistic network represents the possibility distribution and allows to compute possibility degrees of variables of interest  Namely  for any piece of evidence e which is an instantiation of some variables E in the network  we can instantiate fmin as it returns the possibility of e   e   Definition   and Proposition     Definition    The value of the possibilistic function fmin at evidence e  denoted by fmin  e   is the result of replacing each evidence indicator xi in fmin with   if xi is consistent with e  and with   otherwise  Proposition    Let Gmin be a possibilistic network representing the possibility distribution  and having the possibilistic function fmin   For any evidence e  we have fmin  e     e    Figure    A sentence in  DNNF   A sentence in  d DNNF is a sentence in  DNNF satisfying decomposability and determinism  viewing  Let figure   be the min based possibilistic network used throughout the paper  The possibilistic function of the network in figure   has   terms corresponding to the   instantiations of variables F  B  D  Two of these terms are as follows    is outlined by algorithm    Note that the suffix P F is added to signify that this method uses a possibilistic function  fmin   before ensuring the CNF encoding  Algorithm    Inference using  DNNF   DNNFP F    Figure    Example of Gmin    fmin   max min d    f    b    d   f   b    f    b     min  d    f    b    d   f   b    f    b          If the evidence e    d    b    then fmin  d    b    is obtained by applying the following substitutions to fmin   d       d       b       b       f    f       This leads to  e         The possibilistic function fmin is then encoded on a propositional theory  CNF  using xi and xi  ui   For each network variable Xi   the encoding contains the following clauses  xi  xj     xi  xj   i    j       Moreover  for each propositional variable xi  ui   the encoding contains the clause  xi  ui          uim  xi  ui       The CNF encoding  denoted by Kfmin recovers the min joint possibility distribution  proposition     Proposition    The CNF encoding Kfmin of a possibilistic network encodes the joint distribution of given network  Once the CNF encoding is accomplished  it is then compiled into a  DNNF  from which we extract the possibilistic circuit p  definition    that implements the encoded fmin   Definition    A possibilistic circuit p encoded by a  DNNF sentence  c is a DAG in which leaf nodes correspond to circuit inputs  internal nodes correspond to max and min operators  and the root corresponds to the circuit output  As in the probabilistic case  Darwiche         this circuit can be used for linear time inference  More precisely  computing the possibility degree of an event consists on evaluating p by setting each evidence indicator x to   if the event is consistent with x  to   otherwise and applying operators in a bottom up way  This possibility degree corresponds exactly to the one computed from the min joint possibility distribution  proposition     This method referred to  DNNFP F  Data  Gmin   instance of interest x  evidence e Result   x e  begin Compilation into  DNNF Encode Gmin into fmin using equation   EncodeCNF of Gmin into  using equations         Compile  into  c p  Possibilistic Circuit of  c Inference Applying Operators on p  x  e   Root Value  p    x e    e   Root Value  p   e  if  x  e    e  then  x e    x  e  else  x e     return  x e  end  Proposition    Let Gmin be a possibilistic network  Let min be a joint distribution obtained by chain rule  Then for any a  Da and e  DE   we have  A   a E   e    min  A   a E   e  where min  A   a E   e  is obtained from min using equation   and  A   a E   e  is obtained from algorithm    The key point to observe here is that this approach can handle possibilistic circuits of manageable size as in the probabilistic case since some possibility values may have some specific values  for instance  whether they are equal to   or    and whether some possibilities are equal  In this case  we can say that the network exhibit some local structure  By exploiting it  the produced circuits can be smaller  In fact  the normalization constraint relative to the initial network will mean that we will have several values equal to    Thus the idea is to make an advantage from such a local structure which has a particular behavior with the max operator in order to construct more compact possibilistic circuits w r t  standard ones as stated by the following proposition  Proposition    Let N bposs and N bproba be the number of clauses in the possibilistic and probabilistic cases  respectively  Then N bposs  N bproba   Note that for particular situations where probability values are   or    we have N bposs   N bproba   otherwise N bposs  N bproba   Example    To illustrate algorithm   we will consider the min based possibilistic network represented in figure    We are looking for  f   d    with f  as instance of interest and d  as evidence  First  we encode the network as a possibilistic function and encode it on CNF  This latter is then compiled into  DNNF from which a possibilistic circuit is extracted  The possibility degree  f   d    is computed using this circuit in polynomial time  For instance   f    d    is computed using p by just replacing   f    d    b    b      and applying possibilistic operators in a bottom up way as shown in figure    Hence   f   d       f    d          since  f    d               from a function f encoding the CNF  Then  we have min  xi        xj      xi        xj    i e  f recovers the min joint possibility distribution min   Comparing theoretically the probabilistic and the possibilistic case allows us to deduce the following proposition  Proposition    The possibilistic encoding of a possibilistic network given by K  equation     is more compact than the probabilistic encoding given in  Wachter and Haenni         In fact  the number of variables used in K is less than the one used in  Wachter and Haenni         In particular for parameters  our approach uses one variable per different weight  while in the probabilistic encoding one variable per parameter  For each clause in K there exists a clause of the same size in the probabilistic encoding  The converse is false   Figure    Inference using the possibilistic circuit  p          INFERENCE USING POSSIBILISTIC COMPILED REPRESENTATIONS  DNNF plays an interesting role in compiling propositional knowledge bases  It has been used to compile probabilistic networks  More precisely in  Wachter and Haenni         authors have been interested in performing a CNF logical encoding of the probability distribution induced by a bayesian network  then a compilation phase from CNF to d DNNF  In this section  we propose to adapt this encoding in the possibilistic setting by taking into consideration the local structure aspect  This allows to reduce the number of additional variables comparing to the probabilistic encoding  Let  be propositions linked to networks variables and let  be propositions linked to the possibility distribution entries  equal to     We start by looking at the possibility distribution encoding  The logical representation of a network variable Xi is defined by  Xi            ui       uim  xi  ui  xi     ui  xi  ui X  i   ui  By taking the conjunction of all logical representations of variables  we obtain the networks representation  as follows      Xi      Xi   The CNF encoding  denoted by K indeed recovers the min joint possibility distribution  proposition     Proposition    Let min be the joint possibility distribution obtained using the chain rule with the minimum operator and  be the possibility degree computed  Once the qualitative network is encoded by K   it is compiled into a compilation language that supports the transformations conditioning and forgetting and the query possibilistic computation  This language is  DNNF  proposition     Therefore  the CNF encoding is first compiled  and the resulting  DNNF is then used to compute efficiently  i e  in polynomial time a posteriori possibility degrees  proposition     This method referred to  DNNF is outlined by algo     Proposition     DNNF supports conditioning  forgetting and possibilistic computation  Algorithm    Inference using  DNNF Data  Gmin   instance of interest x  evidence e Result   x e  begin Compilation into  DNNF EncodeCNF of Gmin into  using equation    Compile  into pc Inference v   Explore  DNNF x  e  pc   v   Explore  DNNF e  pc   if v   v  then  x e   v  else  x e     return  x e  end  Proposition    Let Gmin be a possibilistic network  Let min be a joint distribution obtained by chain rule  Then for any a  Da and e  DE   we have  A   a E   e    min  A   a E   e  where min  A   a E   e  is obtained from min using equation   and  A   a E   e  is obtained from algorithm    Example    Let us illustrate algorithm    In fact   of the network of figure   is      F  B  D        f         b      f   b      d      f   b      d      f   b      d      f   b      d     such as           and   correspond respectively to               and      To compute  f   d     we should first compute  f    d    using algorithm    The first step is to check if we have at least   Algorithm    Explore  DNNF Data  a set of instances x  compiled representation pc Result   x  begin if  xi  x  xi  Ui is not a leaf node then  x     else y   xi     xi  Ui is a leaf node  Ui  x  c p y  Condition pc on y c pc  y  Forget  from p y c Applying Operators on p  y  x   Root Value of pc  y return  x  end  one  as a leaf node  In this example  we have d   f   b  and d   f   b  as leaf nodes  hence conditioning should be performed  Then  a computation step is required by applying in a bottom up way Min and Max operators on the forgotten  DNNF  Therefore   f   d       f    d               NEW POSSIBILISTIC INFERENCE ALGORITHM  In  Benferhat et al          authors have been interested in the transition of possibilistic networks into possibilistic logic bases  The starting point is that the possibilistic base associated to a possibilistic network is the result of the fusion of elementary bases  Definition   presents the transformation of a min based possibilistic network into a possibilistic knowledge base  Definition    A binary variable Xi of a possibilistic network can be expressed by a local possibilistic knowledge base as follows  Xi     xi  ui   i     i       xi  ui          The possibilistic knowledge base of the whole network is  min   X   X       Xn   In another angle  researchers in  Benferhat et al         have focused on the compilation of bases under the possibilistic logic policy in order to be able to process inference from it in a polynomial time  The combination of these methods allows us to propose a new alternative approach to possibilistic inference  This is justified by the fact that the possibilistic logic reasoning machinery can be applied to directed possibilistic networks  Benferhat et al          The idea is to encode the possibilistic knowledge base min into a classical propositional base  CNF   Let A    a         an   with a          an the different weights used in min   A set of additional propositional variables  denoted by Ai   which correspond exactly to the number of different weights  are incorporated and for each formula i   ai will correspond the propositional formula i Ai   Hence  the propositional  encoding of min   denoted by K is defined by  K    i  Ai    i   ai    min          The following proposition shows that the CNF encoding K recovers the min joint possibility distribution  Proposition    Let min be the joint possibility distribution obtained using the chain rule with the minimum based conditioning and let K be the propositional base associated with the possibilistic network given by equation     Let i be a propositional formula associated with a degree ai   Then            iff  A         An      K is consistent       ai iff  A         Ai      K is inconsistent and  A         Ai       K is consistent  The CNF encoding K is then compiled into a target compilation language in order to compute a posteriori possibility degrees in an efficient way  Here  we are interested in a particular query useful for possibilistic networks  namely what is the possibility degree of an event A   a given an evidence E   e  Therefore  we propose to adapt the algorithm given in  Benferhat et al         in order to respond to this query as shown by algorithm    Proposition    shows that the possibility degree computed using algorithm   and the one computed using the min based joint possibility distribution are equal  Note that this approach is qualified to be flexible since it takes advantage of existing propositional knowledge bases compilation methods  Benferhat et al          This method referred to DNNF PKB is outlined by algorithm    Algorithm    Inference using DNNF Data  Gmin   instance of interest x  evidence e Result   x e  begin Transformation into K Transform Gmin into min using definition   Transform min into K using equation    Inference c K  T arget K   c K  K StopCompute  false i   x e     while  K   Ai  e  and  i  k  and  StopCompute false  do K  condition  K  Ai   if K  x then StopCompute true  x e     degree i  else ii   return  x e  end  Proposition     Let Gmin be a possibilistic network  Let min be a joint distribution obtained by   chain rule  Then for any a  Da and e  DE   we have  A   a E   e    min  A   a E   e  where min  A   a E   e  is obtained from min using equation   and  A   a E   e  is obtained from Algo      Indeed  in  DNNFP F   we associate propositional variables not only to possibility degrees  parameters   but also to each value xi of Xi   While in DNNF PKB only m new variables are added  one variable per different degree    Example    To illustrate algorithm   we will consider  Let us now analyze these three approaches from experimental points of view  Our experimentation is performed on random possibilistic networks  More precisely  we have compared DNNF PKB and DNNFP F on     possibilistic networks having from    to    nodes  As mentioned that the approaches focus mainly on encoding the possibilistic network as a CNF then compile it into the appropriate language  hence  it should be interesting to compare the CNF parameters  the number of variables and clauses  and the DNNF parameters  the number of nodes and edges  for the two methods   the min based possibilistic network represented in figure    The CNF encoding is as follows   K    d   f   b   A       b   A       d   f   b   A       f   A       d   f   b   A       d   f   b   A    such as A         A         A        and A        are propositional variables followed by their weights under c brackets  Compiling K into DNNF results in  K     b   A       A   f      f    d    A   d          b     f    d    A   d         f    A      d    A   d         c The computation of  f   d    using K requires two iterations  Therefore   f   d         degree            Due to the compilation step  this algorithm runs in polynomial time  Moreover  the number of additional variables is low since it corresponds exactly to the number of priority levels existing in the base      COMPARATIVE AND EXPERIMENTAL STUDIES  The paper analyzes three compilation based methods  namely DNNF PKB   DNNF and  DNNFP F   The first dimension that differentiates the three approaches proposed in this paper is the CNF encoding  It consists of specifying the number of variables and clauses per approach  The CNF of DNNF PKB is based on encoding x where x is an instance of interest having a possibility degree different from    In  DNNF  we write implications relative to instances having   as possibility degree  We can notice that the local structure in both methods is exploited in semantically different ways  In DNNF PKB  the encoding uses the number of different weights as the number of additional variables while the  DNNF encoding uses the number of the non redundant possibility degrees different from   in the distributions  Regarding the number of clauses  both methods handle possibility degrees different from    This leads us to the following proposition        CNF PARAMETERS  First we propose to test the CNF encodings characterized by the number of variables and the number of clauses  Regarding DNNF PKB  the number of additional variables correspond to the number of weights which are different  While in  DNNFP F   variables are both those associated to the possibility degrees of each distribution and those to variables instances  The number of clauses for each method is related to the CNF encoding itself  Figure   shows the results of this experimentation  Each approach is characterized by a curve for the average number of variables and a curve for the average number of clauses  It is clear that the higher the number of nodes considered in the possibilistic network  the higher the number of variables and clauses  Figure   shows that DNNF PKB has the lower number of variables and clauses comparing to  DNNFP F   which confirms the theoretical results detailed above   Proposition     The CNF encodings of DNNF PKB and  DNNF have the same number of variables and clauses  The CNF encoding of  DNNFP F is different from the ones of DNNF PKB and  DNNF  Proposition    shows the difference between  DNNFP F and DNNFPKB in terms of number of variables and clauses  Proposition     The number of variables and clauses in  DNNFP F is more important than those in DNNF PKB   Figure    CNF parameters         DNNF PARAMETERS  Once we obtain the CNF encodings  it is important to compare the number of nodes and edges for each compiled base  Figure   represents the average size of the compiled bases for the two methods in terms of nodes and edges numbers  We remark that the number of nodes and edges depends deeply on CNF parameters  More precisely  the number of nodes and edges in DNNF PKB is considered narrow comparing to  DNNFP F   This can be explained by the lower number of variables and clauses on CNFs and the local structure which shrinks the sizes of compiled bases  Comparing DNNF PKB to  DNNFP F   the behavior of DNNF PKB is important    Pearl         Benferhat and Smaoui         Acknowledgements We thank the anonymous reviewers for many interesting comments and suggestions  Also  we wish to thank Mark Chavira for our valuable discussions on this subject  The third author would like to thank the project ANR Placid   
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    B     A               C   D C                 B      B                                                          G                                          G                     B           B   D ID       B            B          B        E                                                                       D    F             E                                   K              I          B    AB           J                  B           B    A                            E       H       D                                   U      A  U    B                B  B   D    B           Y          B   D   Y                  B          P       Q            B  L   BL M   L    B    B              B  L    B  L B          B                                     B              B    M                       D                                                                                                                                          A                      NL     L     L O        L     L O             L                  L                     M                                D  B                                 B                       B                                       B   B          B                                                   B               B                B  L B                B                B  L                          E           E                                                                          A         E                              E                                                                     A                                                            B                                                                       D B    B    B B          B          J           B     D       B          B  B             B                      L             BL M                          LR  I   S DLRD I D S        B            F   B    J            A                                                                                                                                                                 E                            D                                                                                         A   E                                 E                       E  E                         E                                        N                          T           H            E E                            E                                                    K  H         G      K                              G                                                                                                                                                               K              C                                          N          HH                 GG                       V         H                                    G            O         W  GG  GH       V  N                      GG      GG         V           K        GG         X            F        Y   Y                       V                                   GG           V      V  N   E    V                       H       T                        G   W G                D            H                    V              W     G          V                                                                                     UK  U         V                 W          H  V     K   V              G  V    H       
 This paper presents and discusses several methods for reasoning from inconsistent knowledge bases  A so called argumentative consequence relation  taking into account the existence of consistent arguments in favor of a conclusion and the absence of consistent arguments in favor of its contrary  is particularly investigated  Flat knowledge bases  i e  without any priority between their elements  as well as prioritized ones where some elements are considered as m ore strongly entrenched than others are studied under d i f f e r e n t consequence relations  L astly a paraconsistent like treatment of prioritized knowledge bases is proposed  where both the level of entrenchment and the level of paraconsistency attached to a formula are propagated  The priority levels are handled in the framework of possibility theory      Introduction One of the emerging important problems pertaining to the management of knowledge based systems is inconsistency handling  Inconsistency may be present for several reasons  the presence of general rules with exceptions  the existence of several possibly disagreeing sources feeding the knowledge base are among the most common ones  There are two attitudes in front of inconsistent knowledge  One is to revise the knowledge base and restore consistency  The other is to cope with inconsistency  The first approach meets two difficulties  there are several ways of restoring inconsistency yielding different results  and the problem is that part of the information is thrown away and we no longer have access to it  Coping with inconsistency bypasses these difficulties  However we must take a step beyond classical logic  since the presence of inconsistency enables anything to be entailed from a set of formulas  This paper investigates several methods for coping with inconsistency by suitable notions of consequence capable of inferring non trivial conclusions from an inconsistent knowledge base  These consequence relationships coincide with the classical definition when the knowledge base is consistent  When the knowledge base is flat  i e  made of  equally reliable propositional formulas  the proposal made by Rescher and Manor      is very commonly used nowadays  compute the set of maximal consistent subsets of the knowledge base first  then a formula is accepted as a consequence when it can be classically inferred from all maximal consistent subsets of propositions or from at least one maximal consistent subset  However the first consequence relation is very conservative hence rather unproductive while the latter is too permissive may leads to pairs of mutually exclusive conclusions  A mild inference approach is proposed in this paper  that is more productive than the first consequence relation but do not lead to conclusions which are pairwise contradictory  It is based on the idea of arguments that goes back to Toulmin      and is related to previous proposals             and      that were suggested in the framework of defeasible reasoning for handling exceptions  We suggest that a COtlclusion can be inferred from an inconsistent knowledge base if the latter contains an argument that supports this conclusion  but no argument that supports its negation  The paper is organised as follows  Section   deals with flat k n o w l e d g e b a s e s a n d c o m pares s e veral notion s o f consequence relations that are inconsistency tolerant  including several ones that come from the non monotonic  logic literature  Section   contains a thorough analysis of our argumentative inference process  Section   extends the argumentative inference to layered knowledge bases where layers express degrees of certainty as in possibilistic logic       it refines the flat case by allowing for pieces of information of various levels  Section   deals with a paraconsistent like treatment of layered inconsistent knowledge bases  whereby a formula carries two weights  its degree of certainty and the degree of certainty of its negation  Lastly a new way of combining knowledge bases issued from several sources is suggested  inspired by the argumentative inference  Results are given without proofs due to space limitations  Proofs appear in the full report      Arguments in Flat Knowledge bases D efinition of      Consequence Re lation  an  Argumentative  For the sake of simplicity  we consider in this paper only a finite propositional language denoted by     We denote        Benferhat   Dubois  and Prade  the set of classical interpretations by n  by t  the classical consequence relation  Let I  be a set of propositional formulas  possibly inconsistent but not deductively closed  OJ D denotes the deductive closure of I   i e en I       e   I  t       W e also a s s ume that t h e knowledge bases manipulated in this section are flat  which means that all formulas in I  have the same reliability   From now on  we denote by Inc I   the set of propositions belonging to at least one minimal inconsistent sub base of I   namely  Inc I            I i  I   such that     e I i and I i is minimal inconsistent   We now introduce the notion of argument   The set Inc I   is somewhat related to the  base of nogoods  used in the terminology of the ATMS      Once Inc I   is computed  we remove from I  all elements of Inc I    the result base is called the free base of I   denoted by Free I        In other words  Free I   contains all formulae which are not involved in any inconsistency of the knowledge base L  Now  let us introduce the notion of theFree consequence  denoted by t Free   Def     A sub base Li of L is said to be an argument for a formula     if it satisfies the following conditions   i  I i   l   ii  I it     and  iii   i Jfe Li  I i             Def     A formula     is said to be a free consequence  or a sound consequence  of I   denoted by I t   iff    i s logically entailed fromFree L   i e  LI Ftre    iffFree I  t     Notice that this notion of argument is identical to the one proposed in      and is also very similar to the notion of environment used in the terminology of the ATMS        TheFree inference relation is very conservative as we will later  Let us now recall the approach first proposed in       Let I  be a possibly inconsistent base  MC I   be the set of all maximal consistent sub bases of I   The universal  called also the inevitable  consequence relation is defined in      in this way   Def l  A sub base I i of I  is said to be consistent if it is not possible to deduce a contradiction from I i  and is said to be maximally consistent if adding any formula    from I  I i to I  i produces the inconsistency of I iu        Def     A formula    is said to be an argumentative consequence of I   denoted by L t       if and only if   i  there exists an argument for     in I   and  ii  there is no argument for   cj  in I    As a consequence of this defmition  if our knowledge base contains only the two contradictory statements             then the inference    A     t bi  I  does not hold  In other words  our approach is in agreement with the idea of paraconsistent logics      where they reject the principle  ex absurdo quodlibet  which allows the deduction of any formula from an inconsistent base  It is easy to verify that t  bi is non monotonic  Moreover  if I  is consistent then I t    iff I t   t  l  It means that the non monotonicity only appears in the presence of inconsistency  and the argumentative consequence resorts to what Satoh      calls  lazy non monotonic reasoning   an idea also proposed in            Comparative Study o f Inconsistency Tolerant Consequence Relations  In this sub section we compare our approach to reasoning in the presence of inconsistency to the ones reviewed in      We start this comparative study by presenting the different approaches from the most conservative ones to the most adventurous ones But first we need some further definitions  Def     A sub base I i of I  is said to be minimal inconsistent if and only if it satisfies the two following requirements   i  Lit  l  and  ii  i     E Li  I i        l   see  Def     A formula     is said to be a universal consequence or Me consequence of L  denoted by Lt V   iff     is entailed from each element of MC L   namely  I      i  I  iff  i Li E MC I    Li     I  As it has been mentioned above  the Free consequence relation is more conservative than Me consequence  Proposition    Each Free consequence is also a Me consequence  The converse is false   One way of finding the proof of the previous proposition is to notice that  Free I     ni i e MC I   Li since if a formula     does not belong to Free L  then there exists a minimal inconsistent sub base I k containing     and therefore there exists at least one maximally consistent sub base which contains Lk but not     which means that there exists at least one element of MC L  which does not contain     and consequently    does not belong to the intersection of the elements of Me L   The converse is also true  Indeed  if    eo n L iE MC I  Li then there exists a sub base Li such that c e  Li  and LiU     is inconsistent  therefore    is not free  Then from the properties of en  we find  en Free I    Cn ni i e MC I  I i n i  ie MC I  Cn I i   The next propositions compare the Me consequence to the argumentative consequence         Argumentative inference in uncertain and inconsistent knowledge bases  Proposition    A formula ell is an argumentative consequence of I  iff  i    Li E MC I    such that Li f   cj   and  ii   ji j e MC I    such that I j f     cp   Proposition    Each Me consequence of I  is also an  argumentative consequence of I   The converse is false  One of the main drawbacks of Me consequence is the number of elements in MC I   which increases exponentially with the the number of conflicts in t he base and in general  it is not possible to take into account all the elements of MC I    In      it is proposed to select a non empty subset of MC I    denoted by Lex I    and computed in the following manner  Li E Lex I   iff  V I  j E MC I    I L il  I L jl where II I is the cardinality of I   This ordering is called the lexicographical ordering  and corresponds to the property of parsimony advocated in diagnostic problems       A probabilistic justification ofLex I   can be found in      In order to generate the set of plausible inferences based onLex I   from an inconsistent knowledge base  we use a defmition similar to the Me consequence   Def     A formula    is said to be an existential consequence of I   denoted by I  f       iff there exists at  least one element of MC I   which entails cj   namely  iff   Li E MC L    Li f      L f       It is not hard to see that this approach is the most adventurous one  but unfortunately it has an important drawback  since this approach may lead to inconsistent set of results  Indeed  there may exists Li f  cj  and Ljf   cj   in which case both cj  and   cj  will be deduced  The following hierarchy summarizes the links existing between the different consequence relations studied here  the edge means the inclusion set relation between the set of results generated by each inference relation  The top of the diagram thus corresponds to the most conservative inferences  All inferences reduce to the classical one when I  is consistent  Figure    A comparative study of inference relations  Def     A formula cj  is said to be a Lex consequence of  L   denoted by Lf  Lex     iff it is entailed from each element ofLex I    namely   V Li E Lex I    Li f  cj  I  f Lex    iff Proposition    Each Me consequence of I  is also a Lex consequence of I   The converse is false   Argumentative Consequence I A  TheLex consequence and argumentative consequence are not comparable as we see in the following example  Example Let I   A      Bv    A  B      ev    A  e      AvD  We have Lex  L         Bv   A  B     Cv  A  C     AvD     Then   A is a Lex consequence of I  while it is not an argumentative consequence  since A is also present in L   In contrast  D is an argumentative consequence  it derives from  A     Av D   while it is not a Lex consequence  TheLex consequence may appear as an arbitrary selection from MC L   if we consider a semantic point of view  Namely  the following situation may happen  LiELex I    LjE MC I   Lex I   and one may define Lk logically equivalent to I  j but II ki I L  i l  However all introduced consequence relations are syntax sensitive since L  is not closed  Yet  the counterexample demonstrates that the Lex consequence may implicitly delete some useful pieces of knowledge  here A   It may result in destroying some arguments  as well as some rebuttals  i e  formulas whose presence ensure an argument for    cj  that inhibits arguments for cj    Another definition of the consequence relation  called existential relation is also proposed in       namely      Properties of  t    t    Proposition    failure of AND   We may have L        t    j   I  f   t  ljl  and not I      t     A  I  Proposition   must not be seen as a major drawback of f   t  since in some cases we do not want to have the AND property  The f   t  consequence relation captures the cases when we believe in two mutually consistent properties of some object for conflicting reasons  Proposition    f   t  satisfies the property of Right Weakening  i e If cj     I  then I      t     implies I      t   I         Benferhat   Dubois   and Prade  An important issue when reasoning with an inconsistent knowledge base L  is whether it is possible to construct some equivalent consistent base such that plausible inferences from L  are its logical consequences  In this section we try to construct such an equivalent knowledge base using the argumentative inference relation  Propositions   and   are very important to characterise the set of argumentative consequences of a knowledge base L  denoted by Cnc  t L    Cnc  t L    c l  L   c  t cjl   The fact that the argumentative consequence is not closed under conjunction means that Cnc  t  L  is generally not equal to its closure under Cn   namely  Cnc  t L      Cn Cnc  t L   In this section  we assume that we use only the finite set of propositional symbols appearing in the base L   Def     A formula q  is said to be a prime implicate of  L with respect to the argumentative inference relation if and only if  i  Ll c  t q    ii  q   such that cjl   cjl a n d  Ll c  tc l  A prime implicate can be inferred from a maximal consistent subset of L  However  if LiE MC L   then the conjunction of formulas in Li  also denoted by Li  is not a prime implicate since it can be defeated by other maximal consistent subsets of L  Indeed  V i  j  Lii Lj  The construction of prime implicates can be achieved from the semantical point view  Indeed  let  Lil be the set of models of the maximal consistent sub base Li A given model  pL i of  Lil can be viewed as a formula composed of the conjunction of literals it satisfies  Then it can be shown that the following expression is a prime implicate   i   pllV   Vq I i l V L Vq I i l V  Vq I n Moreover  if each maximal consistent sub base is complete  i e  Vae     either aE Li or   aE LV there exists exactly one prime implicate  and in this case the argumentative consequence and MC consequence are equivalent  But in general  the prime implicates can be numerous  Let R      Rn be the set of prime implicates of L   then Cn  can be seen as the union of the deductive closure of     each Ri under Cn  namely  Ch t L  Cn RI u   uCn Rn  And it is easy to check that V i  j   l  n L  c  t Ri  Rj Examples     let L      A v B  A v C  A    A   We have two maximal consistent sub bases  Ll    AvB  AvC  A   L      AvB  AvC    A  Then   Ll   AABAC  AABA  C   L      AABAC    A    BAC   Therefore we have four prime implicates  Rt BA CvA   R   AAB v   BACA  A  R  CA   AvB   R t    A  C v   CAAAB      Consider now L as            A v B  A v B  A    A  C   We have two maximal consistent sub bases  Lt hAvB  AvB  A C   L     AvB  AvB    A  C  Then  rLI   AABAC    LiJ    AABAC  The maximal consistent sub bases are complete  therefore we have only one prime implicate  R  AABAC v   AABAC  BAC Then  Cnc  t L    Cn  B  C    Let us now justify why the previous definition makes sense only if we restrict ourselves to the propositional symbols appearing in the knowledge base  Indeed  let us consider the following example L   A   A   we have only one prime implicate  the tautology T  and therefore it is not possible to deduce Av B from Cn T  which is an argumentative consequence of the knowledge base  The situation   L l  s  R i   L   c  t R j and L c  t R iARj can correspond to two cases   i  No argument supporting RiARj can be found  In that case the arguments Li and Lj supporting R i and Rj respectively are inconsistent  Indeed  if L iULj is consistent then LiULji RiARj and there would exist an argument supporting Ri AR j   ii  There is an argument for   Riv  RjAnyway the arguments supporting the prime implicates can be viewed as a set of scenarii extracted from L  that express different points of views on what is the actual information contained in L   These points of view are incompatible in the sense that the subsets L i and Lj supporting two prime implicates Ri and Rj should not be mixed  even if not inconsistent   The fact that Cnc  t  L  still reflects conflicts lying in L  can be seen as follows  the argumentative inference forbids that two prime implicates Ri and Rj be inconsistent  However the set   R         Rn   can be globally inconsistent  namely one argumentative consequence of L can be defeated by other consequences grouped together     Example Consider the set L     A    B  A  B    Cv  D    AvB  The maximal consistent subsets of L are  Ll      A    B    C v   D    A v B  L       A  B    C v   D    A v B  L     A  B    C v   D    A v B  L       B  A    C v   D  Consider the three formulas  c l     A    B A    Cv  D   v    CADA AvB   c l     A  BA   Cv  D  v CA  D A  A v   B   c l   AABA   Cv  D  v   CA  DA   A v   B   It is easy to see that L ll c IJ  L   c l  and L   c l   but we never have L il   cjlj for i j  M o r e o v er c l  Ac l  Ac l     l    Argumentative inference in uncertain and inconsistent knowledge bases  This result can be viewed as a weakness of the argumentative inference which avoids obvious direct contradictions  but does not escape hidden ones  It confmns the fact that Cn  I   is a heterogeneous set of properties that pertain to distinct views of the world  This means that a question answering system whereby a question  is it true that cj   is answered by yes or no after computing I  f    is not really informative enough  The system must also supply the argument for cj   This way of coping with inconsistency looks natural and the arguments for cj  and  I  should enable the user to decide whether these two plausible conclusions can be accepted together or not     Arguments in prioritized knowledge bases  The use of priorities among formulas is very important to appropriately revise inconsistent knowledge bases  For instance  it is proved in      that any revision process that satisfies natural requirements is implicitly based on such a set of priorities  Similarly a proper treatment of default rules also leads to prescribe priority levels  e g       In these two cases  the handling of priorities has been shown to be completely in agreement with possibilistic logic            Arguments of different levels are also manipulated in      in a way completely consistent with possibilistic logic  In the prioritized case  a knowledge base can be viewed as a layered knowledge base  L B  u    u B n  such that formulas in Bi have the same level of priority or certainty and are more reliable than the ones in Bj where j   i  This stratification is modelled by attaching a weight a e       to each formula with the convention that  cj   Xi  e Bi  Vi and a        a        Xn             A sub base  Li  E  u    uEn of  L   B  u    uBn where V j   n  EjB j is said to be consistent if   Li  l  and is said to be maximal consistent if adding any formula from  I   Li  to Li produces an inconsistent knowledge base        Before introducing the notion of argument in prioritized knowledge base  let us define the notion of entailment in a layered base  named  t entailment  Def      Let I    B  u    uBn be a layered knowledge  base  A formula cj  is said to be a  t consequence of I  with weight  Xi  denoted by I  f  t      Xi   if and only if   i  B  u     uBi is consistent  and Btu    uBif  cj   ii  V j  i  B  u   u Bj   cj   iii     The definition of f  t is identical to the one proposed in possibilistic logic                 It is clear that in the presence of inconsistency the  t entailment and the       classical entailment have not the same behaviour  Indeed in classical logic if our base I  is inconsistent then any formula can be deduced from I  and the base becomes useless  In a stratified base  the situation is better since it is possible to use only a consistent subbase of I   in general not maximal   denoted by  t L   induced by the levels of priority and defined in this way   t L    B  u     uBi  such that  t L  is consistent and Btu    uB i l is inconsistent    The remaining sub base I   t L  is simply inhibited  It is not hard to check that the following result holds  iff  t  L  f  c   I  f  t c      However  this way of dealing with inconsistency is not entirely satisfactory  since it suffers from a principal drawback named  drowning problem  in      as we can see in the following examples  Examples  Let I  be the following stratified knowledge base  L       A v   B    A    B    C   This notation of the form  B   B        Bn   where the weights are omitted is used for the sake of simplicity  This base is of course inconsistent  and only the subset Li       A v   B    A    is kept  and therefore C cannot be deduced despite the fact that C is outside the conflict  A particular case of the drowning effect is called  blocking property inheritance             This can be illustrated by the following set of stratified defaults   L    p      p v b    p v   f      b v f   b v w   where p  b  f and w means respectively penguin  bird  fly and wings  From this base it is not possible for a penguin to inherit properties of birds  in our example to inherit property of having wings   while the only undesirable property for a penguin is  flying       One way of solving the drowning problem is to recover the inhibited free defaults  denoted by IFree I    and defined in this way  IFree  L    Free I   n  L   t L   Then once the inhibited free set has been computed  we define the new inference relation in this way  Def      A formula cj  is said to be a  t Free consequence of I   iff it is logically entailed from  t  L u IFree I    namely  Lf  t freecl  iff  t  L uiFree  L  f cj  Proposition    Each  t consequence of I  is also a  t Free consequence of I    Brewka      see also       has proposed a more adventurous approach to reason with inconsistent and layered knowledge bases  the idea is to take advantage of the stratification of the base to rank  order the maximal consistent sub bases of I  and keep only the best ones  namely the  so called preferred sub bases          Benferhat  Dubois  and Prade  Let L   B  u     u B n be a layered knowledge base  A preferred sub base Li is constructed by starting with a maximal consistent sub base of B    then we add to Li as many formulas of Bz as possible  wrt to consistency criterion  and so on  Formally Li is a preferred sub base of I  if it can be con  s tructed as f ollows  Lj E l uEzu   uEn  where  V j      n  E  u Ez u     uEj is a maximal consistent sub base of B   u Bzu   uB j  Preferred subbases have also been independently introduced in     in the setting of possibilistic logic under the name of strongly maximal consistent subbases  They are such that Lju  cj  a   t  n  l a     V cj  a  e L Li Def      Let Pref I   be the set of preferred sub bases of I   A formula cj  is said to be a preferred consequence of I   denoted by Lt  preflll  iff it is entailed from each element of Pref I    namely  Lt  preflll iff  VI i E Pref I   Lit  e   Proposition    Each  t Free consequence of L is also a preferred consequence of I   The converse is false The Lex consequence relation described in the case of flat knowledge bases has also been proposed in the case of stratified knowledge bases      The objective is to reduce the number of elements of Pref I    by selecting the elements which satisfy the following requirement  Li E u   uEneLex I   iff  V I j E  u    uE n ePref I    li  such that IE i i IEil and  V j i IE ji IEjl The definition of Lex consequence is identical to the one presented in the case of a flat knowledge base namely a formula cj  is a Lex consequence of L if and only if it is entailed from each element of Lex I    Proposition    Each preferred consequence of L is also a Lex consequence of I   The converse is false  Now we propose to extend the argumentative inference to layered knowledge bases  and to compare it with the inferences proposed above  Def      A sub base Li of L is said to be an argument for a formula cj  with a weight a  if it satisfies the following conditions   i  Li   l   ii  Li      t    cj  a    and  iii  V Ijl J   E Li  Li     I  f      t  cj  a    Def      A formula cj  is said to be an argumentative consequence of I   denoted by L       J    cj  a    iff   i  there exists an argument for  cj  a   in L   and  ii  for each argument of      cj  J   in I   we have a  f   We now sketch the procedure which determines if cj  is an argumentative consequence of a stratified knowledge base I  B  u   uB n  The procedure presupposes the existence of an algorithm which checks if there exists an argument for a given formula in some flat base  This can be achieved by using the variant of a refutation method proposed for example in        The procedure is based on a construction of the maximal argument of cj  and its contradiction  First we start with the sub base B    and we check if there is a consistent sub base of B  which entails cj  or    cj   If the response is respectively Yes No then cj  is an argumentative consequence of L with a weight a        by symmetry if the response is No Yes then     cj  is in this case the argumentative consequence of I   Now if the response is Yes Yes then neither cj  nor     cj  are argumentative consequences  If the response corresponds to one of the answers given above then the algorithm stops  If the response is No No we repeat the same cycle described above with B   uBz  The algorithm stops when we have used all the knowledge base I   As discussed in the case of a flat knowledge base  the inference relation        is non monotonic  and if our knowledge base is consistent then the set of formulas generated by        is identical to the one generated by the  possibilistic  inference rule       t   The next proposition shows that       J   is a faithful extension of the inference  t entailment  Proposition     If L       t  cj  a   then L The converse is false          J    cj  a     Proposition     Each  t Free consequence of I  is also an argumentative consequence of I   The converse is false The argumentative consequence is not comparable to the Pref consequence nor the Lex consequence  as we see in the following example  Example Let L     A    Bv   A B  C        Cv   A         AvD   We have    Pref I       A    Bv    A  C        A vD      A B  C       AvD         Bv   A B C        Cv  A       AvD      Lex I           Bv   A B C       C v    A      AvD     Then   A is a Lex consequence of I  while it is not an argumentative consequence  since A is also present in I   Note that one may object to the deletion of A from Lex I    given its high priority  Hence the Lex consequence looks debatable  In contrast  D is an argumentative consequence  it derives from  A   A v D   while it is not a Pref consequence nor a Lex consequence  Again the Pref consequence forgets the argument  because A and   Av D do not belong to all preferred subbases  Let L     A       A      A v     D  A v D    we have Pref I     A       Av    D   AvD      In this case     D is a Pref consequence  while it is not an argumentative consequence of I   Again  the argument for D is killed by Pref I        As we have done in the non stratified case we summarize the relationships between the different consequence relations    Argumentative inference in uncertain and inconsistent knowledge bases  Figure    Acomparative study of inference relations in stratified knowledge  t Consequence l  t  Argumentative Consequenc           Paraconsistent  Like Reasoning Layered Knowledge Bases  in  In the preceding sections we have seen how in the case of flat and prioritized knowledge bases it is possible to use consistent subparts of it in order to define different types of consequences which are still meaningful  Levels of priority or of certainty attached to formulas have also been used to distinguish between strong and less strong arguments in favor of a proposition or of its contrary  However it is possible to go one step further in the use of the certainty or priority levels by i  attaching to a proposition cjl not only the  greatest  weight a attached to a logical proof of  jl  in the sense of section    from a consistent subbase  but also the weight  attached to the strongest argument in favor of   cp if any  and ii  by continuing to infer from premises such  cjl  a      propagating the weights a and    It will enable us to distinguish between consequences obtained only from  free  propositions in the knowledge base I  for which     i e  propositions for which there is no argument in I  in favor of their negation   and consequences obtained using also propositions which are not free  for which there exist both a weighted argument in their favor and a weighted argument in favor of their negation   More formally  the idea is first to attach to any proposition in the considered stratified knowledge base I  two numbers reflecting the extent to which we have some certainty that the proposition is true and to what extent we have sopte certainty that the proposition is false  and then to provide some extended resolution rule enabling us to infer from such propositions  For each cjl  such that  cj a  is in I   we compute the largest weight a  associated with an argument for  jl and the largest weight W associated with an argument for   cp in the sense of Section    If there exists no argument in favor of   cp  we will take       P  O  it means in this case that  cjl a  is among the free elements of I  since cjl is not involved in the inconsistency of I   otherwise there would exist an argument in favor of    cjl   In the general case  we shall say that c   has a level of  paraconsistency  equal to min a  W   Classically and roughly speaking  the idea of paraconsistency  first introduced in      is to say that we have a paraconsistent knowledge about c   if we both want to state cjl and to state   cj   It corresponds to the situation where we have conflicting information about cj   In a paraconsistent logic we do no want to have every formula  I  deducible as soon as the knowledge base contains cjl and   cj   as it is the case in classical logic   The idea of paraconsistency is  local  by constrast with the usual view of inconsistency which considers the knowledge base in a global way  It is why we speak here of paraconsistent information when min a  P        Note that in this process we may improve the lower bound a into a larger one a  if   Li I   Li consistent and Li   t cj a    similarly for W if    cj    is already present in I    Then I  is changed into a new knowledge base I   where each formula  cj  a   of I  is replaced by  cjl a  W   Moreover if a   W  i e  the certainty in favor of    cj is greater than the one in favor of cj   we replace  cjl a  W   by     cj  W a    If c   is under a clausal form       is a conjunction q IA     Aq n   in this case we will replace     cj P  a   by the clauses  q i P  a    i l n in order to keep I   under a clausal form if I  was under a clausal form  Let us consider an example  I      AvB a    A      B    Bo      BvC e      C p    Then  I         Av B max a o  min        A min a          B max    min p e    max o  min a       Bmax o  min a     max     min p e        B v c e min p o       C p min e o      Depending on the ordering between the weights we will keep either    B x y  or  By x  depending if x   y or y   x  If x   y we will keep both of them in I    In a second step an extended resolution rule can be proposed in order to infer from propositions in I    This rule expressed in clausal form is  see the full report for a proof  see     also    A v B a  pt      B v c o  o    A v C e  p   e    min max    P    max a  B    p    max W o    When W o  O   i e  the premises are not paraconsistent  we obtain e  min a        p  O  Clearly we have e p  i e  the inference preserves the inequality between the weights  We also observe that the degree of paraconsistency o f the conclusion namely min e  p   max W B   is equal to the maximum of the degrees of paraconsistency of the two premises namely with        Benferhat  Dubois  and Prade  min a   W  W and min     o   o   Thus the inference rule extends the standard possibilistic resolution         and in case of paraconsistent premise s   propagates this paraconsistency to the conclusion  In the case where one of the premises is not paraconsistent  i e        for instance  the degree of certainty e    min      max a  o    of the conclusion is greater than its degree of paraconsistency p  o  only if the degree of certainty a  of the non paraconsistent premise is greater than o  and  o   i e     t o    Otherwise the conclusion which is obtained is SUCh that E    p        i e  nothing emerges from inconsistency   we have to choose between a highly certain but highly paraconsistent conclusion and a conclusion with low certainty and low paraconsistency  Lastly observe that for the formulae appearing explicitly in  L  the paraconsistent approach gives the same results as t  c    They differ for other conclusions since the paraconsistent approach propagates the effects of local inconsistency          Let us consider an example I     A        AvB          B          Ave                     AvO          AvE          FvG        F        F           Hvl        I         Observe that I     t    L       i e  the global level of inconsistency of the base is      Then we have I       A             AvB              Ave                         AvO              AvE            FvG          F            Hvl          I           Applying the  paraconsistent  resolution rule yields  e            E            G            I         This shows that  non paraconsistent premises such as    Ave        with a rather low degree of certainty resolved with another premise  here  A         whose level of paraconsistency is larger than this degree of certainty  lead to fully blurred paraconsistent conclusions  here  e           By contrast   Free  t would enable to get  e       while the refutation procedure used in    t yields  e       reflecting the global inconsistency of the base  if the non paraconsistent premise is sufficiently certain with respect to the paraconsistency of the other premise  e g     AvE         and  A         the conclusion  here  E          is not completely blurred  This is true even if this certainty is less than the global level of inconsistency of the base  e g   G          obtained from    FvG          F          if the premises are not paraconsistent   e g     H v I          H          we obtain a non paraconsistent conclusion  as with t Free  t since we do not use refutation  Generally speaking  if a clause Av B is more paraconsistent than the clause   BvC is certain  then AvC will be completely blurred by paraconsistency  Indeed from a logical point of view  being more certain that   AA  B is true than w e are certain that   BvC is true  the entailment AvB    A     Bt AvC applies with a greater level of certainty than AvB    Bve    AvC  We can observe that using the  paraconsistent  resolution rule locally in a knowledge base I    may yield the same proposition with different weights  namely   I  a  W     I  a     In this case  a more certain and less paraconsistent conclusion should be preferred  this is less obvious when     Combining knowledge bases In     several approaches are proposed to combine knowledge bases  and one of them is very similar to what     calls  preferred sub theories   The idea is to assume a total ordering between different bases Lt L         Ln  such that Li is more reliable than Lj for j i  A resulting base is constructed from I    by adding as many formulas as possible fromi    wrt consistency criterion   then as many formulas as possible from L    and so on  The principal problem is that the resulting base is not unique  Two approaches are proposed in     to merge bases according to suspicious attitude or to trusting attitude  The suspicious attitude is very conservative since for example the result of merging two knowledge bases L l L   is equal to the union of the two bases if they are not conflictual  and is equal to I    in other cases  In the trusting attitude  the approach is very similar to     and produces always one resulting base  but unfortunately the approach is very restrictive since the knowledges bases to be merged must be sets of literals  In the context of possibilistic logic  an approach has been proposed in      for the fusion of n knowledge bases L l       Ln One way of defining the resulting base is to consider the intersection of the deductively closed bases en I i        en I n   by t  t   It is clear that this approach is very cautious  In the same paper  another approach has been proposed considering now the union of the deductively closed bases Cn Li      Cn I  n   However when the resulting base is inconsistent  then some formulas will be inhibited by the drowning effect     We suggest a new approach to merge n knowledge bases l  t       l n   For this aim we use of a variation of the argumentative consequence relation  denoted by t  J   Jvt  Jvt  for the multi sources  and which is defined in the following way  l        l n t   sfUI    I  a  iff   I i  such that Li      I  a   and  I j  such that I j t     q    such that    a  Then the resulting knowledge base is  I result      I  a  I Ll     In t  tJI     I  a       B e c a u s e Lresult c an b e inconsistent  this approach should be used for question answering purposes only  and each response should be accompanied with its argument    Argumentative inference in uncertain and inconsistent knowledge bases      Conclusion  The proposed notion of argumentative inference is appealing for several reasons  First it is an extension of classical inference  in the flat case  and possibilistic inference  in the layered case  that copes with inconsistency in a very  ecological  way  Namely it is very faithful to the actual contents of the knowledge base  and does not do away with information contained in it as opposed to the approaches based on preferred and lexicographically preferred subbases  It avoids the drowning effect of standard possibilistic logic by salvaging sentences whose level of entrenchment is low but are not involved in any contradiction set  Another advantage is that it is amenable to efficient standard implementation methods based on classical resolution  Also it avoids outright contradictory responses  such that     and   cj    although several deduced sentences can be globally inconsistent  But as pointed out earlier  the arguments supporting a set of more than two globally contradictory sentences are distinct  so that the reality of this contradiction is debatable  and only reflects the presence of different points of view  Anyway it seems that it is the price to pay in order to remain faithful to an inconsistent knowledge base  Another result of the paper is the use of local contradictions as a specific weight attached to sentences  This approach only partially avoids the drowning effect  but leads to more informative responses than possibilistic logic since not only the certainty of the formula is evaluated  but also its level of conflict  In the future  the paraconsistent inference should be positioned with respect to the other inference modes in order to assess the benefits of carrying local weights of conflict  Lastly it would be interesting to apply the above result to default reasoning and compare in such a framework the argumentative inference and the one proposed by       
  Possibilistic logic offers a qualitative frame work for representing pieces of information associated with levels of uncertainty or pri ority  The fusion of multiple sources infor mation is discussed in this setting  Differ ent classes of merging operators are consid ered including conjunctive  disjunctive  rein forcement  adaptive and averaging operators  Then we propose to analyse these classes in terms of postulates  This is done by first ex tending the postulates for merging classical bases to the case where priorities are avail able     Introduction  Possibilistic logic  e g       offers a framework for rea soning with classical logic formulas associated with weights belonging to a totally ordered scale  Weights  which technically speaking are lower bounds of neces sity measures  can either represent the certainty with which the associated formula is held for true  or the expression of a preference under the form of a level of priority  In this case the formula encodes a goal  rather than a piece of knowledge  which has to be considered  The fusion of information expressed in a logical form has raised an increasing interest in the recent past years                         Indeed this problem nat urally occurs when handling multiple sources of infor mation  and trying to extract the common  conflict free part of the information  or when trying to fuse the goals expressed by several agents  Clearly possibilis tic logic  which offers a representation framework more expressive than the one of classical logic  by allowing for an explicit stratification of the sets of formulas  is well suited for handling levels of certainty or priority in the fusion process  In recent works         the authors have on the one hand provided a possibilistic syntactic  counterpart of combination operations defined on pos sibility distributions defined on sets of interpretations  On the other hand  taking advantage of the fact that a classical logic formula can be always associated with a stratified set of formulas  using Hamming distance as suggested by Dalal      which reflects partial levels of satisfaction of the initial formula  the authors have shown the agreement of the possibilistic logic based approach with the recent proposals on fusion in the classical logic setting  In this paper we make a step further by i  distinguish ing between different classes of combination operations capable of coping with redundancy  or with drowning effects of  inconsistency free  formulas     encountered in case of conflicts when weights are just combined by a simple operator like min  and ii  by analysing these classes firstly in terms of information sets that each class retains  and secondly in terms of postulates which are natural extensions of those recently proposed in the classical framework               After briefly recalling the necessary background on possibilistic logic in Sec tion    general classes of combination operators are introduced and studied in Section    The handling of the global reliability of the sources or of priorities be tween agents is also briefly considered in this section  A discussion with respect to postulates is presented in Sections   and       Possibilistic logic and fusion  This section recalls some basic notions of possibilistic logic  See     for more details  Let  C be a finite propo sitionnal language  f  denotes the classical consequence relation and n is the set of classical interpretations       Possibility distributions  At the semantic level  possibilistic logic is based on the notion of a possibility distribution  denoted by  r  which is a mapping from n to       representing the available information   r w  represents the degree of       UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       compatibility of the interpretation w with the avail able beliefs about the real world if we are representing uncertain pieces of knowledge  or the degree of satis faction of reaching state w if we are modelling pref erences   By convention   r w      means that it is totally possible for w to be the real world  or that w is fully satisfactory        r w      means that w is only somewhat possible  or satisfactory   while  r w      means that w is certainly not the real world  or not satisfactory at all   Associated with a possibility dis tribution  r is the necessity degree of any formula   N        II    which evaluates to what extent   is entailed by the available beliefs  and defined from the consistency degree of a formula   w r t  the available information  II     max  r w    w E      where    denotes the set of all the models of    In the rest of the paper  a b  c      reflect the possibility degrees of the interpretations          Possibilistic logic bases  At the syntactic level  uncertain information is repre sented by means of a possibilistic knowledge base which is a set of weighted formulas B        a     i    n   where    is a classical formula and a  belongs to a totally ordered scale such as             a    means that the certainty degree of    is at least equal to a   N        a    We denote by B  the classical base as sociated with B obtained by forgetting the weights  A possibilistic base B is consistent iff its classical base B  is consistent  In the following  a             reflect the necessity degrees associated with formulas  Given B  we can generate a unique possibility distribu tion  denoted by  TB  such that all the interpretations satisfying all the beliefs in B will have the highest pos sibility degree  namely    and the other interpretations will be ranked w r t  the highest belief that they fal sify  namely we get         Definition     TB     Vw E n              iJ V  c J    a   E B   w E   c J   max a   w rf       othe ise   W     rw  Inc  B    max a    B  a  is inconsistent  denotes the inconsistency degree of B  When B is consistent  we have Inc B       Subsumption can now be defined  Definition   Let     a  be a belief in B  Then     a  is said to be subsumed by B if  B       a      a       a  is said to be strictly subsumed by B if B a       It can be checked that if     a  is subsumed  then B and B    B       a    are equivalent      Lastly  weights are propagated in the inference process  possibilistic formula    a   with a   Inc B   is said to be a consequence of B  denoted by B l  r    a   iffB  a      Definition   A       Syntactic fusion  We first recall a general result underlying the fusion process in possibilistic logic      Let B   B  be two possibilistic bases  and  T  and  r  be their associated possibility distributions  Let EB be a two place function whose domain is              to be used for aggregating  r   w  and  r  w    The only requirements for EB are the following properties  i    EB       ii  If a    c  b    d then a EBb    c EB d  monotonicity   The first one acknowledges the fact that if two sources agree that w is fully possible  or satisfactory   then the result should confirm it  The second one expresses that a degree resulting from a combination cannot de crease if the combined degrees increase  In      it has been shown that the syntactic counterpart of the fusion of  T  and  r  is the following possibilistic base  denoted by B   and sometimes by B  EB B   and which is made of the union of    the initial bases with new weights defined by  x             a  EIH      a  EB  u       j     EB           Pj    j EB           and the knowledge common to B  and B  defined by      l V I           a  EB      j        a  EB  and   l        EB    It has been shown that  TBal w     T  w  EB r  w  where is the possibility distribution associated to B  us ing Definition    In the case of n sources  the syntactic computation of the resulting base can be easily applied when EB is associative  Note that it is also possible to provide syntactic counterpart for non associative fusion oper ator  In this case EB is no longer a binary operator  but a n ary operator applied to vectors of possibil ity distributions  The syntactic counterpart is as fol lows  Let B   B          Bn  be a vector of possibilistic bases  Let      rn  be their associated possibil ity distributions and  TBa J be the result of combining  rBal  Further definitions used in the paper are now given  Definition   Let B be a possibilistic kno wledge base  and a E        We call the a cut  resp  strict a cut  of B  denoted by B  a  resp  B a   the set of classical formulas in B having a certainty degree at least equal to a  resp  strictly greater than a    Definition   B and B  are said to be equivalent  de noted by B   B   iffVa E        B  a   B a  where   is the classical equivalence   rr               UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS               with Ef   Then  the base associated to  l l Ell is  B JJ    Dj    x  Ef      Ef    j     n   where Dj are disjunctions of size j between formulas taken from different B   s   i      n  and x  is either equal to   o   or to   depending if c J  belongs to Dj or not   rt     Tn  Xn        Possibilistic merging operators  This section analyses several classes of Ef  which cope with different issues met in merging multiple sources information  In the rest of this paper  we assume that Ef  is associative       Conjunctive operators  One of the important aims in merging uncertain in formation is to exploit complementarities between the sources in order to get a more complete and precise global point of view  Since we deal with prioritized information  two kinds of complementarities can be considered depending on whether we refer to formulas only  or to priorities attached to formulas  In this sub section  we introduce conjunctive operators which ex ploit the symbolic complementarities between sources  Definition    Va E  Ef  is said to be a conjunctive operator if          a EB       Ef  a  a   The following proposition shows indeed that conjunc tive operators  in case of consistent sources of infor mation  exploit their complementarities by recovering all the symbolic information  Proposition   Let B  and B  be such that Bi    B  is consistent  Let Ef  be a conjunctive operator  Then  B   Bi    B     An important feature of a conjunctive operator is its ability to give preference to more specific information  Namely  if an information source S  contains all the information provided by S   then combining St and S  with a conjunctive operator leads simply to St  Proposition   Let B  and B  be such that V    lj       E B  B  f rr        Then  B B   Bi   An example of a conjunctive operator is the minimum  for short min   for which we can easily check that B JJ   B  U B   Other examples are the product  and the geometric average defined by a Ef  b     fCib       Disjunctive operators  Another important issue in fusion information is how to deal with conflicts  When all the sources are equally reliable and conflicting  then one should avoid arbi trary choice by inferring all information provided by  one of the sources  Namely  if B  U B  is inconsistent  then one can require that B JJ neither infers B  nor B   Such a behaviour cannot be captured by any con junctive operator  See Section     This requirement is captured by the disjunctive operators defined by  Definition    Va E  Ef  is said to be a disjunctive operator if          a EB       Ef  a       Then  we have  Proposition   Let B  and B  be such that Bi    B  is inconsistent  Then  there exist  c J  o   E B  and   j       E B  such that B JJiirr  c J o   and B JJiirr         Note that if Ef  is a disjunctive operator then B JJ is of the form  B JJ     c J  V   Jj         o    Ef        j     Now  a second natural requirement that one may ask for  in case of conflicts  is to recover the disjunction of all the symbolic information provided by the sources  Clearly  it is easy to find a disjunctive operator which does not satisfy this second requirement  A trivial case is to take the  vacuous  disjunctive operator defined by  Va  Vb a Ef  b      To satisfy this second requirement we define the notion of regular disjunctive operator  Definition   A  regular if Va  disjunctive operator Ef  is said to be    F    Vb   F    a Ef  b   F     Then  we have  Proposition   Let B  and B  be two bases and Ef  be a regular disjunctive operator  Then  B   Bi V B     Examples of regular disjunctive operators are the max  the so called  probabilistic sum  defined by  a Ef  b   a  b  ab  and the dual of the geometric av erage defined by a Ef  b      J l  a   l  b   Lastly  note that regular disjunctive operators are not appropriate in the case of consistency between sources  in particular they give preference to less specific infor mation       Idempotent operators  Another important problem in fusing multiple sources information is how to deal with redundant informa tion  There are two different situations  either we ignore the redundancies  which is suitable when the sources are not independent  or we view redundancy as a confirmation of the same information provided by independent sources  Idempotent operations are de fined by  Definition   EB  if   a E  is said to be an idempotent operator          a Ef  a   a        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Idempotent operators aim to ignore direct redundan cies  Namely  if two sources of information entail the same formula  to a degree a  then one may require that the fused base should not entail  with a degree higher than a  However  such a requirement is strong since  can be obtained from another path exploiting complementarities between higher level formulas pro vided by the two sources  This is illustrated by the following example  Let B         P               and B        V   If                 Clearly B  f rr        and B  f rr         Now let EB be an ide mpotent operator defined by  a EB b      Then  B         P          V  P          V   If        after re moving subsumed for mulas  We can easily check that B f rr        with           This is mainly due to the two pieces of information   P      and   V   If        pro vided separately by the sources  Example    Now  the following proposition shows the cases where idempotent operators indeed ignore redundancies  Proposition   Let B  and B  be two bases  and EB be an ide mpotent operator  Let  be such that B  f rr   a   B  f rr        with       a  Let r    Bl a UB  a Then  iff If then B f rr         with I     max  a  J    Note that I  may be equal to   in case of inconsistency  r in this proposition is the set of classical formulas in B  and B  having a weight strictly greater than a  If  cannot be deduced from r then the idempotent property only guarantees that the repeated informa tion will not be inferred with a priority higher than the one with which it can be individually obtained from the different sources       Reinforcement operators  The aim of reinforcement operators is to view redun dancy of information as a confirmation of this infor mation  Namely  if the same piece of information is supported by two different sources  then the priority attached to this piece of information should be strictly greater than the one provided by the sources  A first formal class of reinforcement operators can be defined as follows  Definition    EB is said to be a reinforcement opera tor if Va  b    and a  b     a EBb  min  a  b    We can easily check that if we aggregate the two pieces of information   a  and        then the resulting base is      f  a   B    where f  a                a  EB          max a      for a     E         Besides  one can require that reinforcement opera tions recover all the common information with a higher  weight  Namely if the same formula is a plausible con sequence of each base  then this formula should be accepted in the fused base with a higher priority  The following proposition shows a first case where this re sult holds  Proposition   Let B  and B  be such that Bt    B  is consistent  Let  be such that B  f rr   a  and B  f          where a and  B are strictly positive  Let EB be a reinforce ment operator  Then  Btf f rr    f    with I   max a  J  if a   J E         and       if a     or          Now  in case of conflicts  and more precisely  in case of a strong conflict  namely Inc B  U B         then the above proposition does not hold  Indeed  let B            P a   and B            P  J    Then we can check that Inc B         so we cannot infer  P from B  since Inc B  UB      Even if we add   P     to B  U B  explicitly then  P can not be recovered  In possibilistic logic  when there is a strong conflict then only tautologies are plausible con sequences  In this case it is better to use a regular disjunctive operation  So the first condition is to avoid that Inc B  UB      But this is not enough since even if Inc B  U B       one can have Inc B      due to the reinforcement effect which can push the priority of conflicting infor mation to the maximal priority allowed  For instance let us consider the excessively optimistic reinforcement operator defined by   Ia  Vb  a     b     a EBb   b EB a      Then we can check that as soon as there is a conflict between the bases to be merged  the inconsistency de gree of the fuses base will reach the maximal value  The following definition focuses on a more interesting class of reinforcement operations         Definition    A reinforce ment operation EB is said to be progressive if  Ia  b     a EBb      The progressive operation guarantees that if some for mula    a  with a    is inferred by the sources then this formula belongs to B  with a weight    such that a    B      However  this new weight    can be less than the inconsistency degree of B  and therefore  will be drowned by the inconsistency of the database  This situation is illustrated by the following example  Example    Let B        V  P                 P               and B       v   If                   If               Clearly  each base entails  which is largely belo w the inconsistency degree of B  U B   Now  let us compute B  EB B  with the product operator which is a progres sive operator  We get  B  EB B    B  U B  U     V  P V           V    fV          V P         V P         V       UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS                      JV         V           J V                  Note that there is a reinforce ment on  since its ne w weight is      which can be obtained for instance fro m  V       and  V         However  this ne w weight is less than the inconsistency degree of Btf  which is of      higher than Inc B  U B         The following proposition generalizes Proposition    and shows that if the inconsistency degree does not increase  then the common knowledge is entailed  Proposition   Let B   and B  be such that Inc B  U B     f    Let  be such that B  f rr    a  and B  f rr        with a      f       Let EB be a pro gressive reinforce ment operation  Then  if Inc Btf    Inc B  U B   then  Btf f rr      with     max  a       and      if a     or f            Adaptive merging operators  The regular disjunctive operators appear to be ap propriate when the sources are completely conflicting  However  in the case of consistency  or of a low level of inconsistency regular disjunctive operators are very cautious  Besides  reinforcement is not appropriate in the case of complete conflicts  The aim of adaptive operators is to have a disjunctive behaviour in a case of complete contradiction and the progressive reinforcement behaviour in the other case  Let EBd and EBr be respectively a regular disjunctive and progressive reinforcement operators  Let h be ei ther equal to   or to    Then we define an adaptive operation  denoted by EBh  as follows  a EBh b   max min  h   a EBd b   min    h   a EBr b      Then we have the following result  Proposition   Let B  and B  be two possibilistic bases  Let h be equal to   if Inc B  UB      and equal to   otherwise  Let EBh be an adaptive operator  If Inc Btf     lnc Bl U B   then  V  if B  f rr    a  and B  f rr        then we have  Btf hf       with              Averaging operators  A last class of merging operators which is worth considering is the so called averaging operation  well known for aggregating preferences  and defined by  is called an averaging operator if max  a b a EBbmin  a  b   with EB   f max and EB   f min   Definition    EB  One example of averaging operators is the arithmetic mean a EBb    In this case  at the syntactic level  the result of combining B  and B  writes       Y   U      Jj        U     V    Jj  a f i     From this writing  in case of consistency we can check        Accounting for reliabilities of the sources  The possibilistic logic framework enables us to take also into account priorities between sources  or agents   Here priority may mean either that the sources are decreasingly ordered according to their re liability  or that a reliability degree is attached to each source  When we have just a reliability ordering and no commensurability assumption is made between the scales used for stratifying each source  the approach which can be used is known in social choice theory un der the name of  dictatorship   The idea is to refine one ranking by the other  More precisely  let rr  and rr  be two possibility distributions  Assume that  r  has priority over  r   The result of combination defined by  i  If  r  w     r  w   then    fll w       fll w   ii  If  r  w       w   then    fll w      fll w   iff  r  w         w    Clearly the combination result is simply the refinement of  r   the dictator  by  r   Syntactic counterpart of this combination can be found in      When a reliability degree is associated with each source  we may use weighted counterparts of oper ations EB  However in practice  it amounts to per forming a preliminary modification of the degrees at tached to formulas provided by each source and then to performing a non weighted combination operation on the modified possibilistic bases  For instance  using the weighted min conjunction defined by Vw   rtf  w    mini l nmax nj w       Aj   for Aj    V j  the min combination is recovered   It amounts to per forming the union of discounted bases of the form       Discount B                         E B  and f        U              E B  and f          It is worth point  ing out that discounting sources help solve conflicts between sources in a natural way     Postulates for classical merging  Let us first introduce some additional notations  Let E  K         Kn   n     be a multi set of propositional bases to be merged  E is called an infor mation set    E  resp  VE  denotes the conjunction  resp  disjunction  of the propositional bases of E  The symbol U denotes the union on multi sets  For the sake of simplicity  if    and     are proposi tional bases and E an information set we simply write EU   and KUK  instead of EU K  and  K U K   re spectively  We will denote   n the multi set  K       K  of size n  A classical merging operator     is a function applied on E and which returns a classical base denoted by      E   Koniesczny and Pino Perez      have proposed a set of    n          UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       basic properties that a merging operator has to satisfy   At  D   E  is consistent   A   If E is consistent  then D  E      E   A   If E  t  E   then   D   E      D  E     A   If K    K  is inconsistent  then D  K UK   If K   A   D  E      D  E     D   E  U E     A   If D   E      D   E   is consistent  then  D  E  U E      D   El     D   E    where E  t  E  means that there exists a bijection f from E     Kt      K  to E     I        Kn such that VK E E     JK  E E   f  K     K    Liberatore       in the context of commutative belief revision  does not impose A   He allows the result to be inconsistent if the bases to merge are individually inconsistent  Moreover  he gives another postulate in the same spirit as A   namely   A   D  I  U I         I  V K    Two classes of merging operators have been partic ulary analysed in the literature  majority operators defined by   Maj  VK    In  D   E U K n     I   and arbitration operators defined by   Arb  VK  Vn D  E UI  n    D  E U K       Postulates for possibilistic merging  This section relates the general classes of possibilis tic merging operations to the rational postulates re called in the previous section  But first we need an adaptation of these postulates in order to take into account the priorities attached to the information  A first immediate way of adapting the classical postu lates is to require that the result of the merging be a classical base  If BtiJ denotes the result of merging B   B       Bn  with EB  then the classical base result ing from merging B   s with EB is simply  D tiJ  B         a  E BtiJ  a   Inc  BtiJ    However  restricting the result of merging prioritized bases to a classical base is not satisfactory  Indeed it leads to lose the associativity property of associa tive operators  The natural question is how to de fine D tiJ  B  D tiJ  B  B    since B  is a stratified base  while D tiJ B   B   is a classical one  One way of en forcing the iteration is to give to formulas of the result ing classical base a weight equal to    However  this may violate the reliability of the formulas since formu las in D  til  B  B   which were very uncertain become fully reliable  The loss of associativity property is il lustrated by the following example  Let B   B   B  B   such that B              B                     and B              Let EB   min  We have D tiJ  B  B         To be able to merge this result with B  we associate to  a weight equal to    and we get D tiJ  D tiJ  Bl  B    B          We also have D tiJ B  B        and Example    D tiJ  Bl D tiJ  B  B            Then  D tiJ  D tiJ Bl B    B       D tiJ B   D tiJ B   B          Adapting classical postulates  We focus on the approach where the result of the merg ing operation is a stratified base  Therefore  the pro cess of merging can be iterated  Let us now adapt the classical postulates recalled in Section    Let us adapt  A    Possibilistic logic  contrary to clas sical logic  does not infer anything in the presence of inconsistency  Hence a partially inconsistent base BtiJ  with Inc  BtiJ      can be still meaningful  since plau sible conclusions can be inferred from it  by taking its consistent part  i e  the set of formulas having a weight greater than Inc  BtiJ   Thus  the adaptation of  A   can be weakened as follows   Pi  BtiJ is not fully inconsistent i e   I nc BtiJ       Note that if one insists on providing a consistent and stratified base as a result of fusion  then the associa tivity can be lost for associative operations  Let us adapt the second postulate  A    Requiring an equivalence between BtiJ and B  U U Bn in the second postulate is very strong with stratified bases  For instance  assume that two identical formulas   a  have to be aggregated  We have already seen that with a reinforcement operator we get     j    j    a  as a re sult of the merging  So  we do not recover the initial weight of   We propose to weaken A  as follow   P   If B  U U Bn is consistent  then BtiJI        j   iff B  U U Bn            with j      and        This postulate implies that if Bi          B is consistent  then Bffi   Bi       B                          Postulates A  and A  have immediate counterparts  Let B   B     Bn  and B    B    B    P   If B t  B   then BtiJ s B tiJ  where B t  B  means that there exists a bijection f from B to B  such that VB E B    IB  E B  f  B  B    P   If B  U B  is inconsistent  then BtiJif   B  and                BtiJif   B    Concerning postulates A  and A   notice that in clas sical logic  when D   E      D   E   is inconsistent then A  is trivially satisfied  Hence A  is only meaningful when there is no conflict between the sources  There fore A  and A  are adapted as follows   P   If BtiJ is consistent with B tiJ  then  P    BtiJUB tiJI    BUB  tiJ   If BtiJ is consistent with B tiJ  then  BUB  tiJI    BtiJUB tiJ    Let us now see how to adapt the postulate A   The common knowledge in the prioritized case can be defined as follows  If B          a  and B          j     then BtiJI             with        Now  the question is how to fix the value of I Ob   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS           viously   should not be greater than min  o       In deed assume that c   a    b  and B       c   o    and B       c          Then it can be checked that  c      is not a consequence of B   which means that  c      can not be considered as a common information of B  and B   Therefore the adaptation of A  is as follows   P   Vc    if B  f rr  c      and B  f rr  c       then BEJ f rr  c       with           min  o       Lastly  arbitration and majority have immediate ex tensions   Arb  VB  Vn   BuB n      s  BUB    and  Let Bt                 and B              Since In c   BEB       the useful information of BEB is            B   Then     is not satisfied  Let now B                   Then         cannot be inferred from BEB  and P  is not satisfied  min is idempotent  then it cannot be a majority op erator   o     Let B    Bt  and B    B   s t  Bt           and B              We have BEBUB EB                and  BuB  EB                 v           V     cannot be inferred from BEBU B  EB  Then  Ps is not satisfied  We also have Bt EB B                     V       and Bt EB B                      v         then Arb is not satisfied    Maj  VB  n   BuB n   f rr B        Properties of the fusion operations  This section gives the properties of the classes of pos sibilistic operators introduced in Section    Proposition   shows that EEl is syntax independent  Proposition    satisfies P    Any possibilistic merging operation  The next proposition relates the property of idempo tency to the idea of arbitration  Proposition    Any idempotent operation is an ar bitration operation   The following proposition gives the properties of the regular disjunctive operations  Proposition    Let EEl be a regular disjunctive oper ator  Then  EEl satisfies P  P  P  P  but may fail to satisfy P  P  Maj Arb    For P   Ps and Maj we use the max which is a regular disjunctive operation   Counter examples  operator EB        P   Ps  Let B   Bt B   with Bt         and B             Although BtU B  is consistent  we have BEB     Vtjl      and we recover neither B  nor B   Then  EB does not satisfy P   It does not satisfy Ps for the same reason     Maj  max is an idempotent operator  hence it is an arbitration operation  and cannot be a majority op erator   The following proposition relates the property of ma jority to the reinforcement property  Proposition    Let B  be a possibilistic base  and B  another possibilistic base which is not conflicting with completely certain formulas of B   Let EEl be a progres sive reinforcement operator  Denote by B the combi nation of B  n times with E   Then   n V     J      E B   B  EEl B f rr     J     with          t     if           This proposition means that reinforcement operators are majority operators  in the sense that if the same piece of information is repeated enough times then this piece of information will be believed  This proposition does not hold if we only use rein forcement operations which are not progressive  For instance  consider the Luckasie wicz t norm defined by  a EEl b   max O a  b      Then  for instance consider the bases B     c         B       c        and B      c        which are not completely conflicting  Then  we can easily check that Inc  B  EEl B  EEl B      and hence B cannot be deduced  Indeed  we have B  EEl B       c         B     B EEl B      c         We now give the properties of progressive reinforce ment operators        Proposition    Let EEl be a progressive reinforcement operator  Then  EEl satisfies P   provided that Inc  B  U U Bn        P  P  P   provided that Inc BEJ    Inc  B  U U Bn  and Inc  B  U U Bn        Maj but may fail to satisfy P  P  Arb    Arb  consider the probabilistic sum defined by  a EBb  a  b  ab  Let B     a    Then  one can easily check that B EBB       a   a    which is different from B     a    For        Let EEl be a conjunctive operator  Then  EEl satisfies P  P  but may fail to satisfy P  P   P   Arb Maj  Proposition     Counter examples    For P  and P   let us use the junctive operator         since it is a con            Counter example  Let us use the product which is a progressive reinforcement operator        Let Bt            and B              The useful information  above the level of inconsistency  of BEB is    min  For P  and Arb  let us consider the product which is a conjunctive operator   Ps   Arb             Bt   Then     is not satisfied   see the counter example of Proposition      The following proposition summarizes the properties of averaging operators    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Proposition    The Averaging operator satisfies P  P   P  P  and Arb but may fail to satisfy P  and Maj   Lastly  the following tree summarizes the considered operators  with the associated satisfied postulates  General operator  P      Conjunctive  P   P        t Idempotent Progressive  Pt   Arb  reinforcement  P   P    Maj        t Regular disjunctive  P     P   P   P      Idempotent Arb   P   P   P   in the possibilistic setting provides a basis for design ing fusion systems able to propose a synthesis of par tially conflicting goals on the basis of some chosen type of combination  possibly taking into account priori ties between agents or sources  Lastly  this paper has analysed the possibilistic merging operators in terms of postulates  A message from Table   is that some postulates which make sense in classical fusion  like A   are not appropriate for merging prioritized bases  Clearly  future work is to study new postulates proper for prioritized bases  
  considered world in case it is a collection of weighted goals   Possibility theory offers either a qualitative  or a numerical framework for representing uncertainty  in terms of dual measures of pos sibility and necessity  This leads to the ex istence of two kinds of possibilistic causal graphs where the conditioning is either based on the minimum  or on the product opera tor  Benferhat et al      have investigated the connections between min based graphs and possibilistic logic bases  made of classi cal formulas weighted in terms of certainty    This paper deals with a more difficult issue  the product based graphical representation of possibilistic bases  which provides an easy structural reading of possibilistic bases   Apart from the syntactic representation provided by a possibilistic base  an other compact representation  sharing the same semantics is of interest  namely  pos sibilistic directed acyclic graphs   DAGs            The merit of the DAG representation is  as usual  to exhibit some independence structure  here in the possibilistic framework   and to provide a structured decomposi tion of the possibility distribution underlying the base  However  the interest of working with different repre sentation modes has been pointed out in several works           Introduction  Possibilistic logic offers a general framework for rep resenting prioritized information by means of classical logical formulas which are associated with weights be longing to a linearly ordered scale  and which are han dled according to the laws of possibility theory        This leads to a stratification of the information base into layers of formulas according to the strength of the associated weights  This framework can be useful for representing knowledge  and then a weight represents the certainty level with which the associated formula is held for true  A possibilistic logic base can also repre sent desires or goals having different levels of priority  A set of weighted logical formulas  constituting a pos sibilistic logic base is semantically equivalent to a possibility distribution which rank orders the possible worlds according to their levels of possibility  These possibility levels are to be understood as plausibility or normality levels in case the base gathers pieces of knowledge  or as levels of satisfaction of reaching a  Depending if we are using a numerical scale such as        or a simple linearly ordered scale  two types of conditioning can be defined in possibility theory  one based on the product which requires a numerical scale  and one based on minimum operation for which any linearly ordered scale fits  This corresponds to quan titative and qualitative possibility theory respectively      In quantitative possibility theory  possibility de grees can be viewed as upper bounds of probabilities      Until recently  quantitative possibility theory did not have any operational semantics strictly speaking  despite an early proposal by Giles      in the setting of upper and lower probabilities  recently taken over by Walley  De Cooman and Aeyels          One way to avoid the measurement problem is to develop a qual itative epistemic possibility theory where only order ing relations are used      For quantitative  subjec tive   possibilities  an operational semantics has been recently proposed            which differs from the up per and lower probabilistic setting proposed by Giles and followers  It is based on the semantics of the trans ferable belief model       itself based on betting odds  It can be shown that the least informative among the belief structures that are compatible with prescribed betting rates is a possibility measure  Then  it can be also proved that the min based idempotent con junctive combination of two possibility measures cor    which evaluates the extent to which   is entailed by the available beliefs   responds to the hyper cautious conjunctive combina tion of the belief functions induced by the possibility measures  The translation of a possibilistic graph  product based  or minimum based  into a possibilistic logic base has been already provided      as well as the con verse transformation for minimum based possibilistic graphs      The translation of a possibilistic logic base into a product based graph  which is less straightfor ward is now addressed in this paper  The transforma tion is illustrated on a running example dealing with the goals of an agent  The paper is organized as follows  After a minimal background on possibility theory  possibilistic logic  and possibilistic graphs  the transformation of a pos sibilistic logic base into a product based possibilistic graph is discussed in details in the rest of the paper     Background           BENFERHAT ET AL   UAI      When dealing with knowledge  a statement  is thus estimated in terms of two measures II and N wh ich enable us to differenciate between the certainty of   cf   N              and the total lack of certainty in    N         When dealing with desires N cf   refers to the imperativeness of goals    while II   estimates how satisfactory is to reach    The definition of conditioning in possibility theory de pends if we use an ordinal  or a numerical scale  In an ordinal setting  min based conditioning is used and is defined as follows  if II  II t      II cp  if Il cfJ II t J    II cfJ  In a numerical setting  the product based conditioning is used  IT       jJ  II  lx      II       Moreover  ifiT cf        then II     I   IT     p I    Possibility theory  Let  be a finite propositionnal language    is the set of all classical interpretations  Greeck letters            denote formulas  The notation w f    means that w is a model of    A possibility distribution      r is a function mapping a set of interpretations  or worlds  n into a linearly ordered scale  usually the interval          r w  rep resents the degree of compatibility of the interpreta tion w with the available beliefs about the real world in a case of uncertain information  or the satisfaction degree of reaching a state w  when modelling prefer ences  rr w      means that it is totally possible for w to be the real world  or that w is fully satisfactory       rr w      means that w is only somewhat pos sible  or satisfactory   while  r w      means that w is certainly not the real world  or not satisfactory at all   A possibility distribution is said to be normalized  or consistent  if  w s t   r w        Only normalized distributions are considered here  Given a possibility distribution  r  two dual measures are defined which rank order the formulas of the lan guage     The possibility  or consistency  measure of a for mula   II          max  r w    w F     which evaluates the extent to which is consistent with the available beliefs expressed by  r     The necessity  or certainty  measure of a formula   N       IT              Both forms of conditioning satisfy an equation of the form  II         D IT  I     II      which is similar to Bayesian conditioning  for     min or product  In this paper we privilege the numerical setting  It is clear that  r  w     r w I    the result of con ditioning a possibility distribution  r with  is always normalized       Possibilistic knowledge bases  A possibilistic knowledge base is a set of weighted for   i      n  where cp  is a classical formula and  a   belongs to        in a nu merical setting and represents the level of certainty or priority attached to cf    mulas of the form      a     Given a possibilistic base I   we can generate a unique possibility distribution rr  where interpretations will be ranked w r t  the highest formula that they falsify  namely      Definition           Vw E      rE w      if      a  EI  w       max a   cfJ  a  EI  and w   otherwise   Example    Let  su   wi  se be  three  symbols  which  stand for  sun    wind  and   sea  respectively  Let I  be the following possibilistic base    su V   wi        wi V se   su V se       I         wi V   se           BENFERHAT ET AL   These rules express the goals of somebody who likes basking in the sun  or going windsurfing  which re quires wind and sea   The most prioritized formula expresses that the person strongly dislikes to have wind in a non suny day  while the other less prioritary goals express that she dislikes situations with wind but with out sea  or with sea without wind  or with neither sun nor sea  Let n    wo   su    wi    se  wl   su    Wi    s e   w    su    wi    se  ws   su    wi    se  w      su    wi      se  w    su      wi    se  w       su    wi    se  W    su    wi    se   Let  rE be the possibility distribution associated with  E   rE wo     rE ws        rE w      rE w      rE w      rE ws     and  rE w      rE W        The converse transformation from forward   Let             ent weights used in      r        r  to    f n           E  is straight  be the differ  Let r J  be a classical formula  wl wse models are those having the weight     in E          rp              i        n    Then    r   Let  We now give further definitions which will be used lat  B   letter  A  we denote a variable which represents either  the symbol  a or its negation   An interpretation in this  section will be simply denoted by  Let  E be a possibilistic knowledge base  and a E         We call the a cut  resp  strict a cut  of E  denoted by  E a  resp   E a   the set of classical formulas in  E havi g a certainty degree at least equal to a  resp  strictly greater than a   is inconsistent  denotes the  inconsistency degree of  E  When  Inc   E         E  is consistent  we      An   or by  w   Uncertainty is expressed at each node in the follow ing way    For root nodes  A    namely  vide the prior possibility of  a   Par Ai        we pro  and of its negation    a    These priors should satisfy the normalization condi tion     For other nodes of  aj  Aj  we provide conditional possibility  and of its negation   ai given any complete in  stantiation of each variable of parents of  Aj  wPar Ai    These conditional possibilities should also satisfy the normalization condition   A product based possibilistic graph  denoted by  lTG   chain rule  Definition    Let lTG be a product based possibilistic graph  The joint possibility distribution associated with II G is computed with the following equation  called chain rule   rr w     II  a I WPar A     w f  a and w f  WPar A    where     product   The converse transformation from a possibility distri  Subsumption can now be defined   bution  Let  rp  a  be a formula in E  Then   rp o   is said to be subsumed in E if   E    rp a  h   f rp   rp  a  is said to be strictly subsumed in  E if E a f  r J   Definition    Indeed  we have the following proposition        Let  E be a possibilistic knowledge base  and  rp  a  be a subsumed formula in  E  Let  E     E       o     Then   E and  E  are equivalent I  e   rE    rE    Proposition     r  to a product based graph is straightforward   Indeed  given an interpretation  w   A A  An   we  have   Applying repeatedly the Bayesian like rule for an arbi  A    An between variables  we get   r A  An    r A  I A  An    r A  I A  An     r An   I An    r An   trarily ordering  This decomposition is possible since the product is as sociative  The decomposition leads to a causal possi bilistic graph where parent of       A    induces a unique joint distribution using a so called  Definition    have  A to A is said to be a parent of B  The set of parents of a given node A is denoted by Par A   By the capital  between variables  When there exists a link from   rE    r   ter in the paper   Inc  E   max a     Ea   UAI      A   are   Ai l   An   It is clear that for the same joint distribution  we can  Possibilistic networks  construct several causal possibilistic graphs depending  Another possibilistic representation framework of un  on the choice of the ordering between variables   certain information is graphical and is based on con  In possibility theory  there are several definitions of in  ditioning  Information is then represented by possi  dependence relations  see e g   bilistic DAGs  Directed Acyclic Graphs   since we deal with product based conditioning  we will           where              In this paper   nodes represent variables  in this paper  we assume  use the following definition of independence  namely   that they are binary   and edges express influence links  rp and    J are independent if II     J  I rp    II     J     UAI         BENFERHAT ET AL   Fro m possibilistic bases to     Computation of the marginalized  product based graphs   Basic ideas  In      the authors have provided the transformation of possibilistic graphs to possibilistic bases  In      the transformation from    to a min based graph  where conditioning is based on minimum operation rather than on the product  has been given  In the following  we provide the transformation from a possibilistic base    to a product based graph IIG  This transformation is different from the transformation in case of conditioning based on the minimum operation  However there is one common step in both approaches which consists in putting the knowledge base into a clausal form and in removing tautologies  The following proposition shows how to put the base in a clausal form      Let    be a possibilistic base  Let   a  be a formula in     and         n   be the set of clauses encoding   We define E  obtained by replac ing each formula   cr   in E by      a      n a    Then     and E  are equivalent i e    rE    rE        base c  The computation of Ec involves three tasks    decomposing a possibility distributions rr into its restriction on a  and its negation      a   recall that at and     at are the two only instances of At      marginalisation of these two distributions for get ting rid of At     the effective computation of       Decomposing  The basic idea in the transformation from the knowl edge base to a possibilistic graph is first to fix an ar bitrarily ordering of variables A       An  This order ing means that the parents of A  should be among A  l     An  however it can be empty   Then we proceed by successive decompositions of     which is associated with the above decomposition of a possibil ity distribution rr  namely    rr  Let us first define two possibility distributions rra  and rr  a  in the following way  rr  a            w  Proposition    The removing of tautologies is important since it avoids fictitious dependence relations between vari ables  For example  the tautological formula       x V     y V x     might induce a link between X andY   Ec        a   w        rr w   if w f  a     rr w   ifw f    at  otherwise         otherwise    ra   resp  rr  aJ are very similar to conditioning ex cept that we do not normalize after learning at  resp    at      a  and rr    a  are simply the decomposition of rr  Indeed  it can be checked that   r w      max    a   w        a   w     Vw   Example    We consider again the possibilistic base of Example    Let us compute rre and           We get  E   l se wo   rse w          l se w      rse wt    Also   rr     e wt   rr   e w         e wo          l se w    l se ws         se w         and      e w            l      e w      l       e ws        e w    and   rr   e w    l      se w                                         Then  we can check that  The result of each step i is a knowledge base associated with rr A  An   Moreover  this resulting base will enable us to determine Par A   the set of parents of A  and to compute the conditional possibilities rr A  I P a r  A       In the following  we will only consider the first step  i       and we will denote by Ec  C for Current  the base associated with rr A  An   The procedure of this first step can be then iterated at each step  Ec is called the marginalized base  The computation of Ec is provided in the next section  then in Section   we show how to determine parents of each node  and lastly in Section   we compute the conditional possibility degrees    Vw   T E w   rE    max  r   w           w   where is the possibility distribution associated with E   We can easily check that the possibilitic bases associ ated with these distributions are given by  Proposition   rra    resp   The possibilistic base associated with        a   is l  U     a t       resp  EU      a          rr  Proof  The proof is obvious  Indeed  we have two cases    w p   a   then using Definition   l Eu  a      w        max   a        a         since w p   al   E         U   at       w p    Pi    BENFERHAT ET AL         w f  at   then using Definition     ru  a  l   w        max a         a     E I  U   at  l   w           max  a        a   E I  w       since w f  at       rE w   The two following lemmas give two simplifications of  I  U   at        resp  L U     a           Lemma   Let I    E U   a        Let I J     E    at V x  a     a V x a  E E  a base obtained from E  by removing clauses containing at  Then  I   and EJ   are equivalent  in the sense that they generate the same possibility distribution which is  ra  The proof is obvious since   a         at V x  a   is subsumed by  Let E    EU  at       Let LJ   be the possi bilistic base obtained from E  by replacing each clause of the form   at V x a    by  x a       Then  E  and I J   are equivalent   Lemma    The proof can be again easily checked  Indeed    a         at V x  a    implies  x  a    which can be added t  L And once  x o   is added    at V x  o   can be retrieved since it is subsumed by  x  a      and  to       Marginalisation  In this section  we are interested in computing the    resp       atl defined  A      An  Let us denote La   resp  L  a   the result of the ap plication of Lemmas   and   to I  U   at        resp  E U   at         namely the result of removing clauses of the form  x V at  a  from I  and the result of replac ing  x V  a t  a     by  x  a   Therefore  the only clause in I a  which contains a  is  at      Let us denote  r      r the result of marginalisation of  ra  and      a  on  A    An   namely   r    A  An   IIa   A  An   resp       A   An   II    a  A   An    marginal distributions from  ra   on  Let At   SE  Then   reE    su A  l eE    su A wi       reE  su A   wi    wi      and  reE  su A wi       Also   rfe   suAwi       rfe suAwi     rfe   suA  wi     and  rfe su A   wi       Example    NB  ITa   A  An   is the possibility measure associ  ated with  ra  defined on   At  An   The following lemma provides the syntactic counterA   part of  ra     resp  rA a         UAI        The possibilistic base associated with  rA a   resp   r   ts Ea     at       resp  E  a       at         Lemma   A        Proof The proof is obvious  First  note that the only clause  at in Ea  is  a       Then   r     A  An    min   a        a   E Ea     a        A  An     min  I  a         a      E Ea     at       atA  An       max min    a      I   a    E Ea   atA  An    min  I  a           a   E La    atA  An     since min l  a          a   E I a   a A  An  c      D   max  ra   atA  An    l a   atA  An       l a   A  An   containing       Effective computation of  Given Lemma     I c  we are now able to provide the possi  bilistic base associated with   r A   An   by noticing  that    r A  An    max  r    A     An        A    An    Example    We again consider the possibility distri butions  r eE and rrfe computed in Example    We have   r su A   wi    rr su A wi        r     su A   wi     and  r    su    wi     Let Et   Ea      at      and I           L  a    at     The possibilistic base associated with  r A    An  is   Ec        V    Jj  min a      j     I   a   E Et    I Jj     j  E E     Proposition    Proof The proof is obvious  indeed       max min a    j         a    t r c w   Et   I Jj   j  E L    w    V I Jj       min max a       a       E Et W      max f J    I Jj   j  E E  w    Jj    max    max a          a      E  Et w        max fJJ     I Jj   j  E E   w     JJ       max rr     w   rr  w         E  Summary  Let us now summarize the computation of Ec      Add  a        resp    at      to E       at  resp    at   a    resp    at V x a     Remove clauses containing form   a   V  x   of the   BENFERHAT ET AL   UAI          Let Ec  Replace clauses of the form   at E   V x  a   by   resp    x  a    E   the     at V x  a    resp   whose observation influences the conditional possibil ity  TI A  I Par At           Then      o   E Et    at          P  B  E E         at        Let us consider our example again  Let  SE  WI  SU  be the ordering of the variables  namely At   SE  A    WI  and A    SU   We start with the variable SE  Let us compute Ec the possibilistic base associated with  rB A A    namely     E WI A SU    We first add  se  with a degree    we get   Example      wi V   se k     wi V se k   su V se    su V   wi       se         Then we remove clauses containing se  except the added one   we get   wi V   se  k     su V     wi       se       Then we replace all clauses of the form     V se  a  by      o    we get  E e      wi  k     su V   wi  j    se       Similarly  for   se  E    u       wi  k    su  k    su V   wi        se      Finally  using Proposition   we have  Et      wi       su V   wi      E         wi       su      su V   wi      and Ec     su      su V    wi      It can be checked that    E  WI   SU        WI   SU   where     is the possibility distribution computed in Ex ample     Let E     a  V a t         a         then we can easily check in context   a   at is deduced to a degree     which means that TI    at I   a                However  in context      a     a  the certainty of a  is now o  which means that n     ar     a   a         o   l due to presence of a conflict in E U    a         a     Hence A  should also be considered as a parent of At  even if it is not directly involved   see     for the presentation of the inference machinery in possibilistic logic   Example    Algorithm   provides the computation of  Data             Finding immediate parents of At  LA         a        a   E E at or   at   l b  Par At      V       o    E E A  l a   and    contains either  containing an instance of  sical case  without levels of priorities  Both procedures are polynomial   V      Checking for hidden parents   a  B    E  let  xt     Xn  be an instance of  Par At    b  Remove from Beach clause containing  x      c  Replace in B e ach clause of the form        x  V    Xj   V      xk V  P o   by   P o   o   resp      be the certainty degree of at  resp    at  from BU  xt            xn          e  if there exists         E B such that      o   resp            then  Par Al    P a r  At  U   V     contains an instance of V    d  Let  Their ap  to compute the marginalization base  Another small  At  begin  proach is more based on successive resolution in order  difference is that their approach is proposed in the clas  E  Result  Parents of  The problem of finding a syntactic counterpart of marginalization problem addressed in  Par A     Algorithm    Determining Parents oLAt  the marginalization process is clearly similar to the     To show this  let us give the  following illustration   result of the Step        V   J  min a            Go to Step   return  Par At   end Let us briefly explain this algorithm  The first step  Determining parents of A   simply starts with parents of  At  the set of variables At  Step   checks for At  namely if Pa r  At  can be ex  which are directly linked with We are interested in determining the parents of  Par A    which are  This set should be such that   At   TI At I A  An     TI At I Par Al    At are determined  we compute the conditional possibility degrees TI At I Par At   in Sec tion     The determination of parents of At is done in an incre mental way  First  we take Par At  as the set of vari       Once parents of  ables which are directly involved at least in one clause containing  at or    a   Par A   are  obvious parents of  At  However  it may exist other  hidden  variables   hidden parents of tended or not  A set of variables  V  has to be added to  there exists an instance that   resp  where   x      xn    for  Par At   if Par At  such  II at I X t X    Xn  f  TI at I XtX  Xnv   TI   at I XtX    Xn   j   II    at I XtX   xnv      v  is an instance of  V   The computation of  II   at I Xt Xn   can be done syntactically from E U   xt         xn       for a formal computation of IJ      at I XtXn  see Section         BENFERHAT ET AL       This is what it is done in Step    by checking if any additional variables can have influences on II at I Xt  xn  To achieve this goal  we first as sume that  x            xn     are true  Step   b re moves  x  V     a  since the latter are subsumed  Step   c replaces    x  V   V   xk V     a  by      a  since    x  VV  xk V    a  and   x       xk     im plies      a  which subsumes    x  V   V   xk V     a       Now  assume that B U   xi         xn          at  a   Step   d and   e   Let      J  E B  such that J   a  Then  variables which are in   should be added to Par Al   Indeed  let     v  V  Vvn  Then  one can easily check that  II at I Xt   Xn  V    vn    IT a  I Xt  Xn   because B U     x i                vi          vn      is in consistent to a degree    J from which a  can no longer be inferred         UAl      Example       c  se      Hence  II    se       f     wi      su        se   se      n             II          Inc L  U   ql         We recall that L  is assumed to be consistent    Proposition    The proof is immediate since II ql      N     q    and that N   ql    Inc L  U           See       Therefore  to compute II at I Xt  Xn         Add   x            xn      to  E  Let sult of this step      Add   a       to L    Let step   h    II atXt  Xn    Then  II  al I Xt     Xn         L    be the re  Inc L      h represents     su             wz s u         wz  su      WZSU         wi        s  u       is the same as the one associated with the possibilistic  This subsection shows how to compute II A  Par Al   once Par At  is fixed  Let  x     xn  be an instance of Par At   and a  an instance of A  Recall that by definition  n a  r    z II a  I XtX      X n     TI  r z   and that II  at I XtX    Xn      if II xt Xn       The following proposition provides the computation of II ql  syntactically          su  It can be checked that the possibility distribution asso ciated with the constructed    G  using the chain rule   possibility degrees  Compute  Lastly  II   se  II SE I WI  SU      Computation of conditional         IT W IISU   w     h          wz     II x     Xn            Let us consider again the base L       suV     wi V se      wi V   se      su V se          Co mput e  su  l With a similar way  we get the  IT SU   Then  using the above algorithm we get  Par SE    WI  SU   Par W I     SU  and Par SU            following conditional possibilities   Example        wi           wi    continued   Let us illustrate the computation of IT   se I wi    su   We add the instance   wi l   su l   toE  Step     We get L     wi       su      suV  wi i    suVse        wi V se     wi V   se      We have In   L         Then  h     Step     We now add    se     to L    Step     We get L       se        wi       su       su V   wi  i    su V se        wi V     wi V   se      We have Inc L       Then  h    L    I     be the result of this  nc L     h  represents    If  ifh O  otherwise   base  Observe that we have chosen the ordering of the vari ables in the example arbitrarily  Clearly each appli cation suggests orderings which are the more natural ones  or which leads to a simple structure  Note also that the computation of the weights could be also han dled symbolically     Conclusion  In the possibility theory framework  desires or knowl edge can be equivalenty expressed in different formats  This paper has used two compact representations of possibility distribution  a possibilistic knowledge base and possibilistic graph  Each of these representations have been shown in pre vious papers        to be equivalent to a possibility distribution which rank orders the possible worlds ac cording to their level of plausibility  The framework may use a symbolic discrete linearly ordered scale  or can as well be interfaced with numerical settings by us ing the unit interval as a scale  using a different type of conditioning in each case      This paper is on step further in establishing the rela tionship between different compact representation of possibility distribution  The results presented in this paper would enable us also to translate a possibilistic logic base easily into a kappa function graph      since   UAI      BENFERHAT ET AL   there exists direct transformations     between possi bility theory and Spohn s ordinal conditional functions       
