 The Factored Frontier  FF  algorithm is a simple approximate inference algorithm for Dynamic Bayesian Networks  DBNs   It is very similar to the fully factorized version of the Boyen Koller  BK  algorithm  but in stead of doing an exact update at every step followed by marginalisation  projection   it always works with factored distributions  Hence it can be applied to models for which the exact update step is intractable  We show that FF is equivalent to  one iteration of  loopy belief propagation  LBP  on the origi nal DBN  and that BK is equivalent  to one iteration of  LBP on a DBN where we clus ter some of the nodes  We then show em pirically that by iterating more than once  LBP can improve on the accuracy of both FF and BK  We compare these algorithms on two real world DBNs  the first is a model of a water treatment plant  and the second is a coupled HMM  used to model freeway traffic     Introduction  Dynamic Bayesian Networks  DBNs  are directed graphical models of stochastic processes  They gener alise hidden Markov models  HMMs  by representing the hidden  and observed  state in terms of state vari ables  which can have complex interdependencies  The graphical structure provides an easy way to specify these conditional independencies  and hence to pro vide a compact parameterization of the modeL See Figure   for some examples  In this paper  we will be concerned with the task of offline probabilistic inference in DBNs  i e   computing P Xfly  r  fort             T and i           N  where Xi is the i th hidden node at timet  and Yt is evidence vector at time t  this is often called  smoothing   We    will assume that all the hidden nodes are discrete and each has Q possible values  The observed nodes can be discrete or continuous  The simplest way to perform exact inference in a DBN is to convert the model to an HMM and ap ply the forwards backwards algorithm       This takes O TQ N  time  By exploiting the conditional inde pendencies within a slice  it is possible to reduce this to fl TNQN F  time  where F is the maximum fan in of any node  Unfortunately  this is still exponential in N  In fact  this is nearly always the case  assuming the graph is connected   because even if there is no direct connection between two nodes in the same or neighboring  time slices   they will become correlated over time by virtue of sharing common influences in the past  Hence  unlike the case for static networks  we need to use approximations even for  sparse  mod els  In Section      we present a new approximation  called the  factored frontier   FF  algorithm  which repre sents the belief state as a product of marginals  The FF algorithm is thus very similar to the fully fac torized version of the Boyen Koller  BK  algorithm  which we summarise in Section      FF  however  is a more aggressive approximation  and can therefore be applied when even BK is intractable  FF will always take O TNQF l  time  whereas BK can take more  depending on the graph  In Section    we show how both FF and BK are related to loopy belief propagation  LBP                          which is the method of applying Pearl s message pass ing algorithm      to a Bayes net even if it contains  undirected  cycles or loops  In Section    we exper imentally compare all four algorithms   exact  FF  BK   and LBP   on a number of problems  and in Section    we conclude    UAI       MURPHY    WEISS            The frontier algorithm  If Xt is a vector of N hidden nodes  each with Q pos sible values  then X can be in S QN possible states  so the FB algorithm becomes intractable  The fron tier algorithm      is a way of computing a  from O  t l  and similarly for the     s  without needing to form the QN x QN transition matrix  yet alone multiply by it     a Figure    Some chains and T  b DBNs   a  A coupled HMM with  N          timeslices  Clear nodes are hidden  shaded nodes are observed  In the freeway traffic appli cation in Section    Xl represents the hidden traffic sta tus  free flowing or congested  at location i on the free way at time t  this is assumed to generate a local noisy measurement of traffic speed  Yt   and to depend on its previous state and the previous state of its upstream and downstream neighbors   b  A DBN designed to monitor a waste water treatement plant  This model is originally from      and was modified by     to include  discrete  evi dence nodes        Exact inference  We start by reviewing the forwards backwards  FB  algorithm      for HMMs  and then the frontier algo rithm      for DBNs  since this will form the basis of our generalisation   The basic idea is to  sweep  a Markov blanket across the DBN  first forwards and then backwards  We shall call the nodes in the Markov blanket the  frontier set   and denote it by  F  the nodes to the left and right of the frontier will be denoted by  and  R  At every step of the algorithm   F d separates    and  R  We will maintain a joint distribution over the nodes in  F  We can advance the frontier from slice t     to t as follows  We move a node from R to  F as soon as all its parents are in  F  To keep the frontier as small as possible  we move a node from  F to  as soon as all its children are in  F  Adding a node entails multiplying its conditional probability table  CPT  P  Xf iPa Xf    onto the frontier  and removing a node entails marginalising it out of the frontier  This is best explained by example  see Figure     Con sider the coupled HMM  CHMM  shown in Figure    The frontier initially contains all the nodes in slice t    Ft o r O  t     P Xl     YIY  t    We then advance the frontier by moving Xl from R to  F  To do this  we multiply in its CPT P XllXl    Xf      Ft l     P Xf  Xf  lfiYu d   P XfiXf    Xf    x Ft o  Next we add in      Ft     P Xf    X   fiYl t d P X txf   x    Xf     The forwards backwards algorithm  The basic idea of the FB algorithm is to compute d f def P X t   t   Y  t   m the orwards pass   Bit  e ati   P Yt l rlXt   i  in the backwards pass  and then to combine them to produce the final answer   Yt def   P Xt i Yl T  ex af   Let M i j   P Xt l   j Xt   i  be the transition matrix  and Wt i i  f P yt Xt   i  be a diagonal matrix containing the con ditional likelihood of the evidence at time t  The algorithm is just repated matrix vector multiplica tion  Specifically  in the forwards pass we compute at ex WtMT O  t    and in the backwards pass we com   pute f t ex MWt    t l  the constants of proportional ity are simply the normalizing constants  The bound ary conditions are a    W  l  and f r      where  l   d f P X    i  is the prior  If X can be inS possible states  the FB algorithm clearly takes O S T  time       Xf  x  Ft l  Now all of the nodes that depend on Xl   are in the frontier  so we can marginalize Xl   out  move it from  F to    F t        X   P x t  L      Ft   t  NIY l t            The process continues in this way until we compute  Finally  we weight this factor by the likelihood   O t   P Xf NIYl t  oc P yt Xf N   x  Ft N  It is clear that in this example  exact inference takes O TNQN    time and space  since the frontier never        MURPHY    WEISS  UA                                                  add Xl t            add X  t                  removeXl t          remove  add XJ I   X  t          Figure    The frontier algorithm applied to a CHMM  observed leaves are omitted for clarity  Nodes inside the box are in the frontier  The node being operated on is shown shaded  only connections with its parents and children are shown  other arcs are omitted for clarity  See text for details    contains more than N     nodes  and it takes O N  steps to sweep the frontier from t   to t  In general  the running time of the frontier algorithm is exponen tial in the size of the largest frontier  this quantity is also known as the induced width of the underlying or moral graph  We would therefore like to keep the frontiers as small as possible  Unfortunately  comput ing an order in which to add and remove nodes so as to minimize the sum of the frontier sizes is equivalent to finding an optimal elimination ordering  which is known to be NP hard  Nevertheless  heuristics meth ods  such as greedy search       often perform as well as exhaustive search using branch and bound          A special case of the frontier algorithm  applied to fac torial HMMs  was published in Appendix B of       In an FHMM  there are no cross links between the hidden nodes  so there are no constraints on the order in which nodes are added to or removed from the frontier   For regular  DBNs  the frontier algorithm is equivalent to the junction tree algorithm             applied to the  unrolled  DEN  In particular  the frontier sets cor respond to the maximal cliques in the moralized  tri angulated graph  in the junction tree  these cliques are connected together in a chain  possibly with some smaller cliques  hanging off the backbone  to accomo date the non persistent observed leaves  Despite this equivalence to junction tree  the frontier algorithm is   A regular DBN has certain restrictions on its topol ogy  Let Ht denote all the hidden nodes in time slice t  and    all the observed nodes  A regular DBN can have connections from H  to    and to Ht    but to nowhere else  In particular  there cannot be any intra slice connec tions within the H  nodes  Furthermore  we assume each node in Ht connects to one or more nodes in Ht l  i e   is persistent   All the DENs in this paper are regular  The frontier algorithm works for non regular DBNs  but it may be less efficient that junction tree in this case  The factored frontier and loopy belief propagation algorithms also work for non regular DENs   appealingly simple  and will form the basis of the ap proximation algorithm discussed in the next section         Approximate inference The factored frontier algorithm  The problem with the frontier and junction tree algo rithms is that they need exponential space just to rep resent the belief states  and hence need at least that much time to compute them  The idea of the fac tored frontier  FF  algorithm is to approximate the belief state with a product of marginals  P Xt IYu   f   P XfiYt t    The backward messages f t are ap proximated in a similar way   The algorithm proceeds as follows  when we add a node to the frontier  we multiply its CPT by the prod uct of the factors corresponding to its parents  this creates a joint distribution for this family  We then immediately marginalize out the parent nodes  The backwards pass is analogous  This is like the frontier algorithm except that we always maintain the joint dis tribution over the frontier nodes in factored form  This algorithm clearly takes O TNQF l  time  no matter what the topology       The Boyen Koller algorithm  The Boyen Koller algorithm     represents the belief state  O t   P Xt Yt t   as a product of marginals over C  clusters   P Xt IYI t   TI l P  XfiYl t   whe re Xf is a subset of the variables  Xi    The clusters do not need to be disjoint   Given a factored prior  O t    we do one step of exact Bayesian updating to compute the posterior  Ot  In general  Ot will not be factored as above  so we need to project to the space of factored distributions by computing the marginal on each clus    MURPHY    UAI       WEISS                            oi    o  l  a                       a   Figure    Illustration of the clustering process   a  This is a modified version of a CHMM with   chains  The big  mega no es  con ain the joint distribution on the whole slice  We have omitted the observed leaves for clarity  LBP apphed to th s graph is equivalent to BK   b  This is like  a   except we have created overlapping clusters of size    for additional accuracy  ter  The product of these marginals then gives the ap proximate posterior  Ext  We can use a similar method for computing the backward messages in an efficient manner      Boyen and Koller prove  roughly speak ing  that if the error introduced by the projection step isn t much greater than the error incurred by using an approximate prior  both errors relative to the true  uncomputable  distribution  then the overall error is bounded   the previous slice  but not on its neighbors within a slice    the largest clique has size n  and hence the running time of BK is O T NQYN   even in the fully factorized case   The accuracy of the BK algorithm depends on the size of the clusters that we use to approximate the belief state  Exact inference corresponds to using a single cluster  containing all the hidden variables in a time slice  The most aggressive approximation corresponds to using N clusters  one per variable  we call this the  fully factorized  approximation   Pearl s belief propagation algorithm      is a way of computing exact marginal posterior probabilities in graphs with no undirected cycles  loops   Essentially it generalises the forwards backwards algorithm to trees  W hen applied to a graph with loops  the algorithm is sometimes called  loopy belief propagation   LBP   in this case  the resulting  posteriors  may not be cor rect  and can even oscillate  Nevertheless  the out standing empirical success of turbo decoding  which has be shown to be equivalent to LBP        has cre ated great interest in the algorithm   It is clear that the fully factorized version of BK is very similar to the FF algorithm  but there is one im portant difference  BK assumes that we update the factored prior exactly  using  say  junction tree  be fore computing the marginals  whereas FF computes the  approximate  marginals directly  BK is obviously more accurate than FF  but sometimes it cannot be used  because even one step of exact updating is too expensive       The cost of using BK is determined by the size of the maximal cliques of the moralized  triangulated ver sion of the two slice DBN   Unrolling the DBN for many slices induces long distance correlations  and re sults in cliques that span the whole time slice  as we saw above   For the coupled HMM  CHMM  model in Figure    the cliques just correspond to the fami lies  nodes and their parents   so the algorithm takes O T NQF l  time  the same as FF  But for the wa ter model  see Figure     we also get extra  non local  cliques due to triangulation  For more complex mod els  such as the  D generalisation of a CHMM  where each time slice is now an N   n x n lattice  and each cell depends on all the nodes in its  receptive field  in     BK and FF as special cases of loopy belief propagation  LBP has been empirically shown to work well on sev eral kinds of Bayesian networks which are quite differ ent from turbo codes          In addition  a number of theoretical results have now been proved for networks in which all nodes are Gaussian       for networks in which there is only a single loop       and for general networks but using the max product  Viterbi  version instead of the sum product  forwards backwards  ver sion of the algorithm      The key assumption in LBP is that the messages com ing into a node are independent  But this is exactly the same assumption that we make in the FF algorithm  Indeed  we can show that both algorithms are equiv alent if we use a specific order in which to send mes sages  Normally we implement LBP using a decentral ized message passing protocol  in which  at each step  every node computes its own     and     in parallel  based on the incoming message at the previous step   and then sends out     and  l  messages to all its neighbors  However  we can also imagine a forwards backwards   MURPHY   WEISS        FB  protocol  in which each node first sends  r  a  messages from left to right  and then sends      B  mes sages from right to left  A single pass of this FB pro tocol is equivalent to FF   The fixed points of LBP are the same  no matter what protocol is used  If there is not a unique fixed point  the algorithms may end up at different answers  They can also have different behavior in the short term  In particular  if the D BN is in fact an HMM  then a single FB iteration   TN message computations  will result in the exact posteriors  whereas it requires T iterations of the decentralized protocol  each iteration comput ing  TN messages in parallel  to reach the same result  hence the centralized algorithm is more efficient       For loopy graphs  it is not clear which protocol is bet ter  it depends on whether local or global information is more important for computing the posteriors  In this paper  we use the centralized  FB  protocol  It is also easy to see that the fully factorized version of BK is equivalent to a single FB pass of LBP applied to a modified DBN  as shown in Figure    Fo r each slice  we create two  mega nodes  that contains all the  hidden  nodes in that slice  The messages corn ing into the first mega node are assumed independent  they are then multiplied together to form the  approx imate  prior   a single message is then sent to the sec ond mega node  corresponding to an exact update step using the QN x QN transition matrix  finally  the indi vidual marginals are computed  and the process is re peated  Of course  BK does not actually construct the mega nodes  and does the exact update using junction tree  but the two algorithms are functionally equiva lent  To simulate BK when the clusters contain more than one node  we simply create new clustered nodes  in addition to the mega nodes  and run LBP on the new graph  as illustrated in Figure    Since FF and BK are equivalent to one iteration of LBP  on the regular and clustered graphs respectively  we can improve on both of these algorithms by iter ating more than once  This gives the algorithm the opportunity to  recover  from its incorrect indepen dence assumptions  We will see in the Section   that even a small number of iterations can help dramati cally     In the case of noisy or nodes  there are efficient ways to  r messages without having to do work  compute the   and  which is exponential in the number of parents       This reduces the overall complexity of FF from  O TNFQ    For  a  O TNQF l   directe d graph  naive Pearl would take  to  O QN   time to compute  r for the mega node  but we can do this in tim e by exploiting the fact that the CPT factorizes   O QN      Alternatively  we can use an undirected graph in which the computation of messages always takes time linear in  the number  of neighbors        A free  UAI      energy for iterated  BK  BK and a single iteration of on the clustered graph allows us to utilize the re cent result of Yedidia et al      to obtain a free energy for  iterated  BK  We define the  iterated  BK algo rithm as running LBP on the clustered graph using a FB schedule until convergence  The first iteration of iterated BK is equivalent to BK but in subsequent it erations  the a and    messages interact to improve the quality of approximation  The analysis of      shows that iterated BK can only converge to zero gradient points of the Bethe free energy  The equivalence between LBP  This sheds light over the relationship between iterated BK and the mean field  MF  approximation  The MF free energy is the same as the iterated BK free en ergy when joint distributions over pairs of nodes are replaced by a product of marginal beliefs over individ ual nodes  iterated BK captures dependencies between nodes in subsequent slices while MF does not  While this result only holds for iterated BK  ordinary BK can be thought of as a first approximation to iterated BK     Experimental results  In this section  we compare the BK algorithm with k iterations of LBP on the original graph  using the FB protocol  k     iteration corresponds to FF   We used a CHMM model with    chains trained on some real freeway traffic data using exact EM         Q N Ls l We define the Lt error as Dot Li t JP X    sJYt T   F Xf   s Yt r J  where P   is the exact pos terior and F    is the approximate posterior  In Fig ure    we plot this against t for     iterations of LBP  Clearly  the posteriors are oscillating  and this hap pens on many sequences with this model  We there fore used the damping trick described in       In this case  each new message is defined to be a convex com bination of the usual expression and the old messsage  with weight J L given to the old message  Hence J L     corresponds to undamped propagation  and J L     cor responds to not updating the messages at all  i e   only using local evidence  It is easy to show that any fixed points reached using this algorithm are fixed points of the original set of  undamped  equations  It is clear from Figure   that damping helps considerably  The results are summarised in F igure    where we see that after a small number of iterations  LBP with J L       is doing better than BK  Other sequences give similar behavior     To check that these results are not specific to this model  data set  we also compared the algorithms on the water DBN shown in Figure    We generated ob servation sequences of length     from this model us    MURPHY   WEISS  UAI         x      g  Q    i       I  l       Jl    iter       x           m       I                         x        i      H            iter       g  Q    i             iter            iter  Figure      error on marginal posteriors vs  timeslice after iterations     of undamped traffic  CHMM   LBP  applied to the  The   error oscillates with a period of    as seen by the similarity between the graphs for  iterations     and       this implies that the underlying marginals are oscillating with the same period            x         iter     Q    i        Figure traffic     iterations CHMM            iter  g   g      i  X  Q    i  I  I             and    of     iter  LBP       d  l                   BK            with damping factor f l        and after using   iteration of BK  on the        MURPHY   WEISS                 I I                    x                                            e Q       i                                        iterations  error incurred by LBP using damping factor J L                    and the high          Results of applying LBP to the traffic  the lowest curve corresponds to J L        CHMM with    chains  The lower solid horizontal line             is the error incurred by BK  The oscillating line is the  est to f  L                  Figure  UAI                 iterations  Figure    Same as Figure                      but for the water network   The do tted lines  from top to bottom   represent f  l      f  L       and f  L        The solid line represents BK   The upper horizontal line corresponds  to not updating the messages at all   J L        and gives  an indication of performance based on local evidence alone                 o                  jtree  number of iterations and damping factor   we see that there is no oscillation  and that as few as     ing random parameters and binary nodes  and then compared the marginal posteriors as a function of  The results for a typical sequence are shown in Figure    This time two iterations of LBP can outperform BK   how the algorithms compare in terms of speed   We  therefore generated random data from CHMMs with                     chains  and computed the posteri  ors using the different methods  are shown in Figure      The running times  It is clear that both BK and  FF  LBP have a running time which is linear in                            Cij  a         I       In addition to accuracy  it is also interesting to see  N        bk  loopy           loopy          loo py            loopy                                                                            N  for  the CHMM model   but the constant factors of BK      are much higher  due to the complexity of the algo rithm  and in particular  the need to perform repeated marginalisations  This is also why BK is slower than exact inference for N       even though it is asymp totically more efficient    Figure      Running time on CHMMs as a function of  the number of chains   The vertical axis is the total  running time divided by the length of the sequence  The horizontal axis is the number of chains  The dot ted curve is junction tree  the steep straight line is BK  and the shallow straight lines are LBP   loopy k     All algorithms were implemented in Matlab and are in cluded in the Bayes Net Toolbox  which can be downloaded from YY Il  cs  berkeley  edu    murphyk Bayes bnt  html   means k iterations of LBP    UAI          MURPHY    Related work  WEISS       F  V  Jensen  U  Kjaerulff  K  G  Olesen  and J  Ped ersen  An expert system for control of waste water treatment   a pilot project  Technical report  Univ  Aalborg  Judex Datasystemer        In Danish        U  Kjaerulff  Triangulation of graphs   algorithms giving small total state space  Technical Report R       Dept  of Math  and Comp  Sci   Aalborg Univ   Denmark               U  Kjaerulff  A computational scheme for reasoning in dynamic probabilistic networks  In UAIB               J  Kwon and K  Murphy   We have already discussed in detail the connections between LBP  BK and FF  However  there are several other approximate inference algorithms with a very similar  flavor    Perhaps the closest is the expecta  tion propagation algorithm         This is also an itera  tive message passing algorithm  but now the messages encode moments of the variables computed with re spect to some approximating distribution  The mini  bucket algorithm     also approximates joint distribu  tions over collections of nodes as a product of smaller  hence cannot correct for erroneous independence as  nia  Berkeley                  R  J  McEliece  D  J  C  MacKay  and J  F  Cheng  Turbo decoding as an instance of Pearl s  belief prop agation  algorithm      Comm    Conclusions        tions   J  Pearl  mann         U AI  reviewers for their        N         l      and NSF IIS                      X  Boyen and D  Koller  Tractable inference for com plex stochastic processes  In U AI              R  G  Cowell  A  P  Dawid  S  L  Lauritzen  and D  J  Spiegelhalter  Probabilistic Networks and Expert Sys                       R  Dechter  Mini buckets  a general scheme of ap proximating approximations in automated reasoning  IJCAI   Artifi                        P  Smyth  D  Beckerman  and M  I  Jordan  Prob abilistic independence networks for hidden Markov Neural Computation                          X  Boyen and D  Koller  Approximate learning of dy namic models  In NIPS            In  Fusion and propagation  L  R  Rabiner  A tutorial on Hidden Markov Models and selected applications in speech recognition  Proc   probability models   
 Latent topic models have been successfully applied as an unsupervised topic discovery technique in large document collections  With the proliferation of hypertext document collection such as the Internet  there has also been great interest in extending these approaches to hypertext         These approaches typically model links in an analogous fashion to how they model words   the document link co occurrence matrix is modeled in the same way that the document word co occurrence matrix is modeled in standard topic models  In this paper we present a probabilistic generative model for hypertext document collections that explicitly models the generation of links  Specifically  links from a word w to a document d depend directly on how frequent the topic of w is in d  in addition to the in degree of d  We show how to perform EM learning on this model efficiently  By not modeling links as analogous to words  we end up using far fewer free parameters and obtain better link prediction results      Introduction  The need to automatically infer the different topics discussed in a corpus arises in many applications ranging from search engines to summarization software  A prominent approach is modeling the corpus with a latent topic model where each document is viewed as a mixture of latent topics or factors  and the factors  shared by the whole corpus  are related to the terms or words appearing in the documents  Many of the topic models share the bag of words assumption where each document is represented as a  Yair Weiss School of CS and Eng  The Hebrew University Jerusalem       Israel yweiss cs huji ac il  histogram of terms  ignoring the order of terms and the internal structure of the documents  The entire corpus is represented as a document term co occurrence matrix  Semantic analysis is done by projecting the document term co occurrence matrix onto a lower dimensional factor space  In algebraic methods such as Latent Semantic Analysis     it is projected onto a linear factor space using SVD  In statistical methods such as Probabilistic LSA       Latent Dirichlet Allocation     or the somewhat more general formalism  Discrete PCA     the document term co occurrence matrix is projected onto a simplex by maximizing the observations likelihood  In recent years these latent topic models have been extended in various ways  In particular  correlation between topics     and their dynamics over time     have been directly modeled  The use of additional information provided in the corpus such as authorship information has been studied       In addition  novel models that depart from the bag of words assumption and do consider the internal ordering of the words in sentences within a document have been developed  These models combine local dependencies in various ways  for example  combining n grams with a hierarchical topic model       modeling syntax      and modeling the continuous drift from one topic to another within a document      In this paper  we address the question of how to enrich the model by considering links between documents  such as hyperlinks in hypertext or citations in scientific papers  With the emergence and rapid growth of the World Wide Web  hypertext documents containing links to other documents have become ubiquitous  The connectivity between documents has proven to play an important role in determining the importance and relevance of a document for information retrieval or the interest of a certain user in it              In particular  Dietz at al      have recently proposed a generative topic model for the prediction of citation influences  called the citation influence model  It models the particular structure of paper citations where the citations graph can be described by a directed acyclic graph    DAG   a setting that does not hold in the case of the World Wide Web and other hypertext corpora  There are few previous works that extend topic models to include link information  Cohn and Hofmann     introduce a joint probabilistic model for content and connectivity  The model is based on the assumption that similar decomposition of the document term cooccurrence matrix can be applied to the cite document co occurrence matrix in which each entry is a count of appearances of a linked document  or citation  in a source document  In this approach  links are viewed as additional observations and are analogous to additional words in the vocabulary  but with different weight when estimating the topic mixture of the document  Erosheva et al      also makes use of a decomposition of term document and citation document cooccurrence matrices by extending the LDA model to include a generative step for citations  Note that these models only learn from the co occurrence matrix of citations without exploiting the information conveyed by the cited documents text  Thus  if the citationdocument co occurrences matrix is very sparse  the generalization power of the models is very limited  In this paper  we suggest a novel generative model for hypertext document collection that we name the latent topic hypertext model  LTHM   Our approach includes direct modeling of real world complex hypertext collections in which links from every document to every document may exist  including a self reference  a document linking to itself   We model a link as an entity originating from a specific word  or collection of words  and pointing to a certain document  The probability to generate a link from a source document d to a target document d depends on the topic of the word from which the link is originating  on the importance of the target document d  estimated roughly by the in degree  and on the topic mixture of the target document d   In this way  an observed link directly affects the topic mixture estimation in the target document as well as the source document  Moreover  the non existence of a link between two documents is an observation that serves as evidence for the difference between the topic mixtures of the documents  We introduce the LTHM and related models in Section   and describe the approximate inference algorithm in Section    Experimental results obtained by learning two datasets are provided in Section    Finally  we discuss the results in Section     bitrary accordingly  By no means can we assume it forms a DAG  Therefore  we would like to allow each document to link to any other document  allowing for loops  i e  directed cycles of links originating in a certain document and ending in the same document  In particular  we would like to allow for self loops with links where a document links to itself  The solution is a generative model that consists of two stages  In the first stage  the document content  the words  is created  After the text of all the documents has been created  the second stage of creating links takes place  The contribution of this paper is in modeling link generation and suggesting an approximate inference algorithm for studying it  The text in the documents can be generated using several of the various models mentioned in section    For simplicity  we describe text generation  and inference  accordingly  using LDA      In the following section  we first briefly review the LDA model        Second  we describe the second stage of link generation        Finally  we discuss related models  in section            Document generation  LDA   According to the LDA model  a collection of documents is generated from a set of K latent factors or topics  One of the main assumptions in the model is that for each topic there is a single multinomial random variable  that defines the probability for a word given a topic for all documents in the collection  Each document is characterized by a particular mixture of topic distribution defined by the random variable   The generation of the Nd words of each document d in a corpus contains two stages  first  a hidden topic z is selected from a multinomial distribution defined by   Second  given the topic z  a word w is drawn from the multinomial distribution with parameters z   Figure    a illustrates the generative model  Formally  the model can be described as      For each topic z           K choose W dimensional z  Dirichlet       For each document d           D Choose K dimensional   Dirichlet   For each word wi   indexed by i        Nd Choose a topic ziW  Multinomial d       The latent topic hypertext model  The topology of the World Wide Web is complicated and unknown  The corpus we work with is a subset of the World Wide Web and its topology can be ar   Choose a word wi  Multinomial ziW      a    b    c    d   Figure    a  The LDA model  b The link LDA model  c  The LTHM model in a scenario of generating links from document d to document d  d  The LTHM model in a scenario of generating links from document d to any other document in the collection of D documents       Link generation  We assume that links originate from a word  and each word can have at most one link associated with it    For simplicity  we restrict the discussion to the case where a link is anchored to a single word  The generalization to the case where the link is anchored to a sequence of words can be carried out by forcing the topics of these words to be identical  as proposed in       The generation of links is carried out by iterating over all the words in the document and for each word determining whether to create a link and if so  what is the target document  Let us limit the discussion first to the case where the corpus contains two documents  d and d   and links are generated from words in document d to document d  When iterating over the words in d   at the ith word  we need to decide whether to create a link from wi to d or not  This decision contains two steps  at most  as sometimes the first step is sufficient to determine that no link needs to be created   The first step is drawing at random a variable i from a multinomial   In general  i can take values from   to D  and in this degenerated example it can take two values    indicates no link and d indicates a link to document d  Only if i   d do we consider adding a link to document d and then proceed to the next step  which is randomly drawing the topic of the link  z L   The topic assignment z L is drawn from d   the mixture of topics of the document d  A link is created iff ziW   z L   Figure    c illustrates the full generative model for this degenerated example  The generalization to the  still degenerate  case of generating links from a single document d to any other document in a collection of D documents is illustrated in Figure    d  In this case  the generation of links from words in document d starts by select  If a link is anchored to an image  for example  we could substitute a fictitious word for that link   ing i           D    for every word i      Nd of the document d    is drawn at random from   RD     a multinomial distribution indicating the probability of considering a link to each one of the D documents or not having a link at all  It is a measure of the importance  or in degree  of the documents in the corpus   itself is drawn from the hyperparameter   The Dirichlet prior   RD   is not symmetric and favors not creating links  as most words do not have an associated link  i   for i           D  Also  note that links from a document to itself are allowed in this model  as well as in real life   The most general case  in which every document can contain words linked to any other document  is generated by sequentially going through all words and all documents and drawing at random the corresponding  s and z L s in the way described above  Formally  the generative process is       Choose D     dimensional   Dirichlet        For each document d           D For each word wi   indexed by i        Nd Choose i           D     Multinomial   If i     choose a topic zL  Multinomial i   If z L   ziW create a link Li   i from word i to document i       Related Models  Both models of     and      that we refer to as link PLSA and link LDA  respectively  following     s suggestion  are make of the citation document cooccurrence matrix in a similar manner  We focus on   the link LDA model that is somewhat closer to our model  According to this approach two types of observed variables are modeled  words in documents and citation in documents  The generation of these variables is carried out by first selecting a mixture of topics for each of the documents and then for each of the words and citations on the document generating a hidden topic from which the observation is selected at random from the z in the case of words and from z in the case of citations  Here z           K  The model is illustrated in Figure    b  Formally  the model can be described as   For each document d           D Choose K dimensional   Dirichlet   For each word wi   indexed by i        Nd Choose a topic zi  Multinomial d   Choose a word wi  Multinomial zi   For each citation di   indexed by i        Ld Choose a topic zi  Multinomial d   Choose a citation di  Multinomial zi    Note that in the LTHM  the probability to create a link given topic is Pr link   d z    d d  z W    There are only D additional parameters         D to denote the document importance for link creation  whereas in the link LDA model there are DK additional parameters  d z   Pr d   d z   Also  in LTHM  the very existence or non existence of a link is an observation  while this is not explicitly modeled by the link LDA  Moreover  according to the LTHM  a link shares the same topic with the word it originates from and at the same time affects the topic mixture in the cited document      Approximate Inference  Exact inference in hierarchical models such as LDA and PLSA is intractable due to the coupling of the latent topics and the mixing vectors     The hypertext model presented in this paper shares this coupling and adds a unique coupling between topic mixing vectors  hence  exact inference is intractable in it as well  In recent years  several alternatives for approximate inference in such models have been suggested  EM      or variational EM      Expectation propagation  EP       and Monte Carlo sampling           Unlike other hypertext topic models  in LTHM not only the identities  of the ends of a link are observations  but also the links very existence  or non existence   Taking into account the non existence of links in sampling based inference necessitates further approximations  We therefore perform inference using EM  EM deviates from fully Bayesian methods by distinguishing between latent variables and parameters of the model  The latent variables are the latent topic of a word  z W   the latent topic involved in link generation  z L   and the variable    The parameters of the model are the topic mixing vectors d   the word mixing vectors z and the document link importance parameter d   The Dirichlet hyperparameters    and  are fixed  In the link generation process  unless a link is created  the value of  is unknown  It might have not been created because     or because of topic mismatch between the source document and any other document  For this reason  we need to consider all possible options with their probability during inference  for each source document d and each word in it from which there is no outgoing link  we need to consider all D possible z L variables  P The number of the potential latent variables z L is D d N d which is quadratic in the number of documents  It is therefore infeasible to compute explicitly the posterior distribution of each one of these latent variables  However  in the M step  only aggregations of these posterior distributions are needed  The required aggregations can be computed efficiently  in time linear in the size of the corpus  by taking advantage of symmetries in the model as described in section     and in the appendix  We begin with the M step equations  detailing what are the required expectations  Then we describe how the required posteriors and aggregations are computed in the E step       M step  In the M step  MAP estimators for the parameters of the model  d   z and  are found  Let Gz w denote the number of occurrences of a word w with topic z W   z  The update rule for z w is identical to that in standard LDA  z w   E Gz w     w          The MAP estimator for d takes into account topics of words and links that were drawn from d   The word topics  z W   are drawn from d for each of the words in document d  The link topics are the topics zdL  i d drawn from d when considering a link from any other document d to d  These are the cases where d  i   d for any d   i regardless of whether the link has been created or not  For the purpose of inference  we count   the topics zdL  i d separately for links and for non links  Let Fd z denote the number of occurrences of a topic z associated with any word in document d  Let Vd z be the number of occurrences of a topic z associated with any incoming link of document d  Let Ud z be the number of times d  i   d but the topic generated for the link by document d  zdL  i d   does not match the topic of the ith word in the the document d   zdW  i and therefore a link has not been created  d z   E Fd z     E Vd z     E Ud z     z        Note that in the standard LDA model  we would have just the first term  the expected number of times topic z appears in document d  and the Dirichlet prior  In the LTHM  we add two more terms which model the influence of links  or non links  on the topic distribution  The computation of E Vd z   and E Ud z   is described in section      The MAP estimator for  is d    E Td     d    X X  Nd  E Td         d           d  Where Td is the number of times that d  i   d for any d and any word i in it  this includes the case of d   d where a self link is considered   Notice that Td   P  V d z  Ud z    The normalization factor in equations z   and   includes the term    the most frequent case that there is no link at all       a link  as observations  W W Pr zd i   z w  L   d  z  Pr Ld i  zd i   z z  wd i       W where Pr Ld i  zd i   z   the probability of a link observation is  W Pr link d  i   d  zd i   z  P     d d  z   d  z   if a there is a link from word i in document d to document d   and X W Pr no  link d  i  zd i   z  P        d d  z  d  if there is no link associated with word i in document d  Nave computation of E Vd z   and  E Ud z   would require estimating the posterior distributions of d  i and zdL  i d for all triplets  d   i  d   As mentioned before  explicit computation of these posteriors is infeasible due to large number of these variables  Rather than computing this posterior distribution explicitly over z L and    only the aggregations E Vd z    E Ud z   are computed  E Vd z   is the expected number of occurrences of links incoming to document d with topic z  In the case where a link exists  the posterior distributions of z L and the corresponding z W are equal  hence  Vd z can be computed by summing posterior probabilities of z W   X E Vd z     Pr zdL  i d   z O  P        d  i Ad  E step  In the E step  expectations required for the M step are computed with respect to the posterior distribution of the latent variables  The expectations required for the M step are E Gd z    E Fz w    E Vd z    E Ud z   and E Td    E Gd z   is the expected number of occurrences of a topic z in document d as a topic of word and E Fz w   is the expected number of occurrences of a word w with topic z  E Gd z      Nd X  W Pr zd i   z w  L        i    E Fk z      Nd D X X  W Pr zd i   z  wd i   w w  L       d   i    where w   w        wNd and L   L        LNd   The posterior distribution of z W is explicitly computed  taking into account words and links  or the non existence of  X     Pr zdW  i   z O  P     d  i Ad  where Ad     d   i    link d   i   d   O is the set of all observations and P is the model parameters  E Ud z   is the expected number of times d  i   d for any d   i in the corpus  but zdL  i d    zdW  i   The basic idea in the computation of E Ud z   is that it factors into topic dependent terms and document topic dependent terms  The topic dependent terms can be computed in a single linear pass over the corpus  in each iteration   The document topic dependent terms are specific to each Ud z   Combining them with the topic dependent terms to compute Ud z is done in a constant number of operations  for each d  z   The computation is detailed in the appendix  Finally  after E Vd z   and E Ud z   have been computed  X E Td      E Vd z     E Ud z        z   Despite the quadratic number of latent variables  the runtime of both E and M steps is linear in the size of the corpus times the number of topics  There are a number of extensions to the LDA model that can be considered here  as the approximate inference algorithm described above can be easily adapted for many of the alternatives mentioned in section    For example  suppose one wishes to model the text with the HTMM       the difference would be in the computation of the posterior of word topics  Pr z W  w        wNd   L        LNd    In HTMM  this posterior would be computed using the forward backward algorithm  considering both words and links as the topic emission probabilities  Alternatively  if one wishes to make use of authorship information by applying the Author Topic model       it would require to consider an additional latent variable x for the authorship of each word and compute posterior probabilities Pr x  z W  w        wNd   L        LNd   and modify the definition of d   Yet  the modeling of links stays very similar  in addition to latent topic of the link  z L   only a latent author to the link needs to be selected  xL       Experiments  In this section  we explore the relations between links and topics discovered by LTHM and evaluate its predictive power with respect to links  We compare LTHMs link prediction with previous approaches for combining links  link PLSA    and link LDA     We also compare to link prediction by a non topic method  based only on the frequency a web page is linked  ignoring the contents of the text in the corpus  For this comparison  we use two datasets of web pages  the webkb dataset       documents with       links  and a small dataset of Wikipedia web pages      documents with     links   We begin with an example of the strong relationships between topics and links in the Wikipedia dataset learned by LTHM  The Wikipedia dataset is a collection of     web pages with     links between the pages in the dataset  We downloaded these web pages from Wikipedia by crawling within the Wikipedia domain  starting from the NIPS  Wikipedia page  We have made the data set available online at  http   www cs huji ac il amitg lthm html  We used a vocabulary of      words and trained LTHM with    hidden aspects  Figure   shows four of the hidden topics found by the model in the Wikipedia dataset  For each topic we show the ten most probable words and two most probable links  Topic   discusses neural networks  and the two most related links to it  links with high probability to be generated    At the time being  there is no UAI Wikipedia page   from the topic   Similarly  topic   is about speech and pattern recognition  Topic   is about cities  Denver and Vancouver  the current and previous venues of the nips conference   topic   is about cognitive science and neuroscience  All these topics have related links  Due to lack of space  we show only four example topics  but all    topics have clear interpretation and relevant suggested links  A complete list of the topics with top words and top links can be found at http   www cs huji ac il amitg lthm html  We found that the LDA topics on this dataset were of comparable quality  but the assignment of topics to documents can be different in LDA and LTHM  especially for short documents  Table   shows a comparison of the document topic vector d for two short Wikipedia documents  Journal of Machine Learning and Random Forests  in LDA and LTHM  All topics with d        are shown  Since LTHM uses the link information  it assigns more weight to the relevant topics  For quantitative evaluation  we compare LTHM vs  the topic models link PLSA     and link LDA     and a frequency based method in the task of link prediction  The frequency based method ranks the documents in the corpus according to the number of time they were linked to from other documents  This ranking serves as the link prediction for all the documents  This prediction is the same for all the documents in the corpus and does not depend on the topics of the source document  For these experiments we use the Wikipedia dataset and the webkb dataset  The webkb dataset  available online at  http   www cs cmu edu webkb  consists of      html pages  For this dataset  we used a dictionary of      words  built according to their frequency in the data and removing stop words   We extracted       links where both ends of the links belong to the webkb corpus  We split each data set into a train set consisting of     of the documents and a test set of the remaining      During training  the text of all the documents is provided to all the algorithms  but only the links originating from the documents in the train set are visible during training  Both dataset are learned with    hidden aspects  During test  for each test document d we sort all documents d in the corpus according to the probability of having a link from d  outgoing from any word in d  to d   Figures   and   show several measures of the performance of the different algorithms  The first measure is the percentage of documents in the test set for which at least one link prediction among the top N is a true link  The motivation for this measure is the following question  Suppose we want to suggest to an author of   Figure    Four example topics learned by LTHM and the links related to them  For each topic  the ten most probable words are shown along with the two links most probable to be generated from that topic  Next to each word and each link is its probability to be generated from the given topic   topic prob                                     topic prob                                            Journal Of Machine Learning Research html LDA LTHM top words topic prob top words search article navigation        learning machine engineering press university new        card conference credit learning machine algorithms fixes skins import model regression reasoning Random Forests html LDA top words topic prob probability distribution variables        data mining predictive        learning machine algorithms        fixes skins import        stock market price        search article navigation  LTHM top words linear function training fuzzy regression  model bayesian  model  network carlo monte genetic learning machine engineering  Table    A comparison of the document topic vector d for two short Wikipedia documents Journal of Machine Learning and Random Forests in LDA and LTHM  All topics with d        are shown  Since LTHM uses the link information  it assigns more weight to the relevant topics    a web page other documents to link to  If we show this author N suggestions for links  will s he use at least one of them  The other measures we use are precision  Among the top N predictions  what is the percentage of true links   and recall  What percentage of the true links are included in the top N predictions    Figure   shows that LTHM outperforms all three other methods with respect to all three performance measures  Both link PLSA and link LDA do worse than the frequency based method  This result may seem surprising at first  as these methods are more general than the frequency based method  In particular  they could fit the relative frequency of each document as its probability to be drawn from any topic  In this case  they would predict the same as the frequencybased method  When we inspect the performance of these methods on the train set  figure     we see linkPLSA and link LDA fit better than the frequencybased method  This suggests that link PLSA and linkLDA overfit due to the large number of free parameters  KD  these models have for modeling links  LTHM  on the other hand  has only D additional parameters for modeling links  Moreover  link generation probabilities depend on the topic mixtures of the documents at both ends of the link  Unlike link PLSA and linkLDA  no values of the link parameters  can cancel this dependency  Figure   shows the performance of the four methods on the webkb test set  Once again  LTHM outperforms the other methods  The frequency based method outperforms link PLSA and link LDA  As mentioned in section    thanks to the symmetries in LTHM  each EM iteration is computed in time linear in the size of copus times the number of topics  Training on the webkb dataset with    topics took    hours for     EM iterations  Training on the smaller Wikipedia dataset with    topcis took    minutes for     EM iterations      Discussion  In this work we have presented LTHM  a novel topic model for hypertext documents  In LTHM  the generation of hyperlinks depends on the topics of the source word and the target document of the link  as well as the relative importance of the target document  Compared to previous approaches  LTHM introduces a much smaller number of additional link parameters  As a result  LTHM achieves good generalization results in cases where other models overfit and fail to generalize   Acknowledgements Support from the Israeli Science Foundation is gratefully acknowledged   
  algorithms have been proposed  see  e g          for recent reviews    Finding the most probable assignment  MAP  in a general graphical model is known to be NP hard but good approximations have been attained with max product belief propagation  BP  and its variants  In particular  it is known that using BP on a single cycle graph or tree reweighted BP on an arbitrary graph will give the MAP solution if the beliefs have no ties   Linear Programming  LP  Relaxations are a standard method for approximating combinatorial optimization problems in computer science      They have been used for approximating the MAP problem in a general graphical model by Santos       More recently  LP relaxations have been used for error correcting codes     and for protein folding      LP relaxations have an advantage over other approximate inference schemes in that they come with an optimality guarantee  if the solution to the linear program is integer  then it is guaranteed to give the global optimum of the MAP problem   In this paper we extend the setting under which BP can be used to provably extract the MAP  We define Convex BP as BP algorithms based on a convex free energy approximation and show that this class includes ordinary BP with single cycle  tree reweighted BP and many other BP variants  We show that when there are no ties  fixed points of convex max product BP will provably give the MAP solution  We also show that convex sum product BP at sufficiently small temperatures can be used to solve linear programs that arise from relaxing the MAP problem  Finally  we derive a novel condition that allows us to derive the MAP solution even if some of the convex BP beliefs have ties  In experiments  we show that our theorems allow us to find the MAP in many real world instances of graphical models where exact inference using junction tree is impossible      Introduction  The task of finding the maximum aposteriori assignment  or MAP  in a graphical model comes up in a wide range of applications including image understanding       error correcting codes     and protein folding       For an arbitrary graph  this problem is known to be NP hard      and various approximation  The research described here is based on a remarkable recent set of results by Wainwright  Jaakkola and Willsky          who discussed a variant of belief propagation called tree reweighted belief propagation  TRBP   They showed that when the TRBP output satisfied certain easy to check conditions  one could provably extract the MAP assignment from the TRBP output  Furthermore  they showed an intriguing connection between TRBP and LP relaxation  In related work  we have used TRBP on a number of real world applications      and our experience with it raised a number of questions  First  TRBP is based on a distribution over spanning trees of the original graph  We wanted to know whether the properties of TRBP also hold for other BP variants that are not based on spanning trees  Second  in some applications the sufficient conditions given by Wainwright et al       for extracting the MAP do not hold  We wanted to know whether one could extend these conditions  In this paper  we show that the answer to both questions is affirmative  We define a family of algorithms called convex BP which refer to belief propagation with a convex free energy approximation  We show that tree reweighted BP suggested by Wainwright and colleagues      is a special case of convex BP but there are   WEISS ET AL  many convex free energies that cannot be represented as a tree reweighted free energy  This result has theoretical implications since it shows that the property of solving the LP is distinct from the property of providing a rigorous bound on the free energy  as well as practical implications since it provides an expanded family of possible LP algorithms  We also discuss the max product version of convex BP and show that when convex BP has beliefs without ties  the max product assignment is guaranteed to be the MAP assignment  This gives a unified proof for previous results on ordinary BP with a single cycle             and tree reweighted BP       Finally  we give a new theoretical condition that allows us to provably extract the MAP from convex BP beliefs  even if they have ties  We illustrate the power of these theorems on graphical models with hundreds of variables arising from computational biology and errorcorrecting codes       MAP and LP relaxation  Given an observation vector y  we wish to perform inference on Pr x y  which is assumed to factorize into a product of potential functions      Y   x     e Pr x y    Z  Z  E  x    where  is the domain of the potential   the set of all variables that participate in the potential  and we define the energy E  x   as the negative logarithm of the potential  The MAP is the vector x which maximizes the posterior probability  Y X x   arg max   x     arg min E  x   x  x      To define the LP relaxation  we first reformulate the MAP problem as one of integer programming  We introduce indicator variables qi  xi   for each individual variable and additional indicator variables q  x   for all the potential domains  Using these indicator variables we define the integer program  Minimize   XX   q  x  E  x    x  Subject to    x   q  x           X q  x       x  i  xi      i    X  x i  q  x     qi  xi         where the last equation  enforces the consistency of indicator variables for different potential domains  This integer program is completely equivalent to the original MAP problem  and is hence computationally intractable  We can obtain the linear programming relaxation by allowing the indicator variables to take on non integer values  That is  we replace the constraint q  x            with q  x            This problem can now be solved efficiently  and if the solutions to the LP happen to be integer  we have provably found the MAP       Belief Propagation and its variants  As shown by Yedidia et al        there exist a large number of free energy approximations that are based on a set of double counting numbers  These double counting numbers are used to approximate the entropy of x  denoted H  by means of a linear combination of entropies over individual variables i  Hi   and variables that participate in a factor   H   X X H   c H   ci Hi   i  Given a set of double counting numbers c   ci we define the approximate free energy functional  This is a functional that takes as input a set of approximate marginals b  x    bi  xi   and uses them to define the average energy and the approximate entropy  The approximate free energy at temperature T is simply  F  b   bi     U  b    T H b   bi         where the average energy  U  b    and the approximate entropy  H b   bi    are given by  XX U  b     b  x  E  x     H b   bi       x  X  c  X  ci       i  X  b  x   ln b  x    x  X  bi  xi   ln bi  xi    xi  A special case of approximate free energies is when ci      di   c      where di is the number of factors that node i participates in  or equivalently  the degree of node i in the factor graph   In this case the approximate free energy is called the Bethe free energy  Given an approximate free energy  there are many possible algorithms that try to minimize it  For concreteness we give here one possible algorithm  the two way GBP algorithm       but we should emphasize that all our results hold for any algorithm that converges to stationary points of the approximate free energy  e g                 Assuming c     for all factors         WEISS ET AL   the two way algorithm is similar to ordinary BP on a factor graph  but with an additional reweighting step  As in ordinary BP  we denote the messages sent from factor node  to variable node i by mi  xi   and the message from variable node i to factor node  by mi  xi    The messages are updated as follows  X Y m i  xi       T  x   mj  xj   x i  m i  xi       Y  In summary  we have defined the MAP problem  the LP relaxation and a family of belief propagation algorithms  The natural questions that arise are   When can BP algorithms be used to solve the LP relaxation   How are the max product and sum product algorithms related   j  i  mi  xi     When can BP algorithms be used to provably extract the MAP assignment       mi  xi      mi  xi      deg i   ci     i    i m i  xi   m i  xi    i    i m i  xi   m i  xi       The max product belief propagation with i   algorithm is the same  but with the sum replaced with a max  Note that when i      or  equivalently  ci      deg i   the above update equations reduce to ordinary BP  From the messages we calculate the beliefs  Y bi  xi    mi  xi     b  x      T  x    Y  mi  xi         i  Observation  A set of beliefs b   bi are stationary points of an approximate free energy with double counting numbers c   ci and temperature T if and only if they satisfy   Admissibility  for all x  Y Y  Pr x    T  bc  x   bci i  xi         i   Marginalization  The beliefs are positive  sum to one and satisfy  X i     i   b  x     bi  xi       x i  Similarly  it can be shown that a set of beliefs are fixed points of the max product algorithm with double counting numbers c   ci if and only if they satisfy the above admissibility condition and maxmarginalization condition  i     i    max b  x     bi  xi   x i  As we will show subsequently  a key property in analyzing approximate free energies is their convexity over the set of constraints   Heskes        has derived sufficient conditions for an entropy approximation to be convex over the set of constraints  In our setting  we can rewrite these conditions as follows  Definition  An approximate entropy term of the form  X X H  c H   ci Hi       We emphasize again that this is just one possible algorithm to find stationary points of the approximate free energy  In order to deal with any algorithm  we use the following characterization of approximate free energy stationary points  This characterization follows directly from differentiating the Lagrangian of the approximate free energy and was used by                 Convex Free energies       i  is said to be provably convex if there exist non negative numbers ci   d   di such that  H   X  i  i       ci  H  Hi      X  d  H      X  d i Hi  i  Tree Reweighted Free Energies  Wainwright and colleagues have introduced an important subclass of belief propagation algorithms  tree reweighted BP  These are algorithms whose free energy is a linear combination of free energies defined on spanning trees of the graph  They have shown that tree reweighted BP     can be used to obtain a rigorous bound on the free energy and     gives rise to a convex free energy approximation  A natural question that arises is whether these two properties of TRBP are equivalent  do all BP algorithms that arise from convex free energies also give a rigorous bound on the free energy  In this section we show that the answer is negative  In fact tree reweighted BP algorithms represent a small fraction of convex free energy belief propagation algorithms    Convexity over the set of constraints means the function is convex as a function of any beliefs that satisfy the marginalization constraints  This is a weaker assumption from convexity over any beliefs  Henceforth we refer to this weaker assumption as convexity of the entropy approximation    WEISS ET AL  Tree reweighted free energies      use entropy terms of the form  X HT RBP      T HT          T     T     T        T  where T is a spanning tree in the graph  T defines a distribution over spanning trees and HT is the entropy of that tree  Since HT is convex  so is HT RBP   But not every convex free energy can be written in this way  To see this  note that any tree reweighted entropy can be rewritten  X X X HT RBP      ij Hij       ij  Hi  ij   i  j  where ij is the edge appearance probability defined by   In comparing this to the general entropy approximation  equation    we see that tree reweighted entropies are missing a degree of freedom  with ci    In fact  for any TRBP entropy we can add an infinite number of possibile positive combination of single node entropies and still maintain convexity  Thus  TRBP entropies are a measure zero set of all convex entropies  In some cases  we can even subtract single node entropies from a TRBP entropy and still maintain convexity  For example  the Bethe free energy for a single cycle can be shown to be convex but it cannot be represented as tree reweighted free energy       In particular  it does not give rise to a bound on the free energy  This shows that the family of BP algorithms that provide a bound on the free energy is a strict subset of the family of convex BP algorithms      When does sum product BP solve the LP relaxation   Claim  Convex BP LP Let b   bi be fixed point beliefs from running belief propagation with a convex entropy approximation at temperature T   As T    these beliefs approach the solution to the linear program  Proof  We know that the BP beliefs are constrained stationary points of the free energy  equation     The minimization of F is done subject to the following constraints  b  x              X  b  x          X  b  x       bi  xi    x  x i  The energy term is exactly the LP problem  As we decrease the temperature  the approximate free energy  Figure    Contour plots of the Bethe free energy  top  and a convex free energy  bottom  for a  D Ising model with uniform external field at different temperatures  The stars indicate local stationary points  Both free energies approach the LP as temperature is decreased  but for the Bethe free energy  a local minimum is present even for arbitrarily small temperatures  approaches the LP cost  note that the entropy term is bounded   If we assume the entropy function to be convex then the approximate free energy is convex and hence any fixed point corresponds to the global minimum  Note that for any BP algorithm  it is true that the approximate free energy minimization problem approaches the LP problem  In particular  this is true for ordinary BP which minimizes the Bethe free energy  However  when the entropy function is non convex  there is no guarantee that fixed points will correspond to the global optimum  Figure   illustrates the difference  We consider a graphical model corresponding to a toroidal grid  The nodes are binary and all the pairwise potentials are of the form                These potentials correspond to an Ising model with a uniform external field  nodes prefer to be similar to their neighbors and there is a preference for one state over the other  In order to visualize the approximate free energies  we consider beliefs that are symmetric and identical for all pairs of nodes      x y b   y     x    y  Note that the MAP  and the optimum of the LP  occur at x      y     in which case all nodes are in their preferred state  Figure   shows the Bethe free energy  top  and a convex free energy  bottom  for this problem for different temperatures  The stars indicate local stationary points  Both free energies approach the LP        WEISS ET AL   as temperature is decreased  but for the Bethe free energy  a local minimum is present even for arbitrarily small temperatures      How are max product BP and sum product BP related   Although we have shown that one can use sum product convex BP to solve the linear program  one needs to be able to run the sum product algorithm at sufficiently small temperatures and this may cause serious numerical problems  We now show that in certain cases  one can solve the linear program by running the maxproduct algorithm at any temperature  This follows from the interpertation of the max product algorithm as the zero temperature limit of sum product  Zero temperature lemma  Suppose  b  x    bi  xi    are fixed points of the sumproduct algorithm at temperature T   Define b  x    bT  x   and bi  xi    bTi  xi    Then for any T      b  x    bi  xi    approach the conditions for fixed points of the max product BP algorithm at temperature T      Proof  Recall that a set of beliefs are fixed points of the sum product algorithm if and only if they satisfy the admissibility constraint  equation    and the marginalization constraint  equation    and they are fixedpoints of the max product algorithm if and only if they satisfy the admissibility constraint and the maxmarginalization constraint  equation     For any T   if  b  x    bi  xi    satisfy the admissibility constraint at temperature T then b  x    bT  x   and bi  xi    bTi  xi   must satisfy the admissibility constraint at temperature    We just need to show that b   bi also satisfy the max marginalization constraint as T     Since  b  x    bi  xi    are fixed points of the sum product algorithm  they must satisfy summarginalization  and substituting in the definition of  b  x    bi  xi    we obtain  X    T  b  T   x     bi   xi    x i    X  x i  Example  Consider a graphical model with two nodes x    x  and a pairwise factor              x    x            Consider the Bethe approximation for this graph  c        c    c        This entropy approximation is trivially convex  It is easy to show that for any temperature T   the fixed points of sum product BP are          b          b    b                   And  again  for any temperature T   the fixed points of max product BP are          b       b    b                In other words  when we run max product BP we will get uniform beliefs in both nodes and no matter how small we set T   raising the beliefs to the power   T will still give uniform beliefs  However  the sum product beliefs are non uniform for any temperature  Note  however  that the counterexample still satisfies the zero temperature lemma  raising the sum product beliefs to the power T indeed approaches the maxproduct beliefs as T     The counterexample shows the problem with going from max product beliefs to sum product beliefs at T     which are equivalent to the LP solution   the max product beliefs retain the information on the maximum belief  but have lost the information regarding the number of configurations that attained the maximal value  This motivates the following sufficient conditions for going from max product beliefs to sumproduct beliefs  Given a set of beliefs  b   bi we define the sharpened beliefs as follows   This can be rewritten    we could use the beliefs to find fixed points of maxproduct BP at temperature T      But to use maxproduct BP to solve the LP we want to go in the opposite direction  i e  use max product BP to define fixedpoints of sum product BP at small temperatures  It turns out that this direction does not always work  as the following counterexample shows   T     bi  xi   b  T   x    and as T    this approaches the max marginalization constraint  The zero temperature lemma suggests that if we could run sum product BP at arbitrarily small temperatures   q  x   qi  xi      b  x    max b  x    x    bi  xi    max bi  xi    xi  To illustrate this definition  a belief vector            would be sharpened to        and a belief vector                 would be sharpened to                Using this definition it can be shown    O        a  a a      a a  O      WEISS ET AL   a     O       O    a         a     O    O      a  a    a   a    a    Figure    A simple problem for which max product convex BP will converge in a single iteration but the beliefs cannot be used to solve the linear program  a is a real number smaller than    Corollary  Max Product Convex BP LP Let b   bi be max product beliefs at T     for a convex BP algorithm  If the sharpened max product beliefs are sum marginalizable then they are a solution to the LP problem        exists an assignment x such that b  x   maximizes b  x   and bi  xi   maximizes bi  xi   then x is the MAP  Proof  Since we have fixed points of max product BP they are admissible  equation     Using the fact that the entropy is provably convex  we can rewrite this as  Y   b  x    ci Y Y Pr x   bd  x   bdi i  xi       b  x   i i  i  i We have rewritten Pr x  as a product of functions on x   xi   We want to show that x   xi maximize all of these functions  We know that x maximizes b  x   and xi maximizes bi  xi    Therefore  we just need to worry about the quotients  ri  x     b  x   bi  xi    In the simple two node example  the sharpened maxproduct beliefs are simply the original beliefs  so they are not a solution to the LP problem  In this case  however  it is easy to fix the beliefs by defining b    b  as the sum marginals of b     Figure   shows a simple problem for which the problem is much harder to fix  Max product convex BP will converge in a single iteration to beliefs that are proportional to the potentials  but the sharpened beliefs will not be sum marginalizeable  Hence they cannot be used to solve the linear program  However  sum product convex BP at T          gave a solution to the LP   Lemma  Suppose we have a set of beliefs b   bi that are max marginalizable and there exists x such that b  x   maximizes b  x   and bi  xi   maximizes bi  xi    Then x also maximizes b  x   bi  xi     To summarize  our analysis  as well as that by Kolmogorov and Wainwright          shows that the relation between LP relaxation and max product convex BP is subtle  although we can always verify posthoc whether we have obtained the LP solution  and a fixed point corresponding to the LP solution is guaranteed to exist  we are not guaranteed to find that fixed point  On the other hand  for sum product convex BP the connection to LP is much more direct  at sufficiently small temperatures the BP beliefs will approach the LP solution   Corollary Convex BP   MAP without ties Let b   bi be fixed points of max product BP with a provably convex entropy function  If there are no ties in these beliefs  for every i the maximum of bi  xi   is attained at a unique value xi  then x is the MAP      When can we extract the MAP from max product convex BP   Whereas the previous section focused on using the max product algorithm to avoid the numerical instabilities associated with sum product at small temperatures  here we show how to use max product BP directly to obtain a solution to the MAP problem  Theorem    Convex BP   MAP without frustrations  Let b   bi be fixed points of max product BP with a provably convex entropy function  If there  This lemma was proved for the case of pairwise factors in      and the generalization for arbitrary factors is straightforward  Using the Lemma we see that x maximizes all the terms in the decomposition  equation     since each term is either bi  xi    b  x   or ri  xi   x i    raised by the power of a non negative number   Proof  Since the beliefs are max marginalizable the fact that there are no ties in the node beliefs implies there are no ties in the factor beliefs  It follows that x maximizes b  x   for each  and hence the previous theorem holds  Both the previous theorem and the corollary were proven for the case of TRBP by       Our proof extends these results for arbitrary convex BP algorithms       Dealing with frustrations  There are many cases in which it is impossible to find an assignment x that maximizes all the factor beliefs  This happens whenever the beliefs define a frustrated cycle  see figure     Our final theorem shows that it is possible to extract the MAP from convex BP beliefs even if there are frustrated cycles         WEISS ET AL  BP        a         Default CBP  TRBP                                                                Trivial CBP                      Figure    Negative double counting numbers ci for four different free energy approximations on a      grid used to illustrate the different algorithms   b  Figure    An illustration of a frustrated cycle  The tables show pairwise beliefs obtained by a convex BP algorithm  For the four nodes in  a   it is possible to find an assignment x that maximizes the pairwise and singleton beliefs  Theorem   proves that this means that x is the global optimum  For the four nodes in  b  it is impossible to find such an assignment  Theorem    Convex BP   MAP with frustrations  Let b   bi be fixed points of max product BP with a provably convex entropy function  Let N T be the set of non tied variables and T be the set of tied variables  We denote the set of tied nodes that have non tied neighbors as T   Define xN T by maximizing the local beliefs  the maximum here is unique since these are non tied nodes   Define  Y Y Y ci bT  xT     ri  x   bd  x   bdi i  xi    maximize bi  xi    otherwise this would contradict maxmarginalizability   Hence we can use the lemma again to show that x must maximize ri  x    Corollary  For pairwise factors  if all nodes on the boundary of the tied nodes  T   have uniform beliefs  then the non tied beliefs are optimal  that is  xN T   AP xM N T    This is because for uniform beliefs on the boundary  any assignment xT maximizes the beliefs on the boundary  The fact that factors are pairwise means that it also maximizes all factors that include the boundary nodes  This generalizes a result of Kolmogorov and Wainwright      for binary nodes      An illustrative example  If there exists xT that maximizes bT  xT   and for all regions  that contain both tied and non tied nodes b  x   maximizes b  x   then the assignment  xT   xN T   is the MAP assignment   To illustrate the relationship between linear programming  LP   ordinary belief propagation  BP   tree reweighted belief propagation  TRBP  and convex belief propagation  CBP   we conducted simulations with a small grid graphical model    nodes  arranged in a      grid   Proof  Using the decomposition equation  equation    we can write  Y Y Y ci ri Pr x    x   bd  x   bdi i  xi    One of the difficulties in comparing these different variants of belief propagation comes from the fact that there are many ways to construct TRBP or convex BP approximations  We define the default convex BP approximation based on the following observation   i    T  iT  T  iT  T    i     bT  xT     Y  i  ci ri  x    Y   T  bd  x    Y  iT    ci ri  x    i  iT  i  iT      Y  bdi i  xi    Y  bdi i  xi    iT  We want to show that x maximizes all the terms in the decomposition  By construction xT maximizes bT  xT    By the previous lemma  for all regions    T   x maximizes ri  x    Also  for all i    T and    T   bi  xi   and b  x   are maximized by xN T   For i  T   xT maximizes bi  xi   due to the assumption that x maximizes the boundary factors   The only thing to worry about are terms of the sort ri  x    where i is a boundary node  But since the beliefs b   bi are max marginalizable for boundary nodes as well and b  x   maximizes b  x   by assumption  bi  xi   must  Observation  For any factor graph  the free P energy approximation given by c     and ci     i d  is convex  This follows from the convexity decomposition in section   with ci   d      di     and d      where d is the number of nodes that participate in the factor   We consider   different approximate free energies which give the double counting numbers ci in figure    In all of them  all the factors have the same double counting number c     but they differ in the double counting numbers ci for the nodes  In ordinary BP  ci      di   For TRBP we considered two spanning forests  one for the horizontal edges of the grid  and one for the vertical edges  We used the uniform distribution over these two spanning forests so that the   WEISS ET AL  edge appearance probability was     for all edges  To facilitate comparison with the other approximations  we multiplied the entropy approximation by two  so that c     for all edges and ci      deg i  for all nodes  We also considered two convex BP approximations  The default CBP approximation gives ci   di     since all the factors are pairwise   Finally the trivial approximation ci     is trivially convex since it only sums up positive entropies  For all these free energy approximations we ran the max product algorithm with a synchronous updates and a dampening factor of      We generated     samples of these      spin P glasses P the energy was given by E x    i Jii xi   J x x   J and J were sampled from zero ii ij  ij  ij i j mean Gaussians with standard deviations     and      We found that the problems could be subdivided into three classes based on the behavior of the linear programming relaxation  In the easy regime the linear programming solution is all integer and hence solving the LP gives the MAP  this happened in     of our runs   All the convex approximations converged to beliefs without ties  Consistent with the Max Product Convex BP LP corollary  the assignments obtained by all the approximations were indeed the MAP  Additionally  in all the simulations in the easy regime  ordinary BP gave the correct answer  However  whereas the convex algorithms come with a MAP certificate  ordinary BP comes with no such theoretical guarantee  While all algorithms found the right answer in this easy regime  the number of iterations to convergence was different  Ordinary BP converged faster  median number of iterations      then the default CBP      iterations   then TRBP      iterations  and finally the trivial CBP      iterations   In the hard regime the LP solution is all fractional  this happened in     of the runs   Consistent with the Max Product Convex BP LP corollary  all the convex BP algorithms converged in this case to beliefs where all the nodes were tied  For this regime  ordinary BP never converged  Although the zerotemperature lemma guarantees that a fixed point with ties exists for ordinary BP as well  this fixed point was never found  In this regime  TRBP converged the fastest among the convex BP algorithms  median number of iterations       followed by the trivial CBP      iterations  and finally the default CBP      iterations   However  in terms of finding the MAP  all convex algorithms were equally useless  In the intermediate regime the LP solution is partially integer and partially fractional  this happened in     of the runs   Again  all the convex BP algorithms converged to the same solution where part       of the beliefs are tied and others are not  tied beliefs corresponded to fractional solutions to the LP   The default CBP was fastest      iterations  followed by TRBP      iterations  and then trivial CBP        In the majority of these cases    out of      ordinary BP did not converge  To summarize  convex BP algorithms have the greatest practical advantage over ordinary BP in the intermediate regime where the LP is partially fractional   they converge better and allow to provably extract the MAP  All convex BP algorithms are equivalent in terms of finding the MAP but convergence rate can vary drastically      Real World Experiments  The experiments reported here were designed to see how often convex BP will allow us to solve real world instances of the MAP problem  Checking the conditions of theorems   and   requires finding the MAP in a reduced graphical model defined over the tied nodes  We use the junction tree algorithm to solve this task so this becomes infeasible when the subgraph of tied nodes has large induced width  Our first two datasets are based on real world graphical models coming from computational biology  We briefly summarize the construction of these datasets  see      for more details   Proteins are chains of residues  each containing one of    possible amino acids  All amino acids are connected together by a common backbone structure  onto which amino specific side chains are attached  The problem of predicting the residue side chain conformations given a backbone structure is considered of central importance in protein folding and molecular design and has been tackled extensively using a wide variety of methods  for a recent review  see       The typical way to predict side chain configurations is to define an energy function and a discrete set of possible sidechain conformations  and then search for the minimal energy configuration  Even when the energy function contains only pairwise interactions  the configuration space grows exponentially and it can be shown that the prediction problem is NP complete      As a dataset we used     X ray crystal structures with resolution better than or equal to  A  R factor below     and mutual sequence identity less than      Each protein consist of a single chain and up to       residues  Protein structures were acquired from the Protein Data Bank site  http   www rcsb org pdb   For each protein  we have built a graphical model using the ROSETTA energy function       The nodes of this model correspond to residues  and there are   Task Side Chain Design  WEISS ET AL  LP IP           Thm                Thm             Failed             Table    The percentage of real world instances solved by the different theorems presented in this paper  LP IP means that the solution of the LP was integer  For the easier problem of side chain prediction  top  we could find the global optimum for about     of the cases  For the harder task of protein design  there are so many tied nodes that checking the conditions of the theorems becomes infeasible  edges between any two residues that interact       the potentials are inversely related to the energy  The protein design problem is the inverse of the protein folding problem  Given a particular  D shape  we wish to find a sequence of amino acids that will be as stable as possible in that  D shape  Typically this is done by finding a set of     amino acids and     rotamer configurations that minimize an approximate energy       While the protein design problem is quite different from side chain prediction it can be solved using the same graph structure  The only difference is that now the nodes do not just denote rotamers but also the identity of the amino acid at that location  Thus  the state space here is significantly larger than in the side chain prediction problem  We  again  used the ROSETTA energy function to define the pairwise and local potentials  As a dataset we used    X ray crystal structures         amino acids long  For each of these proteins  we allowed all residues to assume any rotamer of any amino acid  There are  therefore  hundreds of possible states for each node  We found that convergence was not an issue  in all the experiments convex BP converged in reasonable time but the number of ties determines the success of the algorithm  Table   shows a breakdown of the success rate for the two problems  In the harder problem of protein design  the number of ties is so large that checking the conditions of theorems   and   is infeasible  But in the side chain problem  even though exact inference is NP hard and the search space can be as large as        largest clique in junction tree          the number of ties is quite manageable  in over     of the instances we can find the global optimum  For this data set  ordinary BP also converged        of the times  and whenever it converged it found the global optimum  Our third dataset was based on a low density parity check code taken from David Mackays encyclopedia of sparse graph codes  http   www inference phy  cam ac uk mackay codes data html   We used the            code which has     bits and     parity  BP  LP  CBP   ties      MAP found                                                      BSC crossover probability       Figure    A comparison of success rates as a function of crossover probability for a LDPC code on a binary symmetric channel  checks  We simulated sending a codeword over a binary symmetric channel  Each received word defines the local factors in a factor graph  and we used trivial convex BP to find the MAP in this graphical model  We repeated this experiment for different signal to noise ratios  SNR   Figure   shows our results  For high SNR  the problem is easy and the LP solution is almost always integer  success of LP corresponds to a fully integer LP solution or  equivalently  max product convex BP having no ties   However  as the SNR decreases  the LP solution is almost always partially fractional but using theorem   allows us to find the MAP decoding in all cases  Thus even though the search space here is of size      and the maximal clique in the junctiontree includes     bits  convex BP allows us to find the global optimum in a matter of minutes      Discussion  Belief Propgation and its variants have shown excellent performance as approximate inference algorithms  In this paper we have focused on conditions under which the MAP can be provably extracted from BP beliefs  We have shown that previous results  BP on a single cycle and TRBP on arbitrary graphs  are special cases of a wider result on BP with a convex free energy  We have also shown that BP with a convex free energy can be used to solve LP relaxations of the MAP problem  Finally  we have proven a novel result that allows us to extract the MAP from convex BP beliefs even when there are frustrated cycles  From a theoretical perspective  one intriguing result arising from our work is the close connection between LP relaxations and a large class of belief propagation variants  including ordinary BP   Given the large amount of literature on the tightness of LP relaxations for combinatorial problems  this connection may enable proving correctness of BP variants on a larger class of problems    WEISS ET AL  From a practical perspective  our theorems proven in section   allow us to go beyond the LP relaxation and provably find the MAP even when the LP relaxation is partially fractional  Our experiments on side chain prediction and error correcting code show that using these theorems it is possible to find the MAP on real world instances of very large graphical models where techniques such as junction tree are intractable  Similarly  in our work reported in      we have used these theorems to find the global optimum on a number of stereo vision problems  Until our results  only local optima for these problems were known  We believe similar results are possible in a wide range of applications  Acknowledgements Supported by the Israeli Science Foundation  We thank Amir Globerson for comments on a previous version of this manuscript   
 Linear Programming  LP  relaxations have become powerful tools for finding the most probable  MAP  configuration in graphical models  These relaxations can be solved efficiently using message passing algorithms such as belief propagation and  when the relaxation is tight  provably find the MAP configuration  The standard LP relaxation is not tight enough in many real world problems  however  and this has lead to the use of higher order cluster based LP relaxations  The computational cost increases exponentially with the size of the clusters and limits the number and type of clusters we can use  We propose to solve the cluster selection problem monotonically in the dual LP  iteratively selecting clusters with guaranteed improvement  and quickly re solving with the added clusters by reusing the existing solution  Our dual message passing algorithm finds the MAP configuration in protein sidechain placement  protein design  and stereo problems  in cases where the standard LP relaxation fails      Introduction  The task of finding the maximum aposteriori assignment  or MAP  in a graphical model comes up in a wide range of applications  For an arbitrary graph  this problem is known to be NP hard      and various approximation algorithms have been proposed  Linear Programming  LP  relaxations are commonly used to solve combinatorial optimization problems in computer science  and have a long history of being used to approximate the MAP problem in general graphical models  e g   see       LP relaxations have an advantage over other approximate inference schemes in  Tommi Jaakkola CSAIL  MIT Cambridge  MA  Yair Weiss Hebrew University Jerusalem  Israel  that they come with an optimality guarantee  if the solution to the linear program is integral  then it is guaranteed to give the global optimum of the MAP problem  An additional attractive quality of LP relaxations is that they can be solved efficiently using messagepassing algorithms such as belief propagation and its generalizations              In particular  by using message passing algorithms  we can now use LP relaxations for large scale problems where standard  offthe shelf LP solvers could not be used       Despite the success of LP relaxations  there are many real world problems for which the basic LP relaxation is of limited utility in solving the MAP problem  For example  in a database of    protein design problems studied in       the standard LP relaxation allowed finding the MAP in only   cases  One way to obtain tighter relaxations is to use clusterbased LP relaxations  where local consistency is enforced between cluster marginals  As the size of the clusters grow  this leads to tighter and tighter relaxations  Furthermore  message passing algorithms can still be used to solve these cluster based relaxations  with messages now being sent between clusters and not individual nodes  Unfortunately  the computational cost increases exponentially with the size of the clusters  and for many real world problems this severely limits the number of large clusters that can be feasibly incorporated into the approximation  For example  in the protein design database studied in       each node has around     states  so even a cluster of only   variables would have     states  Clearly we cannot use too many such clusters in our approximation  In this paper we propose a cluster pursuit method where clusters are incrementally added to the relaxation  and where we only add clusters that are guaranteed to improve the approximation  Similar to the work of      who worked on region pursuit for sumproduct generalized belief propagation       we show   how to use the messages from a given cluster based approximation to decide which cluster to add next  In addition  by working with a message passing algorithm based on dual coordinate descent  we monotonically decrease an upper bound on the MAP value      MAP and its LP Relaxation  We consider functions over n discrete variables x    x            xn   defined as follows  Given a graph G    V  E  with n vertices  and potentials ij  xi   xj   for all edges ij  E  define the function f  x       X  ij  xi   xj      ijE  X  i  xi           iV  Our goal is to find the MAP assignment  xM   that maximizes the function f  x     The MAP problem can be formulated as a linear program as follows  Let  be a vector of marginal probabilities that includes  ij  xi   xj   ijE over variables corresponding to edges and  i  xi   iV associated with the nodes  The set of  that arise from some joint distribution is known as the marginal polytope         M G        p x  s t   p xi   xj     ij  xi   xj   p xi     i  xi         The MAP problem can then be shown to be equivalent to the following LP  max f  x      max      x       M G   refer to the P marginal of c  xc   for the edge  i  j   i e  c  xi   xj     xc i j c  xc    Define MC  G  as     ij  xi   xj     i  xi    P c  xi   xj     ij  xi   xj   c   i  j   c  xc c  xc       P          xj  It is easy to see that MC  G  is an outer bound on M G   namely MC  G   M G   As we add more clusters to C the relaxation of the marginal polytope becomes tighter  Note that similar constraints should be imposed on the cluster marginals  i e   they themselves should arise as marginals from some joint distribution  To exactly represent the marginal polytope  such a hierarchy of auxiliary clusters would require clusters of size equal to the treewidth of the graph  For the purposes of this paper  we will not generate such a hierarchy but instead use the clusters to constrain only the associated edge marginals       Choosing Clusters in the LP Relaxation  Adding a cluster to the relaxation MC  G  requires computations that scale with the number of possible cluster states  The choice of clusters should therefore be guided by both how much we are able to constrain the marginal polytope  as well as the computational cost of handling larger clusters  We will consider a specific scenario where the clusters are selected from a pre defined set of possible clusters C  such as triplet clusters  However  we will ideally not want to use all of the clusters in C    but instead add them gradually based on some ranking criterion   P P where      ijE xi  xj ij  xi   xj  ij  xi   xj     P P   x    x    There always exists a maximizi i i i i xi ing  that is integral  a vertex of the marginal polytope  and which corresponds to xM   Although the number of variables in this LP is only O  E   V     the difficulty comes from an exponential number of linear inequalities typically required to describe the marginal polytope M G    The best ranking of clusters is problem dependent  In other words  we would like to choose the subset of clusters which will give us the best possible approximation to a particular MAP problem  We seek to iteratively improve the approximation  using our current beliefs to guide which clusters to add  The advantage of iteratively selecting the clusters is that we add them only up to the point that the relaxed LP has an integral solution   The idea in LP relaxations is to relax the difficult global constraint that the marginals in  arise from some common joint distribution  Instead  we enforce this only over some subsets of variables that we refer to as clusters  More precisely  we introduce auxiliary distributions over clusters of variables and constrain the edge distributions ij  xi   xj   associated with each cluster to arise as marginals from the cluster distribution   Let C be a set of clusters such that each c  C is a subset of             n   and let c  xc   be any distribution over the variables in c  We also use c  xi   xj   to  Recently  Sontag and Jaakkola      suggested an approach for incrementally adding constraints to the marginal polytope using a cutting plane algorithm  A similar approach may in principle be applied to adding clusters to the primal problem  One shortcoming of this approach is that it requires solving the primal LP after every cluster added  and even solving the primal LP once is infeasible for large problems involving hundreds of variables and large state spaces      Each edge may participate in multiple clusters   In the next section we present a method that incrementally adds clusters  but which works exclusively within the dual LP  The key idea is that the dual LP   provides an upper bound on the MAP value  and we seek to choose clusters to most effectively minimize this bound  Note that an analogous bound minimization strategy is problematic in the primal where we would have to assess how much less the maximum is due to including additional constraints  In other words  obtaining a certificate for improvement is difficult in the primal  Moreover  unlike the dual  the primal algorithm might not give an upper bound on the MAP prior to convergence  Finally  we can warm start our optimization scheme after each cluster addition in order to avoid re solving the dual LP  We do this by reusing the dual variables calculated in the previous iterations which did not have the new clusters      Dual LP Relaxation  The obstacles to working in the primal LP lead us to consider the dual of the LP relaxation  Different formulations of the primal LP have lead to different dual LPs  each with efficient message passing algorithms for solving them                 In this paper we focus on a particular dual formulation by Globerson and Jaakkola     which has the advantage that the message passing algorithm corresponds to performing coordinate descent in the dual LP  Our dual algorithm will address many of the problems that were inherent in the primal approaches  giving us   we show in our experiments that MAP assignments can be found for nearly all of the problems we consider  We next describe the generalized MPLP algorithm for the special case of clusters comprised of three nodes  Although the algorithm applies to general clusters  we focus on triplets for simplicity  and because these are the clusters used in the current paper  MPLP passes the following types of messages   Edge to Node  For every edge e  E  e denotes two indices in V   and every node i  e  we have a message ei  xi     Edge to Edge  For every edge e  E  we have a message ee  xe    where xe is shorthand for xi   xj   and i and j are the nodes in the edge    Triplet to Edge  For every triplet cluster c  C  and every edge e  c  we have a message ce  xe    The updates for these messages are given in Figure    To guarantee that the dual objective decreases  all messages from a given edge must be sent simultaneously  as well as all messages from a triplet to its three edges  The dual objective that is decreased in every iteration is given by   X X g     max i  xi     kii  xi   iV     Simple warm start of tighter relaxation     An efficient algorithm that scales to very large problems       The Generalized MPLP Algorithm  The generalized Max Product LP  MPLP  messagepassing algorithm  introduced in      decreases the dual objective of the cluster based LP relaxation at every iteration  This monotone property makes it ideal for adding clusters since we can initialize the new messages such that the dual value is monotonically decreased  Another key advantage of working in the dual is that the dual objective gives us a certificate of optimality  Namely  if we find an assignment x such that f  x    is equal to the dual objective  we are guaranteed that x is the MAP assignment  since the dual objective upper bounds the MAP value   Indeed  using this property  kN  i            Monotonically decreasing upper bound on MAP     Choosing clusters which give a guaranteed bound improvement   xi     X eE  max ee  xe     xe  X  ce  xe    c ec  It should be noted  however  that not all  are dual feasible  Rather   needs to result from a reparameterization of the underlying potentials  see       However  it turns out that after updating all the MPLP messages once  all subsequent  will be dual feasible  regardless of how  is initialized   By LP duality  there exists a value of  such that g   is equal to the optimum of the corresponding primal LP  Although the MPLP updates decrease the objective at every iteration  they may converge to a  that is not dual optimal  as discussed in      However  as we will show in the experiments  our procedure often finds the exact MAP solution  and therefore also achieves the primal optimum in these cases       Choosing Clusters in the Dual LP Relaxation  In this section we provide a very simple procedure that allows adding clusters to MPLP  while satisfying the    In our experiments  we initialize all messages to zero     Edge to Node  For every edge ij  E and node i  or j  in the edge  iji  xi         hX i     j i  xi     i  xi     max cij  xi   xj     i  x       x   x       x   j ij i j j j j     xj c ijc  j where j i  xi   is the sum of edge to node messages into i that are not from edge ij  namely  i  xi     P kN  i  j iki  xi      Edge to Edge  For every edge ij  E  ijij  xi   xj     i  h   X j  x       x       x   x       x       x   cij  xi   xj     i j i ij i j i i j j i   c ijc   j   Triplet to Edge  For every triplet c  C and every edge e  c  ce  xe           h X   X    ee  xe     c  e  xe     max e  e   xe          xc e     e c e  c     c e  c   X   i c  e   xe     c      c e   c   Figure    The generalized MPLP updates for an LP relaxation with three node clusters  algorithmic properties in the beginning of Section    Assume we have a set of triplet clusters C and now wish to add a new triplet  Denote the messages before adding the new triplet by t   Two questions naturally arise  The first is  assuming we decide to add a given triplet  how do we set t   such that the dual objective retains its previous value g t    The second question is how to choose the new triplet to add  The initialization problem is straightforward  Simply set t   to equal t for all messages from triplets and edges in the previous run  and set t   for the messages from the new triplet to its edges to zero   This clearly results in g t       g t    In order to choose a good triplet  one strategy would be to add different triplets and run MPLP until convergence to find the one that decreases the objective the most  However  this may be computationally costly and  as we show in the experiments  is not necessary  Instead  the criterion we use is to consider the decrease in value that results from just sending messages from the triplet c to its edges  while keeping all other messages fixed   The decrease in g   resulting from such an update has a simple form  as we show next  Assume we are considering adding a triplet c  For every edge e  c  define be  xe   to be X be  xe     ee  xe     c  e  xe         c   ec     It is straightforward to show that t   is dual feasible   where the summation over clusters c  does not include c  those messages are initially zero   The decrease in g   corresponding to updating only messages from c to the edges e  c can be shown to be   d c     X ec  max be  xe    max xe  xc    X  be  xe            ec  The above corresponds to the difference between independently maximizing each edge and jointly maximizing over the three edges  Thus d c  is a lower bound on the improvement in the dual objective if we were to add triplet c  Our algorithm will therefore add the triplet c that maximizes d c        The Dual Algorithm  We now present the complete algorithm for adding clusters and optimizing over them  Let C  be the predefined set of triplet clusters that we will consider adding to our relaxation  and let CL be the initial relaxation consisting of only edge clusters  pairwise local consistency      Run MPLP until convergence using the CL clusters     Find an integral solution x by locally maximizing the P single node beliefs bi  xi    where bi  xi     i  xi     kN  i  kii  xi    Ties are broken arbitrarily     If the dual objective g t   is sufficiently close to the primal objective f  x     terminate  since x is approximately the MAP        Add the cluster c  C  with the largest guaranteed bound improvement  d c   to the relaxation     Construct warm start messages t   from t      Run MPLP for N iterations  and return to    Note that we obtain  at least  the promised bound improvement d c  within the first iteration of step    By allowing MPLP to run for N iterations  the effect of adding the cluster will be propagated throughout the model  obtaining an additional decrease in the bound  Since the MPLP updates correspond to coordinatedescent in the dual LP  every step of the algorithm decreases the upper bound on the MAP  The monotonicity property holds even if MPLP does not converge in step    giving us the flexibility to choose the number of iterations N   In Section   we show results corresponding to two different choices of N   In the case where we run MPLP to convergence before choosing the next cluster  we can show that the greedy bound minimization corresponds to a cutting plane algorithm  as stated below  Theorem    Given a dual optimal solution  if we find a cluster for which we can guarantee a bound decrease  all primal optimal solutions were inconsistent with respect to this cluster  Proof  By duality both the dual optimum and the primal optimum will decrease  Suppose for contradiction that in the previous iteration there was a primal feasible point that was cluster consistent and achieved the LP optimum  Since we are maximizing the LP  after adding the cluster consistency constraint  this point is still feasible and the optimal value of the primal LP will not change  giving our contradiction  This theorem does not tell us how much the given cluster consistency constraint was violated  and the distinction remains that a typical cutting plane algorithm would attempt to find the constraint which is most violated      Related Work  Since MPLP is closely related to the max product generalized belief propagation  GBP  algorithm  our work can be thought of as a region pursuit method for GBP  This is closely related to the work of Welling      who suggested a region pursuit method for sum product GBP  Similar to our work  he suggested greedily adding from a candidate set of possible clusters  At each iteration  the cluster that results in the largest change in the GBP free energy is added  He showed excellent results for  D grids  but on fully connected graphs the performance actually started deteriorating  with additional clusters  In       a heuristic related to maxent normality      was used as a stopping criterion for region pursuit to avoid this behavior  In our work  in contrast  since we are working with the dual function of the LP  we can guarantee monotonic improvement throughout the running of the algorithm  Our work is also similar to Wellings in that we focus on criteria for determining the utility of adding a cluster  not on finding these clusters efficiently  We found in our experiments that a simple enumeration over small clusters proved extremely effective  For problems where triplet clusters alone would not suffice to find the MAP  we could triangulate the graph and consider larger clusters  This approach is reminiscent of the bounded join graphs described in      There is a large body of recent work describing the relationship between message passing algorithms such as belief propagation  and LP relaxations              Although we have focused here on using one particular message passing algorithm  MPLP  we emphasize that similar region pursuit algorithms can be derived for other message passing algorithms as well  In particular  for all the convex max product BP algorithms described in       it is easy to design region pursuit methods  The main advantage of using MPLP is its guaranteed decrease of the dual value at each iteration  a guarantee that does not exist for general convex BP algorithms  Region pursuit algorithms are also conceptually related to the question of message scheduling in BP  as in the work of Elidan et al       One way to think of region pursuit is to consider a graph where all the clusters are present all the time  but send and receive non informative messages  The question of which cluster to add to an approximation  is thus analogous to the question of which message to update next      Experiments  Due to the scalable nature of our message passing algorithm  we can apply it to cases where standard LP solvers cannot be applied to the primal LP  see also        Here we report applications to problems in computational biology and machine vision   We use the algorithm from Section     for all of our experiments  We first run MPLP with edge clusters until convergence or for at most      iterations  whichever comes first  All of our experiments  except those intended to show the difference between schedules  use N      for the number of MPLP iterations run after adding a cluster  While running MPLP we use the messages to decode an integral solution  and compare    Graphical models for these are given in                        Objective                               MPLP for    iterations MPLP until convergence MAP                                        MPLP iterations        Figure    Comparison of different schedules for adding clusters to relaxation on a side chain prediction problem   the dual objective to the value of the integral solution  If these are equal  we have found the MAP solution   Otherwise  we keep adding triplets  Our results will show that we often find the MAP solution to these hard problems by using only a small number of triplet clusters  This indicates both that triplets are sufficient for characterizing M G  near the MAP solution of these problems  and that our algorithm can efficiently find the informative triplets       Side Chain Prediction  The side chain prediction problem involves finding the three dimensional configuration of rotamers given the backbone structure of a protein       This problem can be posed as finding the MAP configuration of a pairwise model  and in      the TRBP algorithm      was used to find the MAP solution for most of the models studied  However  for    of the models  TRBP could not find the MAP solution  In earlier work      we used a cutting plane algorithm to solve these side chain problems and found the MAP solution for all    models  Here  we applied our dual algorithm to the same    models and found that it also results in the MAP solution for all of them  up to a     integrality gap   This required adding between   and    triplets per model  The running time was between   minute and   hour to solve each problem  with over half solved in under   minutes  On average we added only   triplets  median was       another indication of the relative ease with which these techniques can solve the side chain prediction problem    In practice  we terminate when the dual objective is within     of the decoded assignment  so these are approximate MAP solutions  Note that the objective values are significantly larger than this threshold   We also used these models to study different update schedules  One schedule  which gave the results in the previous paragraph  was to first run a pairwise model for      iterations  and then alternate between adding triplets and running MPLP for    more iterations  In the second schedule  we run MPLP to convergence after adding each triplet  Figure   shows the two schedules for the side chain protein  gsk  one of the side chain proteins which took us the longest to solve     minutes   Running MPLP to convergence results in a much larger number of overall MPLP iterations compared to using only    iterations  This highlights one of the advantages of our method  adding a new cluster does not require solving the earlier problem to convergence       Protein Design  The protein design problem is the inverse of the protein folding problem  Given a particular  D shape  we wish to find a sequence of amino acids that will be as stable as possible in that  D shape  Typically this is done by finding a set of amino acids and rotamer configurations that minimizes an approximate energy  While the problem is quite different from side chain prediction  it can be solved using the same graph structure  as shown in       The only difference is that now the nodes do not just denote rotamers  but also the identity of the amino acid at that location  Thus  the state space here is significantly larger than in the sidechain prediction problem  up to     states per variable for most variables   In contrast to the side chain prediction problems  which are often easily solved by general purpose integer linear programming packages such as CPLEXs branch and cut algorithm      the sheer size of the protein design problems immediately limits the techniques by which we can attempt to solve them  Algorithms such as our earlier cutting plane algorithm      or CPLEXs branch and cut algorithm require solving the primal LP relaxation at least once  but solving the primal LP on all but the smallest of the design problems is intractable       Branch and bound schemes have been recently used in conjunction with a message passing algorithm     and applied to similar protein design problems  although not the ones we solve here  We applied our method to the    protein design problems described in       adding   triplets at a time to the relaxation  The key striking result of these experiments is that our method found the exact MAP configuration for all but one of the proteins   up to a precision of     in the integrality gap   This is es   We could not solve  fpo  the largest protein    pecially impressive since  as reported in       only   of these problems were solvable using TRBP  and the primal problem was too big for commercial LP solvers such as CPLEX  For the problem where we did not find the MAP  we did not reach a point where all the triplets in the graph were included  since we ran out of memory beforehand  Among the problems that were solved exactly  the mean running time was     hours with a maximum of    days and a minimum of a few minutes  We note again that most of these problems could not be solved using LP solvers  and when LP solvers could be used  they were typically at least    times slower than message passing algorithms similar to ours  see      for detailed timing comparisons   Note that the main computational burden in the algorithm is processing triplet messages  Since each variable has roughly     states  passing a triplet message requires     operations  Thus the number of triplets added is the key algorithmic complexity issue  For the models that were solved exactly  the median number of triplets added was      min     max        As mentioned earlier  for the unsolved model this number grew until the machines memory was exhausted  We believe however  that by optimizing our code for speed and memory we will be able to accommodate a larger number of triplets  and possibly solve the remaining model as well  Our current code is written mostly in Matlab  so significant optimization may be possible       Stereo Vision  Given a stereo pair of images  the stereo problem is to find the disparity of each pixel in a reference image  This disparity can be straightforwardly translated into depth from the camera  The best algorithms currently known for the stereo problem are those that minimize a global energy function       which is equivalent to finding a MAP configuration in a pairwise model  For our experiments we use the pairwise model described in       and apply our procedure to the Tsukuba sequence from the standard Middlebury stereo benchmark set       reduced in size to contain    x    pixels  Since there are no connected triplets in the grid graph  we use our method with square clusters  We calculate the bound decrease using square clusters  but rather than add them directly  we triangulate the cycle and add two triplet clusters  This results in an equivalent relaxation  but has the consequence that we may have to wait until MPLP convergence to achieve the guaranteed bound improvement  In the first experiment  we varied the parameters of the            x                        Objective Integer solution                                                                         MPLP iterations  Figure    Dual objective and value of decoded integer solution for one of the reduced Tsukuba stereo models  as a function of MPLP iterations  It can be seen that both curves converge to the same value  indicating that the MAP solution was found   energy function to create several different instances  We tried to find the MAP using TRBP  resolving ties using the methods proposed in      In   out of    cases those methods failed  Using our algorithm  we managed to find the MAP for all   cases   Figure   shows the dual objective and the decoded integer solution after each MPLP iteration  for one set of parameters  In the results above  we added    squares at a time to the relaxation  We next contrasted it with two strategies  one where we pick    random squares  not using our bound improvement criterion  and one where we pick the single best square according to the bound criterion  Figure   shows the resulting bound per iteration for one of the models  It can be seen that the random method is much slower than the bound criterion based one  and that adding    squares at a time is better than just one  We ended up adding      squares when adding    at a time  and    squares when adding just one  Overall  adding    squares at a time turned out to be faster  We also tried running MPLP with all of the square clusters  Although fewer MPLP iterations were needed  the cost of using all squares resulted in an overall running time of about four times longer      For one of these models  a few single node beliefs at convergence were tied  and we used the junction tree algorithm to decode the tied nodes  see         
 Message passing algorithms have emerged as powerful techniques for approximate inference in graphical models  When these algorithms converge  they can be shown to find local  or sometimes even global  optima of variational formulations to the inference problem  But many of the most popular algorithms are not guaranteed to converge  This has lead to recent interest in convergent message passing algorithms  In this paper  we present a unified view of convergent message passing algorithms  We present a simple derivation of an abstract algorithm  tree consistency bound optimization  TCBO  that is provably convergent in both its sum and max product forms  We then show that many of the existing convergent algorithms are instances of our TCBO algorithm  and obtain novel convergent algorithms for free by exchanging maximizations and summations in existing algorithms  In particular  we show that Wainwrights non convergent sum product algorithm for tree based variational bounds  is actually convergent with the right update order for the case where trees are monotonic chains      Introduction  Probabilistic inference in graphical models is a key component in learning and using these models in practice  The two key inference problems are calculating the marginals of a model and calculating its most likely assignment  sometimes referred to as the MAP problem   One approach to these generally intractable problems is to use a variational formulation  where approximate  inference is cast as an optimization problem  For the marginals case this usually corresponds to minimization of a free energy functional  and for the MAP problem it corresponds to solving a linear programming  LP  relaxation       A key challenge in both the MAP and marginals case is to devise simple and scalable algorithms for solving the variational optimization problem  In recent years numerous algorithms have been introduced for both tasks  These algorithms typically have a message passing like structure  Perhaps the most widely used message passing algorithms are belief propagation and its generalizations                 These algorithms typically have two variants  sum product which is used to approximate the marginals  and max product which is used to approximate the MAP  Fixed points of these algorithms can be shown to be local  or sometimes even global  optima of the corresponding variational formulation  Yet despite the spectacular empirical success of these algorithms in real world applications  they are not guaranteed to converge  and variants of dampening are often used to improve their convergence              There has therefore been much recent work on convergent message passing algorithms                These algorithms are often very similar in structure to the nonconvergent algorithms and often include local maxproduct or sum product operations  However  for each of these specific algorithms it has been possible to prove that the value of the variational problem  or its dual  improves at each iteration  Perhaps the most intriguing example of this is Kolmogorovs TRW S algorithm     which is simply Wainwrights tree reweighted max product algorithm      with a different update schedule  Here we introduce a unifying framework which encompasses both marginals and MAP approximations  by exploiting the mathematical similarities between these approximations  Specifically  we provide an up         MELTZER ET AL   per bound on the optimum of the variational approximations  and give sufficient conditions that algorithms need to satisfy in order to decrease this bound in a monotone fashion  Any algorithm which satisfies these conditions is guaranteed to decrease the upper bound at every iteration  This property in turn guarantees that such algorithms converge to a global optimum of the variational problem in the marginals case and to a local optimum in the MAP LP approximation case  Our framework involves updating a subset of regions which form a tree in the region graph  A related approach was recently suggested by Sontag et al      in the context of solving the MAP approximation  Their work gives an explicit algorithm for optimizing all edges corresponding to a tree in the original graph  such that an upper bound on the LP optimum is decreased at every iteration  Our formalism does not give an explicit update but rather conditions that guarantee an update to decrease the objective  However  we show that these conditions are satisfied by several known algorithms  Furthermore  since the condition is similar in the marginals and MAP case  specifically a condition of sum and max consistency respectively  it is easy to obtain algorithms for both these cases simultaneously  and to use results in one problem for obtaining algorithms for the other  For instance  we consider the tree reweighted  TRW  free energy minimization problem       Recently two works have provided convergent algorithms for this problem         but these were more involved than standard message passing algorithms  Here we show the surprising result that in fact the original algorithm provided for TRW by Wainwright et al  is convergent  if run with an appropriate schedule of message updates      Bounds for MAP and Log Partition  We consider a graphical model where the joint probability over variables p x  factors Q into a product over clique potentials p x    Z     x   or equivalently  the energy function is a sum over clique enP   ergies p x      x     We also denote exp    Z P  x       x    The problem of calculating marginals and approximation of the partition function Z can be recast as the following maximization problem of the function F  q   the negative of the free energy   log Z   max F  q    max  h x iq   H q   q  q       where q is the set of probability distributions over x  h x iq is the average energy with respect to q and H q  is the entropy function  The maximizing argu   UAI       ment is then the distribution p x   This maximization is in general intractable  so approximate free energies are often used  A class of approximate free energies  discussed in       is based on the concept of a region graph G whose nodes  are regions of the original graph  and whose edges represent subregion relationships  i e  an edge between  in  exists only if      The approximation replaces the joint entropy H q  with a linear combination of local region entropies H  q   where each local entropy is weighted by a double counting number c   HG c  q     X  c H  q             where the subscript G  c indicates the dependence of the approximation on the region graph G and the counting numbers c   With this approximation of H q  the free energy now only depends on local distributions  since the average energyPis a simple function of the q   namely h x iq    h  x  iq   To optimize only over local distributions q   we need to consider only distributions such that there exists a q x  that has these as marginals  This set  called the marginal polytope       cannot be expressed in a compact form  and is typically approximated  One popular approximation is the local polytope of the region graph L G  defined as the set of local distributions that agree on the marginals for any two regions in the region graph that are subsets of each other   P         x q  x     q  x   x  x L G    q    P   x q  x       Take together  this results in the following standard variational approximation       X max F  q    max h  x  iq   HG c  q      qL G   qL G     Similarly the MAP problem is approximated via X max F  q    max h  x  iq qL G   qL G          To obtain a unified formalism for MAP and marginals  we use a temperature parameter T where T     for marginals and T     for MAP and the optimization is  X max F  q    max h  x  iq   T HG c  q      qL G   qL G       Note that this is the local polytope of the region graph not the local polytope of the original graph   UAI           MELTZER ET AL   Positive Counting Numbers  The entropy approximation HG c  q  in Eq    is generally not a concave function of the local distributions q  Thus maximization of F  q  may result in local optima  To avoid this undesirable property  several works  e g        have focused on entropies which are obtained by considering only concave HG c  q  functions  We focus on approximations where all the double counting numbers are non negative  This is a strong restriction but since we are working with a region graph formulation  many approximate free energies which have negative double counting numbers can be transformed into ones with positive double counting numbers on a region graph  Perhaps the most important example are tree reweighted free energies inPwhich the entropy is approximated as HT RBP  q      H  q  with  a probability distribution over trees in the graph and H  q  is the entropy of the distribution on  with marginals given by q  more precisely  the projection of q on the tree     If we consider a region graph with trees and their intersection  Fig     the double counting numbers are non negative  ButP HT RBP can P also be rewritten H    H   T RBP ij ij ij i ci Hi with P ci      j ij and ij is the edge appearance probability of the edge ij under the distribution   In this case  the double counting number for the singletons ci may be negative  However  we will show that it is sometimes advantageous to work in the representation that uses a non negative mixture of trees  since nonnegativity of the counting numbers allows a simpler derivation of algorithms       Optimization and Reparameterization  The vast majority of methods for solving the variational approximation are based on two classes of constraints that local optima should satisfy  It is easy to show using Lagrange multipliers  that local optima of F should satisfy two types of constraints                     Reparametrization Q  or admissibility  or e constraints   P  x    q  x  c   for every x    sum consistency  or m constraints   P x  x q  x     q  x   for all    and x    By enforcing each of these constraints iteratively one obtains many of the popular sum product algorithms  Replacing the sum consistency constraint with maxconsistency gives many of the popular max product algorithms  A simple example is ordinary BP  which maintains admissiblity at each iteration and a message from i to j enforces consistency between bij and bj         In general  simply iteratively enforcing constraints is not guaranteed to give convergent algorithms  However  as we show in this paper  by iterating through the constraints in a particular order  we obtain monotonically convergent algorithms       Bound minimization and reparameterizations  We begin by providing an upper bound on the logpartition function whose minimization is equivalent to the maximization in Eq     We consider marginals b of the exponential form  b  x               exp   x   c Z       and require that these marginals will be admissible  maintain the e constraints   We obtain admissibility by requiring that the variables  will satisfy the following for each x  X X   x       x           The algorithms we propose will optimize over the variables  while keeping the constraint in Eq    satisfied at all times  Moreover  they will monotonically decrease an upper bound on the optimum of Eq     In the following two lemmas we provide this bound for the sum and max cases  Lemma   The approximation to the log partition function is bounded above by  X boundsum          c ln Z   for any reparameterization   i e   any  satisfying Eq      Minimizing boundsum    over the set of reparameterizations  would give the approximated log partition function  min boundsum      max F  q    qL G        This is the optimum of Eq    with T      Proof  Kolmogorov     showed that if  is a reparameterization  i e  keeping the constraint in Eq      it also holds that hiq   hiq for any q  L G   Using this property  we can see that the log partition function is constant under reparameterization  F  q      F  q    for any q  L G   and in particular the maximum value        MELTZER ET AL   will remain the same  Now  using the new variables  we have a trivial bound on the log partition    X  h iq   c H  q   max F  q      max qL G   qL G     X   Algorithm   The tree consistency bound optimization  TCBO  algorithm Iterate over sub graphs T of the region graph that have a tree structure        Choose a tree T      max h iq   c H  q   q     Update the values of t   for all   T such that   Since the counting numbers c are non negative  the marginals defined in Eq    maximize each local functional F  q      c     h iq   c H  q    and the optimal value is c ln Z   Thus  max F  q      qL G   X  c ln Z   re parameterization is maintained  X X t    x      x    t  x    tree consistency is enforced  Define the beliefs          t   bt     x         or optimize the bound to the MAP by enforcing max consistency   qL G   t   t   t   max bt     x        b  x      x   x  Minimizing boundmax    over the set of reparameterizations  would give the optimal value for the regiongraph LP relaxation of the MAP  X min boundmax      max      h  x  iq   This is the optimum of Eq    with T      Proof  The bound follows directly P from the admissiblity constraint so that maxx    x    P  maxx   x    The fact that the tightest bound coincides with the LP relaxation was proven in       We note that the above two lemmas may also be viewed as an outcome of convex duality  In other words  the original variational maximization problem and the equivalent bound minimization problem are convex duals of each other  In the following sections we provide a framework for deriving minimization algorithms for the above two bounds   Zt        exp t    x   c  x   Lemma   The value of the MAP is bounded above by  X boundmax           max   x   for any reparameterization   i e   any  satisfying Eq          For each   T    T     and x   optimize the bound to the log partition function by enforcing sum consistency  X t   t   t   bt     x        b  x       A similar result may be obtained for the MAP case  this result or variants of it appeared in previous works  e g                    T  T  The bound is tight if there exists a reparameterization  such that the marginals b  x      are sumconsistent  i e  b  L G    The existence of such a re parameterization is guaranteed if the maximum of the approximated negative free energy F  q    does not happen at an extreme point          UAI          Bound optimization and consistency  We propose the tree consistency bound optimization  TCBO  algorithm as a general framework for minimizing the bounds in Sec      for the approximated log partition and for the MAP  within a region graph with positive counting numbers c   The idea is to perform updates on trees that are subgraphs of the region graph  The  corresponding to each such tree will be updated simultaneously in a way that will achieve a monotone decrease in the bound  The method we propose  as described in Algorithm   keeps the beliefs admissible with the positive counting numbers c  or equivalently  always maintains  x  that reparameterize the original energy  x    The corresponding  thus satisfy the conditions of the bound in Sec       Furthermore  at each iteration  max or sum consistency of the beliefs is enforced for the subtree T   As mentioned earlier  maintaining consistency on subsets does not generally result in convergent algorithms  However  as the following lemmas show  in our case enforcing consistency is equivalent to block coordinate   UAI       MELTZER ET AL        descent on the bound  Lemma   The sum consistency lemma  Consider the bound minimization problem for the logpartition function with positive counting numbers  Lemma     defined on a subset of regions and intersections T   The part of the bound which is influenced by the beliefs of the subset is  X P BT  T     c ln Z T  The problem is to find     for all   T that minimize P BT  T   subject to  being reparameterizations of the energy      If for some  the be    reparameterization  Figure    A simple  x  grid  with pair regions  Proof  The part of the bound which is dependent on T is bounded below  P BT  T    max T  xT   xT  where  liefs b x       exp   c are sum consistent  then it minimizes the bound   Proof  The part of the bound which is dependent on T the is bounded below  X P BT  T     max F  q      c   T    q  max  qT L G   X  F  q      c    T  Now  if we find variables  for all   T such that they provide global re parameterization  x     x   so we can have a bound   and the marginals b  x       exp   x   c   which maximize each term F  q      c   separately are also sum consistent  bT  L G    then P BT  T   will achieve its optimal value  and thus we perform block coordinate descent on the bound  Note that for optimizing the bound to the logpartition  the subset T does not have to form a tree  and the sum consistency of the beliefs is enough  Yet  it may be easier in practice to enforce sum consistency on trees  Lemma   The max consistency lemma  Consider the bound minimization problem for MAP with positive counting numbers  Lemma     defined on a subset of regions and intersections that form a tree T   The part of the bound which is influenced by the beliefs of the tree is  X P BT  T     max   x   T  x  The problem is to find     for all   T that minimize P BT  T   subject to  being reparameterizations of the energy      If for some  the be    reparameterization liefs b x       exp   c are max consistent  then it minimizes the bound     X   x   T  xT     T  so if we can find an assignment xT whose cost T  xT   equals P BT  T    that means we have the tightest bound  Now  if for some T the be    reparameterization  liefs b  x       exp   c are max marginalizable then we can always find an assignment xT that sits on the maxima of  because the subgraph is a tree  so there cannot be any frustrations   Hence  we obtain T  xT     P BT  T    and the bound achieves its optimal value for the coordinates in T   The above two lemmas show that the TCBO algorithm monotonically decreases the bound after each update  In the log partition case  the bound is strictly convex and thus this strategy finds the global minimum of the bound which is the global maximum of Eq     In the MAP case  the function is not strictly convex and the algorithm may converge to values that are not its global optimum  This phenomenon is shared by most dual descent algorithms  e g                TCBO is a general scheme and can be implemented for different choices of tree sub graphs  In the next section we illustrate some possible choices and their relation to known algorithms      Existing bound minimizing algorithms  We identify some existing convergent algorithms as instances of TCBO  Heskes algorithm     for approximating the marginals  and MPLP     TRW S      maxsum diffusion  MSD       for approximating MAP  Figures     show the reparametrization  region graph and the tree sub graph updated at each iteration of these algorithms  for the simple example of a  x  grid shown in Fig     Note that all algorithms use a reparameterization with positive double counting numbers  Furthermore  they update only a subtree at        MELTZER ET AL   Figure    Illustration of the max sum diffusion  MSD  algorithm as an instance of the TCBO formalism  MSD operates on a region graph containing pairs and singletons  here corresponding to a  x  grid   The subgraph T corresponding to the TCBO update is shown in the blue dashed line   UAI       Figure    Heskes sum product algorithm may be viewed as a TCBO algorithm updating the subtree shown in the blue dashed line  a star graph centered on a singleton node   Algorithm   Heskes sum product algorithm Iterate over intersection regions    Algorithm   The max sum diffusion  MSD  algorithm Iterate over edges between regions                set the message from  to       Set the message from  to    t   m  x       t m  x    t   m  x    v u u maxx  bt  x   t bt  x       t x  b  x   mt  x    P     Update the belief of the intersection region  t    b     Update the beliefs    x     c  c Y  t    m  x     t    b   x      t    m  x     Y  t  m   x        t   b  x        x   Q t mt        m   x     x           set the messages to the parent regions and their beliefs  t    m  x       t   b  x      bt    x    mt     x     c    t     x    m  x    Y  t    m    x          a time  What remains to be shown is that each iteration achieves consistency among the beliefs  in other words  it satisfies the conditions of TCBO framework and thus monotonically decreases the corresponding upper bound   Heskes algorithm can be shown to be an instance of TCBO using direct substitution  The update rules are shown in algorithm    MPLP  algorithm    does not appear at first sight to use the region graph illustrated in Fig     but rather works with edges and singletons  However  as we show in the appendix  there is a way to transform the messages used in the max product version of Heskess algorithm into messages of MPLP using the MPLP region graph  The max consistency achieved by MSD  algorithm    can again be shown directly  It is also possible to use tree graphs  or forests  as regions  and various existing methods indeed use this approach  We may consider a TCBO algorithm which iterates through all edges and nodes  and for each edge or node enforces consistency between it and all trees that contain it  This is illustrated in Fig     A naive  implementation of such a scheme is costly  as it requires multiple tree updates for every edge  However  Kolmogorov     showed that there exists an efficient implementation  which he called TRW S  of such a scheme in the MAP case  This implementation may only be applied if the trees are monotonic chains  defined as follows  given an ordering of the nodes in a graph  a set of chains is monotonic if the order of nodes in the chain respects the given ordering  This structure allows one to reuse messages in a way that simultaneously implements operations on multiple trees  The scheduling of messages is important for guaranteeing convergence in this case  It turns out that one needs to scan nodes along the pre specified order  first forward and then backward  In the marginals case  the TRW algorithm of Wainwright      corresponds to optimizing over tree regions but is not provably convergent  In the next section we show how to derive a convergent algorithm for this case using our formalism    UAI       MELTZER ET AL        Figure    MPLP for pairs and singletons is equivalent to Heskes algorithm with stars and pairs  Algorithm   The max product linear programming  MPLP  algorithm Iterate over pairs of neighbouring nodes   ij       Set the message from i to   ij    t    miij  xi      t  Y  miki  xi    kN  i  j  and equivalently from j to   ij      Update the pairwise beliefs of   ij    t    bij  xi   xj     q t   ij  xi   xj    mt   iij  xi    mjij  xj       Set the messages from   ij   to i  and equivalently from   ij   to j   v   u u max ij  xi   xj    mt   xj u jij  xj   t   miji  xi    t mt   iij  xi      Set the beliefs of i  and same for j   t    bi  t     xi    miji  xi     Y  t  miki  xi    kN  i  j     Figure    A region graph with chains  their pairwise and singleton components  Such graphs are used by the TRW S algorithms  In the above example there are two chains  which are also monotonic chains since they agree with the node ordering               TRW S may be viewed as a TCBO algorithm on the subgraph shown in the blue dashed line  and an additional subgraph corresponding to a pairwise component and all the chains that contain it   since it is an instance of a TCBO algorithm for the sum case  Furthermore  this TRW S variant differs from the algorithm in      only in the scheduling of messages  An additional algorithm that can be easily shown to be convergent is two way GBP      with all double counting numbers c      both in the sum and in the max versions  At each iteration  two way GBP updates only the beliefs of a region and one of its subregions  which is trivially a tree  The fact that it maintains reparameterization and enforces consistency can be shown directly  In fact  it can be shown that two way GBP with c     is identical to MSD   New bound minimizing algorithms    By replacing the max with a sum  or vice versa  in the algorithms discussed in the previous section  we obtain algorithms that enforce a different type of consistency  and keep the same reparameterization and region graph as shown in the figures  Thus  the maxproduct version of Heskes algorithm and the sumproduct versions of TRW S  MPLP and MSD are convergent with respect to the relevant bound  The TRW S sum product case is especially interesting  In this case the relevant bound becomes the treereweighted log partition function bound introduced in       The message passing algorithm suggested in      does not generally converge  In contrast  the TRWS sum product algorithm is guaranteed to converge   Experiments  We present two experiments to illustrate the convergence of our sum and max algorithms  All algorithms were applied to an instance of a   x   spin glass with pairwise terms drawn randomly from        and field from         In each case we tested the new TCBO algorithms  For estimating the log partition  we ran TRW and considered a uniform distribution over   spanning forests in the graph  all horizontal and all vertical chains  These chains are monotone with respect to the node ordering                   We ran TRW S by following the nodes order first forward and then backward  updating each time only the messages in the direction of        MELTZER ET AL   UAI       Algorithm   The sequential tree reweighted BP  TRW        S  algorithm    ij  mij  xj    max i  xi  ij xi   xi   xj    Q  ik kN  i  j mki  xi     mjiij  xi       After each iteration over all edges  update all singleton and pairwise beliefs  bi  xi      i  xi    Y       ik  x   mki i  kN  i  j   mjiij  xi    Q    jk kN  j  i mkj  xj                                   Iterations                 mijij  xj    the scan  Fig    shows a comparison of this schedule to TRW where the node ordering is followed in a forward manner  and all outgoing messages are updated from each node  Both schedules keep a re parameterization and provide a bound to the log partition  yet only TRW S monotonically decreases it at each iteration  For the MAP case  we ran the max product version of Heskes algorithm using a region graph of pairs  with double counting numbers    and singletons  with double counting numbers     We also ran MPLP and MSD on the same problem  Fig    shows the bounds obtained after each iteration  As can be seen  all three algorithms monotonically converged to the same value      sumTRWS sumTRBP               i  xi  j  xj  ij ij  xi   xj    Q         ij mji  xi    jN  i   bij  xi   xj    Bound to LogPartition          Iterate over edges i  j in a certain updating order  and set the message from i to j   Discussion  Despite the empirical success of max product and sumproduct algorithms in applications  the original algorithms are not guaranteed to converge  Much research in recent years has therefore been devoted to devising convergent algorithms  Typically these recent algorithms are either max product or sum product and their proof of convergence is specific to the algorithm  Here we have presented a unified framework for convergent message passing algorithms and showed the importance of enforcing consistency in both sum product and max product algorithms  Not only does this analogy allow us to give a unified derivation for existing algorithms  it also gives an easy way to derive novel algorithms from existing ones by exchanging maximizations and summations  Although many convergent algorithms are instances of our framework  it is worth pointing out two convergent algorithms that are not  The first is Hazan and Shashuas recent algorithm        which works for provably convex double counting numbers  not necessarily  Figure    The bound on the log partition in a   x   spin glass  obtained by sum product TRW and TRW S with edge probability appearances of      Note that the two algorithms differ only in the order of updates they perform  TRW S follows the node ordering                  that agrees with the monotonic chains  first forward and then backward  In the TRW implementation we followed the same nodes order in a forward manner   positive as we are assuming   The second is ordinary BP on a single cycle  which can be shown to be convergent in both its sum and max product forms  We emphasize that even negative counting numbers can be handled by us in some cases  by using larger regions  All the algorithms we discussed here in fact only updated star graphs in the region graph  Our conditions for monotonicity apply to general tree updates  However  it seems less straightforward to obtain general  non star  tree updates that achieve  max or sum  consistency and reparameterization simultaneously  Interestingly  the tree based updates in     do monotonically decrease an upper bound but seem not to satisfy maxconsistency  Thus  it remains an interesting challenge to find general tree updates that satisfy the consistency constraints  as these could be easily used interchangeably for MAP and marginals  Perhaps the most intriguing result of our analysis is the importance of update schedule for obtaining convergence  a non convergent algorithm becomes convergent when the right update schedule is used  It will be interesting to see whether convergent update schedules can be derived for an even larger class of message passing algorithms    UAI       MELTZER ET AL   clusters and hyper stars respectively  in which the centers are the intersections of the clusters            
