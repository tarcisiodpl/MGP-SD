  We investigate the value of extending the completeness of a decision model along dif ferent dimensions of refinement  Specifically  we analyze the expected value of quantita tive  conceptual  and structural refinement of decision models  We illustrate the key dimen sions of refinement with examples  The anal yses of value of model refinement can be used to focus the attention of an analyst or an au tomated reasoning system on extensions of a decision model associated with the greatest expected value     Introduction  The quality of recommendations for action generated by decision analyses hinges on the fidelity of deci sion models  Indeed  the task of framing a decision problem enumerating feasible actions  outcomes  un certainties  and preferences lies at the heart of deci sion analysis  Decision models that are too small or coarse may be blind to details that may have signif icant effects on a decision recommendation  Unfor tunately  the refinement of decision models can take a great amount of time  and can be quite costly in time and expense  In some cases  actions are taken well before a natural stopping point is reached in the modeling process  In other cases  important distinc tions about actions and outcomes are recognized days or months after a model is developed  We have developed methods for probing the value of key dimensions of decision model refinement  We pose the techniques as tools that can direct the attention of an analyst or of an automated reasoning system to re fine aspects of a decision model along dimensions that have the highest expected payoff  The methods also can provide guidance on when it is best to cease addi tional refinement and to take immediate action in the world  Our work differs from previous studies of the  Currently at the Department of Industrial   Systems Engineering  National University of Singapore   Eric J  Horvitz  Palo Alto Laboratory Rockwell International Science Center     High Street  Palo Alto  CA       value of modeling  Watson   Brown        Nickerson   Boyd        in that we develop a unifying framework for probing the values of different classes of refinement  and consider issues surrounding the direction of model building and improvement under resource constraints  Three fundamental dimensions of decision model re finement are     quantitative refinement      concep tual refinement  and     structural refinement  We will explore methods for making decisions about which di mensions to refine  and the amount of effort to expend on each form of refinement  Quantitative refinement is the allocation of effort to re fine the uncertainties and utilities in a decision model  There are two classes of quantitative refinement      uncertainty refinement  and     preference refinement  Uncertainty refinement is effort to increase the accu racy of probabilities in a decision model  For example  assessment may be focused on the tightening of bounds or second order probabilities over probabilities in a de cision model  Preference refinement is refinement of numerical values representing the utilities associated with different outcomes  For example  an analyst may work to refine his uncertainty about the value that a decision maker will associate with an outcome that has not been experienced by his client  Conceptual refinement is the refinement of the seman tic content of one or more distinctions in a decision model  With conceptual refinement  we seek to modify the precision or detail with which actions  outcomes  and related random variables are defined  For exam ple  for a decision m  ker deliberating about whether to locate a party inside his home versus outside on the patio  it may be important to extend the distinc tion  rain  to capture qualitatively different types of precipitation  using such conceptually distinct notions as  drizzle    intermittent showers   and  downpour   Likewise  with additional deliberation  he may real ize that there are additional options available to him  Many of these additional alternatives are those that would not be taken if there were no uncertainty about the weather  For example  he might consider having the party on the porch  or renting a tent to shelter the guests in his yard    Value of Decision Model Refinement  Structural refinement is modeling effort that leads to the addition or deletion of conditioning variables or dependencies in a decision model  For exam ple  a decision maker may discover that an expensive telephone based weather service gives extremely accu rate weather forecasts  and wish to include the results of a query to the service in his decision analysis  These classes of refinement represent distinct dimen sions of effort to enhance a decision model  In the next sections  we will develop equations that describe the expected value of continuing to refine a model for each dimension of refinement     Expected Values of Decision Model Refinement  Let us now formalize measures of the expected value of refinement  EVR     For any dimension of EVR  we seek to characterize our current state of uncertainty about the outcome of an expenditure of effort to re fine a decision model  Experienced decision analysts often have strong intuitions about the expected bene fits of refining a decision model in different ways  This knowledge is based on expertise  and is conditioned on key observables about the history and state of the modeling process  Assume that we assess and repre sent such knowledge in terms of probability distribu tions over the value of the best decision available fol lowing model refinement  conditioned on key modeling contexts  To compute the E V I   we first determine the expected value associated with the set of possible models we cre ate after refinement  We sum together the expected utility of the best decision recommended by each pos sible revised model  weighted by the likelihood of each model  Finally  we subtract this revised expected value from the expected value of the decision recommended by the unrefined model       General Analysis  the weather and A represents the decision on party lo cation  The expected value of taking action ak  given background information e  is  The expected value of the decision offered by this de cision model is E vle    mfCLP xile v ak xi        i  Suppose the decision model can be refined via one of several refinement procedures R  In general  R can be parameterized by amount of effort  e g   as char acterized by time  expended on the refinement  We shall simplify our presentation by initially overlooking such a parameterization  Note that e represents the state prior to any refinement consideration  R repre sents information about the refinement prior to actual refinement  Let R e  denote the state of information after a refinement requiring some prespecified effort  Let J lk denote the expected utility that will be ob tained for action ak  Before the refinement is carried out  the values of J lk are unknown  However  we can assess a probability distribution over each of the val ues  given information about R and e  We denote this distribution as p J lkiR e   The expected utility given refinement R is  If we cease model refinement activity  we commit to an action in the world based on all information available including P J lkIR e   The expected utility without refinement is  The EVR is        J lk P J lkiR e         E viR e    E viR e         E viR e    max k  EVR  R   Figure    A basic decision model       l k  In practice  the values J lk and distributions P J lkIR  e  are dependent on the specific type of refinement and the amount of effort allocated  We shall now describe specific properties of the three types of model refine ment and give examples of the detailed analysis of computing the EVR for each  In each case  we shall show how each of the analyses is related to the general formulation captured in Equation       Consider the simple decision problem with a single state variable X and a single decision variable A  as shown in Figure    In the party problem  X represents         We shall use the EVR to refer generally to the expected value of refinement  but shall use more specific terms to refer to alternate classes of refinement   We start with a consideration of the value of efforts to refine quantitative measures of likelihoods and prefer ences   Expected Value of Quantitative Refinement        Poh and Horvitz         Uncertainty Refinement  Consider the quantitative refinement on the state vari able X of the party location problem  What is the value of  extending the conversation  through expend ing effort to refine the probability distribution p XIe  with additional assessment  Let us first consider the general case where the distrioution p XIe  is continu ous  Assume that a continuous distribution is char acterized or approximated by a named distribution and a parameter or a vector of parameters  Specifi cally  assume a functional form f for the probability density function  such that for every reasonable dis tribution p XIe   there exists a parameter or a set of parameters     so that the the numerical approxi mation p XIe    p X  is within satisfactory limits  Before the assessment is carried out  we cannot be cer tain about the outcome distribution  however  its out come might be described by a distribution of the form p f  R e   which represents the decision maker s un certainty about the primary distribution parameter     The expected value of the refinement is  l    p x v ak x         p f  R e  max k J j The expected value without performing the quantita tive refinement but taking account of knowledge an agent lias about the potential outcome of refinement procedure R is E viR e      E viR e     where  X  l   fp  p x p f  R e v ak x  max     p xiR e v ak x  k l   max k  f   x       r    ii   The  Hence f is linear in  r and therefore expected value given that quantitative refinement is performed is  E viR e        p TriR e max Trv ak x       Tr v ak x         k      The expected value without the refinement but with knowledge about the potential performance of R is E viR eJ   max   rv a k  x          ii  v ak  X         k The above analysis can be extended to the general case where the state variable X has possible states  In this case  f  consists of n   parameters    r          Trn    Our analysis of EVRQU  R  can be related to the gen eral formulation in Equation     by defining the vari able n       J lk   rv ak x         r v ak x   for each action ak E A  The distributions p J lkIR e  can be derived from p TriR e    We shall illustrate the concept of quantitative refine ment with a example drawn from the party problem  Consider the problem of selecting a location for the party given uncertainty about the weather  Let the al ternatives for the location be  Outdoor   at  and  In door   a    and let the weather conditions be  Rain   xl  or  Sunny   a    Let  r denote the probability that it will rain  The utility values are   X  Outdoor Indoor  X  fi XIR e     i fp x p f  R e   is the operative distribution for the authentic distribu tion p XIe   Tani        Logan         The operative distribution is the distribution which the decision maker should use if no further assess ment is performed  Let    be the parameter that best approximates the operative distribution p XIe   i e   the numerical approximation fi XIe   fp X  is within satisfactory limits  This is different from    fp    p f  R e  which denotes the mean of the sec ondary distribution  The expected value of quantitative refinement on the uncertainty on X with respect to assessment procedure R  denoted EVRQU  R  is the difference between     and      Let us consider the case where the state variable X is discrete with two states    e    e    We are interested in the value of improving the probabilities assessed for p x le  and p x le   We denote the assessed values of p x le  and p x le  by  r and      r  respectively  The parameter which describes the primary distribu tion over X is f     r  and we have f   xl     r and   r              Sunny      r             The optimal locations as a function of  r are a   r     Outdoor   Indoor  if  r       if  r             g        Figure    The optimal party location as a function of the probability of rain   r   Let us suppose that the current uncertainty about  r can be described by a probability distribution whose mean is      In this case  the optimal decision  without further assessment  is to hold the party indoors  with an expected utility of       However  a more accurate assessment of the value of  r might change the optimal   Value of Decision Model Refinement  decision resulting in a potentially higher utility  With refinement     p  riR e v a   r   r   E viR e     where     r        O l r  v a   r    r     if  r       if  r                    O l r d r             o o  r U    o                     Notice that the above analysis was performed in the  r domain  An alternative analysis and perspective which will produce equivalent results can be performed in the p domains  This is done by a change of variables from  r to p  and p  via Equation        The resulting anal ysis would have to be displayed as a two dimensional graph         Outdoor Indoor     U                           Let us again use the party problem to illustrate the value of refining preferences  Since we can fix the util ity for the worst outcome  outdoor and rain  at zero  and the utility for the best outcome  outdoors and sunny  at one  we need only to consider the uncer tainty over further assessment of the values  jJ    in door and rain   and  P    indoor and sunny   Let the uncertainty over these values be  U               J    P     U              The operative values for the preference values are     Consider the case where  r is uniformly distributed be tween the interval             The expected value of re finement  EVRQU  R    is then                 r d r     r o   r          Preference Refinement  Let us now consider the expected value of quantitative refinement of preference EVRQP R   We seek to im prove the values of v ak x   for each k and i  Let  Pki denote the value that will be assessed  given that the refinement is carried out  Let p   JkiiR e  denote the uncertainty over the assessment for each v ak  Xi  The expected value  given that the quantitative refinement on preference is carried out  is E viR e    lw l mn p   Ju      Pmn IR e  mkaxp xde  Pki   Sunny                eu            The default choice without any further assessment is to hold the party indoors with an expected utility of        l         Jl        P          J   In the example  there is no uncertainty over p   The utility  Jl   displayed in Figure    is a linear sum of two uniformly distributed variables  with a triangular distribution p J t  R e   if       l           J t                 J t         if      Jl         otherwise and an expected value of                    The expected value without quantitative refinement on preference but with knowledge about the performance of R is       E viR e  m xi   x le ki  Rain                Figure    The pdf for p  Figure      shows the optimal value p    maxk Jlk as function of Jl   where Jl  is fixed at       The EVRQP R  is  i          J t              dJ t                          J t               dJ t                J t          J t dJ t                                       where  r  Pki p  PkiIe      ki is the operative utility value for v ak aki   The EVRQP R  is the difference between       and       This analysis can be related to the general formula tion in Equation      by defining the variable ki        for each action ak E A  The distributions p J tkIR e  can be derived from the distributions p  Pk IR e                Expected Value of Conceptual Refinement  We shall now explore measures of the value of con ceptual refinement       the value of refinement of the        Poh and Horvitz  The expected value given that the refinement is not carried out  but with knowledge about the perfor mance of R is E viR  e   J l               p xule k    p x  le  fok                       definitions of state variables  EVRc  R  and     the refinement of definitions of actions  EVRCA R          Assume that our current decision model has a state variable X   x  x   and decisions A  a  a     Now  let us consider the value of refining the state x  into x u and x    such that the resulting state vari able is X   xu X   x    We further assume that the probability of the refined states p xulx  e  and p  r    x     are known  As a result of the refinement  we need to assess the utilities v ak x i  for k      and j       Before these assessments are carried out  the values v ak  Xlj  are unknown  Let   lkj represent the utilities  ak x i  that will be assessed if the as sessment is performed  In addition  we assume that the decision maker is able to assess a set of probability distributions p ki IR e   k     and j      over these utilities  To assess the probabilities over the utilities  a possi ble conversation between the analyst and the decision maker might be as follows              p xule kl   p x   e k    p x ie v ak  x         for action ak  k       and deriving the distributions P J  k I R e  from the distributions p kjiR  e    To illustrate conceptual refinement  consider the ex pansion of the state of  Rain  into  Downpour  and  Drizzle   Assume that a decision maker s assessment of his uncertainty over the values of     outdoor and drizzle          indoor and downpour    and     indoor and drizzle  are as follows  v     R e  vC P  IR e  p  J   IR e   v           State Variable Refinement                      and j where  foki is the operative value to be used when no refinement is carried out  The EVRc  R  for refining state variable X to X  is just the difference between      and       As before  we can simplify this analysis and relate it to the general formulation of Equation     by defining the variable J lk    Figure    The optimal value JJ  as a function of jJ   p x i  v ak x    Jrf ki ki p kjl     k     In our previous conversation  you assigned a utility u for outcomes at your point of in difference between an outcome and a lottery with probability u for the best prospect and probability    u for the worst prospect  Sup pose I were to ask you to assess the utility of each of the refined outcomes  As we do not have an unlimited amount of time to assess these utilities  please give us an estimate now of the probabilities describing the utility val ues assessed if you were to have enough time to thoroughly reflect on your preferences and   knowledge about the outcomes   The expected value resulting from the conceptual re finement of X to X  is E vjR e    f p u               n R e      u    I                  t   p xule k   p xde k    p x le v ak x               We could also perform this assessment in terms of the utilities that would be assessed after some predefined amount of time for reflection   U            U             U                   The operative utilities are as follows   Outdoor Indoor  Rain      Down  Dnzzle pour                                 Sunny                 EV            Without refinement  the expected utility of holding the party outdoors is      and the utility of having the party indoors is       Since the two expected val ues are very close  further refinement might lead to a better discrimination between the two choices  In lieu of additional refinement  the default decision is to have the party outdoors  Based on the distributions over ki  we define J      J                                             f ll is uniformly distributed between      and i e  P J LI R e  U              while JJ  has a triangular distribution p JJ  R e   depicted in Figure  where                           J                   JJ           if      J         if       J       otherwise   Figure     shows the region over possible values of J    and J    The EVRc  R  is                       JJ        J   dJ l              Value of Decision Model Refinement       The expected value without the conceptual refinement on action is  E  vJR e     where Uki      Figure    The pdf for J l   jill          J l                  J l dJ l     Jir a ll           J l          J l dJ l   dJ l                                                                            llt           i           q n  p         n lR  e    Tt   a k  where Uki      v a k x     Ji   p xde uki         if k       if k     ifk      if k     The EVRCA R  for refining action A to A  is then the difference between      and       We can relate these results to the general formulation of Equation     by defining the variable       for each action ak  Note that J l  and J l  are determin istic  while the probability distribution p J laJR e  can be derived from the distributions p    IR  e    Let us consider the refinement of the example prob lem with the addition of a third action which to hold the party on the porch  aa   To complete the refine ment  we must assess the utility values    porch and downpour      porch and drizzle   and a  porch and sunny   For simplicity  we will assume that the deci sion maker is certain about the value of a  which is       His uncertainty over   and   are U            U             The operative utilities are as follows   Action Refinement  Similar to extending the conversation about the def inition of states  the set of decision alternatives may be increased with continuing modeling effort  Con sider the conceptual refinement of action A   a   a    by the addition of action aa  Let A      a   a  aa   Unlike state variable refinement  the set of refined ac tions need not be mutually exclusive  Indeed  they need not even be mutually exhaustive as some alterna tives can be ruled out immediately  based on common sense knowledge or dominance relationships  Wellman         As the result of action refinement we need to assess the utilities v aa x   for all x  E X  Let   de notes the utility v aa x   for each i  and let p  JR  e  be the uncertainty over each assessment  The expected value offered by the refined model is E vlR       a     UJtt v ak Xi        Figure    The region of values for J l  and J l  where J l   maxk J lk        max L p x Je uki  k        a  Outdoor Indoor Porch  Rain     Down  Dnzzle Sunny pour                                                   EV                 The optimal action without further refinement is to hold the party outdoors  with an expected utility of        J l  J l  J la               t             a  There is no uncertainty on J lt and J l  However  as dis played in Figure    J la is a linear sum of two uniformly distributed variable and has a triangular distribution of the form  p J laJR e             J la               J la          if       J la       if       J l        otherwise  and has an expected value of               Poh and Horvitz  a parameter f y  such that the numerical approxima tion p Y IR e    f  y  Y  is within satisfactory limits  We let Px Y represent the parameter for the distri bution p XIY R e    Let the distributions p  By IR e  and p  BYIX IR e  represent the decision maker s un certainty about the parameters  By and Pxw  respec tively  The expected value that results from the struc tural refinement via the addition ofY as a conditioning variable for X  is E v iR e   Figure    The pdf for J  L      Figure     shows the optimal value J L    max   J lk as a function of J  La  The expected value of conceptual refinement via addition of the third alternative is                      J  La               dJ  La                   J  La               dJ  La     J  La         J  LadJ  La                                                       Figure    The optimal value J L  as a function of J  La      Expected Value of Structural Refinement  Figure    Structural refinement on node X Finally  we consider the value of structural refine ment  EVR  R   the value of increasing the number of conditioning variables  Figure   depicts an exten sion of conversation based on structural refinement of the state variable X of our simple decision prob lem by the addition of Y as a conditioning event for X  For example  in the party problem  we may iden tify  wind speed  as a conditioning variable on the forthcoming weather  We are interested in analyzing the additional value that is gained by the addition of Y as a conditioning variable for X  This struc tural refinement requires the assessment of the prob ability distributions p YIR e   and p XjY  R e    As before  we assume a functional form f where  for ev ery reasonable distribution for p YIR e     there exists        The expected value without structural refinement is  f f Y  y  Jxf fXIY x v a   x        E   v lR  e   max k  y           f p  ByjR e  f P PYixlR e  Jf y Jf YIX m x    y y     xiY x v a   x          where  Jy and Px Y are the parameters for the opera tive distributions fi YIR e      f  jy  Y p pyIR e   y  Y  J jy    and  f fr y X p Pxw IR e  fXIY X  jf xiY respectively  The EVR  R  for the variable X  with respect to adding a new conditioning event Y   is just the difference between      and       The case where X and Y are discrete variables is treated in  Poh   Horvitz         A special form of structural refinement is the famil iar expected value of information  EVI   Within the influence diagram representation  we can view the ob servation of evidence as the addition of arcs between chance nodes and decisions  We describe the relation ship of EVI and other dimensions of model refinement in  Poh   Horvitz         p XjY  R e       Control of Refinement  Measures of EVR  computed from a knowledge base of probabilistic expertise about the progress of model re finement  hold promise for providing guidance in con trolling decision modeling in consultation settings  as well as within automated decision systems  In this sec tion  consider control techniques for making decisions about the refinement of decision models       Net Expected Value of Refinement  So far  we have considered only the value of alternative forms of effort to expending effort to refine a model  To consider the use of EVR measures  we must balance the expected benefits of model refinement with     the cost of the assessment effort  and     the increased   Value of Decision Model Refinement  computational cost of solving more refined  and po tentially more complex  decision models  We define the the net expected value of refinement  NEVR  as the difference between the EVR and the cost of mak ing a refinement and increase in the cost of solving the refined model  That is NEVR R  t       EVR  R t     J  Ca ta    Cc b  tc    where R t  is a refinement parameterized by the time expended on a particular refinement procedure  Ca is a function converting assessment time  ta to cost  and Cc is a function converting changes in the expected com putational time  required to solve the decision prob lem  b  tc   to cost  In offline  consultation settings  we can typically assume that changes in computational costs  associated with the solving decision models of in creasingly complexity  are insignificant compared with the costs of assessment  We can introduce uncertainty into the costs functions with ease          Decisions about Alternative Refinements  Let us assume that we wish to identify the best refine ment procedure to extend a decision model  For now  let us assume that we have deterministic knowledge about the cost of refinements  We shall assume that the cost is a deterministic function of time  and that computational changes with refinement are insignifi cant  We can control model building with a strategic op timization  Horvitz        that seeks to identify the best refinement procedure and the amount of effort to allocate to that procedure  i e        arg maxEVR    R t     J  C t  R t Given appropriate knowledge about decision model re finement  we solve such a maximization problem by computing the ideal amount of effort to expend for each available refinement methodology  choose the pro cedure R  with the greatest NEVR  and apply it for the ideal amount of time  t  computed from the max imization  We halt refinement when all procedures have NEVR R  t      for all times t  However  we need not be limited to considering single procedures  In a more general analysis  we allow for the interleaving of arbitrary sequences of refinement procedures  where each refinement procedure can be allocated an arbitrary amount of effort  and to con sider sequences of refinements with the greatest ex pected value  As any refinement changes a model  and  thus  changes the value of refinement for future model ing efforts  the identification of a theoretically optimal sequence requires a combinatorial search through all possibilities  Let us consider several approximations       A practical approach to dodging the combinatorial control problem is to consider predefined quantities of effort  and to employ a myopic or greedy EVR control procedure  With a greedy assumption  we simplify our analysis of control strategies by making the typically invalid assumption that we will halt modeling  solve the decision model  and take an action following a sin gle expenditure of modeling effort  We can further sim plify such a myopic analysis by assuming a predefined  constant amount of effort to employ in NEVR anal yses  We compute the EVR R T   for all available refinement procedures R  where T is some constant amount of time  or a quantity of time TR T R   a constant amount of time keyed to specific procedures  At each cycle  we compute the NEVR for all proce dures  and implement the refinement procedure with the greatest NEVR  We iteratively repeat this greedy analysis until the cost of all procedures is greater than the benefit  at which time we solve the decision prob lem and take the recommended action  Figure      shows a fragment of the graph of possible model re finement steps         I                       quantitative        structural  J                 quantitative         I   structural           conceptual                      o   o    o   o                quantitative              quantitative  Figure     Greedy control of model refinement with iterative application of NEVR analyses We can relax the myopia of the greedy analysis by al lowing varying amounts of lookahead  For example  we can consider the NEVR of two refinement steps  Such lookahead can be invoked when single steps yield a negative NEVR for all refinement methods  We can also make use of theoretical dominance results  For ex ample  we have shown in a more comprehensive paper that the expected value of perfect information  EVPI  is the upper bound on the value of any structural re finement  Poh   Horvitz          to such an exhaustive search     n practice  a decision consultant may wish to consider such multiattribute cost models as the cost in time  dollars  and frustration associated with pursuit of different kinds of assessments and refinements      Discussion and Related Work  The value of the EVR methods hinges on the avail ability of probability distributions that describe the        Poh and Horvitz  outcomes of extending models in different ways  We suspect that expert analysts rely on such probabilis tic modeling metaknowledge  and that relatively stable probability distributions can be assessed for prototyp ical contexts and states of model completeness  We do not necessarily have to rely on assessing an expert deci sion analyst s probability distributions about alterna tive outcomes of modeling  In an automated decision support setting  we can collect statistics about model ing and modeling outcomes  Such data collection can be especially useful for the application of EVR based control strategies to automated reasoning systems that construct models dynamically  Breese        Goldman   Breese         We are not the first to explore the value of modeling in decision analysis  The value of modeling was first addressed by Watson and Brown        and Nickerson and Boyd         The notion of reasoning about the value of probability assessment with an explicit consid eration of how second order distributions change with assessment effort has been explored rigously by Lo gan         Chang and Fung        have considered the problem of dynamically refining and coarsening of state variables in Bayesian networks  They specified a set of constraints that must be satisfied to ensure that the coarsening and weakening operations do not affect variables that are not involved  In particular  the joint distribution of the Markov blanket excluding the state variable itself must be preserved  However  the value and cost of performing such operations were not addressed  The form of refinement that we re fer to as structural refinement has also been examined by Heckerman and Jimison        in their work on attention focusing in knowledge acquisition  Finally  related work on control of reasoning and rational deci sion making under resource constraints  using analyses of the expected value of computation and considering decisions about the use of alternative strategies and al locations of effort  has been explored by Horvitz              and Russell and Wefald            Summary and Conclusions  We introduced and distinguished the expected value of quantitative  conceptual  and structural refinement of decision models  We believe that the analyses of the value of model refinement hold promise for controlling the attention of decisions makers  and of automated reasoning systems  on the best means of extending a decision model  Such methods can also be employed to determine when it is best to halt refinement proce dures and instead to solve a decision model to identify a best action  We look forward to assessing expert knowledge about the value of decision model refine ment and testing these ideas in real decision analyses  We are striving to automate the assessment of knowl edge about model refinement  as well as the iterative cycle of EVR computation  We are implementing key ideas described in this paper within the IDEAL influ   ence diagram environment  Srinivas   Breese         Reference  Breese  J  S           Knowledge Representation and Inference in Intelligent Decision Systems  Ph D   thesis  Department of EES  Stanford University  Chang  K  C     Fung  R          Refinement and coarsening of bayesian networks  In Proceedings of the Sixth Conference on Uncertainty in Arti ficial Intelligence  pp            Goldman  R  P     Breese  J  S          Integrating model constrution and evaluation  In Proceed ings of the Eighth Conference on Uncertainty in Artificial Intelligence  pp            Heckerman  D  E     Jimison  H          A perspective on confidence and its use in focusing attention during knowledge acquistion  In Proceedings of the Third Workshop on Uncertainty in Artificial Intelligence  pp            Horvitz  E  J          Reasoning about beliefs and ac tions under computational resource constraints  In Proceedings of the Third Workshop on Uncer tainty in Artificial Intelligence  pp           Horvitz  E  J          Computation and action under bounded resources  Ph D  thesis  Depts of Com puter Science and Medicine  Stanford University  Logan  D  M          The Value of Probability Assess ment  Ph D  thesis  Department of Engineering Economic Systems  Stanford University  Nickerson  R  C     Boyd  D  W          The use and value of models in decision analysis  Operations Research                  Poh  K  L     Horvitz  E  J          Probing the value of decision model refinement  Technical Report     Palo Alto Laboratory  Rockwell In ternational Science Center  Palo Alto  CA  Russell  S     Wefald  E          Principles of metar easoning  In Brachman  R  J   Levesque  H  J     Reiter  R   Eds      KR     Proceedings of the F irst International Conference on Principles of Knowledge Representation and Reasoning  pp           Morgan Kaufmann  Srinivas  S     Breese  J          IDEAL  A software package for analysis of influence diagrams  In Proceedings of the Sixth Conference on Uncer tainty in Artificial Intelligence   Tani  S  N          A perspective on modeling in de cision analysis  Mangt Sci                     Watson  S  R     Brown  R  V          The valua tion of decision analysis  Journal of the Royal Statistical Society       Part             Wellman  M  P          Formulation of tradeoffs in planning under uncertainty  Ph D  thesis  De partment of EECS  MIT     
 Probabilistic conceptual network is a knowl edge representation scheme designed for reasoning about concepts and categorical abstractions in utility based categorization  The scheme combines the formalisms of ab straction and inheritance hierarchies from artificial intelligence  and probabilistic net works from decision analysis  It provides a common framework for representing con ceptual knowledge  hierarchical knowledge  and uncertainty  It facilitates dynamic con struction of categorization decision models at varying levels of abstraction  The scheme is applied to an automated machining problem for reasoning about the state of the machine at varying levels of abstraction in support of actions for maintaining competitiveness of the plant   Figure    Using a pc net in utility based categorization    Introduction  A probabilistic conceptual network  pc net  is a knowledge representation scheme designed to support utility based categorization   Poh         In contrast to the traditional approaches which are logic and similarity based   Smith   Medin         utility based categorization considers the usefulness of the infor mation conveyed by the concepts  the actional con sequences  the desirability of the consequences of ac tions  the computational or cognitive resource require ment and availability  and the uncertainty about the environment   limited observations  It must conceptualizes the sit uation and decide on the most appropriate course of action  It does so by solving a categorization decision model  However  different models at different lev els of categorical abstraction can be used  Each of these models has different expected value of the recom mended action and different computational resource requirement  The agent must therefore decide on the best level of abstraction to construct the model so as to achieve the best trade off between the expected value of the recommended action and cost of computation   We have developed a decision theoretic approach for utility based categorical reasoning as shown in Figure    in contrast to previous work on abstraction in prob abilistic reasoning   Horvitz  Heckerman  Ng    Nath wani        Horvtiz   Klein        which were more narrowly focused  In our view  a resource constrained agent operating in an uncertain world is given a set of  A probabilistic representation of conceptual categories called a pc net is used to represent the agent s knowl edge about the world  A level of conceptual abstraction for a building a model is obtained by selecting a con ceptual cover from the pc net  As illustrated in Figure    a conceptual cover is obtained by selecting a set of mutually exclusive and exhaustive concepts from dif ferent levels in the pc net    Also at Dept  of Industrial   Systems Engineering  N a tiona  University of Singapore  Kent Ridge  Singapore          The notion of conceptual coverage in abstraction hier    Probabilistic Conceptual Network  We have developed an incremental algorithm whereby the reasoner iteratively specializes or generalizes the conceptual cover  A concept is specialized by breaking it up into a set of more specific subconcepts  A group of concepts may be generalized by replacing them with a single super concept  At each iteration  changes are made in order to achieve the highest expected im provement in overall utility  Poh         The proce dure applies the principles of decision theoretic control  Horvitz              Fehling   Breese        Russell   Wefald        to iteratively decide between alloca tion of additional resources to refine the current set of concepts  or to act immediately based on the cur rent action recommended by the model  This model refinement approach is a special application of a more general approach for refining general decision models  Poh   Horvitz         In this paper  we describe probabilistic conceptual net works and show how they may be used to repre sent both categorical and uncertain knowledge and to facilitate the dynamic construction of categoriza tion decision models at varying levels of abstraction  We present an example from automated machining  We also compare our scheme with similarity net works  Heckerman        and other approaches to knowledge based decision model construction     Integrating Uncertainty and Categorical Knowledge  To perform utilitY  based categorization  an intelligent actor must expres  different dimensions or perspectives of knowledge  First  she must be able to express cat egorical knowledge with some degree of modularity  Categorical knowledge expresses facts about individ ual concepts in a given domain  i e   it describes the features and properties that characterize the concepts  Second  the actor must represent categorical relations  e g  how one concept subsumes others  In particular  the actor  when problem solving  must decide which concepts to use and at which levels of abstraction in order to obtain a useful solution  In artificial intelligence  abstraction hierarchies and semantics nets  Lehmann        are graph based for malisms that have been advocated for computer rep resentation of concepts and categorical knowledge  They organize conceptual knowledge in levels of ab straction and make use of  inheritance  mechanisms whereby concepts may share features and properties with higher level ones  Since feature information need only be stored at the highest possible level of abstrac tion  maximum elegance and economy of storage is achieved  These formalisms  however  are not easily amenable to representing uncertainty in an elegant and efficient manner  In reasoning and decision making under uncertainty  archies arose in discussions with Eric Horvitz        specialized graph based formalisms like influence di agrams  Howard   Matheson        have been ad vocated for computer representation of probabilistic knowledge and decision models  These formalisms fo cus on the dependencies among the probabilistic vari ables  They encode probabilistic models as directed graphs with the nodes representing the uncertain vari ables and the directed arcs denote possible probabilis tic dependence between variables  Each node encodes a conditional probability distribution of that node s variable given each combination of values of its direct predecessors nodes  Various techniques have been de veloped over the last decade for probabilistic inference and reasoning using this representation  see for exam ple Pearl         Pc nets combine the formalisms of influence diagrams with inheritance hierarchies by representing concepts with influence diagrams and then linking these con ceptual diagrams in a hierarchy  By do so  pc nets are able to capture the best features of both formalisms and to use them effectively in support of utility based categorization     An Application  m  Automated  Machining  We will illustrate the use of pc nets in utility based categorization with a real world example of an auto mated machining problem  This is similar to an appli cation described by Agogino and Ramamurthi         Unattended or automated machining operations are important parts of any intelligent manufacturing sys tem  It requires the automation of the human op erator s efforts to monitor and make appropriate ad justments to the state of the machine  An automated machining system typically has sensors which acquire data on     dimensions of the workpiece      acous tic emission from the machining processes      cutting forces  dynamometer readings   and     electric cur rent  ammeter   etc  These data are then used to determine the state of the machine and appropriate action or actions are taken to ensure the continuous operation of the plant so as to minimize production cost  thereby maintaining competitiveness  The possi ble states of the machining process at various level of abstraction are illustrated in Figure    At the most abstract level  the state of the machin ing process is either  within variability limits  or  out of variability limits   Refining the concept  out of variability limits  are  tool failure    sensor failure  and  transient state   The latter occurs during entry or exit of the cutting tool into the workpiece  Re fining  tool failure  are  tool chatter  which is typi cally characterized by an event in which an acoustic emission signal increases dramatically in amplitude as does the frequency content of the dynamometer  If left unchecked  tool chatter can result in tool  workpiece or machine damage  Remedies for this problem include   Poh and Fehling       Probabilistic Conceptual Network                             oo    t       I       t l t l tool wear chatter breakage I  I  tf                                       I  tooi failure I I  A           fiR      t          I I                           tr          acoustic dynamo  ammeter sensor meter  I I  I      nt     tool entry         tOQl eXIt       rl  ve cal ho zontal chatter cnatter  rti  Figure    Hierarchy for states of a machine reducing the depth of cut or reducing the feed rate   Tool wear  is typically characterized by a gradual in crease in acoustic emissions  and by a gradual increase in cutting force as measured by the dynamometer  A tool that is worn out needs to be re sharpened or re placed in order to achieve the desired surface finish and dimensional tolerances   Tool breakage  is typi cally characterized by an acoustic emission exhibiting a hig amplitude peak at the moment of tool fracture  and followed by a sharp drop in signal amplitude to a level below that of normal  It is also characterized by a large rise in cutting forces  followed by a drop before finally continuing at a value above the average  Tools that are broken cannot perform any machining task and must be replaced immediately  This problem is interesting because under differing op erating conditions and situations  different levels of abstraction in monitoring and reasoning may be de sired  For example  if the tool has been changed re cently  giving it a low prior probability that it will fail soon  it may be more worthwhile to only moni tor at a more general level  i e    tool failure    sensor failure  and  transient state   rather than spent ex tra resources to differentiate the finer details  In other words  the expected value of the information derived from using more detailed concepts may not justify the required additional computational resources  On the other hand  if the tool has already been in used for a long time  then it might be worth the extra effort spent in monitoring and reasoning with more detailed concepts  like for example at the level of  tool chat ter    tool wear    tool breakage    sensor failure  and  transient state   Also if the material currently being machined is a difficult one  e g   a high carbon steel  which is known to have caused occasional tool break age  then it may also be worthwhile to monitor at a deeper level of detail  In another possible scenario  suppose the some critical sensors are out of order  then the only level of detail available might be at the most abstract level whereby only two possible states are be ing monitored  The operator would then need to be alerted to take any corrective action   Definitions  A probabilistic conceptual network  pc net  consists of a probabilistic concept hierarchy  pc hierarchy  con necting a set of probabilistic concept diagrams  pc diagrams   Each node in the pc hierarchy repre sents a concept  and the links in the hierarchy spec ify subsumption relations among the concepts thereby organizing the concepts at various level of abstrac tion or specificity  Associated with each subsumption link is a value indicating the conditional probability that a concept holds given that its immediate super concept holds  Individually  each concept within the pc hierarchy is represented by a pc diagram  We may visualize a pc net as a hierarchical organization of pc diagrams  A pc diagram for a concept is a special probabilistic influence diagram  pid    representing the knowledge about the probabilistic relations between the concept and the features that characterize it  The concept is represented as a deterministic node while the features are represented by chance nodes  As a convention  we direct arcs by default  from the concept to its feature nodes  For each feature node F in the pc diagram for concept ck  we store a probability distribution of the form k p FICk  B  F   where Bk  F  is the set of conditional predecessors  possibly empty   that excludes Ck  We shall assume that background information e is used in all the prob ability distributions  We represent Ck as a determin istic node because we do not need the distribution p Fj   Ck  Bk F    A pc diagram for a concept pro vides information for discriminating that concept from other concepts in a domain  Pc diagrams allow knowl edge to be represented locally providing modularity in the knowledge base  The value of a pc net emanates from its ability to sup port utility based categorization  As shown previously in Figure    given a pc net together  a conceptual cover can be selected at some level of abstraction to con struct a categorization decision model corresponding to that level of abstraction  We shall describe the pro cedures for model construction in Section    Finally  pc net uses an inheritance mechanism whereby a concept may share information about features from a concept higher up the hierarchy  It does so by tak ing advantage of a form of conditional independence called subconcept independence  which is not conve niently represented in ordinary influence diagram rep resentation  A feature is said to be subconcept inde pendent of a concept if knowledge about the feature   A pid is an influence diagram with only probabilistic nodes and conditioning arcs    Section     compares subconcept independence with  subset independence  in similarity networks    Probabilistic Conceptual Network  Feature  Description  AE mag  acoustic emission magnitude  AAE mag  change in acoustic emission magnitude  AE freq  acoustic emission frequency  dyn freq x  cutting force frequency in x direction  dyn freq y  cutting force frequency in y direction  AE mean  mean of the acoustic signal change in the mean of the acoustic signal  AAE mean dyn rms x  cutting force in the x direction  Adyn rms x  change in cutting force in the x direction  dyn rms y  cutting force in the y direction  Adyn rms y  change in cutting force in the y direction  AE peak  acoustic emission peak value  dyn peak x  peak cutting force in x direction  dyn peak y  peak cutting force in y direction  current  motor current  Table    Descriptions of features does not affect the agent s belief about any of that concept s subconcept  We will have more to say about subconcept independence in Section           Automated Manufacturing Example       and the most specific subsumer of a set of concepts S     C       Cn  is denoted by   C          C   or  S   p C ACj e  ci we h avep CiICi  Suppose C   p Ci       But C   Cj implies that p Ci    Cj    p Cj  There fore       In other words the subsumption probability is simply the ratio of the prior probabilities of the concepts it connects       Feature Relations and Conceptual Abstraction  Suppose we have already assessed a set of pc diagrams  we can combine them to produce a more general super concept  For example  given the pc diagrams for  tool chatter    tool wear   and  tool breakage   we can obtain the pc diagram for  tool failure   In general  given Ck  set of its most general subsumees   ck   and suppose Bk F    UciE Ck Bi F  then p F Ck  Bk F     k k l  ciE ck  p FJCj  Ck  B  F  p Ci Ck  B  F    The feature F is independent of Ck given any subcon cept Ci of Ck since once Ci is known to be true then any information about ck will not have any further effect on our belief on F  This implies that  Fi Ci  Ck  B k F     p Fi Ci  Bk F    Like wise  Cj is independent of Bk F  given Ck  Hence p F Ck  Bk F     L p F Cj Bk F  p Cj Ck   CjE Ck   Figure    The pc diagram for  tool chatter   Figure   shows the pc diagram  tool chatter   This diagram comprises a deterministic node representing  tool chatter  and a number of feature nodes whose descriptions are given Table    An arc between two feature nodes indicates that these two features may not be conditionally independent given the concept  tool chatter   For example  the arc between the node  AE mag  and the node  AE mag  indicates that information about the current magnitude of acoustic emission may provides information about the change in magnitude of acoustictemission  The direction of this arc could be reversed without any change in as sertion about possible dependency  Figure   shows a fragment of the full pc net for the automated machining showing the concepts  tool fail ure    tool chatter    tool wear    tool breakage    sensor failure   and  transient state        Probabilistic Subsumption Relations  We shall denote the fact C  is a subconcept of Cj by C   Cj  The set of the most general subsumees  i e  all the direct subconcepts  of Ck is denote by  Ck     which may be rewritten as     p F Ci  Bi F   Bk F    Bi F  p Ci Ck  CjE Ck  But the set of conditioning features Bk F    Bi F  is independent ofF given Cj  Hence p F Ck  Bk F           CjE Ck   p Ci Ck p F Ci  Bi F     Hence if Ck is a concept in the pc net and all the pc diagrams for the concepts in   ck  has been as sessed  then the pc diagram for ck may be derived from those of its subconcepts  Formally  for any fea ture F  p F Ck  Bk F    L  CjE Ck   where Bk F      p Cj Ck p F Cj  Bi F         i F   U ci E  Ck   B  Equation     allows us to build the pc net from bot tom up by propagating the probability distributions in the pc diagrams from the bottom of the hierarchy up to the root of the hierarchy  This allows us to build   Poh and Fehling                          p   I I I  I I  I I I I I  o I I I   I          I I I  I  I I I  I    I I        I I I  I  Figure    A fragment of the pc net for the automated machining problem the pc net by first constructing the pc diagrams for all the terminal or atomic concepts  and then the pc diagrams for the more general concepts may be derived from the pc diagrams below them  However  it is pos sible to simplify the pc net by identifying subconcept independence and take advantage of inheritance       Feature Inheritance for Subconcept Independent Concepts  The principle of inheritance in pc net is based on a special type of independence that can hold among con cepts and features  Formally  we say that a feature F is subconce pt independent of a concept C  given B  if and only if     for all feature values f ofF and for all subconcepts C  of Ck  Intuitively  information about a feature that is subconcept independent of a concept does not affect the agent s belief about any of that concept s subcon cepts  An equivalent criterion for subconcept indepen dence is obtained using using Bayes  rule   The last equation applies that for any pair of sub concepts C  and Cj of Cc  i e   p FjC     p FICJ  Conversely  if the last equation holds then using equation      p FIC     Lj p CJICk p FICJ  p FIC   Lj p CjiCk  Lj p CjiCk p FIC    p FjC    Hence an equivalent criterion for subconcept independence is   We shall denote by F   lCkjB  the fact that F is sub concept independent of Ck given B  In cases where the background knowledge is understood  the B may be omitted  An interesting property about   l is that once it has been established for a concept  it recur sively applies to all of its subconcepts  Poh         That is   The justification for the application of inheritance for subconcept independent concepts for a feature is due to equations     and      Since the probability dis tributions for the feature are identical  we need only store them at the highest possible position  To illustrate the idea of inheritance  consider the frag ment of the pc net for  transient state    tool exit  and  tool entry  shown in Figure    The feature  rms current  is subconcept independent of  tran sient state   We do not need to explicitly store the probability distributions for  rms current  in the pc diagrams for  tool entry  and  tool exit   That is  we may  omit  these probability distributions  and hence the corresponding feature nodes  in their respective pc diagrams  When needed  the probability values are filled in by inheriting them from  transient state     Probabilistic Conceptual Network              Model Construction Constructing Categorization Decision Models  Figure    The categorization decision model  Figure    The categorization prob  influence diagram We shall illustrate how a categorization decision model may be constructed from the pc net for the automated machining problem  In this application  the prefer ence model may be expressed in the form v Ak  C   where Ak is an action that may be taken  like for ex ample   reducing cutting speed    reducing depth of cut   etc  C  is any state of the machining operation we have described earlier  v Ak  C   gives the utility of the outcome by taking action Ak when the state of the machining operation is C   Suppose the sensors report information on  AE mag    AE rms    dyn rms x    dyn rms y   and  rms current   and  our utility based categorical rea soner described earlier  determines that the most ap propriate level of abstraction corresponds to the set of concepts comprising  tool chatter    tool wear    tool breakage    sensor failure  and  transient state   We can combine the respective pc diagrams for these five concepts to construct a categorization probabilistic in fluence diagram as shown in Figure    The graphical structure of the combined categorization influence di agram is obtained by performing graphical union of the individual pc diagrams while treating each central concept node as being the same node in each of the individual pc diagrams  Notice that the concept node in the constructed diagram is now a probabilistic vari able  C  ranging over the five concepts used in its con struction  The conditional probabilities for each of the feature nodes in the constructed diagram is obtain by copying over their respective original values in the in dividual pc diagrams  That is  for any feature F  p FIC   C  B  F     p F IC  B  F       where Bg  F  is the set conditional predecessors of F excluding C  in the constructed diagram  The next step in the construction procedure is to com plete the diagram by turning it into a categorization decision model as shown in Figure    This is done by first  adding the decision and value node to reflect  the preference model described earlier  Next  infor mational arcs from the observed feature nodes to the decision node are added  The completed categoriza tion influence diagram can now be solved using exist ing methods  Shachter              Validity of the Constructions  An important characteristic of our decision model con struction procedure is that the final model so con structed must reflect as accurately as possible the state of information originally asserted by the knowledge base and preference model  Our knowledge base con tains assertions about concepts  their properties  and the probabilistic relationships among them  Validity of a probabilistic model construction depends on the soundness of the construction procedure  Heckerman        suggests that soundness should be character ized by the preservation of the joint distribution of the variables involved across the construction  For pc net  it can be shown that if the pc diagrams in a given con ceptual cover are mutually consistent  then the con struction is indeed sound  Poh                Related Work Probabilistic Similarity Networks  Probabilistic similarity network  Heckerman        is a knowledge engineering tool for building probabilistic influence diagrams  We shall briefly describe the sim ilarities and differences between pc net and similarity network here  A more comprehensive comparison is available in  Poh         Both pc net and similarity networks are capable of building the same type of influ ence diagrams  but pc net is able to do so at varying levels of abstraction  whereas similarity network can only do so at one level  Another major difference is that pc net is capable of representing categorical ab straction relations whereas similarity networks can t  Another difference is that the probabilities in a pc net are assessed before categorical reasoning and model        Poh and Fehling  construction take place whereas in similarity networks  all the knowledge maps are initially unassessed and are carried out only after the global knowledge map has been built  Both pc net and similarity network use some sort of local influence diagrams for concept representation  However  a local knowledge map in similarity network is built based on a pair of concepts  There are also differences between a pc diagram and a hypothesis specific knowledge map  hs map  in similarity net works  First  the concept node is included in the pc diagram  whereas  it is not part of the hs map  Second  a pc diagram is always a connected graph whereas a hs map may not be  Finally  a pc diagram has its probabilities initially assessed whereas  a hs map is not  The notion of subconcept independence in a pc net is analogous to subset independence used in conjunc tion with partitions in similarity networks  Similarity networks use partitions to speed up assessment while pc net saves assessments and storage by using inheri tance mechanisms based on subconcept independence  In pc net terms  a partition for a feature in similarity network can be viewed as an an abstracted concept subsuming all the concepts in the partition  Further more  that feature is subconcept independent of the abstracted concepts  Assessing the probability distri bution for the feature given the abstracted concept and applying inheritance is equivalent to assessing the probabilities within the partition       Knowledge Based Model Construction Methods  Several approaches have been proposed for construc tion or building of influence diagrams  There ap proaches may be classified under two highly contrast ing methodologies  The first  known as the synthetic approach   Horvitz        starts with the empty influ ence diagram  nodes and arcs are added to the model through some methods of inference based on simple rules or relationships  These inferences are usually driven by assertions about the world  goals  or utility  Holtzman        Breese        Goldman   Charniak        Wellman         These approaches however  usually do not have principled control over the degree of abstraction or details in the model that they are building other than using some heuristics  The second  known as the reduction approach   Horvitz        seeks to custom tailor comprehensive  intractable decision problems to specific challenges at run time through a pruning procedure that removes irrelevant distinctions and dependencies  Heckerman   Horvitz         The decision model construction approach based on probabilistic conceptual networks developed in our re search does not commit to either of these two contrast ing approaches  but instead  employs mixed strategies  The approach can be seen as synthetic to some ex tent in that it builds an influence diagram dynami   cally at runtime  However  unlike the pure synthetic approaches  the building blocks used by this approach are not individual nodes and arcs  but rather modules of localized influence diagrams  On the other hand  the approach can be seen to be reducible in that mod ules of local influence diagram have been pre assessed  However  instead of pre assessing a comprehensive in fluence diagram  pc net does not commit to one large influence diagram  but instead  is a comprehensive net works of related local probabilistic influence diagrams  The approach here allows for reasoning about the re lationship among these local influence diagrams  and combines only those that are relevant or are required while discarding those not required in the decision model it is building  The advantage of our approach over that of the com prehensive model reduction approach  is that assess ing smaller and more focused local pc diagrams is usu ally easier and more manageable as compared with at tempting to assess a huge comprehensive influence di agram  This local to global approach to constructing large probabilistic influence diagrams has been demon strated with similarity networks  The advantage of this approach over that of the com plete synthetic approach is that the construction pro cedure is controlled using well founded principles of decision theory  We use a principled approach to rea son about the values of constructing different parts of the model  The model being built can be custom tailored to the optimal level of abstraction and avoid any unnecessary details  This is very important when we consider computational or resource constraints     Conclusion  Previous work on integrating uncertainty   and cate gorical knowledge representation has been done with a broad range of emphases and purposes  Saffiotti        proposed a general framework for integrating categorical and uncertainty knowledge  In particu lar  Shastri        proposed a semantic network like representation language for evidential reasoning using the principle of maximum entropy  Similarly  Lin and Goebel        proposed a graphical scheme integrat ing probabilistic  causal and taxonomic knowledge for abductive diagnostic reasoning  This latter formalism has two types of links  namely  is a  and  causal   In classifier based reasoning  term subsumption lan guages are being extended to accommodate plausible inferences  Yen   Bonisson         More recently  Leong        proposed a network formalism using var ious kinds of links including  a kind of   temporal precedence  qualitative probabilistic influence  Well man        and property relations    Context      Many of these formalisms have desirable features that we need  but none has all  Finally  by combining the formalisms of influence dia grams and abstraction hierarchies  pc nets effectively   Probabilistic Conceptual Network  represent both categorical knowledge relations and uncertainty in a modular and compact way  It can also support dynamic construction of a specific class of decision model at varying levels of abstraction  We have also demonstrated the applicability of pc net to real world applications in automated machining  Acknowledgements  We wish to thank Ross Shachter  Eric Horvitz and the anonymous referees for their helpful comments and suggestions on the content of this paper  Reference  Agogino  A  M     Ramamurthi  K          Real time influence diagrams for monitoring and controling mechanical systems  In Oliver  R  M     Smith  J  Q   Eds    Influence Diagrams  Belief Nets and Decision Analysis  pp           John Wiley  Knowledge Representation and Inference in Intelligent Decision Systems  Ph D   Breese  J  S           thesis  Department of EES  Stanford University  Fehling  M  R     Breese  J  S          A compu tational model for decision theoretic control of problem solving under uncertainty  Technical Memo           Rockwell International Science Center  Palo Alto Laboratory  Palo Alto  CA  Goldman  R  P     Charniak  E          Dynamic con struction of belief networks  In Proceedings of the  th Conference on Uncertainty in Artificial In telligence  pp          Beckerman  D  E          works  MIT Press   Probabilistic Similarity Net  Beckerman  D  E     Horvitz  E  J          Problem formulation as the reduction of a decision model  In Bonissone  P  P   Henrion  M   Kana   L  N     Lemmer  J  F   Eds    Uncertainty in Artificial Intelligence    pp           Elsevier Science  Holtzman  S          Addison Wesley   Intelligent Decision Systems   Horvitz  E  J          Reasoning about beliefs and ac tions under computational resource constraints  In Proceedings of th  Third Workshop on Uncer tainty in Artificial Intelligence  pp           Computation and action under bounded resources  Ph D  thesis  Depts of Com  Horvitz  E  J           puter Science and Medicine  Stanford University  Horvitz  E  J          Problem formulation and deci sions under scarce resources  In Working Notes of the AAAI Workshop on Knowledge Based Con struction of Decision Models   Horvitz  E  J   Beckerman  D  E   Ng  K  C     Nath wani  B  N          Heuristic abstraction in the decision theoretic pathfinder system  Report KSL        Stanford University        Horvtiz  E  J     Klein  A  C          Utility based ab straction for categorization  Technical Memoran dum     Rockwell International Science Center  Palo Alto Laboratory  Howard  R  A     Matheson  J  E          Influence diagrams  In Howard  R  A     Matheson  J  E   Eds    Readings on the principles and applica tions of decision analysis  Vol     pp                 SDG  Menlo Park  California  Lehmann  F   Ed            Semantic Networks in Ar tificial Intelligence  Pergamon Press  Leong  T  Y          Representing context sensitive knowledge in a network formalism  A prelimary report  In Proceedings of the Eighth Confer ence on Uncertainty in Artificial Intelligence  pp           Lin  D     Goebel  R          Integrating probabilistic  taxonomic and causal knowledge in abductive di agnosis  In Proceedings of the  th Conference on Uncertainty in Artificial Intelligence  pp         Pearl  J          Probablistic Reasoning in Intelligent Systems  Networks of Plausible Inference  Mor gan Kaufmann Publishers  San Mateo  CA  Poh  K  L          Utility based categorization  PhD dissertation draft  Department of Engineering Economic Systems  Stanford University  Poh  K  L     Horvitz  E  J          Reasoning about the value of decision model refinement  methods and applications  This volume  Russell  S     Wefald  E          Principles of metar easoning  Artificial Intelligence               Saffiotti  A          A hybrid framework for represent ing uncertain knowledge  In AAAI     Proceed ings of the Eight National Conference on Artifi cial Intelligence  pp            Shachter  R  D          Evaluating influence diagrams  Operations Research                  Shastri  L          Evidential reasoning in semantic networks  A formal theory and its parallel im plementation  Ph D  thesis  Department of Com  puter Science  University of Rochester  Smith  E  E     Medin  D  L          Categories and Concepts  Harvard University Press  Wellman  M  P          Formulation of tradeoffs in planning under uncertainty  Ph D  thesis  Dept of EECS  MIT   Yen  J      Bonisson  P  P          Extending term sub sumption systems for uncertainty management  In Proceedings of the  th Conference on Uncer tainty in Artificial Intelligence  pp              
 In recent years  there have been intense research efforts to develop efficient methods for probabilistic inference in probabilistic influence diagrams or belief networks  Many people have concluded that the best methods are those based on undirected graph structures  and that those methods are inherently superior to those based on node reduction operations on the influence diagram   We show here that these two  approaches are essentially the same  since they are explicitly or implicity building and operating on the same underlying graphical structures  In this paper we examine those graphical structures and show how  this insight can lead to an improved class of directed reduction methods     Introduction  main results in this paper are based on the connections Chyu has established between such diagrams and the  In recent years  there have been intense research efforts  undirected graph methods  allowing similar efficient  to develop efficient methods for probabilistic inference  computations using directed reduction operations  Chyu   in probabilistic influence diagrams or belief networks       a  Chyu   As these networks become increasingly popular  connections and specialized reduction operations which      b    By  recognizing  these  representations for capturing uncertainty in expert  exploit evidence nodes in the probabilistic influence  systems  the performance of inference procedures is  diagram  Shachter         we can obtain complexity of  essential for normative reasoning in real time  To date   the same order with both undirected and directed  the best exact techniques for general probabilistic  reduction methods   influence diagrams appear to those based on analogous undirected graphical structures  Andersen et al          In Sections   and   we introduce the notation and  Jensen et al       a  Jensen et al       b  Lauritzen and  framework of the directed probabilistic influence  Spiegelhalter        Shafer and Shenoy         Some  diagram and undirected moral graph  respectively   people have also concluded that those methods are  Section   explains the use of the arc reversal operation  inherently superior to those based on node reduction  to transform influence diagrams and Section   presents  operations on the influence diagram  Shachter         the corresponding operations to incorporate evidence  We show here that these two  into the diagram  These pieces are integrated into a new  approaches are essentially the same  since they are  directed reduction method in Section    and some  Shachter          explicitly or implicity building and operating on the  conclusons and extensions are presented in Section     same underlying graphical structures  In this paper we  I I I I  examine those graphical structures and show how this insight can lead to an improved class of directed       Probabilistic Influence Diagrams  reduction methods  A probabilistic influence diagram is a network built on The key to this connection is the decomposable  a directed acyclic graph   probabilistic influence diagram  Smith          correspond to uncertain quantities  which can be  The  The nodes in the diagram        observed  while the arcs indicate the conditioning  A PID will be called a  I  decomposable PID OliD if  relationships among those quantities  A decomposable  there is an arc between every two nodes with a common  probabilistic influence diagram is a special type of  child  It will be called decomposable with respect to  influence diagram whose properties will be explored   D dstj if  throughout this paper   decomposable   It can be shown that if a PID is  decomposable   then  A probabilistic influence diagram   fiiD is  a network  the subgraph induced by j s ancestral set is every  subgraph  of  it  is  decomposable as well  Chyu      b    structure built on a directed acyclic graph  Howard and  A list of the nodes N in a directed graph is said to be  Matheson         Each node j in the set N            n   corresponds to a random variable X j  Each variable Xj  ordered if none of the parents of a node follow it in the list  Such a list exists if and only if there is no directed  has a set of possible outcomes and a conditional probability distribution  tj over those outcomes  The conditioning variables for  tj have indices in the set of  cycle among  parents or conditional predecessors CG  c N  and are indicated in the graph by arcs into node j from the nodes in C j   Each variable Xj is initially unobserved  but at  the nodes   Whenever a PID  is  decomposable with respect to a node j  there is a unique ordered list for the ancestral set of j  Chyu      b   One graph will be said to be consistent with another if  both have the same nodes but the former has a subset of  some time its value Xj might become known  At that point it becomes an evidence variable  its index is  conditions is said to be minimal if there is no other  included in the set of evidence variables E  and this is  graph consistent with it that satisfies those conditions   the arcs of the latter   A graph satisfying certain  represented in the diagram by drawing its node with shading       Moral Graphs and Chordal Graphs  As a convention  lower case letters represent single nodes in the graph and exact observations while upper case letters represent sets of nodes and random variables   If J is a set of nodes  J  N  then  XJ  Moral and chordal graphs are undirected graph structures  which correspond closely to PID s  The nodes have the  denotes the vector  same meanings  but there are many directed graphs  For example  the of variables indexed by J  conditioning variables for Xj are denoted by Xc j  and  corresponding to any undirected one  To appreciate the qualities of DPID s we need to explore the relationships  might take on values XC j   between PID s and their undirected analogs   In addition to the parents  we can defme the children or  direct  successors of a node j  It is also convenient to  Given a PID  its corresponding moral mgh is obtained  keep track of the ancestors or indirect predecessors of  by adding undirected arcs between any nodes with a common child and dropping the directions from all of  node j which are defined to include the parents of j   the arcs   Likewise  the ancestral set of node j is the ancestors of  example of incest in genetics  Jensen et al       b  is  For example  a PID corresponding to an  node j  plus j itself  Finally  the nondescendants ND j   shown in Figure la and its corresponding moral graph  of node j are those nodes which are neither direct nor  is shown in Figure lb  The undirected arcs which were  indirect successors of node j  ND j  does not include  added between nodes with common children are drawn  node j itself   with dashed lines  Although the moral graph of a PID  Because there might be some observed evidence nodes in  same moral graph   the diagram  some care must be taken to interpret the meaning of the distribution  t j within node j  When  A moral graph is called a chordal fmWh if every cycle of  there is no evidence then  tj is simply the probability for Xj given its conditioning variables  P  Xj I Xqj l  two nodes in the cycle which is not itself in the cycle   is unique  there can be many PID s corresponding to the  four nodes or more possesses a  h Qa   an  arc  between  However  in generalnj is defmed as conditional on its   Chordal graphs are also called triana ulated  Berge   nondescendant evidence nodes  P  Xj I Xc j  XEnND j    XEriNI  j   of the nodes in an undirected graph is said to be        Golumbic        and      decomposable    A listing    I I I I I I I I I I I I I I I I I I   I I I I       if  for every node j in the list  there are arcs between all  There is a strong relationship between DPID s and  of the nodes which are adjacent to j and precede it in the  chordal graphs  stated in following theorem   list  A moral graph is chordal if and only if it has a perfect list  Golumbic         For example  the graph shown in Figure l d is a minimal chordal graph corresponding to the moral graph shown in Figure lb  It is clearly chordal since   B A C D F G H E I J perfect list     is a  It is minimal because it would not be  Theorem   A PID is decomposable if and only if its ordered list is perfect on its moral graph   frQQt  Given a DPID  its moral graph can be obtained  I  chordal without both of the added arcs  drawn with  without adding any new arcs   dashed lines  It is not unique  however  since there  decomposable its ordered list will be perfect for that  be many minimal chordal graphs corresponding to the  moral graph   I  same moral graph  A perfect ordering can always be  Given a perfect list on a moral graph  directions  found  if one exists  by using maximum  be added to the undirected arcs from earlier nodes in  I    Tarjan and Yannakakis          can  cardinality  Because it is  can  the list to later ones  This will be an ordered list for the PID and the PID wiH be decomposable because the list was perfect      We  can  always obtain a DPID from a chordal graph by  I  using one of its perfect lists as an ordered list  Such a  I  were added or modified from the original PID shown in  DPID is shown in Figure l c using the perfect list   B A  C D F G H E I J     The dashed arcs are the ones that  Figure Ia   that corresponding chordal graph  and any ordered list for  I  the DPID will be perfect for the chordal graph  There is only one other result needed to characterize their relationship  defining the minimal DPID in tenns of an  I I  original PID and a desired or ordered list   Theorem    Given a target ordered list and a moral graph  there  c   corresponds a unique minimal DPID   ftQQf   I  Starting with the last node in the list  add undirected arcs to the moral graph until the ordered list is  perfect  so it is an ordered list for the corresponding  I  DPID  There was no choice which arcs to add  and the list would not be perfect if any of the new arcs were not added  so the DPID is both unique and minimal   I  I I     Although there is no unique minimal chordal graph in general corresponding to a given moral graph  there is  I I  Similarly  the moral graph for a DPID is  only one for which a target ordered list is perfect  As a result  we can summarize the relationships between a PID and its associated DPID s  moral graphs  and Figure    Different graphical representations for the incest example   chordal graphs   Given a PID there is a unique moral  graph  Given that moral graph and a target ordered list         there is a unique minimal DPID  Finally  the moral graph for the DPID is the minimal chordal graph corresponding to the original PID for which the target ordering is perfect  reverse in the target order  This algorithm creates the minimal number of additional arcs  Its correctness is given by the following theorem  Theorem    Transforming toTarget DPID      Influence Diagram Transformations  The arc reversal operation transforms one PID into another with a different ordered list  In the process  extra arcs often must be added to the PID  However in transforming to and from DPID s  we can guarantee limits on the addition of those extra arcs  The arc reversal operation transforms a PID by changing the direction of one of the arcs  Olmsted        Shachter         Afterwards  each of the two nodes inherits their common parents  The operation can be interpreted as momentarily merging the two nodes and then splitting them apart  The arc  i  j  is reversible if it is the only directed path from i to j  Otherwise  a directed cycle would be created by reversing the arc  The general case for arc reversal is shown in Figure    in which the arc  i j  is reversed  Afterwards A B  and C are parents for both i and j   Figure    General Arc Reversal Operation   Given a target ordered list and any PID  we can transform the PID into another PID consistent with the minimal DPID  using only arc reversal operations  Chyu      b  Shachter         The algorithm involves visiting each node j in the reverse target order  reverse all arcs to j s successors which come before it in the target order  Arcs must be reversed in the order they appear in the current PID  but when there is a choice   Given a PID and a target ordered list  a new PID consistent with the corresponding minimal DPID can be obtained through a sequence of arc reversals   frQQt  The proof is by induction as we visit each node k in the reverse target order  We have two induction hypotheses  the current PID contains no arcs outside of the target DPID and the target list after k is an ordered list for the current PID  To prove the theorem we must show that all of the arcs reversed are reversible  and that the induction hypotheses are maintained   First  we show that the target list starting with k will be ordered for the current PID  This follows because we reverse any arcs from k to successors which precede it in the ordered list  and all of the other nodes which follow k are already in the target order  Second  we show that any arc  k  j  to be reversed is indeed reversible  If this were not true  then there must be some node i  k  i  j  If i belongs before k then  k  i  would have been reversed before  k  j   Therefore i must belong after k  but it is not properly ordered since it precedes j  Thus we contradict the induction hypotheses and it must be true that arc  k  j  is reversible  Finally we must show that no arcs are created outside of the target DPID  A new arc is created when we reverse arc  k j  only if there is some node i     which is a parent of k or j and not of the other  Since all nodes following k are in their target order  k must follow both i and j in the target order  Now all current arcs are by induction in the target chordal graph  so this new arc is required for the target   ordering to be perfect As a special case of this result  we can transform between any two DPID s which have the same moral graph and hence correspond to the same chordal graph  Chyu      b  Smith         Corollary I  We can transform one DPID to another DPID corresponding to the same chordal graph through a  I I I I I I I I I I I I I I I I I I I   I       I I  sequence of arc reversals  By similar reasoning  it can be shown that any arc reversal  operations  on  a  DPID  will  keep  it  decomposable  Chyu      b   However  the resulting  conditional distribution  tk  P  Xk I Xqk  Xj   Xj    no longer depends on X j  For example  if C were observed in the DPID shown in Figure  c  then after evidence absorption we obtain the DPID shown in Figure     I  PID will not in general be a minimal DPID for the starting PID and the target order   The operation of evidence absorption does not destroy  I  Theorem     f a sequence of  the observed node to its children are absorbed   I I I I  the decomposability of a PID  since all of the arcs from arc reversal operations are perfonned  Proposition I   on a DPID  it will continue to be decomposable   If evidence absorption is performed on a DPID  it remains decomposable   S   Evidence Transformations When evidence is absorbed at a node the distributions of  New observations are incorporated into a PID in two  its ancestors are affected indirectly  To propagate these  steps  First  the evidence is absorbed into the network  effects throughout the network  we must reorder the PID  and then it is propagated throughout the network using  so that the evidence node has no ancestors   the evidence reversal operation  a variant of arc reversal   reordering process consists of a sequence of specialized  In the process  new arcs are added until the PID  reversal operations  Evidence reversal of the arc  i  j  is  eventually becomes a DPID   At all times  the PID  This  closely related to arc reversal  except that because the  successor node is observed there is no need for it to have  represents the posterior joint distribution   a child afterward  Shachter         The general case is shown in Figure    It can be thought of as arc reversal  I  followed by evidence absorption  but it is more efficient to recognize the  special properties of evidence reversal   I I     I I I I I I I  Figure    DPID after evidence absorption of C   The operation of evi dence absor ption maintains the posterior joint distribution while recognizing the observation of an exact value for a variable in the PID  Lauritzen and Spiegelhalter        Shachter         There is no longer any need to maintain distributions with the other possible outcomes for the variable in its node or in the nodes of its children  Therefore  when Xj is observed at value Xjo the conditional distribution  t j     becomes a likelihood function  P  X j   Xj I Xcu    and the arc to each child k of j is absorbed  since the  Figure    General Evidence Reversal Operation   When evidence reversal is performed on arc  i   j    evidence node j moves one step closer to the start of an ordered list for the PID   In order for it to have no  ancestors  the operation will have to be performed on each ancestor in reverse order   This sequence of  evidence reversal operations is called e vidence propuatio n  Lauritzen and Spiegelhalter          I       Shachter         Afterwards  node j will have neither  generality  suppose the arc from I to the evidence  parents nor children  For example  consider the part of  node is reversed before the arc from m  Afterwards m  the incest PID shown in Figure Sa   will be a parent of I   We have some     evidence about C  but it is not an exact observation of C  so we create a variable K whose exact observation  By this same logic  if the ancestral set is already  describes the evidence for C   Node K is created and  decomposable then no new arcs will be created by  absorbed at the same time  so it has no children and its  evidence propagation  Also  since the ancestral set will  distribution is simply a likelihood function for C   be decomposable  and the ancestral set of the sink nodes  Because K has no children and only one parent  a DPID   nodes without children  is the entire PID  evidence at  would remain decomposable after it was added    all of the sink nodes will result in a DPID   Evidence propagation consists of evidence reversals with C  A  and B in tum until node K is disconnected as shown in Figure  b   c  and  d  There was a choice  Corollary     When evidence propagation is performed on a DPID  no new arcs are created   whether to reverse A before B since they were not ordered beforehand in the PID   Notice that they are  ordered afterward and the PID has become a DPID in the process  as will be proven in general below  Finally   Corollary    Once evidence propagation has been perfomed from  the evidence could have originally related to multiple  all  sink  nodes  nodes as in shown in Figure  e  This works best when  decomposable   in  a  PID   the  PID  will be  the nodes are a subset of a  Golumbic         Each node and its parents are contained in some clique   Evidence propagation can be performed efficiently even when evidence has been absorbed at multiple nodes in  b   a   the PID  Each unobserved node in the network has to be visited once  in reverse graph order   if it has no  evidence children  there is nothing to do  if it has exactly one  then perform evidence reversal  otherwise it must have multiple evidence children  and they can be  d   c   combined into one evidence child by multiplying their likelihood functions so that a single evidence reversal can be performed    If some of the multiple evidence  children have multiple parents  then the resulting product has all of their parents       In summary  the operations of evidence absorption and evidence propagation eventually result in a DPID  If the PID is already decomposable and evidence is only  Figure    Application of evidence propagation on part of the incest example   S    Theorem Once evidence propagation has been performed from node j in a PID  the PID will be decomposable with respect to node j  Proof  Consider any node i with multiple parents in the ancestral set for j  and let I and m be any two of those parents   Both I and m are parents of the evidence  node after the arc from i is reversed  Without loss of  within cliques  then those operations will never add new arcs       I I I I I I I I I I I I I  Putting it All Together  In this section  we assemble the  I  results from  throughout the paper to develop a directed reduction algorithm to compute the posterior joint distribution  Because the choice of chordal graph is arbitrary  we can obtain precisely the same chordal graph as in the best undirected methods  Andersen et al         Jensen et al    I I I I   I I I I I I I I I           a   Jensen  et al       b  Lauritzen and       Shafer and Shenoy       with similar complexity using directed reduction operations   absorbed and propagated while maintaining a PID  The first step in this process is to determine a target  observations  then evidence propagation should be  ordered list for the DPID  The list can either be selected  performed in reverse order thro ughout the PID to avoid  directly or  if a chordal graph is chosen instead  one of  duplicate operations   Spiegelhalter        absorption and propagation  If the evidence is about nodes in the same clique  then that evidence can be consistent with the target DPID  If there are multiple  its perfec t lists should be used  One way to generate the perfect list is to perform maximum cardinality search on  In this method  we maintain an updated posterior joint  the chordal graph  using an ordered list for the original  distribution for the PID given the evidence  If we desire  PID to break ties  Chyu       b  Tarjan and Yannalcakis          obtained through reduction operations  Shachter          any general conditional distributions  they can be If we want posterior marginal distributions for the  Using this list and the algorithm described in Section     variables in the PID  they can be obtained by a  we can pre reverse arcs to obtain a PID consistent with a perfect list for  probability propagation process  Lauritzen and S piegelh alter        Shachter        operati ng on the cliques  By comparing the bask operations performed  the chordal graph is shown in Figure ld  We can pre  by the different methods  we can verify that they have  reverse arcs from the original PID shown in F igure la  the same order of complexity  This is because at each  PID shown in Fig ure    The shaded  dashed arcs would not appear in this PID  but we can  graphical structure and operating on data structures of  the unique minimal DPID  For example  given target ordered list   B A CD F G HE I          to obtain the  infer them from the target ordering   If they were  present  we would have the DPID shown in Figure lc   I  step they are performing similar tasks on the same the same max i ma l dimensions  There can  of course  be significant differences in the actual computation times   Proposition     The directed reduction method and the undirected  methods of HUGIN and Lauritzen Spiegelhalter are of  I  the same order of complexity   I      We have shown that a directed red uction algorithm can  I I I I I I I  Conclusions and Extensions  perform operations on the same graph and of the same order of complexity as the best undirected methods for probabilistic inference  This result can be interpreted in two ways  First  pre reversals allow us to use the best possible choice of chordal graph so we can Jearn from  Figure    riDfor evidence propagation afterpre reversals   the undirected methods a superior straten for reduction  Second  we can plainly see how the chordal graph  structure represents the w ors t case  for posterior ioim    We can n ow perform evidence absorption and  dependence  No matter what evidence  within cliques  is  propagation on the PID  In the process  the shaded   observed  no additional arcs will be necessary to  dashed arcs in Figure   might have to be added  In the  represent the posterior PH    worst case  they will all appear and we will obtain the If the evidence absorption is exact  Some natural extensions to the directed algorithm are to  evidence about nodes in the network  then those nodes and their incident arcs will be absorbed through evidence  exploit efficiencies which have been developed in either  target DPID   the directed or undirected representations         In the directed representation  an important property is that of a deterministic function  a variable whose outcome is known with certainty given its parents  outcomes  This introduces additional conditional independence into the diagram which can be exploited during evidence propagation  At the same  when the PID is only being used to obtain posterior marginal distributions for a subset of variables or with limited observations  then the PID can be preprocessed to eliminate variables that are irrelevant for the desired results  Geiger et al         Shachter        Shachter         This elimination can be performed on the working PID or  if possible  before the target DPID is determined  Another promising hybrid might exploit the impressive speed and simplicity of the HUG IN undirected method  Andersen et al         Jensen et al       a  Jensen et al       b  by maintaining joint distributions for a node and its parents instead of conditional distributions  This simplifies the operation of arc reversal  but does require maintaining the full DPID instead of simply a PID consistent with it  There are a couple of advantages to using this method on undirected graphs which appear applicable to directed methods as well  These advantages are symmetric operations for evidence and probability propagation and the recognition of zeros in the sparse joint distribution matrices       Acknowledgements  We are grateful for the comments and suggestions of Richard Barlow  Stephen Chyu  and Robert Fung       
 Recent interests in dynamic decision modeling have led to the development of several representation and inference methods  These methods however  have limited application under time critical conditions where a trade off between model quality and computational tractability is essential  This paper presents an approach to time critical dynamic decision A knowledge representation and modeling  modeling method called the time critical dynamic influence diagram is proposed  The formalism has two forms  The condensed form is used for modeling and model abstraction  while the deployed form which can be converted from the condensed form is used for inference purposes  The proposed approach has the ability to represent space temporal abstraction within the model  A knowledge based meta reasoning approach is proposed for the purpose of selecting the best abstracted model that provide the optimal trade off between model quality and model tractability  An outline of the knowledge based model construction algorithm is also provided     INTRODUCTION  The goal of dynamic decision making is to select an optimal course of action that satisfies some objectives in a time dependent environment  The decisions may be made in different stages and each stage may varies in duration  A number of dynamic decision modeling formalisms have been proposed by various researchers  These include dynamic influence diagrams  DIDs   Tatman and Shachter        temporal influence diagrams  Provan        Markov cycle trees  Beck and Pauker        stochastic trees  Hazen        and Dynamo  Leong        Although these models provide relatively efficient methods for representing and reasoning in a time dependent domain  the process of computing the optimal  solution has remain intractable  A decision maker may have to spend a large amount of time in the modeling and solution processes that left very little or no time for the action to be carried out  This problem is particularly significant for large models involving temporal relations  Existing approaches to modeling and solving dynamic decision problems are therefore not appropriate for time critical applications  Hence a more effective and practical approach to time critical dynamic decision modeling is needed  Time critical dynamic decision problems have been discussed in several research communities  An important part of decision analysis is the formulation of the decision problem  Modeling time and the needs to deal with time pressured situations are considered to be the greatest challenges in developing time critical dynamic decision support systems  We believe that a major reason for this perceived difficulty is the lack of modeling techniques that provide explicit support for the modeling of temporal processes  and for dealing with time critical situations  In this paper  we propose a formalism called time critical dynamic influence diagrams  TDID   that provide explicit support for the modeling and solution of time critical dynamic decision problems  In our approach we utilize the notion of abstraction to simplify the computational complexity of large and complex models  Previous research efforts on abstraction had mainly focused on data abstraction  For example  the KBTA  knowledge based temporal abstraction  method  Shahar       is a knowledge based framework for the representation and application of the knowledge required for abstraction of high level concepts from time oriented data  Further research is needed on decision model abstraction methods and the selection of best situation specific abstraction model  For a given domain  there exists a suite of possible decision models specified at different levels of space temporal abstraction  Almost all previous research relied on the domain experts to assess the  goodness  of different abstractions  We adopt here the use of meta reasoning to select an optimal model based on the best tradeoffs between decision quality and computational complexity    Time Critical Dynamic Decision Making  This paper is organized as follows  In Section    we present the time critical dynamic influence diagram used for the modeling and representation of time critical dynamic decision problems  In Section    we show the applications of space and temporal abstractions using TDID  In Section    we propose a model based approach to meta reasoning for the selection of the best abstraction model and describe the model construction process  Finally  in Section    we conclude by summarizing and provide directions for further research          dependent  possibly equal  on X  and X  respectively  A formal definition of time critical dynamic influence diagrams is given below   TIME CRITICAL DYNAMIC DECISION MODELING      TIME CRITICAL DYNAMIC INFLUENCE DIAGRAM  TDID  TDID is designed to facilitate the modeling and solution of time critical dynamic decision problems  It extends standard influence diagram  Howard and Metheson       by including the concepts of temporal arcs and time sequences  It also incorporates dynamic influence diagram  Tatman and Shachter       as a representation for inference purposes   Figure    An Example Of A TDID In Condensed Form   Tl l  T     T     T     A TDID model has two forms  the condensed form and the deployed form  Figure   shows an example of the condensed form  This form is mainly used to represent or define the dynamic decision model and is the form used in the modeling process  Figure   shows the deployed form for the same model  It is used only for inference purposes and is constructed from the condensed form  Although in principle both forms can be converted to and from each other  they serve different purposes in the modeling and solution processes  Each node in a TDID represents a set of time indexed variables  The set of time indices may be different from one node to another  but they must be subsets of a master time sequence  The arcs in a TDID are called temporal arcs and they denote both probabilistic and temporal  time lag  relations among the variables  Solid arcs in the condensed form represent instantaneous probabilistic relations  while broken arcs represent time lagged probabilistic relations  TDID allows for the coexistence of nodes of different temporal detail in the same model  The TDID in Figure   has a master time sequence of            Node Y represents a set of chance variables indexed by this time sequence  whereas node X represents a set of chance variables indexed by the subsequence         In this example  variable X has been temporally abstracted by omitting its value at some intermediate time indices  This is evident from the deployed form in Figure   where nodes Yh Y   Y  and Y  are represented while only nodes X  and X  are probabilistically represented  and nodes X  and X  are assumed to be deterministically  Figure    The Deployed Form Of The TDID In Figure     Definition  The condensed form of time critical dynamic influence diagrams is a   tuple  T   D  C  V  A   A   P   where Tm is a set of time indices called the master time sequence   D is a set temporal decision variables  Each D E D is a sequence of decision variables indexed by a time sequence T   T m C is a set of temporal chance variables  Each C E C is a sequence of chance variables indexed by a time sequence Tc Tm         Xiang and Poh  V is a set of utility functions indexed by a time sequence T vk T     Ai       D u C  x  D u C u  V   is a set of instantaneous arcs such that VX E  D uC   and VYE   D u C u  V      X  Y  E Ai if and only if there exists an instantaneous arc from node X to node Y    P is a set conditional probability distributions  For each chance node X E C  we assess a sequence of conditional probability distributions p Xi I n Xi     where i E Tx   r  X    is the set of nodes lj such that  Y  X  E A  and j  max   k I k E Ty  k   i   or  Y  X  E Ai and j   i     A        D u C  x  D u C u  V   is a set of time lag arcs such that VX E  D uC   and VYE   D u C u  V      X  Y  E A  if and only if there exists a time lag arc from node X to node Y                   Intervention   a            b   Figure    TDID For Cardiac Arrest Example   Time Critical Dynamic Decision Making      AN EXAMPLE We shall use a medical example from the domain of cardiac arrest  Ngo et al       to illustrate the use of TDID  In this problem  the goal of the medical treatment is to maintain life and to prevent anoxic injury to the brain  The observable variable is the electrocardiogram or rhythm strip  cr   While patient survival is of primary importance  cerebral damage must be taken into account and can be viewed as part of the cost in a resuscitation attempt  The length of time that patient has been without cerebral blood flow  cbf  determines the period of anoxia  poa   If the patient has ineffective circulation for more than five minutes  there is a likelihood of sustaining cerebral damage  This damage is persistent and its severity increases as the period of anoxia increases  Medical doctors treat a patient experiencing a cardiac arrest with a variety of interventions and medications  The cardiac arrest problem may be represented by the TDID in condensed form as shown in Figure   a   We have assumed that the master time sequence is          and all time sequences are the same as the master sequence  The real time between two time indices is   minute  Figure   b  shows the deployed form for the same model       level of space abstraction and temporal abstraction  Details of model abstraction in TDID are given in Section         TDID models time explicitly  It describes time in a clear and unambiguous manner  For example  in Figure    it is clear that X is a possible cause of Y and that this causal effect is delayed by   time unit  Through the use of temporal arcs and time sequences  a TDID has the ability to explicitly model how the underlying events evolve with time   CONVERTING THE CONDENSED FORM TO DEPLOYED FORM  The algorithm for converting a TDID in condensed form to its deployed form is as follows                 PROPERTIES OF TDID  We describe here the properties of TDID  First  we note that both forms of the TDID can be converted to and from each other  The condensed representation permits parsimonious descriptions of models and model abstraction  Inference using established algorithms for solving dynamic influence diagrams is then carried out by converting the condensed form to the deployed form and then adding a super value node            The time pattern for the deployed form is determined by master time sequence  The graphical structure of the TDID without temporal lag arc is replicated N time  where N is the number of time steps in the master time sequence  Let ID  be the ith influence diagram for i l       N  Connect the nodes in two different time slices according the temporal lag arc  For each the temporal arc do For i     to N   do Add arc from the parent node in time slice i to child node in time slice i     Abstracting nodes For each node X with time sequence Tx c Tm do Partition Tm into abstraction groups with members of Tx as the starting index of each group  For each partition do The node indexed by the Tx is the abstracted node  The other nodes in the same partition are assumed to be equal to the abstracted  Eliminate any barren nodes  Shachter        Insert probability distribution for expanded each node     MODEL ABSTRACTION USING TDID  TDID provides a relatively high degree of reusability and modifiability of models  For instance  we may use the condensed form to describe a concise description of the domain  and it can then be easily reused  TDID also allows for dynamic modification of the model after the deployed form is produced from the condensed form   In this section we show how TDID provides a flexible  expressive and efficient formalism for representing model abstraction  including temporal abstraction and space abstraction  Abstraction of knowledge from domain experts provides high level building blocks that assists in both the development and maintenance of large knowledge bases in decision theoretic applications  Briefly  model abstraction is the task of creating context sensitive interpretations of decision model in terms of higher level space context and temporal patterns  The input to the model abstraction task is a set of abstraction goals and domain specific abstraction knowledge  The output of the model abstraction task is a set of models at a higher level of space temporal abstraction  Shahar       Combi and Shahar         Finally  TDID supports model abstraction  It provides a method to represent model abstraction  including different  In our approach  the model abstraction task is decomposed into three sub tasks  context interpretation   TDID allows for flexible temporal patterns  Compared with dynamic influence diagram in which temporal patterns of interest are predefined  TDID has temporal patterns that can be dynamically modified         Xiang and Poh  space abstraction  and temporal abstraction  These tasks are supported by a domain knowledge base  Context interpretation is a set of relevant interpretation contexts  such as relation between a temporal pattern  the context of state in different abstraction levels and relations  Space abstraction focuses on abstracting the network within a time slice  including abstracting a group of state variables based on concept context  and summarising influence paths based on nodes reduction  Figure   shows an example of space abstraction of the TDID in Figure   a   This abstraction could be the result of a response to a massive myocardial infraction where we need not consider cerebral damage  The CD node and all subsequent barren nodes may be eliminated                            I                          I I L           I               MODEL SELECTION AND CONSTRUCTION In the previous section  we showed how model abstraction may be applied in TDID  However  given a specific problem there exists many different possible space temporal abstractions  and not all abstractions are Most of the previous research had equally good  recognized the usefulness of abstraction and but had relied on the domain experts to indicate the best level of abstraction  Here we address the problem of finding the best model among the set of possible space temporal abstractions using meta reasoning      META REASONING  Intervention  Figure    Space Abstraction Of The Model In Figure   a      interpretations of the variables  In Figure    we assumed that the master time sequence is           and all time sequences are the same as the master sequence  If all time sequences are abstracted to the subsequence  I      then the resulting time abstracted TDID is shown in Figure   where the nodes in time slice   have been omitted      Meta reasoning  Horvitz        Russell         enables a system to direct the course of its computations according to the current situation  In time critical applications  it is necessary that decision making effort be directed towards computation sequences that appear likely to yield good decisions  It is also important to consider tradeoffs between computational complexity and decision quality  Figure   shows a decision theoretic perspective of the meta reasoning process  The goal is to determine the best abstraction model  For a specific problem  there exists a set of possible space temporal abstraction models  We choose the best model based on trade off between the computation cost and model quality  The computation cost is determined by computation time  while the model quality directly affects the quality of the action taken   Figure    Temporal abstraction of the TDID in Figure   a   Time abstraction may be performed on TDID by modifying the time sequences embedded in the model  Temporal abstraction in TDID leads to new context  Figure    A Decision theoretic perspective of meta reasoning   Time Critical Dynamic Decision Making  We first consider model quality  For a given problem or situation  there exists a suite of models for solving the problem  Let M be a set of different space temporal abstraction models for the problem  For each model m  E M  let the maximum expected utility it yields be u  m   which we will use as a measure of the model quality  Next  we consider the computation time  For a specific algorithm used to solve the model  the computation time t can be estimated by assessing a function C  S   N   where S  refers to the space complexity of the model m   and N  refers to the number of time intervals in model m   We use the term comprehensive value Uc m   to refer to the overall utility that include both model quality and computational time  The comprehensive value is a function of the object level utility u   and the inference related cost  u   Horvitz        The object level utility is the value associated with the information represented by the computed result of model m  without regard to the cost of reasoning  The inference related cost is the penalty incurred while delaying action to arrive at the result  We define the comprehensive utility as the difference between the object level utility u  m   u  m    and the inference related cost  u  m   C  S   N    Hence we write        u  u  m   u  m      u  m     C  S   N      m E  M   different space temporal abstraction models in domain specified knowledge base and choose the  best  model based on the consideration of computation cost and quality  Model quality may be approximated as follows  Suppose that the available computation time is t  and M   is the set of models whose computational time is less than or equal to t  then model quality is  Q M  r        A KNOWLEDGE BASED APPROACH TO META REASONING       max u  m    subject to  m  E M  r        M    be the model whose maximum expected   utility is equal to Q M        u   m     i e   model m  has  Let  m  E  the highest quality among all models whose computation time is less than or equal to t  By expending some quantity of reasoning resource t  e g   computation time  model quality can be enhanced since the set of feasible models M    is now larger  We define the comprehensive utility for a given computation time t to be the difference between the object level utility Q M   and the inference related cost u   t   That is       The best model is that model with maximum u         u  Q M   t    Q M     u  t        We define the net change in u   in return for an allocation of some computational resource to reasoning  as the expected value of computation  EVC   Horvitz        If t  is the amount of resources already committed  then the EVC for expending further resources tis  or  We can use the EVC to compare the value of extending the computation by different length of time and identify the ideal computation resource t  with the greatest EVC  i e    t      argmax  EVC t            The best model is then the model m such that Figure    A Practical Knowledge based Approach To Meta reasoning  The approach to meta reasoning given in the previous subsection is intractable  We propose here a tractable approach to perform the meta level analysis as shown in Figure    The goal of the meta analysis is now to determine the length of time for computation  and then to identify the best model for doing so  We will use a knowledge base to support the process  We store a set of    u  m     Q M  r           We observed that under time critical conditions  meta reasoning can provide useful control of the computational complexity by selecting the optimal abstraction model  In particular  it provides a straight forward way of incorporating flexibility to perform tradeoffs between the object level value and the inference related cost              Xiang and Poh  KNOWLEDGE BASED MODEL CONSTRUCTION  We describe briefly here the model construction process  which is supported by a domain knowledge base  The steps for the model construction process are      Given a time critical dynamic decision problem  specify the problem requirements  such as its urgency and deadline       Select a set of models that satisfies the requirements from the domain knowledge base       Select the optimal model from the set of modes based on model quality and computation cost       Modify or customize the TDID according to user requirements if necessary       The TDID is converted into the deployed form  a super value node is added and the optimal solution determined      SUMMARY AND CONCLUSION  In this paper  we have proposed an approach to time critical dynamic decision making  The method is independent of any particular problem domain  A formalism for knowledge and model representation called the time critical dynamic influence diagram was proposed  The TDID has two forms  The condensed form is used mainly for modeling and space temporal abstraction  while the deployed is used only for inference purposes  We have shown how space and temporal abstraction may be carried out with TDID  In order to select the best abstracted model  we introduced the use of meta reasoning  and proposed a practical knowledge based approach to perform tradeoff between decision quality and computational complexity  Finally  we provide an outline of the knowledge based model construction process  Our work here is related to a number of previous work as well as some on going ones  The idea of representing a temporal sequence of probabilistic models into a compact form had been reported by Aiiferis et  al                These related work had mainly focused on bayesian networks while our work here include temporal decisions  The use of knowledge base to support temporal abstraction of data had been investigated by Shahar and Musen         The use of meta reasoning for directing the course of computation have been investigated by Horvitz        and Russell         The idea of using expected value of refinement and value of computation to  direct model refinement and abstraction was briefly described in Poh and Horvitz         Finally  the authors are presently working on the application of the approach to a time critical medical domain involving head injury critical care in a hospital in Singapore  Acknowledgments  The authors would like to thank the Tze Yun Leong  David Harmanec and other research group members for their helpful suggestions and comments on this work  This work is partly supported by a strategic research grant from the Singapore National Science and Technology Board and Ministry of Education  Yanping is supported by a National University of Singapore research scholarship  
