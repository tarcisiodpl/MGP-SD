 Poker is a challenging problem for artificial intelligence  with non deterministic dynamics  partial observability  and the added difficulty of unknown adversaries  Modelling all of the uncertainties in this domain is not an easy task  In this paper we present a Bayesian probabilistic model for a broad class of poker games  separating the uncertainty in the game dynamics from the uncertainty of the opponents strategy  We then describe approaches to two key subproblems   i  inferring a posterior over opponent strategies given a prior distribution and observations of their play  and  ii  playing an appropriate response to that distribution  We demonstrate the overall approach on a reduced version of poker using Dirichlet priors and then on the full game of Texas holdem using a more informed prior  We demonstrate methods for playing effective responses to the opponent  based on the posterior     Introduction The game of poker presents a serious challenge to artificial intelligence research  Uncertainty in the game stems from partial information  unknown opponents  and game dynamics dictated by a shuffled deck  Add to this the large space of possible game situations in real poker games such as Texas holdem  and the problem becomes very difficult indeed  Among the more successful approaches to playing poker is the game theoretic approach  approximating a Nash equilibrium of the game via linear programming         Even when such approximations are good  Nash solutions represent a pessimistic viewpoint in which we face an optimal opponent  Human players  and even the best computer players  are certainly not optimal  having idiosyncratic weaknesses that can be exploited to obtain higher payoffs than the Nash value of the game  Opponent modelling attempts to capture these weaknesses so they can  be exploited in subsequent play  Existing approaches to opponent modelling have employed a variety of approaches including reinforcement learning      neural networks      and frequentist statistics      Additionally  earlier work on using Bayesian models for poker     attempted to classify the opponents hand into one of a variety of broad hand classes  They did not model uncertainty in the opponents strategy  using instead an explicit strategy representation  The strategy was updated based on empirical frequencies of play  but they reported little improvement due to this updating  We present a general Bayesian probabilistic model for hold em poker games  completely modelling the uncertainty in the game and the opponent  We start by describing holdem style poker games in general terms  and then give detailed descriptions of the casino game Texas holdem along with a simplified research game called Leduc holdem for which game theoretic results are known  We formally define our probabilistic model and show how the posterior over opponent strategies can be computed from observations of play  Using this posterior to exploit the opponent is non trivial and we discuss three different approaches for computing a response  We have implemented the posterior and response computations in both Texas and Leduc holdem  using two different classes of priors  independent Dirichlet and an informed prior provided by an expert  We show results on the performance of these Bayesian methods  demonstrating that they are capable of quickly learning enough to exploit an opponent     Poker There are many variants of poker   We will focus on hold em  particularly the heads up limit game  i e   two players with pre specified bet and raise amounts   A single hand consists of a number of rounds  In the first round  players are dealt a fixed number of private cards  In all rounds    A more thorough introduction of the rules of poker can be found in          r  c    f  r    c  r    f  c    c  f  r  c    f  c  Figure    An example decision tree for a single betting round in poker with a two bet maximum  Leaf nodes with open boxes continue to the next round  while closed boxes end the hand  some fixed number  possibly zero  of shared  public board cards are revealed  The dealing and or revealing of cards is followed by betting  The betting involves alternating decisions  where each player can either fold  f   call  c   or raise  r   If a player folds  the hand ends and the other player wins the pot  If a player calls  they place into the pot an amount to match what the other player has already placed in the pot  possibly nothing   If a player raises  they match the other players total and then put in an additional fixed amount  The players alternate until a player folds  ending the hand  or a player calls  as long as the call is not the first action of the round   continuing the hand to the next round  There is a limit on the number of raises  or bets  per round  so the betting sequence has a finite length  An example decision tree for a single round of betting with a two bet maximum is shown in Figure    Since folding when both players have equal money in the pot is dominated by the call action  we do not include this action in the tree  If neither player folds before the final betting round is over  a showdown occurs  The players reveal their private cards and the player who can make the strongest poker hand with a combination of their private cards and the public board cards wins the pot  Many games can be constructed with this simple format for both analysis  e g   Kuhn poker     and Rhode Island holdem      and human play  We focus on the commonly played variant  Texas hold em  along with a simplified and more tractable game we constructed called Leduc hold em  Texas Hold Em  The most common format for hold em is Texas Holdem  which is used to determine the human world champion and is widely considered the most strategically complex variant  A standard    card deck is used  There are four betting rounds  In the first round  the players are dealt two private cards  In the second round  or flop   three board cards are revealed  In the third round  turn   and fourth round  river   a single board card is revealed  We use a four bet maximum  with fixed raise amounts of    units in the first two rounds and    units in the final two rounds  Finally  blind bets are used to start the first round  The first player begins the hand with   units in the pot and the second player with    units  Leduc Hold Em  We have also constructed a smaller version of hold em  which seeks to retain the strategic elements of the large game while keeping the size of the game tractable  In Leduc hold em  the deck consists of two suits with three cards in each suit  There are two rounds  In the first round a single private card is dealt to each player  In the second round a single board card is revealed  There is a two bet maximum  with raise amounts of   and   in the first and second round  respectively  Both players start the first round with   already in the pot  Challenges  The challenges introduced by poker are many  The game involves a number of forms of uncertainty  including stochastic dynamics from a shuffled deck  imperfect information due to the opponents private cards  and  finally  an unknown opponent  These uncertainties are individually difficult and together the difficulties only escalate  A related challenge is the problem of folded hands  which amount to partial observations of the opponents decisionmaking contexts  This has created serious problems for some opponent modelling approaches and our Bayesian approach will shed some light on the additional challenge that fold data imposes  A third key challenge is the high variance of payoffs  also known as luck  This makes it difficult for a program to even assess its performance over short periods of time  To aggravate this difficulty  play against human opponents is necessarily limited  If no more than two or three hundred hands are to be played in total  opponent modelling must be effective using only very small amounts of data  Finally  Texas holdem is a very large game  It has on the order of      states      which makes even straightforward calculations  such as best response  non trivial     Modelling the Opponent We will now describe our probabilistic model for poker  In all of the following discussion  we will assume that Player    P   is modelling its opponent  Player    P    and that all incomplete observations due to folding are from P s perspective      Strategies In game theoretic terms  a player makes decisions at information sets  In poker  information sets consist of the actions taken by all players so far  the public cards revealed so far  and the players own private cards  A behaviour strategy specifies a distribution over the possible actions   for every information set of that player  Leaving aside the precise form of these distributions for now  we denote P s complete strategy by  and P s by   We make the following simplifying assumptions regarding the player strategies  First  P s strategy is stationary  This is an unrealistic assumption but modelling stationary opponents in full scale poker is still an open problem  Even the most successful approaches make the same assumption or use simple methods such as decaying histories to accommodate opponent drift  However  we believe this framework can be naturally extended to dynamic opponents by constructing priors that explicitly model changes in opponent strategy  The second assumption is that the players strategies are independent  More formally  P        P   P     This assumption  implied by the stationarity  is also unrealistic  Hower  modelling opponents that learn  and effectively deceiving them  is a difficult task even in very small games and we defer such efforts until we are sure of effective stationary opponent modelling  Finally  we assume the deck is uniformly distributed  i e   the game is fair  These assumptions imply that all hands are i i d  given the strategies of the players       Probability of Observations Suppose a hand is fully observed  i e   a showdown occurs  The probability of a particular showdown hand Hs occurring given the opponents strategy is     P  Hs      P  C  D  R  k   A  k   B  k      P  D C P  C   k Y    i      P  D C P  C   k Y    i      pshowcards  k Y  P  Bi  D  R  i   A  i   B  i      P  Ai  C  R  i   A  i    B  i    P  Ri  C  D  R  i    Yi  C Ai Zi  D Bi   P  Ri  C  D  R  i     Yi  C Ai Zi  D Bi  i      k Y  Zi  D Bi    i        Hands The following notation is used for hand information  We consider a hand  H  with k decisions by each player  Each hand  as observed by an oracle with perfect information  is a tuple H    C  D  R  k   A  k   B  k   where   C and D denote P  and P s private cards   where for notational convenience  we separate the information sets for P   P   into its public part Yi  Zi   and its private part C  D   So   Yi    R  i   A  i    B  i    Zi    R  i   A  i   B  i       Ri is the set  possibly empty  of public cards dealt before either player makes their ith decision  and  Ai and Bi denote P  and P s ith decisions  fold  call or raise   We can model any limit holdem style poker with these variables  A hand runs to at most k decisions  The fact that particular hands may have fewer real decisions  e g   a player may call and end the current betting round  or fold and end the hand  can be handled by padding the decisions with specific values  e g   once a player has folded all subsequent decisions by both players are assumed to be folds   Probabilities in the players strategies for these padding decisions are forced to    Furthermore  the public cards for a decision point  Ri   can be the empty set  so that multiple decisions constituting a single betting round can occur between revealed public cards  These special cases are quite straightforward and allow us to model the variable length hands found in real games with fixed length tuples   In addition  Yi  C Ai is the probability of taking action Ai in the information set  Yi   C   dictated by P s strategy    A similar interpretation applies to the subscripted   pshowcards is a constant that depends only on the number of cards dealt to players and the number of public cards revealed  This simplification is possible because the deck has uniform distribution and the number of cards revealed is the same for all showdowns  Notice that the final unnormalized probability depends only on   Now consider a hand where either player folds  In this case  we do not observe P s private cards  D  We must marginalize away this hidden variable by summing over all possible sets of cards P  could hold     Strictly speaking  this should be P  H     but we drop the conditioning on  here and elsewhere to simplify the notation    The probability of a particular fold hand Hf occurring is  P  Hf      P  C  R  k   A  k   B  k    X  k Y    P  Bi  D  R  i   A  i   B  i      i   P  Ai  C  R  i   A  i    B  i    D   P  Ri  C  D  R  i      k   k Y XY   pfoldcards  Hf   Zi  D   Bi Yi  C Ai    P  C   P  D C   D   i    i      k XY  Zi  D   Bi  We start with a simple objective  BBR   argmax EH O V  H   X   argmax V  H P  H O        argmax     argmax   D   i        Posterior Distribution Over Opponent Strategies Given a set O   Os  Of of observations  where Os are the observations of hands that led to showdowns and Of are the observations of hands that led to folds  we wish to compute the posterior distribution over the space of opponent strategies  A simple application of Bayes rule gives us  P  O  P    P  O  Y P    Y   P  Hs    P  Hf    P  O  Hs Os Hf Of Y Y P  Hs    P  Hf     P     P   O     Hs Os  Hf Of    Responding to the Opponent Given a posterior distribution over the opponents strategy space  the question of how to compute an appropriate response remains  We present several options with varying computational burdens  In all cases we compute a response at the beginning of the hand and play it for the entire hand      Bayesian Best Response The fully Bayesian answer to this question is to compute the best response to the entire distribution  We will call this the Bayesian Best Response  BBR   The objective here is to maximize the expected value over all possible hands and opponent strategies  given our past observations of hands   X  V  H   HH  X  V  H   X  V  H   HH    argmax   where D  are sets of cards that P  could hold given the observed C and R  i e   all sets D that do not intersect with C  R   and pfoldcards  Hf   is a function that depends only on the number of cards dealt to the players and the number of public cards revealed before the hand ended  It does not depend on the specific cards dealt or the players strategies  Again  the unnormalized probability depends only on    HH  HH  Z Z  Z  P  H     O P   O    P  H     O P  O  P           P  H     O P    k Y Y Zi  D Bi Hs Os i    k Y XY  Zi  D Bi  Hf Of D   i       where H is the set of all possible perfectly observed hands  in effect  the set of all hands that could be played   Although not immediately obvious from the equation above  one algorithm for computing Bayesian best response is a form of Expectimax      which we will now describe  Begin by constructing the tree of possible observations in the order they would be observed by P   including P s cards  public cards  P s actions  and P s actions  At the bottom of the tree will be an enumeration of P s cards for both showdown and fold outcomes  We can backup values to the root of the tree while computing the best response strategy  For a leaf node the value should be the payoff to P  multiplied by the probability of P s actions reaching this leaf given the posterior distribution over strategies  For an internal node  calculate the value from its children based on the type of node  For a P  action node or a public card node  the value is the sum of the childrens values  For a P  action node  the value is the maximum of its childrens values  and the best response strategy assigns probability one to the action that leads to the maximal child for that nodes information set  Repeat until every node has been assigned a value  which implies that every P  information set has been assigned an action  More formally Expectimax computes the following value for the root of the tree  X R   max A   X B     X  Z Y k  Rk  max Ak  XX Bk  V  H   D  Zi  D Bi P  O  P      i    This corresponds to Expectimax  with the posterior inducing a probability distribution over actions at P s action nodes  It now remains to prove that this version of Expectimax   computes the BBR  This will be done by showing that  Z X V  H  P  H     O P  O  P    max       HH  X R   max A   Z Y k  X  X    B   Rk  max Ak  XX Bk  V  H   D  Zi  D Bi P  O  P      i    P First we rewrite max H as  XXXX XXX    max    max       k   Rk Ak Bk  R  A  B   D  where max i  is a max over the set of all parameters inP that govern Pthe ith decision  Then  because maxx y f  x  y   y maxx f  x  y   we get  max    max       k    max    max   XXX  X   k   X  XX  max        R  A  B        R   XXXX  R   max        max  k   D  XXXX    Rk Ak Bk  A  B   X Rk  A  B   Rk Ak Bk  XX  D  XXX Ak Bk  D  Second  we note that  Z P  H     O P  O  P        k Y  Yi  C Ai  Z Y k  Zi  D Bi   i    i    We can distribute parameters from  to obtain  X X X  Y   C A  max R   X Rk       max  k   Z Y k  B   A   X  Yk  C Ak  XX Bk  Ak  D  Zi  D Bi P  O  P     parameter setting is to take the highest valued action with probability    Computing the integral over opponent strategies depends on the form of the prior but is difficult in any event  For Dirichlet priors  see Section     it is possible to compute the posterior exactly but the calculation is expensive except for small games with relatively few observations  This makes the exact BBR an ideal goal rather than a practical approach  For real play  we must consider approximations to BBR  One straightforward approach to approximating BBR is to approximate the integral over opponent strategies by importance sampling using the prior as the proposal distribution  Z X P  H     O P  O   P  H     O P  O  P         where the  are sampled from the prior    P     More effective Monte Carlo techniques might be possible  depending on the prior used  Note that P  O   need only be computed once for each   while the much smaller computation P  H     O  must be computed for every possible hand  The running time of computing the posterior for a strategy sample scales linearly in the number of samples used in the approximation and the update is constant time for each hand played  This tractability facilitates other approximate response techniques      Max A Posteriori Response An alternate goal to BBR is to find the max a posteriori  MAP  strategy of the opponent and compute a best response to that strategy  Computing a true MAP strategy for the opponent is also hard  so it is more practical to approximate this approach by sampling a set of strategies from the prior and finding the most probable amongst that set  This sampled strategy is taken to be an estimate of a MAP strategy and a best response to it is computed and played  MAP is potentially dangerous for two reasons  First  if the distribution is multimodal  a best response to any single mode may be suboptimal  Second  repeatedly playing any single strategy may never fully explore the opponents strategy    i       X R   max A   Z Y k  X B     X Rk  max Ak  XX Bk  D  Zi  D Bi P  O  P       i    which is the Expectimax algorithm  This last step is possible because parameters in  must sum to one over all possible actions at a given information set  The maximizing      Thompsons Response A potentially more robust alternative to MAP is to sample a strategy from the posterior distribution and play a best response to that strategy  As with BBR and MAP  sampling the posterior directly may be difficult  Again we can use importance sampling  but in a slightly different way  We sample a set of opponent strategies from the prior  compute their posterior probabilities  and then sample one strategy according to those probabilities    P  i  H  O  P  i    P j P  j  H  O   This was first proposed by Thompson       Thompsons has some probability of playing a best response to any nonzero probability opponent strategy and so offers more robust exploration     Priors As with all Bayesian approaches  the resulting performance and efficiency depends on the choice of prior  Obviously the prior should capture our beliefs about the strategy of our opponent  The form of the prior also determines the tractability of  i  computing the posterior  and  ii  responding with the model  As the two games of hold em are considerably different in size  we explore two different priors  Independent Dirichlet  The game of Leduc hold em is sufficiently small that we can have a fully parameterized model  with well defined priors at every information set  Dirichlet distributions offer a simple prior for multinomials  which is a natural description for action probabilities  Any strategy  in behavioural form  specifies a multinomial distribution over legal actions for every information set  Our prior over strategies  which we will refer to as an independent Dirichlet prior  consists of independent Dirichlet distributions for each information set  We are using Dirichlet          distributions  whose mode is the multinomial                 over fold  call  and raise  Informed  In the Texas hold em game  priors with independent distributions for each information set are both intractable and ineffective  The size of the game virtually guarantees that one will never see the same information set twice  Any useful inference must be across information sets and the prior must encode how the opponents decisions at information sets are likely to be correlated  We therefore employ an expert defined prior that we will refer to as an informed prior  The informed prior is based on a ten dimensional recursive model  That is  by specifying values for two sets of five intuitive parameters  one set for each player   a complete strategy is defined  Table   summarizes the expert defined meaning of these five parameters  From the modelling perspective  we can simply consider this expert abstraction to provide us with a mapping from some low dimensional parameter space to the space of all strategies  By defining a density over this parameter space  the mapping specifies a resulting density over behaviour strategies  which serves as our prior  In this paper we use an independent Gaussian distribution over the parameter space with means and variances chosen by a domain expert  We omit further details  Table    The five parameter types in the informed prior parameter space  A corresponding set of five are required to specify the opponents model of how we play  Parameter Description Fraction of opponents strength distribution that must be exceeded to r  raise after    bets  i e   to initiate betting   Fraction of opponents strength disr  tribution that must be exceeded to raise after     bets  i e   to raise   Fraction of the game theoretic opb timal bluff frequency  Fraction of the game theoretic opf timal fold frequency  t Trap or slow play frequency  of this model because it is not the intended contribution of this paper but rather a means to demonstrate our approach on the large game of Texas holdem     Experimental Setup We tested our approach on both Leduc holdem with the Dirichlet prior and Texas holdem with the informed prior  For the Bayesian methods  we used all three responses  BBR  MAP  and Thompsons  on Leduc and the Thompsons response for Texas  BBR has not been implemented for Texas and MAPs behaviour is very similar to Thompsons  as we will describe below   For all Bayesian methods       strategies were sampled from the prior at the beginning of each trial and used throughout the trial  We have several players for our study  Opti is a Nash  or minimax  strategy for the game  In the case of Leduc  this has been computed exactly  We also sampled opponents from our priors in both Leduc and Texas  which we will refer to as Priors  In the experiments shown  a new opponent was sampled for each trial      hands   so results are averaged over many samples from the priors  Both Priors and Opti are static players  Finally  for state of the art opponent modelling  we used Frequentist   also known as Vexbot  described fully in     and implemented for Leduc  All experiments consisted of running two players against each other for two hundred hands per trial and recording the bankroll  accumulated winnings losses  at each hand  These results were averaged over multiple trials       trials for all Leduc experiments and     trials for the Texas experiments   We present two kinds of plots  The first is simply average bankroll per number of hands played  A straight line on such a plot indicates a constant winning rate  The second is the average winning rate per number of hands played  i e   the first derivative of the aver           BBR MAP Thompson Freq Opti Best Response       BBR MAP Thompson Freq Opti Best Response          Average Winning Rate  Average Bankroll                                                                                  Hands Played                                                 Hands Played                      Figure    Leduc holdem  Avg  Bankroll per hands played for BBR  MAP  Thompsons  Opti  and Frequentist vs  Priors   Figure    Leduc holdem  Avg  Winning Rate per hands played for BBR  MAP  Thompsons  Opti  and Frequentist vs  Priors   age bankroll   This allows one to see the effects of learning more directly  since positive changes in slope indicate improved exploitation of the opponent  Note that winning rates for small numbers of hands are very noisy  so it is difficult to interpret the early results  All results are expressed in raw pot units  e g   bets in the first and second rounds of Leduc are   and   units respectively    Figures   and   show bankroll and winning rate results for BBR  MAP  Thompsons  Opti  and Frequentist versus Opti on Leduc holdem  Note that  on average  a positive bankroll again Opti is impossible  although sample variance allows for it in our experiments  From these plots we can see that the three Bayesian approaches behave very similarly  This is due to the fact that the posterior distribution over our sample of strategies concentrates very rapidly on a single strategy  Within less than    hands  one strategy dominates the rest  This means that the three responses become very similar  Thompsons is almost certain to pick the MAP strategy  and BBR puts most of its weight on the MAP strategy   Larger sample sizes would mitigate this effect  The winning rate graphs also show little difference between the three Bayesian players     Results     Leduc Holdem Figures   and   show the average bankroll and average winning rate for Leduc against opponents sampled from the prior  a new opponent each trial   For such an opponent  we can compute a best response  which represents the best possible exploitation of the opponent  In complement  the Opti strategy shows the most conservative play by assuming that the opponent plays perfectly and making no attempt to exploit any possible weakness  This nicely bounds our results in these plots  Results are given for Best Response  BBR  MAP  Thompsons  Opti  and Frequentist  As we would expect  the Bayesian players do well against opponents drawn from their prior  with little difference between the three response types in terms of bankroll  The winning rates show that MAP and Thompsons converge within the first ten hands  whereas BBR is more erratic and takes longer to converge  The uninformed Frequentist is clearly behind  The independent Dirichlet prior is very broad  admitting a wide variety of opponents  It is encouraging that the Bayesian approach is able to exploit even this weak information to achieve a better result  However  it is unfair to make strong judgements on the basis of these results since  in general  playing versus its prior is the best possible scenario for the Bayesian approach   Frequentist performs slightly worse than the Bayes approaches  The key problem with it is that it can form models of the opponent that are not consistent with any behavioral strategy  e g   it can be led to believe that its opponent can always show a winning hand   Such incorrect beliefs  untempered by any prior  can lead it to fold with high probability in certain situations  Once it starts folding  it can never make the observations required to correct its mistaken belief  Opti  of course  breaks even against itself  On the whole  independent Dirichlet distributions are a poor prior for the Opti solution  but we see a slight improvement over the pure frequentist approach  Our final Leduc results are shown in Figure    playing against the Frequentist opponent  These results are included for the sake of interest  Because the Frequentist opponent is not stationary  it violates the assumptions upon which the Bayesian  and  indeed  the Frequentist  player are based  We cannot drawn any real conclusions from this data  It is interesting  however  that the BBR response is substantially worse than MAP or Thompsons                                Average Bankroll      Average Bankroll  BBR MAP Thompson Freq Opti                                              BBR MAP Thompson Freq Opti                                              Hands Played                      Figure    Leduc holdem  Avg  Bankroll per hands played for BBR  MAP  Thompsons  Opti  and Frequentist vs  Opti                              Hands Played                      Figure    Leduc holdem  Avg  Bankroll per hands played for BBR  MAP  Thompsons  and Opti vs  Frequentist    BBR MAP Thompson Freq Opti                     Average Winning Rate  Average Winning Rate                                                          BBR MAP Thompson Freq                                                  Hands Played                                 Hands Played                           Figure    Leduc holdem  Avg  Winning Rate per hands played for BBR  MAP  Thompsons  Opti  and Frequentist vs  Opti  It seems likely that the posterior distribution does not converge quickly against a non stationary opponent  leading BBR to respond to several differing strategies simulataneously  Because the prior is independent for every information set  these various strategies could be giving radically different advice in many contexts  preventing BBR from generating a focused response  MAP and Thompsons necessarily generate more focused responses  We show winning rates in Figure   for the sake of completeness  with the same caveat regarding non stationarity      Texas Holdem Figure   show bankroll results for Thompsons  Opti  and Frequentist versus opponents sampled from the informed prior for Texas holdem  Here Thompsons and Frequentist give very similar performance  although there is a small  Figure    Leduc holdem  Avg  Winning Rate per hands played for BBR  MAP  Thompsons  and Opti vs  Frequentist  advantage to Thompsons late in the run  It is possible that even with the more informed prior  two hundred hands does not provide enough information to effectively concentrate the posterior on good models of the opponent in this larger game  It may be that priors encoding strong correlations between many information sets are required to gain a substantial advantage over the Frequentist approach     Conclusion This research has presented a Bayesian model for holdem style poker  fully modelling both game dynamics and opponent strategies  The posterior distribution has been described and several approaches for computing appropriate responses considered  Opponents in both Texas holdem and Leduc holdem have been played against using Thompsons sampling for Texas holdem  and approximate Bayesian best response  MAP  and Thompsons for   telligence                             Thompson Freq Opti      Darse Billings  Aaron Davidson  Terrance Schauenberg  Neil Burch  Michael Bowling  Rob Holte  Jonathan Schaeffer  and Duane Szafron  Game Tree Search with Adaptation in Stochastic Imperfect Information Games  In Nathan Netanyahu and Jaap van den Herik Yngvi Bjornsson  editor  Computers and Games    Springer Verlag              Average Bankroll                                                  Hands Played                          Fredrik A  Dahl  A reinforcement learning algorithm applied to simplified two player Texas Holdem poker  In Proceedings of the   th European Conference on Machine Learning  ECML      pages       September        Figure    Texas holdem  Avg  Bankroll per hands played for Thompsons  Frequentist  and Opti vs  Priors       D  Koller and A  Pfeffer  Representations and solutions for game theoretic problems  Artificial Intelligence                       Leduc holdem  These results show that  for opponents drawn from our prior  the posterior captures them rapidly and the subsequent response is able to exploit the opponent  even in just     hands  On Leduc  the approach performs favourably compared with state of the art opponent modelling techniques against prior drawn opponents and a Nash equilibrium  Both approaches can play quickly enough for real time play against humans       K  Korb  A  Nicholson  and N  Jitnah  Bayesian poker  In Uncertainty in Artificial Intelligence  pages                The next major step in advancing the play of these systems is constructing better informed priors capable of modelling more challenging opponents  Potential sources for such priors include approximate game theoretic strategies  data mined from logged human poker play  and more sophisticated modelling by experts  In particular  priors that are capable of capturing correlations between related information sets would allow for generalization of observations over unobserved portions of the game  Finally  extending the approach to non stationary approaches is under active investigation  Acknowledgements We would like to thank Rob Holte  Dale Schuurmanns  Nolan Bard  and the University of Alberta poker group for their insights  This work was funded by the Alberta Ingenuity Centre for Machine Learning  iCore  and NSERC   
  incomplete on each step  still efficiently computes optimal actions in a timely manner   We consider the problem of efficiently learning optimal control policies and value functions over large state spaces in an online setting in which estimates must be available after each interaction with the world  This paper develops an explicitly model based approach extending the Dyna architecture to linear function approximation  Dynastyle planning proceeds by generating imaginary experience from the world model and then applying model free reinforcement learning algorithms to the imagined state transitions  Our main results are to prove that linear Dyna style planning converges to a unique solution independent of the generating distribution  under natural conditions  In the policy evaluation setting  we prove that the limit point is the least squares  LSTD  solution  An implication of our results is that prioritized sweeping can be soundly extended to the linear approximation case  backing up to preceding features rather than to preceding states  We introduce two versions of prioritized sweeping with linear Dyna and briefly illustrate their performance empirically on the Mountain Car and Boyan Chain problems   The Dyna architecture  Sutton       provides an effective and flexible approach to incremental planning while maintaining responsiveness  There are two ideas underlying the Dyna architecture  One is that planning  acting  and learning are all continual  operating as fast as they can without waiting for each other  In practice  on conventional computers  each time step is shared between planning  acting  and learning  with proportions that can be set arbitrarily according to available resources and required response times   Online learning and planning  Efficient decision making when interacting with an incompletely known world can be thought of as an online learning and planning problem  Each interaction provides additional information that can be used to learn a better model of the worlds dynamics  and because this change could result in a different action being best  given the model   the planning process should be repeated to take this into account  However  planning is inherently a complex process  on large problems it not possible to repeat it on every time step without greatly slowing down the response time of the system  Some form of incremental planning is required that  though  The second idea underlying the Dyna architecture is that learning and planning are similar in a radical sense  Planning in the Dyna architecture consists of using the model to generate imaginary experience and then processing the transitions of the imaginary experience by model free reinforcement learning algorithms as if they had actually occurred  This can be shown  under various conditions  to produce exactly the same results as dynamic programming methods in the limit of infinite imaginary experience  The original papers on the Dyna architecture and most subsequent extensions  e g   Singh       Peng   Williams       Moore   Atkeson       Kuvayev   Sutton       assumed a Markov environment with a tabular representation of states  This table lookup representation limits the applicability of the methods to relatively small problems  Reinforcement learning has been combined with function approximation to make it applicable to vastly larger problems than could be addressed with a tabular approach  The most popular form of function approximation is linear function approximation  in which states or state action pairs are first mapped to feature vectors  which are then mapped in a linear way  with learned parameters  to value or next state estimates  Linear methods have been used in many of the successful large scale applications of reinforcement learning  e g   Silver  Sutton   Muller       Schaeffer  Hlynka   Jussila        Linear function approximation is also simple  easy to understand  and possesses some of the strongest convergence and performance guarantees among function approximation methods  It is   natural then to consider extending Dyna for use with linear function approximation  as we do in this paper  There has been little previous work addressing planning with linear function approximation in an online setting  Paduraru        treated this case  focusing mainly on sampling stochastic models of a cascading linear form  but also briefly discussing deterministic linear models  Degris  Sigaud and Wuillemin        developed a version of Dyna based on approximations in the form of dynamic Bayes networks and decision trees  Their system  SPITI  included online learning and planning based on an incremental version of structured value iteration  Boutilier  Dearden   Goldszmidt        Singh        developed a version of Dyna for variable resolution but still tabular models  Others have proposed linear least squares methods for policy evaluation that are efficient in the amount of data used  Bradtke   Barto       Boyan             Geramifard  Bowling   Sutton        These methods can be interpreted as forming and then planning with a linear model of the worlds dynamics  but so far their extensions to the control case have not been well suited to online use  Lagoudakis   Parr       Peters  Vijayakumar   Schaal       Bowling  Geramifard    Wingate        whereas our linear Dyna methods are naturally adapted to this case  We discuss more specifically the relationship of our work to LSTD methods in a later section  Finally  Atkeson        and others have explored linear  learned models with off line planning methods suited to low dimensional continuous systems      Notation  We use the standard framework for reinforcement learning with linear function approximation  Sutton   Barto        in which experience consists of the time indexed stream s    a    r    s    a    r    s           where st  S is a state  at  A is an action  and rt  R is a reward  The actions are selected by a learning agent  and the states and rewards are selected by a stationary environment  The agent does not have access to the states directly but only through a corresponding feature vector t  Rn    st    The n agent selects actions P according to a policy     R  A         such that aA    a         An important step towards finding a good policy is to estimate the value function for a given policy  policy evaluation   The value function is approximated as a linear function with parameter vector   Rn       X    s   V   s    E  t  rt   s    s   t    where           In this paper we consider policies that are greedy or   greedy with respect to the approximate statevalue function   Algorithm     Linear Dyna for policy evaluation  with random sampling and gradient descent model learning Obtain initial     F  b For each time step  Take action a according to the policy  Receive r          r            F  F       F    b  b    r  b    temp    Repeat p times  planning   Generate a sample  from some distribution     F  r  b         r              temp     Theory for policy evaluation  The natural place to begin a study of Dyna style planning is with the policy evaluation problem of estimating a statevalue function from a linear model of the world  The model consists of a forward transition matrix F  Rn  Rn  incorporating both environment and policy  and an expected reward vector b  Rn   constructed such that F  and b   can be used as estimates of the feature vector and reward that follow   A Dyna algorithm for policy evaluation goes through a sequence of planning steps  on each of which a starting feature vector  is generated according to a probability distribution   and then a next feature vector     F  and next reward r   b   are generated from the model  Given this imaginary experience  a conventional modelfree update is performed  for example  according to the linear TD    algorithm  Sutton              r                   or according to the residual gradient algorithm  Baird              r                         where      is a step size parameter  A complete algorithm using TD     including learning of the model  is given in Algorithm         Convergence and fixed point  There are two salient theoretical questions about the Dyna planning iterations     and      Under what conditions on  and F do they converge  and What do they converge to  Both of these questions turn out to have interesting answers  First  note that the convergence of     is in question in part because it is known that linear TD    may diverge if the distribution of starting states during training does not match the distribution created by the normal dynamics of   the system  that is  if TD    is used off policy  This suggests that the sampling distribution used here    might have to be strongly constrained in order for the iteration to be stable  On the other hand  the data here is from the model  and the model is not a general system  it is deterministic  and linear  This special case could be much better behaved  In fact  convergence of linear Dyna style policy evaluation  with either the TD    or residual gradient iterations  is not affected by   but only by F   as long as  exercises all directions in the full n dimensional vector space  Moreover  not only is the fact of convergence unaffected by   but so is the value converged to  In fact  we show below that convergence is to a deterministic fixed point  a value of  such that the iterations     and     leave it unchanged not just in expected value  but for every individual  that could be generated by   The only way this could be true is if the TD error  the first expression in parentheses in each iteration  were exactly zero  that is  if       r                 b     F          b   F           And the only way that this can be true for all  is for the expression in parenthesis above to be zero     Before verifying the conditions of this result  let us rewrite     in terms of the matrix G   I  F   k      k   k  b  k   k   F  I k  k    b    F    I      k   k sk        I  F      b        where    Rn is P arbitrary  AssumeP that  i  the step size   sequence satisfies k   k     k   k       ii  r F        iii   k   are uniformly   bounded   i i d  random variables  and that  iv  C   E k   is non singular  k Then the parameter vector k converges with probability one to  I  F      b     k   k  b  k  k  Gk  k  Here sk is defined by the last equation       assuming that the inverse exists  Note that this expression for the fixed point does not depend on   as promised  If I  F   is nonsingular  then there might be no fixed point  This could happen for example if F were an expansion  or more generally if the limit  F   were not zero  These cases correspond to world models that say the feature vectors diverge to infinity over time  Failure to converge in these cases should not be considered a problem for the Dyna iterations as planning algorithms  these are cases in which the planning problem is ill posed  If the feature vectors diverge  then so too may the rewards  in which case the true values given the model are infinite  No real finite Markov decision process could behave in this way  It remains to show the conditions on F under which the iterations converge to the fixed point if one exists  We prove next that under the TD    iteration      convergence is guaranteed if the numerical radius of F is less than one   and    k     k   k  b  k   k  F k  k  k  k      b   F       which immediately implies that   Theorem      Convergence of linear TD    Dyna for policy evaluation   Consider the TD    iteration with a nonnegative step size sequence  k     Proof  The idea of the proof is to view the algorithm as a stochastic gradient descent method  In particular  we apply Proposition     of  Bertsekas   Tsitsiklis               then that under the residual gradient iteration      convergence is guaranteed for any F as long as the fixed point exists  That F s numerical radius be less than   is a stronger condition than nonsingularity of I  F     but it is similar in that both conditions pertain to the matrix trending toward expansion when multiplied by itself   The model is deterministic because it generates the expectation of the next feature vector  the system itself may be stochastic    The numerical radius of a real valued square matrix A is defined by r A    maxkxk     xT Ax   The cited proposition requires the definition of a potential function J   and will allow us to conclude that limk J k       with probability one  Let   us choose J         E  b  k     F k    k      Note that by our i i d  assumptions on the features  J   is welldefined  We need to check four conditions  because the step size conditions are automatically satisfied    i  The nonnegativity of the potential function   ii  The Lipschitz continuity of J     iii  The pseudo gradient property of the expected update direction  and  iv  The boundedness of the  expected  magnitude of the update  more precisely that E ksk k    k  O kJ k  k      Nonnegativity is satisfied by definition and the boundedness condition  iv  is satisfied thanks to the boundedness of the features  Let us show now that the pseudo gradient property  iii  is satisfied  This condition requires the demonstration of a positive constant c such that ckJ k  k    J k    E  sk  k           Define sk   E  sk  k     Cb  CG  k   A simple calculation gives J k     Gsk   Hence kJ k  k           s  k G Gsk and  J k    sk   sk Gsk   Therefore           is equivalent to c sk G Gsk  sk Gsk   In order to make this true with a sufficiently small c  it suffices to show that   s  Gs     holds for any non zero vector s  An elementary reasoning shows that this is equivalent to     G   G    being positive definite  which in turn is equivalent to r F       showing that  iii  is satisfied  Hence  we have verified all the assumptions of the cited proposition and can therefore we conclude that limk J k       with probability one  Plugging in the expression of J k    we get limt  CbCG  k        Because C and G are invertible  this latter follows from r F        it follows that the limit of k exists and limk k    G     b    I  F      b   verges with probability one to  I  F      b  assuming that  I  F     is non singular  Proof  As all the conditions of Proposition     of  Bertsekas   Tsitsiklis       are trivially satisfied with the choice J     E  J   k     we can conclude that k converges w p   to the minimizer of J    In the previous theorem we have seen that the minimizer of J   is indeed     I  F      b  finishing the proof       Convergence to the LSTD solution  Several extensions of this result are possible  First  the requirement of i i d  sampling can be considerably relaxed  With an essentially unchanged proof  it is possible to show that the theorem remains true if the feature vectors are generated by a Markov process given that they satisfy appropriate ergodicity conditions  Moreover  building on a result by Delyon         one can show that the result continues to hold even if the sequence of features is generated in an algorithmic manner  again provided that some ergodicity conditions are met  PKThe major assumption then is that C   limK   K k   k   k exists and is nonsingular  Further  because there is no noise to reject  there is noP need to decay the step sizes towards zero  the condi tion k   k      in the proofs is used to filter out noise   In particular  we conjecture that sufficiently small constant step sizes would work as well  for a result of this type see Proposition     by Bertsekas   Tsitsiklis         So far we have discussed the convergence of planning given a model  but we have said nothing about the relationship of the model to data  or about the quality of the resultant solution  Suppose the model were the best linear fit to a finite dataset of observed feature vector to feature vector transitions with accompanying rewards  In this case we can show that the fixed point of the Dyna updates is the least squares temporal difference solution  This is the solution for which the mean TD    update is zero and is also the solution found by the LSTD    algorithm  Barto   Bradtke         On the other hand the requirement on the numerical radius of F seems to be necessary for the convergence of the TD    iteration  By studying the ODE associated with      we see that it is stable if and only if CG is a positive stable matrix  i e   iff all its eigenvalues have positive real part   From this it seems necessary to require that G is positive stable  However  to ensure that CG is positive stable the strictly stronger condition that G   G  is positive definite must be satisfied  This latter condition is equivalent to r F        Proof  It suffices to show that the respective solution sets of the equations  We turn now to consider the convergence of Dyna planning using the residual gradient Dyna iteration      This update rule can be derived by taking the gradient of J   k      b  k     k    k    w r t    Thus  as an immediate consequence of Proposition     of  Bertsekas   Tsitsiklis       we get the following result  Theorem      Convergence of residual gradient Dyna for policy evaluation   Assume that k is updated according to k     k   k  b  k   k  F k  k  k   k  F k    where    Rn is arbitrary  Assume that the non negative step size sequence  k   satisfies the summability condition  i  of Theorem     and that  k   are uniformly bounded i i d  random variables  Then the parameter vector k con   Theorem      Given a training dataset of feature  reward  next state feature triples D        r                 n   rn    n    let F  bPbe the least squares model built on D  Assume that n C   k   k   k has full rank  Then the solution     is the same as the LSTD solution on this training set         n X  k  rk     k        k          k          b    F    I        are the same  This is because the LSTD parameter vectors are obtained by solving the first equation and the TD    Dyna solutions are derived from the second equation  Pn Pn Let D   k   k   k      and r   k   k rk   A standard calculation shows that F      C   D  and b   C   r   Plugging in C  D into     and factoring out  shows that any solution of     also satisfies       r    D  C          If we multiply both sides of     by C   from the left we get      Hence any solution of     is also a solution of      Because all the steps of the above derivation are reversible  we get that the reverse statement holds as well    Algorithm     Linear Dyna with PWMA prioritized sweeping  policy evaluation  Obtain initial     F  b For each time step  Take action a according to the policy  Receive r      r                 F  F       F    b  b    r  b    For all i such that  i        For all j such that F ij       Put j on the PQueue with priority  F ij  i   Repeat p times while PQueue is not empty  i  pop the PQueue   b i      F ei   i   i    i     For all j such that F ij       Put j on the queue with priority  F ij           Algorithm     Linear Dyna with MG prioritized sweeping  policy evaluation  Obtain initial     F  b For each time step  Take action a according to the policy  Receive r      r                 F  F       F    b  b    r  b    For all i such that  i        Put i on the PQueue with priority   i   Repeat p times while PQueue is not empty  i  pop the PQueue For all j such that F ij         b j      F ej   j   j    j     Put j on the PQueue with priority         Linear prioritized sweeping  We have shown that the convergence and fixed point of policy evaluation by linear Dyna are not affected by the way the starting feature vectors are chosen  This opens the possibility of selecting them cleverly so as to speed the convergence of the planning process  One natural ideathe idea behind prioritized sweepingis to work backwards from states that have changed in value to the states that lead into them  The lead in states are given priority for being updated because an update there is likely to change the states value  because they lead to a state that has changed in value   If a lead in state is updated and its value is changed  then its lead in states are in turn given priority for updating  and so on  In the table lookup context in which this idea was developed  Moore   Atkeson       Peng       see also Wingate   Seppi        there could be many states preceding each changed state  but only one could be updated at a time  The states waiting to be updated were kept in a queue  prioritized by the size of their likely effect on the value function  As high priority states were popped off the queue and updated  it would sometimes give rise to highly efficient sweeps of updates across the state space  this is what gave rise to the name prioritized sweeping  With function approximation it is not possible to identify and work backwards from individual states  but alternatively one could work backwards feature by feature  If there has just been a large change in  i   the component of the parameter vector corresponding to the ith feature  then one can look backwards through the model to find the features j whose components  j  are likely to have changed as a result  These are the features j for which the elements F ij of F are large  One can then preferentially construct  starting feature vectors  that have non zero entries at these j components  In our algorithms we choose the starting vectors to be the unit basis vectors ej   all of whose components are zero except the jth  which is     Our theoretical results assure us that this cannot affect the result of convergence   Using unit basis vectors is very efficient computationally  as the vector matrix multiplication F  is reduced to pulling out a single column of F   There are two tabular prioritized sweeping algorithms in the literature  The first  due simultaneously to Peng and Williams        and to Moore and Atkeson         which we call PWMA prioritized sweeping  adds the predecessors of every state encountered in real experience to the priority queue whether or not the value of the encountered state was significantly changed  The second form of prioritized sweeping  due to McMahan and Gordon         and which we call MG prioritized sweeping  puts each encountered state on the queue  but not its predecessors  For McMahan and Gordon this resulted in a more efficient planner  A complete specification of our feature by feature versions of these two forms of prioritized sweeping are given above  with TD    updates and gradient descent model learning  as Algorithms   and    These algorithms differ slightly from previous prioritized sweeping algorithms in that they update the value function from the real experiences and not just from model generated experience  With function approximation  real experience is always more informative than model generated experience  which will be distorted by the function approximator  We found this to be a significant effect in our empirical experiments  Section       Algorithm    Linear Dyna with MG prioritized sweeping and TD    updates  control  Obtain initial     F  b For each time step       a  arg maxa b   or   greedy  a     Fa  Take action a  receive r      r                 Fa  Fa       Fa    ba  ba    r  b  a   For all i such that  i        Put i on the PQueue with priority   i   Repeat p times while PQueue is not empty  i  pop the PQueue ij For all j s t  there   exists an a s t  F   a         maxa ba  j     Fa ej   j   j    j     Put j on the PQueue with priority            Theory for Control  We now turn to the full case of control  in which separate models Fa   ba are learned and are then available for each action a  These are constructed such that Fa  and b  a  can be used as estimates of the feature vector and reward that follow  if action a is taken  A linear Dyna algorithm for the control case goes through a sequence of planning steps on each of which a starting feature vector  and an action a are chosen  and then a next feature vector     Fa  and next reward r   ba  are generated from the model  Given this imaginary experience  a conventional model free update is performed  The simplest case is to again apply      A complete algorithm including prioritized sweeping is given in Algorithm    The theory for the control case is less clear than for policy evaluation  The main issue is the stability of the mixture of the forward model matrices  The corollary below is stated for an i i d  sequence of features  but by the remark after Theorem     it can be readily extended to the case where the policy to be evaluated is used to generate the trajectories  Corollary      Convergence of linear TD    Dyna with action models   Consider the Dyna recursion     with the modification that in each step  instead of F k   we use F k   k   where  is a policy mapping feature vectors to actions and  Fa   is a collection of forward model matrices  Similarly  b  k is replaced by b   k   k   As before  assume that k is an unspecified i i d  process  Let  F  b    be the least squares   model of   F   harg minG E kGk  iF k   k k   and b     arg minu E  u  k  b  If the numerical radius  k   k   of F is bounded by one  then the conclusions of Theo       N                   N                                                                                                                      Figure    The general Boyan Chain problem  rem     hold  the parameter vector k converges with probability one to  I  F      b  Proof  The proof is immediate from equation     the normal     for F   which states that E F k     E F k   k   k k   and once we observe that  in the proof of  Theorem       F appears only in expressions of the form E F k   k   As in the case of policy evaluation  there is a corresponding corollary for the residual gradient iteration  with an immediate proof  These corollaries say that  for any policy with a corresponding model that is stable  the Dyna recursion can be used to compute its value function  Thus we can perform a form of policy iterationcontinually computing an approximation to the value function for the greedy policy      Empirical results  In this section we illustrate the empirical behavior of the four Dyna algorithms and make comparisons to model free methods using variations of two standard test problems  Boyan Chain and Mountain Car  Our Boyan Chain environment is an extension of that by Boyan              from    to    states  and from   to    features  Geramifard  Bowling   Sutton        Figure   depicts this environment in the general form  Each episode starts at state N      and terminates in state    For all states s      there is an equal probability of transitioning to states s    or s    with a reward of    From states   and    there are deterministic transitions to states   and   with respective rewards of   and    Our Mountain Car environment is exactly as described by Sutton        Sutton   Barto        re implemented in Matlab  An underpowered car must be driven to the top of a hill by rocking back and forth in a valley  The state variables are a pair  position velocity  initialized to            at the beginning of each episode  The reward is   per time step  There are three discrete actions  accelerate  reverse  and coast   We used a value function representation based on tile coding feature vectors exactly as in Suttons        experiments  with    tilings over the combined  position  velocity  pair  and with the tiles hashed down to        features  In the policy evaluation experiments with this domain  the policy was to accelerate in         Boyan chain         Mountain Car  x     Dyna Random TD        Loss  Dyna Random Dyna PWMA  Loss         TD         Dyna MG  Dyna PWMA     Dyna MG                      Episode                              Episode             Figure    Performance of policy evaluation methods on the Boyan Chain and Mountain Car environments the direction of the current velocity  and we added noise to the domain that switched the selected action to a random action with     probability  Complete code for our test problems as standard RL Glue environments is available from the RL Library hosted at the University of Alberta  In all experiments  the step size parameter  took the form      t     NN   t       in which t is the episode number and the pair  N        was selected based on empirically finding the best combination out of                 and N                     separately for each algorithm and domain  All methods observed the same trajectories in policy evaluation  All graphs are averages of    runs  error bars indicate standard errors in the means  Other parameter settings were                and       We performed policy evaluation experiments with four algorithms  Dyna Random  Dyna PWMA  Dyna MG  as in Algorithms      and model free TD     In the case of the Dyna Random algorithm  the starting feature vectors in planning were chosen to be unit basis vectors with the   in a random location  Figure   shows the policy evaluation performance of the four methods in the Boyan Chain and Mountain Car environments  For the Boyan Chain domain  the loss was the root mean squared error of the learned value function compared to the exact analytical value  averaged over all states  In the Mountain Car domain  the states are visited very non uniformly  and a more sophisticated measure is needed  Note that all of the methods drive  toward an asymptotic value in which the expected TD    update is zero  we can use the distance from this as a loss measure  Specifically  we evaluated each learned value function by freezing it and then running a fixed set of         episodes with it while running the TD    algorithm  but not allowing  to actually change   The norm of the sum of the  attempted  update vectors was then computed and used as the loss  In practice  this measure can be computed very efficiently as   A   b     in the notation of  LSTD     see Bradtke   Barto        In the Boyan Chain environment  the Dyna algorithms generally learned more rapidly than model free TD     DynaMG was initially slower than the other algorithms  then caught up and surpassed them  The relatively poor early performance of Dyna MG was actually due to its being a better planning method  After few episodes the model tends to be of very high variance  and so therefore is the best value function estimate given it  We tested this hypothesis by running the Dyna methods starting with a fixed  well learned model  in this case Dyna MG was the best of all the methods from the beginning  All of these data are for one step of planning for each real step of interaction with the world  p       In preliminary experiments with larger values of p  up to p       we found further improvements in learning rate of the Dyna algorithms over TD     and again Dyna MG was best  The results for Mountain Car are less clear  Dyna MG quickly does significantly better than TD     but the other Dyna algorithms lag initially and never surpass TD     Note that  for any value of p  Dyna MG does many more  updates than the other two Dyna algorithms  because these updates are in an inner loop  cf  Algorithms   and     Even so  because of its other efficiencies Dyna MG tended to run faster overall in our implementation  Obviously  there is a lot more interesting empirical work that could be done here  We performed one Mountain Car experiment with DynaMG as a control algorithm  Algorithm     comparing it with model free Sarsa  i e   Algorithm   with p       The results are shown in Figure    As before  Dyna MG showed a distinct advantage over the model free method in terms of learning rate  There was no clear advantage for either method in the second half of the experiment  We note that  asymptotically  model free methods are never worse than model based methods  and are often better because the model does not converge exactly to the true system because               Return       Dyna MG           Sarsa                           Episode           Figure    Control performance on Mountain Car  Conclusion  In this paper we have taken important steps toward establishing the theoretical and algorithmic foundations of Dyna style planning with linear function approximation  We have established that Dyna style planning with familiar reinforcement learning update rules converges under weak conditions corresponding roughly  in some cases  to the existence of a finite solution to the planning problem  and that convergence is to a unique least squares solution independent of the distribution used to generate hypothetical experience  These results make possible our second main contribution  the introduction of algorithms that extend prioritized sweeping to linear function approximation  with correctness guarantees  Our empirical results illustrate the use of these algorithms and their potential for accelerating reinforcement learning  Overall  our results support the conclusion that Dyna style planning may be a practical and competitive approach to achieving rapid  online control in stochastic sequential decision problems with large state spaces  Acknowledgements  of structural modeling assumptions   The case we treat herelinear models and value functions with one step TD methodsis a rare case in which asymptotic performance of model based and model free methods should be identical   The benefit of models  and of planning generally  is in rapid adaptation to new problems and situations   The authors gratefully acknowledge the substantial contributions of Cosmin Paduraru and Mark Ring to the early stages of this work  This research was supported by iCORE  NSERC and Alberta Ingenuity   These empirical results are not extensive and in some cases are preliminary  but they nevertheless illustrate some of the potential of linear Dyna methods  The results on the Boyan Chain domain show that Dyna style planning can result in a significant improvement in learning speed over modelfree methods  In addition  we can see trends that have been observed in the tabular case re occurring here with linear function approximation  In particular  prioritized sweeping can result in more efficient learning than simply updating features at random  and the MG version of prioritized sweeping seems to be better than the PWMA version   Atkeson  C          Using local trajectory optimizers to speed up global optimization in dynamic programming  Advances in Neural Information Processing Systems             Baird  L  C          Residual algorithms  Reinforcement learning with function approximation  In Proceedings of the Twelfth International Conference on Machine Learning  pp        Bertsekas  Dimitri P   Tsitsiklis  J          Neuro Dynamic Programming  Athena Scientific        Boutilier  C   Dearden  R   Goldszmidt  M          Stochastic dynamic programming with factored representations  Artificial Intelligence             Bowling  M   Geramifard  A   Wingate  D          Sigma point policy iteration  In Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems  Boyan  J  A          Least squares temporal difference learning  In Proceedings of the Sixteenth International Conference on Machine Learning        Boyan  J  A          Technical update  Least squares temporal difference learning  Machine Learning              Bradtke  S   Barto  A  G          Linear least squares al   Finally  we would like to note that we have done extensive experimental work  not reported here  attempting to adapt least squares methods such as LSTD to online control domains  in particular to the Mountain Car problem  A major difficulty with these methods is that they place equal weight on all past data whereas  in a control setting  the policy changes and older data becomes less relevant and may even be misleading  Although we have tried a variety of forgetting strategies  it is not easy to obtain online control performance with these methods that is superior to modelfree methods  One reason we consider the Dyna approach to be promising is that no special changes are required for this case  it seems to adapt much more naturally and effectively to the online control setting   
