 Recent research has shown that surprisingly rich models of human activity can be learned from GPS  positional  data  However  most effort to date has concentrated on modeling single individuals or statistical properties of groups of people  Moreover  prior work focused solely on modeling actual successful executions  and not failed or attempted executions  of the activities of interest  We  in contrast  take on the task of understanding human interactions  attempted interactions  and intentions from noisy sensor data in a fully relational multi agent setting  We use a real world game of capture the flag to illustrate our approach in a well defined domain that involves many distinct cooperative and competitive joint activities  We model the domain using Markov logic  a statistical relational language  and learn a theory that jointly denoises the data and infers occurrences of high level activities  such as a player capturing an enemy  Our unified model combines constraints imposed by the geometry of the game area  the motion model of the players  and by the rules and dynamics of the game in a probabilistically and logically sound fashion  We show that while it may be impossible to directly detect a multi agent activity due to sensor noise or malfunction  the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it  Further  we show that given a model of successfully performed multi agent activities  along with a set of examples of failed attempts at the same activities  our system automatically learns an augmented model that is capable of recognizing success and failure  as well as goals of peoples actions with high accuracy  We compare our approach with other alternatives and show that our unified model  which takes into account not only relationships among individual players  but also relationships among activities over the entire length of a game  although more computationally costly  is significantly more accurate  Finally  we demonstrate that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks      Introduction Our society is founded on the interplay of human relationships and interactions  Since every person is tightly embedded in our social structure  the vast majority of human behavior can be fully understood only in the context of the actions of others  Thus  not surprisingly  more and more evidence shows that when we want to model behavior of a person  the single best predictor is often the behavior of people in her social network  For instance  behavioral patterns of people taking taxis  rating movies  choosing a cell phone provider  or sharing music are best explained and predicted by the habits of related people  rather than by all the single person attributes such as age  race  or education  Bell  Koren    Volinsky        Pentland         In contrast to these observations  most research effort on activity recognition to date has concentrated on modeling single individuals  Bui        Liao  Fox    Kautz               or statistical properties of aggregate groups of individuals  Abowd  Atkeson  Hong  Long  Kooper    Pinkerton        Horvitz  Apacible  Sarin    Liao         or combinations of both  Eagle   Pentland         c      AI Access Foundation  All rights reserved    S ADILEK   K AUTZ  Notable exceptions to this isolated individuals approach includes the work of Kamar and Horvitz        and Gupta  Srinivasan  Shi  and Davis         where simple relationships among people are just starting to be explicitly considered and leveraged  For instance  Eagle and Pentland        elegantly model the location of individuals from multi modal sensory data  but their approach is oblivious to the explicit effects of ones friends  relatives  etc  on ones behavior  The isolated individuals approximations are often made for the sake of tractability and representational convenience  While considering individuals independently of each other is sufficient for some constrained tasks  in many interesting domains it discards a wealth of important information or results in an inefficient and unnatural data representation  On the other hand  decomposing a domain into a set of entities  representing for instance people  objects in their environment  or activities  that are linked by various relationships  e g   is a  has a  is involved in  is a natural and clear way of representing data  To address the shortcomings of nonrelational behavior modeling  we introduce the capture the flag domain  described below   and argue for a statistical relational approach to learning models of multi agent behavior from raw GPS data  The CTF dataset is on one hand quite complex and recorded by real world sensors  but at the same time it is well defined  as per the rules of the game   thereby allowing for an unambiguous evaluation of the results  Being able to recognize peoples activities and reason about their behavior is a necessary precondition for having intelligent and helpful machines that are aware of what is going on in the human machine as well as human human relationships  There are many exciting practical applications of activity recognition that have the potential to fundamentally change peoples lives  For example  cognitive assistants that help people and teams be more productive  or provide support to  groups of  disabled individuals  or efficiently summarize a long complex event to a busy person without leaving out essential information  Other important applications include intelligent navigation  security  physical as well as digital   human computer interaction  and crowdsourcing  All these applications and a myriad of others build on top of multi agent activity recognition and therefore require it as a necessary stepping stone  Furthermore  as a consequence of the anthropocentrism of our technology  modeling human behavior playsperhaps surprisinglya significant role even in applications that do not directly involve people  e g   unmanned space probes   Furthermore  reasoning about human intentions is an essential element of activity recognition  since if we can recognize what a person  or a group of people  wants to do  we can proactively try to help them  orin adversarial situationshinder them   Intent is notoriously problematic to quantify  e g   Baldwin   Baird         but we show that in the capture the flag domain  the notion is naturally captured in the process of learning the structure of failed activities  We all know perhaps too well that a successful action is often precededand unfortunately sometimes also followedby multiple failed attempts  Therefore  reasoning about attempts typically entails high practical utility  but not just for their relatively high frequency  Consider  for example  a task of real time analysis of a security video system  There  detecting that a person or a group of people  again  relations  intend to steal something is much more important and useful than recognizing that a theft has taken  or even is taking  place  because then it is certainly too late to entirely prevent the incident  and it may also be too late or harder to merely stop it  We believe that recognition of attempts in peoples activities is a severely underrepresented topic in artificial intelligence that needs to be explored more since it opens a new realm of interesting possibilities  Before we delve into the details of our approach in Sections   and    we briefly introduce the CTF dataset  Section     highlight the main contributions of our work  Section     and review      L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  background material  Section     We discuss related work  conclude  and outline future work in Sections      and   respectively  This paper incorporates and extends our previous work  Sadilek   Kautz      a      b       Capture The Flag Domain Imagine two teamsseven players eachplaying capture the flag  CTF  on a university campus  where each player carries a consumer grade global positioning system  GPS  that logs its location  plus noise  every second  see Figure     The primary goal is to enter the opponents flag area  Players can be captured only while on enemy territory by being tagged by the enemy  Upon being captured  they must remain in place until freed  tagged by a teammate  or the game ends  The games involve many competitive and cooperative activities  but here we focus on  both successful and attempted  capturing and freeing  Visualization of the games is available from the first authors website  We collected four games of CTF on a portion of the University of Rochester campus  about    acres  with Columbus V     GPS loggers  one per player  with   GB memory card each that were set to a sampling rate of   Hz  The durations of the games ranged approximately from   to    minutes  Our work is not primarily motivated by the problem of annotating strategy games  although there are obvious applications of our results to sports and combat situations  We are  more generally  exploring relational learning and inference methods for recognizing multi agent activities from location data  We accept the fact that the GPS data at our disposal is inherently unreliable and ambiguous for any one individual  We therefore focus on methods that jointly and simultaneously localize and recognize the high level activities of groups of individuals  Although the CTF domain doesnt capture all the intricacies of life  it contains many complex  interesting  and yet well defined  multi agent  activities  Moreover  it is based on extensive real world GPS data  total of         data points   Thus most of the problems that we are addressing here clearly have direct analogs in everyday life situations that ubiquitous computing needs to addressimagine people going about their daily lives in a city instead of CTF players  and their own smart phones instead of GPS loggers  One of the main challenges we have to overcome if we are to successfully model CTF is the severe noise present in the data  Accuracy of the GPS data varies from   to more than    meters  In open areas  readings are typically off by   meters  but the discrepancy is much higher in locations with tall buildings  which are present within the game area  or other obstructions  Compare the scale of the error with the granularity of the activities we concern ourselves with  both capturing and freeing involves players that are within reaching distance  less than   meter  apart  Therefore  the signal to noise ratio in this domain is daunting  The error has a systematic component as well as a significant stochastic component  Errors between devices are poorly correlated  because subtle differences between players  such as the angle at which the device sits in the players pocket  can dramatically affect accuracy  Moreover  since we consider multi agent scenarios  the errors in individual players readings can add up  thereby creating a large discrepancy between the reality and the recorded dataset  Because players can move freely through open areas  we cannot reduce the data error by assuming that the players move along road or walkways  as is done in much work on GPS based activity recognition  e g   Liao et al          Finally  traditional techniques for denoising GPS data  such as Kalman filtering  are       S ADILEK   K AUTZ  Figure    A snapshot of a game of capture the flag that shows most of the game area  Players are represented by pins with letters  In our version of CTF  the two flags are stationary and are shown as white circles near the top and the bottom of the figure  The horizontal road in the middle of the image is the territory boundary  The data is shown prior to any denoising or corrections for map errors  Videos of the games are available at http   www cs rochester edu u sadilek       L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  of little help  due to the low data rate    sample per second  relative to the small amount of time required for a player to completely change her speed or direction  If we are to reliably recognize events that happen in these games in the presence of such severe noise  we need to consider not only each player  but also the relationships among them and their actions over extended periods of time  possibly the whole length of the game   Consider a concrete task of inferring the individual and joint activities and intentions of the CTF players from their GPS traces  For example  suppose the GPS data shows player A running toward a stationary teammate B  then moving away  What occurred  Possibly player A has just freed player B  but GPS error has hidden the fact that player A actually reached B  Another possibility is that player A had the intention of freeing player B  but was scared off by an opponent at the last second  Yet another possibility is that no freeing occurred nor was even intended  because player B had not been previously captured  Understanding a game thus consists of inferring a complex set of interactions among the various players as well as the players intentions  The conclusions drawn about what occurs at one point in time affect and are affected by inferences about past and future events  In the example just given  recognizing that player B is moving in the future reinforces the conclusion that player A is freeing player B  while failing to recognize a past event of player B being captured decreases confidence in that conclusion  The game of CTF also illustrates that understanding a situation is as much or more about recognizing attempts and intentions as about recognizing successfully executed actions  For example  in course of a    minute game  only a handful of capture or freeing events occur  However  there are dozens of cases where one player unsuccessfully tries to capture an opponent or to free a teammate  A description of a game that was restricted to what actually occurred would be only a pale reflection of the original   Figure    Three snapshots of a game situation where both successful and failed capturing occur  This example also illustrates the need for an approach that exploits both the relational and the far reaching temporal structure of our domain   See text for explanation    As a concrete example  consider a real game situation illustrated in Figure    There we see three snapshots of a game projected over a map of the campus before any modification of the GPS data  The game time is shown on each snapshot  Players D  F  and G are allies and are currently on their home territory near their flag  whereas players L and M are their enemies  In the first snapshot  players L and M head for the opponents flag but thenin the second framethey are intercepted by G  At this point it is unclear what is happening because of the substantial error in the GPS data       S ADILEK   K AUTZ  the three players appear to be very close to each other  but in actuality they could have been    or more meters apart  However  once we see the third snapshot  note that tens of seconds have passed  we realize that player G actually captured only player M and didnt capture L since G is evidently still chasing L  The fact that player M remains stationary coupled with the fact that neither D nor F attempt to capture him suggests that M has indeed been captured  We show that it is possible to infer occurrences of capturing events even for complex situations like these whereas limited approaches largely fail  However  we need to be able to recognize not just individual events  we also need to discover new activities  identify their respective goals  and distinguish between events based on whether their outcomes are favorable or negative  For instance  in the second frame  player G tries to capture both L and M  Although he succeeded in the former case  he failed in the latter  Many different kinds of cooperative and competitive multi agent activities occur in the games  The lowest level joint activities are based on location and movement  and include approaching and being at the same location  Note  that noise in the GPS data often makes it difficult or impossible to directly detect these simple activities  At the next level come competitive multi agent activities including capturing and attacking  cooperative activities include freeing  and there are activities  such as chasing and guarding  that may belong to either category or to both categories  There are also more abstract tactical activities  such as making a sacrifice  and overall strategies  such as playing defensively  In this paper  we concentrate on activities at the first two levels      Our Contributions The main contributions of this paper are as follows  We first present a novel method that simultaneously denoises positional data and learns a model of multi agent activities that occur there  We subsequently evaluate the model on the CTF dataset and show that it achieves high accuracy in recognizing complex game events  However  creating a model by manually writing down new rules or editing existing axioms is laborious and prone to introduction of errors or unnecessarily complex theories  Thus  we would like to automate this process by learning  or inducing  new axioms from training data  For people  it is much easier to provide or validate concrete examples than to directly modify a model  This leads us to our second contribution  We show how to automatically augment a preexisting model of  joint  activities so that it is capable of not only recognizing successful actions  but also identifies failed attempts at the same types of activities  This line of work also demonstrates that explicitly modeling attempted interactions in a unified way improves overall model performance  As our third contribution  we demonstrate that the difference  discussed below  between the newly learned definitions of a failed activity and the original definition of the corresponding successful activity directly corresponds to the goal of the given activity  For instance  as per the rules of the capture the flag game  a captured player cannot move until freed  When our system induces the definition of failed capture  the new theory does not contain such a constraint on the movement of the almost captured player  thereby allowing him to move freely      Background The cores of our models described below are implemented in Markov logic  ML   a statisticalrelational language  In this section  we provide a brief overview of ML  which extends finite firstorder logic  FOL  to a probabilistic setting  For a more detailed  and excellent  treatment of FOL        L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  ML  and inductive logic programming see the work of Shoenfield         Domingos  Kok  Lowd  Poon  Richardson  and Singla         and De Raedt and Kersting         respectively  In order to compare the Markov logic based models to alternative approaches  we consider a dynamic Bayesian network  DBN  model in the experiments below as one of our baselines  We therefore review relevant aspects of DBNs in this section as well      Markov Logic Given the inherent uncertainty involved in reasoning about real world activities as observed through noisy sensor readings  we looked for a methodology that would provide an elegant combination of probabilistic reasoning with the expressive  relatively natural  and compact but unfortunately strictly true or false formulas of first order logic  And that is exactly what Markov logic provides and thus allows us to elegantly model complex finite relational non i i d  domains  A Markov logic network  MLN  consists of a set of constants C and of a set of pairs hFi   wi i such that each FOL formula Fi has a weight wi  R associated with it  Optionally  each weight can be further scaled by a real valued function of a subset of the variables that appear in the corresponding formula  Markov logic networks that contain such functions are called hybrid MLNs  Wang   Domingos         A MLN can be viewed as a template for a Markov network  MN  as follows  the MN contains one node for each possible ground atom of MLN  The value of the node is   if the corresponding atom is false and   otherwise  Two nodes are connected by an edge if the corresponding atoms appear in the same formula  Thus  the MN has a distinct clique corresponding to each grounding of g each formula  By Fi j we denote the j th grounding of formula Fi   The MN has a feature value fi j gj for each Fi such that   g   if Fi j is true fi j     otherwise Each weight wi intuitively represents the relative importance of satisfying  or violating  if the weight is negative  the corresponding formula Fi   More formally  the weight scales the difference in log probability between a world that satisfies n groundings of the corresponding formula and one that results in m true groundings of the formula  all else being equal  cf  Equation     Thus the problem of satisfiability is relaxed in MLNs  We no longer search for a satisfying truth assignment as in traditional FOL  Instead  we are looking for a truth assignment that maximizes the sum of the weights of all satisfied formulas  The weights can be either specified by the knowledge base engineer or  as in our approach  learned from training data  That is  we provide the learning algorithm with labeled capture instances and pairs of raw and corresponding denoised trajectories along with labeled instances of game events and it finds an optimal set of weights that maximize the likelihood of the training data  Weight learning can be done in either generative or discriminative fashion  Generative training maximizes the joint probability of observed  evidence  as well as hidden  query  predicates  whereas discriminative learning directly maximizes the conditional likelihood of the hidden predicates given the observed predicates  Since prior work demonstrated that Markov network models learned discriminatively consistently outperform their generatively trained counterparts  Singla   Domingos         we focus on discriminative learning in our activity recognition domain  Once the knowledge base with weights has been specified  we can ask questions about the state of hidden atoms given the state of the observed atoms  Let X be a vector of random variables  one random variable for each possible ground atom in the MN  and let  be the set of all possible      S ADILEK   K AUTZ  instantiations of X  Then  each x   represents a possible world  If  x    Pr X   x       holds  the probability distribution over these worlds is defined by   X     Pr X   x    exp wi ni x i      Z i  where ni  x i    is the number of true groundings of i th formula with wi as its weight in a world x and   X X   Z  exp wi ni x i      x  i  Equation   can be viewed as assigning a score to each possible world and dividing each score by the sum of all scores over all possible worlds  the constant Z  in order to normalize  Maximum a posteriori  MAP  inference in Markov logic given the state of the observed atoms reduces to finding a truth assignment for the hidden atoms such that the weighed sum of satisfied clauses is maximal  Even though this problem is in general  P complete  we achieve reasonable run times by applying Cutting Plane MAP Inference  CPI   Riedel         CPI can be thought of as a meta solver that incrementally grounds a Markov logic network  at each step creating a Markov network that is subsequently solved by any applicable methodsuch as MaxWalkSAT or via a reduction to an integer linear program  CPI refines the current solution by searching for additional groundings that could contribute to the objective function  Up to this point  we have focused on first order Markov logic  In first order ML  each variable ranges over objects present the domain  e g   apples  players  or cars   On the other hand  in finite second order Markov logic  we variabilize not only objects but also predicates  relations  themselves  Kok   Domingos         Our CTF model contains a predicate variable for each type of activity  For example  we have one variable captureType whose domain is  capturing  failedCapturing  and analogously for freeing events  When grounding the second order ML  we ground all predicate variables as well as object variables  There has also been preliminary work on generalizing ML to be well defined over infinite domains  which would indeed give it the full power of FOL  Singla   Domingos         Implementations of Markov logic include Alchemy  and theBeast    Our experiments used a modified version of theBeast      Dynamic Bayesian Networks A Bayesian network  BN  is a directed probabilistic graphical model  Jordan         Nodes in the graph represent random variables and edges represent conditional dependencies  cf  Figure     For a BN with n nodes  the joint probability distribution is given by Pr X            Xn      n Y i       http   alchemy cs washington edu     http   code google com p theBeast         Pr Xi  Pa Xi            L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  where Pa Xi   denotes the parents of node Xi   In a typical setting  a subset of the random variables is observed  we know their actual values   while the others are hidden and their values need to be inferred  A dynamic Bayesian network  DBN  is a BN that models sequential data  A DBN is composed of slicesin our case each slice represents a one second time interval  In order to specify a DBN  we either write down or learn intra  and inter slice conditional probability distributions  CPDs   The intra slice CPDs typically constitute the observation model while the inter slice CPDs model transitions between hidden states  For an extensive treatment of DBNs  see the work of Murphy         There are a number of parameter learning and inference techniques for DBNs  To match the Markov logic based framework  in the experiments with the DBN model presented below  we focus on a supervised learning scenario  where the hidden labels are known at training time and therefore a maximum likelihood estimate can be calculated directly  We find a set of parameters  discrete probability distributions   that maximize the log likelihood of the training data  This is achieved by optimizing the following objective function        argmax log Pr x  t   y  t              where x  t and y  t represent the sequence of observed and hidden values  respectively  between times   and t  and   is the set of optimal model parameters  In our implementation  we represent probabilities and likelihoods with their log counterparts to avoid arithmetic underflow  At testing time  we are interested in the most likely explanation of the observed data  That is  we want to calculate the most likely assignment of states to all the hidden nodes  i e   Viterbi decoding of the DBN  given by         y  t   argmax log Pr y  t  x  t     y  t  where Pr y  t  x  t   is the conditional probability of a sequence of hidden states y  t given a concrete sequence of observations x  t between times   and t  We calculate the Viterbi decoding efficiently using dynamic programming  Jordan             Methodology In this section  we describe the three major components of our approach  In short  we first manually construct a model of captures and freeings in CTF and optimize its parameters in a supervised learning framework  Section       This constitutes our seed theory that is used for denoising raw location data and recognition of successful multi agent activities  We then show  in Section      how to automatically extend the seed theory by inducing the structure and learning the importance of failed captures and freeings as well as the relationships to their successful counterparts  Finally  in Section      we use the augmented theory to recognize this richer set of multi agent activitiesboth successful and failed attemptsand extract the goals of the activities  Specifically  we investigate the following four research questions  Q   Can we reliably recognize complex multi agent activities in the CTF dataset even in the presence of severe noise  Q   Can models of attempted activities be automatically learned by leveraging existing models of successfully performed actions       S ADILEK   K AUTZ  Q   Does modeling both success and failure allow us to infer the respective goals of the activities  Q   Does modeling failed attempts of activities improve the performance on recognizing the activities themselves  We now elaborate on each of the three components of our system in turn  and subsequently discuss  in light of the experimental results and lessons learned  our answers to the above research questions      Recognition of Successful Activities In this section  we present our unified framework for intelligent relational denoising of the raw GPS data while simultaneously labeling instances of a player being captured by an enemy or freed by an ally  Both the denoising and the labeling are cast as a learning and inference problem in Markov logic  By denoising  we mean modifying the raw GPS trajectories of the players such that the final trajectories satisfy constraints imposed by the geometry of the game area  the motion model of the players  as well as by the rules and the dynamics of the game  In this paper  we refer to this trajectory modification as snapping since we tile the game area with   by   meter cells and snap each raw GPS reading to an appropriate cell  By creating cells only in unobstructed space  we ensure the final trajectory is consistent with the map of the area  We begin by modeling the domain via a Markov logic theory  where we write the logical formulas that express the structure of the model by hand  and learn an optimal set of weights on the formulas from training data in a supervised discriminative fashion  details on the experimental setup are in Section     In the following two subsections  we will show how to augment this seed Markov logic theory to recognize a richer set of events and extract the goals of players multi agent activities  In order to perform data denoising and recognition of successful capturing and freeing  we model the game as weighted formulas in Markov logic  Some of the formulas are hard  in the sense that we are only interested in solutions that satisfy all of them  Hard formulas capture basic physical constraints  e g   a player is only at one location at a time  and inviolable rules of the game  e g   a captured player must stand still until freed or the game ends    The rest of the formulas are soft  meaning there is a finite weight associated with each one  Some of the soft constraints correspond to a traditional low level data filter  expressing preferences for smooth trajectories that are close to the raw GPS readings  Other soft constraints capture high level constraints concerning when individual and multi agent activities are likely to occur  For example  a soft constraint states that if a player encounters an enemy on the enemys territory  the player is likely to be captured  The exact weights on the soft constraints are learned from labeled data  as described below  We distinguish two types of atoms in our models  observed  e g   GPS P                       and hidden  e g   freeing P    P         The observed predicates in the CTF domain are  GPS  enemies  adjacent  onHomeTer  and onEnemyTer   whereas capturing  freeing  isCaptured  isFree  samePlace  and snap are hidden  Additionally  the set of hidden predicates is expanded by the structure learning algorithm described below  see Table   for predicate semantics   In the training phase     Cheating did not occur in our CTF games  but in principle could be accommodated by making the rules highlyweighted soft constraints rather than hard constraints     While the noise in the GPS data introduces some ambiguity to the last two observed predicates  we can still reliably generate them since the road that marks the boundary between territories constitutes a neutral zone        L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  Hard Rules  H   Each raw GPS reading is snapped to exactly one cell  H     a  When player a frees player b  then both involved players must be snapped to a common cell at that time   b  A player can only be freed by a free ally   c  A player can be freed only when he or she is currently captured   d  Immediately after a freeing event  the freed player transitions to a free state   e  A player can only be freed while on enemy territory   H     a  When player a captures player b  then both involved players must be snapped to a common cell at that time   b  A player can only be captured by a free enemy   c  A player can be captured only if he or she is currently free   d  Immediately after a capture event  the captured player transitions to a captured state   e  A player can be captured only when standing on enemy territory   H   All players are free at the beginning of the game  H   At any given time  a player is either captured or free but not both  H   A player transitions from a captured state to a free state only via a freeing event  H   A player transitions from a free state to a captured state only via a capture event  H   If a player is captured then he or she must remain in the same location   Soft Rules  S   Minimize the distance between the raw GPS reading and the snapped to cell  S   Minimize projection variance  i e   two consecutive snappings should be generally correlated  S   Maximize smoothness  both in terms of space and time  of the final player trajectories  S   If players a and b are enemies  a is on enemy territory and b is not  b is not captured already  and they are close to each other  then a probably captures b  S   If players a and b are allies  both are on enemy territory  b is currently captured and a is not  and they are close to each other  then a probably frees b  S   Capture events are generally rare  i e   there are typically only a few captures within a game  S   Freeing events are also generally rare   Figure    Descriptions of the hard and soft rules for capture the flag  our learning algorithm has access to the known truth assignment to all atoms  In the testing phase  it can still access the state of the observed atoms  but it has to infer the assignment to the hidden atoms  Figure   gives an English description of our hard and soft rules for the low level movement and player interactions within capture the flag  Corresponding formulas in the language of ML are shown in Figures   and         S ADILEK   K AUTZ  Predicate capturing a  b  t  enemies a  b  adjacent c    c    failedCapturing a  b  t  failedFreeing a  b  t  freeing a  b  t  isCaptured a  t  isFailedCaptured a  t   Type hidden observed observed hidden hidden hidden hidden hidden  isFailedFree a  t   hidden  isFree a  t   hidden  onEnemyTer a  t  onHomeTer a  t  samePlace a  b  t   observed observed hidden  snap a  c  t   hidden  Meaning Player a is capturing b at time t  Players a and b are enemies  Cells c  and c  are mutually adjacent  or c    c    Player a is unsuccessfully capturing b at time t  Player a is unsuccessfully freeing b at time t  Player a is freeing b at time t  Player a is in captured state at time t  At time t  player a is in a state that follows an unsuccessful attempt at capturing a  a in this state has the same capabilities as when free  At time t  player a is in a state that follows an unsuccessful attempt at freeing a  a in this state has the same capabilities as when captured  Player a is in free state at time t  isFree a  t    isCaptured a  t    Player a in on enemy territory at time t  Player a in on home territory at time t  Players a and b are either snapped to a common cell or to two adjacent cells at time t  Player a is snapped to cell c at time t   Table    Summary of the logical predicates our models use  Predicate names containing the word failed are introduced by the Markov logic theory augmentation method described in Section         We compare our unified approach with four alternative models  The first two models  baseline and baseline with states  are purely deterministic and they separate the denoising of the GPS data and the labeling of game events  We implemented both of them in Perl  They do not involve any training phase  The third alternative model is a dynamic Bayesian network shown in Figure    Finally  we have two models cast in Markov logic  the two step ML model and the unified ML model itself  The unified model handles the denoising and labeling in a joint fashion  whereas the two step approach first performs snapping given the geometric constraints and subsequently labels instances of capturing and freeing  The latter three models are evaluated using four fold crossvalidation where in order to test on a given game  we first train a model on the other three games  All of our models can access the following observed data  raw GPS position of each player at any time and indication whether they are on enemy or home territory  location of each   by   meter cell  cell adjacency  and list of pairs of players that are enemies  We tested all five models on the same observed data  The following describes each model in more detail   Baseline Model  B  This model has two separate stages  First we snap each reading to the nearest cell and afterward we label the instances of player a capturing player b  The labeling rule is simple       L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  we loop over the whole discretized  via snapping  data set and output capturing a  b  t  every time we encounter a pair of players a and b such that they were snapped  in the first step  to either the same cell or to two mutually adjacent cells at time t  they are enemies  and a is on its home territory while b is not  Freeing recognition is not considered in this simple model since we need to have a notion of persisting player states  captured or free  in order to model freeing in a meaningful way   Baseline Model with States  B S  This second model builds on top of the previous one by introducing a notion that players have states  If player a captures player b at time t  b enters a captured state  in logic  isCaptured b  t        Then b remains in captured state until he moves  is snapped to a different cell at a later time  or the game ends  As per rules of CTF  a player who is in captured state cannot be captured again  Thus  this model works just like the previous one except whenever it is about to label a capturing event  it checks the states of the involved players and outputs capturing a  b  t  only if both a and b are not in captured state  Freeing recognition is implemented in an analogous way to capturing recognition  Namely  every time a captured player b is about to transition to a free state  we check if b has a free teammate a nearby  again  within the adjacent cells   If that is the case  we output freeing a  b  t    Dynamic Bayesian Network Model  DBN  The dynamic Bayesian network model can be viewed as a probabilistic generalization of the above baseline model with states  The structure of the DBN model for one player is shown in Figure    In each time slice  we have one hidden node and four observed nodes  all of which represent binary random variables  We want to infer the most likely state S for each player at any given time t over the course of a game  The state is either free or captured and is hidden at testing time  There are four observed random variables per time step that model players motion  M    presence or absence of at least one enemy  EN   and ally  AN   player nearby  and finally players location on either home or enemy territory  ET    Each player is modeled by a separate DBN  Therefore  there are fourteen instantiated DBNs for each game  but within any one game  all the DBNs share the same set of parameters  Note that the DBN model does not perform any GPS trajectory denoising itself  To make a fair comparison with the Markov logic models  we use the denoising component of the Markov logic theory using only constraints H  and S S   in Figure     This produces a denoised discretization of the data that is subsequently fed into the DBN model  The random variables within the DBN that capture the notion of player movement and players being nearby one another is defined on the occupancy grid of the game area  just like in the two deterministic baseline models  Namely  a player is said to be moving between time t and t     when he or she is snapped to two different nonadjacent cells at those times  Similarly  two players are nearby if they are snapped either to the same cell or to two adjacent cells   Two Step ML Model   SML  In the two step approach  we have two separate theories in Markov logic  The first theory is used to perform a preliminary snapping of each of the player trajectories individually us     S ADILEK   K AUTZ  ETt       ENt  ANt  ETt    ENt    St  St    Mt  Mt    ANt         Figure    Two consecutive time slices of our dynamic Bayesian network for modeling the state of an individual player P from observations  Shaded nodes represent observed random variables  unfilled denote hidden variables  All random variables are binary   ETt     when P is on enemy territory at time t  ENt     when there is an enemy nearby at time t  ANt     when there is an ally nearby at time t  and finally Mt     if P has moved between time t    and t  The value of hidden state St is   if P is captured at time t and   when P is free    ing constraints H  and S S   in Figure     This theory is identical to the one used in the discretization step in the DBN model above  The second theory then takes this preliminary denoising as a list of observed atoms in the form preliminarySnap a  c  t   meaning player a is snapped to cell c at time t  and uses the remaining constraints to label instances of capturing and freeing  while considering cell adjacency in the same manner as the previous three models  The two step model constitutes a decomposition of the unified model  see below  and overall contains virtually the same formulas  except  SML operates with an observed preliminarySnap predicate  whereas the unified model contains a hidden snap predicate instead  Thus we omit elaborating on it further here   Unified ML Model  UML  In the unified approach  we express all the hard constraints H H  and soft constraints S  S   Figure    in Markov logic as a single theory that jointly denoises the data and labels game events  Selected interesting formulas are shown in Figure  their labels correspond to the listing in Figure    Note that formulas S S  contain real valued functions d    d    and d  respectively  d  returns the distance between agent a and cell c at time t  Similarly  d  returns the dissimilarity of the two consecutive snapping vectors  given agent as position at time t and t     and the location of the centers of two cells c  and c    Finally  since people prefer to move in straight lines  function d  quantifies the lack of smoothness of any three consecutive segments of the trajectory  Since wp   ws   and wt are all assigned negative values during training  formulas S S  effectively softly enforce the corresponding geometric constraints     The initial point of each snapping  projection  vector is a raw GPS reading and the terminal point is the center of the cell we snap that reading to         L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  The presence of functions d  through d  renders formulas S S  hybrid formulas  This means that at inference time  the instantiated logical part of each formula evaluates to either    true  or    false   which is in turn multiplied by the product of the corresponding function value and the formula weight  We will see how we train  test  and evaluate these four models  and how they perform on the multi agent activity recognition task in Section    Next  we turn to our supervised learning method for augmenting the unified ML model in order to recognize both successful and failed attempts at multi agent activities  Hard formulas  a  t c   snap a  c  t      H         a  c  c   t    snap a  c  t   c    c    snap a  c   t  a    a    t   freeing a    a    t   samePlace a    a    t   isFree a    t    H    enemies a    a     isCaptured a    t   isFree a    t        onEnemyTer a    t   onEnemyTer a    t   a    a    t   capturing a    a    t   samePlace a    a    t   isFree a    t    H    enemies a    a     isFree a    t   isCaptured a    t        onHomeTer a    t   onEnemyTer a    t   a    a    t   samePlace a    a    t   c    c    snap a    c    t   snap a    c    t   adjacent c    c        a  t    t       isFree a  t    H    a  t   isCaptured a  t   isFree a  t    H    a  t    isFree a  t   isCaptured a  t            a    capturing a    a  t     H    a  t    isCaptured a  t   isFree a  t            a    freeing a    a  t     H    a  t  c    isCaptured a  t   isCaptured a  t       snap a  c  t    snap a  c  t        H    Figure    Our hard formulas in Markov logic  See corresponding rules in Figure   for an English description and Table   for explanation of the predicates  In our implementation  the actual rules are written in the syntax used by theBeast  a Markov logic toolkit      denotes unique existential quantification   designates exclusive or        Learning Models of Failed Attempts In the work described above  we manually designed the structure of a Markov logic network that models the capture the flag domain and allows us to jointly denoise the raw GPS data and recognize       S ADILEK   K AUTZ  Soft formulas      a  c  t   snap a  c  t   d   a  c  t   wp   S        a c    c    t   snap a  c    t   snap a  c    t       d   a  c    c    t   ws   S        a c    c    c    t   snap a  c    t   snap a  c    t       snap a  c    t       d   a  c    c    c    t   wt a    a    t     enemies a    a     onHomeTer a    t    S    S    onEnemyTer a    t   isFree a    t  samePlace a    a    t    capturing a    a    t    wc a    a    t     enemies a    a     onEnemyTer a    t    S    onEnemyTer a    t   samePlace a    a    t   isFree a    t   isCaptured a    t    freeing a    a    t    wf     a  c  t   capturing a  c  t   wcb   S    a  c  t    freeing a  c  t    wf b   S    Figure    Soft formulas in Markov logic  See corresponding rules in Figure   for an English description  Each soft formula is written     as a traditional quantified finite first order logic formula  e g   a  c  t   snap a  c  t     followed by an optional function  e g   d   a  c  t    followed by the weight of the formula  e g   wp    This syntax denotes that at inference time  the instantiated logical part of each formula evaluates to either    true  or    false   which is then effectively multiplied by the product of corresponding function value and formula weight   instances of actual capturing and freeing  Now we show how to automaticallyin a supervised learning settingextend this theory to encompass and correctly label not only successful actions  but also failed attempts at those interactions  That is  given the raw GPS data that represent the CTF games  we want our new model to label instances where player a captures  or frees  player b as successful captures  successful frees  and instances where player a almost captures  or frees  player b as failed captures  failed frees   For example  by failed capturing we mean an instance of players interactions whereup to a pointit appeared that a is capturing b  but when we carefully consider the events that  potentially  preceded it as well as the impacts of the supposed capture on the future unfolding of the game  we conclude that it is a false alarm and no capture actually occurred  In other words  the conditions for a capture were right  but later on  there was a pivotal moment that foiled the capturing agents attempt  For both activities  capturing and freeing   our model jointly finds an optimal separation between success and failure  Note that since we cast our model in second order Markov logic  we do not learn  e g   an isolated rule that separates successful freeing from a failed attempt at freeing  Rathersince capturing and freeing events  both actual and failed  are related and thus labeling an activity as  say  successful capturing has far reaching impact on our past  present  and future       L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  labelingwe learn the separations in a joint and unified way  Namely  both the structure  logical form  and importance  weight  of each formula in our theory is considered with all its consequences and influence on other axioms in the theory  Our system thus finds an optimal balance between success and failure in capturing and freeing activities with respect to the training data        T HE T HEORY AUGMENTATION A LGORITHM In what follows  we will describe our Markov logic theory augmentation algorithm  Algorithm     For clarity  we will explain how it works in concrete context of the ML models of capture the flag we discussed in previous sections  However  the underlying assumption that successful actions are in many ways similar to their failed counterparts  and that minorbut crucialdeviations cause the failure to occur  often hold beyond capture the flag  Therefore  the same algorithm is applicable to other domains with different activities  as long as they are modeled in Markov logic  Algorithm     Extend a ML theory to model successful as well as failed activities  Input  A  set of activities MS   ML theory that models successful instances of activities in A S  set of examples of successful activities F   set of examples of failed activities Output  MS F   augmented ML model with learned weights that models both successful and attempted activities in A I  intended goals of the activities                       M S  liftToSecondOrderML MS   A  M S  instantiate M S   A  I  findIncompatibleFormulas F   M S   MS F  M S  I MS F  learnWeights S  F   MS F   MS F  removeZeroWeightedFormulas MS F   return MS F   I  At a high level  the augmentation algorithm belongs to the family of structure learning methods  Starting with a seed model of successful actions  it searches for new formulas that can be added to the seed theory in order to jointly model both successfully and unsuccessfully carried out actions  The declarative language biasessentially rules for exploring the hypothesis space of candidate structuresis defined implicitly by the notion that for any given activity  the structure of unsuccessful attempts is similar to the successful attempts  Therefore  the augmentation algoritm goes through an inflation stage  where formulas in the seed theory are generalized  followed by a refinement stage  where superfluous and incompatible formulas in the inflated model are pruned away  The refinement step also optimizes the weights within the newly induced theory  We will now discuss this process in more detail  The input of our theory augmentation algorithm consists of an initial first order ML theory MS that models successful capturing and freeing  such as the unified ML model defined in Section     that contains formulas shown in Figures   and     a set of activities of interest A  and a set of examples of successful  S  as well as failed  F   captures and frees  MS does not need to have weights for its soft formulas specified  In case they are missing  we will learn them from scratch in        S ADILEK   K AUTZ  the final steps of the augmentation algorithm  If the weights are specified  the final weight learning step for MS F can leverage them to estimate the initial weight values  A can be specified as a set of predicate names  e g    capturing  freeing   Each example in sets S and F describes a game segment and constitutes a truth assignment to the appropriate literals instantiated from MS   Table   shows two toy examples of sets S and F for three time steps  Since the goal is to learn a model of failed  and successful  attempts in a supervised way  the example game segment in F contain activities labeled with predicates failedCapturing   and failedFreeing    If MS contains hybrid formulas  such our formulas S S  in Figure     the appropriate function definitions are provided as part of S and F as well  Each definition consists of implicit mapping from input arguments to function values  For instance  function d  in formula S  quantifies the L  distance between the agent a and cell c at time t in the projected Mercator space  d   a  c  t    p  a gpsXt  c gpsX      a gpsYt  c gpsY      Our system goes through the following process in order to induce a new theory MS F that augments MS with a definition of failed attempts for each activity already defined in MS   First we lift MS to second order Markov logic by variabilizing all predicates that correspond to the activities of interest  step   of Algorithm     This yields a lifted theory M S   More concretely  in order to apply this technique in our domain  we introduce new predicate variables captureType  whose domain is  capturing  failedCapturing    freeType  over  freeing  failedFreeing    and stateType  over  isCaptured  isFailedCaptured  isFree  isFailedFree    For instance  variabilizing a first order ML formula freeing a  b  t   enemies a  b  yields a second order ML formula freeType a  b  t   enemies a  b   note that freeType is now a variable   Instantiating back to first order yields two formulas  freeing a  b  t   enemies a  b  and failedFreeing a  b  t   enemies a  b   As far as agents behavior is concerned  in the CTF domain  isCaptured is equivalent to isFailedFree  and isFree is equivalent to isFailedCaptured  As we will soon see  the theory augmentation process learns these equivalence classes and other relationships between states from training examples by expanding and subsequently refining formula H  in Figure    While we could work with only the isCaptured predicate and its negation to represent agents states  we feel that having explicit failure states makes our discussion clearer  Furthermore  future work will need to address hierarchies of activities  including their failures  In that context  a representation of explicit failure states may not only be convenient  but may be necessary  Next  we instantiate all predicate variables in M S to produce a new first order ML theory M S that contains the original theory MS in its entirety plus new formulas that correspond to failed captures and frees  step     Since events that are  e g   near captures appear similar to actual successful captures  our hypothesis is that we do not need to drastically modify the original successful formulas in order to model the failed activities as well  In practice  the above process of lifting and instantiating indeed results in a good seed theory  While we could emulate the lifting and grounding steps with a scheme of copying formulas and renaming predicates in the duplicates appropriately  we cast our approach in principled second order Markov logic  which ties our work more closely to previous research and results in a more extensible framework  Specifically  second order Markov logic has been successfully used in deep transfer learning  Davis   Domingos        and predicate invention  Kok   Domingos         Therefore  an interesting direction of future work is to combine our theory augmentation and refinement with transfer and inductive learningoperating in second order MLto jointly induce models of failed attempts of different activities in different domains  while starting with a single model of only successful activities in the source domain        L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  Set S  Successful Capture enemies P    P    enemies P    P    onEnemyTer P       onEnemyTer P       capturing P    P       isFree P       isFree P       isFree P       isFree P       isFree P       isCaptured P       snap P    C      snap P    C       snap P    C       snap P    C      snap P    C       snap P    C       samePlace P    P       samePlace P    P       samePlace P    P       samePlace P    P        Set F  Failed Capture enemies P    P    enemies P    P    onEnemyTer P       onEnemyTer P       onEnemyTer P       failedCapturing P    P       isFree P       isFailedCaptured P       isFree P       isFailedCaptured P       isFree P       isFailedCaptured P       isFree P       isFailedCaptured P       isFree P       isFailedCaptured P       isFree P       isFailedCaptured P       snap P    C       snap P    C       snap P    C      snap P    C      snap P    C       snap P    C      samePlace P    P       samePlace P    P        Table    Two examples of a logical representation of successful  S  as well as failed  F   capture events that are input to Algorithm    The closed world assumption is applied  therefore all atoms not listed are assumed to be false  For clarity  we omit listing the adjacent   predicate   Typical structure learning and inductive logic programming techniques start with an initial  perhaps empty  theory and iteratively grow and refine it in order to find a form that fits the training data well  In order to avoid searching the generally huge space of hypotheses  a declarative bias is either specified by hand or mined from the data  The declarative bias then restricts the set of possible refinements of the formulas that the search algorithm can apply  Common restrictions include limiting formula length  and adding a new predicate to a formula only when it shares at least one variable with some predicate already present in the formula  On the other hand  in our approach  we first generate our seed theory by instantiating all the activity related predicate variables  To put it into       S ADILEK   K AUTZ  context of structure learning  we expand the input model in order to generate a large seed theory  and then apply bottom up  data driven  learning to prune the seed theory  whereby the training data guides our search for formulas to remove as well as for an optimal set of weights on the remaining formulas  We conjecture that any failed attempt at an activity always violates at least one constraint that holds for successful executions of the activity  The experiments below support this conjecture  The pruning is done in steps   and   of Algorithm    The function findIncompatibleFormulas F   M S   returns a set of hard formulas in M S that are incompatible with the set of examples of failed interactions F   We say that a formula c is compatible with respect to a set of examples F if F logically entails c  F    c   Conversely  if F does not entail c  we say that c is incompatible w r t  F   We explain how to find incompatible formulas in the next section  In step   of Algorithm    we simply remove all incompatible formulas  I  from the theory  At this point  we have our MS F model  where hard formulas are guaranteed logically consistent with the examples of failed activities  because we removed the incompatible hard formulas   as well as with the successful activities  because they were logically consistent to start with   However  the soft formulas in MS F are missing properly updated weights  in Markov logic  the weight of each hard formula is simply set to     Therefore  we run Markov logic weight learning using theBeast package  step     Recall that theBeast implements the cutting plane meta solving scheme for inference in Markov logic  where the ground ML network is reduced to an integer linear program that is subsequently solved by the LpSolve ILP solver  We chose this approach as opposed to  e g   MaxWalkSAT that may find a solution that is merely locally optimal  since the resulting run times are still relatively short  under an hour even for training and testing even the most complex model   Weights are learned discriminatively  where we directly model the posterior conditional probability of the hidden predicates given the observed predicates  We set theBeast to optimize the weights of the soft formulas via supervised on line learning using margin infused relaxed algorithm  MIRA  for weight updates while the loss function is computed from the number of false positives and false negatives over the hidden atoms  Note that if any of the soft formulas are truly irrelevant with respect to the training examples  they are not picked out by the findIncompatibleFormulas   function  but their weights are set to zero  or very close to zero  in the weight learning step  line   in Algorithm     These zero weighted formulas are subsequently removed in the following step  Note that the weight learning process does not need to experience a cold start  as an initial setting of weights can be inherited from the input theory MS   Finally  we return the learned theory MS F   whose formulas are optimally weighted with respect to all training examples  In the Experiments and Results section below  we will use MS F to recognize both successful and failed activities  Algorithm   also returns the incompatible hard formulas I  We will see how I is used to extract the intended goal of the activities in the Section      but first  let us discuss step   of Algorithm   in more detail        C ONSISTENCY C HECK   F INDING I NCOMPATIBLE F ORMULAS Now we turn to our method for finding incompatible formulas  summarized in Algorithm     Since our method leverages satisfiability testing to determine consistency between candidate theories and possible worlds  examples    Algorithm   can be viewed as an instance of learning from interpretationsa learning setting in the inductive logic programming literature  De Raedt            This is often referred to as the covers relation in inductive logic programming         L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  Algorithm    findIncompatibleFormulas   Find formulas in a ML theory that are logically inconsistent with examples of execution of failed activities  Input  F   a set of examples of failed activities T   unrefined ML theory of successful and failed activities Output  smallest set of formulas that appear in T and are unsatisfiable in the worlds in F                                             O  extractObjects F   Thard  T   Tsoft integer n    boolean result  false while result    false do T c  Thard remove a new n tuple of formulas from T c if for the current n  all n tuples have been tested then nn   end if result  testSAT F   T c   O  end while return Thard   T c  As input  we take a set of examples of failed activities F and a seed theory T  e g   produced in step   of Algorithm     The output is the smallest set of hard formulas that appear in T and are logically inconsistent with F   The algorithm first extracts the set of all objects O that appear in F  step   in Algorithm     while keeping track of the type of each object  For example  suppose there are only two example worlds in F shown in Table    Then extractObjects F   returns  P    P    P    P    C    C           Example   snap P    C       snap P    C       failedCapturing P    P        Example   snap P    C       snap P    C       failedFreeing P    P        Table    Two simple examples of a logical representation a failed capture event   In step    we limit ourselves to only hard formulas when testing compatibility  We do so since we can prove incompatibility only for hard formulas  Soft constraints can be violated many times in the data and yet we may not want to eliminate them  Instead  we want to merely adjust their weights  which is exactly what we do in our approach  Therefore  Thard contains only hard formulas that appear in T   Next  on lines   through     we check if the entire unmodified Thard is compatible  since for n      we do not remove any formulas   If it is compatible  we return an empty set indicating that all the hard formulas in the original seed theory T are compatible with the examples  If we detect incompatibility  we will need to remove some  and perhaps even all  hard formulas in order to arrive at a logically consistent theory  Therefore  we incrementally start removing n tuples of formulas  That is  in the subsequent  Thard   iterations of the while loop  we determine if we can       S ADILEK   K AUTZ  restore consistency by removing any one of the hard formulas in Thard   If we can  we return the set Thard   fi   where fi is the identified and removed incompatible formula  If consistency cannot be restored by removing a single formula  we in turn begin considering pairs of formulas  n       triples  n       etc  until we find a pruned theory T c that is consistent with all examples  In general  we do need to consider n tuples of formulas  rather than testing each formula in isolation  This is due to disjunctive formulas in conjunction with an possibly incomplete truth assignment in the training data  Consider the following theory in propositional logic  f    a  b f    b  c Data  a  c  Following the closed world assumption  the negated atom c would actually not appear in the training data  but we explicitly include it in this example for clarity   While f  and f  are each individually consistent with the data  f   f  is inconsistent with the data  More complicated examples can be constructed  where every group of k formulas is inconsistent with the data  even though the individual formulas are  In a special case where the truth values of all atoms in the training examples are known  the formulas can be tested for consistency individually  which reduces the original exponential number of iterations Algorithm   executes  in the worst case  to a linear complexity  An interesting direction for future work is to explore applications of logical methods to lower the computational cost for the general case of partially observed data  We also note that some hard formulas model physical constraints or inviolable rules of capture the flag  and therefore hold universally  Appropriately  these formulas are not eliminated by Algorithm    As an example  consider formula H  in Figure    which asserts that each player occupies exactly one cell at any given time  This formula is satisfied in games that include both successful and failed activities  On the other hand  consider formula H  in the same figure  It contains a captured player to the cell he was captured in  following the captured players cannot move rule of CTF   While this holds for successful capturing events  it does not necessarily hold for failed attempts at capturing  Therefore  when rule H  is expanded via second order ML  only some of the derived formulas are going to be consistent with the observations  Specifically  the candidate formula in Equation   will be pruned away  as it is inconsistent with the training examples  i e   players that were only nearly captured continue to be free to move about  However  the remaining three variants of formula H  will not be pruned away  Equation   will always evaluate to true  since if someone attempts to re capture an already captured player a  a does indeed remain stationary  Similarly  Equation   is also consistent with all the example CTF games because if there is a failed attempt at capture immediately followed by a successful capture  the captured player does remain in place from time t onward  Finally  Equation   is compatible as well  since it is the original formula H  that is consistent with the observations    a  t  c   isFailedCaptured a  t   isFailedCaptured a  t       snap a  c  t   snap a  c  t            a  t  c   isCaptured a  t   isFailedCaptured a  t       snap a  c  t   snap a  c  t                 L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR    a  t  c   isFailedCaptured a  t   isCaptured a  t       snap a  c  t   snap a  c  t             a  t  c   isCaptured a  t   isCaptured a  t       snap a  c  t   snap a  c  t            The function testSAT    line    in Algorithm    checks whether a given candidate theory T c is compatible with the examples F by the following process  First  we ground T c using the objects in O  thereby creating a ground theory G  For example  if T c    p x   q x   and O    B  W    the grounding would be G    p B   q B   p W    q W     Then we check if G  Fhidden is satisfiable using the miniSAT solver  where Fhidden is simply the set of hidden atoms that appear in F   Intuitively  this corresponds to testing whether we can plug in the worlds in F into T c while satisfying all the hard constraints  Though satisfiability is an NP complete problem  in practice testSAT   completes within tenths of a second even for the largest problems in our CTF domain  For instance  suppose Fhidden    p B   q B    Then we test satisfiability of the formula         p B   q B   p W    q W    p B   q B   In this case we cannot satisfy it since we are forced to set p B  to true and q B  to false  which renders the first clauseand therefore the whole formulafalse  An alternative approach to pruning formulas via satisfiability testing  as we have just described  would be to treat both types of formulas  hard and soft  in the inflated theory M S as strictly soft formulas and learning a weight for each formula from examples of both successful and failed game events  However  this introduces several complications that negatively impact the systems performance as well as model clarity  First  the number of formulas in the inflated theory can be exponentially larger than in the seed theory  While the instantiation of the second order ML representation can be quantified to limit this expansion  we still have worst case exponential blow up  By treating all formulas as soft ones  we now need to potentially learn many more weights  This is especially problematic for activities that occur rarely  as we may not have enough training data to properly learn those weights  Eliminating the hard candidate formulas by proving them inconsistent dramatically reduces the number of parameters we have to model  While satisfiability testing is NP complete  weight learning in Markov logic entails running inference multiple times  which is itself a  P complete problem  The second reason for distinguishing between soft and hard formulas is the resulting clarity and elegance of the final learned model MS F   Even in situations when we have enough training data to properly learn a large number of weights  we run into overfitting problems  where neither the structure nor the parameters of the model represent the domain in a natural way  Our experiments have shown that if we skip the pruning stage  steps   and   in Algorithm     the models recognition performance does not differ from that of a pruned model in a significant way  p value of        However  we end up with a large number of soft formulas with a mixture of positive and negative weights that the learning algorithm carefully tuned and balanced to fit the training data  They however bear little relationship to the concepts in the underlying domain  Not only does this make it very hard for a human expert to analyze the model  but it makes it even harder to modify the model        S ADILEK   K AUTZ  For these reasons  softening all hard formulas is  in general  infeasible  An interesting direction of future work will be to identify a small amount of key inconsistent hard formulas to soften  while eliminating the rest of the inconsistent hard formulas  This however entails searching in a large space of candidate subsets of softened formulas  where each iteration requires expensive re learning of all weights  Note that Algorithm   terminates as soon as it finds a compatible theory that requires the smallest number of formula removals  We also experimented with an active learning component to our system  where we modify Algorithms   and   such that they present several possible refinements of the theory to the user who then selects the one that looks best  The proposed modifications are shown both at the ML theory level with modified sections  formulas  highlighted as well as at the data level where the program shows the inferred consequences of those modifications  For each candidate modification  the corresponding consequences are displayed as a collection of animations where each animation shows what the results of activity recognition would be if we committed to that particular candidate theory  Note that even people who do not have background in ML can interact with such a system since the visualization is easy to understand  Interestingly  in the case of captures and frees  the least modified theory that the off line version of the algorithm finds is also the best one and therefore there is no need to query the user  One can view this as a differential variant of Occams razor  However  for different activities or other domains  the active learning approach may be worth revisiting and we leave its exploration for future work  Finally  general structure learning techniques from statistical relational AI and from inductive logic programming are not applicable as a substitute for our theory augmentation algorithm for several reasons  The main reason is that  for efficiency reasons  existing techniques in the literature typically operate over a very restricted set of formula templates  That is  they consider only Horn clauses  or only formulas without an existential quantifier  or only formulas with at most k literals or with at most l variables  and so on  This set of restrictions is part of the language bias of any given approach  While in principle  structure learning is possible without a language bias  one often has to carefully define one for the sake of tractability  see the Section   for details   In our approach  the language bias is defined implicitly as discussed in Section            Extracting The Goal From Success and Failure Recall that applying the theory augmentation process  Algorithm    on the CTF seed theory of successful interactions  shown in Figures   and    induces a new set of formulas that capture the structure of failed activities and ties them together with the existing formulas in the seed theory  The logically inconsistent formulas I that Algorithm   returns are ones that are not satisfiable in the worlds with failed activities  At the same time  variants of those formulas were consistent with the examples of successful actions occurring in the games  Therefore  I represents the difference between a theory that models only successful activities and the augmented theory of both successful and failed actions  that has been derived from it  Intuitively  the difference between success and failure can be viewed as the intended purpose of any given activity a rational agent executes  and consequently as the goal the agent has in mind when he engages in that particular activity  In the next section  we will explore the goals extracted from the CTF domain in this fashion  This concludes discussion of our models and methodology  and now we turn to experimental evaluation of the framework presented above         L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR     Experiments and Results We evaluate our approach along the three major directions outlined in Section    Methodology   while focusing on answering the four research questions formulated ibidem  The structure of this section closely follows that of the Methodology section  In a nutshell  we are first interested in how our Markov logic models perform on the standard multi agent activity recognition tasklabeling successful activitiesand how their performance compares to the alternative models  Second  we examine the augmented model that captures both successful and failed attempts at activities  This is the model MS F induced by Algorithm    which also lets us extract the intended goal of the activities in question  Third  we compare the performance of MS F on the task of jointly recognizing all four activities with that of an alternative model  Finally  we investigate to what extent the reasoning about failed attempts does help in recognition of successfully executed activities  All experiments are performed on our capture the flag dataset consisting of four separate games  The dataset is summarized in Table    where for each game we list the number of raw GPS readings and the number of instances of each activity of interest  We evaluate the models via four fold crossvalidation  always training on three games  if training is required for a model  and testing against the fourth  For each experimental condition below  we report precision  recall  and F  scores attained by each respective model over the four cross validation runs  We have purposefully chosen to split the data so that each cross validation fold directly corresponds to a separate game of CTF for conceptual convenience and clarity  As we discussed above  the events occurring in the games often have far reaching consequences  For example  most captured players are never freed by their allies  Therefore  a capture at the beginning of a game typically profoundly influences the entire rest of the game  For this reason  splitting the games randomly or even manually would introduce unnecessary complications  as most of the segments would have dependencies on other segments  By enforcing that each fold exactly corresponds with a different game  we make each fold self contained  To quantify the statistical significance of the pair wise differences between models  we use a generalized probabilistic interpretation of F  score  Goutte   Gaussier         Namely  we express F  scores in terms of gamma variates derived from models true positives  false positives  and false negatives          h        cf   Goutte   Gaussier         This approach makes it possible to compare our results to future work that may apply alternative models on similar  but not identical  datasets  A future comparison may  for instance  include additional games or introduce random splits of the data  We note that standard statistical significance tests cannot be applied in those situations  All p values reported are one sided  as we are interested if models performance significantly improves as their level of sophistication increases      Recognition of Successful Activities Recall that for both our two step   SML  and unified  UML  Markov logic models  we specify the Markov logic formulas by hand and optimize the weights of the soft formulas via supervised online learning  We run a modified version of theBeast software package to perform weight learning and MAP inference  theBeast implements the cutting plane meta solving scheme for inference in Markov logic  where the ground ML network is reduced to an integer linear program that is subsequently solved by the LpSolve ILP solver  We chose this approach as opposed to  e g   MaxWalkSAT that can get stuck at a local optimum  since the resulting run times are still relatively short  under an hour even for training and testing even the most complex model         S ADILEK   K AUTZ  Game   Game   Game   Game   Total   GPS                                     AC              FC                 AF             FF            Table    CTF dataset overview   GPS is the total number of raw GPS readings   AC and  FC is the number actual  successful  and failed captures respectively  and analogously for freeings   AF and  FF    At weight learning time  we use the margin infused relaxed algorithm  MIRA  for weight updates while the loss function is computed from the number of false positives and false negatives over the hidden atoms  as described in the Methodology section  The discretization step for the dynamic Bayesian network model  DBN  is implemented in Markov logic and is also executed in this fashion  The DBN model is trained via maximum likelihood as described in Section      The two deterministic baselines  B and B S  do not require any training phase  At inference time  we are interested in the most likely explanation of the data  In Markov logic  maximum a posteriori inference reduces to finding a complete truth assignment that satisfies all the hard constraints while maximizing the sum of the weights of the satisfied soft formulas  At testing time  theBeast Markov logic solver finds the most likely truth assignment to the hidden atoms as described above  and in this section we are specifically interested in the values of the capturing and freeing atoms  In DBNs  the most likely explanation of the observations is equivalent to Viterbi decoding  The DBN model assigns either free or captured state to each player for every time step  We then label all transitions from free to captured state as capturing and all transitions from captured to free as freeing  Note that the DBN model is capable of determining which player is being freed or captured  but it does not model which player does the freeing or capturing  In our evaluation  we give it the benefit of the doubt and assume it always outputs the correct actor  For all models  inference is done simultaneously over an entire game  on average  about    minutes worth of data   Note that we do not restrict inference to a  small  sliding time window  As the experiments described below show  many events in this domain can only be definitely recognized long after they occur  For example  GPS noise may make it impossible to determine whether a player has been captured at the moment of encounter with an enemy  but as the player thereafter remains in place for a long time  the possibility of his capture becomes certain  Figures   and   summarize the performance of our models of successful capturing and freeing in terms of precision  recall  and F  score calculated over the four cross validation runs  For clarity  we present the results in two separate plots  but each model was jointly labeling both capturing and freeing activities  We do not consider the baseline model for freeing recognition as that activity makes little sense without having a notion of player state  captured or free   We see that the unified approach yields the best results for both activities  Let us focus on capturing first  Figure     Overall  the unified model labels    out of    captures correctlythere        L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  Capturing    Recogni on                                                                                                                          Precision                                  Recall                                                              B      B S      F       DBN       SML      UML      Figure    Comparison of performance of the five models on capturing recognition while doing joint inference over both capturing and freeing events  See Table   for statistical significance analysis of the pairwise differences between models   B   baseline model  B S   baseline model with states   SML   two step Markov logic model  UML   unified Markov logic model   are only two false negatives  In fact  these two capture events are missed by all the models because they involve two enemies that appear unusually far apart  about    meters  in the raw data  Even the unified approach fails on this instance since the cost of adjusting the players trajectoriesthereby losing score due to violation of the geometry based constraintsis not compensated for by the potential gain from labeling an additional capture  Note that even the two step approach recognizes    out of    captures  As compared to the unified model  it misses one additional instance in which the involved players  being moderately far apart  are snapped to mutually nonadjacent cells  On the other hand  the unified model does not fail in this situation because it is not limited by prior nonrelational snapping to a few nearby cells  However  the difference between their performance on our dataset is not statistically significant even at the      level  p value of        Both deterministic baseline models  B and B S  perform very poorly  Although they yield a respectable recall  they produce an overwhelming amount of false positives  This shows that even relatively comprehensive pattern matching does not work at all in this domain  Interestingly  the performance of the DBN model leaves much to be desired as well  especially in terms of precision  While the DBN model is significantly better than both baselines  p value less than             it also achieves significantly worse performance than both the Markov logic models  p value less than         see Table     Table   summarizes p values of pairwise differences between models of actual  i e   successful  capturing  While the difference between the Markov logic based models   SML and UML  are not        S ADILEK   K AUTZ  Freeing    Recogni on                                                                                                                                                      Precision                Recall                F                B S      DBN        SML      UML      Figure    Comparison of performance of our three models on freeing recognition while doing joint inference over both capturing and freeing events  See Table   for statistical significance analysis of the pairwise differences between models   B S   baseline model with states   SML   two step Markov logic model  UML   unified Markov logic model   B B S DBN  SML  B S           DBN                       SML                             UML                                    Table    Summary of statistical significance  one sided p values  of the pairwise differences between F  scores for models of actual capturing   B   baseline model  B S   baseline model with states  DBN   dynamic Bayesian network model   SML   two step Markov logic model  UML   unified Markov logic model   statistically significant  p value of        pairwise differences in F  scores between all other models are significant at the      level  and most often even at much lower p values  Though the unified model still outperforms its alternatives in the case of freeing recognition as well  its performance is further from ideal as compared to the capture recognition case  Figure     It correctly identifies only   out of   freeing events in the games  but does not produce any false positives  This is partly due to the dependency of freeing on capturing  A failure of a model to recognize a capture precludes its recognition of a future freeing  Another reason is the extreme sparseness of the freeing events  there are only five of them in         datapoints   Finally  in some        L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  B S DBN  SML  DBN            SML                  UML                       Table    Summary of statistical significance  one sided p values  of the pairwise differences between F  scores for models of actual freeing   B S   baseline model with states  DBN   dynamic Bayesian network model   SML   two step Markov logic model  UML   unified Markov logic model   instances players barely move after they had been freed  This may occur for a number of reasons ranging from already occupying a strategic spot to simply being tired  Such freeing instances are very challenging for any automated system  and even people familiar with the game to recognize  several situations would have been extremely hard to disambiguate if we didnt have access to our notes about data collection   The two step ML model does a slightly worse job than the unified model on freeing recognition  It correctly identifies only   out of   freeings for the same reasons as in the capturing recognition case  Similarly to models of actual captures  the difference between the unified and two step freeing models is not statistically significant  p value of        Table   summarizes p values of pairwise differences between models of actual  i e   successful  freeing  Here we see that only the difference between B S and UML models is statistically significant  p value of        whereas the differences between the rest of the model pairs are not statistically significant  Since there are only five instances of successful freeing  the  SML model does not perform significantly better than the B S model at the      significance level  p value of        However  the UML model achieves better recognition results than even the DBN model with high confidence  p value less than        Therefore  we see that although the  SML model strictly dominates the non Markov logic models when evaluated on capturing recognition  we need the full power of the unified ML model to strictly outperform the nonrelational alternatives for freeing  This suggests that as we move to more complex and more interdependent activities  relational and unified modeling approaches will be winning by larger and larger margins  Even though the statistical significance tests suggest that  SML is likely to give similar results to UML  it is important to note that  SML  by design  precludes recognition of the activities in question in certain situations  Namely  as our experiments demonstrate  when the players are snapped to cells that are too far apart  the two step model does not even consider those instances as candidates for labeling  and inevitably fails at recognizing them  Therefore  one needs to look beyond the p values obtained when comparing the fully unified models to various alternatives  As expected from the experiments with capturing recognition  both deterministic baseline models perform very poorly on freeing recognition as well  Not only do they produce an overwhelming amount of false positives  they also fail to recognize most of the freeing events  Thus  we see that the models cast in Markov logic perform significantly better than both of the deterministic baseline models  and also better than the probabilistic  but nonrelational  DBN model  We note that the DBN model has the potential to be quite powerful and similar DBNs have been applied with great success in previous work on activity recognition from location data  Eagle         S ADILEK   K AUTZ  Pentland        Liao  Patterson  Fox    Kautz         It also has many similarities with the twostep ML model  They both share the same denoising and discretization step  and they both operate on the same observed data  The key difference is that the DBN model considers players individually  whereas the two step ML model performs joint reasoning  Looking at the actual CTF game data  we see several concrete examples of how this hurts DBNs labeling accuracy  For instance  consider a situation where two allies had been captured near each other  Performing inference about individual players in isolation allows the DBN model to infer that the two players effectively free each other  even though in reality they are both captured and cannot do so  This occurs because the DBN model is oblivious to the explicit states of ones teammates as well as opponents  Since capturing and freeing are interdependent  the obliviousness of the DBN model to the state of the actors negatively impacts its recognition performance for both activities  The example we just gave illustrates one type of freeing false positives  The hallucinated freeings create opportunities that often lead to false positives of captures  creating a vicious cycle  False negatives of freeing  capturing  events often occur for players who the model incorrectly believes have already been freed  captured  at a prior time  Since the Markov logic based models are significantly betterwith a high level of confidence than the alternatives that are not fully relational  the experiments above validate our hypothesis that we need to exploit the rich relational and temporal structure of the domain in a probabilistic way and at the same time affirmatively answer research question Q   Can we reliably recognize complex multi agent activities in the CTF dataset even in the presence of severe noise    Namely  we show that although relatively powerful probabilistic models are not sufficient to achieve high labeling accuracy  we can gain significant improvements by formulating the recognition problem as learning and inference in Markov logic networks  Now we turn to the evaluation of our method of learning models of both success and failure in peoples activities      Learned Formulas and Intentions Applying the theory augmentation process  Algorithm    on the CTF seed theory  shown in Figures   and    induces a new set of formulas that capture the structure of failed activities and ties them together with the existing formulas in the theory  We call this model MS F   Figure   shows examples of new weighted formulas modeling failed freeing and capturing attempts that appear in MS F   First  note that our system correctly carries over the basic preconditions of each activity  contrast formulas S  with S   and S  with S   in Figures   and   respectively   This allows it to reliably recognize both successful and failed actions instead of  e g   merely labeling all events that at some point in time appear to resemble a capture as near capture  This re use of preconditions directly follows from the language bias of the theory augmentation algorithm  Turning our attention to the learned hard formulas  we observe that the system correctly induced equivalence classes of the states  and also derived their mutual exclusion relationships  H      It furthermore tied the new failure states to their corresponding instantaneous interactions  H   and H      Finally  the algorithm correctly discovers that the rule If a player is captured then he or she must remain in the same location  H   Figure    is the key distinction between a successful and failed capture  since players who were not actually captured can still move   Therefore  it introduces        L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  an appropriate rule for the failed captures  H     Figure    explicitly stating that failed capturing does not confine the near captured player to remain in stationary  An analogous process yields a fitting separation between failed and successful freeings  Namely  our model learns that an unsuccessfully freed player remains stationary  This learned difference between success and failure in players actions directly corresponds to the goal of the activity and consequently the intent of rational actors  This difference is what our system outputs as the intended goal of capturing activity  and analogously for freeing   These experimental results provide an evidence for a resounding yes to both Q   Can models of attempted activities be automatically learned by leveraging existing models of successfully performed actions   and Q   Does modeling both success and failure allow us to infer the respective goals of the activities   within the CTF domain  We note that instead of applying our automated theory augmentation method  a person could  in principle  manually formulate a Markov logic theory of successful as well as failed activities by observing the games  After all  this is how we designed the initial seed model of successful events  However  this process is extremely time consuming  as one tends to omit encoding facts that to us  humans  seem self evident but need to be explicitly articulated for the machine  e g   a single person cannot be at ten different places at once  or that a player is either free or captured but not both   It is also surprisingly easy to introduce errors in the theory  that are difficult to debug  mostly because of the complex weight learning techniques involved  Therefore  we believe that the theory augmentation method is a significant step forward in enhancing models capabilities while requiring small amounts of human effort  As the complexity of domains and their models increases  this advantage will gain larger and larger importance      Recognition of Both Successful and Failed Activities We now compare the performance of our model MS F to an alternative  baseline  method that labels all four activities in the following way  Similarly to the baseline with states model for successful interactions defined in Section      there are two separate stages  First we snap each GPS reading to the nearest cell by applying only the geometric constraints  H  and S S   of our theory  and afterward we label the instances of our activities  The following labeling rule is applied  We loop over the whole discretized  via snapping  data set and look for instances where a pair of players a and b were snapped  in the first step  to either the same cell or to two adjacent cells at time t  they are enemies  b is not captured already  and a is on its home territory while b is not  If b moves  is snapped to a different cell at a later time  without having an ally nearby  we output failedCapturing a b t   otherwise we output capturing a b t   The labeling rule for freeing is defined analogously and all four events are tied together  We also tested a variant of the DBN model introduced in Section     that has two additional hidden state values for node St   isFailedFree and isFailedCaptured  However  the difference in the results obtained with this model was not statistically significant  p value of        and therefore we focus on the conceptually more straightforward baseline model described above  Model MS F is evaluated using four fold cross validation  always training on three games and testing against the fourth   Figure    compares both models in terms of precision  recall  and F  score  Note that all four activities are modeled jointly in both models  The F  score of the augmented model is significantly better than that of the baseline for all four target activities  p value less than                    S ADILEK   K AUTZ  a    a    t     enemies a    a     onHomeTer a    t    S      onEnemyTer a    t   samePlace a    a    t   isFree a    t   isFree a    t    failedCapturing a    a    t           a    a    t     enemies a    a     onEnemyTer a    t    S      onEnemyTer a    t   samePlace a    a    t   isFree a    t   isCaptured a    t    failedFreeing a    a    t          a    a    t    failedCapturing a    a    t               S      a    a    t    failedFreeing a    a    t              S      a  t   isFailedCaptured a  t   isFree a  t    H      a  t   isCaptured a  t   isFailedFree a  t  a  t   isFailedCaptured a  t   isFree a  t  a  t   isCaptured a  t   isFailedFree a  t  a  t    isFree a  t   isFailedCaptured a  t            a    failedCapturing a    a  t   a  t    isCaptured a  t   isFailedFree a  t            a    failedFreeing a    a  t     H      H      a  t  c    isFailedCaptured a  t   isFailedCaptured a  t       snap a  c  t    snap a  c  t       H      Figure    Example formulas  learned by Algorithm    that model unsuccessful capturing and freeing events  The crucial intent recognition formula  H     is highlighted in bold  Formulas eliminated by Algorithm   are preceded by the  symbol  and are not included in the induced model MS F   The identity isCaptured a  t    isFree a  t  is applied throughout refining to show the formulas in a more intuitive fashion  For concreteness sake  the values of the learned weights here come from one cross validation run  and are similar in other runs    We see that the baseline model has  in general  a respectable recall but it produces a large number of false positives for all activities  The false positives stem from the fact that the algorithm is greedy in that it typically labels a situation where several players appear close to each other for certain period of time as a sequence of many captures and subsequent frees even though none of them actually occurred  Model MS F gives significantly better results because it takes full advantage of the structure of the game in a probabilistic fashion  It has a similar over labeling tendency only in the case of failed captures  where a single capture attempt is often labeled as several consecutive attempts  While this hurts the precision score  it is not a significant deficiency         L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR            Baseline      AC                        FC                                           AF                                          FF            Augmented    ML    Model                          F                           Recall                                 AC                       FC                        AF                       FF                                                                                                      Precision                                                                     Figure     Performance of the baseline and augmented  MS F   models on joint recognition of successful and failed capturing and freeing  The F  score of the augmented model is significantly better than that of the baseline for all four target activities  p value less than             AC   actual  successful  capturing  FC   failed capturing  AF   actual freeing  FF   failed freeing   as in practice  having a small number of short game segments labeled as possible near captures is useful as well  We also note that even though the original model  UML  did not contain any information on failed capturing nor failed freeing  the performance of MS F is respectable even for those two newly introduced activities  We only provided examples of game situations where those attempts occur and the system augmented itself and subsequently labeled all four activities  Thus  we see that we can indeed extend preexisting models in an automated fashion so that the unified model is capable of recognizing not only individual activities  but also both success and failure in peoples behavior      The Effect of Modeling Failed Attempts on Recognition of Successful Activities To address research question Q   Does modeling failed attempts of activities improve the performance on recognizing the activities themselves    we want to see how much does the recognition of attempted activities help in modeling the successful actions  the latter being the standard activity        S ADILEK   K AUTZ  Capturing      F                 Recall                Precision                           F      Freeing                           Recall                                      Precision                                                           Without    Modeling    Failure                                                          With    Modeling    Failure      Figure     Considering unsuccessfully attempted activities strictly improves performance on standard activity recognition  Blue bars show scores obtained with the unified Markov logic model that considers only successful activities  MS    The red bars indicate the additive improvement provided by the augmented model that considers both successful and failed activities  MS F   the output of Algorithm     Each model labels its target activities jointly  we separate capturing and freeing in the plot for clarity  Precision has value of   for both models  F  scores obtained when explicitly modeling failed attempts are not statistically different from F  scores obtained without modeling attempts at a high confidence level  p value of        However  these results still show the importance of reasoning about peoples attempts when recognizing their activities  see text for details   recognition problem   Toward that end  we compare the Markov logic model MS that jointly labels only successful capturing and freeing with model MS F that jointly labels both successful and failed attempts at both capturing and freeing  see Section       for a detailed description of the two models   However  we evaluate them in terms of precision  recall  and F  score only on successful interactions  not all four types of activities  Figure    summarizes the results  We see that when evaluated on actual capturing  MS F performs better than MS   and similarly for freeing  However  the difference in F  scores between a model that captures both attempted and successful activities  MS F   and a model of only successful activities  MS   is not statistically significant  p value of        This is partly because MS already produces very solid results  leaving little room for improvement  Additionally  the CTF dataset contains relatively few events of interest  In terms of labeling performance at testing time  the difference between the two models is more than      MS and MS F recognize  respectively        L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR     and    out of    successful activities correctly   Thus  we believe the trends shown in Figure    are promising and modeling attempted actions does improve recognition performance on both capturing and freeing  but evaluation on a dataset with a larger number of events is needed to show the difference to be statistically significant at a higher confidence level  However  this does not mean that recognizing attempts is unimportant  As we show above  our induced augmented model does recognize failed  as well as successful  activities in the complex CTF domain with high accuracy  and we argue this to be a significant contribution  Finally  the comparison of MS and MS F shows that applying our learning algorithm that augments a model with more recognition capabilities does not hurt model labeling performance  The fact that binary classification problems are typically easier to solve than their multi class counterparts has been well reported on in machine learning literature  Allwein  Schapire    Singer         Therefore  introducing new activities into a model  especially in an automated way  is likely to degrade its performance  Contrary to this intuition  our experiments show that MS F is no worse than MS on successful activity recognition  i e   their intersection  with high confidence  even though MS F is clearly richer and more useful      Related Work In the world of single agent location based reasoning  the work of Bui        presents and evaluates a system for probabilistic plan recognition cast as an abstract hidden Markov memory model  Subsequently  the work of Liao et al         implements a system for denoising raw GPS traces and simultaneously inferring individuals mode of transportation  car  bus  etc   and their goal destination  They cast the problem as learning and inference in a dynamic Bayesian network and achieve encouraging results  In a follow up work  Liao et al         introduce a framework for locationbased activity recognition  which is implemented as efficient learning and inference in a relational Markov network  The work of Ashbrook and Starner        focuses on inferring significant locations from raw GPS logs via clustering  The transition probabilities between important places are subsequently used for a number of user modeling tasks  including location prediction  The work of Eagle and Pentland        explores harnessing data collected on regular smart phones for modeling human behavior  Specifically  they infer individuals general location from nearby cell towers and Bluetooth devices at various times of day  Applying a hidden Markov model  HMM   they show that predicting if a person is at home  at work  or someplace else can be achieved with more than     accuracy  Similarly  the work of Eagle and Pentland        extracts significant patterns and signatures in peoples movement by applying eigenanalysis to smart phone logs  The work of Hu  Pan  Zheng  Liu  and Yang        concentrates on recognition of interleaving and overlapping activities  They show that publicly available academic datasets contain a significant number of instances of such activities  and formulate a conditional random field  CRF  model that is capable of detecting them with high  more than      accuracy  However  they focus solely on single agent household activities  Peoples conversation has been the primary focus of multi agent modeling effort  Barbuceanu   Fox         In the fields of multi agent activity recognition and studies of human behavior  researchers have either modeled conversation explicitly  e g   Busetta  Serafini  Singh    Zini         or have leveraged peoples communication implicitly via call and location logs from mobile phones  This data has been successfully used to infer social networks  user mobility patterns  model socially        S ADILEK   K AUTZ  significant locations and their dynamics  and others  Eagle   Pentland        Eagle  Pentland    Lazer         This is arguably an excellent stepping stone for full fledged multi agent activity recognition since location is  at times  practically synonymous with ones activity  e g   being at a store often implies shopping   Tang  Lin  Hong  Siewiorek    Sadeh         and our social networks have tremendous influence on our behavior  Pentland         Additionally  a number of researchers in machine vision have worked on the problem of recognizing events in videos of sporting events  such as impressive recent work on learning models of baseball plays  Gupta et al          Most work in that area has focused on recognizing individual actions  e g   catching and throwing   and the state of the art is just beginning to consider relational actions  e g   the ball is thrown from player A to player B   The computational challenges of dealing with video data make it necessary to limit the time windows of a few seconds  By contrast  we demonstrate in this work that many events in the capture the flag data can only be disambiguated by considering arbitrarily long temporal sequences  In general  however  both our work and that in machine vision rely upon similar probabilistic models  and there is already some evidence that statistical relational techniques similar to Markov logic can be used for activity recognition from video  Biswas  Thrun    Fujimura        Tran   Davis         Looking beyond activity recognition  recent work on relational spacial reasoning includes an attempt to locateusing spacial abductioncaches of weapons in Iraq based on information about attacks in that area  Shakarian  Subrahmanian    Spaino         Additionally  the work of Abowd et al         presents a location  and context aware system  Cyberguide  that helps people explore and fully experience foreign locations  Other researchers explore an intelligent and nonintrusive navigation system that takes advantage of predictions of traffic conditions along with a model of users knowledge and competence  Horvitz et al          Finally  the work of Kamar and Horvitz        explore automatic generation of synergistic plans regarding sharing vehicles across multiple commuters  An interesting line of work in cognitive science focuses on intent and goal recognition in a probabilistic framework  Baker  Tenenbaum    Saxe               Specifically  they cast goal inference as inverse planning problem in Markov decision processes  where Bayesian inversion is used to estimate the posterior distribution over possible goals  Recent extensions of this work begin to consider simulated multi agent domains  Baker  Goodman    Tenenbaum        Ullman  Baker  Macindoe  Evans  Goodman    Tenenbaum        Baker  Saxe    Tenenbaum         Comparison of the computational models against human judgement in synthetic domains shows a strong correlation between peoples predicted and actual behavior  However  the computational challenges involved in dealing with the underlying partially observable Markov decision processes are prohibitive in more complex domains with large state spaces  such as ours  The focus of our work is on a different aspect of reasoning about peoples goals  Rather than inferring a distribution over possible  a priori known goals  we automatically induce the goals of complex multi agent activities themselves  Other researchers have concentrated on modeling behavior of people and general agents as reinforcement learning problems in both single agent and multi agent settings  The work of Ma        proposes a system for household activity recognition cast as a single agent Markov decision process problem that is subsequently solved using a probabilistic model checker  Wilson and colleagues address the problem of learning agents roles in a multi agent domain derived from a real time strategy computer game  Wilson  Fern  Ray    Tadepalli        Wilson  Fern    Tadepalli         Experiments in this synthetic domain show strongly encouraging results  While we do not perform role       L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  learning ourselves  we anticipate that the work of Wilson et al  is going to play an important role in learning hierarchies of peoples activities  In our capture the flag domain  one can imagine automatically identifying a particular player as  for example  a defender and subsequently leveraging this information to model his or her behavior in a more personalized way  The work of Hong        concentrates on recognizing the goal of an agent in the course of her activities in a deterministic  but relational setting  Interesting work on goal recognition has been also applied to computer aided monitoring of complex multi agent systems  where relationships between agents are leveraged to compensate for noise and sparse data  Kaminka  Tambe  Pynadath    Tambe         By contrast  in our work we focus on learning the respective goals of a given set of multi agent activities in a probabilistic setting  The knowledge is in turn leveraged to achieve a stronger robustness of the other recognition tasks  Similarly to the approach of Hong  our system does not need a supplied plan library either  Our work also touches on anomaly detection since our system reasons about the failed attempts of the players  Anomaly detection concerns itself with revealing segments of the data that in some way violate our expectations  For an excellent survey of the subject  we refer the reader to the results of Chandola  Banerjee  and Kumar         In the realm of anomaly detection within peoples activities  the work of Moore and Essa        addresses the problem of error detection and recovery card games that involve two players recorded on video  Their system models the domain with a stochastic context free grammar and achieves excellent results  We note that recognizing a failed attempt at an activity is more fine grained a problem than anomaly detection  The failed event is not just anomalous in general   Rather  it is the specific distinction between success and failure in human activities that we are interested in  And the distinction lies in the fact that an unsuccessful attempt does not yield a certain desired state whereas a successful action does  This desired state is exactly what our approach extracts for each activity in question  To our knowledge  there exists no prior work on explicit modeling and recognition of attempted activities or on learning the intended purpose of an activity in a multi agent setting  One of the components of our contribution focuses on joint learning and inference across multiple tasks  capturing  freeing  and their respective attempted counterparts   This is in contrast with the traditional pipeline learning architecture  where a system is decomposed into a series of modules and each module performs partial computation and passes the result on to the next stage  The main benefits of this set up are reduced computational complexity and often higher modularity  However  since each stage is myopic  it may not take full advantage of dependencies and broader patterns within the data  Additionally  even though errors introduced by each module may be small  they can accumulate beyond tolerable levels as data passes through the pipeline  An extensive body of work has shown that joint reasoning improves model performance in a number of natural language processing and data mining tasks including information extraction  i e   text segmentation coupled with entity resolution   Poon   Domingos         co reference resolution  Poon   Domingos         information extraction coupled with co reference resolution  Wellner  McCallum  Peng    Hay         temporal relation identification  Yoshikawa  Riedel  Asahara    Matsumoto        Ling   Weld         and record de duplication  Domingos        Culotta   McCallum         Similarly to our work  some of the above models are cast in Markov logic  However  prior work uses sampling techniques to perform learning and inference  whereas we apply    A situation where a player in CTF moves through the campus at a speed of     mph and on her way passes an enemy player is certainly anomalous  and probably caused by GPS sensor noise   but we do not want to say that it is a failed attempt at capturing         S ADILEK   K AUTZ  a reduction to integer linear programming  Interestingly  the work in Denis and Baldridge        jointly addresses the problems of anaphoricity and co reference via a manual formulation of an integer linear program  Joint activity modeling has also been shown to yield better recognition accuracy  as compared to pipeline baselines as well as baselines that make strong inter activity independence assumptions  The work of Wu  Lian  and Hsu        performs joint learning and inference over concurrent singleagent activities using a factorial conditional random field model  Similarly  the work of Helaoui  Niepert  and Stuckenschmidt        models interleaved activities in Markov logic  They distinguish between foreground and background activities and infer a time window in which each activity takes place from RFID sensory data  By contrast  we focus on joint reasoning about multi agent activities and attempts in a fully relationaland arguably significantly more noisysetting  The work of Manfredotti  Hamilton  and Zilles        propose a hierarchical activity recognition system formulated as learning and inference in relational dynamic Bayesian networks  Their model jointly leverages observed interactions with individual objects in the domain and the relationships between objects  Since their method outperforms a hidden Markov model by a significant margin  it contributes additional experimental evidence that a relational decomposition of a domain improves model quality  The work of Landwehr  Gutmann  Thon  Philipose  and De Raedt        casts single agent activity recognition as a relational transformation learning problem  building on transformationbased tagging from natural language processing  Their system induces a set of transformation rules that are then used to infer activities from sensory data  Since the transformation rules are applied adaptively  at each step  the system leverages not only observed data  but also currently assigned labels  inferred activities   However  the transformation rules are learned in a greedy fashion and experiments show that the model does not perform significantly better than a simple HMM  On the other hand  their representation is quite general  intuitive  and extensible  As we will see  our Markov logic model has a similar level of representational convenience while performing global instead of greedyoptimization in a significantly more complex domain  The denoising component of our model can be formulated as a tracking problem  Prior work proposed a relational dynamic Bayesian network model for multi agent tracking  Manfredotti   Messina         Their evaluation shows that considering relationships between tracked entities significantly improves model performance  as compared to a nonrelational particle filter baseline  By contrast  our work explores joint tracking and activity recognition  However  each GPS reading is annotated with the identity of the corresponding agent  The work of Manfredotti and Messina suggests that our model can be generalized  such that the associations between GPS and agent identities are inferred and need not be observed  Our Markov logic theory can be viewed as a template for a conditional random field  Lafferty         an undirected graphical model that captures the conditional probability of hidden labels given observations  rather than the joint probability of both labels and observations  as one would typically do in a directed graphical model  In the relational world  directed formalisms include relational Bayesian networks  Jaeger        and their dynamic counterparts  Manfredotti         probabilistic relational models  Koller        Friedman  Getoor  Koller    Pfeffer         Bayesian logic programs  Kersting   De Raedt         and first order conditional influence language  Natarajan  Tadepalli  Altendorf  Dietterich  Fern    Restificar         Conditional random fields have been extensively applied to activity recognition  and their superior labeling performance over generative models has been demonstrated in a number of both single agent and multi agent domains  Liao       L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR  et al         Limketkai  Fox    Liao        Vail        Vail   Veloso        Hu et al          Since MLNs are often solved as propositionalized CRFs  and the directed alternatives can be compiled into a Bayesian network  it can be expected that discriminative relational models generally outperform their generative counterparts on labeling tasks  However  more work needs to be done to answer this question in its entirety  Since Markov logic is based on  and in fact subsumes  finite first order logic  we immediately gain access to a number of techniques developed in the rich field of traditional logic  Current Markov logic solvers take advantage of the underlying logical structure to perform more powerful optimizations  such as Alchemys lifted inference in belief propagation and MC SAT  Poon   Domingos         Additionally  domain pruning  where one uses hard constraints to infer reduced domains for predicates  has been shown to lead to significant speed ups  Papai  Singla    Kautz         We also leverage this relationship between Markov and first order logic when inducing an augmented model  Furthermore  presence of dependency cycles introduces additional problems in directed graphical  relational  models  Thus  the fact that  in Markov logic  knowledge can be expressed as weighted first order formulas combined with the above factors make it a powerful framework best suited for the multi agent reasoning tasks considered in this work  Traditional hidden Markov models operate over an alphabet of unstructured  i e   flat  symbols  This makes relational reasoning difficult  as one has to either propositionalize the domain  thereby incurring combinatorial increase in the number of symbols and model parameters  or ignore the relational structure and sacrifice information  Logical hidden Markov models  LHMMs  have been proposed to address this problem  Kersting  De Raedt    Raiko         LHMMs are a generalization of standard HMMs that compactly represents probability distributions over sequences of logical atoms rather than flat symbols  LHMMs have been proven strictly more powerful than their propositional counterparts  HMMs   By applying techniques from logic based reasoning  such as unification  while leveraging the logical structure component of the model  Kersting et al  show that LHMMs often require fewer parameters and achieve higher accuracy than HMMs  LHMMs have been recently applied to activity recognition  In the context of intelligent user interfaces  the work of Shen        designs and evaluates a LHMM model for recognition of peoples activities and workflows carried out on a desktop computer  Other researchers proposed a hierarchical extension of LHMMs along with an efficient particle filter based inference technique  and apply it to activity recognition problems in synthetic domains  Natarajan  Bui  Tadepalli  Kersting    Wong         Both lines of work show that LHMMs can be learned and applied efficiently  and perform better than plain HMMs  However  LHMMs are a generative model and therefore are not ideal for pure labeling and recognition tasks  where we typically do not want to make strong independence assumptions about the observations  nor do we want to explicitly model dependencies in the input space  TildeCRFa relational extension of traditional conditional random fieldshas been introduced to address this issue  Gutmann   Kersting         TildeCRF allows discriminative learning and inference in CRFs that encode sequences of logical atoms  as opposed to sequences of unstructured symbols  TildeCRF specifically focuses on efficient learning of models of sequential data via boosting  and is subsumed by Markov logic  which can produce both discriminative and generative models  We cast our model in the latter framework to make it more general  extensible  and interpretable  PRISM  a probabilistic extension of Prolog  has been shown to subsume a wide variety of generative models  including Bayesian networks  probabilistic context free grammars  HMMs  along with their logical extension   Sato   Kameya               However  since the focus of PRISM is       S ADILEK   K AUTZ  on representational elegance and generality  rather than scalability  the sheer size of the state space and complexity of our CTF domain precludes its application here  Finally  our Markov logic theory augmentation process is related to structure learning  transfer learning  and inductive logic programming  In fact  Algorithm   implements a special case of structure learning  where we search for a target theory that explains the training data well  while our declarative bias forces the target theory to differ from the source theory only as much as necessary  Again  with the intuition that failed attempts are similar to their failed counterparts  A number of researchers have focused on structure learning specifically in Markov logic networks  This includes early work on top down structure learning  where clauses in the knowledge base are greedily modified by adding  flipping  and deleting logical literals  Kok   Domingos         This search is guided by the likelihood of the training data under the current model  The work of Mihalkova and Mooney        exploit patterns in the ground Markov logic networks to introduce a bottom up declarative bias that makes their algorithm less susceptible to finding only local optima  as compared to alternative greedy methods  Similarly  the work of Kok and Domingos        introduce a bottom up declarative bias based on lifted hypergraph representation of the relational database  This bias then guides search for clauses that fit the data  Since the hypergraph is lifted  relational path finding tractable  Interesting work on predicate invention applies relational clustering technique formulated in second order Markov logic to discover new predicates from relational databases  Kok   Domingos         The above systems are capable of modeling relatively rich family of logical formulas  Other approaches perform discriminative structure learning and achieve excellent results  but focus on a restricted set of types of formulas  e g   Horn clauses   Huynh   Mooney        Biba  Ferilli    Esposito         The work of Davis and Domingos        successfully uses second order Markov logic in deep transfer learning  They lift the model of the source domain to second order ML and identify high level structural patterns  These subsequently serve as declarative bias for structure learning in the target domain  By its very nature  the inductive logic programming discipline has extensively studied structure learning in deterministic  as well as probabilistic settings  e g   Muggleton        De Raedt        De Raedt  Frasconi  Kersting    Muggleton         In fact  our theory augmentation algorithm can be viewed as an efficient Markov logic based version of theory refinement  a well established ILP technique that aims to improve the quality of a theory in terms of simplicity  fit to newly acquired data  efficiency or other factors  Wrobel         Our approach differs from all this work in three main points  First  our declarative bias is defined implicitly by the seed theory of successful activities  Therefore  our theory augmentation algorithm is not limited to any hard wired set of formula types it can consider  Rather  the search space is defined at run time by extracting motifs from the seed theory  The second distinction lies in computational tractability and exactness of the results  By distinguishing between soft and hard formulas  we are able to search through candidate formulas in a systematic  rather than greedy manner  Consequently  our final learned model requires fewer parameters  which is especially important when the amount of training data is relatively small  Additionally  our weight learning does not experience cold starts  as we leverage the seed theory  The final difference is that  to our knowledge  we are the first to explore structure learning in the context of interplay of success and failure  and their relationship to the intended goals of peoples actions         L OCATION  BASED R EASONING ABOUT C OMPLEX M ULTI  AGENT B EHAVIOR     Conclusions This paper took on the task of understanding the game of capture the flag from GPS data as an exemplar of the general problem of inferring human interactions and intentions from sensor data  We have presented a novel methodologycast in Markov logicfor effectively combining data denoising with higher level relational reasoning about a complex multi agent domain  Specifically  we have demonstrated that given raw and noisy data  we can automatically and reliably detect and recognize both successful and failed interactions in adversarial as well as cooperative settings  Additionally  we have shown that success  failure  and the goal of an activity are intimately tied together and having a model for successful events allows us to naturally learn models of the other two important aspects of life  Specifically  we have demonstrated that the intentions of rational agents are automatically discovered in the process of resolving inconsistencies between a theory that models successful instances of a set of activities and examples of failed attempts at those activities  We have formulated four research questions and designed experiments within the CTF domain that empirically answer them  Compared to alternative approaches to solving the multi agent activity recognition problem  our augmented Markov logic model  which takes into account not only relationships among individual players  but also relationships among activities over the entire length of a game  although computationally more costly  is significantly more accurate on real world data  Furthermore  we have illustrated that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks      Future Work Multi agent activity recognition is especially interesting in the context of current unprecedented growth of on line social networksin terms of their size  popularity  and their impact on our offline lives  In this paper  we show that location information alone allows for rich models of peoples interactions  but in the case of on line social networks  we additionally have access to the content of users posts and both the explicit and the implicit network interactions  For instance  our recent study shows that  interestingly  about     of Twitter status updates reveal their authors location  Sadilek  Kautz    Bigham         These data sources are now available to machines in massive volumes and at ever increasing real time streaming rate  We note that a substantial fraction of posts on services such as Facebook and Twitter talk about everyday activities of the users  Naaman  Boase    Lai         and this information channel has become available to the research community only very recently  Thus  if we are able to reason about human behavior and interactions in an automated way  we can tap the colossal amounts of knowledge that isat presentdistributed across the whole population  We are currently extending our model to handle not only explicit GPS traces  but also be able to infer the location of people who do not broadcast their GPS coordinates  The basic idea is  again  to leverage the structure of relationships among people  The vast majority of us participate in on line social networks and typically some of our friends there do publish their location  We thus view the GPS enabled people as noisy location sensors and use the network interactions and dynamics to estimate the location of the rest of the users  At present  we are testing this approach on public tweets         S ADILEK   K AUTZ  Acknowledgments We thank anonymous reviewers for their constructive feedback  We further thank Sebastian Riedel for his help with theBeast  and to Radka Sadlkova and Wendy Beatty for their helpful comments  This work was supported by ARO grant  W   NF            DARPA SBIR Contract  W  P Q   C       and a gift from Kodak   
  We describe research and results centering on the construction and use of Bayesian mod els that can predict the run time of problem solvers  Our efforts are motivated by observa tions of high variance in the time required to solve instances for several challenging prob lems  The methods have application to the decision theoretic control of hard search and reasoning algorithms  We illustrate the ap proach with a focus on the task of predict ing run time for general and domain specific solvers on a hard class of structured con straint satisfaction problems  We review the use of learned models to predict the ultimate  length of a trial  based on observing the be havior of the search algorithm during an early phase of a problem session  Finally  we dis cuss how we can employ the models to inform dynamic run time decisions     Introduction  The design of procedures for solving difficult problems relies on a combination of insight  observation  and it erative refinements that take into consideration the be havior of algorithms on problem instances  Complex  impenetrable relationships often arise in the process of problem solving  and such complexity le ads to uncer tainty about the basis for observed efficiencies and in effi ciences associated with specific problem instances  We believe that recent advances in Bayesian methods for learning predictive models from data offer valuable tools for designing  controlling  and understanding au tomated reasoning methods  We focus on using machine learning to characterize variation in the run time of instances observed in in herently exponential search and reasoning problems  Predictive models for run time in this domain could  Design  real rime control   World  Context  j  Contex tual  evidence  insights  Run time  Structural evidence  Ex ecution evidence  GQlliJ  Feature refinement  insights  Figure    Bayesian approach to problem solver design and optimization  We seek to learn predictive mod els to refine and control computational procedures as well as to gain insights about problem structure and hardness   provide the basis for more optimal decision making at the microstructure of algorithmic activity as well as inform higher level policies that guide the allocation of resources  Our overall methodology is highlighted in Fig     We seek to develop models for predicting execution time by considering dependencies between execution time and one or more classes of observations  Such classes include evidence about the nat ure of the generator that has provided instances  about the structural properties of instances noted before problem solving  and about the run time behaviors of solvers as they struggle to solve the instances  The research is fundamentally iterative in nature  We exploit learning methods to identify and continue to  refine observational variables and models  balancing the predictive power of multiple observations with the cost of the real time evaluation of such evidential dis    HORVITZ ET AL        tinctions  We seek ultimately to harness the learned models to optimize the performance of automated rea soning procedures  Beyond this direct goal  the overall exploratory process promises to be useful for providing new insights about problem hardness  We first provide background on the problem solving domains we have been focusing on  Then  we describe our efforts to instrument problem solvers and to learn predictive models for run time  We describe the for mulation of variables we used in data collection and model construction and review the accuracy of the in ferred models  Finally  we discuss opportunities for exploiting the models  We focus on the sample appli cation of generating context sensitive restart policies in randomized search algorithms     Hard Search Problems  UAI      distinct symbols in which some cells may be empty but no row or column contains the same element twice  The Quasigroup Completion Problem  QCP  can be stated as follows  Given a partial quasigroup of order n can it be completed to a quasigroup of the same order   n  Figure    Graphical representation of the quasigroup problem  Left  A quasigroup instance with its comple tion  Right  A balanced instance with two holes per row column   We have focused on applying learning methods to char  acterize run times observed in backtracking search pro cedures for solving NP complete problems encoded as constraint satisfaction  CSP  and Boolean satisfiabil ity  SAT   For these problems  it has proven extremely difficult to predict the particular sensitivities of run time to changes in instances  initialization settings  and solution policies  Numerous studies have demon strated that the probability distribution over run times exhibit so called heavy tails         Restart strategies have been used in an attempt to find settings for an instance that allow it to be solved rapidly  by avoiding costly journeys into a long tail of run time  Restarts are introduced by way of a parameter that terminates the run and restarts the search from the root with a new random seed after some specified amount of time passes  measured in choices or backtracks  Progress on the design and study of algorithms for SAT and CSP has been aided by the recent devel opment of new methods for generating hard random problem instances  Pure random instances  such as k Sat  have played a key role in the development of al gorithms for propositional deduction and satisfiability testing  However  they lack the structure that char acterizes real world domains  Gomes and Selman     introduced a new benchmark domain based on Quasi groups  the Quasigroup Completion Problem  QCP    QCP captures the structure that occurs in a variety of real world problems such as timetabling  routing  and statistical experimental design  A quasigroup is a discrete structure whose multipli cation table corresponds to a Latin Square  A Latin Square of order n is an n x n array in which n dis tinct symbols are arranged so that each symbol occurs once in each row and column  A partial quaisgroup  or Latin Square  of order n is an n x n array based on  QCP is an NP complete problem     and random in stances have been found to exhibit a peak in prob lem hardness as a function of the ratio of the number of uncolored cells to the total number of cells  The peak occurs over a particular range of values of this parameter  referred to as a region of phase transition         A variant of the QCP problem  Quasigroup with Holes  QWH         includes only satisfiable instances  The QWH instance generation procedure essentially inverts the completion task  it begins with a randomly generated completed Latin square  and then erases col ors or  pokes holes   Completing QWH is NP Hard      A structural property that affects hardness of in stances significantly is the pattern of the holes in row and columns  Balancing the number holes in each row and column of instances has been found to significantly increase the hardness of the problems         Experiments with Problem Solvers  We performed a number of experiments with Bayesian learning methods to elucidate previously hidden dis tinctions and relationships in SAT and CSP reason ers  We experimented with both a randomized SAT algorithm running on Boolean encodings of the QWH and a randomized CSP solver for QWH  The SAT al gorithm was Satz Rand       a randomized version of the Satz system of Li and Anbulagan       Satz is the fastest known complete SAT algorithm for hard ran dom   SAT problems  and is well suited to many inter esting classes of structured satisfiability problems  in cluding SAT encodings of quasigroup completion prob lems      and planning problems       The solver is a version of the classic Davis Putnam  DPLL  algorithm     augmented with one step lookahead and a sophisti    UAI      cated variable  HORVITZ ET AL   choice heuristic  The lookahead opera            Formulating Evidential Variables  tion is invoked at most choice points and finds any  choices that would immediately lead contradiction after unit propagation  for these  the opposite variable assignment can be immediately made  The variable ch oice heuristic is based on picking a variable that if set would cause the greatest number of ternary clauses to be reduced to binary clauses  The variable choice set was enlarged by a noise parameter of      and value selection was performed determin istically by always branching on  true  first   variable value to a  The second backtrack search algorithm we studied is randomized version of a specialized CSP solver for quasigroup completion problem s  written using the ILOG solver constraint programming library  The backtrack search algorithm uses as a variable choice heuristic a variant of the Brelaz heuristic  Further more  it uses a sophisticated propagation method to enforce the constraints that assert that all the colors in a row  column must be different  We refer to such a constraint as alldiff  The propagation of the alldiff constraint corresponds to solving a matching problem on a bipartite graph using a network flow algorithm              a  learned predictive models for run time  motivated two different classes of target problems  For the first class of problem  we assume that a solver is chal lenged by a n instance and must solve that specific problem as quickly as possible  We term this the Sin gle Instance problem  In a second class of problem  we draw cases from a distribution of instances and are required to solve any instance as soon as possible  or as many instances as possible for any amount of time allocated  We call these challenges Multiple Instance problems  and the subproblems as the Any Instance and Max Instances problems   respectively  We  by  We collected evidence and built models for CSP and Satz solvers applied to the QWH problem for both the Single In st an ce and Multiple Instances challenge  We shall refer to the four problem solving experiments as CSP QWH Single  CSP QWH Multi  Satz  QW H  Single  and S atz Q WH  Multi  Building predictive Bayesian models for the CSP  Q WH S ingle and Satz QWH Single problems centered on gathering data on the probabilistic relationships between observational variables and run time for single instances with ran domized restarts  Experiments for the CSP QWH Multi and S atz  Q WH  Multi problems centered on per forming single runs on multiple instances drawn from the same instance generator   We worked to define variables that we believed could provide information on problem solving progress for a period of observation in an early phase of runs that we refer to as th e observation horizon  The defin iti on of variables was initially guided by intuition  However  results from our early experiments helped us to refine sets of variables and to propose additional candidates  We initially explored a large number of variables  in cluding those that were difficult to compute  Although we planned ultimately to avoid the use of costly ob servations in real time forecasting settings  we were interested in probing the predictive power and inter dependencies among features regardless of cost  Un der st andin g such informational dependencies promised to be useful in understanding the potential losses in predictive power with the removal of costly features  or substitution of expensive evidence with less expen sive  approximate observations  We eventually limited the features explored to those that could be computed with low  constant  overhead  We sought to collect information about base values as well as several variants and combinations of these val ues  For example  we formulated features that could capture higher l evel patterns and dynamics of the state of a prob l em solver that could serve as useful probes of solution progress  Beyond exploring base observa tions about the program state at particular points in a case  we defined new families of observations such as first and second derivatives of the base variables  and summaries of the status of variables over time   Rather than include a separate variable in the model for each feature at each choice point which would have led to an explosion in the number of variables and severely limited generalization features and their dynamics were represented by variables for their sum mary statistics over the observation horizon  The sum mary statistics included initial  final  average  mini mum  and maximum values of the features during the observation period  For example  at each choice point  the SAT solver recorded the current number of binary clauses  The training data would thus included a vari able for the average first derivative of t he number of binary clauses during the observation period  Finally  for several of the features  we also computed a sum mary statistic that measured the number of times the sign of the feature changed from negative to positive or vice versa  We developed distinct sets of observational var iables for the CSP and Satz solvers  The features for the CSP solver included some that were generic to any constraint satisfaction problem  such as the number of backtracks  the depth of the search tree  and the   HORVITZ ET AL        average domain size of the unbound CSP variables  Other features  such as the variance in the distribution of unbound CSP variables between different columns of the square  were specific to Latin squares  As we will see below  the inclusion of such domain specific features was important in learning strongly predictive models  The CSP solver recorded    basic features at each choice point which were summarized by a to tal of     variables  The variables that turned out to be most informative for prediction are described in Sec      below  The features recorded by Satz Rand were largely generic to SAT  We included a feature for the num ber of Boolean variables that had been set positively  this feature is problem specific in the sense that under the SAT encoding we used  only a positive Boolean variable corresponds to a bound CSP variable  i e  a colored squared   Some features measured the current problem size  e g  the number of unbound variables   others the size of the search tree  and still others the effectiveness of unit propagation and lookahead  We also calculated two other features of special note  One was the logarithm of the total number of possible truth assignments  models  that had been ruled out at any point in the search  this quantity can be effi ciently calculated by examining the stack of assumed and proven Boolean variable managed by the DPLL algorithm  The other is a quantity from the theory of random graphs called     that measures the degree of interaction between the binary clauses of the formula       In all Satz recorded    basic features that were summarized in     variables      Collecting Run Time Data  For all experiments  observational variables were col lected over an observational horizon of      solver choice points  Choice points are states in search pnr cedures where the algorithm assigns a value to vari ables heuristically  per the policies implemented in the problem solver  Such points do not include the cases where variable assignment is forced via propagation of previous set values  as occurs with unit propagation  backtracking  lookahead  and forward checking  For the studies described  we represented run time as a binary variable with discrete states short versus long  We defined short runs as cases completed before the median of the run times for all cases in each data set  Instances with run times shorter than the observation horizon were not considered in the analyses   Models and Results  We employed Bayesian structure learning to infer pre dictive models from data and to identify key variables from the larger set of observations we collected  Over the last decade  there has been steady progress on methods for inferring Bayesian networks from data                  Given a dataset  the methods typically perform heuristic search over a space of dependency models and employ a Bayesian score to identify mod els with the greatest ability to predict the data  The Bayesian score estimates p modelldata  by approxi mating p  data lmodel p  model   Chickering  Hecker man and Meek     show how to evaluate the Bayesian score for models in which the conditional distributions are decision trees  This Bayesian score requires a prior distribution over both the parameters and the struc ture of the model  In our experiments  we used a uni form parameter prior  Chickering et al  suggest using a structure prior of the form  p model  r  fP  where                and fp is the number of free parameters in the model  Intuitively  smaller values of r   make large trees unlikely a priori  and thus     can be used to help avoid overfitting  We used this prior  and tuned r   as described below     We employed the methods of Chickering et a   to infer models and to build decision trees for run time from the data collected in experiments with CSP and Satz problem solvers applied to QWH problem instances  We shall describe sample results from the data col lection and four learning experiments  focusing on the CSP QWH Single case in detail            UAI      CSP QWH Single Problem  For a sample CSP QWH Single problem  we built a training set by selecting nonbalanced QWH problem instance of order    with     unassigned variables  We solved this instance      times for the training set and      times for the test data set  initiating each run with a random seed  We collected run time data and the states of multiple variables for each case over an observational horizon of      choice points  We also created a marginal model  capturing the overall run time statistics for the training set  We optimized the r   parameter used in the structure prior of the Bayesian score by splitting the training set       into training and holdout data sets  respectively  We selected a kappa value by identifying a soft peak in the Bayesian score  This value was used to build a dependency model and decision tree for run time from the full training set  We then tested the abilities of the marginal model and the learned decision tree to pre dict the outcomes in the test data set  We computed a classification accuracy for the learned and marginal   UAI      HORVITZ ET AL   models to characterize the power of these models  The classification accuracy is the likelihood that the classi fier will correctly identify the run time of cases in the test set  We also computed an average log score for the models  Fig    displays the learned Bayesian network for this dataset  The figure highlights key dependencies and variables discovered for the data set  Fig    shows the decision tree for run time  The classification accuracy for the learned model is       in contrast with a classification accuracy of       for the marginal model  The average log score of the learned model is        a nd the average log score of the marginal model was         Because this was both the strongest and most com pact model we learned  we will discuss the features it involves in more detail  Following Fig    from left to right  these are  VarRowColumn measures the variance in the number of uncolored cells in the QWH instance across rows and across columns  A low variance indicates the open cells are evenly balanced throughout the square  As noted earlier  balanced instances are harder to solve than unbalanced ones      A rather complex summary statistic of this quantity appears at the root of the de cision tree  namely the minimum of the first derivative of this quantity during the observation period  In fu ture work we will be examining this feature carefully in order to determine why this particular statistic was most relevant  AvgColumn measures the ratio of the number of uncol ored cells and the number of columns or rows  A low value for this feature indicates that the quasigroup is nearly complete  The decision tree shows that a run is likely to be fast if the min i mum value of this quantity over the entire observation period is small  MinDepth is the minimum depth of all leaves of the search tree  and the summary statistic is simply the fi nal value of this quantity  The third and fourth nodes of the decision tree show that short runs are associ ated with high minimum depth and long runs with low minimum depth  This may be interpreted as in dicating the search trees for the shorter runs have a more regular shape  AvgDepth is the average depth of a node in the search tree  The model discovers that short runs are associ ated with a high frequency in the change of the sign of the first derivative of the average depth  In other words  frequent fluctuations up and down in the aver age depth indicate a short run  We do not yet have an intuitive explanation for this phenomena        VarRowColumn appears again as the last node in the decision tree  Here we see that if the maximum vari ance of the number of uncolored cells in the QWH instance across rows and columns is low  i e   the prob lem remains balanced  then the run is long  as might be expected       CSP QWH Multi Problem  For a CSP QWH Multi problem  we built training and test sets by selecting instances of nonbalanced QWH problems of order    with     unassigned variables  We collected data on      instances for the training set and      instances for the test set  As we were running instances of potentially different fundamental hardnesses  we normalized the feature measurements by the size of the instance  measured in CSP variables  after the instances were initially sim plified by forward checking  That is  although all the instances originally had the same number of uncolored cells  polynomial time preprocessing fills in some of the cells  thus revealing the true size of the instance  We collected run time data for each instance over an observational horizon of      choice points  The learned model was found to have a classification accu racy of       in comparison to the marginal model ac curacy of        The average log score for the learned model was found to be        and the average log score for the marginal model was              Satz QWH Single Problem  We performed analogous studies with the Satz solver  In a study of the Satz QWH Single problem  we stud ied a single QWH instance  bqwh             We found that the learned model had a classification ac curacy of        in comparison to a classification accu racy of       for the marginal model  The average log score of the learned model was found to be        and the log score of the marginal model was         The predictive power of the SAT model was less than that of the corresponding CSP model  This is reason able since the CSP model had access to features that more precisely captured special features of quasigroup problems  such as balance   The decision tree was still relatively small  containing    nodes that referred to    different summary variables  Observations that turned out to be most relevant for the SAT model included    The maximum number of variables set to  true  during the observation period  As noted earlier  this corresponds to the number of CSP variables that would be bound in the direct CSP encoding    HORVITZ ET AL        UAl       Figure    The learned Bayesian network for a sample CSP QWH Single problem  Key dependencies and variables are highlighted   I Y  I              Nat                Nat             Not     S                                           Nat                                S        Not                                       Figure    The decision tree inferred for run time from data gathered in a CSP QWH Single experiment  The probability of a short run is captured by the light component of the bargraphs displayed at the leaves    UAI      HORVITZ ET AL     The number of models ruled out     The number of unit propagations performed            The number of variables eliminated by Satz s lookahead component  that is  the effectiveness o f lookahead  The quantity     described in Sec      above  a mea sure of the constrainedness of the binary clause subproblem  Satz QWH Multi Problem  For the experiment with the Satz QWH Multi prob lem  we executed single runs of QWH instances with the same parameters as the instance studied in the Satz QWH Single Problem  bqwh         for the training and test sets  Run time and observational variables were normalized in the same manner as for the CSP QWH Multi problem  The classification ac curacy of the learned model was found to be         The classification accuracy of the marginal model was found to be        The average log score for the model was        and the average log score for the marginal model was              Toward Larger Studies  For broad application in guiding computational prob lem solving  it is important to develop an understand ing of how results for sample instances  such as the problems described in Sections     through      gener alize to new instances within and across distinct classes of problems  We have been working to build insights about generalizability by exploring the statistics of the performance of classifiers on sets of problem instances  The work on studies with larger numbers of data sets has been limited by the amount of time required to generate data sets for the hard problems being stud ied  With our computing platforms  several days of computational effort were typically required to pro duce each data set  As an example of our work on generalization  we re view the statistics of model quality and classification accuracy  and the regularity of discriminatory features for additional data sets of instances in the CSP QWH Single problem class  We defined ten additional nonbalanced QWH problem instances  parameterized in the same manner as the CSP problem described in Section      order    with     unassigned variables   We employed the same data generation and analysis procedures as before  building and testing ten separate models  Generating data for these analyses using the ILOG libary executed on an       Intel Pentium III  running at     Mhz  required ap proximately twenty four hours per      runs  Thus  each CSP dataset required approximately five days of computation  In summary  we found significant boosts in classi fication accuracy for all of the instances  For the ten datasets  the mean classification accuracy for the learned models was       with a standard deviation of        The average log score for the models was        with a standard deviation of        The predictive power of the learned models stands in contrast to the classification accuracy of using background statistics  the mean classification accuracy of the marginal mod els was       with a standard deviation of        The average log score for the marginal models was        with a standard deviation of        Thus  we observed relatively consistent predictive power of the methods across the new instances  We observed variation in the tree structure and dis criminatory features across the ten learned models  Nevertheless  several features appeared as valuable discriminators in multiple models  including statistics based on measures of VarRowColumn  AvgColumn  AvgDepth  and MinDepth  Some of the evidential fea tures recurred for different problems  showing signifi cant predictive value across models with greater fre quency than others  For example  measures of the maximum variation in the number of uncolored cells in the QWH instance across rows and columns  Max VarRowColumn  appeared as being an important dis criminator in many of the models     Generalizing Observation Policies  For the experiments described in Sections   and    we employed a policy of gathering evidence over an obser vation horizon of the initial      choice points  This observational policy can be generalized in several ways  For example  in addition to harvesting evidence within the observation horizon  we can consider the amount of time expended so far during a run as an explicit observation  Also  evidence gathering can be general ized to consider the status of variables and statistics of variables at progressively later times during a run  Beyond experimenting with different observational policies  we believe that there is potential for harness ing value of information analyses to optimize the gath ering of information  For example  there is opportu nity for employing affine analysis and optimization to generate tractable real time observation policies that dictate which evidence to evaluate at different times during a run  conditioned on evidence that has already been observed during that run              HORVITZ ET AL   Time Expended  as  Evidence  In the process of exploring alternate observation policies  we investigated the value of extending the bounded horizon policy described in Section    with a consideration of the status of time expended so far during a run  To probe potential boosts with inclusion of time expended  we divided several of the data sets explored in Section     into subsets based on whether runs with the data set had exceeded specific run time boundaries  Then  we built distinct run time specific models and tested the predictive power of these models on test sets containing instances of appropriate mini mal length  Such time specific models could be used in practice as a cascade of models  depending on the amount of time that had already been expended on a run  We typically found boosts in the predictive power of models built with such temporal decompositions  As we had expected  the boosts are greatest for models conditioned on the largest amounts of expended time  As an example  let us consider one of the data sets generated for the study in Section      The model that had been built previously with all of the data had a classification accuracy of         The median time for the runs represented in the set was nearly        choice points  We created three separate sub sets of the complete set of runs  the set of runs that exceeded       choice points  the set that exceeded       choice points  and the set that had exceeded        choice points  We created distinct predictive models for each training set and tested these mod els with cases drawn from test sets containing runs of appropriate minimal length  The classification accu racies of the models for the low  medium  and high time expenditure were               and       respec tively  We shall be continuing to study the use of time allocated as a predictive variable     Application  Dynamic Restart Policies  A predictive model can be used in several ways to control a solver  For example  the variable selection heuristic used to decompose the problem instance can be designed to minimize the expected solution time of the subproblems  Another application centers on building distinct models to predict the run time as sociated with different global strategies  As an ex ample  we can learn to predict the relative perfor mance of ordinary chronological backtrack search and dependency directed backtracking with clause learn ing       Such a predictive model could be used to decide whether the overhead of clause learning would be worthwhile for a particular instance   UA       Problem and instance specific predictions of run time can also be used to drive dynamic cutoff decisions on when to suspend a current case and restart with a new random seed or new problem instance  depending on the class of problem  For example  consider a greedy analysis  where we deliberate about the value of ceas ing a run that is in progress and performing a restart on that instance or another instance  given predictions about run time  The predictive models described in this paper can provide the expected time remaining until completion of a current run  Initiating a new run will have an expected run time provided by the statistics of the marginal model  From the perspec tive of a single step analysis  when the expected time remaining for the current instance is greater than the expected time of the next instance  as defined by the background marginal model  it is better to cease ac tivity and perform a restart  More generally  we can construct richer multistep analyses that provide the fastest solutions to a particular instance or the highest rate of completed solutions with computational effort  We can also use the predictive models to perform com parative analyses with previous policies  Luby et al       have shown that the optimal restart policy  as suming full knowledge of the distribution  is one with a fixed cutoff  They also provide a universal strat egy   using gradually increasing cutoffs  for minimizing the expected cost of randomized procedures  assum ing no prior knowledge of the probability distribution  They show that the universal strategy is within a log factor of optimal  These results essential settle the distribution free case  Consider now the following dynamic policy  Observe a run for   steps  If a solution is not found  then predict whether the run will complete within a total of L steps  If the prediction is negative  then immediately restart  otherwise continue to run for up to a total of L steps before restarting if no solution is found  An upper bound on the expected run of this policy can be calculated in terms of the model accuracy A and the probability Pi of a single run successfully ending in i or fewer steps  For simplicity of exposition we assume that the model s accuracy in predicting long or short runs is identical  The expected number of runs until a solution is found is E N     A PL  Po   Po   An upper bound on the expected number of steps in a single run can be calculated by assuming that runs that end within   steps take exactly   steps  and that runs that end in      to L steps take exactly L steps  The probability that the policy continues a run past   steps  i e   the prediction was positive  is APL     A       PL   An upper bound on the expected length of a single run is Eub R       L  O  APL       A    PL    Thus  an upper bound on the expected time to        UAI      solve a  HORVITZ ET AL   proble m  E N Eub R    using the policy is  It is important to note that the expected time depends on both the accuracy of the model and the prediction point L  in general  one would want to vary L in or der to optimize the solution time   Furthermore  in  general  it would be better to design more sophisti cated dynamic policies that made use of all informa tion gathered over a run  rather than just during the first   steps  But even a non optimized policy based directly on the models discussed in this paper can out perform the optimal fixed policy  For example  in the CSP QWH single problem case  the optimal fixed pol icy has an expected solution  time of        steps  while  the dynamic policy has an expected solution time of only          steps  Optimizing the choice of L should  provide about an order of magnitude further improve ment        c s ons about the partition of resources  formulation and inference  and Klein        between  re  In other work  Horvitz  constructed Bayesian models consid  ering the time expended so far in theorem proving  They monitored the progress of search in a proposi tional theorem prover and used measures of progress in updating the probability of truth or falsity of as  sertions  A Bayesian model was harnessed to update belief about different outcomes as a function of the amount of time that problem solving continued with out halting  Stepping back to view the larger body of work on the decision theoretic control of computation  measures of  expected value of computation                 employed to guide problem solving  rely on forecasts of the refinements of partial results with future com putation  More generally  representations of problem solving progress have been central in research on flex ible or anytime methods procedures that exhibit a  While it may not be surprising that a dynamic policy  relatively smooth surface of performance  can outperform the optimal fixed policy  it is interest  location of computational resources   with the al  ing to note that this can occur when the observation time   is  greater  than the fixed cutoff   That is  for  proper values of L and A  it may be worthwhile to ob serve each run for       steps  even if the optimal fixed  strategy is to cutoff after     st eps  These and other  issues concerning applications of prediction models to restart policies are examined in detail in a forthcoming paper      Future Work and Directions  This work represents a vector in a space of ongoing re search  We are pursuing several lines of research with the goals of enhancing the power and generalizing the applicability of the predictive methods  We are explor ing the modeling of run time at a finer grain through the use of continuous variables and prototypical named     distributions  We are also exploring the value of de  Related Work  composing the learning problem into models that pre  Learning methods have been employed in previous re search in a attempt to enhance the performance opti mize reasoning systems  In work on  speed up learn ing   investigators have attempted to increase plan ning efficiency by learn i ng goal specific preferences for  plan operators             Khardon and Roth explored  the offline reformulation of representations based on experiences with problem solving in an environment to enhance run time efficiency         Our work on using  probabilistic models to learn about algorithmic perfor m ance and to guide problem solving is most c losely re  lated to research on flexible computation and decision theoretic control  Related work in this arena focused on the use of predictive models to control computa tion  Breese and Horvitz     collected data about the  dict the average execution times seen with multiple runs and models that predict how well a particular in stance will do relative to the overall hardness of the problem   In other extensions  we are exploring the  feasibility of inferring the likelihood that an instance is solvable versus unsolvable and building models that forecast the overall expected run time to completion by conditioning on each situation  We are also inter ested in pursuing more general  dynamic observational policies and in harnessing the value of information to identify a set of conditional decisions about the pattern and timing of monitoring  F inally  we are continuing to investigate the formulation and testing of ideal poli cies for harnessing the predictive models to optimize restart policies   progress of search for graph cliquing and of cutset anal ysis for use in minimizing the time of probabilistic in  ference with Bayesian networks      Summary  The work was mo  tivated by the challenge of identifying the ideal time  We presented a methodology for characterizing the run  for preprocessing graphical models for faster inference before initiating inference  trading off reformulation  time of problem instances for randomized backtrack style search algorithms that have been developed to  time for inference time   Trajectories of progress as  solve a hard class of structured constraint satisfaction  a function of  of Bayesian network prob  parameter s  lem instances were learned for use in dynamic de   problems  The methods are motivated  by recent suc  cesses with using fixed restart policies to address the   HORVITZ ET AL        UAI       high variance in running time typically exhibited by        backtracking search algorithms  We described two dis tinct formulations of problem solving goals and b uilt  D  Beckerman   J  Breese  and K   Rommelse  Decision theoretic troubleshooting  CA CM                           D   Beckerman  D   M   Chickering  C  Meek  R  Roun thwaite  and C  Kadie  Dependency networks for den sity estimation  collaborative filtering  and data visu alization  In Proceedings of UA I       Stanford  CA  pages                     E  Horvitz and A  Klein  Reasoning  metareasoning  and mathematical truth  Studies of theorem proving under limited resources  In Proceedings of UA I      pages          Montreal  Canada  August       Mor gan Kaufmann  San Francisco         E   J   Horvitz  Reasoning under varying and uncer tain resource constraints  In Proceedings of A A A I     pages             Morgan Kaufmann  San Mateo  CA  August        butions and feedback         
