  Probably the most naive strategy is to perform simple gra dient ascent in the likelihood  This method has two prob  Many applications require that we learn the pa rameters of a model from data  EM  E xpectation Maximization  is a method for learning the pa rameters of probabilistic models with missing or hidden data  There are instances in which this method is slow to converge   Therefore  sev  eral accelerations have been proposed to improve the method  None of the proposed acceleration methods are theoretically dominant and experi mental comparisons are lacking  In this paper  we present the different proposed accelerations and compare them experimentally  From the re sults of the experiments  we argue that some acceleration of EM is always possible  but that which acceleration is superior depends on prop erties of the problem   lems  First  it is known to be relatively inefficient among the class of local search methods  Second  it is often the  case that the model class A constrains the choice of param eters     For example  some of the components of      may  be constrained to describe a probability distribution  and so must be between   and   and must sum to  I  Naive gra  dient methods must be specially modified to respect such constraints  The EM algorithm was described by Dempster et al         as a generalization of the Baum Welsh algorithm for learn  ing hidden Markov models  Rabiner         Typically  it is monotonically convergent to a local optimum in likeli hood space for probabilistic models  while directly satisfy ing possible constraints on the parameters  There are two main classes of strategies for finding maximum likelihood models with hidden parameters  ac celerated gradient methods with constraint handling and EM  It has been observed empirically that  in some set  INTRODUCTION     tings one algorithm appears to work better  and in other  There are many applications in artificial intelligence and statistics that require the fitting of a parametric model to data   It is often desired to find the maximum likelihood   ML  or maximum a posteriori probability  MAP  model  of the data  W hen all of the variables of the model are di  settings  other algorithms appear to work better  Further more  a number of researchers  both in the statistics and AI literatures  have proposed extensions  accelerations  and combinations of these methods  In this paper  we seek to understand the relative merits  rectly observable in the data  then this is relatively straight  of these optimization strategies and their extensions  Al  forward  When some variables are hidden  as is common  though there have been some attempts at theoretical com parisons of convergence rate  Xu and Jordan         the re  in popular model classes such as Bayesian networks with hidden variables and hidden Markov models  maximum  sults are never clear cut because they depend on properties  likelihood parameter estimation is much more complicated   of the individual problems to which they are applied  We  The problem can be cast directly as an optimization prob  have undertaken an empirical study in one of the simplest  D and a model of the form A O   find  hidden variable models  density estimation with a mixture  hood p DIA O    orp A O ID  in theMAPcase   Unfor tunately  the obj ective function does not have a form that  ations we can encounter  our only hope is to present em  lem  given a data set  the setting of the parameters      that maximizes the likeli  can be easily optimized globally  so we are generally re duced to local search methods   This material is based upon work supported in part un der a National Science Foundation Graduate Fellowship and by  DARPA Rome Labs Planning Initiative grant F         l          This work was supported in part by DARPA Rome Labs  Planning Initiative grant F         l        of Gaussians  Given the infinite number of possible situ pirical results suggesting the merits and disadvantages of each method in some situations  We believe that this study yields some insight into general properties of the methods  though the results cannot be guaranteed to transfer  In the following sections  we describe the problem  some of the most common optimization methods available to solve it  the experiments we ran and the results we obtained  Please refer to the technical report  Ortiz and Kaelbling    Accelerating EM  An Empirical Study        for additional details      This method does not respect any constraints on the param eters  To satisfy the constraints on aj  we project the part of the gradient relevant to these parameters into the constraint space and take a step size that does not take us out of this space  Bertsekas        Binder et a           The projected gradient is  DENSITY ESTIMATION WITH A MIXTURE OF GAUSSIA NS  One of the simplest ML estimation problems with hidden variables is to model the probability density of a data set with a mixture of Gaussians  The model assumes that there is some number  M  of underlying  centers  in the d dimensional space  Each data point is independently gen erated by first choosing a center with probability aj  and then drawing from a Gaussian distribution  with mean    j  the center  and covariance matrix j  AutoClass  Cheeseman et a          casts the problem of how many centers to use in a Bayesian perspective by stat ing we should use the most probable number of centers given the data  In this paper  we address a subproblem of AutoClass  given a desired number of centers  find the model that maximizes the posterior probability  In this problem  the parameter vector    is made up of the aj     j and j for each j  From the independence of the data points  we can write the logarithm of the likelihood of the parameters with respect to the data  p DIA O   as the sum of the logarithm of the likelihood with respect to the individual points  N  L O    lnp DIA O      np x IA O    i l   I   where  where   v  j denotes the l h component of vector v  Another strategy is to parameterize a in terms of another parameter w such that w is unconstrained and any assign ment to w satisfies the constraints on a  One way of doing this is as follows       To satisfy the constraint on j  we verify that the step size does not take us out of the constraint space and decrease the step size if it does until we get a step size that does not take us out of the constraint space  Taking a fixed or predetermined step size at each step can slow down convergence  Instead  we can optimize the step size at every step by trying to find the largest value of the function in the direction of the gradient at every step by means of a line search  that is  this is the same as in gradient ascent but with  y k     argmaxL  J k   M  p x IA O     I jg x IJJ i j   j  l   Y       and g xiJJ     is the multivariate Gaussian density with pa rameters     and  The constraints on   are   I  for all j  aj          I   aj          for all j  j is a symmetric  positive definite matrix  It is sufficient to maximize L     in order to maximize p DIA O    Unfortunately  there is no direct method for performing ML estimation of             GRADIENT METHODS  The simplest gradient method we can use to find the maximum of the log likelihood function is gradient as cent  Shewchuk        Bertsekas        Polak         In this case  starting at some initial values for the parameters  at each iteration k  we obtain new values as follows   where  i  L  J k    is the gradient of the log likelihood func tion evaluated at the current values of the parameters and  y k  is the step size we take uphill along the gradient di rection  The value of  y k  can be fixed or predetermined to decrease at every iteration      y ilL  J k        This method seems appealing but it has a drawback in that if the Hessian of the function at the local optimum is ill conditioned  i e   the function is  elongated    it exhibits a zig zagging behavior that can significantly slow down con vergence  Bertsekas         The conjugate gradient method tries to eliminate the zig zagging behavior of optimized step size gradient ascent by requiring that we optimize along conjugate directions at every step  Formally  starting with some initial setting of the parameters  J O   r o      i L  J  l   and the first di rection d o     r     at each iteration k  we do as fol lows  line search  y k     argmax    L  J k     fd k    take best step in current direction   J k l      J k       k d  k  r k l      i  L  J k      if  k      mod d      then  B k l           start over weight to current direction  else      new gradient   k l      end if   rl lf  r k I   rll   rll tr k   d k l     r k l     B k l d k   new conjugate direction        Ortiz and Kaelbling  If we disregard the line search iterations  the conjugate gra dient method has the appealing property that the number of iterations to convergence when close to a solution   is roughly equal to the number of parameters  There are other more sophisticated methods  such as New ton and Quasi Newton or variable metric  that try to solve the deficiencies of fixed  and optimized step size gradient ascent by reshaping the function  These methods use ad ditional information about the function  like higher order derivatives  However  using higher order derivatives re quires us to use additional storage and perform additional computations  which typically outweigh the reduction in the number of iterations     EMBASICS  In the case of the mixture of Gaussians  at each iteration  we find  for each data point and each center  the condi tional probability that the center generated the data point and then use that probability distribution to assign new val ues to the probability  mean and covariance matrix of each center  The expectation step is providing the maximization step with the information that will allow it to compute the expected sufficient statistics for each center under the con ditional probability distribution over the center given the data and the current value of the parameters  EM uses the expected sufficient statistics in the maximization step as if they were the true sufficient statistics to obtain new values for the parameters  More specifically  starting from some initial setting of the parameters  at each iteration k  the EM algorithm for mix ture of Gaussians is  IJ k  and D ajk g x IJ  k  r n  E step  compute distribution induced by  fori      to N j       to M do     P C   ilx  A IJ k     ex end for  hl   M step  update parameters for j       to M do   k    f   L   z   h k  j N Jlj k l  f   I     hhTk x        I  j    hj x x   h k    N L        l  IJ     end for  Jlj k l   Jlj k l  T  For the mixture of Gaussians model  the method typically converges to a local maximum of the likelihood function  but it can stop in other stationary points  or it can go to a singular point where the likelihood function grows with out bound  Redner and Walker         A singular point in the mixture model occurs when we use data points as one of the means and let the variance of that center go to zero  Duda and Hart         In practice  we can avoid sin gularities through good initializations  In cases where this is not enough  we can either      assign a small value to vari ances when they go below some small threshold value      delete components with too small variances      use priors on and perform MAP estimation  or     restart EM with a different initial value for the parameters   Ej  EM is a method for optimizing log likelihood functions in the case of missing data or hidden variables  Dempster et al         McLachlan and Krishnan         Starting with some initial value for the parameters  at each iteration  it uses the value of the parameters to compute the distribu tion or density over the hidden variables conditioned on the data  the expectation step  and then uses that distribution to get new values for the parameters  the maximization step   The likelihood function monotonically increases at each it eration  and under some regularity conditions on the like lihood function  the improvement is strict except at a sta tionary point of the likelihood function  Wu          Q          jk l   We can also view EM as a special form of the general gra dient method  This allows us to see how EM reshapes the function it is optimizing to make it better conditioned  Xu and Jordan         It also allows us to analyze its conver gence theoretically  However  in the mixture of Gaussians model  the method gives updates that automatically satisfy the constraints on the parameters  in the case of this is true with probability   for sufficiently large N  Redner and Walker        Xu and Jordan          In practice  numer ical errors can still take us out of the parameter space  In those cases  we can either   l  take a smaller step in the direction of the EM update so as to guarantee constraint satisfaction      eliminate components for which ai     and put small thresholds on or     use priors on IJ and perform MAP estimation   Ej   Ej      ACCELERATION METHODS  Although EM has a very appealing monotonicity prop erty  its convergence rate is significantly slow in some in stances  For mixture of Gaussians  EM slows down when the centers  i e   the Gaussian components  are very close together  Redner and Walker        Xu and Jordan         One alternative is to start the optimization with EM and move to gradient methods when we are close to a solution  Another is to accelerate EM directly by using information about the EM iteration  McLachlan and Krishnan         One of the direct accelerations is parameterized EM  Pe ters and Walker        Redner and Walker        Meilij son        Bauer et al          Starting from some initial setting of the parameters  at each iteration  we get new val ues for the parameters as follows          N  J   N           IJ    More formally  close to a solution means the neighborhood around the local optimum that can be well approximated by a quadratic function      where e  A f   is the EM update with respect to the current parameters IJ k   In this method  we use the change in val ues in the parameters at an EM iteration and take a step uphill in this direction from the current position or value of the parameters  The step size can be fixed as in gradient   Accelerating EM  An Empirical Study  ascent  or optimized at every step as in optimized step size gradient ascent  It is equivalent to EM when l k       In the case of mixture of Gaussians  we can achieve conver gence using this method when we are close to a solution and     l k       Redner and Walker         and improve convergence speed when l k         Xu         Parameterized EM is actually gradient or steepest ascent to find the zero of a function that is the change in parameters provided by EM  i e   finding a fixpoint of the EM update   Another acceleration method is conjugate gradient accel eration of EM  Jamshidian and Jennrich        Thiesson         The idea is to use the change in value of the pa rameters of the EM iteration to find better conjugate di rections when performing conjugate gradient  The method uses the information provided by the EM iteration to re shape the function and improve convergence speed when close to a solution  Formally  starting with some initial set ting of the parameters  J     r       V  L IJ      and the first direction d o        IJ o   at each iteration k  we do as follows  line search l k  f  argmax    L IJ k    jd k    take best step in current direction   J k      IJ k    l k d k  r k I     V  L IJ k I   uik      IJ k I   IJ k  EM if    k      mod d      then f  k I        else  new gradient EM direction  start over weight to current direction using EM information     k    end if d k I   f   f    uiHI  f T r k l  r k      dil   rlk I  r k    u k J    f  k   d k   new conjugate direction  This method is actually a special form of a generalized con jugate gradient method  The interesting aspect of the con jugate gradient acceleration of EM is that  contrary to the traditional generalized conjugate gradient method  it does not require the specification of a preconditioning matrix or the evaluation of second order derivatives or matrix vector multiplications  since the change in parameters provided by the EM update rule approximates the generalized gradient  We conjecture that the relationship between parameterized EM and conjugate gradient using EM information is similar to the relationship between fixed  and optimized step size gradient ascent and regular conjugate gradient  finding a good step size is problem dependent but optimizing the step size might not be a good idea  i e   produces zig zagging behavior  and moving in conjugate directions is better  As pointed out by an anonymous reviewer  there are other extensions of the EM algorithm that can speed up conver gence  Due to lack of space  we refer the reader to McLach lan and Krishnan         for additional information about extensions and variations of EM  Two extensions of the EM algorithm are the ECM  Expectation Conditional Maximization   Meng and Rubin        and the ECME  Expectation Conditional Maximization Either   Liu and       Rubin        algorithms  Both are useful when the regu lar maximization step is complex  i e   no closed form op timization  yet simpler if conditioned on a function of the current value of the parameters  Furthermore  ECME dif fers from ECM only in that it conditionally maximize the log like ihood function directly in some of the steps  ECM typically has a slower convergence rate than EM  although it can be faster in actual total computing time  The con vergence rate of ECME is typically faster than that of EM and ECM  the actual computing time to convergence is also typically faster   Given that the maximization step is simple in the context of the mixture of Gaussians model  these ex tensions do not really apply here except in the case that we reparameterized a as in equation      In this case  a version of an ECM algorithm can speed up convergence with re spect to the typical alternative  which is an algorithm based on the generalized version of EM  However  it will still be typically slower than regular EM  We note that a version of the ECM algorithm can help learning in the context of the mixture of experts architecture  Jordan and Xu            EXPERIMENTS  The theoretical convergence speed of the different methods presented is problem dependent  No one is theoretically dominant  There is empirical evidence that EM is superior to gradient ascent in the mixture of Gaussians case  Xu and Jordan         The conjugate gradient and the accelera tion methods work well when they are close to a solution  We argue that running conjugate gradient by itself is not a good idea since it requires a very precise line search when it is far from a solution  In practice  precise line searches can increase the time to convergence  Hence  the meth ods we compare to EM in this paper are all based on an idea that uses the monotonic convergence properties of EM  The algorithmic description is as follows  Jamshidian and Jennrich        Thiesson         starting from some initial setting of the parameters  repeat Run EM to get us close to a solution Run acceleration until stopping condition When needed  we use inexact line searches during the ac celerations to save time  This seems to work well when we are close to a solution  However  a decrease in log likelihood can occur due to the inexact line search  If a decrease in log likelihood occurs during the acceleration  we return to EM and repeat the process  We interpret the condition close to a solution to be true when the change in log likelihood is less than      This means that we continue to run EM  as long as the x  statistic for testing the equal ity of two successive iterates is more than I   Jamshidian and Jennrich         W hen needed  the line search we use is an adapted version of the secant like line search used by Jamshidian and Jen nrich         Note that even in methods that do not use a line search  like parameterized EM  we still need to make sure that the step size does not take us outside the constraint        Ortiz and Kaelbling  space  In those cases  we reduce the step size until we find one that keeps us inside the parameter space  The basis of our empirical analysis is the idea that the work needed to compute both the gradient and EM update is ap proximately the same  Actually  as long as N is sufficiently large such that the extra computation is not significant  we can compute both in about the same time  since they re quire about the same information  We can see this from the expression of the gradient  Xu and Jordan         Let the expected counts for center j be   I    and let o fM  JJfM and I fM be the result of applying the EM update rule to o J  fJj and L j  then  Nj   L O   o   N             j  V    L O      O j     h j   al M  N             O j   I j   I fM  I J   JJfM  fJJ  JJfM  JJJf  I j        Nivec         First of all  we note that both require the computation of the expected sufficient statistics I   I   and I   to obtain the EM update  This takes O NMd     O NMd  when dealing with independent fea tures     Using the EM update  computing the EM direction takes   Md    extra work and computing the gradient takes O Md   extra work  in case of independent features  both bounds are    Md    We also note that the constants in the bounds for extra work are small  Therefore  for sufficiently large N  the time to compute the expected sufficient statis tics dominates all others  Finally  we say that the computa tion of the gradient  the EM iteration  and both at the same time are all EM equivalent iterations  This way we do not need to compare CPU times  which significantly depend on the implementation details of the different methods  All we need to do is optimize the methods with respect to EM equivalent iterations   h jx xT  h i   h jx    There are many different initialization methods  Some of them have been studied for large dimensional data in the context of the naive Bayesian Network model  Meila and Heckerman         For simplicity  we use the following initialization  Initialize o  as a uniform random sample from the space of all distribution over M events  Initialize fJ for each center by sampling uniformly at ran dom from the space defined by the hypercube of mini mum volume containing all the data points  Initialize L  for each center as a diagonal matrix with variances equal to the square of the distance to the center closest to it  Bishop            This is assuming that the complexity of exponentiation is         One remaining issue is when to stop  Ideally  an iterative method should stop when it has reached values for the pa rameters such that they provide a  good  model  However  there is no clear way for an iterative method to determine this  Therefore  detecting when to stop is a crucial but hard problem in general  For instance  it is common to encounter situations where the function we want to optimize has many areas of large and small changes towards the local opti mum  In tum  this causes some of the methods to produce burst of large and small improvements  which must be han dled by stopping rules  Many different stopping rules have been used in the optimization literature  Bertsekas         In this paper  we do not deal with the stopping problem and use a very simple  typical stopping rule based on the progress of the method in log likelihood  or log posterior  space    We stop when the change in log likelihood from one iteration to the next is less than       We can obtain the information we need to test this condition easily from the EM equivalent iteration  In our experiments on syn thetic data  all the methods that we tested converged to the same point in log likelihood space  and parameter sy ace  sometimes modulo symmetrically equivalent models    All the methods we tested  besides regular EM  EM   have the algorithmic structure presented above and only the ac celeration step differed  We tried the following accelera tions    regular conjugate gradient  CG      conjugate gradient with EM information  CG EM      parameterized EM with inexact line search to optimize the step size  PEM opt       parameterized EM with fixed step sizes  PEM l S   and      PEM           conjugate gradient with EM information on reparam eterized  w  space    CG EM rp          In order to examine the properties of the different methods  we tested them on data we generated from simple models with varying degree of separation between the Gaussians  We generated data from   models with   Gaussians in  dimensions  All   models had the same parameters o  and L   o     o          I     I     J   The first center f Jt for one of the Gaussians in the   models was also the same  J J             The models differed in the center of the second Gaussian      J J                J J            and     f J             We generated one data set of      points from each of the models  See Figure     We then generated    initial sets of parameters using the method presented above and    In theory  convergence speed in log likelihood space is faster  than in parameter space    In almost all cases  the methods converged to the same pa rameter values  i e   equivalent models  and therefore there was no need to compare them in terms of KL divergence of the result ing model from the true model    The EM algorithm in this case is actually a Generalized EM algorithm since there is no exact  i e   closed form  optimization in the maximization step    Accelerating EM  An Empirical Study  Data set         Data set    Data set   Figure    Data sets       and    Data Set  Method EM CG CG EM CG EM rp  PEM opt  PEM l    PEM l                       num  iters                                                               speed up       O D                                                                                                                                                                                                                               Table    This table presents the average number of EM equivalent iterations that each method took to converge on the different data sets for the first set of experiments  Also in the table are the  approximate      confidence intervals on the average speed up of the methods with respect to the number of iterations taken by EM  i e   speed up of a run  number of iterations of EM I number of iterations of acceleration for that run   ran each algorithm on each data set starting from each one of those initial parameters  Table   presents the results  For each data set  the results in the first column are the average number of EM equivalent iterations for each method  The results in the second column are the average speed up  We define the speed up achieved by a proposed acceleration method in a run as the number of iterations of EM divided by the number of EM equivalent iterations of the method for that run  Average speed up may be a better measure because it is not so drastically influenced by cases that are hard for everyone  We performed a bootstrap version  shift method  of the one sided paired sample test  Cohen        to compare the method with the best average number of iterations and or speed up with each of the other methods  For each data set  results are in bold for the method with the best empirical mean  The test did not reject the null hypothesis that the difference in mean was significant  p           K          with respect to the method with the best empirical mean only for the other methods with the results in bold  The results from this experiment show that accelerating with CG EM and CG EM rp  significantly improve con vergence speed as the Gaussians get closer together  When the Gaussians are farther apart  all accelerations except PEM l S  and PEM      can slow down convergence due to false starts of the acceleration  false signalings of close ness to a solution   However  the improvement in conver         Scatter Plot CG EM vs EM  Data  Set      runs                                                          Figure    Scatter plot of the number of EM equivalent iter ations of CG EM vs  the number iterations of EM on data set           Ortiz and Kaelbling  Data set     Data set    Figure    Data set      Method   M  Data Set        d           M      d     hard         M      d     easy                   speed up  num  iters   Table    and     EM       CG EM                                                  PEM opt                             PEM l                              PEM l                                                                                             This table presents the average number of EM equivalent iterations that each method took to converge and the   approximate      confidence intervals on the average speed up on the different data sets for the second set of experiments   gence speed provided by PEM l S  and PEM      is not as impressive as that ofCG EM and CG EM rp  in hard instances  Also  the slow downs produced by the attempted accelerations tend to occur mainly in easier instances  mod Seanar Plot PEM      vs  EM  Data Set       nme         els with means at               and                hard instances  model with means at                                                                the confidence intervals for the speed up for the fixed step  consistent          Figure  step size parameterized EM methods seems in general very                           presents a scatter plot of the behavior of CG EM in data set    Finally  note from Figure   and the small values of  size parameterized EM method that the behavior of fixed        of the prob  lem and are not as severe as the potential improvements in  Although the results are not reported in this paper  we also tried running EM on the reparameterized space alone and running conjugate gradient alone in all the experiments but they had much less success on average             We also ran experiments with models of more Gaussians        and or higher dimensions with similar results  Models       soo        rooo  S      t itera EM        and   are random models with different characteristics   Models   and   both have d      and M         They dif  fer in that in the data generated from model   it is harder to distinguish the different clusters than in that generated  Figure      Scatter plot of the number of EM equivalent it  erations of PEM      vs  the number of EM iterations on data set      This linear behavior is typical of fixed step size  parameterized EM on all the data set   from model  M     model         See Figure     Model is larger with d       We generated a data set of        points from           points from model    and       points    presents the        and    random  from model    Table  results  The results  are averages of  initial settings of the  parameters for models       and   respectively  Again  we  have CG EM being superior in the hard case  and while  it might slow down in the easy cases  the slow down does not seem that severe  We note again that PEM l S  and   Accelerating EM  An Empirical Study  Method     EM  CG EM PEM l          IRAS data results num  iters  speed up                             Table    This table presents the average number of EM equivalent iterations and speed up that each method took to converge on the IRAS data set starting with initially M                                                   nlll lberol iterations                          Figure    Histogram of the number of iteration of EM on data set    PEM      seem almost consistently better than EM  Fi nally  we note that there are   runs missing from the re sults in the table for model    For one of those runs EM took      iterations on that run compared to      for PEM l S        for PEM      and       for PEM opt   We stopped CG EM when it had more iterations than all the others  CG EM and PEM opt  were failing during the line search because they were running out of time    The value of the log likelihood for the point where all the methods converged in that run was smaller than the most common one  We suspect that this is a saddle point or some flat region in log likelihood space  For the other run  textbfEM took      iterations on that run  compared to     for CG EM       for PEM opt        for PEM l S   and      for PEM       Inspection of the behavior of PEM opt  showed that the method was wasting time be cause the line searches were failing almost immediately  and therefore going back to EM immediately after each at tempt  For this run  however  the point where the methods converged was the most common point in log likelihood space  Finally  we note that the behavior of both of these two runs was uncommon as suggested by the histogram of the number of iterations of EM for this data set in Figure    the two largest values in the histogram are for the runs men tioned above  As an anonymous reviewer pointed out  the stopping rule we use does not have a scale  Therefore  it is insensitive to the range of the log likelihood function  Restating the issue of stopping rules  we note that partial preliminary experi ments suggest that a scaled stopping rule   can indeed help reduce the number of iterations required by EM and PEM in hard instances  In such cases  the log likelihood func   The inexact line search that we used had  a  optimum num  ber of trials before failing which was set to     Jamshidian and Jennrich           For instance  as suggested by that anonymous reviewer  we stop when the change in log likelihood at one iteration relative to  the total change so far is smaller than some threshold  i e    w     tion is ill conditioned and stopping anywhere in the rela tively flat region close to the solution produces very good estimates with regard to maximizing log likelihood  It can also help in preventing over fitting when the amount of data is small  a very important issue when we are learning mod els from data  However  partial preliminary experiments also suggest that  unless we use different threshold values for different instances  a scaled stopping rule increases the potential for stopping too early in easier instances  thus producing bad estimates  Other stopping rules have been used for EM  but we do not know of any study that has been done to compare them  Finally  we conjecture that the  right  stopping rule eliminates the need for most of the type of accelerations proposed since it eliminates the basis for them  This is because the proposed accelerations work well when we are close to a solution and the problem is ill conditioned  which is exactly what the  right  stopping rule would detect  We also ran experiments on the Infrared Astronomical Satellite  IRAS  data set  Cheeseman et al         Cheese man and Stutz         This data set contains N        data points in d      dimensions  AutoClass found M      classes using a mixture of Gaussians model with independent features  Given the high dimensionality and other problems that this data poses  we performed MAP es timation over a model with independent features and took careful steps in the way we computed the expected suffi cient statistics and the value of the change in log posterior  Therefore  our implementation is similar to that of Auto Class  We assumed a priori that the parameters were independent  We used a Dirichlet prior on the a parameters with the same prior counts which we set to   M       M  We used a uniform prior over the hypercube of minimum volume containing all the data points for the mean parameters of the Gaussian components  We used a  scaled  inverse x  prior for the variances with   M degrees of freedom and scale    In some instances  the update rule took us close to the boundary constraint  Our solution to this problem was to eliminate components if their probability was less than   N and or at least one of their variances was less than         Once we removed those invalid components  we restarted the method using the value of the parameters of the remaining components as the initial value of the pa rameters         Ortiz and Kaelbling  We ran experiments assuming a mixture of Gaussians model with initially    components  We generated random initial parameters using an adapted version of the initializa tion procedure used by AutoClass C and ran each method starting from each one of the initial parameters  Table   presents the results based on   runs  First  we note that EM is very stable at the beginning and makes most of its progress in the first         iterations  This behavior has also been noted by others  Redner and Walker        Xu and Jordan         Fixed step size parameterized EM tends to perform best in this data set with respect to speed of convergence  Conjugate gradient acceleration of EM can slow down convergence  To understand this result we note that the shape of the log posterior seems to have many val leys  as suggested by the typical behavior of EM on this data as shown in Figure    Therefore  we make many false signalings of closeness and start the acceleration before it is really close to a solution  The methods that use line searches wasted time  since most of the line searches fail and therefore the attempt to accelerate fails  Again  this type of behavior suggests that we need better ways of sig naling closeness or stopping  Preliminary analysis seems to indicate that this behavior of EM does not simply result from performing MAP estimation  Therefore  we wonder whether the cause of this is related to the fact that the ratio of the number of parameters to the number of samples is not small enough  or this is just a consequence of the high dimensionality  or both        a   t     CONCLUSIONS  First of all  we note  as many others authors have before us  that the performance of EM away from the solution is im pressive  particularly in log likelihood  and log posterior  space  Given the  right  stopping rule for a problem  it typically produces reasonable estimates relatively fast  In addition  it typically exhibits global convergence   in prac tice  These properties  along with its monotonicity prop erty and its simplicity  make EM a very powerful method and a first choice for finding ML  or MAP  estimates in the context of the mixture of Gaussians and other probabilistic models  Nevertheless  using the assumption that the amount of data is sufficiently large such that the extra computation of the gradient and the EM direction is not significant and a simple albeit conservative stopping rule  our exper imental results on synthetic data suggest that the method based on conjugate gradient acceleration of EM can be a good choice for finding ML estimates for the mixture of Gaussians model  This is because it significantly improves convergence in the hard cases and  while it can slow down convergence in the easy cases  the slow down is not as se vere  given the relatively low number of iterations required to converge in those cases  In addition  although it is more   Numerical instability can certainly be a reason too    Here  global convergence means the property of an optimiza tion method to be able to converge to a stationary point  not nec essarily a global optimum  in function space from any initial point in the parameter space    b   EM behavior altar  liQnallng clo  nBSS   c  Figure    Plots of  a  the log posterior  without constants  divided by N   b  the change in log posterior and  c  the relative change in log posterior after signaling closeness for a typical run of EM on the IRAS data set  We plot the log posterior N as opposed to the log posterior itself to reduce the scale of the y axis  The relative change is the current change divided by the previous change and can be used as an approximation of the convergence rate    Accelerating EM  An Empirical Study  complicated to implement than parameterized EM  it elim inates the setting of the step size parameter  Furthermore  although it requires line searches  those searches can be simple and inexact in a neighborhood close to a solution  Finally  the behavior of conjugate gradient acceleration of EM seems best when the function is smooth but very flat and  elongated  in the neighborhood of the local optimum  On the other hand  the results from the experiments on the IRAS data suggest that it is a good idea to attempt a sim ple acceleration method  such as fixed step size parameter ized EM  before trying the more complicated conjugate gradient based accelerations  This is because there are cases in which the surface of the log likelihood  or log posterior  is relatively flat but not very smooth in the neigh borhood of the local optimum  We can attempt those more complicated methods once we note that the simple acceler ation method is still too slow  We think that it is necessary to perform a similar compar ison analysis in the context of learning Bayesian networks and HMMs to verify that the same characterization of su periority of the accelerations based on  easy  and  hard  instances suggested in this paper carries over  
 We apply decision theoretic techniques to construct nonplayer characters that are able to assist a human player in collaborative games  The method is based on solving Markov decision processes  which can be difficult when the game state is described by many variables  To scale to more complex games  the method allows decomposition of a game task into subtasks  each of which can be modelled by a Markov decision process  Intention recognition is used to infer the subtask that the human is currently performing  allowing the helper to assist the human in performing the correct task  Experiments show that the method can be effective  giving nearhuman level performance in helping a human in a collaborative game   Introduction Traditionally  the behaviour of Non Player Characters  NPCs  in games is hand crafted by programmers using techniques such as Hierarchical Finite State Machines  HFSMs  and Behavior Trees  Champandard        These techniques sometimes suffer from poor behavior in scenarios that have not been anticipated by the programmer during game construction  In contrast  techniques such as Hierarchical Task Networks  HTNs  or Goal Oriented Action Planner  GOAP   Orkin       specify goals for the NPCs and use planning techniques to search for appropriate actions  alleviating some of the difficulties of having to anticipate all possible scenarios  In this paper  we study the problem of creating NPCs that are able to help players play collaborative games  The main difficulties in creating NPC helpers are to understand the intention of the human player and to work out how to assist the player  Given the successes of planning approaches to simplifying game creation  we examine the application of planning techniques to the collaborative NPC creation problem  In particular  we extend a decision theoretic framework Copyright c       Association for the Advancement of Artificial Intelligence  www aaai org   All rights reserved   for assistance used in  Fern and Tadepalli       to make it appropriate for game construction  The framework in  Fern and Tadepalli       assumes that the computer agent needs to help the human complete an unknown task  where the task is modeled as a Markov decision process  MDP   Bellman        The use of MDPs provide several advantages such as the ability to model noisy human actions and stochastic environments  Furthermore  it allows the human player to be modelled as a noisy utility maximization agent where the player is more likely to select actions that has high utility for successfully completing the task  Finally  the formulation allows the use of Bayesian inference for intention recognition and expected utility maximization in order to select the best assistive action  Unfortunately  direct application of this approach to games is limited by the size of the MDP model  which grows exponentially with the number of characters in a game  To deal with this problem  we extend the framework to allow decomposition of a task into subtasks  where each subtask has manageable complexity  Instead of inferring the task that the human is trying to achieve  we use intention recognition to infer the current subtask and track the players intention as the intended subtask changes through time  For games that can be decomposed into sufficiently small subtasks  the resulting system can be run very efficiently in real time  We perform experiments on a simple collaborative game and demonstrate that the technique gives competitive performance compared to an expert human playing as the assistant   Scalable Decision Theoretic Framework We will use the following simple game as a running example  as well as for the experiments on the effectiveness of the framework  In this game  called Collaborative Ghostbuster  the assistant  illustrated as a dog  has to help the human kill several ghosts in a maze like environment  A ghost will run away from the human or assistant when they are within its vision limit  otherwise it will move randomly  Since ghosts can only be shot by the human player  the dogs   role is strictly to round them up  The game is shown in Figure    Note that collaboration is often truly required in this game   without surrounding a ghost with both players in order to cut off its escape paths  ghost capturing can be quite difficult   This algorithm is guaranteed to converge to the optimal value function V   s   which gives the expected cumulative reward of running the optimal policy from state s  The optimal value function V  can be used to construct the optimal actionsPby taking action a in state s such that a   argmaxa   s  Ta  s  s   V   s      The optimal Qfunction is constructed from V  as follows  X Q  s  a    Ta  s  s    Ra  s  s      V   s      s   The function Q  s  a  denotes the maximum expected longterm reward of an action a when executed in state s instead of just telling how valuable a state is  as does V     Figure    A typical level of Collaborative Ghostbuster  The protagonists  Shepherd and Dog in the bottom right corner  need to kill all three ghosts to pass the level   Markov Decision Processes We first describe a Markov decision process and illustrate it with a Collaborative Ghostbuster game that has a single ghost  A Markov decision process is described by a tuple  S  A  T  R  in which  S is a finite set of game states  In single ghost Collaborative Ghostbuster  the state consists of the positions of the human player  the assistant and the ghost   A is a finite set of actions available to the players  each action a  A could be a compound action of both players  If each of the human player and the assistant has   moves  north  south  east and west   A would consist of the    possible combination of both players moves   Ta  s  s      P  st     s   st   s  at   a  is the probability that action a in state s at time t will lead to state s  at time t      The human and assistant move deterministically in Collaborative Ghostbuster but the ghost may move to a random position if there are no agents near it   Ra  s  s    is the immediate reward received after the state transition from s to s  triggered by action a  In Collaborative Ghostbuster  a non zero reward is given only if the ghost is killed in that move  The aim of solving an MDP is to obtain a policy maximizes the expected cumulative reward P that t t    R st    st   st     where          is the discount factor  Value Iteration  An MDP can be effectively solved using a simple algorithm proposed by Bellman in       Bellman        The algorithm maintains a value function V  s   where s is a state  and iteratively updates the value function using the equation   X       Vt    s    max Ta  s  s   Ra  s  s     Vt  s      a  s   Intractability  One key issue that hinders MDPs from being widely used in real life planning tasks is the large state space size  usually exponential in the number of state variables  that is often required to model realistic problems  Typically in game domains  a state needs to capture all essential aspects of the current configuration and may contain a large number of state variables  For instance  in a Collaborative Ghostbuster game with a maze of size m  number of valid positions  consisting of a player  an assistant and n ghosts  the set of states is of size O mn      which grows exponentially with the number of ghosts   Subtasks To handle the exponentially large state space  we decompose a task into smaller subtasks and use intention recognition to track the current subtask that the player is trying to complete   Figure    Task decomposition in Collaborative Ghostbuster  In Collaborative Ghostbuster  each subtask is the task of catching a single ghost  as shown in Figure    The MDP for a subtask consists of only two players and a ghost and hence has manageable complexity    Human Model of Action Selection In order to assist effectively  the AI agent must know how the human is going to act  Without this knowledge  it is almost impossible for the AI to provide any help  We assume that the human is mostly rational and use the Q function to model the likely human actions  Specifically  we assume maxaAI Q i  si  ahuman  aAI    P  ahuman  wi   si      e     where  is the normalizing constant  wi represents subtask i and si is the state in subtask i  Note that we assume that the human player knows the best response from the AI sidekick and plays his part in choosing the action that matches the most valued action pair  However  the human action selection can be noisy  as modelled by Equation       Intention Recognition and Tracking We use a probabilistic state machine to model the subtasks for intention recognition and tracking  At each time instance  the player is likely to continue on the subtask that he or she is currently pursuing  However  there is a small probability that the player may decide to switch subtasks  This is illustrated in Figure    where we model a human player who tends to stick to his chosen sub goal  choosing to solve the current subtask     of the times and switching to other sub tasks     of the times  The transition probability distributions of the nodes need not be homogeneous  as the human player could be more interested in solving some specific subtask right after another subtask  For example  if the ghosts need to be captured in a particular order  this constraint can be encoded in the state machine  The model also allows the human to switch back and forth from one subtask to another during the course of the game  modelling change of mind   where T  wj  wi   is the switching probability from subtask j to subtask i  Next  we compute the posterior belief distribution using Bayesian update  after observing the human action a and subtask state si t at time t  as follows  Bt  wi  at   a  st   t       Bt  wi  t    P  at   a wi   si t       where  is a normalizing constant  Absorbing current human action a and current state into t  gives us the game history t at time t  Complexity This component is run in real time  and thus its complexity dictates how responsive our AI is  We are going to show that it is at most O k      with k being the number of subtasks  The first update step as depicted in Equation   is executed for all subtasks  thus of complexity O k      The second update step as of Equation   requires the computation of P  at   a wi   si    Equation     which takes O  A   with A being the set of compound actions  Since Equation   is applied for all subtasks  that sums up to O k A   for this second step  In total  the complexity of our real time Intention Recognition component is O k     k A    which will be dominated by the first term O k     if the action set is fixed   Decision theoretic Action Selection Given a belief distribution on the players targeted subtasks as well as knowledge to act collaboratively optimally on each of the subtasks  the agent chooses the action that maximizes its expected reward      X i  Bt  wi  t  Qi  st   a  a   argmaxa i  CAPIR  Collaborative Action Planner with Intention Recognition We implement the scalable decision theoretic framework as a toolkit for implementing collaborative games  called Collaborative Action Planner with Intention Recognition  CAPIR   Figure    A probabilistic state machine  modeling the transitions between subtasks  Belief Representation and Update The belief at time t  denoted Bt  wi  t    where t is the game history  is the conditional probability of that the human is performing subtask i  The belief update operator takes Bt   wi  t    as input and carries out two updating steps  First  we obtain the next subtask belief distribution  taking into account the probabilistic state machine model for subtask transition T  wk  wi   X Bt  wi  t      T  wj  wi  Bt   wj  t        j  CAPIRs Architecture Each game level in CAPIR is represented by a GameWorld object  which consists of two Players and multiple SubWorld objects  each of which contains only the elements required for a subtask  Figure     The game objective is typically to interact with these NPCs in such a way that gives the players the most points in the shortest given time  The players are given points in major events such as successfully killing a monster type NPC or saving a civilian type NPC  these typically form the subtasks  Each character in the game  be it the NPC or the protagonist  is defined in a class of its own  capable of executing multiple actions and possessing none or many properties  Besides movable NPCs  immobile items  such as doors or   Figure    GameWorlds components  shovels  are specified by the class SpecialLocation  GameWorld maintains and updates an internal game state that captures the properties of all objects  At the planning stage  for each SubWorld  an MDP is generated and a collaboratively optimal action policy is accordingly computed  Figure     These policies are used by the AI assistant at runtime to determine the most appropriate action to carry out  from a decision theoretic viewpoint                                                                                                                                                                                                                     Figure    CAPIRs action planning process   a  Offline subtask Planning   b  in game action selection using Intention Recognition   busters  We chose five levels  see Appendix  with roughly increasing state space size and game play complexity to assess how the technique can scale with respect to these dimensions  The participants were requested to play five levels of the game as Shepherd twice  each time with a helping Dog controlled by either AI or a member of our team  the so called human expert in playing the game  The identity of the dogs controller was randomized and hidden from the participants  After each level  the participants were asked to compare the assistants performance between two trials in terms of usefulness  without knowing who controlled the assistant at which turn  In this set of experiments  the players aim is to kill three ghosts in a maze  with the help of the assistant dog  The ghosts stochastically  run away from any protagonists if they are   steps away  At any point of time  the protagonists could move to an adjacent free grid square or shoot  however  the ghosts only take damage from the ghost buster if he is   steps away  This condition forces the players to collaborate in order to win the game  In fact  when we try the game with non collaborative dog models such as random movement  the result purely relies on chance and could go on until the time limit      steps  runs out  as the human player hopelessly chases ghosts around obstacles while the dog is doing some nonsense at a corner  Oftentimes the game ends when ghosts walk themselves into dead end corners  The twenty participants are all graduate students at our school  seven of whom rarely play games  ten once to twice a week  and three more often  When we match the answers back to respective controllers  the comparison results take on one of three possible values  being AI assistant performing better  worse or indistinguishable to the human counterpart  The AI assistant is given a score of   for a better    for an indistinguishable and    for a worse evaluation  Qualitative evaluation For simpler levels      and    our AI was rated to be better or equally good more than     the times  For level    our AI rarely got the rating of being indistinguishable  though still managed to get a fairly competitive performance  Subsequently  we realized that in this particular level  the map layout is confusing for the dog to infer the humans intention  there is a trajectory along which the human players movement could appear to aim at any one of three ghosts  In that case  the dogs initial subtask belief plays a crucial role in determining which ghost it thinks the human is targeting  Since the dogs belief is always initialized to a uniform distribution  that causes the confusion  If the human player decides to move on a different path  the AI dog is able to efficiently assist him  thus getting good ratings instead  In level    our AI gets good ratings only for less than one third of the times  but if we count indistinguishable ratings as satisfactory  the overall percentage of positive ratings exceeds       Experiment and Analysis In order to evaluate the performance of our AI system  we conducted a human experiment using Collaborative Ghost     The ghosts run away     of the times and perform some random actions in the remaining                                                                                                             Figure    Qualitative comparison between CAPIRs AI assistant and human expert  The y axis denotes the number of ratings                          AI              Human                                                             Figure    Average time  with standard error of the mean as error bars  taken to finish each level when the partner is AI or human  The y axis denotes the number of game turns  Quantitative evaluation Besides qualitative evaluation  we also recorded the time taken for participants to finish each level  Figure     Intuitively  a well cooperative pair of players should be able to complete Collaborative Ghostbusters levels in shorter time  Similar to our qualitative result  in levels      and    the AI controlled dog is able to perform at near human levels in terms of game completion time  Level    which takes the AI dog and human player more time on average and with higher fluctuation  is known to cause confusion to the AI assistants initial inference of the humans intention and it takes a number of game turns before the AI realizes the true target  whereas our human expert is quicker in closing down on the intended ghost  Level    larger and with more escape points for the ghosts but less ambiguous  takes the protagonist pair  AI  human  only      more on average completion time   Related Work Since plan recognition was identified as a problem on its own right in       Schmidt  Sridharan  and Goodson        there have been various efforts to solve its variant in different domains  In the context of modern game AI research  Bayesian based plan recognition has been inspected using  different techniques such as Input Output Hidden Markov Models  Gold        Plan Networks  Orkin and Roy        text pattern matching  Mateas and Stern        n gram and Bayesian networks  Mott  Lee  and Lester       and dynamic Bayesian networks  Albrecht  Zukerman  and Nicholson        As far as we know  our work is the first to use a combination of precomputed MDP action policies and online Bayesian belief update to solve the same problem in a collaborative game setting  Related to our work in the collaborative setting is the work reported by Fern and Tadepalli  Fern and Tadepalli       who proposed a decision theoretic framework of assistance  There are however several fundamental differences between their targeted problem and ours  Firstly  they assume the task can be finished by the main subject without any help from the AI assistant  This is not the case in our game  which presents many scenarios in which the effort from one lone player would amount to nothing and a good collaboration is necessary to close down on the enemies  Secondly  they assume a stationary human intention model  i e  the human only has one goal in mind from the start to the end of one episode  and it is the assistants task to identify this sole intention  In contrary  our engine allows for a more dynamic human intention model and does not impose a restriction on the freedom of the human player to change his mind mid way through the game  This helps ensure our AIs robustness when inferring the human partners intention  In a separate effort that also uses MDP as the game AI backbone  Tan and Cheng  Tan and Cheng       model the game experience as an abstracted MDP   POMDP couple  The MDP models the game worlds dynamics  its solution establishes the optimal action policy that is used as the AI agents base behaviors  The POMDP models the human play style  its solution provides the best abstract action policy given the human play style  The actions resulting from the two components are then merged  reinforcement learning is applied to choose an integrated action that has performed best thus far  This approach attempts to adapt to different human play styles to improve the AI agents performance  In contrast  our work introduces the multi subtask model with intention recognition to directly tackle the intractability issue of the game worlds dynamics   Conclusions We describe a scalable decision theoretic approach for constructing collaborative games  using MDPs as subtasks and intention recognition to infer the subtask that the player is targeting at any time  Experiments show that the method is effective  giving near human level performance  In the future  we also plan to evaluate the system in more familiar commercial settings  using state of the art game platforms such as UDK or Unity  These full fledged systems offer development of more realistic games but at the same time introduce game environments that are much more complex to plan  While experimenting with Collaborative Ghostbuster  we have observed that even though Value Iteration is a simple naive approach  in most cases  it suffices  converging in reasonable time  The more serious issue is the   state space size  as tabular representation of the states  reward and transition matrices takes much longer to construct  We plan to tackle this limitation in future by using function approximators in place of tabular representation   Appendix Game levels used for our experiments   Acknowledgments This work was supported in part by MDA GAMBIT grant R                and AcRF grant T     RES     in Singapore  The authors would like to thank Qiao Li  NUS   Shari Haynes and Shawn Conrad  MIT  for their valuable feedbacks in improving the CAPIR engine  and the reviewers for their constructive criticism on the paper       
 The ways in which an agents actions affect the world can often be modeled compactly using a set of relational probabilistic planning rules  This paper addresses the problem of learning such rule sets for multiple related tasks  We take a hierarchical Bayesian approach  in which the system learns a prior distribution over rule sets  We present a class of prior distributions parameterized by a rule set prototype that is stochastically modified to produce a task specific rule set  We also describe a coordinate ascent algorithm that iteratively optimizes the task specific rule sets and the prior distribution  Experiments using this algorithm show that transferring information from related tasks significantly reduces the amount of training data required to predict action effects in blocks world domains      Introduction  One of the most important types of knowledge for an intelligent agent is that which allows it to predict the effects of its actions  For instance  imagine a robot that performs the familiar task of retrieving items from cabinets in a kitchen  This robot needs to know that if it grips the knob on a cabinet door and pulls  the door will swing open  if it releases its grip when the cabinet is only slightly open  the door will probably swing shut  and if it releases its grip when the cabinet is open nearly    degrees  the door will probably stay open  Such knowledge can be encoded compactly as a set of probabilistic planning rules  Kushmerick et al         Blum and Langford         Each rule specifies a probability distribution over sets of changes that may occur in the world when an action is executed and certain preconditions hold  To represent domains concisely  the rules must be relational rather than propositional  for example  they must make statements about cabinets in general rather than individual cabinets   Luke S  Zettlemoyer MIT CSAIL Cambridge  MA       lsz csail mit edu  Leslie Pack Kaelbling MIT CSAIL Cambridge  MA       lpk csail mit edu  Algorithms have been developed for learning relational probabilistic planning rules by observing the effects of actions  Pasula et al         Zettlemoyer et al          But with current algorithms  if a robot learns planning rules for one kitchen and then moves to a new kitchen where its actions have slightly different effects  because  say  the cabinets are built differently   it must learn a new rule set from scratch  Current rule learning algorithms fail to capture an important aspect of human learning  the ability to transfer knowledge from one task to another  We address this transfer learning problem in this paper  In statistics  the problem of transferring predictions across related data sets has been addressed with hierarchical Bayesian models  Lindley         The first use of such models for the multi task learning problem appears to be due to Baxter         the approach has recently become quite popular  Yu et al         Marx et al         Zhang et al          The basic idea of hierarchical Bayesian learning is to regard the task specific models R            RK as samples from a global prior distribution G  This prior distribution over models is not fixed in advance  but is learned by the system  thus  the system discovers what the task specific models have in common  However  applying the hierarchical Bayesian approach to sets of first order probabilistic planning rules poses both conceptual and computational challenges  In most existing applications  the models Rk are represented as real valued parameter vectors  and the hypothesis space for G is a class of priors over real vectors  But a rule set is a discrete structure that may contain any number of rules  and each rule includes a precondition and a set of outcomes that are represented as arbitrary length conjunctions of first order literals  How can we define a class of prior distributions over such rule sets  Our proposal is to let G be defined by a rule set prototype that is modified stochastically to create the task specific rule sets  Our goal is to take data from K source tasks  plus a limited set of examples from a target task K      and find the  rule set RK   for the target task with the greatest posterior probability  In principle  this involves integrating out the       DESHPANDE ET AL   pickup X  Y                           on X  Y    clear X   inhand nil  block Y    wet inhand X   clear X   inhand nil       on X  Y    clear Y        on X  TABLE   on X  Y         no change       noise  pickup X  Y                                             s in sentence   and a   pickup B A  B B   both of the rules in Fig    would have the binding     X B A  Y  B B   The first rule would apply  since its preconditions are all satisfied  while the second one would not because wet is not true in s  We disallow rule sets in which two or more rules apply to the same  s  a  pair  these are called overlapping rules   In cases where no rules apply  a default rule is used that has an empty context and two outcomes  no change and noise  which will be described shortly   on X  Y    clear X   inhand nil  block Y    wet inhand X   clear X   inhand nil  on X  Y    clear Y   on X  TABLE   on X  Y   no change noise  Figure    Two rules for the pickup action in the slippery gripper blocks world domain  other rule sets R            RK and the rule set prototype G  As an approximation  however  we use estimates of G and  found by a greedy local search algorithm  We R            RK present experiments with this algorithm on blocks world tasks  showing that transferring data from related tasks significantly reduces the number of training examples required to achieve high accuracy on a new task      Probabilistic Planning Rules  Probabilistic planning rule sets define a state transition distribution p st  st    at    In this section  we present a simplified version of the representation developed by  Zettlemoyer et al          A state st is represented by a conjunctive formula with constants denoting objects in the world and proposition and function symbols representing the objects properties and relations  The sentence inhand nil  on B A  B B   on B B  TABLE   clear B A  block B A   block B B   table TABLE        represents a blocks world where the gripper holds nothing and the two blocks are in a single stack on the table  This is a full description of the world  all of the false literals are omitted for compactness  Block B A is on top of the stack  while B B is below B A and on the table TABLE  Actions at are ground literals where the predicate names the action to be performed and the arguments are constant terms that correspond to the objects which will be manipulated  For example  at   pickup B A  B B  would represent an attempt to pick block B A up off of block B B  Each rule r has two parts that determine when it is applicable  an action z and a context  that encodes a set of preconditions  Both of the rules in Fig    model the pickup X  Y   action  Given a particular state st  and action a  we can determine whether a rule applies by computing a binding  that finds objects for all the variables  by matching against a  and then testing whether the preconditions hold for this binding  For example  for the state  Given the applicable rule r  the discrete distribution p over outcomes O  described on the right of the   defines what changes may happen from st  to st   Each non noise outcome o  O implicitly defines a successor state function fo with associated probability po   an entry in p  The function fo builds st from st  by copying st  and then changing the values of the relevant literals in st to match the corresponding values in  o   In our running example of executing pickup B A  B B  in sentence    for the first outcome of the first rule  where the picking up succeeds  fo would set five truth values  including setting on B A  B B  to be false  In the third outcome  which indicates no change  fo is the identity function  In this paper  we will enforce the restriction that outcomes do not overlap  for each pair of outcomes o  and o  in a rule r  there cannot exist a stateaction pair  s  a  such that r is applicable and fo   s    fo   s   In other words  if we observe the state that results from applying a rule  then there is no ambiguity about which outcome occurred   Finally  the noise outcome is treated as a special case  There is no associated successor function  which allows the rule to define a type of partial model where r does not describe how to construct the next state with probability pnoise   Noise outcomes allow rule learners to ignore overly complex  rare action effects and have been shown to improve learning in noisy domains  Zettlemoyer et al          Since rules with noise outcomes are partial models  the distribution p st  st    at   is replaced with an approximation   p st  st    at      po pnoise pmin  if fo  st      st otherwise       where the set of possible outcomes o  O is determined by the applicable rule  The probabilities po and pnoise make up the parameter vector p  The constant pmin can be viewed as an approximation to a distribution p st  st    at   onoise   that would provide a complete model      Hierarchical Bayesian Model  In a hierarchical Bayesian model  as illustrated in Fig     the data points xkn in task k come from a task specific dis   This restriction simplifies parameter estimation  as we will see in Sec     without limiting the class of transition distributions that can be defined  Any rule with overlapping outcomes can be replaced by an equivalent set of rules applying to more specific contexts  with non overlapping outcomes    DESHPANDE ET AL  G  R        R   x n  x n N   RK  xKn N   NK  Figure    A hierarchical Bayesian model with K tasks  where the number of examples for task k is Nk   tribution p xkn  Rk    and the task specific parameters Rk are in turn modeled by a prior distribution p Rk  G   The hyperparameter G has its own prior distribution p G   By observing data from the first K tasks  the learner gets information about R            RK and hence about G  For instance  the learner can compute  perhaps approximately   the values  R            RK   G   that have maximum a posteriori  MAP  probability given the data on the first K tasks  Then when it encounters task K     the learners estimates of the task specific model RK   are influenced by both the data observed for task K     and the prior p RK    G    which captures its expectations about the model based on the preceding tasks       Rule Set Prototypes  In the context of learning planning rules  the task specific models Rk are rule sets  Our intuition is that if the tasks are related  then these rule sets have some things in common  Certain rules may appear in the rule sets for many tasks  perhaps with some modifications to their contexts  outcomes  and outcome probabilities  To capture these commonalities  we assume that the rule sets are all generated from an underlying rule set prototype G  A rule set prototype consists of a set of rule prototypes  A rule prototype is like an ordinary rule  except that rather than specifying a probability distribution over its outcomes  it specifies a vector of Dirichlet parameters that define a prior over outcome distributions  For a rule prototype with n explicit outcomes  this is a vector  of n   non negative real numbers  n   corresponds to a special seed outcome on   that generates new outcomes in local rules  and n   accounts for the noise outcome  Unlike in local rule sets  we allow overlapping rules and outcomes in rule set prototype to allow for better generalization           can be found by identifying the single rule in Rk that applies to  st    at    or the default rule  if no explicit rule applies  and using Eq     Then theQprobability of the entire Nk data set for task k is p xk  Rk     n   p xkn  Rk    The distribution for G and R            Rk is defined by a generative process that first creates G  and then creates R            Rk by modifying G  Note that this generative process is purely a conceptual device for defining our probability model  we never actually draw samples from it  As we will see in Sec     our learning algorithm uses the generative model solely to define a scoring function for evaluating rule sets and prototypes  Two difficulties arise in using our generative process to define a joint distribution  One is that the process can yield rule sets Ri that are invalid  in the sense of containing overlapping rules or outcomes  It is difficult to design a generative process that avoids creating invalid rule sets  but still allows the probability of a rule set to be computed efficiently  Intuitively  we want to discard runs of the generative process that yield invalid rule sets  The other difficulty is that there may be many possible runs of a generative process that yield the same rule set  For instance  as we will see  a rule set prototype is generated by choosing a number m  generating a sequence of m rule prototypes independently  and then returning the set of distinct rule prototypes that were generated  In principle  a set of m distinct rules could be created by generating a list of any length m  m  with duplicates   we do not want to force ourselves to sum over all these possibilities to compute the probability of a given rule set prototype  Again  it is convenient to discard certain non canonical runs of the generative process  in this case  runs where the same rule prototype is generated twice  Thus  we will define measures PG  G  and Pmod  Rk  G  that give the probability of generating a rule set prototype G  or a rule set Rk   through a valid sampling run  Because some runs are considered invalid  these measures do not sum to one  The resulting joint distribution is  p G  R            RK   x            xK     K  Y   PG  G  Pmod  Rk  G p xk  Rk   Z       k    The normalization constant Z is the total probability of valid runs of our generative process  Since we are just interested in the relative probabilities of hypotheses  we never need to compute this normalization constant    Overview of Model  Our hierarchical model defines a joint probability distribution p G  R            RK   x            xK    In our setting  each example xkn is a state st obtained by performing a known action at in a known initial state st    Thus  p xkn  Rk       One might be tempted to define a model where the normalization is more local  for instance  to replace the factor Pmod  Rk  G  in Eq    with a normalized distribution Pmod  Rk  G  Z G   However  the normalization factor Z G  is not constant  so it would have to be computed to compare alternative values of G        DESHPANDE ET AL        set R of size m from a prototype G of size m is   Modifying the Rule Set Prototype  Pmod  R G     We begin the discussion of our generative process by describing how a rule set prototype G is modified to create a rule set R  the process that generates G will be a simplified version of this process   The first step is to choose the rule set size m from a distribution Pnum  m m    where m is the number of rule prototypes in G  We define Pnum  m m   so that all natural numbers have non zero probability  but m is likely to be close to m   and the probability drops off geometrically for greater values of m   Pnum  m m      Geom   m  m        Binom m     m   if m   m otherwise       Here Geom   is a geometric distribution with success probability   Thus  if m   m   then Pnum  m m            mm     We set  to a small value to discourage the rule set R from being much larger than G  The sum of the Geom   distribution over all values greater than zero is   leaving a probability mass of     to be apportioned over rule set sizes from   through m   The binomial distribution Binom m      which yields the probability of getting exactly m heads when flipping m coins with heads probability   is a convenient distribution over this range of integers  We set  to a value close to   to express a preference for local rule sets that are not much smaller than the prototype set  Next  for i     to m  we generate a local rule ri   The first step in generating ri is to choose which rule prototype in G it will be derived from  This choice is represented by an assignment variable Ai   whose value is either a rule prototype in G  or a special value NIL indicating that this rule is generated from scratch with no prototype  The distribution PA  ai  G  assigns the probability rule to NIL and spreads the remaining mass uniformly over the rule prototypes  Since the Ai are chosen independently  a single rule in G may serve as the prototype for several rules in R  or for none  Next  given the rule prototype  or null value  ai   the local rule ri is generated according to a distribution Prule  ri  ai    We discuss this distribution in Section      The rule set generated by this process is the set of distinct rules in the list r            rm   We consider a run of the generative process to be invalid if any of these rules have overlapping contexts  in particular  this constraint rules out cases where the same rule occurs twice  So the probability of generating a set  r            rm   on a valid run is the sum of the probabilities of all permutations of this set  This is m  times the probability of generating the rules in any particular order  Thus  the probability of getting a valid local rule  Pnum  m m    m    m Y i         X  PA  ai  G Prule  ri  ai         ai  G NIL   Modifying and Creating Rules  We will now define the distribution Prule  r r    where r may be either a rule prototype  or the value NIL  indicating that r is generated from scratch  Suppose r consists of a context formula   an action term z  a set of non noise outcomes O  and a probability vector p  The corresponding parts of r will be referred to as    z    O   and   recall that this last component is a vector of Dirichlet parameters   If r   NIL  then  is an empty formula  z  is NIL  O consists of just the seed outcome  and  is a two element vector consisting of a   for the seed outcome and a   for the noise outcome  For rules derived from a rule prototype  we assume the action term is unchanged  So if z  is not NIL  we use the distribution Pact  z z    that assigns probability one to z    If a rule is generated from scratch  we need to generate its action term  For simplicity  we assume that each action term consists of an action symbol and a distinct logical variable for each argument  we do not allow repeated variables or more complex terms in the argument list  The distribution Pact  z z    chooses the action term uniformly from the set of such terms when z    NIL  The next step in generating r is to choose its context   We define the distribution for  by means of a general formulamodification distribution Pfor      v   where v is the set of logical variables that occur in z and thus are eligible to be included in   This distribution is explained in Sec       To generate the outcome set O from O   we use essentially the same method we used to generate the rule set R from G  We begin by choosing the size n of the outcome set from the distribution Pnum  n n    where n    O    The distribution Pnum here is the same one used in Sec       one could use different  and  parameters here   Then  for i     to n  we choose which prototype outcome serves as the source for the ith local outcome  This choice is represented by an assignment variable Bi   As in the case of rules  we allow some local outcomes to be generated from scratch rather than from a prototype  this choice is represented by the seed outcome  The value of Bi is chosen from PB  bi  O    which assigns probability out to the seed outcome and is uniform over the rest of the outcomes  Once the source for each local outcome has been chosen  the next step is to generate the outcomes themselves  Recall that an outcome is just a formula  Thus  we define the outcome modification distribution using the general formula    DESHPANDE ET AL  modification process Pfor  oi  bi   v  that we will discuss in Sec       again  v is the set of logical variables in z   If bi is the seed outcome  then Pfor treats it as an empty formula  A list of outcomes is considered valid if it contains no repeats and no overlapping outcomes  Since repeats are excluded  the probability of a set of n outcomes is n  times the probability of any corresponding list  Thus  we get the following probability of generating a valid outcome set O and an assignment vector b  given that the prototype outcome set is O and the number of local outcomes is n  Pout  O  b O   n    n   n Y  PB  bi  O  Pfor  oi  bi   v        i    The last step is to generate the outcome probabilities p  These probabilities are sampled from a Dirichlet distribution whose parameter vector depends on the prototype parameters  and the assignment vector b   b            bn    Specifically  define the function f    b  to yield a parameter vector                n     such that   i            b i C b bi    n    if i  n       if i   n          where T is a set of simple terms and I is a function from elements of T to values  This representation guarantees that the elements of T are unordered  and each element is mapped to only one value  So to define our formula modification distribution Pfor      v   we will suppose     T  I  and     T    I     Recall that v is the set of logical variables that may be used in  and    To generate   we first choose a set Tkeep  T    where each term in T  is included in Tkeep independently with probability term   The terms in Tkeep will be included in T   Next  we generate a set Tnew of new terms to include in T   The size of Tnew   denoted knew   is chosen from a geometric distribution with parameter term   Then  for i     to knew   we generate a term ti according to a distribution Pterm  ti  v   This distribution chooses a predicate or function symbol f uniformly at random  and then chooses each argument of f uniformly from the set of constant symbols plus v  We consider a run invalid if any element of Tnew is in T    this ensures that while computing the probability of a term set T given a prototype term set T    we can recover Tkeep as T  T  and Tnew as T   T     This definition says that if oi is generated from prototype outcome bi  including the seed outcome   then  i is obtained by dividing up bi over all the local outcomes derived from bi   The number of such outcomes is computed by the function C b  bi    which returns the number of indices j              n  such that bj   bi   Finally  for the noise outcome  we have  n     n      Next  we choose the term to value function I  For a term t  T  T    the value I t  is equal to I   t  with probability   and with probability       it is sampled according to a distribution Pvalue  x v   If t    T    then I t  is always sampled from Pvalue  x v   This distribution Pvalue  x v  is uniform over the constant symbols in the language  plus v   To define the overall distribution for a local rule r given a rule prototype r   we sum out the assignment variables Bi   For valid rules r  we get        Prule  r r     Pact  z z    Pfor      v  Pnum  n n    X Pout  O  b O   n  Dir f    b   p        b  O  NIL  n   Here Dir f    b   is the Dirichlet distribution with parameter vector f    b    Generative Model for Rule Set Prototypes  The process that generates rule set prototypes G is similar to the process that generates local rule sets from G  but all the rule prototypes are generated from scratch  there are no higher level prototypes from which they could be derived  We assume that the number of rule prototypes in G has a geometric distribution with parameter proto   Thus the probability of a rule set prototype G of size m with  rule prototypes  r            rm    is         Modifying Formulas  The formulas that serve as contexts and outcomes are very simple  they are just conjunctions of literals  where a literal has the form t   x for some term t and value x  The term must be simple in the sense that each of its arguments is either a constant symbol or a logical variable  similarly  x must be a constant symbol or a logical variable   We do not care about the order of literals in a formula  and we would also like to rule out self contradictory formulas in which multiple values are assigned to the same term  It is convenient to think of a formula  as a pair  T  I     We are treating true and false as constant symbols  so a literal such as on X  Y   is represented as on X  Y     false   PG  G    Geom proto   m    m     m Y  Pproto  ri         i    We consider a generative run to be invalid if it generates the same rule prototype more than once  although we allow rule prototypes to have overlapping contexts  The rule prototypes are generated independently from the distribution Pproto  r    This is similar to the distribution for generating a local rule from scratch  as given by Eq      The action term z  is chosen from the uniform distribution Pact  z   NIL   the context formula  is generated by running our formula modification process on the empty formula  given the logical variables v from z    the number of outcomes n has a geometric distribution  and each outcome o in the outcome set O is also generated from       DESHPANDE ET AL   Pfor  o    v   The main difference from the case of local rules is that rather than generating an outcome probability vector p  we generate a vector of Dirichlet weights   defining a prior over outcome distributions  We use a hyperprior P   n   on  in which the sum of the Dirichlet weights has an exponential distribution  Thus  if r consists of an action term z  containing logical variables v  a context    and an outcome set O of size n   then  Pproto  r     Pact  z   NIL  Pfor      v  Y  Geom   n  P   n   Pfor  o   v  oO      Learning  In our problem formulation  we are given sets of examples x         xK from K source tasks  and a set of examples  xK     from the target task   In principle  one could maximize the objective in Eq    using the data from the source and target tasks simultaneously  However  if K is fairly large  the data from task K     is unlikely to have a large effect on our beliefs about the rule set prototype G  Thus  we work in two stages  First  we find the best rule set prototype G given the data for the K source tasks  Then   given G holding G fixed  we find the best rule set RK   and xK     This approach has the benefit of allowing us to throw away our data from the source tasks  and just transfer the relatively small G   Our goal in the first stage  then  is to find the prototype G with the greatest posterior probability given x            xK   Doing this exactly would involve integrating out the source rule sets R            RK   It turns out that if we think of each rule set Rk as consisting of a structure RkS and parameters RkP  namely the outcome probability vectors for all the rules   then we can integrate out RkP efficiently  However  summing over all the discrete structures RkS is difficult  Thus  we apply another MAP approximation  searching for S the prototype G and rule set structures R S           RK that together have maximal posterior probability  It is important that we integrate out the parameters RkP   because the posterior density for RkP is defined over a union of spaces of different dimensions  corresponding to different numbers of rules and outcomes in Rk    The heights of density peaks in spaces of differing dimension are not necessarily comparable  So it would not be correct to use a MAP estimate of RkP obtained by maximizing this density   the outcome probabilities  S P  G  R S           RK    PG  G   K Z Y k    Scoring Function  S In our search over G and R S           RK   our goal is to maximize the marginal probability obtained by integrating out        This equation trades off three factors  the complexity of the rule set prototype  represented by PG  G   the differences between the local rule sets and the prototype  Pmod  Rk  G   and how well the local rule sets fit the data  P  xk  Rk    Computing the value of Eq     for a given choice of G and R            RK is expensive  because it involves summing over all possible mappings from local rules to global rules  the a values in Eq     and all mappings from local outcomes to prototype outcomes  the b values in Eq      Integrating out the outcome probabilities p in each rule is not a computational bottleneck  we can push the integral inside the sums over a and b  and use a modified version of a standard estimation technique  Minka        for the Polya  or Dirichlet multinomial  parameters   Rather than summing over all possible local to global correspondences for rules and outcomes  we approximate by using a single correspondence  Specifically  for each rule set Rk   r            rm    we choose the rule correspondence vector a that maximizes the probability of the local rule contexts i given the global rule Qmcontexts  ai    ignoring outcomes  a   argmaxa i   PA  ai  G Pfor  i   ai     vi    Since each factor contains only one assignment variable ai   we can find the corresponding rule prototype for each local rule separately  Given the rule correspondence a  we next construct an outcome correspondence for each rule ri   We use the outcome correspondence that maximizes the probability of the local outcomes o            on given the outcome set O of the rule prototype Qn ai  ignoring the outcome probabilities  b   argmaxb i   PB  bi  O  Pfor  oi  bi   v   Again  the maximization decomposes into a separate maximization for each outcome  This greedy matching scheme can yield a poor result if a local rule ri has a context similar to a prototype rule  but very different outcomes  So as a final step  we compute the probability of each ri being generated from scratch  and set ai to NIL if this is a better correspondence  These approximations yield the following scoring function  an approximate version of Eq       which we use to guide our search  S Score G  R S           RK    K Z Y PG  G  k         Pmod  Rk  G P  xk  Rk   P Rk  P Rk  Pbmod  Rk  G P  xk  Rk            We modify the standard technique to take into account our hyperprior P   Also  we adjust for cases where some global outcomes are not included in a corresponding local rule  For a more detailed explanation  see the masters thesis by Deshpande           DESHPANDE ET AL   Here Pbmod is a version of the measure Pmod from Eq    in which we simply use a rather than summing over ai values  and we replace Prule with a modified version that uses b rather than summing over b vectors       Coordinate Ascent  We find a local maximum of Eq    using a coordinate ascent algorithm  We alternate between maximizing over local rule set structures given an estimate of the rule set prototype G  and maximizing over the rule set prototype given S estimates of the rule set structures  R S        RK    K Z Y  argmaxRS      RS    K  k    argmaxG P  G   K Y  P  xk  Rk  P  Rk  G   P  RkS  G   We begin with an empty rule set prototype  and use a greedy local search algorithm  described below  to optimize the local rule sets  Since R            RK are conditionally independent given G  we can do this search for each task separately  When these searches stabilize  that is  no search operator improves the objective function  we run another greedy local search to optimize G  We repeat this alternation until no more changes occur  Learning Local Rule Sets  During the coordinate ascent one task is to find the highest scoring local rule set Rk given the rule set G  The search is closely related the rule set learning algorithm problem in Zettlemoyer et al          There are three major differences      G provides a prior that did not exist before      the outcomes O for each rule are constrained to be nonoverlapping  and     the rule parameters p are integrated out instead of being set to maximum likelihood estimates         uses a subalgorithm to find the best set of outcomes  This outcome learning is done with a greedy search algorithm  as described in the next section  The following operators construct changes to the current rule set  Add Remove Rule  Two types of new rules can be added to the set  Rules can be created by an ExplainExamples procedure  Zettlemoyer et al         which uses a heuristic search to find high quality potential rules in a data driven manner  In addition  rules can be created by copying the action and context of one of the prototypes in the global rule set  This provides a strong search bias towards rules that have been found to be useful for other tasks  New rule sets can also be created by removing one of the existing rules in the current set   P Rk  k             Rule Set Search  In this section  we briefly outline a local rule learning algorithm that is a direct adaptation of the approach of Zettlemoyer et al         and highlight the places where the two algorithms differ  The search starts with a rule set that contains only the noisy default rule  At every step  we take the current rule set and apply a set of search operators to create new rule sets  Each of these new rule sets is scored  as described in section      The highest scoring set is selected and set as the new Rk   and the search continues until no new improvements are found  The operators create new rule sets by directly manipulating the current set  either adding or removing some number of the existing rules  Whenever a new rule is created  the relevant operator constructs the rules action and context and  Add Remove Literal  This operator selects a rule in the current rule set  and replaces it with a new rule that is the same except that one literal is added or removed from the context  All possible additions and deletions are proposed  Split on Literal  This operator chooses an existing rule and a new term that does not occur in that rules context  It removes the chosen rule and adds multiple new rules  one for each possible assignment of a value to the chosen term  Any time a new rule is added to a rule set  there is a check to make sure that only one rule is applicable for each training example  Any preexisting rules with overlapping applicability are removed from the rule set         Outcome Search  Given a rule action z and a context   the set of outcomes O is learned with a greedy search that optimizes the score  computed as described in section      This algorithm is a modified version of a previous outcome search procedure  Pasula et al          which has been changed to ensure that the outcomes do not overlap  Initially  O contains only the noise outcome  which can never be removed  It each step  a set of search operators is applied to build new outcome sets  which are scored and the best one is selected  The search finishes when no improvements can be found  The operators include  Add Remove Outcome  This operator adds or removes an outcome from the set  Possible additions include any outcomes from the corresponding prototype rule or an outcome derived from concatenating the changes seen as a result of action effects in a training example  following  Pasula et al           Any existing outcome can be removed  Add Remove Literal  This operator appends or removes a literal from a specific outcome in the set  Any literal that is not present can be added and any currently present literal can be removed        DESHPANDE ET AL   Split on Literal  This operator takes an existing outcome and replaces it with multiple new outcomes  each containing one of the possible value assignments for a new term  Merge Outcomes  This operator creates a new outcome computing the union of an existing outcome and one that could be added by the add operator described above  The original outcome is removed from the set  Two of the operators  add outcome and remove function  have the potential to create overlapping outcomes  To fix this condition  functions are greedily added to overlapping outcomes until no pair of outcomes overlap  This new outcome set is scored  and the search continues       Estimating the Dirichlet parameters for the Polya distribution does not have a closed form solution  but gradient ascent techniques have been developed for the maximum likelihood solution  Minka         To estimate the parameters for a rule prototype r   the required occurrence counts are computed for each prototype outcome and each local rule that corresponds to r  under the correspondence a described in Sec        If a local rule contains several outcomes corresponding to the same prototype outcome  under b   their counts are merged   Experiments  We evaluate our learning algorithm on synthetic data from four families of related tasks  all variants of the classic blocks world  We restrict ourselves to learning the effects of a single action  pickup X  Y    Adding more actions would not significantly change the problem  since the action is always observed  one can learn a rule set for multiple actions by learning a rule set for each action separately          For each source task  generate a set of Nsource state transitions to serve as a training set  In each state transition  the action is pickup A  B  and the initial state is created by assigning random values to all functions on  A  B    Then the resulting state is sampled according to the task specific rule set  Note that the state transitions are sampled independently of each other  they do not form a trajectory   Learning the Rule Set Prototype  The second optimization involves finding the highest scor    ing rule set prototype G given rule sets  R         RK Again  we adopt an approach based on greedy search through the space of possible rule sets  This search has exactly the same initialization and uses all of the same search operators as the local rule set search  There are three differences      the AddRule operator tries to add rules that are present in the local rule sets  without directly referencing the training sets      we relax the restriction that rules and outcomes can not overlap  simplifying some of the checking that the operators have to perform  and     we need to estimate the Dirichlet parameters for the outcomes for each new prototype rule considered by the structure search         Generate K source task rule sets from a prior distribution  This prior distribution is implemented by a special purpose program for each family of tasks  This is slightly more realistic than generating the rule sets from a rule set prototype expressed in our modeling language   Methodology     Run our full learning algorithm on the K source task training sets to find the best rule set prototype G      Generate a target task rule set RK   from the same distribution used in Step       Generate a training set of Ntarget state transitions as in Step    using RK   as the rule set  bK   for the target task using the    Learn a rule set R algorithm from Sec       with G as the fixed rule set prototype     Generate a test set of      initial states using the same distribution as in Step    For each initial state s  compute the variational distance between the next state distributions defined by the true rule set RK   and bK     This is defined in our case the learned rule set R as follows  with a equal to pickup A  B  and s  ranging over possible next states  X  Finally  compute the average variational distance over the test set  Variational distance is a measure of error  but we would like the y axis in our graphs to be a measure of accuracy  so we use     variational distance   The free parameters in our hierarchical Bayesian model  and hence in our scoring function  are set to the same values in all experiments  While we found that the scoring function in Eq     leads to good results on large training sets  we also saw that with small training sets  the very small probabilities of formulas  in contexts and outcomes  tend to dominate the score  For the experiments reported    Each run of our experiments consists of the following steps   bK     p s   s  a  RK      p s   s  a  R  s   The distribution used here is biased so that A is always a block and the robots gripper is usually empty  this focuses our evaluation on cases where pickup A  B  has a chance of success    DESHPANDE ET AL      Slippery Gripper Domain    No Transfer  x      x                Variational Distance      Variational Distance   Gripper Size Domain                                                                     No Transfer  x      x                                         Target Task Examples           a    b   Slippery Gripper with Size Domain  Random Domain                       No Transfer  x      x                                                 Target Task Examples   c      Variational Distance        Variational Distance                                 Target Task Examples              No Transfer  x      x      x                                                   Target Task Examples   d   Figure    Accuracy using an empty rule set prototype  labeled No Transfer  and transfer learning  labeled KxN where K represents the number of source tasks and N represents the number of examples per source task  here  we use a modified scoring function in which each occurrence of the formula distribution Pfor is raised to the power      The fact that this ad hoc modification yields better results suggests that our distribution over formulas is overly flat  and it would be worthwhile to develop a formula distribution that gives common literals or subformulas higher probability       Results  In this section  we present results in the four blocks world domains  For each domain  we briefly describe the task generation distribution and then present results   For each experiment  we graph variational distance as a function of the number of training examples in the target task  Each experiment was repeated    times  our graphs show the average results with     confidence bars  The time required for each run varied from    seconds to    minutes depending on the complexity of the domain  Our first experiment investigates transfer learning in a domain where the rule sets are very simple  just single rules  but the rule contexts vary across tasks  We use a family of tasks where the robot is equipped with grippers of varying sizes  There are seven different sizes of   Deshpande        presents a more detailed description of these domains   blocks on the table  the robot can only pick up blocks that are the same size as its gripper  Thus  each task can be described by a single rule saying that if block X has the proper size  then pickup X  Y   succeeds with some significant probability  this probability also varies across tasks   If X has the wrong size  then no rule applies and there is no change  Since the proper size varies from task to task  the rules for different tasks have different contexts  To increase the learning difficulty  two extra distracter predicates  color and texture  are randomly set to different values in each example state  Fig    a  shows the transfer learning curves for this domain  The transfer learners are consistently able to learn the dynamics of the domain with fewer examples than the non transfer learner  In practice  in each source task  the algorithm learns the specific pickup rule with the appropriate size literal in the context  The algorithm learns a single rule prototype whose context also contains some size literal  This rule prototype provides a strong bias for learning the correct target task rule set  the learner only has to replace the size literal in the prototype with the correct size literal for the given task  To see how transfer learning works for more complex rule sets  our next experiment uses a slippery gripper domain adapted from  Kushmerick et al          The correct model for this domain has four fairly complex rules  describing       DESHPANDE ET AL   cases where the gripper is wet or not wet  which influences the success probability for pickup  and the block is being picked up from the table or from another block  in the latter case  the rule must include an additional outcome for the block falling on the table   The various tasks are all modeled by rules with the same structure  but include relatively large variation in outcome probabilities  Fig    b  shows the transfer learning curves for the slippery gripper domain  Again  transfer significantly reduces the number of examples required to achieve high accuracy  We found that the transfer learners create prototype rule sets that effectively represent the dynamics of the domain  However  the structure of the prototype rules do not exactly match the structure of the four specific rules that are present in each source task  Despite this fact  these prototypes still capture common structure that can be specialized to quickly learn the correct rules in the target task  Our third domain  the slippery gripper domain with size  is a cross between the slippery gripper domain and the gripper size domain  In this domain  all four rules of the slippery gripper domain apply with the addition that each rule can only succeed if the targeted block is of a certain taskspecific size  Thus  the domain exhibits both structural and parametric variation between tasks  As can be seen in Fig    c   the transfer learners perform significantly better than the non transfer learner  In this case  the rule set prototype provides both a parametric and structural bias to better learn the domain  Our final experiment investigates whether our algorithm can avoid erroneous transfer when the tasks are actually unrelated  For this experiment  we generate random source and target rule sets with   to   rules  Rule contexts and outcomes are of random length and contain random sets of literals  Since rule sets sampled this way may contain overlapping rules or outcomes  we use rejection sampling to ensure that a valid rule set is generated for each task  As can be seen in Fig    d   the transfer and non transfer learners performances are statistically indistinguishable  The learning algorithm often builds a rule set prototype containing a few rules with random structure and high variance outcome distribution priors  These prototype rules do not provide any specific guidance about the structure or parameters of the specific rules to be learned in the target task  However  their presence does not lower performance in the target task      Conclusion  In this paper  we developed a transfer learning approach for relational probabilistic world dynamics  We presented a hierarchical Bayesian model and an algorithm for learning a generic rule set prior which  at least in our initial experiments  holds significant promise for generalizing across  different tasks  This learning problem is particularly difficult due to the need to learn relational structure along with probabilities simultaneously for a large number of tasks  The current approach addresses many of the fundamental challenges for this task and provides a strong example that can be extended to work in more complex domains and with a wide range of representation languages  
 Sampling is an important tool for estimating large  complex sums and integrals over high dimensional spaces  For instance  importance sampling has been used as an alternative to exact methods for inference in belief networks  Ideally  we want to have a sampling distribution that pro vides optimal variance estimators  In this paper  we present methods that improve the sampling distribution by systematically adapting it as we obtain information from the samples  We present a stochastic gradient descent method for sequen tially updating the sampling distribution based on the direct minimization of the variance  We also present other stochastic gradient descent meth ods based on the minimization of typical notions of distance between the current sampling distri bution and approximations of the target  optimal distribution  We finally validate and compare the different methods empirically by applying them to the problem of action evaluation in influence diagrams      INTRODUCTION  Often  we are interested in computing quantities involving large sums  such as expectations in uncertain  structured domains  For instance  belief inference in Bayesian net works  BNs  requires that we sum or marginalize over the remaining variables that are not of interest  Similarly  in order to solve the problem of action selection in influence diagrams  we sum over the variables that are not observed at the time of the decision in order to compute the value of different action choices  We can represent the uncertainty in structured environ ments using a BN  A BN allows us to compactly define a joint probability distribution over the relevant variables in a domain  It provides a graphical representation of the  distribution by means of a directed acyclic graph  DAG   It defines locally a conditional probability distribution for each relevant variable  represented as a node in the graph  given the state of its parents in the graph  This decomposi tion can help in the evaluation of the sums  However  due to factors regarding the connectivity of the graph  in gen eral this is not sufficient to allow an efficient computation of the exact value of the sums of interest  Sampling provides an alternative tool for approximately computing these sums  Sampling methods have been pro posed as an alternative to exact methods for such problems  In particular  importance sampling  see Geweke          and the references therein  has been applied to the prob lem of belief inference in BNs  Fung and Chang        Shachter and Peot        and action selection in IDs  see Charnes and Shenoy         and the references therein  and Ortiz and Kaelbling          In its simpler form  the importance sampling distribution used is the  prior  dis tribution of the BN resulting from setting the value of the evidence  It has been noted early on that this sampling dis tribution is far from optimal in the sense that it provides es timates with larger variance than necessary  Shachter and Peot         For instance  the optimal sampling distribu tion in the case of belief inference is to sample the unob served variables from the posterior distribution over them given the observed evidence  If we knew this distribution we would know the answer to the belief inference problem  Several modifications have been proposed to improve the estimation of the simple importance sampling distribu tion discussed above  based on information obtained from the samples  Fung and Chang        Shachter and Peot        Shwe and Cooper          In this paper  we pro pose methods to systematically and sequentially update the importance sampling distribution  We view the updating process as one of learning a separate BN just for sampling  The learning objective is to minimize some error criterion  A stochastic gradient method results from the direct min imization of the variance of the estimator with respect to the importance sampling distribution as an error function  Other stochastic gradient methods result from minimizing   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            error functions based on typical measures of the notion of distance between the current sampling distribution and ap proximations of the optimal sampling distribution      DEFINITIONS  We begin by introducing some notation used throughout the paper  We denote one dimensional random variables by capital letters and denote multi dimensional random variables by bold capital letters  For instance  we de note a multi dimensional random variable by X and de note all its components by where is the ith one dimensional random variable  We use small let ters to denote assignments to random variables  For in stance  X   x means that for each component of X    We denote the set of possible values that can take by Ox  and the set of possible values that X can take   X        Xn   Xi  Xi xi  xi Xi   by Ox X    nx   We also denote by capital letters the nodes in a graph  We denote by the parents of node in a directed graph     Pa Y   Y  We now introduce notation that will become useful dur ing the description of the methods presented in this pa per  We denote by the operator Lz the sum over the possible values of the individual variables forming Z  Lz Lz     Lz   For any function h with vari        n   abies Z and    the expression h Z O lo o stands for a function f  over variables Z that results from setting the values of   in h with assignment o while letting the values for Z remain unassigned  In other words  f  Z    h Z O lo o   h Z    o    The notation X    Z     means that the variable X is formed by all the variables that form Z and    That is  X    Z O      Z         Znp           X       where n   n    n   Note that we are assuming that the set of variables forming Z and those forming  are disjoint  The notation Z    f means that the random variable Z is distributed according to probability distribution f        Xn    On       A Bayesian network  BN  is a graphical probabilistic model used to represent uncertainty in structured domains  It com pactly represents the joint probability distribution over the relevant variables of the system of interest  It uses a di rected acyclic graph  DAG  to represent the relationship between the relevant variables  A node in the graph rep resents a variable  The model defines a local conditional distribution for each node or variable I given its parents in the graph  The joint distribution is then  P Xi Pa Xi   Pa Xi   Xi  For instance  we can define a BN on the graph given in Figure l  a   The inference problem in BNs is that of computing the pos terior probability of an assignment to a subset of variables   b  Figure    Example of  a  Bayesian network and  b  influ ence diagram  given evidence about another subset of variables in the sys tem  Assume that the variables are discrete and their sam ple spaces or the possible values each variable can take are finite  In general  let X    Z     where   is the set of variables of interest  o is an assignment to it and Z are the remaining variables  For this problem we want to compute probabilities of the kind  P O    o        LzP Z O   o      Often  the local decomposition of the joint distribution still leads to the evaluation of sums over a large number of variables  In general  this problem is intractable  Cooper         An influence diagram  ID  is a probabilistic model for decision making under uncertainty  We can think of an ID as a BN with decision and utility nodes added  For instance  we can use our example BN to build an ID as shown in Fig ure   b   The square is a decision node  The diamond is a utility node  We now have potentially different joint distri butions over the variables  for each action choice available  Assume for simplicity that there is a single decision node in the graph  The joint distribution over the variables  given the action choice aassigned to the decision variable  is  P X I A  a     TI l P Xi I Pa Xi  IA a    The decision associated with a decision node is a function of its parent nodes in the graph  We will have access to        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       the value of these variables at the time of making the deci sion  Similarly  the utility associated with a utility node is a function of its parent nodes in the graph  Assume that we have a finite number of discrete action choices  Then  one problem is to select the best strategy or function  r  mapping each possible value of the parents of the decision node to an action choice  The best strategy is the strategy with highest expected utility  Let X    Z     where the variables in  are parents of the decision node and Z are the remaining variables  The problem of ob taining an optimal strategy reduces to obtaining  for each assignment     o  the action that maximizes the value associated with the action and the assignment   V  a   I  z P Z      o I A  a  U Z     o  A  a    Note once again that computing this value requires the eval uation of a sum  For the same reasons as in the previous problem of belief inference in BNs  the exact computation of this value is intractable in general      to it the value given by the evidence assignment o  There fore  the resulting samples will be assignments to those variables that are not in the evidence set according to the  prior  distribution of the BN  We call the method resulting from this importance sampling distribution the traditional method  In the context of belief inference  this method is called likelihood weighting  LW  since the weight function is a  likelihood  and thus each sample is weighted by its  likelihood   We can similarly apply this technique in the context of ac tion selection in IDs to evaluate V   a    In general  we let  g Z  f Z   P Z  O   o I A a  U Z  O  o  A  a    TIl P ZiI Pa Zi    lo o A a   w Z   IJ   P OjI Pa Oj    U Z      A  O oA    a          N  l  GA   N ul  lw  z           We can apply this technique to the problem of belief infer ence in BNs  Typically  we let  g Z   P Z         o     I    TI P ZiI Pa Zi    IJ  P Oj I Pa Oj        O o f Z  TI  P ZiI Pa Zi    lo o  which implies w Z   I  TI   P OjI Pa Oj    O o   Note that we are defining the importance sampling distri bution to be the  prior  distribution of the BN  We obtain samples from this distribution by sampling the variables in the  partial  order defined by the DAG and according to the local conditional distribution of the original BN for each variable  As we obtain samples from each variable by traversing the nodes in the graph and sampling the variable corresponding to it  if we get to a node or variable that is in the evidence set    we do not sample it  Instead  we assign    In particular  for our example   g Z   IMPORTANCE SAMPLING  Importance sampling provides an alternative to the exact methods for evaluating sums  Let the quantity of inter est be G   I  z g  Z  for some real function g  We can turn the sum into an expectation by expressing G   I  z f Z   g Z   f Z    where f is a probability distribu tion over Z satisfying  for all Z  g Z   f      f Z   f    We call f the importance sampling distribution  We de fine the weight function w Z    g Z   f Z  which al lows us to express G I  z f Z w Z   Hence  we can obtain an unbiased estimate of G by obtaining N samples z  l         z N  from Z    f and computing the estimate  I  f Z   P Xl  P X  I Xl  P X  I Xl  X P X  I x   A  a  P X  I X   X   X P X   X  I X   P Xs  Xs I x   X   U X   A   a    P Xl  P X  I Xl  P X  I Xl  X P X  I x   A a  P X  I x   X     P X  X  I X   P Xs  Xs I x   X   U X   A a     X     w Z      X  An important property of the estimator G is the variance of the weights associated with the importance sampling dis tribution  This is  Var w Z       I  z f Z w Z    G   Recall that G   I  z g Z  by definition and assume that g is a positive function  From this we can derive that the optimal or minimum variance importance sampling distri bution is proportional to g Z    f  Z    g Z   I  z g Z         The weights will have zero variance in that case  since the weight function will always output our value of interest G  We also note that we need to avoid letting f Z  be too small with respect to g Z   since this will increase the variance  As a matter of fact  Var w Z         oo as f Z        for at least one value of Z  This implies that we should use importance sampling distributions with suf ficiently  fat tails       ADAPTIVE IMPORTANCE SAMPLING  The traditional method presented above uses as the importance sampling distribution the  prior  distribution        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       of the BN which can be far from optimal in the sense that it can have higher variance than necessary  In the case of evaluating actions in IDs  it also completely ignores poten tially useful information about the utility values  Therefore  we try to learn the optimal importance sampling distribu tion by adapting the current sampling distribution as we obtain samples from it  We view the adaptive process as one of learning a distribu tion over the variables the sum is over to use specifically as an importance sampling distribution  In particular  we can view this process as learning BNs from the samples just for sampling  From the expression of the optimal importance sampling distribution given in equation    and  in particu lar  from the factorization of the function g for the different estimation problems   we can deduce that in order to be able to represent this distribution graphically using a BN we need to add arcs that connect every pair of nodes that are parents of observations and or utility nodes  if they are not already connected  However  doing so can increase the size of the model  particularly in cases where the local con ditional probabilities and the utilities have a smaller  more compact parametric representation  i e   noise or s   In this paper  we do not deal with this issue and instead concen trate on the problem of learning a BN with the same struc ture as the original BN  or ID   Hence  we only need to update the local conditional probability distributions as we obtain samples  We can parameterize the importance sampling distribution using a set of parameters E   Let the indicator function I Zi k  Pa Zi    j I Z    if the condition zi   k and Pa Zi    j agrees with the value assigned to Z    otherwise  Then  we can express the importance sampling distribution as       n  J  Z I  E     II  II elCkZ  k PaCZ   jiZ    J  II      where for each i j  k  eijk P Zi k I Pa Zi    j  E    Hence  for all i j  L k eijk    and for all k  eijk      Note that this representation uses the assumptions of global and local parameter independence typically used in BNs  The weight function is also parameterized and defined as          w Z  IE     g Z   f Z I E          LEARNING CRI TERIA AND UPDATE RULES  In the following subsections we present different methods for updating the sampling distribution  The update rules are all based on gradient descent  Hence  at each time t  we update the parameters as follows       oCt      oCt   a t  i Pe OCt    In the update rule above  a t  denotes the learning rate or the step size rule and  i Pe E   denotes the gradient of error  function e  appropriately projected to satisfy the constraints onE   The methods differ in how they define  i Pe oCtl    In the discussion below we denote the N t  i i d  samples as zCt           zCt NCt   drawn according to Z f  Z I oCtJ   If we gather samples to estimate G using many dif ferent sampling distributions  how can we combine them to get an unbiased estimate  It is sufficient to weight them using any weighting function that is independent of the sub estimates obtained by using just the samples for one sam pling distribution  For instance  the estimator     C CT    I       W t G oCtl         I       W t      and W t           for all t  and  NCt  w zCt l  I oCt   G oCt     N  Ct   L        l   is unbiased as long as W t  and G oCt   are independent for each t  Letting W t    T will produce an unbi where        ased estimate  This is the weight we use in the experi ments  In general  we would like to give more weight to importance sampling distributions with smaller variances  Assuming that the variance decreases with t  we would like W t  to be an increasing sequence oft  Note that using W t  ex    where is the sample variance at time t  though appealing  does not necessarily lead to an unbiased estimator since W t  and G oCt   are not independent    f    f  We will consider three general strategies  minimizing vari ance directly  minimizing distance to global approxima tions of the optimal sampling distribution  and minimizing distance to the empirical distribution of the optimal sam pling distribution based on local approximations  For the first two strategies  we will find that we can express the partial derivatives that form the gradient as  for all i j  k    eCE     Jijk    I  z J  Z  aCZ   jiZ  IE     JCZ  k P  Jijk  p Z  E     X  where  p  Z  E   is a function that depends on the error func tions  Note that this is an expectation  Then  the methods update the parameters by estimating the value of the partial derivatives evaluated at the current setting of the parame ters oCt  as  ae co       J jk                  r     C     JCZ  k PaCZ    jiZ z t l   X  t  NCt  L   l  t  Jijk  p zCt l   oCt      Minimizing Variance Directly  As we noted above  the optimal importance sampling dis tribution for estimating G is that which minimizes the variance of w  Using that as our objective  we derive a stochastic gradient update rule for the parameters of the   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            importance sampling distribution  Let the error function be  evar     Var w Z I     Ez f Z I e w Z I        eKL       Ez f Z I    log  f Z I     f  Z          The corresponding function for the gradient is    KL  Z      log  f  Z   f Z I         log  w z t t    o tJ fGCtl             Approximate Global Minimization  Recall the optimal importance sampling distribution f  for estimating G given in equation    The update rules of the following subsection are all motivated by the idea of reduc ing some notion of distance between the current sampling distribution and this optimal sampling distribution  Note that we cannot really compute the values of the optimal dis tribution since that requires knowing the normalizing con stant Ez g Z    G which is exactly the value we want to estimate  We approximate the optimal distribution using the current estimate of G as follows      In the following  we will consider four error functions  one based on the sum squared error and three based on versions of the Kullback Leibler divergence   eKL        eKL        eKL      We can obtain the partial derivatives for this error function and their approximation accordingly          Heuristic Local Minimization Based on Empirical Distribution  The update methods in this subsection are motivated by the idea of minimizing different notions of distance between the current sampling distribution and an empirical distribu tion of the optimal importance sampling distribution that we build from the samples  The hope is that the empirical distribution is a good approximation of the optimal sam pling distribution  We define the empirical distribution  pa rameterized by   locally as follows  for all i  j  k   z       l I Z   k Pa Z    jiZ  z t l  w z t l  l  t    t  Bijk z       I Pa Z    jiZ  z t l  w z t l            If we use the    norm or sum squared error function as a notion of distance between the distributions  then the error function is  f  Z   f Z I    f z t l  I o t   X  w z t t    o t   c t              where the approximation results from using  t Z  as de fined in equation   as an approximation to f  Z   An alternative  commonly used notion of distance between two probability distributions is given by the Kullback Leibler  KL  divergence  This measure is not symmetric  One version of the KL divergence in this context is given by the error function  eKL       Ez f  Z  log  f  Z  f Z I       if  L  g  I Pa Zi    j  I  z       l    z t l  w z t l  I oCtl  of      e     e otherwise  We are essentially defining the em  pirical distribution using the samples if there are samples that can be used to define it  otherwise  we revert to the current distribution  We try to minimize the distance be tween the current sampling distribution and the empirical distribution locally   The corresponding function for the gradient is            A  symmetrized  version of KL sometimes used is given by the error function  Minimizing Variance Indirectly via   t Z   g Z jc tl          Another version of the KL divergence is given by the error function  Note that using this definition of  p yields an unbiased es timate of the gradient  This is because the gradient is the expectation of a particular function and  in this case  we can always evaluate the function exactly  Hence  we can obtain an unbiased estimate by sampling from f Z I         f  Z   f Z I    w z t l  I  J tl jG tJ           G    The corresponding function for the gradient is   t Var Z  e   w Z I  The corresponding function for the gradient is  Similar to the case of the previous strategies  we will find that we can express the partial derivatives that form the gra dient of the error functions discussed in this subsection as  for all i  j  k    e  E       k           cp  eijk  eijk    where cp  Bijk  eijk  is a function that depends on the error functions  Then  the methods update the parameters by es timating the value of the partial derivatives evaluated at the current setting of the parameters o  t  as ae            ae j k     C J    t  tJ  cp  eijk  eijk     UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       el    The local Lz error function  leads to an update rule for which the step size has a very intuitive interpretation as a weighting between the current importance sampling distribution and the empirical distribution  In the case of   the update direction is proportional to the ratio of the empirical distribution with respect to the current importance sampling distribution  On the other hand  for the update direction is proportional to the logarithm is not defined if at least of the same ratio  Note  We define the local L  norm error function as  el   e      i j k  eijk  eijk I        the error function for one version of KL as  ekLl       i j k eijk log   eijk Bijk I  ekL      ekL    and the other as  ekL     i j k eijk log   eijk Bijk I   From this we obtain the corresponding functions for the gradient   v   eijk  eijk   PkL  Bijk  Bijk   PkL  Bijk  Bijk   DISCUSSION OF UPDATE RULES  First  note that of all the update rules  only the one derived for clearly uses an unbiased estimate of the gradient  It is not immediately apparent whether the update rules based and on use unbiased estimates   evar  eKL   Note also that the magnitude of the components of the re sulting gradients are different  as suggested by their respec tive functions  The function has magnitude propor tional to the squares of the weights  The magnitudes of and are linear in the weights  However  the magni tude of is potentially smaller since it has the probabil ity of the sample as a factor  The magnitude of is logarithmic in the weights    p   pvar   pL    PKL   pL    PKL   Because we assume that g is positive  the weights are pos itive  Hence  and are always positive  The function is positive if w Z I      Similarly  is positive if log w Z I the function      If w Z I   then the sampling distribution under estimates the value of g while if w Z I      then it overestimates the value  Therefore  the sign of and depends on whether we under  or over estimated the value of g  Similarly  the magnitudes of and are related to the amount of under  or over the magnitude is larger estimation  For and when the sampling distribution underestimates than when it overestimates  For the logarithm brings the amount of over  and underestimation to the same scale  Note that for the approximations of and G can not be zero  and in addition for w Z I    cannot be zero  These conditions hold from the assumption that g is positive  Note that unless we constrain the importance sampling distribution  all the functions and will be unbounded even if g is bounded    PL    PVar   PKL     G   PKL      G   PKL      G   G  PL    PKL    PVar   PL    PKL     PVar   pL    PKL    PKL     PL    PKL    PKL     PKL         This is essentially imposing a Dirichlet prior with parame ters equal to the current probability values on the empirical distribution parameters   eijk   eijk  eijk eijk  log  eijk Bijk I       We can obtain an update rule based on the  symmetrized  version of KL accordingly   eL   eKL    PKL   eiJ    We can fix this by letting  for each i  j  k    z         I Z  k Pa Z   jJZ z t l  w z t l  JO tl    iii  t  B ijk     z         I Pa Z   jJZ z t l   w z t l  IO t        one                PKL     PVar   PL    PKL   We can interpret the update rules based on local KL divergence as adding weights to the elements of the domain of the importance sampling distribution and renormalizing  For the version of KL divergence with respect to the em pirical distribution  we are always adding weights  We add values relative to the amount we underestimated or over estimated the magnitude of the distribution for a particu lar state  If we underestimated  we add weights larger than one  If we overestimated  we add weights smaller than one  For the other version of KL divergence  due to the loga rithm function  we add weight if we underestimated while we subtract weight if we overestimated  Therefore  the log arithm brings the amount of underestimation and overesti mation to the same scale and adds or subtracts weight ac cordingly   evar  eL    Note that when approximating the gradients for and we can use as little as one sample to obtain      This is not ad an estimate of the gradient  i e   N t  visable for the method based on the local heuristic since the empirical distribution of the optimal sampling distribution will be highly inaccurate  Hence  the update rules based on the empirical distribution will work better when we take a larger number of samples between updates  Finally  note that when t   and N t          and therefore  the parameters will not change in the first iteration   eKL   eKL                 pL   RELATED WORK  Different variations of importance sampling have been used for the problems discussed in this paper  See Lin and Druzdzel        and the references therein   Our methods belong to the class of forward samplers since they sam ple from a distribution based on the original structure of the BN  Of these  self importance sampling  Shachter and Peot        Shwe and Cooper        is the method closest to the methods proposed in this paper since it also updates the sampling distribution as it obtains information from the samples  This method has an update rule that is very sim ilar to the one derived for It updates the distribution  el    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            after obtaining the empirical distribution  but the update is a weighting between the empirical distribution and the first sampling distribution used  Shwe and Cooper          The update rule is  eiJ t           e      a t   e tlk   m   a t BiZ      eiJ iJ a t  e       a t   a t    Bijk   a t       o    In our framework  we can think of this update rule as re sulting from the error function  esJs E   t                                   a t  Bijk   a t Bijk  a t  k Bijk        Annealed importance sampling  Neal        is a related technique in that it tries to obtain samples from the opti mal sampling distribution  As we understand it  the user sets up a sequence of distributions  the last distribution be ing the optimal distribution  typically defined by Markov chains  We move from one distribution to another as we  anneal  and the sequence converges to the optimal sam pling distribution  The hope is that we can get an inde pendent sample from that distribution  then we restart the process to try to obtain another independent sample  and so on  Finally  it uses those independent samples to obtain an estimate  Notice that each  traversal  of the sequence of distributions  or Markov chains  produces a single sam ple  The technique is very general and we are unaware of whether it has been applied to the problems considered in this paper  We are currently investigating possible connec tions between our methods and this technique      EMPIRICAL RESULTS  We implemented all of the adaptive importance sampling methods described above  We Jet the learning rate a t      jt  where    is a value that depends on the updating method  We need different values of    for the different methods because of the differences in magnitude of their gradients  We impose an additional constraint on the pa rameters which we call the  boundary  We require that for all i  j  k  Bijk    E IDx l      IDx l  where   is a con stant factor  In our experiments  we Jet        We do this so that our sampling distribution has  fat tails   avoid ing extrema in probability and hence the possibility of in finite variance  We initialize the parameters o O  such that the starting importance sampling distribution is the  prior  probability distribution of the original BN  However  if one of the local conditional probability values does not satisfy the E boundary constraint  we change the distribution so that it does        Figure    Graphical representation of the ID for the com puter mouse problem  In order to satisfy the constraint that for all i  j  Lk Bijk    we project the approximation of the gradients onto the     simplex of the local conditional probability distribution  We do so by Jetting  for all i  j  k    Pe    ao jk        e    ao jk         lnx   I ae    Jnx    L    k l ao jk           Note that this is not enough to guarantee that after taking a step in the projected direction  the parameters will remain in the constraint space  If  when updating a local condi tional probability distribution  its respective parameters do not satisfy the constraint  we find the minimum step a  that will allow them to remain inside the constraint space and take a step of size a     along the gradient direction  i e   half the distance between the current position of the param eter we are updating in the simplex and the closest point on the  boundary along the gradient direction   We tested the methods on the computer mouse prob lem  Ortiz and Kaelbling         a simple made up ID shown in Figure    We added one to all the utility val ues presented in Ortiz and Kaelbling        to make g positive  We will consider the problem of obtaining the value VMP   A  for the action A     and the observation  MPt   l   We evaluated each method by computing the mean squared error  MSE  between the true value of the expec tation of interest  VM p   A   and the estimate generated us ing the adaptive sampling method  The first results show how the methods achieve better MSEs with fewer samples for this problem  We only show results for those methods that were the most competitive  We denote by  Var  the method based on the minimization of the variance  and by  L     KL    and  KLS  the methods based on the global minimization of     KL  and KLs respectively  For the   for all t  We take into update methods we use N t         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            LW    Var    L           Kl             M   KLS      LW    Var    L           KL        oEJ       KLS                                             number of samples  Figure    Average mean squared error  over    runs  as a function of the number of samples taken  We allow LW twice as many samples   account that the update methods have to traverse the graph once every iteration to update the parameters relevant to the sample taken  To compensate for this time  we allow the es timate based on LW to use twice as many samples  Figure   shows the results  The graph shows the average MSE over    runs as a function of the total number of samples taken  times   for LW  by the methods  We note that Var and L  achieve better MSEs than LW and converge to them faster  With significance level       we can state  individually  for each total number of samples N                 that Var and L   individually  are better with respect to MSE than LW  Also  for N        KLS is better than LW  We also ran the methods with N t        including the local heuristic methods  They were only competitive after a larger total number of samples  N         Although fur ther analysis is necessary  we would like to convey some general observations  We believe that in general there is a tradeoff in the setting of N t  and     We note that  of the updates based on the two KL versions  KLl typically per forms better than KL   We believe this is because the error function eKL  is defined with respect to the optimal sam pling distribution while eKL  is with respect to the current sampling distribution  KLS seems to perform better than both  L  is more stable than any of the other methods  sug gesting further theoretical analysis which we are currently undertaking  Several possible reasons for this behavior are     the variance of the gradient might be smaller than in other cases      the error function is bounded  and or     the error surface might be smoother than in other cases  We conjecture that L  converges to a stationary point of eL  The second result shows that the update methods indeed lead to importance sampling distributions with smaller variance relatively quickly for this problem  Figure                                       number of samples  Figure    Average of the true variance of the weight func tion  over    runs  as a function of the total number of sam ples taken   shows a graph of the true variance of the sampling distribu tion learned using the different update methods as a func tion of the total number of samples used  The horizontal line shows the variance associated with the sampling dis tribution used by LW  i e   the  prior  distribution of the original BN   These experiments are all carried out on a single prob lem  Although they must clearly be extended to a variety of larger problems  they indicate that adaptive importance sampling methods  particularly those that minimize vari ance and the    norm  can lead to significant improvements in the efficiency of sampling as a method for computing large expectations  Acknowledgments  The dynamic weighting scheme and the    CJ  recommen dation in Section     and the E boundary in Section   were independently developed by Jian Cheng and Marek Druzdzel  Both heuristics are reported in a manuscript that the first author saw while he was working on this paper  We would like to thank Milos Hauskrecht  Thomas Hof mann  Kee Eung Kim and Thomas Dean for many discus sions and feedback  Also  our implementation uses some of the functionality of the Bayes Net Toolboxfor Matlab  Mur phy         for which we thank Kevin Murphy  We would also like to thank the anonymous reviewers for their in sightful comments  Luis E  Ortiz was supported in part by an NSF Gradu ate Fellowship and in part by NSF IGERT award SBR          Leslie Pack Kaelbling was supported in part by a grant from NTT and in part by DARPA Contract  DABT                           UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       
  the expeeted number of actions required to reach the goal   We describe a method for time critical de cision making involving sequential tasks and stochastic processes  The method employs several iterative refinement routines for solv ing different aspects of the decision mak ing problem  This paper concentrates on the meta level control problem of delibera tion scheduling  allocating computational re sources to these routines  We provide dif ferent models corresponding to optimization problems that capture the different circum stances and computational strategies for de cision making under time constraints  We consider precursor models in which all deci sion making is performed prior to execution and recurrent models in which decision mak ing is performed in parallel with execution  accounting for the states observed during ex ecution and anticipating future states  We describe algorithms for precursor and recur rent models and provide the results of our empirical investigations to date   We represent goals of aehievement in terms of an opti mal sequential decision making problem in which there is a reward function specially formulated for a partie ular goal  For the goal of achieving p as quiekly as possible  the reward is   for all states satisfying p and    otherwise  The optimization problem is to find a policy  a mapping from states to actions  maximiz ing the expected discounted cumulative reward with respect to the underlying stochastic process and the specially formulated reward function  In our formula tion  a policy is nothing more than a conditional plan for achieving goals quickly on average   Introduction  We are interested in solving sequential decision making problems given a model of the underlying dynamical system specified as a stochastic automaton  i e   a set of states  actions  and a transition matrix which we assume is sparse     In the following  we refer to the specified automaton as the system automaton  Our approach builds on the theoretical work in operations research and the decision sciences for posing and solv ing sequential decision making problems  but it draws its power from the goal directed perspective of artifi cial intelligence  Achieving a goal corresponds to per forming a sequence of actions in order to reach a state satisfying a given proposition  In general  the shorter the sequence of actions the better  Because the state transitions are governed by a stochastic proeess  we cannot guarantee the length of a sequenee achieving a given goal  Instead  we are interested in minimizing  Instead of generating an optimal policy for the sys tem automaton  which would be impractical for an automaton with a large state space  we formulate a simpler or restrictert stochastic automaton and then search for an optimal policy in this restricted automa ton  At all times  the system maintains a restricted au tomaton  The restricted automaton and correspond ing policy are improved as time permits by successive refinement  This approach was inspired by the work of Drummond and Bresina  Drummond and Bresina        on anytime synthetic projeC tion  The state space for the restricted automaton corre sponds to a subset of the states of the system au tomaton  this subset is called the envelope of the re stricted automaton  and a special state OUT that rep resents being in some state outside of the envelope  For states in the envelope  the transition funetion of the restricted automaton is the same as in the system automaton  The pseudo state OUT is a sink  i e   all actions result in transitions back to OUT  and  for a given action and state in the envelope  the probability of making a transition to OUT is one minus the sum of the probabilities of making a transition to the same or some other state in the envelope  There are two basic types of operations on the re stricted automaton  The first is called envelope al teration and serves to increase or decrease the num ber of states in the restricted automaton  The second is called policy generation and determines a policy for        Dean et al   b  i   ii   Figure    Stochastic process and a restricted version  the system automaton using the restricted automaton  Note that  while the policy is constructed using the re stricted automaton  it is a complete policy and applies to all of the states in the system automaton  For states outside of the envelope  the policy is defined by a set of reflexes that implement some default behavior for the agent  In this paper  deliberation scheduling refers to the problem of allocating processor time to envelope alteration and policy generation  There are several different methods for envelope al teration  In the first method  we simply search for a  new  path or trajectory from the initial state to a state satisfying the goal and add the states traversed in this path to the state space for the restricted automa ton  This method need not make use of the current restricted automaton  A second class of methods op erates by finding the first state outside the envelope that the agent is most likely to transition to using its current policy  given that it leaves the set of states corresponding the current envelope  There are several variations on this  add the state  add the state and the n next most likely states  add all of the states in a path from the state to a state satisfying the goal  add all of the states in a path from the state to a state back in the current envelope  Finally  there are methods that prune states from the current envelope on the grounds that the agent is unlikely to end up in those states and therefore need not consider them in formulating a policy  Figure   i shows an example system automaton con sisting of five states  Suppose that the initial state is    and state   satisfies the goal  The path         goes from the initial state to a state satisfying the goal and the corresponding envelope is            Fig ure   ii shows the restricted automaton for that en velope  Let  r  x  be the action specified by the pol icy  r to be taken in state x  the optimal policy for the restricted automaton shown in Figure l ii is de fined by  r         r        r       a on the states of the envelope and the reflexes by  r OUT  b  i e    ifX                    r  X    b     All of our current policy generation techniques are based on iterative algorithms such as value iteration  Bellman        and policy iteration  Howard         In this paper  we use the latter  These techniques can be interrupted at any point to return a policy whose  value improves in expectation on each iteration  Each iteration of policy iteration takes    IE    where E is the envelope or set of states for the restricted automa ton  The total number of iterations until no further improvement is possible varies but is guaranteed to be polynomial in lEI  This paper is primarily concerned with how to allocate computational resources to enve lope alteration and policy generation  In the following  we consider several different models  In the simpler models called precursor deliberation models  we assume that the agent has one opportu nity to generate a policy and that  having generated a policy  the agent must use that policy thereafter  Precursor deliberation models include    a deadline is given in advance  specifying when to stop deliberating and start acting according to the generated policy    the agent is given an unlimited amount of time to respond  with a  linear cost of delay There are also more complicated precursor deliberation models  which we do not address in this paper  such as the following two models  in which a trigger event occurs  indicating that the agent must begin following its policy immediately with no further refinement     the trigger event can occur at any time in a fixed interval with a uniform distribution    the trigger event is governed by a more compli cated distribution  e g   a normal distribution cen tered on an expected time In more complicated models  called recurrent deliberation models  we assume that the agent period ically replans  Recurrent deliberation models include    the agent performs further envelope alteration and policy generation if and only if it  falls out  of the envelope  defined by the current restricted automaton    the agent performs further envelope alteration and policy generation periodically  tailoring the restricted automaton and its corresponding pol icy to states expected to occur in the near future The rest of this paper assumes some familiarity with basic methods for sequential decision making in stochastic domains  A companion paper  Dean et al         provides additional details regarding algo rithms for precursor deliberation models  In this pa per  we dispense with the mathematical preliminaries  and concentrate on conveying basic ideas and empir ical results  A complete description of our approach including relevant background material is available in a forthcoming technical report     Deliberation Scheduling  In the previous section  we sketched an algorithm that generates policies  Each policy  r has some value with   Deliberation Scheduling for Time Critical Sequential Decision Making       respect to an initial state x   this value is denoted V   x   and corresponds to the expected cumulative reward that results from executing the policy starting in x  Given a stochastic process and reward function  V   x   is well defined for any policy  r and state x   We are assuming that  in time critical applications  it is impractical to compute V    x   for a given policy and initial state and  more importantly  that it is im practical to compute the optimal policy for the entire system automaton  In order to control complexity  in generating a pol icy  our algorithm considers only a subset of the state space of the stochastic process  The algorithm starts with an initial policy and a restricted state space  or envelope   extends that envelope  and then computes a new policy  We would like it to be the case that the new policy  r  is an improvement over  or at the very least no worse than   the old policy  r in the sense that V r   xo   V    xo         In general  however  we cannot guarantee that the pol icy will improve without extending the state space to be the entire space of the system automaton  which results in computational problems  The best that we can hope for is that the algorithm improves in expecta tion  Suppose that the initial envelope is just the ini tial state and the initial policy is determined entirely by the reflexes  The difference Vrr  xo   V r xo  is a random variable  where  r is the reflex policy and  r  is the computed policy  We would like it to be the case that E V r  x    V r x         where the expectation is taken over start states and goals drawn from some fixed distribution  Although it is possible to construct system automata for which even this improvement in expectation is impossible  we believe most moderately benign navigational environments  for instance  are well behaved in this respect  Our algorithm computes its own estimate of the value of policies by using a smaller and computationally more tractable stochastic process  Ideally  we wo uld like to show that there is a strong correllation be tween the estimate that our algorithm uses and the value of the policy as defined above with respect to the complete stochastic process  but for the time be ing we show empirically that our algorithm provides policies whose values increase over time  Our basic algorithm consists of two stages  envelope alteration  EA  followed by policy generation  PG   The algorithm takes as input an envelope and a policy and generates as output a new envelope and policy  We also assume that the algorithm has access to the state transition matrix for the stochastic process  In general  we assume that the algorithm is applied in the manner of iterative refinement  with more than one invocation of the algorithm  We will also treat en velope alteration and policy generation as separate  so we east the overall process of poliey formation in terms of some number of rounds of envelope alteration fol lowed by poliey generation  resulting in a sequenee of  Figure    Sequenee of restrieted automata and associ ated paths through state space polieies  Figure   depicts a sequenee of automata gen erated by iterative refinement along with the associ ated paths through state spaee traversed in extending the envelope  Envelope alteration can be further classified in terms of three basic operations on the envelope  trajectory planning  envelope extension  and envelope pruning  Trajectory planning eonsists of searching for some path from an initial state to a state satisfying the goal  En velope extension consists of adding states to the enve lope  Envelope pruning involves removing states from the envelope and is generally used only in recurrent deliberation models  Let   r   represent the policy after the ith round and let  tEA  be the time spent in the ith round of envelope alteration  We say that poliey generation is inflexi ble if the ith round of poliey generation is always run to completion on IEil   Policy generation is itself an  iterative algorithm that improves an initial policy by estimating the value of policies with respect to the re stricted stochastic  process mentioned earlier  W hen run to eompletion  policy generation continues to iter ate until it finds a policy that it cannot improve with respect to its estimate of value  The time spent on the ith round of policy generation tpa  depends on the size of the state space IEil   In the following  we present a number of deeision mod els  Note that for each instance of the problems that we eonsider  there is a large number of possible deci sion models  Our seleetion of which decision models to investigate is guided by our interest in providing some insight into the problems of time critical deeision mak ing and our antieipation of the combinatorial problems involved in deliberation scheduling     Precursor Deliberation  In this section we consider the first precursor deliberation model  in which there is a fixed deadline known in advance  It is straightforward to extend this to model    where the agent is given an unlimited re sponse time with a Linear eost of delay  models    and   are more eomplicated and and are not eonsidered in this paper              Dean et al   The Model  Let troT be the total amount of time from the current time until the deadline  If there are k rounds of enve lope alteration and policy generation  then we have tEA    tpa     tEAk  tpak  trOT Case    Single round  inflexible policy genera tion In the simplest case  policy generation does not  inform envelope alteration and so we might as well do all of the envelope alteration before policy generation  and tEA    tpa    troT Here is what we need in order to schedule time for EA  and PG      the expected value  taken over randomly chosen pairs of initial states and goals  of the improve ment of the value of the initial state  given a fixed amount of time allocated to envelope alteration  E V r   xo   V ro xo itEA Ji    the expected size of the envelope given the time allocated to the first round of envelope alteration  E IE IItEA    and    the expected time required for policy generation  given the size of the envelope after the first round of envelope alteration  E  tpa IIE I   Note that  because policy generation is itself an iterative refinement algorithm  we can interrupt it at any point and obtain a policy  for instance  when policy generation takes longer than pre dicted by the above expectation  Each of          and     can be determined empm cally  and  at least in principle  the optimal allocation to envelope alteration and policy generation can be determined  Case II  Multiple rounds  inflexible policy gen eration Assume that policy generation can prof  itably inform envelope alteration  i e   the policy after round i provides guidance in extending the environ ment during round i     In this case  we also have k rounds and tEA   tpa    tEAk  tpak  troT  Informally  let the fringe states for a given envelope and policy correspond to those states outside the enve lope  that can be reached with some probability greater than zero in a single step by following the policy start ing from some state within the envelope  Let the most likely falling out state with respect to a given envelope and policy correspond to that fringe state that is most likely to be reached by following the policy starting in the initial state  We might consider a very simple method of envelope alteration in which we just add the most likely falling out state and then the next most likely and so on  Suppose that adding each additional state takes a fixed amount of time  Let  E V r   xo   V r     xo IIE       m  IE I   m   n  denote the expected improvement after the ith round of envelope alteration and policy generation given that  there are n states added to the m states already in the envelope after round i      Again  the expectations described above can be ob tained empirically  Coupled with the sort of expecta tions described for Case I  e g   E  tPa IIE I      one could  in principle  determine the optimal number of rounds k and the allocation to tEA  and tpa  for    j  k   In practice  we use slightly different statis tics and heuristic methods for deliberation scheduling to avoid the combinq torics  Case III  Single round  flexible policy genera tion Actually  this case is simpler in concept than  Case I  assuming that we can compile the following statistics  Case IV  Multiple round  flexible policy gener ation Again  with  tdditional statistics  e g    E V r  xo  V r     xo IIE       m  IE I   m n  tpa      this case is not much more difficult than Case II       Algorithms and Experimental Results  Our initial experiments are based on stochastic au tomata with up to several thousand states  automata were chosen to be small enough that we can still compute the optimal policy using exact techniques for comparison  but large enough to exercise our ap proach  The domain  mobile robot path planning  was chosen so that it would be easy to understand the poli cies generated by our algorithms  For the experiments reported here  there were     locations that the robot might find itself in and four possible orientations re sulting in     states  These locations are arranged on a grid representing the layout of the fourth floor of the Brown University Computer Science department  The robot is given a tasK to navigate from some starting location to some target location  The robot has five ac tions  stay  go forward  turn right  turn left  and turn about  The stay action succeeds with probability one  the other actions succeed with probability      except in the case of sinks corresponding to locations that are difficult or impossible to get out of  In the mobile robot domain  a sink might correspond to a stairwell that the robot could fall into  The reward function for the sequential des  ision problem associated with a given initial and target location assigns   to the four states corresponding to the target location and    to all other states  We gathered a variety of statistics on how extend ing the envelope increases value  The statistics that proved most useful corresponded to the expected im provement in value for different numbers of states added t o the envelope  Instead of conditioning just on the size of the envelope prior to alteration we found it necessary to condition on both the size of the envelope and the estimated value of the current policy  i e   the   Deliberation Scheduling for Time Critical Sequential Decision Making  value of the optimal policy computed by policy itera tion on the restricted automaton   At run time  we use the size of the automaton and the estimated value of the current policy to index into a table of performance profiles giving expected improvement as a function of number of states added to the envelope  Figure   de picts some representative functions for different ranges of the value of the current policy        Value       s oo           r      s   t                    r   hCXi ie                  n      t        t        t      J          i          l  t        t    r        t     j                            i             t     j            fl  L             t      t                                        Time  secoo ds   Figure    Comparison of the greedy algorithm with standard  inflexible  policy iteration and interruptable  flexible  policy iteration                              Figure    Expected improvement as a function of the number of states n added to initial envelope of size m In general  computing the optimal deliberation sched ule for the multiple round precursor deliberation mod els described above is computationally complex  We have experimented with a number of simple  greedy and myopic scheduling strategies  we report on one such strategy here  Using the mobile robot domain  we generated         data points to compute statistics of the sort shown in Figure   plus estimates of the time required for one round of envelope alteration followed by policy gen eration given the size of the envelope  the number of states added  and value of the current policy  We use the following simple greedy strategy for choosing the number of states to add to the envelope on each round  For each round of envelope alteration followed by pol icy generation  we use the statistics to determine the number of states which  added to the envelope  max imizes the ratio of performance improvement to the time required for computation  Figure   compares the greedy algorithm with the standard  inflexible  pol icy iteration on the complete automaton and with an interruptable  flexible  version of policy iteration on the complete automaton  The data for Figure   was determined from one representative run of the three algorithms on a particular initial state and goal  In another paper   Dean et al         we present results for the average improvement of the start state under the policy available at time t as a function of time         Recurrent Deliberation The Model  In recurrent deliberation models  the agent has to re peatedly decide how to allocate time to deliberation  taking into account new information obtained during execution  In this section  we consider a particular  model for recurrent deliberation in which the agent al locates time to deliberation only at prescribed times  We assume that the agent has separate deliberation and execution modules that run in parallel and com municate by message passing  the deliberation module sends policies to the execution module and the execu tion module sends observed states to the deliberation module  We also assume that the agent correctly iden tifies its current state  in the extended version of this paper  we consider the case in which there is uncer tainty in observation  We call the model considered in this section the dis crete  weakly coupled  recurrent deliberation model  It is discrete because each tick of the clock corresponds to exactly one state transition  recurrent because the exe cution module gets a new policy from the deliberation module periodically  weakly coupled in that the two modules communicate by having the execution mod ule send the deliberation module the current state and the deliberation module send the execution module the latest policy  In this section  we consider the case in which communication between the two modules occurs exactly once every n ticks  at times n   n   n         the deliberation module sends off the policy generated in the last n ticks  receies the current state from the ex ecution module  and begins deliberating on the next policy  In the next section  we present an algorithm for the case where the interval between communications is allowed to vary  In the recurrent models  it is often necessary to remove states from the envelope in order to lower the compu tational costs of generating policies from the restricted automata  For instance  in the mobile robot domain  it may be appropriafe to remove states corresponding to portions of a path the robot has already traversed if there is little chance of returning to those states  In general  there are many more possible strategies for deploying envelope alteration and policy generation in recurrent models than in the case of precursor mod els  Figure   shows a typical sequence of changes to the envelope corresponding to the state space for the restricted automaton  The current state is indicated        Dean et al   Find path to the goal   Extend the envelope  Extend and then prune the envelope      Find path back to the  interval  the execution module is given a new policy       and the deliberation module is given the current state x   It is possible that x  is not included in the enve lope for       if the reflexes do not drive the robot inside the envelope then the agent s behavior throughout the next n tick interval will be determined entirely by the reflexes  Figure   shows a possible run depicting inter vals in which the system is executing reflexively and intervals in which it is using the c urrent policy  for this example  we assume reflexes that enable an agent to remain in the same state indefinitely   Let  n  x  r  x   be the probability of ending up in x  starting from x and following  r for n steps  Suppose that we are given a set of strategies  F  F      As Extend and then prune the envelope is usual in such combinatorial problems with indefi nite horizons  we adopt a myopic decision model  In particular  we assume that  at the beginning of each n tick interval  we are planning to follow the current policy  r for n steps   follow the policy F  r  generated Figure    Typical sequence of changes to the envelope by some strategy F attempting to improve on  r for the next n steps  and thereafter follow the optimal policy intervals during which the system is executing reflexively  r   If we assume that it is impossible to get to a goal    n  n n  n state in the next  n steps  the expected value of using strategy F is given by falls outofthe envelo  n l n curre nt state happens tt be contaied in the new envelo Z  l i  L  n  x  r x   L n  x  F   r  x  V      x          I  falls out of the envelope again     r  ctuTent state is not in the new envelope     current state is in the new envelope  Figure    Recurrent deliberation by   and the goal state is indicated by D  To cope with the attendant combinatorics  we raise the level of abstraction and assume that we are given a small set of strategies that have been determined empirically to improve policies significantly in vari ous circumstances  Each strategy corresponds to some fixed schedule for allocating processor time to envelope alteration and policy generation routines  Strategies would be tuned to a particular n tick deliberation cy cle  One strategy might be to use a particular pruning algorithm to remove a specified number of states and then use whatever remains of the n ticks to generate a new policy  In this regime  deliberation scheduling consists of choosing which strategy to use at the begin ning of each n tick interval  In this section  we ignore the time spent in deliberation scheduling  in the next section  we will arrange it so that the time spent in deliberation scheduling is negligible  Before we get into the details of our decision model  consider some complications that arise in recurrent deliberation problems  At any given moment  the agent is exec uting a polic y  call it  r  defined on the cur rent envelope and augmented with a set of reflexes for states falling outside the envelope  The agent begins exec uting  r in state x  At the end of the c urrent n tick  x EX  i O          x EX  l  where            i  a discounting factor  controlling the degree of influence of future results on the current decision  Extending the above model to account for the possi bility of getting to the goal state in the next  n steps is straightforward  computing a good estimate of v     is not  however  We might use the value of some pol icy other than  r   but then we risk choosing strategies that are optimized to support a particular suboptimal policy when in fact  the agent should be able to do much better  In general  it is difficult to estimate the value of prospects beyond any given limited horizon for sequential decision problems of indefinite duration  In the next section  we consider one possible practical expedient that appears to have heuristic merit       Algorithms and Experimental Results  In this section  we present a method for solving recurrent deliberation problems of indefinite duration using statistical estimates of the value of a variety of deliberation strategies  We deviate from the decision model described in the previous sec tion in one addi tional important way  we allow variable length inter vals for deliberation  Although fixed length facilitate exposition  it is much easier to collect useful statistical estimates of the utility of deliberation strategies if the deliberation interval is allowed to vary  For the remainder of this section  a deliberation strat egy is just a particular sequence of invocations of enve lope alteration and policy generation routines  Delib    Deliberation Scheduling for Time Critical Sequential Decision Making  eration strategies are parameterized according to at tributes of the policy such as the estimated value of policies and the size of the envelopes  The function EIV  F V    IE  l  provides an estimate of the expected improvement from using the strategy F assuming that the estimated value of the current policy and the size of the corresponding envelope fall within the speci fied ranges  This function is implemented as a table in which each entry is indexed by a strategy F and a set of ranges  e g      minV   maxV      miniE  I maxiE             online deliberation scheduling   We determine the EIV function off line by gathering statistics for F running on a wide variety of policies  The ranges are established so that  for values within the specified ranges the expected improvements have low variance  At run time  the deliberation scheduler computes an estimate of the current policy V    deter mines the size IE    I of the corresponding envelope and chooses the strategy F maximizing EIV  F  V    IE       To avoid complicating the online decision making  we have adopted the following expedient which allows us to keep our one step lookahead model  We modify the transition probabilities for the restricted automaton so that there is always a non zero probability of getting back into the envelope having fallen out of it  Exactly what this probability should be is somewhat eompli cated  The particular value chosen will determine just how concerned the agent will be with the prospect of falling out of the envelope  In fact  the value is depen dent on the actual strategies chosen by deliberation scheduling which  in our particular case  depends on EIV and this value of falling back in  We might pos sibly resolve the circularity by solving a large and very complicated set of simultaneous equations  instead  we have found that in practice it is not difficult to find a value that works reasonably well   To build a table of estimates of function EIV off line  we begin by gathering data on the performance of strategies ranging over possible initial states  goals  and policies  For a particular strategy F   initial state x  and policy  r  we run F on  r  determine the elapsed number of steps k  and compute estimated improve ment in value   The experimental results for the recurrent model were obtained on the mobile robot domain with      possi ble locations and hence      states  The actions avail able to the agent were the same as those used to obtain the precursor model results  The transition probabil ities were also the same  except that the domain no longer contained sinks        Yi     Yk  x  k x            x  VF  r  x     V   x    where the first term corresponds to the value of using  r for the first k steps and F   r  there after and the second term corresponds to the case in which we do no deliberation whatsoever and use  r forever  As in the model described in the previous section  we assume that the goal cannot be reached in the next k steps  again it is simple to extend the analysis to the case in which the goal may be reached in less than k steps  Given data of the sort described above  we build the table for EIV  F V    IE   I   by appropriately dividing the data into subsets with low variance  One unresolved problem with this approach is exactly how we are to compute V     x   Recall that  r is only a partial policy defined on a subset of X augmented with a set of reflexes to handle states outside the cur rent envelope  In estimating the value of a policy  we are really interested in estimating the value of the aug mented partial policy  If the reflexes kept the agent in the same place indefinitely  then as long as there was some nonzero probability of falling out of the envelope with a given policy starting in a given state the actual value of the policy in that state would be            Of course  this is an extremely pessimistic estimate for the long term value of a particular policy since in the recurrent model the agent will periodically compute a new policy based on where it is in the state space  The problem is that we cannot directly account for these subsequent policies without extending the horizon of the myopic decision model and absorbing the associ ated computational costs in offline data gathering and       We used a set of    hand crafted strategies  which were combinations of envelope optimization  a  and the following types of envelope alteration     findpath  FP   if the agent s current state X cur is not in the envelope  find a path from Xcur to a goal state  and add this path to the envelope    robustify  R N    we used the following heuris tic to extend the envelope  find the N most lkely fringe states that the agent would fall out of the envelope into  and add them to the envelope    prune  P N    of the states that have a worse value than the current state  remove the N least likely to be reached using the current policy  Each of the strategies used began with findpath and ended with optimization  Between the first and last phases  robustification  pruning and optimization were used in different combinations  with the number of states to be added or deleted E                    exam ples of the strategies we used are  FP R     a    FP P     a    FP P     R     o    FP R      P          FP R     a P         We collected statistics over about      runs generat ing         data points for strategy execution  The start goal pairs were  chosen uniformly at random and we ran the simulated robot in parallel with the plan ner until the goal was reached  The planner executed the following loop  choose one of the    strategies uni formly  at random  execute that strategy  and then pass the new policy to the simulated robot  We found the following conditioning variables to be significant  the envelope size  lEI  the value of the current state V    the  fatness  of the envelope  the ratio of envelope        Dean et al   size to fringe size   and the Manhattan distance  M  between the start and goal locations  We then build a lookup table of expected improvement in value over the time the strategy takes to compute   V      k  as a function of E  V     the fatness  M and the strategy s  To test our algorithm  we took    pairs of start and goal states  chosen uniformly at random from pairs of Manhattan distance less than one third of the diameter of the world  For each pair we ran the simulated robot in parallel with the following deliberation mechanisms   recurrent deliberation with strategies chosen us ing statistical estimates of EIV  LOOKUP   dynamic programming policy iteration over the entire domain  with a new policy given to the robot after each iteration  ITER  and only after it has been optimized   wHOLE   The average number of steps taken by LOOKUP  and WHOLE were        and     respectively  ITER  W hile the improvement obtained using the recurrent deliberation algorithm is only small it is statistically significant  These preliminary results were obtained when there were still bugs in the implementation  how ever  since we have determined that the strategies are in fact being pessimistic  we expect to obtain further performance improvement using LOOKUP  Recall also that we are still working in the comparatively small domain necessary to be able to compute the optimal policy over the whole domain  for larger domains  ITER and WHOLE are computationally infeasible     Related Work and Conclusions  Our primary interest is in applying the sequential de cision making techniques of Bellman  Bellman        and Howard  Howard        in time critical applica tions  Our initial motivation for this research arose in attempting to put the anytime synthetic projec   tion work of Drummond and Bresina  Drummond and Bresina        on more secure theoretical foundations  The approach described in this paper represents a particular instance of time dependent planning  Dean and Boddy        and borrows from  among others  Horvitz   Horvitz        approach to flexible compu tation  Hansson and Mayer s BPS  Bayesian Problem Solver   Hansson and Mayer        supports general state space search with decision theoretic control of in ference  it may be that BPS could be used as the basis for envelope alteration  Boddy  Boddy        describes solutions to related problems involving dynamic pro gramming  For an overview of resource bounded de cision making methods  see chapter   of the text by Dean and Wellman  Dean and Wellman         We have presented an approach to coping with un certainty and time pressure in decision making  The approach lends itself to a variety of online computa tional strategies  a few of which are described in this paper  Our algorithms exploit both the goal directed   state space search methods of artificial intelligence and the dynamic programming  stochastic decision making methods of operations research  Our empirical results demonstrate that it is possible to obtain high perfor mance policies for large stochastic processes in a man ner suitable for time critical decision making  Acknowledgements  Tom Dean s work was supported in part by a Na tional Science Foundation Presidential Young Investi gator Award IRI          by the Advanced Research Projects Agency of the DoD monitored by the Air Force under Contract No  F         C       and by the National Science foundation in conjunction with the Advanced Research Projects Agency of the DoD under Contract No  IRI          Leslie Kaelbling s work was supported in part by a National Science Foundation National Young Investigator Award IRI        and in part by ONR Contract N              ARPA Order       Thanks also to Moises Lejter for his input during the development and implementa tion of the recurrent deliberation model  
