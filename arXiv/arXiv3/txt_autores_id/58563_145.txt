  These values  referred to as query responses  clearly de pend on the training sample used to instantiate the param  A Bayesian Belief Network  BN  is a model of  eter values   i e   different training samples will produce  a joint distribution over a finite set of variables   different parameters and hence different responses   with a DAG structure to represent the immedi  This paper investigates how sampling variability in the  ate dependencies between the variables  and a set of parameters  aka CPTables  to represent the  training data is related to uncertainty about a query re  local conditional probabilities of a node  given  sponse  We follow the Bayesian paradigm  where uncer  each assignment to its parents  In many situa  tainty is quantified in terms of random variation  and we  tions  the parameters are themselves treated as  present a technique for computing Bayesian credible in  random variables  reflecting the uncertainty re  tervals  aka  error bars   for query responses  Our algo  maining after drawing on knowledge of domain  rithm takes as inputs a belief net structure  which we as  experts and or observing data generated by the  sume is correct  i e   an accurate   map of true distribu  network  A distribution over the CPtable param  tion  Pea      a data sample generated from the true belief  eters induces a distribution for the response the  net distribution  and a specific query of the form   What is  BN will return to any  W hat is  Q    Pr  HIE      Pr  H     hIE    e       After determining the  This paper investigates the distribution  conditional  posterior  distribution of the belief net param  of this response  shows that it is asymptotically  eters given the sample  the algorithm produces an estimate  query    posterior mean value  of Q  e g   estimate Q to be  asymptotic variance  We show that this compu  To quantify uncertainty about this estimate  the algorithm  tation has the same complexity as simply com  computes an approximate posterior variance for Q and uses  puting the  this variance to construct error bars  a Bayesian credible in   mean value of the  response  i e    O n exp w    ables and  w  where  n  terval  for Q  e g   assert that Q is in the interval  is the number of vari  is the effective tree width   with     probability   We  also provide empirical evidence showing that the            There are several obvious applications for these error bars   error bars computed from our estimates are fairly  First  error bars can help a user make decisions  especially  accurate in practice  over a wide range of belief  in safety critical situations   e g   take action if we are  net structures and queries        sure that Q    Pr  H     hIE    e     is on one  side of a decision boundary  Second  error bars can     Introduction  can make appropriate guarantees about the answers to cer  model of a joint probability distribution  are used in an ever increasing range of applications  Hec     nets  are  typically  built by  Be  first finding an ap  propriate structure  either by interviewing an expert  or by  selecting  a  good model  from training  data    then using a training sample to fill in the parame ters  sug  gest that more training data is needed before the system  Bayesian belief nets  BNs   which provide a succinct  lief        normal  and derives expressions for its mean and   Hec     T he resulting belief net is then used to an  swer questions  e g   compute the conditional probability  tain queries  This information is especially valuable when additional training data  while available  is costly  and its acquisition needs to be justified  Similarly  the user might decide that more evidence is needed about a specific in stance  before he can render a meaningful decision   Fi  nally  if an expert is available and able to provide  correct answers  to some specific questions  error bars can be used to validate the given belief net structure  E g   if the expert claims that Q         but our algorithm asserts that Q is in   UAJ       VAN ALLEN ET AL      a I                                       G l  x   x   Figure      o       Simple Example  Diamond Graph  the interval            with       probability  then we may question whether the structure provided is correct  as suming we believe the expert   By contrast  we might not question this structure if our algorithm instead asserted that Q is in the interval            with       probability  Section   provides background results and notation con cerning belief nets and Dirichlet distributions for belief net parameters  Section   presents the theoretical results under lying our error bars  a derivation of an approximate poste rior variance for a query probability Q  and a proof that the posterior distribution of Q is asymptotically normal  Com putational issues related to calculation of the variance are briefly discussed  Section   presents the results of an em pirical study using Monte Carlo simulations to validate our error bar methodology over a wide range of belief net struc tures and queries  Section   briefly surveys related work  placing our results in context            Belief nets and Dirichlet distributions  We encode the joint distribution of a vector of discrete ran dom variables X    Xv vEV as a belief net  aka Bayesian network  probability net   A belief net  V  A     is a directed acyclic graph whose nodes V index the random variables and whose arcs A represent dependencies  Let Pa v  C V be the immediate parents of node v  and let Fv    Xw wEPa v  be the corresponding vector of parent variables  In a belief net  a variable Xv is independent of its nondescendents  given Fv  The elements of the vector   are the CPtable entries  Let Xv and Fv   TiwEPa v Xw be the domains of Xv and Fv We assume that the domains are finite  The CPtable for Xv contains IXvl X IFvl entries  v xJf Figure   provides a simple example of a belief network with specific CPtable entries  Here X  has no parents  so we write F       We have F        Xt   Fa   Xt   F      X   Xa   and for each value a  b  c  d  we have    al    Pr  xl   a IE    e  bJa   Pr  x    b I Xt   a  E     and e  djb c   Pr   x    d I x    b  X a   c  e      Hence  using Figure    we have                Note that the values in each row add up to    In general  the variables need not be binary  but can have larger  finite  domains  The CPtable entries are estimated using training data and  possibly  expert opinion  The latter information is incor porated using the Bayesian paradigm  where   is mod eled as a random variable and expert opinion is expressed through an a priori distribution for    We adopt indepen dent Dirichlet priors  for the various CPtable rows  Specit cally  let  vl      v zl  xEX  denote the CPtable row for Fv   f  e g   e J                                 denotes the entries for the X  variable associated with the parental assignment X      and X       We assume that  be fore observing the training data  the evil are independent  Dir  a        x E Xv   random vectors  where a            An absence of expert opinion is often expressed by setting a  xlf     for all  v x  f   e g     J   o    Dir  I       which yields a uniform  flat  prior  Stronger opinion is expressed through larger values of a  v x     Expressions for the mean and variance of a Dirichlet distribution are given below  Now suppose that the training data consist of m indepen dent replicates of vectors X  generated using the given structure and a fixed set of CPtable entries e  Let mv xlf denote the number of cases in the training set with  Xv  Fv     x  f   Under the posterior distribution  the conditional distribution given the training data   the E  vlf are independent Vir  O v zl  x E Xv  random vectors  with O v xlf   a  xlf   m v xlf  BFH     This posterior distri bution underlies our derivation of Bayesian credible inter vals  Several properties of the Dirichlet distribution will be needed  Setting O v  J    l  xEXv O v xJ f the posterior means and  co variances for CPtable entries are  BFH         E G v xJf      Cov  v zJ   v yJ       f lv xlf  O v zl              O v  J  llv xjt c xy  llv yJf      O v  J       Readers unfamiliar with these assumptions  or with Dirichlet  distributions  are referred to  Hec     Note that a Dirichlet distri bution over a binary variable is a Beta distribution    VAN ALLEN ET AL          if x    y and c  y     otherwise  The ran Elvlf are asymptotically normal  in the limit as min  av xl    t oo  Aki     More precisely  the nor malized variables    av   f  v x J     Lv xJJ  converge in  where  c xy    dom vectors  distribution to jointly normal random variables with mean zero and covariances  J tv xJJ o y  J tv yJJ    This asymp  totic framework is applicable as the amount of training data increases   v xJJ   m   t oo  provided all of the CPtable entries  are positive  This condition occurs with probability  one under a Dirichlet prior      Bayesian Credible Intervals for Query Re sponses  It is well known that the CPtable entries determine the  joint distribution of X   Pr  Xv   Xv  v  E  vIe      I vEV ev x  II   where   vlvEV is determined by  xv   vEV   see  Pea     Users are typically interested in one or more  specific  queries  asked of this joint distribution  where a query is expressed as a conditional probability of the form Q     q B      Pr H hiE e e          UAI      partial derivatives  Let Pv  h  x fIe  denote the probabil  ity  Pr  H  h  Xv   x  Fv  fIE  e  e and let pv  x   f     f       le  Pv h f le  pv f le   andp h le  be  defined in a similar manner  Note that the subscript  v  is  Xv or F u is involved  and all probabilities are evaluated at e     Let q    zl denote the partial derivative  q B   Bv zlf evaluated at      IL We will use the following identity  derived by needed to identify the node when      GGS    DarOO      qv zlf  Pv h x f le    p h le pv x f le  J tv zlf       We now derive an expression for  i       S   and demonstrate  asymptotic validity of the credible interval  Equation      given a sufficiently large training sample     We assume that   is a random vector with posterior Dirichlet distribution described in Section    and approximate the variance of Q q    by  Theorem     where H and E are subvectors of X  and h and e are legal  assignments to these subvectors  Note also the dependency one  In our Bayesian context  Q is a random variable with a  the oretically  known distribution determined by the posterior  ij  the posterior mean J LQ    E  Q    the iden tity  CH        E  q     Set It    defined  E E    where the components  by Equation  J tv zJ   of     are      W hile a point estimate    Q     q t   can be useful  one  often requires some information concerning the potential error in the estimate   In the Bayesian context  this can  be achieved by plotting the posterior distribution of Q  Alternatively  one may construct a           r     cally not analytically tractable  but simple approximations We  ill show that the distribution of Q is  w  approximately normal  and derive an approximation i Q for  the standard deviation of Q  We then propose the following interval as an approximate        J LQ       r    credible interval   zo   ifQ   where zo      I              is the upper standard normal distribution        J    value of the  Our derivation is based on a first order Taylor expansion of  q E    about  q J     Some notation is needed to express the     Bvf    av  Jf                Pv h x  Je  p hle pv x f e P   J t v  zlf  Consider an asymptotic framework where the poste rior means J tv xlf are fixed  positive values  and min   nv zl   T hen the random variable    oo   Q  J LQ   i Q converges in distribution to the standard nonnal distribution  Proof  Our proof uses the Delta method  sider the Taylor expansion  credi  ble interval for Q  i e   an interval  L  U  defined so that Pr  L      Q      U       o  Exact calculations are typi  are available   L  T his value can be calcu  q  E E       vEV  E Fv  where  distribution of e  For a point estimate of Q  one may use  lated using  L L  Avt     q        q fJ     D      BFH     Con  R   where  D  L L L q l l   E v zlf    Jv z f     vEV  E Fv zEXv       and the remainder term R can be expressed in terms of the matrix of second derivatives of q  e  evaluated at a pointe between Band J t  Since the variances for E u zJ  in EqUCJ tion   are of order   av zlf   t    and since the second derivatives remain bounded in a neighbourhood of f i  the remainder R is asymptotically negligible compared with D    b  We define to be the variance of D  Equation     As the CPtable rows El vlf are statistically independent  but   UAI      VAN ALLEN ET AL   e ntries within a row are correlated  the variance of be expressed as  D  can       Table    Gold Standard for Validity Estimates d  After substituting Equation   for the covariances and sim plifying  we obtain Equation   with  L  q xlt   Lx vlf   Avf  A substitution of Equation   then yields the equivalent ex pressions or Avt and Bvt within Equation     Dfuq                   Mean                        Std Dev                        q xlf in time O n w    see  DarOO   Given these deriva tives  the summations in Equation   can be performed with one additional pass over the values  of time    n    The extended paper  VGHOl  describes an algorithm for computing UQ  The main challenge  computing all of the derivatives q xlf  is accomplished by  back propagat ing  intermediate  results obtained by the Bucket Elimina tion  Dec    algorithm   We observe that is a random variable with mean   and variance    It remains to show that D   ijQ is asymp totically normal  This result follows from the asymptotic multivariate normality of the components of    after suit able standardization  see Section     and the fact that D   is a linear function ofEl    VGHOl  also provides additional comments on the proper interpretation and application of this theorem   There are exceptional situations where the posterior distribution of q    is analytically tractable and exact credible intervals are available  In the degener ate situation where the network structure has arcs connect ing all pairs of nodes  and hence imposes no assumptions about conditional independence   the assumption of inde pendent Dirichlet distributions for CPtable rows is equiva lent to an assumption of a single Dirichlet distribution over unconditional probabilities Pr  Xv   Xv  v E V   It is then straightforward to derive the distribution of the query probability using properties of the Dirichlet distribution  see  Mus      Note that this exact approach is not cor rect in general i e   it does not hold for networks with non trivial structure            Empirical Study  Theorem   proves that the interval   LQ  z   uq is asymp totically valid  More precisely  let  Degenerate Case      The computational problem of computing J LQ   q p   is known to be NP hard  Coo     when all variables Xv are binary  the most effective ex act algorithms require time O n w   where n   lVI is the number of nodes and w is the induced tree width of the graph  Dec    LS     The variance uq can also be com puted in time O n w   This result follows from the exis tence of algorithms that can compute all of the derivatives Computational Issues   Assuming a uniform prior and a sample of size m  we can compute the posterior variance of Pr  HIE  as P HIE  x   F HIE      m x P E       where F x  is the expected value of x  wrt the given belief net   This follows from a dimensionality argument  in a non trivial structure  the  n dimensional vector of unconditional probabili ties is constrained to lie in a lower dimensional submanifold of t e  n    dimensional simplex  This cannot be represented by a smgle Dirichlet distribution because  wpl  the constraints would not be satisfied       be the probability that the query response Q falls outside of the credible interval  based on our UQ estimate of standard deviation  Equation    The values   o and    are the nominal and actual coverage probabilities for the credible interval  The value is a function of o  the graph  V  A   the query q  and the posterior distribution of    The pos terior distribution depends on the prior distribution and the training sample  Thus typically varies from one applica tion to the next  While Theorem   implies that  o when the training sample is sufficiently large  it does not tell us whether this approximation is valid in practice  particularly for small samples  In general  the validity of the approxi mation depends on all of the factors determining   We carried out a number of experiments to assess how these factors affect validity        Given a fixed set of factors  we estimate the correspond ing by a simple Monte Carlo strategy  Using the  fixed  posterior distribution of    calculate ILQ and uq  Simulate r replicates ei from the posterior distribution  calculate Qi   q      then let    be the proportion of the  Qi  with IQ    LQI   Zof UQ  In our experiments  each  was based on r     replicates        To quantify the validity of the approximation employ average absolute differences  validity estimate     average I    Jl     o  we       The absolute differences are averaged as we vary one or more of the the factors determining   The validity es timates are presented as percentages in our tables  When   VAN ALLEN ET AL        Diamood Gqaph Qu riat wit   lilO  E rorBa   Ee  r        m  UAI            Analyiic  r    QyeryII OnM   Mon le Carlo        D             lo    i                      QO      Figure     Results for the Diamond Graph  We studied the following inferential patterns in the dia mond graph  Figure I   Pr Xl       IE    Q    Pr X       IX        IX X   Qi           el ll         E           E    Q      Pr Xt  Q  Qs     IIX       E   Pr X X    IX     E   Pr X   Q                    Standard Normal Ouanmea             B  QQ plot showing relation to Normal J E                        The resulting validity es timates are listed in Table    Each cell in the table is an average of    values   Figure   A  shows the error bars returned by our approx imation  and also the Monte Carlo system  on a random network posterior  for the error bars for     credible inter vals  We see the two methods give similar answers  Figure   B  uses a quantile quantile  QQ  plot to address the validity of the normality assumption  independently of the linear approximation  Each  line  in this figure corre sponds to z scores of the     query responses generated by our Monte Carlo simulation  plotted against standard normal quantiles  This figure shows six such lines  each corresponding to a single query in   Q          Q    given a sample of size m     A straight line would correspond to data produced by a  perfect  normal distribution  we see each dataset is close   Of course  this is only suggestive  the real proof comes first from Theorem    and then from the data  e g   Table    which demonstrates that our approach  which assumes normality  produces reasonable results                La e  lla e    o       e      e  qt e   IO  La e  tla e  t a e    o                       Lb c e  t b c e  bll es c   e   IO L   b c e  llb c e  bla e  cla e    o     Pr X           A  Examples of Error Bars   viewing these values  it is helpful to have a gold standard for comparison  Consider the validity estimate          for a single    l  The minimum expected value is obtained when    l    S  i e   when       has the Binomial lOO  S  distribu tion  Table   presents means and standard deviations under these ideal circumstances  Now suppose a validity estimate is obtained by averaging k independent terms  Its standard deviation is typically greater than the value Std Dev   v k suggested by Table   because there is usually variation in the underlying  values              IXt        E       L o c    tlb c    olt    cll  The six queries cover a range of different inferential pat terns  The first is basically a  sanity check   as it is a triv ial inference  the fourth is also straightforward  although it does involve a multiplication  The sixth is slightly more complex  but it is still only a summation of a set of prod ucts  The remaining queries involve divisions of increas ingly complicated expressions  For each m E                    we carried out    trials of the form      generate E  from a uniform Dirichlet prior distribution      generate a training sample of size m based on E  and use the result to obtain a posterior dis tribution      generate     Monte Carlo replicates from the posterior distribution and use these to obtain an esti mate    for each pair  Q  J   for Q E  QI        Q   and       Results for Alarm Network  The Alarm network  BSCC    is a benchmark network based on a medical diagnosis domain  commonly used in belief network studies  The network variables are all dis crete  but many range over   or more values  The network includes a CPtable for each node  i e   a particular   is spec ified  Table   summarizes the results for experiments on the Alarm network  where we varied both J and m  For each m  we generated a single random sample of size m from    and used this to determine a posterior distribution  as suming a uniform prior   Validity estimates were obtained by averaging over randomly chosen queries  The queries Pr H  hIE  e     were chosen by determining an as signment H   h to one randomly chosen query variable  and assignments E   e to five randomly chosen evidence variables   Here  we used  HC    to determine which vari         VAN ALLEN ET AL   UAI      Table    Results for Random Networks  E  H          Table    Results for Diamond Graph m  Ql  Q                                                                                                                                                                                                                                                                                                                                                                                                                                                          Q   Q   Q   Q                                                                                                                                                                                                                       fJ                                                                                                                                   abies could be query as opposed to evidence variables   Some or all of the evidence variables might have had no effect on the query variable  others might have had a pro found effect  Each cell in Table   represents an average from     queries on a single posterior distribution        Results for Random Networks  Although random networks tend not to reflect typical  or natural  domains  they complement more focussed studies by exposing methods to a wide range of inputs and help to support claims of generality  We carried out experiments on networks with    binary variables and    links  gener ating gold models from a uniform prior distribution on e  and generating random queries of various types  Here we used sample size m        throughout  and varied the type of query  Table   displays the results of our experiments  Each query was of the form Pr H  hIE  e      with varying dimensionalities forE and H  Let  E and  H denote the number of variables comprising E and H  re spectively  Each cell of Table   is based on     trials  I   queries on    networks  with both structure and posterior generated randomly                                                                                                                                                                                                                                                                                                                                                                                                                           S                                     Table    Results for Alarm Network m                                                                                                                                                                         Discussion  Our hypothesis was that our Bayesian error bars algorithm would be accurate for essentially all cases  We tried to falsify our hypothesis by varying the following experimental factors    Network structure  V  A     Credibility level      Query type  Diamond network  Alarm     Number of evidence variables  Random networks     Number of query variables  Random networks      c   In no case did we observe a result where averagelto    exceeded      In most cases  the validity estimate was less than      As noted in Table    even if our error bars were exact  we would still get positive validity estimates due to the variance in to about   We therefore believe that these results comfortably bound the expected error of our method under the experimental conditions   None of the factors that we manipulated had a profound effect  T he strongest ef fect  observed in Table    was that increasing the number of variables assigned in a query tended to increase the er ror I     see also  Kle     One possible explanation is that  as  E and  H increase  the query function q tends to become more complex  and the local linear approxima tion of q becomes less reliable  Another possibility is that        VAN ALLEN ET AL        the query probability  Q  tends to become very small  mak  UAI      bution over CPtables  but for different purposes  For exam  ing the normal approximation less accurate  Further exper  ple  Cooper and Herskovits  CH    use it to compute the  iments could address this issue   expected response to a query  by contrast  we also approx  We found these results very encouraging   Our method  appears to give reasonable error bars for a wide range of queries and network types   This makes the technique a  promising addition to the array of data analysis tools re lated to belief networks  especially as the algorithm is rea sonably efficient   only  roughly doubling the computation time per inference  W hile there may be pathological cases  imate the posterior variance in that response  Similarly  while many BN learning algorithms compute the posterior distribution over CPtables  Hec     most of these systems seek a single set of CPtable entries that maximize the like lihood  which again is different from our task   e g    their  task is not relative to a specific query  but see  GGS      Many other projects consider sensitivity analyses  provid  where our method will not give reasonable results   per  ing mechanisms for propagating ranges of CPtable values  haps because the local linear approximation and the asymp  to produce a range in the response  cf    BKRK    Las     totic normality are far off the truth   we  did not find such  CNKE    DarOO   W hile these papers assume the user is explicitly specifying the range of a local CPtable value  our  cases in our experiments   work considers the source of these variances based on a Other Experiments   We also ran a number of other ex  periments  One set computed the average  A      data sample  This also means our system must propagate  scores  all of the  ranges   most other analyses consider only prop  in each situation  to determine if there was any systematic  agating a single range  The  DarOO  system is an excep     bias   Note this score differs from Equation   by not tak  tion  as it can simultaneously produce all of the derivatives   ing absolute values   We found that our bounds were typi  However  Darwiche does not consider our error bar appli  cally a bit too wide for most queries  i e   we often found the        a interval included slightly more than          of the  cases  We are currently investigating this  to see if there are straight forward refinements we can incorporate   cation  and so does not include the additional optimizations we could incorporate  Excluding the  DarOO  result  none of the other projects provides an efficient way to compute that information   We also computed error bars based on the  incorrect    Also  some of those other papers focus on properties of   complete structure  assumption  which implies the re  this derivative   e g   when it is  sponse will have a simple Dirichlet distribution  see Foot  able entry   note  ately from our expression  Equation     Finally  our anal     We found that  as anticipated  the approach de  scribed in this paper  using Equation    consistently out     for some specific CPt  Note that this information falls out immedi  ysis holds for arbitrary structures  by constrast some other  performed that case  in that our approach was consistently  results  e g    CNKE     deal only with singly connected  closer to the Monte Carlo estimates   networks  trees     VGHOl  discusses these results in detail   It also inves  Lastly  our analysis also connects to work on abstractions   tigates techniques for dealing with extreme values  where  which also involves determining how influential a CPtable  the normal distribution may be sub optimal   entry is  with respect to a query  towards deciding whether     typically computational efficiency in computing that re  to include a specific node or arc  GDSOl   Their goal is  Related Work  sponse  By contrast  our focus is in computing the error Our results provide a way to compute the variance of  bars around the response  independent of the time required  a BN s response to a query   to determine that result   which depends on the  posterior distribution over the space of CPtable entries  based on a data sample  method   BFH      This is done using the  Delta  first determine the variance of each  CPtable row  then propagate this variance using a sensi tivity analysis   i e   the partial derivatives   see Equation     Kleiter  Kle    performs a similar computation  parts of his analysis are more general  in that he considers incomplete      discuss how to deal structures      show how to deal  data  However  he does not  with  general graphical  with  the correlations encountered with general Dirichlet distri butions  nor       provide an efficient way to compute this  information  Moreover  our empirical data provide addi tional evidence that the approximations inherent in this ap proach are appropriate  even for small sample sizes  Several other researchers also consider the posterior distri      Conclusion  Further Extensions   Our current system has been im  plemented  and works very effectively  There are several obvious ways to extend it   One set of extensions corre  spond to discharging assumptions underlying Theorem      computing error bars when the data was used to learn the structure  as well as the parameters  dealing with param eters that are drawn from a distribution other than inde pendent Dirichlets  perhaps even variables that have con tinuous domains  dealing with a training sample whose instances are  not completely specified   Our work deals  with fully parameterized CP tables  It would be interesting to investigate techniques capable of dealing with CPtables   UAI      VAN ALLEN ET AL   represented as  say  decision tree functions  BFGK     etc  Contributions  Many real world systems work by rea soning probabilistically  based on a given belief net modeL When knowledge concerning model parameters is condi tioned on a random training sample  it is useful to view the parameters as random variables  this characterizes our uncertainty concerning the responses generated to specific queries in terms of random variation  Bayesian error bars provide a useful summary of our current knowledge about questions of interest  and so provide valuable guidance for decision making or learning   This paper addresses the challenge of computing the error bars around a belief net s response to a query  from a Bayesian perspective  We first motivated and formally de fined this task  finding the         o   credible interval for a query response with respect to its posterior distribu tion  conditioned on a training sample  We then investi gated an application of the  Delta method  to derive these intervals  This required determining both the covariance matrix interrelating all of the parameters  and the derivative of the query response with respect to each parameter  We produced an effective system that computes these quanti ties  and then combines them to produce the error bars  The fact that our approximation is guaranteed to be cor rect in the limit does not mean it will work well in practice  We therefore empirically investigated these claims  by test ing our system across a variety of different belief nets and queries  and over a range of sample sizes and credibility levels  We found that the method works well throughout   Acknowledgements We are grateful for the many comments and suggestions we received from Adnan Darwiche and the anonymous review ers  and for the fairness of the UAI     programme chairs  All authors greatfutly acknowledge the generous support provided by NSERC  iCORE and Siemens Corporate Re search  Most of this work was done while the first author was a student at the University of Alberta  
 A Bayesian belief network models a joint distribution with an directed acyclic graph representing dependencies among variables and network parameters characterizing conditional distributions  The parameters are viewed as random variables to quantify uncertainty about their values  Belief nets are used to compute responses to queries  i e   conditional probabilities of interest  A query is a function of the parameters  hence a random variable  Van Allen et al               showed how to quantify uncertainty about a query via a delta method approximation of its variance  We develop more accurate approximations for both query mean and variance  The key idea is to extend the query mean approximation to a doubled network involving two independent replicates  Our method assumes complete data and can be applied to discrete  continuous  and hybrid networks  provided discrete variables have only discrete parents   We analyze several improvements  and provide empirical studies to demonstrate their effectiveness      INTRODUCTION  Consider a simple example  Suppose A represents presence absence of a medical condition while B and Y are test results  Variables B and Y are conditionally independent given A  with A and B binary and Y continuous  The conditional independence assumption is represented by the directed acyclic graph structure in Figure   a   Let a   P  A   a   b a   P  B   b   A   a   and let p y   a   a   be the conditional density of Y given A   a  assumed normal with mean a and variance a    We want to estimate the probability that condition A is present given  specified results from the two tests B and Y   Let  represent all of the parameters  If  were known  we would use the formula  a b a p y   a   a     a  a  b a  p y   a    a     q     qa b y      P       In the Bayesian paradigm  uncertainty about  is quantified by modeling parameters as random variables  It follows that query probabilities such as     are also random  A query response is usually estimated by approximating its posterior mean  This approximation is similar to expression      but with a and b a replaced by their posterior means and with the normal densities replaced by Students t densities  One may want more than just a point estimate  Van Allen et al               showed  for discrete networks  how one can approximate the variance and posterior distribution of a query  Their variance derivation employs the delta method  i e   a first order Taylor series expansion of the function q   about the posterior mean of   They provide asymptotic theory and empirical experiments supporting this approach  They also showed how these approximations can be used to construct a Bayesian credible interval  error bars  for q    Guo and Greiner        applied this delta method approximation as part of a mean squared error  i e   squared bias   variance  measure designed to estimate the quality of different belief net structures when seeking a best classifier  Lee et al         provide a technique for combining independent belief net classifiers that involves weighting their respective mean probability values by their inverse variances  and they show that this works well in practice  We propose new approximations for the mean and variance based on a simple trick  Suppose  A    B    Y    and  A    B    Y    are replicates of the network variables  conditionally independent given   We represent the paired replicates as nodes in a doubled network with the same structure  see Figure    The squared query q    can be expressed as a query in this doubled net    UAI       b   a  b   a   b   a  b   a   HOOPER ET AL     a  a  A                                            b  a b  a    b   a  b   a   a    a     Y B       a    a       b   a  b   a  b   a  b   a        a  a   b   a  b   a  b   a  b   a  b   a  b   a  b   a  b   a   b   a  b   a  b   a  b   a  b   a  b   a  b   a  b   a   a  a   b   a  b   a  b   a  b   a  b   a  b   a  b   a  b   a   a  a   a  a       A    A                             a    a     a    a                a    a     a    a     Y    Y  B    B   a          a                   a         a           a a a a  Figure     a  A simple Bayesian network   b  The corresponding doubled network  Figure    A simple Bayesian net   work  P  A    A    a   B    B    b  Y    Y    y      The method used to approximate the mean of q   can be extended to the doubled network to approximate the mean of q    and hence to approximate the variance  Unlike the delta method  our approach does not rely on approximate local linearity of q    It does involve the addition of two incomplete observations to the data set when calculating the posterior mean of q      In some situations  this addition results in under estimation of the desired variance  This deficiency is largely eliminated by a simple adjustment  A similar adjustment substantially improves the usual query mean approximation  Section   reviews pertinent models and methods for belief networks  The network doubling technique is described in Section   for discrete  continuous  and hybrid networks  Proposed adjustments and numerical results are presented in Sections   and   for discrete networks  Corresponding work for continuous and hybrid networks is ongoing  Computational issues are discussed in Section    Contributions and plans for further work are summarized in Section            BACKGROUND NETWORK VARIABLES  We assume network structure is known  Let B denote a discrete network variable taking values b  DomB   Let Y denote a continuous network variable taking values y on the real line  Vectors of variables are denoted by boldface  A for discrete and X for continuous  Let  be a random vector comprising all unknown network parameters  i e    determines all conditional distributions of variables given their parents  We assume that discrete variables have only discrete parents  Suppose pa B    A  i e   the parents of B are the variables comprising the vector A  The conditional probability that B   b given A   a is denoted b a   B b A a   P  B   b   A   a         Variables associated with values will be clear from context  We employ similar abbreviations for other parameters and hyperparameters  The b a parameters are often presented in conditional probability tables  CPtables  with rows indexed by a and columns by b  e g   see Figure    Note that we use superscripts b    b  to list the distinct values in DomB   We use subscripts b    b  to denote arbitrary values in DomB   often related to replicated variables B    B    Continuous variables can have both discrete and continuous parents  Suppose pa Y     hA  Xi with X   hX            Xd i  The conditional distribution of Y is    Y   A   a  X   x     N     xT   a   a        i e   normally distributed  conditional mean related to x by a linear regression model with coefficients depending on a  Here xT is the transpose of the ddimensional column vector x while  a is an  d     dimensional column vector of regression coefficients  the first entry is the constant term        PRIOR AND POSTERIOR  The network parameters represented by  consist of CPtable parameters b a   regression coefficient vectors  a   and variances a    We assume the prior distribution for  has the following form  e g   see Gelman et al           CPtable rows follow Dirichlet distributions  B a    hb a   b  DomB i  Dir B a    where B a    hb a   b  DomB i    The regression coefficients and variance together have a normal  inverse chi square  distribution      a   a     Nd   a   a   a a      a     a  a     a    I e   dropping subscripts for a moment   conditioned on    is multivariate normal with mean        HOOPER ET AL  vector  and covariance matrix          and        has a   distribution with       not necessarily an integer   Note that        has mean   and variance       Parameters are assumed to be statistically independent except where joint distributions are specified above  In particular  we assume global independence  the parameters determining the conditional distribution of one variable given its parents are independent of all other parameters  The prior is conjugate  given a data set D consisting of n independent replicates of complete tuples of network variables  the prior hyperparameter values are updated as follows  Let nab and na be the number of tuples in D with  A  B     a  b  and A   a  respectively  Let  xi   yi   be the observations of  X  Y   for the na tuples with A   a  Let X a be the na   d      matrix with rows     xTi    Let y a be the column vector with entries yi   In the five equations below  the prior hyperparameter values appear on the right hand side and are identified with tildes  e g      b a   b a   nab a   a   na a a   a a   X Ta X a a a a   a a a   X Ta y a i h       a a   Ta a a   a a    Ta a a   y Ta y a P P The values a b b a and a a are called the effective sample sizes for variables B and Y   respectively  Our adjustments developed in Section   are motivated by large m asymptotics  where m is proportional to the effective sample size for each of the variables  i e     b a   mb a and a   ma    with  b a   a    a   a   a    fixed        Large m asymptotics are similar to but not the same as large n asymptotics  As the sample size n increases  the posterior mean E b a   D    b a   a varies and converges to some value   Here and elsewhere  the dot P subscript indicates summation   a   b b a    Under assumption      the posterior mean remains fixed as m varies       APPROXIMATING A QUERY MEAN  Consider a query involving outcomes of hypothesis variables H given values for evidence variables E  It is convenient to represent the query in terms of a function w H   E g   suppose H   A  E    B  Y    e    b  y   and q      P  A   a   B   b  Y   y      E w A    B   b  Y   y       UAI       where w A      for A   a and w A      otherwise  For discrete networks  query responses q   are usually estimated by q    where     E    D  is the posterior mean of the parameter vector  This plugin estimate usually differs slightly from the posterior query mean E q     D   Cooper and Herskovits        expression     showed that the plug in estimate equals E q     D  e   i e   the posterior query mean given an augmented data set consisting of D and an additional partial observation of the evidence variables E   e  Cooper and Herskovits        derived a formula for E q     D  e  that is valid for discrete  continuous  and hybrid networks  This formula provides a useful approximation of the less tractable E q    D   The plug in estimate is a special case of this formula for discrete networks  The formula is important for our network doubling technique  so is reviewed here  In the integral expression below  Z represents all variables not included in  H  E   dh and dz refer to product measures allowing both integration for continuous variables  Lebesgue measure  and summation for discrete variables  counting measure   Some manipulation yields E q     D  e    E w H    E   e  D    E   E w H    E   e      D       RR R w h  p h  e  z    p    D ddhdz RRR     p h  e  z    p    D ddhdz Now p h  e  z     factors as a product of conditional probabilities and densities  one for each variable in the network  Due to global independence  the inteR gral p h  e  z    p    D d factors into a product of integrals  one for each variable  The result is a product of probabilities and densities described in Section     below  It follows that E q    D  e  can be calculated in essentially the same manner as the function q    but with two modifications   For discrete variables  parameters b a are replaced by their posterior means  If all network variables are discrete  then we have the plug in estimate  E q     D  e    q E    D           For continuous variables  the normal densities are replaced by the St            densities described below  Note that this is not the same as replacing  and    parameters with their posterior means       PREDICTIVE DISTRIBUTIONS  The predictive distribution of the network variables is obtained by integrating out their joint conditional dis    UAI       HOOPER ET AL   tribution given  with respect to the posterior distribution of   Global independence allows this integration to be carried out separately for each conditional distribution of a variable given its parents  The predictive distribution for a discrete variable B is b a    P  B   b   A   a  D    E b a   D     b a    a  The predictive distribution for a continuous variable is a location scale version of the Students t distribution with  degrees of freedom  We need the multivariate form of this distribution in Section    so we define it here  Suppose T      U      Z     where Z and U are independent  Z  Np       U          and  is a nonsingular covariance matrix  It follows that T has the following density function  Johnson and Kotz        page          p               p                 p             t   T    t     We refer to this as the Stp        distribution  For p      we write St             Note that St           is Students t distribution  We claim that  Y   A   a  X   x  D   St            with    a          xT  a   and        a      xT   a a        xT  T           To see this  let us suppress subscripts for a moment  Let Z   N        be independent  of       Put Z               Nm            We then have  Y   a  x  D       xT     Z                 xT  Z     Z        NETWORK DOUBLING  In Section     we noted that E q     D  is usually approximated by the more tractable E q     D  e   Here we propose approximating Var q     D  by Var q     D  e  e   i e   the posterior variance given D and additional replicates E   and E   of the vector of evidence variables  both having the same value e  We develop a formula for this latter variance by imagining a doubled network  see Figure   b   These mean and variance approximations can be improved by adjustments described in Section    Consider two replicated tuples of network variables  conditionally independent and identically distributed given   Use these to replace each variable in the       original network by a pair of variables  e g   B is replaced by B      B    B    with possible values b    b    b     DomB    DomB  DomB   If pa B    A  then pa B      A     A    A     Conditional distributions of doubled variables given parents are obtained by multiplying probabilities or densities for single variables  For discrete variables  we have P  B    b   A   a       b   a  b   a    E g   if A   A  DomA    a    a     and DomB    b    b     then the CPtable for B  is the      array shown in Figure   b   More generally  if a CPtable in the original network involves dr  dc parameters  then corresponding table in the doubled network has d r  d c entries  Note that CPtable rows in the doubled network are not independent  local independence does not hold  and do not have Dirichlet distributions  Fortunately  these properties are not needed for the factorization described following      For continuous variables  the conditional density of Y     Y    Y    given  A   a   X    x     is the product of the densities for two normal distributions of the form     with subscript i        on a and x  Put H     H     H      w  H      w H    w H      E     E     E      and e    e  e   Some manipulation using conditional independence yields q      E w  H      E    e       q     E w H      E    e       We thus have Var q     D  e  e            E q     D  e  e    E q     D  e  e     E w  H      e   D    E w H      e   D      The doubled network satisfies global independence assumptions  so we can follow the approach of Section     to evaluate the two expected values in      To accomplish this task  we need bivariate predictive distributions for the doubled network  For discrete variables  the calculation follows from the means and covariances of a Dirichlet distribution  Let b  b  be the Kronecker delta function  We have b  a     P  B    b   A   a   D    E b   a  b   a    D     b   a  b   a    a  a   b   a   b  b   b   a       a       If all network variables are discrete  then we have an identity corresponding to      Let  be the vector        HOOPER ET AL   of all CPtable entries in the doubled network  e g   b   a  b   a  appears in row a and column b for the CPtable of B    We then have E q        D  e  e    q   E    D         with the entries in E    D  given by the b  a values above  The two expected values in the variance approximation     are calculated by applying     twice  with q        q    and with q        q    For continuous variables  we need the density for   Y    Y      a    a    x    x    D   There are two cases to consider   If a     a    then the parameters   a    a      and   a    a      are mutually independent  Consequently  the joint distribution factors as a product of two St            densities  see expression       If a    a      a  say   then the joint distribution is St         with    a      X   a   and o n    a  X    a a    X T    I     where X   is the         d  matrix whose rows are each     xTi   and I   is the      identity matrix  The derivation is similar to that following      Note that   a   a    is the same for both Y  and Y  in this case      UAI       Table    Summary of approximations for q and qq   Means q    E q     D  e  q    E q     D  e  e  q    q    q   q    q    q   qr  r   verify that the distribution of m Q  q   R  r   converges to bivariate normal by modifying the proof of Theorem   in Van Allen et al          Asymptotic normality implies that   qqrr   qr  qq rr    at rate m     T  v    g Cg    E R  r   Q    Q  q    We use approximations for higher moments motivated by large m asymptotics  i e   a sequence of posterior distributions of the form     with m    One may   qr qq      q     q     q     qq        Switching the roles of Q and R gives qrr        For conciseness we suppress D in our expressions  i e   we implicitly assume that expectations are conditioned on D  Put Q   q     P  H   h   E   e    and R   P  E   e      Note that R is an unconditional query  with hypothesis E   e and no evidence variables  Let q   r   qq   rr   and qr denote the means  variances  and covariance for  Q  R   We extend this notation to higher moments  e g   qqr   E  Q  q     R  r      qr qq  and hence qqr  qqq qr  qq   Now qqq     for normal distributions  however  Van Allen et al         argue that query distributions are usually better approximated by beta distributions  Substituting the third moment of a beta distribution for qqq   we obtain qqr   where g is the gradient vector of q   and C is the covariance matrix of   both evaluated at E    D   The second variance approximation v  is the doubling method introduced in Section    The simple adjustments  q    v    and more complex adjustments  q    v    are developed in this section         while qrr and qqr converge to zero at rate m    We considered approximating qqr and qrr by zero but found that more accurate approximations give better results  Asymptotic bivariate normality suggests  ADJUSTMENTS  We now narrow our focus to discrete networks and consider the four mean and variance approximations in Table    The delta method approximation is  Variances v    delta method     v    Var q     D  e  e  v    expression      v    expression        qr rr      r     r     r     rr        Before proceeding  we observe that r and rr can be calculated exactly because R can be expressed as a sum of products of independent terms  For queries with this property  all approximations  except v    are exact  i e   additional observations of evidence variables have no effect on the posterior mean or variance  E g   given a discrete network with structure P E  B  H  we have q     b h b b e   Since parameters in each product are independent  it follows that q    q    q and v    qq   We begin with adjustments to improve q    Bayes rule and some manipulation yields q      q      E QR  qr   q        E R  r E QR     r qr   qrr   q       E R    r   rr   We approximate qqrr using       qqr by       qr by       q by q    and replace qq by v    Rearranging terms yields the identity  v        r   rr   v     q   q         qr         r   rr    r qr      q     q      q      v     Notice that v  appears in the denominator of       We initially set this value to v    then iteratively solve for v    The values converge in a few iterations  We observe that replacing rr by zero has negligible effect on      as m    By also replacing q  by q  and qr  r by q   q    we obtain a simpler identity  v     v      q   q                  q   q        q     q      q      v     We again initialize by v    then iteratively solve for v    The approximations q  and v  may be preferred to q  and v  since r and rr are not required  Rates of convergence are summarized in Proposition   below  The proof of this result follows easily from Van Allen et al         and the development above  Proposition    Assume a discrete network satisfying     and let m    The query mean q remains constant while the variance qq approaches zero at rate m    The mean approximations have errors qj  q approaching zero at rate m  for j     and    and at the faster rate m    for j     and    All four variance approximations have relative errors  vj qq   qq approaching zero at rate m                Scaled Error q   q   q   q                   Scaled Error       Scaled Error  q    b  Diamond   m             a  NB   m              r qq    r qqr   qqrr E  Q  q    R             E R     r   rr       q   In trying to improve v    we began with the idea of replacing q  with q    This suggests an approximation v     q   q       which does help to reduce the under estimation problem  however  a greater improvement is obtained by further analysis of              The formula for q  in Table   follows from       Now recall that  under condition      r remains fixed while rr    as m    It follows that setting rr     in      will have negligible effect for large m  We thus obtain qr   q   q   r   leading to the simpler q  approximation   E  Q  q      e  e    v     q   q                               q   q   r   r   rr   r     r     rr        r     r     r      r  rr  rr  Scaled Error  If r      then set qr      Otherwise  substituting      for qrr and solving yields qr              HOOPER ET AL         UAI       q   q   q    c  NB   m        q   q   q    d  Diamond   m        Figure    Boxplots of scaled errors m qj  q    for j             m             and network structures NB and Diamond  Each boxplot shows variation in errors for a set of distinct queries              for NB and     for Diamond  Errors for q  and q  are nearly identical  Errors for q  are often much larger  Results for q  are not plotted since q   q     q   q         NUMERICAL RESULTS  We evaluated accuracy of approximations qj and vj using highly accurate empirical estimates of q and qq   These estimates q  and v  were obtained by simulating k       replicates of  from the posterior distribution  evaluating q   for each replicate  then calculating the sample mean and sample variance  Computational costs preclude using empirical variance estipaper R figures mates Users peterhooper Documents Research Doubling in practice  When m is large  asymptotic normality of q   implies that the distribution of v   qq is approximately    k  k with variance   k p Consequently v   qq varies over the interval        k for roughly     of samples  Since our variance approximations have relative errors of order m    it follows that k should be of order at least m  for v  to have substantially smaller relative error  When comparing approximate relative errors  vj  v    v  with k         variation in v  has a noticeable effect for m        see Figure   f   Our examples differ with respect to network structure  posterior distribution  and query  All variables are binary  All posterior distributions satisfy BDe constraints  e g   see Hooper        so all variables have the same effective sample size m  Hyperparameters are thus determined by m and the poste         HOOPER ET AL     E   all children of H  e varies over all combinations     for NB       for NB               Diamond network with   variables         all     distinct queries with one hypothesis variable           Scaled Relative Error                Scaled Relative Error           v   v   v   v   v   v   v                Scaled Relative Error                    Scaled Relative Error  v    b  Diamond   m           a  NB   m       v   v   v   v   v   v   v   v    d  Diamond   m        COMPUTATIONAL ISSUES                 Scaled Relative Error             Approximations for means are compared in Figure   and for variances in Figure    The errors and relative errors are multiplied by m in these figures to facilitate comparisons across a range of effective sample sizes  Boxplots for m            and     are shown  Plots for other values of m are similar  By Proposition    relative errors  vj  qq   qq should approach zero at rates cj  m  where cj depends implicitly on the network  E    D   and the query  This theory is supported by Figure   and additional plots  not shown  comparing the four methods for individual queries  Our results suggest that c   c  while c  and c  tend to be further from zero  Relative errors can be interpreted in terms of variances or standard deviations  If  vj  qq   qq   cj  m  then we have p r vj cj cj cj vj     and            qq m qq m  m          Scaled Relative Error           c  NB   m             UAI       v   v   v    e  NB   m        v   v   v   v   v    f  Diamond   m        Figure    Boxplots of relative errors m vj  v    v  for j                m                  and network structures NB and Diamond  Each boxplot shows variation among values for a set of distinct queries     for NB and     for Diamond  We observe that  relative errors tend to be larger for NB compared with Diamond  v  and v  tend to over estimate qq for NB and are more accurate than v    the three methods v    v   and v  have similar accuracy for Diamond  v  is less accurate than the other methods  The four methods appear to have paper R figures similar accuracy in  f   but these plots are mislead Users peterhooper Documents Research Doubling ing  Many of the Diamond queries have the property described following       where v    v    v    qq   We would therefore expect the Diamond results for m       to be similar to those for m        It appears that the variation among relative errors for m       is due in large part to variation in v    rior means E    D   Our examples are from three small networks  each with one vector E    D  and m                            Two nave Bayes networks  NB   and NB   with   and   features plus the root variable   H   root   Inference in Bayesian networks is in general an NPcomplete problem  Cooper         For instance  the complexity of the Variable Elimination  VE  Algorithm is O dr    where d is an upper bound on the number of values that a variable can take and r is an upper bound on the size of a factor generated by the VE Algorithm  Koller and Friedman         Network doubling uses essentially the same technique to calculate a variance as that used to evaluate a query  resulting in corresponding computational complexity  The doubled CPtables are larger  squared number of rows and columns   so the computational complexity of VE is increased to O d r    The delta method retains O dr   complexity  Van Allen et al          so is typically faster in large networks  see Table   below  In some cases  we can exploit the structure of the network or query to achieve a polynomial time inference algorithm  For poly tree Bayesian networks  i e  networks with at most one undirected path between any pair of nodes   there exist inference algorithms with linear time complexity  Reduced complexity is also available when the query can be expressed in terms of probabilities of hypothesis and evidence nodes conditioned on their Markov blanket  i e   the parents  the children and the parents of the children  Once again  we have a polynomial time inference algorithm  These techniques translate directly to efficient algorithms for computing all of the variance approximations in Table      UAI       HOOPER ET AL   Table    Timing results in seconds  Network NB   Diamond Alarm    Queries                      Delta                       Doubling                            Acknowledgements We are grateful for helpful comments from the anonymous reviewers  We acknowledge support provided by NSERC  iCORE  and the Alberta Ingenuity Centre for Machine Learning  
