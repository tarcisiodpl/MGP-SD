 A Bayesian belief network models a joint distribution with an directed acyclic graph representing dependencies among variables and network parameters characterizing conditional distributions  The parameters are viewed as random variables to quantify uncertainty about their values  Belief nets are used to compute responses to queries  i e   conditional probabilities of interest  A query is a function of the parameters  hence a random variable  Van Allen et al               showed how to quantify uncertainty about a query via a delta method approximation of its variance  We develop more accurate approximations for both query mean and variance  The key idea is to extend the query mean approximation to a doubled network involving two independent replicates  Our method assumes complete data and can be applied to discrete  continuous  and hybrid networks  provided discrete variables have only discrete parents   We analyze several improvements  and provide empirical studies to demonstrate their effectiveness      INTRODUCTION  Consider a simple example  Suppose A represents presence absence of a medical condition while B and Y are test results  Variables B and Y are conditionally independent given A  with A and B binary and Y continuous  The conditional independence assumption is represented by the directed acyclic graph structure in Figure   a   Let a   P  A   a   b a   P  B   b   A   a   and let p y   a   a   be the conditional density of Y given A   a  assumed normal with mean a and variance a    We want to estimate the probability that condition A is present given  specified results from the two tests B and Y   Let  represent all of the parameters  If  were known  we would use the formula  a b a p y   a   a     a  a  b a  p y   a    a     q     qa b y      P       In the Bayesian paradigm  uncertainty about  is quantified by modeling parameters as random variables  It follows that query probabilities such as     are also random  A query response is usually estimated by approximating its posterior mean  This approximation is similar to expression      but with a and b a replaced by their posterior means and with the normal densities replaced by Students t densities  One may want more than just a point estimate  Van Allen et al               showed  for discrete networks  how one can approximate the variance and posterior distribution of a query  Their variance derivation employs the delta method  i e   a first order Taylor series expansion of the function q   about the posterior mean of   They provide asymptotic theory and empirical experiments supporting this approach  They also showed how these approximations can be used to construct a Bayesian credible interval  error bars  for q    Guo and Greiner        applied this delta method approximation as part of a mean squared error  i e   squared bias   variance  measure designed to estimate the quality of different belief net structures when seeking a best classifier  Lee et al         provide a technique for combining independent belief net classifiers that involves weighting their respective mean probability values by their inverse variances  and they show that this works well in practice  We propose new approximations for the mean and variance based on a simple trick  Suppose  A    B    Y    and  A    B    Y    are replicates of the network variables  conditionally independent given   We represent the paired replicates as nodes in a doubled network with the same structure  see Figure    The squared query q    can be expressed as a query in this doubled net    UAI       b   a  b   a   b   a  b   a   HOOPER ET AL     a  a  A                                            b  a b  a    b   a  b   a   a    a     Y B       a    a       b   a  b   a  b   a  b   a        a  a   b   a  b   a  b   a  b   a  b   a  b   a  b   a  b   a   b   a  b   a  b   a  b   a  b   a  b   a  b   a  b   a   a  a   b   a  b   a  b   a  b   a  b   a  b   a  b   a  b   a   a  a   a  a       A    A                             a    a     a    a                a    a     a    a     Y    Y  B    B   a          a                   a         a           a a a a  Figure     a  A simple Bayesian network   b  The corresponding doubled network  Figure    A simple Bayesian net   work  P  A    A    a   B    B    b  Y    Y    y      The method used to approximate the mean of q   can be extended to the doubled network to approximate the mean of q    and hence to approximate the variance  Unlike the delta method  our approach does not rely on approximate local linearity of q    It does involve the addition of two incomplete observations to the data set when calculating the posterior mean of q      In some situations  this addition results in under estimation of the desired variance  This deficiency is largely eliminated by a simple adjustment  A similar adjustment substantially improves the usual query mean approximation  Section   reviews pertinent models and methods for belief networks  The network doubling technique is described in Section   for discrete  continuous  and hybrid networks  Proposed adjustments and numerical results are presented in Sections   and   for discrete networks  Corresponding work for continuous and hybrid networks is ongoing  Computational issues are discussed in Section    Contributions and plans for further work are summarized in Section            BACKGROUND NETWORK VARIABLES  We assume network structure is known  Let B denote a discrete network variable taking values b  DomB   Let Y denote a continuous network variable taking values y on the real line  Vectors of variables are denoted by boldface  A for discrete and X for continuous  Let  be a random vector comprising all unknown network parameters  i e    determines all conditional distributions of variables given their parents  We assume that discrete variables have only discrete parents  Suppose pa B    A  i e   the parents of B are the variables comprising the vector A  The conditional probability that B   b given A   a is denoted b a   B b A a   P  B   b   A   a         Variables associated with values will be clear from context  We employ similar abbreviations for other parameters and hyperparameters  The b a parameters are often presented in conditional probability tables  CPtables  with rows indexed by a and columns by b  e g   see Figure    Note that we use superscripts b    b  to list the distinct values in DomB   We use subscripts b    b  to denote arbitrary values in DomB   often related to replicated variables B    B    Continuous variables can have both discrete and continuous parents  Suppose pa Y     hA  Xi with X   hX            Xd i  The conditional distribution of Y is    Y   A   a  X   x     N     xT   a   a        i e   normally distributed  conditional mean related to x by a linear regression model with coefficients depending on a  Here xT is the transpose of the ddimensional column vector x while  a is an  d     dimensional column vector of regression coefficients  the first entry is the constant term        PRIOR AND POSTERIOR  The network parameters represented by  consist of CPtable parameters b a   regression coefficient vectors  a   and variances a    We assume the prior distribution for  has the following form  e g   see Gelman et al           CPtable rows follow Dirichlet distributions  B a    hb a   b  DomB i  Dir B a    where B a    hb a   b  DomB i    The regression coefficients and variance together have a normal  inverse chi square  distribution      a   a     Nd   a   a   a a      a     a  a     a    I e   dropping subscripts for a moment   conditioned on    is multivariate normal with mean        HOOPER ET AL  vector  and covariance matrix          and        has a   distribution with       not necessarily an integer   Note that        has mean   and variance       Parameters are assumed to be statistically independent except where joint distributions are specified above  In particular  we assume global independence  the parameters determining the conditional distribution of one variable given its parents are independent of all other parameters  The prior is conjugate  given a data set D consisting of n independent replicates of complete tuples of network variables  the prior hyperparameter values are updated as follows  Let nab and na be the number of tuples in D with  A  B     a  b  and A   a  respectively  Let  xi   yi   be the observations of  X  Y   for the na tuples with A   a  Let X a be the na   d      matrix with rows     xTi    Let y a be the column vector with entries yi   In the five equations below  the prior hyperparameter values appear on the right hand side and are identified with tildes  e g      b a   b a   nab a   a   na a a   a a   X Ta X a a a a   a a a   X Ta y a i h       a a   Ta a a   a a    Ta a a   y Ta y a P P The values a b b a and a a are called the effective sample sizes for variables B and Y   respectively  Our adjustments developed in Section   are motivated by large m asymptotics  where m is proportional to the effective sample size for each of the variables  i e     b a   mb a and a   ma    with  b a   a    a   a   a    fixed        Large m asymptotics are similar to but not the same as large n asymptotics  As the sample size n increases  the posterior mean E b a   D    b a   a varies and converges to some value   Here and elsewhere  the dot P subscript indicates summation   a   b b a    Under assumption      the posterior mean remains fixed as m varies       APPROXIMATING A QUERY MEAN  Consider a query involving outcomes of hypothesis variables H given values for evidence variables E  It is convenient to represent the query in terms of a function w H   E g   suppose H   A  E    B  Y    e    b  y   and q      P  A   a   B   b  Y   y      E w A    B   b  Y   y       UAI       where w A      for A   a and w A      otherwise  For discrete networks  query responses q   are usually estimated by q    where     E    D  is the posterior mean of the parameter vector  This plugin estimate usually differs slightly from the posterior query mean E q     D   Cooper and Herskovits        expression     showed that the plug in estimate equals E q     D  e   i e   the posterior query mean given an augmented data set consisting of D and an additional partial observation of the evidence variables E   e  Cooper and Herskovits        derived a formula for E q     D  e  that is valid for discrete  continuous  and hybrid networks  This formula provides a useful approximation of the less tractable E q    D   The plug in estimate is a special case of this formula for discrete networks  The formula is important for our network doubling technique  so is reviewed here  In the integral expression below  Z represents all variables not included in  H  E   dh and dz refer to product measures allowing both integration for continuous variables  Lebesgue measure  and summation for discrete variables  counting measure   Some manipulation yields E q     D  e    E w H    E   e  D    E   E w H    E   e      D       RR R w h  p h  e  z    p    D ddhdz RRR     p h  e  z    p    D ddhdz Now p h  e  z     factors as a product of conditional probabilities and densities  one for each variable in the network  Due to global independence  the inteR gral p h  e  z    p    D d factors into a product of integrals  one for each variable  The result is a product of probabilities and densities described in Section     below  It follows that E q    D  e  can be calculated in essentially the same manner as the function q    but with two modifications   For discrete variables  parameters b a are replaced by their posterior means  If all network variables are discrete  then we have the plug in estimate  E q     D  e    q E    D           For continuous variables  the normal densities are replaced by the St            densities described below  Note that this is not the same as replacing  and    parameters with their posterior means       PREDICTIVE DISTRIBUTIONS  The predictive distribution of the network variables is obtained by integrating out their joint conditional dis    UAI       HOOPER ET AL   tribution given  with respect to the posterior distribution of   Global independence allows this integration to be carried out separately for each conditional distribution of a variable given its parents  The predictive distribution for a discrete variable B is b a    P  B   b   A   a  D    E b a   D     b a    a  The predictive distribution for a continuous variable is a location scale version of the Students t distribution with  degrees of freedom  We need the multivariate form of this distribution in Section    so we define it here  Suppose T      U      Z     where Z and U are independent  Z  Np       U          and  is a nonsingular covariance matrix  It follows that T has the following density function  Johnson and Kotz        page          p               p                 p             t   T    t     We refer to this as the Stp        distribution  For p      we write St             Note that St           is Students t distribution  We claim that  Y   A   a  X   x  D   St            with    a          xT  a   and        a      xT   a a        xT  T           To see this  let us suppress subscripts for a moment  Let Z   N        be independent  of       Put Z               Nm            We then have  Y   a  x  D       xT     Z                 xT  Z     Z        NETWORK DOUBLING  In Section     we noted that E q     D  is usually approximated by the more tractable E q     D  e   Here we propose approximating Var q     D  by Var q     D  e  e   i e   the posterior variance given D and additional replicates E   and E   of the vector of evidence variables  both having the same value e  We develop a formula for this latter variance by imagining a doubled network  see Figure   b   These mean and variance approximations can be improved by adjustments described in Section    Consider two replicated tuples of network variables  conditionally independent and identically distributed given   Use these to replace each variable in the       original network by a pair of variables  e g   B is replaced by B      B    B    with possible values b    b    b     DomB    DomB  DomB   If pa B    A  then pa B      A     A    A     Conditional distributions of doubled variables given parents are obtained by multiplying probabilities or densities for single variables  For discrete variables  we have P  B    b   A   a       b   a  b   a    E g   if A   A  DomA    a    a     and DomB    b    b     then the CPtable for B  is the      array shown in Figure   b   More generally  if a CPtable in the original network involves dr  dc parameters  then corresponding table in the doubled network has d r  d c entries  Note that CPtable rows in the doubled network are not independent  local independence does not hold  and do not have Dirichlet distributions  Fortunately  these properties are not needed for the factorization described following      For continuous variables  the conditional density of Y     Y    Y    given  A   a   X    x     is the product of the densities for two normal distributions of the form     with subscript i        on a and x  Put H     H     H      w  H      w H    w H      E     E     E      and e    e  e   Some manipulation using conditional independence yields q      E w  H      E    e       q     E w H      E    e       We thus have Var q     D  e  e            E q     D  e  e    E q     D  e  e     E w  H      e   D    E w H      e   D      The doubled network satisfies global independence assumptions  so we can follow the approach of Section     to evaluate the two expected values in      To accomplish this task  we need bivariate predictive distributions for the doubled network  For discrete variables  the calculation follows from the means and covariances of a Dirichlet distribution  Let b  b  be the Kronecker delta function  We have b  a     P  B    b   A   a   D    E b   a  b   a    D     b   a  b   a    a  a   b   a   b  b   b   a       a       If all network variables are discrete  then we have an identity corresponding to      Let  be the vector        HOOPER ET AL   of all CPtable entries in the doubled network  e g   b   a  b   a  appears in row a and column b for the CPtable of B    We then have E q        D  e  e    q   E    D         with the entries in E    D  given by the b  a values above  The two expected values in the variance approximation     are calculated by applying     twice  with q        q    and with q        q    For continuous variables  we need the density for   Y    Y      a    a    x    x    D   There are two cases to consider   If a     a    then the parameters   a    a      and   a    a      are mutually independent  Consequently  the joint distribution factors as a product of two St            densities  see expression       If a    a      a  say   then the joint distribution is St         with    a      X   a   and o n    a  X    a a    X T    I     where X   is the         d  matrix whose rows are each     xTi   and I   is the      identity matrix  The derivation is similar to that following      Note that   a   a    is the same for both Y  and Y  in this case      UAI       Table    Summary of approximations for q and qq   Means q    E q     D  e  q    E q     D  e  e  q    q    q   q    q    q   qr  r   verify that the distribution of m Q  q   R  r   converges to bivariate normal by modifying the proof of Theorem   in Van Allen et al          Asymptotic normality implies that   qqrr   qr  qq rr    at rate m     T  v    g Cg    E R  r   Q    Q  q    We use approximations for higher moments motivated by large m asymptotics  i e   a sequence of posterior distributions of the form     with m    One may   qr qq      q     q     q     qq        Switching the roles of Q and R gives qrr        For conciseness we suppress D in our expressions  i e   we implicitly assume that expectations are conditioned on D  Put Q   q     P  H   h   E   e    and R   P  E   e      Note that R is an unconditional query  with hypothesis E   e and no evidence variables  Let q   r   qq   rr   and qr denote the means  variances  and covariance for  Q  R   We extend this notation to higher moments  e g   qqr   E  Q  q     R  r      qr qq  and hence qqr  qqq qr  qq   Now qqq     for normal distributions  however  Van Allen et al         argue that query distributions are usually better approximated by beta distributions  Substituting the third moment of a beta distribution for qqq   we obtain qqr   where g is the gradient vector of q   and C is the covariance matrix of   both evaluated at E    D   The second variance approximation v  is the doubling method introduced in Section    The simple adjustments  q    v    and more complex adjustments  q    v    are developed in this section         while qrr and qqr converge to zero at rate m    We considered approximating qqr and qrr by zero but found that more accurate approximations give better results  Asymptotic bivariate normality suggests  ADJUSTMENTS  We now narrow our focus to discrete networks and consider the four mean and variance approximations in Table    The delta method approximation is  Variances v    delta method     v    Var q     D  e  e  v    expression      v    expression        qr rr      r     r     r     rr        Before proceeding  we observe that r and rr can be calculated exactly because R can be expressed as a sum of products of independent terms  For queries with this property  all approximations  except v    are exact  i e   additional observations of evidence variables have no effect on the posterior mean or variance  E g   given a discrete network with structure P E  B  H  we have q     b h b b e   Since parameters in each product are independent  it follows that q    q    q and v    qq   We begin with adjustments to improve q    Bayes rule and some manipulation yields q      q      E QR  qr   q        E R  r E QR     r qr   qrr   q       E R    r   rr   We approximate qqrr using       qqr by       qr by       q by q    and replace qq by v    Rearranging terms yields the identity  v        r   rr   v     q   q         qr         r   rr    r qr      q     q      q      v     Notice that v  appears in the denominator of       We initially set this value to v    then iteratively solve for v    The values converge in a few iterations  We observe that replacing rr by zero has negligible effect on      as m    By also replacing q  by q  and qr  r by q   q    we obtain a simpler identity  v     v      q   q                  q   q        q     q      q      v     We again initialize by v    then iteratively solve for v    The approximations q  and v  may be preferred to q  and v  since r and rr are not required  Rates of convergence are summarized in Proposition   below  The proof of this result follows easily from Van Allen et al         and the development above  Proposition    Assume a discrete network satisfying     and let m    The query mean q remains constant while the variance qq approaches zero at rate m    The mean approximations have errors qj  q approaching zero at rate m  for j     and    and at the faster rate m    for j     and    All four variance approximations have relative errors  vj qq   qq approaching zero at rate m                Scaled Error q   q   q   q                   Scaled Error       Scaled Error  q    b  Diamond   m             a  NB   m              r qq    r qqr   qqrr E  Q  q    R             E R     r   rr       q   In trying to improve v    we began with the idea of replacing q  with q    This suggests an approximation v     q   q       which does help to reduce the under estimation problem  however  a greater improvement is obtained by further analysis of              The formula for q  in Table   follows from       Now recall that  under condition      r remains fixed while rr    as m    It follows that setting rr     in      will have negligible effect for large m  We thus obtain qr   q   q   r   leading to the simpler q  approximation   E  Q  q      e  e    v     q   q                               q   q   r   r   rr   r     r     rr        r     r     r      r  rr  rr  Scaled Error  If r      then set qr      Otherwise  substituting      for qrr and solving yields qr              HOOPER ET AL         UAI       q   q   q    c  NB   m        q   q   q    d  Diamond   m        Figure    Boxplots of scaled errors m qj  q    for j             m             and network structures NB and Diamond  Each boxplot shows variation in errors for a set of distinct queries              for NB and     for Diamond  Errors for q  and q  are nearly identical  Errors for q  are often much larger  Results for q  are not plotted since q   q     q   q         NUMERICAL RESULTS  We evaluated accuracy of approximations qj and vj using highly accurate empirical estimates of q and qq   These estimates q  and v  were obtained by simulating k       replicates of  from the posterior distribution  evaluating q   for each replicate  then calculating the sample mean and sample variance  Computational costs preclude using empirical variance estipaper R figures mates Users peterhooper Documents Research Doubling in practice  When m is large  asymptotic normality of q   implies that the distribution of v   qq is approximately    k  k with variance   k p Consequently v   qq varies over the interval        k for roughly     of samples  Since our variance approximations have relative errors of order m    it follows that k should be of order at least m  for v  to have substantially smaller relative error  When comparing approximate relative errors  vj  v    v  with k         variation in v  has a noticeable effect for m        see Figure   f   Our examples differ with respect to network structure  posterior distribution  and query  All variables are binary  All posterior distributions satisfy BDe constraints  e g   see Hooper        so all variables have the same effective sample size m  Hyperparameters are thus determined by m and the poste         HOOPER ET AL     E   all children of H  e varies over all combinations     for NB       for NB               Diamond network with   variables         all     distinct queries with one hypothesis variable           Scaled Relative Error                Scaled Relative Error           v   v   v   v   v   v   v                Scaled Relative Error                    Scaled Relative Error  v    b  Diamond   m           a  NB   m       v   v   v   v   v   v   v   v    d  Diamond   m        COMPUTATIONAL ISSUES                 Scaled Relative Error             Approximations for means are compared in Figure   and for variances in Figure    The errors and relative errors are multiplied by m in these figures to facilitate comparisons across a range of effective sample sizes  Boxplots for m            and     are shown  Plots for other values of m are similar  By Proposition    relative errors  vj  qq   qq should approach zero at rates cj  m  where cj depends implicitly on the network  E    D   and the query  This theory is supported by Figure   and additional plots  not shown  comparing the four methods for individual queries  Our results suggest that c   c  while c  and c  tend to be further from zero  Relative errors can be interpreted in terms of variances or standard deviations  If  vj  qq   qq   cj  m  then we have p r vj cj cj cj vj     and            qq m qq m  m          Scaled Relative Error           c  NB   m             UAI       v   v   v    e  NB   m        v   v   v   v   v    f  Diamond   m        Figure    Boxplots of relative errors m vj  v    v  for j                m                  and network structures NB and Diamond  Each boxplot shows variation among values for a set of distinct queries     for NB and     for Diamond  We observe that  relative errors tend to be larger for NB compared with Diamond  v  and v  tend to over estimate qq for NB and are more accurate than v    the three methods v    v   and v  have similar accuracy for Diamond  v  is less accurate than the other methods  The four methods appear to have paper R figures similar accuracy in  f   but these plots are mislead Users peterhooper Documents Research Doubling ing  Many of the Diamond queries have the property described following       where v    v    v    qq   We would therefore expect the Diamond results for m       to be similar to those for m        It appears that the variation among relative errors for m       is due in large part to variation in v    rior means E    D   Our examples are from three small networks  each with one vector E    D  and m                            Two nave Bayes networks  NB   and NB   with   and   features plus the root variable   H   root   Inference in Bayesian networks is in general an NPcomplete problem  Cooper         For instance  the complexity of the Variable Elimination  VE  Algorithm is O dr    where d is an upper bound on the number of values that a variable can take and r is an upper bound on the size of a factor generated by the VE Algorithm  Koller and Friedman         Network doubling uses essentially the same technique to calculate a variance as that used to evaluate a query  resulting in corresponding computational complexity  The doubled CPtables are larger  squared number of rows and columns   so the computational complexity of VE is increased to O d r    The delta method retains O dr   complexity  Van Allen et al          so is typically faster in large networks  see Table   below  In some cases  we can exploit the structure of the network or query to achieve a polynomial time inference algorithm  For poly tree Bayesian networks  i e  networks with at most one undirected path between any pair of nodes   there exist inference algorithms with linear time complexity  Reduced complexity is also available when the query can be expressed in terms of probabilities of hypothesis and evidence nodes conditioned on their Markov blanket  i e   the parents  the children and the parents of the children  Once again  we have a polynomial time inference algorithm  These techniques translate directly to efficient algorithms for computing all of the variance approximations in Table      UAI       HOOPER ET AL   Table    Timing results in seconds  Network NB   Diamond Alarm    Queries                      Delta                       Doubling                            Acknowledgements We are grateful for helpful comments from the anonymous reviewers  We acknowledge support provided by NSERC  iCORE  and the Alberta Ingenuity Centre for Machine Learning  
