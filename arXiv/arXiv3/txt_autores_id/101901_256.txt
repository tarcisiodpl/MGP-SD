  When using Bayesian networks for modelling the behavior of man made machinery  it usu ally happens that a large part of the model is deterministic  For such Bayesian networks the deterministic part of the model can be represented as a Boolean function  and a cen tral part of belief updating reduces to the task of calculating the number of satisfying configurations in a Boolean function  In this paper we explore how advances in the calcu lation of Boolean functions can be adopted for belief updating  in particular within the context of troubleshooting  We present ex perimental results indicating a substantial speed up compared to traditional junction tree propagation     INTRODUCTION  When building a Bayesian network model it frequently happens that a large part of the model is determinis tic  This happens particularly when modelling the be havior of man made machinery  Then the situation is that we have a deterministic kernel with surrounding chance variables  and it seems excessive to use stan dard junction tree algorithms for belief updating  First of all  the calculations in the deterministic kernel are integer calculations and double precision calculations are unnecessary complex  However  there may be room for further improvements  If the deterministic part of the model is represented as a Boolean function  we may exploit contemporary advances in calculation of Boolean functions  A major advance in Boolean calculation is Binary Decision Diagrams  particularly Reduced Ordered Bi nary Decision Diagrams  ROBDDs Bryant         An ROBDD is a DAG representation of a Boolean func tion  The representation is tailored for fast calculation  Uffe Kjcerulff  of values  but the representation can also be used for fast calculation of the number of satisfying configura tions given an instantiation of a subset of the variables  To be more precise  let B X  be a Boolean function over the Boolean variables X  and let Y  X with X Y  Define Cards      on a configuration iJ Z of y as the number of configurations z over Z such that B   j  z  true      It turns out that given iJ an ROBDD representation of B can be constructed such that Cards can be calculated in time linear in the num ber of nodes in the ROBDD  However  the number of nodes in an ROBDD may be exponential in the num ber of variables in the domain of the Boolean function        In this paper we exploit the ROBDD representation for propagation through a Boolean kernel in a Bayesian network  and we illustrate that a central part of this propagation is to calculate Cards  i    We use the tech nique on models for troubleshooting  These models are particularly well suited for ROBDD calculation as the size of the ROBDD is quadratic in the size of the domain  In section   we illustrate the use of Cards for prob  ability updating in Bayesian networks  Section   is a brief introduction to ROBDDs and in section   we show how to calculate Cards in an ROBDD  Section   introduces the troubleshooting task and the type of Bayesian network models used  In section   the de terministic kernel of these models is represented as an ROBDD and it is shown that the size of this represen tation is quadratic in the number of Boolean variables  In section   we outline the propagation algorithms for various troubleshooting tasks  and in section   we re port on empirical results indicating a substantial speed up compared to traditional junction tree propagation     TWO MOTIVATING EXAMPLES  To illustrate the special considerations in connection with Boolean kernels we shall treat a couple of exam         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS          ples  First consider the situation in Figure     BOOLEAN FUNCTIONS AND ROBDDS  This section is a survey of classical logic in the context of binary decision diagrams  Figure    The Boolean variable A has a parent net work of proper chance variables and a child network representing a Boolean function B  For the situation in Figure   we have  U P  U         WUVU A     Q  W A I t B  V A    where   l      Lvu A  B  V A  is a normalization con stant  Assume we have evidence e ew u ev   where ev is a configuration y of the variables Y  V  then     P A e     l    l  L L w w  Q W ew A  L B Z y A  z  If we extend the example s t  a Boolean variable C E V has a child network R  T  C  of proper chance variables  we get   the normalization constant is omitted    P  U    Q  W A B  V A R  T C  Assume we have evidence e   ew u ev u er  where ev is a configuration y of the variables Y  V  If er is empty then R does not contribute  and the calculations are as for Figure    If not  we have   L L  L w  Q W ew A     B  Z y  A  C   Z  C  L     T  w   L L  B  Z y A C     y   Z     T  B  Z y A  C  n    Z    L Q W ew A   R  T e  C  n       T   w     CardB y A  C   y      Card  Y A  C   n   R  T er  C    R  T er  C     y    n    All operators in propositional logic can be expressed using only this operator and this can be done s t  tests are only performed on unnegated variables  Definition    An If then else Normal Form  INF  is a Boolean function built entirely from the if then else operator and the constants   and   s t  all tests are performed only on variables   B  R  T er C   y    L L  Let X     Y   Y  denote the if then else operator  Then X     Y    Y  is true if either X and Y  are true or X is false and Y  is true  the variable X is said to be the test expression  More formally we have   Consider the Boolean function B and let B X H    denote the Boolean function produced by substituting   for X in B  The Shannon expansion of B w r t  X is defined as   R  T e   C   L Q  W ew A      A truth assignment to a Boolean function B is the same as fixing a set of variables in the domain of B  i e   if X is a Boolean variable in the domain of B  then X can be assigned either   or     denoted  X H    and  X H     respectively    A Boolean function is said to be a tautology if it yields true for all truth assignments  and it is satisfiable if it yields true for at least one truth assignment   Q W ew A CardB  y A    As the example illustrates  an efficient procedure for calculating CardB is central for probability updating   P A e     The classical calculus for dealing with truth assign ments consists of Boolean variables  the constants true     and false     and the operators     conjunction   V  disjunction       negation       implication  and        bi implication   A combination of these entities form a Boolean function and the set of all Boolean functions is known as propositional logic      Again  calculation of Card  is part of belief updating       X  t B      H      B  X H     From the Shannon expansion we get that any Boolean function can be expressed in INF by iteratively using the above substitution scheme on B  By applying the Shannon expansion to a Boolean func tion B w r t  an ordering of all the variables in the do main of B we get a set of if then else expressions which can be represented as a binary decision tree  The de cision tree may contain identical substructures and by  collapsing  such substructures we get a binary deci sion diagram   BDD  which is a directed acyclic graph  The ordering of the variables  corresponding to the order in which the Shannon expansion is performed         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS  is encoded in the BDD hence  we say that the BDD is an ordered binary decision diagram  OBDD   the variables occur in the same order on all paths from the root  If all redundant tests are removed in an OBDD it is said to be reduced and we have a reduced ordered binary decision diagram  ROBDD   Definition    A reduced ordered binary decision di agram  ROBDD  is a rooted  directed acyclic graph with         ROBDD  The algorithm basically propagates a num ber    n   where n is the number of distinct variables in the corresponding Boolean function  from the root of the ROBDD to the terminal node  The value sent from a node  including the root  to one of its children is the value associated with that node divided by    The value associated with a node  except the root  is the sum of the values sent from its parents  see Fig ure      one or two terminal nodes labeled   and   respec tively  a set of non terminal nodes of out degree two with one outgoing arc labeled   and the other    a variable name attached to each non terminal node s t  on all paths from the root to the ter minal nodes the variables respect a given linear ordering  no two nodes have isomorphic subgraphs   We will use Eo to denote the set of   arcs  drawn as dashed arcs  and    to denote the set of l ares  drawn as solid arcs   Theorem     Bryant          For any Boolean function f      l n           there is exactly one ROBDD B with variables X    X      Xn s t  B X  H b   X  H b        Xn H bnJ f  b  b       bn    i  b  b       bn  E    l n    Figure    There are   satisfying configurations for the Boolean function  Exactly one variable among A    A    A   is true  represented by this ROBDD  Definition    Let B    U   be an ROBDD  Propa gation in B is the computation of v  u   where u E U and v   U     lR is defined as      From Theorem   we have that in order to calculate the number of satisfying configurations in a Boolean function B we can produce an ROBDD equivalent to B and then count in this structure  In the remainder of this paper we assume that an ROBDD has exactly one terminal node labeled    as we are only interested in the number of satisfying configu rations  in this situation we allow non terminal nodes with out degree one  Additionally  we will use the term  nodes  in the context of ROBDDs and  variables  when referring to a Boolean function or a Bayesian network BN   nodes and variables will be denoted with lower case letters and upper case letters  respectively  the nodes representing a variable Xi will each be de noted Xi if this does not cause any confusion            CALCULATION OF CARDB USING ROBDDS  Given an ROBDD representation of a Boolean func tion B  the number of satisfying configurations can be calculated in time linear in the number of nodes in the      v  r    n   where r is the root in B and n is the number of distinct variables in B  LpEnd  v p    where nu repre iu E U  r   v  u        sents the set of parents for u in B   So  in order to determine Cards for some Boolean function B  U  we only need one propagation in the corresponding ROBDD since Cards v  l   In case evidence y has been received on the variables Y  U we simply modify the algorithm s t  configurations  in consistent with y  does not contribute to the propaga tion  i e   given a configuration y the function v  u y is defined as      iu E U  r    v  u y       LvEna v p y      where nR    p E nul p  j  Yl or  y p    i and  p  u  E i    y p  is the state ofp E Y under y and v  r   n  n being the number of distinct variables in B including those on which evidence has been received  In partic ular we have that Cards  i i    v    y  Notice  that the structure of the ROBDD is not changed when evidence is received     The size of the ROBDD has a significant impact on the performance of the algorithm and the problem of        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       identifying a minimal sized ROBDD is NP complete  Thus  in the remainder of this paper we shall mainly focus on troubleshooting models as it turns out that the structure of such a model ensures that the size of the corresponding ROBDD is at most quadratic in the size of the domain     TROUBLESHOOTING  Definition    A troubleshooting model is a con nected BN T     U  Us U Uc U UA    P   where      The set Us contains a distinct variable   with no successors  and for each    E Us     there exists a directed path from    to    For each variable C EUc there exists an    EUs s t  C Ens  and nc       When troubleshooting a device which is not working properly we wish to determine the cause of the problem or find an action sequence repairing the device  At any time during the process there may be numerous differ ent operations that can be performed e g  a component can be repaired replaced or the status of a component can be investigated  Because such operations can be expensive and may not result in a functioning device  it is expedient to determine a sequence of operations that minimizes the expected cost and  eventually  re pairs the device   Breese and Heckerman        presents a method to myopicly determine such a sequence  The method as sumes a BN representing the device in question  and the BN is assumed to satisfy the following properties     There is only one problem defining variable in the BN and this variable represents the functional sta tus of the device      The device is initially faulty     Exactly one component is malfunctioning causing  the device to be faulty  single fault   A central task of troubleshooting  within the frame work of  Breese and Heckerman         is the calcula tion of Pi  P  C i  faulty e  which denotes the prob ability that component ci is the cause of the problem given evidence e  So we are looking for a way to exploit the logical structure of the model when calculating the probabilities Pi As such a scheme is strongly depen dent on the structure of the troubleshooting model we give a syntactical definition of this concept  The def inition is based on BNs  a BN consists of a directed acyclic graph G   U    and a joint probability dis tribution P U   where U is a set of variables and  is a set of edges connecting the variables in U  we use sp X  to denote the state space for a variable X E U  The joint probability distribution P U  factorizes over U s t     P U            For each variable A EUA there does not exist an X EU s t  A E nx  sp X    ok  ok   VX EUs U Uc  For each X E Us  P xly     or P xly  sp X  and Vy E sp nx          Vx E  The variable   is termed the problem defining variable and the variables Us are termed system variables  The variables Uc  termed cause variables  represent the set of components which can be repaired  and the vari ables in UA  termed action variables  represent user performable operations such as observations and sys tem repairing actions  notice that UA is not part of the actual system specification  In the remainder of this paper we shall extend the single fault assump tion to include the system variables also  That is  if a system variable  i is faulty  then there exists ex actly one variable X E ns  which is faulty also  see  Skaanning et al         for further discussion of this extension and how the single fault assumption can be enforced using so called  constraint variables     Figure   depicts a troubleshooting model  where A is an action variable and   represents the problem defin ing variable  The variables      z    and    repre sent subsystems  which should be read as  the sys tem   can be decomposed into two subsystems    and  z  and subsystem    can be decomposed into    and     Component C   can cause either    or    to fail  whereas C z can cause either  z or    to fail  neither C   nor C z can cause two subsystems to fail simultane ously   Notice that A is not part of the actual system model   IJ P XInx    XEU  where nx is the parents of X in G  The set of con ditional probability distributions factorizing P U  ac cording to G is denoted P   Figure    A troubleshooting model with five system variables  two cause variables and one action variable    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            From assumption     and     we have that   P C   y  e   P C    J  Cz  n          Cm  n  e  m   P C   y  P Ci n  i    tB C    y  Cz n         Cm  n  Us  e   II      L      Us  II P Ci     i    n    tCard s  C    y  e     where B Us  Uc  is a Boolean function  specified in the following section  and J t is a normalization constant  Now  P C  el   P C   e  P e  and P e  is given by   m      IJ P Cd   B C      Cm  S  S          Sn  el u  i    In the remainder of this paper we omit the normaliza tion constant       m    P C   y   P e      and only one  of its subsystems  Sc  is faulty  if a cause is not present we can not say anything about its subsystems   M says that there can be either zero or at most one cause present  consistent with the system state   B U  is the Boolean function representing the system as a whole  Note that   ROBDDS AS TROUBLESHOOTING MODELS    The Boolean function is a list of expressions for local constraints and it can therefore be built in an incremental fashion  The Boolean function can easily be modified to represent any logical relation between the compo nents  The expression ensures the single fault assump tion based on the structure of the model  i e   it is not necessary to introduce  constraint variables     Example    The Boolean function representing the troubleshooting model depicted in Figure   is specified by B     SA S  l l Sz   V  SA        Sz   B A  S A S  l l S    V  S As S     In what follows we shall assume single fault and use the truth values   and   to denote the state of a com ponent subsystem    indicates a fault    B A Cz      Sz Zl S      A  SA C  IZl Cz   V  SAC ACz    Now  let nsi be the subsystems which immediately compose Si E Us and let Sc  Us be the subsys tems that component C E Uc can cause to fail  Sc is the immediate successors of C  The Boolean func tion representing the logical kernel of a troubleshoot ing model TS    Us UUc UUA     P  is then given by B U   Us UUc    Given the ordering S  S   Sz  S   S   C   C   the ROBDD corresponding to B is depicted in Figure    Note that all paths from the root S to the terminal node are consistent with the ordering above   D  F T   G C        TA       s  v rA  S EnT  Q  T  C        s   S EnT  TESc  M           f      f  c  sA  v  sA  CEUc  B U    F T  A  TEUs     CEUc  c           G C  AM   CEUc  where     xi denotes an exclusive or between the variables  X           Xn   F T  specifies that if the sys tem Tis malfunctioning then one  and only  of its sub systems is faulty  and if the system is functioning prop erly then all of its subsystems are functioning properly also  G C  states that if a cause is present then one  Figure    An ROBDD representation of the trou bleshooting model depicted in Figure     The ROBDD was generated by the software tool http   fwww cs auc dk behrmann iben    iben         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Now  as indicated in Section    the size of the ROBDD is dependent on the ordering of the variables  So we are looking for a general  rule of ordering  producing ROBDDs of  small  size  Consider an ordering of the variables where each sys tem variable occurs before all the variables repre senting its subsystems  and where all the cause vari ables occur last in the ordering  By constructing the ROBDD according to this ordering we get the node representing the problem defining variable as root and the nodes representing the cause variables at the bot tom  see Figure     Moreover  we get an upper bound on the size of the ROBDD as stated in the following theorem  note that the action variables are not part of the logical kernel  Theorem    Let TS   U  UAUUsUUc    P  be a troubleshooting model  Then the size of the ROBDD  representing the Boolean function B Us U Uc   is O IUsi   Ucl    if the ordering a  UsUUc H IUsUUcl satisfies      VX E Us  a X    a Y  for each Y E nx  VZ E Uc there does not exist an X a Z    a X    E  Us s t   Proof  Assume an indexing of the layers in the ROBDD s t  the layers containing the root node and the terminal node have index   and IUs U Ucl      re spectively  a layer is the set of nodes representing a distinct variable  Now  consider the layers consisting of system nodes but no cause nodes  The number of nodes in the i th layer either equals the number of nodes in the i th     layer or it has exactly one more node than the i th    layer  This is the same as saying that at most one node in the i th    layer branches in two  if two differ ent nodes in a layer branched into two we would have two distinct paths from a node at a higher level to these nodes however  this contradicts the single fault assumption due to the ordering of the nodes  Thus  the number of nodes in the layers containing system   nodes is at most L  Z     i  IUs l ls I     For the cause nodes  there can be at most one distinct path for each of their possible configurations  This means that the number of nodes in the layers contain ing cause nodes is at most IUcl  cl     Ucl   Hence  D the size of the ROBDD is O IUsl    IUcl    In the ROBDDs  we have an all false path from the root to the terminal node  Indeed the Boolean function is true when the model has no fault  However  we can force S to be true  faulty  to avoid this path      PROPAG ATION USING ROBDDS  For our context  we need to compute the number of satisfying configurations for each instantiation of the cause variables  see Section     Now  if we order the variables as described in Theorem   we get an ROBDD where the nodes representing the cause variables are the nodes closest to the terminal node  This means that after one propagation we can determine all the values needed  i e   the number of configurations con sistent with ci  J and evidence e is given by   CardB C y e    c     v   ie    L  CtECt     where Ci is the set of nodes Ci with an outgoing   arc and   li is the number of arcs on the path li from the Ci in question to the terminal node  the single fault assumption ensures that there exist exactly one path from each Ci to the terminal node which include the   arc emanating from Ci  However  this scheme does not take user performable operations  i e  UA  into account  and in the follow ing section we extend the algorithm to include such scenarios       Inserting evidence  After an action has been performed we may gain new knowledge about the system  This knowledge is incor porated into the model by instantiating the appropri ate variable  If either a system variable or a cause vari able is instantiated we can use the method described in Section    So  let A E UA be a binary variable associ ated with a proper conditional probability distribution P AISi  and assume that A  y is observed  In order to take the state of A into consideration we get   P C   y  A y   P Cl   J Cz n       Cm n  A y   P C   y  IT P Ci n  L  P A yiSd Us B C   y  Cz n        Cm  n Us   By expanding the sum in the above equation we get   L P A yiSi B Cl  y  Cz n        Cm  n Us  Us  P A yiSi y CardB C   y  Si y   P A yiSi n CardB Cl   J Si n       Thus  with one piece of evidence we can retrieve the probabilities with two propagations  However  if we have a set of actions u      UA with parents u   Us   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            we need to count the number of satisfying configura tions consistent with each configuration of U   So  by using the above approach  the number of times we need to count is exponential in the number of variables on which evidence has been received  In what follows we will consider a different algorithm  where all values can be found after one propagation  Initially we assume that evidence has been received on exactly one variable  but the algorithm can easily be generalized to any number of variables  In order to prove the soundness of the algorithm we will use the following notation  If B is an ROBDD with root r  then l     v i r   v           V     vis a directed path in B  is termed the i th layer of B  the layers l  and ln    contain the root node and the terminal node  respectively  So  given a Boolean function over the variables U    X    Xz          X n   or dered by index   the corresponding ROBDD can be   specified as B   Us   U  lk       U o   assum ing that the variable X   is represented by the layer l     Now  let f   sp W   t IR be a function where W   Xi          Xi   U  and assume that the variables are ordered by index  We define the following parti   tioning of B   Us   u    lk    w r t  f           The root part of B w r t  f is given by BT      Uf   f    where Uf    u   lk  The conditioning part of B w r t  f is given by lk  Be   U      where U    ul i  The terminal part of B w r t  f is given by Bt      U     where U   u  i   lk   For ease of exposition  we shall in the remainder of this section assume that no evidence has been received on any variable in Us UUc  the results presented can easily be generalized to this situation also  Algorithm    Let B   U   U  li     U o  be an ROBDD corresponding to a Boolean function over the variables U    X    Xz          Xn   and assume that the variables are ordered by index  Let f   sp W   t IR be a function with W  U and let Q  W  Xj   where X i E W is the variable with highest index     i  Propagate from the root to the terminal nodes in the root part of B  ii  Use the values obtained in step  i  to perform a propagation in the conditioning part of B  i  e    for each q E sp  Q    a  Propagate to layer li  b  If there exists an arc  p  u  E Ci from a node p E lj to a node u E li    add the value to the value ofu   c ii X   i v p q   iii  Use the values obtained in step  ii  to propagate in st   Note  that the number of variables in the domain off determines the number of iterations performed by the algorithm  In particular  if IWI     we only need one iteration  Theorem    Let B    U   U   li       Uo  be an ROBDD and let f   sp W   t IR be a function where W  U  If Algorithm   is invoked on B  then      v l      L f w Cards w   wEsp W   Proof  Let Q  W  Xj   where X i E W is the variable i with highest index  Let q E sp  Q  and let nq       p E nuiP f   W or  p  u  E      Then  v u E li  we have      LqEsp Q   v u   LqEsp Q    L bE      pEn i b l v p qf q  bl      L bE O l  pEn l bl v p   q b f q  bl       LwEsp W   L vEn v p wf wl    f     LwEsp Wl w LvEn v p w   L f w v u w wEsp W        t     Let u E lt  for l   j      Suppose that  v p E lv p    LwEsp W  f w v P lw  Then  v p  LvEnu LwEsp W  f w v p w   v u  LvEnu        In particular we have that for l  n       v      LwEsp W  f w  LvEnu v p w    L f w Card  w  wEsp W  Thereby completing the proof   D  By performing induction in the number of operations the algorithm can easily be extended to handle multi ple functions  assuming that the variables in the do main of the functions do not overlap  the variables in the domain of two functions f and g are said to over lap w r t  the ordering a if a Xd   a X k     a Xj    where xk is a variable in the domain of g  and xi and Xi are the variables in the domain of f with lowest and highest index  respectively  If the variables of two functions overlap we can multiply these functions and consider the resulting function         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Example    Consider the troubleshooting model de picted in Figure    and assume that action A E UA is associated with the conditional probability distribu tion specified in Table    nA     z        B   B   C                    Table    The conditional probability function P AI z       Sz Sz                                        The ROBDD corresponding to this specification is de picted in Figure    In the naive approach  if A   y is observed  we perform three propagations to the termi nal node  one propagation for each configuration of  z and    except for   z            due to the single fault assumption   The resulting counts are weighted with the appropriate values and then added  see equa tion                                                        When using algorithm   we start off by propagating to the layer l   the nodes representing  z   after propa gation  each node in l  is associated with     We then perform two propagations to the layer ls  the nodes representing      each propagation is conditioned on the state of  z  i e     and    respectively  After each propagation  the resulting value is multiplied with the appropriate value from the conditional probability ta ble and then added to the value associated with its child  So  the final value can be found with less than two full propagations  see Figure     note that we only D perform one propagation in W and in Bt  Step  ii  of Algorithm   can be optimized by start ing the iteration with the variable with highest in dex  and then iterate in reverse order of their in dex  That is  when iterating over the variables  X   Xz          Xt    Xt  we can start off by propagat ing to the layer containing Xt  for some configuration of  X   Xz          Xt     The values associated with the nodes Xt   can then be used when propagating from the nodes Xt  for each instance of Xt  The same ap plies when considering variables of lower index  i e   we can reuse previous computations  For instance  in Figure   we can use the value from the first iteration when computing the value        associated with c   consistent with   z                        RESULTS  We have measured the performance of the ROBDD algorithm by comparing it to the Shafer Shenoy algo rithm  Shafer and Shenoy        and the Hugin algo rithm  Jensen et al         w r t  the number of opera tions performed during inference  the number of opera               B                                              r  a       Q                                                 i            bl  Figure    Figure  a  depicts the ROBDD after propa gation w r t  the configuration   z               Fig ure  b  depicts the ROBDD after the full propagation  no propagation is performed w r t   Sz          due to the single fault assumption        tions refers to the number of additions  multiplications and divisions  The tests were performed on     randomly generated troubleshooting models  see Definition    which dif fered in the number of system variables  cause vari ables and action variables  the total number of vari ables varied from    to     and for a fixed set of vari ables    different troubleshooting models were gener ated  As the single fault assumption is not ensured in the troubleshooting models we augmented these mod els with constraint variables when using the Hugin al gorithm and the Shafer Shenoy algorithm  the single fault assumption is naturally ensured in the ROBDD architecture   Finally  evidence were inserted on the problem defining variable and on the constraint vari ables  Figure   show plots of the number of operations per formed as a function of the number of variables in the models  Note that we use a logarithmic scale on the y axis and that the numbers on the x axis do not rep resent the actual number of variables in the models  The plots show that  w r t  the number of operations  propagation using ROBDDs is considerably more ef ficient than both Shafer Shenoy and Hugin propaga tion  Moreover  as indicated in Section    the tradi tional tradeoff between time and space is less apparent in the ROBDD architecture  as the space complexity is O IUcl    IUsl     It should be noted that the tests were designed to        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS                  I       ll  I    I  I  I  I  I     I  I       K         I                         I    Variable       Variable Sluorer Sheuoy  Figure    A plot of the number of operations per formed by Hugin  Shafer Shenoy and ROBDD prop agation as a function of the number of variables in randomly generated troubleshooting models  logarith mic scale   compare ROBDD propagation with Shafer Shenoy and Hugin propagation  and they should not be seen as a comparison of Shafer Shenoy propagation and Hugin propagation  In particular  we have only considered troubleshooting models and not Bayesian networks in general  The efficiency of the ROBDD architecture is partly based on the single fault assumption  However  this assumption can also be exploited in certain trou bleshooting models by compiling the original model TS   Us UUc UUA    P  into a secondary Bayesian network BN   UA U CU    S      P    where Cis a variable having a state for each cause variable in the original model together with a state representing the situation where no fault is present  S is a problem defining variable having Cas parent  and UA is the set of action variables in the original model each having C as parent  We have compared the ROBDD architec ture with this approach using the randomly generated troubleshooting models from the previous tests  see Figure        By using this secondary representation the speed up is less apparent  However  if we allow multiple faults then this representation can not be used  Moreover  a troubleshooting model allowing multiple faults will in general not be simpler than a model with no con straints on the number of faults  In the case of ROB DDs  assume that the single fault assumption still ap plies to the system variables and consider the case where exactly m components can fail simultaneously  m is generally  small    In this situation the number of nodes in the layers containing system nodes does not change but the number of nodes in the layers con taining cause nodes do  there can be a distinct path for each configuration of the cause nodes so the num   Figure    A plot of the number of operations per formed by ROBDD propagation and Hugin propaga tion with a single cause node  ber of nodes in the layers containing cause nodes is at most IUcl l    Hence the size of the ROBBD is O IUsl    IUcl l     note that in an ROBDD there does not exist two nodes having isomorphic subgraphs so the size of the ROBDD is usually much smaller  Now  as the complexity of propagation in an ROBDD is linear in its size  the maximum number of operations performed for m     increases by a factor of n l   with m faults the maximum number of operations increases i  This corresponds to adding a by a factor of constant value to the ROBDD plots in Figure   since we use a logarithmic scale on the y axis   n    Furthermore  if we redefine the m faults assump tion to cover at most m faults then the number of nodes in the layers containing cause nodes is at most IUcl Ll euicl   Again  it should be noticed that the actual number of nodes is usually significantly smaller as isomorphic subgraphs are collapsed  In case m faults is extended to include system vari ables also  it can be shown that the variables can be ordered s t  the number of nodes in the layers contain ing system nodes is exponential in m but quadratic in the number of system variables if m  maxsEUs Ins   see Figure     Finally  as the single fault assumption no longer ap plies  the number of configurations consistent with Ct  y and evidence y is given by  Cards Ct  y   y      v c     L    iy  Ci ECi li E Ci  L  where Ct is the set of nodes Ct with an outgoing   arc  Lt is the set of distinct paths from the Ct in question to the terminal node and   lt is the number of arcs on such a path  Having multiple faults also supports other frame    UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            be ordered s t  the size of the ROBDD is quadratic in the size of the domain           so       Numb rofy  emvuiableinlhed omaoll  Figure    The number of nodes in the layers containing system nodes as a function of the number of system variables  works like  de Kleer and Williams        and  Williams and Nayak         For instance  in cir cuit diagnosis  de Kleer and Williams        uses a logical model of the system to be diagnosed and determines the next action based on expected Shan non entropy  To calculate the expected Shannon entropy they require the conditional probability of a set of failed components  termed a candidate in  de Kleer and Williams         given some observa tion  As their framework does not yield an easy way to obtain this probability they use an approximation  In our framework the logical circuits can be represented as ROBDDs which makes the necessary probabilities easily available  So far we have not established a practical upper bound on the size of ROBDDs with m faults  but all the examples we have worked with until now have been of a  small  size  Moreover  several heuristic methods have been devised for finding a good order ing of the variables  see e g   Malik et al         and  Fujita et al              CONCLUSION  When modelling the behavior of man made machinery using Bayesian networks it frequently happens that a large part of the model is deterministic  In this pa per we have reduced the task of belief updating in the deterministic part of such models to the task of calculating the number of configurations satisfying a Boolean function  In particular  we have exploited that a Boolean function can be represented by an ROBDD  and in this particular framework the number of satisfying configurations can be calculated in time linear in the size of the ROBDD  The use of ROBDDs for belief updating was exempli fied in the context of troubleshooting  which is partic ular well suited as it was shown that the variables can  The performance of ROBDD propagation was com pared with Shafer Shenoy and Hugin propagation us ing randomly generated troubleshooting models  The results showed a substantial speed up and it was argued that the single fault assumption  underlying troubleshooting models  can be weakened without sig nificantly affecting the performance of the algorithm in case the number of faults is  small    
  A computational scheme for reasoning about dynamic systems using  causal  probabilistic networks is presented  The scheme is based on the framework of Lauritzen and Spiegel halter         and may be viewed as a gen eralization of the inference methods of clas sical time series analysis in the sense that it allows description of non linear  multi variate dynamic systems with complex con ditional independence structures  Further   the scheme provides a met hod for efficient backward smoothing and possibilities for effi cient  approximate forecasting methods  The scheme has been implemented on top of the HUGIN shell     INTRODUCTION  The application of probabilistic graphical models  belief nets  influence diagrams  etc   for model ing domains with inherent uncertainties has become widespread  A common trait of the domains  where such applications turn out most successfully  is their static nature  That is  each observable quantity is ob served once and for all  and confidence in the observa tions remaining true is not questioned  However  do mains involving repeated observations of a collection of random quantities arise in many fields of science  e g  medical  economic  biological   For such domains a static model is not very useful  the estimation of probability distributions of domain variables based on appropriate prior knowledge and observation of other domain variables is reliable only for a limited period of time  and further  upon arrival of new observations  both these and the old observations must be taken into account in the reasoning process  Thus  to cope with such dynamic systems using probabilistic networks we need to interconnect multiple instances of static net works  Obviously  as time evolves  new  slices  must be added to the model and old ones cut off  This introduces the notion dynamic probabilistic networks  DPNs    In general  a dynamic model may be defined as a se quence of submodels each representing the state of a dynamic system at a particular point or i nte rval in time  henceforth  such a time instance will be referred to as a times ice  Hence  a DPN consists of a series of  most often structurally identical  subnetworks inter connected by temporal relations  To make es t i mates of variables of a dynamic system in a way that makes full use of the information about past observations of t he system  requires a compact representation of this in formation  The creation of this representation is part of the process of reducing the dynamic model  This reduction process includes elimination of parts of the model representing past time slices  and should have no effect on future estimates  that is  the information conveyed by the eliminated part of the model should be completely represented in the remaining part  The complementary process of expanding the model must be carried out whenever new time slices have to be included in the model  In classical time series analysis  see e g  Box and  Jenk ins        or West and Harrison         the emphasis is on model assessment  i e  estimation of model pa rameters given a time series of observations of some stochastic process  The model thereby selected is then used for making predictions about future behaviour of the time series  Although the classical time series analysis techniques have been quite successful  their ability to cope with such important issues as complex independence structures and non linear relationships of have appeared to be rather modest  By formu lating the analysis in terms of DPNs both of these limitations vanish  Attempts to integrate methods of classical time series analysis with network representa tion and inference techniques have been presented hy Dagum  Galper and Horvitz         This paper  how ever  does not address the issue of model assessment  but merely problems related to making inferences  in cluding prediction and backward smoothing  in classi cal time series analysis terms   That is  the dynamic model is assumed to be given  Among research activities applying DPNs  as defined above  are a model for glucose prediction and insulin dose adjustment by Andreassen  Hovorka  Benn  Ole         Kjcerulff  sen and Carson         an approach to building plan ning and control systems by Dean  Basye and Lejter         a model for making judgements concerning persistence of propositions by Dean and Kanazawa         and a model for sensor validation by Nicholson and Brady         However  none of these activities have dealt with the issues of reasoning in DPNs  In Section   we briefly review some relevant graph the oretic concepts as well as some fundamental charac teristics of conventional  static  probabilistic networks and some of the DPNs introduced  The processes of re ducing and expanding DPNs are described in detail in Section   as well as the processes of backward smooth ing and forecasting  Section   briefly summarizes the presented scheme and provides a list of some of the yet unresolved issues      TERMINOLOGY  Commonly used graphtheoretic terms like  directed graph    undirected graph    triangulated graph    par ent    children    cliques    paths    cycles   etc  shall be used without formal definitions  see e g  Lauritzen and Spiegelhalter        for details on relevant the termi nology  We shall use the following abbreviations  the set of parents  children  ancestors  and neighbours of a vertex a are denoted by  respectively  pa a   ch a   an   a   and adj a   In the sequel the symbol denotes the binary operator producing the set of all unordered pairs of distinct elements of its arguments  In the fol lowing two paragraphs we review some less common graphtheoretic notation  For a directed graph g    V  E   gm denotes its moral graph obtained by adding edges between pairs of ver tices with common children and dropping the direc tions of the edges  A decomposition of an undirected graph g    V  E  is a triple  A  B  C  of non empty and disjoint subsets of V such that V   AU BU C  C separates A from B  and C is a complete subset of V  i e  each pair of vertices in C are neighbours   A decomposition  A  B  C  decomposes g into subgraphs OAuC and  Buc  i e  subgraphs induced by AUG and BUG  respectively   g is decomposable  triangulated  if and only if  A  B  C  decomposes g and both  AuC and fJBuC are decomposable  When a vertex a E V and the edges incident to o  are removed from g    V  E   o  is said to be deleted  but when adj a  are made a complete subset by adding the necessary edges  if any  to the graph before o  and the edges incident to a are removed  then o  is said to be eliminated  Note that connectivity of a graph is invariant under elimination  but not necessarily under deletion  The set  say T  of edges added by eliminating all vertices in V in any order is called a triangulation of g as  V  E U T  is triangulated  The edges of T are called fill edges or fill ins  An elimination order is a bijection     V                  lVI   g  is an ordered graph  The triangulation T  i   is the set of edges  produced by eliminating the vertices of g in order    An elimination order   is perfect if T g         A probabilistic network  as used in this paper  is built on a directed  acyclic graph  DAG  g    V  E   where each vertex a E V corresponds to a discrete random variable X   with finite state space X   For A  V  XA denotes the vector of variables indexed by A  Sim ilarly  XA denotes an element of the joint state spA ce XA   XaeAXa  Each random variable X   of a  proba bilistic network is described in terms of a conditional probability distribution p xa I Xpa a   over X    where p x   I Xpa a   reduces to an unconditional distribution if pa a       In  i  the conditioning variables of Xa are represented by pa a   The joint probability  p   p    over Xv is the product of all conditional and uncon ditional probabilities   i is called the independence graph of p  since for each non adjacent pair a     E V  all     r if and only if any path between a  and f  in Am contains at least one member of r  V  wher e A is the subgraph of  i induced by   o      U an  a Uan   J   Lauritzen  Dawid  Larsen and Leimer        Let V be a set of non empty subsets of V  Then p has potent ial representation if  p x     z         c      z    IT    A xA    AEV  where    A are called potentials and z is called the nor malization constant  In particular  the product of all p x   I Xpa a    a E V  is a potential representation wit h normalization constant    By exploiting the conditional independence relations represented by g  the joint probability space  Xv  may be decomposed into a set of subspaces  Xc  cec  where C is the set of cliques of  V  EU T O     Spiegelhal ter       Lauritzen and Spiegelhalter        such t ha t computation of marginal distributions can be done in a junction tree T    C     Jensen       Jensen  Lauritzen and Olesen       with nodes C and arcs   C   C representing clique intersections  where for each path  C   C         Ck  D  in T  CnD c CinCi for all     i      j   k  The existence of a potential rep resentation is guaranteed in a junction tree  and the tree is said to be calibrated if f lc xcnD    T JD  xcnD  for all xcnD E XcnD and all C  D E C  where C n D         Two junction trees T     Ct     and T     C     with non empty and complete intersec tion S   Ct n C   where Ct E C  and C  E C   are said to be jointly calibrated if both T  and T   are calibrated and    c   cs       c xs  for all xs E Xs  Calculation of marginal distributions in a junction tree is done in a two stage process involving collection and distribution of marginal potentials between all neigh bours in the tree  These two operations performed in sequence are jointly referred to as propagation  or fusion and propagation    A DPN represents a finite  though possi b l y varying  number  say n  of time slices  Thus  the vertices V of the graph g    V  E  of the network consists of disjoint subsets each representing the random variables   A  X t  of a particular time slice appropriately chosen t V   V t  n      U  Computational Scheme for Reasoning in Dynamic Probabilistic Networks  t  That is  for some      U  The subset int t  r  V t  is called the is defined as int t    a E  V t    The time slices of a DPN are assumed to be chosen such that the DPN obeys the Markov property  the future is conditionally independent of the past given the present  Formally this may be written as  interface of time  slice t and  V t       E  V t       a j    E  Eint t     The moralized graph of the sample DPN DAG in Fig ure   appears in Figure    where the interfaces are indicated by filled circles   note that int O         X O         X t     ll X t            X t   k  I X t   for all t     and k time slice         Time slice   is called the initial  The set of directed edges    a      I a E V t         E V t   r  E  is called the temporal edges  or temporal relations   of time slice t and express conditional independence assumptions between slices t   and t  Thus  temporal edges are those between vertices of adjacent time slices   see Figure            Figure      n      future  slices  Figure    Time slices of a DPN  of the corresponding graph  V     V t   r  u      and the edges by    u  g    V  E  is given by  V t   r J    E  E t   r  U E  t   r     U     where  At any point in time  there is a series P         PN of distinct but strongly related models  where each Pn        n     N  is specified by the quadruple  p   in t    n  t p n    where t    n    t p n  is the old est and t p n  the newest time slice represented by P    and where gn    Vn   En  is the independence graph of the probability p  At any time   PN refers to the most recent model called the current model    Vn  En    BG V  Figure    Sample initial DPN moral graph   Sample initial DPN DAG    past  slices          r J        U       E  t   r J         E t  r  V t  V t   E  t    E t U Eint t     E int t  r  V t     V t    Obviously  the set of temporal edges of time slice a subset of Eint t    n    By the series P         PN we understand the following  For any       n     N the graph  n of Pn is given by  At time slice t      a DPN represents  r  past  slices and rjJ   future  slices  see Figure      Thus  the vertices  If       V  E     V U in t      E U Emt t     if n   N  if n    N         where t    t    n     and VandE are given by     and        respectively  with t   r   t    n    t      t p n   Although Pn  n   N  contains variables of Pn l we define tr   n    t   n          Thus t   n  represents the latest time slice about which Pn is guaranteed to be capable of containing complete information  For any       n   N  t    n  and t p n  are fixed  Also t     N    t p N         is fixed  but t p N  is a non decreasing number meaning that the expanded model generated by including new time slices to PN is still referrecl t o  asPN   Finally  by  gN  Q CQ  Q    Vn  En gn   n l l composite graph of g           N      we denote the     REASONING IN DPNs  The time slices  t is     t or N         t p N  of the current moclel   PN  are divided into two groups  the first w slices  con stitute a group referred to as the window of time slices             Kj l rulff   or simply the window   and the remaining time slices comprising t    N    w       tq  N  are referred to as the forecasting slices  see Figure    Similarly  the time Forecasting  Backward smoolhillg                   h                                                                          l   N    a    s         J                                  i p                            w l  i i Window                                                       Figure    The current model  of w time slices   PN                                           includes a window  slices           tq  N     are referred to as the backward smoothing slices  Note that the term forecasting slices is slightly imprecise since all inference concerning vari ables of time slices for which no observations have been entered  actually are forecasts  even if such time slices belong to the window  For similar reasons the term backward smoothing slices is also slightly imprecise  For the purpose of making inferences  the window is assumed to consist of a triangulated version of the composite graph of the time slices involved  Hence a junction tree is associated with the window such that inferences in it are carried out as in a conventional static network  Inferences involving backward smooth ing and forecasting are described in Sections     and        The process of moving the window forward involves the two more or less separate processes of model expansion and model reduction discussed in detail in Sections     and      Since the window is represented by a junction tree  these processes roughly amount to  respectively  adding a new subtree to the junction tree and cutting off a part of the tree  Model expansion by  say  k new time slices consists of  a  adding k new slices  conditional representation  to the current model  i e  t N        tq  N    k    b  moralizing the hybrid composite graph of the trian gulated graph of the window and the DAGs of the k consecutive time slices starting at t   N    w   c  tri angulating that graph and identifying the new clique set   d  constructing the new  expanded  junction tree  and  e  calibrating the new clique potentials with ap propriate consideration of the old ones  As discussed in Section      the last step is optional  Expanding the current model by k new time slices causes the width of the window to b e increased by k  while the number of forecasting slices remains unchanged  Model reduction by k time slices involves elim ination of all variables pertaining to time slices t    N         t    N    k     Recall that elimination of a vertex a  variable Xcr  forms a complete subset of the vertices adj  a    unless the y already constitute a com plete set  The end product of the elimination process is a potential involving the variables int t   N    k   This potential  represented in one of the cliques of the reduced junction tree  say  represents all informa   TN  tion about the past necessary for the reduced m o del to take full account of the knowledge about t he history of the system  Reducing the current model by k time slices causes the number of backward smoothing slices to be increased by k and the width of the wi nd o w t o he decreased by k  while the number of forecasting slicr s remains unchanged  Two issues are of major importance here   a  if back ward smoothing is to be performed  the cliques of t he triangulated graph resulting from the reduction pro cess must be linked together in a new junction t r ee   say  such that backward smoothing can be per formed by passing messages from   N to   N    via the potential involving variables int t   N    k   and  b  since both the expansion and the reduction process performs a triangulation  i e  finds an elimination or der  of  basically  the same model  these two processes should be coordinated such that the same elimination order is employed   TN    The triangulation carried out as a subtask of the ex pansion process is unconstrained in the sense that the search space of elimination orders consists of all per mutations of the set V of vertices of the  expanded  window  whereas the reduction process may be per ceived as a constrained triangulation  where the ver tices eliminated define the prefix of orders compris  ing all vertices in V  Then obviously it might be ad vantageous to make a constrained decomposition in the first place  rendering the reduction process triv ial  provided it is carried out in the fundamental way described above  i e  assuming the reduction concerns k lumps of  PN  where each lump inclu des all vertices of a particular time slice   This introduces the notion of a constrained elimination order which is discut  sed further in Section            CONSTRAINED ELIMINATION ORDERS  A constrained elimination order is defined  as  follows   gN    Lj I gn    V  E  be a com posite graph and let     V                  lVI  define nn elimination order  This order is said to be constrained if   a           for all       i   j     N  a  E V  and    E Vj  Similarly  T g  is said to be a constrained triangulation of gN    Definition   Let  Constrained elimination orders have a number of i m portant properties which shall be used in Sections     and       First  we observe that the order in which the vertices are eliminated does not affect the com plexity of PN  This fact follows from L emma   Lhe proof of which has been made by Rose  Tarjan and Lueker          Uo n N Vn  Lemma    Rose et al          Let g     V  E  be an ordered graph  Then  a       E E U T     if   A  Computational Scheme for Reasoning in Dynamic Probabilistic Networks  and only if there is a path  o               O k       such that   o i    min    o       B   for al      i   k   implicit as part of the operation of moving the window k time slices forward   This property implies that  under constrained elimina tion  an optimal elimination order for gN   U l fJn is given by optimal orders for fJn         n      N   A new time slice is added to the current model via con ditional probability relations such that the variables added have parents among the variables of the current model  relations in the opposite direction are not al lowed   The structure of the DAGs of the conditional models of individual time slices will most often be iden tical  Note  however  that we make no structural or logical restrictions as to the conditional networks and temporal relations added  Thus  if an initial assump tion implying identical time slice models turn ou I  t o be inadequate or erroneous  the presented scheme poses no obstacles to changing such assumptions   Let  P          PN be a series of conditional models with composite moral graph    N m  and let Pi         Piv be the corresponding constrainedly decom posable models with composite Jraph      N  Then for any        t      t   N   int t  in      m is a complete sep arator of  fJ  N  Lemma       Proof  From the definition of int t  it follows that int t  is a separator of    N m  Since  Pi       Piv are constrainedly decomposable it follows from Lemma   that for all paths  o    o         o           where o  E V t     and    E V t  int t    o          a   nint t       That is   int t  is also a separator of     N  Also due to the constrained elimination order it follows from Lemma   that int t  induces a complete subgraph of D       N     Thus under constrained elimination the interface of time slice t         t      t   N   is identical in the moral and the corresponding decomposable graphs  This re sult is used in the following    Let  P          P be a series of constrainedly N decomposable models with composite graph  Lemma  gN  N     U fJn     In order to produce a junction tree for the expanded window we perform the operations of moralization and triangulation  The moralization step involves moral ization of the hybrid composite graph  of the triangu lated graph of the window and the DAGs of the k new time slices  and implies that the conditional probabil ities of the k new time slices of the window are con ceived as potentials  These potentials are in turn at tached to appropriate cliques of the triangulated graph resulting by employing the constrained triangulation scheme to the moralized graph  A sample model ex pansion is shown in F igure    where the dashed lines are the edges added by moralization  In this example  the window is assumed to consist of a single time slice  the initial one     V E    n l  Then gN is constrainedly triangulated  Proof  From Lemma   we have that for any        t      t   N   int t  is a complete separator of gN and hence  A  B  int t   is a decomposition of gN  where A   V l UUV t     and B   V  AUint t    The graphs  fuint t  and  f uint t  have complete sep arators int l         int t  and int t         int t   N    re spectively  Continuing this argument we end up with subgraphs           N all of which are constrainedly D triangulated  and the result follows     This shows that backward smoothing  at least in prin ciple  can be accomplished by constructing a junction tree for gN and performing propagation in that tree  However  a less space consuming technique exists as described in Section           MODEL EXPANSION  The operation of expanding the current model by  say  k new time slices t   N            t   N    k is carried out for the purpose of including k new time slices  not necessarily tq  N            t p N    k  into the window  The wish to expand the window may be explicit or                                             rune  sliceO  Time slice    Figure    Sample model expansion  Obviously  in finding an optimal elimination order  we have to take into consideration the topology of the graph as it appears after addition of the next time slice  Since we want the model complexity in terms of the state space size to be as low as possible to minimize the complexity of inference  and since the state space size varies heavily over the range of elimination orders  a careful analysis must be conducted to establish an appropriate order  To find an optimum elimination order for an arbitrary graph is  however  an NP hard problem as proved by Wen         Yet  in practice             Kj erulff it turns out that near optimum triangulations may be found using simple heuristic ordering strategies  Rose       Kjrerulff        In Figure   the applied elimina tion order is b  e     c  g  d  a  h   The original directed and moral graphs are shown in Figures   and     Having found the cliques of the new expanded graph on the basis of an appropriate elimination order  the next step concerns construction of a junction tree for those cliques  As much as possible of the junction tree  l     C    in existence prior to the expansion should be reused in order to minimize the amount of work required to construct the expanded junction tree l      C      Note that as a direct consequence of the constrained decomposition scheme there is for each  old  clique C E C a  new  clique C  E C  such that C  C   For some cliques the containment might be strict  The creation of l   can be described as follows     Identify the set C  of cliques of        Construct a  skeleton  of l     a  Create clique objects for all members of C  C and clique intersection objects for all mem bers of   C   b  Initiate the potential tables of these new clique and clique intersection objects to unity   The potentia   tables of the cliques in C n C  and of the clique intersections in en  remain unchanged      For each C E C  C  and each E E    i e   old  cliques rendered redundant and their associated intersections  attach  by multiplication  the asso ciated potential tables to the tables of appropriate clique and clique intersection objects     Attach the conditional probability tables of the variables of the new time slices to appropriate new cliques   The term  appropriate  in points   and   refers to the index set of the table to be attached being a subset of the clique or clique intersection upon which it is at tached   The expanded junction tree    has now been created  That is  a potential representation for the joint probability distribution for the expanded win dow has been established  In Figure   the cliques and clique intersections remaining unchanged are shown in bold and the attachment of potential tables of redun dant  old  cliques and clique intersections are indicated by dashed arrows  Note that the cliques has been numbered according to the order of creation using the above elimination order and that clique   in part a is a proper subset of clique   in part b   Now  if we have an immediate interest in the marginal distributions of variables  or sets of variables  in the k new time slices of the window  a propagation can be performed  otherwise we might postpone the propaga tion step until e g  new observations has been recorded  If l  was calibrated immediately before the model ex pansion was executed  we only need to perform prop   a  b  Figure    Sample junction tree expansion agation cliques       m  the subtree induced by the set of  new  MODEL REDUCTION  Due to the constrained decomposition scheme em ployed by the model expansion process  model reduc tion becomes a relatively easy task as previously dis cussed  In developing a model reduction scheme it is important to recognize the requirements for convenient backward smoothing beyond time slice tr N   Below we develop a reduction scheme which meets such re quirements and which is based on the results of the following theorem  Theorem   Let  P          PN be a series of con strainedly decomposable models  where each  P   l  i  N  is calibrated  Assume  Pn   and Pn are jointly uncalibrated for some     n   N  Complete informa tion required to calibrate  Pn I to  Pn or vice versa is represented by the marginal rf int t   n   where there is a  clique of n   and a clique c  of n such that int t     n   c Ct and int tr n    C    cl    Proof  From Lemma   we have that int tr n   is a complete separator of Yn   U Yn and hence tPint t  n   contains complete mutual information between  Pn   and  Pn  From the definition of Yi     i   N   cf       we have that int tr n   C V  t cl   n        and since for each pair  a P   where a E V tcl  n      and P E int tr n      a      P   int tr n   induces a complete subgraph of Yn l Hence there is a clique C  of Yn   such that int t     n   C Ct  Since int t     n    is complete in Yn   it follows immediately that it is also complete in  n and hence there is a clique C  in  n   such that int t      n    c    So far we have not been concerned with the pro cess of creating new models to be added to a series P            PN  However  the reduction process partition  PN into two models  one representing the time slices eliminated and the other the remaining time slices of  PN  subsequently defining the new current model   That is  whenever  PN is subjected to reduction  the number  N  of models is increased by one  Thus  r on forming to      we define the reduction of  PN by the   A  Computational Scheme for Reasoning in Dynamic Probabilistic Networks  k oldest time slices by sequentially executing the fol lowing steps       Let        g     V   E    to   t r N   tk k   where    k   t N   t r N  and  V      V to  U        U     t   N     V tk  u int tk        E to  U E  t   U U E  tk     Let N    N         Let  PN    g N    VN EN  t lr N    tk      t N    t N       where E      VN          VN I    V    int t r N     EN  EN I  E       Let   PN l           In terms of operations on the junction tree of  PN  ac tually the junction tree of the window  an equivalent description of the reduction process may be formulated as follows  where t   t r N    k     is the oldest time slice of the window when the reduction has been com pleted               Prior to the reduction  let tree for  PN  Let C       C  E  i    C    be a junction  C I C n U V i    f     be the  cliques containing variables to be eliminated  and C    C   C  the remaining cliques   Let i    iC  an d i    ien be the junction trees induced by C  and C   respectively  see Figure         Let B    C  E  C  I adj C   n C    f   in i    E B such that int t        C then add int t  to C  and let adj int t     B  otherwise add B   C  to adj C        If there is no C     Let  N      N       First assume that the condition of the  if  part of Step   holds  Since the constrained decomposition forces int t  to induce a complete subgraph of g N and since there is no clique in C  containing int t   then int  t  itself must be a clique of g N  The subset B       C   where for each B E B there is a non empty intersec tion between the adjacency set of B and C  in T  is then made the adjacency set of int t   Since the path in   between any pair of elements of B includes ele ments of C   i e  C  separates the elements of B from one another   this does not violate the tree structure ofi  Neither does it violate the property ofi being a junction tree  as the intersection of any pair  C   C    of cliques  where C  E C  and C  E C   is a subset of int  t      Next  assume the condition to fail  i e  there is a clique E C  such that int t      C     in which case B   C  is made a subset of the adjacency set of C in     With arguments similar to those above it is readily reali r ed that the property of T  being a junction tree is not violated   C       BACKWARD SMOOTHING  Clearly  the arrival of external evidence  observations  affects not only the estimates of  unobserved  variables of the relevant time slice s   but may also have sig nificant effect on estimates of variables of other time slices  The process of re estimating variables of past slices in light of new evidence  retrospective assess ment  is often referred to as backward smoothing  If the variables for which re estimated probability distri butions are required  are all included in the current model   PN  backward smoothing is an implicit part of propagation in the window of time slices  However  if we want to backward smooth from  PN to  PN   special actions should be taken  Specifically  complete infor mation about observations pertaining to the window should be transferred from  PN to  PN l Given the model reduction strategy described in Sec tion     the process of propagating complete relevant information backward from  Pn to  Pn   or forward from  Pn   to Pn becomes very simple  Consider t he example where         PN are calibrated  but jointly uncalibrated  Let the inconsistency be caused by a series E     EN of sets of external evidence  such that  Pn     n  N  is uninformed of En l         EN  Now   Pn may become informed of En l EN by the following calibration process  see also Figure     For convenience we first define the concept of an inl er fa ce clique as follows    T            Figure    Partitioning i into  i   and      After the execution of Steps       PN   is given by i  and  PN by   which is the result of modifying i  as described in Step   above  It is easily verified that i   is a junction tree for g N of Step   of the four step description of the reduction process   Definition   Let             PN be a series of con strainedly decomposable models  Then for any    n  N let I   denote the set of cliques of n such that for any IC   E I    int t r n         IC    Similarly  for any    n   N let I t denote the set of cliques of  n such tha t for any IC t E I t  int t r n       C IC t  IC   a nd  c  are called interface cliques of Pn              Kjcerulff     Initially let i   N  Then repeat steps   and   sequentially while i   n     Let ICi  E Ii    Ct   E It    and I  int t    i       tP c      L  cv    Ic       L w     I t JJet    where superscript     denotes the updated poten tial     Calibrate Pi t by propagation and decrement i by one   cases even larger than those required by exact meth ods  but of course with much less space requirements since the sampling is performed in the DAG struc ture involving relatively low dimensional probability tables  Another important feature of sampling meth ods is that the time complexity grows only linearly in the dimensionality of the tables involved  whereas it grows exponentially for exact methods  Another method that might be fruitful is based on the fact that  a subset of  the conditional probabilities of a probabilistic model quite often exhibits linearity in the sense that they are  approximately  linear functions in the variables upon which they are given  That is              Figure    Backward smoothing from PN to Pn       FORECASTING  In time series analysis applications there is typically a desire to make optimal forecasts of the random pro cess considered  Within the computational framework presented above  forecasts which do not exceed the extent of the window are an implicit part of propaga tion in the junction tree of the window  otherwise it may be performed by expanding the window by the required number time slices  If forecasts are wanted for a large number of time slices ahead of the window  the complexity of the resulting decomposable model might  however  easily exceed the capacity of the avail able computing resources  Such cases may be solved in a number of ways  One is to move the window the required number of steps  where propagation is performed in each step  and subsequently  moving it back again  This might  however  be a very time consuming operation  and fur thermore  a lot of unnecessary calculations will quite often be carried out as we typically only want the fore casts for a limited number of variables  Therefore  there is a demand for alternative forecasting methods which either avoids the junction tree approach and or exploits the fact that forecasts are only required for a limited number of variables  Concerning non junction tree methods  i e  no trian gulation   various Monte Carlo sampling schemes may be useful  A common trait of these schemes is the fact that the variance of the resulting distributions can be made arbitrarily small  In fact  some of the most fruit ful approaches to variance reduction is Monte Carlo sampling  Ripley        Note  however  that a reduc tion of the standard error of an estimator by a factor of k requires an increase in the sampling size  n  by around a factor of k  due to the ubiquitous  j  fii  law of statistical variation  Thus  to get forecasts within a small distance from the  exact  values  we should ex pect the computing time to be relatively large  in some  p xa    L  Xpa      p xOI I Xpa a    rr  PEpa a   p xp    The method is then simply given by calculating n ll such approximate marginal probability distributions in an appropriate order  i e  the distributions of all parents of a variable should be calculated before the distribution of the variable itself   Given that the di vergence between such approximate distributions and the  exact  ones are below an acceptable upper bound for the variables of interest  this is a very fast fore casting method  The interesting point concerning the exactness of the method is that an upper bound on the error can be computed in advance by application of theorems of linear algebra     SUMMARY  We have presented a computational scheme for rea soning in dynamic probabilistic networks featuring de scription of non linear  multivariate dynamic systems with complex conditional independence structures and providing a mechanism for efficient backward smooth ing  As opposed to a static network representing a finite and fixed number of time slices  i e  capable of reasoning only about a finite series of observations of a dynamic system  the proposed scheme can handle infi nite series of observations  Further  in applying static networks representing a fixed number of time slices as models of dynamic systems  there is typically a desire to include as many time slices as possible in the model  Thus  inference easily becomes time consuming and in flexible  i e  propagation involves all time slices in the model even if updated distributions are wanted only for a limited number of time slices   The proposed scheme  on the other hand  provides a high degree of flexibility in the reasoning process  since the widt h of the window of time slices can be changed dynamically as well as the number of  backward smoothing slices  and the number of  forecasting slices   In addition  the scheme provides selective inference in the sense t hat inference can be performed in  i  the window  as  ii  backward smoothing  or as  iii  forecasting  Since the presented model reduction scheme supports a convenient and efficient backward smoothing method   A  Computational Scheme for Reasoning in Dynamic Probabilistic Networks  it also supports inclusion and modification of obser vations pertaining to time slices  to the left of   the window  Delayed observations is a quite typical phe nomenon  for example  in a medical setting delays may be caused by processing time in a laboratory  e g  anal ysis of a blood sample    Dean  T  and Kanazawa  K            A model for rea soning about persistence and causation  Compu  Although we have presented a scheme for reasoning in dynamic networks  a range of issues still remain to be dealt with  A couple of the most important issues are the following   Workshop on Innovative Approaches to Planning  Scheduling  and Control  pp            Only preliminary studies has been carried out to inves tigate the applicabilities the various forecasting meth ods discussed in Section      Especially  a scheme for establishing an upper bound on the forecast error by applying the linear approximation algorithm is desir able  But also a study of the applicability of various Monte Carlo sampling schemes should be conducted  Since many applications feature a large number of tem poral relations  the state space sizes of the interface cliques of the time slices of the window and of the  backward smoothing slices  may become unmanage ably large  In such cases there will be a need for approximations  One obvious way of approximating the inference is to exclude some of the edges required between members of the interface set of a time slice  An extreme approach could be assumption of indepen dence between all parents of interface variables  i e  no fill edges at all added between interface vertices   To that end  studies on the upper bounds of the resulting error and its attenuation as time evolves  should be conducted  An implementation of the computational scheme pre sented in this paper has been built on top of the H U GIN shell   Acknowledgements I wish to thank Steffen L  Lauritzen for his valuable comments on an earlier draft of this paper and other members of the ODIN group at Aalborg University for stimulating discussions   
  To investigate the robustness of the output probabilities of a Bayesian network  a sensi tivity analysis can be performed  A one way sensitivity analysis establishes  for each of the probability parameters of a network  a func tion expressing a posterior marginal proba bility of interest in terms of the parameter  Current methods for computing the coeffi cients in such a function rely on a large num ber of network evaluations  In this paper  we present a method that requires just a single outward propagation in a junction tree for es tablishing the coefficients in the functions for all possible parameters  in addition  an in ward propagation is required for processing evidence  Conversely  the method requires a single outward propagation for computing the coefficients in the functions expressing all possible posterior marginals in terms of a sin gle parameter  We extend these results to an n way sensitivity analysis in which sets of pa rameters are studied      INTRODUC TION  The robustness of the output probabilities of a Bayesian network can be investigated by performing a sensitivity analysis of the network  For mathemat ical models in general  sensitivity analysis serves to identify the effects of the inaccuracies in a model s pa rameters on its output  Morgan   Henrion        For a Bayesian network  more specifically  performing a sensitivity analysis yields insight in the relation be tween the probability parameters of the network and its posterior marginals  The simplest type of sensi tivity analysis is a one way analysis in which a single parameter is studied  a more general n way analysis serves to investigate the joint effects of inaccuracies in  a set of parameters  In the brute force approach to performing a one way sensitivity analysis of a Bayesian network  each proba bility parameter of the network is varied systematically and the effect on the output probabilities of the net work is investigated  Performing a sensitivity analysis in this way requires thousands of network evaluations  or  full  propagations  and is  therefore  much too time consuming to be of any practical use  Laskey        has been the first to address the compu tational complexity of sensitivity analysis of Bayesian networks  She has introduced a method for computing the partial derivative of a posterior marginal proba bility with respect to a parameter under study  Her method thus yields a first order approximation of the effect of varying a single probability parameter on a posterior marginal  Compared to the brute force ap proach  her method requires considerably less compu tational effort  The method  however  provides insight only in the effect of small variations of parameters  when larger variations are considered  the quality of the approximation may rapidly break down  The relation between a posterior marginal probability of interest and a parameter under study can be ex pressed through a simple mathematical function  The function expressing the posterior marginal is a quo tient of two linear functions in the parameter  as has been shown by Castillo et al          Building upon this property  it suffices to establish the coefficients in this function to determine the effect of parameter variation  Castillo et al         and Coupe   van der Gaag        have designed methods to this end  These methods require a single network evaluation for each coefficient to be established  Although these methods currently are the most efficient available  they rely on a large number of network evaluations and  as a con sequence  are infeasible for large realistic networks  In this paper  we present a new method for sensitivity analysis of Bayesian networks  Our method  like the        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       two methods mentioned above  exploits the property that a posterior marginal probability relates by a sim ple mathematical function to a parameter under study  It requires just a single outward propagation in a junc tion tree  however  to compute the coefficients in the functions for all possible parameters  in addition  it re quires an inward propagation for processing evidence  Conversely  the method requires a single outward prop agation for establishing the coefficients in the functions expressing all possible posterior marginals in terms of a single parameter  Our method can be readily ex tended to an n way sensitivity analysis in which sets of parameters are varied  In addition to a sensitivity analysis  an uncertainty analysis can be performed for investigating the robust ness of the output probabilities of a Bayesian network  In an uncertainty analysis  all parameters are varied si multaneously through sampling  it therefore provides little insight into the effects of variation of specific pa rameters  Experiments with uncertainty analysis have led to the suggestion that Bayesian networks are in sensitive to inaccuracies in their parameters  Pradhan et al        Henrion et al         In these experiments  however  a measure of model robustness was obtained by assuming a lognormal distribution for each parame ter and averaging over the probability of the true diag nosis for various diagnostic situations in a medical ap plication  Rather than in the average of the probabili ties of the true diagnosis  however  it is in the variation of these probabilities that inaccuracies in parameters are reflected  From these experimental results  there fore  no decisive conclusions can be drawn as to the sensitivity of Bayesian networks  In fact  Coupe et al         have reported high sensitivities in an emprical study in the medical domain  involving real patient data  We feel that these and emerging similar expe riences warrant further investigation into sensitivity analysis of Bayesian networks      THE BASIC P ROPERT Y  Sensitivity analysis of a Bayesian network basically amounts to establishing  for each of the network s pa rameters  a function expressing an output probabil ity in terms of the parameter under study  For out put probabilities  we shall consider posterior marginal probabilities of the form y   p a I e   where a is a value of a variable A and e denotes the evidence avail able  Each of the network s parameters is of the form x   p bi l r   where b  is a value of a variable B and  r is an arbitrary combination of values of the set of parents II  pa B  of B  We will write p ale  x  to denote the function expressing the posterior marginal p a I e  in terms of the parameter x  In the sequel  we will assume that in a sensitivity anal ysis  upon varying a parameter x p b  l r   each of the other probabilitiesp bj l r  is co varied accordingly  by scaling by the ratio between the probability masses left  More formally  let the variable B have for its do main dom B   b      bm   m       Note that the parameters p bj l r   j  f  i   are functions of x  We now assume for these functions that       if j  i otherwise  with p b  l r               With the assumption of co variation as outlined above  the function y x  yielded by a sensitivity analysis is a quotient of two linear functions in x  The following theorem reviews this important property  the associ ated proof provides the basis for the algorithms pre sented in Sections   and    Theorem   Let p be the probability function defined by a Bayesian network over a set of variables V  Let y   p a I e  and x p b  l r  be as indicated above  Then     The paper is organised as follows  Section   reviews the important basic property that a posterior marginal probability can be expressed as a quotient of two linear functions in a parameter under study  In Section    we briefly describe currently available methods for sensi tivity analysis that build upon this property  In Sec tion    we present our method for computing the co efficients in the functions for all possible parameters  using just one propagation in a junction tree  In Sec tion    we describe a similar method for computing the coefficients in the sensitivity functions relating all possible posterior marginals to a single probability pa rameter  These methods are generalised to an n way sensitivity analysis in Section    The paper ends with some concluding remarks in Section     y  where a            p a e  x  p e  x   ax  f    X           and   are constants with respect to x   Proof  The joint probability p a e  can be expressed  in terms of x as  p a e  x        V    x    where Lv a     dp V  denotes summation over the variables V   A          D  with A        DE V fixed at values a        d  respectively  The sum Lv a ep V  in the above equation can be split into n    separate sums  such that the first sum        UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       includes only terms with the value b  for B and the state  r for II  the second sum includes only terms with the value b  for B and II in state  r  and so on  and the last sum includes the remaining terms  So           as described by Castillo et a           After having identified the set of relevant parameters  the sensitivity analysis can be restricted to this set  Building upon the set of n relevant parameters  x           Xn  the algorithm of Castillo et a          iden tifies sets of monomials for which the coefficients will be zero in the linear function p a  e  x           Xn    For the resulting m monomials  the algorithm constructs a system of m independent equations of the form i y p a  e  xi          x   where  for each j  xj denote arbitrary values for parameter Xj  The corresponding values yi  i          m  are obtained through m net work evaluations  The coefficients in the function are now determined by solving the set of equations thus obtained  Coupe   van der Gaag independently de scribed a similar method  also based on the idea of solving a system of independent equations  They fur ther argue that in a one way sensitivity analysis three network evaluations suffice per relevant parameter        p       p V     l v a e b      V   L L   p bl r    V a e n f    j i For the probability p e  we derive a similar expression by summing  in the above derivation  over all values of the variable A instead of keeping it fixed at a  From the resulting expressions p a  e  x  and p e  x   it is readily seen that the output y p a I e  can be written as a quotient of two functions that are linear in x  D    From Theorem   we have that the function that ex presses a posterior marginal probability y in terms of a single parameter x is characterised by at most three coefficients  The theorem is easily extended to n parameters  The function then includes the prod ucts of all possible combinations of parameters  termed monomials  in both its numerator and its denomina tor  The numerator as well as the denominator are characterised by  n coefficients  many of which may be zero     CURRENT METHODS  The most efficient methods for sensitivity analysis of Bayesian networks currently available exploit the basic property reviewed in the previous section  We briefly review these methods  Not all parameters in a Bayesian network can influ ence a posterior marginal probability of interest  The subset of parameters  possibly  influencing the poste rior marginal is dependent upon the evidence e  The set of relevant parameters is easily identified using a variation of the algorithm described by Geiger et a    The methods reviewed above have a computational complexity that is considerably less than the brute force approach of systematic variation of parameters  However  the methods can still be quite time consum ing  for a network of realistic size  it can easily require several hundreds of network evaluations to perform a one way sensitivity analysis  An more general n way sensitivity analysis to study the joint effect of simulta neous variation of n parameters can in fact be so time consuming that it is infeasible in practice     ANALYSIS OF ONE OUTPUT WRT  ALL PARAMETERS  The new methods for sensitivity analysis presented in this paper have been tailored to Bayesian networks in their junction tree representation  The methods basi cally perform a single or a few outward propagations in a junction tree and  as a result  are much less time consuming than the methods reviewed in the previous section  In this section  we present our method for computing the coefficients in the sensitivity functions expressing a posterior marginal of interest y p a I e  in terms of all possible probability parameters x  Recall that these functions are of the form presented in Theorem    Our method now builds on the idea that  in a junction tree  the expressions for p a  e  and p e  in terms of x can be derived from the potential of a clique containing both the variable and the parents to which the parameter x pertains  The following theorem details the coefficients to be computed     Theorem   Let p be the probability function defined by a Bayesian network and let T be a junction tree   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            for the network  Let y   p aie  and x   p b  l r  be as before  Suppose that  in T  an inward propagation has been performed towards a clique containing the variable of interest A  suppose that subsequently an outward propagation from this clique has been per formed with the value a for A  Now  let K be a clique in T containing both the variable B and its par ents II  pa B   let x   p K a e  be the potential of clique K after the abovementioned propagations  Then  p a e  x   ax     with  a   LK b      x    LK b      x p b l r      p b l r         J  r          L   i  LK b      x   L x     p b    r  K ll  f           Proof  The property follows directly from the proof of Theorem   by observing that p a e       K  K  D  Building upon similar observations  we have the fol lowing corollary  Corollary   Letp be the probability function defined by a Bayesian netwerk and let T be a junction tree for the network  Let x   p b l r  and K be as before  Suppose that the evidence e has been processed in T by an inward and subsequent outward propagation  Let  K   p K e  be the potential of clique K after the propagation  Then  p e  x     X    with              K      K     LK b              p b i r  j  f i K       r     Compute the coefficients a and     using the equa tions     and     from Theorem    for all relevant parameters  locally per clique  We would like to note that our method requires just one inward and two outward propagations to estab lish all sensitivity functions for a specific posterior marginal  whereas the methods reviewed in the pre vious section require three inward and outward prop agations per parameter  The method described above outlines the basic idea  The method  however  may be easier to implement in the alternative form based upon Theorems   and    Theorem   Let the junction tree T be as before  Also  let y   p aie  and x   p b l r  be as before and let K be a clique in T including both B and II   pa B   Now  let x  be the initially specified value for x and let x  denote an arbitrary other value for x  Suppose that  in T  an inward propagation has been performed towards a clique containing the vari able of interest A  suppose that subsequently an out ward propagation has been performed with the value a for A  Now  let x  p K a e  be the resulting clique potential for clique K  Let  y   p a e  x    L x  p  Bi r  y  p a e  x        K    p B i  r      Enter the evidence e into the junction tree and perform an inward and an outward propagation using an arbitrary root clique     Compute the coefficients  Y and    using the equa tions     and     from Corollary    for all relevant parameters  locally per clique     Perform an outward propagation from a clique containing the variable of interest A  with the ad ditional evidence A  a        where p B l r  and p  B l r  denote parameter vectors with x   x  and x   x   respectively  Then  y   ax     with       Theorem   and Corollary   provide the basis for our method for computing the coefficients in the func tions expressing the posterior marginal of interest y  p a I e  in terms of all possible parameters x  The method is composed of the following steps        K      Since both variable B and its parents are in cluded in clique K  we can obtain from the parameter vector  Proof   p Bi r      q  x           Qi   x     x   Qi l  x         Qn x       the parameter vector p  Bi r    q  x        q   x   x  q l  x       q x       where q and q  are parameters co varying according to equation      by multiplication of the potential x by p  Bl r jp Bl r   From Theorem    we have that y  ax      The expressions for a and    now follow D from simple mathematical manipulation         UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS       Let the junction tree T be as before  Also  let y   p afe  and x   p bi f r  be as before  Suppose that  in T  an inward propagation has been performed towards a clique including the variable of interest A  Then  p e  x     X     with     Compute the coefficients      from Theorem     Theorem        eta   aa and    f a   f a         where aa f a and aa f a are as in equation      ob tained from outward propagations with the evidence A   a and A        a  respectively  Proof  We begin by observing that p e  x    p a e  x    p a e  x   By entering the evidence A   a in a clique H containing the variable A and propagating outwards  we obtain the potential p K  a  e  for clique K  From this potential  K p a  e  Y   LK cPK is readily computed  as de scribed in equations     from Theorem    Similarly  by entering the evidence that A does not have the value a  that is  by multiplying the clique potential for H with a vector over dom A   in which the en try corresponding to state a is zero and all other en tries equal    and propagating outwards  we obtain the potential  p K  a  e   From this poten tial  Ya   p a e    LK  is readily computed  Using equation     from Theorem    we get y and Ya Now  using equation      we find eta  aa  f a  and f a Inserting these coefficients into the expres sion p e  x   p a  e  x    p a  e  x  yields the result D stated in the theorem              Our alternative method  building upon Theorems   and    is composed of the following steps     Enter the evidence e into the junction tree and perform an inward propagation towards a clique H containing the variable of interest A     Perform an outward propagation from H with the additional evidence A a        Compute y    and y  using the equations     and     from Theorem       Compute the coefficients a   aa and f    f a  using      for all relevant parameters  locally per clique     Retract the evidence A the evidence e      a without retracting     Perform an outward propagation from H with the additional evidence A        a     Compute Ya and Ya  using the equations     and     from Theorem     and    using equation  To allow for retracting the evidence A   a in Step   of our method without retracting e  fast retraction prop agation  Cowell   Dawid       is used in Step    Comparing the computational costs of the two al ternative methods  we note that they both require one inward and two outward propagations  Consider ing the first method  we observe that Steps   and   are equally costly  The computation of the coeffi cients a and  Y costs   Jdom K J m operations  where m   fdom pa B  J  the computation of j  and   costs  fdom K  I arithmetic operations  Thus  the addi tional cost of the first method is roughly in the or der of   to   times Jdom K f  The additional costly steps in the second method are Steps   and    both costing approximately   fdom K J operations  Thus  the additional cost of the second method is roughly   fdom K f arithmetic operations  This rough com parison of the computational costs of the two methods only addresses the number of arithmetic operations in volved  The first method  however  has a much larger overhead in terms of computing indices in performing the various summations  Thus  depending on the im plementation  the two methods might very well have comparable performance       ANALYSIS OF ALL OUTPUTS WRT  ONE PARAMETER  Having identified a parameter x to which an output probability of a Bayesian network is particularly sensi tive  one might be interested in establishing sensitivity functions for all possible posterior marginals in terms of this parameter  Such an analysis amounts to com puting the coefficients in these functions  Note that while the method described in Section   provides for evaluating the overall robustness of a Bayesian net work  the method described in this section is provides for getting insight in the spread of influence from sep arate parameters  The method of Castillo et a          and the method of Coupe   van der Gaag        can be exploited for es tablishing the coefficients in the sensitivity functions for all possible output probabilities  requiring three propagations per posterior marginal  Building upon the ideas put forward in the previous section  how ever  a more efficient method is obtained  Theorem   provides the basis for our method  Let the junction tree T be as before  Also  let y p afe  and x p bl r  be as be fore  Suppose that  in T  an inward propagation has Theorem          Compute aa and f a  using        Y      UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            been performed towards a clique containing the vari able B to which the parameter x pertains  Then  p e  x     X  J with   Y   O a   O a and J  f a   f a         where O a f a and a a f a are as in equation      ob tained from two outward propagations with two dis tinct values for x  Proof  Let K be a clique containing both the variable B and its set of parents II   pa B   Let x  and x  denote two different values of the parameter x  From the outward propagation using x  from clique K  we obtain the probability vector p A e  x      y  Ya  through marginalization of the clique potential for a clique H containing A  Similarly  from the outward propagation with x   we find the vector p  A e  x       y  Ya   From     n WAY SENSITIVITY ANALYSIS  So far  we have addressed one way sensitivity analy ses only  in which the effects of separate parameters are studied  In this section  we turn our attention to more general n way analyses in which the effects of simultaneous variation of n parameters are studied  One can regardn way sensitivity analysis as involving analyses of joint effects for all subsets of size n or less of all  say m  relevant parameters  Using the method of Castillo et al          this would involve L    l        separate analyses and I   I ri probability propa gations to compute the  n coefficients  assuming r ary variables         Provided the n parameters all belong to the same clique  Theorem   below states that we only need one propagation to compute the  n coefficients  but I    I local computations involving marginaliza tions of clique potentials            we get the result stated in the theorem      Theorem   provides the basis for our method for com puting the coefficients in the functions expressing all possible output probabilities y   p a I e  in a single pa rameter x p blrr   The method is composed of the following steps        Enter the evidence e into the junction tree and perform an inward and an outward propagation using an arbitrary root clique     Compute the probability vector p A e    y  Ya  through marginalization of the clique po tential for a clique H containing A  for all vari ables of interest     Change the value of parameter x and perform an outward propagation from a clique containing both the variable B and its parents     Compute the probability vector p  A e     y  Ya  through marginalization of the potential for clique H  for all variables of interest   In essence  Theorem   states that the mathematical expression for a probability  p e  x   of a vector of in stantiations  e  as a function of a probability parame ter  x  takes the form of a linear function of x  The orem   generalizes this statement to the case with n parameters  x          Xn  and states that the resulting function is a multilinear function in x         Xn  To simplify the exposition  we shall assume that the parameters are independent  that is  for each pair of parameters  x    p bx  lrrx   and Xj   p bx  lrrx    with the associated variables Bx   IIx   Bx   and IIx   it holds true that rrx    rrx   Bx   j  IIx   and Bx   j  IIx   To generalize the theorem to cover the case of dependent parameters is fairly straightforward  Let p be the probability function for a Bayesian network  where evidence e has been prop agated in a junction tree  T  for the network  Let X   x           Xn  be a set of parameters of the net work  where  for each i             n   Theorem       X   p bx  lrrx    with the associated variables  Bx  and IIx   being members of a clique  C  in T  Then  p e  X        Compute a a  a a  f a  and f a  using the equation     from Theorem       Compute   Y  and J  using the equation        We would like to note that our method requires just one inward and two outward propagations to estab lish all sensitivity functions for a specific probability parameter   L  Yx Z  II  zx  z     zEZ  L    Jc   C ll  r  where II   IIx      IIxn    rr   rrx      rrxn    and   Yx Z         X ZI where  fx Y        X YI  L  x Y    yz   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            If  instead of summing over subsets S  X Z  we sum over the subsets of Z and takes care that the signs of the terms are preserved  we get the desired result  D where c   p C e   W   X Y  w    wk    k     W I   by   byp byiY    b    b n        b  n w    and Px  denotes the initial value of Xi           Proof  Using the same procedure as in the proof of Theorem   we get  p e  X        F  c o     L L  X    c     c        b zt   c   X   c   P b    l x   xr     p bn   l xn   xn  b Zn  Lc b zl       b  l n   f  c p b   l x      p bn   l xJ     L  C        c   L   li  L        z  p b    ru   ur       flzEZ p bz   rz   fluEU p b i ru   C      r     p b  l ru   ur   p b u  l ru u   uiUI      uEU    II     p b   ru  p b   ru   U   p bul ru     p bu l ru       b  l u   p  u I u  uEU       II x          ISI   xES sr   u   L     Inserting      in      and rearranging terms yields p e  X      L IT L z  Z   X zEZ S   U  n     p e  X        ISI  IT x  xES  L r Z  II  Z   X  z     zEZ  J         Through one way analysis involving the parameter x we obtain the constants ax and f x in    axx   f x  The  n constants in      are related to ax and f x in the way that ax equals the sum of all coefficients  r  Z   for which xE Z  and f x equals the sum of the remaining coefficients  That is  ax   f x         c            and  where U   X  Z  u       uiU I   Now  expand ing the terms p b   ru  u   uE U  using      an easy calculation yields  II     p b u   l uiui  uiUI   Lc bz b         c  L    p e  x   The multiple sum can be grouped into sums over the subsets Z  X such that b   bz for each zE Z  p e  X   Note that  since the computation of  X  X  ranges over all subsets of X  rx Z  can be computed  for each Z C X  as a sum over a subset of the terms involved in the computation of  Yx X    The result presented in Theorem   is limited in the sense that all parameters under investigation must belong to the same clique in the junction tree  We shall now present a more general method which uti lizes the results obtained by lower order analyses  Let X n   be the parameters under investi X   x    gation and write     L L  r  Z         r  Z    J         Z   X xEZ  Z   X x lZ        n      Thus  for each one way analysis of a parameter Xi  i    we obtain equations of the form      and       In addition  we obtain the equation    p e      L  Z   X  r  Z   II Zo   J         zEZ   n  where z  denote the value specified for parameter z in the Bayesian network  This gives us a total of     equations  However  we need  at least   n equations to compute the  n coefficients  Now  if for each parameter  z E Z  we assign a new value and perform a full propagation  then we obtain an additional     equations  Thus  we can obtain at least  n equations by performing   n    l n    J  full propagations with different parameter values  in addition to the initial propagation performed for the   UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS            one way analyses  For example  to perform   way analyses  a total of two full propagations is sufficient  This result can be generalized very easily  since each m way analysis gives rise to  m equations of the form      and       Thus  to perform n way analyses  where n   m  we need at most       additional propagations  as there are     relevant m way analyses  So  for example  if we have performed   way analyses and want to perform   way analyses  no further propagations are needed     CONC LUD ING REMARKS  We have presented methods for sensitivity analysis of Bayesian networks which are significantly more ef ficient than current methods  In the case of one way analysis  the number of probability propagations of current methods grows linearly in the number of relevant parameters  whereas the methods presented above only requires one inward and one or two out ward propagations  no matter the number of relevant parameters  To substantiate the importance of this difference  we have investigated three real world networks to get an idea of the typical number of relevant parameters in a realistic scenario  All three networks are from the medical domain  a subnetwork of Munin  Andreassen et a         containing      variables  a network mod elling the pathophysiology of ventricular septal defect  Coupe et al        containing    variables  and a net work related to disorders in the oesophagus containing    variables  The investigation were conducted using real patient data involving  respectively         and   patients  The average number of relevant parameters were found to be             and      respectively   Since no censoring of parameters representing func tional relationships were performed on parameters for the Munin network  the figure       is probably some what overestimated   Efficient methods for sensitivity analysis play an im portant role in both the knowledge acquisition and the validation phases for manually constructed Bayesian network models  Coupe et al         reports on an empirical study using sensitivity analysis to focus attention on the most in fluential parameters in the knowledge acquisition pro cess  thereby considerably reducing the time required to acquire the parameter values  The validation phase involves two aspects  fine tuning and robustness analysis  The fine tuning aspect in   volves adjustment of the parameter values to make the network respond correctly to a number of test cases  A gradient descent approach is useful for that purpose  cf  neural network type training   where the gradi ent of a posterior marginal with respect to a subset of parameters can easily be computed through a minor modification of the algorithms for sensitivity analy sis  Based on the work described in the present paper  Jensen        has suggested a method for gradient de scent training of Bayesian networks  Once a network has been fine tuned  and thus responds correctly on a selection of test cases  the robustness of the network may be investigated  This involves  in essence  determining lower and upper bounds for parameter values for which the output of the network still agrees with the test cases  A parameter value close to one of the bounds indicate a possible lack of robustness  Given analytic expressions for the outputs in terms of the parameters  derived by e g  methods described in the present paper  these bounds are easily determined  The time complexity ofn way sensitivity analysis may be fairly high for large n  even with the methods pre sented in this paper  Also  our method assumes that the variables and the parents associated with then pa rameters reside in the same clique in a junction tree  The method of Coupe et al         for n way sensitiv ity analysis is based on propagation of tables of coeffi cients in a junction tree  and  therefore  has a  poten tially very much  larger space requirement  However  their method is general in the sense that it does not put any restrictions on the location of the parameters  During the initial phase of the work  the authors received valuable comments from Finn V  Jensen  Also  he suggested the basis for the general method for n way analysis  described at the end of Section    Acknowledgements  The research has been partly funded by the Danish National Centre for IT research  Project no        
  in depth presentations  see above references  We shall assume that all variables of a Bayesian network are  The efficiency of inference in both the Hugin and  most notably  the Shafer Shenoy archi tectures can be improved by exploiting the independence relations induced by the incom ing messages of a clique   That is  the mes  sage to be sent from a clique can be com  discrete  A Bayesian network consists of an independence graph   G      V E     which is an acyclic  directed graph or   more generally  a chain graph  and a probability func tion   p   which factorizes according to G  T hat is   puted via a factorization of the clique poten  Pv  tial in the form of a junction tree  In this pa per we show that by exploiting such nested     IT p v I pa v     vEV  junction trees in the computation of messages  where pa v  denotes the parents of v  i e   the set of  both space and time costs of the conventional  vertices of G from which there are directed links to  propagation methods may be reduced  The  The junction tree corresponding to G is constructed  paper presents a structured way of exploit  v    via the operations of moralization and triangulation  ing the nested junction trees technique to  such that the nodes of the junction tree correspond  achieve such reductions   to the cliques  i e   maximal complete subgraphs  of the triangulated graph  To each clique  C  and each  The usefulness of  the method is emphasized through a thor ough empirical evaluation involving ten large  separator  S   i e   link between a pair of neighbouring  real world Bayesian networks and the Hugin  cliques of the junction tree  is associated potential ta bles    c and   Js  respectively  by which  at any time   inference algorithm   we shall denote the current potentials associated with     C and S   INTRODUCTION  Now define     Jc  Inference in Bayesian networks can be formulated as   II   Jv    message passing in a junction tree corresponding to the network  Jensen  Lauritzen   Olesen Shenoy                 Shafer    More precisely  a posterior probability  distribution for a particular variable can be computed by sending messages inward from the leaves of the tree toward the clique  root  containing the variable of in terest  If a subsequent outward propagation of mes  where v  p viiV    vi   and pa v     V     v    That is  for each clique  C  is associated a subset of    the conditional probabilities specified for the Bayesian network  and the function     Jc  tree is given as  sages from the root toward the leaves is performed  all cliques will then contain the correct posterior distribu tions  at least up to a normalizing constant   In many situations  however  we are only interested in the pos terior distribution s  for one or a few variables  which makes the outward pass redundant   represents the product  over this subset  Initially the potentials of the junction    Jc   lj c  and  s        for each clique  C  and each separator  S  Propagation is based on the operation of absorption  Assume that clique C is absorbing from neighbouring  The Hugin and the Shafer Shenoy propagation meth  cliques C           Cn via separators St          Sn   ods will be reviewed briefly in the following  for more  two architectures  this is done as indicated in Table     In the   Nested Junction Trees Hugin                c  L     C  S      c  IT         s                c          Shafer Shenoy  c    i l  i  i             n                L     c    C  S   i               n  n          s                         Jc IJ      i l          n         s            i                 n  Table    Absorption in the Hugin and the Shafer Shenoy architectures    X   X   X   X     that C receives  Propagation can be defined as a sequence of inward  contains variables  message absorptions followed by a sequence of outward message absorptions  where inward means from leaf  messages   x   x   and  x  x    and that the poten tial  x  x   was initially associated with C  Themes  cliques of the junction tree towards a root clique  and  sage   outward means from the root clique towards the leaves    X  X     to be sent to Dis thus  Note that the    s are called messages  In the inward   pass  since then s     for all separators  the only difference between the two architectures is step   of the Hugin procedure  see Table      In the outward pass  on the other hand  the differ ence between the two architectures becomes more pro nounced  Consider clique C which  in the inward pass  has absorbed messages from C           Cn and sent a message to C   Now  having received a message from   L    x  x  x  x    L   x  x     x  x   x  x        X  X     X  X    However  since  X  X   does not depend on x   we can compute  x  x   as  cP x   x    cl in the outward pass  it is going to send messages     L xl x   L x   x   xJ x    X   X        to             Cn  In the two architectures  this is done as  which reduces both space and time complexities  as  indicated in Table  suming all binary variables  Eq       Note that in the Hugin architecture  when a clique C absorbs a message         Ps   it is always true that    L c   C S   This fact is exploited in the Hugin architecture to avoid performing repeated multiplications  Hence  in the outward pass of the Hugin algorithm a clique C can compute the product of all messages from its neigh bours simply by  substituting  one term of c using division  Thus  the main difference between the Hugin and the Shafer Shenoy architectures is the use of di vision  As we shall see later  avoiding division is ad vantageous when we use nested junction trees for in ference      implies a space cost  of    and a time cost  i e   number of arithmetic op erations  of         x     for the multiplications and     for the marginalization   whereas Eq     implies a space  cost of   and a time cost of     Basically  the trick in Eq    is all there is to inference in Bayesian networks  In fact  the first general infer ence methods for probabilistic networks developed by Cannings  Thompson   Skolnick        used exactly  that method  Their method is referred  to as  peel  ing   since the variables are peeled off one by one un til the desired marginal has been computed  In most inference methods for Bayesian networks  finding the peeling order  elimination order  is done  as  an off line   one off process  That is  the acyclic  directed graph of the Bayesian network is moralized and triangulated  Lauritzen   Spiegelhalter        and a secondary  The computation of messages is carried out as indi  structure referred to as a junction tree  Jensen        cated in Tables   and  is constructed once and for all  The junction tree is      namely by multiplying all   Pv   s and    s together and marginalizing from that    product  However  often   can be computed via a series of marginalizations over smaller tables   which can  greatly reduce both space and time complexities  As a small illustrative example  assume that clique C  then used as an efficient and versatile computational device  Now  since Eq      expresses nothing but inference in a  probabilistic network consisting of four variables and three probability potentials  the computation of the   Kjrulff       Hugin              cc      For j      j  Table  message       Clique   x  x     C  r  s       r  c              r  s              to  n  For j    to n   do  II  i l       j   l  j  l        n           Si                  rt  C                Jc  do  C Si  C S       r  s                  C  absorbs a message from clique  C   and sends messages to its remaining neighbours   can be formulated as inference in  a junction tree with cliques   X   X         Shafer Shenoy   X  X     X  X    and  Thus  we have a junction tree in the clique  of another junction tree   For slightly more compli  cated examples the nesting level might even be larger than two as shall be exemplified in Section      where  we describe the construction of nested junction trees  Section     describes the space and time costs associ  ated with computation in nested junction trees  and Section    briefly explains how the space and time costs  of an inward probability propagation can be computed through propagation of costs  Section     presents the  results of an empirical study of the usefulness of nested junction trees  Finally  in Section      we conclude the  work by discussing the benefits and as well as the lim itations of nested junction trees      CONSTRUCTING NESTED JUNCTION TREES  s      s      s      v                                                                                     To illustrate the process of constructing nested junc tion trees  we shall consider the situation where clique  c    is going to send a message to clique  the junction tree of a subnet  here called of the  Munin  c   in Munin    network  Andreassen  Jensen  Ander  sen  Falck  Kjrerulff  Woldbye  Srensen  Rosenfalck   Jensen           Clique  C    and its neighbours are  shown in F igure    For simplicity  the variables of C                                              are named corre  sponding to their node identifiers in the network  and they have                          and    states  respec  tively  The set of probability potentials for a Bayesian net work defines the cliques of the moral graph derived from the acyclic  directed graph associated with the network  notice that the directed graph is also de That is  each potential r  v  e g   given by a conditional probability table  induces  fined by the potentials    F igure    and  r  s   Clique  C    receives messages  from cliques c    c    and  c     s   r  s    respectively   Based on these messages and the probability potential  r  v      P   l         cl   sent to clique  a message must be generated and   Nested Junction Trees       a complete subgraph  A junction tree is then con structed through triangulation of the moral graph  Thus  in our example  the undirected graph induced by the potentials sll c Js   s   and v  may be depicted as in Figure    At first sight this graph looks quite messy  and it might be hard to be lieve that its triangulated graph will be anything but a complete graph  However  a closer exam ination reveals that the graph is already triangu lated and that its cliques are                        and                              Figure    The undirected graph induced by potentials s   c Js   and  v    down  The two potentials induce the graph shown in Figure    hence a further break down is possible as the graph is triangulated and contains two cliques   Figure    The undirected graph induced by potentials sl  s   s   and v   So  the original   clique  i e   clique containing nine variables  with a table of size           has been re duced to a junction tree with a   clique and an   clique with tables of total size          including a separator table of size       Thus encouraged we shall try to continue our clique break down  In the two clique junction tree  the  clique has associated with it only potential s   so it cannot be further broken down  The   clique  on the other hand  has got the remaining three potentials associated with it  These potentials  i e   s   s   and v   induce the graph shown in Figure    This graph also appears to be triangulated and con tains the   clique                    and the   clique                             with tables of total size         including a separator table of size       The reduced space cost is                             In this junction tree  the   clique cannot be further broken down since it contains only one potential  The   clique  however  contains two potentials  s  and v    and can therefore possibly be further broken  Figure    The undirected graph induced by potentials s  and v    Now  no further break down is possible  The result ing nested junction tree  shown in Figure    has a total space cost of         which is significantly less than the original             Carrying out the nesting to this depth  however  have a big time cost  since  for example      message passings is needed through the separator                 in order to generate the message from C   to C    A proper balance between space and time costs will most often be of interest  We shall address that issue in the next section     SPACE AND TIME COSTS  As already discussed in Section    the smallest space cost of sending a message from a clique C equals the accumulated size of the clique and separator tables of the nested junction tree s  induced by the potentials of C  i e   messages sent to C and potentials initially        Kjrulff                 t                                                           x   Figure    The nested junction tree for clique C   in  Munin     Only the connection to neighbour C   is shown  The  small figures on top of the cliques and separators indicate table sizes  assuming no nesting  The labels attached to the arrows indicate      the  time cost of sending a single message  and  compute the separator marginal one nesting level up       the  number of messages required to        Nested Junction Trees  associated with it   For example  sending a message  from clique Cb  see Figure    has a smallest space cost of                                                Note that  this smallest space cost results from choosing clique  Cc  as root  Choosing clique Cd as root instead would  make the inner most junction tree  cliques  Ce  and C    collapse to a single clique with a table of size resulting in an overall space cost of                    A  sim  case be              and    x                                respectively  The similar costs in the four level nest ing case are          and              which are       times larger  respectively  than conventional costs  A more satisfying result is ob  times smaller and the  tained by avoiding the two inner most nestings  i e   collapsing cliques Cc  Cd  Cd  and c   which happens  if instead of Ca w e let Cb be root    in which case we get  ilar analysis shows that  depending on which cliques  costs  are selected as roots  the space cost of generating the  only slightly larger than in the conventional case  but  message for clique cl  varies from        to  with a   times reduction in the space cost                     and             with the time cost being  Let us consider the time cost of sending a single mes sage from clique  Cb  to clique Ca  and assume that     PROPAGATION OF COSTS  clique Cc is selected as root  To generate a message from Cb   Cc  must receive five messages from Cd  corre  sponding to the number of states of variable     which  The calculation of the costs of performing inward prob ability propagation toward a root clique  C  can be for  is a member of the  Ca  Cb  separator but not a mem  mulated elegantly through propagation of costs in the  ber of Cc  The time cost of each message from Cd is  junction tree  Let  namely  each clique send a cost mes          as  shall be explained shortly   sage  consisting of a space cost and a time cost  being  Given this information we can now do our calculations  Generating a message involves the operations of multi plication  of the relevant potentials  and marginaliza tion  The cost of multiplying the two potentials onto the clique table is   times the table size  The cost of marginalization equals the table size  Therefore  the time cost of sending a message from clique Cb is   X               X                            Notice that the cost of marginalization equals the size of the larger of the clique table and the separator ta  the sum of the costs of sending a message  i e  a proba bility potential  and the sum of the cost messages from its remaining neighbours  Then a cost message states the cost of letting the sender be root in the subtre e containing the sender and the subtrees from which it received its messages  Thus  when C has received cost messages from all of its neighbours  the overall cost of an inward probability propagation is given by the  sum of its cost messages plus the co st of computing the C marginal potential  Now  if we perform an outward propagation of costs  C   ble  For example  whenever Ca has received a message  from  from Cb  it must basically run through the  cost of an inward probability propagation to any other  separator table    Cl  Cl    Smart indexing procedures might   we will subsequently be able to compute the  clique  just as we did for clique C   however  reduce that cost dramatically  So  our cost considerations given here are worst case      EXPERIMENTS  The time cost of         for sending a single message from  Cd  to Cc is found when selecting c  as root  se  lecting Ce as root instead would have a cost of            Clique C   must send    messages for C  to be able to generate a single message to Cc  Notice  again  that this could be done much more efficiently  since for each message variable      is fixed  effectively split  ting the inner most junction tree into two  However   To investigate the practical relevance of nested junc tion trees  the cost propagation scheme described above has been implemented as an extension to the Hugin algorithm   In order to find a proper balance  between space and time costs  the algorithm makes a junction tree representation of a clique only if  to keep the exposition as clear and general as possible   space cost     y time cost    we shall refrain from introducing smart special case procedures  Now using the same line of reasoning as above  we get the time cost of     X            X                           where each marginalization has a cost of      since the  table of C   is smaller than the  Cc  Cd  separator ta ble   is smaller than it is using conventional representation  The time factor     is chosen by the user  Cost measurements have been made on the following ten large real world networks  The KK network is an early prototype model for growing barley   The Link  network is a version of the LQT pedigree by Professor Brian Suarez extended for linkage analysis  Jensen Kong  nested  message generation would in the C   to C    agnosing lymph node diseases  Heckerman  Horvitz             The Pathfinder network is a tool for di  The space and time costs of conventional  i e   non      Kjrulff       Nathwani          The  Pignet  network is a small subnet  of a pedigree of breeding pigs  The Diabetes network is  method for inward probability propagation  as indi cated in Section    However  in the example shown in the peeling method is not able to exploit e g   a time sliced network for determining optimal insulin  Figure  dose adjustments  Andreassen  Hovorka  Benn  Olesen  the conditional independence of variable    Carson        The Munin    networks are different  ables  subnets of the        The  Water  Munin  sy stem  Andreassen et al           network is a time sliced model of the bio  logical processes of a water treatment plant  Jensen  Kjrerulff  Olesen   Pedersen          ward probability propagation is measured for each of obtained                and      given variables      of vari              and  So  the technique presented in this paper is much  more general than peeling  Note that if the triangulated version of the graph in duced by the separators of a clique is not complete  i e    The average space and time costs of performing an in these ten networks      Table   summarizes the results  All space time figures should be read  as  contains more than one clique   then one  fill in links  of  that clique are  more of the  or  redundant  that is  the  clique can be split into two or more cliques  Therefore  assuming triangulations without redundant  fill ins  the  The first pair of space time columns lists  nested junction trees technique cannot be exploited in  the costs associated with conventional junction tree  the outward pass of the Hugin algorithm  since mes  millions   propagation  The remaining three pairs of space time  sages have been received from all neighbours  includ  columns show  respectively   least possible space  ing the recipient of the message   In the Shafer Shenoy  cost with its associated time cost  the costs corre  algorithm  on the hand  there is no difference between  sponding to the highest average relative saving  and  the inward and the outward passes  which makes the  the  the least possible time cost with its associated space  nested junction trees technique well suited for that al  cost  The percentages in parentheses indicate the rela  gorithm  A detailed comparison study should be con  tive savings calculated from the exact costs  The high  ducted to establish the relative efficiency of the nested  est average relative savings were found by running the  junction trees technique in the two architectures   algorithm with various   values for each network  The optimal value      varied from        to         Table   shows that the time costs associated with min imum space costs are much larger than the time costs of conventional  inward  propagation  Thus  although maximum nesting yields minimum space cost  it is not recommended in general  since the associated time cost may be unacceptably large   Acknowledgements  I  wish to thank Steffen L  Lauritzen for suggesting the  cost propagation scheme  Claus S  Jensen for provid ing the  Pignet networks  David Beckerman Pathfinder network  Kristian G  Ole sen for providing the Munin networks  and Steen An dreassen for providing the Diabetes network  Link  and  for providing the  However  as the        columns show  a moderate increase in the space costs tremendously reduces the time costs    The example in Figure   demonstrates  the dramatic effect on the time cost as the degree of nesting is varied    In fact  the time costs of conven  tional and nested computation are roughly identical  for            while space costs are  still  significantly re  duced for most of the networks   
  The paper presents a method for reducingthe computational complexity of Bayesian net works through identification and removal of weak dependences  removal of links from the  moralized  independence graph   The re moval of a small number of links may re duce the computational complexity dramat ically  since several fill ins and moral links may be rendered superfluous by the removal  The method is described in terms of im pact on the independence graph  the junc tion tree  and the potential functions associ ated wit h these  An empirical evaluation of the method using large real world networks demonstrates the applicability ofthe method  Further  the method  which has been imple mented in Hugin  complements the approxi mation method suggested by Jensen   An dersen            INTRODUCTION  bronchitis and lung cancer  shorthand  b Jl l   and between coughing and lung cancer  c Jl l   It might  however  be quite sensible to replace c Jl l with c Jl ll  b  d   that is  conditional independence be tween coughing and lung cancer given bronchitis and dyspnoea  The independence graph of this alterna tive model could be achieved through replacement of the directed link from coughing to dyspnoea with an undirected one  whereby the chain graph of Figure  b emerges  Semantically  this implies that the relation ship between coughing and dyspnoea is non causal   Note that an independence graph equivalent to that of Figure  b might be obtained by simple reversal of the directed link from coughing to dyspnoea     a    b   Figure    Removal of the moral link between coughing and lung cancer in part  a  results in the  less demand ing  independence graph of part  b    Decision making in domains wit h inherent uncertainty using Bayesian  belief  networks and exact computa tions often involve very high dimensional probability tables  Hence  for many practical problems  exact computations are prohibitive  Therefore  approximate solutions are often the best that can be hoped for  Such solutions can be provided through simulation or model simplification  We shall address the latter  al though methods of the former type shall play an im portant role in our approach  which involves enforce ment of additional conditional independence assump tions through removal of links from the moralized in dependence graph   Specification of conditional probabilities for model  a  involves a four dimensional table for  d I b  c  l  and a two dimensional one for  c I b   whereas for model  b  it suffices to specify two three dimensional tables  one for  c  d Ib  and one for  d I b  l   If  for example  each of the four variables is described in terms of five discrete states  this meansthat model  a  requires specification of                  conditional probabilities  whereas model  b  requires  only                    To illustrate the approach  consider the following toy example  Assume that dyspnoea  shortness of breath   d  can be caused by one or more of the  diseases  coughing  c    bronchitis  b   and lung cancer  l   and further that bronchitis causes coughing  Figure la   This model suggests marginal independence between  Briefly  the method provides a systematic way of per forming model transformations as illustrated in Fig ure   such that one additional conditional indepen dence assumption is explicitly being enforced  and pos sibly some implicit ones  which follow naturally  and such that an  sub optimal balance between reduction             Usingthe suggested approximation method  model  b  can be obtained from model  a  by removal of the moral link between coughing and lung cancer    Link Removal in Bayesian Networks  of computational complexity and approximation er ror is achieved  A candidate new  explicit  assump tion takes the form a ll     C    a       where C is a clique in a junction tree corresponding to an indepen dence graph Q  such that a and f  are connected in the moral graph corresponding to Q and such that C is the unique clique containing both a and     That is  the method aims at splitting  large  cliques into smaller ones while keeping a small  distance  between the exact and the approximate distributions  This distance is computed using either exact or simulated clique potentials of a  imaginary  junction tree  where the storage requirements of simulated potentials  ob tained through Monte Carlo sampling  depends only linearly on both the clique size and the sample size  The rest of the paper is organized as follows  Section   reviews the key features of graphical chain models and junction trees necessary for the presentation  Section   presents the method  including descriptions of its im pact on the junction tree  the independence graph  and the potential functions associated with these  Please note that the results are stated without proofs  the in terested reader is referred to Kjrerulff         Section   demonstrates the applicability of the method by pre senting some results of applying it on large real world networks  Section   summarizes the features of the presented approach and argues that it complements the approach of Jensen   Andersen         For a discussion of the choice of criterion for selecting the optimal link to remove and a presentation of the implications of link removal in terms of correctness of inference  the reader is referred to Kjrerulff            GRAPHICAL CHAIN MODELS AND JUNCTION TREES  The term Bayesian networks has traditionally been used as a synonym for recursive graphical models  Wermuth   Lauritzen       for which the indepen dence structure is encoded by directed acyclic graphs  In the present paper we shall  however  use  Bayesian networks  as a synonym for the more general class of models denoted graphical chain models  Lauritzen   Wermuth       Lauritzen   Wermuth       for which the independence structure is encoded by chain graphs  Notice that the class of graphical chain models also contains the subclass of graphical models  Dar roch  Lauritzen   Speed       with independence structure encoded by undirected graphs       CHAIN GRAPHS  In the following the notion of chain graphs shall be reviewed briefly and fairly informally  For a more thorough treatment of the subject see e g  Frydenberg          Let Q  V  E   Ed U Eu  be a graph with nodes  vertices  V and links  edges  E  V x V  where Ed              a      E E I      o    E  is the subset of directed links and Eu   a      E E I      a  E E  the subset of undirected links  If there is a link between o  and     denoted a           they are said to be connected  A directed link between a and f  is denoted a    t f  or a       and an undirected link is denoted a       We shall use a        etc  to denote either  a and f  are connected  or  the link between a and     depending on the context     A path  a a         O k     from o  to    in Q is an ordered sequence of distinct nodes such that ai   O i   for each i            k      The path is undirected if            k      The path is directed if a     for each i either a    or ai    t ai l for each i             k     and the path includes at least one directed link  A cycle is a path  r     o    a         ak       with the exception that o                 For A  B  C  V  C is said to separate A from B if for all paths  a   a     ak        where a E A and f  E B   at       ak  n C      A graph Q is connected if there is a path between each pair of nodes of Q  Unless otherwise stated  connectivity shall henceforth be assumed         Now  Q is a chain graph if it contains no directed cycles  If Q is a chain graph  then   K         Kn  are called the chain components of Q if  K         Kn  is the set of connected components of  V  Eu   There are two important special classes of chain graphs  If n   lVI  i e   one node per chain component   Q is called a directed acyclic graph  DAG   If n      Q is called an undirected graph  A subset A  V induces a subgraph QA    A  EA  of Q  where EA   En  A x A    Note that any subgraph of a chain graph is a chain graph   A graph is complete if all nodes are pairwise connected  A subset A c     V is complete if it induces a complete subgraph  and if A is maximal  i e   there is no complete subset B c     V such that A C B   then it is called a clique  The parents of A V is the subset pa A   V   A such that for each f  E pa A  there is an a E A for which f   t a  The set of children of A  denoted ch A   is defined analogously  The neighbours of A is the subset nb A   V  A such that for each f  E nb A  there is an a E A for which a     The ancestral set of A c     V is the subset An  A   V such that for each f  E An  A  either f  E A or there is a directed or undirected path from f  to at least one o  E A  The moral graph gm of a chain graph Q is obtained by first adding undirected links between each pair of unconnected nodes in pa K  for each chain component K  and then replacing all directed links by undirected ones  An undirected graph Q    V  E  is triangulated  also  decomposable or chordal  if each cycle of length greater than   has a chord  i e   a link between two non consecutive nodes of the cycle               Kj  erulff       GRAPHICAL CHAIN MODELS  For a chain graph Q    V  E  we consider a collection of discrete random variables  Xa aEV taking values in probability spaces Sp Xa   For brevity we shall interchangeably refer to a E V as both a node and a variable  Thus we shall write e g  a instead of Xa  For a subset A s    V we let Sp A    XaEASp a   i e   the Cartesian product of the state spaces of the variables in A   A probability function p   pv is said to factorize ac cording to a chain graph Q    V  E  if there exist non negative functions  A defined on Sp A  such that p ex  II  A        AEA  where A is the set of cliques of g   The functions  A shall be called component potentials of p  For Q being a DAG this simplifies to p   IJ      p v I pa v    vEV A similar factorization exists in the general case  Let namely p K I pa K         II   I  L IT   AEAK   A  K AEAK   A       where K is a chain component of Q and AK    A AI As    K U pa K  A n K          Then P   II p KI pa K     KEIC  E       where JC is the set of chain components of Q  If p factorizes according to Q  then Q is said to be an in dependence graph of p  and p is a graphical chain model  a probability function of a Bayesian network with Q as underlying graph    The phrase  p is Markov with respect to Q  is a synonym for  p factorizes according to Q    In the special case of Q    V  E  being a DAG all conditional independence statements captured by Q can be found using the d separation criterion of Pearl        or the equivalent criterion of Lauritzen  Dawid  Larsen   Leimer         But in the general case the Markov properties  i e   conditional independence properties  captured by g are expressed by the follow ing theorem  Frydenberg        Theorem    Let p factorize according to a chain graph  Q    V  E   Then A ll BI C with respect to p for any subsets A  B  C  V whenever C separates A from B in   Q An AuBuo  m   Note that the formulation of this theorem  describing the global chain Markov property  is identical to the theorem of Lauritzen et al         describing the di rected global Markov property for recursive graphical models  i e   where Q is a DAG    JUNCTION TREES  By exploiting the conditional independence relations among the variables of a Bayesian network  the under lying joint probability space may be decomposed into a set of subspaces corresponding to a decomposable  hy pergraph  cover of the moralized graph such that exact inference can be performed by simple message passing in a maximal spanning tree of the cover  Lauritzen   Spiegelhalter       Jensen       Jensen  Lauritzen   Olesen        Technically  a decomposable cover of a Bayesian network with underlying chain graph   is created by triangulating gm  i e   adding undirected links  so called fill ins  to gm to make it triangulated   That is  the set of cliques of the triangulated graph is a decomposable cover of the network  Jensen        has shown that any maximal spanning tree of a decomposable cover  C  can be used as the basis for a simple inward outward message passing scheme for propagation of evidence  belief updating  in Bayesian networks  where maximality is defined in terms of the sum of cardinalities of the intersections between adjacent nodes  cliques  of the tree  Jensen named these trees junction trees  The intersections be tween neighbouring cliques of a junction tree are called separators  Jensen et al         We shall henceforth refer to a junction tree by the pair  C  S  of cliques and separators  It can be shown that for each path  C C       Ck D  in a junction tree  c n D s    ci for all        i      k  implying that A ll B I s for each S E S  where A and Bare the sets of variables of the two subtrees  except S  induced by the removal of the link corresponding to S  Jensen                    To each clique and each separator is associated a belief potential   A  The joint probability distribution  Pv  of a Bayesian network with a junction tree  C  S  is proportional to the joint  system  belief v given by  IToEC   c       ITSES  S A belief potential tPA is normalized if  EA tPA      If all belief potentials of a junction tree are normalized  then  v is normalized  i e   Pv    v   ex  Pv  A junction tree Y      v       C  S  is said to be consistent if  L  Pc L  Pn ex    D  D C  for all C D  E  C   i e   the marginal potentials for C n D with respect to  c and  Pv are proportional   Consistency of T shall interchangeably be referred to as consistency of its associated joint belief   v     ENFORCING INDEPENDENCE ASSUMPTIONS  The computational complexity imposed by a particu lar junction tree  C  S  is roughly determined by the   Link Removal in Bayesian Networks       clique  C E C  with the largest state space  Thus by splitting C into smaller cliques a significant reduction of the computational complexity might be obtained  If   o        C such that there is no other clique in C containing  o        then adding o  ll     C     o       to the set of independence statements amounts to split C    o    which ting C into Ca    C        and Cf  might or might not become new cliques of the modified junction tree  see the examples of Figure      c  enforce dl  a  e   remove red   ll  fill in     o                 o      IX       o             c    b    a   I  I  Figure    Removal of o        a   both  o   f  and         become new cliques   b           become a new clique  but   o       does not   c   neither  o   nor      become new cliques  The requirement that C must be the only clique con taining  o       ensures that o  ll     C  o   B  or  equiv alently  that the graph obtained by removing o     in the triangulated graph corresponding to  C  S  is tri angulated  see Kjrerulff        for details       AN EXAMPLE  To understand the main issues of the proposed ap proximation method we shall present a small example  Consider the sample chain graph of Figure  a with corresponding moral graph of Figure  b  solid links   The dashed link is a fill in added to make the graph triangulated  The junction tree corresponding to the triangulated graph of Figure  b is shown in Figure  a  a b  c d   a b  e d      a    c    b   Figure     a  Junction tree corresponding to Figure  b   b  Removal of c  d causes clique  a  c  d  e  to dis appear   c  The fill in a   c is rendered redundant  splitting clique  a  b  c  e  into two smaller ones  induce several new independence statements  c ll d  d ll e  a ll dIe  etc   which do not follow as natural consequences of c ll d I  a  e   The set of independence statements displayed by each chain graph of Figure   is a subset of I  I U   c ll d I  a  e    This follows from the fact that the three moral graphs are identical to the moral graph of Figure  b with c d removed  Thus  each graph of Figure   is a correct representation of I   but none of the graphs are perfect representa tions  since they fail to represent e g  the statements d ll e I c and a ll e I b     a  w    d   a      d   b    c   Figure    Competing independence graphs obtained by adding c Jl d I  a  e    Figure     a  Sample independence graph   b  Corre sponding moral graph  solid links  and a triangulated graph  all links    Notice that the moral graph corresponding to the graphs of Figure   is triangulated  This eliminates the need for the fill in between a and c  allowing clique  a  b  c  e  to be split into the two smaller cliques  a  b  e  and  b  c  e   Figure  c   In general  a possi bly large number of fill ins and moral links might be rendered redundant by the removal of a single link  If  for example  b   e is removed in Figure  a  the moral link b e disappears   Reduction of the computational complexity of the junction tree could be accomplished by extending the set of conditional independence statements displayed by the tree  Adding e g  the statement c ll d I  a  e   i e   removal of c  d from the triangulated graph  causes clique  a  c  d  e  to split into the sets  a  d  e  and  a  c  e  neither of which appear to be cliques of the reduced graph  Figure  b    Enforcement of the conditional independence state ment c ll d I  a  e  thus provided a reduction of com plexity in terms of sizes of cliques from three   cliques  i e   cliques of four variables  to one   clique and two   cliques  This corresponds to at least a     reduction of space requirements  binary variables  even though the resulting independence graph s  at first glance seems more  complicated    J   a   f   b   Since we wish to add just one statement to the set I of independence statements displayed by the original independence graph of Figure  a  the revised indepen dence graph is  in general  not obtained through sim ple link removal  Removal of c d in Figure  a would       OUTLINE OF METHOD  The above example provided insight into some of the issues related to the approximation method  Before        Kjrulff  presenting the technicalities of the method  let us sum marize the underlying philosophy and list the issues to be dealt with in more detail      Js   s  That is  the potentials of the  possible  new cliques are C  a  and cPC  f   and the potentials of the cliques in  W hen attempts to compile a Bayesian network into a junction tree fails on account of excessive memory re quirements  the problems are often caused by a small number of cliques  The proposed method is based on the idea of splitting these cliques into smaller ones  i e   extending the set of independence statements   There fore  the first step is to create a junction tree with exact or simulated clique potentials   Although exact clique potentials can be created  there might still be a wish to reduce the space requirements if this can be done with out attaining an unacceptable level of imprecision   Clique potentials  whether exact or simulated  must be provided such that the deviation between these  cor rect  potentials and the approximate ones can be com puted  These measures of deviation  or distance  must then be used as the basis of a criterion for selecting the link to be removed  Simulated clique potentials can be provided through various kinds of Monte Carlo simulation like Gibbs sampling and  forward sampling  which have complex ities proportional to the moral graph  We shall not discuss this issue any further  even though there are some interesting points concerning optimal choice of simulation method  especially when the underlying in dependence graph is not a DAG        Let  C    C   remain unaltered   JUNCTION TREE  cl  I      I  ck  be the neighbours of c in a junc  tion tree T    C  S  and S          S c the associated separators  where C is the unique clique containing  a  f    As demonstrated in F igure    the removal of the link between a and f  produces two  one  or zero new cliques  That is   a  both Get   C    f   and Cf    C    a  are cliques in the revised junction tree T    b  Co   Cf   is a clique in T  and c    G     is not  or  c  neither Co nor C   are cliques in T   It is easy to see that T  is constructed from T as indicated in Figure    where the dashed parts illustrate the cliques  separators  and links to be added toT  with C and its incident links removed  and the dotted parts the sep arators and links to be removed  see Kj Erulff          for details  Note that in all three cases we have S   C   meaning that S separates T  into two subtrees   a  f   T     C S  and T    Ck Sk   where A and B are the corresponding sets of variables such that a E A and t  E B  From the discussion in Section     it follows trivially that  A Bayesian network with underlying probability model  p may be exhaustively described in terms of four com  ponents      a potential representation of p based on component potentials  cf  Equation           an inde pendence graph     of p      a junction tree  decom posable hypergraph cover of gm   and     a poten tial representation of p based on belief potentials  cf   Equation       Notice that it suffices to include one of the potential representations for an exhaustive de scription of a Bayesian network  We shall  however  include them both as a matter of convenience  We shall now detail the impacts on these four compo nents when removing a     from the moral graph       Let T    C  S  be a junction tree  C E C the unique clique containing  a       and a consistent joint belief for T  Let further  L  c     Pc L    L   f  c  with respect to     J        Since  L  Pc   L I Jc   r  a Jl     C    a        r   a       and Cis the unique clique containing  a  t   it follows that for each separator  S  between C and its neigh bours in T either S  C    a  or S  C        implying   Bus  Therefore      cPAuScPBuS cPs        The reduction       o   f    of the computational complex ity achieved by the removal of a f  can be expressed as  a  IICII CliO   II  IIC  II   II SII     b  IICII  IIColl  II SII  II Skll  or  c  IICII II SII liSt II II Scll  where II II   ISp  l  cf  Figure    This can be expressed   compactly  as     where   r         lall     l       all     IIS II           lo  II Stll       lfj II Skll  IICII      Y  E   a        if  C r  is a clique and     otherwise  Note that  IISII             IICII II SII   II S  II   II Scll  where    reaches its lower bound when llall              and  o          and its upper bound when  lex          Y E              O  o       BELIEF POTENTIALS     Jc    and similarly  If Bu S                  INDEPENDENCE GRAPH  Since  If A    If A us Aus  the independence relations among the variables of the set A remain unaltered by    the removal of o       where A  B  S    and  If  are given in Section      The same applies to B  That is  the marginal independence graphs for A  B and S are   Link Removal in Bayesian Networks        b    a   Figure    Removal of the link between a and j  results in a junction tree with a new separator S    C    a      separating the tree into a subtree containing a but not j  and a subtree containing j  but not a  In parts  b  and  c  we assume  without loss of generality  that Ca C Ct  part  c  only  and C   C Ck  i e   Ca S  and cf    sk     identical for  and   J  Therefore  the problem of de termining the independence graph of  ljJ may be formu lated as the problem of combining marginal indepen dence graphs such that the independence statements expressed by these are not violated and such that the combined graph represents the fact that A Jl B I S  or A   S Jl B   S I S to be exact   Given an independence graph of a probability function  belief potential   p   pv  the following theorem pro vides a way of establishing an independence graph of any marginal PA  A  V   E   Let the chain graph Q    V  be an independence graph of p   pv and a E V  Then is an independence graph gt  a      V   a    Theorem    Eh a    ofPV  a    LaPv  where Qt  a  is constructed from  g by rendering nb a  complete by adding undirected  links if necessary  adding        for each    E pa a  and   E nb a  U ch  a   unless          adding        for each    E nb a  and   E ch a    unless          rendering ch a  complete in such a way that no directed paths are introduced  and removing a and the links incident t o it   In proving Theorem    it is profitable to note that correctness of Q     t  a  follows if separation of A and B by C in  Qn AuBuC  m implies separation in  fhn AuBuC  m as well  and that perfectness of  t  a  follows if separation in   An AuBuc  m implies separa tion in  Qn AuBuC  m provided Q is perfect  It should be noticed that perfectness of Q does not  gt  a    necessarily imply perfectness of The following example illustrates this point  Let V    a               c   and let the DAG of Figure  a be an independence graph of p  Since j       I    c   with respect to p  and PV   a    j  and   must be connected in an inde pendence graph of PV  a    Lap  and  since   Jl c  and      e I     a candidate independence graph of PV  a  could be the one of Figure  b  However  since   Jl e IS w ith respect top  and PV        this graph is not perfect  but it is correct  since it does not repre sent non existing independence statements  Thus  all  marginalize w r t   a   a     b   Figure      Jl e    with respect to top  and LaP  which is Markov with respect to the DAG in part  a   However  La p is not Markov with respect to the graph in part  b   since according to that     e     the independence properties of PV  a  cannot be rep resented by a single chain graph  If we want a perfect representation  a more sophisticated language must be adopted  One such language may be given by the class of annotated graphs  Geva   Paz        However  in the present paper we shall refrain from pursuing this any further  Theorem   provides a method for constructing an in dependence graph of the marginal distribution PV   a  However  the construction of an independence graph of the approximate joint belief  ljJ   AusB   s involves combination of a marginal independence graph and a conditional  marginal  independence graph   The in dependence graph of the conditional distribution Pv   a is obtained simply by moralizing the subgraph induced by An a  and removing a and the links incident to it  Let the chain graph Q    V  E  be an in dependence graph of p   Pv and let be the links of t An A  m  Then Q     V  E U EA      A   is a chain graph and a conditional independence graph of Pv   A   Theorem    EAn A   By the methods of Theorem   and Theorem   we can construct any marginal independence graph  possibly conditional on a set of variables  by successive removal of the relevant variables  Note that the presence of the set A and the links incident to A in the independence graph of Pv   A is unnecessary for a correct interpretation of the condi tional independence relations among variables in V  A   For brevity we shall refer to an independence graph of a marginal distribution as a marginal independence graph   and similarly for the conditional case         Kjrerulff  given A  However  when combining a conditional and a marginal independence graph to obtain a joint in dependence graph  A and some links connecting A to V   A are needed  In fact  when constructing a condi tional independence graph we shall proceed as follows  Let p and    be given as in Theorem    The graph obtained by  i  removing all links between nodes in S and  ii  making all links between S and nb  S  undirected is a conditional independence graph of Pv I A  a b  Ei      E         Returning to the example in Section      we iden tify the sets A    a  b  c e   B    a d e      and S    a e   Following the above results we deter mine a marginal independence graph and a conditional one  and then combine these into a new joint inde pendence graph  This combination can involve one of three principally different sets of marginal and con ditional graphs      marginal graph for AU S plus conditional one for B IS      marginal graph forB US plus conditional one for A IS  or     conditional graphs for A I S and B I S plus marginal one for S  reflecting the factorizations   J   rPAuSrPB   s    J   rPA   srPBus  and   J A   sB   st Js  respectively  The relevant marginal and conditional graphs are a  e for S and the ones of Figure    Forming the independence graph of   J through graph union  we find the three possi ble solutions displayed in Figure  a c corresponding  respectively  to combination alternatives         with the modifications that a d  solutions  a  and  c   and a b  solutions  b  and  c   have been replaced with a   d and a  b to avoid directed cycles   Note that these modifications do not alter the represented independence statements   Since we shall prefer so lutions representing the largest sets of independence statements  there is a clear preference order among the three alternatives  solution  a  is preferable to so lution  b  which is preferable to solution  c     a  AUS a  Theorem   below states that a joint independence graph can be formed by simple graph union of a con ditional and a marginal independence graph  Let the chain graph  ius  A  be a marginal independence graph of PAus and the chain graph  t        B  a conditional indepen dence graph of pB   s complying with Corollary    where AUBUS   V such that AnB  Sand All B IS with respect to p pv  If       ius u  t     is not a chain graph  i e   it contains directed cycle s    replace links  Y   with       where   E Sand   E nb S  n B until    becomes a chain graph  Then    is an independence graph of p  Further     is perfect if both  ius and         are perfect   c  d    f  Corollary    Theorem         e    BUS d  r   a b    c      e  f  BIS  AIS  Figure    Marginal and conditional independence graphs of the graph of Figure  a with A    a  b  c e   B    a  d  e     and S    a  e        COMPONENT POTENTIALS  Given a joint belief    J  and a chain graph     obtained through enforcement of one or more conditional inde pendence assumptions  we wish to determine an asso ciated set of component potentials  Furthermore  we have available a set of belief potentials associated with a junction tree corresponding to    Notice that if   J and   are produced as described in Sections            J is guaranteed to factorize according to    That is  there exist component potentials A such that   J ex nA A  cf  Equation       Following Equation     the problem can be divided into n subproblems  where n is the number of chain components of    More specifically  since  ljJ   n   J K I pa K    we must determine potentials A for each chain component K such that     A similar analysis can be performed for the dyspnoea example in the Introduction  Again there appears to be a clear preference order among the solutions  with the optimal solution displayed in Figure  b    cf  Equation       where K    K U pa K  and AK is the set of cliques in   K    m containing at least one node in K  Notice that  since belief potentials are available    J K  can be computed  The potentials A can be found via Mobius inversion when   JK  is positive  see e g  Lauritzen   Wermuth         Unfortunately  this is rarely the case  How ever  it seems plausible that an extended version of the Mobius inversion exists when   JK  is known to factorize according to    Lauritzen   Wermuth        has shown that for any decomposable graphical chain model there exists an equivalent recursive model  that is  if   K   m is tri angulated for each K  Thus  if   is decompos able  we may generate the equivalent DAG and com pute conditional probabilities  component potentials    J  v I pa v    cf  Equation       If   is not decompos able  we may triangulate each subgraph   K    m by   Link Removal in Bayesian Networks  inserting fill ins and then generate a DAG      from the resulting graph  In the latter case the resulting recursive model will be suboptimal in two ways  First     fails to represent all the independence statements represented by    Second  the computational complex ity imposed by the optimal triangulation of     m is at least as large as the computational complexity im posed by the optimal triangulation of gm  since the triangulation of each   K    m  constrains  the triangu lation     EXPERIMENTS  Since  from a theoretical point of view  not much can said about the practical importance of link removal  we shall now report on some results of an empirical study conducted on a number of real world networks  The networks are Pathfinder  Heckerman  Horvitz   Nathwani        including     nodes  for diagnosing lymph node pathology  two subnetworks of MUNIN  Andreassen  Woldbye  Falck   Andersen        in cluding about     nodes each  for diagnosing disor ders in the peripheral nervous system  and a time sliced network model of the biological processes of a water treatment plant including    process variables  Jensen  Kjcerulff  Olesen   Pedersen        The criterion applied for selecting a link a       to be re moved from a clique C is based on the reduction of the total state space and the  distance  between the exact  c   and the approximate    Jc   clique potentials  The distance  D c   Jc    is measured as the conditional mutual information between a and    given C    a       also  the Kullback Leibler divergence between c and   Jc  given as I   a  JI C    a  J     D c   l Jc      Elog c Nc    with expectation taken with respect to   and where    Jc      A useful relationship     when c between the absolute divergence and the Kullback Leibler divergence  see e g  Kullback         states that     IA     JAI  S jD c  I Jc   I     for any A  C  In the experiments links with lower mutual informa tion were preferred  and savings  reduction of state space  were used only to break ties  Further  links were removed until a total divergence of at most       was reached  the total divergence after a series of re movals equals the sum of the individual divergences  Kjcerulff         Using Inequality     the theoreti cally upper bound on the absolute error is found to be     Table   displays the results  The  size  of a net work equals the sum of the sizes of the state spaces of the cliques after sensible triangulation  For all networks except MUNIN  the Kullback Leibler diver gences were computed using exact clique potentials        For the MUNIN  network simulated potentials based on         iterations of forward sampling were used  Links removed Reduction Network  size  Pathfinder                     MUNIN                          MUNIN                            Water                         Table    Empirical results of applying link removal to real world networks  The savings obtained for the Pathfinder and the MUNINl networks are relatively modest  whereas sig nificant savings are obtained for the MUNIN  and the Water networks  The reduction from       M to     M for the MUNIN  network makes it possible to perform exact computations using the junction tree methodol ogy  The large savings for the MUNIN  and the Water networks are due partly to the fact that a number of the orphan nodes are instantiated to their  normal  state     DISCUSSION  An important feature of a clique potential approxi mation is the attenuation of its impact with increas ing distance from the target clique  Kjcerulff        This feature is especially important in connection with time sliced Bayesian networks  An additional property of the method  is the property of errors remaining lo calized in absence of posterior evidence and  under cer tain conditions  even in presence of posterior evidence  Kjcerulff        The presented approximation method has been com pared with the method suggested by Jensen   Ander sen         Briefly  their method is based on anni hilation of small probabilities by setting the k small est probabilities to zero for each clique potential of a junction tree  where k is chosen such that the sum of the k smallest probabilities is less than a predeter mined threshold  After annihilation  the belief tables are compressed in order to take advantage of the in troduced zeros  The comparison  reported in Kjcerulff         demon strates that link removal in some cases is significantly better than annihilation  In other cases  however  a comparison turns out to the disadvantage of link re moval  Intuitively  this seems absolutely reasonable  since a model including links representing weak de pendences will be almost equivalent to a model which lacks these links  but it might be quite different from a model obtained by uniformly removing a correspond ing amount of probability mass from the belief tables  On the other hand  link removal is unsuited in cases where there are no  weak links   Thus  to approxi mate a given network using these two methods  link removal should be tried first and when all  weak links  have been removed  annihilation should take over         Kja rulff  Application of link removal does not require the con struction of exact clique potentials  as opposed to an nihilation   Further  the creation of simulated clique potentials  through e g  forward sampling  and possi ble subsequent link removal provides a way of estab lishing an annihilated and compressed junction tree representation of a network without first creating ex act potentials  Inequality     is essential  since the key indicator asso ciated with an approximation is most often the max imum absolute error  However  under arrival of pos terior evidence  the inequality can only be used as a rough guideline  Thus  among directions for future re search  an important one is assessment of a good upper bound on the error given evidence  Acknowledgements  I am indebted to Steffen L  Lauritzen for providing many valuable comments on earlier drafts  The re search has been funded partly by the Danish Research Councils through the PIFT programme  
