  without employing extensive terminology  thus mak ing the algorithms accessible to researchers working  This paper describes a class of probabilistic approximation algorithms based on bucket elimination which offer adjustable levels of accuracy and efficiency  We analyze the ap proximation for several tasks  finding the most probable explanation  belief updat  in diverse areas   More important  their uniformity  facilitates the transfer of ideas  applications  and solutions between disciplines   Indeed  all bucket  elimination algorithms are similar enough to make any improvement to a single algorithm applicable to all others in its class   ing and finding the maximum a posteriori  Normally  the input to a bucket elimination algo  hypothesis   rithm consists of a knowledge base theory specified  We identify regions of com  pleteness and provide preliminary empiri  by a collection of functions or relations   e g   clauses  cal evaluation on randomly generated net  for propositional satisfiability  constraints  or condi  works   tional probability matrices for belief networks    In  its first step  the algorithm partitions the functions     into buckets  each associated with a single variable   Overview  Given a variable ordering  the bucket of a partic ular variable contains the functions defined on that  Bucket elimination  is a unifying algorithmic frame  variable  provided the function is not defined on vari  work that generalizes dynamic programming to en  ables higher in the ordering  Next  buckets are pro  able many complex problem solving and reasoning activities   Among the algorithms that can be ac  commodated within this framework are directional resolution for propositional satisfiability  adaptive consistency for constraint satisfaction  Fourier and  cessed from top to bottom  When the bucket of vari able  X  in its bucket  The result is a new function defined over all the variables mentioned in the bucket  ex  Gaussian elimination for linear equalities and in  cluding  equalities  and dynamic programming for combina  of  torial optimization        Many algorithms for proba  bilistic inference  such as belief updating  finding the  is processed  an elimination procedure or an  inference procedure is performed over the functions  X  X   This function summarizes the  effect   on the remainder of the problem   The new  function is placed in a lower bucket  For illustration we include algorithm elim mpe  a bucket elimination  most probable explanation  finding the maximum a  algorithm for computing the maximum probable ex  posteriori hypothesis  and calculating the maximum  planation in a belief network  expected utility  also can be expressed as bucket elimination algorithms        The main virtues of this framework are simplic ity and generality   By simplicity  we mean that  complete specification of these algorithms is feasible Thls work was partially supported by NSF grant IRI         and by Air Force Office of Scientific Re    Figure          An important property of variable elimination algo rithms is that their performance can be predicted using a graph parameter called induced width    also called        tree width l      which is the largest clus  ter in an optimal tree embedding of the graph  In general  a given theory and its query can be asso ciated with an interaction graph describing various  search grant  AFOSR         Rockwell International  dependencies between variables  The complexity of  and Amada of America   bucket elimination algorithms is time and space  ex         A scheme for approximating probabilistic inference ponential in the induced width of the problem s in teraction graph   Depending on the variable order  ing  the size of the induced width will vary and this leads to different performance guarantees  When a problem has a large induced width bucket elimination is unsuitable because of its extensive memory demand  Approximation algorithms should be attempted instead  We present here a collection of parameterized approximation algorithms for prob abilistic inference that approximate bucket elimina tion with varying degrees of accuracy and efficiency  In a companion paper        we presented a similar ap  proach for dynamic programming algorithms  solv ing combinatorial optimization problems  and belief updating  Here we focus on two tasks  finding the most probable explanation and finding the maxi mum a posteriori hypothesis  We also show under what conditions the approximations are complete and provide preliminary empirical evaluation of the algorithms on randomly generated networks  After some preliminaries   section       we develop the  approximation scheme for the most probable expla nation task      and for  section        section        for belief updating  section  the maximum a posteriori hypothesis We summarize the results of our em  pirical evaluation in section      Related work and  conclusions are presented in section         Preliminaries  Definition      belief networks   Let X    X        Xn  be a set of random variables over multivalued domains D        Dn  A belief net work  BN  is a pair  G  P  where G is a directed acyclic graph and P    P    P     P X Ipa X     are the conditional probability matrices associated with X   An a  signment  X     llt       Xn     ln   can be  abbreviated to  z       ct         cn  The BN represents a probability distribution P zt           en     n lP  z  l z pa x     where  Zs i l the projection of z over a sub  et S  if u is a tuple over a  ubset X  then us denotes that assignment  which is restricted to the variables in S n X  An evidence set e is an instantiated subset of variables  We use  us   cp  to denote the tuple us appended by a value  z P of Xp  where Xp is not in S  We define i      c      z   and i     z   z i l       Zj   Definition      elimination functions  Given a function h defined over subset of variables S  where X E S  the functions  minxh    ma cxh    meanxh   and CL x h  are defined over U u  For every U S    X  as follows   mi h u z    ma z xh  u   minxh  u  I     h u   z    and h u  z     L x h  u   meanxh  u    L     hij   where lXI i  the car dinality of X  s domain  Given a  et of functions ht      h  defined over the   ubset              S   the prod uct function  Tij hi  and L    h  are defined over U   UjSj  For every U   u   TI h   u    TI hi us   and  L   hj  u    Lj hj us    m  Definition      graph concepts   A directed graph is a pair  G  A poly tree is an acyclic directed graph whose under lying undirected graph  ignoring the arrows  has no loops  The moral graph of a directed graph G is the undirected graph obtained by connecting the parents of all the node   in G and then removing the arrows       V E   where  V    X         Xn  is a set of elements and E     X  Xi IX  Xj E V   is the set of edges  If  X  Xj  E E  we say that X  points to Xi  For each variable X   pa X   or pa   is the   et of variable   pointing to X  in G   while the    et of child node   of X   denoted ch X    compri  e l the variables that X  point   to  The family of X   F   includes X  and its  child variable    A directed graph is acyclic if it ha   no directed cycle    An ordered graph i l a pair  G  d  where G i   an undirected graph and d   X        Xn is an ordering of the nodes  The width of a node in an ordered graph i l the number of the node    neig hbors that precede it in the ordering  The width of an or dering d  denoted w d   is the maximum width over all nodes  The induced width of an ordered graph  w    d   is the width of the induced ordered graph ob tained by processing the node l from last to first  when node X is p rocessed  all its neighbor   that precede it in the ordering are connected  The induced width of a graph  W   is the minimal induced width over all it l ordering lj it is al lo known as the tree width       Definition        probabilistic tasks   The mo  t probable explanation  mpe  task is to find an anignment  z        z              Z  n  such that p  z      ma x z   Ilf   P  c   el z p     The belief assessment task of X    z  is to find bel  z     P X    z le    Given a set of hypothe  ize d variables A    At       A    A  X  the maximum a posteriori hypothesis  map  task is to find an as ignment a     a         a c   such that p a     ma x     L    x A Ilf      P  z  l cpa  e        Approximating the mpe  Figure  mpe         shows bucket elimination algorithm  for computing mpe   elim  Given a variable or  dering and a partitioning of the conditional proba bilities into their respective buckets  the algorithm        Dechter and Rish  Algorithm elim mpe Input  A belief network BN    P        Pn   an or dering of the variables  d  observations e  Output  The most probable assignment     Initialize  Partition BN into buckeh       bucketn  where bucket  contains all matrices whose highest vari able is X   Put each observed variable into its appro priate bucket  Let S          S  be the subset of variables in the processed bucket on which matrices  new or old  are defined     Backward  For p    n downto    do for h  h       h  in bucket   do   bucket with observed variable  If bucket  contains X     c   BIIBign X    a   to each h  and put each resulting function into its appropriate bucket     Else  generate the functions h   h    ma cx ll     l h  and  r     argm azx  h   Add h  to the bucket of the  UL   largest index variable in U   S    X       Forward  Assign values in the ordering d using the recorded functions  c  in each bucket   Figure    Algorithm  bottom  When processing the bucket of Xp  a new function is generated by taking the maximum rel ative to Xp  over the product of functions in that bucket  The resulting function is placed in the ap propriate lower bucket  The complexity of the al gorithm which is determined by the complexity of       is time and space  exponential in the number of variables in the bucket  namely the bucket s variable induced width  and is  therefore  time and space exponential in the induced width  w  of the network s moral graph        Since the complexity of processing a bucket is tied to the arity of the functions being recorded  we pro pose to approximate these functions by a collection of smaller arity functions  Let  ht      h   be the func  tions in the bucket of Xp  and let St        Sj be the variable subsets on which those functions are d fined  When  elim mpe processes the bucket of Xp  it hP  hJ    mazx  II     One  computes the function  brute force approximation method involves generat ing  instead  by migrating the maximization oper ator inside the multiplication  a new function  gP     rr  lmazx  hs   Since each function  product of hJ  is replaced by  uct defining  gP  hJ   gP   mazx  hs  hs  gP   in the  in the prod  We see that  gP has  a  product form in which the maximizing elimination operator  gP s  mazx   hs  This idea can be generalized to yield a collection of parameterized approximation algorithms having varying degrees of accuracy and efficiency  Instead of applying the elimination operator  i e   multiplica tion and maximization  to each singleton function in a bucket as suggested in our brute force approxima tion above  it can be applied to a more coerced par titioning of the buckets into mini buckets  Let   Qt      Q     Ql     be a partitioning into mini buckets of  ht        hi in Xp s bucket  The mini Q  contains the functions h         hlr Algo rithm elim mpe computes hP   l index the mini buckets  hP   rnazx  IIf t   mazx  II   II  h    the functions  bucket  By migrating the maximization operator into each mini bucket  we get         II    mazx  II  h     As  the partitionings are more coerced  both the com plexity and the accuracy of the algorithm increase   elim mpe  starts processing buckets successively from top to  processing each bucket  step  mpe   is applied separately to each of  component functions  The resulting functions  will never have dimensionality higher than     and  each of these functions is moved  separately  to a  Definition     Partitioning Q  i  a refinement of Q  iff for every  et A E Q  there ea i U a  et B E Q  such that A C B  Proposition     I   in the bucket of Xp  Ql i  a refinement of Q   then hP        Algorithm  approz mpe i m  is described  in Figure      It is parameterized by two indexes that control the partitionings   Let H be a collection of function  ht       hi defined on subseu of variables            Si  A partitioning of H u canonical if any function whose argumenu are sub umed by another function belong  to a bucket with one of those  ubsuming function   A partitioning Q into mini buckeu u an  i  m  partitioning iff    it u canonical     at most m nomubsumed function  participate in each mini bucket     the total number of variables in a mini bucket doe  not ezceed i  and    the partition ing is refinement maximal  namely  there is no other   i m  partitioning that it refines  Definition      If indez i is at least as large a  a family size  then there ea i t an  i  m  partitioning of each bucket     Proposition      Algorithm approz mpe i m  com putes an upper bound to the mpe in time O m ezp  i   and space O m e z p i    where i  n and m  i  Theorem          as m  lower bucket  When the algorithm reaches the first  Clearly  in general   variable  it has computed an upper bound on the  accurate approximations   and  i  increase we get more        A scheme for approximating probabilistic inference  Algorithm approx rnpe i rn  Input  A belief network BN     Pt        Pn i  ordering of the variables  d  Output  An upper bound on the most probable as  signment  given evidence e     Initialize  Partition into buclcet        bucketn  where bucket  contains all matrices whose highest vari  able is X   Let St        S  be the subset of variables in bucketp on which matrices old or new are defined              For p f  n downto    do lfbucketp contains Xp   Zp  assign Xp     Backward   h  and put each in appropriate bucket   el s e  for h   h         h  in buclcetp  do  Generate an   Q        Qr    Zp  to each   i  m  mini bucket partitioning  Q   For each  do   Qr  E  Q     containing hr       hr           Generate function h   h   mazxPII   hr   Add h to the bucket of the largest index variable in Ur f   ULl s      Xp       Forward  For i     to n do  given Zt      z  p   choose a value   p of Xp that maximizes the product of all the functions in Xp s bucket   Figure      algorithm  In maznP H E  F  h  H  G   and  o on  bucket B  obtain the mpe value mazBP B   hc B   and then can generate the mpe tuple while going for ward  If we proceu by approx mpe   n l   in tead  we get  we denote by  Y the function  computed by appro z   elim n     that differ from tho e generated by elim mpe   bucket      P IIH  G  bucleet H    P H E  F   h  H G  bucket  G    P G E   D   fH G  bucket F    P F B    YH  E F   bucket E    P E C B  y E   yG  E D  bucleet D    P D C   fE D  bucket C    P C   yE C B   YD C  bucket B    P B   yc B   fF  B   Algorithm  elim mpe and approx mpe   n l   fir t dif fer in their processing of bucket  G   There  in tead of recording a function on three variable   hH E  F  G   iu t like elim mpe  appro z   mpe n l  record  two function   one on G alone and one on E and F  Once approz mpe n l  ha  proceued all bucket   we can generate a tuple in a greedy fa h ion a  in elim mpe  we choo e the value of B tiWJt marimize  the product of function  in B  s bucket  then a value of C marimizing the product function  in bucket C   and so on    and an  appro c mpe i m   There is no guarantee on the quality of the tuple we generate  Nevertheless  we can bound the error     A belief network P i h g e d c b   P i h  g  P  h e   f  P gje d P  e  c  b P d c P b P c   Figure  of  appro z  mpe  by evaluating the probability of the  generated tuple against the derived upper bound  since the tuple generated provides a lower bound on the mpe   Example     Con ider the network in Figure    A uume we we the ordering  B  C  D  E F  G H I  to which we apply both algorithm elim mpe and it   imple t approrimation where m     and i   n  Ini tially the bucket of each variable will have at mod one conditional probability  bucket      P I  H G   bucleet H    P H  E  F   bucket  G    P G E D   bucleet F    P F  B     bucket E    P E C B   bucket D    P D   C   bucket C    P C    bucleet B    P B   Proce  ing the buckeb from top to bottom by elim mpe generate  functiom that we denote by h function   buclcet I    P I H  G  bucket H    P H  E  F   h  H G  bucleet G    P GIE D  hH E F G  bucket F    P F B  hG E  F  D  bucket E    P E C  B   h F E B  D  bucket D    P D C  hE C B D  bucket C    P C  hn C  B  bucleet B    P B  hc B  Where h  H G    mazrP I H G   hH E F G     Alternatively  we can use the recorded bound in each bucket as heuristics in subsequent search  the functions computed by  approz mpe i  m   Since are al  ways upper bounds of the exact quantities  they can be viewed  as  over estimating heuristic func  tions in a maximization problem  We can associate  ip      z          l p t   f i  l     g h  z     rrr   P zi Zpo   and h ip       with each partial assignment an evaluation function    g ip      IT ebuclcetp  h   It is easy to see that the evaluation function f provides an upper bound on the mpe re stricted to the assignment i    Consequently  we where  can conduct a best first search using this heuristic evaluation function   From the theory of best first  search we know that       when the algorithm ter  minates with a complete assignment  it has found an optimal solution        the sequence of evaluation  functions of expanded nodes are non increasing  as       the heuristic function becomes more accurate   fewer nodes will be expanded  and       if we use the        Dechter  and Rish  full bucket elimination algorithm  best first search will become a greedy and complete algorithm for the mpe task             Cases of completeness  Clearly  approz mpe n  n  is identical to elim mpe because a full bucket is always a refinement maximal  n  n  partitioning  There are additional cases for i and m where the two algorithms coincide  and in such cases approz mpe i  m  is complete  One case is when the ordering d used by the algorithm has induced width less than i  Formally  Theorem       Algorithm appro z  mpe i  n  i  com plete for ordered network  having w  d   i   Another interesting case is when m      Algo rithm approz mpe n     under some minor modifica tions and if applied to a poly tree along some legal orderings coincides with Pearl s poly tree algorithm       A legal ordering of a poly tree is one in which observed variables appear last in the ordering and otherwise  each child node appears before its par ents  and all the parents of the same family are con secutive  Algorithm approz mpe n     will solve the mpe task on poly trees with a legal variable ordering in time and space O ezp IFI    where IFI is the cardinality of the maximum family size  In other words  it is complete for poly trees and  like Pearl s algorithm  it is tractable  Note  however  that Pearl s algorithm records only unary functions on a single variable  while ours records intermediate results whose arity is at most the size of the fam ily  To restrict space needs  we modify elim mpe and approz mpe i  m  as follows  Whenever the al gorithm reaches a set of consecutive buckets from the same family  all such buckets are combined into one  uper bucket indexed by a ll the constituent buckets  variables  In summary   in variable X   namely  to compute P  z     e    L z      II P  z i  e  z pa    When processing each bucket  we multiply all the bucket s matrices  At           i  defined over subsets S         S   and then eliminate the bucket s variable by summation      In     we presented the mini bucket approximation scheme for belief updating  For completeness  we summarize this scheme next  Let Ql    Q     Q    be a partitioning into mini buckets of the func tions                in Xp  s bucket  Algorithm elim bel com putes    P   l index the mini buckets     P   Lx  II l    i   l x  III   lit         Separating the processing of one mini bucket  call it first  from the rest  we get   P L x   ITt         W  ziTt   t    and migrating the summation into each mini bucket yields      rrr l Lx  rr     l   This  however  amounts to computing an unnecessarily bad upper bound on P because the product IIt    l  for i     is bounded by  L x  II        Instead of bounding a function of X by  ts sum over X  we can bound by its maximizing function  which yields      L x    Iltl  h  rrr  ma z x  III AI    Clearly  for ev ery partitioning Q     P    yq  In summary  an upper bound gP of    P can be obtamed by processing one of Xp  s mini buckets by summation  and then process ing the rest of Xp s mini buckets by maximization  In addition to approximating by an upper bound  we can approximate by a lower bound by applying the min operator to each mini bucket or by computing a mean value approximation using the mean value operator in each mini bucket  Algorithm  appro c bel maz i  m    that uses the maximizing elimination operator is described in      In analogy to the mpe task  we can conclude that  approz bel ma c i m  has time complexity O m  ezp  i    is complete when      w  d   i  and      when m     and i   n  if given a poly tree            Prop o s ition     Algorithm approz mpe n I  with        The bucket elimination algorithm for computing the map  elim map  presented in     is a combination of elim mpe and elim bel  some of the variables are eliminated by summation  others by maximization  Consequently  its mini bucket approximation is com posed of approz mpe i m  and appro c bel ma c i m    the  uper bucket modification  applied along a legal ordering  i  complete for poly tree  and i  identical to Pearl   poly tree algo l ithm for mpe  The modi fied algorithm   complexity i  time ezponential in the family  ize  but it require  only linear  pace  D Approximating belief updating  The algorithm for belief assessment  elim bel  is iden tical to elim mpe with one change  it uses sum mation rather than maximization  Given some evidence e  the probl em is to assess the belief  Approximating the map  Given a belief network BN    Pt           P  J  a  sub set of hypothesis variables A       A         Ar    and some evidence e  the problem is to find an assign ment to the hypothesized variable that maximizes   A scheme for approximating probabilistic inference       their probability  Formally  we wish to compute  m axP aole     max L rr lP  z   el z pa    P e  a           l k l  when  z     a         ao   Z Jo l        Z n    Algorithm elim map  the bucket elimination algorithm for map  as sumes only orderings in which the hypothesized vari ables appear first  The algorithm has a backward and a forward phase  but its forward phase is only The ap relative to the hypothesized variables  plication of the mini bucket scheme to elim map is a straightforward extension of approz mpe i m  and approz bel maz i m   We partition each bucket into mini buckets as before  If the bucket s variable is a summation variable  we apply the rule we have in appro c bel maz i m   that is  one mini bucket is approximated by summation and the rest by maxi mization  When the algorithm reaches the buckets with hypothesized variables  their processing is iden tical to that of approz mpe i m   Algorithm approz map i m  is described in Figure     Theorem     Algorithm approz map i  m  com pute   an upper bound of the map  in time    e cp m  e cp  i    and  pace O e cp m ezp i     Algorithm approx map i  n  i   complete when w    d   S i  and algorithm approx map n     u complete for poly tree   D   Consider a belief network appropriate for decoding a multiple turbo code  that has M code fragments  see Figure    which is taken from Figure   in       In this example  the Ufs are the information bits  the X  s are the code fragments  and the Yi s and Y   s are the output of the channel  The task is to assess the most likely values for the U s given the observed Y s  Here  the X s are summation variables  while the U s are maximization variables  After the ob servation s buckets are processed   lower case char acters denoted observed variables  we process the first three buckets by summation and the rest by maximization using appro c map n      we get that all mini buckets are full buckets due to subsurnp tion  The resulting buckets are  bucket X     P  y IX    P  X I Ut  U z   U   U   bucket X   P  y z IX    P  X I Ut  U   U   U   f x   Ut  U z   U   U   bucket X   P  y IX    P  X IU   U   U      x  Ull U   U   U   bucket Ut    P  Ut    P y   Ut   f x  U   U   U   U   bucket U z     P  U    P y   IU       u   U    U   U   u bucket U     P  U    P  y  IU          U   U   u bucket U     P  U    P y IU    f   U     Therefore  approz map n     coincides with elim map for this network   Algorithm approx map i m  Input  A belief network BN   Pt       P     a sub  Ao   an ordering of the    set of variables  A      At     variables  d  in which the A s are first in the ordering   evidence  e   Output   An upper bound maximum a posteriori hy  pothesis  A   a     Initialize  Partition BN into  bucket      bucket     where bucket  contains all matrices whose highest vari able is X       Backward  For p  f  n downto    do in bucketp  do for all the matrices f  t   f Jl     bucket with observed variable  if bucketp contains the observation Xp   zp  then assign Xp   Xp to each     and put each resulting function into its appropriate bucket             A  for            in bucketp  do    i  m  partitiorung q of the matrices fl into miru buckets Q         Qr     processing first bucket  For Q  first in q containing   else  if Xp is not in  generate an  f t    f  t    do       II           Add   J    x     f to the bucket of the largest index variable in   generate function  L   U   Xp   U  l St  I  f r    do  For ea Qr   l     in Q contairung f      J  functions the Generate  Xp  Sr  Ur  U  l   nf    J     Add maxx   index variable in U     else  if       to the bucket of the largest  Xp E A  for f t f l       an  Qt       Qr      j in bucketp  do   i  m   miru bucket partitiorung  generate  For each  do generate function  Q     J     J   E      q  contairung  f J     maxx  m    Jr        s       Xp   orward     q         Jr    Add  to the bucket of the largest index variable in  U       U   Assign values  in the ordering d           At        A o using the information recorded in each bucket   Figure    Algorithm approz map maz i m   Figure    Belief network for decoding multiple turbo codes          Dechter and Rish  Experimental evaluation  diagnosis purposes  we also recorded the maximum family size in the input network  F    and the max  Our preliminary empirical evaluation is focused on the trade off between accuracy and efficiency of the approximation algorithms for the mpe task  We wish      to understand  the sensitivity of the approxima  tions to the parameters  i  and m       The effective  ness of the approximations on sparse networks vs dense networks  and on uniform probability tables vs   structured ones  e g   noisy ORs   and      the  extent to which a practitioner can tailor the approx imation level to his own application   approz approz  mpe m   as  We focused on two extreme schemes of  mpe  i  m    the first one  called  i and varying m  while the sec approz  mpe i   assumes unbounded i   sumes unbounded ond one  called m and varying Given  the  values  of  i  and  m   many   i  m   partitionings are feasible  and preferring a particu lar one may have a significant impact on the quality of the result   Instead of trying to optimize parti  tioning  we settled on a simple strategy   We first  created a canonical partitioning in which subsumed functions are combined into mini buckets   appro rJ mpe m   Then   combines each m successive mini  buckets into one mini bucket  while  approz mpe i   generates an i  partitioning by processing the canoni cal mini bucket list sequentially  merging the current mini bucket with a previous one provided that the resulting number of variables in the resulting mini bucket does not exceed  i   imum arity of the recorded functions   F   We also  report the maximum number of mini buckets that occurred in any bucket during processing        mb    Results  We report on four sets of uniform random networks  we had experimented with more sets and observed similar behavior        having  a   set of      nodes and  instances having           hundred instances       edges  set  nodes and              a  a set of  edges  set      instances having     nodes and     edges    and a set of     instances having     nodes     edges  set     The first and the forth sets  set of  set and  represent dense networks while the second and the third represent sparse networks  For noisy OR net         edges  set   has    instances and uses one evidence  set   has     instances and uses three evidence nodes and set   has     instances and uses works we experimented with three sets having  nodes and  ten evidence nodes          Uniform random networks  On the relatively small networks  sets we applied  elim mpe and     and      compared its performance  with the approximations  The results on these two        sets appear in Tables  Table    reports averages  i  Rather than  where the first column depicts m or  displaying the mpe  the lower bound  and the upper bound  often  these values are very small  of order  The algorithms were evaluated on belief networks        generated randomly  The random acyclic graph gen  accuracy of the approximation  Thus  the second  n   column displays Ml   the ratio between the value of  erator  takes as an input the number of nodes  and the number of edges  ates  e  e   and randomly gener  directed edges  ensuring no cycles  no parallel  and less   we report ratios which capture the  an mpe tuple   Ma z     Upper  and Ma z   and the fourth col time ratio  TR between the CPU running times for elim mpe and appro rJ mpe m  or approx  mpe i   The next column gives the CPU time  T    of appro rJ mpe m  or approz  mpe i   Fi nally  F   F  and mb  are reported   edges  and no self loops  Once the graph is available   upper bound  for each node  z    a conditional probability function  umn contains the  P zl z        is generated  For uniform random net works the tables were created by selecting a random number between   and   for each combination of val ues of  z   and  z        and then normalizing  For ran dom noisy  OR networks the conditional probability functions were generated as noisy OR gates by se  lecting a random probability q c for each  inhibitor   Algorithm  approz mpe i m   computes  an  upper  and the lower bound  Lower    the third column shows the U IM ratio between the  Table     gives an alternative summary for the same  appro mpe m  only  Three UIM ratios  vs  the Time Ratio  are  two sets  focusing on statistics MIL   reported  For each bound and for each m  we display        bound and a lower bound on the mpe  The latter is  the percent of instances  out of total  provided by the probability of the generated tuple   the corresponding ratio  MI  for the lower bound   For each problem instance  we computed the mpe by  U IM for the upper bound  belongs to the interval  elim mpe    E     E  where the threshold value  E  changes from  the upper bound and the lower bound by  the approximation  either  approx mpe m  or approx  mpe i     and the running time ofthe algorithms   For    to     on which  We also display the corresponding mean T R   For example  from Table   s first few lines we see   A scheme for approximating probabilistic inference  that       instances out of the     were solved by appro tJ mpe m    with accuracy factor of   or less      achieved this accuracy with m      The speed up over m    instances was     while the speed up for m    was       Table    elim mpe vs  appro tJ mpe i m  on     in stances of random networks with    nodes     edges  and with    nodes     edges  Lntance  ppNe mpt m form        Mean value on      lim mp v  m  MJL              t O                   a      MfL                                  I II      UJM  I I                        a      elim m pe  i  I II  I      V   UfM  I  TR  I  I I     nod               t eo nod                 I       T  for  ma   mb  max                               I I                 t    II        max P  n n n     n     ma   P                                                                                             I  I I                     U      node      ed e                                                                 max P         aluel per node  T  node   AO edae                          I I  ao da                 dsu                CpprOit fi I J      TR  I  Yalue per node          From these runs we observe a considerable efficiency gain      orders of magnitude  relative to elim mpe for     of the probelm instances for which the ac curacy factor obtained was bounded by    We also observe that  as expected  sparser networks require lower levels of approximations than those required by dense networks  in order to get similar levels of accuracy  In particular  the performance of appro tJ mpe i a  gave a     orders of magnitude perfor mance speedup while accompanied with an accuracy factor bounde by    to    percent of the instances on dense networks  and to    percent of the sparse net works  From table   we also observe that controlling the approximation by i provides a better handle on accuracy vs efficiency tradeoff  Finally  we observe that approz mpe m l  can be quite bad for arbi trary networks  We experimented next with larger networks  sets   and     on which running the complete elimina tion algorithm was sometimes computationally pro hibitive  The results are reported in Tables   and    Since we did not run the complete algorithm on those networks  we report the ratio U L  We see that the approximation is still effective  a factor of accu racy bounded by    achieved very effectively  for sparse networks  set     However  on set    appro tJ        Table    Summary of the results  M L  U  M and TR statistics for the algorithm approz   mpe m  with m           on random networks    l J  Random network w th  l               i  oO   l            r       l                   t  oO      l                l r M                    l r                        t       oO                                                                              tU o                                                                       Random notworko W th eo m              Yo                                           ooy  o  o      U IIY       edc  Upper bound UfM Mean TR      o                                        i           nodoo     da  Lower bound M    uTR  MfL           nodes   Lower bound Moan TK MfL                 J  L  m                    H                                                o o o o                                            o o  Upper bound M   uTR  UfM o    oYo  Yo                 U     yj  Yo oYo OYo  o o o o                      I e    t                       mpe m  was too expensive to run for m         and too inaccurate for m         For this difficult class  an acceptable accuracy was not obtained         Noisy OR networks  We experimented with several sets of random noisy OR networks and we report on three sets with    variables and     edges  The results are summa rized in Figure   and Table    In the first  we display all instances of set   plating the accuracy  M L and U  M  vs T R  for all    instances  In the second we display the results on sets   and   in a manner similar to Table    T    gives the time of elim mpe  The results for the noisy OR networks are much more impressive than for the uniform random net works  The approximation algorithms often get a correct mpe while still accompanied by     orders of magnitue of speed up  see cases when i      and i        Although the mean values of U  M and M   can be large on average due to rare instances  see Figure     in many of the cases both ratios are close or equal to    In summary  for random uniform and noisy OR net works     we observe that very efficient approxi mation algorithms can obtain good accuracy for a considerable number of instances     appro tJ mpe i  allows a more gradual control of the accuracy vs  ef    Dechter and Rish       Table      Summary of the results  M L  U  M and  TR statistics for the algorithm appr ox  mpe   i  with  i                      l                        oo                                                                    J                    r                            oo                  J      oo   on random networks  Random net    orkl Wlth    nodelt eo edge Lower bound I Upper bound  MfJ    Mean                            i                                                        n n                                                                           u s ith     Random network  Lower bound  I  MfJ    o s   o               ro                  i     IT       UfM                 i       S    i         u s  I I                                                                                                    U         T        t                         U          o  a                                  mb  max  Pi       T                     T               Accuracy vs efficiency                    c MIL  UIM                                 H  O  ficiency relative to the complete elimination by one  max  MIL and U M vs TR  I                     U                                           approx mpe m      on random appro c mpe i  obtains a good ap M L        while still improving ef      lntance  UfL                mean          noisy OR networks  TR                  Time Ratio versus M L and U  M bounds appro c mpe m  with i      on noisy OR ne towrks with    nodes      edges  and one evidence Figure  for  node  or two orders of magnitude      Mean vlllne on      R  ficiency tradeoff than proximation    on  Upp er bound              U                                                                    ao                   au     u s             Men  edgea  node   SID  M ean                                               UfM   R     elim mpe vs  approz  mpe i  for i               instances of random networks with     nodes and     edges  Table  z         elimination framework  both the parameterized al  Conclusions and related work  The paper describes a collection of parameterized algorithms that approximate bucket elimination al gorithms   gorithms and their approximations will apply uni formly across many areas  We presented and ana  Due  to the  generality of the bucket   lyzed the approximation algorithms in the context of several probabilistic tasks   We identified regions  of completeness and provided preliminary empirical evaluations on randomly generated networks  Our empirical evaluations have interesting negative     elim mpe vs  appro c mpe i  m  on     in stances of random networks with     nodes and     Table  edges  Mean vala m  on     ltan aea  mj   tdim m pe v  e ppr  rt   mp UfL  T  mn mb          i            t                         T          T  S                     l T              o s     H ll  I  u             elim  m pa v   ppPoc mpe i UfL                max mb      l   l    max                      max                      and positive results   On the negative side  we see  that when the approximation algorithm coincides with Pearl s poly tree propagation algorithm  i e   when we use  approz mpe m l      it can produce ar  bitrarily bad results  which contrasts recent suc cesses with Pearl s poly tree algorithm when ap plied to examples coming from coding problems            On the positive side  we see that on many prob  lem instances the approximations can be quite good  As theory dictates  we observe substantial improve ments in approximation quality as we increase the parameters   m  or  i    This allows the user to an  alyze in advance  based on memory considerations and given the problem s graph  what would be the best  m and i he can effort to use   In addition  the ac    A scheme for approxim ating probabilistic inference  Table    Summary of the results  M    U  M and TR statistics for the appro t  mpe i  on noisy OR net works with    nodes      edges   ranae                                                                      i                                 i                                                    ran e                            i   i                                 i                                            oo                               oo   I  i      I    I  I                 n                            upper and lower bounds approximations can be de rived for sigmoid belief networks  Specifically  each Sigmoid function in a bucket  is approximated by a Gaussian function   e   tdenee node   l of O problem  ndnce   M IL  Lower bound                                                                                                                     Yo     Y                                                      rR                                                                                                                                                                                                   n t                 u r  endence node        Lower bound          MfL                                    Jio                                                          Jio                                                                  U                                                                        r    UfM  Upper bound                                                                   u sy                                                        rR                                                                                                                         T l                      i                n                  u  s        u s                                 
 We consider the problem of diagnosing faults in a system represented by a Bayesian network  where diagnosis corresponds to recovering the most likely state of unobserved nodes given the outcomes of tests  observed nodes   Finding an optimal subset of tests in this setting is intractable in general  We show that it is difficult even to compute the next most informative test using greedy test selection  as it involves several entropy terms whose exact computation is intractable  We propose an approximate approach that utilizes the loopy belief propagation infrastructure to simultaneously compute approximations of marginal and conditional entropies on multiple subsets of nodes  We apply our method to fault diagnosis in computer networks  and show the algorithm to be very effective on realistic Internet like topologies  We also provide theoretical justification for the greedy test selection approach  along with some performance guarantees     Introduction The problem of fault diagnosis appears in many places under various guises  Examples include medical diagnosis  computer system troubleshooting  decoding messages sent through a noisy channel  etc  In recent years  diagnosis has often been formulated as an inference problem on a Bayesian network  with the goal of assigning most likely states to unobserved nodes based on outcome of test nodes  An important issue in diagnosis is the trade off between the cost of performing tests and the achieved accuracy of diagnosis  It is often too expensive or even impossible to perform all tests  In this paper  we concentrate on the problem of active diagnosis  in which tests are selected sequentially to minimize the cost of testing  We use entropy as  Alina Beygelzimer IBM T J  Watson Research Center    Skyline Drive Hawthorne  NY       beygel us ibm com  the cost function and select a set of tests providing maximum information  or minimum conditional entropy  about the unknown variables  However  exact computation of conditional entropies in a general Bayesian network can be intractable  While much existing research has addressed the problem of efficient and accurate probabilistic inference  other probabilistic quantities  such as conditional entropy and information gain  have not received nearly as much attention  There is a vast amount of literature on value of information and mostinformative test selection                 but none of the previous work appears to focus on the computational complexity of most informative test selection in a general Bayesian network setting  We propose an approximation algorithm for computing marginal conditional entropy  The algorithm is based on loopy belief propagation  a successful approximate inference method  We illustrate the algorithm at work in the setting of fault diagnosis for distributed computer networks  and demonstrate promising empirical results  We also apply existing theoretical results on the optimality of certain greedy algorithms to our test selection problem  and analyze the effect of approximation error on the expected cost of active diagnosis  Our method is general enough to apply to other applications of Bayesian networks that require the computation of information gain and conditional entropies of subsets of nodes  In our application  it can efficiently compute the information gain for all candidate tests simultaneously  The paper is structured as follows  Section   introduces necessary background and definitions  In section    we describe the general problem of active diagnosis and the computational complexity issue thereof  We propose a solution to this problem in section    Section   discusses an application of our approach in the context of distributed computer system diagnosis  while section   presents empirical results  We survey related work in section    and conclude in section        Background and Definitions  process could diverge  convergence is guaranteed only for polytrees   Let X    X    X            XN   denote a set of N discrete random variables and x a possible realization of X  A Bayesian network is a directed acyclic graph  DAG  G with nodes corresponding to X     X            XN and edges representing direct dependencies       The dependencies are quantified by associating each node X i with a local conditional probability distribution P  xi   pai    where pai is an assignment to the parents of X i  nodes pointing to X i in the Bayesian network   The set of nodes  x i   pai   is called a family  The joint probability distribution function  PDF  over X is given as product  Let a denote a factor node and i one of its variable nodes  Let N  a  represent the neighbors of a  i e   the set of variable nodes connected to that factor  Let N  i  denote the neighbors of i  i e   the set of factor nodes to which variable node i belongs  The BP message from node i to factor a is defined as  see  e g           mci  xi        nia  xi       P  x     N    P  xi   pai          i    We use E  X to denote a possibly empty set of evidence nodes for which observation is available  For ease of presentation  we will also use the terminology of factor graphs      which unifies directed and undirected graphical representations of joint PDFs  A factor graph is an undirected bipartite graph that contains factor nodes  usually shown as squares  and variable nodes  shown as circles    See Fig    for an example   There is an edge between a variable node and a factor node if and only if the variable participates in the potential function of the corresponding factor  The joint distribution is assumed to be written in a factored form P  x         fa  xa    Z a       where Z is a normalization constant called the partition function  and the index a ranges over all factors f a  xa    defined on the corresponding subsets X a of X  The computation complexity of many probabilistic inference problems can be related to graphical properties  Exact inference algorithms require time and space exponential in the treewidth      of the graph  which is defined to be the size of the largest clique induced by inference  and can be as large as the size of the graph  Many common probabilistic inference problems are NP complete      This includes our problem of probabilistic diagnosis  which can be formulated as a Maximum A Posteriori  MAP  probability problem  given a set of observations  find the most likely states of unobserved variables  Although probabilistic inference can be intractable in general  there exists a simple linear time approximate inference algorithm known as belief propagation  BP         BP is provably correct on polytrees  i e  Bayesian networks with no undirected cycles   and can be used as an approximation on general networks  In belief propagation  probabilistic messages are iterated between the nodes  The  cN  i  a  and the message from factor a to node i is defined as     fa  xa   nja  xj    mai  xi      xa  xi       jN  a  i  Based on these messages  we can compute the beliefs for each node and the probability potential for each factor    mai  xi        bi  xi    aN  i   ba  xa    fa  xa       nia  xi          iN  a   Observations are incorporated into the process via functions as local potentials for the evidence nodes  In that case  bi  xi   becomes the approximation of the posterior probability P  x i   e      The Active Test Selection Problem In many diagnosis problems  the user has an opportunity to actively select tests in order to improve the accuracy of diagnosis  For example  in medical diagnosis  doctors face the experiment design problem of choosing which medical tests to perform next  Let S    S    S            SN   denote a set of unobserved random variables we wish to diagnose  and let T    T    T            TM   denote the available set of tests  Our objective is to maximize diagnostic quality while minimizing the cost of testing  The diagnostic quality of a subset of tests T can be measured by the amount of uncertainty about S that remains after observing T    From the information theoretic perspective  a natural measurement of uncertainty is the conditional entropy H S   T     Clearly  H S   T   H S   T   for all T  T  Thus the problem is to find T   T which minimizes both H S   T   and the cost of testing  When all tests have equal cost  this is equivalent to minimizing the number of tests  This problem is known to be NP hard       A simple greedy approximation is to choose the next test to be T    arg minT H S   T  T     where T  is the currently selected   test set  The expected number of tests produced by the greedy strategy is known to be within a O log N   factor from optimal  see Appendix   The same result holds for approximations  within a constant multiplicative factor  to the greedy approach  Furthermore  our empirical results show that the approach works well in practice  We make a distinction between off line test selection and online test selection  In online selection  previous test outcomes are available when selecting the next test  Off line test selection attempts to plan a suite of tests before any observations have been made  We will focus on the online approach  sometimes called active diagnosis  which is typically much more efficient in practice than its off line counterpart       Active Test Selection Problem  Given the observed outcome t  of previously selected sequence of tests T     select the next test to be arg minT H S   T  t     In a Bayesian network  the joint entropy H X  can be decomposed into sum of entropies over the families and thus can be easily computed using the input potential functions  Conditional marginal entropies  on the other hand  do not generally have this property  Under certain independence conditions they decompose into functions over the families  But computing those functions will require inference   See Appendix for proofs   Lemma    Given a Bayesian network representing a joint PDF P  X   the joint entropy H X  can be decomposed into the sum of entropies over the families  H X     N i   H Xi   Pai    Lemma    Given a Bayesian network representing a joint PDF P  S  T   where i   paTi  S  i e  tests Ti and Tj are independent given a subset of S   the observation t   of previously selected test set  and a candidate test T   the conditional marginal entropy H S   T  t     can be written as   H S   T  t       P  spaT   t   t    log P  t   spaT   t spaT        P  t   t    log P  t   t      const       t  where const is a constant expression     BP for Entropy Approximation Let us consider the problem of computing the conditional marginal entropy   P  xa   e  log P  xa   e       H Xa   e     xa     where P  xa   e    x xa P  x   e   x xa representing variable nodes not in x a   The trick is to replace the marginal posterior P  xa   e  with its factorized BP approximation  and make use of the BP message passing mechanism to perform the summation over x a   We call this process Belief Propagation for Entropy Approximation  BPEA   Pick any node X   from Xa and designate it as the root node  We modify the final message passed to X   as follows    m a   x            ba  xa   log ba  xa    xa  x   Here  ba  xa   is the unnormalized belief of X a  i e     ba  xa     ba  xa    where    xa ba  xa     Plugging in ba  xa   in place of P  x a   e  in Eqn     we see that it only remains to sum over the root node X   and normalize properly    h Xa   e     m a   x          x   h Xa   e       h Xa   e    log           It follows immediately that BPEA is exact whenever BP is exact  The normalization constant  is already computed during normal BP iterations  The computation of ba     m ai   and h   can all be piggy backed onto the same BP infrastructure  and therefore does not impact its overall complexity  Furthermore  due to the local and parallel message update procedure in BP  we can compute the marginal posterior entropies of multiple families in one single sweep  This is an important advantage for the active probing setup   Minimizing conditional entropy is a particular instance of value of information  VOI  analysis      where tests are selected to minimize the expected value of a certain cost function c s  t  t     The result of Lemma   can be generalized to this case if the cost function is decomposable over the families  See Lemma   in the Appendix for details   It is also easy to show that the approach is extendible beyond the entropy computation  to an arbitrary cost function decomposable over families  see Lemma   in the Appendix   The cost function replaces the negative logarithm in Eqns      and       Since observations of test outcome correlate the parent nodes  the exact computation of all the posterior probabilities in Eqn      is intractable  We can certainly use an existing approximation method to compute P  s paT   t   t    and P  t   t     But a more efficient approach is possible if we exploit the belief propagation infrastructure     Application  Fault Diagnosis in Computer Networks Suppose we wish to monitor a system of networked computers  Let S represent the binary state of N network elements  Si     indicates that the element is in normal   is the cross entropy between the posterior probability of T and its parents  and the conditional probability of T given its parents  The second term in Eqn      is simply the negative conditional entropy H T   t        S   S   S   T   T     SM    TN  Figure    Factor graph of the fault diagnostic Bayes net  operation mode  and S i     indicates that the element is faulty  We can take S i to be any system component whose state can be measured using a suite of tests  If the system is large  it is often impossible to test each individual component directly  A common solution is to test a subset of components with a single test probe  If all the test components are okay  the test would return a    Otherwise the test would return    but it does not reveal which components are faulty   We deal with the two entropy terms separately  For H T   t     we may use approximation methods such as BP or GBP to calculate the belief b t   t     which can then be used to directly compute H T   t       Note that the summation over values of T is simple since T is binary valued   To calculate A T  SpaT   t     we use the entropy approximation method BPEA  as described in Section    Because BP message updates are done locally  we can compute A T  SpaT   t    for all unobserved T nodes during a single application of BP  Thus  picking the next probe requires only one run of the BPEA approximation algorithm  For each candidate probe  we designate the probe node T itself as the root node  The unnormalized belief has the form   bt  t  spaT      P  t   spaT   njt  sj         jpaT  We assume there are machines designated as probe stations  which are instrumented to send out probes to test the response of the network elements represented by S  Let T denote the available set of probes  A probe can be as simple as a ping request  which detects network availability  A more sophisticated probe might be an e mail message or a webpage access request  In the absence of noise a probe is a disjunctive test  it fails if an only if there is at least one failed node on its path  More generally  it is a noisyOR test       The joint PDF of all tests and network nodes forms the well known QMR DT model        This is used to calculate the modified message m  at  t   cf  Eqn        However  since A T  S paT   t    is a cross entropy term  we do not take the log of b  but rather take the logarithm of the known probabilities P  t   s paT    This simplifies the normalization step described in Eqn       to A T  SpaT   t      A T  SpaT   t      where      t spa  T   bt  t  spaT     P  sj      j  sj     j    sj       s P  ti       spai     i  ijj    We conduct experiments on network topologies built by the INET generator       which simulates an Internet like topology at the Autonomous Systems level  Our dataset includes a set of networks of     nodes  where the number of probe stations varies from   to      P  s  t       i             jpai  P  ti   spai       P  sj           j  Here  j    P  sj      is the prior fault probability   ij is the so called inhibition probability  and    i    is the leak probability of an omitted faulty element  The inhibition probability is a measurement of the amount of noise in the network  Fig    shows a factor graph representation of our model  As discussed in Section    we adopt the active probing framework for fault diagnosis  sequentially selecting probes to minimize the conditional entropy  Our previous work      makes the single fault assumption  which effectively reduces S to one random variable with N    possible states  In general  however  multiple faults could exist in the system simultaneously  which requires the more complicated conditional entropy given in Eqn        Let A T  SpaT   t    denote the first term in Eqn       This    Empirical Results  The connections between probe nodes and network nodes are generated with two goals in mind  detection and diagnosis  A detection probe set needs to cover all network components  so that at least one probe has a positive probability of returning   when a component fails  A diagnosis probe set needs to further distinguish between faulty components  Optimal probe set design is NP hard for either detection or diagnosis  For the datasets used here  we first use a greedy approach to obtain a probe set that covers all network components  then augment this set with additional probes in order to guarantee single fault diagnosis  Interested readers may find detailed discussions of probe set design for diagnostic Bayesian networks in           In our experiments  we measure the effects of prior fault probability  and inhibition probability  on approximation and diagnostic quality  We compare the approximate entropy values and the quality of the selected probe set                                                                                                     inhibition prob                                                                            inhibition prob  c  Test set entropy                                        inhibition prob  d  Test set size                                                   inhibition prob  Figure    Approximation errors and diagnostic quality for an augmented detection network  Each curve represents a different prior fault probability  against the ground truth  which is obtained via the junction tree exact inference algorithm  In subsection      we also summarize how the type of network may effect computational efficiency  Since all measurements depend on the particular set of probe outcomes  we repeat all experiments on    different samples of the Bayes net  We use the diagnostic quality of the probe set to determine when to stop the probe selection process  when the reduction in entropy for the past   iterations is no more than          the selection process is deemed to converge  Otherwise we continue until all probes have been picked      Approximation accuracy First  we look at approximation accuracy  Recall that at each time step of the active probing process  we obtain a vector of approximate entropy values  one for each candidate probe T   We average the relative error between the approximate values and the exact values for all candidate probes  and further average over all time steps and samples  Let M denote the total number of probes  n the number of selected probes  h ij the approximate value for probe j at the ith time step of probe selection  and H ij the corresponding exact value  We compute R h  H                     diag                  Network type                 diag                                          seconds saved     Figure    Efficiency of approximate method   a  Average number of BP iterations saved by re using messages   b  CDF of speed up  in CPU seconds  compared to exact method                              b       cumulative distribution           reduction in bit entropy  ave relative abs error        size of final probe set  ave relative abs error         a    b  Second term approx errors                            iters saved per node   a  First term approx errors           n  Mi          hij  Hij     n i   M  i j    Hij          We conduct this experiment on the detection network with    probe stations  augmented with single node probes  Fig    a b  contains plots of the average  the minimum  and the maximum approximation errors  taken over    samples of probe outcomes  Relative error values are shown separately for the first term  A T  S paT   t     and the second term  H T   t     For both terms  the approximation errors  are generally lower at lower  values  The average errors do not exceed     with the only exception being the BP error for term two at        and       which reaches up to      BP approximaton errors of the second term seem to be generally higher than BPEA approximations of the first term  At the maximum  the approximation error never exceeds     for term one  and     for term two  BP errors for term two does not seem to contain any linear trends with respect to   However  BPEAs approximation quality of term one does seem to become slightly worse at higher levels of the inhibition probability      Diagnostic quality The quality of diagnosis is taken to be the reduction in conditional bit entropy of the hidden states  If t   represents the observed outcomes of the final set  of selected probes    we measure H S   H S   t      s P  s  log  P  s          P  s   t   log P  s   t      s Fig    c  compares the diagnostic quality of approximate and exact algorithms on the augmented detection network with    probe stations  Overall  the reduction in bit entropy is larger for higher values of   This is due to the fact that H S  is higher when  is larger  The quality of the exact algorithm is almost identical to that of the approximate algorithm  The two are virtually indistinguishable  except at        and         There is an outlier at this combination  For one of the samples  the value of the entropy H S   t    plateaued unusually early during the active probing process  fooling the algorithm into believing that it had converged  even though the amount of reduction in entropy is still very small  Fig    d  shows that the process terminated after selecting only a small set of tests  This outlier is an artifact of our convergence criterion  not of the approximate algorithm itself  Fig    d  looks at the size of the final selected probe set when active probing converges  Here again  the two algorithms have almost identical behavior  The value of  does not have much impact on the number of selected tests  except when       i e   no noise in the tests   in which case   fewer tests are needed for diagnosis at lower levels of   These results demonstrate that  while the approximated entropy values may deviate from the truth  the diagnostic quality of the approximate method is virtually identical to that obtained using the exact method  Combined with its speed advantages as described in the next section  these results make a strong case for why the approximate method is preferable over the exact one      Implementation and speed We use the junction tree inference engine in Kevin Murphys Bayes Net Toolbox      for Matlab to obtain exact singleton posterior probabilities  The approximate method is implemented on top of the belief propagation C   mex code developed by Yair Weiss and Talya Meltzer  Additionally  we speed up the approximate active probing process by re using BP messages at the start of each round of test selection  thereby maintaining BPs state from the end of the selection round  We find that BP converges in substantially fewer iterations this way  Fig    a  plots the average  maximum  and minimum number of BP iterations that we save by re using BP messages  The results are aggregated over   samples of the Bayes net  The x axis denotes the type of network used  The label diag represents the diagnosis network with   probe station  and the rest are detection networks with various numbers of probe stations  In the detection network with    probe stations  we save up to     iterations per test node at the maximum  On average  re using messages shortens the BP convergence time by       iterations per test  If active probing selected     tests  say  then re using messages would require      to      fewer iterations of belief propagation  Fig    b  is a plot of the empirical cumulative distribution of the speed up using the approximate method  For all of the detection networks  the approximate method is at least   CPU second faster than the exact method for     of the test nodes  The speed up is even higher for the diagnostic network  where for     of all test nodes the approximate method saves at least   CPU seconds per node  This amounts to substantial savings over the entire active probing process  Also keep in mind that  for networks with large tree width  the exact method is not even computationally feasible  Hence  approximation may be the only realistic option     Related Work The problem of most informative test selection was previously addressed in various areas including diagnosis  decision analysis  and feature selection in machine learning  Given a cost function  a common decisiontheoretic approach is to compute the expected value of   information      of a candidate test  i e   the expected cost of making a decision after observing the test outcome  When entropy is used as the cost function  the approach is called most informative test selection  In particular  mostinformative test selection was considered in the context of model based diagnosis     and probabilistic diagnosis       Previous research        on VOI analysis has made various simplifying assumptions such as binary hypothesis and direct observations  An interesting but tangential approach was taken in      which proposes to select a set of tests based on a law of large numbers approximation of the VOI  Up to now  however  no one seems to have addressed the efficiency of computing single test information gain in a generic Bayesian network  Most informative test selection is quite similar to the optimal coding problem      Namely  the hidden state vector S is the input message  and the test outcomes T the output message from some noisy channel  The goal of mostinformative test selection is to minimize the number of bits sent through the channel while still accurately decoding the input message  There is  however  an important difference between the two  In the coding domain  one may separate source coding from channel coding  Fault diagnosis  on the other hand  has to deal with a combination of the two  represented by the conditional probability P  T i   Spai    We may have no control over the source coding function  but we can still aim to select the smallest  most informative subset of tests  In the context of probing  optimal test selection is very similar to the group testing problem      Given a set of Boolean variables  the objective of group testing is to find all failed objects by using a sequence of disjunctive tests  Particularly  sequential test selection is known as adaptive group testing       There is also a direct connection between adaptive group testing and Golomb codes        Note that group testing assumes no constraints on the tests  i e   any subset of objects can be tested together   while in Bayesian networks the tests can be only selected from a fixed set  Even in a less restrictive case of probe selection  we are still constrained by the network topology  Theoretical analysis of constrained group testing is difficult     Conclusions We propose an entropy approximation method based on loopy belief propagation  and examine its behavior on the application of active probing for fault diagnosis in a networked computer system  The level of approximation error varies slightly with the level of noise  But even so  the diagnosis quality is practically identical to that of the exact method  Furthermore  the approximate method can handle larger networks than the exact method  and is almost always faster on the smaller ones  This highlights a promising direction for active probing and fault diagnosis  as well   as for entropy approximation on Bayesian networks in general   
