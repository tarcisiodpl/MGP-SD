  A pseudo independent  PI  model is a proba bilistic domain model  PDM  where proper subsets of a set of collectively dependent variables display marginal independence  PI models cannot be learned correctly by many algorithms that rely on a single link search  Earlier work on learning PI models has sug gested a straightforward multi link search al gorithm  However  when a domain contains recursively embedded PI submodels  it may escape the detection of such an algorithm  In this paper  we propose an improved al gorithm that ensures the learning of all em bedded PI submodels whose sizes are upper bounded by a predetermined parameter  We show that this improved learning capability only increases the complexity slightly beyond that of the previous algorithm  The perfor mance of the new algorithm is demonstrated through experiment  Keywords   Belief networks  probabilistic domain model  learning  search     INT RODUCTION  Learning belief networks has been researched actively by many as an alternative to elicitation in knowledge acquisition               A pseudo independent  PI  model is a probabilistic domain model  PDM  where proper subsets of a set of collectively dependent vari ables display marginally independence  hence pseudo independent          Commonly used algorithms for learning belief networks rely on a single link locka head search to identify local dependence among vari ables  These algorithms cannot learn correctly when the domain model unknown to us is a P I model      If an incorrectly learned model is used for subsequent  inference  it will cause decision mistakes  Worse yet  the mistakes will be made without even knowing  The pseudo independent property of PI models requires multi link lookahead search in order to detect the col lective dependency      As the computational complex ity increases exponentially with the number of links to lookahead  a multi link search must be performed cau tiously  In order to manage the increased complexity  it is suggested     that the single link search should be performed first and then the number of links to lookahead should be increased one by one  Several issues remain open  A straightforward multi link lookahead search as suggested in     will perform a single link lookahead search  then a double link locka head search  and then a triple link lookahead search  etc  It turns out that some PI models will escape such a multi link search  to be detailed below   Therefore  Xiang     suggested to perform a single link locka head search first  followed by a combination of dou ble link lookahead and single link lookahead search  followed by a combination of triple  double and single link lookahead search  etc  However  it is unclear what is the most effective way to combine lookahead search of different number of links  In this paper  we propose an algorithm for learning be lief networks from PI domains  We focus on learning decomposable Markov networks      although the algo rithm can be extended to learning Bayesian networks  We show that our algorithm will ensure correct learn ing of PI models that contain no embedded submodels beyond a predetermined size  The time complexity of the algorithm is analyzed  We assume that readers are familiar with commonly used graph theoretic terminologies such as connected graph  component of a graph  chordal graph  clique  I map  Bayesian networks  Markov networks  etc  The rest of the paper is organized as follows  In section    we briefly introduce PI models  In section    we present the algorithm  The property of the algorithm   Learning in Domains with Recursively Embedded PI Submodels  is analyzed in section    The complexity is analyzed in section    We present our experimental results in section       BACKGROUND  To make this paper self contained  we introduce the basic concepts on PI models briefly in this section  We will use freely the formal definitions in      More detailed discussions and examples can be found in the above reference  If each variable X in a subset A is marginally inde pendent of A   X   we shall say that variables in A are marginally independent  A set N of variables are collectively dependent if for each proper subset A C N  there exists no proper subset C C N  A such that P AIN A    P AIC   A set N of vari ables are generally dependent if for any proper subset A  P AIN A   P A   A pseudo independent  PI  model is a probabilistic do main model  PDM  where proper subsets of a set of collectively dependent variables display marginal inde pendence  PI models can be classified into three types  In a full PI model  every proper subset of variables are marginally independent  Definition    Full PI model  A PDM over a set N   NI        of variables is a full PI model if the following two conditions hold       For each X E N  variables zn N   X  are marginally independent       Variables in N are collectively dependent   In a partial PI model  not every proper subset of vari ables are marginally independent  Definition     Partial PI model  A PDM over a set N   NI        of variables is a partial PI model if the following three conditions hold        There exists a partition  Nt       Nk   k        of N such that variables in each subset N  are generally dependent  and for each X E N  and each Y E Nj  i   j   X andY are marginally independent       Variables in N are collectively dependent   In a PI model  it may be the case that not all vari ables in the domain are collectively dependent  An embedded PI submodel displays the same dependence pattern of the previous PI models but involves only a proper subset of domain variables        Definition    Embedded PI submodel  Let a PDM be over a set N of generally dependent variables  A proper subset N  C N   N I        of vari ables forms an embedded PI submodel if the following two conditions hold       N  forms a partial PI model       The partition  N         Nk  of N  by     extends into N  That is  there is a partition   A         Ak  of N such that N  s  A    i           k   and for each X E A  and each Y E Aj  i   j   X and Y are marginally independent   In general  a PI model can contain one or more PI submodels  and this embedding can occur recursively for any finite number of times  P DMs can often be concisely represented by a graph called an   map     of the PDM  In this paper  we shall mainly use undirected I maps  In particular  we focus on learning an I map that is a decomposable Markov network  DMN   A DMN consists of a graphical struc ture and a probability distribution factorized accord ing to the structure  The structure is a chordal graph whose nodes are labeled by domain variables  Since variables in a PI submodel are collectively de pendent  in a minimal I map of the PDM  the vari ables in the submodel is completely connected  The marginal independence between subsets in the sub model is thus unrepresented  The undirected   maps can be extended into coloredI maps      The marginal independence between subsets are highlighted in a col ored I map by coloring the corresponding links  Definition   An undirected gmph G is a colored I map of a PDM M over N if     G is a minimal   map of M  and     for each PI submodel m  links between each pair of nodes from distinct marginally independent subsets in m are colored  Other links are referred to as black   A partial P I model is shown in Table     The PDM has four variables  which are partitioned into three inde pendent subsets  The PDM contains three embedded PI submodels over N     a  b  c   N     d  a   c   N    d  b  c    Figure   shows the colored I map of this model  The colored links are drawn as dotted  For example  from the distribution P a  c  d   it is easy to verify that N  forms a partial PI submodel with the marginally in dependent partition    a      c d    S     This partition extends into a marginally independent partition   a    b  c  d                Hu and Xiang  Table     A model with embedded P I submodels   d  a  b  c                                                                                               P                                               d  a  b  c                                                                                                P              O Dl                               a       Figure     Colored   map of the model in Table     It has been shown     that common algorithms for learning belief networks cannot learn a P I model cor rectly because they rely on a single link lookahead search to identify local dependence among variables  For example  if these algorithms are used to learn the above model  assuming learning starts with an empty graph  only the link  d  c   can be connected and the returned graph is not an I map of the PDM     THE LE A RNING ALGORITHM  The pseudo independence property of PI models re quires more sophisticated search procedures in learn ing  Suppose a PI submodel over N  C N is parti tioned into k marginally independent subsets  If we lookahead by multiple links at each search step such that N  is completely connected by a set of new links  and test P XIY  N   X Y    P XIN   X  Y   where  X  Y  is one of the new links  we will get a negative answer  This prompts the completion of N  in the learned graph  Based on this observation  a straight forward multi link search is suggested in      Such a search will perform a single link lookahead  followed by a double link lookahead  followed by a triple link lookahead  etc  A multi link search is more expensive than a single link search since O INI  i  sets of links need to be tested before one set of links is adopted  Since the complex ity increases exponentially with the number of links to lookahead  an multi link search must be performed cautiously  Three strategies are proposed in     to manage the computational complexity       perform ing single link search first      increasing the number  of links to search one by one  and     making learning inference oriented  Although the previous straightforward multi link search can learn correctly many PI models  it was found that some PI submodels may still escape the learning algorithm  For example  if we apply such a search to the P I model in Table    the single link search will add the link  d  c   The following double link search will first discover the PI submodel over N  and add links  d a  and  a c   It then discovers the P I submodel N  and add links  d  b  and  b  c   But the P I submodel over N   will never be learned by the double link lookahead or lookahead with higher num ber of links  since only a single link   a  b  is uncon nected  Consequently  the learning outcome will not be an I map  Realizing this deficiency of the straightforward multi link search  an improved multi link search algorithm was proposed in      In addition to the incorporation of the above three strategies  the search is performed in the following manner  A single link lookahead is performed first  followed by a combination of double link lookahead and single link lookahead  followed by a combination of triple  double and single link looka head  etc  We shall refer to such a systematic search that lookaheads by no more than i     links as an i link search  We refer to a multi link search which examines only j    links at each step until no more links can be learned as an j link only search  The algorithm proposed in      however  did not specify what is the most effective way to combine lookahead search of different number of links  This is the issue we address in this paper  We start by asking the question why some P I models may escape the straightforward multi link search  The previous example shows that the main reason is the recursive embedding of P I sub models  If a P I submodel M  is embedded in another P I model M    M  will be learned first  After that  if the number of unlearned links in M  is less than the current number of links to lookahead  M  will not be learned correctly in the later search steps  In order to learn M   backtracking to lower number of lookahead links is necessary  Hence the problem translates to a proper arrangement of backtracking during learning  We propose a multi link search algorithm  ML  which overcomes the deficiency mentioned above  The learn ing outcome is represented as DMN  The algorithm focus on learning the chordal structure  Once the chordal graph is obtained  the numerical probability distribution can be estimated from the data  ML starts with an empty graph  It performs a single link search first  The first stage of the search now ends    Learning in Domains with  ML then performs a double link only search  If some links are learned during the double link only search  ML backtracks to perform another single link search  Afterwards  it performs double link only search again and backtracks if necessary as before  The combi nation of double link only and single link search will continue until no link is learned in a double link only search  We shall refer to this repeated combination of the double link only search and the single link search as a combined double link search  Now the second stage of the search ends   Next  ML will perform a triple link only search  If some links are learned during the search  ML back tracks to repeat the previous two stages  Afterwards  it performs another triple link only search and back tracks if necessary as before  We shall refer to this repeated combination of the triple link only  double link only and single link search as a combined triple link search  Note that a combined triple link search can include several combined double link search  Now the third stage of the search ends  ML continues with a combined four link search  fol lowed by a combined five link search  etc   until a combined k link search  where k     is a predeter mined integer  The pseudo code of this algorithm is presented below  Algorithm ML Input  A dataset D over a set N of variables  a maximum number k of lookahead links  Return  The learned graph  Comment  lookahead i  is the function for an i link only search  begin   initialize a graph G    N  E         for j     to k do              while i      j do     modified    lookahead  i      if  i      AND  modified  true    then i        backtracking    else i    i        return G and halts  end     In algorithm ML  the search stages are indexed by j  line    and each iteration of the outer for loop corre  sponds to one stage  The first iteration has i   j      lines   and     The single link search lookahead l   line    will be performed  The test in line   will fail and i becomes    line     This terminates the while loop as well as the first iteration of the for loop  It corresponds to the first stage of search  The next iteration of for loop has i   j      The double link only search lookahead    will be per formed  If some links have been added  the test in line   will succeed and i becomes    This causes the  Recursively Embedded PI       Submodels  execution of another single link search lookahead l   Afterwards  i becomes   and another double link only search will be performed  If nothing has been added  modified is false and i becomes    This terminates the while loop and the second iteration of the for loop  It corresponds to the second stage of search  The next iteration of for loop has i   j      The triple link only search lookahead    will be performed  If some links have been added  the test in line   will succeed and i becomes    This causes the repeti tion of the previous two stages  This execution of lookahead    and repetition of stages   and   contin ues until an execution of lookahead    returns false  Afterwards  i becomes   and the while loop will be terminated  It will also terminate the third iteration of the for loop and end the third stage of search  The function lookahead  i  performs an i link only search  It consists of multiple passes and each pass is composed of multiple steps  Each step tests one set of i links  Each pass learns one set of i links after testing all distinct and legal combinations  one at each search step  of i links  This function may be implemented using different scoring metrics  We defer the presen tation of our implementation using the cross entropy scoring metric to section         r            h  a   Stage I  il     r  Stage Z    b        b   a                   c  h      c   Stage    Figure    The process of learning the model in Table    Figure   shows the execution of ML in learning the PI model in Table   with the value of k set as k      ML starts with a single link search  The first stage   After all links are examined  one set of links L      d  c   is learned  The learned graph is shown in Figure    a   In the second stage  ML performs the double link only search first  which learns two sets of links L      d  a     a  c    L      d  b    b  c    These links are contained in the P I submodels over N  and N   The corresponding graph is shown in Figure    b   Since some new links are added after the double link only search  ML backtracks to perform the sin gle link search again  During this search one set of links L       a  b   is added and Figure    c  is ob tained  ML continues to perform another double link only search but no more links can be learned  The ML halts with a complete graph which is a correct I map     PROPERTY  Can ML learn any P I model correctly  Clearly the an swer is no as ML only searchs up to a predetermined        Hu and Xiang  number i of lookahead links  A PI submodel that con tains more than i colored links may escape ML  Then what is the characteristics of the PI models that can be learned by ML  The following theorem answers this question  Theorem   Let M be a PI model such that each em bedded PI submodel in M contains no more than i col ored links  the algorithm ML with parameter i will re turn an   map of M      COMPLEXITY ANALYSIS  For each pass in ani link only search  O N i  sets of i links need to be tested  one set at each step  Therefore each pass contains O N i  steps  Since each pass adds one set of i links  an i link only search contains    l  passes  Table   shows the relation among the index i  the num ber of steps per pass and the number of passes in an i link only search   Proof  Let GM be the minimal colored I map of M  All black links in GM can be learned by the initial single link search lookahead    in the first stage  We show that ML will learn colored links in every embedded PI sub model  When i      M contains no embedded PI submodels  no colored links  and the result is trivially true  When i      each embedded PI submodel in M con tains only three variables  There are two colored links and one black link among these three variables  The black link will be learned by the initial single link search as mentioned above  The two colored links can be learned by lookahead    in the second stage  Now we assume that when i   k  if an embedded PI submodel has no more than k unlearned colored links  then these links can be learned by the first k stages  Suppose i   k      Every PI submodel with no more than k colored links in M can be learned by assump tion  For each PI submodel x with k     colored links in M  x either contains one or more embedded PI sub models or contains none  If x contains at least one embedded PI submodel y of j    colored links  then we have j  k and y must have been learned in the first k stages by assumption  Since the number of remaining colored links in x is k     j      k    these links must also have been learned in the first k stages by assumption        If x contains no embedded PI submodel  then it can be learned by lookahead  k      at the beginning of stage D k      The theorem is proven  Given the parameter k for ML  some PI submodels with more than k colored links may still be learned  Suppose a PI submodel x has more thank colored links and has two other PI submodels y and z embedded in it  If the number of colored links in y or z is no more than k  then y and z can be learned by ML  If the number of remaining colored links in x is no more than k  then x can also be learned by ML  A formal treatment of such cases will be included in a longer version of this paper   Table    The relation among i  number of steps per pass and number of passes in an i link only search           o f stepsfpass O N   O N     O N        o f passes O N        l            k   k  O N  k     O N k              k       o Jr   In order to derive the upper bound of the total number of passes in a k link search  we construct a directed graph such that each node in the graph corresponds to one pass during the search and each arrow indi cates the chronological order of successive passes  We shall label each node by the number of links to locka head in the pass  For example  a pass in a single link search will be labeled by    and a pass in a double link only search will be labeled by    etc  A graph so constructed will be a directed chain  For the purpose of a later conversion  nodes with the same label will be drawn at the same level and levels are arranged in the decreasing order of the labels  Figure   shows such a graph for the execution of a   link search  The four nodes in the bottom left correspond to the four passes in the first stage during the search  The next three nodes  labeled    correspond to the three passes in the first double link only search  Since links are learned  they are followed by backtracking to a single link search  shown by the three nodes labeled   in the middle bottom of the graph   Figure    The execution chain of a   link search  Once we obtain such a chain  it can be converted into a set of trees  a forest  as follows  Each node not at the top level will be assigned a parent at the next higher   Learning in Domains with Recursively Embedded PI Submodels  level  and the child and the parent will be connected by an undirected link  The parent of a node is assigned as the first node in the next higher level down the chain  For example  the first node labeled   in the chain will be the parent of the first four nodes labeled   in the chain  The first three nodes labeled   in the chain will have the first node labeled   as their parent  After each node not at the top level has been assigned a parent  we remove all arrows from the graph  The resultant graph is shown in Figure    Each component of the graph is a tree  This is because each node not at the top level has a unique parent  We shall refer to the graph as an execution forest        I   Figure    The execution forest of a   link search  We now use the execution forest to analyze the com plexity of an i link search  For each node at level i      i  k   some child nodes correspond to learning passes each of which adds a set of i     links  Other child nodes correspond to non learning passes that add no links  The number of non learning passes can not be more than the number of  earning passes  The num ber of learning passes is bounded by             according to Table    Hence each node at level i      i  k   has   ren     i I N   ch ld Next  we derive the number of passes at each level  The number of passes at the level k   top level  is     i       The number of passes at the level k    is  The number of passes at the level   is  Finally  the number of passes at the level   is  Therefore  according to Table    the total number of search steps is               Since the factor  t   h Ll              is          upper bounded by    the total number of search steps in a k link search is  h k     O N  k  l    In order to complete the complexity analysis  we need to take into account of the complexity of each search step  which is dependent on the choice of scoring met ric used in lookahead i   Our implementation  to be detailed in the next section  is based on the algorithm in      The complexity of one search step is  O n   ry ry log             where n I S the number of cases in the dataset and    is the maximum size of cliques  Hence the overall complexity of the algorithm is  Compared with the complexity of a straightforward multi link search algorithm      the complexity of a k link search using ML is higher but not much higher  The benefit of the slightly in creased complexity is the capability of learning recur sively embedded PI models     IMPLEMENTATION AND EXPERIMENTAL RESULTS  Given the algorithm ML  the only missing detail in implementation is the function lookahead  i   Our im plementation of this function is based on the algo rithm in      Instead of testing the conditional inde pendence directly  a test of whether new links decrease the Kullback Leibler cross entropy is performed  This is justified the following shown in            Minimiz ing the K L cross entropy between a dataset D and a DMN obtained from D is equivalent to minimizing the entropy of the DMN      A learning process start ing with an empty DMN structure and driven by the minimization of the above K L cross entropy is par alleled by the process of removing false independence  missing links relative to some minimal   map  in the intermediate DMNs  The pseudo code of the lookahead i  function is shown below  A threshold d is used to differentiate between a strong dependence and a weak one   may be due to noise    A greedy search can thus be applied   line   through    to avoid adding unnecessary links and links due to weak dependence      The condition that L is implied by a single clique C means that all links in L are contained in the subgraph induced by C  This requirement helps to reduce the search space         Hu and Xiang  Function BOOL lookahead  int i    Input  i is the number of lookahead links  Comment   his a threshold  begin   modified   false    repeat   initialize the entropy decrement dh          for each set L of links  ILl   i  L n E        do   if G      N  E u L  is chordal and L is implied by a clique  then compute the entropy decrement dh       if dh    dh   then dh     dh   G      a    if dh    oh  then G    G   done   false    modified     true     else done    true     until done   true      return end  The PDM contains five embedded PI submodels over Nt  The following demonstrates our implementation with two datasets  Our primary emphasis is the capability of learning correctly PDMs with recursively embedded PI submodels  First  a dataset of      cases was gen erated from the PDM shown in Table    The successful run used k      Jh          The learning process is the same as Figure    It is summarized in Table     N     ball  ball   music box    N     ball   ball   ball   musicJJOx   N     lightl  light   dog    Ns    music box  dog  John   Note that the first two PI submodels are recursively embedded in the third PI submodel   bl  bl b  m  Table    Summary of learning the PDM in Table   i  learned  graphs cross entropy tested decrement link set            d c               a  c     d  a J             d  b    b c               a  b     lmk            ball  ball  music  box    We generated a dataset of      cases from the music box dog John domain  Using k     and  h          the algorithm learned the   map successfully  The learning process is shown in Figure     modified   only search     Next  we use a PDM from     described below  Three balls are drawn each from a different urn  Urn   has     white balls and the rest of the balls black  Urn   and urn   have     and     of white balls  respectively  A music box plays if all three balls are white or exactly one is white  A dog barks if two random lights are both on or both off  John complains if it s too quiet  neither the box plays nor the dog barks  or too noisy  both the box plays and the dog barks   The model is specified as a Bayesian network shown in Figure    Its colored   map is shown in Figure     II  I   Jo  b  v            bl             b  II  m   a   J  Stage I      bl        b             c   Stage    v              t   d   Stage    Figure    The process of learning the music box model  The algorithm started by performing the single link search  In the first pass  one link was learned  Lt     light   dog    It took    steps     candidate graphs tested   In the second pass  after    steps  another link was learned  L      ball   music box     Note that a standard single link search learning algo rithm will halt and returns this graph which is not an   map of the domain  Since nothing was learned in the third pass  a   link only search was performed next  After     steps  three sets of links were learned in the following order  Ls     light    light     light   dog          ball   ball     ball   music box     ball e b  r               r   music box                 e hn  Figure    Colored   map of the music box example          ball   ball     balll  music box     Then the algorithm backtracked to perform a single link search with one link learned  Ls      balll  ball      During the next single link search and the following   link only search  no link was added  Hence a   link only search was performed  which learned the links    Learning in Domains with Recu rsively Embedded PI Submodels  ball   b all  bal l  l ight I light   m        f  dog   b ark quiet  music box   play quiet  John    complain satisfied   ball  white bl ack  P b alll  w      light   on off  P ball  w      P b all  w      P lightl on      P light  on        P John clmusic box p dog b  l P John clmusic box q dog b  O P John clmusic box p dog q  O P John clmusic box q dog q  l  P dog bllight      an light  on      P music box plall balls w  l P music box plone b all w  l P music box plall balls b    P music box plone b all b  O  P dog bllight    on light  off     P dog   bllight    off light  on    P dog bllight  off light  off  l       Figure    The specification of the music box model  L        music hox  dog    dog  John    John  music hox     The backtracking occurred afterwards  but no more links was learned  Finally  the algorithm halted and returned the correct I map  A total of      candidate graphs were tested  A summary of the experiment is shown in Table          Table    Summary of learning result  i  link  only search                 learned link set           Ls  Ls       graphs cr oss entropy tested                              decrement                                                   We believe that no search steps in the improved algo rithm may be deleted without jeopardizing the above learning capability  We are currently working to for mally establish this result  Acknowledgements This work is supported by grant OGP        from the Natural Sciences and Engineering Research Council and grant from the Institute for Robotics and Intelligent Sys tems in the Networks of Centres of Excellence Program of Canada   
A Robust Quantum Random Access Memory Fang Yu Hong   Yang Xiang   Zhi Yan Zhu   Li zhen Jiang   and Liang neng Wu     arXiv          v   quant ph     Jan       Department of Physics  Center for Optoelectronics Materials and Devices  Zhejiang Sci Tech University  Hangzhou  Zhejiang         China   School of Physics and Electronics  Henan University  Kaifeng  Henan         China   College of Information and Electronic Engineering  Zhejiang Gongshang University  Hangzhou  Zhejiang        China   College of Science  China Jiliang University  Hangzhou  Zhejiang         China  Dated  January           A bucket brigade architecture for a quantum random memory of N    n memory cells needs n n      times of quantum manipulation on control circuit nodes per memory call  Here we propose a scheme  in which only average n   times manipulation is required to accomplish a memory call  This scheme may significantly decrease the time spent on a memory call and the average overall error rate per memory call  A physical implementation scheme for storing an arbitrary state in a selected memory cell followed by reading it out is discussed  PACS numbers        Lx        Ud        Ff Keywords  quantum random memory  bucket brigade  microtoroidal resonator  A random access memory  RAM  is a fundamental computing device  in which information  bit  can be stored in any memory cell and be read out at discretion         A RAM is made up of an input address register  a data register  an array of memory cells  and a controlling circuit  A unique address is ascribed to each memory cell  When the address of a memory cell is loaded into the address register  the memory cell is selected and the information in the data register can be stored in it or the information of the cell can be read out to the data register  Like its classic counterpart  quantum random access memory  QRAM  is the building block of large quantum computers  A QRAM is a RAM working in a way with quantum characteristic  the address and data registers are comprised of qubits instead of bits  and every node of the controlling circuit is composed of a quantum P object  When the address state is in superposition  Pi i  xi i  the read out operation gives the output state i i  qi i in the data register  where  qi i is the quantum information stored in the memory cell i associated with the address  xi i  Quantum random access memories storing classic data can exponentially speed up the pattern recognition       discrete logarithm         and quantum Fourier transform  and quantum searching on a classical database      A general QRAM is an indispensable for the performance of many algorithms  such as quantum searching       element distinctness           collision finding       general NAND tree evaluation       and signal routing       In a seminal paper           Giovannetti et al   GLM  proposed a promising bucket brigade architecture for QRAMs  which exponentially reduce the requirements for a memory call  However  in GLM scheme  n times of quantum unitary transformations per memory call is required to turn one quantum trit initialized in  waiti in each node of the control circuit into  lef ti or  righti  and all flying qubits including address qubit and bus qubit can pass through an arbitrary node of the controlling cir   cuit only if a successful quantum manipulation has been performed on the trits  leading to the times of manipulations on the nodes Nc   n n        per memory call for  n memory cells  where n is the number of bits in the address register  Here we present a QRAM scheme  where the quantum object in every node have only two possible states  lef ti and  righti  On average the times of quantum manipulations on the nodes per memory call can be reduced to Nc   n    significantly decreasing both the decoherence rate and the time spent on a QRAM call  A physical implementation for information storage and read out on a QRAM is presented  The main idea is shown in Fig    The N memory cells are positioned at the end of a bifurcation control circuit with n   log  N levels  At each node of the control circuit there is a qubit with two states  lef ti and  righti  The state of the jth qubit in the address register controls which route to follow when a signal arrives at a node in the jth level of the circuit  if the node qubit is   i  the left path is chosen  if it is   i  the right path is chosen  For example  an address register     i means that left at the  th level  left at the next  and right at the second  Illuminated by a control laser pulse a node qubit in state  lef ti will flip to  righti if the incoming address qubit is   i  or remain in  lef ti if the address qubit is   i  Without the control pulse  a node qubit in  lef ti   righti will deviate any incoming signal along the left right  side route  First  all the node qubits are initialized in state  lef ti  Then the first qubit of the address register is dispatched through the circuit  At the first node  the address qubit incurs a unitary transformation U on the node qubit with the help of a control pulse  t   U   i lef ti     i lef ti and U   i lef ti     i righti  Next the second qubit of the address register is dispatched through the circuit  follow left or right route relying on the state of the first node qubit  and arrives at one of the two nodes on the second level of the circuit  The node qubit illuminated by the control pulse will make a corresponding state change     according to the state of the second address qubit  and so on  Note that the ith control pulse  t  address all the nodes of the ith level control circuit simultaneously  After all the n qubits of the address register have gone through the whole circuit  a unique path of n qubits has been singled out from the circuit  see Fig     Subsequently  a single photon is sent along the selected path to single out a memory cell  After that an arbitrary unknown state in the data register can be transferred to the selected memory cell along the selected path  or the state of the selected memory cell can be read out to the data register along the path with black squares in Fig    Finally  all the node qubits are reset to  lef ti for a next memory address  Because the state of a node qubit  lef ti will not be affected by the control pulse illuminating should the qubit of the address register be in state   i  on average there are n   node qubits will flip to  righti in each memory call  This means that on average only n   times of control manipulations are really performed in each memory call  As a result  the mean comprehensive error rate per memory address is no        log  N o with the assumed error rate o per node qubit flip event  In contrast  the GLM scheme requires n times state flip for a memory call  In addition  in the GLM scheme a photon may pass through a node only when a control pulse is applied on the quantum trit  resulting the overall times of quantum manipulations on the quantum trits per memory call be n n        which has included  n times of manipulations for a signal photon going to a memory cell and back to a data register along a same selected path  Here a singlephoton can pass through a node without any quantum manipulation  therefore the average times of quantum manipulation really performed on node qubits per memory call is n    Thus this scheme may significantly decrease the average overall error rate and shorten the time required for a memory call  Now we discuss a physical implementation  The node qubit is encode on an atom with level  lef ti   righti  and an intermediate state  ei  see Fig   A   The transition between  lef ti and  ei is coupled to the evanescent fields of modes a and b of of frequency c of a microtoroidal resonator  State  righti is coupled to  ei by classical control field  t   A tapered fiber and the resonator are assumed to be in critical coupling where the input photons of frequency p   c are all reflected back and the forward flux in the fiber drops to zero when the atom transition   lef ti   ei  is far detuned from the resonator frequency c       If the atomic transition   righti   ei  is on resonant with the resonator  the input photons can transmit the resonator and travel forward one by one       A single photon can be coherently stored in the atom initialized in  lef ti by applying the control pulse  t  simultaneous with the arrive of the photon which is equally divided and incident from both sides of the tapered fiber simultaneously  see Fig          The photon storage results in a state flip of the atom to  righti  If no single photon is contained in the incoming field  the  data register           address register  left left right  left left  left  left  memory cells  FIG      color online   Schematics for a quantum random access memory  In each node of the binary control circuit  a qubit in  righti  lef ti  routes the approaching signals right  left   A single photon can excite the qubit from  lef ti to  righti with the aid of a classical impedance matched control field  t   Here the third level memory cell     i is addressed through the selected path marked with red circles  The read out state is transferred to a data register along the path marked with black squares   atom does not affected by the control pulse illuminating and remains in  lef ti       The switch function of a node qubit in a QRAM can be realized as follows  first  the address qubit is encoded as   ip    ip with Fock state  nip  n         and arbitrary unknown complex coefficients  and   the first qubit of the address register is sent out along the control circuit and is coherently stored in the atom in the first node by applying the control pulse  t  simultaneous with the arrival of the address qubit equally split and incident from both sides of the tapered fiber simultaneously  This storing process will incur a state flip of the node atom to  righti if the address qubit is   i  or make no change in the atom state  lef ti if the address qubit is   i  When the second address qubit is sent out and meet the first node  it will be reflected back and travel along the left path by applying an optical circulator in one side of the tapered fiber  see Fig   b  if the atom in the first node is in  lef ti  or will transmit the resonator and go along the right path if the atom is in  righti  When the second address qubit arrive at one of the two nodes on the second level  it will be coherently stored in the node atom and left the atom in  lef ti or  righti dependent on the photon number contained in the address qubit  and so on  We assume that each quantum memory cell in the memory array consists of a memory atom m and an ancillary atom a  which are confined in two harmonic traps and positioned inside a high quality cavity  see Fig  A   The ancillary atom has a three level structure   gia is coupled to  eia by the field of the cavity mode with strength ga    sia is coupled to  eia by a classic control field    t   where the subscript a denotes an ancillary atom  After a path to the memory array is singled out  a single photon is sent along the path to the selected memory cell  A control laser pulse    t  is applied to the ancillary atoms initialized in state  gia at the moment when     A  B   eU    t    g   rightU   leftU  leftU  atom a   rightU  microtoroidal b resonator  optical circulator  a b  left  right  FIG      Color online   Schematic diagram of a node consisting of a three level atom and a microtoroidal resonator   A  An address qubit consisting of zero or one photon is split equally in counter propagating directions and coherently stored using an impedance matched control field  t   leading to a state flip of the atom conditioned on the photon number   B  By employing an optical circulator an photon travels along a tapered fiber being in critical coupling to the resonator will go along the left  right  path if the atom is in state  lef ti  righti    the photon arrives at the memory cell  resulting a state flip of atom a to  sia           To avoid to be involved into the quantum operations aimed on the selected memory cell  the ancillary atoms non selected are excited to a stable state  tia by a  pulses on transition  gia   tia   Next we can do some quantum manipulations either to save an arbitrary unknown quantum state in the selected memory cell or to read out its content  In the first place  we discuss how to save an arbitrary unknown quantum state in the memory cell identified by the address register  First  an initialization operation on the memory cell array is performed  This can be realized as follows  atom a in state  sia is excited to a Rydberg state  ria by two photon stimulated Raman  TWSR  pulses       TWSR pulses on the transition  sim   rim  in terms of the perturbed state   of the memory atoms are applied  resulting the selected memory atom being excited to  rim and immediately flipping to the ground state  gim through spontaneous radiation  see Fig  B   In this way only the memory atom in the selected memory cell is reset in state  gim   leaving the content of the others unchanged  because the states  rim of the non selected memory atoms are off resonant with the TWSR pulses due to the absence of the strong Rydberg dipole interactions       The ancillary atom in Rydgerg  ria is brought to the ground state  gia by applying a  pulse on its transition  ria   gia   Second  an arbitrary unknown state   ip     ip with Fock bases  nip  n         is transferred along the selected path to the memory cells  On the arrival of the signal a classic control pulse    t  is applied on the ancillary atoms initialized in the ground state  gia   leading to a state map    ip     ip   gia    ip   gia    sia             State  gia    sia is then transferred to a memory atom by employing the strong dipole interaction between  two Rydberg atoms           When both of the memory atom and the ancillary atom are in Rydberg states  the strong dipole interaction between them will couple their motion  which is best described in the basis of normal modes  jin  j                  All the motion modes of the atoms are initially cooled to near their ground state by Raman sideband cooling on them       Third  we drive a  pulse on transition on  gim   rim on memory atoms to excite the memory atoms from state  gim to state  rim   Fourth  a  pulse on transition  gia   ria   in and a blue sideband  BSB  pulse on transition  sia   ria   in are applied on the ancillary atoms  leading to state  i     ria  rim   in    ria  rim   in       for the selected memory cell  see Fig   bB  and state  i     x  rim   x  sim   tia for other memory cells with their initial states state x  gim   x  sim  see Fig  C   Here we have used the fact that the normal mode of motion is shared by the memory atom and the ancillary atom  both of which are in Rydberg states  Fifth  a  pulse on the transition  rim   gim unperturbed by the strong dipole interaction between two Rydberg atoms is applied on the non selected memory atoms to restore them to their initial states x  rim   x  sim   Sixth  a  pulse on transition  rim   in   r im   in illuminates the memory atoms  resulting a state mapping  i    i     ria  rim   in    ria  r im   in           Seventh  two  pulses on  r im   in   si and  rim   in   gim   respectively  are applied on the memory atom  leading to an unitary transformation  i    i      gim    sim   ria   the unknown state   i     i has been stored on the selected memory atom  Note that this two pulses will not affect the states of the non selected memory atoms since the pulses is detuned from the transitions  r im   sim and  rim   gim   which are free of the influence of strong dipole interaction between two Rydberg atoms  Eighth  the ancillary atoms is restored to the ground state  gia by a a  pulse on the transition  ria   gia  see Fig  E   and the non selected ancillary atoms are restored to the ground state  gi for the next memory call by flipping to a intermediate level with a pulse and then falling to  gi through spontaneous radiation  In this way an arbitrary unknown state can be transferred to the selected memory atom  leaving the states of other memory atoms unchanged  Now we discuss how to read out the content of the selected memory atom x  gim   x  sim   First  a  pulse on transition  sia   ria is employed to excite the selected ancillary atom to state  ria   Second  we employ two  pulses on transition  gim   rim   in and  sim   rim   in   respectively  see Fig  F   driving the system of the selected atoms m and a into state  i    x  rim  ria   in   x  rim  ria   in         Third  a  pulse on transition  ria   in   r ia   in drive the selected system into state  rim  x  ria          x  r ia    in  see Fig  G   Fourth  the selected memory atom is sent to the ground state by a  pulse on the transition  rim   in   gim   Fifth  two pulses on the transition  ria   gia and  r ia   sia   respectively  set the selected ancillary atom in state x  gia   x  sia  see Fig  H   the content of the select memory atom is transferred to the ancillary atom  Note that these pulses do  not influence the non selected atoms a and m because they have no strong dipole interaction  By applying a classical impedance matched control field  t   a matterphoton mapping  x  gia   x  sia    ip   gia  x   ip   x   ip   can be accomplished           transferring the state of the selected memory cell to the flying qubit and leaving the ancillary atom in state  gia   The flying qubit goes along the path with black squares to the data register  see Fig     Finally  the non selected ancillary atoms are initialized to state  gia for a next task  In summary  we have presented a scheme for a quantum random access memory  With three level memory system been substituted by a qubit in every node of the control circuit  this structure may significantly reduce overall error rate per memory address and the memory address time  In addition  we have discussed a physical implementation based on microtoroidal resonator and strong dipole interaction between two Rydberg atoms for a QRAM writing and read out  The microtoroidal resonator and the tapered fiber may be replaced by a surface plasmon propagating on the surface of a nanowireconductor dielectric interface           This work was supported by the National Natural Science Foundation of China             and            by Zhejiang Provincial Natural Science Foundation of China  Grant No  Y        and Y          and by Scientific Research Fund of Zhejiang Provincial Education Department  Grant No  Y          and Y                 R  Feynman  Feynman Lectures on Computation  Perseus Books Group  New York             R  C  Jaeger and T  N  Blalock  Microelectronic Circuit Design  McGraw Hill  Dubuque         p           G  Schaller and R  Schutzhold  Phys  Rev  A                        R  Schutzhold  Phys  Rev  A                        C  A  Trugenberger  Phys  Rev  Lett                                            D  Curtis and D  A  Meyer  Proc  SPIE                       A  Ambainis  Proceedings of the   th IEEE Symposium on Foundations of Computer Science  FOCS            p     SIAM J  Comput                      A  M  Childs  A  W  Harrow  and P  Wocjan  Proceedings of the   th Symposium on Theoretical Aspects of Computer Science  STACS        Lecture Notes in Computer Science vol               p           M  A  Nielsen and I  L  Chuang  Quantum Computation and Quantum Information  Cambridge University Press  Cambridge              L  K  Grover  in Proceedings of the   th Annual Symposium on the Theory of Computing ACM Press  New York         p           A  Ambainis  in Proceedings of the   th IEEE FOCS    IEEE Computer Society  Rome         p      arXiv quant ph               A  M  Childs  A W  Harrow  and P  Wocjan  in Proceedings of the   th Symposium on Theoretical Aspects of Computer Science  STACS        Lecture Notes in Com   puter Science  Springer  New York         Vol        p       arXiv quant ph          G  Brassard  P  Her  and A  Tapp  ACM SIGACT News  Cryptology column                 e print arXiv quant ph          A  M  Childset al  in Proceedings of the   th IEEE Symposium on Foundations of Computer Science  FOCS     to be published   arXiv quant ph          V  Giovannetti  S  Lloyd  and L  Maccone  Phys  Rev  A                    V  Giovannetti  S  Lloyd  and L  Maccone  Phys  Rev  Lett                      T  Aoki  B  Dayan  E  Wilcut  W  P  Bowen  A  S  Parkins  T  J  Kippenberg  K  J  Vahala  and H  J  Kimble  Nature  London                   B  Dayan  A  S  Parkins  T  Aoki  E  P  Ostby  K  J  Vahala  H  J  Kimble  Science                   F  Y  Hong and S  J  Xiong  Phys  Rev  A                    W  Yao  R  B  Liu  and L  J  Sham  Phys  Rev  Lett                     W  Yao  R  B  Liu  and L  J  Sham  J Opt B    S            E  Urban  T  A  Johnson  T  Henage  L  Isenhower  D  D  Yavuz  T  G Walker  and M  Saffman  Nature Phys                 D  Jaksch  J  I  Cirac  P  Zoller  S  L  Rolston  R  Cote  and M  D  Lukin Phys  Rev  Lett                   F  Y  Hong  Y  Xiang  Z Y  Zhu  and W H  Tang  Quan    eUa  m     t    ga  a   sUa  gU a                        rU     UU  r U  n n   sU  gU    Un   Un   Un   Un  m  a  m  a  m  a  m  a  m  a  m  a  m  a  FIG      color online   Schematics for writing to and reading out of a memory cell   A  A single photon can be coherently stored in a memory cell consisting of two atoms m and a positioned inside of a high Q cavity with a time dependent control pulse    t   Diagrams of energy levels and pulses sequences for storing unknown state to the selected memory cell  B to E  and for reading out of the content of the selected memory cell  F to H                                                                       tum Inf  Comput                        P  O  Schmidt  T  Rosenband  C  Langer  W  M  Itano  J  C  Bergquist  and D  J  Wineland  Science                       C  Monroe  D  M  Meekhof  B  E  King  W  M  Itano  and D  J  Wineland  Phys  Rev  Lett                        D E  Chang  A S  Srensen  E A  Demler  and M D  Lukin  Nature Phys                      F  Y  Hong  S J  Xiong  Nanoscale Res  Lett                    
  these algorithms  a single link lookahead search is com monly adopted for efficiency  In a single link lookahead  It has been shown that a class of probabilistic domain models cannot be learned correctly by several existing algorithms which employ a single link lookahead search  When a multi link lookahead search is used  the computa tional complexity of the learning algorithm increases   We study how to use parallelism  to tackle the increased complexity in learn ing such models and to speed up learning in large domains  An algorithm is proposed to decompose the learning task for parallel pro cessing  A further task decomposition is used  search  consecutive network structures adopted differ by only one link   However  it has been shown that  there exists a class of domain models termed pseudo independent   PI   models which cannot be learned cor  rectly by a single link lookahead search  Xiang et al           One alternative for learning PI models is to  use multi link lookahead search  Xiang et a            where consecutive network structures may differ by more than one link  Increasing the number of links to lookahead  however  increases the complexity of learn ing computation  In this work  we study parallel learning of belief net  to balance load among processors and to in  works  Parallel learning not only can be used to tackle  crease the speed up and efficiency  For learn  the increased complexity during multi link lookahead  ing from very large datasets  we present a re  search  but also can speed up learning computation  grouping of the available processors such that  during single link lookahead search in a large domain   slow data access through file can be replaced  Although parallel learning of rules have been studied  by fast memory access  Our implementation in a parallel computer demonstrates the ef fectiveness of the algorithm    Cook   Holder       Provost   Aronis       Shaw   Sikora        we do not realize other works on par allel learning of belief networks   Our study focuses  on learning decomposable Markov networks  DMNs   although our result can be generalized to learning     INTRODUCTION  As the applicability of belief networks has been demon  Bayesian networks  We study the parallelism using a message passing MIMD  multiple instruction multiple data  parallel computer   strated in different domains  and many effective infer  We shall assume that readers are familiar with com  ence techniques have been developed  the acquisition  monly used graph theoretic terminologies such  of such networks from domain experts through elicita  cle  connected graph  DAG  chordal graph  clique   tion becomes a bottleneck  As an alternative to man  junction tree  JT   sepset in a JT  I map  etc  A junc  as  cy  ual knowledge acquisition  many researchers have ac  tion forest  JF  F of chordal graph G is a set of JTs   tively investigated methods for learning such networks  each of which is a JT of one component of G   from data  Cooper   Herskovits       Beckerman et al        Herskovits   Cooper       Lam   Bacchus         Spirtes et al        Xiang et al          The paper is organized  as  follows  In Section    we in  troduce PI models and multi link lookahead search  In section      we propose parallel algorithms for learning  Since learning belief networks in general is NP hard  belief networks  We also analyze the problems of load   Chickering et al         it is justified to use heuristic  balancing and local memory limitation  and present  search in learning  Many algorithms developed use a  our solutions  In section tal results   scoring metric combined with a search procedure  In      we present our experimen   Exploring Parallelism in Learning Belief Networks  B ACKGROUND     Xl   v     y   l  To make this paper self contained  we give a brief introduction of PI models and multi link lookahead search       XI  PSEUDO INDEPENDENT MODELS   a  Structure define   by given PI model  It has been shown  Xiang et a         that there exists a class of probability domain models where proper sub sets of a set of collectively dependent variables display marginal independence  Examples of PI models are parity problems and Modulus addition problems  Xi ang        Several algorithms for learning belief net works have been shown being unable to learn correctly when the underlying domain model is a PI model  A simple PI model with four variables is shown in Ta ble    More examples can be found in  Xiang et a          Table    An example of PI models  X   X   Xa  X                                                                    P N                                                      X  X  X  X                                                                    P N                                             It can be verified that X  and X  are condition ally independent given X  and X   In the sub set  X   X  X    each pair is marginally dependent  e g   P X   X     P X  P X    and is still depen dent given the third  e g   P X IX  X    I P X IX    However  a special dependence relationship exists in the subset  X  X  X    Although each pair is dependent given the third  e g   P XdX   X    f   P XtiX    X  and X  are marginally independent  i e   P Xo X     P Xl P X    so are Xt and X    X  X  X   are said to be pairwise independent but collectively dependent  They form an embedded PI submodel  The minimal I map of this model is shown in Figure    a   Suppose learning starts with an empty graph or struc ture  with all nodes but without any link   A single link lookahead search will not connect X  and X  since the two variables are marginally independent  Nei ther will X  and X  be connected  This results in the learned DMN structure in Figure    b   which is incor rect  On the other hand  if we perform a double link search after the single link search  which can effectively test whether P X IX  X     P XtiX   holds  then the answer will be negative and the two links  Xt  X     b  Structure learned by ingle link search      XI   c  Structure learned by double link  earch  Figure    Comparison of learning results  and   X    X   will be added  The learned DMN struc ture is shown in Figure    c        A MULTI LINK LOOKAHEAD SEARCH ALGORITHM  As our parallel learning algorithm is developed based on a multi link lookahead search algorithm  Xiang et al         the latter is briefly introduced below  Algorithm  Input    Sequential   A  dataset D over a set N of variables  a maximum size    of clique  a maximum number K                   of lookahead links  and a threshold  h   begin initialize an empty graph G     N  E   G   G  for i   to r    do repeat initialize the entropy decrement dh        for each set L of i links   Ln E      do if G     N  E u L  is chordal and Lis im plied by a single clique of size         then compute the entropy decrement dh   if dh    dh   then dh     dh   G     G   if dh     h  then G    G   done    false  else done    true  until done true  return G  end       The search is structured into levels and the number of lookahead links is identical in the same level  Each level consists of multiple passes  Each pass at the same level tries to add the same number i of links  that is  al ternative structures that differ from the current struc ture by i links are evaluated  For instance  level one search adds a single link in each pass  level two search adds two links  and so on  Search at each pass selects i links that decrease the cross entropy maximally after testing all distinct and legal combinations of i links  If the corresponding entropy decrement is significant enough  the i links will be adopted and search contin ues at the same level  Otherwise  the next higher level of search starts  Note that each intermediate graph is chordal as indi cated by the if statement in the inner most loop  The   Chu and Xiang      condition that L is implied by a single clique C means that all links in L are contained in the subgraph in duced by C  This requirement helps to reduce the search space  PARALLEL LEARNING OF     BELIEF NETWORKS  Learning a belief network using a single link lookahead search requires checking of O N   alternative struc tures before a link is added  In an m link lookahead search  O N m  structures must be checked before m links can be added  We view parallel learning as an alternative to tackle the increased complexity in multi link search  as well as to speed up the single link search when the domain is large       alternative graphs based on the current graph  It then partitions these graphs into n sets and distributes one set to each explorer  Each explorer executes Algorithm  Explorer     It checks chordality for each graph re ceived and computes the cross entropy decrement dh  for each valid chordal graph  It then chooses the best graph a and reports dh  and a to manager  Man ager collects the reported graphs from all explorers  se lects the best  and then starts the next pass of search  Algorithm  begin receive D  Nand repeat receive G and  PARALLEL LEARNING  Algorithm   Manager    Input  A dataset D over a set N of variables  a maximum size rJ of clique  a maximum number     rJ TJ       of lookahead links  the total number n of explorers  and a threshold oh for the cross entropy decrement  begin send D  N and    to each explorer  initialize an empty graph G    N  E   G   G   for i      to      do  repeat  end  initialize the cross entropy decrement dh        partition all graphs that differ from G by i links into n sets  send one set of graphs and G to each explorer  for each explorer receive dh  and G   if dh    dh  then dh     dh   G     G   if dh    oh  then G    G   done   false  else done    true  until done   true  send a termination signal to each explorer  return G   As mentioned earlier  our study is performed in an environment where processors communicate through message passing only  vs  shared memory   We par tition the processors as follows  One processor is des ignated as the search manager and the others are net work structure explorers  The manager executes Al gorithm  Manager     It is responsible for generating  end  TJ  from the manager   set of graphs from the manager    and G      G  for each received graph G     N  L U E   do if G  is chordal and L is implied by a single clique of size     r   then compute dh  locally  if dh    dh   then dh     dh   G      G   send dh  and c to the manager  until termination signal is received  initialize dh   ALGORITHMS  We extend Algorithm  Sequential  to parallel learning based on the following observation  at each pass of search  the exploration of alternative structures are coupled only through the current structure  i e   given the current structure  tests of alternative structures are independent of each other  Hence the tests can be performed in parallel    Explorer     a      Figure   illustrates the parallel learning process with two explorers and a dataset of four variables u  v  x and y  Only a single link search is performed for sim plicity  Manager starts with an empty current graph in  a   It sends six alternative graphs in  b  through  g  to explorer   and    Explorer   checks graphs in  b    c  and  d   selects the one in  b   and reports to manager  Explorer   reports the one in  e  to man ager  After collecting the two graphs  manager chooses the one in  b  as the new current graph  It then sends graphs in  i  through  m   Repeating the above pro cess  manager finally gets the graph in  n  and sends graphs in  o  and  p  to all explorers  Since none of them decreases the cross entropy significantly  man ager chooses the graph in  n  as the final result and terminates explorers   tomanager    J    fj   tomanage    n   terminal signal  M   v     y      tennination         to manager to manager   m        I       I  p   lenninalion  Figure    An example of parallel learning of  DMN    Exploring Parallelism in Learning Belief Networks       LOAD BALANCING                The Need of Load Balancing  Balancing load among processors is critical to the ef ficiency of parallel learning  In Algorithm  Manager l   alternative graphs are evenly allocated to explor ers  However  the amount of computation in checking each graph tends to switch between two extremes  If a graph is non chordal  it is ignored immediately with out having to compute the cross entropy decrement  For example  suppose the current graph is shown in Figure    a   There are six graphs that differ from it by only one link  If any of the dotted links in  b  is added to  a   the resultant graph is non chordal  Since the complexity of checking chordality is O INI  lEI   where N is the number of variables and E is the number of edges in the graph  the amount of com putation is very small  On the other hand  if any of the dashed links in  c  is added to  a   the resultant graph is chordal  Since the complexity of comput ing cross entropy decrement by local computation is    n         J log  J         Xiang et a          where n is the number of cases the dataset and    is the maximum size of the cliques involved  the amount of computa tion is much larger  As a result  even job allocation may require significantly different amount of computa tion among explorers  As manager must collect reports from all explorers before a decision on the new current graph can be made  some explorers will be idle while other explorers are completing their jobs  u   J      n       l                                     a   y  X                           y  w   c    b   Figure    Two types of alternative structures  Figure   shows the time taken by each of the six ex plorers in a particular search step  Explorer   takes much longer than others  This illustrates the needs for more sophisticated job allocation strategy in order to improve the efficiency of the parallel system      t  s                     s        N    L L       L L    g                    Figure    The time needed for each explorer       Two stage Loading Method  To improve load balancing  we modify Algorithms  Manager  I  and  Explorer  I  such that jobs are a lo cated in two stages  In the first stage  manager parti tions alternative graphs evenly and distributes one set to each explorer  Each explorer checks the chordal ity for each graph received and reports to manager valid candidates  chordal graphs   Since the amount of computation for checking chordality is small  this stage can be completed quickly and the computation among explorers tends to be even  In the second stage  manager partitions all received graphs evenly and dis tributes one set to each explorer  Each explorer com putes cross entropy decrement for each graph received  It then chooses the best graph c and reports dh  and G  to manager  Manager collects the reported graphs  selects the best  and then starts the next pass of the search  Since all graphs are chordal in the sec ond stage  the degree of load balance mainly depends on the variability of the sizes of the largest cliques       MARGINAL SERVER  During learning  each explorer needs to extract marginal probabilities  marginals  for cliques from the dataset  If each processor must extract marginals by file access each time  the file system will become a bot tleneck  One alternative is to compress the dataset and download one copy at each processor s local memory  This allows us to handle a dataset up to about    MB in our parallel computer  However  when the size of dataset further increases  more sophisticated methods are needed  According to the size of dataset  we partition the avail able processors into one manager  n explorers and m marginal servers  Each server s task is to compute marginals from the data stored in its local memory based on the request of explorers  Servers are con nected logically into a pipeline indexed from   to m  The dataset is partitioned into m     sets  Each server stores one distinct set in its local memory  The last set is duplicated at each explorer s local memory  Algo rithms  Manager    and  Explorer    are modified into Algorithms  Manager      Explorer    and  Server   The manger executes Algorithm  Manager     It per forms data distribution as mentioned above  It then initializes an empty graph and starts the search  It generates alternative graphs based on the current graph  partitions into m   n sets and distributes one set to each explorer and each server  It receives the valid candidates from explorers and servers  partitions them into n sets  and send one to each explorer  It then collects the reported graphs from explorers  se lects the best  sends a signal to each server  and starts   Chu and Xiang      the next pass of search  Algorithm   Manager    D over N variables   Input  A dataset  a maximum size                  of clique  a maximum number       of lookahead links  the total num ber n of explorers  the total numb er m of servers and a threshold oh   begin partition D into m     set s   send one distinct set to each server and broadcast the last set to explorers  initialize an empty graph G    N  E   G    G  fori    to    do repeat initialize the cross entropy decrement dh        partition all graphs t hat differ from G by i links into m   n sets  send one set of graphs and G to each explorer and each server   for each explorer and server  do receive a set of valid graphs  partition all received graphs into n sets  send one set of graphs to each explorer  for each explorer receive dh  and G   if dh    dh  then dh     dh   G     G   if dh    oh  then a   G   done   false  else done        true  send a signal to each server  until done   true  send a signal to each explorer and server  return a  end  Algorithm  Each server executes Algorithm  Server   It does the same as an explorer for checking chordality  It then processes requests from explorers until a signal is re ceived to start the next pass of search  After rec eiving a set of nodes  variables  from an explorer  it computes the sub marginal using its local data  sums with the sub marginal from its preceding server  and passes the sum to the next server or the requesting explorer  Algorithm   Ser v er   begin  receive a set of data over a set maximum size    of clique   receive a set of data over a set maximum size    of clique   N of variables  receive G and a set of graphs from the manager  do for each received graph a    N  L U if G  is chordal and L is implied by a single clique of size   t   then mark it valid  send th e all valid candidates to manager  repeat  E    receive a set of nodes from an explorer  compute the sub marginal of the nodes received  if the server is not server    then  receive the sub marginal from its predecessor  sum the su b  marginal  if the server is not server m  then  send the sub marginal to the next server  else send the marginal to the req ues t ed explorer   until received signal  end  Since each server has to serve n ex plorers   the pro cessing of each server must be n times as fast as an explorer  This means nTm   Te  where Tm and Te are the computing time of each marginal server and each explorer  respectively  Let IDml  IDel  be the size of local data at a s erv er  explorer   Tm and Te can be expressed as Tm   kdiDm I and Te      kgN   kdiDel  where kd and kg are coefficients  N is the total number of variables in the domain  and k N is the computa tion time to get the subgraph for computing the cross entropy decrement  Therefore  we have  receive a and a set of graphs from the manager  initialize dh       and G     G    N  L U E   do if G  is chordal and L is implied by a single clique of size        then mark it valid  send the valid candidates to manager  receive a set of graphs from manager  for each received graph a     N  L U E   do for each clique involved in computing dh   se nd the nodes of the clique to serve rs   compute marginal based on its local data  receive marginal from the server m  sum the two marginals up  compute the entropy decrement dh  locally  if dh    dh   then dh     dh   a    G   send dh  and a t o manager  until received signal  for each r e ce iv ed graph a    Al gorithm   Explorer      of variables and a  until received signal   and a  repeat  Each explorer executes  N  repeat   Explorer     begin  end  based on its local data  receives a sub marginal from the last server and sums them up  It then computes the cross entropy decrement dh  Finally  it reports the best graph c to manager        Recall that  we parti tion the dataset D into  m     sets       It  checks chordality for each graph received and reports to manager the chordal candidates  Next  it receives a set of chordal gr aphs from manager  For each g r aph received  it sends the nodes of each clique involved in computing the cross entropy decrement dh to servers  After sending a request  it computes a sub marginal  Solving equation   and   and W    m   n  where W   W      is the total number of processors available  we get W  aN   IDel  n  aN IDI W  IDI ID el  m  aN IDI   Exploring Parallelism in Learning Belief Networks  N DI IDm I   o    I W         the root of the tree  and an explorer is Tmaz   flog W l l    The maximum number of links be tween an explorer and a server  Tmax  In general  Tmax is smaller than Dma r   For example  if W       then Dmax     but Tmaz      Therefore  the best topology is a ternary tree for our parallel learning al gorithms  Figure   illustrates such a configuration  W hen there is no need for servers  all non root proces sors are explorers     where a is a coefficient presented by a   kg  kd  whose value is between       to       in our exper imental environment  Furthermore  IDe I has to sat isfy IDel       Md  where Md is the maximum local memory available to store the data  For example  T             sec   Tm            sec   and a             in learning the ALARM network with n      m      IDel   O OBMB  and IDml          MB  If learning is performed on a large domain with a very large dataset  more marginal servers are needed  As an example  suppose IDI   lOOMB  N         W       and o           If we choose IDel       MB  we obtain n      m      IDml          MB  where nand mare rounded to the nearest integers   Host Computer                 Explorers  EXPERIMENTAL RESULTS     Figure    A ternary tree configuration       CONFIGURATION  T he previous algorithms are implemented on an ALEX AVX Series   parallel computer with a MIMD distributed memory architecture  It has      M Hz processors each with    MB local memory and can be directly linked to at most four others  Communication among processors are through message passing at    Mbps for simplex and    Mbps for duplex communi cation  Message passing time increases as the number of links between the communicating processors and the length of the message  Table   shows the relation of the mes sage passing time with the message length and the number of links between the communicating proces sors  It is important to link the processors such that the number oflinks involved in each message passing is minimized  Since each processor has up to four links and communication is to be performed among man ager  explorers and servers  candidate topologies are a  D mesh and a ternary tree        RESULTS AND PERFORMANCE ANALYSIS  Our experiments are intended to check the correctness of the parallel algorithms and the speed up through parallelism  A network structure learned from a dataset generated from the Alarm network  Beinlich et a         is shown in Figure    The result obtained using the parallel algorithms is identical with that obtained using the sequential algorithm   Table    Message passing time Length bytes         link  O QlS    links                                   links  links     links    links                                                                                                                                 In a  D mesh with W processors  the maximum num ber of links between any two processors is Dmax      vW     In a ternary tree with W processors  the maximum number of links between manager  as    Figure    The learned Markov network  Alarm We generated four control PI models and tested us ing the parallel algorithms with single link and multi link lookahead search  The model PIMl has    vari ables and contains one PI sub model of three variables  PIM  and PIM  have    and    variables  respec tively  Each contains two PI sub models each of which has three variables  PIM  has    variables and con tains a PI sub model of four variables  The datasets   Chu and Xiang      of cases are generated by sampling these models with                     and       cases  respectively  For each dataset  our parallel algorithms were able to learn an approximate I map of the control model  The network learned from PIM  is shown in Figure    The two subsets of variables involved in the two P I sub models are   x   XB  xg   and  x    Xts  Xi     respec tively  Using a single link lookahead search  the dashed links  corresponding to the two P I sub models  are missing  hence not an I map  in the learning outcome  Using a triple link lookahead search  the structure is learned correctly            I                                      network in       seconds  The parallel program took      seconds using   processors       seconds using   processors  and      seconds using   processors  The speed up is very small as the number of processors increases and the efficiency is very low  The speed up with four processors was even lower than with three processors  This appears to be due to the bottleneck in file access and the corresponding increase in overhead  The result suggests that the file access method should be avoided due to the extensive data access needed during learning  When the entire dataset can be loaded into the local memory  about    MB is available in our environ ment  of each processor  loading the dataset to the memory is performed once for all and each marginal can be extracted directly from the memory  We refers to this as the memory access method  We conducted a comparison between even job allocation and two stage job allocation using the memory access method  Table    Results on even and two stage job allocation n    Figure    The structure learned from a P I model P IM  Next  we present the performance result  The perfor mance of a parallel program are commonly measured by speed up  S  and efficiency  E   Given a particular task  let T l  be the execution time of a sequential program and T W  be that of a parallel program with W processors  The two measurements are defined as S   T l  T W  and E   SjW  In practice  the performance of a parallel program is affected by many factors  For our learning problem  the size of the input data is not trivial  Hence how the dataset is accessed by processors affects the per formance significantly  Our experiments were intended to test the learning program using different data access methods  We also tested different job allocation meth ods which also affect the performance significantly  For efficient I   and storage  the original dataset of cases are converted into a compressed frequency table where the value of each variable is represented by one byte  This file is used as the input to the learning program  In the parallel computer we used  file access by all processors must be performed through a special processor  The simplest way for a learning program to extract marginals from the data is to access the file directly for each marginal  This file access method was tested using a dataset of       cases generated from the ALARM network  The sequential program  one processor  learned the                            Even loading tJmels                                                                                                                    t                                                                        two stage loading t  time ls                                                                                                                                                                                          The experiment used a dataset of       cases gener ated from the ALARM network  The result is shown in Table    Each row is the result obtained by using n explorers as indicated in the first column  Columns   through   present the result obtained with even job allocation and columns   through   present the result obtained with two stage job allocation  Columns   and   show that as the number of explor ers increases  the speed up increases as well with either job allocation method  It demonstrates that our paral lel algorithm can effectively reduce the learning time  This provides positive evidence that parallelism is an alternative to tackle the computational complexity in learning large belief networks  Comparing column   with   and column   with    it can be seen that the two stage allocation further speeds up the learning process and improves the effi ciency compared with even job allocation  For exam ple  when eight explorers are used  the speed up is            Exploring Parallelism in Learning Belief Networks  and efficiency is       for even allocation  and      and       for two stage allocation   creased computational complexity in learning PI mod els   The result also shows a gradual decrease in efficiency as the number of explorers increases  This efficiency decrease is mainly due to the job allocation overhead  Manager must allocate job to each explorer sequen tially at the beginning of each search step  Therefore  each explorer is idle after its report in the previous search step is submitted and before the next job is assigned to it   When the size of dataset is beyond the available lo cal memory of each processor  we suggest the use of marginal servers  Comparison of using different num ber of servers was performed in learning ALARM net work  Figure   shows the speed up comparison and Figure   shows the efficiency comparison  The verti cal axis is labelled by S for speed up or E for efficiency  The horizontal axis is labelled by W   m   n  where m is the number of marginal servers and n is the num ber of explorers  De is the size of data stored in the local memory of each explorer  and Dm is that of each server  The speed up is calculated using sequential learning with file access as this is considered the alter native when marginal server is not used   Table    Results on learning PI n        T  S  E T min   T min     t           PIMl                      T min          T min                              s J    s E  PIM                                                     models  PIM                                                          PIM                                                          s            m    De  Dm  The second row shows the computation time in sequen tial learning  It increases from PIM   to PIM   This is because the increased size of the domain for PIM   through P IM                variables  and the increased size of the dataset                        PIM  used the most computation time because it has a PI sub model of   variables  which requires six link lookahead search  For all models  the speed up increases as more explor ers are employed  On the other hand  when more ex plorers are used  PIM  has the fastest decrease in ef ficiency and PIM  has the slowest decrease with the other two models in between  This is highly correlated with the increase of computation time from PIM   to PIM   This is because as the search space becomes larger  the number of alternative graphs to be explored in each job allocation becomes larger  The conse quence is that the message passing overhead becomes less significant compared with the search time and hence the efficiency improves  This result shows that parallel learning are quite suited for tackling the in     o m     De     Dm       D            Table   lists the experimental result in learning the four PI models mentioned above  Triple link locka head search is used for learning PIM    PIM  and P IM   respectively  Six link lookahead search is used for learning PIM   The first column indicates the number of explorers used  Each row shows computa tion time  speed up or efficiency as indicated by the second column  Each of the last four columns shows the result for learning one PI model      D rn    De   Drn       D                  D  II                      w             Figure    Speed up by using marginal servers       E                                 D    D         D              D  m    De  Dm    m    De    Dm             D  m   De   Dm w                     II      Figure    Efficiency by using marginal servers In the Figure    the maximum efficiency is       for m    n     and De   Dm        form     n     and De    Dm  and       for m      n     and De    Dm  The corresponding speed up is              and        respectively  The speed up is more than the number of processors since marginal servers allow much faster memory access compared with the file access when a single processor is used             Chu and Xiang  REMARKS  We have studied parallelism in learning to tackle the increased computational complexity in learning belief networks in difficult domains  PI models  as well as in learning from large domains  Parallel algorithms were proposed that decompose the learning task such that multiple processors can be used without incurring ad ditional error  In order to improve the efficiency of the parallel system  we proposed a two stage job allocation method to handle the variation in computation time in searching different candidate networks  In order to overcome the bottleneck by file access  we proposed the parallel learning algorithm using marginal servers  This allows fast memory access of data when the size of the dataset is much larger than the local memory of each processor  The parallel learning algorithms are implemented on an AVX Series   parallel computer with a MIMD distributed memory architecture  Our experimental result showed that parallel learning can effectively speed up learning PI models as well as learning non PI models in large domains  Acknowledgements  This work is supported by grants OGP         CRD       from the Natural Sciences and Engineer ing Research Council of Canada  and by the Institute for Robotics and Intelligent Systems in the Networks of Centres of Excellence Program of Canada  
 Real World  Current Bayesian net representations do not consider structure in the domain and include all variables in a homogeneous network  At any time  a human reasoner in a large do main may direct his attention to only one of a number of natural subdomains  i e   there is  localization  of queries and evidence  In such a case  propagating evidence through a homogeneous network is inefficient since the entire network has t o be updated each time  This paper presents multiply sectioned Bayesian networks that enable a  localization preserving  representation of natural subdo mains by separate Bayesian subnets  The subnets are transformed into a set of perma nent junction trees such that evidential rea soning takes place at only one of them at a time  Probabilities obtained are identical to those that would be obtained from the homo geneous network  We discuss attention shift to a different junction tree and propagation of previously acquired evidence  Although the overall system can be large  computational requirements are governed by the size of only one junction tree         Domain Gather Evidence  Evidence     Figure  Recommen   dations      An illustration of the context  a property of large domains with respect to human cognitive activity  There are fixed  natural  subdomains  At any time  a human reasoner focuses attention at only one of them  He can acquire evidence from and form queries about only one of them at a time  He may shift attention from one to another from time to time   WHAT IS LOCALIZATION   localization  t  System User  System  LOCALIZATION  define informally what we will call   t  Actions  Expert  We consider the following general context where an ex pert system is to be used  Figure     The human user plays the central role between an expert system and a real world domain  To know the state o f the domain  e g  diagnosis   the user gathers evidence from the domain and enters the evidence along with queries to the expert system  The system may provide a recom mendation or may prompt further gathering of infor mation  We would like the system to be efficient in inference computation  How can this aim be realized when the domain is large  We  Queries   t  as  Localization can be seen in many large domains  For example  in many medical domains  e g  neuromuscu lar diagnosis  Xiang et al         medical practitioners acquire information about a patient by history taking  physical examination  and performing a set of special ized tests  Each such activity involves a subdomain which contains possibly dozens of alternative ques tions  procedures  test sites  etc   and diseases which can be differentiated  Each such subdomain can hold the person s attention for a period of time  During this period  he updates his belief on disease hypothe ses based on acquired evidence  carefully weighs the   Exp l ori ng Localization  importance of alternative means to gather information under the current situation  and selects the best alter native to perform next  Although one subdomain may have an influence on another  the influence is summa rized by the common disease hypotheses which they both can  partially  differentiate  We distinguish  interesting  variables from  relevant  variables  Call the set U of variables in a subdomain  interesting  to the human reasoner if this subdomain captures his current attention  A set V of variables outside the current subdomain may have a bearing on U due to two reasons  First  the background knowl edge on U may provide partial background knowledge on V  Second  obtained evidence on U may change one s belief on V  Thus V is  relevant  to U but is not currently  interesting  to the reasoner  However  we can often find a set I  I C U  of variables which summarizes all the influence on U from V such that V can be discarded from the reasoner s current attention  This issue is treated more formally in Section           IS LOCALIZATION USEFUL   W hen localization exists  a large domain can be rep resented in an expert system according to the natu ral subdomains  Each subdomain is represented by a subsystem  During a consultation session  only the subsystem corresponding to the current subdomain consumes computational resources  This subsystem is called active  When a user s attention shifts to a differ ent subdomain  the evidence acquired in the previously active subsystem can be absorbed by the newly ac tive subsystem through summarizing variables  Since no computational resources are consumed by the in active subsystems  computational savings can be ob tained without loss of inference accuracy  Our observation of localization was made based on our experience in the domain of neuromuscular diagnosis  ib    We believe that it is common in many large do mains  How to exploit it when it arises  how to con struct the above ideal representation in the context of Bayesian networks  and how to guarantee the correct ness of inference in the representation is the subject of this paper     in Bayesian Networks for Large Expert Systems  halter       Jensen  Lauritzen and Olesen     a  Baker and Boult       Suermondt  Cooper and Heck erman        This paper takes the exact approach  For general but sparse nets  efficient computation has been achieved by creating a secondary directed  Lauritzen and Spiegel halter        or undirected clique tree  junction tree   Jensen  Lauritzen and Olesen     a  structure  which also offers the advantage of trading compile time with running time for expert systems  Both methods and many others are based on a net representation which does not consider domain structure and lumps all vari ables into a homogeneous network  Pruning Bayesian nets with respect to each query in stance is another exact method with savings in com putational cost  Baker and Boult        The method does not support incremental evidence  i e  all evi dence must be entered at one time   Heckerman      b  partitions Bayesian nets into small groups of naturally related variables to ease the con struction of large networks  But once the construction is finished  the run time representation is still homo geneous  Suermondt  Cooper and Heckerman        combine cutset conditioning with the clique tree method and convert the original net into a set of clique trees to obtain computational savings  The cutset is chosen mainly based on net topology  It does not lead to the exploration of localization in general        OBVIOUS  WAYS TO EXPLORE LOCALIZATION  Splitting homogeneous nets  One obvious way to explore localization is to split a homogeneous Bayesian net into a set of subnets ac cording to localization  Each subnet can then be used as a separate computational object  This is not always workable as is shown by the following example   EXPLORE LOCALIZATION IN BAYESIAN NETS       B ACKGROUND  Cooper        has shown that probabilistic inference in a general Bayesian net is NP hard  Several different approaches have been pursued to avoid combinatorial explosion for typical cases  and thus to reduce compu tational cost  Two classes of approaches can be identi fied  One class explores approximation  Hention       Pearl       Jensen and Andersen        Another class explores specificity in computing exact probabilities  Pearl       Heckerman     a  Lauritzen and Spiegel   Figure    Left  a DAG D  Right  A set of subnets formed by sectioning D  Suppose the directed acyclic graph  DAG  D in Figure   is split according to localization into  Dl  D   D    Suppose variable G is instantiated by evidence  According to d separation  Pearl        now             Xiang  Poole  and Beddoes  both paths between E and F are active  Therefore  in order to pass a new piece of evidence on E to F  the joint distribution on   B  C  needs to be passed from  D  D   to D    However  this is not possible because neither D  nor D  contains this joint distribu tion  This shows that arbitrarily partitioning Bayesian nets causes loss of information and is incorrect in gen eral   Figure      An unsectioned Bayesian net   Splitting the junction tree  Another obvious way to explore localization is to pre serve localization within subtrees of a junction tree  Jensen  Lauritzen and Olesen     a  by clever choice in triangulation and junction tree construction  If this can be done  the junction tree can be split and each subtree can be used as a separate computational ob ject  The following example shows that this is also not always workable  Consider the DAG   in Figure    Suppose variables in the DAG form three naturally related groups which satisfy localization  Gr G  Ga           Ar A  Aa Hr H  Ha H    FI F  HI H    E  E     H   H  Ho    We would like to construct a junction tree which would preserve the localization within three subtrees  The graph  pin Figure   is the moral graph of    Only the cycle A    H    E  E    H    A  needs to be trian gulated  There are six distinct ways of triangulation out of which only two do not mix nodes in different groups  The two triangulations have the link  Ha H   in common but they do  wt make a significant differ ence in the following analysis  The graph A in Figure   shows one of the two triangulations  All the cliques in A appear as nodes of graph r     The junction tree r does not preserve localization since cliques         and   correspond to group G  but are connected via cliques   and   which contain Ea from group Ga  This is unavoidable  When there is evi dence for A  or A  in A  updating the belief in group G  requires passing the joint distribution of H  and   Passing only the marginal distributions on B and on C is not correct   r  S  Figure     P  the moral graph of   in Figure    A  a triangulated graph of ci   r  the junction tree con structed from A  H  But updating the belief in Aa only requires pass ing the marginal distribution of H   That is to say  updating the belief in A  needs less information than group G  In the junction tree representation  this be comes a path from cliques      and   to clique   via cliques   and     In general  let X and Y be two sets of variables in the same natural group  and let Z be a set of variables in a distinct group  Suppose the information exchange between pairs of them requires the exchange of distri bution on sets lxy  lxz and lyz of variables respec tively  Sometime lxy is a subset of both lxz and  yz  When this is the case  a junction tree representation will always indirectly connect cliques corresponding to X and Y through cliques corresponding to Z if the method in Jensen  Lauritzen and Olesen       a  is fol lowed  A brute force method  There is  however  a way around the problem with a brute force method  In the above example  when there is evidence for A  or A   the brute force method pre tends that updating the belief in A  needs as much information as Ga  What one does is to add a dummy link  H  A   to the moral graph  I  in Figure    Then   Exploring Localization in Bayesian Networks for Large Expert Systems  triangulating the augmented graph gives the graph A  in Figure    The resultant junction tree r  in Fig ure   does have three subtrees Which correspond to the three groups desired  However  the largest cliques now have size four instead of three as before  In the bi nary case  the size of the total state space is    instead of    as before  In general  the brute force method preserves natural lo calization by congregation of a set of interfacing nodes  nodes H   H   H  above  between natural groups  In this way  the joint distribution on interfacing nodes can be passed between groups  and preservation of lo calization and preservation of tree structure can be compatible  However  in a large application domain with the original network sparse  this will greatly in crease the amount of computation in each group due to the exponential enlargement of the clique state space  The required increase of computation could outweigh the savings gained by exploring localization in general      MULTIPLY SECTIONED BAYESIAN NETS  This section introduces a knowledge representation formalism  Multiply Sectioned Bayesian Networks  MSBNs   as our solution to explore localization  We want to partition a large domain according to nat ural localization into subdomains such that each can be represented separately by a Bayesian subnet  Each subnet then stands as a computational object  and dif ferent subnets cooperate with each other during atten tion shift by exchanging a small amount of information between them  We call such a set of subnets a MSBN  The construction of a MSBN can be formulated con ceptually in the opposite direction  Suppose the do main has been represented with a homogeneous net work  We frequently refer to a homogeneous net as an UnSectioned Bayesian network  USBN   A MSBN is a set of Bayesian subnets resulted from the sectioning of the corresponding USBN  For example the DAG   in Figure   is sectioned into             in Figure   according to the localization described in Section      A variable shared by  ad jacent  subnets appears in both subnets  The set of shared variables is subject to a technical constraint  in addition to localization  as will be discussed in Sec tion       Figure    A  is a triangulated graph  r  is a junction tree of A   The trouble illustrated in the above two situations can be traced to the tree structure of a junction tree repre sentation which requites a single path between any two cliques in the tree  In the normal triangulation case  one has small cliques but one loses localization  In the brute force case  one preserves localization but one does not have small cliques  To summarize  the preser vation of natural localization and small cliques can not coexist by the method of Andersen et al       J and Jensen  Lauritzen and Olesen       a   It is claimed here that this is due to a single information channel between local groups of variables  This paper present a representation which  by introducing multiple infor mation channels between groups and by exploring con ditional independence  allows passing the joint distri bution on a set of interfacing variables b e tween groups by passing only marginal distributions on subsets of the set   Figure    The set              forms a MSBN for the USBN   in Figure          TRANSFORMATION OF MSBNS INTO JUNCTION FORES TS  The junction tree representation  Andersen et al        Jensen  Lauritzen and Olesen     a  allows efficient computation for general but sparse networks in expert systems  Thus it is desirable to transform each subnet of a MSBN into a junction tree  The resultant set of junction trees is called a junction forest  For example  the MSBN             in Fiure   is transformed into the junction forest  r   r  r   in Figure    Omit the ribbed bands for the moment which will be introduced shortly  In order to propagate evidence between junction trees during attention shift  information channeJs need to  be created between them  As discussed in Section      multiple channels are required in general to preserve             Xiang  Poole  and Beddoes  probabilities in this tree are the same as in a globally consistent junction forest  Xiang  Poole and Beddoes        It is this feature of MSBNsfjunction forests that allows the exploitation of localization   Figure    The set  f   rz  f   is a junction forest transformed from the MSBN          e   in Figure    Linkages between junction trees are shown by ribbed bands  both localization and a small clique size  These chan nels are called linkages between the two junction trees being connected  Each linkage connects two cliques in different trees  The two cliques being connected are called the host cliques of the linkage  A linkage is the intersection of its two host cliques  Host cliques are selected such that the union of linkages is the set of in terfacing variables between the corresponding subnets  and each linkage is maximal  With multiple linkages created between junction trees  we have a linked junc tion forest   For example  in Figure    linkages between junction trees are indicated with ribbed bands connecting the corresponding host cliques  There is only one linkage between clique   of f  and clique   of f   namely   H  H    The two linkages between f  and f  are  H   H   and  H   H    Up to here  we have only discussed the manipula tions of graphical structures of a MSBN  As other approaches based on secondary structures  Lauritzen and Spie elhalter       Jensen  Lauritzen and Ole sen     aj  there needs to be a corresponding conver sion from the probability distribution of the MSBN to the belief table  potential  of the linked junction for est  Readers are refened to Xiang  Poole and Beddoes        for details regarding the conversion       EVIDENTIAL REASONING  Afte  a l inked junction fores t is created  it becomes the permanent representation of the corresponding MSBN  The evidential reasoning during a consultation session will be performed solely in the junction forest  Due to localization  only one junction tree in a junc tion forest is active during evidential reasoning  When new evidence becomes available to the currently active junction tree  it is entered and the tree is made consis tent  The operations to enter evidence and to main tain consistency within a junction tree are the same as Jensen  Lauritzen and Olesen      a   We only main tain consistency in the currently active tree  All the  When the user shifts attention from the currently ac tive tree to a  destination  tree  all previously acquired evidence is absorbed through an operation ShiftAt tention  The operation swaps in and out sequentially a chain of  intermediate  junction trees between the currently active tree and the destination tree  It has been shown  ib   that  with a properly structured junc tion forest  the following is true  Start with any active junction tree in a globally consistent junction forest  Repeat the following cycle a finite number of times     Enter evidence to the currently active tree and make the tree consistent a fi nite number of times     Use ShiftAttention to shift attention to any destination tree  The marginal distributions obtained in the final active tree are identical to those of a globally consistent forest  The above property shows the most important charac terization of MSBNs and junction forests  namely  the capability of exploiting localization to reduce the com putational cost  Note that the above statement only requires the initial global consistency of the junction forest  With localization  the user s interest and new evidence remain in the sphere of one junction tree for a pe riod of time  Thus the time and space requirement  while reasoning within a junction tree  is bounded above by what is required by the largest junction tree  The judgments obtained take into account all the rele vant background knowledge and evidence  Compared to the USBN and the single junction tree representa tion where each piece of evidence has to be propagated through the entire system  this leads to computational savings  When the user shifts interest to another set of vari ables contained in a different destination tree  only the intermediate trees need to be updated  The time required is linear to the number of intermediate trees and to the number of linkages between each pair of neighbours  ib    No matter how large the entire junc tion forest  the time requirement for attention shift is fixed once the destination tree and mediating trees are fixed  The space requirement is upper bounded by what is needed by the largest junction tree  With localization  the computational cost for attention shift is incurred only occasionally  Given the above analysis  the computational complex ity of evidential reasoning in a MSBN with jJ subnets of equal size is about      of the corresponding USBN system given localization  The actual time require    Exploring Localization in Bayesian Networks for Large Expert Systems  ment is a little more than    f  due to the computation required for attention shift  The actual space require ment is a little more than     f  due to the repetition of interfacing nodes   Section     has shown that we cannot divide a homo geneous Bayesian net or its junction tree arbitrarily in order to explore localization  This section discusses major technical issues in the MSBN junction forest representation   For example  the sectioning in Figure   satisfies the d sepset condition  but the resultant MSBN does not guarantee correct inference  This is because the sec tioning has an unsound overall organization of subnets  Intuitively  the overall structure of a MSBN should en sure that evidence acquired in any subnet be able to propagate to a different subnet by a unique chain of subnets  In the example of Figure    after a piece of evidence is available on G  a new piece of evidence on E has to propagate to F through two different chains of subnets D    D  and D    D    D  This violates the above requirement and causes the problem  The issue of overall structure is treated formally in  ib J                TECHNICAL ISSUES  INTERFACE BETWEEN S UBNETS  Localization does not dictate exactly what should be the boundary between different subnets  The intuitive criterion is that the interface should allow evidence acquired to be propagated to adjacent subnets dur ing attention shift by a small amount of information exchange  We define d sepset as the criterion of inter face  which makes use of Pearl s d separation concept  Pearll      We denote the union D of DAGs D  and D  by D   D  UD     N  uN  E  u E    Definition      d sepset  Let D   D  U D  be a DAG  The set of nodes I   N  n N  is a d sepset between subDAG D  and D  if the following condition holds  For every A  E I with its parents      in either            N   or            N   D   Elements of a d sepset are called d sepnodes  When the above condition holds  D is said to be sectioned into  Dl D     The following theorem and corollary  Xiang  Poole and Beddoes        say that a d sepset d separates subnets in a MSBN and is a sufficient information channel   MORAL I TRIANGULATION BY LOCAL COMPUTATION  Transformation of a MSBN into a junction forest requires moralization and triangulation conceptually the same way as the other approaches based on sec ondary structures  Lauritzen and Spiegelhalter       Andersen et al        Jensen  Lauritzen and Olesen      a    However  in the MSBN context  the transfor mation can be performed globally or by local compu tation at the level of the subnets  The global compu tation performs moralization and triangulation in the same way as the other approaches with care not to mix the nodes of distinct subnets into one clique  An addi tional mapping of the resultant moralized and triangu lated graph into subgraphs corresponding to the sub nets is needed  But where space saving is concerned  local computation is desired  Since the number of parents for a d sepnode may be different for different subnets  the moralization in MSBN cannot be achieved by  pure  local computation in each subnet  Communication between the subnets is required to ensure that the parents of d sepnodes are moralized identically in different subnets   The d sepset criterion concerns with the interface be tween each pair of subnets  This is not sufficient for a workable MSBN   The criterion of triangulation in a MSBN is to en sure the  int ac tness   of a resulting hypergra ph f rom the corresponding homogeneous net  Problems arise if one insists on triangulation by local computation at the level of subnets  One problem is that an inter subnet cycle will be triangulated in the homogeneous net  but the cycle cannot be identified by examining each of the subnets involved individually  Another problem is that cycles involving d sepnodes may be triangulated differently in different subnets  The so lution is to let the subnets communicate during tri angulation  Since moralization and triangulation both involve adding links and both require communication between subnets  the corresponding local operations in each subnet can be performed together and messages to other subnets can be sent together  Therefore  oper ationally  moralization and triangulation in MSBN are not separate steps as in the single junction tree repre sentation  The corresponding integrated operation is t ermed morali triangulation to reflect this fac t    They are simplified here to the MSBN of two subnets   For example  the MSBN in Figure   is morali   Theorem     Let a DAG   D   D   and I   N  nN  be N    I from N    I   a  D be sectioned into d sepset  I d separates  Co r ollary     Let  D  P  be a Bayesian net  D be sectioned into   D   D    and I   N  n N  be the d sepset  When evidence is available at variables in N   the propagation of the joint distribution on I from D  to D   is sufficient in order to obtain posterior distri bution on N       OVERALL S TRUCTURE OF MSBNS             Xiang  Poole   and Beddoes  Figure    Morali triangulated graphs of the MSBN in Figure    The meaning of the different line types is explained in Section      triangulated to the graphs in Figure    Thin solid lines   e g   A   HI   are from the original arcs by drop ping directions  Thin dotted lines  e  g    A   A    are links added by local moralization  Thick dotted lines  e g   H  H    are  moral  links added through com munication  Thin dashed lines  e g   H  H    are added through communication for triangulation  The thick solid line   Ea  H    is added by local triangula tion  A formal treatment and an algorithm for morali triangulation are given in Xiang  Poole and Beddoes              PROPAGATING INFORMATION THROUGH MULTIPLE L INKAGES  Propagating information between junction trees of a junction forest is required in two different situations  belief initialization and evidential reasoning  In both cases  information needs to be propagated between junction trees of a junction forest through multiple linkages  Care is to be taken against potential errors   Belief initialization serves the same purpose as in other approaches based on secondary structures  Lauritzen and S iegelhalter       Jensen  Lauritzen and Olesen     a   It establishes the global consistency before any evidence is available  This requires the propagation of knowledge stored in each junction tree to the rest of the forest  When doing so  redundant information could be passed through multiple linkages  We must make sure that the information is passed only once   f  In evidential reasoning  evidence acquired in one junc tion tree needs to be propagated to the destination tree during attention shift  The potential error in this case takes a different form from the case of initializa tion  Passing information through multiple linkages from one junction tree to another can  confuse  there ceiving tree such that the correct consistency between  the two cannot be established  Detailed illustrations of these potential problems and the operations which avoid them are given in Xiang  Poole and Beddoes            CONCLUSION  This paper overviews MSBNs and junction forests as a flexible knowledge representation and as an efficient inference formalism  This formalism is suitable for ex pert systems which reason about uncertain knowledge in large domains where localization exists  MSBNs allow partitioning of a large domain into smaller natural subdomains such that each of them can be represented as a Bayesian subnet  and can be tested and refined individually  This makes the representa tion of a complex domain easier for knowledge engi neers and may make the resultant system more natural and more understandable to system users  The mod ularity facilitates implementation of large systems in an incremental fashion  When partitioning  a knowl edge engineer has to take into account the technical constraints imposed by MSBNs which are not very re strictive  Each subnet in the MSBN is transformed into a junc tion tree such that the MSBN is transformed into a junction forest where evide ntial reasoning takes place  Each subnet junction tree in the MSBN  junction for est stands as a separate computational object  Since the representation allows transformation by local com putation at the level of subnets  and allows reason ing to be conducted with junction trees  the space re quirement is governed by the size of the largest sub net junction tree  Hence large applications can be built and run on relatively small computers wherever hardware resources are of concern  This was  in fact    Exploring Localization in Bayesian Networks for Large Expert Systems  our original motivation for developing the MSBN rep resentation  During a consultation session  the MSBN representa tion allows only the  interesting  junction tree to be loaded while the rest of the forest remains inactive and uses no computational resources  The judgments made on variables in the active tree are consistent with all the knowledge available  including both prior knowl edge and all the evidence contained in the entire forest  When the user s attention shifts  inactive trees can be made active and previous accumulation of evidence is preserved  This is achieved by passing the joint beliefs on d sepsets  The overall computational resources re quired are governed by the size of the largest subnet  and not by the size of the application domain  The MSBN has been applied to an expert system PAINULIM for diagnosis of neuromuscular diseases characterized a painful or impaired upper limb  Xi ang et al          br   The MSBN representation makes the localization as sumption about the large domain being represented  Our justification of the generality of localization has been intuitive and has been based on our experience in PAINULIM  We are prepared to test its generality in other large domains  Acknowledgements  This work is supported by Operating Grants A      OGP        and OGP        from NSERC  and CRD     from the Centre for Systems Science at SFU  We are grateful to Stefan Joseph and anonymous re viewers for helpful comments to an earlier draft  
