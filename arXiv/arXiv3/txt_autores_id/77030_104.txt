 Bayesian reinforcement learning  BRL  encodes prior knowledge of the world in a model and represents uncertainty in model parameters by maintaining a probability distribution over them  This paper presents Monte Carlo BRL  MC BRL   a simple and general approach to BRL  MC BRL samples a priori a finite set of hypotheses for the model parameter values and forms a discrete partially observable Markov decision process  POMDP  whose state space is a cross product of the state space for the reinforcement learning task and the sampled model parameter space  The POMDP does not require conjugate distributions for belief representation  as earlier works do  and can be solved relatively easily with pointbased approximation algorithms  MC BRL naturally handles both fully and partially observable worlds  Theoretical and experimental results show that the discrete POMDP approximates the underlying BRL task well with guaranteed performance      Introduction A major obstacle in reinforcement learning is slow convergence  requiring many trials to learn an effective policy  Model based Bayesian reinforcement learning  BRL  provides a principled framework to tackle this difficulty  To speed up convergence  BRL encodes prior knowledge of the world in a model  It explicitly represents uncertainty in model parameters by maintaining a probability distribution over them and chooses actions that maximize the expected long term reward with respect to this distribution  One approach to BRL is to cast it as a partially observable Markov decision process  POMDP  P  Duff         The state of P Appearing in Proceedings of the    th International Conference on Machine Learning  Edinburgh  Scotland  UK        Copyright      by the author s  owner s    is a pair  s     where s is the discrete world state for the reinforcement learning task and  is the unknown continuous model parameter  POMDP policy computation automatically analyzes both aspects of each action  its reward and its contribution towards inferring unknown model parameters  thus achieving optimal trade off between exploration and exploitation  Despite its elegance  this approach is not easy to use in practice  Since model parameters are continuous in general  P has a hybrid state space and requires the restrictive assumption of conjugate distributions to represent beliefs during the policy computation  Duff        Poupart et al         Ross et al         Poupart   Vlassis         We propose Monte Carlo Bayesian Reinforcement Learning  MC BRL   a simpler and more general approach to BRL  based on the following observation  although there are infinitely many parameter values  it may be possible to compute an approximately optimal policy without considering all of them  if the objective is good average performance with respect to a prior distribution b P of model parameters  We sample a finite set of values from b P and form a discrete POMDP P whose state is  s     with  taking values from the sampled set only  This discrete POMDP P approximates the hybrid POMDP P  P does not require conjugate distributions for belief representation and can be solved much more easily with existing point based approximation algorithms  e g    Kurniawati et al          MCBRL also naturally handles both fully and partially observable worlds  We show that MC BRL is approximately Bayes optimal with a bounded error in the average case  The outputsensitive bound indicates that if a small approximately optimal policy exists  then a small number of samples is sufficient for P to approximate P well  In other words  if we treat P as a generalization of P with a richer model parameter space  a small policy results in better generalization  This nicely mirrors similar results in learning theory  We also provide experimental results evaluating MC BRL on four distinct domains  including one from an application in   Monte Carlo Bayesian Reinforcement Learning  autonomous vehicle navigation      Background      MDP and POMDP An MDP is a tuple hS  A  T  R  i  where S is a set of world states  A is a set of actions  T  s  a  s    specifies the transition probability of reaching state s  when taking action a in state s  R s  a  s    specifies the reward received when taking action a in state s and reaching state s    and  is a discount factor  A policy    S  A for an MDP is a function that specifies which action to take in each state s  S  The value of a policy  is defined as the expected cumulative discounted reward    X t E  R  st    st    st       t    where the expectation is with respect to the random variable st   the state at step t  The aim of the MDP is to find an optimal policy    with maximum value  MDPs assume that the agent can directly observe the world state  POMDPs generalize MDPs by allowing partially observable states  Formally  a POMDP is a tuple hS  A  O  T  Z  R  i  where S  A  T   R   are as defined in the case of MDP  O is a set of observations  and Z s    a  o  is the observation function that specifies the probability of observing o when action a was taken in the previous step and the current state is s    In a POMDP  the agent does not know for sure its state  Instead  it maintains a probability distribution or belief b s  over the state space S  A policy    B  A for a POMDP is a mapping from the belief space to actions  The value of  at a belief b is defined as    X t V  b    E  R  bt    bt    bt       b    b   t    where the expectation is with respect to the random variable bt   the belief at step t  Given an initial belief b    the aim of the POMDP is to find an optimal policy    with maximum value at b         Related Works One common approach to BRL adopts the proposal in  Duff        and casts BRL as a POMDP P with a hybrid state space  Wang et al         Poupart et al         Ross et al         Castro   Precup        Poupart   Vlassis        Ross   Pineau         To maintain the posterior belief of continuous model parameters  it requires either a closed form representation or effective approximate inference techniques  Instead of solving P directly  MC BRL  approximates it with a discrete POMDP P by sampling from the prior distribution and takes advantage of the recent advances in point based discrete POMDP algorithms  This way  we avoid the restrictive assumption of close form belief representation and obtain a simpler and more general approach  Sampling has been used extensively in BRL  Castro   Precup        Ross et al         Poupart   Vlassis        Ross   Pineau        Asmuth et al          However  the earlier works draw samples from the posterior distributions to speed up planning for P or to maintain beliefs efficiently  This is conceptually different from our approach  which samples hypotheses from the model parameter space a priori to form P and works exclusively with the sampled hypotheses afterwards  Our theoretical result shares a similar idea with that for the  PO MDP algorithm PEGASUS   Ng   Jordan         The PEGASUS analysis bounds the number of samples required to find a good policy in a policy class with finite VC dimension  Our result does not assume such a policy class  It provides an output sensitive bound that depends on the size of the policy actually computed  instead of a worst case bound for all policies in a class      Monte Carlo BRL      BRL as POMDP To simplify the presentation  let us first consider BRL of an MDP  Given an MDP hS  A  T  R  i  the task of BRL is to find an optimal policy when the transition function T is unknown  Let     sas   s  s   S  a  A  denote the collection of unknown parameters of the MDP  where sas    T  s  a  s     It has been shown that the BRL problem can be formulated as a POMDP P   hSP   AP   OP   TP   ZP   RP     b P i  Duff         The state space SP   S   is the cross product of the MDP states S and the parameter space   A state  s    consists of a world state s of the MDP and a hypothesized value  of the unknown parameter  The actions AP are identical to the actions A in the MDP  Assuming the parameter  does not change over time  the transition function is defined as TP  s    a  s           Pr s       s    a    Pr s   s    a     Pr    s    a    sas       where   is the Kronecker delta that takes value   if      and value   otherwise  The observation of the POMDP P indicates the current MDP state  Therefore  we define OP   S and ZP  s        a  o    s  o   The reward does not depend on the parameter   so we have RP  s    a  s          R s  a  s     Finally  we put a prior distribution b P    over   which reflects our initial belief   Monte Carlo Bayesian Reinforcement Learning  of the unknown parameter  This formulation explicitly represents the uncertainty in the unknown parameter  The parameter  forms a component of the POMDP state  which is partially observable and can be inferred based on the history of the observed MDP state action pairs  By solving the POMDP P  one plans against both the uncertainty in the dynamics and the uncertainty in the model parameter  An optimal policy for P thus yields an optimal strategy for action selection that balances exploration with exploitation  Since the parameter sas  takes continuous value  P has a hybrid state space  Two difficulties arise as a result  The first is how to efficiently maintain a belief for the continuous state variable  In order to attain a closed form representation  most existing work assumes a conjugate prior b P over the parameter   such as the Dirichlet distribution  Dearden et al         Duff        Poupart et al         Ross et al         Poupart   Vlassis         The second difficulty is how to solve the hybrid POMDP P efficiently  Although several approximate algorithms based on function approximation and online planning have been proposed  Duff        Poupart et al         Ross et al          there is no satisfactory answer in general       Algorithm MC BRL is motivated by the following observation  Although there are infinitely many possible values for the parameter   it may be possible to compute an approximately optimal policy without considering all of them  MC BRL consists of two phases  offline and online  Given a prior distribution b     and a sample size K  the offline phase of the algorithm works in three steps         Sample K hypotheses                 K independently from b         Form a discrete POMDP P   hSP   AP   OP   TP   ZP   RP     b P i  The state space is the cross product SP   S                 K   A state  s  k  consists of an MDP state s and an indicator k of the sampled hypotheses for   The actions AP   A and observations OP   S are defined in the same way as in Section      The transition  observation  and reward functions are defined as k     TP  s  k  a  s    k       sas     kk     ZP  s   k   a  o        s o   and RP  s  k  a  s   k     R s  a  s     respectively  Finally  the initial belief b P  k  is defined as the uniform distribution over                K      Solve the POMDP P and output a policy   In the online phase  the agent then follows the policy  to select actions   MC BRL sidesteps the two technical obstacles of the existing approach based on the hybrid POMDP P  The discrete POMDP P can be readily solved with point based approximation algorithms  Pineau et al         Smith   Simmons        Kurniawati et al          There is also no restrictive assumption on the form of the prior distribution b      The only requirement is that it is easy to sample from  We further note that P falls into the class of mixed observability MDPs  MOMDPs   Its state  s  k  has mixed observability  While the second component k is hidden  the first component s is fully observable  It has been shown that MOMDPs admit a compact factored representation of the state space  which can be exploited to speed up POMDP planning  Ong et al          In this paper  we use SARSOP  Ong et al         to solve P which readily takes advantage of the MOMDP representation  MC BRL takes a prior distribution b     as input  In practice  if we know nothing about the true parameter  we use a non informative prior such as uniform distribution  When there is prior knowledge about the true parameter  more informative prior can be used to bias the hypotheses towards the ground truth       Generalization to Partially Observable Environments MC BRL can be readily generalized to BRL problems under partially observable environments  Suppose we are given a POMDP hS  A  O  T  Z  R  i  and we aim to find an optimal policy when both the transition function T and the observation function Z are unknown  The unknown parameters can be denoted as a pair       where  is as defined before  while     s  ao  s   S  a  A  o  O  denotes the observation function and s  ao   Z s    a  o   MC BRL can be naturally adapted to address this problem with two modifications to the offline phase  First  it samples the hypotheses from a joint prior distribution b       instead of b      Second  the POMDP P is modified by set  ting OP   O and ZP  s    k     a  o    sk  ao   The modified observation function ZP now incorporates the uncertainty in the unknown parameter  of the underlying POMDP      Theoretical Analysis MC BRL uses the discrete POMDP P to approximate the hybrid POMDP P  To analyze the quality of this approximation  we derive a probably approximately correct  PAC  bound on the regret of MC BRLs solution  compared with the optimal solution to P  We assume that a POMDP policy  is represented as a policy graph G  which is a directed graph with labeled nodes and edges  Each node of G is labeled with an action a  A   Monte Carlo Bayesian Reinforcement Learning  and has  O  outgoing edges  each labeled with a distinct observation o  O  The size of the policy   denoted as     is the number of nodes in G  To execute the policy  the agent first picks a node in G according to the initial belief  It then takes the action associated with the node  receives an observation  and transits to the next node by following the edge labeled with that observation  The process then repeats  The policy graph representation allows us to establish the correspondence between policies for P and P  If  is a policy for P  then it is also a valid policy for P  and vice versa  as P and P share the same action space A and observation space O  Suppose that MC BRL forms the discrete POMDP P by taking K samples from the initial belief b P of P  There are three policies of interest  an optimal policy    for P  an optimal policy    for P  and the policy  that MC BRL actually computes  We want to bound the regret of  against      Define V as the value of a policy  for P with initial belief b P   and V as the value of  for P with initial belief b P   The following theorem states our main theoretical result  The proof is given in the supplementary material    Theorem    Suppose that    is an optimal policy for P and  is the policy that MC BRL computes by taking K samples to form a discrete POMDP P  Let Rmax   maxs s  S aA  R s  a  s      If V   V    then for any           q       O     ln       ln  A  ln       max V   V   R   K   with probability at least       The theorem says that MC BRL with a small set of samples produces a good approximate solution  to P with high probability  provided that there exists a simple approximate solution  to P  It is interesting to observe that although we formulate and solve the underlying reinforcement learning task as a planning problem  this analysis closely mirrors similar results in learning  if we think of P as a generalization of P with a richer model parameter space  then the theorem implies that a small policy results in better generalization  The error bound consists  of two terms  The first term decays at the rate O    K   We can reduce it by sampling more hypotheses from the prior  but at the cost of potentially increasing the complexity of the discrete POMDP P and the resulting policy   The second term  bounds the error in the approximate solution to the discrete POMDP   Available at http   www comp nus edu sg  leews publications icml     supp pdf     Figure    Chain problem   P  Algorithms such as HSVI  Smith   Simmons        and SARSOP  Kurniawati et al         output such bounds as a by product of POMDP policy computation  We can reduce  by running these algorithms longer towards convergence  It is also important to observe that the approximate Bayesoptimality of   quantified by V   guarantees the average performance of  with respect to the prior distribution b P of models  It does not guarantee the performance of  on any particular model  Our analysis assumes a policy graph representation of POMDP policies  In practice  point based discrete POMDP algorithms  such as HSVI and SARSOP  typically output policies represented as a set of  vectors  which in principle can be converted to policy graphs      Experiments We now experiment with MC BRL on both fully observable and partially observable reinforcement learning tasks  First  we evaluate MC BRL on two small synthetic domains widely used in the existing work on BRL  Sections     and       Here the standard setup requires us to measure the performance of an algorithm on particular model parameter values rather than the average performance with respect to a prior distribution of model parameters  Therefore the bound in Theorem   is not applicable here  Next  we test MC BRL on two more realistic domains  Sections     and       where we measure the average performance of MC BRL and show that it performs well in this sense  as our theoretical result guarantees  All the experiments are conducted on a    core Intel Xeon    GHz server       Chain We start with the Chain problem used in  Dearden et al         Poupart et al          This problem consists of a chain of   states and   actions  a  b   The actions cause the transitions between states and receive corresponding rewards  as shown in Figure    The actions are noisy  They slip with probability     and cause the opposite effect  The optimal policy of this problem is to always perform action a  We consider two versions of the Chain problem  In the   Monte Carlo Bayesian Reinforcement Learning Table    Average total rewards  reported with two standard errors  for the Chain problem  The results for Beetle and Exploit are from  Poupart et al          Semi Tied  Full  MC BRL  K       MC BRL  K        MC BRL  K                                                      Upper Bound Beetle Exploit Q Learning                                                         MC BRL   K       MC BRL   K        MC BRL   K                                    semi tied version  we assume that the structure of transitions between states in Figure   are given  The only unknown parameters are the   slipping probabilities  one for each action  In the full version  we assume that the transition function T  s  a  s    is completely unspecified  This leads to    unknown parameters  We evaluate MC BRL algorithm using     simulations with      steps in each simulation  We test K            and       and use the uniform prior to sample hypotheses  Since it is a stochastic algorithm  we rerun the offline phase of MC BRL before each simulation  obtain a policy  and then execute that policy online  We run the offline phase up to     seconds  The online time is negligible  Table   reports the average  undiscounted  total rewards of MC BRL  For comparison  we also report an upper bound on the reward that could be achieved only if we had known the true model parameters  as well as the rewards of three alternatives  the Beetle algorithm  Poupart et al          the Exploit heuristic  which never explores but takes the optimal action with respect to the expected MDP under the current belief  and Q learning with   greedy exploration and linear learning rate  For Q learning  we test a wide range of   values from   to      The reward for the optimal value is reported  MC BRL achieves good performance in the semi tied version  It obtains near optimal reward with      samples and is comparable to Beetle  It outperforms Exploit and Q learning  In the full version  MC BRL is still better than Q learning  However  it performs slightly worse than Beetle and is unable to improve the performance substantially with increased number of samples  Exploit performs much better than both MC BRL and Beetle  However  Exploit relies on a myopic heuristic and does not explore well in general  For example  it performs much more poorly than MC BRL and Beetle in the semi tied version  MC BRLs performance degrades in the full version  be   Table    Average total rewards for the Tiger problem  Total Reward MC BRL  K       MC BRL  K                                Upper Bound Prior Model                       cause the sample size is too small to cover the neighborhood of the true parameters within the    dimensional parameter space using the uniform prior  To verify this  we conduct another experiment by inserting the true parameter values as one of the samples of MC BRL  The results  denoted as MC BRL  in Table    show that MCBRL achieves good performance in this case  Constructing effective sampling strategies is an important direction for future research       Tiger We next test MC BRL on the Tiger problem  Kaelbling et al         with partial observability  In this problem  the agent must decide whether to open one of two doors or to listen for the position of the tiger at each time step  Opening the wrong door will cause the agent to be eaten by a tiger with a penalty of      while opening the correct door will give a reward of     Listening costs   and gives the true position of the tiger with     error  We assume that the transition and reward functions are given  but the observation error rates are unknown  We evaluate MC BRL using      simulations  Each simulation consists of     episodes  In each episode  the agent takes actions and receives observation sequentially  The episode ends when the agent opens a door and the position of the tiger is reset  We test MC BRL with K      and      Following  Ross et al          we use Dirichlet       as the prior distribution to sample the unknown parameters  This prior corresponds to an expected error rate        We run the offline phase of MCBRL up to     seconds  Table   shows the total reward gained by MC BRL in     episodes  averaged over the      simulations  For reference  we also include the upper bound induced by the true model  and the reward of the prior model in which the observation error rate is set to the prior expectation        With K        MC BRL achieves performance close to the upper bound  and is far better than the prior model  We further look into the evolution of the reward over episodes  Figure   shows the reward gained by MC BRL per episode  averaged over the      simulations  As we do not have the exact settings used in  Ross et al          we cannot directly compare with their experimental results  However  we can see that MC BRL quickly learns the un    Monte Carlo Bayesian Reinforcement Learning Table    Average total rewards over      random opponents for the IPD problem  Total Reward  Figure    Average reward evolving over episodes for the Tiger problem   known parameters and improves over the prior model  It achieves near optimal performance after about    episodes       Iterated Prisoners Dilemma The Prisoners Dilemma  Poundstone        is a well known one shot two player game in which each player tries to maximize his own reward by cooperating with or betraying the other  In this section  we studied its repeated version  the Iterated Prisoners Dilemma  IPD   Axelrod         and show that MC BRL can achieve excellent performance on this problem  In IPD  the game is played repeatedly and each player knows the history of his opponents moves  A key factor for an agent to gain high reward is the capability to model the opponents behaviour based on history  It has been shown that any memoryless and one stage memory opponent can be modeled using   parameters hPS   PT   PR   PP i  which are the probabilities that the opponent will cooperate in the next step  given the   possible situations of the current step      the agent cooperates while the opponent defects  denoted by S       the agent defects while the opponent cooperates  T        mutual cooperation  R   and     mutual defection  P    Kraines   Kraines         Suppose the agent knows the parameters of its opponent  Then the IPD can be naturally formulated as an MDP  The state of the MDP is the current move of the two players  which takes values from  S  T  R  P    The agent needs to select between cooperating or defecting for the next move  The transition function is defined based on the parameters of the opponent  The reward depends on the next state  and is set to            for S  T  R  P respectively  following the setting commonly used in IPD tournaments  In reality  the parameters of the opponent are unknown  The agent needs to explore the opponents strategy and at the same time maximize its reward  This leads to a RL problem and we apply MC BRL to solve it  We are interested in the average performance of MC BRL when facing various opponents  Therefore  we randomly  MC BRL  K        MC BRL  K                                   Upper Bound OTFT Q Learning Pavlov TFT AP                                                                          select      test opponents by uniformly sampling their parameters  For each opponent  we run the offline phase of MC BRL for     seconds and obtain a policy  We then use the policy to play against the opponent for     steps and collect the total reward  This is repeated for    times to account for the stochastic behaviour of the opponent  For MC BRL  we test K       and       and use the uniform prior to sample the parameters  We set the discount factor          Table   shows the total rewards averaged over the      opponents  With K        MC BRL already achieves good rewards  With K         it approaches the upper bound  which is achieved by solving the underlying MDP with the true parameters of the opponents  For reference  we also compare MC BRL with two classic hand crafted strategies  Tit for Tat  TFT   Axelrod        and Pavlov  Nowak   Sigmund         and the two winning entries of the      IPD tournament  Adaptive Pavlov  AP   Li        and Omega Tit for Tat  OTFT   Slany   Kienreich         These four strategies are used to play against the same      test opponents under the same setting as MC BRL  The results are summarized in Table    MC BRL achieves comparable reward to OTFT  and significantly outperforms all the others  It is interesting to note that AP  the tournament winner  performs very poorly  TFT  Pavlov  AP  and OTFT are all specially designed to win the IPD tournaments  while MC BRL is a general algorithm for BRL and is not optimized for competitions  On the other hand  one should not directly translate the good performance of MC BRL here to the IPD tournaments  as it is unlikely to face random opponents  However  MC BRL can use more informative priors to exploit domain knowledge on the opponents  as the other algorithms do  We further compare MC BRL with Q learning  We follow the setting suggested by  Littman   Stone         The result is shown in Table    We can see that MC BRL significantly outperforms Q learning on this task  While MC BRL achieves good average performance    STOP  Monte Carlo Bayesian Reinforcement Learning  A  R  Figure    A near miss accident during the      DARPA Urban Challenge   as our theorem guarantees  it can perform worse than other algorithms when faced with particular opponents  For instance  for the opponent parameterized by h                          i  MC BRL obtains a much lower reward than that of Q learning         versus             Intersection Navigation This problem is motivated by an accident in the      DARPA Urban Challenge  Leonard et al          In that event  two autonomous vehicles  R and A  approached an uncontrolled traffic intersection as shown in Figure    R had the right of way and proceeded  However  possibly due to sensor failure or imperfect driving strategy  A did not yield to R and caused a near miss  This situation is quite common and occurs frequently even with human drivers  Crossing the intersection safely and efficiently without knowing the driving strategy of A poses a significant challenge  We formulate the problem as a RL problem  The underlying model is a POMDP  The state consists of the positions and velocities of R and A  For simplicity  we discretize the environment into a uniform grid  In each step  the agent R can take three actions  accelerate  maintain speed  and decelerate  It then receives an observation on its own state and the state of A  Both actions and observations are noisy  The transition function is defined based on the driving strategy of A  which is unknown to the agent R  The agent receives a reward for crossing the intersection safely  and a large penalty for collision with A  A small penalty is given in each step to expedite the agent to cross the intersection faster  Due to space limitation  we give the detailed settings in the supplementary material  The driving strategy of A is unknown to the agent  We parameterize the driving strategy using   parameters      driver imperfection                driver reaction time             s      acceleration  a           m s    and     deceleration  d           m s    A preliminary study shows that this parameterization can cover a variety of drivers such as a reckless driver who never slows down at the intersection and an impatient driver who performs a  Figure    Average discounted total reward for the Intersection Navigation problem versus sample size K  reported with two standard error bar   rolling stop near the intersection  The agent needs to learn the parameters of A and cross the intersection at the same time  We test MC BRL on this RL problem  We test a range of K values and sample the parameters from the uniform distribution  Similar to the IPD problem  we are interested in the average performance of MC BRL with respect to different drivers A  Therefore  we uniformly sampled     test drivers  For each driver  we run the offline phase of MCBRL for     hours and obtain a policy  We then evaluate the policy against that test driver using     simulations with    steps in each simulation  Figure   shows the average discounted total rewards with discount factor          We can see that  as the sample size K increases  the performance of MC BRL improves quickly  With K        it gets close to the upper bound  which is achieved when the true parameters of the driver A are known  We also compare MC BRL to a hand crafted intersection policy that is commonly used in the traffic modeling community  Liu   Ozguner         With K       and above  MC BRL significantly outperforms that policy  While the hand crafted policy is not designed to handle noisy observations  we think that the performance gap between the hand crafted policy and MC BRL is more likely to be caused by insufficient adaptivity of the hand crafted policy in learning the driving strategy of A  As a final remark  this problem gives an example where it is more natural to define the prior over the physical properties of the environment  MC BRL handles such priors easily  although they are challenging to specify using methods that rely on conjugate distributions      Conclusion We have presented MC BRL  a simple and general approach to Bayesian reinforcement learning  We prove that by sampling a finite set of hypotheses from the model   Monte Carlo Bayesian Reinforcement Learning  parameter space  MC BRL generates a discrete POMDP that approximates the underlying BRL problem well with guaranteed performance  We provide experimental results demonstrating strong performance of the approach in practice  Furthermore  MC BRL naturally handles both fully and partially observable worlds   Kraines  D  and Kraines  V  Evolution of learning among pavlov strategies in a competitive environment with noise  Journal of Conflict Resolution                    One important issue for MC BRL is to sample the model parameter space effectively  A naive method is to discretize the parameter space uniformly and treat the fixed grid points as samples  This method  however  suffers from the curse of dimensionality and is difficult to scale up as the number of parameters increases  Poupart et al          MC BRL takes one step further and samples a set of hypotheses independently from a given prior distribution  The promising results obtained in this work open up many possibilities for future investigation  e g   constructing better informed prior distributions by exploiting domain knowledge and adaptive sampling   Leonard  J   How  J   and Teller  S  A perception driven autonomous urban vehicle  Journal of Field Robotics                         Acknowledgments  Kurniawati  H   Hsu  D   and Lee  W  S  SARSOP  Efficient pointbased POMDP planning by approximating optimally reachable belief spaces  In RSS         Li  J  How to design a strategy to win an IPD tournament  In The Iterated Prisoners Dilemma     Years On        Littman  M  L  and Stone  P  Leading best response strategies in repeated games  In IJCAI Workshop on Economic Agents  Models  and Mechanisms        Liu  Y  and Ozguner  U  Human driver model and driver decision making for intersection driving  IEEE Intelligent Vehicles Symposium  pp                Ng  A  and Jordan  M  PEGASUS  A policy search method for large MDPs and POMDPs  In UAI  pp                Nowak  M  and Sigmund  K  A strategy of win stay  lose shift that outperforms tit for tat in the prisoners dilemma game  Nature              Y  Wang and D  Hsu are supported in part by MoE AcRF grant      T        and MDA GAMBIT grant R                 K S  Won is supported by an NUS Presidents Fellowship  W S  Lee is supported in part by the Air Force Research Laboratory  under agreement number FA                The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements  either expressed or implied  of the Air Force Research Laboratory or the U S  Government   Poundstone  W  Prisoners Dilemma  Doubleday  New York  NY         
 We apply decision theoretic techniques to construct nonplayer characters that are able to assist a human player in collaborative games  The method is based on solving Markov decision processes  which can be difficult when the game state is described by many variables  To scale to more complex games  the method allows decomposition of a game task into subtasks  each of which can be modelled by a Markov decision process  Intention recognition is used to infer the subtask that the human is currently performing  allowing the helper to assist the human in performing the correct task  Experiments show that the method can be effective  giving nearhuman level performance in helping a human in a collaborative game   Introduction Traditionally  the behaviour of Non Player Characters  NPCs  in games is hand crafted by programmers using techniques such as Hierarchical Finite State Machines  HFSMs  and Behavior Trees  Champandard        These techniques sometimes suffer from poor behavior in scenarios that have not been anticipated by the programmer during game construction  In contrast  techniques such as Hierarchical Task Networks  HTNs  or Goal Oriented Action Planner  GOAP   Orkin       specify goals for the NPCs and use planning techniques to search for appropriate actions  alleviating some of the difficulties of having to anticipate all possible scenarios  In this paper  we study the problem of creating NPCs that are able to help players play collaborative games  The main difficulties in creating NPC helpers are to understand the intention of the human player and to work out how to assist the player  Given the successes of planning approaches to simplifying game creation  we examine the application of planning techniques to the collaborative NPC creation problem  In particular  we extend a decision theoretic framework Copyright c       Association for the Advancement of Artificial Intelligence  www aaai org   All rights reserved   for assistance used in  Fern and Tadepalli       to make it appropriate for game construction  The framework in  Fern and Tadepalli       assumes that the computer agent needs to help the human complete an unknown task  where the task is modeled as a Markov decision process  MDP   Bellman        The use of MDPs provide several advantages such as the ability to model noisy human actions and stochastic environments  Furthermore  it allows the human player to be modelled as a noisy utility maximization agent where the player is more likely to select actions that has high utility for successfully completing the task  Finally  the formulation allows the use of Bayesian inference for intention recognition and expected utility maximization in order to select the best assistive action  Unfortunately  direct application of this approach to games is limited by the size of the MDP model  which grows exponentially with the number of characters in a game  To deal with this problem  we extend the framework to allow decomposition of a task into subtasks  where each subtask has manageable complexity  Instead of inferring the task that the human is trying to achieve  we use intention recognition to infer the current subtask and track the players intention as the intended subtask changes through time  For games that can be decomposed into sufficiently small subtasks  the resulting system can be run very efficiently in real time  We perform experiments on a simple collaborative game and demonstrate that the technique gives competitive performance compared to an expert human playing as the assistant   Scalable Decision Theoretic Framework We will use the following simple game as a running example  as well as for the experiments on the effectiveness of the framework  In this game  called Collaborative Ghostbuster  the assistant  illustrated as a dog  has to help the human kill several ghosts in a maze like environment  A ghost will run away from the human or assistant when they are within its vision limit  otherwise it will move randomly  Since ghosts can only be shot by the human player  the dogs   role is strictly to round them up  The game is shown in Figure    Note that collaboration is often truly required in this game   without surrounding a ghost with both players in order to cut off its escape paths  ghost capturing can be quite difficult   This algorithm is guaranteed to converge to the optimal value function V   s   which gives the expected cumulative reward of running the optimal policy from state s  The optimal value function V  can be used to construct the optimal actionsPby taking action a in state s such that a   argmaxa   s  Ta  s  s   V   s      The optimal Qfunction is constructed from V  as follows  X Q  s  a    Ta  s  s    Ra  s  s      V   s      s   The function Q  s  a  denotes the maximum expected longterm reward of an action a when executed in state s instead of just telling how valuable a state is  as does V     Figure    A typical level of Collaborative Ghostbuster  The protagonists  Shepherd and Dog in the bottom right corner  need to kill all three ghosts to pass the level   Markov Decision Processes We first describe a Markov decision process and illustrate it with a Collaborative Ghostbuster game that has a single ghost  A Markov decision process is described by a tuple  S  A  T  R  in which  S is a finite set of game states  In single ghost Collaborative Ghostbuster  the state consists of the positions of the human player  the assistant and the ghost   A is a finite set of actions available to the players  each action a  A could be a compound action of both players  If each of the human player and the assistant has   moves  north  south  east and west   A would consist of the    possible combination of both players moves   Ta  s  s      P  st     s   st   s  at   a  is the probability that action a in state s at time t will lead to state s  at time t      The human and assistant move deterministically in Collaborative Ghostbuster but the ghost may move to a random position if there are no agents near it   Ra  s  s    is the immediate reward received after the state transition from s to s  triggered by action a  In Collaborative Ghostbuster  a non zero reward is given only if the ghost is killed in that move  The aim of solving an MDP is to obtain a policy maximizes the expected cumulative reward P that t t    R st    st   st     where          is the discount factor  Value Iteration  An MDP can be effectively solved using a simple algorithm proposed by Bellman in       Bellman        The algorithm maintains a value function V  s   where s is a state  and iteratively updates the value function using the equation   X       Vt    s    max Ta  s  s   Ra  s  s     Vt  s      a  s   Intractability  One key issue that hinders MDPs from being widely used in real life planning tasks is the large state space size  usually exponential in the number of state variables  that is often required to model realistic problems  Typically in game domains  a state needs to capture all essential aspects of the current configuration and may contain a large number of state variables  For instance  in a Collaborative Ghostbuster game with a maze of size m  number of valid positions  consisting of a player  an assistant and n ghosts  the set of states is of size O mn      which grows exponentially with the number of ghosts   Subtasks To handle the exponentially large state space  we decompose a task into smaller subtasks and use intention recognition to track the current subtask that the player is trying to complete   Figure    Task decomposition in Collaborative Ghostbuster  In Collaborative Ghostbuster  each subtask is the task of catching a single ghost  as shown in Figure    The MDP for a subtask consists of only two players and a ghost and hence has manageable complexity    Human Model of Action Selection In order to assist effectively  the AI agent must know how the human is going to act  Without this knowledge  it is almost impossible for the AI to provide any help  We assume that the human is mostly rational and use the Q function to model the likely human actions  Specifically  we assume maxaAI Q i  si  ahuman  aAI    P  ahuman  wi   si      e     where  is the normalizing constant  wi represents subtask i and si is the state in subtask i  Note that we assume that the human player knows the best response from the AI sidekick and plays his part in choosing the action that matches the most valued action pair  However  the human action selection can be noisy  as modelled by Equation       Intention Recognition and Tracking We use a probabilistic state machine to model the subtasks for intention recognition and tracking  At each time instance  the player is likely to continue on the subtask that he or she is currently pursuing  However  there is a small probability that the player may decide to switch subtasks  This is illustrated in Figure    where we model a human player who tends to stick to his chosen sub goal  choosing to solve the current subtask     of the times and switching to other sub tasks     of the times  The transition probability distributions of the nodes need not be homogeneous  as the human player could be more interested in solving some specific subtask right after another subtask  For example  if the ghosts need to be captured in a particular order  this constraint can be encoded in the state machine  The model also allows the human to switch back and forth from one subtask to another during the course of the game  modelling change of mind   where T  wj  wi   is the switching probability from subtask j to subtask i  Next  we compute the posterior belief distribution using Bayesian update  after observing the human action a and subtask state si t at time t  as follows  Bt  wi  at   a  st   t       Bt  wi  t    P  at   a wi   si t       where  is a normalizing constant  Absorbing current human action a and current state into t  gives us the game history t at time t  Complexity This component is run in real time  and thus its complexity dictates how responsive our AI is  We are going to show that it is at most O k      with k being the number of subtasks  The first update step as depicted in Equation   is executed for all subtasks  thus of complexity O k      The second update step as of Equation   requires the computation of P  at   a wi   si    Equation     which takes O  A   with A being the set of compound actions  Since Equation   is applied for all subtasks  that sums up to O k A   for this second step  In total  the complexity of our real time Intention Recognition component is O k     k A    which will be dominated by the first term O k     if the action set is fixed   Decision theoretic Action Selection Given a belief distribution on the players targeted subtasks as well as knowledge to act collaboratively optimally on each of the subtasks  the agent chooses the action that maximizes its expected reward      X i  Bt  wi  t  Qi  st   a  a   argmaxa i  CAPIR  Collaborative Action Planner with Intention Recognition We implement the scalable decision theoretic framework as a toolkit for implementing collaborative games  called Collaborative Action Planner with Intention Recognition  CAPIR   Figure    A probabilistic state machine  modeling the transitions between subtasks  Belief Representation and Update The belief at time t  denoted Bt  wi  t    where t is the game history  is the conditional probability of that the human is performing subtask i  The belief update operator takes Bt   wi  t    as input and carries out two updating steps  First  we obtain the next subtask belief distribution  taking into account the probabilistic state machine model for subtask transition T  wk  wi   X Bt  wi  t      T  wj  wi  Bt   wj  t        j  CAPIRs Architecture Each game level in CAPIR is represented by a GameWorld object  which consists of two Players and multiple SubWorld objects  each of which contains only the elements required for a subtask  Figure     The game objective is typically to interact with these NPCs in such a way that gives the players the most points in the shortest given time  The players are given points in major events such as successfully killing a monster type NPC or saving a civilian type NPC  these typically form the subtasks  Each character in the game  be it the NPC or the protagonist  is defined in a class of its own  capable of executing multiple actions and possessing none or many properties  Besides movable NPCs  immobile items  such as doors or   Figure    GameWorlds components  shovels  are specified by the class SpecialLocation  GameWorld maintains and updates an internal game state that captures the properties of all objects  At the planning stage  for each SubWorld  an MDP is generated and a collaboratively optimal action policy is accordingly computed  Figure     These policies are used by the AI assistant at runtime to determine the most appropriate action to carry out  from a decision theoretic viewpoint                                                                                                                                                                                                                     Figure    CAPIRs action planning process   a  Offline subtask Planning   b  in game action selection using Intention Recognition   busters  We chose five levels  see Appendix  with roughly increasing state space size and game play complexity to assess how the technique can scale with respect to these dimensions  The participants were requested to play five levels of the game as Shepherd twice  each time with a helping Dog controlled by either AI or a member of our team  the so called human expert in playing the game  The identity of the dogs controller was randomized and hidden from the participants  After each level  the participants were asked to compare the assistants performance between two trials in terms of usefulness  without knowing who controlled the assistant at which turn  In this set of experiments  the players aim is to kill three ghosts in a maze  with the help of the assistant dog  The ghosts stochastically  run away from any protagonists if they are   steps away  At any point of time  the protagonists could move to an adjacent free grid square or shoot  however  the ghosts only take damage from the ghost buster if he is   steps away  This condition forces the players to collaborate in order to win the game  In fact  when we try the game with non collaborative dog models such as random movement  the result purely relies on chance and could go on until the time limit      steps  runs out  as the human player hopelessly chases ghosts around obstacles while the dog is doing some nonsense at a corner  Oftentimes the game ends when ghosts walk themselves into dead end corners  The twenty participants are all graduate students at our school  seven of whom rarely play games  ten once to twice a week  and three more often  When we match the answers back to respective controllers  the comparison results take on one of three possible values  being AI assistant performing better  worse or indistinguishable to the human counterpart  The AI assistant is given a score of   for a better    for an indistinguishable and    for a worse evaluation  Qualitative evaluation For simpler levels      and    our AI was rated to be better or equally good more than     the times  For level    our AI rarely got the rating of being indistinguishable  though still managed to get a fairly competitive performance  Subsequently  we realized that in this particular level  the map layout is confusing for the dog to infer the humans intention  there is a trajectory along which the human players movement could appear to aim at any one of three ghosts  In that case  the dogs initial subtask belief plays a crucial role in determining which ghost it thinks the human is targeting  Since the dogs belief is always initialized to a uniform distribution  that causes the confusion  If the human player decides to move on a different path  the AI dog is able to efficiently assist him  thus getting good ratings instead  In level    our AI gets good ratings only for less than one third of the times  but if we count indistinguishable ratings as satisfactory  the overall percentage of positive ratings exceeds       Experiment and Analysis In order to evaluate the performance of our AI system  we conducted a human experiment using Collaborative Ghost     The ghosts run away     of the times and perform some random actions in the remaining                                                                                                             Figure    Qualitative comparison between CAPIRs AI assistant and human expert  The y axis denotes the number of ratings                          AI              Human                                                             Figure    Average time  with standard error of the mean as error bars  taken to finish each level when the partner is AI or human  The y axis denotes the number of game turns  Quantitative evaluation Besides qualitative evaluation  we also recorded the time taken for participants to finish each level  Figure     Intuitively  a well cooperative pair of players should be able to complete Collaborative Ghostbusters levels in shorter time  Similar to our qualitative result  in levels      and    the AI controlled dog is able to perform at near human levels in terms of game completion time  Level    which takes the AI dog and human player more time on average and with higher fluctuation  is known to cause confusion to the AI assistants initial inference of the humans intention and it takes a number of game turns before the AI realizes the true target  whereas our human expert is quicker in closing down on the intended ghost  Level    larger and with more escape points for the ghosts but less ambiguous  takes the protagonist pair  AI  human  only      more on average completion time   Related Work Since plan recognition was identified as a problem on its own right in       Schmidt  Sridharan  and Goodson        there have been various efforts to solve its variant in different domains  In the context of modern game AI research  Bayesian based plan recognition has been inspected using  different techniques such as Input Output Hidden Markov Models  Gold        Plan Networks  Orkin and Roy        text pattern matching  Mateas and Stern        n gram and Bayesian networks  Mott  Lee  and Lester       and dynamic Bayesian networks  Albrecht  Zukerman  and Nicholson        As far as we know  our work is the first to use a combination of precomputed MDP action policies and online Bayesian belief update to solve the same problem in a collaborative game setting  Related to our work in the collaborative setting is the work reported by Fern and Tadepalli  Fern and Tadepalli       who proposed a decision theoretic framework of assistance  There are however several fundamental differences between their targeted problem and ours  Firstly  they assume the task can be finished by the main subject without any help from the AI assistant  This is not the case in our game  which presents many scenarios in which the effort from one lone player would amount to nothing and a good collaboration is necessary to close down on the enemies  Secondly  they assume a stationary human intention model  i e  the human only has one goal in mind from the start to the end of one episode  and it is the assistants task to identify this sole intention  In contrary  our engine allows for a more dynamic human intention model and does not impose a restriction on the freedom of the human player to change his mind mid way through the game  This helps ensure our AIs robustness when inferring the human partners intention  In a separate effort that also uses MDP as the game AI backbone  Tan and Cheng  Tan and Cheng       model the game experience as an abstracted MDP   POMDP couple  The MDP models the game worlds dynamics  its solution establishes the optimal action policy that is used as the AI agents base behaviors  The POMDP models the human play style  its solution provides the best abstract action policy given the human play style  The actions resulting from the two components are then merged  reinforcement learning is applied to choose an integrated action that has performed best thus far  This approach attempts to adapt to different human play styles to improve the AI agents performance  In contrast  our work introduces the multi subtask model with intention recognition to directly tackle the intractability issue of the game worlds dynamics   Conclusions We describe a scalable decision theoretic approach for constructing collaborative games  using MDPs as subtasks and intention recognition to infer the subtask that the player is targeting at any time  Experiments show that the method is effective  giving near human level performance  In the future  we also plan to evaluate the system in more familiar commercial settings  using state of the art game platforms such as UDK or Unity  These full fledged systems offer development of more realistic games but at the same time introduce game environments that are much more complex to plan  While experimenting with Collaborative Ghostbuster  we have observed that even though Value Iteration is a simple naive approach  in most cases  it suffices  converging in reasonable time  The more serious issue is the   state space size  as tabular representation of the states  reward and transition matrices takes much longer to construct  We plan to tackle this limitation in future by using function approximators in place of tabular representation   Appendix Game levels used for our experiments   Acknowledgments This work was supported in part by MDA GAMBIT grant R                and AcRF grant T     RES     in Singapore  The authors would like to thank Qiao Li  NUS   Shari Haynes and Shawn Conrad  MIT  for their valuable feedbacks in improving the CAPIR engine  and the reviewers for their constructive criticism on the paper       
