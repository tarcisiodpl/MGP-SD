  of our approach on a complex dynamic domain of a persons transportation routines   This paper describes a general framework called Hybrid Dynamic Mixed Networks  HDMNs  which are Hybrid Dynamic Bayesian Networks that allow representation of discrete deterministic information in the form of constraints  We propose approximate inference algorithms that integrate and adjust well known algorithmic principles such as Generalized Belief Propagation  Rao Blackwellised Particle Filtering and Constraint Propagation to address the complexity of modeling and reasoning in HDMNs  We use this framework to model a persons travel activity over time and to predict destination and routes given the current location  We present a preliminary empirical evaluation demonstrating the effectiveness of our modeling framework and algorithms using several variants of the activity model   Focusing on algorithmic issues  the most popular approximate query processing algorithms for dynamic networks are Expectation propagation EP   Heskes and Zoeter        and Rao Blackwellised Particle Filtering  RBPF   Doucet et al          We therefore extend these algorithms to accommodate and exploit discrete constraints in the presence of continuous probabilistic functions  Extending Expectation Propagation to handle constraints is easy  extension to continuous variables is a little more intricate but still straightforward  The presence of constraints introduces a principles challenge for Sequential Importance Sampling algorithms  however  Indeed the main algorithmic contribution of this paper in presenting a class of Rao Blackwellised Particle Filtering algorithm  IJGP RBPF for HDMNs which integrates a Generalized Belief Propagation component with a Rao Blackwellised Particle Filtering scheme     INTRODUCTION Modeling sequential real life domains often requires the ability to represent both probabilistic and deterministic information  Hybrid Dynamic Bayesian Networks  HDBNs  were recently proposed for modeling such phenomena  Lerner         In essence  these are factored representation of Markov processes that allow discrete and continuous variables  Since they are designed to express uncertain information they represent constraints as probabilistic entities which may have negative computational consequences  To address this problem  Dechter and Mateescu        Larkin and Dechter        introduced the framework of Mixed Networks  In this paper we extend the Mixed Networks framework to dynamic environments  allow continuous Gaussian variables  yielding Hybrid Dynamic Mixed Networks  HDMN   We address the algorithmic issues that emerge from this extension and demonstrate the potential  Our motivation for developing HDMNs as a modeling framework is a range of problems in the transportation literature that depend upon reliable estimates of the prevailing demand for travel over various time scales  At one end of this range  there is a pressing need for accurate and complete estimation of the global origins and destinations  O D  matrix at any given time for an entire urban area  Such estimates are used in both urban planning applications  Sherali et al         and integrated traffic control systems based upon dynamic traffic assignment techniques  Peeta and Zilaskopoulos         Even the most advanced techniques  however  are hamstrung by their reliance upon out dated  pencil and paper travel surveys and sparsely distributed detectors in the transportation system  We view the increasing proliferation of powerful mobile computing devices as an opportunity to remedy this situation  If even a small sample of the traveling public agreed to collect their travel data and make that data publicly available  transportation management systems could significantly improve their operational efficiency  At the   other end of the spectrum  personal traffic assistants running on the mobile devices could help travelers replan their travel when the routes they typically use are impacted by failures in the system arising from accidents or natural disasters  A common starting point for these problems is to develop an efficient formulation for learning and inferring individual traveler routines like travelers destination and his route to destination from raw data points  The rest of the paper is organized as follows  In the next section  we discuss preliminaries and introduce our modeling framework  We then describe two approximate inference algorithms for processing HDMN queries  an Expectation Propagation type and a Particle Filtering type  Subsequently  we describe the transportation modeling approach and present preliminary empirical results on how effectively a model is learnt and how accurately its predictions are given several models and a few variants of the relevant algorithms  We view the contribution of this paper in addressing a complex and highly relevant real life domain using a general framework and domain independent algorithms  thus allowing systematic study of modeling  learning and inference in a non trivial setting     PRELIMINARIES AND DEFINITIONS Hybrid Bayesian Networks  HBN   Lauritzen        are graphical models defined by a tuple B    X  G  P   where X is the set of variables partitioned into discrete and continuS ous ones X      respectively  G is a directed acyclic graph whose nodes corresponds to the variables  P    P         Pn   is a set of conditional probability distributions  CPDs   Given variable xi and its parents in the graph pa xi    Pi   P xi  pa xi     The graph structure G is restricted in that continuous variables cannot have discrete variables as their child nodes  The conditional distribution of continuous variables are given by a linear Gaussian model  P xi  I   i  Z   z    N  i     i   z   i  xi   where Z and I are the set of continuous and discrete parents of xi   respectively and N     is a multi variate normal distribution  The network represents a joint distribution over all its variables given by a product of all its CPDs  A Constraint Network  Dechter        is a graphical model R    X  D C   where X    x            xn   is the set of variables  D    D            Dn   is their respective discrete domains and C    C   C           Cm   is the set of constraints  Each constraint Ci is a relation Ri defined over a subset of the variables Si  X and denotes the combination of values that can be assigned simultaneously  A Solution is an assignment of values to all the variables such that no constraint is violated  The primary query is to decide if the constraint network is consistent and if so find one or all solutions   The recently proposed Mixed Network framework  Dechter and Mateescu        for augmenting Bayesian Networks with constraints  can immediately be applied to HBNs yielding the Hybrid Mixed Networks  HMNs   Formally  given a HBN B    X  G  P  that expresses the joint probability PB and given a constraint network R    X  D C  that expresses a set of solutions   an HMN is a pair M    B   R    The discrete variables and their domains are shared by B and R and the relationships are those expressed in P and C  We assume that R is consistent  The mixed network M    B   R   represents the conditional probability PM  x    PB  x x    i f x   and   otherwise  Dynamic Bayesian Networks are Markov models whose state space and transition functions are expressed in a factored form using Bayesian Networks  They are defined by a prior P X    and a state transition function P Xt    Xt    Hybrid Dynamic Bayesian Networks  HDBNs  allow continuous variables while Hybrid Dynamic Mixed Networks  HDMNs  also permit explicit discrete constraints  D EFINITION     A Hybrid Dynamic Mixed Network  HDMN  is a pair  M    M    defined over a set of variables X    x         xn    where M  is an HMN defined over X representing P X     M is a   slice network defining the stochastic process P Xt    Xt    The   time slice Hybrid    Mixed network    THMN  is an HMN defined over X    X      such that X and X are identical to X  The acyclic graph   of the probabilistic portion is restricted so that nodes in X are root nodes and have no CPDs associated with them  The constraints are defined the usual way  The   THMN      represents a conditional distribution P X  X    The semantics of any dynamic network can be understood by unrolling the network to T time slices  Namely  T P X  t     P X     t   P Xt  Xt    where each probabilistic component can be factored in the usual way  yielding a regular HMN over T copies of the state variables  The most common task over Dynamic Probabilistic Networks is filtering and prediction Filtering is the task of determining the belief state P Xt  e  t   where Xt is the set of variables at time t and e  t are the observations accumulated at time slices   to t  Filtering can be accomplished in principle by unrolling the dynamic model and using any stateof the art exact or approximate reasoning algorithm  The join tree clustering algorithm is the most commonly used algorithm for exact inference in Bayesian networks  It partitions the CPDs and constraints into clusters that interact in a tree like manner  the join tree  and applies messagepassing between clusters  The complexity of the algorithm is exponential in a parameter called treewidth  which is the maximum number of discrete variables in a cluster  However  the stochastic nature of Dynamic Networks restricts the applicability of join tree clustering considerably  In the discrete case the temporal structure implies   tree width which equals to the number of state variables that are connected with the next time slice  thus making the factored representation ineffective  Even worse  when both continuous and discrete variables are present the effective treewidth is O T   when T is the number of time slices  thus making exact inference infeasible  Therefore the applicable approximate inference algorithms for Hybrid Dynamic Networks are either sampling based such as Particle Filtering or propagation based such as Expectation Propagation  In the next two sections  we will extend these algorithms to HDMNs     EXPECTATION PROPAGATION In this section we extend an approximate inference algorithm called Expectation Propagation  EP   Heskes and Zoeter        from HDBNs to HDMNs  The idea in EP  forward pass  is to perform Belief Propagation by passing messages between slices t and t     along the ordering t     to T   EP can be thought of as an extension of Generalized Belief Propagation  GBP  to HDBNs  Heskes and Zoeter         For simplicity of exposition  we will extend a GBP algorithm called Iterative Join graph propagation  Dechter et al         to HDMNs and call our technique IJGP i  S where S denotes that the process is sequential  The extension is rather straightforward and can be easily derived by integrating the results in  Murphy        Dechter et al         Lauritzen        Larkin and Dechter         IJGP  Dechter et al         is a Generalized Belief Propagation algorithm which performs message passing on a join graph  A join graph is collection of cliques or clusters such that the interaction between the clusters is captured by a graph  Each clique in a join graph contains a subset of variables from the graphical model  IJGP i  is a parameterized algorithm which operates on a join graph which has less than i     discrete variables in each clique  The complexity of IJGP i  is bounded exponentially by i   also called the i bound  In the message passing step of IJGP i   a message is sent between any two nodes that are neighbors of each other in the join graph  A message sent by node Ni to N j is constructed by multiplying all the functions and messages in a node  except the message received from N j   and marginalizing on the common variables between N j and Ni  see  Dechter et al           IJGP i  can be easily adapted to HDMNs  which we call IJGP i  S  and we describe some technical details here rather than a complete derivation due to lack of space  Note that because we are performing online inference  we need to construct the join graph used by IJGP i  S in an online manner rather than recomputing the join graph every time new evidence arrives  Murphy  Murphy        describes a method to compute a join tree in an online manner by pasting together join trees of individual time slices using  special cliques called the interface   Dechter et al         describe a method to compute join graphs from join trees  The two methods can be combined in a straightforward way to come up with an online procedure for constructing a join graph  In this procedure  we split the interface into smaller cliques such that the new cliques have less than i     variables  This construction procedure is shown in Figure    Message passing is then performed in a sequential way as follows  At each time slice t  we perform message passing over nodes in t and the interface of t with t    and t      shown by the ovals in Figure     The new functions computed in the interface of t with t     are then used by t      when we perform message passing in t      Three important technical issues remain to be discussed  First  message passing requires the operations of multiplication and marginalization to be performed on functions in each node  These operators can be constructed for HDMNs in a straightforward way by combining the operators by  Lauritzen        and  Larkin and Dechter        that work on HBNs and discrete mixed networks respectively  We will now briefly comment on how the multiplication operator can be derived  Let us assume we want to multiply a collection of probabilistic functions P  and a set of constraint relations C   which consist of only discrete tuples allowed by the constraint  to form a single function PC  Here  multiplication can be performed on the functions in P  and C  separately using the operators in  Lauritzen        and  Dechter        respectively to compute a single probabilistic function P and a single constraint relation C  These two functions P and C can be multiplied by deleting all tuples in P that are not present in C to form the required function PC  Second  because IJGP i  S constructs join graphs sequentially  the maximum i bound for IJGP i  S is bounded by the treewidth of the time slice and its interfaces and not the treewidth of the entire HDMN model  see Figure      Figure    Schematic illustration of the Procedure used for creating join graphs and join trees of HDMNs   Algorithm IJGP RBPF  Input  A Hybrid Dynamic Mixed Network  X  D  G  P C   T and a observation sequence e  T Integer N  w and i   Output  P XT  e  T    For t     to T do  Sequential Importance Sampling step     Generalized Belief Propagation step Use IJGP i  to compute the proposal distribution app    Rao Blackwellisation step Partition the Variables Xt into Rt and Zt such that the treewidth of a join tree of Zt is w     Sampling step For i     to N do  a  Generate a Rti from app    b  reject sample if rti is not a solution  i i     c  Compute the importance weights wti of Rti   ci      Normalize the importance weights to form w t  Selection step   Resample N samples from Rbti according to the normalized importance weights ci to obtain new N random samples  w t   Exact step   for i     to N do i   e   Rbi and Use join tree clustering to compute the distribution on Zti given Zt  t t d i R   t   Figure    IJGP RBPF for HDMNs Third  IJGP i  guarantees that the computations will be exact if i is equal to the treewidth  This is not true for IJGP i S in general as shown in  Lerner         It can be proved that  T HEOREM     The complexity of IJGP i  S is O    t     n   d i  t     T   where  t   is the number of discrete variables in time slice t  d is the maximum domain size of the discrete variables  i is the i bound used  n is the number of nodes in a join graph of the time slice  t is the maximum number of continuous variables in the clique of the join graph used and T is the number of time slices     RAO BLACKWELLISED PARTICLE FILTERING In this section  we will extend the Rao Blackwellised Particle filtering algorithm  Doucet et al         from HDBNs to HDMNs  Before  we present this extension  we will briefly review Particle Filtering and Rao Blackwellised Particle Filtering  RBPF  for HDBNs  Particle filtering uses a weighted set of samples or particles to approximate the filtering distribution  Thus  given a set of particles Xt            XtN approximately distributed according to the target filtering distribution P Xt   M e  t    the filtering distribution is given by P Xt   M e  t       N Ni    Xti   M  where  is the Dirac delta function  Since we cannot sample from P Xt   M e  t   directly  Parti   cle filtering uses an appropriate  importance  proposal distribution Q X  to sample from  The particle filter starts by generating N particles according to an initial proposal distribution Q X   e     At each step  it generates the next state i for each particle X i by sampling from Q X i Xt   t    Xt   e  t    t It then computes the weight of each particle based given by wt   P X  Q X  to compute a weighted distribution and then re samples from the weighted distribution to obtain a set of un biased or un weighted particles  Particle filtering often shows poor performance in highdimensional spaces and its performance can be improved by sampling from a sub space by using the RaoBlackwellisation  RB  theorem  and the particle filtering is called Rao Blackwellised Particle Filtering  RBPF    Specifically  the state Xt is divided into two sets  Rt and Zt such that only variables in set Rt are sampled  from a proposal distribution Q Rt     while the distribution on Zt is computed analytically given each sample on Rt  assuming that P Zt  Rt   e  t   Rt    is tractable   The complexity of RBPF is proportional to the complexity of exact inference step i e  computing P Zt  Rt   e  t   Rt    for each sample Rtk   w cutset  Bidyuk and Dechter        is a parameterized way to select Rt such that the complexity of computing P Zt  Rt   e  t   Rt    is bounded exponentially by w  Below  we use the w cutset idea to perform RBPF in HDMNs  Since exact inference can be done in polynomial time if a HDBN contains only continuous variables  a straightforward application of RBPF to HDBNs involves sampling only the discrete variables in each time slice and exactly inferring the continuous variables  Lerner         Extending this idea to HDMNs  suggests that in each time slice t we sample the discrete variables and discard all particles that violate the constraints in the time slice  Let us assume that we select a proposal distribution Q that is a good approximation of the probabilistic filtering distribution but ignores the constraint portion  The extension described above can be inefficient because if the proposal distribution Q is such that it makes non solutions to the constraint portion highly probable  most samples from Q will be rejected  because these samples Rti will have P Rti       and so the weight will be zero   Thus  on one extreme sampling only from the Bayesian Network portion of each time slice may lead to potentially high rejection rate  On the other extreme  if we want to make the sample rejection rate zero we would have to use a proposal distribution Q  such that all samples from this distribution are solutions  One way to find this proposal distribution is to make the constraint network backtrack free  using adaptive consistency  Dechter        or exact constraint propagation  along an ordering of variables and then sample along the reverse ordering  Another approach is to use join tree clustering which combines probabilistic and deterministic information and then sample from the join    tree  However  both join tree clustering and adaptiveconsistency are time and space exponential in treewidth and so they are costly when the treewidth is large  Thus on one hand  zero rejection rate implies using a potentially costly inference procedure while on the other hand sampling from a proposal distribution that ignores constraints may result in a high rejection rate  We propose to exploit the middle ground between the two extremes by combining the constraint network and the Bayesian Network into a single approximate distribution app using IJGP i  which is a bounded inference procedure  Note that because IJGP i  has polynomial time complexity for constant i  we would not eliminate the samplerejection rate completely  However  by using IJGP i  we are more likely to reduce the rejection rate because IJGP i  also achieves Constraint Propagation and it is well known that Constraint Propagation removes many inconsistent tuples thereby reducing the chance of sampling a nonsolution   Dechter         Another important advantage of using IJGP i  is that it yields very good approximations to the true posterior  Dechter et al         thereby proving to be an ideal candidate for proposal distribution  Note that IJGP i  can be used as a proposal distribution because it can be proved using results from  Dechter and Mateescu        that IJGP i  includes all supports of P Xt  e  t   Xt     i e  P Xt  e  t   Xt        implies that the output of IJGP i  viz  Q      Note that IJGP i  we use here is different from the algorithm IJGP i  S that we described in the previous section  This is because in our RBPF procedure  we need to comk  e   pute an approximation to the distribution P Rt  Rt    t k given the sample Rt  on variables Rt  and evidence e  t   IJGP i  as used in our RBPF procedure works on HMNs and can be derived using the results in  Dechter et al         Lauritzen        Larkin and Dechter         For lack of space we do not describe the details of this algorithm  see a companion paper  Gogate and Dechter        for details   The integration of the ideas described above into a formal algorithm called IJGP RBPF is given in Figure    It uses the same template as in  Doucet et al         and the only step different in IJGP RBPF from the original template is the implementation of the Sequential Importance Sampling step  SIS   SIS is divided into three steps      In the Generalized Belief Propagation step of SIS  we first perform Belief Propagation using IJGP i  to form an approximation of the posterior  say app   as described above      In the Rao Blackwellisation step  we first partition the variables in a  THMN into two sets Rt and Zt using a method due to  Bidyuk and Dechter         This method  Bidyuk and Dechter        removes minimal variables Rt from Xt such that the treewidth of the remain   Figure    Car Travel Activity model of an individual ing network Zt is bounded by w      In the sampling step  the variables Rt are sampled from app   To generate a sample from app   we use a special data structure of ordered buckets which is described in a companion paper  Gogate and Dechter         Importance weights are computed as usual  Doucet et al          Finally  the exact step computes a distribution on Zt using join tree clustering for HMNs  see a companion paper  Gogate and Dechter        for details on join treeclustering for HMNs   It can be proved that  T HEOREM     The complexity of IJGP RBPF i w  is O  NR  d w            n    d i           T   where    is the number of discrete variables  d is the maximum domain size of the discrete variables  i is the adjusted i bound  w is defined by w cutset  n is the number of nodes in a joingraph   is the number of continuous variables in a  THMN  NR is the number of samples actually drawn and T is the number of time slices      THE TRANSPORTATION MODEL  In this section  we describe the application of HDMNs to a real world problem of inferring car travel activity of individuals  The major query in our HDMN model is to predict where a traveler is likely to go and what his her route to the destination is likely to be  given the current location of the travelers car  This application was described in  Liao et al         in a different context for detecting abnormal behavior in Alzheimers patients and they use a Abstract Hierarchical Markov Models  AHMM  for reasoning about this problem  The novelty in our approach is not only a more general modeling framework and approximate inference algorithms but also a domain independent implementation which allows an expert to add and test variants of the model  Figure   shows a HDMN model for modeling the car travel activity of individuals  Note that the directed links express the probabilistic relationships while the undirected  bold    edges express the constraints  We consider the roads as a Graph G V  E  where the vertices V correspond to intersections while the edges E correspond to segments of roads between intersections  The variables in the model are as follows  The variables dt and wt represent the information about time of day and dayof week respectively  dt is a discrete variable and has four values  morning  a f ternoon  evening  night  while the variable wt has two values  weekend  weekday   Variable gt represents the persons next goal  e g  his office  home etc   We consider a location where the person spends significant amount of time as a proxy for a goal  Liao et al          These locations are determined through a preprocessing step by noting the locations in which the dwell time is greater than a threshold     minutes   Once such locations are determined  we cluster those that are in close proximity to simplify the goal set  A goal can be thought of as a set of edges E   E in our graph representation  The route level rt represents the route taken by the person to move from one goal to other  We arbitrarily set the number of values it can take to  gt      The persons location lt and velocity vt are estimated from GPS reading yt   ft is a counter  essentially goal duration  that governs goal switching  The Location lt is represented in the form of a two tuple  a  w  where a    s    s    a  E and s    s   V is an edge of the map G V  E  and w is a Gaussian whose mean is equal to the distance between the persons current position on a and one of the intersections in a  The probabilistic dependencies in the model are straightforward and can be found by tracing the arrows  see Figure     The constraints in the model are as follows  We assume that a person switches his goal from one time slice to another when he is near a goal or moving away from a goal but not when he is on a goal location  We also allow a forced switch of goals when a specified maximum time that he is supposed to spend at a goal is reached  This is modeled by using a constant D  These assumptions of switching between goals is modeled using the following constraints between the current location  the current goal  the next goal and the switching counters     If lt    gt  and Ft      Then Ft   D      If lt    gt  and Ft      Then Ft   Ft          If lt     gt  and Ft      Then Ft     and     If lt     gt  and Ft      Then Ft          If Ft      and Ft     Then gt is given by P gt  gt         If Ft     and Ft     Then gt is same as gt        If Ft      and Ft     gt is same as gt  and     If Ft      and Ft     gt is given by P gt  gt        EXPERIMENTAL RESULTS The test data consists of a log of GPS readings collected by one of the authors  The test data was collected over a six month period at intervals of     seconds each  The data consist of the current time  date  location and veloc   ity of the persons travel  The location is given as latitude and longitude pairs  The data was first divided into individual routes taken by the person and the HDMN model was learned using the Monte Carlo version of the EM algorithm  Liao et al         Levine and Casella         We used the first three months data as our training set while the remaining data was used as a test set  TIGER Line files available from the US Census Bureau formed the graph on which the data was snapped  As specified earlier our aim is two fold   a  Finding the destination or goal of a person given the current location and  b  Finding the route taken by the person towards the destination or goal  To compare our inference and learning algorithms  we use three HDMN models  Model   is the model shown in Figure    Model   is the model given in Figure   with the variables wt and dt removed from each time slice  Model  is the base model which tracks the person without any high level information and is constructed from Figure   by removing the variables wt   dt   ft   gt and rt from each timeslice  We used   inference algorithms  Since EM learning uses inference as a sub step  we have   different learning algorithms  We call these algorithms as IJGP S     IJGPS    and IJGP RBPF     N  and IJGP RBPF     N  respectively  Note that the algorithm IJGP S i   described in Section    uses i as the i bound  IJGP RBPF i w N   described in Section    uses i as the i bound for IJGP i   w as the w cutset bound and N is the number of particles at each time slice  Three values of N were used           and      For EM learning  N was      Experiments were run on a Pentium       GHz machine with  G of RAM  Note that for Model    we only use IJGP RBPF      and IJGP    S because the maximum i bound in this model is bounded by    see section         FINDING DESTINATION OR GOAL OF A PERSON The results for goal prediction with various combinations of models  learning and inference algorithms are shown in Tables      and    We define prediction accuracy as the number of goals predicted correctly  Learning was performed offline  Our slowest learning algorithm based on GBP RBPF      used almost   days of CPU time for Model    and almost   days for Model  significantly less than the period over which the data was collected  The column Time in Tables      and   shows the time for inference algorithms in seconds while the other entries indicate the accuracy for each combination of inference and learning algorithms  In terms of which model yields the best accuracy  we can see that Model   achieves the highest prediction accuracy   of     while Model   and Model   achieve prediction accuracies of     and     respectively or lower  For Model    to verify which algorithm yields the best learned model we see that IJGP RBPF      and IJGP   S yield an accuracy of     and     respectively while for Model    we see that the average accuracy of IJGPRBPF      and IJGP    S was     and     respectively  From these two results  we can see that IJGP RBPF      and IJGP    S are the best performing learning algorithms  For Model   and Model    to verify which algorithm yields the best accuracy given a learned model  we see that IJGP    S is the most cost effective alternative in terms time versus accuracy while IJGP RBPF yields the best accuracy  Table    Goal prediction  Model   N                          Inference IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP    S IJGP    S Average  Time                                          LEARNING IJGP RBPF IJGP S                                                                                                                                                    FINDING THE ROUTE TAKEN BY THE PERSON To see how our models predict a persons route  we use the following method  We first run our inference algorithm on the learned model and predict the route that the person is likely to take  We then super impose this route on the actual route taken by the person  We then count the number of roads that were not taken by the person but were in the predicted route i e  the false positives  and also compute the number of roads that were taken by the person but were not in the actual route i e  the false negatives  The two measures are reported in Table   for the best performing learning models in each category  viz GBP RBPF      for Model   and Model   and GBP RBPF      for Model    As we can see Model   and Model   have the best route prediction accuracy  given by low false positives and false negatives    Table    Goal Prediction Model   N              Inference IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP    S Average  Time                     LEARNING IJGP RBPF      IJGP    S                                       RELATED WORK  Liao et al         and  Patterson et al         describe a model based on AHMEM  Bui        and Hierarchical Markov Models  HMMs  respectively for inferring highlevel behavior from GPS data  Our model goes beyond their model by representing two new variables day of week and time of day which improves the accuracy in our model by about     A mixed network framework for representing deterministic and uncertain information was presented in  Dechter and Larkin        Larkin and Dechter        Dechter and Mateescu         These previous works also describe exact inference algorithms for mixed networks with the restriction that all variables should be discrete  Our work goes beyond these previous works in that we describe approximate inference algorithms for the mixed network framework  allow continuous Gaussian nodes with certain restrictions in the mixed network framework and model discrete time stochastic processes  The approximate inference algorithms called IJGP i  described in  Dechter et al         handled only discrete variables  In our work  we extend this algorithm to include Gaussian variables and discrete constraints  We also develop a sequential version of this algorithm for dynamic models  Particle Filtering is a very attractive research area  Doucet et al          Particle Filtering in HDMNs can be inefficient if non solutions of constraint portion have high probability of being sampled  We show how to alleviate this difficulty by performing IJGP i  before sampling  This algorithm IJGP RBPF yields the best performance in our settings and might prove to be useful in applications in which particle filtering is preferred   Table    Goal Prediction  Model   N                          Inference IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP    S IJGP    S Average  Time                                            LEARNING IJGP RBPF IJGP S                                                                                                                                             Table    False positives  FP  and False negatives for routes taken by a person  FN  N                   INFERENCE IJGP    IJGP    IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF       Model  FP FN                                      Model  FP FN                                      Model  FP FN                       CONCLUSION AND FUTURE WORK In this paper  we introduced a new modeling framework called HDMNs  a representation that handles discrete timestochastic processes  deterministic and probabilistic information on both continuous and discrete variables in a systematic way  We also propose a GBP based algorithm called IJGP i  S for approximate inference in this framework  The main algorithmic contribution of this paper is presenting a class of Rao Blackwellised particle filtering algorithm  IJGP RBPF for HDMNs which integrates a generalized belief propagation component with a RaoBlackwellised Particle Filtering scheme for effective sampling in the presence of constraints  Another contribution of this paper is addressing a complex and highly relevant real life domain using a general framework and domain independent algorithms  Directions for future work include relaxing the restrictions made on dependencies between discrete and continuous variables and developing an efficient EM algorithm   ACKNOWLEDGEMENTS The first and third author were supported in part by National Science Foundation under award numbers         and          The second author was supported in part by the NSF grant IIS           
