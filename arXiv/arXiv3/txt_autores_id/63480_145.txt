 Efficient planning plays a crucial role in model based reinforcement learning  Traditionally  the main planning operation is a full backup based on the current estimates of the successor states  Consequently  its computation time is proportional to the number of successor states  In this paper  we introduce a new planning backup that uses only the current value of a single successor state and has a computation time independent of the number of successor states  This new backup  which we call a small backup  opens the door to a new class of model based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods  We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations      Introduction In reinforcement learning  RL   Kaelbling et al         Sutton   Barto         an agent seeks an optimal control policy for a sequential decision problem in an initially unknown environment  The environment provides feedback on the agents behavior in the form of a reward signal  The agents goal is to maximize the expected return  which is the discounted sum of rewards over future timesteps  An important performance measure in RL is the sample efficiency  which refers to the number of environment interactions that is required to obtain a good policy  Many solution strategies improve the policy by iteratively improving a state value or action value function  which provide estimates of the expected return under a given policy for  environment  states or state action  pairs  respectively  Different approaches for updating these value functions exist  In terms of sample efficiency  one of the most effective approaches is to estimate the environment model using observed samples and to compute  at each time step  the  action  value function that is optimal with respect to the model estimate using planning techniques  A popular planning technique used for this is value iteration  VI   Sutton        Watkins         which performs sweeps of backups through the state or state action space  until the  action  value function has converged  A drawback of using VI is that it is computationally very expensive  making it infeasible for many practical applications  Fortunately  efficient approximations can be obtained by limiting the number of backups that is performed per timestep  A very effective approximation strategy is prioritized sweeping  Moore   Atkeson        Peng   Williams         which prioritizes backups that are expected to cause large value changes  This paper introduces a new backup that enables a dramatic improvement in the efficiency of prioritized sweeping  The main idea behind this new backup is as following  Consider that we are interested in some estimate A that is constructed from a sum of other estimates Xi   The estimate A can be computed using a full backup  X Xi   A i  If the estimates Xi are updated  A can be recomputed by redoing the above backup  Alternatively  if we know that only Xj received a significant value change  we might want to update A for only Xj   Let us indicate the old value of Xj   used to construct the current value of A  as xj   A can then be updated by subtracting this old value and adding the new value  A  A  xj   Xj   This kind of backup  which we call a small backup  is computationally cheaper than the full backup  The   Planning by Prioritized Sweeping with Small Backups  trade off is that  in general  more memory is required for storing the estimates xi associated with A  In planning  where the X estimates correspond to state value estimates and A corresponds to a state or state action estimate  this is not a serious restriction  because a full model is stored already  The additional memory required has the same order of complexity as the memory required for storage of the model  The core advantage of small backups over full backups is that they enable finer control over the planning process  This allows for more effective update strategies  resulting in improved trade offs between computation time and quality of approximation of the VI solution  and hence sample efficiency   We demonstrate this empirically by showing that a prioritized sweeping implementation based on small backups yields a substantial performance improvement over the two classical implementations  Moore   Atkeson        Peng   Williams         In addition  we demonstrate the relevance of small backups in domains with severe constraints on computation time  by showing that a method that performs one small backup per time step has an equal computation time complexity as TD     the classical method that performs one sample backup per timestep  Since sample backups introduce sampling variance  they require a step size parameter to be tuned for optimal performance  Small backups  on the other hand  do not introduce sampling variance  allowing for a parameter free implementation  We empirically demonstrate that the performance of a method that performs one small backup per time step is similar to the optimal performance of TD     achieved by carefully tuning the step size parameter      Reinforcement Learning Framework RL problems are often formalized as Markov decision processes  MDPs   which can be described as tuples hS  A  P  R  i consisting of S  the set of all states  A  s the set of all actions  Psa   P r s  s  a   the transition probability from state s  S to state s when action a  A is taken  Rsa   E r s  a   the reward function giving the expected reward r when action a is taken in state s  and   the discount factor controlling the weight of future rewards versus that of the immediate reward  Actions are selected at discrete timesteps t                according to a policy    S  A          which defines for each action the selection probability conditioned on the state  In general  the goal of RL is to improve the policy in order to increase the return G  which is  the discounted cumulative reward Gt   rt      rt        rt             X   k  rt k    k    where rt   is the reward received after taking action at in state st at timestep t  The prediction task consists of determining the value function V   s   which gives the expected return when policy  is followed  starting from state s  V   s  can be found by making use of the Bellman equations for state values  which state the following  X  V   s    Rs    Pss V   s         s  P P  s   where Rs   a  s  a Rsa and Pss   a  s  a Psa Model based methods use samples to update estimates  of the transition probabilities  Pss   and reward function  Rs   With these estimates  they can iteratively improve an estimate V of V    by performing full backups  derived from Equation      X  V  s   Rs    Pss V  s         s  In the control task  methods often aim to find the optimal policy     which maximizes the expected return  This policy is the greedy policy with respect to the optimal action value function Q  s  a   which gives the expected return when taking action a in state s  and following   thereafter  This function is the solution to the Bellman optimality equation for action values  X  s Q  s  a    Rsa    Psa max Q  s   a          s  a  The optimal value function is related to the optimal action value function through  V   s    maxa Q  s  a   Model based methods can iteratively improve estimates Q of Q by performing full backups derived from Equation      X  s Q s  a   Rsa    Psa max Q s   a          s  a      s s where Rsa and Psa are estimates of Rsa and Psa   respectively   Model free methods do not maintain an model estimate  but update a value function directly from samples  A classical example of a sample backup  based on sample  s  r  s   is the TD    backup  V  s   V  s      r   V  s    V  s     where  is the step size parameter         Planning by Prioritized Sweeping with Small Backups     Small Backup This section introduces the small backup  We start with small state value backups for the prediction task  Section     discusses small action value backups for the control task       Value Backups In this section  we introduce a small backup version of the full backup for prediction  backup     In the introduction  we showed that a small backup requires storage of the component values that make up the current value of a variable  In the case of a small value backup  the component values correspond to the values of successor states  We indicate these values by the function Us   S  S  IR  So  Us  s   is the value estimate of s associated with s  Using Us   V  s  can be updated with only the current value of a single successor state  s   as demonstrated by the following theorem  The three backups shown in the theorem form together the small backup  Theorem     If the current relation between V  s  and Us is given by X  V  s    Rs        Pss Us  s     s  then  after performing the following backups  tmp  V  s     Us  s     V  s   V  s          Pss  V       s    Us  s     tmp             relation     still holds  but Us  s   is updated to V  s    Proof Backup     subtract the component in relation     corresponding to s from V  s  and adds a new component based on the current value estimate of s       V  s   V  s    Pss Us  s      Pss V  s     Hence  relation     is maintained  while Us  s   is updated  Note that V  s   needs to be stored in a temporary variable  since backup     can alter the value of V  s   if s   s       Value Correction after Model Update Theorem     relies on relation     to hold  If the model gets updated  this relation now longer holds  In this section  we discuss how to restore relation     in a computation efficient way for the commonly used model estimate   Pss    Rs     Nss  Ns Rssum  Ns                where Ns counts the number of times state s is vis ited  Nss counts the number of times s is observed as successor state of s  and Rssum is the sum of observed rewards for s  Theorem     If currently  the following relation holds  X  Pss Us  s     V  s    Rs    s    and a sample  s  r  s   is observed  then  after performing the backups      Ns  Ns      Nss  Nss          h i V  s   V  s  Ns       r   Us  s    Ns        the relation still holds  but with updated values for Rs  and Pss   Proof  sketch  Backup      updates V  s  by computing a weighted average of V  s  and r   Us  s    The value change this causes is the same as the value change caused by updating the model and then performing a full backup of s based on Us   Algorithm   shows pseudo code for a general class of prediction methods based on small backups  Surpisngly  while it is a planning method  Rs is never explicitly computed  saving time and memory  Note that the computation per time step is fully independent of the number of successor states  Members of this class need to specify the number of iterations  line    as well as a strategy for selecting state successor pairs  line     Algorithm   Prediction with Small Backups    initialize V  s  arbitrarily for all s    initialize Us  s     V  s   for all s  s     initialize Ns   Nss to   for all s  s    loop  over timesteps     observe transition  s  r  s        Ns  Nsh      Nss  Nss     i                                V  s   V  s  Ns       r    Us  s    Ns loop  for a number of iterations   select a pair  s  s   with Nss     tmp  V  s    V  s   V  s    Nss  Ns   V  s    Us  s    Us  s    tmp end loop end loop       Action value Backups Before we can discuss small action value backups  we have to discuss a more efficient implementation of the   Planning by Prioritized Sweeping with Small Backups  full action value backup  Backup     has a computation time complexity of O  S  A    A more efficient implementation can be obtained by storing statevalues  besides action values  according to V  s    maxa Q s  a   Backup     can then be implemented by  X Q s  a   Rsa    V  s        s  V  s   max Q s  a     a        The combined computation time of these backups is O  S     A    a considerable reduction  Backup      is similar in form as the prediction backup  Hence  we can make a small backup version of it similar to the one in the prediction case  The theorems below are the control versions of the theorems for the prediction case  They can be proven in a similar way as the prediction theorems  Theorem     If the current relation between Q s  a  and Usa is given by X s Q s  a   Rsa   Psa Usa  s          s  then  performing the following backups    s Q s  a   Q s  a    Psa  V  s    Usa  s    Usa  s    V  s      maintains this relation while updating Usa  s   to V  s    Theorem     If relation      holds and a sample  s  a  r  s   is observed  then  after performing backups     s s  Nsa      Nsa  Nsa    h i Q s  a   Q s  a  Nsa       r   Usa  s    Nsa    Nsa  relation      still holds  but with updated values for s Rsa and Psa   A small action value backup is a finer grained version of backup       performing a small backup of Q s  a  for each successor state is equivalent  in computation time complexity and effect  as performing backup      once  While in principle  backup      can be performed after each small backup  it is not very efficient to do so  since small backups make many small changes  More efficient planning can be obtained when backup      is performed only once in a while  In Section    we discuss an implementation of prioritized sweeping based on small action value backups        Small Backups versus Sample Backups A small backup has in common with a sample backup that both update a state value based on the current value of only one of the successor states  In addition  they share the same computation time complexity and their effect is in general smaller than that of a full backup  A disadvantage of a sample backup  with respect to a small backup  is that it introduces sampling variance  caused by a stochastic environment  This requires the use of a step size parameter to enable averaging over successor states  and rewards   A small backup does not introduce sampling variance  since it is implicitly based on an expectation over successor states  Hence  it does not require tuning of a step size parameter for optimal performance  A second disadvantage of a sample backup is that it affects the perceived distribution over action outcomes  which places some restrictions on reusing samples  For example  a model free technique like experience replay  Lin         which stores experience samples in order to replay them at a later time  can introduce bias  which reduces performance  if some samples are replayed more often than others  For small backups this does not hold  since the process of learning the model is independent from the backups based on the model  This allows small backups to be combined with effective selection strategies like prioritized sweeping      Prioritized Sweeping with Small Backups Prioritized sweeping  PS  makes the planning step of model based RL more efficient by using a heuristic  a priority  for selecting backups that favours backups that are expected to cause a large value change  A priority queue is maintained that determines which values are next in line for receiving backups  There are two main implementations  one by Moore   Atkeson        and one by Peng   Williams            All PS methods have in common that they perform backups in what we call update cycles  By adjusting the number of update cycles that is performed per time step  the computation time per time step can be controlled  Below  we discuss in detail what occurs in an update cycle for the two classical PS implementations     We refer to the version of queue Dyna for stochastic domains  which is different from the version for deterministic domains    Planning by Prioritized Sweeping with Small Backups       Classical Prioritized Sweeping Implementations  answer is to put states in the priority queue and to perform backup      for the top state   In the Moore   Atkeson implementation the elements in the queue are states and the backups are full value backups  In control  a full value backup is different from backup      Instead  it is equivalent  in effect and computation time  to performing backup      for each action  followed by backup       Hence  the associated computation time has complexity O  S  A     A     The priority associated with a state is based on the change in action value that has occurred due to small backups  since the last value backup  This priority assures that states with a large discrepancy between the state value and action values  receive a value backup first   An update cycle consists of the following steps  First  the top state is removed from the queue  and receives a full value backup  Let s bet the top state and Vs the value change caused by the backup  Then  for all predecessor state action pairs  s  a  a priority p is computed  using  s p  Psa   Vs            If s is not yet on the queue  then it is added with priority p  If s is on the queue already  but its current priority is smaller than p  then the priority of s is upgraded to p  The Peng   Williams implementation differs from the Moore   Atkeson implementation in that the backup is not a full value backup  Instead  it is a backup with the same effect as a small action value backup  but with a computational complexity of O  S     A    So  it is a cheaper backup than a full backup  but its value change is  much  smaller  The backup requires a state action successor triple  Hence  these triples are the elements on the queue  Predecessors are added to the queue with a priorities that estimate the actionvalue change       Small Backup implementation A natural small backup implementation might appear to be an implementation similar to that of Peng   Williams  but with the main backup implemented more efficiently  The low computational cost of a small backup  however  allows for a much more powerful implementation  The pseudo code of this implementation is shown in Algorithm    Below  we discuss some key characteristics of the algorithm  The computation time of a small backup is so low  that it is comparable to the priority computation in the classical PS implementations  Therefore  instead of computing a priority for each predecessor and performing a backup for the element with the highest priority in the next update cycle  we can perform a small backup for all predecessors  This raises the question of what to put in the priority queue and what type of backup to perform for the top element  The natural  One surprising aspect of the algorithm is that it does not use the function Usa   which forms an essential part of small action value backups  The reason is that due to the specific backup strategy used by the algorithm  Usa  s   is equal to V  s   for all state action pairs  s  a  and all successor states s   Hence  instead of using Usa   V can be used  saving memory and simplifying the code  Table   shows the computation time complexity of an update cycle for the different PS implementations  The small backup implementation is the cheapest one among the three   Moore   Atkeson Peng   Williams small backups  top element backups O  S  A     A   O  S     A   O  A    other O Pre   O Pre   O Pre    Table    Computation time associated with one update cycle for the different PS implementations  Pre indicates the number of predecessors  state action pairs that transition to the state whose value has just been updated      Experimental Results In this section  we evaluate the performance of a minimal version of Algorithm    as well as the performance of Algorithm         Small backup versus Sample backup We compare the performance of TD     which performs one sample backup per time step  with a version of Algorithm   that performs one small backup per time step  Specifically  its number of iterations  line    is    and the selected state successor pair  line    is the pair corresponding to the most recent transition  Their performance is compared on two evaluation tasks  both consisting of    states  laid out in a circle  State transitions only occur between neighbours  The transition probabilities for both tasks are generated by a random process  Specifically  the transition probability to a neighbour state is generated by a random number between   and   and normalized such that   Planning by Prioritized Sweeping with Small Backups  Algorithm   Prioritized Sweeping with Small Backups    initialize V  s  arbitrarily for all s    initialize Q s  a    Qprev  s  a    V  s  for all s  a s    initialize Nsa   Nsa to   for all s  a  s    loop  over episodes     initialize s    repeat  for each step in the episode     select action a  based on Q s       take action a  observe r and s s s    Nsa  Nsa       Nsa  Nsa          Q s  a   Q s  a  Nsa      r   V  s    Nsa  Each time a transition is observed and the corresponding backup is performed  the root mean squared  RMS  error over all states is determined  The average RMS error over the first        transitions  normalized with the initial error  determines the performance  Figure   shows this performance  averaged over     runs  The standard error is negligible  the maximum standard error in the first task was         after normalization  and in the second task         Note that the performance for d     is equal to the performance for       as it should  by definition  The normalized performance for      is    since no learning occurs in this case   p   Q s  a   Qprev  s  a   if s not on queue or p   current priority s  then promote s to p for a number of update cycles do remove top state s from queue for all b  Qprev  s   b   Q s   b  tmp  V  s   V  s    maxb Q s   b  V  V  s    tmp s for all  s  a  pairs with Nsa     do s Q s  a   Q s  a    Nsa  Nsa  V p   Q s  a   Qprev  s  a   if s not on queue or p   current priority s  then promote s to p end for end for s  s until s is terminal end loop  These experiments demonstrate three things  First  the optimal step size can vary a lot between different tasks  Second  selecting a sub optimal step size can cause large performance drops  Third  a small backup  which is parameter free  has a performance similar to the performance of TD    with optimized step size  Since the computational complexity is the same  the small backup is a very interesting alternative to the sample backup in domains with tight constraints on the computation time  where previously only sample backups where viable  Keep in mind that a sample backup does require a model estimate  so if there are also tight constraints on the memory  a sample backup might still be the only option                                                                        the sum of the transition probabilities to the left and right neighbour is    The reward for counter clockwise transitions is always     The reward for clockwise transitions is different for the two tasks  In the first task  a clockwise transition results in a reward of     in the second task  it results in a reward of     The discount factor  is      and the initial state values are    For TD     we performed experiments with a constant step size for values between   and   with steps of       In addition  we performed experiments with a decaying  state dependent step size  according to  s       d   Ns                   where Ns is the number of times state s has been visited  and d specifies the decay rate  We used values of d between   and   with steps of       Note that for d       s       and for d       s      Ns         Prioritized Sweeping We compare the performance of prioritized sweeping with small backups  Algorithm    with the two classical implementations of Moore Atkeson and Peng Williams on the maze task depicted in the top of Figure    The reward received at each time step is    and the discount factor is       The agent can take four actions  corresponding to the four compass directions  which stochastically move the agent to a different square  The bottom of Figure   shows the relative action outcomes of a north action  In free space  an action can result in    possible successor states  each with equal probability  When the agent is close to a wall  this number decreases  To obtain an upper bound on the performance  we also compared against a method that performs value iteration  until convergence  at each time step  using the most recent model estimate  As exploration strategy  the agent select with    probability a random action  instead of the greedy one  On top of that  we use the optimism in the face of uncertainty principle  as also used by Moore   Atkeson  This means that as long as a state action pair has not been visited for at least M times  its value is defined as   Planning by Prioritized Sweeping with Small Backups  normalized RMS error         TD     constant stepsize TD     decaying stepsize small backup                              d                       normalized RMS error         TD     constant stepsize TD     decaying stepsize small backup  two classical implementations  The results also show that the Peng   Williams method performs considerably worse than the one of Moore   Atkeson in the considered domain  This can be explained by the different backups they perform  The effect of the backup of Peng   Williams is proportional to the transition     In contrast  probability  which in most cases is    the Moore   Atkeson method performs a full backup each update cycle  While the small backup implementation also uses backups that are proportional to the transition probability  it performs a lot more backups per update cycle  Specifically  a number that is proportional to the number of predecessors  In general  this number will increase when the stochasticity of the domain increases                               d       Figure    Average RMS error over the first        observations  normalized by the initial error  for different values of the step size parameter   in case of constant step size  or different values of the decay parameter d  in case of decaying step size  The top graph corresponds with the first evaluation task  the bottom graph with the second   some optimistically defined value    for our maze task   instead of the value based on the model estimate  We optimized M for the value iteration method  resulting in M      and used this value for all methods   S G                                                                                                                                               We performed experiments for         and    update cycles per time step  Figure   shows the average return over the first     episodes for the different methods  The results are averaged over     runs  The maximum standard deviation is     for all methods  except for the method of Peng   Williams  which had a maximum standard deviation of       Figure    Above  the maze task  in which the agent must travel from S tothe G  Below  transition probabilities     of a north action for different positions of the agent       indicated by the circle  with respect to the walls  black squares    The computation time per update cycle was about the same for the three different PS implementations  with a small advantage for the small backup implementation  which shows that the O Pre   computation  see Table    is dominant in this task  The computation time per observation of the value iteration method was more than     times as high as a single update cycle      Discussion  PS with small backups turns out to be very effective  With only a single update cycle  the value iteration result can be closely approximated  in contrast to the  Prioritized sweeping can be viewed as a generalization of the idea of replaying of experience in backward order  Lin         which by itself is related to eligibility traces  Sutton        Watkins        Sutton   Singh         What all these techniques have in common is that new information  which can be value changes  but at its core all value changes originate from new data  is propagated backwards  Whereas backward replay and eligibility traces use the recent trajectory for backward   Planning by Prioritized Sweeping with Small Backups  makes new  more efficient  update strategies possible  In addition  small backups can be useful in domains with very tight time constraints  offering a parameterfree alternative to sample backups  which were up to now often the only feasible option for such domains       avg  return over first     episodes                  
  incomplete on each step  still efficiently computes optimal actions in a timely manner   We consider the problem of efficiently learning optimal control policies and value functions over large state spaces in an online setting in which estimates must be available after each interaction with the world  This paper develops an explicitly model based approach extending the Dyna architecture to linear function approximation  Dynastyle planning proceeds by generating imaginary experience from the world model and then applying model free reinforcement learning algorithms to the imagined state transitions  Our main results are to prove that linear Dyna style planning converges to a unique solution independent of the generating distribution  under natural conditions  In the policy evaluation setting  we prove that the limit point is the least squares  LSTD  solution  An implication of our results is that prioritized sweeping can be soundly extended to the linear approximation case  backing up to preceding features rather than to preceding states  We introduce two versions of prioritized sweeping with linear Dyna and briefly illustrate their performance empirically on the Mountain Car and Boyan Chain problems   The Dyna architecture  Sutton       provides an effective and flexible approach to incremental planning while maintaining responsiveness  There are two ideas underlying the Dyna architecture  One is that planning  acting  and learning are all continual  operating as fast as they can without waiting for each other  In practice  on conventional computers  each time step is shared between planning  acting  and learning  with proportions that can be set arbitrarily according to available resources and required response times   Online learning and planning  Efficient decision making when interacting with an incompletely known world can be thought of as an online learning and planning problem  Each interaction provides additional information that can be used to learn a better model of the worlds dynamics  and because this change could result in a different action being best  given the model   the planning process should be repeated to take this into account  However  planning is inherently a complex process  on large problems it not possible to repeat it on every time step without greatly slowing down the response time of the system  Some form of incremental planning is required that  though  The second idea underlying the Dyna architecture is that learning and planning are similar in a radical sense  Planning in the Dyna architecture consists of using the model to generate imaginary experience and then processing the transitions of the imaginary experience by model free reinforcement learning algorithms as if they had actually occurred  This can be shown  under various conditions  to produce exactly the same results as dynamic programming methods in the limit of infinite imaginary experience  The original papers on the Dyna architecture and most subsequent extensions  e g   Singh       Peng   Williams       Moore   Atkeson       Kuvayev   Sutton       assumed a Markov environment with a tabular representation of states  This table lookup representation limits the applicability of the methods to relatively small problems  Reinforcement learning has been combined with function approximation to make it applicable to vastly larger problems than could be addressed with a tabular approach  The most popular form of function approximation is linear function approximation  in which states or state action pairs are first mapped to feature vectors  which are then mapped in a linear way  with learned parameters  to value or next state estimates  Linear methods have been used in many of the successful large scale applications of reinforcement learning  e g   Silver  Sutton   Muller       Schaeffer  Hlynka   Jussila        Linear function approximation is also simple  easy to understand  and possesses some of the strongest convergence and performance guarantees among function approximation methods  It is   natural then to consider extending Dyna for use with linear function approximation  as we do in this paper  There has been little previous work addressing planning with linear function approximation in an online setting  Paduraru        treated this case  focusing mainly on sampling stochastic models of a cascading linear form  but also briefly discussing deterministic linear models  Degris  Sigaud and Wuillemin        developed a version of Dyna based on approximations in the form of dynamic Bayes networks and decision trees  Their system  SPITI  included online learning and planning based on an incremental version of structured value iteration  Boutilier  Dearden   Goldszmidt        Singh        developed a version of Dyna for variable resolution but still tabular models  Others have proposed linear least squares methods for policy evaluation that are efficient in the amount of data used  Bradtke   Barto       Boyan             Geramifard  Bowling   Sutton        These methods can be interpreted as forming and then planning with a linear model of the worlds dynamics  but so far their extensions to the control case have not been well suited to online use  Lagoudakis   Parr       Peters  Vijayakumar   Schaal       Bowling  Geramifard    Wingate        whereas our linear Dyna methods are naturally adapted to this case  We discuss more specifically the relationship of our work to LSTD methods in a later section  Finally  Atkeson        and others have explored linear  learned models with off line planning methods suited to low dimensional continuous systems      Notation  We use the standard framework for reinforcement learning with linear function approximation  Sutton   Barto        in which experience consists of the time indexed stream s    a    r    s    a    r    s           where st  S is a state  at  A is an action  and rt  R is a reward  The actions are selected by a learning agent  and the states and rewards are selected by a stationary environment  The agent does not have access to the states directly but only through a corresponding feature vector t  Rn    st    The n agent selects actions P according to a policy     R  A         such that aA    a         An important step towards finding a good policy is to estimate the value function for a given policy  policy evaluation   The value function is approximated as a linear function with parameter vector   Rn       X    s   V   s    E  t  rt   s    s   t    where           In this paper we consider policies that are greedy or   greedy with respect to the approximate statevalue function   Algorithm     Linear Dyna for policy evaluation  with random sampling and gradient descent model learning Obtain initial     F  b For each time step  Take action a according to the policy  Receive r          r            F  F       F    b  b    r  b    temp    Repeat p times  planning   Generate a sample  from some distribution     F  r  b         r              temp     Theory for policy evaluation  The natural place to begin a study of Dyna style planning is with the policy evaluation problem of estimating a statevalue function from a linear model of the world  The model consists of a forward transition matrix F  Rn  Rn  incorporating both environment and policy  and an expected reward vector b  Rn   constructed such that F  and b   can be used as estimates of the feature vector and reward that follow   A Dyna algorithm for policy evaluation goes through a sequence of planning steps  on each of which a starting feature vector  is generated according to a probability distribution   and then a next feature vector     F  and next reward r   b   are generated from the model  Given this imaginary experience  a conventional modelfree update is performed  for example  according to the linear TD    algorithm  Sutton              r                   or according to the residual gradient algorithm  Baird              r                         where      is a step size parameter  A complete algorithm using TD     including learning of the model  is given in Algorithm         Convergence and fixed point  There are two salient theoretical questions about the Dyna planning iterations     and      Under what conditions on  and F do they converge  and What do they converge to  Both of these questions turn out to have interesting answers  First  note that the convergence of     is in question in part because it is known that linear TD    may diverge if the distribution of starting states during training does not match the distribution created by the normal dynamics of   the system  that is  if TD    is used off policy  This suggests that the sampling distribution used here    might have to be strongly constrained in order for the iteration to be stable  On the other hand  the data here is from the model  and the model is not a general system  it is deterministic  and linear  This special case could be much better behaved  In fact  convergence of linear Dyna style policy evaluation  with either the TD    or residual gradient iterations  is not affected by   but only by F   as long as  exercises all directions in the full n dimensional vector space  Moreover  not only is the fact of convergence unaffected by   but so is the value converged to  In fact  we show below that convergence is to a deterministic fixed point  a value of  such that the iterations     and     leave it unchanged not just in expected value  but for every individual  that could be generated by   The only way this could be true is if the TD error  the first expression in parentheses in each iteration  were exactly zero  that is  if       r                 b     F          b   F           And the only way that this can be true for all  is for the expression in parenthesis above to be zero     Before verifying the conditions of this result  let us rewrite     in terms of the matrix G   I  F   k      k   k  b  k   k   F  I k  k    b    F    I      k   k sk        I  F      b        where    Rn is P arbitrary  AssumeP that  i  the step size   sequence satisfies k   k     k   k       ii  r F        iii   k   are uniformly   bounded   i i d  random variables  and that  iv  C   E k   is non singular  k Then the parameter vector k converges with probability one to  I  F      b     k   k  b  k  k  Gk  k  Here sk is defined by the last equation       assuming that the inverse exists  Note that this expression for the fixed point does not depend on   as promised  If I  F   is nonsingular  then there might be no fixed point  This could happen for example if F were an expansion  or more generally if the limit  F   were not zero  These cases correspond to world models that say the feature vectors diverge to infinity over time  Failure to converge in these cases should not be considered a problem for the Dyna iterations as planning algorithms  these are cases in which the planning problem is ill posed  If the feature vectors diverge  then so too may the rewards  in which case the true values given the model are infinite  No real finite Markov decision process could behave in this way  It remains to show the conditions on F under which the iterations converge to the fixed point if one exists  We prove next that under the TD    iteration      convergence is guaranteed if the numerical radius of F is less than one   and    k     k   k  b  k   k  F k  k  k  k      b   F       which immediately implies that   Theorem      Convergence of linear TD    Dyna for policy evaluation   Consider the TD    iteration with a nonnegative step size sequence  k     Proof  The idea of the proof is to view the algorithm as a stochastic gradient descent method  In particular  we apply Proposition     of  Bertsekas   Tsitsiklis               then that under the residual gradient iteration      convergence is guaranteed for any F as long as the fixed point exists  That F s numerical radius be less than   is a stronger condition than nonsingularity of I  F     but it is similar in that both conditions pertain to the matrix trending toward expansion when multiplied by itself   The model is deterministic because it generates the expectation of the next feature vector  the system itself may be stochastic    The numerical radius of a real valued square matrix A is defined by r A    maxkxk     xT Ax   The cited proposition requires the definition of a potential function J   and will allow us to conclude that limk J k       with probability one  Let   us choose J         E  b  k     F k    k      Note that by our i i d  assumptions on the features  J   is welldefined  We need to check four conditions  because the step size conditions are automatically satisfied    i  The nonnegativity of the potential function   ii  The Lipschitz continuity of J     iii  The pseudo gradient property of the expected update direction  and  iv  The boundedness of the  expected  magnitude of the update  more precisely that E ksk k    k  O kJ k  k      Nonnegativity is satisfied by definition and the boundedness condition  iv  is satisfied thanks to the boundedness of the features  Let us show now that the pseudo gradient property  iii  is satisfied  This condition requires the demonstration of a positive constant c such that ckJ k  k    J k    E  sk  k           Define sk   E  sk  k     Cb  CG  k   A simple calculation gives J k     Gsk   Hence kJ k  k           s  k G Gsk and  J k    sk   sk Gsk   Therefore           is equivalent to c sk G Gsk  sk Gsk   In order to make this true with a sufficiently small c  it suffices to show that   s  Gs     holds for any non zero vector s  An elementary reasoning shows that this is equivalent to     G   G    being positive definite  which in turn is equivalent to r F       showing that  iii  is satisfied  Hence  we have verified all the assumptions of the cited proposition and can therefore we conclude that limk J k       with probability one  Plugging in the expression of J k    we get limt  CbCG  k        Because C and G are invertible  this latter follows from r F        it follows that the limit of k exists and limk k    G     b    I  F      b   verges with probability one to  I  F      b  assuming that  I  F     is non singular  Proof  As all the conditions of Proposition     of  Bertsekas   Tsitsiklis       are trivially satisfied with the choice J     E  J   k     we can conclude that k converges w p   to the minimizer of J    In the previous theorem we have seen that the minimizer of J   is indeed     I  F      b  finishing the proof       Convergence to the LSTD solution  Several extensions of this result are possible  First  the requirement of i i d  sampling can be considerably relaxed  With an essentially unchanged proof  it is possible to show that the theorem remains true if the feature vectors are generated by a Markov process given that they satisfy appropriate ergodicity conditions  Moreover  building on a result by Delyon         one can show that the result continues to hold even if the sequence of features is generated in an algorithmic manner  again provided that some ergodicity conditions are met  PKThe major assumption then is that C   limK   K k   k   k exists and is nonsingular  Further  because there is no noise to reject  there is noP need to decay the step sizes towards zero  the condi tion k   k      in the proofs is used to filter out noise   In particular  we conjecture that sufficiently small constant step sizes would work as well  for a result of this type see Proposition     by Bertsekas   Tsitsiklis         So far we have discussed the convergence of planning given a model  but we have said nothing about the relationship of the model to data  or about the quality of the resultant solution  Suppose the model were the best linear fit to a finite dataset of observed feature vector to feature vector transitions with accompanying rewards  In this case we can show that the fixed point of the Dyna updates is the least squares temporal difference solution  This is the solution for which the mean TD    update is zero and is also the solution found by the LSTD    algorithm  Barto   Bradtke         On the other hand the requirement on the numerical radius of F seems to be necessary for the convergence of the TD    iteration  By studying the ODE associated with      we see that it is stable if and only if CG is a positive stable matrix  i e   iff all its eigenvalues have positive real part   From this it seems necessary to require that G is positive stable  However  to ensure that CG is positive stable the strictly stronger condition that G   G  is positive definite must be satisfied  This latter condition is equivalent to r F        Proof  It suffices to show that the respective solution sets of the equations  We turn now to consider the convergence of Dyna planning using the residual gradient Dyna iteration      This update rule can be derived by taking the gradient of J   k      b  k     k    k    w r t    Thus  as an immediate consequence of Proposition     of  Bertsekas   Tsitsiklis       we get the following result  Theorem      Convergence of residual gradient Dyna for policy evaluation   Assume that k is updated according to k     k   k  b  k   k  F k  k  k   k  F k    where    Rn is arbitrary  Assume that the non negative step size sequence  k   satisfies the summability condition  i  of Theorem     and that  k   are uniformly bounded i i d  random variables  Then the parameter vector k con   Theorem      Given a training dataset of feature  reward  next state feature triples D        r                 n   rn    n    let F  bPbe the least squares model built on D  Assume that n C   k   k   k has full rank  Then the solution     is the same as the LSTD solution on this training set         n X  k  rk     k        k          k          b    F    I        are the same  This is because the LSTD parameter vectors are obtained by solving the first equation and the TD    Dyna solutions are derived from the second equation  Pn Pn Let D   k   k   k      and r   k   k rk   A standard calculation shows that F      C   D  and b   C   r   Plugging in C  D into     and factoring out  shows that any solution of     also satisfies       r    D  C          If we multiply both sides of     by C   from the left we get      Hence any solution of     is also a solution of      Because all the steps of the above derivation are reversible  we get that the reverse statement holds as well    Algorithm     Linear Dyna with PWMA prioritized sweeping  policy evaluation  Obtain initial     F  b For each time step  Take action a according to the policy  Receive r      r                 F  F       F    b  b    r  b    For all i such that  i        For all j such that F ij       Put j on the PQueue with priority  F ij  i   Repeat p times while PQueue is not empty  i  pop the PQueue   b i      F ei   i   i    i     For all j such that F ij       Put j on the queue with priority  F ij           Algorithm     Linear Dyna with MG prioritized sweeping  policy evaluation  Obtain initial     F  b For each time step  Take action a according to the policy  Receive r      r                 F  F       F    b  b    r  b    For all i such that  i        Put i on the PQueue with priority   i   Repeat p times while PQueue is not empty  i  pop the PQueue For all j such that F ij         b j      F ej   j   j    j     Put j on the PQueue with priority         Linear prioritized sweeping  We have shown that the convergence and fixed point of policy evaluation by linear Dyna are not affected by the way the starting feature vectors are chosen  This opens the possibility of selecting them cleverly so as to speed the convergence of the planning process  One natural ideathe idea behind prioritized sweepingis to work backwards from states that have changed in value to the states that lead into them  The lead in states are given priority for being updated because an update there is likely to change the states value  because they lead to a state that has changed in value   If a lead in state is updated and its value is changed  then its lead in states are in turn given priority for updating  and so on  In the table lookup context in which this idea was developed  Moore   Atkeson       Peng       see also Wingate   Seppi        there could be many states preceding each changed state  but only one could be updated at a time  The states waiting to be updated were kept in a queue  prioritized by the size of their likely effect on the value function  As high priority states were popped off the queue and updated  it would sometimes give rise to highly efficient sweeps of updates across the state space  this is what gave rise to the name prioritized sweeping  With function approximation it is not possible to identify and work backwards from individual states  but alternatively one could work backwards feature by feature  If there has just been a large change in  i   the component of the parameter vector corresponding to the ith feature  then one can look backwards through the model to find the features j whose components  j  are likely to have changed as a result  These are the features j for which the elements F ij of F are large  One can then preferentially construct  starting feature vectors  that have non zero entries at these j components  In our algorithms we choose the starting vectors to be the unit basis vectors ej   all of whose components are zero except the jth  which is     Our theoretical results assure us that this cannot affect the result of convergence   Using unit basis vectors is very efficient computationally  as the vector matrix multiplication F  is reduced to pulling out a single column of F   There are two tabular prioritized sweeping algorithms in the literature  The first  due simultaneously to Peng and Williams        and to Moore and Atkeson         which we call PWMA prioritized sweeping  adds the predecessors of every state encountered in real experience to the priority queue whether or not the value of the encountered state was significantly changed  The second form of prioritized sweeping  due to McMahan and Gordon         and which we call MG prioritized sweeping  puts each encountered state on the queue  but not its predecessors  For McMahan and Gordon this resulted in a more efficient planner  A complete specification of our feature by feature versions of these two forms of prioritized sweeping are given above  with TD    updates and gradient descent model learning  as Algorithms   and    These algorithms differ slightly from previous prioritized sweeping algorithms in that they update the value function from the real experiences and not just from model generated experience  With function approximation  real experience is always more informative than model generated experience  which will be distorted by the function approximator  We found this to be a significant effect in our empirical experiments  Section       Algorithm    Linear Dyna with MG prioritized sweeping and TD    updates  control  Obtain initial     F  b For each time step       a  arg maxa b   or   greedy  a     Fa  Take action a  receive r      r                 Fa  Fa       Fa    ba  ba    r  b  a   For all i such that  i        Put i on the PQueue with priority   i   Repeat p times while PQueue is not empty  i  pop the PQueue ij For all j s t  there   exists an a s t  F   a         maxa ba  j     Fa ej   j   j    j     Put j on the PQueue with priority            Theory for Control  We now turn to the full case of control  in which separate models Fa   ba are learned and are then available for each action a  These are constructed such that Fa  and b  a  can be used as estimates of the feature vector and reward that follow  if action a is taken  A linear Dyna algorithm for the control case goes through a sequence of planning steps on each of which a starting feature vector  and an action a are chosen  and then a next feature vector     Fa  and next reward r   ba  are generated from the model  Given this imaginary experience  a conventional model free update is performed  The simplest case is to again apply      A complete algorithm including prioritized sweeping is given in Algorithm    The theory for the control case is less clear than for policy evaluation  The main issue is the stability of the mixture of the forward model matrices  The corollary below is stated for an i i d  sequence of features  but by the remark after Theorem     it can be readily extended to the case where the policy to be evaluated is used to generate the trajectories  Corollary      Convergence of linear TD    Dyna with action models   Consider the Dyna recursion     with the modification that in each step  instead of F k   we use F k   k   where  is a policy mapping feature vectors to actions and  Fa   is a collection of forward model matrices  Similarly  b  k is replaced by b   k   k   As before  assume that k is an unspecified i i d  process  Let  F  b    be the least squares   model of   F   harg minG E kGk  iF k   k k   and b     arg minu E  u  k  b  If the numerical radius  k   k   of F is bounded by one  then the conclusions of Theo       N                   N                                                                                                                      Figure    The general Boyan Chain problem  rem     hold  the parameter vector k converges with probability one to  I  F      b  Proof  The proof is immediate from equation     the normal     for F   which states that E F k     E F k   k   k k   and once we observe that  in the proof of  Theorem       F appears only in expressions of the form E F k   k   As in the case of policy evaluation  there is a corresponding corollary for the residual gradient iteration  with an immediate proof  These corollaries say that  for any policy with a corresponding model that is stable  the Dyna recursion can be used to compute its value function  Thus we can perform a form of policy iterationcontinually computing an approximation to the value function for the greedy policy      Empirical results  In this section we illustrate the empirical behavior of the four Dyna algorithms and make comparisons to model free methods using variations of two standard test problems  Boyan Chain and Mountain Car  Our Boyan Chain environment is an extension of that by Boyan              from    to    states  and from   to    features  Geramifard  Bowling   Sutton        Figure   depicts this environment in the general form  Each episode starts at state N      and terminates in state    For all states s      there is an equal probability of transitioning to states s    or s    with a reward of    From states   and    there are deterministic transitions to states   and   with respective rewards of   and    Our Mountain Car environment is exactly as described by Sutton        Sutton   Barto        re implemented in Matlab  An underpowered car must be driven to the top of a hill by rocking back and forth in a valley  The state variables are a pair  position velocity  initialized to            at the beginning of each episode  The reward is   per time step  There are three discrete actions  accelerate  reverse  and coast   We used a value function representation based on tile coding feature vectors exactly as in Suttons        experiments  with    tilings over the combined  position  velocity  pair  and with the tiles hashed down to        features  In the policy evaluation experiments with this domain  the policy was to accelerate in         Boyan chain         Mountain Car  x     Dyna Random TD        Loss  Dyna Random Dyna PWMA  Loss         TD         Dyna MG  Dyna PWMA     Dyna MG                      Episode                              Episode             Figure    Performance of policy evaluation methods on the Boyan Chain and Mountain Car environments the direction of the current velocity  and we added noise to the domain that switched the selected action to a random action with     probability  Complete code for our test problems as standard RL Glue environments is available from the RL Library hosted at the University of Alberta  In all experiments  the step size parameter  took the form      t     NN   t       in which t is the episode number and the pair  N        was selected based on empirically finding the best combination out of                 and N                     separately for each algorithm and domain  All methods observed the same trajectories in policy evaluation  All graphs are averages of    runs  error bars indicate standard errors in the means  Other parameter settings were                and       We performed policy evaluation experiments with four algorithms  Dyna Random  Dyna PWMA  Dyna MG  as in Algorithms      and model free TD     In the case of the Dyna Random algorithm  the starting feature vectors in planning were chosen to be unit basis vectors with the   in a random location  Figure   shows the policy evaluation performance of the four methods in the Boyan Chain and Mountain Car environments  For the Boyan Chain domain  the loss was the root mean squared error of the learned value function compared to the exact analytical value  averaged over all states  In the Mountain Car domain  the states are visited very non uniformly  and a more sophisticated measure is needed  Note that all of the methods drive  toward an asymptotic value in which the expected TD    update is zero  we can use the distance from this as a loss measure  Specifically  we evaluated each learned value function by freezing it and then running a fixed set of         episodes with it while running the TD    algorithm  but not allowing  to actually change   The norm of the sum of the  attempted  update vectors was then computed and used as the loss  In practice  this measure can be computed very efficiently as   A   b     in the notation of  LSTD     see Bradtke   Barto        In the Boyan Chain environment  the Dyna algorithms generally learned more rapidly than model free TD     DynaMG was initially slower than the other algorithms  then caught up and surpassed them  The relatively poor early performance of Dyna MG was actually due to its being a better planning method  After few episodes the model tends to be of very high variance  and so therefore is the best value function estimate given it  We tested this hypothesis by running the Dyna methods starting with a fixed  well learned model  in this case Dyna MG was the best of all the methods from the beginning  All of these data are for one step of planning for each real step of interaction with the world  p       In preliminary experiments with larger values of p  up to p       we found further improvements in learning rate of the Dyna algorithms over TD     and again Dyna MG was best  The results for Mountain Car are less clear  Dyna MG quickly does significantly better than TD     but the other Dyna algorithms lag initially and never surpass TD     Note that  for any value of p  Dyna MG does many more  updates than the other two Dyna algorithms  because these updates are in an inner loop  cf  Algorithms   and     Even so  because of its other efficiencies Dyna MG tended to run faster overall in our implementation  Obviously  there is a lot more interesting empirical work that could be done here  We performed one Mountain Car experiment with DynaMG as a control algorithm  Algorithm     comparing it with model free Sarsa  i e   Algorithm   with p       The results are shown in Figure    As before  Dyna MG showed a distinct advantage over the model free method in terms of learning rate  There was no clear advantage for either method in the second half of the experiment  We note that  asymptotically  model free methods are never worse than model based methods  and are often better because the model does not converge exactly to the true system because               Return       Dyna MG           Sarsa                           Episode           Figure    Control performance on Mountain Car  Conclusion  In this paper we have taken important steps toward establishing the theoretical and algorithmic foundations of Dyna style planning with linear function approximation  We have established that Dyna style planning with familiar reinforcement learning update rules converges under weak conditions corresponding roughly  in some cases  to the existence of a finite solution to the planning problem  and that convergence is to a unique least squares solution independent of the distribution used to generate hypothetical experience  These results make possible our second main contribution  the introduction of algorithms that extend prioritized sweeping to linear function approximation  with correctness guarantees  Our empirical results illustrate the use of these algorithms and their potential for accelerating reinforcement learning  Overall  our results support the conclusion that Dyna style planning may be a practical and competitive approach to achieving rapid  online control in stochastic sequential decision problems with large state spaces  Acknowledgements  of structural modeling assumptions   The case we treat herelinear models and value functions with one step TD methodsis a rare case in which asymptotic performance of model based and model free methods should be identical   The benefit of models  and of planning generally  is in rapid adaptation to new problems and situations   The authors gratefully acknowledge the substantial contributions of Cosmin Paduraru and Mark Ring to the early stages of this work  This research was supported by iCORE  NSERC and Alberta Ingenuity   These empirical results are not extensive and in some cases are preliminary  but they nevertheless illustrate some of the potential of linear Dyna methods  The results on the Boyan Chain domain show that Dyna style planning can result in a significant improvement in learning speed over modelfree methods  In addition  we can see trends that have been observed in the tabular case re occurring here with linear function approximation  In particular  prioritized sweeping can result in more efficient learning than simply updating features at random  and the MG version of prioritized sweeping seems to be better than the PWMA version   Atkeson  C          Using local trajectory optimizers to speed up global optimization in dynamic programming  Advances in Neural Information Processing Systems             Baird  L  C          Residual algorithms  Reinforcement learning with function approximation  In Proceedings of the Twelfth International Conference on Machine Learning  pp        Bertsekas  Dimitri P   Tsitsiklis  J          Neuro Dynamic Programming  Athena Scientific        Boutilier  C   Dearden  R   Goldszmidt  M          Stochastic dynamic programming with factored representations  Artificial Intelligence             Bowling  M   Geramifard  A   Wingate  D          Sigma point policy iteration  In Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems  Boyan  J  A          Least squares temporal difference learning  In Proceedings of the Sixteenth International Conference on Machine Learning        Boyan  J  A          Technical update  Least squares temporal difference learning  Machine Learning              Bradtke  S   Barto  A  G          Linear least squares al   Finally  we would like to note that we have done extensive experimental work  not reported here  attempting to adapt least squares methods such as LSTD to online control domains  in particular to the Mountain Car problem  A major difficulty with these methods is that they place equal weight on all past data whereas  in a control setting  the policy changes and older data becomes less relevant and may even be misleading  Although we have tried a variety of forgetting strategies  it is not easy to obtain online control performance with these methods that is superior to modelfree methods  One reason we consider the Dyna approach to be promising is that no special changes are required for this case  it seems to adapt much more naturally and effectively to the online control setting   
 This paper presents the first actor critic algorithm for off policy reinforcement learning  Our algorithm is online and incremental  and its per time step complexity scales linearly with the number of learned weights  Previous work on actor critic algorithms is limited to the on policy setting and does not take advantage of the recent advances in offpolicy gradient temporal difference learning  Off policy techniques  such as Greedy GQ  enable a target policy to be learned while following and obtaining data from another  behavior  policy  For many problems  however  actor critic methods are more practical than action value methods  like Greedy GQ  because they explicitly represent the policy  consequently  the policy can be stochastic and utilize a large action space  In this paper  we illustrate how to practically combine the generality and learning potential of off policy learning with the flexibility in action selection given by actor critic methods  We derive an incremental  linear time and space complexity algorithm that includes eligibility traces  prove convergence under assumptions similar to previous off policy algorithms    and empirically show better or comparable performance to existing algorithms on standard reinforcement learning benchmark problems   The reinforcement learning framework is a general temporal learning formalism that has  over the last    See errata in section B  Appearing in Proceedings of the    th International Conference on Machine Learning  Edinburgh  Scotland  UK        Copyright      by the author s  owner s    few decades  seen a marked growth in algorithms and applications  Until recently  however  practical online methods with convergence guarantees have been restricted to the on policy setting  in which the agent learns only about the policy it is executing  In an off policy setting  on the other hand  an agent learns about a policy or policies different from the one it is executing  Off policy methods have a wider range of applications and learning possibilities  Unlike onpolicy methods  off policy methods are able to  for example  learn about an optimal policy while executing an exploratory policy  Sutton   Barto         learn from demonstration  Smart   Kaelbling         and learn multiple tasks in parallel from a single sensorimotor interaction with an environment  Sutton et al          Because of this generality  off policy methods are of great interest in many application domains  The most well known off policy method is Q learning  Watkins   Dayan         However  while Q Learning is guaranteed to converge to the optimal policy for the tabular  non approximate  case  it may diverge when using linear function approximation  Baird         Least squares methods such as LSTD  Bradtke   Barto        and LSPI  Lagoudakis   Parr        can be used off policy and are sound with linear function approximation  but are computationally expensive  their complexity scales quadratically with the number of features and weights  Recently  these problems have been addressed by the new family of gradientTD  Temporal Difference  methods  e g   Sutton et al          such as Greedy GQ  Maei et al          which are of linear complexity and convergent under off policy training with function approximation  All action value methods  including gradient TD methods such as Greedy GQ  suffer from three important limitations  First  their target policies are deterministic  whereas many problems have stochastic optimal policies  such as in adversarial settings or in par    Off Policy Actor Critic  tially observable Markov decision processes  Second  finding the greedy action with respect to the actionvalue function becomes problematic for larger action spaces  Finally  a small change in the action value function can cause large changes in the policy  which creates difficulties for convergence proofs and for some real time applications  The standard way of avoiding the limitations of actionvalue methods is to use policy gradient algorithms  Sutton et al         such as actor critic methods  e g   Bhatnagar et al          For example  the natural actor critic  an on policy policy gradient algorithm  has been successful for learning in continuous action spaces in several robotics applications  Peters   Schaal         The first and main contribution of this paper is to introduce the first actor critic method that can be applied off policy  which we call Off PAC  for Off Policy ActorCritic  Off PAC has two learners  the actor and the critic  The actor updates the policy weights  The critic learns an off policy estimate of the value function for the current actor policy  different from the  fixed  behavior policy  This estimate is then used by the actor to update the policy  For the critic  in this paper we consider a version of Off PAC that uses GTD    Maei         a gradient TD method with eligibitity traces for learning state value functions  We define a new objective for our policy weights and derive a valid backward view update using eligibility traces  The time and space complexity of Off PAC is linear in the number of learned weights  The second contribution of this paper is an off policy policy gradient theorem and a convergence proof for Off PAC when       under assumptions similar to previous off policy gradient TD proofs   Our third contribution is an empirical comparison of Q    Greedy GQ  Off PAC  and a soft max version of Greedy GQ that we call Softmax GQ  on three benchmark problems in an off policy setting  To the best of our knowledge  this paper is the first to provide an empirical evaluation of gradient TD methods for off policy control  the closest known prior work is the work of Delp          We show that Off PAC outperforms other algorithms on these problems      Notation and Problem Setting In this paper  we consider Markov decision processes with a discrete state space S  a discrete action space A  a distribution P   S  S  A          where P  s   s  a     See errata in section B  is the probability of transitioning into state s  from state s after taking action a  and an expected reward function R   S  A  S  R that provides an expected reward for taking action a in state s and transitioning into s    We observe a stream of data  which includes states st  S  actions at  A  and rewards rt  R for t               with actions selected from a fixed behavior policy  b a s           Given a termination condition    S          Sutton et al          we define the value function for    S A         to be  V    s    E  rt             rt T  st   s  s  S       where policy  is followed from time step t and terminates at time t   T according to   We assume termination always occurs in a finite number of steps  The action value function  Q   s  a   is defined as  Q   s  a    X P  s   s  a  R s  a  s       s   V    s           s  S  for for all s  S  Note that V    s    P all a  A and    s  a   for all s  S  aA  a s Q The policy u   A  S         is an arbitrary  differentiable function of a weight vector  u  RNu   Nu  N  with u  a s      for all s  S  a  A  Our aim is to choose u so as to maximize the following scalar objective function  J  u      X  db  s V u    s        sS  where db  s    limt P  st   s s    b  is the limiting distribution of states under b and P  st   s s    b  is the probability that st   s when starting in s  and executing b  The objective function is weighted by db because  in the off policy setting  data is obtained according to this behavior distribution  For simplicity of notation  we will write  and implicitly mean u       The Off PAC Algorithm In this section  we present the Off PAC algorithm in three steps  First  we explain the basic theoretical ideas underlying the gradient TD methods used in the critic  Second  we present our off policy version of the policy gradient theorem  Finally  we derive the forward view of the actor and convert it to a backward view to produce a complete mechanistic algorithm using eligibility traces    Off Policy Actor Critic       The Critic  Policy Evaluation Evaluating a policy  consists of learning its value function  V    s   as defined in Equation    Since it is often impractical to explicitly represent every state s  we learn a linear approximation of V    s   V  s    vT xs where xs  RNv   Nv  N  is the feature vector of the state s  and v  RNv is another weight vector  Gradient TD methods  Sutton et al         incrementally learn the weights  v  in an off policy setting  with a guarantee of stability and a linear per time step complexity  These methods minimize the  weighted mean squared projected Bellman error  MSPBE v      V  T  V    D where V   Xv  X is the matrix whose rows are all xs    is the decay of the eligibility trace  D is a matrix with db  s  on its diagonal   is a projection operator that projects a value function to the nearest representable value function given the function approximator  and T  is the  weighted Bellman operator for the target policy  with termination probability   e g   see Maei   Sutton         For a linear representation     X X T DX   X T D  In this paper  we consider the version of Off PAC that updates its critic weights by the GTD   algorithm introduced by Maei          The two theorems below provide justification for this approximation    Theorem    Policy Improvement   Given any policy parameter u  let u    u    g u  Then there exists an       such that  for all positive       J  u     J  u  Further  if  has a tabular representation  i e   separate weights for each state   then V u     s   V u    s  for all s  S   Proof in Appendix     In the conventional on policy theory of policy gradient methods  the policy gradient theorem  Marbach   Tsitsiklis        Sutton et al         establishes the relationship between the gradient of the objective function and the expected action values  In our notation  that theorem essentially says that our approximation is exact  that g u    u J  u   Although  we can not show this in the off policy case  we can establish a relationship between the solutions found using the true and approximate gradient  Theorem    Off Policy Policy Gradient Theorem   Given U  RNu a non empty  compact set  let       Off policy Policy gradient Theorem  Z    u  U   g u        Like other policy gradient algorithms  Off PAC updates the weights approximately in proportion to the gradient of the objective  ut    ut  u t u J  ut         where u t  R is a positive step size parameter  Starting from Equation    the gradient can be written      X X b   u J  u    u d  s   a s Q  s  a  sS     X sS  db  s   Z    u  U   u J  u       where Z is the true set of local maxima and Z the set of local maxima obtained from using the approximate gradient  g u   If the value function can be represented by our function class  then Z  Z  Moreover  if we use a tabular representation for   then Z   Z   Proof in Appendix      aA  X   u  a s Q   s  a   aA     a s u Q   s  a    The final term in this equation  u Q   s  a   is difficult to estimate in an incremental off policy setting  The first approximation involved in the theory of OffPAC is to omit this term  That is  we work with an approximation to the gradient  which we denote g u   RNu   defined by X X u J  u   g u    db  s  u  a s Q   s  a  sS  The proof of Theorem    showing that Z   Z  requires tabular  to avoid update overlap  updates to a single parameter influence the action probabilities for only one state  Consequently  both parts of the gradient  one part with the gradient of the policy function and the other with the gradient of the action value function  locally greedily change the action probabilities for only that one state  Extrapolating from this result  in practice  more generally a local representation for  will likely suffice  where parameter updates influence only a small number of states  Similarly  in the non tabular case  the claim will likely hold if  is small  aA          See errata in section B   Off Policy Actor Critic  Algorithm   The Off PAC algorithm Initialize the vectors ev   eu   and w to zero Initialize the vectors v and u arbitrarily Initialize the state s For each step  Choose an action  a  according to b  s  Observe resultant reward  r  and next state  s    r    s   vT xs   vT xs   u  a s  b a s  Update the critic  GTD   algorithm   ev    xs     s ev       T v  v   v e    w ev  xs   v   s       w  w   w ev   wT xs  xs Update thehactor  i u  a s     s eu eu   uu a s  u  u   u eu s  s    the return is myopic   again because changes to the policy mostly affect the action value function locally  Fortunately  from an optimization perspective  for all u  Z Z  J  u    minu  Z J  u     in other words  Z represents all the largest local maxima in Z with respect to the objective  J   Local optimization techniques  like random restarts  should help ensure that we converge to larger maxima and so to u  Z  Even with the true gradient  these approaches would be incorporated into learning because our objective  J   is non convex       The Actor  Incremental Update Algorithm with Eligibility Traces We now derive an incremental update algorithm using observations sampled from the behavior policy  First  we rewrite Equation   as an expectation      X g u    E u  a s Q   s  a  s  db aA      a s  u  a s    Q  s  a  s  db  E b a s  b a s   a s  aA       E  s  a  s  a Q   s  a  s  db   a  b  s  X    Eb   st   at   st   at  Q   st   at    u  a s  where  s  a     a s  b a s     s  a     a s    and we introduce the new notation Eb    to denote the expectation implicitly conditional on all the random variables  indexed by time step  being drawn from their limiting stationary distribution under the behavior policy  A standard result  e g   see Sutton et al         is that an arbitrary function of state can be introduced into these equations as a baseline without changing the expected value  We use the approximate state value function provided by the critic  V   in this way  h    i g u    Eb  st   at   st   at   Q   st   at    V  st    The next step is to replace the action value  Q   st   at    by the off policy  return  Because these are not exactly equal  this step introduces a further approximation  h    i     Eb  st   at   st   at   R  V  st   g u   g u  t where the off policy  return is defined by  Rt   rt           st    V  st         st     st     at    Rt    Finally  based on this equation  we can write the forward view of Off PAC      ut    ut   u t  st   at   st   at   Rt  V  st       The forward view is useful for understanding and analyzing algorithms  but for a mechanistic implementation it must be converted to a backward view that does not involve the  return  The key step  proved in the appendix  is the observation that h    i Eb  st   at   st   at   Rt  V  st     Eb  t et       where t   rt      st    V  st      V  st   is the conventional temporal difference error  and et  RNu is the eligibility trace of   updated by  et    st   at     st   at     et    Finally  combining the three previous equations  the backward view of the actor update can be written simply as  ut    ut   u t t et The complete Off PAC algorithm is given above as Algorithm    Note that although the algorithm is written in terms of states s and s    it really only ever needs access to the corresponding feature vectors  xs and xs    and to the behavior policy probabilities  b  s   for the current state  All of these are typically available in large scale applications with function approximation  Also note that Off PAC is fully incremental and has per time step computation and memory complexity that is linear in the number of weights  Nu   Nv   With discrete actions  a common policy distribution is the Gibbs distribution  which uses a linear combiT  nation of features  a s     eu s a P uT  s b be  where s a are  state action features for state s  action a  and where   Off Policy Actor Critic  P u  a s   s  a     a s    s a  b  b s s b   The stateaction features  s a   are potentially unrelated to the feature vectors xs used in the critic      Convergence Analysis Our algorithm has the same recursive stochastic form as the off policy value function algorithms ut     ut   t  h ut   vt     Mt     N  N  where h   R  R is a differentiable function and  Mt  t  is a noise sequence  Following previous offpolicy gradient proofs  Maei         we study the behavior of the ordinary differential equation u t    u h u t   v   The two updates  for the actor and for the critic  are not independent on each time step  we analyze two separate ODEs using a two timescale analysis  Borkar         The actor update is analyzed given fixed critic parameters  and vice versa  iteratively  until convergence   We make the following assumptions   A   The policy viewed as a function of u      a s    RNu          is continuously differentiable  s  S  a  A   A   The update on ut includes a projection operator     RNu  RNu   that projects any u to a compact set U    u   qi  u      i              s   RNu   where qi      RNu  R are continuously differentiable functions specifying the constraints of the compact region  For u on the boundary of U  the gradients of the active qi are linearly independent  Assume the compact region is large enough to contain at least one  local  maximum of J    A   The behavior policy has a minimum positive value bmin          b a s   bmin s  S  a  A  A   The sequence  xt   xt     rt    t  is i i d  and has uniformly bounded second moments   A   For every u  U  the compact region to which u is projected   V     S  R is bounded  Remark    It is difficult to prove the boundedness of the iterates without the projection operator  Since we have a bounded function  with range          we could instead assume that the gradient goes to zero exponentially as u    ensuring boundedness  Previous work  however  has illustrated that the stochasticity in practice makes convergence to an unstable equilibrium unlikely  Pemantle         therefore  we avoid restrictions on the policy function and do not include the projection in our algorithm  Finally  we have the following  standard  assumptions on features and step sizes   P     xt        t  where xt  RNv T   P   Matrices C   E xt xt T    A   E xt  xt  xt       are non singular and uniformly bounded  A  C and E rt   xt   are well defined because the distribution of  xt   xt     rt     does not depend on t   S    are deterministic P such that u t      tP Pv t   w t   P             and v t w t u t tP t t v t   Pt   u t     t w t    and t u t    with v t        S   Define H A     A   AT     and let   min  C H A   be the minimum eigenvalue of the matrix C   H A     Then w t   v t for some    max    min  C   H A     Remark    The assumption u t  v t    in  S   states that the actor step sizes go to zero at a faster rate than the value function step sizes  the actor update moves on a slower timescale than the critic update  which changes more from its larger step sizes   This timescale is desirable because we effectively want a converged value function estimate for the current policy weights  ut   Examples of suitable step sizes are   v t    t   u t     t log t or v t   t      u t    t    with w t   v t for  satisfying  S     The above assumptions are actually quite unrestrictive  Most algorithms inherently assume bounded features with bounded value functions for all policies  unbounded values trivially result in unbounded value function weights  Common policy distributions are smooth  making  a s  continuously differentiable in u  The least practical assumption is that the tuples  xt   xt     rt     are i i d   in other words  Martingale noise instead of Markov noise  For Markov noise  our proof as well as the proofs for GTD   and GQ    require Borkars        two timescale theory to be extended to Markov noise  which is outside the scope of this paper   Finally  the proof for Theorem   assumes       but should extend to      similarly to GTD    see Maei        Section      for convergence remarks   We give a proof sketch of the following convergence theorem    with the full proof in the appendix  Theorem    Convergence of Off PAC   Let      and consider the Off PAC iterations with GTD     for the critic  Assume that  A    A     P    P   and  S   S   hold  Then the policy weights  ut   converge to    Minimum exists as all eigenvalues real valued  Lemma    See errata in section B   GTD    is GTD   with       not the different algorithm called GTD    by Sutton  Szepesvari   Maei            Off Policy Actor Critic  d      and the value function Z    u  U   g u  weights  vt   converge to the corresponding TD solution with probability one  Proof Sketch  We follow a similar outline to the two timescale analysis for on policy policy gradient actor critic  Bhatnagar et al         and for nonlinear GTD  Maei et al          We analyze the dynamics for our two weights  ut and zt T    wt T vt T    based on our update rules  The proof involves satisfying seven requirements from Borkar        p      to ensure convergence to an asymptotically stable equilibrium   Behavior  Greedy GQ  Softmax GQ  Off PAC     Empirical Results This section compares the performance of Off PAC to three other off policy algorithms with linear memory and computational complexity     Q    called QLearning when           Greedy GQ  GQ   with a greedy target policy   and    Softmax GQ  GQ   with a Softmax target policy   The policy in Off PAC is a Gibbs distribution as defined in section      We used three benchmarks  mountain car  a pendulum problem and a continuous grid world  These problems all have a discrete action space and a continuous state space  for which we use function approximation  The behavior policy is a uniform distribution over all the possible actions in the problem for each time step  Note that Q   may not be stable in this setting  Baird         unlike all the other algorithms  The goal of the mountain car problem  see Sutton   Barto        is to drive an underpowered car to the top of a hill  The state of the system is composed of the current position of the car  in             and its velocity  in              The car was initialized with a position of      and a velocity of    Actions are a throttle of            The reward at each time step is    An episode ends when the car reaches the top of the hill on the right or after       time steps  The second problem is a pendulum problem  Doya         The state of the system consists of the angle  in radians  and the angular velocity  in                 of the pendulum  Actions  the torque applied to the base  are            The reward is the cosine of the angle of the pendulum with respect to its fixed base  The pendulum is initialized with an angle and an angular velocity of    i e   stopped in a horizontal position   An episode ends after       time steps  For the pendulum problem  it is unlikely that the behavior policy will explore the optimal region where the pendulum is maintained in a vertical position  Consequently  this experiment illustrates which algorithms make best use of limited behavior samples   Figure    Example of one trajectory for each algorithm in the continuous  D grid world environment after       learning episodes from the behavior policy  Off PAC is the only algorithm that learned to reach the goal reliably   The last problem is a continuous grid world  The state is a   dimensional position in           The actions are the pairs                                                               representing moves in both dimensions  Uniform noise in              is added to each action component  The reward at each time step for arriving in a position  px   py   is defined as        N  px            N  py              N  px           N  py           N  px           N  py              p   where N  p        e         The start position is            and the goal position is             An episode ends when the goal is reached  that is when the distance from the current position to the goal is less than      using the L  norm   or after       time steps  Figure   shows a representation of the problem  The feature vectors xs were binary vectors constructed according to the standard tile coding technique  Sutton   Barto         For all problems  we used ten tilings  each of roughly        over the joint space of the two state variables  then hashed to a vector of dimension       An addition feature was added that was always    State action features  s a   were also         dimensional vectors constructed by also hashing the actions  We used a constant          All the weight vectors were initialized to    We performed a parameter sweep to select the following parameters     the step size v for Q       the step sizes v and w for the two vectors in Greedy GQ     v   w and the temperature  of the target policy distribution for Softmax GQ and    the step sizes v   w and u for Off PAC  For the step sizes  the sweep was done over the following values                                          Off Policy Actor Critic Mountain car     Pendulum        Continuous grid world                                              Average Reward  Average Reward  Average Reward         Behaviour Q Learning Greedy GQ Softmax GQ Off PAC         Behaviour Q Learning Greedy GQ Softmax GQ Off PAC               Behaviour Q Learning Greedy GQ Softmax GQ Off PAC                                                                                                           Time steps  Episodes  Mountain car w u    v  Reward Behavior   Q                                             Pendulum Continuous grid world w u    v  Reward w u    v  Reward  final  na  na  na na         na  na  na na         na  na  na na           na  na  na na         na  na  na na         na  na  na na  final  na  na                 na  na                  na  na                   overall  na  na               na  na                 na  na                          na                      na                                                na                       na                     na                                                                                        overall Softmax GQ final  na                                      overall                                                         final                                                                                                                                              overall                  Episodes  overall  Greedy GQ  final  Off PAC        Figure    Performance of Off PAC compared to the performance of Q    Greedy GQ  and Softmax GQ when learning off policy from a random behavior policy  Final performance selected the parameters for the best performance for the last     of the run  whereas the overall performance was over all the runs  The plots on the top show the learning curve for the best parameters for the final performance  Off PAC had always the best performance and was the only algorithm able to learn to reach the goal reliably in the continuous grid world  Performance is indicated with the standard error   divided by          that is the number of tilings plus    To compare TD methods to gradient TD methods  we also used w      The temperature parameter     was chosen from                                       and  from                           We ran thirty runs with each setting of the parameters  For each parameter combination  the learning algorithm updates a target policy online from the data generated by the behavior policy  For all the problems  the target policy was evaluated at    points in time during the run by running it   times on another instance of the problem  The target policy was not updated during evaluation  ensuring that it was learned only with data from the behavior policy  Figure   shows results on three problems  SoftmaxGQ and Off PAC improved their policy compared to the behavior policy on all problems  while the improvements for Q   and Greedy GQ is limited on the continuous grid world  Off PAC performed best on all problems  On the continuous grid world  Off PAC was the only algorithm able to learn a policy that reliably found the goal after       episodes  see Figure     On all problems  Off PAC had the lowest standard error      Discussion Off PAC  like other two timescale update algorithms  can be sensitive to parameter choices  particularly the step sizes  Off PAC has four parameters   and the three step sizes  v and w for the critic and u for the actor  In practice  the following procedure can be used to set these parameters  The value of   as with other algorithms  will depend on the problem and it is often better to start with low values  less than      A common heuristic is to set v to     divided by the norm of the feature vector  xs   while keeping the value of w low  Once GTD   is stable learning the value function with u      u can be increased so that the policy of the actor can be improved  This corroborates the requirements in the proof  where the step sizes should be chosen so that the slow update  the actor  is not changing as quickly as the fast inner update to the value function weights  the critic   As mentioned by Borkar        p       another scheme that works well in practice is to use the restrictions on the step sizes in the proof and to also subsample updates for the slow update  Subsampling updates means only updating every  tN  t      for some N     Off Policy Actor Critic     the actor is fixed in between tN and  t     N while the critic is being updated  This further slows the actor update and enables an improved value function estimate for the current policy    In this work  we did not explore incremental natural actor critic methods  Bhatnagar et al          which use the natural gradient as opposed to the conventional gradient  The extension to off policy natural actorcritic should be straightforward  involving only a small modification to the update and analysis of this new dynamical system  which will have similar properties to the original update   Finally  as pointed out by Precup et al          offpolicy updates can be more noisy compared to onpolicy learning  The results in this paper suggest that Off PAC is more robust to such noise because it has lower variance than the action value based methods  Consequently  we think Off PAC is a promising direction for extending off policy learning to a more general setting such as continuous action spaces      Conclusion This paper proposed a new algorithm for learning control off policy  called Off PAC  Off Policy ActorCritic   We proved that Off PAC converges in a standard off policy setting  We provided one of the first empirical evaluations of off policy control with the new gradient TD methods and showed that Off PAC has the best final performance on three benchmark problems and consistently has the lowest standard error  Overall  Off PAC is a significant step toward robust off policy control      Acknowledgments This work was supported by MPrime  the Alberta Innovates Centre for Machine Learning  the Glenrose Rehabilitation Hospital Foundation  Alberta Innovates Technology Futures  NSERC and the ANR MACSi project  Computational time was provided by Westgrid and the Mesocentre de Calcul Intensif Aquitain  Appendix  See http   arXiv org abs           
