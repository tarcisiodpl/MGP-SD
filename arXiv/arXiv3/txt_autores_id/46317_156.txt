 We present a probabilistic model of events in continuous time in which each event triggers a Poisson process of successor events  The ensemble of observed events is thereby modeled as a superposition of Poisson processes  Efficient inference is feasible under this model with an EM algorithm  Moreover  the EM algorithm can be implemented as a distributed algorithm  permitting the model to be applied to very large datasets  We apply these techniques to the modeling of Twitter messages and the revision history of Wikipedia      Introduction  Real life observations are often naturally represented by eventsbundles of features that occur at a particular moment in time  Events are generally nonindependent  one event may cause others to occur  Given observations of events  we wish to produce a probabilistic model that can be used not only for prediction and parameter estimation  but also for identifying structure and relationships in the data generating process  We present an approach for building probabilistic models for collections of events in which each event induces a Poisson process of triggered events  This approach lends itself to efficient inference with an EM algorithm that can be distributed across computing clusters and thereby applied to massive datasets  We present two case studies  the first involving a collection of Twitter messages on financial data  and the second focusing on the revision history of Wikipedia  The latter example is a particularly large scale problem  the data consist of billions of potential interactions among events  Our approach is based on a continuous time formal   Michael I  Jordan Depts  of EECS and Statistics University of California  Berkeley jordan cs berkeley edu  ism  There have been a relatively small number of machine learning papers focused on continuous time graphical models  examples include the Poisson networks of Rajaram et al         and the continuoustime Bayesian networks described in Nodelman et al                These approaches differ from ours in that they assume a small set of possible event labels and do not directly apply to structured label spaces  A more flexible approach has been presented by Wingate et al         who define a nonparametric Bayesian model with latent events and causal structure  This work differs from ours in several ways  most importantly in that it is a discrete time model that allows for interaction only between adjacent time steps  Finally  this work is an extension and generalization of the continuous time noisy or presented in Simma et al          There is also a large literature in statistics on point process modeling that provides a context for our work  A specific connection is that the fundamental stochastic process in our model is known in statistics as a mutually self exciting point process  Hawkes         There are also connections to applications in seismology  notably the Epidemic Type AftershockSequences framework of Ogata         which involves a model similar to ours that is applied to earthquake prediction      Modeling Events with Poisson and Cox Processes  Our representation of collections of events is based on the formalism of marked point processes  Let each event be represented as a pair  t  x   R  F  where t is the timestamp and x the associated features taking values in a feature space  A dataset is a sequence of observations  t  x   R   F  We use Da b to denote the events occuring between times a and b  Within the framework of marked point processes  we have several modeling issues to address     how many   the density   f  t  x  the data   D  T  Baseline Event   Event   Event   Event   Event   Poisson Intensity  events occur     when do events occur     what features they possess  A classical approach to answering these questions proceeds as follows     the number is distributed Poisson       the timestamps associated with event are independent and identically distributed  iid  from a fixed distribution     the features are drawn independently from a fixed distribution g   Z      f   T    h t g x   Z   Z B   PP  f      Time  Arrows denote event occurances   where  is the average occurrence rate  h is a density for locations  g is the marking density and PP denotes the inhomogeneous Poisson process  We might wish for the density h to capture periodic activity due to time of day effects  for example by having the intensity be a step function of the time  However  real collections of events often exhibit dependencies that cannot be captured by a standard Poisson process  the Poisson process makes the assumption that the number of events that occur in two nonoverlapping time intervals must be independent   One way to capture such dependencies is to consider Cox processes  which are Poisson processes with a random mean measure  In particular  consider mean measures that take the form of latent Markov processes  In queueing theory  this kind of model is referred to as a Markov Modulated Poisson Process  Ryden        and it has been used as a model for packets in networks  Fischer and Meier Hellstern              Events Causing Other Events  In this paper we take a different approach to modeling collections of dependent events in which the occurrence of an event  t  x  triggers a Poisson process consisting of other events  Specifically  we model the triggered Poisson process as having intensity k t x   t    x        x g  x   x h  t   t         x   is  the expected number of events  h  t   is  the delay density  g  is  the label transition density   Denote by   the events caused by a baseline Poisson process with mean measure   and let i be the events triggered by events in i    D   i i          PP       i  PP    X   t x i   kt x         Figure    A diagram of the overlapping intensities and one possible forest that corresponds to these events  Alternatively  we can use the superposition property of Poisson processes to write a recursive definition    X D  PP     k  t    x           t x D  This definition makes sense only when k t   x    is positive only for t   t    since an event  t  x  can only cause resulting events at a later time  requiring that h  t      for t     View as a Random Forest In our model  each event is either caused by the background Poisson process or a previous event  see Figure     If we augment the representation to include the cause of each event  the object generated is a random forest  where each event is a node in a tree with timestamp and features attached  The parent of each event is the event that caused it  if that does not exist  it must be a root node  Let  p  be the event that caused p  or  if the parent does not exist  Usually  this parenthood information is not available and must be estimated  which corresponds to estimating the tree structure from an enumeration of the nodes  their topological sort  timestamps and features  We show how this distribution over  p  can be estimated by an EM algorithm       Model Fitting  The parameters of our model can be estimated with an EM algorithm  Dempster et al          If  p   the cause of the event  was known for every event  then it would be possible to estimate the parameters       g and h using standard results for maximum likelihood estimation under a Poisson distribution  Since    is not observed  we can use EM to iteratively estimate the latent variables and maximize the parameters  For uniformity of notation  assume that there is a dummy event       and k      t  x    fbase  t  x  so that we can treat the baseline intensity the same as all the other intensities resulting from events  We introduce z t   x   t x  as expectations of the latent  where z t   x   t x  corresponds to the expectation of    t x   t   x       Neglecting terms that dont depend on the EM variables z    X X L   log  k t   x     t  x   t   x   D  t   t x D     X   t x D  s t    X  X  z t   x   t x  log k t   x     t  x    t   x   D  t  z t   x   x y        t   x   The bound is tight when z t   x   t x    P  log k t   x     t  x     t   x    log k t   x     t  x   These z variables act as soft assignment proxies for  and allow us to compute expected sufficient statistics for estimating the parameters in fbase and k  The specific details of this computation depend on the specific choices made for fbase and k  but this basically reduces the estimation task to that of estimating a distribution from a set of weighted samples  For example  if fbase  t  x       tT   g x  where g x  P is some labeling distribution  then M LE   T    t x  z    t x    Regardless of the delay and labeling distributions and the relative intensities of different events  the total intensity of the total mean measure should be equal to the number of events observed  This can either be treated as a constraint during the M step if possible  for example  if  x  has a simple form   or the results of the M step should be projected onto this set of solutions by scaling k and fbase   increasing the likelihood in the process   handled by introducing more latent variablesone for each element  Thus the credit assigning step builds a distribution not only over the past events that were potential causes  but also the individual components of the mixture       The Fertility Model  A key design choice is the choice of  x   the expected number of events  When x ranges over a small space it may be possible to directly estimate  x  for each x  However  with a larger feature space  this approach is infeasible for both computational and statistical reasons and so a functional form of the fertility function must be learned  In presenting these fertility models  we assume for simplicity that x is a binary feature vector  Linear Fertility We consider  x        T x with the restriction            By Poisson P additivity it is possible to factor  x  into     i xi    i and  as part of the EM algorithm  build a distribution over the allocation of features to events  collecting sufficient statistics to estimate the values  Note that     is an important restriction  since the mean of each of the constituent Poisson random variables must be nonnegative  This can be somewhat relaxed by considering  x    P  T      T x      x  where      Foregoing   i i P the    i i restriction allows the intensity to be negative which does not make probabilistic sense  Multiplicative Fertility The linear model of fertility places significant limits on the negative influence that features are allowed to exhibit and also implies that the fertility effect of any feature will always be the same regardless of its context    QAlternatively  we can estimate  x    exp  T x   i wixi for w   exp   where we assume that one of the dimensions of x is a constant    leading to derivatives having the form  Y X X X xj  L  xj wixi   z t   x   t x    wj wj     t xD  Additive components  It is possible to develop more sophisticated models by making k t x  more complex  Consider a mixture k t x   t    x      PL  l       l  are individual densities  l   k t x   t   x   where k For example  in the Wikipedia edit modeling domain      k t x  can produce events similar to x at a time close     k t x   to t  whereas can correspond to more thoughtful responses that occur later but also differ more substantially from the event that caused them  Since the EM algorithm introduces a latent variable for every additive component inside the logarithm  the separation of some components into a further sum can be  i  j  t xD t  x D  t  The exact solution for a single wj is readily obtained  so we can optimize L by either coordinate descent or gradient steps  An alternative approach based on Poisson thinnings is described in Simma         Combining Fertilities It is also possible to build a fertility model that combines additive and multiplicative components           x            T x   exp          T x        The EM algorithm distributes credit between the con  stant term     T x and the terms exp          T x     A possible concern is that this requires fitting a large number of parameters  A special case is when x has a particular structure and there is reason to believe that it is composed of groups of variables that interact multiplicatively within the group  but linearly among groups  in which case the multiplicative models can be used on only a subset of variables  Additionally  it is possible to build a fertility model of the form          x            T x  exp          T x by using linearity to additively combine intensities and using thinning to handle the multiplicative factors  Simma              Computational Efficiency  In this section we briefly consider some of the principal challenges that we needed to face to fit models to massive data  in particular for the Wikipedia data   For certain selections of delay and transition distributions  it is possible to collapse certain statistics together and significantly reduce the amount of bookkeeping required  Consider a setting in which there are a small number of possible labels  that is  xi           L  for small L  and the delay distribution h t  is the exponential distribution h  t        exp  x   We can use the memorylessness of the exponential distribution to avoid the need to explicitly build a distribution over the possible causes of each event  Order the events by their times t            tn and let lij     exp  ti   ti   bi  j  li  j   ti  ti     bij  bij     exp  ti   ti   bi  j    xi  g j xi     Let i s    inf ti   ti   s  and note that the intensity at time s for a label of type j is   exp ti s   s bi s  j   fbase  s  j   and the weighted average delay is li s  j   s  ti s    Counting the number of type j events triggering type k can be done with similar techniques by letting bi j k  the intensity at time i s  for events j caused by k  change only when an event k is encountered  If the transition density is sparse  only some bij need to be incremented and the rest may be left unmodified  as long as the missing exponential decay is accounted for later  While this computational technique works for only a restricted set of models and has computational complexity O  D z  where z is the average number of non zero k   x  entries  it is much more computationally efficient than the direct method when there are a large number of somewhat closely spaced events   For large scale experiments on Wikipedia  we use Hadoop  an open source implementation of MapReduce  Dean and Ghemawat         The object that we map over is a collection of a page and its neighbors in the link graph   Each map operation also accesses the hyperparameters shared across pages and runs multiple EM iterations over the events associated with that page  The learned parameters are returned to the reducer which updates the hyperparameters and another MapReduce job fits models with these updated hyperparameters  Thus  the reduce step only accumulates statistics for the hyperparameters  as well as collects log likelihoods  Hadoop requires that each object being mapped over be kept in memory  which requires careful attention to representation and compression  these memory limits have been the key challenge in scaling  If each neighborhood does not fit in memory  it is possible to break it into pieces  run the E step in the Map phase and then use the Reduce phase to sum up all the sufficient statistics and maximize parameters  but this requires many more chained MapReduce jobs  which is inefficient  For our experiments  careful engineering and compression was sufficient      Twitter Messages  Twitter is a popular microblogging website that is used to quickly post short comments for the world to see  We collected Twitter messages  composed of the sender  timestamp and body  that contained references to stock tickers in the message body  Some messages form a conversation  others are posted as a result of a real world event inspiring the commentary  The dataset that we collected contains       messages and covers a period of    days  For modeling  each message can be represented as a triple of a user  timestamp and a binary vector of features  A typical message User   SchwartzNow  Time             T          Body  also for tommorow expect high volume options traded stocks like  aapl  goog graviate around the strikes due to the delta hedging   This is generated with a sequence of MapReduce jobs where we first compute diffs and featurize  then for each page we gather a list of neighbors that require that pages history  and finally each page sends a copy of itself to all its neighbors  A pages body is insufficient to determine its neighbors since the body only contains outgoing  not incoming  links so the incoming links need to be collected first    occurs on            at          and has the features  AAPL and  GOOG and is missing features such as  MSFT and HAS LINK  Due to length constraints and Internet culture  the messages tend to not be completely grammatical English and often a message is simply a shortened Web link with brief commentary  In addition to the stocks involved and whether links are involved  features also denote the presence or absence of keywords such as buy or option   Train logliklelihood  Test logliklelihood  Mix   Unif Mix   Unif Unif         Unif         Gamma k      Gamma k      Gamma k       Baseline Intensities The simplest possible baseline intensity is a time homogeneous Poisson process  but the empirical intensity is very periodic  A better baseline is to break up the day into intervals of  for example  an hour  assume that the intensity is uniform within the hour and that the pattern repeats  So  h t    pbt   c   The log likelihoods for these baselines are reported in Table    It is worth noting that the gain from incorporating periodicity in the baseline is much smaller than the gain from the other parts of the model  This timing model must be combined with a feature distribution  We use a fully independent model  where each feature is present independently of the others  Q g  x   g  x  That is  g x    i pi i     pi   i   where gi is the ith feature  Clearly  the MLE estimates for pi are simply the empirical fraction of the data that contains that feature       Intensity and Delay Distributions  When events can trigger other events  each induces a Poisson process of successor events  We factor the intensity for that process as k t x   t    x       x g x   x h t   t   with the constituents described in Eq     For the intensity  we implemented a multiplicative model where the expected number of events is  x    exp  T x   The delay distribution h must capture the empirical fact that most responses occur shortly after the original message  but there exist some responses that take significantly longer  meaning that h needs a sufficiently heavy tail  As candidates  we consider uniform  piecewise uniform  exponential and gamma distributions  Log likelihoods for different delays are reported in Figure    The transition function used  g   is described later  The best performing delay distribution is the gamma  with shape parameters less than    the shape parameter is also estimated in the results of Table    Note that the results show that the choice of a delay distribution has a smaller impact on the overall likelihood than the transition distribution  This is due in part to the fact that for an individual event the features are embedded in a large space and there is  Gamma k      Gamma k      Exponential                                   Loglikelihood   e             Loglikelihood   e          Figure    Log likelihoods for various delay functions  more to explain  The predictive ability of the Poisson process associated with an event to explain the specific features of a resultant event is the predominant benefit of the model       Transition Distribution  The remaining aspect of the model is the transition distribution g x x    that specifies the types of events that are expected to result from an event of type x    Lets consider the possible relationships between a message and its trigger     A simple retweeta duplication of the original message     A responsea message either prompts a specific response to the content of the message  or motivates another message on a similar topic     After a message  the probability of another  possibly unrelated  message is increased because the original event acts as a proxy for general user activity  These kinds of messages represent variation in the baseline event rate not captured by the baseline process and are unrelated to the triggering message in content  so they should take on a distribution from the prior  We construct a transition function parametrized by  that is a product of independent per feature transitions  each a mixture of the identity function and the prior       Y  x   x  g  x  x              xi  x      pi i    pi i   i  i  Note that g is not a mixture of the identity and the prior    Table    Log likelihoods for models of increasing sophistication                    Proportion of Intensity  Mean Delay for Component secs   Component wise mean delay                                                                   Iteration  Independent Component g Component Identical Component Overall                 Mixture Components Independent Component g Component Identical Component                Iteration                  Figure    Trace of parameters of the individual mixture components in model    We denote two important special cases as g    where each resultant event is drawn independently  and g    where the caused events must be identical to the trigger  With an exponential delay distribution and  x  fixed at    g  is equivalent to setting the Poisson intensity to an exponential moving average with decay parameter determined by   The EM algorithm can be used to find the optimal decay parameter  but as the reported results show  this model is inferior to one that utilizes the features of the events  Earlier  we enumerated relationships between a message and its trigger  For example  the retweets are completely identical to the original  with the possible exception of a  username reference tag  so the transition would be g    A response would have similar features but may differ in a few features  and a densityproxy message would have features independent of the causing message  corresponding to g for           g  models the density proxy phenomenon  Let us now consider some possible models  where the Greek letters represent parameters to be estimated    k  t x   t    x      exp      T x h   t   t  g   x  x      k  t x   t    x      exp      T x h   t   t  g  x  x      k  t x   t    x      exp      T x h   t   t  g   x  x      k  t x   t    x      exp      T x h   t   t    Type Homogeneous Baseline Only Periodic Baseline Only Exp Delay  Independent transition k    Intensity doesnt depend on features  Exp Delay  g transition Feature dependent intensity  Exp Delay  Identity transition  k    Exp Delay  h transition  k    Shared intensity  shared Exp delay  mixture transition  k    Mixture of  intensity  exp delay  different transitions   k    Mixture of  intensity  gamma delay  different transitions   Train                          Test                                                                                                                           the ith phenomenon  while k  and k  are intended to capture all three effects    Both g and h are densities  so its easy to compute k t x   t  x  t    x   dt  dx    The results  shown in Figure    indicate that models   and   are significantly superior to the first three  demonstrating that separating the multiple phenomena is useful  For h  we use an exponential distribution  In model    all the transition distributions share the same fertility and delay functions whereas in model    each distribution has its own fertility and delay  As shown in Figure    the latter performs significantly better  indicating that the three different categories of message relationships have different associated fertility parametrizations and delays  The top plot shows the proportions of each component in the mixture  defined as the ratio of the average fertility of the component to the total fertility  The bottom plot demonstrates that while the mean delay of the overall mixture remains almost constant throughout the EM iterations  different individual components have substantially different delay means       Results and Discussion  Table   reports the results for a cascade of models of increasing sophistication  demonstrating the gains that result from building up to the final model  The first stage of improvements  from the homogeneous to the periodic baseline and then to the independent transition model focuses on the times at which the events occur  and shows that roughly equivalent gains follow          g   x  x       g  x  x       g   x  x    from modeling periodicity and from further capturing less periodic variability with an exponential moving   X average  The big boost comes from a better labeling         k  t x   t   x     ki t x   t   x    distribution that allows the features of events to dei   pend on the previous events  capturing both the topicThe models ki for i from   to   are designed to capture wise hot trends and specific conversations    Of course  the shape of the induced Poisson process has an effect  The different types of transitions have distinctly different estimated means for their delay distributions  which is to be expected since they capture different effects  As seen in Figure   the overall intensity proxying independent transition has the highest mean  since the level of activity  averaged over labels  changes slower than the activity for a particular stock or topic  For shape  lower k  higher variance gamma distributions work best  The final component is a fertility model that depends on the features of the event and allows some events to cause more successors than others  This actually has less impact on the log likelihood than the other components of the model      Wikipedia  Wikipedia is a public website that aims to build a complete encyclopedia through user edits  We work to build a probabilistic model for predicting edits to a page based on revisions of the pages linking to it  Causes outside of that neighborhood are not considered  The reasons for that restriction are primarily computationalconsidering all edits as potential causes for all other edits  even within a short time window  is impractical on such a large scale  As a demonstration of scale  we model         pages with a total of            revisions  the raw datafile is    TB in size   involving billions of considered interactions between events       Structure in Wikipedias History  As we build up a probabilistic model for edits  its useful to consider the kinds of structure we would like the model to capture  Edits can be broadly categorized into  Minor Fixes  small tweaks that include spelling corrections  link insertion  etc  Only one or a few words in the document are affected  Major Insert  Often  text is migrated from a different page such that we obtain the addition of many words and the removal of none or very few  From the users perspective  this corresponds to typing or pasting in a body of text with minimal editing of the context  Major Delete  The opposite of a major insert  Often performed by vandals who delete a large section of the page  Major Change  An edit that affects a significant number of words but is not a simple insert or delete   Self delay  component           Neighbor delay  component            Mean  hours  Self delay  component                        Mean  hours  Neighbor delay  component              Mean  hours  Self delay  component                                Mean  hours                           Mean  hours  Neighbor delay  component            Mean  hours                     Figure    Delay distribution histogram over all pages   Revert  Any edit that reverts the content of the page to a previous state  Often  this is the immediately previous state but sometimes it goes further back  A revert is typically a response to vandalism  though edits done in good faith can also be reverted  Other Edit  A change that affects more than a couple of words but is not a major insert or delete        Delay Distributions  Since most pages have many neighbors  each event has a large number of possible causes and the mean measure at each event is the sum over many possible triggers  This means the exact shape of the delay distribution is not as important as in cases when only a few possible triggers are considered  We model the delay as a mixture of three exponentials  intending them to capture short  medium and longer term effects  For each page  we estimate both the parameters and the mixing weights  Figure   shows a histogram of the estimated means  One component is a very fast response  with an average of     minutes for the same page and      minutes for the adjacent page delay  On the same page  the component captures edits caused by each other  either when an individual is making multiple modifications and saving the page along the way  or when a different user noticing the revisions on a news feed and instantly responding by changing or undoing them  The remaining components capture the periodic effects and time varying levels of interest in the topic  as well as reactions to specific edits    The model needs to capture the significant attributes of the revision  in addition to its timestamp  but we dont aim to completely model the exact content of the edit  as the inadequacies of that aspect of the model would dominate the likelihood  Instead  we identify key features  typerevert  major insert  etcwhether the edit was made by a known user  and the identity of the page  of the edits and build a distribution over events as described by those features  not the raw edits   Log likelihood  Transition Distribution        e                                               No Neighbors        e   Train  Neighbors  Neighbors  Same Transition Test Diff Transition  Unregularized Transition Regularized Transition Neighbors  Own Intensity       Log likelihood                         Unregularized        Regularized  Transition  Transition  No Neighbors        When a page with features x triggers an event with features x    the latter vector is drawn from a distribution over possible features  When the number of possible feature combinations is small  the transition matrix can be directly learned  but when there are multiple features  or features which can take on many values  we need to fit a structured distribution  We partition the features into two parts as x    x    x     where x  are features that can appear in any revision  such as the type of the edit and whether the editor is anonymous  and where x  is the identity of the page  Note that x  can take on very many values  each one appearing relatively infrequently  There are a vast number of observations and we can directly learn the transition matrix h   x    x      For each target page x     we model an x  transition as x    x    x  x   x    Dirichlet  x      No Neighbors  The revisions on each page can be caused either by the baseline or a previous revision on that page but not by revisions of the neighbors            x   x    s g x   x hs  t   t        x  x    n g x   x hn  t   t      Figure   shows log likelihoods of successive iterations of the model  The regularized versions use the Dirichlet prior  the others estimate  on each page independently  The bars correspond to      Neighbors  Own Intensity  Poisson process of edits on the page  That process has its own delay distribution and intensity  but those are the same for all neighbors  The transition conditional distribution is the same for both events x   which  due to conjugacy  corresponds to shrinkage towards x    As more transitions are observed  the pages transition probability becomes more driven by the specific observed probabilities on that page  The allocation over components of  is directly maximized  while the magnitude of  is chosen over a validation set  x  is handled by fixing a particular page that we refer to as x   and fitting a model for revisions of that page   x    x      Then  the process over all the pages is a superposition of processes over each possible x     x     Neighbors  Diff Transition  Figure    Log Likelihoods of various models  Models with regularized transition matrices perform significantly better on unseen data  but non trivially worse on the training set  indicating strong regularization  The baseline only is not shown but has           training and           test log likelihoods     k t x   t    x      Multinomial  x   x     Neighbors  Same Transition     Parameters for functions with different subscripts are estimated separately   Neighbors  Different Transitions  Same as above  but uses different transition distributions for x   and its neighbors  x     k t x   t    x            x   x    s gs  x   x hs  t   t      x  x    n gn  x   x hn  t   t      Here  the parameters for the two different g are estimated separately and are regularized towards  same or  neighbor   respectively   Neighbors  Own Intensities  Each neighbor has its own  parameter   x  x        x   x   x  neighbor of      x    x         k t x   t   x       x   x    g x  x h x  x   t  t       Neighbors  Same Transition  Revisions to the neighbors of the page in the link graph cause a  For most pages there is insufficient data to estimate the individual s accurately  regularization of  is required and is discussed later    agonal is predominantly positive  indicating that an event of a particular type on a neighbor makes an event of the same type more likely on the current page  Note the significantly positive rectangle for transitions between massive inserts  deletions and changes  The magnitude of the ratio is almost identical in the rectangle  significant modifications induce other large modifications but the specific type of modification  or whether it is made by a known user  are irrelevant  Large changes act as indications of interest in the topic or significant structural changes in the related pages   REVERT UNKNOWN CONTRIB  REVERT KNOWN CONTRIB  MINOR TWEAK KNOWN CONTRIB  MINOR TWEAK UNKNOWN CONTRIB  Caused Event  MASS INS UNKNOWN CONTRIB  MASS INS KNOWN CONTRIB  MASS DEL UNKNOWN CONTRIB  MASS DEL KNOWN CONTRIB  MASS CHANGE KNOWN CONTRIB  MASS CHANGE UNKNOWN CONTRIB  DEFAULT UNKNOWN CONTRIB  REVERT UNKNOWN CONTRIB  SELF REVERT KNOWN CONTRIB  SELF MINOR TWEAK UNKNOWN CONTRIB  SELF MINOR TWEAK KNOWN CONTRIB  SELF MASS INS UNKNOWN CONTRIB  SELF MASS INS KNOWN CONTRIB  SELF MASS DEL UNKNOWN CONTRIB  SELF MASS DEL KNOWN CONTRIB  SELF MASS CHANGE UNKNOWN CONTRIB  SELF MASS CHANGE KNOWN CONTRIB  SELF DEFAULT UNKNOWN CONTRIB  SELF DEFAULT KNOWN CONTRIB  SELF REVERT UNKNOWN CONTRIB SELF REVERT KNOWN CONTRIB SELF MINOR TWEAK UNKNOWN CONTRIB SELF MINOR TWEAK KNOWN CONTRIB SELF MASS INS UNKNOWN CONTRIB SELF MASS INS KNOWN CONTRIB SELF MASS DEL UNKNOWN CONTRIB SELF MASS DEL KNOWN CONTRIB SELF MASS CHANGE UNKNOWN CONTRIB SELF MASS CHANGE KNOWN CONTRIB SELF DEFAULT UNKNOWN CONTRIB SELF DEFAULT KNOWN CONTRIB SELF DEFAULT KNOWN CONTRIB  Features of the Causing Event  Baseline  Figure    Learned Transition Matrix  The area of the circles corresponds to the logarithm of the conditional probability of the observed feature  divided by the marginal  The yellow  light colored circles correspond to the transition being more likely than average  red correspond to the transition being less likely        Learned Transition Matrices  Figure   shows the estimated transition matrix  Each circle denotes log g x  x    p x      when it is high  that label of the caused event is much more likely than it would be otherwise  The top row represents the intensity for the baseline  the labels of events whose cause is not a previous event  Positive values correspond to event types that the events triggering events aspect of the model is less effective in capturing and thus are over represented in the otherwise unexplained column  Reverts  both by known and anonymous contributors  are significantly underrepresented  indicating that the rest of the model is effective in capturing them  Revisions made by known contributors are under represented  as the rest of the model captures them better than the edits made by anonymous contributors  Events generated from this row account for        of total observed events  The next block corresponds to edits on neighbors causing revisions of the page under consideration and are responsible for        of observed events  The di   The remaining block represents edits on a page causing further changes on the same page and is responsible for        of the observations  There is a stronger positive diagonal component here than above  as similar events co occur  Large changes  especially by anonymous users  lead to an over representation of reverts following them  On the other hand  reverts result in extra large changes  as large modifications are made  reverted and come back again feeding an edit war  Reverts actually over produce reverts  This is not a first order effect  since reverts rarely undo the previous undo  but rather captures controversial moments  The presence of a revert is an indication that previously  an unmeritorious edit was made  which suggests that future unmeritorious edits  that tend to be long and spammy  that need to be reverted are likely       Regularizing Intensity Estimates  When for a fixed page x   an edit occurs on its neighbor  one would expect the identity of the neighbor to affect its likelihood of causing an event on x     As it turns out  effectively estimating the intensities between a pair of pages is impractical unless a very large number of revisions have been observed  Even in the high data regimes  strong regularization is required  We tried regularizing fertilities both towards zero and toward a common per page mean  using both L  and L  penalties  but these regularizers empirically led to poorer likelihoods than using a single scalar  for all neighbors  suggesting that there is not enough data to accurately estimate individual s  One reason is that pages with a large number of events also have a large number of neighbors  so the estimation is always in a difficult regime  Furthermore  the hypothetical true values of these parameters will change with time  as new neighbors appear and change  Let mi be the number of revisions of the ith neighbor page and let ni be the expected number of events triggered by that neighbors revisions  One approach that works in high data regimes is to let P ni j nj           i REG    P m m j i j   Table    Sample list of pages  in bold  and the intensities estimated for them and their top neighbors  This is under strong regularization  which explains the similarity of the weights  Page AH    Apache  Int        Page South Pole  Int        AH   Cobra CH    Chinook    st Airborne Division Mil Mi    Flight simulator List of Decepticons                     Equator Roald Amundsen Ernest Shackleton                                                           Tom Clancys Ghost Recon Advanced Warfighter Command   Conquer         Geography of Norway Navigation South Georgia and the South Sandwich Islands National Geographic Society List of cities by latitude                       for a parameter  between zero and one  which yields an average between the aggregate and individual maximizers  The regularizer forces the P lower weights to P clump as each is lower bounded by  nj   mj   On a subset of the Wikipedia graph that includes only pages with more than     revisions  this improves held out likelihoods compared to having a single  for all neighbors  The improvement is very small  however  certainly smaller than the impact of other aspects of the model  Example pages and intensities estimated for their neighbors are shown in Table        Conclusions  We have presented a framework for building models of events based on cascades of Poisson processes  demonstrated their applications and demonstrated scalability on a massive dataset  The techniques described in this paper can exploit a wide range of delay  transition and fertility distributions  allowing for applications to many different domains  One direction for further investigation is to provide support for latent events that are root causes for some of the observed data  Another is a Bayesian formulation that integrates instead of maximizes parameters  this may work better for complex fertility or transition distributions that lack sufficient observations to be accurately fit with maximum likelihood  Both extensions complicate inference and reduce scalability  indeed  Wingate et al         propose a Bayesian model with latent events but scaling is an issue  Furthermore  allowing the parameters of the model to depend on time  for example  letting the fertility be a draw from a Gaussian process  would be very useful  though again  computational issues are a concern      Acknowledgements  We gratefully acknowledge support for this research from Google  Intel  Microsoft and SAP   
 We present a generative model for representing and reasoning about the relationships among events in continuous time  We apply the model to the domain of networked and distributed computing environments where we fit the parameters of the model from timestamp observations  and then use hypothesis testing to discover dependencies between the events and changes in behavior for monitoring and diagnosis  After introducing the model  we present an EM algorithm for fitting the parameters and then present the hypothesis testing approach for both dependence discovery and change point detection  We validate the approach for both tasks using real data from a trace of network events at Microsoft Research Cambridge  Finally  we formalize the relationship between the proposed model and the noisy or gate for cases when time can be discretized      Introduction  The research described in this paper was motivated by the following real life application in the domain of networked distributed systems  In a modern enterprise network of scale  dependencies between hosts and network services are surprisingly complex  typically undocumented  and rarely static  Even though network management and troubleshooting rely on this information  automated discovery and monitoring of these dependencies remains an unsolved probJohn is now with Dickinson College  PA  Work done while a Researcher with Microsoft Research  Alex is with the University of California  Berkeley  CA  Work done while an intern with Microsoft Research    lem  In     we described a system called Constellation in which computers on the network cooperate to make this information available to all users of the network  Constellation takes a black box approach to locally  at each computer server in the network  learn explicit dependencies between its services using little more than the timings of packet transmission and reception  The black box approach is necessary since any more processing of the incoming and outgoing communication packages would imply prohibitive amounts of overhead on the computer server  The local models of dependency can then be recursively and distributively composed to provide a view of the global dependencies  In Constellation  computers on the network cooperate to make this information available to all users in the network  Constellation and its application to system wide tasks such as characterizing a networking site service and hosts dependencies for name resolution  web browsing  email  printing  reconfiguration planning and end user diagnosis are described in      This paper focuses on the probabilistic and statistical building blocks of that system  the probabilistic model used in the local learning  the EM algorithm used to fit the parameters of the model  and the statistics of the hypothesis testing used to determine the local dependencies  The model  which we call Continuous Time Noisy Or  CT NOR   takes as input sequences of input events and output events and their time stamps  It then models the interactions between the input events and output events as Poisson processes whose intensities are modulated by a  parameterized  function taking into account the distance in time between the input and output events  Through this function the domain expert is able to explicitly encode knowledge about the domain  The paper makes the following contributions       Develops an EM algorithm for fitting all the parameters of this model and an algorithm for dependence discovery and change point detection based on statistical hypothesis testing     Evaluates the performance of the model and the inference procedures both on synthetic data and on real life data taken from a substantial trace of a large computer network     Formalizes the relationship between CT NOR and the noisy or  NOR  gate      when the time between the events can be discretized  This paper is organized as follows  Section   describes the model and Section   describes the EM algorithm for fitting the parameters  Section   is concerned with the relation to the NOR gate  The algorithms and framework for applying the model to dependency discovery and change point detection is described in Section    That section also contains validation experiments with synthetic data  Section   contains experiments on real data and results  Finally  Section   has some conclusions and future work      The CT NOR model  In this section we formally describe the CT NOR model with the objective of building the likelihood equation  First  we provide some background on Poisson Processes  and then we use them to construct the model  Eq      A Poisson Process  can be thought of as random process  samples from which take the form of a set of times at which events occurred  A Poisson Process is defined over a mean  base  measure f  t  and is characterized the property that for any interval  t    t     the number of events that occur in that interval follows the Poisson distribution with the param t eter t   f  t dt  Furthermore  the number of events that occur on two disjoint intervals are independent    j   time of the lth output event and ik the time of the kth input on channel j  Furthermore  let n denote the number of output events and n j  the number of input events on channel j  Then event k in input channel j generates a Poisson process  j  of output events with the base measure pk  t     j  w j  f  t  ik    The term w j  represents the average number of output events that we expect each input event on channel j to be responsible for  and f  t  is the distribution of the delay between an input and the output events caused by it  taking as its argument the delay between the time of the output ol and the time of  j  the input ik   The mathematical structure of the intensity makes intuitive sense  the probability that a given input event caused a given output event depends on both the expected number of events it generates and the distance in time between them  We recall that given multiple independent Poisson processes  denoted as P P   we can use the sum of their intensities to construct a global Poisson proP Pn j   j  pk  t   as the cess and write  ol    P P   j k   probability of the set of n outputs  ol       l  n  The double sum runs over all the channels and over all input events in the channels  Intuitively  and similar to the NOR gate in graphical models       the independence between the between input channels translates into a model where the events in the output channel are caused by the presence of any  a disjunction  of input events in the input channels  with some uncertainty   The formal relation with NOR is presented in Section    We now proceed to write the likelihood of the data given P  j the j model and the input events  Let    w   the total mass of the Poisson base meajn sure  The number n of outputs is distributed as a Poisson distribution  n         P oisson        Let us use channel to denote a sequence of events  The CT NOR model considers a single output channel and a set of input channels  Let ol denote the   This overview is very informal  The more general and formal measure theoretic definition can be found in        In the domain of computer networks  a channel refers to a unidirectional flow of networked packets  Thus a channel will be identified by the service  e g   HTTP  LDAP  etc  and the IP address of the source or destination  In this paper we identified the packets with events as it is only their time stamp that matters   and the location of a specific output event ol is distributed with the probability density  ol      P Pn j  j  k     P Pn j  j  k     j   pk  ol    for l           n        j   w j  f  ol  ik           The likelihood of observing a set  ol   of outputs is     L o i    n  e  n X  j   j  Y w f  ol  i   k  l   jk         Before concluding this section  we expand a bit on the function f as it is an important part of the model  This function provides us with the opportunity of encoding domain knowledge regarding the expected shape of the delay between input and output events  In our experience using CT NOR to model an enterprise network we used two specific instantiations  a mixture of a narrow uniform and a decaying exponential and a mixture of a uniform and Gaussian  The uniform distribution captures the expert knowledge that a lot of the protocols involve a response within a window of time  we call this co occurrence   The Gaussian delay distribution extends the intuitions of co occurrence within a window to also capture dependencies that can be relatively far away in time  such as with the printer   The left tail of the Gaussian corresponding to negative delays is truncated  The exponential distribution captures the intuition that the possibility of dependency decays as the events are further away in time  this is true for the HTTP protocol   We will not explicitly expand these functions in the derivations as they tend to obscure the exposition  Needless to say that the parameters of these functions are all fitted automatically using EM as described in the next section  Groups of channels may have different delay distributions  in which case the delay distribution can be indexed by the channel group and all the derivations in this paper remain the same  For example  channels can be grouped by network service  where all HTTP channels have the same delay distribution  thus allowing data from multiple channels to assist in parameter fitting   but the DNS channels are allowed a different delay distribution  All the experiments in the paper use a leak  a pseudo channel with a single event at the start of the observation period and a delay distribution that is uniform over the length of the observations  This leak captures events which are not explained by the remaining channels      Fitting a CT NOR model  We perform inference and estimation on the model through the EM algorithm  We first set the stage by finding a suitable bounding function B z  for the likelihood  The EM algorithm iteratively chooses a tight bound in the E step and then maximizes the  j  bound in the M step  Let zkl be some positive vector P  j   j  such that jk zkl     for each l  For a fixed l  zkl is the probability of the latent state indicating that packet k on channel j caused output l  Then from Eq     log L o i        n X  X  w j  f  ol  ik    log  X  zkl  l          n X  jk  l          n X   j   log   j  w  Since the Poisson Process produces unordered outputs but the events are considered to be sorted  a permutation factor of n  is required  It cancels out the n  in the Poisson density    j   f  ol  ik    j   zkl  jk  log Ez   j   w j  f  ol  ik    j   zkl  l    Now  by Jensens inequality  log L o i   B z  where   j   B z        X  Ez log  w j  f  ol  ik    j   zkl  l       E Step  For a particular choice of   the parameters of the f function  and w j    the bound above is tight when  j    j   zkl   P  w j  f  ol  ik    j     j  k  because in that case   w j    f  ol  ik    j   w  j  f  ol ik    is a constant for   j  zkl  a fixed l and E log C   log EC   log C  Therefore   j  we use these choice of zkl        M step  j   For a fixed choice of zkl   we need to maximize the bound with respect to w j  and   Optimizing with respect to w j    we notice that the derivative is X X  j    B  j    n   zkl  j  w j  w l      j   yielding  w   j       j  kl zkl n j   P  k   With respect to   we can say that X  j   j  zkl log f  ol  ik      arg max   jkl  which is simply the parts of the objective function that depend on   This can be a very easy optimization problem for a large class of distributions  as it is of the same form as maximum likelihood parameter observation given observed data points and corresponding counts  For example  for the exponential family  this simply requires moment matching        P   j   jkl   j   zkl T  ol ik   P  j  jkl zkl  where    is the mean  parameterization of the estimated parameter  and T    are the sufficient statistics for the family      Relation to Noisy Or  As an alternative model  consider binning the observed data into windows of width  and modeling the presence or absence of output events in a particular bin as a NOR       The possible explanations  parents  are the presence of input events in preceding windows  We will show that a particular  natural parameterization of the NOR model is equivalent to CT NOR in the limit  as the bin width approaches zero  This relationship is important because it provides a nontrivial extension of NOR to domains with continuous time and provides insight into the independence structure of the two models  Let Ot be an indicator of presence of output events  j  between the times t and t    and It be the indicator for input events from channel j in that same time period  We will use PNOR to denote the probability under the NOR model and PCT NOR for probability under CT NOR   of distributions  this parameterization imposes only minor constraints on the weights  but will be useful for reasoning about NOR models which model the same data but with differing bin widths  When the bin width is halved  the probability that one of the sub bins has an output event must be equal to the probability that the large bin has an output event plus a second order term  This condition is required for a coherent parameterization of a family of NOR distributions and follows from the technical conditions placed on f   We argue that as the bin width  decreases  this model becomes equivalent to a CT NOR with a suitable choice of parameters  Choose a  sufficiently small that each bin contains at most one input event per channel  and at most one output event  We will t use PNOR to denote PNOR  Ot     Input   the probability that the tth bin has no output events falling into it   t PNOR       YY     w j  f   t  s  I j  s j s t     YY  j     k        j     w j  f  t  ik     o       XX  j  k  Under a CT NOR model which uses the same w j  and the same f   the probability of not observing any outputs is very similar  We use  to denote the parameter of the Poisson random variable governing the number of outputs in the interval        XX j  PNOR  Ot      Input     YY        j    p ts  I j  s      j s t   j   The p ts  is the weight associated with the possible  j   explanation Is   To prevent the number of parameters from increasing as the bin size becomes small  reparameterize with  j   p ts    w j  f   t  s   for any distribution f that satisfies some technical conditions   Since f may be a very flexible family    It is sufficient for the density to exist and be Lips      j  w j  f  t  ik     o         t   t  k  XX j  t PCT NOR  w j  w   j    j   f  x  ik  dx  j   f  t  ik     o       k    P  Poisson          exp           o      t   PNOR   o       chitz  which means that there exists a constant C such that  f  a   f  b    C a  b  for any a  b  Any continuously differentiable function with a bounded derivative satisfies this condition  It is easy to extend this proof to any bounded density with a finite number of discontinuities which has a bounded derivative everywhere except for the discontinuities    These results can be combined to demonstrate that the probability assigned to any set of output events by the two models is equal up a factor of      o n   which converges to   as  decreases to zero  The asymptotics are in terms of bin width  decreasing to zero for a fixed set of observations  so n and T are constant   PNOR  Out In  PCT NOR  Out In    Ot    Ot T     t t Y    PNOR PNOR   t t PCT NOR    PCT NOR t          o      T  n       o   n          T    n o             no            T   n o           o    CT NOR and NOR with an increasingly small bin size assign equivalent probability to any sequence of output events  indicating that the two classes of models are closely related  and that CT NOR is the model that emerges as the limit when the NORs bin size is decreased toward zero      Dependence discovery and change point detection  With the probabilistic framework described in the previous section  we can use statistical machinery to perform inference for two applications  a  inputoutput relation discovery and b  change point detection  The next two subsections describe the algorithms in detail and also validate the main assumptions using synthetically generated data  The final subsection       describes a computationally efficient approximation to the hypothesis test procedures       Dependence discovery  For the purposes of network management  a crucial problem is dependence discovery  For each computer in the network  we are interested in automatically finding out from observations which input channels have a causal effect on an output channel  We can frame the dependency discovery task as hypothesis testing  Specifically  testing whether an input channel j causes output events corresponds to testing the hypothesis that w j       One way of testing this hypothesis is through the likelihood ratio test       We fit two models  Mfull   under which   all the parameters are unrestricted  and Mres   under which w j  is constrained to be zero  The test statistic in this case is   log      log  LMres  Data  LMfull  Data   The asymptotic distribution of this test statistic is called a   and is a mixture of   with different degrees of freedom  The weights depend on the Fisher information matrix and are difficult to compute     but the significant terms in the mixture are    and    which is a delta function at zero  The   emerges as the null distribution instead of the more familiar   because the weight parameters w   are constrained to be non negative  and when an estimated w j  is zero in the unconstrained model  imposing the constraint does not change the likelihood  If a set of true null hypotheses is known  the mixture coefficients can be trivially estimated  with the weight of    being the proportion of test statistics that are    When no ground truth is available  the proportion of null hypotheses can be estimated using the method described in      and then used to estimate the mixture proportions  To demonstrate that the model efficiently recovers the true causal channels and has the proper teststatistic distribution under the null hypothesis  we first test the model on synthetic data that is generated according to some instantiation of the model     input channels are generated  half of them have no causal impact on output events and half produce a Poisson       number of output events with the delay distribution of Exponential       Note that the causality is weak  very few input events actively produce an output  For each hour      input events per channel  the corresponding output events  and     uniformly random noise events  which are not caused by any input activity  are produced  The resulting p values are plotted in Figure    Observe that the null p values  conditioned on the test statistic being non zero  are distributed uniformly  This is evidenced by the p values following the diagonal on the quantile quantile plot  The alternative p values  without any conditioning  for channels which exhibit causality are mostly very low  with     being below      Furthermore  the specific parameter estimates  the delay distribution parameter and w j    are in line with their true values       Changepoint Detection  When the relationship between events is altered  it can be an indication of a significant change in the                                 PValues                             PValues                                          Uniform                      Uniform  Figure    Quantile quantile plot of dependency discovery p values for   hours of synthetic data  The red circles are the distribution of p values for the null hypotheses  and are uniform  The blue triangles show p values of the alternative hypotheses and are small  indicating power   Figure    Quantile quantile plot of the p values for changepoint detection on synthetic data  The red circles are null hypotheses  no changepoint   the green diamonds are a weak alternative  w j  increases from      to       and the blue triangles are a strong alternative  w j  increases from      to         system  in the case of Constellation  this is of interest to the system administrators  We describe a building block for identifying whether the parameters w j  change between two time periods and demonstrate its correct functionality  Changepoint algorithms have long been studied in machine learning and statistics  and our test for whether the behavior of a parameter is altered between two time periods can be plugged into one of many existing algorithms  Furthermore  the simple two period test described here is sufficient for many monitoring applications   the weight changes   We again use the log likelihood ratio test methodology  In order to do that  it is necessary to extend the model to allow the parameters to depend on time  The model can be written as   X X  j   j   o   P P  w  j  f  ol  ik     j  k  ik  Detecting changepoints is accomplished by testing two hypotheses  The null is that the weights do not change between two time periods  and can be written  j  as wt   w j    Under the alternative  for a particular channel of interest m and an interval of time S   j    m   j   wt   m   wt    w j    w m  if t  S  w m  otherwise   The existence of a changepoint is equivalent to rejecting the null hypothesis  Fitting the alternative model is a simple modification of the EM procedure described for the null model  for fast performance  it is possible to initialize at the null models parameter values and take a single M step  reusing the latent variable distribution estimated in the E step  The test statistic in this case will again be   log  and its null distribution will be   if the true w m      and   otherwise  Figure   shows a quantile quantile plot of the pvalues  computed using the   distribution  under the null hypothesis  computed for causal channels of the same synthetic data as in section      there are two hours of data with     input events per channel per hour  As expected  the quantile quantile plot forms a straight line  demonstrating that on the synthetic dataset  the null test statistic has a   distribution  When a strong changepoint is observed  w j  changes from      to         the p values are very low  When a weak changepoint is observed  w j  changes   from      to       the p values are lower than under the null distribution but power is significantly lower than when detecting the major changepoint   ROC for HTTP         Bounding the log likelihood ratio  Computing the log likelihood ratio requires refitting a restricted model  though only a small number of EM steps is typically required  However  it is possible to bound the log likelihood ratio for dependency discovery very efficiently   True Positive Rate                 NoisyOR with bounds NoisyOR with exact computations Unamb  coocc  Std  coocc               For the restricted model testing channel ms causality  we must compute the likelihood under the constraint that w m       Take the estimates of w of the  unrestricted model and let    w m    Instead n m  of computing the ratio with the true maximum likelihood parameters for the restricted model  we propose a set of restricted parameters  and compute the ratio using them  We produce a restricted version of parameters w   by setting w m  to zero and inflating the rest by a factor of   That simply corresponds to imposing the restriction  and redistributing the weight among the rest of the parameters  so that the expected number of output packets remains the same  In that case   LMres  Data  LMf ull  Data  P Y j  m k w j  f  ol  i j  k      log P  j   j  f  ol  ik   l jk w   P  m   m  Y f  ol  ik   kw     log    P  j  f  o  i j     l l jk w k   Y X  j      log    zml    log      log  l  k  j As a reminder  zml is the latent variable distribution estimated in the E step of EM  Since the numerator of the log likelihood ratio is a lower bound and the denominator exact  this expression is a lower bound P  j    Q   on   Intuitively  log l    k zml corresponds to the probability that channel m has exactly   output events assigned to it when causality is assigned according to the EM distribution on the latent variables   The log  term corresponds to the increase in likelihood from redistributing channel ms weight among the other channels                            FDR  Figure    ROC for CT NOR and competing algorithms on data from a real enterprise network  Both the exact an approximate CT NOR tests produce detection results superior to the alternative methods      Results  We describe the results of applying the algorithms of the previous section to a subset of a real dataset consisting of a trace comprising headers and partial payload of around    billions packets collected over a     week period in      at Microsoft Research in Cambridge  England  This site contains about     networked machines and the trace captures conversations over      off site IP addresses  Ground truth for dependence discovery and change point detection is not readily available and it has to be manually generated  We took    hours of data at the web proxy and manually extracted ground truth for the HTTP traffic at this server by deep inspection of HTTP packets  It is with this part of the data that we validate our algorithms  as it provides us with objective metrics  such as precision and recall  to assess the performance of our algorithms       Dependency Discovery  First  we are interested in assessing the performance of the dependence discovery capabilities of our model and hypothesis testing algorithm  In the application of diagnosis and monitoring of networked systems it is crucial to maintain a consistent map of all the server and services inter dependencies and their changes  Finding dependencies at the server level is the main building block used by Constellation     in building this global map  We compare our method to two other alternatives  One is a simple binomial test  for each input channel  we count the number of output packets falling within a W width window of        Changepoint Detection  Since the true presence or absence of a changepoint is unknown  we estimate it from the actual packet causes  obtained through deep inspection of HTTP packets  We collect a set of input and output channel pairs for which there is no evidence of change  We regard these as coming from the null hypothesis  A set of pairs for which the ground truth provides strong evidence of a change are collected  and considered to be from the alternative hypothesis  We apply our changepoint test to that population  and report the results in Figure    The CT NOR changepoint detection algorithm produces uniformly distributed p values for channels which come from the null hypothesis and do not exhibit a changepoint  confirming that our null hypothesis distribution is calibrated  On the other hand  the test on alterna  As sometimes an input package generates more than one output packet  we enabled our model to account for this by allowing autocorrelations to take place  Namely a packet in an output channel can depend on an input channel or on the  time wise  preceding output packet                            As can be seen on the ROC curve in Figure    CT NOR successfully captures     of the true correlations with a    false positive rate  In total  the model detects     of the true correlations at     of false positives  We want to additionally point out that some of correlations present are very subtle      of the correlations are evidenced by a single output packet  We also point out that CT NOR performs significantly better than both alternatives based on co occurrence of input packets  providing even more conclusive evidence that CT NOR is capturing nontrivial dependencies  The approximation error from using the bound of section     is minimal  while the computation savings are significant  On a relatively slow laptop  the bounds on log likelihood ratio test for a hour of traffic on a busy HTTP proxy can be computed in   seconds  exact computations take    seconds   PValues  an input packet  and determine whether that number is significantly higher than if the output packets were uniformly distributed  We call this standard co occurrence  The second alternative considers an input and output channel to be dependent only if there is a unique input packet in the immediate vicinity of an output packet  The reason we select these two alternatives is that a  they reflect  by and large  current heuristics used in the systems community     and b  they will capture essentially the easy dependencies  as our results indicate                                   Uniform  Figure    Quantile Quantile plot of changepoint pvalues  The red circles are channel pairs which  according to the ground truth  not exhibit a changepoint  The blue triangles represent channel pairs exhibiting change according to the ground truth  tive hypothesis channels produces a large proportion of very small p values  indicating confidence that a changepoint occurred      Conclusions and Future Work  We presented a generative model based on Poisson processes called CT NOR  to model the relationship between events based on the time of their occurrences  The model is induced from data only containing information about the time stamps for the events  This capability is crucial in the domain of networked systems as collecting any other type of information would entail prohibitive amounts of overhead  Specific domain knowledge about the expected shape of the distribution of the time delay between events can be incorporated to the model using a parameterized function  The EM algorithm used to fit the parameters of the model given the data also induces the parameters of this function  The combination of knowledge engineering and learning from data is clearly exemplified in the application we presented to the domain of computer systems  where we used a mixture model consisting of an exponential and a uniform distribution  In terms of applying the model we focused on providing building blocks for diagnosis and monitoring    We provided algorithms based on statistical hypothesis testing for  a  discovering the dependencies between input and output channels in computer networks  and for  b  finding changes in expected behavior  change point detection   We validated these algorithms first on synthetic data  and then on a subset  HTTP traffic  of a trace of real data from events in a corporate communication network containing     computers and servers  The relationship presented in Section   between CT NOR and the NOR gate is interesting for multiple reasons  First  as the NOR gate has been extensively studied in this community in modeling and learning environment and in causal discovery      the immediate benefits are a  increasing the applicability to continuous time  and b  augmenting its modeling capabilities using the time delay functions used in this work  Second  this correspondence provide us with another intuition on the independence assumptions behind the Poisson process  as applied to the characterization of the relationship between the events in various inputs to the events in a specific output  For the particular application of dependency discovery between channels in a computer network we explored a varied set of alternative approaches  They all failed miserably  Among these  we briefly discuss two  We cast the problem as one of classification  and tried a host of Bayesian network classifiers      The idea was to first discretize time into suitable periods  and then have as features the existence or absence of events in the input channels and as the class the existence or absence of events in the output channel  The accuracy was abysmal  The main problem with this approach is that the communication in these networks is bursty by nature with relatively large periods of quiet time  Once we started to look at Poisson as the appropriate way to quantify the distributions in these classifiers the choice of the Poisson process became clear  We also explored the use of hypothesis testing comparing the inter time between events in the input and output channels to the inter time between the input and a fictitious random channel  The accuracy in terms of false positives and true positives was worse than those based on co occurrence  The main problem here is that we are considering pairwise interactions and there are many confounder in all the other channels  With regards to related approaches  both the work on continuous time Bayesian networks      and in general about dynamic Bayesian networks  e g        are obviously very different in terms of the parameterization of the models  the assumptions  and the  intended application  The work that is closest to ours is contained in the paper by Rajaram et al      where they propose a  graphical  model for point processes in terms of Poisson Networks  The main difference between their work and ours is the tradeoff between representation capabilities and complexity in inference that the different foci of our respective papers entails  Due to the distributed nature of our application domain  we concentrate on modeling the families  local parent child relationship  and basically assume that we can reconstruct  in a distributed manner based on the local information  the topology of the network  This enables us to induce families with large numbers of parents  and with relatively complex interactions as given by the delay function f   while performing inference efficiently  In the Poisson Networks paper       the number of parents of each node are restricted  and the rate function is parameterized by a generalized linear model  Even with these  relatively benign  restrictions inference is non trivial in terms of finding the structure of the Bayesian network and indeed this is a contribution of that paper  Obviously  future work includes merging both approaches  an immediate benefit would be to decrease the vulnerability of our approach to spurious causal dependencies due to ignoring the global structure in the estimation  There are other three threads that we are currently investigating for future work  The first one involves recasting the fitting and inference procedures described in the model in the Bayesian framework  An advantage of the Bayesian approach will be on the inclusion of priors  As channels differ greatly on the number of events this can further increase the accuracy of discovery  A second direction is that of incorporating False Discovery Rates     calculations in order to accurately estimate false positives when we dont have ground truth regarding the relationship between the channels  As we are performing a large number of hypothesis tests  this becomes a necessity  In     we experimented with the basic approach described in      and we verified that the approach is very conservative in the context of the HTTP and DNS protocols where we do have ground truth  We plan to explore less conservative approaches such as the one described in      or adapt the one explored in      Finally we are in the process of getting suitable data and plan to apply this model to biological networks such as neurons that communicate with other neurons using spikes in electrical potential       Acknowledgments  We thank T  Graepel for comments on a previous version of this paper  We are also grateful for the helpful suggestions of the anonymous reviewers which we hope we have addressed to their satisfaction   
