 We study contextual bandits with ancillary constraints on resources  which are common in realworld applications such as choosing ads or dynamic pricing of items  We design the first algorithm for solving these problems that handles constrained resources other than time  and improves over a trivial reduction to the non contextual case  We consider very general settings for both contextual bandits  arbitrary policy sets  Dudik et al          and bandits with resource constraints  bandits with knapsacks  Badanidiyuru et al       a    and prove a regret guarantee with near optimal statistical properties      Introduction Contextual bandits is a machine learning framework in which an algorithm makes sequential decisions according to the following protocol  in each round  a context arrives  then the algorithm chooses an action from the fixed and known set of possible actions  and then the reward for this action is revealed  the reward may depend on the context  and can vary over time  Contextual bandits is one of the prominent directions in the literature on online learning with exploration exploitation tradeoff  many problems in this space are studied under the name multi armed bandits  A canonical example of contextual bandit learning is choosing ads for a search engine  Here  the goal is to choose the most profitable ad to display to a given user based on a search query and the available information about this user  and optimize the ad selection over time based on user feedback such as clicks  This description leaves out many important details  one of which is that every ad is associated with a budget which constrains the maximum amount of revenue which that ad can generate  In fact  this issue is so important that in some formulations it is the primary problem  e g   Devanur and Vazirani         The optimal solution with budget constraints fundamentally differs from the optimal solution without constraints  As an example  suppose that one ad has a high expected revenue but a small budget such that it can only be clicked on once  Should this ad be used immediately  From all   This is the full version of a paper in the   th Conf  on Learning Theory  COLT         The present version includes a correction for Theorem    a corollary for contextual dynamic pricing with discretization  and an updated discussion of related work  The main results have been obtained while A  Badanidiyuru was a research intern at Microsoft Research New York City  A  Badanidiyuru was also partially supported by NSF grant AF         of Robert Kleinberg   c A  Badanidiyuru  J  Langford   A  Slivkins    BADANIDIYURU L ANGFORD S LIVKINS  reasonable perspectives  the answer is no  From the users or advertisers perspective  we prefer that this ad be displayed for the user with the strongest interest rather than for a user who simply has more interest than in other options  From a platforms viewpoint  it is better to have more ads in the system  since they effectively increases the price paid in a second price auction  And from everyones viewpoint  it is simply odd to burn out the budget of an ad as soon as it is available  Instead  a small budget should be parceled out over time  To address these issues  we consider a generalization of contextual bandits in which there are one or several resources that are consumed by the algorithm  This formulation has many natural applications  Dynamic ad allocation follows the ad example described above  here  resources correspond to advertisers budgets  In dynamic pricing  a store with a limited supply of items to sell can make customized offers to customers  In dynamic procurement  a contractor with a batch of jobs and a limited budget can experiment with prices offered to the workers  e g  workers in a crowdsourcing market  The above applications have been studied on its own  but never in models that combine contexts and limited resources  We obtain the first known algorithm for contextual bandits with resource constraints  other than time  that improves over a trivial reduction to the non contextual version of the problem  As such  we merge two lines of work on multi armed bandits  contextual bandits and bandits with resource constraints  While significant progress has been achieved in each of the two lines of work  in particular  optimal solutions have been worked out for very general models   the specific approaches break down when applied to our model  Our model  We define resourceful contextual bandits  in short  RCB   a common generalization of two general models for contextual bandits and bandits with resource constraints  respectively  contextual bandits with arbitrary policy sets  e g   Langford and Zhang        Dudik et al         and bandits with knapsacks  Badanidiyuru et al       a   There are several resources that are consumed by the algorithm  with a separate budget constraint on each   Time is one of these resources  with deterministic consumption of   for every action   In each round  the algorithm receives a reward and consumes some amount of each resource  in a manner that depends on the context and the chosen action  and may be randomized  We consider a stationary environment  in each round  the context and the mapping from actions to rewards and resource consumption is sampled independently from a fixed joint distribution  called the outcome distribution  Rewards and consumption of various resources can be correlated in an arbitrary way  The algorithm stops as soon as any constraint is violated  Initially the algorithm is given no information about the outcome distribution  except the distribution of context arrivals   In particular  expected rewards and resource consumptions are not known  An algorithm is given a finite set  of policies  mappings from contexts to actions  We compete against algorithms that must commit to some policy in  before each round  Our benchmark is a hypothetical algorithm that knows the outcome distribution and makes optimal decisions given this knowledge and the restriction to policies in   The benchmarks expected total reward is denoted OPT    Regret of an algorithm is defined as OPT   minus the algorithms expected total reward  For normalization  per round rewards and resource consumptions lie in         We assume that the distribution of context arrivals is known to the algorithm  Discussion of the model  Allowing stochastic resource consumptions and arbitrary correlations between per round rewards and per round resource consumptions is essential  this is why our model      R ESOURCEFUL C ONTEXTUAL BANDITS  subsumes diverse applications such as the ones discussed above   and many extensions thereof  Further discussion of the application domains can be found in Appendix A  Intuitively  the policy set  consists of all policies that can possibly be learned by a given learning method  such as linear estimation or decision trees  Restricting to  allows meaningful performance guarantees even if competing against all possible policies is intractable  The latter is common in real life applications  as the set of possible contexts can be very large  Our benchmark can change policies from one round to another without restriction  As we prove  this is essentially equivalent in power to the best fixed distribution over policies  However  the best fixed policy may perform substantially worse   Our stopping condition corresponds to hard constraints  an advertiser cannot exceed his budget  a store cannot sell more items than it has in stock  etc  An alternative stopping condition is to restrict the algorithm to actions that cannot possibly violate any constraint if chosen in the current round  and stop if there is no such action  This alternative is essentially equivalent to the original version   Moreover  we can w l o g  allow our benchmark to use this alternative  Our contributions  main algorithm  We design an algorithm  called MixtureElimination  and prove the following guarantee on its regret  Theorem   For all RCB problems with K actions  d resources  time horizon T   and for all policy sets   Algorithm MixtureElimination achieves expected total reward  p REW  OPT    O     B  OPT   dKT log  dKT          where B   mini Bi is the smallest of the resource constraints B            Bd    This regret guarantee is optimal in several regimes  First  we achieve an optimal square root scaling of regret  if all constraints are scaled by the same parameter       thenregret scales    T  i e   there are no constraints   we recover the optimal O  KT   regret  as   Second  if B  Third  we achieve O  KT   regret for the important regime when OPT   and B are at  least a constant fraction of T   In fact  Badanidiyuru et al       a  provide a complimentary   KT   lower bound forpthis regime  which holds in a very strong sense  for any given tuple  K  B  OPT    T    The log    term in Theorem   is unavoidable  Dudik et al          The dependence on the minimum of the constraints  rather than  say  the maximum or some weighted combination thereof  is also unavoidable  Badanidiyuru et al       a   For strongest results  one can rescale per round rewards and per round consumption of each resource so that they can be as high as     Note that the regret bound in Theorem   does not depend on the number of contexts  only on the number of policies in   In particular  it tolerates infinitely many contexts  On the other hand  if the set X of contexts is not too large  we can also obtain a regret bound with respect to the best policy among all possible policies  Formally  take     all policies  and observe that     K  X    Further  Theorem   extends to policy sets  that consist of randomized policies  mappings from contexts to distributions over actions  This may significantly reduce     as a given randomized    For example  in dynamic pricing the algorithm receives a reward and loses an item only if the item is sold     The expected total reward of the best fixed policy can be half as large as that of the best distribution  This holds for several different domains including dynamic pricing   procurement  even without contexts  Badanidiyuru et al       a   Note that without resource constraints  the two benchmarks are equivalent     Each budget constraint changes by at most one  which does not affect our regret bounds in any significant way      then multiplying it by    would    E g   if per round consumption of some resource i is deterministically at most    effectively increase the corresponding budget Bi by a factor of     and hence can only improve the regret bound       BADANIDIYURU L ANGFORD S LIVKINS  policy might not be representable as a distribution over a small number of deterministic policies   We assume deterministic policies in the rest of the paper  Computational issues  This paper is focused on proving the existence of solutions to this problem  and the mathematical properties of such a solution  The algorithm is specified as a mathematically well defined mapping from histories to actions  we do not provide a computationally efficient implementation  Such information theoretical results are common for the first solutions to new  broad problem formulations  e g  Kleinberg et al         Kleinberg and Slivkins        Dudik et al          Inparticular  in the prior work for RCB without resource constraints there exists an algorithm with O  KT   regret  Auer et al         Dudik et al          but for all known computationally efficient algorithms regret scales with T as T      Langford and Zhang         Our contributions  partial lower bound  We derive apartial lower bound  we prove that RCB is essentially hopeless for the regime OPT    B  KT     The condition OPT    B is satisfied  for example  in dynamic pricing with limited supply  Theorem   Any algorithm for RCBincurs regret  OPT    in the worst case over all problem instances such that OPT    B  KT     using the notation from Theorem     The above lower bound is specific to the general  contextual  case of RCB  In fact  it points to a stark difference between RCB and the non contextual version  in the latter  o OPT  regret is achievable as long as  for example  B  log T  Badanidiyuru et al       a   While Theorem   is concerned  with the regime of small B  note that in the opposite regime in Theorem   is quite low  it can be of very large B namely B  KT   the regret achieved  expressed as O  KT   o  OPT     where B    o KT   Our contributions  discretization  In some applications of RCB  such as dynamic pricing and dynamic procurement  the action space is a continuous interval of prices  Theorem   usefully applies whenever the policy set  is chosen so that the number of distinct actions used by policies in  is finite and small compared to T    Because one can w l o g  remove all other actions   However  one also needs to handle problem instances in which the policies in  use prohibitively large or infinite number of actions  We consider a paradigmatic example of RCB with an infinite action space  contextual dynamic pricing with a single product and prices in the        interval  We derive a corollary of Theorem   that applies to an arbitrary finite policy set   To the best of our knowledge  this is the first result on contextual dynamic pricing with infinite price set  We use discretization  we reduce the original problem to one in which actions  i e   prices  are multiples of some carefully chosen o      Our approach proceeds as follows  For each o     and each policy  let o be a policy that takes the price computed by  and rounds it down to the nearest multiple of o  We define the discretized policy set o    o        We use Theorem   to obtain a regret bound relative to o   Here the o controls the tradeoff between the number of actions in that regret bound and the discretization error of o   Then we optimize the choice of o to obtain    We can reduce RCB with randomized policies to RCB with deterministic policies simply by replacing each context x with a vector  a x          such that a x       x   and encoding the randomization in policies through the randomization in the context arrivals  While this blows up the context space  it does not affect our regret bound       R ESOURCEFUL C ONTEXTUAL BANDITS  the regret bound relative to   The technical difficulty here is to bound the discretization error in terms of o  for this purpose we assume Lipschitz demands   Theorem   Consider contextual dynamic pricing with a single product and prices in         Use standard notation  supply B  policy set  and time horizon T   Assume Lipschitz demands with Lipschitz constant L  Then algorithm MixtureElimination with discretized policy set o  defined as above  and o suitably chosen as a function of  B  T  L      achieves expected total reward REW  OPT    O T     B         L log  T               This regret bound is most interesting for the important regime B   T    studied  for example  in Besbes and Zeevi               Wang et al           Then regret is O T        L log  T           It is unclear whether this regret bound is optimal  When specialized to the non contextual case  it is not optimal  The optimal regret is then O B        even for an arbitrary budget B and even without the Lipscitz assumption  Babaioff et al          Extending the discretization approach beyond dynamic pricing with a single product is problematic even without contexts  see Section    for further discussion  Discussion  main challenges in RCB  The central issue in bandit problems is the tradeoff between exploration  acquiring new information  and exploitation  making seemingly optimal decisions based on this information  In this paper  we resolve the explore exploit tradeoff in the presence of contexts and resource constraints  Each of the three components  explore exploit tradeoff  contexts  and resource constraints  presents its own challenges  and we need to deal with all these challenges simultaneously  Below we describe these individual challenges one by one  A well known naive solution for explore exploit tradeoff  which we call pre determined exploration  decides in advance to allocate some rounds to exploration  and the remaining rounds to exploitation  The decisions in the exploration rounds do not depend on the observations  whereas the observations from the exploitation rounds do not impact future decisions  While this approach is simple and broadly applicable  it is typically inferior to more advanced solutions based on adaptive exploration  adapting the exploration schedule to the observations  so that many or all rounds serve both exploration and exploitation   Thus  the general challenge in most explore exploit settings is to design an appropriate adaptive exploration algorithm  Resource constraints are difficult to handle for the following three reasons  First  an algorithms ability to exploit is constrained by resource consumption for the purpose of exploration  the latter is stochastic and therefore difficult to predict in advance  Second  the expected per round reward is no longer the right objective to optimize  as the action with the highest expected per round reward could consume too much resources  Instead  one needs to take into account the expected reward over the entire time horizon  Third  with more than one constrained resource  incl  time  the best fixed policy is no longer the right benchmark  instead  the algorithm should search over distributions over policies  which is a much larger search space  In contextual bandit problems  an algorithm effectively chooses a policy    in each round  Naively  this can be reduced to a non contextual bandit problem in which actions correspond to    Lipschitz demands is a common assumption in some of the prior work on  non contextual  dynamic pricing  even with a single product  Besbes and Zeevi        Wang et al          However  the optimal algorithm for the single product case  Babaioff et al         does not need this assumption      For example  the difference in regret between pre determined and adaptive exploration is O  KT   vs  O K log T           for stochastic K armed bandits  and O T   vs  O B   for dynamic pricing with limited supply       BADANIDIYURU L ANGFORD S LIVKINS  policies  In particular  the main results in Badanidiyuru et al       a  directly apply to this reduced problem  However  the action space in the reduced problem has size     accordingly  regret scales p as    in the worst case  The pchallenge in contextual bandits is to reduce this dependence  In particular  note that we replace    with log     an exponential improvement   Organization of the paper  We start with a survey of related work and preliminaries  Sections       We define the main algorithm  prove its correctness  and describe the key steps of regret analysis in Sections      The remaining details of the regret analysis are in Section    We prove the lower bound in Section    We conclude with an extensive discussion of the state of art for RCB and the directions for further work  Sections      Appendix A contains a discussion of the main application domains for RCB      Related work Multi armed bandits have been studied since Thompson        in Operations Research  Economics  and several branches of Computer Science  see  Gittins et al         Bubeck and Cesa Bianchi        for background  This paper unifies two active lines of work on bandits  contextual bandits and bandits with resource constraints  Contextual Bandits  Auer        Langford and Zhang        add contextual side information which can be used in prediction  This is a necessary complexity for virtually all applications of bandits since it is far more common to have relevant contextual side information than no such information  Several versions have been studied in the literature  see  Bubeck and Cesa Bianchi        Dudik et al         Slivkins        for a discussion  For contextual bandits with policy sets  there exist two broad families of solutions  based on multiplicative weight algorithms  Auer et al         McMahan and Streeter        Beygelzimer et al         or confidence intervals  Dudik et al         Agarwal et al          We rework the confidence interval approach  incorporating and extending the ideas from the work on resource constrained bandits  Badanidiyuru et al       a   Prior work on resource constrained bandits includes dynamic pricing with limited supply  Babaioff et al         Besbes and Zeevi               dynamic procurement on a budget  Badanidiyuru et al         Singla and Krause        Slivkins and Vaughan         dynamic ad allocation with advertisers budgets  Slivkins         and bandits with a single deterministic resource  Guha and Munagala        Gupta et al         Tran Thanh et al                Badanidiyuru et al       a  define and optimally solve a common generalization of all these settings  the non contextual version of RCB  An extensive discussion of these and other applications  including applications to repeated auctions and network routing  can be found in  Badanidiyuru et al       a   To the best of our knowledge  the only prior work that explicitly considered contextual bandits with resource constraints is  Gyorgy et al          This paper considers a somewhat incomparable setting with arbitrary policy sets and a single constrained resource  time  whose consumption is stochastic and depends on the context and the chosen action  Gyorgy et al         design an algorithm whose regret scales O f  t  log t  for any time t  where f is any positive diverging function and the constant in O   depends on the problem instance and on f   Our setting can be seen as a contextual bandit version of stochastic packing  e g  Devanur and Hayes        Devanur et al          The difference is in the feedback structure  in stochastic packing  full information about each round is revealed before that round       R ESOURCEFUL C ONTEXTUAL BANDITS  While we approximate our benchmark OPT   with a linear program optimum  our algorithm and analysis are conceptually very different from the vast literature on approximately solving linear programs  and in particular from LP based work on bandit problems such as Guha et al          Concurrent and independent work  Agrawal and Devanur        study a model for contextual bandits with resource constraints that is incomparable with ours  The model for contexts is more restrictive  contexts do not change over time   and expected outcome of each round is linear in the context  Whereas the model for rewards and resource constraints is more general  the total reward can be an arbitrary concave function of the time averaged outcome vector v  and the resource constraint states that v must belong to a given convex set  which can be arbitrary       Problem formulation and preliminaries We consider an online setting where in each round an algorithm observes a context x from a possibly infinite known set of possible contexts X and chooses an action a from a finite known set A  The world then specifies a reward r         and the resource consumption  There are d resources that can be consumed  and the resource consumption is specified by numbers ci         for each resource i  Thus  the world specifies the vector  r  c            cd    which we call the outcome vector  this vector can depend on the the chosen action a and the round  There is a known hard constraint Bi  R  on the consumption of each resource i  we call it a budget for resource i  The algorithm stops at the earliest time  when any budget constraint is violated  its total reward is the sum of the rewards in all rounds strictly preceding    The goal of the algorithm is to maximize the expected total reward  We are only interested in regret at a specific time T  time horizon  which is known to the algorithm  Formally  we model time as a specific resource with budget T and a deterministic consumption of   for every action  So d    is the number of all resources  including time  W l o g   Bi  T for every resource i  We assume that an algorithm can choose to skip a round without doing anything  Formally  we posit a null action  an action with   reward and   consumption of all resources except the time  This is for technical convenience  so as to enable Lemma    Stochastic assumptions  We assume that there exists an unknown distribution D x  r  ci    called the outcome distribution  from which each rounds observations are created independently and identically  where the vectors are indexed by individual actions  In particular  context x is drawn from the marginal distribution DX     and the observed reward and resource consumptions for each action a are drawn from the conditional distribution D r a   cia  x   We assume that the marginal distribution over contexts D x  is known  Policy sets and the benchmark  An algorithm is given a finite set  of policies  mappings from contexts to actions  Our benchmark is a hypothetical algorithm that knows the outcome distribution D  and makes optimal decisions given this knowledge  The benchmark is restricted to policies in   before each round  it must commit to some policy     and then choose action  x  upon arrival of any given context x  The expected total reward of the benchmark is denoted OPT    Regret of an algorithm is OPT   minus the algorithms expected total reward     Agrawal and Devanur        also claimed an extension to contexts that change over time  which has subsequently been retracted  see Footnote   in Agrawal and Devanur          This extension constitutes the main result in Agrawal and Devanur         which is subsequent work relative to the present paper        BADANIDIYURU L ANGFORD S LIVKINS  Uniform budgets  We say that the budgets are uniform if Bi   B for each resource i  Any problem instance can be reduced to one with uniform budgets by dividing all consumption values for every resource i by Bi  B  where B   mini Bi    That is tantamount to changing the units in which we measure consumption of resource i   We assume uniform budgets B from here on  Notation  Let r     E x r D  r  x    and ci      E x ci  D  ci  x    be the expected per round reward and the expected per round consumption of resource i for policy   Similary  define r P     EP  r    and ci  P     EP  ci     as the natural extension to a distribution P over policies  The tuple       r    c               cd            is called the expected outcomes tuple  For a distribution P over policies  let PP    is the probability that P places over policy   By a slight abuse of notation  let P  a x     x  a P    be the probability that P places on action a given context x  Thus  each context x induces a distribution P   x  over actions       Linear approximation and the benchmark We set up a linear relaxation that will be crucial throughout the paper  As a by product  we  effectively  reduce our benchmark OPT   to the best fixed distribution over policies  A given distribution P over policies defines an algorithm ALGP   in each round a policy  is sampled independently from P   and the action a    x  is chosen  The value of P is the total reward of this algorithm  in expectation over the outcome distribution  As the value of P is difficult to characterize exactly  we approximate it  generalizing the approach from  Babaioff et al         Badanidiyuru et al       a  for the non contextual version   We use a linear approximation where all rewards and consumptions are deterministic and the time is continuous  Let r P    and ci  P    be the expected per round reward and the expected per round consumption of resource i for policy   P   given expected outcomes tuple   Then the linear approximation corresponds to the solution of a simple linear program  Maximise t r P    in t  R subject to t ci  P     B for each i t           The solution to this LP  which we call the LP value of P   is LP P      r P    mini B ci  P           Denote OPTLP   supP LP P     where the supremum is over all distributions P over   Lemma   OPTLP  OPT    Therefore  it suffices to compete against the best fixed distribution over   as approximated by OPTLP   even though our benchmark OPT   allows unrestricted changes over time  Note that proving regret bounds relative to OPTLP rather than to OPT   only makes our results stronger  A distribution P over  that attains the supremum value OPTLP is called LP optimal  Such P is called LP perfect if furthermore  support P     d and ci  P     B T for each resource i  We find it useful to consider LP perfect distributions throughout the paper  Lemma   An LP perfect distribution exists for any instance of RCB       R ESOURCEFUL C ONTEXTUAL BANDITS  Lemma   and Lemma   are proved for the non contextual version of RCB in Badanidiyuru et al       a   The general case can be reduced to the non contextual version via a standard reduction where actions in the new problem correspond to policies in  in the original problem  For Lemma    Badanidiyuru et al       a  obtain an LP perfect distribution by mixing an LP optimal distribution with the null action  this is why we allow the null action in the setting      The algorithm  MixtureElimination The algorithms goal is to converge on a LP perfect distribution over policies  The general design principle is to explore as much as possible while avoiding obviously suboptimal decisions  Overview of the algorithm  In each round t  the following happens     Compute estimates  We compute high confidence estimates for the per round reward r   and per round consumption ci     for each policy    and each resource i  The collection I of all expected outcomes tuple that are consistent with these high confidence estimates is called the confidence region     Avoid obviously suboptimal decisions  We prune away all distributions P over policies in  that are not LP perfect with high confidence  More precisely  we prune all P that are not LP perfect for any expected outcomes tuple in the confidence region I  the remaining distributions are called potentially LP perfect  Let F be the convex hull of the set of all potentially LP perfect distributions     Explore as much as possible  We choose a distribution P  F which is balanced  in the sense that no action is starved  see Equation     for the precise definition  Note that balanced distributions are typically not LP perfect     Select an action  We choose policy    independently from P   Given context x  the action a is chosen as a    x   The algorithm adds some random noise  with probability q    the action a is instead chosen uniformly at random  for some parameter q    The algorithm halts as soon as the time horizon is met  or one of the resources is exhausted  The pseudocode can be found in Algorithm    Some details  After each round t  we estimate the per round consumption ci    and the per round reward r    for each policy    and each resource i  using the following unbiased estimators  e ci       r   a  x   ci   a  x   and re       P  a    x    x  P  a    x    x   The corresponding time averages up to round t are denoted ct i         t   t  X s    e cs i    and rt         t   t  X s    res      We show that with high probability these time averages are close to their respective expectations  To express the confidence p term in a more lucid way  we use the following shorthand  called confidence radius  radt      Crad  t  where Crad    log d T      is a parameter which we will fix later  We show that w h p  the following holds   r    rt      radt  K  t       ci     ct i      radt   K  t          for all i         BADANIDIYURU L ANGFORD S LIVKINS  Algorithm   MixtureElimination    Parameters   actions K  time horizon T   budget B  benchmark set   context distribution DX      Data structure  confidence region I   all feasible expected outcomes tuples                                      For each round t           T do t    distributions P over   P is LP perfect for some   I   Let Ft be the convex hull of t   Let  t   maxP Ft P         Choose a balanced distribution Pt  Ft   any P  Ft such that          q    K   K    E   where q    min     T log K T       q xDX     q    P   x  x    K   t       Observe context xt   choose action at to play  with probability q    draw at u a r  in A  else  draw   Pt and let at    xt    Observe outcome vector  r  c            cd    Halt if one of the resources is exhausted  Eliminate expected outcomes tuples from I that violate equations         Here  t   maxP Ft P     as in Algorithm         Correctness of the algorithm We need to prove that in each round t  some P  Ft satisfies      and Equations       hold for all policies    with high probability  Notation  Recall that Pt is the distribution over  chosen in round t of the algorithm  and q  is the noise probability  The noisy version of Pt is defined as Pt  a x        q    Pt  a x    q   K   x  X  a  A    Then action at in round t is drawn from distribution Pt   xt    Lemma   In each round t  some P  Ft satisfies      Proof First we prove that Ft is compact  here each distribution over  is interpreted as a   dimensional vector  and compactness is w r t  the Borel topology on R     This can be proved via standard real analysis arguments  we provide a self contained proof in Appendix B  In what follows we extend the minimax argument from Dudik et al          Our proof works for any q            and any compact and convex set F  F   Denote    maxP F P     for each     Let F be the set of all distributions over   Equation     holds for a given P  F if and only if for every distribution Z  F we have that      E f  P  Z    E   K  xDX Z P    x  x  where P  is the noisy version of P   It suffices to show that min max f  P  Z    K   P F ZF            R ESOURCEFUL C ONTEXTUAL BANDITS  We use a min max argument  noting that f is a convex function of P and a concave function of Z  by the Sions minimax theorem  Sion        we have that min max f  P  Z    max min f  P  Z    P F ZF  ZF P F       For each policy     let   argmaxF    be a distribution which maximizes the probability of selecting   Such distribution exists because       is a continuous function on a compact set F  Recall that         P Given any Z  F   define distribution PZ  F by PZ       Z        Note that PZ is a convex combination of distributions in F  Since F is convex  it follows that PZ  F  Also  P note that PZ  a x      x  a Z      Letting PZ be the noisy version of PZ   we have      X Z    min f  P  Z   f  PZ   Z    E P F xDX PZ   x  x         P X X X Z     Z       x  a    E   E  xDX xDX PZ  a x      q   PZ  a x    q   K aX aA    x  a     X   K   K     E xDX    q     q  aX     Thus  by Equation     we obtain Equation       To analyze Equations        we will use Bernsteins inequality for martingales  Freedman         via the following formulation from Bubeck and Slivkins         Lemma   Let G   G          Gn be a filtration  and X            Xn be real random variables P such that Xt is Gt  measurable  E Xt  Gt        and  Xt    b for some b      Let Vn   nt   E Xt   Gt     Then with probability at least     it holds that q Pn X   Vn log n       b  log   n     t   t  Lemma   With probability at least   T    Equations       hold for all rounds t and policies      Proof Let us prove Equation       The proof of     is similar   Fix round t and policy     We bound the conditional variance of the estimators ret     Specifically  let Gt be the  algebra induced by all events up to  but not including  round t  Then                 r    K   x  a  t   E ret      Gt     E  E       xDX Pt   x  x  Pt  a x   t xDX   aPt  The last inequality holds by the algorithms choice of distribution Pt   Since the confidence region I in our algorithm is non increasing over time  it follows that  t is non increasing in t  too  We conclude that Var  e rs      Gs     K  t for each round s  t  Therefore  noting that ret       P    xt   xt    K q    we obtain Equation     by applying Lemma   with Xt   ret     r           BADANIDIYURU L ANGFORD S LIVKINS     Regret analysis  proof of Theorem   We provide the key steps of the proof  the details can be found in Section    Let It and t be  resp   the confidence region I and the set  of potentially LP perfect distributions computed in round t  Let Conv t   be the convex hull of t   First we bound the deviations within the confidence region  Lemma   For any two expected outcomes tuples      It and a distribution P  Conv t     ci  P      ci  P       radt  dK    Proof  for each resource i     r P      r P       radt  dK              Let us prove Equation        Equation      is proved similarly   By definition of It   P  r P      r P        P     r       r       P   P    radt  K  t      It remains to prove that the right hand side is at most radt  dK   By linearity  it suffices to prove this for P  t   So let us assume P  t from here on  Recall that  support P     d since P is LP perfect  and P      t for any policy     Therefore  P P  P    radt  K  t     radt  KP       P    radt dK  P      radt  dK    Using Lemma   and a long computation  fleshed out in Section     we prove the following   Lemma    For any two expected outcomes tuples      It and a distribution P  Conv t    LP P      LP P        B  LP P           T  radt  dK   Let REWt and Ct i be  respectively  the  realized  total reward and average consumption of resource i up to and including round t  Recall that Pt is the noisy version of distribution Pt chosen by the algorithm in round t  Given Pt   the expected revenue in round t is  P and resource i consumption P respectively  r Pt     and ci  Pt      Denote r t    t ti   r Pt     and ci t    t ti   ci  Pt       Analysis of a clean execution  Henceforth  without further notice  we assume a clean execution where several high probability conditions are satisfied  Formally  the algorithms execution is clean   if in each round t Equations       are satisfied  and moreover min    t REWt  rt     Ct i  ct i    radt      In particular  the set t of potentially LP perfect distributions indeed contains a LP perfect distribution  By Lemma   and Azuma Hoeffding Inequality  clean execution happens with probability at least    T    Thus  it suffices to lower bound the total reward REWT for a clean execution  Lemma    For any distribution P   Conv t   and any expected outcomes tuple   It   min LP P     LP P       max LP P     P t  P t             R ESOURCEFUL C ONTEXTUAL BANDITS  Proof The proof consists of two parts  The second inequality in Equation      follows easily because the distribution which maximizes LP P    by definition belongs to t   and so LP P        max  P Conv t    LP P      max LP P     P t  To prove the first inequality in Equation       we first argue that LP P    is a quasi concave function of P   Denote i  P      B  r P    ci  P    for each resource i  Then i is a quasiconcave function of P since each level set  the set of distributions P that satisfy i  P      for some   R  is a convex set  Therefore LP P      mini i  P    is a quasi concave function of P as a minimum of quasi concave functions  P P Since P   Conv t    it is a convex combination P    Qt Q Q with Qt Q      Therefore    X Q Q   LP P        LP  Qt    min  Qt  Q     LP Q     By definition of quasi concave functions      min LP Q     Qt  The following lemma captures a crucial argument  Denote          t       B max LP P     T  radt  dK  P F   It  Lemma    For any expected outcomes tuple      It and distributions P    P   Conv t      LP P        LP P          t            Proof Assume P   P  t   In particular  P   P are LP perfect for some expected outcomes tuples      It   resp  Also  some distribution P   t is LP perfect for   by Lemma     Therefore  LP P        LP P        t   by Lemma     P   P      LP P        t   LP P         t   by Lemma     P   P      LP P         t   We proved Equation      for P    P   t   Thus   max LP P      min LP P       t    P t  P t        Next we generalize to P    P   Conv t     LP P        min LP P     P t   by Lemma       max LP P       t P t   LP P         t   by Equation         by Lemma       We proved Equation      for       We obtain the general case by plugging in Lemma             BADANIDIYURU L ANGFORD S LIVKINS  Next  we upper bound t in terms of t           B  OPTLP    T  radt  dK    Corollary    t   t   assuming that B     T  radt  dK   Proof Follows from Lemma    via a simple computation  see Section        Corollary    LP Pt      OPTLP     t   where  is the actual expected outcomes tuple  Proof Follows from Lemma    and Corollary     observing that Pt  Conv t   and OPTLP   LP P      for some P   t     In the remainder of the proof  which is fleshed out in Section    we build on the above lemmas and corollaries to prove the following sequence of claims  t  OPTLP  O t    T  B T   O radt  dK    REWt  Ct i        REWT  OPTLP  O T     To complete the proof of Theorem    we re write the last equation as REWT  f  OPTLP   for an appropriate function f     and observe that f  OPTLP    f  OPT  because function f    is increasing      Regret analysis  remaining details for the proof of Theorem        Proof of Lemma    We restate the lemma for convenience  Lemma For any two expected outcomes tuples      It and a distribution P  Conv t    Proof  LP P      LP P        B  LP P           T  radt  dK   For brevity  we will denote  LP   LP P     and  LP   LP P      r    r P     and  r    r P      ci   ci  P     and  ci   ci  P       By symmetry  it suffices to prove the upper bound for LP  LP   Henceforth  assume LP   LP   We consider two cases  depending on whether T  B ci  for all resources i         Case    Assume Equation      holds  Then LP   T r    Therefore by Lemma   LP  LP  T r   T r   T radt  dK   Case    Assume Equation      fails  Then LP   B r   ci for some resource i  We consider two subcases  depending on whether T  B cj  for all resources j             R ESOURCEFUL C ONTEXTUAL BANDITS  Subcase    Assume Equation      holds  Then  LP   T r                 LP  T  min r   r    LP        Equation      follows from      and LP   LP   For       ci    define r     r     ci      ci    f      B r   ci      Then f    is monotonically and continuously increasing function  with f      as   ci   For convenience  define f  ci       Let     min ci   radt  dK    By Lemma    we have f       Br   ci   Therefore  f       LP   LP  Br   ci  f       Thus  by Equation       we can fix           such that f      T  min r    r     r  r      B ci ci           r      B     ci    ci    r   B  B  f     LP  ci    ci         r   B       ci    ci        f    f          B r       T r  T r       B r        LP     T B    LP  B      T  radt  dK   LP   B  LP  f      T r   T min r  r      T  radt  dK      LP  LP   LP  f      f     LP    LP  B      T  radt  dK       Subcase    Assume Equation      fails  Then LP   B r   cj for some resource j  Note that ci  cj and cj  ci by the choice of i and j       BADANIDIYURU L ANGFORD S LIVKINS  From these inequalities and Lemma   we obtain ci  cj   radt  dK   Therefore  B  r   radt  dK  r   B ci cj   radt  dK  r   radt  dK  B cj  LP  LP   B   by Lemma    radt  dK    cj        r  r  B cj ci    r B   B    radt  dK   cj  cj       T radt  dK   T   LP B    LP  B      T  radt  dK            Remainder of the proof after Lemma    We start with Corollary     which we restate here for convenience  Corollary t   t   assuming that B     T  radt  dK   Proof Let    maxP F  It LP P     Note that   T   Then from Lemma    we obtain    OPTLP     B       T  radt  dK             T  radt  dK    Using      and Lemma    we get the desired bound  t    B       T  radt  dK        OPTLP       T  radt  dK       T  radt  dK   B       OPTLP      T  radt  dK     t      B In the remainder of this appendix  we prove the claims in Equation      one by one  Corollary    REWt  Tt  OPTLP  O t    for each round t     Proof From Lemma    we obtain T r Pt          q    LP Pt           q     OPTLP    t     OPTLP    t    Summing up and taking average over rounds  we obtain  Pt T r t  OPTLP     s   s  OPTLP  O t    t By definition of clean execution  we obtain   REWt  t r t  radt  rt      t T  OPTLP       O t                R ESOURCEFUL C ONTEXTUAL BANDITS  Corollary    Ct i  B T   O radt  dK   for each round t      Proof Let  be the  actual  expected outcomes tuple  and recall that Pt is LP optimal for some expected outcomes tuple   t   Then  by Lemma    it follows that ci  Pt      ci  Pt        radt  dK   Furthermore since Pt is LP optimal for  we have ci  Pt       B T   Therefore  ci  Pt       B T    B T  ci  Pt        radt  dK        q    ci  Pt       q    O radt  dK    B T  Now summing and taking average we obtain ct i  clean execution  it follows that Ct i  ct i   radt  ct i     B T    O radt  dK    Using the definition of      O radt  dK     Lemma    REWT  OPTLP  O T     Proof  Either    T or some resource i gets exhausted  in which case  using Corollary        B  C i  B T  B   rad  dK   B T    rad  dK   B  B T   T radT  dK   B   T    T B    radT  dK           Using this lower bound and Corollary     we obtain the desired bound on the total revenue REWT   REWT   REW      OPTLP  O      T   OPTLP      T B  radT  dK      OPTLP  T   O     T  O       T  In the above  the first inequality holds by Corollary     the second by Equation       and the third by definition of T   Finally  we note that   is an increasing function of    and substitute    T T      We complete the proof of Theorem   as follows  Re writing Lemma    as REWT  f  OPTLP    for an appropriate function f     note that REWT  f  OPT  because function f    is increasing      Lower bound  proof of Theorem   In fact  we prove a stronger theorem that implies Theorem     Theorem    Fix any tuple  K  T  B  such that K      T   and B  KT     Any algorithm for RCB incurs regret  OPT    in the worst case over all problem instances with K actions  time horizon T   smallest budget B  and policy sets  such that OPT    B       BADANIDIYURU L ANGFORD S LIVKINS  We will use the following lemma  which follows from simple probability arguments   Lemma    Consider two collections of n balls I  and I    each numbered from   to n  Let I  consists of all red balls  while I  consist of n    red balls and   green ball  with labels chosen uniformly at random   In this setting  let an algorithm is given access to random samples from one of Ii with replacement  The algorithm is allowed to first look at the balls number and then decide whether to inspect its color  Then any algorithm A which with probability at least    can distinguish between I  and I  must inspect color of at least n   balls in expectation  In the remainder of this section we prove Theorem     Let us define a family of problem instances as follows  Let the set of arms be  a    a            aK    There are T  B different contexts labelled  x         xT  B   and there is a uniform distribution over contexts  The policy set  consists of T  K     B policies i j   where    i  K and    j  T  B  Define them as follows  i j  xl     ai for l   j  and i j  xl     a  for l    j  There is just one resource constraint B  apart from time   Pulling arm a  always costs   and arm ai   i      always costs    Now consider the following problem instances    Let F  be the instance in which every arm always gives a reward    Note that OPT F           Let Fi j be the instance in which arm ai on context xj gives reward    otherwise every arm on every context gives reward    Note that in this case the optimal distribution over policies is just to follow i j and gets reward  B  Now consider any algorithm A and let the expected number of times it pulls arm ai be pi on input F    Let i   i      be the arm for which this is minimum  Then by simple linearity of expectation we get that B   K    pi   It is also simple to see that for the algorithm to get a regret better than  OPT  it should be able to distinguish between F  and Fi    at least with probability       B   Combining the two equations we get     From lemma    this can be done iff pi  T B   K    T    B   Solving for B we get B  KT         Discretization for contextual dynamic pricing  proof of Theorem    We consider contextual dynamic pricing with B copies of a single product  The action space consists of all prices p          We obtain regret bounds relative to an arbitrary policy set   Preliminaries  Let S p x  be the contextual sales rate  the probability of a sale for price p and context x  Note that S p x  is non increasing in p  for any given x  The assumption of Lipschitz demands is stated as follows   S p x   S p  x    L   p  p   for all contexts x         for some constant L called the Lipschitz constant  For simplicity  assume L     For a  possibly randomized  policy   define the contextual sales rate S  x    Ep x    S p x    and the absolute sales rate S     Ex   S  x     The latter is exactly the expected per round resource consumption for   Let r   be the expected per round reward for   As discussed in the Introduction  we define the discretization with step o as follows  For each price p  let fo  p  be p rounded down to the nearest multiple of o  i e  the largest price p  p such      R ESOURCEFUL C ONTEXTUAL BANDITS  that p  oN  For each policy  we define a discretized policy o   fo     The discretized policy set is then o    o        Note that for all policies  and all contexts x we have  x   o  x    x   o  By monotonicity of the sales rate and the Lipschitz assumption  resp   it follows that S  x   S o  x   S  x    oL  Consequently  S    S o    S     oL  Discretization error  The key technical step is to bound the discretization error of the discretized policy set o compared to the original policy set   as quantified by the difference in OPTLP     Our proof will use an intermediate policy class     S       where       First we bound the discretization error relative to    Lemma    OPTLP      OPTLP  o       o     L     B  for each o        Proof Using a trivial reduction to the non contextual case  when a policy corresponds to an action in the bandits with knapsacks problem   one can use a generic discretization result from Badanidiyuru et al       a   According to this result  specialized to contextual dynamic pricing   it suffices to prove that for each policy    the following two properties hold   P   S o    S     P   r o   S o    r   S    o     L     as long as S o        In words  the sales rate of the discretized policy o is at least the same  and the reward to consumption ratio is not much worse  Property  P   holds trivially because o    deterministically and for every context   and the contextual sales rate S p x  is decreasing in p for any fixed context x  r o     E   fo   x    S o  x    x    E     x   o   S o  x    x    E    x   S  x     o E   S o  x    x   x     r    o S o     r o   S o    r   S o    o  Now  by the Lipschitz assumption  S o    S     oL  so to complete the proof r   r   oL r   oL r o    o  o     o    S o   S     oL S    S    S        Now we bound the loss in OPTLP between  and    Lemma    OPTLP     OPTLP      T   for each       Proof If   B T   the statement is trivial because OPTLP     B  So w l o g  assume    B T   By Lemma    there exists an LP perfect distribution P over policies in   Recall that P is a mixture of  at most  two policies  say  and     and c P    B T   W l o g  assume S    S      If S     then         so OPTLP      OPTLP           BADANIDIYURU L ANGFORD S LIVKINS  The remaining case is S       Then S      B T     so       Therefore  OPTLP      LP P    LP     LP      LP     OPTLP      It remains to prove that LP    T   Indeed  r     E    x   S  x     E   S  x      S      x   x   LP     r   min T  B S     r   T  T      Putting Lemma    and Lemma    together and optimizing   we obtain  Lemma    For each o      letting      oBL T        we have OPTLP     OPTLP  o     T    oB  Plugging in the general result  Let REW    be the expected total reward when MixtureElimination is run with policy set  which uses only K distinct actions  Recall that we actually prove a somewhat stronger version of Theorem    the same regret bound      but with respect to OPTLP     rather than OPT     In our setting we have d     resource constraints  incl  time  and OPTLP      B  Therefore     p KT log  KT        REW     OPTLP      O Plugging in    o and K    o   and using Lemma     we obtain     q   T T REW o    OPTLP     O oB   T   o log o  o            for each o     and      oBL T        We obtain Theorem   choosing o    BL     T      log T  o        and noting  o             Conclusions and open questions We define a very general setting for contextual bandits with resource constraints  denoted RCB   We design an algorithm for this problem  and derive a regretpbound which achieves the optimal root T scaling in terms of the time horizon T   and the optimal log    scaling in terms of the policy set   Further  we consider discretization issues  and derive a specific corollary for contextual dynamic pricing with a single product  we obtain a regret bound that applies to an arbitrary policy set   Finally  we derive a partial lower bound which establishes a stark difference from the non contextual version  These results set the stage for further study of RCB  as discussed below  The main question left open by this work is to combine provable regret bounds and a computationally efficient  CE  implementation  While we focused on the statistical properties  we believe our techniques are unlikely to lead to CE implementations  Achieving near optimal regret bounds in a CE way has been a major open question for contextual bandits with policy sets  without resource constraints   This question has been resolved in the positive in a simultaneous and independent work  Agarwal et al          Very recently  a follow up paper  Agrawal et al         has      R ESOURCEFUL C ONTEXTUAL BANDITS  achieved the corresponding advance on RCB  by combing the techniques from Agarwal et al         and Agrawal and Devanur         which  in turn  builds on Badanidiyuru et al       a    Computational issues aside  several open questions concern our regret bounds  First  it is desirable to achieve the same regret bounds without assuming a known time horizon T  as it is in most bandit problems in the literature   This may be difficult because time is one of the resource constraints in our problem  and our techniques rely on knowing all resource constraints in advance  More generally  one can consider a version of RCB in which some of the resource constraints are not fully revealed to an algorithm  instead  the algorithm receives updated estimates of these constrains over time  Second  while our main regret bound in Theorem   is optimal in the important regime when OPT   and B are at least a constant fraction of T   it is not tight for some other regimes  For a concrete comparison  consider problem instances with a constant number of resources  d   a constant number  of actions  K   and OPT     B   Then  ignoring logarithmic factors  weobtain regret OPT   T  B  whereas the lower bound in Badanidiyuru et al       a  is OPT    B  So there is a gap when B  T   Likewise  for contextual dynamic pricing with a single product  there is a gap between our algorithmic result  Theorem    and the B     lower bound for the non contextual case from Babaioff et al          In both cases  both upper and lower bounds can potentially be improved  Third  for special cases when actions correspond to prices one would like to extend the discretization approach beyond contextual dynamic pricing with a single product  However  this is problematic even without contexts  essentially  nothing is known whenever one has multiple resource constraints  and even with a single resource constraint  besides time  the solutions are very non trivial  see Badanidiyuru et al       a  for more discussion  Fourth  if there are no contexts or resource constraints then one can achieve O log T   regret with an instance dependent constant  it is not clear whether one can meaningfully extend this result to contextual bandits with resource constraints  The model of RCB can be extended in several directions  two of which we outline below  The most immediate extension is to an unknown distribution of context arrivals  This extension has been addressed  among other results  in the follow up paper  Agrawal et al          The most important extension  in our opinion  would be from a stationary environment to one controlled by an adversary  perhaps restricted in some natural way   We are not aware of any prior work in this direction  even for the non contextual version   
 We present a new algorithm for the contextual bandit learning problem  where the learner repeatedly takes one of K actions in response to the observed context  and observes the reward only for that chosen action  Our method assumes access to an oracle for solving fully supervised cost sensitive clasp sification problems and achieves the statistically optimal regret guarantee with only O  KT   log N   oracle calls across all T rounds  where N is the number of policies in the policy class we compete against  By doing so  we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes  We further conduct a proof of concept experiment which demonstrates the excellent computational and prediction performance of  an online variant of  our algorithm relative to several baselines      Introduction  In the contextual bandit problem  an agent collects rewards for actions taken over a sequence of rounds  in each round  the agent chooses an action to take on the basis of  i  context  or features  for the current round  as well as  ii  feedback  in the form of rewards  obtained in previous rounds  The feedback is incomplete  in any given round  the agent observes the reward only for the chosen action  the agent does not observe the reward for other actions  Contextual bandit problems are found in many important applications such as online recommendation and clinical trials  and represent a natural half way point between supervised learning and reinforcement learning  The use of features to encode context is inherited from supervised machine learning  while exploration is necessary for good performance as in reinforcement learning  The choice of exploration distribution on actions is important  The strongest known results  Auer et al         McMahan and Streeter        Beygelzimer et al         provide algorithms that carefully control the exploration distribution to achieve an optimal regret after T rounds of    p KT log        O  with probability at least      relative to a set of policies   AX mapping contexts x  X to actions a  A  where K is the number of actions   The regret is the difference between the cumulative reward of the best policy in  and the cumulative reward collected by the algorithm  Because the bound has a mild logarithmic dependence on     the algorithm can compete with very large policy classes that are likely     to yield high rewards  in which case the algorithm also earns high rewards  However  the computational complexity of the above algorithms is linear in     making them tractable for only simple policy classes  A sub linear in    running time is possible for policy classes that can be efficiently searched  In this work  we use the abstraction of an optimization oracle to capture this property  given a set of context reward vector pairs  the oracle returns a policy in  with maximum total reward  Using such an oracle in an i i d  setting  formally defined in Section       it is possible to create o greedy  Sutton and Barto        or epoch greedy  Langford and Zhang        algorithms that run in time O log     with only a single call to the oracle per round  However  these algorithms have suboptimal regret bounds of O  K log        T       because the algorithms randomize uniformly over actions when they choose to explore  The Randomized UCB algorithm of Dudk et al       a  achieves the optimal regret bound  up to logarithmic factors  in the i i d  setting  and runs in time poly T  log     with O T     calls to the optimization oracle per round  Naively this would amount to O T     calls to the oracle over T rounds  although a doubling trick from our analysis can be adapted to ensure only O T     calls to the oracle are needed over all T rounds in the Randomized UCB algorithm  This is a fascinating result because it shows that the oracle can provide an exponential speed up over previous algorithms with optimal regret bounds  However  the running time of this algorithm is still prohibitive for most natural problems owing to the O T     scaling  In this work  we prove the following    Theorem    an algorithm for the i i d  contextual bandit problem with an optimal regret bound  qThere is   KT requiring O calls to the optimization oracle over T rounds  with probability at least      ln       p p Concretely  we make O  KT   ln       calls to the oracle with a net running time of O T     K log      vastly improving over the complexity of Randomized UCB  The major components of the new algorithm are  i  a new coordinate descent procedure for computing a very sparse distribution over policies which can be efficiently sampled from  and  ii  a new epoch structure which allows the distribution over policies to be updated very infrequently  We consider variants of the epoch structure that make different computational trade offs  on one extreme we concentrate the entire computational burden on O log T   p KT   ln       oracle calls each time  while on the other we spread our computation rounds with O  p  over T rounds with O  K  ln       oracle calls for each of these rounds  We stress that in either case  the total number of calls to the oracle is only sublinear in T   Finally  we develop a more efficient online variant  and conduct a proof of concept experiment showing low computational complexity and high reward relative to several natural baselines  Motivation and related work  The EXP  family of algorithms  Auer et al         McMahan and Streeter        Beygelzimer et al         solve the contextual bandit problem with optimal regret by updating weights  multiplicatively  over all policies in every round  Except for a few special cases  Helmbold and Schapire        Beygelzimer et al          the running time of such measure based algorithms is generally linear in the number of policies  In contrast  the Randomized UCB algorithm of Dudk et al       a  is based on a natural abstraction from supervised learningthe ability to efficiently find a function in a rich function class that minimizes the loss on a training set  This abstraction is encapsulated in the notion of an optimization oracle  which is also useful for o greedy  Sutton and Barto        and epoch greedy  Langford and Zhang        algorithms  However  these latter algorithms have only suboptimal regret bounds  Another class of approaches based on Bayesian updating is Thompson sampling  Thompson        Li         which often enjoys strong theoretical guarantees in expectation over the prior and good empirical performance  Chapelle and Li         Such algorithms  as well as the closely related upper confidence bound algorithms  Auer        Chu et al          are computationally tractable in cases where the posterior distribution over policies can be efficiently maintained or approximated  In our experiments  we compare to a strong baseline algorithm that uses this approach  Chu et al            Throughout this paper  we use the O notation to suppress dependence on logarithmic factors in T and K  as well as log       i e  terms which are O log log             To circumvent the      running time barrier  we restrict attention to algorithms that only access the policy class via the optimization oracle  Specifically  we use a cost sensitive classification oracle  and a key challenge is to design good supervised learning problems for querying this oracle  The Randomized UCB algorithm of Dudk et al       a  uses a similar oracle to construct a distribution over policies that solves a certain convex program  However  the number of oracle calls in their work is prohibitively large  and the statistical analysis is also rather complex   Main contributions  In this work  we present a new and simple algorithm for solving a similar convex program as that used by Randomized UCB  The new algorithm is based on coordinate descent  in each iteration  the algorithm calls the optimization oracle to obtain a policy  the output is a sparse distribution over p these policies  The number of iterations required to compute the distribution is smallat most O  Kt  ln       in any round t  In fact  we present a more general scheme based on p epochs and warm start in which the total number of calls to the oracle is  with high probability  just O  KT   ln       over all T rounds  we prove that this is nearly optimal for a certain class of optimization based algorithms  The algorithm is natural and simple to implement  and we provide an arguably simpler analysis than that for Randomized UCB  Finally  we report proof of concept experimental results using a variant algorithm showing strong empirical performance      Preliminaries  In this section  we recall the i i d  contextual bandit setting and some basic techniques used in previous works  Auer et al         Beygelzimer et al         Dudk et al       a         Learning Setting  Let A be a finite set of K actions  X be a space of possible contexts  e g   a feature space   and   AX be a finite P set of policies that map contexts x  X to actions a  A   Let      Q  R   Q           Q       be the set of non negative weights over policies with total weight at most A one  and let RA       r  R   r a     a  A  be the set of non negative reward vectors  Let D be a probability distribution over X        A   the joint space of contexts and reward vectors  we assume actions rewards from D are always in the interval         Let DX denote the marginal distribution of D over X  In the i i d  contextual bandit setting  the context reward vector pairs  xt   rt    X        A over all rounds t               are randomly drawn independently from D  In round t  the agent first observes the context xt   then  randomly  chooses an action at  A  and finally receives the reward rt  at           for the chosen action  The  observable  record of interaction resulting from round t is the quadruple  xt   at   rt  at    pt  at     X  A                  here  pt  at           is the probability that the agent chose action at  A  We let Ht  X  A                 denote the history  set  of interaction records in the b xH    to denote expectation when a context x is chosen first t rounds  We use the shorthand notation E t from the t contexts in Ht uniformly at random  Let R      E x r D  r  x    denote the expected  instantaneous  reward of a policy     and let     arg max R   be a policy that maximizes the expected reward  the optimal policy   Let Reg      R     R   denote the expected  instantaneous  regret of a policy    relative to the optimal policy  Finally  the  empirical cumulative  regret of the agent after T rounds  is defined as T X t      The    rt    xt     rt  at      paper of Dudk et al       a  is colloquially referred to  by its authors  as the monster paper  Langford         to VC classes is simple using standard arguments    We have defined empirical cumulative regret as being relative to    rather than to the empirical reward maximizer  p PT arg max t   rt   xt     However  in the i i d  setting  the two do not differ by more than O  T ln       with probability at least        Extension           Inverse Propensity Scoring  An unbiased estimate of a policys reward may be obtained from a history of interaction records Ht using inverse propensity scoring  IPS  also called inverse probability weighting   the expected reward of policy    is estimated as t X ri  ai       xi     ai   b t         R       t i   pi  ai    This technique can be viewed as mapping Ht   IPS Ht   of interaction records  x  a  r a   p a   to context reward vector pairs  x  r   where r  RA   is a fictitious reward vector that assigns to the chosen action a a scaled reward r a  p a   possibly greater than one   and assigns to all other actions zero rewards  This transformation IPS Ht   is detailed in Algorithm    in Appendix A   we may equivalently b t by R b t       t  P define R  x r IPS Ht   r  x    It is easy to verify that E r  x    x  r     r  x    as b t    is p a  is indeed the agents probability  conditioned on  x  r   of picking action a  This implies R an unbiased estimator for any history Ht   b t    denote a policy that maximizes the expected reward estimate based Let t    arg max R d t       R b t  t    R b t    on inverse propensity scoring with history Ht    can be arbitrary   and let Reg d denote estimated regret relative to t   Note that Regt    is generally not an unbiased estimate of Reg    because t is not always          Optimization Oracle  One natural mode for accessing the set of policies  is enumeration  but this is impractical in general  In this work  we instead only access  via an optimization oracle which corresponds to a cost sensitive learner  Following Dudk et al       a   we call this oracle AMO    Definition    For a set of policies   the arg max oracle  AMO  is an algorithm  which for any sequence of context and reward vectors   x    r      x    r              xt   rt    X  RA     returns arg max        t X  r   x           Projections and Smoothing  In each round  our algorithm chooses an action by randomly drawing a policy  from a distribution over   and then picking the action  x  recommended by  on the current context x  This is equivalent P to drawing an action according to Q a x       x  a Q    a  A  For keeping the variance of reward estimates from IPS in check  it is desirable to prevent the probability of any action from being  too small  Thus  as in P previous work  we also use a smoothed projection Q   x  for         K    Q  a x         K    x  a Q       a  A  Every action has probability at least  under Q   x   For technical reasons  our algorithm maintains non negative weights Q   over policies that sum to at most one  but not necessarily equal to one  hence  we put any remaining mass   policy Pon a default    to obtain a legitimate probability distribution over policies Q   Q       Q       We then pick an action from the smoothed projection Q   x  of Q as above  This sampling procedure Sample x  Q      is detailed in Algorithm    in Appendix A       Algorithm and Main Results  Our algorithm  ILOVETOCONBANDITS  is an epoch based variant of the Randomized UCB algorithm of Dudk et al       a  and is given in Algorithm    Like Randomized UCB  ILOVETOCONBANDITS solves   Cost sensitive  learners often need a cost instead of reward  in which case we use ct      rt        an optimization problem  OP  to obtain a distribution over policies to sample from  Step     but does so on an epoch schedule  i e   only on certain pre specified rounds                The only requirement of the epoch schedule is that the length of epoch m is bounded as m    m   O m    For simplicity  we assume m     m for m     and     O     The crucial step here is solving  OP   Before stating the main result  let us get some intuition about this problem  The first constraint  Eq       requires the average estimated regret of the distribution Q over policies to be small  since b is a rescaled version of the estimated regret of policy   This constraint skews our distribution to put more mass on good policies  as judged by our current information   and can be seen as the exploitation component of our algorithm  The second set of constraints  Eq       requires the distribution Q to place sufficient mass on the actions chosen by each policy   in expectation over contexts  This can be thought of as the exploration constraint  since it requires the distribution to be sufficiently diverse for most contexts  As we will see later  the left hand side of the constraint is a bound on the variance of our reward estimates for policy   and the constraint requires the variance to be controlled at the level of the estimated regret of   That is  we require the reward estimates to be more accurate for good policies than we do for bad ones  allowing for much more adaptive exploration than the uniform exploration of o greedy style algorithms  This problem is very similar to the one in Dudk et al       a   and our coordinate descent algorithm in Section     gives a constructive proof that the problem is feasible  As in Dudk et al       a   we have the following regret bound  Theorem    Assume the optimization problem   OP  can be solved whenever required in Algorithm    With probability at least      the regret of Algorithm    ILOVETOCONBANDITS  after T rounds is    p KT ln T        K ln T        O Algorithm   Importance weighted LOw Variance Epoch Timed Oracleized CONtextual BANDITS algorithm  ILOVETOCONBANDITS  input Epoch schedule                      allowed failure probability              Initial weights Q       p   initial epoch m               K    for all m     Define m    min    K   ln   m m    for round t               do    Observe context xt  X      at   pt  at       Sample xt   Qm    m     m        Select action at and observe reward rt  at               if t   m then    Let Qm be a solution to  OP  with history Ht and minimum probability m      m    m         end if     end for  Optimization Problem  OP  d   t    Given a history Ht and minimum probability m   define b    Reg m for          and find Q   such that X Q  b   K      b xH      E t          m Q   x  x           K   b               Solving  OP  via Coordinate Descent  We now present a coordinate descent algorithm to solve  OP   The pseudocode is given in Algorithm    Our analysis  as well as the algorithm itself  are based on a potential function which we use to measure progress  The algorithm can be viewed as a form of coordinate descent applied to this same potential function  The main idea of our analysis is to show that this function decreases substantially on every iteration of this algorithm  since the function is nonnegative  this gives an upper bound on the total number of iterations as expressed in the following theorem  Theorem    Algorithm    with Qinit       halts in at most solution Q to   OP      ln    Km    m  iterations  and outputs a  Algorithm   Coordinate Descent Algorithm Require  History Ht   minimum probability   initial weights Qinit        Set Q    Qinit      loop    Define  for all     V  Q  S  Q  D  Q  if                                   P  b xH    Q   x  x     E t     b xHt    Q   x  x      E   V  Q     K   b      Q    K   b      K then Replace Q by cQ  where  c    P   K      Q    K   b          end if if there is a policy  for which D  Q      then Add the  positive  quantity V  Q    D  Q    Q         K S  Q   to Q   and leave all other weights unchanged  else Halt and output the current set of weights Q  end if end loop       Using an Optimization Oracle  We now show how to implement Algorithm   via AMO  c f  Section       Lemma    Algorithm   can be implemented using one call to AMO before the loop is started  and one call for each iteration of the loop thereafter  Proof  At the very beginning  before the loop is started  we compute the best empirical policy so far  t   by calling AMO on the sequence of historical contexts and estimated reward vectors  i e   on  x   r    for                  t  Next  we show that each iteration in the loop of Algorithm   can be implemented via one call to AMO  Going over the pseudocode  first note that operations involving Q in Step   can be performed efficiently since Q has sparse support  Note that the definitions in Step   dont actually need to be computed for all policies     as long as we can identify a policy  for which D  Q       We can identify such a policy using one call to AMO as follows      First  note that for any policy   we have b xHt V  Q    E  and  b         t      X     Q   x  x  t     Q   x   x    t d t    b t  t   Reg R   X    r   x       t      Now consider the sequence of historical contexts and reward vectors   x   r   for                  t  where for any action a we define          r  a        r  a     t Q  a x   It is easy to check that t  Since  K    b t  t   R     X D  Q    r   x           b t  t   R  K           is a constant independent of   we have arg max D  Q    arg max     t X  r   x           and hence  calling AMO once on the sequence  x   r   for                  t  we obtain a policy that maximizes D  Q   and thereby identify a policy for which D  Q      whenever one exists        Epoch Schedule  Recalling the setting of m in Algorithm    Theorem   shows that Algorithm   solves  OP  with p if we use the epoch schedule m   m  i e   run O  Kt  ln       calls to AMO in round t  Thus  p Algorithm   in every round   then we get a total of O  KT    ln       calls to AMO over all T rounds  This number can be dramatically reduced using a more carefully chosen epoch schedule  p Lemma    For the epoch schedule m     m    the total number of calls to AMO is O  KT   ln        Proof  The epoch schedule satisfies the requirement m   p   m   With this epoch schedule  Algorithm   is run only O log T   times over T rounds  leading to O  KT   ln       total calls to AMO over the entire period        Warm Start  We now present a different technique to reduce the number of calls to AMO  This is based on the observation that practically speaking  it seems terribly wasteful  at the start of a new epoch  to throw out the results of all of the preceding computations and to begin yet again from nothing  Instead  intuitively  we expect computations to be more moderate if we begin again where we left off last  i e   a warm start approach  Here  when Algorithm   is called at the end of epoch m  we use Qinit    Qm   the previously computed weights  rather than    p We can combine warm startwith a different epoch schedule to guarantee O  KT   ln       total calls to AMO  spread across O  T   calls to Algorithm    Lemma    Define the epoch schedule                    and m    m  for m p   this satisfies m     m    With high probability the warm start variant of Algorithm   makes O  KT   ln       calls to AMO over T rounds and O  T   calls to Algorithm              Computational Complexity  So far  we have only considered computational complexity in terms of the number of oracle calls  However  the reduction also involves the creation of cost sensitive classification examples  which must be accounted for in the net computational cost  As observed in the proof of Lemma    specifically Eq        this requires the computation of the probabilities Q  a x   for                  t when the oracle has to be invoked at round p t  According to Lemma    the support of the distribution Q at time t can be over at most O  Kt  ln       p policies  same as the number of calls to AMO   This would suggest a computational complexity of O  Kt    ln       for querying the oracle at time t  resulting in an overall computation cost scaling with T     We can  however  do better with some natural bookkeeping  Observe that at the start of round t  the conditional distributions Q a xi   for i                 t    can be represented as a table of size K   t      where rows and columns correspond to actions and contexts  Upon receiving the new example in round p t  the corresponding t th column can be added to this table in time K   supp Q     O K Kt  ln        where supp Q    denotes the support of Q   using the projection operation described in Section      Hence the net cost of these updates  as a function of K and T   scales with as  KT        Furthermore  the cost sensitive examples needed for the AMO can be obtained by a simple table lookup now  since the action probabilities are directly available  This involves O Kt  table lookups when the oracle is invoked at time t  and again results in an overall cost scaling as  KT        Finally  we have to update the table when the distribution Q is updated in Algorithm    If we find ourselves in the rescaling step    we can simply store the constant c  When we enter step   of the algorithm  we can do a linear scan over the table  rescaling and incrementing the entries  This also resutls in a cost of O Kt  when the update happens at time t  resulting in a net scaling as  KT        Overall  p we find that the computational complexity of our algorithm  modulo the oracle running time  is O   KT      ln              A Lower Bound on the Support Size  An attractive feature of the coordinate descent algorithm  Algorithm    is that the number of oracle calls is directly related to the number of policies in the support of Qm   Specifically  for the doubling schedule m    of Section      Theorem   implies that we never have non zero weights for more than   ln    K policies m in epoch m  Similarly  the total number of oracle calls for the warm start approach in Section     bounds the total number of policies which ever have non zero weight over all T rounds  The support size of the distributions Qm in Algorithm   is crucial to the computational complexity of sampling an action  Step   of Algorithm     In this section  we demonstrate a lower bound showing that it is not possible to construct substantially sparser distributions that also satisfy the low variance constraint     in the optimization problem  OP   To formally define the lower bound  fix an epoch schedule                    and consider the following set of non negative vectors over policies  Qm    Q     Q satisfies Eq      in round m     The distribution Qm computed by Algorithm   is in Qm    Recall that supp Q  denotes the support of Q  the set of policies where Q puts non zero entries   We have the following lower bound on  supp Q    Theorem    For any epoch schedule                    and any M  N sufficiently large  there exists a distribution D over X        A and a policy class  such that  with probability at least      s   KM inf inf  supp Q        mN  QQm ln   M    m M     The proof of the theorem is deferred to Appendix E  In the context of our problem  this lower bound shows that the bounds in Lemma   and Lemma   are unimprovable  since the number of calls to AMO is at least the size of the support  given our mode of access to          Regret Analysis  In this section  we outline the regret analysis for our algorithm ILOVETOCONBANDITS  with details deferred to Appendix B and Appendix C  b t    are controlled by  a bound on  the variance of The deviations of the policy reward estimates R b xH    replaced by each term in Eq       essentially the left hand side of Eq      from  OP   except with E t ExDX     Resolving this discrepancy is handled using deviation bounds  so Eq      holds with ExDX     with worse right hand side constants  The rest of the analysis  which deviates from that of Randomized UCB  compares the expected regret d t    using the variance constraints Eq       Reg   of any policy  with the estimated regret Reg Lemma    Informally   With high probability  for each m such that m  O K log      each round t in d t      O Km    epoch m  and each     Reg     Reg  This lemma can easily be combined with the constraint Eq      from  OP   since the weights Qm  P d used in any round t in epoch m satisfy  Qm    Reg m          Km     we obtain a bound on the  conditionally  expected regret in round t using the above lemma  with high probability  X e m  Reg    O Km     Q   Summing these terms up over all T rounds and applying martingale concentration gives the final regret bound in Theorem        Analysis of the Optimization Algorithm  In this section  we give a sketch of the analysis of our main optimization algorithm for computing weights Qm on each epoch as in Algorithm    As mentioned in Section      this analysis is based on a potential function  Since our attention for now is on a single epoch m  here and in what follows  when clear from context  we drop m from our notation and write simply    m      m   etc  Let UA be the uniform distribution over the action set A  We define the following potential function for use on epoch m    P b x  RE  UA kQ     x    E  Q  b m  Q                 K  K The function in Eq      is defined for all vectors Q     Also  RE  pkq  denotes the unnormalized relative entropy between two nonnegative vectors p and q over the action space  or any set  A  X RE  pkq     pa ln pa  qa     qa  pa    aA  This number is always nonnegative  Here  Q   x  denotes the distribution  which might not sum to    over A induced by Q for context x as given in Section      Thus  ignoring constants  this potential function is a combination of two terms  The first measures how far from uniform are the distributions induced by Q   and the second is an estimate of expected regret under Q since b is proportional to the empirical regret of   Making m small thus encourages Q to choose actions as uniformly as possible while also incurring low regret  exactly the aims of our algorithm  The constants that appear in this definition are for later mathematical convenience  For further intuition  note that  by straightforward calculus  the partial derivative m  Q   is roughly proportional to the variance constraint for  given in Eq       up to a slight mismatch of constants   This shows that if this constraint is not satisfied  then m  Q   is likely to be negative  meaning that m can be decreased by increasing Q    Thus  the weight vector Q that minimizes m satisfies the variance constraint for every policy   It turns out that this minimizing Q also satisfies the     low regret constraint in Eq       and also must sum to at most    in other words  it provides a complete solution to our optimization problem  Algorithm   does not fully minimize m   but it is based roughly on coordinate descent  This is because in each iteration one of the weights  coordinate directions  Q   is increased  This weight is one whose corresponding partial derivative is large and negative  To analyze the algorithm  we first argue that it is correct in the sense of satisfying the required constraints  provided that it halts  Lemma    If Algorithm   halts and outputs a weight vector Q  then the constraints Eq      and Eq      must hold  and furthermore the sum of the weights Q   is at most    The proof is rather straightforward  Following Step    Eq      must hold  and also the weights must sum to    And if the algorithm halts  then D  Q     for all   which is equivalent to Eq       What remains is the more challenging task of bounding the number of iterations until the algorithm does halt  We do this by showing that significant progress is made in reducing m on every iteration  To begin  we show that scaling Q as in Step   cannot cause m to increase  P Lemma    Let Q be a weight vector such that  Q    K   b      K  and let c be as in Eq       Then m  cQ   m  Q   Proof sketch  We consider m  cQ  as a function of c  and argue that its derivative  with respect to c  at the value of c given in the lemma statement is always nonnegative  Therefore  by convexity  it is nondecreasing for all values exceeding c  Since c      this proves the lemma  Next  we show that substantial progress will be made in reducing m each time that Step   is executed  Lemma    Let Q denote a set of weights and suppose  for some policy   that D  Q       Let Q be a new set of weights which is an exact copy of Q except that Q      Q      where      Q       Then          m  Q   m  Q         K  Proof sketch  We first compute exactly the change in potential for general   Next  we apply a secondorder Taylor approximation  which is maximized by the  used in the algorithm  The Taylor approximation  for this   yields a lower bound which can be further simplified using the fact that Q  a x    always  and our assumption that D  Q       This gives the bound stated in the lemma  So Step   does not cause m to increase  and Step   causes m to decrease by at least the amount given in Lemma    This immediately implies Theorem    for Qinit      the initial potential is bounded by   ln    K       K   and it is never negative  so the number of times Step   is executed is bounded by   ln    K    as required        Epoching and Warm Start  As shown in Section      the bound on the number of iterations of the algorithm from Theorem   also gives a bound on the number of times the oracle is called  To reduce the number of oracle calls  one approach is the doubling trick of Section      which enables p us to bound the total combined number of iterations of Algorithm   in the first T rounds is p only O  KT   ln        This means that the average number of calls to the arg max oracle is only O  K  T ln        per round  meaning that the oracle is called far less than once per round  and in fact  at a vanishingly low rate  We now turn to warm start approach of Section      where in each epoch m     we initialize the coordinate descent algorithm with Qinit   Qm   i e  the weights computed in the previous epoch m  To analyze this  we bound how much the potential changes from m  Qm   at the end of epoch m to m    Qm   at the very start of epoch m      This  combined with our earlier results regarding how quickly Algorithm   drives down the potential  we are able to get an overall bound on the total number of updates across T rounds        Table    Progressive validation loss  best hyperparameter values  and running times of various algorithm on RCV   Algorithm P V  Loss Searched Seconds  o greedy             o     Explore first              first      Bagging          bags      LinUCB           dim  minibatch              Online Cover       cover n         Supervised       nothing      Lemma    Let M be the largest integer for which M    T   With probability at least       for all T   the total epoch to epoch increase in potential is   r M X T ln       m    Qm    m  Qm     O   K m   where M is the largest integer for which M    T   Proof sketch  The potential function  as written in Eq       naturally breaks into two pieces whose epoch to epoch changes can be bounded separately  Changes affecting the relative entropy term on the left can be bounded  regardless of Qm   by taking advantage of the manner in which these distributions are smoothed  For the other term on the right  it turns out that these epoch to epoch changes are related to statistical quantities which can be bounded with high probability  Specifically  the total change in this term is related first to how the estimated reward of the empirically best policy compares to the expected reward of the optimal policy  and second  to how the reward received by our algorithm compares to that of the optimal reward  From our regret analysis  we are able to show that both of these quantities will be small with high probability  This lemma  along with Lemma   can be used to further establish Lemma    We only provide an intuitive sketch here  with the details deferred to the appendix  As we p observe in Lemma    the total amount that the potential increases across T rounds is at most O  T ln      K   On the other hand  Lemma   shows that each time Q is updated by Algorithm   the potential decreases by at least  ln      K   using our choice p of    Therefore  the total number of updates of the algorithm totaled  over all T rounds is at most O  KT   ln        For instance   if we use                    and m    m T times in T rounds  and on each of those for m     then the weight vector Q is only updated about p rounds  Algorithm   requires O  K  ln       iterations  on average  giving the claim in Lemma        Experimental Evaluation  In this section we evaluate a variant of Algorithm   against several baselines  While Algorithm   is significantly more efficient than many previous approaches  the overall computational complexity is still at least O  KT        plus the total cost of the oracle calls  as discussed in Section      This is markedly larger than the complexity of an ordinary supervised learning problem where it is typically possible to perform an O    complexity update upon receiving a fresh example using online algorithms  A natural solution is to use an online oracle that is stateful and accepts examples one by one  An online cost sensitive classification  CSC  oracle takes as input a weighted example and returns a predicted class  corresponding to one of K actions in our setting   Since the oracle is stateful  it remembers and uses examples from all previous calls in answering questions  thereby reducing the complexity of each oracle invocation to O    as in supervised learning  Using several such oracles  we can efficiently track a distribution over good policies and sample from it  We detail this approach  which we call Online Cover  in the full version of the paper  The algorithm maintains a uniform distribution over a fixed number n of policies where n is a parameter of the algorithm  Upon receiving a fresh example  it updates all n policies with the suitable CSC examples  Eq        The specific CSC oracle we use is a reduction to      squared loss regression  Algorithms   and   of Beygelzimer and Langford         which is amenable to online updates  Our implementation is included in Vowpal Wabbit   Due to lack of public datasets for contextual bandit problems  we use a simple supervised to contextualbandit transformation  Dudk et al       b  on the CCAT document classification problem in RCV   Lewis et al          This dataset has        examples and       TF IDF features  We treated the class labels as actions  and one minus     loss as the reward  Our evaluation criteria is progressive validation  Blum et al         on     loss  We compare several baseline algorithms to Online Cover  all algorithms take advantage of linear representations which are known to work well on this dataset  For each algorithm  we report the result for the best parameter settings  shown in Table        o greedy  Sutton and Barto        explores randomly with probability o and otherwise exploits     Explore first is a variant that begins with uniform exploration  then switches to an exploit only phase     A less common but powerful baseline is based on bagging  multiple predictors  policies  are trained with examples sampled with replacement  Given a context  these predictors yield a distribution over actions from which we can sample     LinUCB  Auer        Chu et al         has been quite effective in past evaluations  Li et al         Chapelle and Li         It is impractical to run as is due to high dimensional matrix inversions  so we report results for this algorithm after reducing to      dimensions via random projections  Still  the algorithm required    hours    An alternative is to use diagonal approximation to the covariance  which runs substantially faster    hour   but gives a worse error of           Finally  our algorithm achieves the best loss of         Somewhat surprisingly  the minimum occurs for us with a cover set of size  apparently for this problem the small decaying amount of uniform random sampling imposed is adequate exploration  Prediction performance is similar with a larger cover set  All baselines except for LinUCB are implemented as a simple modification of Vowpal Wabbit  All reported results use default parameters where not otherwise specified  The contextual bandit learning algorithms all use a doubly robust reward estimator instead of the importance weighted estimators used in our analysis Dudk et al       b   Because RCV  is actually a fully supervised dataset  we can apply a fully supervised online multiclass algorithm to solve it  We use a simple one against all implementation to reduce this to binary classification  yielding an error rate of       which is competitive with the best previously reported results  This is effectively a lower bound on the loss we can hope to achieve with algorithms using only partial information  Our algorithm is less than     times slower and nearly achieves the bound  Hence on this dataset  very little further algorithmic improvement is possible      Conclusions  In this paper we have presented the first practical algorithm to our knowledge that attains the statistically optimal regret guarantee and is computationally efficient in the setting of general policy classes  A remarkable feature of the algorithm is that the total number of oracle calls over all T rounds is sublinear a remarkable improvement over previous works in this setting  We believe that the online variant of the approach which we implemented in our experiments has the right practical flavor for a scalable solution to the contextual bandit problem  In future work  it would be interesting to directly analyze the Online Cover algorithm    http   hunch net    The    vw  The implementation is in the file cbify cc and is enabled using   cover  linear algebra routines are based on Intel MKL package        Acknowledgements We thank Dean Foster and Matus Telgarsky for helpful discussions  Part of this work was completed while DH and RES were visiting Microsoft Research   
 We study the problem of multiclass classification with an extremely large number of classes  k   with the goal of obtaining train and test time complexity logarithmic in the number of classes  We develop top down tree construction approaches for constructing logarithmic depth trees  On the theoretical front  we formulate a new objective function  which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure  in terms of class labels  and balanced  We demonstrate that under favorable conditions  we can construct logarithmic depth trees that have leaves with low label entropy  However  the objective function at the nodes is challenging to optimize computationally  We address the empirical problem with a new online decision tree construction procedure  Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches  which makes it a plausible method in computationally constrained large k applications      Introduction  The central problem of this paper is computational complexity in a setting where the number of classes k for multiclass prediction is very large  Such problems occur in natural language  Which translation is best    search  What result is best    and detection  Who is that   tasks  Almost all machine learning algorithms  with the exception of decision trees  have running times for multiclass classification which are O k  with a canonical example being one against all classifiers      In this setting  the most efficient possible accurate approach is given by information theory      In essence  any multiclass classification algorithm must uniquely specify the bits of all labels that it predicts correctly on  Consequently  Krafts inequality      equation      implies that the expected computational complexity of predicting correctly is  H Y    per example where H Y   is the Shannon entropy of the label  For the worst case distribution on k classes  this implies  log k   computation is required  Hence  our goal is achieving O log k   computational time per example  for both training and testing  while effectively using online learning algorithms to minimize passes over the data  The goal of logarithmic  in k  complexity naturally motivates approaches that construct a logarithmic depth hierarchy over the labels  with one label per leaf  While this hierarchy is sometimes available through prior knowledge  in many scenarios it needs to be learned as well  This naturally leads to a partition problem which arises at each node in the hierarchy  The partition problem is finding a classifier  c   X         which divides examples into two subsets with a purer set of labels than the original set  Definitions of purity vary  but canonical examples are the number of labels remaining in each subset  or softer notions such as the average Shannon entropy of the class labels  Despite resulting in a classifier  this problem is fundamentally different from standard binary classification  To see this  note that replacing c x  with c x  is very bad for binary classification  but has no impact on the quality of a partition    The partition problem is fundamentally non convex      Throughout the paper by logarithmic time we mean logarithmic time per example  The problem bears parallels to clustering in this regard       for symmetric classes since the average c x c x  of c x  and c x  is a poor partition  the always     function places all points on the same side   The choice of partition matters in problem dependent ways  For example  consider examples on a line with label i at position i and threshold classifiers  In this case  trying to partition class labels        from class label   results in poor performance   accuracy  The partition problem is typically solved for decision tree learning via an enumerate and test approach amongst a small set of possible classifiers  see e g        In the multiclass setting  it is desirable to achieve substantial error reduction for each node in the tree which motivates using a richer set of classifiers in the nodes to minimize the number of nodes  and thereby decrease the computational complexity  The main theoretical contribution of this work is to establish a boosting algorithm for learning trees with O k  nodes and O log k  depth  thereby addressing the goal of logarithmic time train and test complexity  Our main theoretical result  presented in Section      generalizes a binary boosting by decision tree theorem     to multiclass boosting  As in all boosting results  performance is critically dependent on the quality of the weak learner  supporting intuition that we need sufficiently rich partitioners at nodes  The approach uses a new objective for decision tree learning  which we optimize at each node of the tree  The objective and its theoretical properties are presented in Section    A complete system with multiple partitions LOMtree vs oneagainstall could be constructed top down  as the boost  OAA ing theorem  or bottom up  as Filter tree       LOMtree A bottom up partition process appears impossi    ble with representational constraints as shown in Section   in the Supplementary material so we focus on top down tree creation      Whenever there are representational constraints on partitions  such as linear classifiers   finding a strong partition function requires an efficient search over this set of classifiers  Ef    ficient searches over large function classes are routinely performed via gradient descent tech  niques for supervised learning  so they seem                          number of classes like a natural candidate  In existing literature  Figure    A comparison of One Against  examples for doing this exist when the problem All  OAA  and the Logarithmic Online Multi  is indeed binary  or when there is a prespeciclass Tree  LOMtree  with One Against All con  fied hierarchy over the labels and we just need strained to use the same training time as the to find partitioners aligned with that hierarchy  LOMtree by dataset truncation and LOMtree con  Neither of these cases applieswe have multistrained to use the same representation complex  ple labels and want to dynamically create the ity as One Against All  As the number of class choice of partition  rather than assuming that labels grows  the problem becomes harder and the one was handed to us  Does there exist a purity criterion amenable to a gradient descent apLOMtree becomes more dominant  proach  The precise objective studied in theory fails this test due to its discrete nature  and even natural approximations are challenging to tractably optimize under computational constraints  As a result  we use the theoretical objective as a motivation and construct a new Logarithmic Online Multiclass Tree  LOMtree  algorithm for empirical evaluation       Creating a tree in an online fashion creates a new class of problems  What if some node is initially created but eventually proves useless because no examples go to it  At best this results in a wasteful solution  while in practice it starves other parts of the tree which need representational complexity  To deal with this  we design an efficient process for recycling orphan nodes into locations where they are needed  and prove that the number of times a node is recycled is at most logarithmic in the number of examples  The algorithm is described in Section   and analyzed in Section      And is it effective  Given the inherent non convexity of the partition problem this is unavoidably an empirical question which we answer on a range of datasets varying from    to    K classes in Section    We find that under constrained training times  this approach is quite effective compared to all baselines while dominating other O log k  train time approaches  Whats new  To the best of our knowledge  the splitting criterion  the boosting statement  the LOMtree algorithm  the swapping guarantee  and the experimental results are all new here            Prior Work  Only a few authors address logarithmic time training  The Filter tree     addresses consistent  and robust  multiclass classification  showing that it is possible in the statistical limit  The Filter tree does not address the partition problem as we do here which as shown in our experimental section is often helpful  The partition finding problem is addressed in the conditional probability tree      but that paper addresses conditional probability estimation  Conditional probability estimation can be converted into multiclass prediction      but doing so is not a logarithmic time operation  Quite a few authors have addressed logarithmic testing time while allowing training time to be O k  or worse  While these approaches are intractable on our larger scale problems  we describe them here for context  The partition problem can be addressed by recursively applying spectral clustering on a confusion graph      other clustering approaches include       Empirically  this approach has been found to sometimes lead to badly imbalanced splits       In the context of ranking  another approach uses k means hierarchical clustering to recover the label sets for a given partition       The more recent work      on the multiclass classification problem addresses it via sparse output coding by tuning high cardinality multiclass categorization into a bit by bit decoding problem  The authors decouple the learning processes of coding matrix and bit predictors and use probabilistic decoding to decode the optimal class label  The authors however specify a class similarity which is O k     to compute  see Section       in        and hence this approach is in a different complexity class than ours  this is also born out experimentally   The variant of the popular error correcting output code scheme for solving multi label prediction problems with large output spaces under the assumption of output sparsity was also considered in       Their approach in general requires O k  running time to decode since  in essence  the fit of each label to the predictions must be checked and there are O k  labels  Another approach      proposes iterative least squares style algorithms for multi class  and multi label  prediction with relatively large number of examples and data dimensions  and the work of      focusing in particular on the cost sensitive multiclass classification  Both approaches however have O k  training time  Decision trees are naturally structured to allow logarithmic time prediction  Traditional decision trees often have difficulties with a large number of classes because their splitting criteria are not well suited to the large class setting  However  newer approaches          have addressed this effectively at significant scales in the context of multilabel classification  multilabel learning  with missing labels  is also addressed in        More specifically  the first work      performs brute force optimization of a multilabel variant of the Gini index defined over the set of positive labels in the node and assumes label independence during random forest construction  Their method makes fast predictions  however has high training costs       The second work      optimizes a rank sensitive loss function  Discounted Cumulative Gain   Additionally  a well known problem with hierarchical classification is that the performance significantly deteriorates lower in the hierarchy      which some authors solve by biasing the training distribution to reduce error propagation while simultaneously combining bottom up and top down approaches during training       The reduction approach we use for optimizing partitions implicitly optimizes a differential objective  A non reductive approach to this has been tried previously      on other objectives yielding good results in a different context      Framework and theoretical analysis  In this section we describe the essential elements of the approach  and outline the theoretical properties of the resulting framework  We begin with high level ideas       Setting  We employ a hierarchical approach for learning a multiclass decision tree structure  training this structure in a top down fashion  We assume that we receive examples x  X  Rd   with labels y                 k   We also assume access to a hypothesis class H where each h  H is a binary classifier  h   X           The overall objective is to learn a tree of depth O log k   where each node in the tree consists of a classifier from H  The classifiers are trained in such a way that hn  x       hn denotes the classifier in node n of the tree    means that the example x is sent to the right subtree of node n  while hn  x      sends x to the left subtree  When we reach a leaf  we predict according to the label with the highest frequency amongst the examples reaching that leaf    Further in the paper we skip index n whenever it is clear from the context that we consider a fixed tree node       In the interest of computational complexity  we want to encourage the number of examples going to the left and right to be fairly balanced  For good statistical accuracy  we want to send examples of class i almost exclusively to either the left or the right subtree  thereby refining the purity of the class distributions at subsequent levels in the tree  The purity of a tree node is therefore a measure of whether the examples of each class reaching the node are then mostly sent to its one child node  pure split  or otherwise to both children  impure split   The formal definitions of balancedness and purity are introduced in Section      An objective expressing both criteria  and resulting theoretical properties are illustrated in the following sections  A key consideration in picking this objective is that we want to effectively optimize it over hypotheses h  H  while streaming over examples in an online fashion    This seems unsuitable with some of the more standard decision tree objectives such as Shannon or Gini entropy  which leads us to design a new objective  At the same time  we show in Section     that under suitable assumptions  optimizing the objective also leads to effective reduction of the average Shannon entropy over the entire tree       An objective and analysis of resulting partitions  We now define a criterion to measure the quality of a hypothesis h  H in creating partitions at a fixed node n in the tree  Let i denotes the proportion of label i amongst the examples reaching this node  Let P  h x       and P  h x      i  denote the fraction of examples reaching n for which h x       marginally and conditional on class i respectively  Then we define the objective    k X J h      i  P  h x        P  h x      i         i    We aim to maximize the objective J h  to obtain high quality partitions  Intuitively  the objective encourages the fraction of examples going to the right from class i to be substantially different from the background fraction for each class i  As a concrete simple scenario  if P  h x             for some hypothesis h  then the objective prefers P  h x      i  to be as close to   or   as possible for each class i  leading to pure partitions  We now make these intuitions more formal  Definition    Purity   The hypothesis h  H induces a pure split if k X     i min P  h x      i   P  h x      i      i    where             and  is called the purity factor  In particular  a partition is called maximally pure if       meaning that each class is sent exclusively to the left or the right  We now define a similar definition for the balancedness of a split  Definition    Balancedness   The hypothesis h  H induces a balanced split if c  P  h x           c   z        where c            and  is called the balancing factor  A partition is called maximally balanced if         meaning that an equal number of examples are sent to the left and right children of the partition  The balancing factor and the purity factor are related as shown in Lemma    the proofs of Lemma   and the following lemma  Lemma    are deferred to the Supplementary material   Lemma    For any hypothesis h  and any distribution over examples  x  y   the purity factor  and the balancing factor  satisfy   min     J h                A partition is called maximally pure and balanced if it satisfies both      and         We see that J h      for a hypothesis h inducing a maximally pure and balanced partition as captured in the next lemma  Of course we do not expect to have hypotheses producing maximally pure and balanced splits in practice  Lemma    For any hypothesis h   X           the objective J h  satisfies J h           Furthermore  if h induces a maximally pure and balanced partition then J h          We want an objective to achieve its optimum for simultaneously pure and balanced split  The standard entropy based criteria  such as Shannon or Gini entropy  as well as the criterion we will propose  posed in Equation    satisfy this requirement  for the entropy based criteria see      for our criterion see Lemma       Our algorithm could also be implemented as batch or streaming  where in case of the latter one can for example make one pass through the data per every tree level  however for massive datasets making multiple passes through the data is computationally costly  further justifying the need for an online approach    The proposed objective function exhibits some similarities with the so called Carnaps measure          used in probability and inductive logic            Quality of the entire tree  The above section helps us understand the quality of an individual split produced by effectively maximizing J h   We next reason about the quality of the entire tree as we add more and more nodes  We measure the quality of trees using the average entropy over all the leaves in the tree  and track the decrease of this entropy as a function of the number of nodes  Our analysis extends the theoretical analysis in      originally developed to show the boosting properties of the decision trees for binary classification problems  to the multiclass classification setting  Given a tree T   we consider the entropy function Gt as the measure of the quality of tree      k X X   Gt   wl l i ln l i i   lL  where l i s are the probabilities that a randomly chosen data point x drawn from P  where P is a fixed target distribution over X   has label i given that x reaches node l  L denotes the set of all tree leaves  t denotes the number of internal tree nodes  and wl is the weight P of leaf l defined as the probability a randomly chosen x drawn from P reaches leaf l  note that lL wl       We next state the main theoretical result of this paper  it is captured in Theorem     We adopt the weak learning framework  The weak hypothesis assumption  captured in Definition    posits that each node of the tree T has a hypothesis h in its hypothesis class H which guarantees simultaneously a weak purity and a weak balancedness of the split on any distribution P over X   Under this assumption  one can use the new decision tree approach to drive the error below any threshold  Definition    Weak Hypothesis Assumption   Let m denote any node of the tree T   and let m   P  hm  x       and Pm i   P  hm  x      i   Furthermore  let   R  be such that for all m        min m      m     We say that the weak hypothesis assumption is satisfied when for any distribution P over X at each node m of the tree T there exists a hypothesis hm  H such that Pk J hm       i   m i  Pm i  m      Theorem    Under the Weak Hypothesis Assumption  for any           to obtain Gt   it suffices to make t              ln k    splits   We defer the proof of Theorem   to the Supplementary material and provide its sketch now  The analysis studies a tree construction algorithm where we recursively find the leaf node with the highest weight  and choose to split it into two children  Let n be the heaviest leaf at time t  Consider splitting it to two children  The contribution of node n to the tree entropy changes after it splits  This change  entropy reduction  corresponds to a gap in the Jensens inequality applied to the concave function  and thus can further be lower bounded  we use the fact that Shannon entropy is strongly concave with respect to     norm  see e g   Example     in Shalev Shwartz         The obtained lower bound turns out to depend proportionally on J hn      This implies that the larger the objective J hn   is at time t  the larger the entropy reduction ends up being  which further reinforces intuitions to maximize J  In general  it might not be possible to find any hypothesis with a large enough objective J hn   to guarantee sufficient progress at this point so we appeal to a weak learning assumption  This assumption can be used to further lower bound the entropy reduction and prove Theorem        The LOMtree Algorithm  The objective function of Section   has another convenient form which yields a simple online algorithm for tree construction and training  Note that Equation   can be written  details are shown in Section    in the Supplementary material  as J h     Ei   Ex    h x         Ex    h x      i      Maximizing this objective is a discrete optimization problem that can be relaxed as follows J h     Ei   Ex  h x    Ex  h x  i     where Ex  h x  i  is the expected score of class i  We next explain our empirical approach for maximizing the relaxed objective  The empirical estimates of the expectations can be easily stored and updated online in every tree node  The decision whether to send an example reaching a node to its left or right child node is based on the sign of the difference between the two expectations  Ex  h x   and Ex  h x  y   where y is a label of the data point  i e  when Ex  h x  Ex  h x  y      the data point is sent to the left  else it is sent to the right  This procedure is conveniently demonstrated on a toy example in Section    in the Supplement  During training  the algorithm assigns a unique label to each node of the tree which is currently a leaf  This is the label with the highest frequency amongst the examples reaching that leaf  While     Algorithm   LOMtree algorithm  online tree training  Input  regression algorithm R  max number of tree non leaf nodes T   swap resistance RS Subroutine SetNode  v  mv     mv  y    sum of the scores for class y  lv     lv  y    number of points of class y reaching v  nv     nv  y    number of points of class y which are used to train regressor in v  ev     ev  y    expected score for class y  Ev      expected total score  Cv      the size of the smallest leaf  in the subtree with root v  Subroutine UpdateC  v  While  v    r AND CPARENT v     Cv   v   PARENT v   Cv   min CLEFT v    CRIGHT v     Subroutine Swap  v  Find a leaf s for which  Cs   Cr   sPA PARENT s   sGPA  GRANDPA s   sSIB SIBLING s   If  sPA   LEFT sGPA    LEFT sGPA     sSIB Else RIGHT sGPA     sSIB UpdateC  sSIB    SetNode  s   LEFT v    s  SetNode  sPA    RIGHT v    sPA Create root r      SetNode  r   t     For each example  x  y  do Set j   r Do If  lj  y      mj  y       lj  y       nj  y       ej  y      lj  y    If j is a leaf  If lj has at least   non zero entries  If t T OR Cjmaxi lj  i  RS  Cr     If  t T   SetNode  LEFT j    SetNode  RIGHT j    t   Else Swap j  CLEFT j  bCj   c  CRIGHT j  CjCLEFT j    UpdateC  LEFT j   If j is not a leaf  If  Ej   ej  y   c     Else c     Train hj with example  x  c   R x  c  Pk mj  i     nj  y      mj  y     hj  x   ej  y    mj  y  nj  y   Ej   Pi   k i   nj  i  Set j to the child of j corresponding to hj Else Cj    break  testing  a test example is pushed down the tree along the path from the root to the leaf  where in each non leaf node of the path its regressor directs the example either to the left or right child node  The test example is then labeled with the label assigned to the leaf that this example descended to  The training algorithm is detailed in Algorithm   where each tree node contains a classifier  we use linear classifiers   i e  hj is the regressor stored in node j and hj  x  is the value of the prediction of hj on example x     The stopping criterion for expanding the tree is when the number of non leaf nodes reaches a threshold T       Swapping Consider a scenario where the current training example descends to leaf j  The leaf can split  create two children  if the examples that reached it in the past were coming from at least two different    The smallest leaf is the one with the smallest total number of data points reaching it in the past  PARENT  v   LEFT  v  and RIGHT  v  denote resp  the parent  and the left and right child of node v    GRANDPA  v  and SIBLING  v  denote respectively the grandparent of node v and the sibling of node v  i e  the node which has the same parent as v     In the implementation both sums are stored as variables thus updating Ev takes O    computations     We also refer to this prediction value as the score in this section         r     j  r                     sGPA     s       j  sPA  s  sSIB           sPA  sGPA     sSIB                  Figure    Illustration of the swapping procedure  Left  before the swap  right  after the swap  classes  However  if the number of non leaf nodes of the tree reaches threshold T   no more nodes can be expanded and thus j cannot create children  Since the tree construction is done online  some nodes created at early stages of training may end up useless because no examples reach them later on  This prevents potentially useful splits such as at leaf j  This problem can be solved by recycling orphan nodes  subroutine Swap in Algorithm     The general idea behind node recycling is to allow nodes to split if a certain condition is met  In particular  node j splits if the following holds  Cj   max i         k   lj  i    RS  Cr             where r denotes the root of the entire tree  Cj is the size of the smallest leaf in the subtree with root j  where the smallest leaf is the one with the smallest total number of data points reaching it in the past  lj is a k dimensional vector of non negative integers where the ith element is the count of the number of data points with label i reaching leaf j in the past  and finally RS is a swap resistance  The subtraction of maxi         k  lj  i  in Equation   ensures that a pure node will not be recycled  If the condition in Inequality   is satisfied  the swap of the nodes is performed where an orphan leaf s  which was reached by the smallest number of examples in the past  and its parent sPA are detached from the tree and become children of node j whereas the old sibling sSIB of an orphan node s becomes a direct child of the old grandparent sGPA   The swapping procedure is shown in Figure    The condition captured in the Inequality   allows us to prove that the number of times any given node is recycled is upper bounded by the logarithm of the number of examples whenever the swap resistance is   or more  Lemma     Lemma    Let the swap resistance RS be greater or equal to    Then for all sequences of examples  the number of times Algorithm   recycles any given node is upper bounded by the logarithm  with base    of the sequence length      Experiments  We address several hypotheses experimentally     The LOMtree algorithm achieves true logarithmic time computation in practice     The LOMtree algorithm is competitive with or better than all other logarithmic train test time algorithms for multiclass classification     The LOMtree algorithm has statistical performance close to more common O k  approaches  To address these hypotheses  we conTable    Dataset sizes  ducted experiments on a variety of Isolet Sector Aloi ImNet ODP benchmark multiclass datasets  Isosize     MB   MB     MB   GB    GB let  Sector  Aloi  ImageNet  Im  features       K             M Net  and ODP     The details of the   examples              K     M         datasets are provided in Table    The datasets were divided into training   classes               K    K       and testing        Furthermore      of the training dataset was used as a validation set  The baselines we compared LOMtree with are a balanced random tree of logarithmic depth  Rtree  and the Filter tree      Where computationally feasible  we also compared with a one against all classifier  OAA  as a representative O k  approach  All methods were implemented in the Vowpal Wabbit      learning system and have similar levels of optimization  The regressors in the tree nodes for LOMtree  Rtree  and Filter tree as well as the OAA regressors were trained by online gradient descent for which we explored step sizes chosen from the set                                We used        compressed The details of the source of each dataset are provided in the Supplementary material       linear regressors  For each method we investigated training with up to    passes through the data and we selected the best setting of the parameters  step size and number of passes  as the one minimizing the validation error  Additionally  for the LOMtree we investigated different settings of the stopping criterion for the tree expansion  T    k      k      k      k       k       k       k      and swap resistance RS                                 In Table   and   we report respectively train time and per example test time  the best performer is indicated in bold   Training time  and later reported test error  is not provided for OAA on ImageNet and ODP due to intractability    both are petabyte scale computations     Table    Training time on selected problems  Table    Per example test time on all problems  Isolet Sector Aloi Isolet Sector Aloi ImNet ODP LOMtree      s      s      s LOMtree     ms     ms     ms     ms     ms OAA      s      s   m    s OAA      ms     ms     ms     s     s  log  time ratio   The first hypothesis is consistent with the experimental results  Time wise LOMtree significantly outperforms OAA due to building only close to logarithmic depth trees  The improvement in the training time increases with the number of classes in the classification problem  For instance on Aloi training with LOMtree is      times faster than with OAA  The same can be said about the test time  where the per example test time for Aloi  ImageNet and ODP are respectively            and        times faster than OAA  The significant advantage of LOMtree over OAA is also captured in Figure    Next  in Table    the best logarithmic time perLOMtree vs oneagainstall former is indicated in bold  we report test error    of logarithmic train test time algorithms  We    also show the binomial symmetrical     confidence intervals for our results  Clearly the sec  ond hypothesis is also consistent with the experimental results  Since the Rtree imposes a   random label partition  the resulting error it ob  tains is generally worse than the error obtained by the competitor methods including LOMtree   which learns the label partitioning directly from the data  At the same time LOMtree beats Fil                ter tree on every dataset  though for ImageNet log  number of classes  Figure    Logarithm of the ratio of per example and ODP  both have a high level of noise  the advantage of LOMtree is not as significant  test times of OAA and LOMtree on all problems  Table    Test error     and confidence interval on all problems  LOMtree Rtree Filter tree OAA Isolet                                        Sector                                         Aloi                                          ImNet                               NA ODP                               NA The third hypothesis is weakly consistent with the empirical results  The time advantage of LOMtree comes with some loss of statistical accuracy with respect to OAA where OAA is tractable  We conclude that LOMtree significantly closes the gap between other logarithmic time methods and OAA  making it a plausible approach in computationally constrained large k applications      Conclusion  The LOMtree algorithm reduces the multiclass problem to a set of binary problems organized in a tree structure where the partition in every tree node is done by optimizing a new partition criterion online  The criterion guarantees pure and balanced splits leading to logarithmic training and testing time for the tree classifier  We provide theoretical justification for our approach via a boosting statement and empirically evaluate it on multiple multiclass datasets  Empirically  we find that this is the best available logarithmic time approach for multiclass classification problems     Note however that the mechanics of testing datastes are much easier   one can simply test with effectively untrained parameters on a few examples to measure the test speed thus the per example test time for OAA on ImageNet and ODP is provided     Also to the best of our knowledge there exist no state of the art results of the OAA performance on these datasets published in the literature       Acknowledgments We would like to thank Alekh Agarwal  Dean Foster  Robert Schapire and Matus Telgarsky for valuable discussions   

 We show how to reduce the process of predicting conditional quantiles  and the median in particular  to solving classification  The accompanying theoretical statement shows that the regret of the classifier bounds the regret of the quantile regression under a quantile loss  We also test this reduction empirically against existing quantile regression methods on large real world datasets and discover that it provides state of theart performance     Introduction Regression is the problem of estimating a mapping from some feature space X to a real valued output Y   given a finite sample of the form  x  y  drawn from a distribution D over X  Y   Typically  the goal of regression is to minimize the squared error loss over the distribution D  that is  E x y D  y  f  x      One standard justification for this form of regression is that the minimizer is the mean  f   x    EyD x  y    Bianca Zadrozny Universidade Federal Fluminense Rua Passo da Patria      Bl  E    andar Niteroi  RJ            Brazil bianca ic uff br  sons for doing quantile regression as opposed or in addition to typical regression include     Quantiles tend to behave well under noise  For instance  the median  i e  the     quantile  equals the mean under Gaussian noise  but the median is often inherently more robust to heavy tailed and non Gaussian noise     As mentioned above  many important practical problems are naturally expressed in terms of quantiles  For instance  wallet estimation  e g  estimating the potential amount of money a customer can spend on computer hardware  as opposed to the expected amount  can be done by looking at the conditional upper quantiles of expenditures          Quantile regression also been applied to many other problems in Econometrics  Sociology and Ecology  among other fields            The actual distribution of conditional noise can be estimated as required for some applications     using quantile regression   However  there are many important applications for which mean estimates are either irrelevant or insufficient  and quantiles  also known as general order statistics  are the main quantities of interest  For instance  consider trying to assess the risk of a business proposal  Estimates of the lower quantiles of the conditional return distribution would give a better indication of how worthwhile the proposal is than a simple estimate of the mean return  which could be too high because of very unlikely high profits    This paper shows that the quantile regression problem can be reduced to classification  The Quanting algorithm that we introduce takes as input an instance of quantile regression and outputs a family of classification problems such that solving the latter problems with small average error leads to a provably accurate estimate of the conditional quantile  Reducing quantile to classification automatically gives us access to a large array of quantile regression methods  since the reduction applies to any existing or future classification method   The process of estimating the quantiles of a conditional distribution is known as quantile regression  More specifically  the goal of quantile regression is to obtain estimates on the q quantiles of the conditional distribution D x  Intuitively  q quantiles for different q describe different segments of the conditional distribution D x and thus offer more refined information about the data at hand  Other rea   We compare empirically the Quanting algorithm with other methods for quantile regression in the literature  Koenker     has developed a linear quantile regression method  while Takeuchi et al      have recently devised a kernelbased quantile estimation method  Our approach  which is intrinsically non linear and conceptually simpler  compares favorably with the existing alternatives in our experiments       Algorithm   Quanting train  importance weighted classifier learning algorithm A  training set S  quantile q     For t in         q     q      q          quantile loss      a  St        b  For each  x  y  in S  St   St    x  I y  t   qI y  t        q I y   t     c  ct   A St          Return the set of classifiers  ct       ball loss       Pictorially  this is a tilted absolute loss as in figure    Mathematically  this is Ex yD lq  y  f  x    where                lq  y  f  x      q y  f  x  I y  f  x        q  f  x   y I y   f  x         error   prediction  actual  and I       if its argument is true and   otherwise  Figure    Loss functions which induce quantile regression     Basic Details The quantile regression problem is defined in a setting where we have a measure D over a set of features X and real valued outputs Y   Definition    Conditional q quantile  Let    q    f   f  x  is a conditional q quantile  or conditional qorder statistic  for D if for  D almost every  x  X D y  f  x  x   q and D y  f  x  x      q  The     quantile is also known as the median  Note that the q quantile may not be unique when the conditional distribution has regions with zero mass      Optimization It is well known that the optimal estimator for the absoluteerror loss is a median      In other words  we have that for every regression problem D  arg min Ex yD  y  f  x   is a  conditional  median  f  This can be verified by considering two equal point masses at locations y  and y    The absolute value loss for a point y   y    y    is  y  y       y   y     y   y    which is constant independent of y  whereas any y    y    y    yields a larger value  Since we can take any distribution over y and break into equal mass pairs with y  above and y  below the median  the absolute error loss is minimized when f  x  is a median  The generalization of absolute error loss for arbitrary order statistics is the quantile loss function  also known as pin   The correctness of this can be  again  seen by considering two points y  and y  with probability ratio  q q   or by noting that the result is implied by equation     in the proof of Lemma    the integral is positive unless Q x    q x  or D y   t x    q for all t between Q x  and q x       An Algorithm In this section and the next  we assume that we are given samples  x  y  from a distribution D  where    y     if this is not the case  we can re normalize the data   Given these samples  our proposed algorithm  which we call Quanting  estimates the qth quantile of the conditional distribution D x using any importance weighted classification algorithm A  In fact  using an extra reduction discussed in Corollary   below  one can also do Quanting via an unweighted binary classification algorithm  but we defer any further discussion of this to the next section  The Quanting algorithm has two parts  Algorithm   receives a set of training examples S of the form  x  y  and a quantile q as input and uses algorithm A to compute a family of binary classifiers ct   We assume that algorithm A receives as input a set of training examples of the form  x  y  w  where w is a weight and attempts to minimize the weighted error  In the algorithm  positive examples receive weight q while negative examples receive weight     q   Using the classifiers ct   Algorithm   produces a prediction of the q quantile for each x in a test set S     in precisely the same manner as the Probing algorithm for estimating conditional class probabilities using     classifiers      The essential idea of Quanting is also similar to Probing  Each ct attempts to answer the question is the q quantile above or below t  In the  idealized  scenario where A is perfect  one would have ct  x      if and only if t    Algorithm   Quanting test  set of classifiers  ct    test set S      For each x in S     Q x    EtU        ct  x    Applying this formula to f  x    Q x  and f  x    q x  and taking the difference yields Ex yD  lq  y  Q x    lq  y  q x       R q x  qD y  u x  Ex Q x  du     q D y   u x     R q x  q  qD y   u x  du Ex Q x      q D y   u x  R q x  Ex Q x   q  D y   u x   du     q x  for a q quantile q x   hence Algorithm   would output R q x  dt   q x  exactly  Our analysis shows that if the   error of A is small on average over t  the quantile estimate is accurate  We note in passing that in reality  one cannot find a different classifier for each t          Constructing classifiers ct for t in a discrete mesh       n    n           n     n     will add a   n term to the error bound     Quanting Reductions Analysis The Lemma we prove next relates the average regret of the classifiers ct  how well the classifiers do in comparison to how well they could do  to the regret of the quantile loss incurred by the Quanting algorithm  For each x  the output produced by the quanting algorithm is denoted by Q x   whereas q x  is a correct q quantile  In this analysis  we use the standard one classifier trick      instead of learning different classifiers  we learn one classifier c    ct   with an extra feature t used to index classifier ct   Lemma    Quanting Regret Transform  For all D  c           We will show that e D  c   minc  e D  c    is at least this last expression  The expected importance weighted error incurred by the classifiers  ct   is    R  qI y  t     ct  x   e D  c    Ex yD   dt      q I y   t ct  x     R  qD y  t x    Ex   dt   D y   t x   q ct  x  R    qEx  y    Ex    D y   t x   q ct  x  dt     R Q x   qEx  y    Ex    D y   t x   q dt      Here only the last line is non trivial  and it follows from the fact that D y   t x   q is increasing in t  Thus the smallest possible value for the integral in     is achieved by placing as much weight ct  x  as possible on the smallR  est t while respecting the constraints   ct  x  dt   Q x  and    ct  x      This corresponds precisely to setting ct  x    I t  Q x    from which     follows  On can show that the inequality     is in fact an equality when instead of  ct   we use the  optimal  classifiers  Ex yD  lq  y  Q x     Ex yD  lq  y  q x      ct  x    I D y  t x   q     e D  c   min e D  c       and substitute q x  for Q x   Therefore   c  where e D  c  is the expected importance weighted binary loss of c over D  Proof  For any function f   f  x   Ex yD lq  y  f  x   is given by eqn       qEx yD  y  f  x  I y  f  x            q Ex yD  f  x   y I f  x   y       R   It is known that E XI X           Pr X  t dt   R   Pr X   t dt for any random variable X  so we   rewrite  Ex yD lq  y  f  x   R   qEx   D y  f  x   t   x dt  R      q Ex   D f  x   y   t   x dt  R    qEx f  x  D y  u x  du R f  x       q Ex   D y   u x  du        e D  c   e D  c   R Q x  Ex q x   D y   t x   q dt     Ex yD  lq  y  Q x    lq  y  q x      using      This finishes the proof  We now show how to reduce q quantile estimation to unweighted binary classification using the results of previous work      We apply rejection sampling  we feed the unweighted classifier samples of the form   x  t   I y  t    each of the samples being independently discarded with probability    w I y  t    where w b    qb       q     b  is the examples weight  Notice that by     Theorem       sample complexity is not significantly affected by this  Corollary    Quanting to Binary Regret  For D as above and unweighted binary classifier c    ct    let D be the distribution produced by rejection sampling  Then       Ex yD  lq  y  Q x     Ex yD  lq  y  q x       e D  c   min e D  c       c  Proof  Let c    ct  t be the importance weighted classifiers induced by the rejection sampling procedure  A folk theorem     implies that e D  c   minc  e D  c      e D  c   minc  e D  c    and the result follows from Lemma       Related Work A standard technique for quantile regression that has been developed and extensively applied in the Econometrics community     is linear quantile regression  In linear quantile regression  we assume that the conditional quantile function is a linear function of the features of the form x and we estimate the parameters  that minimize the quantile loss function  Equation     It can be shown that this minimization is a linear programming problem and that it can be efficiently solved using interior point techniques      Implementations of linear quantile regression are available in standard statistical analysis packages such as R and SAS  The obvious limitation of linear quantile regression is that the assumption of a linear relationship between the explanatory variables and the conditional quantile function may not be true  Recently  Takeuchi et al  have recently proposed a technique for nonparametric quantile estimation      that applies the two standard features of kernel methods to conditional quantile estimation  regularization and the kernel trick  They show that a regularized version of the quantile loss function can be directly minimized using standard quadratic programming techniques  By choosing an appropriate kernel  such as a radial basis function kernel  one can obtain nonlinear conditional quantile estimates  They compare their method experimentally to linear quantile regression and to a nonlinear spline approach suggested by Koenker     on many small datasets for different quantiles and find that it performs the best in most cases     Experiments Here we compare experimentally the Quanting algorithm to the two existing methods for quantile regression described in section    linear quantile regression and kernel quantile regression  We compare the methods on three different performance metrics     The quantile loss  Equation        The percentage of examples for which the prediction f  x  exceeds the actual value y  which should be close to the quantile q for which we are optimizing      The running time  Pentium     GHz      GB RAM   As base classifier learners for Quanting  we use two algorithms available in the WEKA machine learning software       the J   decision tree learner and logistic regression  For both methods  we use the default parameters provided by WEKA  We use rejection sampling to perform importance weighted classification with standard unweighted classifiers  We use an adaptive discretization scheme to choose the thresholds t in algorithm    the same scheme that we use in the Probing reduction      We fix the number of classifiers at     for all the datasets  For the kernel quantile regression  we have followed the same experimental setup as described by Takeuchi et al        We use a radial basis function kernel and choose its radius and the regularization parameter using crossvalidation on the training data  We also scale the features and the label to have zero mean and a standard deviation of    as required for kernel methods  When predicting  we convert the label back to the original scale to compute the quantile loss  As our benchmarks  we use four large  publicly available datasets  from real world domains where quantile regression is clearly applicable     Adult  available from the UCI Machine Learning Repository     as a classification dataset  The data was originally extracted from the Census Bureau Database and describes individual demographic characteristics of such as age  education  sex and occupation  For the original dataset  the objective is to predict a label that indicates whether or not the individuals income is above    K  We have retrieved the original numerical income values from the Census Bureau Database and used the income as the dependent variable in the quantile regression  Our objective is to predict the quantiles of the conditional income distribution  which is useful if we want to determine what is a low or a high income for a given individual     KDD Cup       available from the UCI KDD Archive      This dataset consists of records of individuals who have made a donation in the past to a particular charity  Each example consists of attributes describing each individuals donation history over a series of donation campaigns  as well as demographic information  such as income and age  The dependent variable is the individuals donation amount in the most recent donation campaign  The original dataset contains       training records and       test records  but only    of the individuals donated in the current campaign  Our objective is to predict the quantiles of the conditional donation amount for individuals who donate  For this reason  we only use      donor examples in the training set and the        Dataset Adult KDD Cup      California Housing Boston Housing  Features             Training                       Test                     Table    Number of features and examples  training and test  of each of the datasets  donor examples in the test set  Predicting quantiles of the conditional donation distribution is important for anchoring  i e   deciding how much to suggest as possible donation values when soliciting donations  Anchoring is a well established concept in marketing  see  for example           California Housing  available from the StatLib repository      It contains data on California housing characteristics aggregated at the block level  a sample block group on average includes        individuals living in a geographically compact area   The independent variables are median income  housing median age  total rooms  total bedrooms  population  households  latitude  and longitude  The dependent variable is the median house value  Our objective is to predict the quantiles of the conditional house value distribution  This information is very valuable for house sellers and buyers  since it indicates what would be a lower bound and an upper bound on the house value  given its characteristics     Boston Housing  available from the StatLib repository      It contains data on Boston housing characteristics and values  The prediction task is analogous to the one described for the California Housing dataset  Table   shows the number of features and the number of examples for each dataset  We use the standard train test splits for training and testing  Because the kernel quantile regression method has very high memory and computational time requirements  we could not run it using all the examples in the training set for the Adult  KDD Cup      and California Housing  We have run it for the maximum number of examples possible  which in this case was      for the three datasets  after trying with                         etc    The      examples were chosen at random from the training data  We have run the methods for   different quantile values for each dataset           and      The results are shown in tables         and    In terms of running time  it is clear that the linear method is the most efficient and that the kernel method is inefficient  Even with the number of examples limited at       the kernel method takes more than one hour to run on the larger datasets  Quanting is relatively efficient  with our choice of  classifier learners it does not take more than   minutes to run with     classifiers even on the larger datasets  In terms of the quantile loss  Quanting J   is clearly the best performer for the Adult and California Housing datasets  The kernel method and Quanting LogReg are performing about the same for these two datasets  while the linear method is inferior  For the KDD Cup      dataset  the linear method is the best for q     and q      while Quanting J   is the best for q      This can be explained by the fact that there is a strong linear correlation between the label and one of the features in this dataset  which is well captured by the linear quantile regression but not so easily captured by Quanting and by the kernel method  Finally  for the Boston Housing dataset  the best method depends on the particular value of q but the linear method is performing consistently worse than the others  In terms of the percentage of examples for which the prediction exceeds the actual value  all the methods come close to desired value  the same as q  in most of the cases  But we can observe that the linear method is consistently close  while the kernel method shows the largest deviations  To give an idea of how the Quanting algorithm progresses as we add more classifiers  in figure   we plot the quantile loss as a function of the number of classifiers for the Adult and Boston Housing datasets  q         For comparison  we also plot the values of the quantile loss for the linear and kernel methods as horizontal lines in the picture  It is clear that Quanting converges very fast  In both cases  the convergence occurs with about    classifiers     Conclusion In this paper  we present a reduction from quantile regression to classification  Theoretically  we are now able to quantify the regret of quantile regression under a quantile loss in terms of the error rate of a base classifier  In practice  this means that we can apply classifier learning methods to solve quantile regression problems  which appear very often in real world applications  Our experiments show that the Quanting reduction is efficient in terms of computational time and performs well compared to existing quantile regression methods  Acknowledgements We thank Saharon Rosset and Claudia Perlich for useful discussions on the topic of this paper   
  Keywords  In evaluating prediction markets  and other crowd prediction mechanisms   investigators have repeatedly observed a socalled wisdom of crowds effect  which can be roughly summarized as follows  the average of participants performs much better than the average participant  The market price an average or at least aggregate of traders beliefsoffers a better estimate than most any individual traders opinion  In this paper  we ask a stronger question  how does the market price compare to the best traders belief  not just the average trader  We measure the markets worst case log regret  a notion common in machine learning theory  To arrive at a meaningful answer  we need to assume something about how traders behave  We suppose that every trader optimizes according to the Kelly criteria  a strategy that provably maximizes the compound growth of wealth over an  infinite  sequence of market interactions  We show several consequences  First  the market prediction is a wealthweighted average of the individual participants beliefs  Second  the market learns at the optimal rate  the market price reacts exactly as if updating according to Bayes Law  and the market prediction has low worst case log regret to the best individual participant  We simulate a sequence of markets where an underlying true probability exists  showing that the market converges to the true objective frequency as if updating a Beta distribution  as the theory predicts  If agents adopt a fractional Kelly criteria  a common practical variant  we show that agents behave like full Kelly agents with beliefs weighted between their own and the markets  and that the market price converges to a time discounted frequency  Our analysis provides a new justification for fractional Kelly betting  a strategy widely used in practice for ad hoc reasons  Finally  we propose a method for an agent to learn her own optimal Kelly fraction   Auction and mechanism design  electronic markets  economically motivated agents  multiagent learning  Categories and Subject Descriptors I       Artificial Intelligence   Distributed Artificial IntelligenceIntelligent agents  Multiagent systems  General Terms Economics  Short Version Appears in  Proceedings of the   th International Conference on Autonomous Agents and Multiagent Systems  AAMAS        Conitzer  Winikoff  Padgham  and van der Hoek  eds    June            Valencia  Spain       INTRODUCTION  Consider a gamble on a binary event  say  that Obama will win the      US Presidential election  where every x dollars risked earns xb dollars in net profit if the gamble pays off  How many dollars x of your wealth should you risk if you believe the probability is p  The gamble is favorable if bp  p       in which case betting your entire wealth w will maximize your expected profit  However  thats extraordinarily risky  a single stroke of bad luck loses everything  Over the course of many such gambles  the probability of bankruptcy approaches    On the other hand  betting a small fixed amount avoids bankruptcy but cannot take advantage of compounding growth  The Kelly criteria prescribes choosing x to maximize the expected compounding growth rate of wealth  or equivalently to maximize the expected logarithm of wealth  Kelly betting is asymptotically optimal  meaning that in the limit over many gambles  a Kelly bettor will grow wealthier than an otherwise identical non Kelly bettor with probability                      Assume all agents in a market optimize according to the Kelly principle  where b is selected to clear the market  We consider the implications for the market as a whole and properties of the market odds b or  equivalently  the market probability pm          b   We show that the market prediction pm is a wealth weighted average of the agents predictions pi   Over time  the market itselfby reallocating wealth among participantsadapts at the optimal rate with bounded log regret to the best individual agent  When a true objective probability exists  the market converges to it as if properly updating a Beta distribution according to Bayes rule  These results illustrate that there is no price of anarchy associated with well run prediction markets  We also consider fractional Kelly betting  a lower risk variant of Kelly betting that is popular in practice but has less theoretical grounding  We provide a new justification for fractional Kelly based on agents confidence  In this case  the market prediction is a confidence and wealth weighted average that empirically converges to a time discounted version of objective frequency  Finally  we propose a method for agents to learn their optimal fraction over time       KELLY BETTING   When offered b to   odds on an event with probability p  the Kelly optimal amount to bet is f  w  where bp      p  b is the optimal fixed fraction of total wealth w to commit to the gamble  If f  is negative  Kelly says to avoid betting  expected profit is negative  If f  is positive  you have an information edge  Kelly says to invest a fraction of your wealth proportional to how advantageous the bet is  In addition to maximizing the growth rate of wealth  Kelly betting maximizes the geometric mean of wealth and asymptotically minimizes the mean time to reach a given aspiration level of wealth       Suppose fair odds of   b are simultaneously offered on the opposite outcome  e g   Obama will not win the election   If bp      p       then betting on this opposite outcome is favorable  substituting   b for b and    p for p  the optimal fraction of wealth to bet becomes    p  bp  An equivalent way to think of a gamble with odds b is as a prediction market with price pm          b   The volume of bet is specified by choosing a quantity q of shares  where each share is worth    if the outcome occurs and nothing otherwise  The price represents the cost of one share  the amount needed to pay for a chance to win back     In this interpretation  the Kelly formula becomes f    f    p  pm      pm  The optimal action for the agent is to trade q    f  w pm shares  where q      is a buy order and q      is a sell order  or a bet against the outcome  Note that q  is the optimum of expected log utility p ln     pm  q   w        p  ln pm q   w    odds reached when all agents are optimizing  and supply and demand are precisely balanced  Recall that the markets probability implied by Pthe odds of b is pm          b   We will show that pm is i wi pi         Payout balance  The first approach well use is payout balance  the amount of money at risk must be the same as the amount paid out  Theorem     Market Pricing  For all normalized agent wealths wi and agent beliefs pi   X pi wi pm   i  Proof  To see this  recall that fi    pi  pm       pm   for pi   pm   For pi   pm   Kelly betting prescribes taking the other side of the bet  with fraction     pi        pm   pm  pi            pm   pm So the market equilibrium occurs at the point pm where the payout is equal to the payin  If the event occurs  the payin is X pi  pm X pi  pm   wi   wi        b     pm pm i p  p    pm i p  p Thus we want X   pm i p  p i     pm pm  X pi  pm pi  pm wi   wi      pm    pm i pi  pm X pm  pi wi   pm i p  p  i pi  pm  X  pi w i    i  X  pm wi    i  Using  P       Log utility maximization  wi      we get the theorem   An alternate derivation of the market prediction utilizes the fact that Kelly betting is equivalent to maximizing expected log utility  Let q   x b      be the gross profit of an agent who risks x dollars  or in prediction market language the number of shares purchased  Then expected log utility is E U  q     p ln     pm  q   w        p  ln pm q   w   The optimal q that maximizes E U  q   is q pm      w p  pm    pm    pm       Proposition    In a market of agents each with log utility and initial wealth w  the competitive equilibrium price is X pm   wi pi      MARKET PREDICTION  In order to define the prediction markets performance  we must define its prediction b  or the equilibrium payoff  or  m  X pm  pi pi  pm wi   wi   or    pm pm i pi  pm i pi  pm X X  pi  pm  wi    pm  pi  wi   or  i pi  pm  i  m  X  MARKET MODEL  Suppose that we have a prediction market  P where participant i has a starting wealth wi with i wi      Each participant i uses Kelly betting to determine the fraction fi of their wealth bet  depending on their predicted probability pi   We model the market as an auctioneer matching supply and demand  taking no profit and absorbing no loss  We adopt a competitive equilibrium concept  meaning that agents are price takers  or do not consider their own effect on prices if any  Agents optimize according to the current price and do not reason further about what the price might reveal about the other agents information  An exception of sorts is the fractional Kelly setting  where agents do consider the market price as information and weigh it along with their own  A market is in competitive at price pm if all P equilibrium  agents are optimizing and q      or every buy order i i and sell order are matched  We discuss next what the value of pm is       m  i  i  This is not a coincidence  Kelly betting is identical to maximizing expected log utility       m  i  i  where we assume absolute wealth   P  i wi      or w is normalized wealth not   P Proof  These prices satisfy i qi      the condition for competitive equilibrium  supply equals demand   by substitution    This result can be seen as a simplified derivation of that by Rubinstein              and is also discussed by Pennock and Wellman          and Wolfers and Zitzewitz            as L       Wealth redistributed according to Bayes Law  In an individual round  if an agents belief is pi   pm   i pm wi and have a total wealth afterward then they bet p p m dependent on y according to   I yt      log  t          I yt      log   pt    pt  Similarly  we measure the quality of market participant making prediction pit as Li   LEARNING PREDICTION MARKETS  Individual participants may have varying prediction qualities and individual markets may have varying odds of payoff  What happens to the wealth distribution and hence the quality of the market prediction over time  We show next that the market learns optimally for two well understood senses of optimal   T X  T X  I yt      log  t          I yt      log   pit    pit  So after T rounds  the total wealth of player i is  y     yt T   Y pit t    pit wi   pt    pt t   where wi is the starting wealth  We next prove a well known theorem for learning in the present context  see for example       Theorem    For all sequences of participant predictions pit and all sequences of revealed outcomes yt   L  min Li   ln i     If  y       If  y           pi  pm pi   wi   wi   wi pm    pm pm pi  pm    pi     wi   wi   wi    pm    pm  Similarly if pi   pm   we get  If  y       If  y       pm  pi pi wi   wi   wi pm pm     pm  pi    pi   wi   wi   wi      pm pm    pm         This theorem is extraordinarily general  as it applies to all market participants and all outcome sequences  even when these are chosen adversarially  It states that even in this worst case situation  the market performs only ln   wi worse than the best market participant i  P Proof  Initially  we have that i wi      After T rounds  the total wealth of any participant i is given by  y     yt T   Y pit t    pit wi   wi eLLi     p    p t t t   where the last inequality follows from wealth being conserved  Thus ln wi   L  Li     yielding  which is identical  If we treat the prior probability that agent i is correct as wi   Bayes law states that the posterior probability of choosing agent i is P  i   y         P  y       i P  i  p i wi pi w i       P P  y      pm i pi wi  which is precisely the wealth computed above for the y     outcome  The same holds when y      and so Kelly bettors redistribute wealth according to Bayes law        Market Sequences  It is well known that Bayes law is the correct approach for integrating evidence into a belief distribution  which shows that Kelly betting agents optimally summarize all past information if the true behavior of the world was drawn from the prior distribution of wealth  Often these assumptions are too strongthe world does not behave according to the prior on wealth  and it may act in a manner completely different from any one single expert  In that case  a standard analysis from learning theory shows that the market has low regret  performing almost as well as the best market participant  For any particular sequence of markets we have a sequence pt of market predictions and yt         of market outcomes  We measure the accuracy of a market according to log loss      wi  L  Li   ln          wi  FRACTIONAL KELLY BETTING  Fractional Kelly betting says to invest a smaller fraction f  of wealth for       Fractional Kelly is usually justified on an ad hoc basis as either     a risk reduction strategy  since practitioners often view full Kelly as too volatile  or     a way to protect against an inaccurate belief p  or both       Here we derive an alternate interpretation of fractional Kelly  In prediction market terms  the fractional Kelly formula is p  pm       pm With some algebra  fractional Kelly can be rewritten as p   pm    pm where p    p        pm         In other words   fractional Kelly is precisely equivalent to full Kelly with revised belief p    pm   or a weighted average of the agents original belief and the markets belief  In   this light  fractional Kelly is a form of confidence weighting where the agent mixes between remaining steadfast with its own belief        and acceding to the crowd and taking the market price as the true probability         The weighted average form has a Bayesian justification if the agent has a Beta prior over p and has seen t independent Bernoulli trials to arrive at its current belief  If the agent envisions that the market has seen t  trials  then she will update her belief to p        pm   where    t  t   t                 The agents posterior probability given the price is a weighted average of its prior and the price  where the weighting term captures her perception of her own confidence  expressed in terms of the independent observation count seen as compared to the market            MARKET PREDICTION WITH FRACTIONAL     KELLY  When agents play fractional Kelly  the competitive equilibrium price naturally changes  The resulting market price is easily compute  as for fully Kelly agents  Theorem     Fractional Kelly Market Pricing  For all agent beliefs pi   normalized wealths wi and fractions i P i wi pi       pm   Pi l l wl Prices retain the form of a weighted average  but with weights proportional to the product of wealth and self assessed confidence  Proof  The proof is a straightforward corollary of Theorem    In particular  we note that a  fractional Kelly agent of wealth w bets precisely as a full Kelly agent of wealth w  Consequently  we can apply theorem   with wi    Piwiiwi i and p i   pi unchanged       MARKET DYNAMICS WITH STATIONARY OBJECTIVE FREQUENCY  The worst case bounds above hold even if event outcomes are chosen by a malicious adversary  In this section  we examine how the market performs when the objective frequency of outcomes is unknown though stationary  The market consists of a single bet repeated over the course of T periods  Unbeknown to the agents  each event unfolds as an independent Bernoulli trial with probability of success   At the beginning of time period t  the realization of event Et is unknown and agents trade until equilibrium  Then the outcome is revealed  and the agents holdings pay off accordingly  As time period t     begins  the outcome of Et   is uncertain  Agents bet on the t     period event until equilibrium  the outcome is revealed  payoffs are collected  and the process repeats  In an economy of Kelly bettors  the equilibrium price is a wealth weighted average      Thus  as an agent accrues relatively more earnings than the others  its influence on price increases  In the next two subsections  we examine how this adaptive process unfolds  first  with full Kelly agents and second  with fractional Kelly agents  In the former case  prices react exactly as if the market were a single agent updating a Beta distribution according to Bayes rule                    a                                                    b                                                       Figure     a  Price  black line  versus the observed frequency  gray line  of the event over     time periods  The market consists of     full Kelly agents with initial wealth wi           b  Wealth after    time periods versus belief for     Kelly agents  The event has occurred in    of the    trials  The solid line is the posterior Beta distribution consistent with observing    successes in    independent Bernoulli trials         Market dynamics with full Kelly agents  Figure   a plots the price over     time periods  in a market composed of     Kelly agents with initial wealth wi          and pi generated randomly and uniformly on         In this simulation the true probability of success  is      For comparison  the figure also shows the observed frequency  or the number of times that E has occurred divided by the number of periods  The market price tracks the observed frequency extremely closely  Note that price changes are due entirely to a transfer of wealth from inaccurate agents to accurate agents  who then wield more power in the market  individual beliefs remain fixed  Figure   b illustrates the nature of this wealth transfer  The graph provides a snapshot of agents wealth versus their belief pi after period     In this run  E has occurred in    out of the    trials  The maximum in wealth is near       or      The solid line in the figure is a Beta distribution with parameters        and        This distribution is precisely the posterior probability of success that results from the observation of    successes out of    independent Bernoulli trials  when the prior probability of success is uniform on        The fit is essentially perfect  and can be proved in the limit since the Beta distribution is conjugate to the Binomial distribution under Bayes Law  Although individual agents are not adaptive  the markets composite agent computes a proper Bayesian update  Specifically  wealth is reallocated proportionally to a Beta distribution corresponding to the observed number of successes and trials  and price is approximately the expected value of this Beta distribution   Moreover  this correspondence holds regardless of the number of successes or failures  or the temporal order of their occurrence  A kind of collective Bayesianity emerges from the interactions of the group  We also find empirically that  even if not all agents are Kelly bettors  among those that are  wealth is still redistributed according to Bayes rule                             where  E t  is the indicator function for the event at period t  and  is the discount factor  Note that      recovers the standard observed frequency    As t grows  this expected value rapidly approaches the observed frequency plotted in Figure                                                     Market dynamics with fractional Kelly agents  In this section  we consider fractional Kelly agents who  as we saw in Section    behave like full Kelly agents with belief p        pm   Figure   a graphs the dynamics of price in an economy of     such agents  along with the observed frequency  Over time  the price remains significantly more volatile than the frequency  which converges toward         Below  we characterize the transfer of wealth that precipitates this added volatility  for now concentrate on the price signal itself  Inspecting Figure   a  price changes still exhibit a marked dependence on event outcomes  though at any given period the effect of recent history appears magnified  and the past discounted  as compared with the observed frequency  Working from this intuition  we attempt to fit the data to an appropriately modified measure of frequency  Define the discounted frequency at period n as Pn nt      t    P E t  nt       dn   Pn nt   E t      n   E t    t    t          a    b                                  Figure     a  Price  black line  versus observed frequency  gray line  over     time periods for     agents with Kelly fraction         As the frequency converges to         the price remains volatile   b  Price  black line  versus discounted frequency  gray line   with discount factor          for the same experiment as  a     For example  if you allocate an initial weight of     to your predictions and     to the markets prediction  then the regret guarantee of section     implies that at most half of all wealth is lost                                                                 Figure     a  Wealth wi versus belief pi at period     of the same experiment as Figure   with     agents with Kelly fraction         The observed frequency is        and the solid line is Beta                  The wealth distribution is significantly more evenly dispersed than the corresponding Beta distribution   Figure   b illustrates a very close correlation between discounted frequency  with          hand tuned   and the same price curve of Figure   a  While standard frequency provides a provably good model of price dynamics in an economy of full Kelly agents  discounted frequency     appears a better model for fractional Kelly agents  To explain the close fit to discounted frequency  one might expect that wealth remains dispersedas if the markets composite agent witnesses fewer trials than actually occur  Thats true to an extent  Figure   shows the distribution of wealth after    successes have occurred in     trials  Wealth is significantly more evenly distributed than a Beta distribution with parameters      and       also shown  However  the stretched distribution cant be modeled precisely as another  less informed Beta distribution       LEARNING THE KELLY FRACTION  In theory  a rational agent playing against rational opponents should set their Kelly fraction to       since  in a rational expectations equilibrium      the market price is by definition at least as informative as any agents belief  This is the crux of the no trade theorems      Despite the theory      people do agree to disagree in practice and  simply put  trade happens  Still  placing substantial weight on the market price is often prudent  For example  in an online prediction contest called ProbabilitySports        of participants were outperformed by the unweighted average predictor  a typical result   In this light  fractional Kelly can be seen as an experts algorithm     with two experts  yourself and the market  We propose dynamically updating  according to standard experts algorithm logic  When youre right  you increase  appropriately  when youre wrong  you decrease   This gives a long term procedure for updating  that guarantees   You wont do too much worse than the market  which by definition earns     You wont do too much worse than Kelly betting using your original prior p    http   www overcomingbias com         how and when to html  DISCUSSION  Weve shown something intuitively appealing here  selfinterested agents with log wealth utility create markets which learn to have small regret according to log loss  There are two distinct logs in this statement  and its appealing to consider what happens when we vary these  When agents have some utility other than log wealth utility  can we alter the structure of a market so that the market dynamics make the market price have low log loss regret  And similarly if we care about some other losssuch as squared loss      loss  or a quantile loss  can we craft a marketplace such that log wealth utility agents achieve small regret with respect to these other losses  What happens in a market without Kelly bettors  This cant be described in general  although a couple special cases are relevant  When all agents have constant absolute risk aversion  the market computes a weighted geometric average of beliefs               When one of the bettors acts according to Kelly and the others in some more irrational fashion  In this case  the basic Kelly guarantee implies that the Kelly bettor will come to dominate non Kelly bettors with equivalent or worse log loss  If non Kelly agents have a better log loss  the behavior can vary  possibly imposing greater regret on the marketplace if the Kelly bettor accrues the wealth despite a worse prediction record  For this reason  it may be desirable to make Kelly betting an explicit option in prediction markets        
 We improve learning to search approaches to structured prediction in two ways  First  we show that the search space can be defined by an arbitrary imperative program  reducing the number of lines of code required to develop new structured prediction tasks by orders of magnitude  Second  we make structured prediction orders of magnitude faster through various algorithmic improvements      Introduction  In structured prediction problems  the goal is creating a good set of joint predictions  As an example  consider recognizing a handwritten word where each character might be recognized in turn to understand the word  Here  it is commonly observed that exposing information from related predictions  i e  adjacent letters  aids individual predictions  Furthermore  optimizing a joint loss function can improve the gracefulness of error recovery  Despite this  it is empirically common to build independent predictors in settings where structured prediction naturally applies  Why  Because independent predictors are much simpler  easier and faster to train  Our primary goal is to make structured prediction algorithms as easy and fast as possible to both program and compute  A new programming abstraction  together with several algorithmic pearls  radically reduce the complexity of programming and the running time of our solution     We enable structured prediction as a library which has a function PREDICT      returning predictions  The PREDICT      interface is the minimal complexity approach to producing a structured prediction  Surprisingly  this single library interface is sufficient for both testing and training  when augmented to include label advice from a training set  This means that a developer need only code desired test time behavior and gets training for free     Although the PREDICT      interface is the same as the interface for an online learning algorithm  the structured prediction setting commonly differs in two critical ways  First  the loss may not be simple     loss over subproblems  For optimization of a joint loss  we add a LOSS      function which allows the declaration of an arbitrary loss for the joint set of predictions  The second difference is that predictions are commonly used as features for other predictions  This can be handled either implicitly or explicitly  but the algorithm is guaranteed to work either way  Here PREDICT      and LOSS      enable a concise specification of structured prediction problems  Basic sequence labeling as shown in algorithm   is the easiest possible structured prediction problem  so it forms a good use case  The algorithm takes as input a sequence of examples  consider features of handwritten digits in words   and predicts the meaning of each element in turn  This is a specific case of sequential decision making  in which the ith prediction may depend on previous predictions  In this example  we make use of the librarys support for implicit feature based dependence on previous predictions      Algorithm   S EQUENTIAL  RUN examples     for i     to LEN  examples  do    prediction  PREDICT examples i   examples i  label     make a prediction on the ith example    if output good then    output     prediction    if we should generate output  append our prediction    end if    end for  The use of this function for decoding is clear  but how can the PREDICT      interface be effective  There are two challenges to overcome in creating a viable system     Given the available information  are there well founded structured prediction algorithms  For Conditional Random Fields  Lafferty et al         and structured SVMs  Taskar et al         Tsochantaridis et al          the answer is no  because we have not specified the conditional independence structure of the system of predicted variables  Instead  we use a system that implements search based structured prediction methods such as Searn  Daume III et al         or DAgger  Ross et al          These have formal correctness guarantees which differ qualitatively from the conditional log loss guarantees of CRFs  For example  given a low regret cost sensitive classification algorithm  Searn guarantees competition according to LOSS      with an oracle policy and local optimality w r t  one step deviations from the learned policy  We discuss how these work below     A sequential program has only one execution stack  which is used by the decoding algorithm above  This conflicts because the learning algorithm would naturally also use the stack  We refactor the learning algorithm into a state machine which runs before the RUN function is called and after the various library calls are made  In essence  RUN is invoked many times with different example sequences and different versions of PREDICT      so as to find a version of PREDICT      with a small LOSS       Given this high level design  the remaining challenge is computational  How do we efficiently and effectively find a PREDICT      which achieves a small LOSS           Learning to Search  A discrete search space is defined by states s  S and a mapping m   S   S defining the set of valid next states  One of the states is a unique start state a while some of the others are end states s  E  A loss function l s  is defined for any end state s  E on the training set  We are interested in algorithms which learn the transition function f   Xs  S which uses the features of an input state  Xs   to choose a next state so as to minimize the loss l on a heldout test set  Two canonical algorithms to solve this problem are Searn  Daume III et al         and DAgger  Ross et al         which we review next  Searn uses some oracle transition function f  which is defined on the training set  but not on the heldout test set  As searn operates it learns a sequence of transition functions f    f          fn where f    f  and fn is entirely learned  At each iteration i           n   Searn uses fi  to generate a set of cost sensitive examples  A cost sensitive example is defined using local features  features which express previous predictions  and a set of costs defined for each possible next state  The costs are derived by rollouts  for each s   m s   the transition function is applied until an end state s  E is observed and a loss l s  is computed  This vector of losses  one for each s   m s   forms the vector of costs  Together with local features it is fed to the cost sensitive learning algorithm  The cost sensitive learning algorithm generates a classifier ci   Xs  S  then a new policy fi        fi    ci is defined using stochastic interpolation  In essence  with probability  ci is used to define the transition while with probability     fi  is used to define the transition matrix  Since the probability of calling f  decreases exponentially  a fully learned policy is quickly found  DAgger differs from Searn in two computationally helpful ways  it mixes datasets rather than policies and uses a loss function l  defined on all states  so rollouts are not required  When a loss is only defined for end states  a DAgger style algorithm can operate with rollouts      Algorithm   TDOLR X     s  a    while s   E do    Compute Xs from X and s    s  O Xs      end while    return L OSS  s   The rollout versions of the previous algorithms require O t  knp  where t is the average end state depth  i e  sequence length   k    m s   is the number of next states  i e  branching factor   n is the number of distinct searches  i e  sequences   and p is the number of data passes  Three computational tricks  online learning  Collins        Bottou         memoization  and rollout collapse allow the computational complexity to be reduced to O tkn   similar to independent prediction  For example  we can train a part of speech tagger     times faster than CRF    Kudo        which is unsurprising since Viterbi decoding in a CRF is O tk      Surpringly we can do it with   lines of user code  versus almost       We show that learning to search can be implemented with the library interface in section    This provides a radical reduction in the coding complexity of solving new structured prediction problems as discussed  We also radically reduce the computational complexity as discussed next in section    then conduct experiments in section        System Equivalences  Here we show the equivalence of a class of programs and search spaces  The practical implication of this equivalence is that instead of specifying a search space  we can specify a program  which can radically reduce the programming complexity of structured prediction  Search spaces are defined in the introduction  so we must first define the set of programs that we consider  Terminal Discrete Oracle Loss Reporting  TDOLR  programs     Always terminate     Takes as input any relevant feature information X     Make zero or more calls to an oracle O   X    Y which provides a discrete outcome     Report a loss L on termination  To show equivalence  we prove a theorem  This theorem holds for the case where the number of choices is fixed in a search space  and  hence  m s  is implicitly defined   Theorem    For every TDOLR program there exist an equivalent search space and for every search space there exists an equivalent TDOLR program  The practical implication of this theorem is that instead of speicfying search spaces  we can specify a TDOLR program  such as algorithm     and apply any learning to search algorithm such as Searn  DAgger  or variants thereof  Proof  A search space is defined by  a  E  S  l   We show there is a TDOLR program which can simulate the search space in algorithm    This algorithm does a straightforward execution of the search space  followed by reporting of the loss on termination  This completes the second claim  For the first claim  we need to define   a  E  S  l  given a TDOLR program such that the search space can simulate the TDOLR program  At any point in the execution of TDOLR  we define an equivalent state s    O X          O Xn    where n is the number of calls to the oracle  We define a as the sequence of zero length  and we define E as the set of states after which TDOLR terminates  For each s  E we define l s  as the loss reported on termination  This search space manifestly outputs the same loss as the TDOLR program       Algorithm   L EARN X     T       ex        Define P REDICT  x  y       ex   T   x  return fi  x  y       Define S NAPSHOT          R ECORD S NAPSHOT          RUN  X     for t      to T do    losses        for a      to M ex t     do  return a  if t   t     Define P REDICT         return fi       if t    t    J UMP T O t    if t   t  T RY FAST F ORWARD      if t   t      Define S NAPSHOT         no op if t   t      Define L OSS val       losses a       val       RUN X      end for     Online update with cost sensitive example  ex t     losses      end for     Imperative Structured Prediction  The full learning algorithm  for a single structured input  X  is depicted in Algorithm    In lines     an initialization pass of RUN is executed  RUN can generally be any TDOLR program as discussed in appendix    with a specific example being algorithm    In this pass  predictions are made according to the current policy  fi   and every time S NAPSHOT is called  the results are memoized for future use  on the current example   Furthermore  the examples  feature vectors  encountered during prediction are stored in ex  indexed by their position in the sequence  T   The algorithm then initiates one step deviations from this initial trajectory  For every time step   line     we generate a single cost sensitive classification example  its features are ex t     and there are M ex t     possible labels   actions   For each action  line     we compute the cost of that action  To do so  we execute RUN again  line     with a tweaked P REDICT that at a particular time step  t    simply returns the perturbed action a    Finally  the L OSS function simply accumulates the loss for the query action  Finally  a cost sensitive classification is generated  line     and fed into an online learning algorithm  When the learning to search algorithm is Searn  this implies a straightforward update of the next policy  The situation with online DAgger is more subtlein essence a dependence on the reference policy must be preserved for many updates to achieve good performance  We do this using policy interpolation  as in Searn  between the reference policy and learned policy  Without any speed enhancements  each execution of RUN takes O t  time  and we execute it tk     times  yielding an overall complexity of O kt    per structured example  For comparison  structured SVMs or CRFs with first order Markov dependencies run in O k   t  time  To improve this running time  we make two optimizations using the idea of S NAPSHOTs  Together  they reduce the overall runtime to O kt   when paths collapse frequently  this is tested empirically in Section       These optimizations take advantage of the fact that most predictions only depend on a small subset of previous predictions  not all of them  In particular  if the ith prediction only depends on the i   st prediction  then there are at most tk unique predictions ever made   This is what enables dynamic programming for sequences  the Viterbi algorithm   We capitalize on this observation in a more generic way  memoization  A program is allowed to S NAPSHOT its state before making a prediction  Because the S NAPSHOT encapsulates its entire state  we can efficiently store that state together with relevant statistics in a hash table      We use tied randomness  Ng and Jordan        to ensure that for any time step  the same policy is called            Optimization    JumpTo  In Algorithm    suppose that when we execute RUN on line     we have t    T     Naively  one must execute T    P REDICTs in order to reach the desired state at which we vary a    This is inefficient  Instead  assuming that RUN recorded a snapshot at time T    during the initialization  line     we simply restore that stored state the first time S NAPSHOT is called  the t   t  condition in line     Even when we cannot restore the state precisely to T     for instance  perhaps the most recent snapshot was at T      we can additionally memoize the previous results of P REDICT and regurgitate those predictions  This alone saves O td  time  where d is the time to make a prediction  Correctness  In line     the learned policy changes  For policy mixing algorithms  like Searn   this is fine and correctness is guaranteed  However  for data mixing algorithms  like DAgger   this potentially changes fi   implying the memoized predictions may no longer be up to date so the recorded snapshots may no longer be accurate  Thus  for DAgger like algorithms  this optimization is okay if the policy does not change much  We evaluate this empirically in Section      The next section has the same correctness properties       Optimization    TryFastForward  The second optimization is fast forwarding to the end of the sequence using T RY FAST F ORWARD  For example  suppose t       After perturbing the action at time point   we have t      Every time S NAPSHOT is called  the snapshotted data might exactly match a previous snapshot  Suppose at t     it does not  because the perturbation at t     cascaded and changed the prediction at t      But perhaps at t     there is a perfect match  paths have collapsed   We remember that a match has occurred and then at t      we can fast forward to t   T because all subsequent predictions are identical  This intuitive explanation is correct  except for accumulating LOSS       If LOSS      is only declared at the end of RUN  then we must execute T  t  time steps making  possibly memoized  predictions  However  for many problems  it is possible to declare loss early as with Hamming loss    number of incorrect predictions   There is no need to wait until the end of the sequence to declare a persequence loss  one can declare it after every prediction  and have the total loss accumulate  hence the    on line      We generalize this notion slightly to that of a history independent loss  Definition    History independent loss   A loss function is history independent at state s  if  for any final state e reachable from s    and for any sequence s  s  s        si   e  it holds that L OSS e    A s      B s  s        si    where B does not depend on any state before s    For example  Hamming loss is history independent  A s    corresponds to Hamming loss up to and including s  and B s        si   is the Hamming loss after s     When the loss function being optimized is history independent  we allow LOSS      to be declared early  allowing an additional S NAPSHOT optimization  In the previous example  at time t     the snapshot matched  Suppose that at this time  a total loss of   had been accumulated  this corresponds to A          Then at time t     we can immediately jump to the end of the sequence t  T   provided that weve memoized the total loss incurred from t     to t   T on this trajectory  this corresponds to B          which may be    The total cost for this a  perturbation is then                 Overall Complexity  Suppose that the cost of calling the policy is d   Then the complexity of the unoptimized learning function is O t  kd   By adding the memoization optimizations only  and assuming paths collapse after a constant number of steps  this drops to O t  k tkd    The first term is from retrieving memoized predictions  the second from executing the policy a constant number of times for each perturbed sequence   Adding the S NAPSHOT restoration in addition to the memoization  the complexity drops   Any loss function that decomposes over structure  as required by structured SVMs  is guaranteed to also be history independent  the reverse is not true  Furthermore  when structured SVMs are run with a nondecomposible loss function  their runtime becomes exponential in t  When our approach is used with a loss function thats not history independent  our runtime increases by a factor of t    Because the policy is a multiclass classifier  d might hide a factor of k or log k       POS NER  NNP  NNP    CD NNS  JJ   MD  VB  DT  NN  IN DT  JJ  NN  Pierre Vinken      years old   will join the board as a nonexecutive director       LOC ORG PER z                z z Germany s rep to the European Union s committee Werner Zwingmann said        Figure    Example inputs  below  black  and desired outputs  above  blue  for part of speech tagging task and named entity recognition task  further to O tkd   In comparison  a first order CRF or structured SVM for sequence labeling has a complexity of O tk   f    where f is the number of features and d  f k or  f log k depending on the underlying classifier used      Experimental Results  We conduct two experiments based on variants of the sequence labeling problem  Algorithm     The first is a pure sequence labeling problem  Part of Speech tagging based on data form the Wall Street Journal portion of the Penn Treebank  The second is a sequence chunking problem  named entity recognition using data from the CoNLL      dataset  See Figure   for example inputs and outputs for these two tasks  We use the following freely available systems algorithms as points of comparison  CRF   The popular CRF   toolkit  Kudo        for conditional random fields  Lafferty et al           which implements both L BFGS optimization for CRFs  Nash and Nocedal        Malouf        as well as structured MIRA  Crammer and Singer        McDonald et al          CRF SGD A stochastic gradient descent conditional random field package  Bottou         Structured Perceptron An implementation of the structured perceptron  Collins        due to  Chang et al          Structured SVM The cutting plane implementation  Joachims et al         of the structured SVMs  Tsochantaridis et al         for HMM problems  Structured SVM  DEMI DCD  A multicore algorithm for optimizing structured SVMs called DEcoupled Model update and Inference with Dual Coordinate Descent  VW Search Our approach is implemented in the Vowpal Wabbit  Langford et al         toolkit on top of a cost sensitive classifier  Beygelzimer et al         that reduces to regression trained with an online rule incorporating AdaGrad  Duchi et al          per feature normalized updates  Ross et al          and importance invariant updates  Karampatziakis and Langford         VW Classification An unstructured baseline that predicts each label independently  using oneagainst all multiclass classification  Beygelzimer et al          These approaches vary both objective function  CRF  MIRA  structured SVM  learning to search  and optimization approach  L BFGS  cutting plane  stochastic gradient descent  AdaGrad   All implementations are in C C    except for the structured perceptron and DEMI DCD  Java        Methodology  Comparing different systems is challenging because one wishes to hold constant as many variables as possible  In particular  we want to control for both features and hyperparameters  In general  if a methodological decision cannot be made fairly  we made it in favor of competing approaches  To control for features  we use the built in feature template approach of CRF    duplicated in CRF SGD  to generate features  The other approaches  Structured SVM  VW Search and VW Classification  all use the features generated  offline  by CRF    For each task  we tested six feature templates and picked the one with best development performance using CRF    The templates included neighboring words and  in the case of NER  neighboring POS tags  However  because VW Search is also     POS NER  Sents   k   k  Toks    k    k  Training Labels Features          k        k  Unique Fts    k    k  Heldout Sents Toks    k    k    k   k  Test Sents Toks    k    k    k   k  Table    Basic statistics about the data sets used for part of speech  POS  tagging and named entity recognition  NER    able to generate features from its own templates  we also provide results for VW Search  own fts  in which it uses its own  internal  feature template generation  which were tuned to maximize its heldout performance on the most time consuming run    passes  and include neighboring words  and POS tags  for NER  and word prefixes suffixes   In all cases we use first order Markov dependencies  which lessens the speed advantage of search based structured prediction  To control for hyperparameters  we first separated each systems hyperparameters into two sets      those that affect termination condition and     those that otherwise affect model performance  When available  we tune hyperparameters for  a  learning rate and  b  regularization strength    Additionally  we vary the termination conditions to sweep across different amounts of time spent training  For each termination condition  we can compute results using either the default hyperparameters or the tuned hyperparameters that achieved best performance on heldout data  We report both conditions to give a sense of how sensitive each approach is to the setting of hyperparameters  the amount of hyperparameter tuning directly affects effective training time   One final confounding issue is that of parallelization  Of the baseline approaches  only CRF   supports parallelization via multiple threads at training time  In our reported results  CRF  s time is the total CPU time  i e   effectively using only one thread   Experimentally  we found that wall clock time could be decreased by a factor of     by using   threads  a factor of   using   threads  and a  plateaued  factor of   using   threads  This should be kept in mind when interpreting results  DEMI DCD  for structured SVMs  also must use multiple threads  To be as fair as possible  we used   threads  Likewise  it can be sped up more using more threads  Chang et al          VW  Search and Classification  can also easily be parallelized using AllReduce  Agarwal et al          We do not conduct experiments with this option here because none of our training times warranted parallelization  a few minutes to train  max         Task Specifics  Part of speech tagging for English is based on the Penn Treebank tagset that includes    discrete labels for different parts of speech  The overall accuracy reported is Hamming accuracy  number of tokens tagged correctly   This is a pure sequence labeling task  We use    k tokens  words  of training data and approximately   k tokens of heldout data and test data  The CRF   templates generate    k unique features for the training data  additional statistics are in Table    Named entity recognition for English is based on the CoNLL      dataset that includes four entity types  Person  Organization  Location and Miscellaneous  We report accuracy as macro averaged F measure over the correct identification and labeling of these entity spans  the standard evaluation metric   In order to cast this chunking task as a sequence labeling task  we use the standard BeginIn Out  BIO  encoding  though some results suggest other encodings may be preferable  Ratinov and Roth         The example sentence from Figure   in this encoding is  LOC ORG PER z      z      z      Germany s rep to the European Union s committee Werner Zwingmann said       B LOC  O O  O  O  B ORG  I ORG O  O  B PER  I PER  O  In our system  the only change made to the sequence labeling algorithm  Algorithm    is that I x may only follow B x or I x  We still optimize Hamming loss because macro averaged F measure does not decompose over individual sentences      Part of speech tagging  tuned hps                                  Named entity recognition  tuned hps                     m                                                    VW Search VW Search  own fts       VW Classification CRF SGD CRF   Str  Perceptron SVM   m   m  hStructured Str SVM  DEMI DCD              Training Time  minutes   F score  per entity   Accuracy  per tag                                                                      s         m       Training Time  minutes     m       Figure    Training time versus evaluation accuracy for part of speech tagging  left  and named entity recognition  right   X axis is in log scale  Different points correspond to different termination criteria for training  Both figures use hyperparameters that were tuned  for accuracy  on the heldout data   Note  lines are curved due to log scale x axis         Efficiency versus Accuracy  In Figure    we show trade offs between training time  x axis  log scaled  and prediction accuracy  y axis  for the six systems described previously  The left figure is for part of speech tagging     k training tokens  and the right figure is for named entity recognition     k training tokens   For POS tagging  the independent classifier is by far the fastest  trains in less than one minute  but its performance peaks at     accuracy  Three other approaches are in roughly the same time accuracy tradeoff  VW Search  VW Search  own fts  and Structured Perceptron  All three can achieve very good prediction accuracies in just a few minutes of training  CRF SGD takes about twice as long  DEMI DCD eventually achieves the same accuracy  but it takes a half hour  CRF   is not competitive  taking over five hours to even do as well as VW Classification   Structured SVM  cutting plane implementation  looks promising  but runs out of memory before achieving competitive performance  likely due to too many constraints   For NER the story is a bit different  The independent classifiers are quite fast  a few seconds to train  but are far from being competitive    Here  the two variants of VW Search totally dominate     In this case  Structured Perceptron  which did quite well on POS tagging  is no longer competitive and is essentially dominated by CRF SGD  The only system coming close to VW Searchs performance is DEMI DCD  although its performance flattens out after a few minutes   To achieve the results in Figure   required fairly extensive hyperparameter tuning  on the order of    to     different runs for each system   To see the effects of hyperparameter tuning  we also ran each system with the built in hyperparameter options   The trends in the runs with default hyperparame   The exact templates used are provided in the supplementary materials  Precise details of hyperparameters tuned and their ranges is in the supplementary materials    When evaluating F measure for a system that may produce incoherent tag sequences  like O I LOC we replace any malpositioned I x with B x  of all heuristics we tried  this worked best    We verified the prediction performance with great care hereit is the first time we have observed learning to search approaches significantly exceeding the prediction performance of other structured prediction techniques when the feature information available is precisely the same    We also tried giving CRF SGD the features computed by VW Search  own fts  on both POS and NER  On POS  its accuracy improved to     on par with VW Search  own fts with essentially the same speed  On NER its performance decreased  For both tasks  clearly features matter  But which features matter is a function of the approach being taken    The only exceptions is Structured SVMs  which do not have a default C value  we used C       because that setting won most often across all experiments          Part of speech tagging  default hps                                                                                          F score  per entity         Accuracy  per tag   Named entity recognition  default hps                m    m  h                            Training Time  minutes                                m                   VW Search VW Search  own fts  VW Classification CRF SGD CRF   Str  Perceptron      Structured SVM   s  m Str SVM  DEMI DCD           Training Time  minutes     m       Figure    Training time versus evaluation accuracy for POS tagging  left  and NER  right   X axis is in log scale  Different points correspond to different termination criteria for training  Both figures use default hyperparameters   Prediction  test time  Speed POS                               NER             VW Search VW Search  own fts  CRF SGD CRF   Str  Perceptron Structured SVM Str  SVM  DEMI DCD                                         Thousands of Tokens per Second Figure    Comparison of test time efficiency of the different approaches in thousands of tokens per second  For NER  this ranges from  k tokens sec  DEMI DCD  to over a quarter million tokens sec  These numbers include feature computation time only for the two CRF approaches  ters  Figure    show similar behavior to those with tuned  though some of the competing approaches suffer significantly in prediction performance  Structured Perceptron has no hyperparameters      Test time Prediction Performance  In addition to training time  one might care about test time behavior  On NER  prediction times varied from    k tokens second  DEMI DCD and Structured Perceptron to around   k  CRF SGD and Structured SVM  to    k  CRF    to    k  VW  own fts   and    k  VW   Although CRF SGD and Structured Perceptron fared well in terms of training time  their test time behavior is suboptimal  When looking at POS tagging  the effect of the O k     dependence on the size of the label set further increased the  relative  advantage of VW Search over the alternatives  Figure   shows the speed at which the different systems can make predictions on raw text  Structured SVMs and friends  DEMI DCD and Structured Perceptron  are by far the slowest  NER    k tokens per second   followed closely by CRF SGD  NER    k t s   This is disappointing because CRF SGD performed very well in terms of training efficiency  CRF   achieves respectable test time efficiency      NER  almost    k t s     VW Search using CRF  s features is the fastest  NER     k t s  but  like Structured SVM  this is a bit misleading because it requires feature computation from CRF   to be run as a preprocessor  A fairer comparison is VW Search  own fts   which runs on  nearly  raw text and achieves a speed of    k t s for NER  One thing that is particularly obvious comparing the prediction speed for VW Search against the other three approaches is the effect of the size of the label space  When the number of labels increases from    NER  to     POS   the speed of VW Search is about halved  For the others  it is cut down by as much as a factor of    This is a direct complexity argument  Prediction time for VW Search is O tkf   versus O tk   f   for all other approaches  Overall  we found our approach to achieve comparable or higher accuracy in as little or less time  both with tuned hyperparameters and default hyperparameters  The closest competitor for POS tagging was the Structured Perceptron  but that did poorly on NER   the closest competitor for NER was CRF SGD  but that was several times slower on POS tagging        Empirical evaluation of path collapse  In Section    we discussed two approaches for computational improvements  First  memoization  avoid re predicting on the same input multiple times  which is fully general   Second  snapshot restoration  to jump to an arbitrary desired position in the search space  which requires a historyindependent loss function   Both are effectively only when paths collapse frequently  The effect of different optimizations  none  memoization alone  or memoization combined with snapshot restoration  is shown below  Columns are internal loss  F measure on heldout data  number of predictions made  time to train and one standard deviation of training time over   runs  Optimization All Memoization None  heldout loss                    heldout F score                      training predictions                                 training time     m          m          m       The above results show the effect of these optimizations on the best NER system we trained  which achieved a test F score of       in     m  In this table  we can see that memoization alone reduces the number of predictions made by over      with only a very small increase of       in loss on the heldout data  the loss reported here in the internal average per sequence Hamming loss  rather than F measure   Recall that this optimization is only provably correct in Searn mode  not DAgger mode as run here  The memoization reduces overall runtime by about     because not all time is being spent making predictions  When the second optimization  snapshot jumps  is enabled  the total number of predictions drops imperceptibly  but the runtime improves by another      yielding a total improvement of about     over the baseline  Note that the loss here actually goes down slightly  perhaps due to a slightly less noisy cost function      Relation to Probabilistic Programming  Probabilistic programming  Gordon et al         has been an active area of research for the past decade or more  While our approach bears a family resemblace to the idea of probabilistic programming  it differs in two key ways  First  we have not designed a new programming language  Instead we have a three function library  This is advantageous because it makes adoption easier  Moreover  our library is in C C    which makes integration into existing code bases  relatively  easy  Second  the abstract we focus on is that of prediction  In contrast  the typical abstraction for probabilistic programming is distributions  We believe that prediction is a more natural abstraction for a lay programmer to think about than probability distributions  The closest work to ours is Factorie  McCallum et al          Factorie is a domain specific language embedded in Scala  and is essentially an embedded language for writing factor graphs  It compiles    This suggests gains are possible by combining mizer   CRF  s      decoder and I O system with  CRF SGDs  opti    them into Scala  which in turn produces JVM code that can be run reasonably efficiently  Nonetheless  as far as we are aware  Factorie based implementations of simple tasks like sequence labeling are still less efficient than systems like CRF SGD  Factorie  more than other probabilistic programming languages we are aware of  acts more like a library than a language  though its abstraction is still distributions  more precisely  factor graphs   Another approach which takes the approach of formulating a library is Infer NET  from Minka et al          Infer NET is a library for constructing probabilistic graphical models in a  NET programming framework  It supports approximate inference methods for things like variational inference and message passing  In the same spirit as Factorie of having a concise programmic method of specifying factors in a Markov network are  Markov Logic Networks  MNLs  due to Richardson and Domingos        and Probabilistic Soft Logic  PSL  due to Kimmig et al          Although neither of these was derived specifically from the perspective of formulating factors in a conditional random field  or hinge loss Markov network   that is the net result  Neither of these is an embedded language  one must write declarative code and provide data in an appropriate format  which makes it somewhat difficult to use in complex systems  BLOG  Milch et al         falls in the same category  though with a very different focus  Similarly  Dyna  Eisner et al         is a related declarative language for specifying probabilistic dynamic programs which can be compiled into C    and then used as library code inside another C   program   All of these example have picked particular aspects of the probabilistic modeling framework to focus on  Beyond these examples  there are several approaches that essentially reinvent an existing programming language to support probabilistic reasoning at the first order level  IBAL  Pfeffer        derives from OCaml  Church  Goodman et al         derives from LISP  IBAL uses a  highly optimized  form of variable elimination for inference that takes strong advantage of the structure of the program  Church uses MCMC techniques  coupled with a different type of structural reasoning to improve efficiency  It is worth noting that most of these approaches have a different goal than we have  Our goal is to build a framework that allows a developer to solve a quite general  but still specific type of problem  learning to solve sequencial decision making problems  learning to search   The goal of  most  probabilistic programming languages is to provide a flexible framework for specifying graphical models and performing inference in those models  While these two goals are similar  they are different enough that the minimalistic library approach we have provided is likely to be insufficient for general graphical model inference      Discussion  We have shown a new abstraction for a structured prediction library that yields state of the art or better prediction accuracies on two tasks  with runtimes up to two orders of magnitude faster than competing approaches  Moreover  we achieve this with minimal programming effort on the part of the developer who must implement RUN  Our sequence labeling implementation is   lines of code  compared to  CRF SGD at      LOC  CRF   at     LOC and Structured SVM at     LOC    Somewhat surprisingly  this is all possible through a very simple  three function  library interface which does not require the development of an entirely new programming language  This is highly advantageous as it allows very easy adoption  Moreover  since our library functions in a reduction stack  as base classifiers and reductions for cost sensitive classification improve  so does structured prediction performance   
 Contextual bandit learning is a reinforcement learning problem where the learner repeatedly receives a set of features  context   takes an action and receives a reward based on the action and context  We consider this problem under a realizability assumption  there exists a function in a  known  function class  always capable of predicting the expected reward  given the action and context  Under this assumption  we show three things  We present a new algorithmRegressor Elimination with a regret similar to the agnostic setting  i e  in the absence of realizability assumption   We prove a new lower bound showing no algorithm can achieve superior performance in the worst case even with the realizability assumption  However  we do show that for any set of policies  mapping contexts to actions   there is a distribution over rewards  given context  such that our new algorithm has constant regret unlike the previous approaches     Introduction We are interested in the online contextual bandit setting  where on each round we first see a context x  X   based on which we choose an action a  A  and then observe a reward r  This formalizes several natural scenarios  For example  a common task at major internet engines is to display the best ad from a pool of options given some context such as information about the user  the page visited  the search query issued etc  The action set consists of the candidate ads and the reward is typically binary based on whether the user clicked the displayed ad or not  Another Appearing in Proceedings of the   th International Conference on Artificial Intelligence and Statistics  AISTATS        La Palma  Canary Islands  Volume XX of JMLR  W CP XX  Copyright      by the authors   natural application is the design of clinical trials in the medical domain  In this case  the actions are the treatment options being compared  the context is the patients medical record and reward is based on whether the recommended treatment is a success or not  Our goal in this setting is to compete with a particular set of policies  which are deterministic rules specifying which action to choose in each context  We note that this setting includes as special cases the classical K armed bandit problem  Lai and Robbins        and associative reinforcement learning with linear reward functions  Auer        Chu et al          The performance of algorithms in this setting is typically measured by the regret  which is the difference between the cumulative reward of the best policy and the algorithm  For the setting with an arbitrary set of policies  the achieved rep gret guarantee is O  KT ln N    where K is the number of actions  T is the number of rounds  N is the number of policies and  is the probability of failing to achieve the regret  Beygelzimer et al         Dudk et al          While this bound has a desirably small dependence on the parameters T  N   the scaling with respect to K is often too big to be meaningful  For instance  the number of ads under consideration can be huge  and a rapid scaling with the number of alternatives in a clinical trial is clearly undesirable  Unfortunately  the dependence on K is unavoidable as proved by existing lower bounds  Auer et al          Large literature on linear bandits manages to avoid this dependence on K by making additional assumptions  For example  Auer        and Chu et al         consider the setting where the context x consists of feature vectors xa  Rd describing each action  and the expected reward function  given a context x and action a  has the form wT xa for some fixed vector w  Rd   Dani et al         consider a continuous action space with a  Rd   without contexts  with a linear expected reward wT a  which is generalized by Filippi et al         to  wT a  with a known Lipschitzcontinuous link function   A striking aspect of the linear and generalized linear setting is that while the regret   Contextual Bandit Learning with Predictable Rewards  grows rapidly with the dimension d  it grows either only gently with the number of actions K  poly logarithmic for Auer         or is independent of K  Dani et al         Filippi et al          In this paper  we investigate whether a weaker dependence on the number of actions is possible in more general settings  Specifically  we omit the linearity assumption while keeping the realizabilityi e   we still assume that the expected reward can be perfectly modeled  but do not require this to be a linear or a generalized linear model  We consider an arbitrary class F of functions f    X   A          that map a context and an action to a real number  We interpret f  x  a  as a predicted expected reward of the action a on context x and refer to functions in F as regressors  For example  in display advertising  the context is a vector of features derived from the text and metadata of the webpage and information about the user  The action corresponds to the ad  also described by a set of features  Additional features might be used to model interaction between the ad and the context  A typical regressor for this problem is a generalized linear model with a logistic link  modeling the probability of a click  The set of regressors F induces a natural set of policies F containing maps f   X  A defined as f  x    argmaxa f  x  a   We make the assumption that the expected reward for a context x and action a equals f   x  a  for some unknown function f   F   The question we address in this paper is  Does this realizability assumption allow us to learn faster  We show that for an arbitrary function  class  the answer to the above question is no  The K dependence in regret is in general unavoidable even with the realizability assumption  Thus  the structure of linearity or controlled non linearity was quite important in the past works  Given this answer  a natural question is whether it is at least possible to do better in various special cases  To answer this  we create a new natural algorithm  Regressor Elimination  RE   which takes advantage of realizability  Structurally  the algorithm is similar to Policy Elimination  PE  of Dudk et al          designed for the agnostic case  i e  the general case without realizability assumption   While PE proceeds by eliminating poorly performing policies  RE proceeds by eliminating poorly predicting regressors  However  realizability assumption allows much more aggressive elimination strategy  different from the strategy used in PE  The analysis of this elimination strategy is the key technical contribution of this paper  Thepgeneral regret guarantee for Regressor Elimination is O  KT ln N T      similar to the agnostic case  However  we also show that for all sets of policies  there exists a set of regressors F such that    F and the regret of Regressor Elimination is O ln N     i e   independent of the number of rounds and actions  At the first sight  this  seems to contradict our worst case lower bound  This apparent paradox is due to the fact that the same set of policies can be generated by two very different sets of regressors  Some regressor sets allow better discrimination of the true reward function  whereas some regressor sets will lead to the worst case guarantee  The remainder of the paper is organized as follows  In the next section we formalize our setting and assumptions  Section   provides our algorithm which is analyzed in Section    In Section   we present the worst case lower bound  and in Section    we show an improved dependence on K in favorable cases  Our algorithm assumes the exact knowledge of the distribution over contexts  but not over rewards   In Section   we sketch how this assumption can be removed  Another major assumption is the finiteness of the set of regressors F   This assumption is more difficult to remove  as we discuss in Section       Problem Setup We assume that the interaction between the learner and nature happens over T rounds  At each round t  nature picks a context xt  X and a reward function rt   A         sampled i i d  in each round  according to a fixed distribution D x  r   We assume that D x  is known  this assumption is removed in Section     but D r x  is unknown  The learner observes xt   picks an action at  A  and observes the reward for the action rt  at    We are given a function class F   X  A         with  F     N   where  F   is the cardinality of F   We assume that F contains a perfect predictor of the expected reward  Assumption    Realizability   There exists a function f   F such that Er x  r a     f   x  a  for all x  X   a  A  We recall as before that the regressor class F induces the policy class F containing maps f   X  A defined by f  F as f  x    argmaxa f  x  a   The performance of an algorithm is measured by its expected regret relative to the best fixed policy  regretT   sup  T h i X   f  xt   f  xt    f   xt   at      f F t    By definition of f   this is equivalent to regretT    T h i X   f  xt   f   xt    f   xt   at     t      Algorithm Our algorithm  Regressor Elimination  maintains a set of regressors that accurately predict the observed rewards  In each round  it chooses an action that sufficiently explores   Agarwal  Dudk  Kale  Langford and Schapire  among the actions represented in the current set of regressors  Steps      After observing the reward  Step     the inaccurate regressors are eliminated  Step     Sufficient exploration is achieved by solving the convex optimization problem in Step    We construct a distribution Pt over current regressors  and then act by first sampling a regressor f  Pt and then choosing an action according to f   Similarly to the Policy Elimination algorithm of Dudk et al          we seek a distribution Pt such that the inverse probability of choosing an action that agrees with any policy in the current set is in expectation bounded from above  Informally  this guarantees that actions of any of the current policies are chosen with sufficient probabilities  Using this construction we relate the accuracy of regressors to the regret of the algorithm  Lemma       A priori  it is not clear whether the constraint       is even feasible  We prove feasibility by a similar argument as in Dudk et al          see Lemma A   in Appendix A   Compared with Dudk et al         we are able to obtain tighter constraints by doing a more careful analysis  Our elimination step  Step    is significantly tighter than a similar step in Dudk et al          we eliminate regressors according to a very strict O   t  bound on the suboptimality of the least squares error  Under the realizability assumption  this stringent constraint will not discard the optimal regressor accidentally  as we show in the next section  This is the key novel technical contribution of this work  Replacing D x  in the Regressor Elimination algorithm with the empirical distribution over observed contexts is straightforward  as was done in Dudk et al          and is discussed further in Section     Algorithm   Regressor Elimination Input  a set of reward predictors F    f    X   A           distribution D over contexts  confidence parameter   Notation  f  x     argmaxa f  x  a    Pt Rt  f       t t     f  xt   at    rt  at       For F   F   define A F    x      a  A   f  x    a for some f  F         min    K     T    For a distribution P on F   F   define conditional distribution P    x  on A as  w p         sample f  P and return f  x   and w p    return a uniform random a  A F    x   t     N t  log   t   for t                 T   Algorithm  F   F For t                 T      Find distribution Pt on Ft  such that           f  Ft    E   E  A Ft    x         x Pt  f  x  x  x    Observe xt and sample action at from Pt   xt       Observe rt  at       Set        ln   t   Ft   f  Ft    Rt  f     min Rt  f      f Ft  t    Regret Analysis Here we prove an upper bound on the regret of Regressor Elimination  The proved bound is no better than the one for existing agnostic algorithms  This is necessary  as we will see in Section    where we prove a matching lower bound  Theorem      For all sets of regressors F with  F     N and all distributions D x  r   with probability      the p regret of Regressor Elimination is O  KT ln N T       Proof  By Lemma      proved below   in round t if we sample an action by sampling f frompPt and choosing f  xt    then the expected regret is O  K ln N T    t  with probability at least      t    The excess regret for sampling a uniform random action is at most Summing up over all the    T per round  T rounds and taking a union bound  the total exp   pected regret is O KT ln N T    with probability at least      Further  the net regret is a martingale  hence the Azuma Hoeffding inequality with range        applies  So with probability at least     we  p p   have a regret of O KT ln N T      T ln       p   O KT ln N T       Lemma      With probability at least    t N t log   t        t    we have     f   Ft      For any f  Ft   E  r f  x    r f   x      x r  r     K ln   t     t  Proof  Fix an arbitrary function f  F   For every round t  define the random variable Yt    f  xt   at    rt  at       f   xt   at    rt  at       Here  xt is drawn from the unknown data distribution D  rt is drawn from the reward distribution conditioned on xt   and at is drawn from Pt  which is defined conditioned on   Contextual Bandit Learning with Predictable Rewards  the choice of xt and is independent of rt    Note that this random variable is well defined for all functions f  F   not just the ones in Ft   Let Et    and Vart    denote the expectation and variance conditioned on all the randomness up to round t  Using a form of Freedmans inequality from Bartlett et al          see Lemma B    and noting that Yt     we get that with probability at least    t log   t   we have t X  t     Et  Yt     t X  Summing up over all t  t  and using       along with Jensens inequality we get that r    K ln   t     E  r f  x    r f   x     t x r Lemma      Fix a function f  F   Suppose we sample x  r from the data distribution D  and an action a from an arbitrary distribution such that r and a are conditionally independent given x  Define the random variable Y    f  x  a   r a      f   x  a   r a       Yt  t     Then we have  v u t uX   t Vart  Yt   ln   t       ln   t            E  Y     E  f  x  a   f  x  a    From Lemma      we see that Vart  Yt      Et  Yt   so t X  t     Et  Yt     t X  Yt  Var Y      E  Y    x r a      ra     Y    fxa  fxa   fxa   fxa    E  Y     E   fxa  fxa   fxa   fxa   ra     x r a  qP t  x a  X    Z   CX    C     X   C    Z    C     This gives Z    C     Since Z   t Rt  f    Rt  f      we get that  Furthermore  suppose f is also not eliminated and survives in Ft   Then we must have Rt  f    Rt  f       C    t  or in other words  Z    C     Thus   X   C      C     which implies that X       C     and hence         By Lemma     and since Pt is measurable with respect to the past sigma field up to time t     for all t  t we have   E  r f  x    r f   x      K        proving the first part of the lemma  From        noting that  fxa   fxa   ra are between   and    we obtain       fxa  fxa      yielding the second part of the lemma         Var Y    E  Y        E  fxa  fxa   x r a  and so f  is not eliminated in any elimination step and remains in Ft    x r  x a     fxa        Y     fxa  fxa    fxa   fxa   ra        ln   t   t  Et  Yt        ln   t     r x       E  fxa     C     t  By a union bound  with probability at least    t N t log   t   for all f  F and all rounds t  t  we have  t     x r a      fxa   fxa   ra      E E   fxa  fxa x a r x            E  fxa  fxa   fxa   fxa    E  ra    For notational convenience  define X   t    Et  Yt    p Pt Z   t    Yt   and C   ln   t    The above inequality is equivalent to   t X         Hence  we have  t     Rt  f     Rt  f      x r a  Proof  Using shorthands fxa for f  x  a  and ra for r a   we can rearrange the definition of Y as  t     v u t uX Et  Yt   ln   t       ln   t      t  Rt  f     Rt  f      x a  x r a  t     Et  Yt    xt  rt  at  x r a  x r a      E  Y     x r a  Next we show how the random variable Y defined in Lemma     relates to the regret in a single round  Lemma      In the setup of Lemma      assume further that the action a is sampled from a conditional distribution p  x  which satisfies the following constraint  for f    f and f    f           K        E x p f   x  x  Then we have h    i    K E  Y    E r f   x   r f  x  x r  x r a   Agarwal  Dudk  Kale  Langford and Schapire  This lemma is essentially a refined form of theorem     in Beygelzimer and Langford        which analyzes the regression approach to learning in contextual bandit settings  Proof  Throughout  we continue using the shorthand fxa for f  x  a   Given a context x  let a   f  x  and a   f   x   Define the random variable h    i   x   E r f   x   r f  x    fxa   fxa   r x  Note that x    because f  prefers a over a for context x  Also we have fxa  fxa since f prefers a over a for context x  Thus    fxa  fxa   fxa   fxa  x           As in proof of Lemma             E  Y     E  fxa  fxa   a x  r a x         p a x  fxa fxa    p a  x  fxa  fxa      p a x p a  x      p a x    p a  x  x         The last inequality follows by first applying the chain ax    by      ab x   y      ax  by   ab   x   y   a b a b   valid for a  b       and then applying inequality        For convenience  define Qx    p a x p a  x            i e       p a x    p a  x  Qx p a x  p a  x   Now  since p satisfies the constraint       for f    f and f    f    we conclude that                     K           E E E x p a  x  x p a x  x Qx    We now have    p    Qx x E x     E  x x Qx             E E Qx x x Qx x          K E  Y     x r a  where the first inequality follows from the CauchySchwarz inequality and the second from the inequalities       and           Lower bound Here we prove a lower bound showing that the realizability assumption is not enough in general to eliminate a dependence on the number of actions K  The structure of this proof is similar to an earlier lower bound  Auer et al         differing in two ways  it applies to regressors of the sort we consider  and we work N   the number of regressors  into the lower bound  Since for every policy there exists a regressor with argmax on that regressor realizing the policy  this lower bound also applies to policy based algorithms  Theorem      For every N and K such that ln N  ln K  T   and every algorithm A  there exists a function class F of cardinality at most N and a distribution D x  r  for which the realizability assumption holds  but the expected regret p of A is   KT ln N  ln K   Proof  Instead of directly selecting F and D for which the p expected regret of A is   KT ln N  ln K   we create a distribution over instances p  F  D  and show that the expected regret of A is   KT ln N  ln K  when the expectation is taken also over our choice of the instance  This will immediately yield a statement of the theorem  since the algorithm must suffer at least this amount of regret on one of the instances   The proof proceeds via a reduction to the construction used in the lower bound of Theorem     of Auer et al          We will use M different contexts for a suitable number M   To define the regressor class F   we begin with the policy class G consisting of all the K M mappings of the form g   X  A  where X                  M   and A                  K   We require M to be the largest integer such that K M  N   i e   M   ln N  ln K  Each mapping g  G defines a regressor fg  F as follows          o if a   g x  fg  x  a        otherwise  The rewards are generated by picking a function f  F uniformly at random at the beginning  Equivalently  we choose a mapping g that independently maps each context x  X to a random action a  A  and set f   fg   In each round t  a context xt is picked uniformly from X   For any action a  a reward rt  a  is generated as a        Bernoulli trial with probability of   being equal to f  x  a   Now fix a context x  X   We condition on all of the randomness of the algorithm A  the choices of the contexts xt for t                 T   and the values of g x   for x    x  Thus the only randomness left is in the choice of g x  and the realization of the rewards in each round  Let P denote the reward distribution where the rewards of any action a for context x are chosen to be        uniformly at random  the rewards for other contexts x    x are still chosen according to f  x   a   however   and let E denote the expectation under P     Contextual Bandit Learning with Predictable Rewards  Let Tx be the rounds t where the context xt is x  Now fix an action a  A and let Sa be a random variable denoting the number of rounds t  Tx when A chooses at   a  Note that conditioned on g x    a  the random variable Sa counts the number of rounds in Tx that A chooses the optimal action a   for t                 T by taking an expectation  we get the following lower bound on the expected regret of A     X  o       o E  Tx      E  Tx       K xX  We use a corollary of Lemma A   in Auer et al           Note   that    Tx   is distributed as Binomial T    M    Thus  E  Tx     T  M   Furthermore  by Jensens inequality  Corollary      Auer et al          Conditioned on the choices of the contexts xt for t                 T   and the values of g x   for x    x  we have p  E Sa  g x    a   E  Sa      Tx    o  E  Sa     The proof uses the fact that when g x    a  rewards chosen using P are identical to those from the true distribution except for the rounds when A chooses the action a  Thus  if Nx is a random variable that counts the number the rounds in Tx that A chooses the optimal action for x  without conditioning on g x    we have E Nx     E  E Sg x     g x  q h i  E E  Sg x       Tx    o  E  Sg x    g x   r i h      E E  Sg x       Tx    o  E E  Sg x      g x   g x   by Jensens inequality  Now note that        h i X     at   g x   E E  Sg x      E E g x   g x      tTx  X  E   E    at   g x      X  E    g x   tTx     tTx         K         Tx     K  The third equality follows because g x  is independent of the choices of the contexts xt for t                 T   and g x   for x    x  and its distribution is uniform on A  Thus r  Tx    Tx      Tx    o    E Nx    K K Since in the rounds in Tx   Nx   the algorithm A suffers an expected regret of o  the expected regret of A  over all the     rounds in Tx is at least  o Tx    oK  Tx        Note that  this lower bound is independent of the choice of g x   for x    x  Thus  we can remove the conditioning on g x   for x    x and conclude that only conditioned on the choices of the contexts xt for t                 T   the expected regret over   of the algorithm   all the rounds in Tx is at least    o Tx    oK  Tx        Summing up over all x  and removing the conditioning on the choices of the contexts xt  q              E  Tx   E  Tx            T  T     T  T     T     T       M M  M        T    M      as long as M  T   Plugging these bounds in  the lower bound on the expected regret becomes     o          oT  T KM p   Choosing o    KM T   we get that the expected regret of A is lower bounded by p    KM T       KT ln N  ln K       Analysis of nontriviality Since the worst case regret bound of our new algorithm is the same as for agnostic algorithms  a skeptic could conclude that there is no power in the realizability assumption  Here  we show that in some cases  realizability assumption can be very powerful in reducing regret  Theorem      For any algorithm A working with a set of policies  rather than regressors   there exists a set of regressors F and a distribution D satisfying the realizability assumption such that the regret of A using the set F is    T K ln N    but the expected regret  of Regressor Elimination using F is at most O ln N      Proof  Let F  be the set of functions and D the data distribution that achieve the lower bound of Theorem     for the algorithm A  Using Lemma      see below   there exists a set of functions F such that F   F  and the expected regret   of Regressor Elimination using F is at most O ln N     This set of functions F and distribution D satisfy the requirements of the theorem   Lemma      For any distribution D and a set of policies  containing the optimal policy  there exists a set of functions F satisfying the realizability assumption  such that    F and the regret   of regressor elimination using F is at most O ln N       Agarwal  Dudk  Kale  Langford and Schapire  Proof  The idea is to build a set of functions F such that    F   and for the optimal policy   the corresponding function f  exactly gives the expected rewards for each context x and a  but for any other policy  the corresponding function f gives a terrible estimate  allowing regressor elimination to eliminate them quickly  The construction is as follows  For     we define the function f  as f   x  a    Ex r  r a    By optimality of     f        For every other policy  we construct an f such that    f but for which f  x  a  is a very bad estimate of Ex r  r a   for all actions a  Fix x and consider two cases  the first is that Er x  r  x           and the other is that Er x  r  x           In the first case  we let f  x   x           In the second case we let f  x   x          Now consider each other action a in turn  If Er x  r a           then we let f  x  a        and if Er x  r a          we let f  x  a          The regressor elimination algorithm eliminates regressor with a too large squared loss regret  Now fix any policy         and the corresponding f   define  as in the proof of Lemma      the random variable Yt    f  xt   at    rt  at       f   xt   at    rt  at       Note that     Et  Yt     E   f  xt   at    f  xt   at          xt  at               since for all  x  a    f  x  a f   x  a        by construction  This shows that the expected regret is significant   Now suppose f is not eliminated and remains in Ft   Then by equation     we get  t X t  Et  Yt        ln   t        t     The above bound holds with probability    t N t log   t  uniformly for all f  Ft   Using the choice of t     N t  log   t   we note that the bound fails to hold when t       ln N    Thus  within     ln N   rounds all suboptimal regressors are eliminated  and the algorithm suffers no regret thereafter  Since the rewards are bounded in         the total regret in the first     ln N   rounds can be at most     ln N    giving us the desired bound     Removing the dependence on D While Algorithm   is conceptually simple and enjoys nice theoretical guarantees  it has a serious drawback that it depends on the distribution D from which the contexts xt s are drawn in order to specify the constraint        A similar issue was faced in the earlier work of Dudk et al          where they replace the expectation under D with a sample  average over the contexts observed  We now discuss a similar modification for Algorithm   and give a sketch of the regret analysis  The key change in Algorithm   is to replace the constraint       with the sample version  Let Ht    x    x            xt     and denote by x  Ht the act of selecting a context x from Ht uniformly at random  Now we pick a distribution Pt on Ft  such that f  Ft    E  xHt              E  A Ft    x    Pt  f  x  x  xHt        Since Lemma A   applies to any distribution on the contexts  in particular  the uniform distribution on Ht   this constraint is still feasible  To justify this sample based approximation  we appeal to Theorem   of Dudk et al         which shows that for any o         and t    K ln  KN    with probability at least           E  xD Pt  f  x  x                 K        o  E  o xHt Pt  f  x  x  Using Equation        since  A Ft    xt     K  we get            K  E  xD Pt  f  x  x  using o          The remaining analysis of the algorithm remains the same as before  except we now apply Lemma     with a worse constant in the condition           Conclusion The included results gives us a basic understanding of the realizable assumption setting  it can  but does not necessarily  improve our ability to learn  We did not address computational complexity in this paper  There are some reasons to be hopeful however  Due to the structure of the realizability assumption  an eliminated regressor continues to have an increasingly poor regret over time  implying that it may be possible to avoid the elimination step and simply restrict the set of regressors we care about when constructing a distribution  A basic question then is  can we make the formation of this distribution computationally tractable  Another question for future research is the extension to infinite function classes  One would expect that this just involves replacing the log cardinality with something like a metric entropy or Rademacher complexity of F   This is not completely immediate since we are dealing with martingales  and direct application of covering arguments seems   Contextual Bandit Learning with Predictable Rewards   to yield a suboptimal O    t  rate in Lemma      Extending the variance based bound coming from Freedmans inequality from a single martingale to a supremum over function classes would need a Talagrand style concentration inequality for martingales which is not available in the literature to the best of our knowledge  Understanding this issue better is an interesting topic for future work   Proof  Let t  refer to the space of all distributions on Ft    We observe that t  is a convex  compact set  For a distribution Q  t    define the conditional distribution Q  x  on A as sample f  Q  and return f  x   Note that Q  a x         Q a x     Kx  where Kx     A Ft    x   for notational convenience   Acknowledgements This research was done while AA  SK and RES were visiting Yahoo    The feasibility of constraint       can be written as       min max E  E   A Ft    x      Pt t  f Ft  x Pt  f  x  x  x  
