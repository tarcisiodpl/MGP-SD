 Bayesian priors offer a compact yet general means of incorporating domain knowledge into many learning tasks  The correctness of the Bayesian analysis and inference  however  largely depends on accuracy and correctness of these priors  PAC Bayesian methods overcome this problem by providing bounds that hold regardless of the correctness of the prior distribution  This paper introduces the first PAC Bayesian bound for the batch reinforcement learning problem with function approximation  We show how this bound can be used to perform model selection in a transfer learning scenario  Our empirical results confirm that PAC Bayesian policy evaluation is able to leverage prior distributions when they are informative and  unlike standard Bayesian RL approaches  ignore them when they are misleading      Introduction  Prior distribution along with Bayesian inference have been used in multiple areas of machine learning to incorporate domain knowledge and impose general variance reducing constraints  such as sparsity and smoothness  within the learning process  These methods  although elegant and concrete  have often been criticized not only for their computational cost  but also for their strong assumptions on the correctness of the prior distribution  Bayesian guarantees often fail to hold when the inference is performed with priors that are different from the distribution of the underlying true model  Frequentist methods such as Probably Approximately Correct  PAC  learning  on the other hand  provide distribution free convergence guarantees  Valiant   Csaba Szepesvari Department of Computing Science University of Alberta Edmonton  Canada szepesva ualberta ca         These bounds  however  are often loose and impractical  reflecting the inherent difficulty of the learning problem when no assumptions are made on the distribution of the data  Both Bayesian and PAC methods have been proposed separately for reinforcement learning  Kearns and Singh        Brafman and Tennenholtz        Strehl and Littman        Kakade        Duff        Wang et al         Poupart et al         Kolter and Ng         where an agent is learning to interact with an environment to maximize some objective function  These methods are mostly focused on the so called explorationexploitation problem  where one aims to balance the amount of time spent on gathering information about the dynamics of the environment and the time spent acting optimally according to the current estimates  PAC methods are much more conservative and spend more time exploring the system and collecting information  Bayesian methods  on the other hand  are greedier and only solve the problem over a limited planning horizon  The PAC Bayesian approach  McAllester        Shawe Taylor and Williamson         takes the best of both worlds by combining the distribution free correctness of PAC theorems with the data efficiency of Bayesian inference  PAC Bayesian bounds do not require the Bayesian assumption to hold  They instead measure the consistency of the prior over the training data  and leverage the prior only when it seems informative  The empirical results of model selection algorithms for classification tasks using these bounds are comparable to some of the most popular learning algorithms  such as AdaBoost and Support Vector Machines  Germain et al          Fard and Pineau        introduced the idea of PACBayesian model selection in reinforcement learning  RL  for finite state spaces  They provided PACBayesian bounds on the approximation error in the value function of stochastic policies when a prior distribution is available either on the space of possible   models  or on the space of value functions  Model selection based on these bounds provides a robust use of Bayesian priors outside the Bayesian inference framework  Their work  however  is limited to small and discrete domains  and is mostly useful when sample transitions are drawn uniformly across the state space  This is problematic as most RL domains are relativity large  and require function approximation over continuous state spaces  This paper provides the first PAC Bayesian bound for value function approximation on continuous state spaces  We use results by Samson        to handle non i i d  data that are collected on Markovian processes  and use this  along with general PAC Bayesian inequalities  to get a bound on the approximation error of a value function sampled from any distribution over measurable functions  We empirically evaluate these bounds for model selection on two different continuous RL domains in a knowledge transfer setting  Our results show that a PAC Bayesian approach in this setting is indeed able to use the prior distribution when it is informative and matches the data  and ignore it when it is misleading      the transition and reward models are not known  one can use a finite sample set of transitions to learn an approximate value function  Least squares temporal difference learning  LSTD  and its derivations  Boyan        Lagoudakis and Parr        are among the methods used to learn a value function based on a finite sample      A general PAC Bayes bound  We begin by first stating a general PAC Bayes bound  In the next section  we use this result to derive our main bound for the approximation error in an RL setting  Let F be a class of real valued functions over a wellbehaved domain X  i e   X could be a bounded measurable subset of a Euclidean space   For ease of presentation  we assume that F has countably many functions  For a measure  over F  and a functional  def R R   F  R  we define R   R f  d f    Theorem    Let R be a random functional over F with a bounded range  Assume that for some C      c      for any          and f  F  w p        Background and Notation r  A Markov Decision Process  MDP  M    X   A  T  R  is defined by a  possibly infinite  set of states X   a set of actions A  a transition probability kernel T   X  A  M X    where T    x  a  defines the distribution of next state given that action a is taken in state x  and a  possibly stochastic  reward function R   X  A  M     Rmax     Throughout the paper  we focus on finite action  continuous state  discountedreward MDPs  with the discount factor denoted by           At discrete time steps  the reinforcement learning agent chooses an action and receives a reward  The environment then changes to a new state according to the transition kernel  A policy is a  possibly stochastic  function from states to actions  The value of a state x for policy   denoted the expected value of the discounted sum by V   x   isP of rewards   t  t rt   if the agent starts in state x and acts according to policy   The value function satisfies the Bellman equation  Z  V  x    R x   x      V   y T  dy x   x        There are many methods developed to find the value of a policy  policy evaluation  when the transition and reward functions are known  Among these there are dynamic programming methods in which one iteratively applies the Bellman operator  Sutton and Barto        to an initial guess of the optimal value function  When  R f     log C     c       Then  for any measure   over F  w p       for all measures  over F  s R   log    C c       K          c        where K       denotes the Kullback Leibler divergence between  and     The proof  included in the appendix  is a straightforward generalization of the proof presented by Boucheron et al              Application to RL  Consider some MDP  with state space X   and a policy  whose stationary distribution  exists  Let Dn       Xi   Ri   Xi    ni     be a random sample of size n such    that Xi      Xi     Ri    P    Xi    where P  is the Markov kernel underlying policy   The ith datum    Xi   Ri   Xi     is an elementary transition from state   Xi to state Xi   while policy  is followed and the reward associated with the transition is Ri   Further  to simplify the exposition  let  X  R  X     be a transition whose joint distribution is the same as the common   joint of  Xi   Ri   Xi        Define the functionals R  Rn over the space of realvalued  bounded measurable functions over X as follows  Let V   X  R be such a function  Then R V   Rn  V      E     n o    R   V  X      V  X         Antos et al            Thus  for    V     kV  V  k      V        R V      V                   Combining this inequality with      we get the following result   n o    Xn Ri   V  Xi     V  Xi     n i    The functional R is called the squared sample Bellman error  while Rn is the empirical squared sample Bellman error  Clearly  E  Rn  V      R V   holds  The following lemma  proved in the appendix  is a concentration bound connecting R and Rn   Lemma    Under proper mixing conditions for the sample  and assuming that the random rewards are sub Gaussian  there exists constants c       c     which depend only on P  such that for any Vmax      for any measurable function V bounded by Vmax   and any           w p        Theorem    Fix a countable set F of real valued  measurable functions with domain X   which are bounded by Vmax   Assume that the conditions of Lemma   hold and let c    c  be as in this lemma  Fix any measure   over these functions  Assume that   n   Vmax c    Then  for all           with probability      for all measures  over F  v     u   u log c  n   K         u c V      max t R      n n          Vmax c         E           r R V    Rn  V      c     c Vmax     log   n        Hence  by Theorem    for any countable set F of functions V bounded by Vmax   for any distribution   over   c    then for all           these functions  if n   Vmax w p       for all measures  over F  v     u u log c  n   u c  Vmax    K        R  Rn    t   n V   c           Further  the same bound holds for V  V     where R V   V d V   is the  average of value functions from F  Proof  The first statement follows from     combined with      as noted earlier  To see this just replace R V   in     with R V    Rn  V     Rn  V    Then  integrate both sides with respect to  and apply     to bound  R  Rn    The second part follows from the first part  Fubinis theorem and Jensens R       V  V  d V     inequality  V  V     R   kV   V k d V          max  Now  we show how this bound can be used to derive a PAC Bayes bound on the error of a value function V that is drawn from an arbitrary distribution over measurable functions  For a distribution  over the state L  norm  kV k    Rspace X   let k  k be the weighted   V  x   d x   Further  let B be the Bellman operator underlying   B  V  x    E  R   V  X     X   x   Fix some V   Since B  is a  contraction w r t  the norm k  k   a standard argument shows that  Bertsekas and Tsitsiklis          kV  V  k   kB  V  V k            Now  variance decomposition Var  U        using the   E U   E  U     we get R V     kB  V V k     V    where   V     E  Var  R   V  X     X    see  e g    The theorem bounds the expected error of approximating V  with a value function drawn randomly from some distribution   Note that in this theorem    must be a fixed distribution  chosen a priori  i e  prior distribution   but  can be chosen in a data dependent manner  i e   it can be a posterior distribution  Notice that there are three elements to the above bound  right hand side   The first term is the empirical component of the bound  which enforces the selection of solutions with smaller empirical Bellman residuals  The second term is the Bayesian component of the bound  which penalizes distributions that are far from the prior  The third term corrects for the variance in the return at each state         Here  Var  U  V     E  U  E  U  V      V is the conditional variance of U as usual  We shall also use the similarly defined conditional covariance   Cov   U    U    V       E  U   E  U   V    U   E  U   V      V   When U    U    def  we will also use Cov  U   V     Cov   U    U    V      If we can empirically estimate the right hand side of the above inequality  then we can use the bound in an algorithm  For example  we can derive a PACBayesian model selection algorithm that searches in the space of posteriors  so as to minimize the upper bound       Linearly parametrized classes of functions  Theorem   is presented for any countable families of functions  One can extend this result to sufficiently regular classes of functions  which can carry measures  without any problems   Here we consider the case where F is the class of linearly parametrized functions with bounded parameters    FC        kk  C where    X  Rd is some measurable function such def that Fmax   supxX k x k      In this case  the measures can be put on the ball      kk  C    Let us now turn to the estimation of the variance term  Assuming that the reward for each transition is independent of the next state  one gets Var  R   V  X     X     Var  R X      Var  V  X     X     Now  if V       then  Var  V  X     X       Cov   X     X     Assuming homoscedastic variance for the rewards  and     Var  R  and    E  Cov   X     X    we defining R get    Var  R   V  X     X    R                   Estimating the constants    In some cases the terms R and  are known  e g     R     when the rewards are a deterministic function of the start state and action  and      when the dynamics is deterministic   An alternative is to estimate these terms empirically  This can be done by  e g   double sampling of next states  assuming one has access to a generative model  or if one can reset the state    If such estimates are generated based on finite sample sets  then we might need to add extra deviation terms to the bound of Theorem    For simplicity     The extension presents only technical challenges  but leaves the result intact and hence is omitted    Alternately  one can co estimate the mean and variance terms  Sutton et al          keeping a current guess of them and updating both estimates as new transitions are observed   we assume that these terms are either known or can be estimated on a separate dataset of many transitions  Examples of such cases are studied in the empirical results  The constant c    which depends on the mixing condition of the process  can also be estimated if we have access to a generative model  There are upper bounds for c  when the sample is a collection of independent trajectories of length less than h      Empirical Results  In this section  we investigate how the bound of Theorem   can be used in a model selection mechanism for transfer learning in the RL setting  One experiment is presented on the well known mountain car problem  the other focuses on a generative model of epileptic seizures built from real world data       Case Study  Mountain Car  We design a transfer learning experiment on the Mountain Car domain  Sutton and Barto         where the goal is to drive an underpowered car beyond a certain altitude up a mountain  We refer the reader to the reference for details of the domain  We learn the optimal policy  name it   on the original Mountain Car problem          reward     passed the goal threshold and   otherwise   Note that the reward and       and the dynamics are deterministic  therefore R       The task is to learn the value function on the original domain  and use that knowledge in similar  though not identical  environments to accelerate the learning process in those new environments   The other environments will be described later   We estimate the value of  on the original domain with tile coding    tiles of size        Let   be the LSTD solution on a very large sample set in the original domain  To transfer the domain knowledge from this problem  we construct a prior distribution     product of Gaussians with mean   and variance            In a new environment  we collect a set of trajectories      trajectories of length     and search in the space of  parametrized posterior measures  defined as follows  measure  is the product of Gaussians with mean                            and variance                                where  is the LSTD solution based on the sample set on the new environment  and     variance of the empirical estimate  is set to       The search for the best  parameterized posterior is driven by our proposed PAC Bayes upper bound on the approximation error  When        will be a purely empirical estimate  whereas when       we get the Bayesian posterior for   We test this model selection method on two new environments  The first is a mountain domain very similar to the original problem  where we double the effect of the acceleration of the car  The true value function of this domain is close the original domain  and so we expect the prior to be informative  and thus  to be close to     In the second domain  we change the reward function such that it decreases  inversely proportional to the cars altitude  r x       h x   where h x          is the normalized altitude at state x  The value function of  under this reward function is largely different from that of the original one  which means that the prior distribution is misleading  and the empirical estimate should be more reliable  and  close to     Table   reports the average true error of approximating V  using different methods over     runs  purely empirical method is when       Bayesian is when        This corresponds to the left hand side of Theorem   for these methods  For the similar environment  the PAC Bayes bound is minimized consistently with       indicating that the method is fully using the Bayesian prior  The error is thus decreased to less than a half of that of the empirical estimate  For the environment with largely different reward function  standard Bayesian inference results in poor approximation  whereas the PAC Bayes method is selecting small values of  and is mostly ignoring the prior  Table    Error in the estimated value function V  R     kV  V  k d V    on the Mountain Car domain  The last row shows the value of the  parameter selected by the PAC Bayesian method  Purely empirical Bayesian PAC Bayes PACBayes  Similar Env  Different Env                                                                                   To further investigate how the value function estimate changes with these different methods  we consider an estimate of the value for the state when the car is at the bottom of the hill  This point estimate is constructed from the PAC Bayes estimate using the value function obtained by using only the mean of    To  Figure     right  compares the distribution of the estimated values for the highly different environment  We can see that  as expected  the Bayesian estimate is heavily biased due to the use of a misleading prior  The PAC Bayes estimate is only slightly biased away from the empirical one with the same variance on the value  Again  this confirms that PAC Bayes modelselection is largely ignoring the prior when the prior is misleading            Emp      Bayes  PacBayes      Emp  frequency  Note that because the Mountain Car is a deterministic domain  the variance term of Theorem   is    As we use trajectories with known length  we can also bound the other constants in the bound and evaluate the bound completely empirically based on the observed sample set   get a sense of the dependence of this estimate on the randomness of the sample the estimate is constructed over     runs  We also obtain these estimates using a Bayes estimate and purely empirical estimate  Figure     left  shows a normal fit to the histogram of the resulting estimates  for the purely empirical and the PAC Bayes estimates  As it can be seen  the distribution of PAC Bayes estimates  which coincides with the Bayesian posterior as the best  is consistently   in this case  is centered around the correct value  but is more peaked than the empirical distribution  This shows that the method is using the prior to converge faster to the correct value   frequency  the mean of a Gaussian with known variance  standard Bayesian inference with empirical priors        PacBayes                                              value    value       Figure    Distribution of the estimated value function on similar  left  and different  right  environments      Case Study  Epilepsy Domain  We also evaluate our method on a more complex domain  The goal of the RL agent here is to apply direct electrical neurostimulation such as to suppress epileptiform behavior  We use a generative model constructed from real world data collected on slices of rat brain tissues  Bush et al          the model is available in the RL Glue framework  Observations are generated over a   dimensional real valued state space  The action choice corresponds to selecting the frequency at which neurostimulation is applied  The reward is   for steps when a seizure is occurring       for each stimulation pulse  and   otherwise  We first apply the best clinical fixed rate policy  stimulation is applied at a consistent  Hz  to collect a large sample set  Bush et al          We then use LSTD to learn a linear value function over the original feature space  Similar to the experiment described above  we construct a prior  with a similar mean and variance structure   and use it for knowledge transfer in two   new cases  This time  we keep the dynamics and reward function intact and instead change the policy  The first modified policy we consider applies stimulation at a fixed rate of  Hz  this is expected to have a similar value function as the original   Hz  policy  The other policy we consider applies no stimulation  this is expected to have a very different value function as the seizures are not suppressed  Table    The error of value function estimates R     kV  V  k d V    on the Epilepsy domain  Empirical Bayesian PAC Bayes PACBayes    Hz Stimulation  No Stimulation                                                                                                        We sample        on policy trajectories of length   and use them with the PAC Bayes model selection mechanism described previously  with similar parametrized posterior family on the  parameters          to get estimates of the value function  Table   summarizes the performance of different methods on the evaluation of the new policies  averaged over    runs   The results are not as polarized as those of the Mountain Car experiment  partly because the domain is noisier  and because the prior is neither exclusively informative or misleading  Nonetheless  we observe that the PAC Bayes method is using the prior more   averaging around       in the case of the  Hz policy  which is consistent with clinical evidence showing that  Hz and  Hz have similar effect  Bush et al          whereas the prior is considered less   averaging around       in the case of the  Hz policy which has substantially  though not entirely  different effects      Discussion  This paper introduces the first PAC Bayesian bound for policy evaluation with function approximation and general state spaces  We demonstrate how such bounds can be used for value function estimation based on finite sample sets  Our empirical results show that PAC Bayesian model selection uses prior distributions when they are informative  and ignores them when they are misleading  Our results thus far focus on the policy evaluation case  This approach can be used in a number of applications  including transfer learning  as explored above  Model selection based on error bounds has been studied previously with regularization techniques  Farahmand et al          These bounds are generally tighter for point estimates  as compared to the distributions  used in this work  However  our method is more general as it could incorporate arbitrary domain knowledge into the learning algorithm with any type of prior distribution  It can of course use sparsity or smoothness priors  which correspond to well known regularization methods  An alternative is to derive margin bounds  similar to those of large margin classifiers  using PAC Bayes techniques  This was recently done by Fard and Pineau        in the discrete case  The extension to continuous domains with general function approximation is an interesting future work  This work does not address the application of PACBayes bounds to derive exploration strategies for the RL problem  Seldin et al       b a  have studied the exploration problem for multiarmed bandits and have provided algorithms based on PAC Bayes analysis of martingales  Extensions to contextual bandits and more general RL settings remain interesting open problems   Acknowledgements This work was supported in part by AICML  AITF  formerly iCore and AIF   the PASCAL  Network of Excellence under EC  grant no           the NSERC Discovery Grant program and the National Institutes of Health  grant R   DA          
 Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed  web based learning problems  In this paper we provide a systematic study of the topic  and analyze the effect of delay on the regret of online learning algorithms  Somewhat surprisingly  it turns out that delay increases the regret in a multiplicative way in adversarial problems  and in an additive way in stochastic problems  We give meta algorithms that transform  in a black box fashion  algorithms developed for the non delayed case into ones that can handle the presence of delays in the feedback loop  Modifications of the well known UCB algorithm are also developed for the bandit problem with delayed feedback  with the advantage over the meta algorithms that they can be implemented with lower complexity      Introduction In this paper we study sequential learning when the feedback about the predictions made by the forecaster are delayed  This is the case  for example  in web advertisement  where the information whether a user has clicked on a certain ad may come back to the engine in a delayed fashion  after an ad is selected  while waiting for the information if the user clicks or not  the engine has to provide ads to other users  Also  the click information may be aggregated and then periodically sent to the module that decides about the ads  resulting in further delays   Li et al         Dudik et al          Another example is parallel  distributed learning  where propagating information among nodes causes delays  Agarwal   Duchi         Proceedings of the    th International Conference on Machine Learning  Atlanta  Georgia  USA        JMLR  W CP volume     Copyright      by the author s    While online learning has proved to be successful in many machine learning problems and is applied in practice in situations where the feedback is delayed  the theoretical results for the non delayed setup are not applicable when delays are present  Previous work concerning the delayed setting focussed on specific online learning settings and delay models  mostly with constant delays   Thus  a comprehensive understanding of the effects of delays is missing  In this paper  we provide a systematic study of online learning problems with delayed feedback  We consider the partial monitoring setting  which covers all settings previously considered in the literature  extending  unifying  and often improving upon existing results  In particular  we give general meta algorithms that transform  in a blackbox fashion  algorithms developed for the non delayed case into algorithms that can handle delays efficiently  We analyze how the delay effects the regret of the algorithms  One interesting  perhaps somewhat surprising  result is that the delay inflates the regret in a multiplicative way in adversarial problems  while this effect is only additive in stochastic problems  While our general meta algorithms are useful  their time  and spacecomplexity may be unnecessarily large  To resolve this problem  we work out modifications of variants of the UCB algorithm  Auer et al         for stochastic bandit problems with delayed feedback that have much smaller complexity than the black box algorithms  The rest of the paper is organized as follows  The problem of online learning with delayed feedback is defined in Section    The adversarial and stochastic problems are analyzed in Sections     and      while the modification of the UCB algorithm is given in Section    Some proofs  as well as results about the KL UCB algorithm  Garivier   Cappe        under delayed feedback  are provided in the appendix      The delayed feedback model We consider a general model of online learning  which we call the partial monitoring problem with side in    Online Learning under Delayed Feedback  Parameters  Forecasters prediction set A  set of outcomes B  side information set X   reward function r   X  A  B  R  feedback function h   X  A  B  H  time horizon n  optional   At each time instant t                 n     The environment chooses some side information xt  X and an outcome bt  B     The side information xt is presented to the forecaster  who makes a prediction at  A  which results in the reward r xt   at   bt    unknown to the forecaster      The feedback ht   h xt   at   bt   is scheduled to be revealed after t time instants     The agent observes Ht     t   ht     t  t  t   t   t   i e   all the feedback values scheduled to be revealed at time step t  together with their timestamps  Figure    Partial monitoring under delayed  timestamped feedback   formation  In this model  the forecaster  decision maker  has to make a sequence of predictions  actions   possibly based on some side information  and for each prediction it receives some reward and feedback  where the feedback is delayed  More formally  given a set of possible side information values X   a set of possible predictions A  a set of reward functions R   r   X  A  R   and a set of possible feedback values H  at each time instant t                the forecaster receives some side information xt  X   then  possibly based on the side information  the forecaster predicts some value at  A while the environment simultaneously chooses a reward function rt  R  finally  the forecaster receives reward rt  xt   at   and some time stamped feedback set Ht  N  H  In particular  each element of Ht is a pair of time index and a feedback value  the time index indicating the time instant whose decision the associated feedback corresponds to  Note that the forecaster may or may not receive any direct information about the rewards it receives  i e   the rewards may be hidden   In standard online learning  the feedback set Ht is a singleton and the feedback in this set depends on rt   at   In the delayed model  however  the feedback that concerns the decision at time t is received at the end of the time period t t   after the prediction is made  i e   it is delayed by t time steps  Note that t    corresponds to the non delayed case  Due to the delays multiple feedbacks may arrive at the same time  hence the definition of Ht    The goal of P the forecaster is to maximize its cumulative reward nt   rt  xt   at    n      The performance of the forecaster is measured relative to the best static strategy selected from some set F   f   f   X  A  in hindsight  In particular  the forecasters performance is measured through the regret  defined by Rn   sup  n X  aF t    rt  xt   a xt      n X  rt  xt   at     t    A forecaster is consistent if it achieves  asymptotically  the average reward of the best static strategy  that is E  Rn    n     and we are interested in how fast the average regret can be made to converge to    The above general problem formulation includes most scenarios considered in online learning  In the full information case  the feedback is the reward function itself  that is  H   R and Ht     t  rt      in the non delayed case   In the bandit case  the forecaster only learns the rewards of its own prediction  i e   H   R and Ht     t  rt  xt   at      In the partial monitoring case  the forecaster is given a reward function r   X  A  B  R and a feedback function h   X  A  B  H  where B is a set of choices  outcomes  of the environment  Then  for each time instant the environment picks an outcome bt  B  and the reward becomes rt  xt   at     r xt   at   bt    while Ht     t  h xt   at   bt      This interaction protocol is shown in Figure   in the delayed case  Note that the bandit and full information problems can also be treated as special partial monitoring problems  Therefore  we will use this last formulation of the problem  When no stochastic assumption is made on how the sequence bt is generated  we talk about the adversarial model  In the stochastic setting we will consider the case when bt is a sequence of independent  identically distributed  i i d   random variables  Side information may or may not be present in a real problem  in its absence X is a singleton set  Finally  we may have different assumptions on the delays  Most often  we will assume that  t  t  is an i i d  sequence  which is independent of the past predictions  as  st of the forecaster  In the stochastic setting  we also allow the distribution of t to depend on at   Note that the delays may change the order of observing the feedbacks  with the feedback of a more recent prediction being observed before the feedback of an earlier one       Related work The effect of delayed feedback has been studied in the recent years under different online learning scenarios   Online Learning under Delayed Feedback  Stochastic Feedback  Full Info  No Side Info      R n   R  n    O E t     Agarwal   Duchi        L  Side Info  Bandit Feedback  No Side Info Side Info  Partial  No Side Info  Monitoring  Side Info  R n   R  n    O D    Mesterharm        R n   C  R  n    C  max log max    Desautels et al          R n   R  n    O const log n   Dudik et al         Rn  R  n    O Gn    General  Adversarial  Feedback L R n   O const    R  n const    Weinberger   Ordentlich         Langford et al          Agarwal   Duchi        L R n   O D   R  n D   Mesterharm        R n   O const    R n const    Neu et al            n        E  Gn     n Rn       E  Gn     R     E  Gn   Rn       E  Gn     R     Table    Summary of work on online learning under delayed feedback  R n  shows the  expected  regret in the delayed setting  while R  n  shows the  upper bound on  the  expected  regret in the non delayed setting  L denotes a matching lower bound  D and D indicate the maximum and average gap  respectively  where a gap is a number of consecutive time steps the agent does not get any feedback  in the adversarial delay formulation used by Mesterharm                The term const indicates that the results are for constant delays only  For the work of  Desautels et al          C  and C  are positive constants  with C       and max denotes the maximum delay  The results presented in this paper are shown in boldface  where Gt is the maximum number of outstanding feedbacks during t time steps  In particular      the firstp   Gn  max when the delays have an upper bound max   and we show that Gn   O E  t     E  t   log n   log n when the delays t are i i d  The new bounds for the partial monitoring problem are automatically applicable in the other  spacial  cases  and give improved results in most cases   and different assumptions on the delay  A concise summary  together with the contributions of this paper  is given in Table    To the best of our knowledge  Weinberger   Ordentlich        were the first to analyze the delayed feedback problem  they considered the adversarial full information setting with a fixed  known delay const   They showed that the minimax optimal solution is to run const     independent optimal predictors on the subsampled reward sequences  const     prediction strategies are used such that the ith predictor is used at time instants t with  t mod  const             i  This approach forms the basis of our method devised for the adversarial case  see Section       Langford et al         showed that under the usual conditions  a sufficiently slowed down version of the mirror descent algorithm achieves optimal decay rate of the average regret  Mesterharm              considered another variant of the full information setting  using an adversarial model on the delays in the label prediction setting  where the forecaster has to predict the label corresponding to a side information vector  xt   While in the full information online prediction problem Weinberger   Ordentlich        showed that the regret increases by a multiplicative factor of const   in the work of Mesterharm              the important quantity becomes the maximum average gap defined as the length of the largest time interval the forecaster does not receive feedback  Mesterharm              also shows that the minimax regret in the adversarial case increases multiplicatively by the average gap  while it increases only in an additive fashion in the stochastic case  by the maximum gap  Agarwal   Duchi        considered the problem of online stochastic optimization and showed that  for i i d  random delays  the increases with an   regret   additive factor of order E       Qualitatively similar results were obtained in the bandit setting  Considering a fixed and known delay const   Dudik et al         showed an additive O const log n  penalty in the regret for the stochastic setting  with side information   while  Neu et al         showed a multiplicative regret for the adversarial bandit case  The problem of delayed feedback has also been studied for Gaussian process bandit optimization   Online Learning under Delayed Feedback   Desautels et al          resulting in a multiplicative increase in the regret that is independent of the delay and an additive term depending on the maximum delay  In the rest of the paper we generalize the above results to the partial monitoring setting  extending  unifying  and often improving existing results      Black Box Algorithms for Delayed Feedback In this section we provide black box algorithms for the delayed feedback problem  We assume that there exists a base algorithm Base for solving the prediction problem without delay  We often do not specify the assumptions underlying the regret bounds of these algorithms  and assume that the problem we consider only differs from the original problem because of the delays  For example  in the adversarial setting  Base may build on the assumption that the reward functions are selected in an oblivious or non oblivious way  i e   independently or not of the predictions of the forecaster   First we consider the adversarial case in Section      Then in Section      we provide tighter bounds for the stochastic case       Adversarial setting We say that a prediction algorithm enjoys a regret or expected regret bound f          R under the given assumptions in the non delayed setting if  i  f is nondecreasing  concave  f          and  ii  supb       bn B Rn  f  n  or  respectively  supb       bn B E  Rn    f  n  for all n  The algorithm of Weinberger   Ordentlich        for the adversarial full information setting subsamples the reward sequence by the constant delay const     and runs a base algorithm Base on each of the const     subsampled sequences  Weinberger   Ordentlich        showed that if Base enjoys a regret bound f then their algorithm in the fixed delay case enjoys a regret bound  const     f  n  const        Furthermore  when Base is minimax optimal in the non delayed setting  the subsampling algorithm is also minimax optimal in the  full information  delayed setting  as can be seen by constructing a reward sequence that changes only in every const     times  Note that Weinberger   Ordentlich        do not require condition  i  of f   However  these conditions imply that yf  x y  is a concave function of y for any fixed x  a fact which will turn out to be useful in the analysis later   and are satisfied by all regret bounds we are aware of  e g   for multi armed bandits  contextual bandits  partial monitoring  etc     e   which all have a regret upper bound of the form O n for some         with  typically         or          In this section we extend the algorithm of Weinberger   Ordentlich        to the case when the delays are not constant  and to the partial monitoring setting  The idea is that we run several instances of a non delayed algorithm Base as needed  an instance is free if it has received the feedback corresponding to its previous prediction  before this we say that the instance is busy  waiting for the feedback  When we need to make a prediction  we use one of existing instances that is free  and is hence ready to make another prediction  If no such instance exists  we create a new one to be used  a new instance is always free  as it is not waiting for the feedback of a previous prediction   The resulting algorithm  which we call Black Box Online Learning under Delayed feedback  BOLD  is shown below  note that when the delays are constant  BOLD reduces to the algorithm of Weinberger   Ordentlich           Algorithm   Black box Online Learning under Delayed feedback  BOLD  for each time instant t                 n do Prediction  Pick a free instance of Base  independently of past predictions   or create a new instance if all existing instances are busy  Feed the instance picked with xt and use its prediction  Update  for each  s  hs    Ht do Update the instance used at time instant s with the feedback hs   end for end for  Clearly  the performance of BOLD depends on how many instances of Base we need to create  and how many times each instance is used  Let Mt denote the number of Base instances created by BOLD up to and including time t  That is  M       and we create a new instance at the beginning of any time instant whenP all instances are waiting for their feedback  t  Let Gt   s   I  s   s  t  be the total number of outstanding  missing  feedbacks when the forecaster is making a prediction at time instant t  Then we have Gt algorithms waiting for their feedback  and so Mt  Gt      Since we only introduce new instances when it is necessary  and each time instant at most   e n   means that there is a     such that un   O v limn un   vn log n         Online Learning under Delayed Feedback  one new instance is created   it is easy to see that Mt   Gt      Now  using the fact that fBase is an  expected  regret bound  we obtain      for any t  where Gt   max st Gt    E  Rn              n     We can use the result above to transfer the regret guarantee of the non delayed base algorithm Base to a guarantee on the regret of BOLD  Theorem    Suppose that the non delayed algorithm Base used in BOLD enjoys an  expected  regret bound fBase   Assume  furthermore  that the delays t are independent of the forecasters prediction at   Then the expected regret of BOLD after n time steps satisfies        n E  Rn    E  Gn     fBase G         n n     E  Gn       fBase E  Gn       Proof  As the second inequality follows from the concavity of y   yfBase  x y   x  y       it remains to prove the first one  For any    j  Mn   let Lj denote the list of time instants in which BOLD has used the prediction chosen by instance j  and let nj    Lj   be the number of time instants this happens  Furthermore  let Rnj j denote the regret incurred during the time instants t with t  Lj   X  Rnj j   sup  aF tL j  rt  xt   a xt      X  rt  xt   at     tLj  where at is the prediction made by BOLD  and instance j  at time instant t  By construction  instance j does not experience any delays  Hence  Rnj j is its regret in a non delayed online learning problem    Then  Rn   sup  n X  aF t      sup  rt  xt   a xt      Mn X X  aF j   tLj      Mn X j    Mn X     sup  n X  rt  xt   at    t    rt  xt   a xt      X  aF tL j  Mn X X  j   tLj  rt  xt   a xt      X  tLj  rt  xt   at     rt  xt   at    Rnj j    j       Note that Lj is a function of the delay sequence and is not a function of the predictions  at  t    Hence  the reward sequence that instance j is evaluated on is chosen obliviously whenever the adversary of BOLD is oblivious   Mn X j    i h E Rnj j              n  Mn X   fBase  nj   Mn j   j         Mn X   n  Mn fBase    nj    Mn fBase Mn Mn j      Mn X  fBase  nj     Mn  where the first inequality follows since Mn is a deterministic function of the delays  while the last inequality follows from Jensens inequality and the concavity of fBase   Substituting Mn from     and taking the expectation concludes the proof  Now  we need to bound Gn to make the theorem meaningful  When all delays are the same constants  for n   const we get Gn   t   const   and we get back the regret bound     n E  Rn     const     fBase const     of Weinberger   Ordentlich         thus generalizing their result to partial monitoring  We do not know whether this bound is tight even when Base is minimax optimal  as the argument of Weinberger   Ordentlich        for the lower bound does not work in the partial information setting  the forecaster can gain extra information in each block with the same reward functions   Assuming the delays are i i d   we can give an interesting bound on Gn   The result is based on the fact that although Gt can be as large as t  both its expectation and variance are upper bounded by E       Lemma    Assume             n is a sequence of i i d  random variables withfinite expected value  and let B n  t    t     log n    t log n  Then E  Gn    B n  E            Proof  First consider the expectation and the variance of Gt   For any t    t    t  X X E  Gt     E I  s   s  t    P  s   s  t  s       t  X s    P      s   E         s     Online Learning under Delayed Feedback  and  similarly     Gt      t  X s        I  s   s  t     t  X s    P  s   s  t     so     Gt    E      in the same way as above  By Bernsteins inequality  Cesa Bianchi   Lugosi        Corollary A     for any          and any t we have  with probability at least      q Gt  E  Gt    log          Gt   log     Applying the union bound for      n    and our previous bounds on the variance and expectation of Gt   we obtain that with probability at least      n  p max Gt  E          log n    E      log n   tn  Taking into account that max tn Gt  n  we get the statement of the lemma  Corollary    Under the conditions of Theorem    if the sequence of delays is i i d  then     n   E  Rn     B n  E           fBase B n  E            Note that although the delays can be arbitrarily large  whenever the expected value is finite  the bound only increases by a log n factor       Finite stochastic setting In this section  we consider the case when the prediction set A of the forecaster is finite  without loss of generality we assume A                  K   We also assume that there is no side information  that is  xt is a constant for all t  and  hence  will be omitted  the results can be extended easily to the case of a finite side information set  where we can repeat the procedures described below for each value of the side information separately   The main assumption in this section is that the outcomes  bt  t  form an i i d  sequence  which is also independent of the predictions of the forecaster  When B is finite  this leads to the standard i i d  partial monitoring  IPM  setting  while the conventional multi armed bandit  MAB  setting is recovered when the feedback is the reward of the last prediction  that is  ht   rt  at   bt    As in the previous section  we will assume that the feedback delays are independent of the outcomes of the environment  The main result of this section shows that under these assumptions  the penalty in the regret grows in an additive fashion due to the delays  as opposed to the multiplicative penalty that we have seen in the adversarial case   By the independence assumption on the outcomes  the   sequences of potential rewards rt  i    r i  bt   and feed  backs ht  i    h i  bt   are i i d   respectively  for the same prediction i  A  In this setting we also assume that the feedback and reward sequences of different predictions are independent of each other  Let i   E  rt  i   denote the expected reward of predicting i     maxiA i the optimal reward and i    the optimal prediction  Moreover  let with i P Ti  n    nt   I  at   i  denote the number of times i is predicted by the end of time instant n  Then  defining the gaps i     i for all i  A  the expected regret of the forecaster becomes E  Rn      n X t      at    K X  i E  Ti  n           i    Similarly to the adversarial setting  we build on a base algorithm Base for the non delayed case  The advantage in the IPM setting  and that we consider expected regret  is that here Base can consider a permuted order of rewards and feedbacks  and so we do not have to wait for the actual feedback  it is enough to receive a feedback for the same prediction  This is the idea at the core of our algorithm  Queued Partial Monitoring with Delayed Feedback  QPM D   Algorithm   Queued Partial Monitoring with Delays  QPM D  Create an empty FIFO buffer Q i  for each i  A  Let I be the first prediction of Base  for each time instant t                 n do Predict  while Q I  is not empty do Update Base with a feedback from Q I   Let I be the next prediction of Base  end while There are no buffered feedbacks for I  so predict at   I at time instant t to get a feedback  Update  for each  s  hs    Ht do Add the feedback hs to the buffer Q as    end for end for Here we have a Base partial monitoring algorithm for the non delayed case  which is run inside the algorithm  The feedback information coming from the environment is stored in separate queues for each prediction value  The outer algorithm constantly queries Base  while feedbacks for the predictions made are available in the queues  only the inner algorithm Base runs  that is  this happens within a single time instant   Online Learning under Delayed Feedback  in the real prediction problem   When no feedback is available  the outer algorithm keeps sending the same prediction to the real environment until a feedback for that prediction arrives  In this way Base is run in a simulated non delayed environment  The next lemma implies that the inner algorithm Base actually runs in a non delayed version of the problem  as it experiences the same distributions  Lemma    Consider a delayed stochastic IPM problem as defined above  For any prediction i  for any s  N let hi s denote the sth feedback QPM D receives for predicting i  Then the sequence  hi s  sN is an i i d  sequence with the same distribution as the sequence of feedbacks  ht i  tN for prediction i  To relate the non delayed performance of Base and the regret of QPM D  we need a few definitions  For any t  let Si  t  denote the number of feedbacks for prediction i that are received by the end of time instant t  Then the number of missing feedbacks for i when making a prediction at time instant t is Gi t   Ti  t      Si  t      Let Gi n   max tn Gi t   Furthermore  for each i  A  let Ti  t   be the number of times algorithm Base has predicted i while being queried t times  Let n denote the number of steps the inner algorithm Base makes in n steps of the real IPM problem  Next we relate n and n   as well as the number of times QPM D and Base  in its simulated environment  make a specific prediction  Lemma    Suppose QPM D is run for n    time instants  and has queried Base n times  Then n  n and    Ti  n   Ti  n    Gi n       Proof  Since Base can take at most one step for each feedback that arrives  and QPM D has to make at least one step for each arriving feedback  n  n  Now  fix a prediction i  A  If Base  and hence  QPM D  has not predicted i by time instant n      trivially holds  Otherwise  let tn i denote the last time instant  up to time n  when QPM D predicts i  Then Ti  n    Ti  tn i     Ti  tn i          Suppose Base has been queried n  n times by time instant tn i  inclusive   At this time instant  the buffer Q i  must be empty and Base must be predicting i  otherwise QPM D would not predict i in the real environment  This means that all the Si  tn i    feedbacks that have arrived before this time instant have been fed to the base algorithm  which has also made an extra step  that is  Ti  n    Ti  n     Si  tn i          Therefore  Ti  n   Ti  n    Ti  tn i           Si  tn i           Gi tn i  Gi n    We can now give an upper bound on the expected regret of Algorithm    Theorem    Suppose the non delayed Base algorithm is used in QPM D in a delayed stochastic IPM environment  Then the expected regret of QPM D is upper bounded by K       X   i E Gi n   E  Rn    E RnBase         i        where E RnBase is the expected regret of Base when run in the same environment without delays  When the delay t is bounded by max   for all t  we also have Gi n  max   and E  Rn    E RnBase   O max    When the sequence of delays for each prediction is i i d  with a finite expected value but unbounded support  we can use Lemma   to bound Gi n   and obtain p     a bound E RnBase   O E        E      log n   log n    Proof  Assume that QPM D is run longer so that Base is queried for n times  i e   it is queried n  n more times   Then  since n  n  the number of times i is predicted by the base algorithm  namely Ti  n   can only increase  that is  Ti  n    Ti  n   Combining this with the expectation of     gives     E  Ti  n    E  Ti  n     E Gi n   which in turn gives  K X i    i E  Ti  n     K X i    i E  Ti  n      K X i        i E Gi n         As shown in Lemma    the reordered rewards and feedbacks hi     hi             hi T   n           hi Ti  n  are i i d  with i the same distribution as the original feedback sequence  ht i  tN   The base algorithm Base has worked on the first Ti  n  of these feedbacks for each i  in its extended run   and has therefore operated for n steps in a simulated environment with the same reward and feedback distributions  but without delay  Hence  the first     summation in the right hand side of     is in fact E RnBase   the expected regret of the base algorithm in a nondelayed environment  This concludes the proof      UCB for the Multi Armed Bandit Problem with Delayed Feedback While the algorithms in the previous section provide an easy way to convert algorithms devised for the nondelayed case to ones that can handle delays in the feedback  improvements can be achieved if one makes modifications inside the existing non delayed algorithms   Online Learning under Delayed Feedback  while retaining their theoretical guarantees  This can be viewed as a white box approach to extending online learning algorithms to the delayed setting  and enables us to escape the high memory requirements of black box algorithms that arises for both of our methods in the previous section when the delays are large  We consider the stochastic multi armed bandit problem  and extend the UCB family of algorithms  Auer et al         Garivier   Cappe        to the delayed setting  The modification proposed is quite natural  and the common characteristics of UCB type algorithms enable a unified way of extending their performance guarantees to the delayed setting  up to an additive penalty due to delays          uses UCBs of the form Bi s t   i s   p Ps   log t  s  where i s    s t   hi t is the average of the first s observed rewards  Using this UCB in our decision rule      we can bound the regret of the resulting algorithm  called Delayed UCB   in the delayed setting   Recall that in the stochastic MAB setting  which is a special case of the stochastic IPM problem of Section      the feedback at time instant t is ht   r at   bt    and there is a distribution i from which the rewards of each prediction i are drawn in an i i d  manner  Here we assume that the rewards of different predictions are independent of each other  We use the same notation as in Section       Note that the last term in the bound is the additive penalty  and  under different assumptions  it can be bounded in the same way as after Theorem    The proof of this theorem  as well as a similar regret bound for the delayed version of the KL UCB algorithm  Garivier   Cappe        can be found in Appendix B   Several algorithms devised for the non delayed stochastic MAB problem are based on upper confidence bounds  UCBs   which are optimistic estimates of the expected reward of different predictions  Different UCB type algorithms use different upper confidence bounds  and choose  at each time instant  a prediction with the largest UCB  Let Bi s t denote the UCB for prediction i at time instant t  where s is the number of reward samples used in computing the estimate  In a non delayed setting  the prediction of a UCB type algorithm at time instant t is given by at   argmaxiA Bi Ti  t   t   In the presence of delays  one can simply use the same upper confidence bounds only with the rewards that are observed  and predict     Conclusion and future work  at   argmaxiA Bi Si  t   t       at time instant t  recall that Si  t     is the number of rewards that can be observed for prediction i before time instant t   Note that if the delays are zero  this algorithm reduces to the corresponding non delayed version of the algorithm  The algorithms defined by     can easily be shown to enjoy the same regret guarantees compared to their non delayed versions  up to an additive penalty depending on the delays  This is because the analyses of the regrets of UCB algorithms follow the same pattern of upper bounding the number of trials of a suboptimal prediction using concentration inequalities suitable for the specific form of UCBs they use  As an example  the UCB  algorithm  Auer et al    Theorem    For any n     the expected regret of the Delayed UCB  algorithm is bounded by E  Rn       X K X     log n     i E Gi n        i   i i    i i     We analyzed the effect of feedback delays in online learning problems  We examined the partial monitoring case  which also covers the full information and the bandit settings   and provided general algorithms that transform forecasters devised for the non delayed case into ones that handle delayed feedback  It turns out that the price of delay is a multiplicative increase in the regret in adversarial problems  and only an additive increase in stochastic problems  While we believe that these findings are qualitatively correct  we do not have lower bounds to prove this  matching lower bounds are available for the full information case only   It also turns out that the most important quantity that determines the performance of our algorithms is Gn   the maximum number of missing rewards  It is interesting to note that Gn is the maximum number of servers used in a multi server queuing system with infinitely many servers and deterministic arrival times  It is also the maximum deviation of a certain type of Markov chain  While we have not found any immediately applicable results in these fields  we think that applying techniques from these areas could lead to an improved understanding of Gn   and hence an improved analysis of online learning under delayed feedback      Acknowledgements This work was supported by the Alberta Innovates Technology Futures and NSERC    Online Learning under Delayed Feedback  
 We study the problem of learning Markov decision processes with finite state and action spaces when the transition probability distributions and loss functions are chosen adversarially and are allowed to change with time  We introduce an algorithm whose regret with respect to any policy in a comparison class grows as the square root of the number of rounds of the game  provided the transition probabilities satisfy a uniform mixing condition  Our approach is efficient as long as the comparison class is polynomial and we can compute expectations over sample paths for each policy  Designing an efficient algorithm with small regret for the general case remains an open problem      Notation  Let X be a finite state space and A be a finite action space  Let S be the space of probability distributions over set S  Define a policy  as a mapping from the state space to A      X  A   We use  a x  to denote the probability of choosing action a in state x under policy   A random action under policy  is denoted by  x   A transition probability kernel  or transition model  m is a mapping from the direct product of the state and action spaces to X   m   X  A  X   Let P    m  be the transition probability matrix of policy  under transition model m  A loss function is a bounded real valued function over state and action spaces  l   X  A  R  For a P vector v  define kvk    i  vi    For a real valued function f defined over X  A  define kf k     P maxxX aA  f  x  a    The inner product between two vectors v and w is denoted by hv  wi      Introduction  Consider the following game between a learner and an adversary  at round t  the learner chooses a policy t from a policy class   In response  the adversary chooses a transition model mt from a set of models M and a loss function lt   The learner takes action at  t    xt    moves to state xt    mt    xt   at   and suffers loss lt  xt   at    To simplify the discussion  we assume that the adversary is oblivious  i e  its choices do not depend on the previous choices of the learner  We assume that lt          In this paper  we study the full information version of the game  where the learner observes the transition model mt and the loss function lt at the end of round t  The game is shown in Figure    The objective of the learner is to suffer low loss over a period of T rounds  while the performance of the learner is measured using its regret with respect to the total loss he would have achieved had he followed the stationary policy in the comparison class  minimizing the total loss  Even Dar et al         prove a hardness result for MDP problems with adversarially chosen transition models  Their proof  however  seems to have gaps as it assumes that the learner chooses a deterministic policy before observing the state at each round  Note that an online learning algorithm only needs to choose an action at the current state and does not need to construct a complete deterministic policy at each round  Their hardness result applies to deterministic transition models  while we make a mixing assumption in our analysis  Thus  it is still an open problem whether it is possible to obtain a computationally efficient algorithm with a sublinear regret  Yu and Mannor      a b  study the same setting  but obtain only a regret bound that scales with the amount of variation in the transition models  This regret bound can grow linearly with time    Initial state  x  for t                do Learner chooses policy t Adversary chooses model mt and loss function lt Learner takes action at  t    xt   Learner suffers loss lt  xt   at   Update state xt    mt    xt   at   Learner observes mt and lt end for Figure    Online Markov Decision Processes  Even Dar et al         prove regret bounds for MDP problems with a fixed and known transition model and adversarially chosen loss functions  In this paper  we prove regret bounds for MDP problems with adversarially chosen transition models and loss functions  We are not aware of any earlier regret bound for this setting  Our approach is efficient as long as the comparison class is polynomial and we can compute expectations over sample paths for each policy  MDPs with changing transition kernels are good models for a wide range of problems  including dialogue systems  clinical trials  portfolio optimization  two player games such as poker  etc      Online MDP Problems  Let A be an online learning algorithm that generates a policy t at round t  Let xA t be the state at round t if we have followed the policies generated by algorithm A  Similarly  xt denotes the state if we have chosen the same policy  up to time t  Let l x      l x   x    The regret of algorithm A up to round T with respect to any policy    is defined by RT  A       T X t    t  xA t     lt  xA t   at     T X  lt  xt        t    where at   Note that the regret with respect to  is defined in terms of the sequence of states xt that would have been visited under policy   Our objective is to design an algorithm that achieves low regret with respect to any policy   In the absence of state variables  the problem reduces to a full information online learning problem  Cesa Bianchi and Lugosi         The difficulty with MDP problems is that  unlike the full information online learning problems  the choice of policy at each round changes the future states and losses  The main idea behind the design and the analysis of our algorithm is the following regret decomposition  RT  A       T X t    Let  lt  xA t   at     T X  BT  A     T X  t    t    CT  A       lt  xt t   t      T X t    T X t    lt  xA t   at     T X  lt  xt t   t     lt  xt t   t     T X  lt  xt             t    lt  xt t   t      t    T X  lt  xt        t    Notice that the choice of policies has no influence over future losses in CT  A     Thus  CT  A    can be bounded by a specific reduction to full information online learning algorithms  to be specified later   Also  notice that the competitor policy  does not appear in BT  A   In fact  BT  A  depends only on the algorithm A  We will show that if algorithm A and the class of models satisfy the following two smoothness assumptions  then BT  A  can be bounded by a sublinear term  Assumption A  Rarely Changing Policies Let t be the probability that algorithm A changes its policy at round t  There exists a constant D such  that for any    t  T   any sequence of models m            mt and loss functions l            lt   t  D  t      N   number of experts  T   number of rounds  Initialize wi       for each expert i  W    N   for t                do For any i  pi t   wi t   Wt    Draw It such that for any i  P  It   i    pi t   Choose the action suggested by expert It   The adversary chooses loss function ct   The learner suffers loss ct  It    For expert i  wi t   wi t  ect  i    P Wt   N i   wi t   end for Figure    The EWA Algorithm N   number p of experts  T   number of rounds     min  log N T         Initialize wi       for each expert i  W    N   for t                do For any i  pi t   wi t   Wt    With probability t   wIt   t   wIt   t  choose the previously selected expert  It   It  and with probability    t   choose It based on the distribution qt    p  t           pN t    Learner takes the action suggested by expert It   The adversary chooses loss function ct   The learner suffers loss ct  It    For all experts i  wi t   wi t       ct  i    PN Wt   i   wi t   end for Figure    The Shrinking Dartboard Algorithm  Assumption A  Uniform Mixing There exists a constant      such that for all distributions d and d over the state space  any deterministic policy   and any model m  M   kdP    m   d P    m k   e   kd  d k    As discussed by Neu et al          if Assumption A  holds for deterministic policies  then it holds for all policies       Full Information Algorithms We would like to have a full information online learning algorithm that rarely changes its policy  The first candidate that we consider is the well known Exponentially Weighted Average  EWA  algorithm  Vovk        Littlestone and Warmuth        shown in Figure    In our MDP problem  the EWA algorithm chooses a policy    according to distribution   t  X E  ls  xs                   qt     exp  s    The policies that this EWA algorithm generates most likely are different in consecutive rounds and thus  the EWA algorithm might change its policy frequently  However  a variant of EWA  called Shrinking Dartboard  SD   Geulen et al         and shown in Figure    satisfies Assumption A   Our algorithm  called SD MDP  is based on the SD algorithm and is shown in Figure    Notice that the algorithm needs to know the number of rounds  T   in advance      T   number p of rounds     min  log     T         For all policies                    w        for t                do For any   p t   w t   Wt    With probability t   wt   t   wt   t  choose the previous policy  t   t    while with probability    t   choose t based on the distribution qt    p  t           p   t    Learner takes the action at  t    xt   Adversary chooses transition model mt and loss function lt   Learner suffers loss lt  xt   at    Learner observes mt and lt   Update state  xt    mt    xt   at     For allP policies   w t   w t       E lt  xt       Wt    w t   end for Figure    SD MDP  The Shrinking Dartboard Algorithm for Markov Decision Processes  Consider a basic full information problem with N experts  Let RT  SD  i  be the regret of the SD algorithm with respect to expert i up to time T   We have the following results for the SD algorithm  Theorem    For any expert i              N    p RT  SD  i     T log N   log N    and also for any    t  T    P  Switch at time t    r  log N   T  Proof  The proof of the regret bound can be found in  Geulen et al         Theorem     The proof of the bound on the probability of switch is similar to the proof of Lemma   in  Geulen et al         and is as follows  As shown in  Geulen et al         Lemma     the probability of switch at time t is t    Wt   Wt   Wt   Thus  Wt       t  Wt    Because the loss function is bounded in         we have that Wt    N X i    wi t    N X i    wi t       ct  i    Thus     t       and thus   t     r  N X i    wi t               Wt     log N   T      Analysis of the SD MDP Algorithm The main result of this section is the following regret bound for the SD MDP algorithm  Theorem    Let the loss functions selected by the adversary be bounded in         and the transition models selected by the adversary satisfy Assumption A   Then  for any policy     p E  RT  SD MDP                 T log      log       In the rest of this section  we write A to denote the SD MDP algorithm  For the proof we use the regret decomposition      RT  A      BT  A    CT  A                Bounding E  CT  A     Lemma    For any policy       T   T X X p t  E  CT  A       E lt  xt   t    lt  xt        T log      log      t    t    Proof  Consider the following imaginary game between a learner and an adversary  we have a set of experts  policies                          At round t  the adversary chooses a loss vector ct           whose ith element determines the loss of expert  i at this round  The learner chooses a distribution over experts qt  defined by the SD algorithm   from which it draws an expert t   Next  the learner observes the loss function ct   From the regret bound for the SD algorithm  Theorem     it is guaranteed that for any expert   T X t    hct   qt i   T X t    p ct       T log      log       Next  we determine how the adversary h chooses ithe loss vector  At time t  the adversary chooses a i i i loss function lt and sets ct       E lt  xt        Noting that hct   qt i   E  lt  xt t   t    and ct       E  lt  xt      finishes the proof         Bounding E  BT  A   First  we prove the following two lemmas  Lemma    For any state distribution d  any transition model m  and any policies  and     kdP    m   dP      m k   k    k     Proof  Proof is easy and can be found in  Even Dar et al          Lemma      p Lemma    Let t be the probability of a policy switch at time t  Then  t  log    T    Proof  Proof is identical to the proof of Theorem    Lemma    We have that E  BT  A     E    T X t    lt  xA t   at     T X t       lt  xt t   t         p log   T    Proof  Let Ft                t    Notice that the choice of policies are independent of the state variables  We can write   T   T X X t A E  BT  A     E lt  xt   at    lt  xt   t   t     E  E         E     E      E     E     T X  t    X   t   xX  T X X  t   xX  T X X  t   xX  T X t    T X t    T X t          I xA  I xt t  x  lt  x  t  x   t  x   E  h      I xA  I xt t  x  lt  x  t  x   FT t  x     i  i   h  t F lt  x  t  x  E I xA  I T  xt  x  t  x   klt k E  h   I xA  I xt t  x  t  x   klt k kut  vt t k     kut  vt t k             FT  i                 i h     A t where us   E I xA is the is the distribution of x for s  t and v   E I F F s t T T  x  s  x  x  s s distribution of xs t for s  t   Let Et be the event of a policy switch at time t  From inequality ktk  t k    ktk  tk   k          kt   t k       t X  I Es      s tk    and Lemma    we get that h  i  E ktk  t k       r  log    k  T       Let Pt   P    mt    We have that         t  t E kut  vt t k    E ut  Pt   vt  t Pt        t  t t t   E ut  Pt   ut  Pt    ut  Pt   vt  t Pt        t  t t t   ut  Pt   vt  t Pt   E ut  Pt   ut  Pt      h i  E kt   t k     e   kut   vt  t k  h t  t  E kt   t k     e     ut  Pt   ut  Pt    i t t     ut  Pt   vt  t Pt    h i  E kt   t k     e   kt   t k     e   kut   vt  t k          t X  k   t X  k       r  h i ek  E ktk  t k     et  ku   v  t k   e  k   r  log    k   T  By      log         T       where we have used the fact that ku   v  t k       because the initial distributions are identical  By     and      we get that r T X p log      E  BT  A            log   T   T t    What makes the analysis possible is the fact that all policies mix no matter what transition model is played by the adversary  Proof of Theorem    The result is obvious by Lemmas   and    The next corollary extends the result of Theorem   to continuous policy spaces  Corollary    Let  be an arbitrary policy space  N  o  be the o covering number of space    k k      and C o  be an o cover  Assume that we run the SD MDP algorithm on C o   Then  under the same assumptions as in Theorem    for any policy     p E  RT  SD MDP                 T log N  o    log N  o     T o      Notice that FT contains only policies  which are independent of the state variables       hP i T  Proof  Let LT      E l  x     be the value of policy   Let u t  x    P  xt   x   First  t t t   we prove that the value function is Lipschitz with Lipschitz constant  T   The argument is similar to the argument in the proof of Lemma    For any   and       T   T X X      LT       LT         E lt  xt        lt  xt       t         T X t    T X t    t    ku   t  u   t k  klt k ku   t  u   t k     With an argument similar to the one in the proof of Lemma    we can show that ku   t  u   t k    k     k     Thus   LT       LT         T k     k      Given this and the fact that for any policy     there is a policy    C o  such that k    k    o  we get that p E  RT  SD MDP                 T log N  o    log N  o     T o   In particular if  is the space of all policies  N  o     A  o  A  X    so regret is no more than r  A   A       A  X   log   T o   E  RT  SD MDP               T  A  X   log o o p By the choice of o   T    we get that E  RT  SD MDP       O    T  A   X   log  A T      
 We consider the problem of simultaneously learning to linearly combine a very large number of kernels and learn a good predictor based on the learnt kernel  When the number of kernels d to be combined is very large  multiple kernel learning methods whose computational cost scales linearly in d are intractable  We propose a randomized version of the mirror descent algorithm to overcome this issue  under the objective of minimizing the group p norm penalized empirical risk  The key to achieve the required exponential speed up is the computationally efficient construction of low variance estimates of the gradient  We propose importance sampling based estimates  and find that the ideal distribution samples a coordinate with a probability proportional to the magnitude of the corresponding gradient  We show the surprising result that in the case of learning the coefficients of a polynomial kernel  the combinatorial structure of the base kernels to be combined allows the implementation of sampling from this distribution to run in O log d   time  making the total computational cost of the method to achieve an  optimal solution to be O log d        thereby allowing our method to operate for very large values of d  Experiments with simulated and real data confirm that the new algorithm is computationally more efficient than its state of the art alternatives      Introduction  We look into the computational challenge of finding a good predictor in a multiple kernel learning  MKL  setting where the number of kernels is very large  In particular  we are interested in cases where the base kernels come from a space with combinatorial structure and thus their number d could be exponentially large  Just like some previous works  e g  Rakotomamonjy et al         Xu et al         Nath et al         we start with the approach that views the MKL problem as a nested  large scale convex optimization problem  where the first layer optimizes the weights of the kernels to be combined  More specifically  as the     objective we minimize the group p norm penalized empirical risk  However  as opposed to these works whose underlying iterative methods have a complexity of  d  for just any one iteration  following  Nesterov              Shalev Shwartz and Tewari        Richtarik and Takac        we use a randomized coordinate descent method  which was effectively used in these works to decrease the per iteration complexity to O     The role of randomization in our method is to use it to build an unbiased estimate of the gradient at the most recent iteration  The issue then is how the variance  and so the number of iterations required  scales with d  As opposed to the above mentioned works  in this paper we propose to make the distribution over the updated coordinate dependent on the history  We will argue that sampling from a distribution that is proportional to the magnitude of the gradient vector is desirable to keep the variance  actually  second moment  low and in fact we will show that there are interesting cases of MKL  in particular  the case of combining kernels coming from a polynomial family of kernels  when efficient sampling  i e   sampling at a cost of O log d   is feasible from this distribution  Then  the variance is controlled by the a priori weights put on the kernels  making it potentially independent of d  Under these favorable conditions  and in particular  for the polynomial kernel set with some specific prior weights   the complexity of the method as a function of d becomes logarithmic  which makes our MKL algorithm feasible even for large scale problems  This is to be contrasted to the approach of Nesterov              where a fixed distribution is used and where the a priori bounds on the methods convergence rate  and  hence  its computational cost to achieve a prescribed precision  will depend linearly on d  note that we are comparing upper bounds here  so the actual complexity could be smaller   Our algorithm is based on the mirror descent  or mirror descent  algorithm  similar to the work of Richtarik and Takac        who uses uniform distributions   It is important to mention that there are algorithms designed to handle the case of infinitely many kernels  for example  the algorithms by Argyriou et al                Gehler and Nowozin         However  these methods lack convergence rate guarantees  and  for example  the consistency for the method of Gehler and Nowozin        works only for small d  The algorithm of Bach         though practically very efficient  suffers from the same deficiency  A very interesting proposal by Cortes et al         considers learning to combine a large number of kernels and comes with guarantees  though their algorithm restricts the family of kernels in a specific way  The rest of the paper is organized as follows  The problem is defined formally in Section    Our new algorithm is presented and analyzed in Section    while its specialized version for learning polynomial kernels is given in Section    Finally  experiments are provided in Section        Preliminaries  In this section we give the formal definition of our problem  Let I denote a finite index set  indexing the predictors  features  the set of predictors considered   to be combined  and define P over the input space X as F   fw   X  R   fw  x    iI hwi   i  x i   x  X   Here Wi is a Hilbert space over the reals  i   X  Wi is a feature map  hx  yi is the inner product   over the Hilbert space that x  y belong to and w    wi  iI  W   iI Wi  as an example  Wi may just be a finite dimensional Euclidean space   The problem we consider is to solve      the optimization problem minimize Ln  fw     Pen fw    subject to w  W   and Ln  fw     n  convex losses  t      Pn  where Pen fw   is a penalty that will be specified later  t    t  fw  xt   is the empirical risk of predictor fw   defined in terms of the   R  R     t  n  and inputs xt  X     t  n   The solution w of the above penalized empirical risk minimization problem is known to have favorable generalization properties under various conditions  see  e g   Hastie et al          In supervised learning problems  t  y      yt   y  for some loss function     R  R  R  such as the squared loss    yt   y        y  yt      or the hinge loss   t  yt   y    max    yyt       where in the former case yt  R  while in the latter case yt           We note in passing that for the sake of simplicity  we shall sometimes abuse notation and write Ln  w  for Ln  fw   and even drop the index n when the sample size is unimportant  As mentioned above  in this paper we consider the special case in     when the penalty is a so called group p norm penalty with    p     a case considered earlier  e g   by Kloft et al          Thus our goal is to solve    p   X p p minimize Ln  w    i kwi k        wW   iI  where the scaling factors i      i  I  are assumed to be given  We introduce the notation u    ui    RI to denote the column vector obtained from the values ui   The rationale of using the squared weighted p norm is that for    p     it is expected to encourage sparsity at the group level which should allow one to handle cases when I is very large  and the case p     comes for free from the same analysis   The actual form  however  is also chosen for reasons of computational convenience  In fact  the reason to use the   norm of the weights is to allow the algorithm to work even with infinite dimensional feature vectors  and thus weights  by resorting to the kernel trick  To see how this works  just notice that the penalty in     can also be written as        p   kw k  X p X  i   i i kwi kp    inf    p    p i iI  iI  where for                   I    kk     is the positive quadrant of the  I  dimensional    ball  see  e g   Micchelli and Pontil        Lemma      Hence  defining   X  i kwi k   J w      L w      i iI  for any w  W           I    an equivalent form of     is minimize J w     wW        where    p     p         and we define         and u      for u      which implies that wi     if i      That this minimization problem is indeed equivalent to our original task     for the chosen value of  follows from the fact that J w    is jointly convex in  w         Here and in what follows by equivalence we mean that the set of optimums in terms of w  the primary optimization variable  is the same in the two problems       Let i   X  X  R be the reproducing kernel underlying i   i  x  x      hi  x   i  x   i  x  x   X   and let Hi   Hi the corresponding reproducing kernel Hilbert space  RKHS   Then  for any given fixed value of   the above problem becomes an instance P of a standard penalized learning problem in the RKHS H underlying the kernel    iI i   i i   In particular  by the theorem on page     in Aronszajn         the problem of finding w  W for fixed  can be seen to be equivalent to minimizef H L f        kf k H   and thus     is seen to be equivalent to minimizef H   L f        kf k H   Thus  we see that the method can be thought of as finding the weights of a kernel  and a predictor minimizing the H norm penalized empirical risk  This shows that our problem is an instance of multiple kernel learning  for an exhaustive survey of MKL  see  e g   Gonen and Alpaydn       and the references therein       The new approach  When I is small  or moderate in size  the joint convexity of J allows one to use off the shelf solvers to find the joint minimum of J  However  when I is large  off the shelf solvers might be slow or they may run out of memory  Targeting this situation we propose the following approach  Exploiting again that J w    is jointly convex in  w     find the optimal weights by finding the minimizer of   J     inf J w     w    or  alternatively  J     J w        where w      arg minw J w     here we have slightly abused notation by reusing the symbol J   Note that J   is convex by the joint convexity of J w     Also  note that w    exists and is well defined as the minimizer of J     is unique for any     see also Proposition     below   Again  exploiting the joint convexity of J w     we find that if  is the minimizer of J    then w     will be an optimal solution to the original problem      To optimize J   we propose to use stochastic gradient descent with artificially injected randomness to avoid the need to fully evaluate the gradient of J  More precisely  our proposed algorithm is an instance of a randomized version of the mirror descent algorithm  Rockafellar        Martinet        Nemirovski and Yudin         where in each time step only one coordinate of the gradient is sampled        A randomized mirror descent algorithm  Before giving the algorithm  we need a few definitions  Let d    I   A  Rd be nonempty with a convex interior A   We call the function    A  R a Legendre  or barrier  potential if it is strictly convex  its partial derivatives exist and are continuous  and for every sequence  xk    A approaching the boundary of A  limk k xk  k     Here  is the gradient  operator   x      x  x    is the gradient of   When  is applied to a non smooth   convex function J     J may be such without additional assumptions  then J      is defined as any subgradient of J   at   The corresponding Bregman divergence D   A  A  R is defined as D                    h          i  The Bregman projection  K   A  K corresponding to the Legendre potential  and a closed convex set K  Rd such that K  A     is defined  for all   A as  K      arg min  KA D         Algorithm   shows a randomized version of the standard mirror descent method with an unbiased gradient estimate  By assumption  k     is deterministic  Note that step   of the     Algorithm   Randomized mirror descent algorithm    Input  A  K  Rd   where K is closed and convex with K  A         A  R Legendre  step sizes  k    a subroutine  GradSampler  to sample the gradient of J at an arbitrary vector        Initialization         arg minKA     k         repeat    k   k       k        Obtain gk   GradSampler     k        arg minA k  hgk   i   D     k           k     K   k        until convergence  algorithm is well defined since  k   A by the assumption that k x k tends to infinity as x approaches the boundary of A  The performance of Algorithm   is bounded in the next theorem  The analysis follows the standard proof technique of analyzing the mirror descent algorithm  see  e g   Beck and Teboulle         however  in a slightly more general form than what we have found in the literature  In particular  compared to  Nemirovski et al       a  Nesterov              Shalev Shwartz and Tewari        Richtarik and Takac         our analysis allows for the conditional distribution of the noise in the gradient estimate to be history dependent  The proof is included in Section A in the appendix  Theorem      Assume that  is  strongly convex with respect to some norm k  k  with dual norm k  k   for some       that is  for any   A      A                       k   k          Suppose  furthermore  that Algorithm   is run for T time steps  For    k  T    let Fk denote the  algebra generated by             k   Assume that  for all    k  T   gk  Rd is an unbiased estimate of J  k     given Fk    that is  E   gk   Fk      J  k            Further  assume that there exists a deterministic constant B    such that for all    k  T       E kgk k  Fk   B a s      Finally  assume that    sup  KA              is finite  Then  if k    k     it holds that      r T   X  k    B E J   inf J      KA T T  q    BT  for all       k    Furthermore  if kgk k   B       a s         B   for some deterministic constant and k    it holds with probability at least      that J  T   X  k    T  q    B T     r  inf J     k    for all k    then  for any              B    T  KA  s     B    log      T       The convergence rate in the above theorem can be improved if stronger assumptions are made on J  for example if J is assumed to be strongly convex  see  for example   Hazan et al         Hazan and Kale         Efficient implementation of Algorithm   depends on efficient implementations of steps      namely  computing an estimate of the gradient  solving the minimization for  k    and projecting it into K  The first problem is related to the choice of gradient estimate we use  which  in turn  depends on the structure of the feature space  while the last two problems depend on the choice of the Legendre function  In the next subsections we examine how these choices can be made to get a practical variant of the algorithm        Application to multiple kernel learning  It remains to define the gradient estimates gk in Algorithm    We start by considering importance sampling based estimates  First  however  let us first verify whether the gradient exist  Along the way  we will also derive some explicit expressions which will help us later  Closed form expressions for the gradient  Let us first consider how w    can be calculated for a fixed value of   As it will turn out  this calculation will be useful not only when the procedure is stopped  to construct the predictor fw    but also during the iterations when we will need to calculate the derivative of J with respect to i   The following proposition summarizes how w    can be obtained  Note that this type of result is standard  see  e g   Shawe Taylor and Cristianini        Scholkopf and Smola         thus we include it only for the sake of completeness  the proof is included in Section A in the appendix   Proposition      For    t  n  let  t   R  R denote the convex conjugate of  t    t  v       i  and let K   sup R  v   t       v  R  For i  I  recall that i  x  x      hi  x   i  xP i  i  xt   xs    t sn be the n  n kernel matrix underlying i and let K   iI  i Ki be the i P i kernel matrix underlying       Then  for any fixed   the minimizer w    of i   iI i J     satisfies n i X  wi           i  xt    i  I        i t   t where          arg min Rn  n       X   K     t  nt     n             t    Based on this proposition  we can compute the Ppredictor fw    usingPthe kernels  i  iI and the dual variables  t     tn   fw     x    iI hwi     i  x i   nt   t     xt   x         Let us now consider the differentiability of J   J   and how to compute its derivatives  Under proper conditions with standard calculations  e g   Rakotomamonjy et al         we find that J is differentiable over  and its derivative can be written as              Ki     J               i iI Importance sampling based estimates  Let d    I  and let ei   i  I denote the ith unit vector of the standard basis of Rd   that is  the ith coordinate of ei is   while the others are    Introduce D E gk i   J  k      ei   i  I      to denote the ith component of the gradient of J in iteration k  that is  gk i can be computed based on        Let sk         I be a distribution over I  computed in some way based on the information available up to the end of iteration k    of the algorithm  formally  sk  is Fk   measurable   Define the importance sampling based gradient estimate to be gk i    I Ik  i  gk Ik   sk  Ik  i  I  where Ik  sk            That is  the gradient estimate is obtained by first sampling an index from sk   and then setting the gradient estimate to be zero at all indices i  I except when i   Ik in which gk I case its value is set to be the ratio sk  Ik   It is easy to see that as long as sk  i     holds k  whenever gk i       then it holds that E   gk   Fk      J  k     a s  Let us now derive the conditions under which the second moment of the gradient estimate stays bounded  Define Ck    J  k         Given the expression for the gradient of J shown in       we see that supk  Ck     will always hold provided that     is continuous since   k    k  is guaranteed to belong to a compact set  the continuity of  is discussed in Section B in the appendix     Define the probability distribution qk   as follows  qk  i   Ck   gk i     i  I  Then   qk  I      keIk k    s  k Ck  keIk k    Therefore  it also holds it holds that kgk k    s    gk I k k  Ik k  Ik       P qk  i qk  i         that E kgk k  Fk    Ck  iI sk  i kei k  Ck  maxiI sk  i kei k   This shows that     q supk  E kgk k  Fk     will hold as long as supk  maxiI sk  i    and supk  Ck    k  i   Note that when sk    qk    the gradient estimate becomes gk i   Ck  I It  i    That is  in this case we see that in order to be able to calculate gk i   we need to be able to calculate Ck  efficiently   Choosing the potential   The efficient sampling of the gradient is not the only practical issue  since the choice of the Legendre function and the convex set K may also cause some P complications  For example  if  x    iI xi  ln xi      then the resulting algorithm is exponential weighting  and one needs to store and update  I  weights  which is clearly infeasible if  I  is very large  or infinite   On the other hand  if  x       kxk   and we project    For completeness  the calculations are given in Section B in the appendix       Algorithm   Projected stochastic gradient algorithm                          Initialization   x       kxk     i     for all i  I  k      step sizes  k    repeat k   k      Sample a gradient estimate gk of g  k    randomly according to        k         k    k  gk    until convergence   to K       the positive quadrant of the     ball  with A        I    we obtain a stochastic projected gradient method  shown in Algorithm    This is in fact the algorithm that we use in the experiments  Note that in     this corresponds to using p        The reason we made this choice is because in this case projection is a simple scaling operation  Had we chosen K       the     projection would very often cancel many of the nonzero components  resulting in an overall slow progress  Based on the above calculations and Theorem     we obtain the following performance bound for our algorithm  Corollary      Assume that     is continuous on     Then there exists a C     such q  that k  J  k   C for all       Let B      C   maxiI  kT sk  i   If Algorithm   is run k  i  for T steps with k          BT   k              T   then  for all            r T   X  k   B  J       E J T T k    Note that to implement Algorithm   efficiently  one has to be able to sample from sk   and compute the importance sampling ratio gk i  sk i efficiently for any k and i      Example  Learning polynomial kernels  In this section we show how our method can be applied in the context of multiple kernel learning  We provide an example when the kernels in I are tensor products of a set of base kernels  this we shall call learning polynomial kernels   The importance of this example follows from the observation of Gonen and Alpaydn        that the non linear kernel learning methods of Cortes et al          which can be viewed as a restricted form of learning polynomial kernels  are far the best MKL methods in practice and can significantly outperform state of the art SVM with a single kernel or with the uniform combination of kernels  Assume that we are given a set of base kernels              r    In this section we consider the set KD of product kernels of degree at most D  Choose I     r            rd        Q d  D     ri  r  and the multi index r  d    r            rd    I defines the kernel r  d  x  x      di   ri  x  x     For d     we define r     x  x         Note that indices that are the permutations of each other define the same kernel  On the language of statistical modeling  r  d models interactions of order d between the features underlying the base kernels             r   Also note that  I     rD    that is  the cardinality of I grows exponentially fast in D  We assume that r  d depends only on d  the order of interactions in r  d   By abusing      Algorithm   Polynomial kernel sampling  The symbol denotes the Hadamard product power     Input    Rn   the solution to the dual problem  kernel matrices  K            Kr    the degree of the polynomial kernel  the weights                D    PD r      S  j   KDj   M   E         d     d  M  S d   d               D  PD    Sample d from     d      d       for i     to d do tr M S  di  K       j   tr M S  di    j    j              r     Sample zi from       M  M K zi    end for     return  z            zd   notation  we will write d in the rest of this section to emphasize this   Our proposed algorithm to sample from qk   is shown in Algorithm    The algorithm is written to return a multi index  z      P       zd   that isP drawn from qk     The key idea underlying the algorithm r d is to exploit that   j   j     r  d I r  d   The correctness of the algorithm is shown in Section      In the description of the algorithm denotes the matrix entrywise product  a k a  Schur  or Hadamard product  and A s denotes A   z     A   and we set the priority   s  of A  to be higher than that of the ordinary matrix product  by definition  all the entries of   are     Let us now discuss the complexity of Algorithm    For this  first note that computing all   the Hadamard products S d   d               D requires O Dn    computations  Multiplication with Mk  can be done in O n    steps  Finally  note that each iteration of the for loop takes O rn    steps  which results in the overall worst case complexity of O rn  D  if   k    is readily available  The computational complexity of determining   k    depends on the exact form of  t   and can be done efficiently in many situations  if  for example   t is the squared loss  then  can be computed in O n    time  An obvious improvement to the approach described here  however  would be to subsample the empirical loss Ln   which can bring further computational improvements  However  the exploration of this is left for future work  Finally  note that despite the exponential cardinality of  I   due to the strong algebraic structure of the space of kernels  Ck  can be calculated In fact  it is not hard to P efficiently       This also shows that if  see that with the notation of the algorithm  Ck    D  d   d d    decays fast enough  Ck  can be bounded independently of the cardinality of I        Correctness of the sampling procedure  In this section we prove the correctness of Algorithm    As said earlier  we assume that r  d depends only on d  the order of interactions in r  d    Using importance sampling  more general weights can also be accommodated  too without effecting the results as long as the range of weights  r  d   is kept under control for all d       and  by abusing notation  we will write d to emphasize this  Let us P how one can Pnow consider sample from qk     The implementation relies on the fact that   rj   j  d   r  d I r  d   Remember that we denoted the kernel matrix underlying some kernel k by Kk   and recall that Kk is an n  n matrix  For brevity  in the rest of this section for    r  d we will write Kr  d instead of Kr  d   Define Mk      k     k       Thanks to      and the rotation property of trace  we have      gk r  d     d tr Mk  Kr  d     P The plan to sample from qk      gk     r  d I  gk r  d   is as follows  We first draw the order of interactions     d  D  Given d   d  we restrict the draw of the random multi index  R  d to the set  r  d  I   A multi index will be sampled in a d step process  in each step we will randomly choose an index from the indices of base kernels according to the following distributions  Let S   K            Kr   let       tr Mk  S d   P d   d Fk    PD d   d    d     d  tr Mk  S and  with a slight abuse of notation  for any    i  d define     P Ri   ri  Fk    d   d  R  i    r  i          i  di  tr Mk  K S j   rj           P r i   di    tr M K K S   r k  r j r    j   i i  where we used the sequence notation  namely  s  p denotes the sequence  s            sp     We have  by the linearity of trace and the definition of S that r X    tr Mk      i  j   Krj     Kri   S   di      ri          tr Mk      i  j   Krj     S   di        Thus  by telescoping      P d   d  R  d   r  d  Fk             Krd  Krd   d tr Mk  Kr    PD   d    d     d  tr Mk  S  as desired  An optimized implementation of drawing these random variables is shown as Algorithm    The algorithm is written to return the multi index R  d       Experiments  In this section we apply our method P to the problem of multiple kernel learning in regression with the squared loss  L w       nt    fw  xt    yt      where  xt   yt    Rr  R are the inputoutput pairs in the data  In these experiments our aim is to learn polynomial kernels  cf  Section          We compare our method against several kernel learning algorithms from the literature on synthetic and real data  In all experiments we report mean squared error over test sets  A constant feature is added to act as offset  and the inputs and output are normalized to have zero mean and unit variance  Each experiment is performed with    runs in which we randomly choose training  validation  and test sets  The results are averaged over these runs        Convergence speed  In this experiment we examine the speed of convergence of our method and compare it against one of the fastest standard multiple kernel learning algorithms  that is  the p norm multiple kernel learning algorithm of Kloft et al         with p       and the uniform coordinate descent algorithm that updates one coordinate per iteration uniformly at random  Nesterov              Shalev Shwartz and Tewari        Richtarik and Takac         We aim to learn polynomial kernels of up to degree   with all algorithms  Our method uses Algorithm   for sampling with D      The set of provided base kernels is the linear kernels built from input variables  that is   i   x  x      x i  x  i    where x i  denotes the ith input variable  For the other two algorithms the kernel set consists of product kernels from monomial terms for D               built from r base kernels   where r is the number of input variables  The number of distinct product kernels is r D D   In this experiment for all algorithms we use ridge regression with its regularization parameter set to       Experiments with other values of the regularization parameter achieved similar results  We compare these methods in four datasets from the UCI machine learning repository  Frank and Asuncion        and the Delve datasets    The specifications of these datasets are shown in Table    We run all algorithms for a fixed amount of time and measure the value Table    Specifications of datasets used in experiments  Dataset german ionosphere ringnorm sonar splice waveform    of variables                    Training size                         Validation size                           Test size                             of the objective function      that is  the sum of the empirical loss and the regularization term  Figure   shows the performance of these algorithms  In this figure Stoch represents our algorithms  Kloft represents the algorithm of Kloft et al          and UCD represents the uniform coordinate descent algorithm  The results show that our method consistently outperforms the other algorithms in convergence speed  Note that our  stochastic method updates one kernel coefficient per iteration  while Kloft updates r D kernel coefficients D per iteration  The difference between the two methods is analogous to the difference between stochastic gradient vs  full gradient algorithms  While UCD also updates one kernel    Note that p     in Kloft et al         notation corresponds to p       or      in our notation  which gives the same objective function that we minimize with Algorithm      See  www cs toronto edu  delve data datasets html               german  objective function  ionosphere        Kloft Stoch UCD  ringnorm                   waveform                                                                                                         time  sec                                       time  sec                               time  sec                        time  sec         Figure    Convergence comparison of our method and other algorithms  coefficient per iteration its naive method of selecting coordinates results in a slower overall convergence compared to our algorithm  In the next section we compare our algorithm against several representative methods from the MKL literature        Synthetic data  In this experiment we examine the effect of the size of the kernel space on prediction accuracy and training time of MKL algorithms  We generated data for a regression problem  Let r denote the number of dimensions of the input space  The inputs are chosen uniformly at random from       r   The output of each instance is the uniform combination of    monomial terms of degree   or less  These terms are chosen uniformly at random among all possible terms  The outputs are noise free  We generated data for r                            with     training and      test points  The regularization parameter of the ridge regression algorithm was tuned from                      using a separate validation set with      data points  We compare our method  Stoch  against the algorithm of Kloft et al          Kloft   the nonlinear kernel learning method of Cortes et al          Cortes   and the hierarchical kernel learning algorithm of Bach         Bach    The set of base kernels consists of r linear kernels built from the input variables  P Recall that the method of Cortes et al         only considers kernels of the form      ri   i i  D   where D is a predetermined integer that specifies the degree of nonlinear kernel  Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too  We provide all possible product kernels of degree   to D to the kernel learning method of Kloft et al          For our method and the method of Bach        we set the maximum kernel degree to D      The results are shown in Figure    the mean squared errors are on the left plot  while the training times are on the right plot  In the training time plot the numbers inside brackets   While several fast MKL algorithms are available in the literature  such as those of Sonnenburg et al          Rakotomamonjy et al          Xu et al          Orabona and Luo         Kloft et al          a comparison of the reported experimental results shows that from among these algorithms the method of Kloft et al         has the best performance overall  Hence  we decided to compare against only this algorithm  Also note that the memory and computational cost of all these methods still scale linearly with the number of kernels  making them unsuitable for the case we are most interested in  Furthermore  to keep the focus of the paper we compare our algorithm to methods with sound theoretical guarantees  As such  it remains for future work to compare with other methods  such as the infinite kernel learning of Gehler and Nowozin         which lack such guarantees but exhibit promising performance in practice               Kloft Stoch Cortes Bach Uniform               training time  sec         MSE                                                        number of dimensions of input space                                                                                Figure    Comparison of kernel learning methods in terms of test error  left  and training time  right   indicate the total number of distinct product kernels for each value of r  This is the number of kernels fed to the Kloft algorithm  Since this method deals with a large number of kernels  it was possible to precompute and keep the kernels in memory   GB  for r      Therefore  we ran this algorithm for r      For r       we could use on the fly implementation of this algorithm  however that further increases the training time  Note that the computational cost of this method depends linearly on the number of kernels  which in this experiment  is cubic in the number of input variables since D      While the standard MKL algorithms  such as Kloft  cannot handle such large kernel spaces  in terms of time and space complexity  the other three algorithms can efficiently learn kernel combinations  However their predictive accuracies are quite different  Note that the performance of the method of Cortes et al         starts to degrade as r increases  This is due to the restricted family of kernels that this method considers  The method of Bach         which is well suited to learn sparse combination of product kernels  performs better than Cortes et al         for higher input dimensions  Among all methods  our method performs best in predictive accuracy while its computational cost is close to that of the other two competitors        Real data  In this experiment we aim to compare several MKL methods in real datasets  We compare our new algorithm  Stoch   the algorithm of Bach         Bach   and the algorithm of Cortes et al          Cortes   For each algorithm we consider learning polynomial kernels of degree   and P    We also include uniform combination of product kernels of degree D  i e  D     ri   i  D   for D             Uniform   To find out if considering higherorder interaction of input variables results in improved performance we also included a MKL algorithm to which we only feed linear kernels  D       We use the MKL algorithm of Kloft et al         with p          Kloft   We compare these methods on six datasets from the UCI machine learning repository       ionosphere  german  ringnorm        Bach  d    Bach  d     MSE  Stoch  d    Stoch  d    Stoch  d    prior                                                               Cortes  d    Cortes  d    Kloft  p    Kloft  p    Uniform  d    Uniform  d    Uniform  d     sonar  splice  waveform                                                                                 Figure    Prediction error of different methods in the real data experiment and Delve datasets  In these datasets the number of dimensions of the input space is    and above  The specifications of these datasets are shown in Table    The regularization parameter is selected from the set                      for all methods using a validation set  The results are shown in Figure    Overall  we observe that methods that consider non linear variable interactions  Stoch  Bach  and Cortes  perform better than linear methods  Kloft   Among non linear methods  Cortes performs worse than the other two  We believe that this is due to the restricted kernel space considered by this method  The performance of Stoch and Bach methods is similar overall  We observe that our method overfits when it considers kernels of degree    However  one can easily prevent overfitting by assigning larger  values to higher degree kernels such that the stochastic algorithm selects lower degree kernels more often  For this purpose  we repeat this experiment for D     with a modified set of  values  where we use  d     for kernels of degree   or less and  d     for kernels of degree    With the new  coefficients we observe an improvement in algorithms performance  See Stoch  D      prior  error values in Figure        Conclusion  We introduced a new method for learning a predictor by combining exponentially many linear predictors using a randomized mirror descent algorithm  We derived finite time performance bounds that show that the method efficiently optimizes our proposed criterion  Our proposed method is a variant of a randomized stochastic coordinate descent algorithm  where the main trick is the careful construction of an unbiased randomized estimate of the gradient vector that keeps the variance of the method under control  and can be computed efficiently when the base kernels have a certain special combinatorial structure  The efficiency of our method      was demonstrated for the practically important problem of learning polynomial kernels on a variety of synthetic and real datasets comparing to a representative set of algorithms from the literature  For this case  our method is able to compute an optimal solution in polynomial time as a function of the logarithm of the number of base kernels  To our knowledge  ours is the first method for learning kernel combinations that achieve such an exponential reduction in complexity while satisfying strong performance guarantees  thus opening up the way to apply it to extremely large number of kernels  Furthermore  we believe that our method is applicable beyond the case studied in detail in our paper  For example  the method seems extendible to the case when infinitely many kernels are combined  such as the case of learning a combination of Gaussian kernels  However  the investigation of this important problem remains subject to future work  Acknowledgements This work was supported by Alberta Innovates Technology Futures and NSERC   A  Proofs  In this section we present the proofs of Theorem     and Proposition      The proof of Theorem     is based on the standard proof of the convergence rate of the proximal point algorithm  see  for example   Beck and Teboulle         or the proof of Proposition     of Nemirovski et al       b   which carry over the same argument to solve very similar but less general problems  We also provide some improvements and simplifications at the end  Before giving the actual proof  we need the following standard lemma  Lemma A    Lemma     of Nemirovski et al      b   Assume that  is  strongly convex with respect to some norm k  k  i e       holds   Let    K  A     K  A  and g  Rd   Define     arg min  KA  hg    i   D            Then hg     i  D         D           kgk       We provide an alternate proof that is based on the so called   DIV lemma  The   DIV lemma  e g   Lemma       Cesa Bianchi and Lugosi        allows one to express the sum of the divergences between the vectors u  v and v  w in terms of the divergence between u and w and an additional error term  where u  A  v  w  A   D  u  v    D  v  w    D  u  w    h w    v   u  vi   Proof  Note that    A due to behavior of  at the boundary of A  Thus   is differentiable at   and   D                                where   denotes differentiation of D w r t  its first variable  Let f        hg    i D           By the optimality property of   and since   K  A  we have hf          i           Plugging in the definition of f together with the identity      gives hg                  i             Now  by the   DIV Lemma  D          D            D          h                i   D          hg                  i   hg      i   Hence  by reordering and using the inequality      we get D         D         hg      i  D            hg       i  D            hg      i   kgk    hg      i      where in the last line we used Youngs inequality  and that due to the strong convexity of     D             k     k    Theorem      Assume that  is  strongly convex with respect to some norm k  k  with dual norm k  k   for some       that is  for any   A      A                       k   k          Suppose  furthermore  that Algorithm   is run for T time steps  For    k  T    let Fk denote the  algebra generated by             k   Assume that  for all    k  T   gk  Rd is an unbiased estimate of J  k     given Fk    that is  E   gk   Fk      J  k            Further  assume that there exists a deterministic constant B    such that for all    k  T       E kgk k  Fk   B a s      q Finally  assume that    sup  KA              is finite  Then  if k      BT for all k     it holds that      r T   X  k    B E J   inf J          KA T T k    Furthermore  if kgk k   B   a s      q   for some deterministic constant B   and k    B   T for all k    then  for any            it holds with probability at least      that s   r T B    log      X  k    B    J   inf J             KA T T T k       Youngs inequality states that for any x  y vectors and       hx  yi  kxk kyk               kxk         kyk      P  T   Proof  Introduce the average learning rates  k   k   Tk   k    k              T   the averaged parameter estimates T X  T    T       k   k   k    and choose some   K  A  To prove the first part of the theorem  it suffices to show that    T      k   the bound holds for J     J     Define gk   J    By the convexity of J    we have T             X  T    k  J  k    J    J  T     J           k   T X k   T X  D E  T    k  gk    k     T E X E D D  T    T    k  gk    k        k  gk  gk    k          k    k    Notice that the first term on the right hand side above is the sum of linearized losses appearing in the standard analysis of the proximal point algorithm with loss functions gk and learning  T   rates  k    and the second sum contains the term that depends on how well gk estimates the gradient gk   Thus  in this way  it is separated how the proximal point algorithm and the gradient estimate effect the convergence rate of the algorithm  The first sum can be bounded by invoking the standard bound for the proximal point algorithm  we will give the very short proof for completeness  based on Lemma A     while the second sum can be analyzed by noticing that  by assumption      its elements form an  Fk   adapted martingale difference sequence  To bound the first sum  first note that the conditions of Lemma A   are satisfied for  T        k           g    k  gk   since    K  A  as mentioned beforehand  this follows from the behavior of  at the boundary of A   Further  note that due to the so called projection lemma  i e   the D  projection of the unconstrained optimizer is the same as the optimizer of the constrained optimization problem  we can conclude that  k        where   is defined in Lemma A    Thus  Lemma A   gives D E    kgk k  k  gk    k      D      k      D      k     k      Summing the above inequality for k              T   the divergence terms cancel each other  yielding   T T D E X X      T      k  gk    k      PT k  kgk k    D            D      T          k   k  k   k        Let us now turn to the second sum  We start with developing a bound on the expected  T   regret  For any    k  T   by construction  k  and  k   are Fk   measurable  This  together with     gives D D h E i E  T    T   E  k  gk  gk      k   Fk     k  gk  E   gk   Fk         k                   Combining this result with      and      yields h     i E J  T    J            D       k  k     PT        k   k  B   PT k   k  PT            D        T      T          X        k  E E kgk k Fk    k          where we used the tower rule to bring in the bound      the nonnegativity of Bregman divergences  and D                       the latter holds as                  q since     minimizes  on K  Substituting k         BT   k              T finishes the proof of        To prove the high probability result      notice that thanks to     k  gk  gk      k   is an  Fk   adapted martingale difference sequence  cf         By the strong convexity of  we have   k   k   k     k               Furthermore  conditions     and     imply that kgk k   B   a s   and so by     we have  kgk  gk k    B   a s  Then by Holders inequality r E D  B      k     k   gk  gk       kgk  gk k k   k     Thus  by the Hoeffding Azuma inequality  see  e g   Lemma A    Cesa Bianchi and Lugosi         for any           we have  with probability at least       v   u T T D E u B  X X      T     k   t    k  gk  gk       PT k  ln            k  k   k   k   Combining      with     implies an almost sure upper bound on the first sum on the right hand side of      as in      with B   in place of B  This  together q with      proves the required high probability bound     when substituting k            B T      Proposition      For    t  n  let  t   R  R denote the convex conjugate of  t    t  v       i  and let K   sup R  v   t       v  R  For i  I  recall that i  x  x      hi  x   i  xP i  i  xt   xs    t sn be the n  n kernel matrix underlying i and let K   iI  i Ki be the i P i  kernel matrix underlying    iI  i i   Then  for any fixed   the minimizer w    of J     satisfies n i X  wi           i  xt    i  I        i t   t where            arg min Rn  n       X   K     t  nt     n t                    Proof  By introducing the variables     t   tn  Rn and using the definition of L we can write the optimization problem     as the constrained optimization problem n  minimizen  wW  R   X   X  i kwi k    t  t     n   i t    s t  t    X  hwi   i  xt  i          iI  iI  In what follows  we call this problem the primal problem  The Lagrangian of this problem is     n n X   X  i kwi k   X    X hwi   i  xt  i   L w         t  t       t t  n   i t    t    iI  iI  Rn  where     t   tn  is the vector of Lagrange multipliers  or dual variables  associated   with the n equality constraints  The Lagrange dual function  g     inf w  L w       can be readily seen to satisfy   n      X  g       K     t  nt       n t    Now  since the objective function of the primal problem is convex and the primal problem involves only affine equality constraints and the primal problem is clearly feasible  by Slaters condition  p      Boyd and Vandenberghe         if     is the maximizer of g   then w      arg min infn L w         wW  R     n X   kwi k  X   i   arg min  t hwi   i  xt  i    i wW t    iI  The minimum of the last expression is readily seen to be equal to the expression given in       thus finishing the proof     B  Calculating the derivative of J    In this section we show that under mild conditions the derivative of J exist and we also give explicit forms  These derivations are quite standard and a similar argument can be found in the paper by  e g   Rakotomamonjy et al         specialized to the case when  t is the hinge loss  As it is well known  thanks to the implicit function theorem  e g   Brown and Page           Theorem         provided that J   J w    is such that w J w    and w J w    are continuous  the gradient of J   can be computed by evaluating the partial derivative     J w    of J w    with respect to  at  w         that is   J      J w    w w     Note that the derivative is well defined only if       that is  when no coordinates of  is zero  in which case        i kwi   k     J w                  i  iI If i     for some i  I  we define the derivative in a continuous manner as  J       lim                     J              assuming that the limit exists  From      we get  for any i  I  kwi   k     Combining with      we obtain             Ki        J w            i iI  i        Ki       i  Now  by      and the implicit function theorem      is a continuous function of  provided that the functions  t     t  n  are twice continuously differentiable  This shows that under the conditions listed so far  the limit in      exists  In the application we shall be concerned with  these conditions can be readily verified   
 Motivated by value function estimation in reinforcement learning  we study statistical linear inverse problems  i e   problems where the coefficients of a linear system to be solved are observed in noise  We consider penalized estimators  where performance is evaluated using a matrix weighted two norm of the defect of the estimator measured with respect to the true  unknown coefficients  Two objective functions are considered depending whether the error of the defect measured with respect to the noisy coefficients is squared or unsquared  We propose simple  yet novel and theoretically well founded data dependent choices for the regularization parameters for both cases that avoid datasplitting  A distinguishing feature of our analysis is that we derive deterministic error bounds in terms of the error of the coefficients  thus allowing the complete separation of the analysis of the stochastic properties of these errors  We show that our results lead to new insights and bounds for linear value function estimation in reinforcement learning      Introduction Let A be a real valued md matrix  b be a real valued m dimensional vector  M be an m  m positive semidefinite matrix  and consider the loss function LM   Rd  R defined by   LM      kA  bkM   Appearing in Proceedings of the    th International Conference on Machine Learning  Edinburgh  Scotland  UK        Copyright      by the author s  owner s    where kkM denotes the M matrix weighted two norm  We consider the problem of finding a minimizer of this loss when instead of A  b  one has access only to their respective noisy versions  A  b  We call this problem a statistical linear inverse problem  Our main motivation to study this problem is to better understand the so called least squares approach to value function estimation in reinforcement learning  whose goal is to estimate the value function that corresponds to a Markov reward process   The leastsquares approach originates from the work of Bradtke and Barto         who proposed to find the parametervector  of a linear in the parameters value function by solving A   b where the noisy matrix vector pair   A  b   is computed based on a finite sample  They have proven the almost sure convergence of  to    the solution of A   b  under appropriate conditions on the sample as the sample size converges to infinity  In particular  they assumed that the sample is generated from either an absorbing or an ergodic Markov chain  More recently  several studies appeared where the finite sample performance of LSTDlike procedures were investigated  see  e g    Antos et al         Ghavamzadeh et al         Lazaric et al         Ghavamzadeh et al           The nonparametric variant has also received some attention  Farahmand et al         Maillard         One of the difficulties in the analysis of these procedures is that in these problems the sample is correlated  so the standard techniques of supervised learning that assume independence cannot be used  The approach followed by the above mentioned papers is to extend the existing techniques on an individual basis to deal with correlated samples  However  this   For background on this problem the reader may consult  e g   the books by Bertsekas and Tsitsiklis         Sutton and Barto         Szepesvari           Linear statistical estimation  might be quite laborious  even only considering the relatively easier case of regression   e g   Farahmand and Szepesvari        Thus  a more appealing approach might be to first derive error bounds as a function of the errors A  A  b  b  The advantage of this approach is that it allows one to decouple the technical issue of studying the concentration of the errors AA  b  b from the error  or stability  analysis of the estimation procedures  This is the approach that we advocate and follow in this paper  Consequently  our results will always be applicable when one can prove the concentration of the errors A  A  b  b  leading to an overall elegant  modular approach to deriving finite sample bounds  In some way  our approach parallels the recent trend in learning theory where sharp finite sample bounds are obtained by first proving deterministic regret bounds  e g   Cesa Bianchi et al          A second unique feature of our approach is that we derive our results in the above introduced framework of general statistical linear inverse problems  This allows us to concentrate on the high level structure of the problem and yields cleaner proofs and results  Furthermore  we think that the problem of linear estimation is interesting on its own due to its mathematical elegance and its applicability beyond value function estimation  a number of specific linear inverse problems  ranging from computer tomography to time series analysis  are discussed in the books by Kirsch        and Alquier et al           We will also place special emphasis in statistical linear inverse problems whose underlying system is inconsistent  i e   when there is no solution to A   b   In value function estimation  such inconsistency may arise in the so called off policy version of the problem  Understanding the inconsistent case is important because results that apply to it may shed light on issues arising when learning in badly conditioned systems       Goals In this paper  our goal will be to derive exact  uniform  fast  high probability oracle inequalities for the estimation procedures we study  That is  our goal is to prove that for our choice of an estimator   for any           with probability      n o LM     inf LM      cA b              where  for fixed values of     cA b        O max kA  Ak  kb  bk           Regression is a special case of value function estimation  Szepesvari          for some appropriate norm kk  The above is called an oracle inequality since the performance of   as measured with the loss  is compared to that of an oracle that has access to the true loss function  The term cA b      expresses the regret permitted due to the lack of knowledge of the true loss function  The scaling of this term with   or a norm of it  and  will also be of interest  Let us now explain the special attributes of the above inequality  We call the rate in the above inequality fast when     holds  Such a fast rate is possible in simple settings  e g   when d      A   A       hence it is natural to ask whether such rates are still possible in more general settings  The oracle inequality above is called exact because the leading constant  the constant multiplying LM     equals to    When L   inf  LM    is positive  implying that the system is inconsistent   then only a leading constant of one can guarantee the convergence of the loss to the minimal loss  i e   the consistency of the estimator  We call the above inequality uniform because it holds for any value of   This should be contrasted with inequalities where the range of  is lower bounded and or the estimator uses its value as input  which may be useful in some cases but falls short of fully characterizing the tail behavior of the loss of the resulting estimator  With some abuse of terminology  an inequality of the above form that holds for all small values of  shall be also called uniform  Uniform bounds seem to be harder to prove than their non uniform counterparts  and we do not know of any uniform  high probability exact oracle inequality with fast rates  not even in the case of linear regression  Unfortunately  we were also unable to derive such results  When deriving the estimators  we shall see that a major challenge is to control the magnitude of   Indeed  it follows from our objective function that the size of A must be controlled  and when A is unknown the magnitude of  must be controlled  This might be difficult when following a naive approach of solving A   b to get   e g   when A is singular  or near singular  as might be the case frequently in practice   To cope with this issue  in this paper we study procedures built around penalized estimators where a penalty Pen   is combined with the empirical loss LM      kA  bkM   The penalty is assumed to be some norm of   We study two procedures  In the first one  the loss is combined directly with the penalty  in an additive way to get the objective function LM      kk  while in the second one the square of the empirical loss is combined with the penalty  L M      kk  Note that both objective functions are convex  We note in passing that the second objective function when kk is the     norm   Linear statistical estimation  gives a Lasso like procedure  but we postpone further discussion of these choices to later sections of the paper  In the case of both objective functions the main issue becomes selecting the regularization coefficient       In this paper we give novel procedures to this end and show that these procedures have advantageous properties  we are able to derive oracle inequalities with fast rates for our procedures  although the inequalities will be either exact or uniform  but not both   To the best of our knowledge our general approach  our procedures  analytic tools and results are novel  The organization of the paper is as follows  in the next section  to motivate the general framework  we briefly describe value function estimation and how it can be put into our general framework  This is followed by a brief section that gives some necessary definitions  Section   contains our main results for the two approaches mentioned above  Section   discusses the results in the context of value function estimation  The paper is concluded and future work is discussed in Section        Value estimation in Markov Reward Processes The purpose of this section is to show how our results can be applied in the context of valueestimation in Markov Reward Processes  Consider a Markov Reward Process  MRP   X    R    X    R           over a  topological  state space X   By this we mean that  X    R    X    R           is a stochastic process   Xt   Rt      X  R for t    and given the history Ht    X    R    X    R            Xt   up to time t  the distribution of state Xt   is completely determined by Xt   while the distribution of the reward Rt   is completely determined by Xt and Xt   given the history Ht     Denote by PM the distribution of  Rt     Xt     given Xt   We shall call PM a transition kernel  Assume that support of the distribution of X  covers the whole state space PX   Define the value of a state x  X by V  x    E   t    t Rt    X    x   where          is the so called discount factor One central problem in reinforcement learning is to estimate the value function V given the trajectory  X    R    X    R            Sutton and Barto         One popular method is to exploit that the value function is the unique solution to the so called Bellman equation  which takes the form T W  W      where W   X  R and T   RX  RX is the so called Bellman operator defined using  T W   x    E  Rt     W  Xt     Xt   x   Note that T is affine linear   Given a finite sample  X    R    X    R            Xn      the LSTD algorithm of Bradtke and Barto        finds an approximate solution to the Bellman equation by solving the linear system n X  Rt     W  Xt      W  Xt    Xt             t    in   Rd   Here                 d    is a vector of d basis functions  i   X  R     i  d  and W   X  R is defined using W  x    h   x i  Denoting by  the solution to      W is the approximate value function computed by LSTD  This method can be derived as an instrumental variable method to find an approximate fixed point of T  Bradtke and Barto        or as a Bubnov Galerkin method  Yu and Bertsekas         In any case  the method can be viewed as solving a noisy version of the linear system A   b             st st   st and Here   A   E   X t     Xt      Xt     st   where  X st   R st X st   R st          is b   E  Xtst  Rt   a steady state MRP with transition kernel PM    The linear system     can be shown to be consistent  Bertsekas and Tsitsiklis          Note that     can also be writtenPin the compact form A   b  where n A     n t     Xt     Xt      Xt    and b   Pn   n t   Rt    Xt    By thinking of A  b as noisy versions of A  b and observing that for any M     solutions to     coincide with the minimizers of LM      kA  bkM we see that the least squares approach to value function estimation can be cast as an instance of   statistical   linear inverse   problems  When M   C     C   E  Xt   Xt     LM    becomes identical to the so called projected Bellman error loss which can also be written as LM      k   T W  W  k     where  is the steady state distribution underlying PM   k  k   is the weighted L     norm over X and    L   X      L   X     is the projection on the linear space spanned by  with respect to the kk    norm  Antos et al          Note that under mild technical assumptions  to be discussed later  one can show that  An   bn      A  b  gets concentrated around  A  b  at the usual parametric rate as the sample size n diverges  Thus  we can indeed view A  b as noisy approximations to  A  b      The MRP is said to be in a steady state if the distribution of Xt is independent of t    For a discussion of how well W approximates V the reader is directed to consult the paper by Scherrer        and the references therein  In this paper  we do not discuss this interesting problem but accept     as our starting point    Linear statistical estimation  One variation of this problem  the so called off policy problem  gives further motivation to recast the problem in terms of a loss function LM    to be minimized  In the off policy problem the data comes in the form of triplets    X    R    X      X    R    X             where the distribution of  Rt     Xt     is again independent of Ht     X    R    X      X    R    X              Xt    Rt   Xt    given Xt and is equal to the transition kernel PM   Further  it is assumed that  Xt  t  is a Markov process  The previous setting  also called the on policy case  is replicated when Xt   Xt   thus this new setting is more general than the previous one  The straightforward generalization h of the least squares approach i is st to define A   E   Xtst     Xt      Xtst    and h i st b   E Rt    Xtst   for the steady state process st st  Xtst   Rt     Xt    t    In this case  the linear system A   b is not necessarily consistent but one can still aim for minimizing  for example  Pn the projected Bellman error  Using A     n t     Xt    Pn  Xt      Xt    and b     n t   Rt    Xt   we can again cast the problem as a statistical linear inverse problem      Results In this section we give our main results for statistical linear inverse problems  We start with a few definitions  For real numbers a  b  we use a  b to denote max a  b   The operator norm of a matrix S with respect to the Euclidean norm k  k  is known to satisfy kSk    max  S   In what follows  we fix a vector norm k  k  Define the errors of A and b with the following respective equations  let         A   kM    A  A k     b   kM    b  b k         where kXk   denotes the operator norm of matrix X with respect to the norms k  k  and k  k  meaning that kXk     supv    kXvk   kvk  Although our main results are oracle inequalities  it will also be interesting to name a minimizer of LM    to explain the structure of some bounds  For this  we introduce   Rd as a vector such that   arg minRd LM    where if multiple minimizers exist we choose one with the minimal norm k  k     suitable high probability bounds on A and b are available  Assumption      There exist known scaling constants sA   sb     and known tail functions zA    zb             s t  for any           the following hold simultaneously with probability  w p   at least      A  sA zA     b  sb zb     To fix the scales of these bounds  we restrict zA    zb  so that zA   e   zb   e      where e is the base of natural logarithm  The reason to have two terms on the right hand side in the above inequalities as opposed to having a single term only is because we wish to separate the terms attributable to  and the sample size  The intended meaning of sa  and sb   is to capture how the errors behave as a function of the sample size n  typically  we expect sA   sb   O n        while the terms zA    zb  capture how the errors behave p as a function   e g   they are typically of size O  ln        In particular  sA   sb should be independent of  and zA    zb  should be independent of the sample size  This separation will allow us to distinguish between uniform and nonuniform versions of our oracle inequalities       Minimizing the unsquared penalized loss In this section  we present the results for the unsquared penalized loss  Choose k  k to be some norm of the d dimensional Euclidean space  For       define o     arg min LM      kk       Rd  where LM      kA  bkM   Our first result gives an oracle inequality for  as a function of A and b   Lemma      Consider  as defined in      Then  h i   LM         A inf LM       A    kk Rd              A b    In general  A   b are unknown  As it will turn out  in order to properly tune the penalized estimation methods we consider  we need at least upper bounds on these quantities  in particular  on A    To stay independent of sampling assumptions  we assume that  The proof  which is attractively simple and thus elegant  is given in the appendix  The result suggests that the ideal choice for  is A   Since A is unknown  we use its upper bound to choose   Depending on whether we allow  to depend on  or not  we get a non uniform or uniform oracle inequality  In all cases  the rate in the oracle inequality will be fast  We start with the uniform version  non exact version     Since our loss function is convex one can always find at least one minimizer   Theorem      Let Assumption     hold and consider  as defined in     where    sA   Then  for any   Linear statistical estimation            w p  at least     it holds that h i LM  sA    zA   inf LM      sA      zA   kk Rd    sb      zA   zb    By allowing  to depend on   we get an exact  nonuniform oracle inequality with a fast rate  Theorem      Let Assumption     hold  Fix          arbitrarily and choose  as defined in     with    sA zA    Then  w p  at least     it holds that h i LM  sA zA     inf LM       sA zA  kk    sb zb    Rd  Note that this bound is as tight as if we had first chosen    A and then applied the stochastic assumptions to obtain a high probability  h p   bound  When the linear system defined by  A  b  is consistent  LM          In this case one may prefer Theorem     to Theorem      Indeed  focusing on the behavior at  we get from Theorem     the bound sA zA       zA   k k   sb      zA   zb  that holds w p      for any value of   while from Theorem     we conclude the bound  sA zA   k k    sb zb     which however  holds only for            Minimizing the squared penalized loss A more traditional estimator uses the square of the empirical loss function  n o    arg min L M      kk              We now have two parameters that need tuning  However  as we will see  the tuning of these parameters is very similar to what we have seen in the previous section  The reason for this is that  is rich enough to contain a value  that makes LM     k k comparable to  not much larger than  LM      kk no matter what  one selects  This is in fact the key to the proof of the following lemma  which gives a deterministic oracle inequality for  c   Lemma      Let  c be as in      Then  h i   LM   c       A inf LM       A     kk Rd                A b      A c  With the  unattainable  choice    A   c   b we get h i LM   c    inf LM       A kk    b   Rd  These choices are impractical but  as it happened with in the previous section  we can obtain uniform nonexact or non uniform exact oracle inequalities with fast rates  The non exact uniform oracle inequality is formalized as follows  Theorem      Let Assumption     hold and choose  c be as in     with    sA and c   sb   Then  for any          w p  at least     it holds that h i LM   c        zA    inf LM      sA  zA      kk Rd             zA     sb zb        zA    sb    Rd  To be able to handle Lasso like procedures  we decided to avoid squaring the norm of   Moreover  not squaring this term is convenient for the proof techniques we used  The extension of our results for other types of penalties  in particular kk    is left for future work  Unlike the previous case where the loss function and the norm were both unsquared  in this case the selection of the regularization parameter  will be more involved  In practice  one often uses a hold out estimate to choose the best value of  amongst a finite number of candidates on an exponential grid  Here  we propose a procedure that avoids splitting the data  but uses the unsquared penalized loss with the same data  The new procedure is defined as follows  For some   c     to be chosen later  let n o    c   arg min LM       k k         c       where    c     k   c   k  N    c     c     and define      The next theorem gives a non uniform  exact oracle inequality with fast rates  Theorem      Let Assumption     hold  Fix          and choose  c be as in     with    sA zA  and c   sb zb    Then  w p  at least     it holds that h i LM   c    inf LM       sA zA  kk    sb zb    Rd  The relative merits of the uniform and non uniform oracle inequalities are unchanged compared to what we have seen in the previous section      Value estimation in Markov Reward Processes  Results Let us now return to value estimation in Markov Reward Processes  We consider the projected Bellman error objective  LM      kA  bkM   where M   C    for the definitions see Section     Assume that A   b are concentrated as in Assumption      with known   Linear statistical estimation  bounds  This can be arranged for example if the features i  Xt   and rewards Rt   are a s  bounded  and if we assume appropriate mixing  such as exponential  mixing  Yu         or when the Markov chain  Xt  t  forgets its past sufficiently rapidly  Samson         Note that in these cases  A  b  gets concentrated around rate  i e   p p  A  b  at the usual parametric sA   sb   O    n  and zA    zb    O  ln       For simplicity  assume first that C is given and consider the on policy case  As mentioned previously  in this case the system A   b is guaranteed to have a solution and therefore LM          Consider the estimator that minimizes the unsquared penalized loss  Then  Theorem     shows a uniform fast rate when using    sA   h i LM  sA         zA    sA zA  k k   sb zb    We get a similar inequality for the squared penalized loss using the result Theorem     with a slightly larger bound  In the off policy case  the linear system A   b may not have a solution  When it does  the previous bound applies  However  when this linear system does not have a solution  to get an exact oracle inequality we are forced to choose   in the case of minimizing the unsquared penalized loss  based on   In particular  with the choice    sA zA    Theorem     gives h i LM  sA zA     inf LM       sA zA  kk    sb zb    Rd       p Again  this p inequality gives fast  O    n  rates when sA   sb   O    n   Similar results hold for the procedure defined for the squared penalized loss where the bound is given by the inequality of Theorem      When C is unknown  one may resort replacing it by M      Then  a non exact oracle inequality can be derived using kxk P  max  Q    P Q     kxk Q    For a matrix S  we denote by max  S   max  S  its largest and smallest singular values  respectively   Consider first the unsquared penalized loss  In this case  kA      bkC    max  M     C   M      kA  bkM   Assume thathfor an estimator  iti holds that kA  bkM   inf  kA  bkM   cA b      Then  from kA  bkM       max  C     M C      kA  bkC   we get h i kA  bkC    inf     kA  bkM        cA b        where    max  C     M C       min  M     CM       is the conditioning number of M     CM     and     min  M     CM        In the on policy case  for example  this gives bounds of the form h i LM  sA              zA    sA zA  k k   sb zb    The bound for the off policy case derived from      takes the form LM  sA zA     h i inf     LM            sA zA  kk         sb zb    Rd  Similar inequalities can be derived for our procedures that minimize the squared penalized loss  Finally  let us discuss the dependence of our bounds on the choice of the basis functions  This dependence comes through Assumption      As an example  assume that i   X         and k  k   k  kp with    p     In this case  the bound on A is expected to scale linearly  with d  while b is expected to scale linearly with d  To see why A is expected to scale linearly with d note that A  kM      A  A k      kM      A  A kF   where k  kF denotes the Frobenius norm  Now  the Frobenius norm is the norm underlying the Hilbert space of square matrices with the inner product hP  Qi   trace P   Q  and thus an application of any concentration inequality for Hilbertspace valued random variables  e g    Steinwart and Christmann         gives a bound that scales with the range of N   kM       Xt     Xt      Xt    kF   Using the rotation property of trace  we get that N   k Xt     Xt    kM k Xt  k  The first term can be bounded using the triangle inequality as a function of k Xt  kM and k Xt    kM   Assuming  e g    that M is the identity matrix  we get that both  k Xt  kM   k Xt  k and k Xt    k are of size O  d   Hence  their product scales linearly with d  The above bound on A is naive  we believe using A  max  A  A  may yield a tighter dependency on d  E g   for d  d matrices with i i d standard normal  entries  the maximum eigenvalue is O  d   Vershynin         Furthermore  note that if the basis functions are correlated  or if they are sparse  the dimension will not necessarily appear linearly in the bound either  For a discussion of when to expect a milder dependence of the norm of  on d  the interested reader may consult the paper by Maillard and Munos              Related work Antos et al         proved a uniform high probability inequality both for the on policy and the off policy cases for LSTD  takes the form LM       Their bound           LM       O d ln d  n   which is a slower rate   Linear statistical estimation  than the rate we are able to obtain  Further  with our bounding method the ln d factor can be removed from this bound  There are more results available for the on policy case  As mentioned earlier  in this case the system A   b is consistent and thus our bound  under appropriate mixing conditions  takes the form   r d      R    LM      O L n       where    min  M   CM      L is the worst case norm   of features in the dual norm  L    supxX k x k   as discussed previously  L may be O  d   and R is a worst case bound on the norm of the parameter vector  i e   k k  R   In the next two results  the norm k  k is the   norm  Lazaric et al         for their  unregularized  path wise LSTD method obtain   r d log d      R  LM      O L n   cf  Theorem   in their work   Although this is a fast rate  it also shares the undesirable dependence on     Non uniform  slow rates can be extracted from the paper by Ghavamzadeh et al         for LSTD with random projections  The result with our notation would look like  cf  Theorem      r          log d LR LM      O L    R    n n More recently  for the so called Lasso TD method  Ghavamzadeh et al         showed non uniform            O n  rates  but only for the so called in sample error  i e   the empirical norm at the states used by the algorithm  These rates depend on the     norm of  and have no dependence on the minimum eigenvalue  but they are slow in n  At the expense of additional assumptions on the Gram matrix C  a sample estimate of C   they have also derived fast rates      Conclusion and future work We have shown performance bounds for two estimators in linear inverse problems  Each of these minimizes one of LM    and L M     plus a penalty kk  The penalty weight  can be chosen a priori without the need for a separate validation data set  and the bounds were presented in a general form that apply to many different instances of statistical linear inverse problems  requiring only that A and b concentrate around zero  Our split analysis  into a deterministic  step and a stochastic step  allows us to decouple the behavior of A   b from that of the estimators  We have recovered     penalized variations of LSTD  Bradtke and Barto        for value function estimation in MRPs  We have shown fast  uniform rates  which  in the on policy case  are exact and competitive with those existing in the literature  In the off policy case  the rates are non exact  and the non uniform bound is also competitive with existing results  Finally  we would like to point out interesting ways to further develop our work      penalties  The choice when the norm used in the penalty is the     norm has been extensively studied in the supervised learning literature  see  e g    Bickel et al         Koltchinskii        Buhlmann and Geer        and the references therein   as well as in the reinforcement learning setting  Kolter and Ng        Ghavamzadeh et al               Maillard         mainly because it allows for non trivial performance bounds even when the dimension d of the parameter vector is comparable to the sample size n  or even larger than n  provided that the true parameter vector is sparse  i e   there are many zeroes in it   In this paper we decided not to specialize to this case but rather to focus on the problem of proving fast  exact and  possibly uniform  oracle inequalities  Our results  when applied to the case of an     penalty show that in a way adding an     penalty does not hurt performance  as we expect that the oracle inequalities with the said properties should hold for a decent method  even if the conditions ideal for the     penalty do not hold  We do not know of performance bounds  ours included  for     penalized estimation have all of the characteristics we are after in a bound  viz  bounds that are exact  fast and uniform   Linear regression  Our results are also worth investigating in the context of linear regression  It is easy to cast regression as a statistical linear estimation problem whose underlying system is always consistent  If we use k  k as the     norm  we recover procedures similar to the square root Lasso  Belloni et al         and the Lasso  Tibshirani        for the estimators studied in Sections     and      respectively  We believe that confronting the bounds that can be derived from our results with bounds for linear regression in the literature can be very instructive  Connection to Inverse Problems  The theory of Inverse Problems is very pertinent to this work  and it is important to study our results under the light of those shown in Chapter   of Kirsch         Alquier et al          The existing knowledge of inverse prob    Linear statistical estimation  lems may help us better understand which choices of kk allow A to concentrate around zero  and how fast this concentration occurs  The idea of having learning problems as inverse problems is not new  Rosasco         Vito et al         study regression in Hilbert spaces as an inverse problem   Acknowledgements This work was supported by AITF and NSERC   
 In real supervised learning scenarios  it is not uncommon that the training and test sample follow different probability distributions  thus rendering the necessity to correct the sampling bias  Focusing on a particular covariate shift problem  we derive high probability confidence bounds for the kernel mean matching  KMM  estimator  whose convergence rate turns out to depend on some regularity measure of the regression function and also on some capacity measure of the kernel  By comparing KMM with the natural plug in estimator  we establish the superiority of the former hence provide concrete evidence understanding to the effectiveness of KMM under covariate shift      Introduction In traditional supervised learning  the training and test sample are usually assumed to be drawn from the same probability distribution  however  in practice  this assumption can be easily violated for a variety of reasons  for instance  due to the sampling bias or the nonstationarity of the environment  It is therefore highly desirable to devise algorithms that remain effective under such distribution shifts  Needless to say the problem is hopeless if the training and test distribution share nothing in common  On the other hand  if the two distributions are indeed related in a nontrivial manner  then it is a quite remarkable fact that effective adaptation is possible  Under reasonable assumptions  this problem has been attacked by researchers from statistics  Heckman        Shimodaira        and more recently by many researchers from machine learning  see for instance  Zadrozny Appearing in Proceedings of the    th International Conference on Machine Learning  Edinburgh  Scotland  UK        Copyright      by the author s  owner s            Huang et al          Bickel et al          BenDavid et al          Blitzer et al          Cortes et al          Sugiyama et al          Kanamori et al          We focus in this paper on the covariate shift assumption which was first formulated by Shimodaira        and has been followed by many others  The assumption that the conditional probability distribution of the output variable given the input variable remains fixed in both the training and test set is termed covariate shift  i e  the shift happens only for the marginal probability distributions of the covariates  It is well known that under this setting  the key to correct the sampling bias caused by covariate shift is to estimate the Radon Nikodym derivative  RND   also called the importance weight or density ratio  A number of methods have been proposed to estimate the RND from finite samples  including kernel mean matching  KMM   Huang et al          logistic regression  Bickel et al          Kullback Leibler importance estimation  Sugiyama et al          least squares  Kanamori et al          and possibly some others  Despite of the many algorithms  our current understanding of covariate shift still seems to be limited  From the analyses we are aware of  such as  Gretton et al         on the confidence bound of the RND by KMM   Kanamori et al         on the convergence rate of the least squares estimate of the RND  and  Cortes et al         on the distributional stability  they all assume that certain functions lie in the reproducing kernel Hilbert space  RKHS  induced by some user selected kernel  Since this assumption is impossible to verify  even worse  almost certainly violated in practice   one naturally wonders if we can replace it with something more reasonable  Such goal is pursued in this paper and constitutes our main contribution  We consider the following simple problem  Given the tr training sample   Xitr   Yitr   ni   and the test sample te nte  Xi  i     how well can we estimate the expected value EY te   provided covariate shift has happened  Note that we do not observe the output Yite on the test sample  This problem  at a first glance  ought to be   Analysis of Kernel Mean Matching under Covariate Shift  easy  after all we are humbly asking for estimating a scalar  Indeed  under usual assumptions  plus the nearly impossible assumption that the regression function lies in the RKHS  we prove a parametric rate      that is O ntr     nte      for the KMM estimator in Theorem   below  to fix ideas  we focus exclusively on KMM in this paper   For a more realistic assumption on the regression function that we borrow from learning theory  Cucker   Zhou         the convergence rate  proved in Theorem    degrades gracefully         to O ntr        nte          where      is a smoothness parameter measuring certain regularity of the regression function  in terms of the kernel   Observe that in the limit when     the regression function eventually lies in the RKHS and we recover the previous parametric rate  In this regard our bound in Theorem   is asymptotically optimal  A very nice feature we discovered for the KMM estimator is that it does not require knowledge of the smoothness parameter   thus  it is in some sense adaptive  On the negative side  we show that  if the chosen kernel does not interact very well with the unknown regression function  the convergence rate of the KMM estimator could be exceedingly slow  roughly nte    where s     again measures certain O logs nntrtr n te regularity of the regression function  This unfortunate result should draw attention to the importance of selecting which kernel to be used in practice  A thorough comparison between the KMM estimator and the natural plug in estimator  conducted in Section      also reveals the superiority of the former  We point out that our results are far from giving a complete picture even for the simple problem we consider here  for instance  it is unclear to us whether or not the rate in Theorem   can be improved  eventually  to the parametric rate in Theorem    Nevertheless  we hope that our paper will convince others about the importance and possibility to work with more reasonable assumptions under covariate shift  and as an example  suggest relevant tools which can be used to achieve that goal      Preliminaries In this section we formally state the covariate shift problem under our consideration  followed by some relevant discussions       Problem Setup Consider the familiar supervised learning setting  where we are given independent and identically disntr tributed  i i d   training samples   Xitr   Yitr   i   from  the joint  Borel  probability measure Ptr  dx  dy  on the  topological  domain X  Y  and i i d  test samte from the joint probability measure ples  Xite  ni   Pte  dx  dy  on the same domain  Notice that we do not observe the output Yite on the test sample  and more importantly  we do not necessarily assume that the training and test sample are drawn from the same probability measure  The problem we consider in this paper is to estimate the expected value EY te from the tr and the test sample training sample   Xitr   Yitr   ni   te nte  Xi  i     In particular  we would like to determine how fast  say  the     confidence interval for our estimate shrinks to   when the sample sizes ntr and nte increase to infinity  This problem  in its full generality  cannot be solved simply because the training probability measure can be completely irrelevant to the test probability measure that we are interested in  However  if the two probability measures are indeed related in a nontrivial way  our problem becomes solvable  One particular example  which we focus on hereafter  is known in the literature as covariate shift  Shimodaira         Assumption    Covariate shift assumption  Ptr  dy x    Pte  dy x         We use the same notation for the joint  conditional and marginal probability measures  which should cause no confusion as the arguments would reveal which measure is being referred to  Note that the equality P dx  dy    P dy x P dx  holds from the definition of the conditional probability measure  whose existence can be confirmed under very mild assumptions  Under the covariate shift assumption  the difficulty of our problem  of course  lies entirely on the potential mismatch between the marginal probability measures Ptr  dx  and Pte  dx   But the Bayes rule already suggests a straightforward approach  Pte  dx  dy    Pte  dy x Pte  dx    Ptr  dx  dy   dPte  x   dPtr  where the three quantities on the right hand side can all be estimated from the given samples  However  in order for the above equation to make sense  we need Assumption    Continuity assumption  The te Radon Nikodym derivative  x     dP dPtr  x  is welldefined and bounded from above by B     Note that B    due to the normalization constraint R  x P tr  dx       The Radon Nikodym derivative X  RND  is also called the importance weight or the density ratio in the literature  Evidently  if  x  is   Analysis of Kernel Mean Matching under Covariate Shift  not well defined  i e   there exists some measurable set A such that Pte  A      and Ptr  A       then in general we cannot infer Pte  dx  dy  from merely Ptr  dx   Pte  dx  and Ptr  dy x   even under the covariate shift assumption  The bounded from above assumption is more artificial  Recently  in a different setting   Cortes et al         managed to replace this assumption with a bounded second moment assumption  at the expense of sacrificing the rate a bit  For us  since the domain X will be assumed to be compact  the bounded from above assumption is not too restrictive  automatically holds when  x  is  say  continuous   Once we have the RND  x   it becomes easy to correct the sampling bias caused by the mismatch between Ptr  dx  and Pte  dx   hence solving our problem  Formally  let Z m x     y Pte  dy x      Y  be the regression function  then Z Z EY te   m x  Pte  dx    m x  x  Ptr  dx   X  X  By the i i d  assumption  P a reasonable estimator for ntr  Xitr    Yitr   Hence  EY te would then be n tr i   similarly to most publications on covariate shift  our problem boils down to estimating the RND  x    KMM tries to match the mean elements in a feature space induced by a kernel k     on the domain X  X         ntr nte   X   X min L      i  Xitr     Xite   ntr i   nte i   i  s t     i  B   H       where    X   H denotes the canonical feature map  H is the reproducing kernel Hilbert space   RKHS  induced by the kernel k and k  kH stands for the norm in H  To simplify later analysis  we have chosen to omit Pntr i        the normalization constraint n tr i   where   is a small positive number  mainly to reflect the fluctuation caused by random samples  It is not hard to verify that     is in fact an instance of quadratic programming  hence can be efficiently solved  More details can be found in the paper of Gretton et al          A finite sample   confidence bound for L    similar as      below  is established in Gretton et al          This bound is further transferred into a confidence bound for the generalization error of some family of loss minimization algorithms in Cortes et al          under the notion of distributional stability  However  neither results can provide a direct answer to our problem  a finite sample confidence bound on the estimate of EY te         A Naive Estimator       Plug in Estimator An immediate solution for estimating  x  is to estimate the two marginal measures from the training sample  Xitr   and the test sample  Xite    respectively  For instance  if we know a third  Borel  measure Q dx   usually the Lebesgue measure on Rd   such that dPtr te both dP dQ  x  and dQ  x  exist  we can employ standard density estimators to estimate them and then set dPtr te  x    dP dQ  x   dQ  x   However  this naive approach is known to be inferior since density estimation in high dimensions is hard  and moreover  small estimation ertr ror in dP dQ  x  could change  x  significantly  To our knowledge  there is little theoretical analysis on this seemingly naive approach       A Better Estimator  It seems more appealing to directly estimate the RND  x   Indeed  a large body of work has been devoted to this line of research  Zadrozny        Huang et al         Sugiyama et al         Cortes et al         Bickel et al         Kanamori et al          From the many references  we single out the kernel mean matching  KMM  algorithm  first proposed by Huang et al         and is also the basis of this paper   Another natural approach is to estimate the regression function from the training sample and then plug into the test set  We postpone the discussion and comparison with respect to this estimator until section          Motivation We motivate the relevance of our problem in this section  Suppose we have an ensemble of classifiers  say   fj  N all trained on the training sample j     tr   Xitr   Yitr   ni     A useful task is to compare  hence rank  the classifiers by their generalization errors  This is usually done by assessing the classifiers te on some hold out test sample   Xite   Yite   ni     It is not uncommon that the test sample is drawn from some different probability measure than the training sample  i e  covariate shift has happened  Since it could be too costly to re train the classifiers when the test sample is available  we nevertheless still like to   A thorough background on the theory of reproducing kernels can be found in Aronszajn           Analysis of Kernel Mean Matching under Covariate Shift  where recall that m x  is the regression function defined in     and  x  is the true RND   have a principled way to rank the classifiers  Let       be the users favourite loss function  and set tr te Zij     fj  Xitr    Yitr    Zij     fj  Xite    Yite    then we te nte  i   to estimate can use the empirical average of  Zij te the generalization error  that is E Zij    of classifier fj   But what if we do not have access to Yite hence te consequently Zij   Can we still accomplish the ranking job   The equality in     indeed holds under at least two conditions  respectively   First  if the regression function m  H  then taking inner products with m in     and applying the reproducing property we get      Second  if the kernel k is characteristicR Sriperumbudur et al          meaning that the map X  x P dx  from the space of probability measures to the RKHS H is injective  then we conclude      from     hence follows       The answer is yes  and it is precisely the covariate shift problem under our consideration  To see that  tr ntr te   Under consider the pair  Xitr   Zij  i   and  Xite  ni   the covariate shift assumption  that is Ptr  dy x    Pte  dy x   it is not hard to see that Ptr  dz x    Pte  dz x   hence the covariate shift assumption holds for the ranking problem  therefore the confidence bounds derived in the next section provide an effective solution   The above two cases suggest the possibility of solving our problem by KMM  Of course  in reality one only has finite samples from the underlying probability measures  thus calls for a thorough study of the empirical KMM  i e       Interestingly  our analysis reveals that in the first case above  we indeed can have a parametric rate while in the second case the rate becomes nonparametric  hence inferior  but does not seem to rely on the characteristic property of the kernel    We do not report numerical experiments in this paper for two reasons      Our main interest is on theoretical analysis      Exhaustive experimental results on KMM can already be found in Gretton et al                The empirical version In this subsection we analyze KMM in details  The following assumption will be needed      Theoretical Analysis This section contains our main contribution  i e   a theoretical analysis of the KMM estimator for EY te    Assumption    Compactness assumption  X is a compact metrizable space  Y          and the kernel k is continuous  whence kkk  C             The population version Let us first take a look at the population version of KMM    which is much easier to analyze and provides valuable insights  Z  x  x Ptr  dx    x Pte  dx   X X Z s t       B   x Ptr  dx           arg min  Z  X  The minimum value is   since the true RND  x  is apparently feasible  hence at optimum we always have Z Z  x    x Ptr  dx     x Pte  dx       X  X  The question is whether the natural estimator R    x y Ptr  dx  dy  is consistent  In other words  X Y is Z Z   m x    x Ptr  dx    EY te   m x  x Ptr  dx   X  X        All Hilbert space valued integrals in this paper are to be understood as the Bochner integral  Yosida          H  We use kk for the supremum norm  Under the above assumption  the feature map  is continuous hence measurable  with respect to the Borel  fields   and the RKHS is separable  therefore the Bochner integrals in the previous subsection are well defined  Moreover  the conditional probability measure indeed exists under our assumption  We are now ready to deriveP a finite sample confidence ntr i Yitr  EY te    where bound for our estimate   n tr i   i is a minimizer of      We start by splitting the sum  ntr   X i Yitr  EY te ntr i       ntr   X i  Yitr  m Xitr    ntr i       ntr   X  i  i   m Xitr    h Xitr    ntr i       ntr   X  i  i  h Xitr   ntr i    ntr   X i m Xitr    EY te     ntr i         where i     Xitr   and h  H is to be specified later    Analysis of Kernel Mean Matching under Covariate Shift  We bound each term individually  For the last term in      we can apply Hoeffdings inequality  Hoeffding        to conclude that with probability at least      ntr   X i m Xitr    EY te  B ntr i    r      log    ntr        The first term in     can be bounded similarly  Conditioned on  Xitr   and  Xite    we apply again Hoeffdings inequality  Note that i  Yitr  m Xitr      i m Xitr    i     m Xitr      therefore its range is of size i   With probability at least      v r u ntr ntr u   X   X   tr tr   t i  Yi  m Xi     i  log ntr i   ntr i    ntr r         B log    ntr  The second and third terms in     require more work  Consider first the third term   expectation  and then bound the expectation straightforwardly  In general  Pineliss inequality will lead to  slightly  tighter bounds due to its known optimality  in certain sense   Finally  we come to the second term left in      which is roughly the approximation error in learning theory  Cucker   Zhou         Note that all confidence bounds we p have derived so far shrink at the parametric rate O    ntr     nte    However  from here on we will have to tolerate nonparametric rates  Since we are going to apply different approximation error bounds to the second term in      it seems more convenient to collect the results separately  We start with   an encouraging result   Theorem   Under Assumptions      if the regression function m  H  the RKHS induced by the kernel k   then with probability at least       s     ntr B        X tr te i Yi  EY M     log   ntr i   ntr nte   ntr ntr   X   X  i  i  h Xitr      i  i  hh   Xitr  i ntr i   ntr i     khkH   ntr   X  i  i   Xitr   ntr i    H   khkH   L     L   ntr     khkH   L   ntr          where   ntr denotes the restriction of  to the training sample  Xitr    L   is defined in      and the equality is because h  H  and the reproducing property of the canonical feature map   the first inequality is by the Cauchy Schwarz inequality  the second inequality is due to the triangle inequality  and the last inequality is by the optimality of  and the feasibility of   ntr in problem      Next  we bound L   ntr    ntr nte   X   X i  Xitr     Xite   ntr i   nte i   H s     B      C     log      ntr nte   L   ntr       with probability at least      where the inequality follows from the Hilbert space valued Hoeffding inequality in  Pinelis        Theorem       Note that Pinelis proved his inequality for martingales in any   smooth separable Banach space  Hilbert spaces are bona fide   smooth   We remark that another way  see for instance  Gretton et al         Lemma       is to use McDiarmids inequality to bound L   ntr   by its  where M       CkmkH and i is computed from      Proof  By assumption  setting h   m zeros out the second term in      A standard union bound combining          completes the proof  and we simplified the bound by slightly worsening the constant   The confidence bound shrinks at the parametric rate  although the constant depends on kmkH   which in general is not computable  but can be estimated from the training sample   Xitr   Yitr    at a rate worse than parametric  Since this estimate inevitably introduces other uncomputable quantities  we omit the relevant discussion  On the other hand  our bound suggests that if a priori information about m is indeed available  one should choose a kernel that minimizes its induced norm on m  The case when m   H is less satisfactory  despite of its practicality  We point out that a denseness argument cannot resolve this difficulty  To be more precise  let us assume for a moment m  C  X    the space of continuous functions on X   and k be a universal kernel  Steinwart         meaning that the RKHS induced by k is dense in  C  X    k  k    By the assumed universal property of the kernel  there exists suitable h  H that makes the second term in     arbitrarily small  in fact  can be made vanishing   however  on the other hand  recall that the bound     on the third term in     depends on khkH hence could blow up  If we trade   Throughout this paper  the confidence parameter  is always taken arbitrarily in           Analysis of Kernel Mean Matching under Covariate Shift  off the two terms appropriately  we might get a rate that is acceptable  but worse than parametric   The next theorem concretizes this idea   since    m    by Assumption    The quantity A   m  R  is called the approximation error in learning theory and its polynomial decay is known   Theorem   Under Assumptions      if A   m  R     inf km  gkLP   C  R   for some      and kgkH R  tr  constant C      then with probability at least      ntr   X  i Yitr  EY te ntr i   r        B log   C  BC       D       ntr  r     q   where D      C   nBtr   n te log    BC  n tr log         C                  and i is computed from       Proof  By the triangle inequality  ntr   X  i  i   m Xitr    h Xitr    ntr i    B  ntr   X  m Xitr    h Xitr     ntr i    to be  almost  equivalent to m  Range Tk       see for instance Theorem     of Cucker   Zhou operator  Tk f   x      R         Here Tk is the integral   k x   x f  x Ptr  dx  on LPtr   The smoothness paX rameter      measures the regularity of the regression function  and as it increases  the range space of   Tk    becomes smaller  hence our decay assumption on A   m  R  becomes more stringent  Note that the  is necessarily smaller than      but apexponent     proaches     when     because by Mercers theo   rem Tk  is onto H  in which case the range assumption would bring us back to Theorem     Theorem   shows that the confidence bound now  shrinks at a slower rate  roughly O ntr             nte          which  as     approaches the paramet      ric rate O ntr     nte     derived in Theorem   where we assume m  H  We point out that the source of this slower rate comes from the irregular nature of the regression function  in the eye of the kernel k    The polynomial decay assumption on A   m  R  is not always satisfied  for instance  it is shown in Theorem Not surprisingly  we apply yet again Hoeffdings in    of Cucker   Zhou        that for C   indefinite equality to relate the last term above to its expectatimes differentiable  kernels  such as the popular Gaustion  Since sian kernel   polynomial decay implies that the regression function m  C   X    under mild assumptions on km  hk      k hh    i k      CkhkH   X and Ptr  dx    Therefore  as long as one works with we have with probability at least      smooth kernels but nonsmooth regression functions  the approximation error has to decay logarithmically r ntr   X     slowly  We give a logarithmic bound for such cases  tr tr  m Xi  h Xi        CR  log  A   m  R   ntr i    ntr  Theorem   Under Assumptions      if A  m  R     inf km  gk  C  log R s for some s     and where R    khkH   Combining this bound with    kgkH R      and applying our assumption on A   m  R   constant C     assuming R      then  for ntr and ntr   X  i  i   m Xitr    h Xitr    ntr i   s   r       B      B log    RC     log ntr  ntr nte  r       BC  R     B     CR  log    ntr   Setting R       BC   D           nte larger than some constant      s    s ntr   X   sBC tr te BC log i Yi  EY     ntr i   s D r   s     s    B log    sBC   s   D ntr  holds probability at least      where D   r with     B   C   ntr   n te log   and i is computed from       completes the proof   In Theorem   we do not even assume m  C  X    all we need is m  LP tr   the space of Ptr  dx  square integrable functions  The latter condition always holds  The proof is similar as that of Theorem   except that s  s   we set R     sBC     D Theorem   shows that in such unfavourable cases  the confidence bound shrinks at an exceedingly slow   Analysis of Kernel Mean Matching under Covariate Shift nte rate  roughly  O logs nntrtr n    The reason  of course  te is due to the slow decay of the approximation error A  m  R   It is proved in Theorem     of Cucker   Zhou        that for the Gaussian kernel k x    x    exp kx  x  k          if X  Rd has smooth boundary and the regression function m  H s  X   with index s   d    then the logarithmic decay assumed in Theorem   holds  Here H s  X   is the Sobolev space  the completion of C   X   under the inner product R P   hf  gis    X   s ddxf ddxg   assuming s  N   Similar bounds also hold for the inverse multiquadrics kernel k x    x     c    kx  x  k     with       We remark that in this regard Theorem   disrespects the popular Gaussian kernel used ubiquitously in practice and should draw the attention of researchers        Discussion It seems worthwhile to devote a subsection to discussing a very natural question that the reader might already have  why not estimate the regression function m on the training set and then plug into the test set  after all m does not change under the covariate shift assumption  Algorithmically  this is perfectly doable  perhaps conceptually even simpler since the algorithm does not need to see the test data beforehand  We note that estimating the regression function from i i d  samples has been well studied in the learning theory literature  see for instance  Chapter   of Cucker   Zhou        and the many references therein  The difficulty  though  lies in the appropriate error metric on the estimate  Recall that when estimating the regression function from i i d  training samples  one usually measures the progress  i e  the discrepancy between the estimate m and m  by the L   norm under the training probability measure Ptr  dx   while what we really want is a confidence bound on the term nte   X m Xite    EY te   nte i          Since Ptr    Pte   there is evidently a probability measure mismatch between the bound we have from estimating m and the true interested quantity  Indeed  conditioned on the training sample   Xitr   Yitr     using the triangle inequality we can bound      by   Z nte   X te m Xi    m x Pte  dx    km  mkLP    te nte i   The first term above can be bounded again through Hoeffdings inequality  while the second term is close to what we usually have from estimating m  the only difference being that the L    norm is now under the test probability measure Pte  dx   Fortunately  since the norm of the identity map id          X   k  kLP             X   k  kLP    is te tr  bounded by B  see Assumption     we can deduce a bound for      based upon results from estimating m  though less appealingly  a much looser bound than the one given in Theorem    We record such a result for the purpose of comparison  Theorem   Under Assumptions      if the regression   function m  Range Tk      for some       then with probability at least      nte   X m Yite    EY te  nte i    r          log   BC  ntr          nte   where C  is some constant that does not depend on ntr   nte   and m is the  regularized least squares  estimate of m in Smale   Zhou         The theorem follows from the bound on km  mkLP  tr in Corollary     of Sun   Wu         which is an improvement over Smale   Zhou         Carefully comparing the current theorem with Theorem    we observe      Theorem    which is based on the regularized least squares estimate of the regression function  needs to know in advance the parameter   in order to tune the regularization constant  while Theorem    derived for KMM  does not require any such information  hence in some sense KMM is adaptive      Theorem   has much worse dependence on the training sample size ntr   it does not recover the parametric rate even when the smoothness parameter          goes to   we get ntr   instead of ntr    On the other hand  Theorem   has better dependence on the test sample size nte   which is  however  probably not so important since usually one has much more test samples than training samples because the lack of labels make the former much easier to acquire      Theorem   seems to have better dependence on the parameter B      Given the fact that KMM utilizes both the training data and the test data in the learning phase  it is not entirely a surprise that KMM wins in terms of convergence rate  nevertheless  we find it quite stunning that by sacrificing the rate slightly on nte   KMM is able to improve the rate on ntr so significantly      Conclusion For estimating the expected value of the output on the test set where covariate shift has happened  we have derived high probability confidence bounds for the kernel mean matching  KMM  estimator  which   Analysis of Kernel Mean Matching under Covariate Shift       converges  roughly O ntr     nte     when the regression function lies in the RKHS  and more generally       Heckman  James J  Sample selection bias as a specification error  Econometrica                         O ntr         nte         when the regression function exhibits certain regularity measured by   An exnte    is also tremely slow rate  roughly O logs nntrtr n te provided  calling attention of choosing the right kernel  From the comparison of the bounds  KMM proves to be much more superior than the plug in estimator hence provides concrete evidence understanding to the effectiveness of KMM under covariate shift   Hoeffding  Wassily  Probability inequalities for sums of bounded random variables  Journal of the American Statistical Association                       Although it is unclear to us if it is possible to avoid approximating the regression function  we suspect the bound in Theorem   is in some sense optimal and we are currently investigating it  We also plan to generalize our results to the least squares estimation problem   Kanamori  Takafumi  Hido  Shohei  and Sugiyama  Masashi  A least squares approach to direct importance estimation  JMLR                      Acknowledgements  Huang  Jiayuan  Smola  Alexander J   Gretton  Arthur  Borgwardt  Karsten M   and Scholkopf  Bernhard  Correcting sample selection bias by unlabeled data  In NIPS  pp                 Kanamori  Takafumi  Suzuki  Taiji  and Sugiyama  Masashi  Statistical analysis of kernel based leastsquares density ratio estimation  Machine Learning                    This work was supported by Alberta Innovates Technology Futures and NSERC   Pinelis  Iosif  Optimum bounds for the distributions of martingales in Banach spaces  The Annals of Probability                         
 In this paper we propose a novel gradient algorithm to learn a policy from an experts observed behavior assuming that the expert behaves optimally with respect to some unknown reward function of a Markovian Decision Problem  The algorithms aim is to find a reward function such that the resulting optimal policy matches well the experts observed behavior  The main difficulty is that the mapping from the parameters to policies is both nonsmooth and highly redundant  Resorting to subdifferentials solves the first difficulty  while the second one is overcome by computing natural gradients  We tested the proposed method in two artificial domains and found it to be more reliable and efficient than some previous methods      INTRODUCTION  The aim of apprenticeship learning is to estimate a policy of an expert based on samples of the experts behavior  This problem has been studied in the field of robotics for a long time and due to the lack of space we cannot give an overview of the literature  The interested reader might find a short overview in the paper by Abbeel and Ng         In apprenticeship learning  a k a  imitation learning  one can distinguish between direct and indirect approaches  Direct methods attempt to learn the policy  as a mapping from states  or features describing states to actions  by resorting to a supervised learning method  They do this by optimizing some loss function that measures the deviation between the experts   Computer and Automation Research Institute of the Hungarian Academy of Sciences  Kende u         Budapest       Hungary  Csaba Szepesvari Department of Computing Science University of Alberta Edmonton T G  E   AB  Canada  policy and the policy chosen  The main problem then is that in parts of the state space that the expert tends to avoid the samples are sparse and hence these methods may have difficulties with learning a good policy at such places  In an indirect method it is assumed that the expert is acting optimally in the environment  In particular  in inverse reinforcement learning the environment is modelled as a Markovian decision problem  MDP   Ng and Russell         The dynamics of the environment is assumed to be known  or it could be learnt from samples which might even be unrelated to the samples come from the expert   However  the reward function that the expert is using is unknown  Recently Abbeel and Ng        gave an algorithm which was proven to produce a policy which performs almost as well as the expert  even though it is not guaranteed to recover the experts reward function  recovering the reward function is an ill posed problem   This approach might work with less data since it makes use of the knowledge of model of the environment  which can help it in generalizing to the less frequently visited parts of the state space  One problem is that the algorithm of Abbeel and Ng        relies on the precise knowledge of the features describing the reward function  which is not a realistic assumption  for a discussion of this  see Section     In particular  we will show that even the correct scales of the features have to be known  In this paper we propose a gradient algorithm that combines the two approaches by minimizing a loss function that penalizes deviations from the experts policy like in supervised learning  but the policy is obtained by tuning a reward function and solving the resulting MDP  instead of finding the parameters of a policy  We will demonstrate that this combination can unify the advantages of the two approaches in that it can be both sample efficient and work even when the features are just vaguely known         NEU   SZEPESVARI     BACKGROUND  Let us first introduce some notation  For a subset S of some topological space  S  will be used to denote its For a finite dimensional vector x  Pinterior  d kxk   i   x i shall denote its     norm  Random variables will be denoted by capital letters  e g   X A   E    stands for expectations  We assume that the reader is familiar with basic concepts underlying Markovian decision processes  MDPs   e g   Puterman        hence we introduce these concepts only to fix the notation  A finite  discounted infinite horizon total reward MDP is defined by a   tuple M    X   A    P  r   where X is a finite set of states  A is a finite set of actions           is the discount factor  P gives the transition probabilities  P  x   x  a  stands for the probability of the transition from state x to x  upon taking action a  x  x   X   a  A   r is the reward function  r   X  A  R  r x  a  gives the reward incurred when action a  A is executed from state x  X   A stationary stochastic policy  in short  P policy  is a mapping    A  X         satisfying aA  a x       x  X    The value of  a x  is the probability of taking action a in state x  A policy is called deterministic if for any x    x  is concentrated on a single action  The class of all stationary stochastic policies will be denoted by   For a fixed policy  the value of a state x  X is defined by       X    t r Xt   At     X    x       V   x    E t    where  Xt   At  t  is the sequence of random stateaction pairs generated by executing the policy   The function V    X  R is called the value function underlying policy   We will also need action value functions  The actionvalue function  Q   X  A  R  underlying policy  is defined by       X    t Q  x  a    E  r Xt   At     X    x  A    a     t      Instead of  a  x  we use  a x  to emphasize that    x  is a probability distribution  Note that in finite MDPs one can always find optimal  stochastic  stationary policies  Puterman          with the understanding that for t      At    Xt    A policy that maximizes the expected total discounted reward over all states is called an optimal policy  The optimal value function is defined by V   x    sup V   x   while the optimal action value function is defined by Q  x  a    sup Q  x  a   It turns out that V  and Q satisfy the so called Bellman optimality equations  e g   Puterman        In particular  X Q  x  a    r x  a     P  y x  a  max Q  x  b       yX  bA  P We call a policy that satisfies aA  a x Q x  a    maxaA Q x  a  at all states x  X greedy w r t  the function Q  It is known that all policies that are greedy w r t  Q are optimal and all stationary optimal policies can be obtained these way      APPRENTICESHIP LEARNING  Assume that we observe a sequence of state action pairs  Xt   At   tT   the trace of some expert  We assume that the expert selects the actions by some unknown policy E   At  E   Xt    The goal is to recover E from the observed trace  The simplest solution is of course to use a supervised learning approach  we select a parametric class of policies              Rd   and try to tune the parameters so as to minimize some loss JT      such as X JT      T  x   a x   E T  a x          xX  aA  where T  x  could be defined by T  x       T   PT    t   I Xt  x  are the empirical occupation frequencies under the experts policy and E T  a x    PT PT t   I Xt  x  is the empirical est   I Xt  x At  a    timate of the experts policy   It is easy to see that JT approximates the squared loss X J     E  x   a x   E  a x        xX  aA  uniformly in   the usual concentration results hold for JT   e g  Gyorfi et al           The reason E T is not used directly as a solution is that if the state space is large then it will be undefined for a large number of states  where E T  x       with high probability unless the number of samples is enormous  An alternative to direct policy learning is inverse reinforcement learning  Ng and Russell         The idea   If a state is not visited by the expert  the policy is defined arbitrarily    NEU   SZEPESVARI is that given the experts trace  we find a reward function that can be used to explain the performance of the expert  More precisely  the problem is to find a reward function that the behavior of the expert is optimal for  Once the reward function is found  existing algorithms are used to find a behavior that is optimal with respect to it  One difficulty in IRL is that solutions are non unique  e g  if r is a reward function that recovers the experts policy then for any      r is also a solution  r     is always a solution   For non trivial problems there are many solutions besides the variants that differ in their scale only  We propose here to unify the advantages of the direct and indirect approaches by  i  taking it seriously that we would like to recover the experts policy and  ii  achieve this through IRL so that we can achieve good generalization at parts of the state space avoided by the expert  We thus propose to find the parameters given a parametric family of rewards  r     such that the corresponding  near  optimal policy     matches the experts policy E  more precisely  its empirical estimate   The proposed method can be written succinctly as the optimization problem   J     min  s t      G Q          where J is a loss function  such as     or      aimed at measuring the distance of E and its argument  Q is the optimal action value function corresponding to the reward function r and G is a suitable smooth mapping that returns  near  greedy policies with respect to its argument  One possibility  utilized in our experiments  is to use Boltzmann action selection policies  see        In this paper we consider gradient methods to solve the above optimization problem  One difficulty with such an approach is that there could be many parameterizations that yield to the same loss  This will be helped with the method of natural gradients  for which the theory is worked out in the next section  Another difficulty is that the mapping    Q is nondifferentiable We will  however  show that it is Lipschitz when r is Lipschitz and hence  by Rademachers theorem it is differentiable almost everywhere  w r t  the Lebesgue measure        NATURAL GRADIENTS  Our ultimate goal is to find some parameters  in a parameter space   Rd such that the policy  determined by  matches the experts policy E   For       facilitating the discussion let us denote the map from the parameter space  to the policy space by h  i e   h         Thus  our objective function can be writ    J h     where J     R is a  differenten as J   tiable  objective function defined over   such as        Incremental gradient and the goal is to minimize J  methods implement t     t  t gt   where t    is an appropriate step size sequence and gt   g   points in the direction of steepest ascent on the surface      J      The gradient method with an infinitesimal step size gives rise to a trajectory   t  t    This in turn determines a trajectory   t  t  in the policy space  where  t    h  t    Since our primary interest is the trajectory in the policy state  it makes sense to determine the gradient direction g in each step such that  t  moves in the steepest descent direction on the surface of    J      We call g   g   the natural gradient if this holds  Amari        gives a method to find the natural gradients using the formalism of Riemannian spaces  The advantage of this procedure is that the resulting trajectories will be the same for any equivalent parameterization  i e   if the parameter space is replaced by some other space that is related to the first one through a smooth invertible mapping  with a smooth inverse   In addition  the gradient algorithm that uses natural gradients can be proven to be asymptotically efficient in a probabilistic sense and has the tendency to alleviate the problem of plateaus  Amari         In order to define natural gradients we need some definitions  First  we need the generalization of derivatives for mappings f between Banach spaces   The underlying idea is that the gradient  derivative  of f   U  V provides a linear approximation to the change f  u   h   f  u   Definition    Frechet derivative   Let U  V be Banach spaces  A is the Frechet derivative of f at u if A   U  V is a bounded linear operator and kf  u   h   f  u   AhkV   o khkU    The mapping f then is called Frechet differentiable at u  In what follows we view  both as a vector space and a complete metric space with some metric d  In our application this metirc will be derived from the  unweighted      norm  but other choices would also work  The following definition suggests a geometry induced on   Definition    Induced metric   Let   Rd        We say that G  Rdd is a pseudo metric induced by     The benefit of choosing strictly stochastic policies is that if the experts policy is deterministic  they force the uniqueness of the solution     A Banach space is a complete normed vector space  In our case it will usually be a Euclidean space  e g  Rd          NEU   SZEPESVARI   h    d  at  if G is positive semidefinite and d h       h      T G    o kk     The essence of this definition is that if the distance between  and     is given by T G  then this distance will match the distance of h   and h       as kk     It follows from the definition that the induced pseudo metric is unique  In the rest of the paper we assume that  is finite dimensional to make the presentation of the results easier  The following proposition is an immediate consequence of the definition of induced pseudo metrics and the definition of Frechet differentiability  Proposition    Assume that h      is Frechet differentiable at        Rd         d  is a complete  linear metric space  Then h    T h     is the pseudo metric induced by  h    d  at   Natural gradients can be obtained by the follow    ing procedure  Let g       argmaxS    J       J   be the direction of steepest ascent over the warped sphere S          Rd   kh       h  k       Then the set of natural gradients is given by    h  J     def    lim inf   g          Here the limes inferior of the sets  g        is meant in the sense of the Painleve Kuratowski convergence  Kuratowski         It then holds that no matter how    h  J      converges to zero  g   defines a direction of steepest ascent on the surface of J at h    The following theorem holds  Theorem    Let J     R  h       J    J  h  Assume that J is Frechet differentiable and locally Lipschitz and h      is Frechet differentiable at      Let G   h    T h     be the pseudo metric         h  J    at  induced by  h    d   Then G J       where J   is the ordinary gradient of J at  and G denotes the Moore Penrose generalized inverse of G   For the sake of specificity  when it does not cause con  the natural gradient of J  at   fusion  we call G J   Note that from the construction it follows immediately   are covariant for that the trajectories of    G J   any initial condition  The proof borrows some ideas from the proof of Theorem   in  Amari         In order to spare some space we only give an outline here  The basic idea is to replace the warped sphere S     by the sphere    Note that g     is set valued   SG           Rd   T G         This is justified since the sphere SG      becomes arbitrarily close to S     as     and J  is sufficiently regular  The next step is to show that for some C        CG J   is a solution of the optimization problem argmaxSG     J       and this solution tracks         J     when closely that of argmaxSG     J            CALCULATING THE GRADIENT  In order to calculate the natural gradient we need to calculate the  Frechet  derivative of h     G Q   and the gradient of J h      By the chain rule we obtain J h      J    h   h      Since calculating the derivative of J  or JT   is trivial  we are left with calculating the derivative of h    As suggested previously  we use a smooth mapping G  One specific proposal  that we actually used in the experiments assigns Boltzmann policies to the action value functions  G Q  a x    P  exp Q x  a     bA exp Q x  b         where      is a parameter that controls how close G Q  is to a greedy action selection  With this choice   ln   a x    a x      a x  k k A    Q  x  b  Q  x  a  X     a x     b x    k k bA      Hence  we are left with calculating Q  x  a  k   We will show that these derivatives can be calculated almost everywhere on  by solving some fixed point equations similar to the Bellman optimality equations  For this  we will need the concept of subdifferentials and some basic facts  Definition    Frechet Subdifferentials   Let U be a Banach space  U  be its topological dual   The Frechet subdifferential of f   U  R at u  U   denoted by   f  u  is the set of u  U  such that lim inf khk   f  u   h   f  u   hu   ui       h  h     The following elementary properties follow immediately from the definition  e g   Kruger        Proposition    Let  fi  iI be a family of real valued functions defined over U and let f  u    maxiI fi  u      Remember that G maps action value functions to policies and J measures deviations to the experts policy    When U   Rd with the     norm then U    Rd and for u  U  v   U    hv    vi is the normal inner product    NEU   SZEPESVARI Then if u    fi  u  and fi  u    f  u  then u    f  u   If f    f    U  R           then     f        f        f      f     The next result states some conditions under which  in a generalized sense  taking a derivative and a limit is interchangeable  It is extracted from the proof of Proposition     of Penot         Proposition    Assume that  fn  n is a sequence of real valued functions over U which converge to some function f pointwise  Let u  U   un    fn  u  and assume that  un   is weak  convergent to u and is bounded  Further  assume that the following holds at u  For any       there exists some index N     and a real number      such that for any n  N   h  BU        fn  u   h   fn  u     hun   hi   khk   Then u    f  u   Now  we state the main result of this section  Proposition    Assume that the reward function r is differentiable w r t   with uniformly bounded derivatives  sup  x a Rd X A kr   x  a k      The following statements hold      Q is uniformly Lipschitz continuous as a function of  in the sense that for any  x  a  pair       Rd    Q  x  a   Q   x  a    L  k    k with some L           Except on a set of measure zero  the gradient   Q   is given by the solution of the following fixed point equation    x  a     r   x  a  T P P   yX P  y x  a  bA  b y   y  b       where  is any policy that is greedy with respect to Q   Note that  r   x  a  T  Rd   In fact  the above equation can be solved componentwise  The kth component of the derivative can be obtained computing the action  value function for the policy  using r k in place of   the reward function  Proof  Let T   RX A  RX A be the Bellman operator X  T Q  x  a    r  x  a     P  y x  a  max Q y  b   yX  bA      Here r k is the kth component of the derivative of the reward function with respect to   We also note in passing that if r is convex in  then so is Q   This follows with the reasoning followed in the proof of the first part        By elementary arguments  if Q is L Lipschitz in   then T Q is R   L Lipschitz in   where R is such that for any      Rd    x  a   X  A   r  x  a   r   x  a    Rk    k  Choose Q       As is well known  e g   Puterman          Qn   T n Q  converges to Q   Q   limn T n Q    Hence  by the previous argument Q is R   R      R           R       Lipschitz  proving the first part of the statement  For the second part  for a policy   let us define the operator S   acting over the space of functions    X  A  Rd   by  S   x  a     r   x  a  T P P   yX P  y x  a  bA  b y  y  b   Let  denote a greedy policy w r t  Q and let n be a sequence of policies that are Pgreedy w r t  Qn and where ties are broken so that xX  aA   a x   n  a x   is minimized  It follows that for n large enough  n     Now  consider the sequence        n     Sn n   Then for n large enough we have n     S n   By induction  n  x  a    Qn  x  a  holds for any n     Indeed  this clearly holds for n      while the general case follows by Proposition    Now  observe that S acts separately on each of the d components of its argument and when it is restricted to any of these components  it is a contraction  Hence  n converges to the fixed point of S   i e   the solution of      By Proposition   the limit is a subdifferential of limn Qn   Q  that the condition of this proposition is satisfied follows from the uniform convergence of n in   which follows since kr  k is uniformly bounded in both  and  x  a    Now  since by the first part Q is Lipschitz continuous in   by Rademachers theorem it is differentiable almost everywhere  It is well known that if a function is differentiable then its subderivative coincides with its derivative  see e g  Kruger          This finishes the proof of the statement      COMPUTER EXPERIMENTS  The goal of the experiments was to assess the efficiency of the algorithm and to test its robustness  We were also interested in how it compares with the algorithm of Abbeel and Ng         We have implemented three versions of our algorithm   i  gradient descent using plain gradients   ii  gradient descent using natural gradients  iii  RPROP using plain gradients   RPROP is a popular adaptive step size selection algorithm that proved to be very competitive in a number of settings Riedmiller and    We tried a natural RPROP variant as well  RPROP using natural gradients   but perhaps suprisingly  it give much poorer results than the other algorithms    NEU   SZEPESVARI  Braun         We have implemented the variant described in Igel and Husken         We also implemented the max margin and the projection algorithms described in Abbeel and Ng        to be able to compare the different approaches  Results will be shown for max margin  The projection algorithm is computationally more efficient  but we have found it less reliable and less data efficient                J         Natural grad   We decided to use two test environments  The familiar grid world that has also been used by Abbeel and Ng        and the sailing problem due Vanderbei         The reward function was linear in the unknown parameters              RPROP        Plain grad  Max margin             Number of training samples  GRID WORLD  We have run the first series of experiments in grid worlds  where each state is a grid square and the four actions correspond to moves in the four compass directions with     success  We constructed the reward function as a linear combination of   features  i   X  R  i                  where the features were essentially randomly constructed  The optimal parameter vector  consists of evenly distributed random values from         In general we try to approximate the reward function with the use of the same set of features that has been used to construct it  but we also examine the situation of unprecisely known features  The size   of the grid worlds was set to         Value iteration was used for finding the optimal policy  or gradients  in all cases  Unless otherwise stated the data consists of    independent trajectories following the optimal policy  each having a length of     steps  The learning rate was hand tuned  with a little effort  and the number of iterations is kept at      usually  convergence happens much earlier   In all cases  the performance measure is the error function JE   defined by     and we measure the performance of the optimal policy computed for the found reward function  For the max margin algorithm we show the performance of the overall best policy found during the first     iterations  thus optimistically biasing these measurements  We examined the algorithms behavior when  i  the number of the training samples was varied  Figure      ii  the features were linearly transformed  Figure    Table    row     and when  iii  the features were perturbed  Table    row     We see from Figure   that for small sample sizes plain gradient is doing the best  while eventually natural gradient becomes the winner  Note that the scale on the y axis is logarithmic  so the differences between    Preliminary experiments confirm that our conclusions would not change significantly for other sizes   Figure    Performance as a function of the number of training samples  Each curve is an average of    runs using different samples  with      s e  error bars  these algorithms is not big  Max margin also catches up at the end  just like RPROP  Figure   shows the effect of transforming the features linearly  the true reward function still remains in the span of the features   Clearly  Max Marging suffers badly  while the natural gradient algorithm and RPROP are little affected  Plain gradient descent is slowed down  but eventually converges to good solutions  In practice  it is not realistic to assume that a subspace containing the reward function is known  To test how the algorithms behave without this assumption we perturbed the features by adding uniform   max i      max i      random numbers to them  Results are shown in row   of Table    The results indicate the robustness of natural gradients and RPROP  Both plain gradients and max margin suffer large losses under these adverse conditions       SAILING  We also applied the algorithms to the problem of sailing proposed by Vanderbei         In this problem the task is to navigate a boat from one point to another in the shortest possible time  Thus  this is a stochastic shortest path  SSP  problem  Formally  we have a grid of waypoints connected by legs  at each waypoint the sailor has to select one of these eight legs to move on to the next waypoint  The state space in this setting is constructed from the actual situation of the boat and the direction from where the wind is blowing at the specific moment  The eight actions of selecting the next waypoint have different costs depending on the direction of the wind  e g  it costs more time to   NEU   SZEPESVARI  Original Transformed Perturbed  Natural Mean                  gradients Deviation                  RPROP Mean                            Plain gradients Mean Deviation                                            Deviation                       Max margin Mean Deviation                                            Table    Means and deviations of errors  The row marked original gives results for the original features  the row marked transformed gives results when features are linearly transformed  the row marked perturbed gives results when they are perturbed by some noise                  Natural gradients Natural gradients       Rprop      Plain gradients       Error rate  J       Max margin            Maximum margin                             Number of iterations                                      Number of training episodes  Figure    Performance with linearly transformed features  The features were transformed by a  nonsingular  square matrix with uniform        random elements  Each curve is an average of    runs with different scalings of the features  the      s e  error bars are also plotted   Figure    Performance as a function of the number of training episodes  The fraction of states where the found policy differs from the actual optimal policy is plotted against the number of episodes observed   measured by the mean of   runs  The     s e  error bars are also plotted for both methods   sail    degrees against the wind than to sail    degrees in the wind direction etc   We assume that the wind changes follow a Markov process  The reward function is given using a linear combination of the six features of  away  down  cross  up  into  delay   as defined in Vanderbei         all defined as a map    X  A  R   The following weighting was used in the experiments                            T    again that the gradient method outperforms the max margin algorithm by a significant amount   Results as a function of the number of episodes is shown in Figure   for natural gradients and the max margin algorithm  In this case the number of iterations is set to      and we again computed the optimal policy with the reward found by the algorithm  As a more tangible performance measure in this case  we show the number of states where the actions selected by the found policy differ from the ones selected by the policy followed by the expert  The results here are shown fro a small lake of size       The conclusion is    Our preliminary experiments show that the new algorithm performs reasonably for larger problems  too      RELATED WORK  Our main concern in this section is the algorithm of Abbeel and Ng         This algorithm returns policies that come with the guarantee that their average total discounted reward computed from the experts unknown reward function is in the  vicinity of the experts performance  We claim that this guarantee will be met only when the scaling of the features in the method and the true scaling match each other  Actually  this observation led us to the algorithms proposed here  In order to explain why the algorithm of Abbeel and Ng        is sensitive to scalings  we need some background on the algorithm  A crucial assumption in this algorithm is that the reward function is linearly parameterized  i e   r x    T  x   where    X  Rd        NEU   SZEPESVARI  and   Rd is the vector of unknown parameters  It follows that the expected total discounted reward is T E   where E  Rd is the so called feature expectation underlying the expert  From the trajectory of the expert this can be estimated  In fact  we can define  for any policy  and express the expected total discounted reward as T    The main idea of Abbeel and Ng        is then that it suffices to find a policy  whose feature expectations  matches E since  T   T E    k k  k  E k    However  a major underlying hidden assumption  implicit in the formalism of Abbeel and Ng         is that the scaling of the features isknown  To see this assume that d                     k  E k    and in particular E        E                       Further  assume that the features are rescaled by              In the scale the experts perfor new T  mance is  E           E and s performance is          T        T E       A natural requirement is that for any scaling       E    should be lower bound by a positive number  or rather a number close to       By straightforward calculations      E               E      hence although k  E k     the actual performance of  can be quite far from the performance of the expert if the scaling of the features does not match the scaling used in the algorithm  More recently  Ratliff et al         have proposed an algorithm which uses similar ideas to the ones of Abbeel and Ng         Just like Abbeel and Ng        they measure performance with respect to the original reward function and not by the difference of the experts policy and the policy returned      CONCLUSIONS  In the paper we have argued for the advantages of unifying the direct and indirect approaches to apprenticeship learning  The proposed procedure attempts to optimize a cost function  yet it chooses the policy based on a model and thus may overcome problems usually associated with method that directly try to match the experts policy  Although our method has shown stable behaviour in our experiments  more work is needed to fully explore the limitations of the method  One significant barrier for applying the method  as well as other methods based on IRL  is that it needs to solve MDPs many times  This is problematic since solving an MDP is a challenging problem on its own  One idea is to turn to two time scale algorithms that run two incremental procedures in parallel  exploiting that a small change to the parameters would likely cause small changes in the solutions  as confirmed by our theoretical results  There are many important direc   tions to continue this work  The present work assumed that states are observed  This could be replaced by the assumption that sufficiently rich features are observed  however  when this is not satisfied the method wont work  For large state spaces one needs to use function approximation techniques to carry out the computations  It is an open question if the methods would generalize to such settings  Another important direction is to consider infinite MDPs  This presents some technical difficulties  but we expect that the methods could still be generalized to such settings  Yet another interesting direction is to replace the parametric framework with a non parametric one  Acknowledgements Csaba Szepesvari greatly acknowledges the support received through the Alberta Ingenuity Center for Machine Learning  AICML   This work was supported in part by the PASCAL pump priming project Sequential Forecasting and Partial Feedback  Applications to Machine Learning  This publication only reflects the authors views   
 We present a new anytime algorithm that achieves near optimal regret for any instance of finite stochastic partial monitoring  In particular  the new algorithm achieves the minimax regret  within logarithmic factors  for both easy and hard problems  For easy problems  it additionally achieves logarithmic individual regret  Most importantly  the algorithm is adaptive in the sense that if the opponent strategy is in an easy region of the strategy space then the regret grows as if the problem was easy  As an implication  we show that under some reasonable additional  assumptions  the algorithm enjoys an O  T   regret in Dynamic Pricing  proven to be hard by Bartok et al              Introduction Partial monitoring can be cast as a sequential game played by a learner and an opponent  In every time step  the learner chooses an action and simultaneously the opponent chooses an outcome  Then  based on the action and the outcome  the learner suffers some loss and receives some feedback  Neither the outcome nor the loss are revealed to the learner  Thus  a partialmonitoring game with N actions and M outcomes is defined with the pair G    L  H   where L  RN M is the loss matrix  and H  N M is the feedback matrix over some arbitrary set of symbols   These matrices are announced to both the learner and the opponent before the game starts  At time step t  if It  N                  N   and Jt  M                  M   denote the  possibly random  choices of the learner and the opponent  respectively then  the loss suffered by the learner in that time step is L It   Jt    while the Appearing in Proceedings of the    th International Conference on Machine Learning  Edinburgh  Scotland  UK        Copyright      by the author s  owner s    feedback received is H It   Jt    The goal of the learner  or player  is to minimize his PT cumulative loss t   L It   Jt    The performance of the learner is measured in terms of the regret  defined as the excess cumulative loss he suffers compared to that of the best fixed action in hindsight  RT    T X t    L It   Jt    min iN  T X  L i  Jt      t    The regret usually grows with the time horizon T   What distinguishes between a successful and an unsuccessful learner is the growth rate of the regret  A regret linear in T means that the learner does not approach the performance of the optimal action  On the other hand  if the growth rate is sublinear  it is said that the learner can learn the game  In this paper we restrict our attention to stochastic games  adding the extra assumption that the opponent generates the outcomes with a sequence of independent and identically distributed random variables  This distribution will be called the opponent strategy  As for the player  a player strategy  or algorithm  is a  possibly random  function from the set of feedback sequences  observation histories  to the set of actions  In stochastic games  we use a slightly different notion of regret  we compare the cumulative loss with that of the action with the lowest expected loss  RT    T X t    L It   Jt    T min E L i  J       iN  The hardness of a game is defined in terms of the minimax expected regret  or minimax regret for short   RT  G    min max E RT     A pM  where M is the space of opponent strategies  and A is any strategy of the player  In other words  the   Adaptive Stochastic Partial Monitoring  minimax regret is the worst case expected regret of the best algorithm  A question of major importance is how the minimax regret scales with the parameters of the game  such as the time horizon T   the number of actions N   the number of outcomes M   In the stochastic setting  another measure of hardness is worth studying  namely the individual or problem dependent regret  defined as the expected regret given a fixed opponent strategy       Related work Two special cases of partial monitoring have been extensively studied for a long time  full information games  where the feedback carries enough information for the learner to infer the outcome for any action outcome pair  and bandit games  where the learner receives the loss of the chosen action as feedback  Since Vovk        and Littlestone   Warmuth        we know that for full information games  the  minimax regret scales as   T log N    For bandit games   the minimax regret has been proven to scale as   N T    Audibert   Bubeck          The individual regret of these kind of games has also been studied  Auer et al         showed that given any opponent strategy  P the expected regret can be upper bounded by c iN  i      i log T   where i is the expected difference between the loss of action i and an optimal action  Finite partial monitoring problems were introduced by Piccolboni   Schindelhauer         They proved that a game is either hopeless  that is  its minimax regret scales linearly with T    or the regret can be upper bounded by O T        They also give a characterization of hopeless games  Namely  a game is hopeless if it does not satisfy the global observability condition  see Definition   in Section     Their upper bound for non hopeless games was tightened to O T       by Cesa Bianchi et al          who also showed that there exists a game with a matching lower bound  Cesa Bianchi et al         posted the problem of characterizing partial monitoring games with minimax regret less than  T        This problem has been solved since then  The first steps towards classifying partialmonitoring games were made by Bartok et al          who characterized almost all games with two outcomes  They proved that there are only  four categories  games e T     T        and  T    with minimax regret      and named them trivial  easy  hard  and hopeless  re   The Exp  algorithm due to Auer et al         achieves almost the same regret  with an extra logarithmic term   spectively   They also found that there exist games that are easy  but can not easily be transformed to a bandit or full information game  Later  Bartok et al         proved the same results for finite stochastic partial monitoring  with any finite number of outcomes  The condition that separates easy games from hard games is the local observability condition  see Definition     The algorithm Balaton introduced there works by eliminating actions that are thought to be suboptimal with high confidence  They conjectured in their paper that the same classification holds for nonstochastic games  without changing the condition  Recently  Foster   Rakhlin        designed the algorithm NeighborhoodWatch that proves this conjecture to be true  Foster   Rakhlin prove an upper bound on a stronger notion of regret  called internal regret       Contributions In this paper  we extend the results of Bartok et al          We introduce a new algorithm  called CBP for Confidence Bound Partial monitoring  with various desirable properties  First of all  while Balaton only works on easy games  CBP can be run on any non hopeless game  and it achieves  up to logarithmic factors  the minimax regret rates both for easy and hard games  see Corollaries   and     Furthermore  it also achieves logarithmic problem dependent regret for easy games  see Corollary     It is also an anytime algorithm  meaning that it does not have to know the time horizon  nor does it have to use the doubling trick  to achieve the desired performance  The final  and potentially most impactful  aspect of our algorithm is that through additional assumptions on the set of opponent strategies  the minimax regret  e T    of even hard games can be brought down to   While this statement may seem to contradict the result of Bartok et al          in fact it does not  For the precise statement  see Theorem    We call this property adaptiveness to emphasize that the algorithm does not even have to know that the set of opponent strategies is restricted      Definitions and notations Recall from the introduction that an instance of partial monitoring with N actions and M outcomes is defined by the pair of matrices L  RN M and H  N M   where  is an arbitrary set of symbols  In each round t  the opponent chooses an outcome Jt  M and simultaneously the learner chooses an action It  N   Then     Note that these results do not concern the growth rate in terms of other parameters  like N      Adaptive Stochastic Partial Monitoring  the feedback H It   Jt   is revealed and the learner suffers the loss L It   Jt    It is important to note that the loss is not revealed to the learner     Note that the neighborhood action set Ni j naturally   contains i and j  If Ni j contains some other action k then either Ck   Ci   Ck   Cj   or Ck   Ci  Cj    As it was previously mentioned  in this paper we deal with stochastic opponents only  In this case  the choice of the opponent is governed by a sequence J    J          of i i d  random variables  The distribution of these variables p  M is called an opponent strategy  where M   also called the probability simplex  is the set of all distributions over the M outcomes  It is easy to see that  given opponent strategy p  the expected loss of action i can be expressed as    i p  where  i is defined as the column vector consisting of the ith row of L   In general  the elements of the feedback matrix H can be arbitrary symbols  Nevertheless  the nature of the symbols themselves does not matter in terms of the structure of the game  What determines the feedback structure of a game is the occurrence of identical symbols in each row of H  To standardize the feedback structure  the signal matrix is defined for each action   The following definitions  taken from Bartok et al          are essential for understanding how the structure of L and H determines the hardness of a game  Action i is called optimal under strategy p if its expected loss is not greater than that of any other ac  tion i   N   That is     i p   i  p  Determining which action is optimal under opponent strategies yields the cell decomposition   of the probability simplex M   Definition    Cell decomposition   For every action i  N   let Ci    p  M   action i is optimal under p   The sets C            CN constitute the cell decomposition of M   Now we can define the following important properties of actions  Definition    Properties of actions    Action i is called dominated if Ci     If an action is not dominated then it is called non dominated   Action i is called degenerate if it is nondominated and there exists an action i  such that Ci   Ci     If an action is neither dominated nor degenerate then it is called Pareto optimal  The set of Pareto optimal actions is denoted by P  From the definition of cells we see that a cell is either empty or it is a closed polytope  Furthermore  Paretooptimal actions have  M     dimensional cells  The following definition  important for our algorithm  also uses the dimensionality of polytopes  Definition    Neighbors   Two Pareto optimal actions i and j are neighbors if Ci  Cj is an  M    dimensional polytope  Let N be the set of unordered pairs over N that contains neighboring action pairs  The neighborhood action set of two neighboring ac  tions i  j is defined as Ni j    k  N   Ci  Cj  Ck      The concept of cell decomposition also appears in Piccolboni   Schindelhauer          Definition    Let si be the number of distinct symbols in the ith row of H and let             si   be an enumeration of those symbols  Then the signal matrix Si        si M of action i is defined as Si  k  l    I H i l  k     The idea of this definition is that if p  M is the opponents strategy then Si p gives the distribution over the symbols underlying action i  In fact  it is also true that observing H It   Jt   is equivalent to observing the vector SIt eJt   where ek is the k th unit vector in the standard basis of RM   From now on we assume without loss of generality that the learners observation at time step t is the random vector Yt   SIt eJt   Note that the dimensionality of this vector depends on the action chosen by the learner  namely Yt  RsIt   The following two definitions play a key role in classifying partial monitoring games based on their difficulty  Definition    Global observability  Piccolboni   Schindelhauer          A partial monitoring game  L  H  admits the global observability condition  if for all pairs i  j of actions   i   j  kN Im Sk    Definition    Local observability  Bartok et al           A pair of neighboring actions i  j is said to be locally observable if  i   j  kN   Im Sk    We i j denote by L  N the set of locally observable pairs of actions  the pairs are unordered   A game satisfies the local observability condition if every pair of neighboring actions is locally observable  i e   if L   N   The main result of Bartok et al          is that loe T   minimax regret  cally observable games have O  It is easy to see that local observability implies global observability  Also  from Piccolboni   Schindelhauer        we know that if global observability does not hold then the game has linear minimax regret  From now on  we only deal with games that admit the global observability condition  A collection of the concepts and symbols introduced in this section is shown in Table      Adaptive Stochastic Partial Monitoring Table    List of basic symbols  Symbol N  M  N N  M  RM p  M L  RN M H  N M  i  RM C i  M PN N  N    Ni j N Si        si M LN Vi j  N vi j k  Rsk   k  Vi j Wi  R  Definition number of actions and outcomes             N    set of actions M  dim  simplex  set of opponent strategies opponent strategy loss matrix feedback matrix  i   L i      loss vector underlying action i cell of action i set of Pareto optimal actions set of unordered neighboring action pairs neighborhood action set of  i  j   N signal matrix of action i set of locally observable action pairs observer actions underlying  i  j   N observer vectors confidence width for action i  N     The proposed algorithm Our algorithm builds on the core idea underlying algorithm Balaton of Bartok et al          so we start with a brief review of Balaton  Balaton uses sweeps to successively eliminate suboptimal actions  This is done by estimating the differences between the expected losses of pairs of actions  i e   i j     i  j    p  i  j  N    In fact  Balaton exploits that it suffices to keep track of i j for neighboring pairs of actions  i e   for action pairs i  j such that  i  j   N    This is because if an action i is suboptimal  it will have a neighbor j that has a smaller expected loss and so the action i will get eliminated when i j is checked  Now  to estimate i j for some  i  j   N one observes that under thePlocal observability condition  it holds that  i   j   kN   Si  vi j k for some veci j  tors vi j k  Rk   This yields that i j     i   j    p   P def       v i j k Sk p   Since k   Sk p is the vector kNi j of the distribution of symbols under action k  which can be estimated by k  t   the empirical frequencies of the individual symbols under k up to time P observed   k  t  to estimate i j   t  Balaton uses kN   vi j k i j    Since none of the actions in Ni j can get eliminated before one of  i  j  gets eliminated  the estimate of i j gets refined until one of  i  j  is eliminated   The essence of why Balaton achieves a low regret is as follows  When i is not a neighbor of the optimal action i one can show that it will be eliminated before all neighbors j between i and i  get eliminated  Thus  the contribution of such far actions to the re   Found in at  Definition Definition Definition Definition Definition Definition Definition Definition Definition                     gret is minimal  When i is a neighbor of i   it will   be eliminated in time proportional to i i    Thus the contribution to the regret of such an action is propordef tional to i    where i   i i   It also holds that the contribution to the regret of i cannot be larger than i T   Thus  the contribution of i to the regret is at  most min i T  i     T   When some pairs  i  j   N are not locally observable    one needs to use actions other than those in Ni j to construct anP estimate of i j   Under global observability   i  j   kVi j Si  vi j k for an appropriate subset Vi j  N and an appropriate set of vectors vi j    Thus  if the actions in Vi j are kept in play  Pone can  estimate the difference i j as before  using kN   vi j k k  t   i j This motivates the following definition  Definition    Observer sets and observer vectors   The observer set Vi j  N underlying a pair of neighboring actions  i  j   N is a set of actions such that  i   j  kVi j Im Sk    The observer vectors  vi j k  P kVi j are defined to satisfy the equation  i   j   kVi j Sk  vi j k   In particular  vi j k  Rsk   In what follows  the choice of the observer sets and vectors is restricted so that Vi j   Vj i and vi j k   vj i k   Furthermore  the ob  server set Vi j is constrained to be a superset of Ni j and in particular when a pair  i  j  is locally observ  able  Vi j   Ni j must hold  Finally  for any action S   k   i j N Ni j   let Wk   maxi j kVi  j kvi j k k be the confidence width of action k    The reason of the particular choice Vi j   Ni j for lo    Adaptive Stochastic Partial Monitoring  cally observable pairs  i  j  is that we plan to use Vi j  and the vectors vi j    in the case of locally observable pairs  too  For not locally observable pairs  the whole action set N is always a valid observer set  thus  Vi j can be found   However  whenever possible  it is better to use a smaller set  The actual choice of Vi j  and vi j k   is postponed until the effect of this choice on the regret becomes clear  With the observer sets  the basic idea of the algorithm becomes as follows   i  Eliminate the suboptimal actions in successive sweeps   ii  In each sweep  enrich the set of remaining actions P t  by adding the observer actions underlying theS remaining neighboring pairs  i  j   N  t   V t     i j N  t  Vi j    iii  Explore the actions in P t   V t  to update the symbol frequency estimate vectors k  t   Another refinement is to eliminate the sweeps so as to make the algorithm enjoy an advantageous anytime property  This can be achieved by selecting in each step only one action  We propose the action to be chosen should be the one that maximizes the reduction of the remaining uncertainty   This algorithm could be shown to enjoy T regret for locally observable games  However  if we run it on a non locally observable game and the opponent strategy is on Ci  Cj for  i  j   N   L  it will suffer linear regret  The reason is that if both actions i and j are optimal  and thus never get eliminated  the algorithm   will choose actions from Vi j   Ni j too often  Furthermore  even if the opponent strategy is not on the boundary the regret can be too high  say action i is optimal but j is small  while  i  j   N   L  Then a third action k  Vi j with large k will be chosen proportional to   j  times  causing high regret  To combat this we restrict the frequency with which an action can be used for information seeking purposes  For this  we introduce the set of rarely chosen actions  R t     k  N   nk  t   k f  t     where k  R  f   N  R are tuning parameters to be chosen later  Then  the set of actions available at time t is restricted N    t    V t   R t    where S to P t       N  t     i j N  t  Ni j   We will show that with these modifications  the algorithm achieves O T       regret in the general  case  while it will also be shown to achieve an O  T   regret when the opponent uses a benign strategy  A pseudocode for the algorithm is given in Algorithm    It remains to specify the function getPolytope  It gets the array halfSpace as input  The array halfSpace stores which neighboring action pairs have a confident estimate on the difference of their expected losses  along with the sign of the difference  if confi   Algorithm   CBP Input  L  H                N   f   f    Calculate P  N   Vi j   vi j k   Wk for t     to N do Choose It   t and observe Yt  Initialization  nIt       times the action is chosen  It  Yt  Cumulative observations  end for for t   N      N            do for eachP  i  j   N do k   i j  kVi j vi j k nk q Loss diff  estimate  P t  Confidence  ci j  kVi j kvi j k k  nlog k if  i j    ci j then half Space i  j   sgn i j else half Space i  j     end if end for  P t   N  t    getPolytope P  N   half Space    N    t     i j N  t  Nij V t     i j N  t  Vij R t     k  N   nk  t   k f  t   S t    P t   N    t    V t   R t   W  Choose It   argmaxiS t  nii and observe Yt It  It   Yt nIt  nIt     end for dent   Each of these confident pairs define an open halfspace  namely    i j    p  M   half Space i  j   i   j    p       The function getPolytope calculates the open polytope defined as the intersection of the above halfspaces  Then for all i  P it checks if Ci intersects with the open polytope  If so  then i will be an element of P t   Similarly  for every  i  j   N   it checks if Ci  Cj intersects with the open polytope and puts the pair in N  t  if it does  Note that it is not enough to compute P t  and then drop from N those pairs  k  l  where one of k or l is excluded from P t   it is possible that the boundary Ck  Cl between the cells of two actions k  l  P t  is included in the rejected region  For an illustration of cell decomposition and excluding cells  see Figure    Computational complexity The computationally heavy parts of the algorithm are the initial calculation of the cell decomposition and the function getPolytope  All of these require linear programming  In the preprocessing phase we need to solve N   N   linear   Adaptive Stochastic Partial Monitoring                                                                    a  Cell decomposition    b  Gray indicates cluded area   ex   Figure    An example of cell decomposition  M        programs to determine cells and neighboring pairs of cells  Then in every round  at most N   linear programs are needed  The algorithm can be sped up by caching previously solved linear programs   short explanation of the different terms in the bound  The first term corresponds to the confidence interval failure event  The second term comes from the initialization phase of the algorithm  The remaining four terms come from categorizing the choices of the algorithm by two criteria      Would It be different if R t  was defined as R t    N       Is It  P t   Nt    These two binary events lead to four different cases in the proof  resulting in the last four terms of the bound  An implication of Theorem   is an upper bound on the individual regret of locally observable games  Corollary    If G is locally observable then     X   E RT      Vi j             i j N     N X  k    Wk   k    d k  log T   k     Analysis of the algorithm The first theorem in this section is an individual upper bound on the regret of CBP  Theorem    Let  L  H  be an N by M partialmonitoring game  For a fixed opponent strategy p  M   let i denote the difference between the expected loss of action i and an optimal action  For any time horizon T   algorithm CBP with parameters           k   Wk   f  t        t    log    t has expected regret     Vi j        X  E RT      i j N N X     k   k        X   Wk                N X  k  kV N    d l k    l k    log T                  Wk T      X  log       The following corollary is an upper bound on the minimax regret of any globally observable game  Corollary    Let G be a globally observable game  Then there exists a constant c such that the expected regret can be upper bounded independently of the choice of p as E RT    cT     log    T    k    d k  log T k  k min  Wk   Proof  If a game is locally observable then V  N       leaving the last two sums of the statement of Theorem   zero   T       k     Wk T     log    T  kV N       dk     W     T     log    T   where W   maxkN Wk   V    i j N Vi j   N        i j N Ni j   and d            dN are game dependent constants  The proof is omitted for lack of space   Here we give a   For complete proofs we refer the reader to the supplementary material   The following theorem is an upper bound on the minimax regret of any globally observable game against benign opponents  To state the theorem  we need a new definition  Let A be some subsetTof actions in G  We call A a point local game in G if iA Ci      Theorem    Let G be a globally observable game  Let    M be some subset of the probability simplex such that its topological closure   has    Ci  Cj    for every  i  j   N   L  Then there exists a constant c such that for every p      algorithm CBP with     parameters       k   Wk   f  t        t    log    t achieves p E RT    cdpmax bT log T   where b is the size of the largest point local game  and dpmax is a game dependent constant  In a nutshell  the proof revisits the four cases of the proof of Theorem    and shows that the terms which would yield T     upper bound can be non zero only for a limited number of time steps    Adaptive Stochastic Partial Monitoring  Remark    Note that the above theorem implies that CBP does notneed to have any prior knowledge about   to achieve T regret  This is why we say our algorithm is adaptive  An immediate implication of Theorem   is the following minimax bound for locally observable games  Corollary    Let G be a locally observable finite partial monitoring game  Then there exists a constant c such that for every p  M   p E RT    c T log T   Remark    The upper bounds in Corollaries   and   both have matching lower bounds up to logarithmic factors  Bartok et al          proving that CBP achieves near optimal regret in both locally observable and non locally observable games      Experiments We demonstrate the results of the previous sections using instances of Dynamic Pricing  as well as a locally observable game  We compare the results of CBP to two other algorithms  Balaton  Bartok et al         which is  as mentioned earlier  in the paper  the first e T   minimax regret for all algorithm that achieves O  locally observable finite stochastic partial monitoring games  and FeedExp   Piccolboni   Schindelhauer         which achieves O T       minimax regret on all non hopeless finite partial monitoring games  even against adversarial opponents       A locally observable game The game we use to compare CBP and Balaton has   actions and   outcomes  The game is described with the loss and feedback matrices            a b b L           H   b a b         b b a We ran the algorithms    times for    different stochastic strategies  We averaged the results for each strategy and then took pointwise maximum over the    strategies  Figure   a  shows the empirical minimax regret calculated the way described above  In addition  Figure   b  shows the regret of the algorithms against one of the opponents  averaged over     runs  The results indicate that CBP outperforms both FeedExp and Balaton  We also observe that  although the asymptotic performace of Balaton is proven to be better than that of FeedExp  a larger constant factor makes Balaton lose against FeedExp even at time step ten million        Dynamic Pricing In Dynamic Pricing  at every time step a seller  player  sets a price for his product while a buyer  opponent  secretly sets a maximum price he is willing to pay  The feedback for the seller is buy or no buy  while his loss is either a preset constant  no buy  or the difference between the prices  buy   The finite version of the game can be described with the following matrices           N    y y  y  c      N    n y    y      L       H                                              c  c   n  n y This game is not locally observable and thus it is hard  Bartok et al          Simple linear algebra gives that the locally observable action pairs are the consecutive actions  L     i  i        i  N       while quite surprisingly  all action pairs are neighbors  We compare CBP with FeedExp on Dynamic Pricing with N   M     and c      Since Balaton is undefined on not locally observable games  we can not include it in the comparison  To demonstrate the adaptiveness of CBP  we use two sets of opponent strategies  The benign setting is a set of opponents which are far away from dangerous regions  that is  from boundaries between cells of non locally observable neighboring action pairs  The harsh settings  however  include opponent strategies that are close or on the boundary between two such actions  For each setting we maximize over    strategies and average over    runs  We also compare the individual regret of the two algorithms against one benign and one harsh strategy  We averaged over     runs and plotted the    percent confidence intervals  The results  shown in Figures   and    indicate that CBP has a significant advantage over FeedExp on benign settings  Nevertheless  for the harsh settings FeedExp slightly outperforms CBP  which we think is a reasonable price to pay for the benefit of adaptivity   
  incomplete on each step  still efficiently computes optimal actions in a timely manner   We consider the problem of efficiently learning optimal control policies and value functions over large state spaces in an online setting in which estimates must be available after each interaction with the world  This paper develops an explicitly model based approach extending the Dyna architecture to linear function approximation  Dynastyle planning proceeds by generating imaginary experience from the world model and then applying model free reinforcement learning algorithms to the imagined state transitions  Our main results are to prove that linear Dyna style planning converges to a unique solution independent of the generating distribution  under natural conditions  In the policy evaluation setting  we prove that the limit point is the least squares  LSTD  solution  An implication of our results is that prioritized sweeping can be soundly extended to the linear approximation case  backing up to preceding features rather than to preceding states  We introduce two versions of prioritized sweeping with linear Dyna and briefly illustrate their performance empirically on the Mountain Car and Boyan Chain problems   The Dyna architecture  Sutton       provides an effective and flexible approach to incremental planning while maintaining responsiveness  There are two ideas underlying the Dyna architecture  One is that planning  acting  and learning are all continual  operating as fast as they can without waiting for each other  In practice  on conventional computers  each time step is shared between planning  acting  and learning  with proportions that can be set arbitrarily according to available resources and required response times   Online learning and planning  Efficient decision making when interacting with an incompletely known world can be thought of as an online learning and planning problem  Each interaction provides additional information that can be used to learn a better model of the worlds dynamics  and because this change could result in a different action being best  given the model   the planning process should be repeated to take this into account  However  planning is inherently a complex process  on large problems it not possible to repeat it on every time step without greatly slowing down the response time of the system  Some form of incremental planning is required that  though  The second idea underlying the Dyna architecture is that learning and planning are similar in a radical sense  Planning in the Dyna architecture consists of using the model to generate imaginary experience and then processing the transitions of the imaginary experience by model free reinforcement learning algorithms as if they had actually occurred  This can be shown  under various conditions  to produce exactly the same results as dynamic programming methods in the limit of infinite imaginary experience  The original papers on the Dyna architecture and most subsequent extensions  e g   Singh       Peng   Williams       Moore   Atkeson       Kuvayev   Sutton       assumed a Markov environment with a tabular representation of states  This table lookup representation limits the applicability of the methods to relatively small problems  Reinforcement learning has been combined with function approximation to make it applicable to vastly larger problems than could be addressed with a tabular approach  The most popular form of function approximation is linear function approximation  in which states or state action pairs are first mapped to feature vectors  which are then mapped in a linear way  with learned parameters  to value or next state estimates  Linear methods have been used in many of the successful large scale applications of reinforcement learning  e g   Silver  Sutton   Muller       Schaeffer  Hlynka   Jussila        Linear function approximation is also simple  easy to understand  and possesses some of the strongest convergence and performance guarantees among function approximation methods  It is   natural then to consider extending Dyna for use with linear function approximation  as we do in this paper  There has been little previous work addressing planning with linear function approximation in an online setting  Paduraru        treated this case  focusing mainly on sampling stochastic models of a cascading linear form  but also briefly discussing deterministic linear models  Degris  Sigaud and Wuillemin        developed a version of Dyna based on approximations in the form of dynamic Bayes networks and decision trees  Their system  SPITI  included online learning and planning based on an incremental version of structured value iteration  Boutilier  Dearden   Goldszmidt        Singh        developed a version of Dyna for variable resolution but still tabular models  Others have proposed linear least squares methods for policy evaluation that are efficient in the amount of data used  Bradtke   Barto       Boyan             Geramifard  Bowling   Sutton        These methods can be interpreted as forming and then planning with a linear model of the worlds dynamics  but so far their extensions to the control case have not been well suited to online use  Lagoudakis   Parr       Peters  Vijayakumar   Schaal       Bowling  Geramifard    Wingate        whereas our linear Dyna methods are naturally adapted to this case  We discuss more specifically the relationship of our work to LSTD methods in a later section  Finally  Atkeson        and others have explored linear  learned models with off line planning methods suited to low dimensional continuous systems      Notation  We use the standard framework for reinforcement learning with linear function approximation  Sutton   Barto        in which experience consists of the time indexed stream s    a    r    s    a    r    s           where st  S is a state  at  A is an action  and rt  R is a reward  The actions are selected by a learning agent  and the states and rewards are selected by a stationary environment  The agent does not have access to the states directly but only through a corresponding feature vector t  Rn    st    The n agent selects actions P according to a policy     R  A         such that aA    a         An important step towards finding a good policy is to estimate the value function for a given policy  policy evaluation   The value function is approximated as a linear function with parameter vector   Rn       X    s   V   s    E  t  rt   s    s   t    where           In this paper we consider policies that are greedy or   greedy with respect to the approximate statevalue function   Algorithm     Linear Dyna for policy evaluation  with random sampling and gradient descent model learning Obtain initial     F  b For each time step  Take action a according to the policy  Receive r          r            F  F       F    b  b    r  b    temp    Repeat p times  planning   Generate a sample  from some distribution     F  r  b         r              temp     Theory for policy evaluation  The natural place to begin a study of Dyna style planning is with the policy evaluation problem of estimating a statevalue function from a linear model of the world  The model consists of a forward transition matrix F  Rn  Rn  incorporating both environment and policy  and an expected reward vector b  Rn   constructed such that F  and b   can be used as estimates of the feature vector and reward that follow   A Dyna algorithm for policy evaluation goes through a sequence of planning steps  on each of which a starting feature vector  is generated according to a probability distribution   and then a next feature vector     F  and next reward r   b   are generated from the model  Given this imaginary experience  a conventional modelfree update is performed  for example  according to the linear TD    algorithm  Sutton              r                   or according to the residual gradient algorithm  Baird              r                         where      is a step size parameter  A complete algorithm using TD     including learning of the model  is given in Algorithm         Convergence and fixed point  There are two salient theoretical questions about the Dyna planning iterations     and      Under what conditions on  and F do they converge  and What do they converge to  Both of these questions turn out to have interesting answers  First  note that the convergence of     is in question in part because it is known that linear TD    may diverge if the distribution of starting states during training does not match the distribution created by the normal dynamics of   the system  that is  if TD    is used off policy  This suggests that the sampling distribution used here    might have to be strongly constrained in order for the iteration to be stable  On the other hand  the data here is from the model  and the model is not a general system  it is deterministic  and linear  This special case could be much better behaved  In fact  convergence of linear Dyna style policy evaluation  with either the TD    or residual gradient iterations  is not affected by   but only by F   as long as  exercises all directions in the full n dimensional vector space  Moreover  not only is the fact of convergence unaffected by   but so is the value converged to  In fact  we show below that convergence is to a deterministic fixed point  a value of  such that the iterations     and     leave it unchanged not just in expected value  but for every individual  that could be generated by   The only way this could be true is if the TD error  the first expression in parentheses in each iteration  were exactly zero  that is  if       r                 b     F          b   F           And the only way that this can be true for all  is for the expression in parenthesis above to be zero     Before verifying the conditions of this result  let us rewrite     in terms of the matrix G   I  F   k      k   k  b  k   k   F  I k  k    b    F    I      k   k sk        I  F      b        where    Rn is P arbitrary  AssumeP that  i  the step size   sequence satisfies k   k     k   k       ii  r F        iii   k   are uniformly   bounded   i i d  random variables  and that  iv  C   E k   is non singular  k Then the parameter vector k converges with probability one to  I  F      b     k   k  b  k  k  Gk  k  Here sk is defined by the last equation       assuming that the inverse exists  Note that this expression for the fixed point does not depend on   as promised  If I  F   is nonsingular  then there might be no fixed point  This could happen for example if F were an expansion  or more generally if the limit  F   were not zero  These cases correspond to world models that say the feature vectors diverge to infinity over time  Failure to converge in these cases should not be considered a problem for the Dyna iterations as planning algorithms  these are cases in which the planning problem is ill posed  If the feature vectors diverge  then so too may the rewards  in which case the true values given the model are infinite  No real finite Markov decision process could behave in this way  It remains to show the conditions on F under which the iterations converge to the fixed point if one exists  We prove next that under the TD    iteration      convergence is guaranteed if the numerical radius of F is less than one   and    k     k   k  b  k   k  F k  k  k  k      b   F       which immediately implies that   Theorem      Convergence of linear TD    Dyna for policy evaluation   Consider the TD    iteration with a nonnegative step size sequence  k     Proof  The idea of the proof is to view the algorithm as a stochastic gradient descent method  In particular  we apply Proposition     of  Bertsekas   Tsitsiklis               then that under the residual gradient iteration      convergence is guaranteed for any F as long as the fixed point exists  That F s numerical radius be less than   is a stronger condition than nonsingularity of I  F     but it is similar in that both conditions pertain to the matrix trending toward expansion when multiplied by itself   The model is deterministic because it generates the expectation of the next feature vector  the system itself may be stochastic    The numerical radius of a real valued square matrix A is defined by r A    maxkxk     xT Ax   The cited proposition requires the definition of a potential function J   and will allow us to conclude that limk J k       with probability one  Let   us choose J         E  b  k     F k    k      Note that by our i i d  assumptions on the features  J   is welldefined  We need to check four conditions  because the step size conditions are automatically satisfied    i  The nonnegativity of the potential function   ii  The Lipschitz continuity of J     iii  The pseudo gradient property of the expected update direction  and  iv  The boundedness of the  expected  magnitude of the update  more precisely that E ksk k    k  O kJ k  k      Nonnegativity is satisfied by definition and the boundedness condition  iv  is satisfied thanks to the boundedness of the features  Let us show now that the pseudo gradient property  iii  is satisfied  This condition requires the demonstration of a positive constant c such that ckJ k  k    J k    E  sk  k           Define sk   E  sk  k     Cb  CG  k   A simple calculation gives J k     Gsk   Hence kJ k  k           s  k G Gsk and  J k    sk   sk Gsk   Therefore           is equivalent to c sk G Gsk  sk Gsk   In order to make this true with a sufficiently small c  it suffices to show that   s  Gs     holds for any non zero vector s  An elementary reasoning shows that this is equivalent to     G   G    being positive definite  which in turn is equivalent to r F       showing that  iii  is satisfied  Hence  we have verified all the assumptions of the cited proposition and can therefore we conclude that limk J k       with probability one  Plugging in the expression of J k    we get limt  CbCG  k        Because C and G are invertible  this latter follows from r F        it follows that the limit of k exists and limk k    G     b    I  F      b   verges with probability one to  I  F      b  assuming that  I  F     is non singular  Proof  As all the conditions of Proposition     of  Bertsekas   Tsitsiklis       are trivially satisfied with the choice J     E  J   k     we can conclude that k converges w p   to the minimizer of J    In the previous theorem we have seen that the minimizer of J   is indeed     I  F      b  finishing the proof       Convergence to the LSTD solution  Several extensions of this result are possible  First  the requirement of i i d  sampling can be considerably relaxed  With an essentially unchanged proof  it is possible to show that the theorem remains true if the feature vectors are generated by a Markov process given that they satisfy appropriate ergodicity conditions  Moreover  building on a result by Delyon         one can show that the result continues to hold even if the sequence of features is generated in an algorithmic manner  again provided that some ergodicity conditions are met  PKThe major assumption then is that C   limK   K k   k   k exists and is nonsingular  Further  because there is no noise to reject  there is noP need to decay the step sizes towards zero  the condi tion k   k      in the proofs is used to filter out noise   In particular  we conjecture that sufficiently small constant step sizes would work as well  for a result of this type see Proposition     by Bertsekas   Tsitsiklis         So far we have discussed the convergence of planning given a model  but we have said nothing about the relationship of the model to data  or about the quality of the resultant solution  Suppose the model were the best linear fit to a finite dataset of observed feature vector to feature vector transitions with accompanying rewards  In this case we can show that the fixed point of the Dyna updates is the least squares temporal difference solution  This is the solution for which the mean TD    update is zero and is also the solution found by the LSTD    algorithm  Barto   Bradtke         On the other hand the requirement on the numerical radius of F seems to be necessary for the convergence of the TD    iteration  By studying the ODE associated with      we see that it is stable if and only if CG is a positive stable matrix  i e   iff all its eigenvalues have positive real part   From this it seems necessary to require that G is positive stable  However  to ensure that CG is positive stable the strictly stronger condition that G   G  is positive definite must be satisfied  This latter condition is equivalent to r F        Proof  It suffices to show that the respective solution sets of the equations  We turn now to consider the convergence of Dyna planning using the residual gradient Dyna iteration      This update rule can be derived by taking the gradient of J   k      b  k     k    k    w r t    Thus  as an immediate consequence of Proposition     of  Bertsekas   Tsitsiklis       we get the following result  Theorem      Convergence of residual gradient Dyna for policy evaluation   Assume that k is updated according to k     k   k  b  k   k  F k  k  k   k  F k    where    Rn is arbitrary  Assume that the non negative step size sequence  k   satisfies the summability condition  i  of Theorem     and that  k   are uniformly bounded i i d  random variables  Then the parameter vector k con   Theorem      Given a training dataset of feature  reward  next state feature triples D        r                 n   rn    n    let F  bPbe the least squares model built on D  Assume that n C   k   k   k has full rank  Then the solution     is the same as the LSTD solution on this training set         n X  k  rk     k        k          k          b    F    I        are the same  This is because the LSTD parameter vectors are obtained by solving the first equation and the TD    Dyna solutions are derived from the second equation  Pn Pn Let D   k   k   k      and r   k   k rk   A standard calculation shows that F      C   D  and b   C   r   Plugging in C  D into     and factoring out  shows that any solution of     also satisfies       r    D  C          If we multiply both sides of     by C   from the left we get      Hence any solution of     is also a solution of      Because all the steps of the above derivation are reversible  we get that the reverse statement holds as well    Algorithm     Linear Dyna with PWMA prioritized sweeping  policy evaluation  Obtain initial     F  b For each time step  Take action a according to the policy  Receive r      r                 F  F       F    b  b    r  b    For all i such that  i        For all j such that F ij       Put j on the PQueue with priority  F ij  i   Repeat p times while PQueue is not empty  i  pop the PQueue   b i      F ei   i   i    i     For all j such that F ij       Put j on the queue with priority  F ij           Algorithm     Linear Dyna with MG prioritized sweeping  policy evaluation  Obtain initial     F  b For each time step  Take action a according to the policy  Receive r      r                 F  F       F    b  b    r  b    For all i such that  i        Put i on the PQueue with priority   i   Repeat p times while PQueue is not empty  i  pop the PQueue For all j such that F ij         b j      F ej   j   j    j     Put j on the PQueue with priority         Linear prioritized sweeping  We have shown that the convergence and fixed point of policy evaluation by linear Dyna are not affected by the way the starting feature vectors are chosen  This opens the possibility of selecting them cleverly so as to speed the convergence of the planning process  One natural ideathe idea behind prioritized sweepingis to work backwards from states that have changed in value to the states that lead into them  The lead in states are given priority for being updated because an update there is likely to change the states value  because they lead to a state that has changed in value   If a lead in state is updated and its value is changed  then its lead in states are in turn given priority for updating  and so on  In the table lookup context in which this idea was developed  Moore   Atkeson       Peng       see also Wingate   Seppi        there could be many states preceding each changed state  but only one could be updated at a time  The states waiting to be updated were kept in a queue  prioritized by the size of their likely effect on the value function  As high priority states were popped off the queue and updated  it would sometimes give rise to highly efficient sweeps of updates across the state space  this is what gave rise to the name prioritized sweeping  With function approximation it is not possible to identify and work backwards from individual states  but alternatively one could work backwards feature by feature  If there has just been a large change in  i   the component of the parameter vector corresponding to the ith feature  then one can look backwards through the model to find the features j whose components  j  are likely to have changed as a result  These are the features j for which the elements F ij of F are large  One can then preferentially construct  starting feature vectors  that have non zero entries at these j components  In our algorithms we choose the starting vectors to be the unit basis vectors ej   all of whose components are zero except the jth  which is     Our theoretical results assure us that this cannot affect the result of convergence   Using unit basis vectors is very efficient computationally  as the vector matrix multiplication F  is reduced to pulling out a single column of F   There are two tabular prioritized sweeping algorithms in the literature  The first  due simultaneously to Peng and Williams        and to Moore and Atkeson         which we call PWMA prioritized sweeping  adds the predecessors of every state encountered in real experience to the priority queue whether or not the value of the encountered state was significantly changed  The second form of prioritized sweeping  due to McMahan and Gordon         and which we call MG prioritized sweeping  puts each encountered state on the queue  but not its predecessors  For McMahan and Gordon this resulted in a more efficient planner  A complete specification of our feature by feature versions of these two forms of prioritized sweeping are given above  with TD    updates and gradient descent model learning  as Algorithms   and    These algorithms differ slightly from previous prioritized sweeping algorithms in that they update the value function from the real experiences and not just from model generated experience  With function approximation  real experience is always more informative than model generated experience  which will be distorted by the function approximator  We found this to be a significant effect in our empirical experiments  Section       Algorithm    Linear Dyna with MG prioritized sweeping and TD    updates  control  Obtain initial     F  b For each time step       a  arg maxa b   or   greedy  a     Fa  Take action a  receive r      r                 Fa  Fa       Fa    ba  ba    r  b  a   For all i such that  i        Put i on the PQueue with priority   i   Repeat p times while PQueue is not empty  i  pop the PQueue ij For all j s t  there   exists an a s t  F   a         maxa ba  j     Fa ej   j   j    j     Put j on the PQueue with priority            Theory for Control  We now turn to the full case of control  in which separate models Fa   ba are learned and are then available for each action a  These are constructed such that Fa  and b  a  can be used as estimates of the feature vector and reward that follow  if action a is taken  A linear Dyna algorithm for the control case goes through a sequence of planning steps on each of which a starting feature vector  and an action a are chosen  and then a next feature vector     Fa  and next reward r   ba  are generated from the model  Given this imaginary experience  a conventional model free update is performed  The simplest case is to again apply      A complete algorithm including prioritized sweeping is given in Algorithm    The theory for the control case is less clear than for policy evaluation  The main issue is the stability of the mixture of the forward model matrices  The corollary below is stated for an i i d  sequence of features  but by the remark after Theorem     it can be readily extended to the case where the policy to be evaluated is used to generate the trajectories  Corollary      Convergence of linear TD    Dyna with action models   Consider the Dyna recursion     with the modification that in each step  instead of F k   we use F k   k   where  is a policy mapping feature vectors to actions and  Fa   is a collection of forward model matrices  Similarly  b  k is replaced by b   k   k   As before  assume that k is an unspecified i i d  process  Let  F  b    be the least squares   model of   F   harg minG E kGk  iF k   k k   and b     arg minu E  u  k  b  If the numerical radius  k   k   of F is bounded by one  then the conclusions of Theo       N                   N                                                                                                                      Figure    The general Boyan Chain problem  rem     hold  the parameter vector k converges with probability one to  I  F      b  Proof  The proof is immediate from equation     the normal     for F   which states that E F k     E F k   k   k k   and once we observe that  in the proof of  Theorem       F appears only in expressions of the form E F k   k   As in the case of policy evaluation  there is a corresponding corollary for the residual gradient iteration  with an immediate proof  These corollaries say that  for any policy with a corresponding model that is stable  the Dyna recursion can be used to compute its value function  Thus we can perform a form of policy iterationcontinually computing an approximation to the value function for the greedy policy      Empirical results  In this section we illustrate the empirical behavior of the four Dyna algorithms and make comparisons to model free methods using variations of two standard test problems  Boyan Chain and Mountain Car  Our Boyan Chain environment is an extension of that by Boyan              from    to    states  and from   to    features  Geramifard  Bowling   Sutton        Figure   depicts this environment in the general form  Each episode starts at state N      and terminates in state    For all states s      there is an equal probability of transitioning to states s    or s    with a reward of    From states   and    there are deterministic transitions to states   and   with respective rewards of   and    Our Mountain Car environment is exactly as described by Sutton        Sutton   Barto        re implemented in Matlab  An underpowered car must be driven to the top of a hill by rocking back and forth in a valley  The state variables are a pair  position velocity  initialized to            at the beginning of each episode  The reward is   per time step  There are three discrete actions  accelerate  reverse  and coast   We used a value function representation based on tile coding feature vectors exactly as in Suttons        experiments  with    tilings over the combined  position  velocity  pair  and with the tiles hashed down to        features  In the policy evaluation experiments with this domain  the policy was to accelerate in         Boyan chain         Mountain Car  x     Dyna Random TD        Loss  Dyna Random Dyna PWMA  Loss         TD         Dyna MG  Dyna PWMA     Dyna MG                      Episode                              Episode             Figure    Performance of policy evaluation methods on the Boyan Chain and Mountain Car environments the direction of the current velocity  and we added noise to the domain that switched the selected action to a random action with     probability  Complete code for our test problems as standard RL Glue environments is available from the RL Library hosted at the University of Alberta  In all experiments  the step size parameter  took the form      t     NN   t       in which t is the episode number and the pair  N        was selected based on empirically finding the best combination out of                 and N                     separately for each algorithm and domain  All methods observed the same trajectories in policy evaluation  All graphs are averages of    runs  error bars indicate standard errors in the means  Other parameter settings were                and       We performed policy evaluation experiments with four algorithms  Dyna Random  Dyna PWMA  Dyna MG  as in Algorithms      and model free TD     In the case of the Dyna Random algorithm  the starting feature vectors in planning were chosen to be unit basis vectors with the   in a random location  Figure   shows the policy evaluation performance of the four methods in the Boyan Chain and Mountain Car environments  For the Boyan Chain domain  the loss was the root mean squared error of the learned value function compared to the exact analytical value  averaged over all states  In the Mountain Car domain  the states are visited very non uniformly  and a more sophisticated measure is needed  Note that all of the methods drive  toward an asymptotic value in which the expected TD    update is zero  we can use the distance from this as a loss measure  Specifically  we evaluated each learned value function by freezing it and then running a fixed set of         episodes with it while running the TD    algorithm  but not allowing  to actually change   The norm of the sum of the  attempted  update vectors was then computed and used as the loss  In practice  this measure can be computed very efficiently as   A   b     in the notation of  LSTD     see Bradtke   Barto        In the Boyan Chain environment  the Dyna algorithms generally learned more rapidly than model free TD     DynaMG was initially slower than the other algorithms  then caught up and surpassed them  The relatively poor early performance of Dyna MG was actually due to its being a better planning method  After few episodes the model tends to be of very high variance  and so therefore is the best value function estimate given it  We tested this hypothesis by running the Dyna methods starting with a fixed  well learned model  in this case Dyna MG was the best of all the methods from the beginning  All of these data are for one step of planning for each real step of interaction with the world  p       In preliminary experiments with larger values of p  up to p       we found further improvements in learning rate of the Dyna algorithms over TD     and again Dyna MG was best  The results for Mountain Car are less clear  Dyna MG quickly does significantly better than TD     but the other Dyna algorithms lag initially and never surpass TD     Note that  for any value of p  Dyna MG does many more  updates than the other two Dyna algorithms  because these updates are in an inner loop  cf  Algorithms   and     Even so  because of its other efficiencies Dyna MG tended to run faster overall in our implementation  Obviously  there is a lot more interesting empirical work that could be done here  We performed one Mountain Car experiment with DynaMG as a control algorithm  Algorithm     comparing it with model free Sarsa  i e   Algorithm   with p       The results are shown in Figure    As before  Dyna MG showed a distinct advantage over the model free method in terms of learning rate  There was no clear advantage for either method in the second half of the experiment  We note that  asymptotically  model free methods are never worse than model based methods  and are often better because the model does not converge exactly to the true system because               Return       Dyna MG           Sarsa                           Episode           Figure    Control performance on Mountain Car  Conclusion  In this paper we have taken important steps toward establishing the theoretical and algorithmic foundations of Dyna style planning with linear function approximation  We have established that Dyna style planning with familiar reinforcement learning update rules converges under weak conditions corresponding roughly  in some cases  to the existence of a finite solution to the planning problem  and that convergence is to a unique least squares solution independent of the distribution used to generate hypothetical experience  These results make possible our second main contribution  the introduction of algorithms that extend prioritized sweeping to linear function approximation  with correctness guarantees  Our empirical results illustrate the use of these algorithms and their potential for accelerating reinforcement learning  Overall  our results support the conclusion that Dyna style planning may be a practical and competitive approach to achieving rapid  online control in stochastic sequential decision problems with large state spaces  Acknowledgements  of structural modeling assumptions   The case we treat herelinear models and value functions with one step TD methodsis a rare case in which asymptotic performance of model based and model free methods should be identical   The benefit of models  and of planning generally  is in rapid adaptation to new problems and situations   The authors gratefully acknowledge the substantial contributions of Cosmin Paduraru and Mark Ring to the early stages of this work  This research was supported by iCORE  NSERC and Alberta Ingenuity   These empirical results are not extensive and in some cases are preliminary  but they nevertheless illustrate some of the potential of linear Dyna methods  The results on the Boyan Chain domain show that Dyna style planning can result in a significant improvement in learning speed over modelfree methods  In addition  we can see trends that have been observed in the tabular case re occurring here with linear function approximation  In particular  prioritized sweeping can result in more efficient learning than simply updating features at random  and the MG version of prioritized sweeping seems to be better than the PWMA version   Atkeson  C          Using local trajectory optimizers to speed up global optimization in dynamic programming  Advances in Neural Information Processing Systems             Baird  L  C          Residual algorithms  Reinforcement learning with function approximation  In Proceedings of the Twelfth International Conference on Machine Learning  pp        Bertsekas  Dimitri P   Tsitsiklis  J          Neuro Dynamic Programming  Athena Scientific        Boutilier  C   Dearden  R   Goldszmidt  M          Stochastic dynamic programming with factored representations  Artificial Intelligence             Bowling  M   Geramifard  A   Wingate  D          Sigma point policy iteration  In Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems  Boyan  J  A          Least squares temporal difference learning  In Proceedings of the Sixteenth International Conference on Machine Learning        Boyan  J  A          Technical update  Least squares temporal difference learning  Machine Learning              Bradtke  S   Barto  A  G          Linear least squares al   Finally  we would like to note that we have done extensive experimental work  not reported here  attempting to adapt least squares methods such as LSTD to online control domains  in particular to the Mountain Car problem  A major difficulty with these methods is that they place equal weight on all past data whereas  in a control setting  the policy changes and older data becomes less relevant and may even be misleading  Although we have tried a variety of forgetting strategies  it is not easy to obtain online control performance with these methods that is superior to modelfree methods  One reason we consider the Dyna approach to be promising is that no special changes are required for this case  it seems to adapt much more naturally and effectively to the online control setting   
ions  Alejandro Isaza and Csaba Szepesvari and Vadim Bulitko and Russell Greiner Department of Computing Science  University of Alberta Edmonton  Alberta  T G  E   CANADA  isaza szepesva bulitko greiner  cs ualberta ca  Abstract In this paper  we consider planning in stochastic shortest path  SSP  problems  a subclass of Markov Decision Problems  MDP   We focus on medium size problems whose state space can be fully enumerated  This problem has numerous important applications  such as navigation and planning under uncertainty  We propose a new approach for constructing a multi level hierarchy of progressively simpler abstractions of the original problem  Once computed  the hierarchy can be used to speed up planning by first finding a policy for the most abstract level and then recursively refining it into a solution to the original problem  This approach is fully automated and delivers a speed up of two orders of magnitude over a state of the art MDP solver on sample problems while returning near optimal solutions  We also prove theoretical bounds on the loss of solution optimality resulting from the use of abstractions      Introduction and Motivation  We focus on planning in stochastic shortest path problems  the problem of reaching some goal state under uncertainty  when planning time is critical  a situation that arises  for instance  in path planning for agents in commercial video games  where map congestions are modeled as uncertainty of transitions  Another example is path planning for multi link robotic manipulators  where the uncertainty comes from unmodeled dynamics as well as sensor and actuator noise  More specifically  we consider the problem of finding optimal policies in a sequence of stochastic shortest path problems  Bertsekas   Tsitsiklis         where the problems share the same dynamics and transition costs  and differ only in the location of the goal state  When the state space underlying the problems is sufficiently large  exact planning methods are unable to deliver a solution within the required time  forcing the user to resort to approximate methods in order to scale to large domains  Exploiting the fact that multiple planning  problems share the same dynamics and transition costs  we build an abstracted representation of the shared structure where planning is faster  then map the individual planning problem into the abstract space and derive a solution there  The solution is then refined back into the original space  In a related problem of path planning under real time constraints in deterministic environments  e g   Sturtevant         a particularly successful approach is implemented in the PR LRTS algorithm  Bulitko  Sturtevant  Lu    Yau         which builds an abstract state space by partitioning the set of states into cliques  i e   each state within each cluster is connected to each other state in that cluster with a single action   Each such cluster becomes a single abstract state  Two abstract states are connected by an abstract transition if there is a pair of non abstract states  one from each abstract state  connected by a single action  The resulting abstract space is smaller and simpler  yet captures some of the structure of the original search problem  Thus  an abstract solution can be used to guide and constrain the search in the original problem  yielding a significant speed up  Further speed ups can be obtained by building abstractions on top of abstractions  which creates a hierarchy of progressively smaller abstract search spaces  PR LRTS can then be tuned to meet strict real time constraints while minimizing solution suboptimality  Note that state cliques produced by PR LRTS make good abstract states because landing anywhere in such a cluster puts the agent a single action away from any other state in the clique  This also means that the costs of the resulting actions are similar  and that the cost of a single action is negligible compared with the cost of a typical path  Finally  any  optimal  path in the original problem can be closely approximated at the abstract level  as an agent following an  optimal  path has to traverse from cluster to cluster  Since all neighboring clusters are connected in the abstract problem  it is always possible to find a path in the abstract problem that is close to the original path  Given the attractive properties or PR LRTS  it is natural to ask whether the ideas underlying it can be extended to stochastic shortest path problems  with arbitrary cost structures  In a stochastic problem  the result of planning is a closed loop policy that assigns actions to states  A successful ab    straction must be suitable for approximating the execution trace of an optimal policy  Imagine that clustering has been done in some way  The idea is again to have abstract actions that connect neighboring clusters cheaply  that is  the system should not produce expensive connections  Intuitively  we want to connect one cluster to another if  from any state of the first cluster  we can reliably get to some state of the second cluster at roughly a fixed cost  the same for any state in the first cluster   This way  simulating a policy of the original problem becomes possible at a small additional cost  the meaning of simulation will become clear later   This means that a connection between clusters is implemented by a policy with a specific set of initial states that brings the agent from any state of the source cluster to some state of the target cluster  We will use options  Sutton  Precup    Singh        for such policies  and choose clusters to allow such policies for any two neighboring clusters  Thus  it is natural to look for clusters of states that allow one to reliably simulate any trajectory from any of the states to any other state  Finally  we need an extra mechanism  the goal approach  that deals with the challenge of reaching the base level goal itself from states that are close to the goal  Thus  our planner first plans in the abstract space to reach the goal cluster  After arriving at some state of the goal approach region  the planner then uses the goal approach policy that  with high probability  moves the agent to the goal state itself  These ideas form the core of our algorithm   The MDP is undiscounted if       An action a  xX A x  is called admissible in state x if a  A x   Definition   A  generic  policy is a mapping that assigns to each history  x    a    c            xt    at    ct    xt   an action admissible in the most recent state xt   In general  a mapping that maps possible histories to some set is called a history dependent mapping  Under mild conditions  it suffices to consider only stationary  deterministic policies  Bertsekas   Tsitsiklis         on which we will focus  Definition   A stationary and deterministic policy  is a mapping of states to actions such that  x   A x  holds for any state x  X  In what follows  we will use policy to mean stationary and deterministic policies  unless otherwise mentioned  The expected cost of policyP when the system starts  in state x  is v  x      E   t    t c Xt    Xt    Xt      where Xt is a Markov chain with P  Xt     y Xt   x    p y x   x    The function v is called the value function underlying policy   One solves an MDP by finding a policy that minimizes the cost from every state  simultaneously  In this paper we deal only with stochastic shortest path problems  a subclass of MDPs  In these MDPs the problem is to get to a goal state with the least cost   The three major contributions of the paper are   i  a novel theoretical analysis of option based abstractions   ii  an effective algorithm for constructing high quality option based abstractions  and  iii  experimental results demonstrating that our algorithm performs effectively over a range of problems of varying size and difficulty   Definition   A finite stochastic shortest path  SSP  problem is a finite undiscounted MDP that has a special state  called the goal state g  such that a  A g   we have p g g  a      and c g  a  g      and the immediate costs for all the other transitions are positive   Section   formally describes our problem  and provides the theoretical underpinning of our approach  Section   then presents our algorithm for automatically building options based abstractions  and Section    our planning algorithm that uses these abstractions  Section   empirically evaluates this approach  in terms of both efficiency and effectiveness  suboptimality   Finally  Section   summarizes related work   Consider a finite SSP  X  A  p  c   Let  be a stationary policy  We say that this policy is proper if it reaches the goal state g with probability one  regardless of the initial state  Let T   RX  RX be the policys evaluation operator  X  T v  x    p y x   x    c x   x   y    v y     yX     Problem Formulation and Theory  This section formally defines stochastic shortest path problems and the abstractions that we will consider  It also presents a theoretical result that characterizes the relationship between the performance of abstract policies and policies of the original problem  Definition   A Markov Decision Process  MDP  is defined by a finite state space X               n   a finite set of actions A x  for each state x  X  transition probabilities p y x  a          that correspond to the probability that the next state is y when action a is applied in state x  immediate cost c x  a  y   R for all x  y  X and all a  A x  and a discount factor            Bertsekas and Tsitsiklis        prove that T is a contraction with respect to a weighted maximum norm  kkw    with some positive weights  w  RX     where kvkw    maxx  v x   w x   In particular  w x  can be chosen to be the expected number of steps until  reaches the goal state when started from x  The contraction coefficient of T      satisfies            maxxX w x   Thus           is the maximum of the expected number of steps to reach the goal state  or in other words  the maximum expected time policy  spends in the MDP  cf  Prop     in Bertsekas   Tsitsiklis         We adopt the notion of options from Sutton et al          Definition   An option is a triple    I     where I  X is the set of initial states   is a  generic  policy that is   defined for histories that start with a state in I and  is a history dependent mapping with range         called the terminating condition  We say that the terminating condition fires when  ht        Let T     denote the random time when the terminating condition fires for the first time while following    Note that T     is not allowed   We assume that P  T           independent of the initial state when the policy  is started  i e   the option terminates in finite time with probability one   As suggested in the introduction  an abstraction is a way to group states and the abstract actions correspond to options  Definition   We say that the MDP  X   A  p  c  is an option based abstraction of  X  A  p  c   if there exists a mapping  S   X   X specifying the states S x   X that correspond to an abstract state x  X   a set  of options abstracting the actions of the MDP and a mapping    xX A x    such that for any a  A x   if  a     I      then S x   I   Henceforth we will use abstraction instead of optionbased abstraction and will call  X   A  p  c  the abstract MDP  X the set of abstract states  A the set of abstract actions  etc  Notationally  we call  X  A  p  c  the ground level MDP  and we will identify quantities related to the abstract MDP by using a tilde        For simplicity  we will identify the abstract actions with their corresponding options  In particular  we will call a both an abstract action and an option  depending on the context  In the following  we will assume that  S x    x  X   is a partition of X  we can then let x   X  X denote the  unique  abstract state that includes x  x x   X such that x  S x x    and say that  X   S  is an aggregation of the states in X  We also define S x    S x x   as the set of states in X that are in the same partition with x  The restriction on  in the above definition ensures that the execution of any policy  in the abstract MDP is well defined and proceeds as follows  Initially  there is no active option  In general  whenever there is no active option  we look up the abstract state x   x x  based on the current state x and activate the option   x    When there is an active option  the option remains active until the corresponding terminating condition fires  When an option is active  the options policy selects the actions in the ground level MDP  This way a policy  in the abstract MDP induces a policy in the ground level MDP  Our goal now is to characterize what makes an abstraction accurate  The following theoretical analysis is novel as it considers abstractions where the action set is changed  In particular  the action set can potentially be reduced and the abstract actions can be options  To our knowledge  such options based abstractions have not been analyzed previously  the closest results are probably Theorem   of Kim and Dean        and Theorem   of Dean  Givan  and Leach         The proof is rather technical and is given   X     denotes the power set of X  the set of all subsets of X   in the extended version of our paper  Isaza  Szepesvari  Bulitko    Greiner         Consider a proper policy  of the ground level MDP  We want abstractions such that one can always find a policy in the abstract MDP  X   A  p  c  that approximates  well  no matter how  was chosen  Clearly  this depends on how the action set A and the corresponding transitions and costs are defined in the abstract MDP  Quantifying this requires a few definitions  Let p  x  y  be the probability of landing in some state of S y  when following policy  until it leaves the states of S x   when the initial state is selected at random from the states of S x  based on the distribution S x    Let c  x  denote the corresponding expected immediate cost  Now pick a proper policy  of the abstract MDP  Let w be the weight vector that makes T a contraction in the abstract MDP  P Further  define p  x  y    p y x   x   and c  x    yX p  x  y c  x  y  and the mixed l   l norm kkw      kp   p  kw      max xX  X  yX   p   x  y   p   x  y    w y    w x   Let     kc  c kw    cmax kp  p kw            where cmax is the maximum of the immediate costs in the ground level MDP  Hence    measures how well the costs and the transition probabilities induced by  after state aggregation match those of   Introduce c x    as the expected total cost incurred  conditioned on that policy  starting in state x and stopping when it exits S x   Further  introduce p y x    as the probability that  given that policy  is started in state x  when it exits S x  it enters S y   y    x x    Now fix an abstract state x  X   If the costs  c x    xS x  and probabilities  p y x    xS x    y    x  have a small range then we can model closely the behavior of  locally at S x  by introducing an option with initial states in S x  which mimics the expected behavior of  as it leaves S x   assuming  say  that the initial state in S x  is selected at random according to the distribution S x    If we do so for all abstract states x  X then we can make sure that min   is small  If the above range conditions hold for all policies  of the ground level MDP and all abstract states x  X then by introducing a sufficiently large number of abstract actions it is possible to keep max min   small  Further  notice that max p y x    is zero unless there exists a transition from some state of S x  to some state of S y   in which case we say that S x  is connected to S y   Hence  no abstract action is needed between x and y  unless S x  is connected in the ground level MDP to S y   Define TP   B X   B X    T v  x     c  x    p Since  is proper in the y   x  y v y   ground level MDP  it is not difficult to show that T is a contraction with respect to an appropriately defined weighted supremum norm  The next result gives a bound on the difference of value functions of  and  in terms of       Theorem   Let  be a proper policy in the ground level MDP and let  be a proper policy in the abstract MDP  Let w  resp   w   be the weight vector that makes T  resp   T   a contraction and let the corresponding contraction factor be   resp       Let v be the value function of  and v be the value function of   Then kv  Ev kw    kAv  v kw                    where the operator E extends functions defined over X to functions defined over X in a piecewise constant manner  E   B X    B X    Ev  x    v x x    and A   B X   B X  is the aggregation operator defined by X  AV   x    S x   z V  z   zS x   and    maxxX w  x x   w  x   The factor  measures how many more steps are needed to reach the goal if the execution of policy  is modified such that  whenever the policy enters a new cluster x  the state gets perturbed  by choosing a random state according to S x    The theorem provides a bound on the difference between the value function of a ground level policy  and the value function of an abstract policy when its value function is extended to the ground level states  The bound has two terms  The first bounds the loss due to state abstraction  while the second bounds the loss due to action abstraction  When a similar range condition holds for the abstract actions  too  then it is possible to bound the difference between the value function of the policy induced in the ground level MDP by  and Ev   yielding a difference on the value functions of  and the policy induced by   Isaza et al         provides further details  If we apply this result to an optimal policy   of the ground level MDP  we immediately get a bound on the quality of the abstraction  We may conclude then that the quality of abstraction is determined by the following factors   i  whether states with different optimal values are aggregated   ii  whether the random perturbation described in the previous paragraph can increase the number of steps to the goal substantially  and  iii  whether the immediate costs c and transition probabilities p can be matched in the abstract MDP  Since we want to build abstractions that work independently of where the goal is placed  the knowledge of the optimal policy with respect to a particular goal cannot be exploited when constructing the abstractions  In order to prevent large errors due to  i  and  ii   we restrict aggregation such that only a few states are grouped together  This makes the job of creating an aggregation easier  Fortunately  we can achieve higher compression by adding additional layers of abstractions  We can address  iii  by creating a sufficiently large number of abstract actions  Here  we use the simplifying assumption that we only create abstract actions that bring the agent from some cluster of states to some neighboring cluster  These can  serve as a basis for matching any complex next state distribution over the clusters by choosing an appropriate stochastic policy in the abstract MDP  We also want to ensure that the initial state within a cluster has a small influence on the probability of transitioning to some neighboring cluster and the associated costs  We use two constants   and   to bound the amount of variation with respect to initial states  note this allows us to control the difference between the value function of a policy induced in the ground level MDP by some abstract policy  and the extension of the value function of  defined in the abstract MDP to the ground level states  Ev   This is necessary to ensure that a good policy in the abstract MDP produces a good policy in the ground level MDP  ultimately assuring that the optimal policy of the abstract MDP will give rise to a close to optimal policy in the ground level MDP  The resulting procedure is described in the next section      Abstracting an SSP  This section describes our algorithm BuildAbstraction for automatically building options based abstractions  These abstractions are goal independent and thus apply to a series of SSPs that share the state space and transition dynamics  The process consists of four main steps  Figure         Cluster proposes candidates for abstract states      GenerateLinkCandidates proposes candidates for abstract actions  or links       Repair validates and  if necessary  repairs the links in order to satisfy the so called      connectivity property  the formal definition is given later  and Prune discards excessive links  Once an abstraction is built  we use a special purpose planning procedure  described in Section    to solve specific SSPs  The rest of this section describes the four steps of our BuildAbstraction algorithm in detail  Step    Cluster  A straightforward cluster er will cluster a state with some of its immediate neighbors  Unfortunately  this approach may group states with diverging trajectories  the trajectories from one state can differ from those of the other state   By looking for the peers of a state  predecessors of its successors  line    Figure    we hope to find a peer whose trajectories are similar to the trajectories of the first state  Note that the clustering routine creates minimal clusters  This is advantageous as it means the subsequent steps  which connect clusters  is more likely to succeed  Unfortunately  it also means relatively low reduction in the number of states  Several layers of abstractions can help increase this reduction  Step    Generate Link Candidates  After forming the initial clusters  i e   the initial abstract states   BuildAbstraction generates candidates for abstract actions  One approach is simply to propose abstract actions for all pairs of abstract states  in the hope that only important ones will remain after pruning  We use a less expensive strategy and propose abstract action candidates only for nearby clusters  line     For each such pair we add two candidate links  one in the forward and another in the backward direction  this heuristic quickly generates reasonable link candidates  We typically use k      Our experiments confirm this is   BuildAbstraction k  p  M      M  ground level MDP Cluster   for each unmarked ground state x do   Find P  x   all the predecessors of successors of x   Find y  P  x  that has the most successors in common with x   Add x to X with S x     x  y    Mark states  x  y    end for GenerateLinkCandidates   repairQ     for every x  y  X  where any state in S y  is within k ground transitions of some state in S x  do   repairQ  repairQ    x  y    y  x      end for Repair    while repairQ     do     x  y   pop an element from repairQ    set up an SSP  S  with domain R  X where S x S y   R with states in S y  as goals    attempt to find an optimal policy S in S with IPS    if no policy found then    continue    else if S does not meet the      conditions then    split the cluster adding both parts to repairQ    else    add a to A x  with  a     S x   S   IS y         set c x  a  to be the expected cost of executing a from a random state of x    set p y x  a       p y   x  a      for y     y     end if    end while Prune    for each state x do    find A  x     a            am    all abstract actions that connect clusters that are neighbors in M    order A x    A  x  to create  am             an   such that c x  ai    c x  ai      i   m              n       let A x     a            ap      end for    return  X  A  p  c    Figure    The abstraction algorithm  sufficient  increasing k results in slightly better quality  but slower running times when solving the planning problems  Step    Repair  For each candidate abstract action connecting abstract states x and y  we first need to derive an option that  starting in any state in cluster x leads the agent to some state in cluster y with a minimum total expected cost  We derive this option by setting up a shortest path problem S  whose domain includes S x  and S y   We set the domain of S to be sufficiently large that a policy within this domain can reliably take the agent from any state of S x  to some state of S y   BuildAbstraction builds this domain by performing a breadth first search from S y   proceeding backwards along the transitions  stopping at depth D   m  where D is the search depth from S y  and m is the margin to leave after all states of S x  were added to the domain  If there is any state of S x  that was not included at depth D  the Repair routine reports no solution  The transitions  actions and costs of S are inherited from the MDP M   We also add a new terminating state  which is the destination   Here IS is the characteristic function of S  IS  x      iff x  S and IS  x      otherwise   of transitions leaving the region  i e   those transitions are redirected to this new terminal  with a transition cost that exceeds the maximum of the total expected costs of the ground level MDP  The high cost discourages the solutions to enter the extra terminating state  The optimal solution to S is obtained by using the Improved Prioritized Sweeping  IPS  algorithm of McMahan and Gordon          line      We selected this algorithm based on its excellent performance and known optimality properties  IPS reduces to Dijkstras method in deterministic problems   The resulting policy  is checked against      connectivity  defined as follows  we first compute the expected total cost of reaching some states in S y  for all states of S x   let the resulting costs be c x     Similarly  we compute the probabilities p S y  x    for every x  S x   Then we check if maxx x S x   c x     c x        and maxx x S x   p S y  x     p S y  x        both hold  If these constraints are met  a new abstract action is created and is added to the set of admissible actions at x and the policy is stored as the option corresponding to this new abstract action  lines        Otherwise  the cluster is split  since every cluster has two states  this is trivial  and the appropriate link candidates are added to the repair queue so that no link between potentially connected clusters is missed  Step    Prune  After step    we have an abstract SSP whose abstract states are      connected  However  our abstract action generation mechanism may produce too many actions  which may slow down the planning algorithm  see Section     We address this problem using a pruning step that leaves only the critical and cheapest abstract actions  An action is critical if it connects clusters that are connected at the ground level with a single transition  these actions are important to keep the structure of the ground level MDP  We also keep the cheapest abstract actions as they are likely to help achieve high quality solutions  The pruning parameter  p  specifies the total number of actions to keep   If p is smaller or equal than the number of ground actions  then only the critical actions are kept   BuildAbstraction runs in time linear in the size of the input MDP  as every step is restricted to some fixed size neighborhood of some state  i e   every step is local   Further  employing a suitable data structure  the memory requirements can also be kept linear in the size of the input  These properties are important when scaling up to realistic  real world problem sizes      Planning with an Abstraction  After building an abstraction  we can use it to solve particular SSP problems  When we specify a new goal  our abstraction planner  AbsPlanner  then creates a goal approach region in the abstract MDP that includes the goal and is large enough to include all states of the cluster containing the goal  AbsPlanner builds this region by starting with the ground goal and adding states and transitions in a breadth first fashion to a certain depth  proceeding backwards along the transitions  stopping only after adding all states of the goal cluster  After building the region  AbsPlanner produces an SSP  The domain of this   SSP includes the states found in the breadth first search  and also a new terminal state that becomes the destination of transitions leaving the region  i e   those transitions are redirected to this new terminal  with a high transition cost  All other costs and transitions of this SSP are inherited from the ground level MDP  AbsPlanner uses IPS to solve the local MDP  and saves the resulting goal approach policy  It then solves the abstract MDP  where the goal cluster is set as the goal  When executing the resulting policy   AbsPlanner proceeds normally until reaching a state of the goal approach region  it then switches to the goal approach policy  which it follows until reaching the goal or leaving the region  When this latter event happens and the state is x  execution switches to the option  x x    When using multiple levels of abstraction  AbsPlanners execution follows a recursive  hierarchical strategy  Note that the size of the goal approach region is independent of the size of the MDP  Thus  the planning time will depend on the size of the top level abstract MDP  For an MDP of size n  by using log n levels of hierarchy  in theory it is then possible to achieve planning times that scale with O log n   However  depending on the problem  it might be hard to guarantee high quality solutions when using many levels of abstraction  Furthermore  in practice  over the problems used in our tests   the computation time is dominated by the time needed to set up and solve the goal approach SSPs  which is required for even one layer of abstraction  This is partly because our abstractions result in deterministic shortest path problems  whose solutions can be found significantly faster than those of stochastic problems      Empirical Evaluation  This section summarizes our empirical evaluation of this approach  in terms of the quality  suboptimality  of the solutions and the solution times  Here we report the tradeoffs of using different levels of abstraction as well as the dependence on the stochasticity of the transitions   Note that stochasticity makes it difficult to build abstractions   We also tested the performance of the algorithm on more practical problems  In addition to the results presented here  we conducted extensive experiments  studying the trade off between solution quality and solution time as a function of the various parameters of our algorithm  e g   the values of p  k  or the number of abstraction levels   the scaling behavior of our algorithm in terms of its resource usage  the quality of solutions and the solution time  These results  appearing in  Isaza et al          confirm that the algorithm is robust to the choices of its parameters and scales as expected by increasing problem sizes  We run experiments over three domains  noisy gridworlds  a river and congested game maps  The gridworlds are empty and have four actions  up  down  left  and right  each with cost    The probability that an action leaded to the expected position  e g   the action up moves the agent up one cell  is      while the probability of reaching any of the other three adjacent cells is      The river is similar to the gridworld  its dimensions are w  h  but there is a current flowing from left to right  and a fork corresponding to a line connecting the points  w    h    and  w  h      The flow is represented by modifying both the cost structure and the transition probabilities of the actions  action forward costs    backward costs    diagonally up and forward and diagonally down and forward each cost    These actions are also stochastic  For the backward action  the probabilities are     for going back and     for each of the other actions  For the other three actions  the anticipated move occurs with probability     and the other moves except backwards occur each with probability      and backwards has probability    We include the river domain to determine whether our system can deal with non uniform structures and because the fork complicates the task of creating abstractions  We empirically found the time to build abstractions for the n state gridworld was close to n     seconds  and around n    for an n state river domain  The build time for the maps  using k      was between    and     seconds   The congested game maps are again similar to gridworlds  but with obstacles and with transitions probabilities that depend on the congestion  The obstacle layout comes from commercial game maps  and the stochastic dynamics simulate what happens if multiple units traverse the same map  in narrow passages  the units to become congested  which means an agent trying to traverse such a passage is likely to be blocked  We model this by modifying each action by including a probability that the action will fail and cause the agent to stay at the same position  This failure probability depends on the position on the game map  calculated by simulating many units randomly traversing the game maps and measuring the average occupation of the individual cells  then turning the occupation numbers into probabilities  The optimal policy of an agent in a congested game map will then try to avoid narrow passages  since the higher probability of traffic congestion in such regions means an agent takes much longer to get through those regions  The baseline performance measures are obtained by running the state of the art SSP solver algorithm IPS  For each study  we generate the abstraction and then use it to solve       problems  whose start and goal locations are selected uniformly at random  For each problem we measure the solution time in seconds and the total solution cost for both IPS and our method  then compute the geometric average of the individual suboptimalities and the individual solution time ratios       Abstraction level trade offs  We used a          gridworld to analyze the trade offs of different abstraction levels  with several different parameter configurations  We say a configuration is dominant if it was a Pareto optimal  i e   if no other configuration is better in both time and suboptimality  Figure   presents properties of the dominant configurations   See  Isaza et al         for more details  including relevant pictures    We ran all experiments on a  GHz AMD Opteron tm  Processor with  GB of RAM running Linux with kernel                                                                                                                                                Solution time ratio  Figure    Subobtimality versus the solution time ratio as compared to IPS for different parameter configurations  The dominant configurations are shown for different levels of abstraction  for various abstraction levels  We see that using a smaller number of abstractions required more time but produced better solutions  i e   lower suboptimality   and higher levels of abstractions required less solution time but produced inferior solutions  i e   increased suboptimality   Note that there are dominant configurations for every level of abstraction  from   to    We obtain a level   abstraction by converting the given ground level SSP to deterministic shortest path problem with the same states   Recall that our abstraction process abstracts the state space and produces a deterministic SSP  here we just used the original state space   Figure   shows that this transformation provides solutions whose quality is slightly inferior to the original problem  but it finds this solution significantly faster  e g   in       to        of the time   We also see that these level   solutions are superior to those based on higher abstraction levels  but one can obtain these level i solutions in yet less time                 Suboptimality  level   level   level   level   level   level    Suboptimality vs  speed up on a   x   gridworld             Figure   plots the suboptimality and the speed up of finding a solution using our method  as compared to IPS  for different values of P   We see that our method loses optimality as the dynamics becomes noisier  i e   when P gets smaller   This is because our abstract actions  trying to move the agent from one abstract state to the next will fail with higher probability for noisier dynamics  Note        that the advantage of our method  in terms of planning time  becomes larger with increased stochasticity  This is because our abstractions are deterministic and planning in a deterministic system is much faster than planning with a stochastic system  Figure    plotting the absolute values of cost and time for both our method and IPS  provides another insight  It shows that for increasing stochasticity both methods are slowed down  but our method can cope better with this situation  This figure also confirms that this leads to a loss in solution quality  For our method the typical parameters produce a suboptimality of around     for the river  and around      for the gridworld domain  The speed up for the gridworld is around     while for the river it is around       Sensitivity to Stochasticity of the Dynamics  As the environment becomes noisier  it becomes more difficult to construct a high quality abstraction  This section quantifies how the solution quality and construction time relate to noise in the dynamics  In general  we consider an action successful if the agent moves to the appropriate direction  our gridworld model set the success probability to P        leaving a probability of     P     to moving in each of the other three directions  Here  we vary the value of P   All of these experiments use a        gridworld with k     and p      which means we keep only the critical actions  see Section                Solution time ratio  Figure    Subobtimality versus the solution time ratio as compared to IPS for different values of P    Cost  Suboptimality  Suboptimality vs  Speed up on a    x    gridworld                                                        Cost vs  solution time trade off in a   x   gridworld          IPS     Abstraction                                                                                                                Solution time  s   Figure    Cost versus solution time for IPS and abstraction at different values of P        Congested Game Maps  To test the performance of our approach in a more practical application  we used maps modeled after game environments from commercial video games  We first created simplified gridworlds that resemble some levels from a   Figure    A congested game map  Darker redder color refers to high congestion  Dark blue regions are impassable obstacles  popular role playing and real time strategy game  We then converted the gridworlds into congested maps as described earlier  This produced maps with state space sizes of       BG          BG          BG           WC   and       WC    Figure   provides one such map  where each states color indicates the associated congestion  warmer redder colors indicates high congestion  i e   low probability of success P   while colder bluer colors indicates low congestion  i e   high value of P    Very dark blue indicates impassable obstacles  We see that many of the states in cluttered regions are highly congested and should therefore be avoided  Figure   shows the solution time and the solution suboptimalities for both our method and IPS  for two maps from WarCraft  Blizzard Entertainment        and three maps from Baldurs Gate  BioWare Corp          including Figure    using only a single layer of abstraction  We see that our approach is indeed successful in speeding up the planning process  while keeping the quality of the resulting solutions high      Related Work  Due to space constraints we review only the most relevant work  references to other related works can be found in the extensive bibliography lists of the cited works  Dean et al         introduced the notion of  homogeneous partitions and analyzed its properties  but without giving explicit loss bounds  Kim and Dean        developed some loss bounds  Their Theorem   can be strengthened with our proof method to kv   vP k  kT vP  vP k         using our notation   basically dropping the first term in their bound  Here v  is the optimal value function in the original MDP  vP is the optimal value function of the aggregated MDP extended back to the original state space in a piecewise constant manner and T is the Bellman optimality operator in the original MDP  This bound is problematic as it does not show how the quality of  partitions influences the loss  Our bound improves on this bound in this respect  and also by extending it to the case when the abstract actions correspond to options  While Asadi and Huber        also considered such optionsbased abstractions  they assume that the abstract actions  options  are given externally  possibly by specifying goal states for each of them  and they do not develop bounds  In a number of subsequent papers  the authors refined their methods  In particular  they became increasingly focussed on learning problems  For example  in the recent follow up work  Asadi and Huber        provide a method to learn an abstract hierarchical representation that uses state aggregation and options  Since they are interested in skill transfer through a series of related problems that can differ in their cost structure  they introduce a heuristic to discover subgoals based on bottleneck states  They learn options for achieving the discovered subgoals and introduce a partitioning that respects the learned options  in the clusters typically there are many states   The success of the approach relies critically on the existence of meaningful bottleneck states  This leads to a granularity issue  identifying the bottleneck states requires computing a statistic for each state visited  meaning bottlenecks will not be pronounced if resolution is increased in narrow pathways  Nevertheless  the approach has been successfully tested in a non trivial domain of        states  Hauskrecht  Meuleau  Kaelbling  Dean  and Boutilier        introduce a method that also uses options  but the abstract states correspond to boundary states of regions  The regions are assumed to be given a priori  The idea is similar to using bottleneck states  In contrast to that work  we do not assume any prior knowledge  but construct the abstractions completely autonomously  Further  we deal with undiscounted SSPs  while Hauskrecht et al         dealt with discounted MDPs  but this difference is probably not crucial       Discussion and Future Directions  In the approach presented  options serve as closed loop abstract actions  Another way to use an abstract solution would be to use the abstract value function to guide local search initiated from the current state  These ideas has proven successful in pattern database research where the cost of an optimal solution of an abstract problem is used as a powerful heuristic for the original problem  Such a procedure has the potential to improve solution quality  while keeping low the cost of the planning steps interleaved with execution  Another idea is to use the abstraction to select the amount of such local search  i e   the depth of the rollouts   these ideas has proven successful in deterministic environments  Bulitko  Bjornsson  Lustrek  Schaeffer    Sigmundarson        Bulitko  Lustrek  Schaeffer  Bjornsson    Sigmundarson         Presently  our abstractions are deterministic  This suggests two avenues for future work  First  applying advanced heuristic search methods to such abstractions may lead to performance gains  Second  in highly stochastic domains  the abstractions determinism may lead to a poor quality of solution  as the cost of ensuring arrival at an abstract   Solution time for different game maps  Solution suboptimality for different game maps       IPS Abstraction       Suboptimality  Solution time  s                                       WC    WC    BG    BG    BG    WC    Map  WC    BG    BG    BG    Map  Figure    Solution times  left  and suboptimalities  right  for several game maps  state with certainty  or very high probability  can lead to very conservative and costly paths  Thus  it would be of interest to investigate stochastic abstractions  One idea is to modify the way abstract actions are defined  When planning to connect to abstract states after a solution of the local SSP is found  with a little extra work we can compute the probabilities of reaching various neighboring abstract states under the policy found when the policy leaves the region of interest  Yet another avenue for future work would be to move from a state based problem formulation to a feature based one  assuming that the features describe the states  The challenge is to design an algorithm that can construct an abstraction without enumerating all the states  as ours currently does  Although this paper has not attempted to address this problem  we believe that the approach proposed here  i e   incremental clustering and defining options by solving local planning problems  is applicable  Finally  although the present paper dealt only with undiscounted  stochastic shortest path problems  the approach can be extended to work for discounted problems  This holds because a discounted problem can always be viewed as an undiscounted stochastic shortest path problem where every time step a transition is made to some terminal state with probability      where          is the discount factor      Conclusions  This paper has explored ways to speed up planning in SSP problems via goal independent state and action abstraction  We strengthen existing theoretical results  then provide an algorithm for building abstraction hierarchies automatically  Finally  we empirically demonstrate the advantages of this approach by showing that it works effectively on SSPs of varying size and difficulty   Acknowledgements We gratefully acknowledge the insightful comments by the reviewers  This research was funded in part by the National Science and Engineering Research Council  NSERC   iCore and the Alberta Ingenuity Fund   
