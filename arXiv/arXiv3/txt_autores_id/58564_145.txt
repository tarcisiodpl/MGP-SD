 A Bayesian belief network models a joint distribution with an directed acyclic graph representing dependencies among variables and network parameters characterizing conditional distributions  The parameters are viewed as random variables to quantify uncertainty about their values  Belief nets are used to compute responses to queries  i e   conditional probabilities of interest  A query is a function of the parameters  hence a random variable  Van Allen et al               showed how to quantify uncertainty about a query via a delta method approximation of its variance  We develop more accurate approximations for both query mean and variance  The key idea is to extend the query mean approximation to a doubled network involving two independent replicates  Our method assumes complete data and can be applied to discrete  continuous  and hybrid networks  provided discrete variables have only discrete parents   We analyze several improvements  and provide empirical studies to demonstrate their effectiveness      INTRODUCTION  Consider a simple example  Suppose A represents presence absence of a medical condition while B and Y are test results  Variables B and Y are conditionally independent given A  with A and B binary and Y continuous  The conditional independence assumption is represented by the directed acyclic graph structure in Figure   a   Let a   P  A   a   b a   P  B   b   A   a   and let p y   a   a   be the conditional density of Y given A   a  assumed normal with mean a and variance a    We want to estimate the probability that condition A is present given  specified results from the two tests B and Y   Let  represent all of the parameters  If  were known  we would use the formula  a b a p y   a   a     a  a  b a  p y   a    a     q     qa b y      P       In the Bayesian paradigm  uncertainty about  is quantified by modeling parameters as random variables  It follows that query probabilities such as     are also random  A query response is usually estimated by approximating its posterior mean  This approximation is similar to expression      but with a and b a replaced by their posterior means and with the normal densities replaced by Students t densities  One may want more than just a point estimate  Van Allen et al               showed  for discrete networks  how one can approximate the variance and posterior distribution of a query  Their variance derivation employs the delta method  i e   a first order Taylor series expansion of the function q   about the posterior mean of   They provide asymptotic theory and empirical experiments supporting this approach  They also showed how these approximations can be used to construct a Bayesian credible interval  error bars  for q    Guo and Greiner        applied this delta method approximation as part of a mean squared error  i e   squared bias   variance  measure designed to estimate the quality of different belief net structures when seeking a best classifier  Lee et al         provide a technique for combining independent belief net classifiers that involves weighting their respective mean probability values by their inverse variances  and they show that this works well in practice  We propose new approximations for the mean and variance based on a simple trick  Suppose  A    B    Y    and  A    B    Y    are replicates of the network variables  conditionally independent given   We represent the paired replicates as nodes in a doubled network with the same structure  see Figure    The squared query q    can be expressed as a query in this doubled net    UAI       b   a  b   a   b   a  b   a   HOOPER ET AL     a  a  A                                            b  a b  a    b   a  b   a   a    a     Y B       a    a       b   a  b   a  b   a  b   a        a  a   b   a  b   a  b   a  b   a  b   a  b   a  b   a  b   a   b   a  b   a  b   a  b   a  b   a  b   a  b   a  b   a   a  a   b   a  b   a  b   a  b   a  b   a  b   a  b   a  b   a   a  a   a  a       A    A                             a    a     a    a                a    a     a    a     Y    Y  B    B   a          a                   a         a           a a a a  Figure     a  A simple Bayesian network   b  The corresponding doubled network  Figure    A simple Bayesian net   work  P  A    A    a   B    B    b  Y    Y    y      The method used to approximate the mean of q   can be extended to the doubled network to approximate the mean of q    and hence to approximate the variance  Unlike the delta method  our approach does not rely on approximate local linearity of q    It does involve the addition of two incomplete observations to the data set when calculating the posterior mean of q      In some situations  this addition results in under estimation of the desired variance  This deficiency is largely eliminated by a simple adjustment  A similar adjustment substantially improves the usual query mean approximation  Section   reviews pertinent models and methods for belief networks  The network doubling technique is described in Section   for discrete  continuous  and hybrid networks  Proposed adjustments and numerical results are presented in Sections   and   for discrete networks  Corresponding work for continuous and hybrid networks is ongoing  Computational issues are discussed in Section    Contributions and plans for further work are summarized in Section            BACKGROUND NETWORK VARIABLES  We assume network structure is known  Let B denote a discrete network variable taking values b  DomB   Let Y denote a continuous network variable taking values y on the real line  Vectors of variables are denoted by boldface  A for discrete and X for continuous  Let  be a random vector comprising all unknown network parameters  i e    determines all conditional distributions of variables given their parents  We assume that discrete variables have only discrete parents  Suppose pa B    A  i e   the parents of B are the variables comprising the vector A  The conditional probability that B   b given A   a is denoted b a   B b A a   P  B   b   A   a         Variables associated with values will be clear from context  We employ similar abbreviations for other parameters and hyperparameters  The b a parameters are often presented in conditional probability tables  CPtables  with rows indexed by a and columns by b  e g   see Figure    Note that we use superscripts b    b  to list the distinct values in DomB   We use subscripts b    b  to denote arbitrary values in DomB   often related to replicated variables B    B    Continuous variables can have both discrete and continuous parents  Suppose pa Y     hA  Xi with X   hX            Xd i  The conditional distribution of Y is    Y   A   a  X   x     N     xT   a   a        i e   normally distributed  conditional mean related to x by a linear regression model with coefficients depending on a  Here xT is the transpose of the ddimensional column vector x while  a is an  d     dimensional column vector of regression coefficients  the first entry is the constant term        PRIOR AND POSTERIOR  The network parameters represented by  consist of CPtable parameters b a   regression coefficient vectors  a   and variances a    We assume the prior distribution for  has the following form  e g   see Gelman et al           CPtable rows follow Dirichlet distributions  B a    hb a   b  DomB i  Dir B a    where B a    hb a   b  DomB i    The regression coefficients and variance together have a normal  inverse chi square  distribution      a   a     Nd   a   a   a a      a     a  a     a    I e   dropping subscripts for a moment   conditioned on    is multivariate normal with mean        HOOPER ET AL  vector  and covariance matrix          and        has a   distribution with       not necessarily an integer   Note that        has mean   and variance       Parameters are assumed to be statistically independent except where joint distributions are specified above  In particular  we assume global independence  the parameters determining the conditional distribution of one variable given its parents are independent of all other parameters  The prior is conjugate  given a data set D consisting of n independent replicates of complete tuples of network variables  the prior hyperparameter values are updated as follows  Let nab and na be the number of tuples in D with  A  B     a  b  and A   a  respectively  Let  xi   yi   be the observations of  X  Y   for the na tuples with A   a  Let X a be the na   d      matrix with rows     xTi    Let y a be the column vector with entries yi   In the five equations below  the prior hyperparameter values appear on the right hand side and are identified with tildes  e g      b a   b a   nab a   a   na a a   a a   X Ta X a a a a   a a a   X Ta y a i h       a a   Ta a a   a a    Ta a a   y Ta y a P P The values a b b a and a a are called the effective sample sizes for variables B and Y   respectively  Our adjustments developed in Section   are motivated by large m asymptotics  where m is proportional to the effective sample size for each of the variables  i e     b a   mb a and a   ma    with  b a   a    a   a   a    fixed        Large m asymptotics are similar to but not the same as large n asymptotics  As the sample size n increases  the posterior mean E b a   D    b a   a varies and converges to some value   Here and elsewhere  the dot P subscript indicates summation   a   b b a    Under assumption      the posterior mean remains fixed as m varies       APPROXIMATING A QUERY MEAN  Consider a query involving outcomes of hypothesis variables H given values for evidence variables E  It is convenient to represent the query in terms of a function w H   E g   suppose H   A  E    B  Y    e    b  y   and q      P  A   a   B   b  Y   y      E w A    B   b  Y   y       UAI       where w A      for A   a and w A      otherwise  For discrete networks  query responses q   are usually estimated by q    where     E    D  is the posterior mean of the parameter vector  This plugin estimate usually differs slightly from the posterior query mean E q     D   Cooper and Herskovits        expression     showed that the plug in estimate equals E q     D  e   i e   the posterior query mean given an augmented data set consisting of D and an additional partial observation of the evidence variables E   e  Cooper and Herskovits        derived a formula for E q     D  e  that is valid for discrete  continuous  and hybrid networks  This formula provides a useful approximation of the less tractable E q    D   The plug in estimate is a special case of this formula for discrete networks  The formula is important for our network doubling technique  so is reviewed here  In the integral expression below  Z represents all variables not included in  H  E   dh and dz refer to product measures allowing both integration for continuous variables  Lebesgue measure  and summation for discrete variables  counting measure   Some manipulation yields E q     D  e    E w H    E   e  D    E   E w H    E   e      D       RR R w h  p h  e  z    p    D ddhdz RRR     p h  e  z    p    D ddhdz Now p h  e  z     factors as a product of conditional probabilities and densities  one for each variable in the network  Due to global independence  the inteR gral p h  e  z    p    D d factors into a product of integrals  one for each variable  The result is a product of probabilities and densities described in Section     below  It follows that E q    D  e  can be calculated in essentially the same manner as the function q    but with two modifications   For discrete variables  parameters b a are replaced by their posterior means  If all network variables are discrete  then we have the plug in estimate  E q     D  e    q E    D           For continuous variables  the normal densities are replaced by the St            densities described below  Note that this is not the same as replacing  and    parameters with their posterior means       PREDICTIVE DISTRIBUTIONS  The predictive distribution of the network variables is obtained by integrating out their joint conditional dis    UAI       HOOPER ET AL   tribution given  with respect to the posterior distribution of   Global independence allows this integration to be carried out separately for each conditional distribution of a variable given its parents  The predictive distribution for a discrete variable B is b a    P  B   b   A   a  D    E b a   D     b a    a  The predictive distribution for a continuous variable is a location scale version of the Students t distribution with  degrees of freedom  We need the multivariate form of this distribution in Section    so we define it here  Suppose T      U      Z     where Z and U are independent  Z  Np       U          and  is a nonsingular covariance matrix  It follows that T has the following density function  Johnson and Kotz        page          p               p                 p             t   T    t     We refer to this as the Stp        distribution  For p      we write St             Note that St           is Students t distribution  We claim that  Y   A   a  X   x  D   St            with    a          xT  a   and        a      xT   a a        xT  T           To see this  let us suppress subscripts for a moment  Let Z   N        be independent  of       Put Z               Nm            We then have  Y   a  x  D       xT     Z                 xT  Z     Z        NETWORK DOUBLING  In Section     we noted that E q     D  is usually approximated by the more tractable E q     D  e   Here we propose approximating Var q     D  by Var q     D  e  e   i e   the posterior variance given D and additional replicates E   and E   of the vector of evidence variables  both having the same value e  We develop a formula for this latter variance by imagining a doubled network  see Figure   b   These mean and variance approximations can be improved by adjustments described in Section    Consider two replicated tuples of network variables  conditionally independent and identically distributed given   Use these to replace each variable in the       original network by a pair of variables  e g   B is replaced by B      B    B    with possible values b    b    b     DomB    DomB  DomB   If pa B    A  then pa B      A     A    A     Conditional distributions of doubled variables given parents are obtained by multiplying probabilities or densities for single variables  For discrete variables  we have P  B    b   A   a       b   a  b   a    E g   if A   A  DomA    a    a     and DomB    b    b     then the CPtable for B  is the      array shown in Figure   b   More generally  if a CPtable in the original network involves dr  dc parameters  then corresponding table in the doubled network has d r  d c entries  Note that CPtable rows in the doubled network are not independent  local independence does not hold  and do not have Dirichlet distributions  Fortunately  these properties are not needed for the factorization described following      For continuous variables  the conditional density of Y     Y    Y    given  A   a   X    x     is the product of the densities for two normal distributions of the form     with subscript i        on a and x  Put H     H     H      w  H      w H    w H      E     E     E      and e    e  e   Some manipulation using conditional independence yields q      E w  H      E    e       q     E w H      E    e       We thus have Var q     D  e  e            E q     D  e  e    E q     D  e  e     E w  H      e   D    E w H      e   D      The doubled network satisfies global independence assumptions  so we can follow the approach of Section     to evaluate the two expected values in      To accomplish this task  we need bivariate predictive distributions for the doubled network  For discrete variables  the calculation follows from the means and covariances of a Dirichlet distribution  Let b  b  be the Kronecker delta function  We have b  a     P  B    b   A   a   D    E b   a  b   a    D     b   a  b   a    a  a   b   a   b  b   b   a       a       If all network variables are discrete  then we have an identity corresponding to      Let  be the vector        HOOPER ET AL   of all CPtable entries in the doubled network  e g   b   a  b   a  appears in row a and column b for the CPtable of B    We then have E q        D  e  e    q   E    D         with the entries in E    D  given by the b  a values above  The two expected values in the variance approximation     are calculated by applying     twice  with q        q    and with q        q    For continuous variables  we need the density for   Y    Y      a    a    x    x    D   There are two cases to consider   If a     a    then the parameters   a    a      and   a    a      are mutually independent  Consequently  the joint distribution factors as a product of two St            densities  see expression       If a    a      a  say   then the joint distribution is St         with    a      X   a   and o n    a  X    a a    X T    I     where X   is the         d  matrix whose rows are each     xTi   and I   is the      identity matrix  The derivation is similar to that following      Note that   a   a    is the same for both Y  and Y  in this case      UAI       Table    Summary of approximations for q and qq   Means q    E q     D  e  q    E q     D  e  e  q    q    q   q    q    q   qr  r   verify that the distribution of m Q  q   R  r   converges to bivariate normal by modifying the proof of Theorem   in Van Allen et al          Asymptotic normality implies that   qqrr   qr  qq rr    at rate m     T  v    g Cg    E R  r   Q    Q  q    We use approximations for higher moments motivated by large m asymptotics  i e   a sequence of posterior distributions of the form     with m    One may   qr qq      q     q     q     qq        Switching the roles of Q and R gives qrr        For conciseness we suppress D in our expressions  i e   we implicitly assume that expectations are conditioned on D  Put Q   q     P  H   h   E   e    and R   P  E   e      Note that R is an unconditional query  with hypothesis E   e and no evidence variables  Let q   r   qq   rr   and qr denote the means  variances  and covariance for  Q  R   We extend this notation to higher moments  e g   qqr   E  Q  q     R  r      qr qq  and hence qqr  qqq qr  qq   Now qqq     for normal distributions  however  Van Allen et al         argue that query distributions are usually better approximated by beta distributions  Substituting the third moment of a beta distribution for qqq   we obtain qqr   where g is the gradient vector of q   and C is the covariance matrix of   both evaluated at E    D   The second variance approximation v  is the doubling method introduced in Section    The simple adjustments  q    v    and more complex adjustments  q    v    are developed in this section         while qrr and qqr converge to zero at rate m    We considered approximating qqr and qrr by zero but found that more accurate approximations give better results  Asymptotic bivariate normality suggests  ADJUSTMENTS  We now narrow our focus to discrete networks and consider the four mean and variance approximations in Table    The delta method approximation is  Variances v    delta method     v    Var q     D  e  e  v    expression      v    expression        qr rr      r     r     r     rr        Before proceeding  we observe that r and rr can be calculated exactly because R can be expressed as a sum of products of independent terms  For queries with this property  all approximations  except v    are exact  i e   additional observations of evidence variables have no effect on the posterior mean or variance  E g   given a discrete network with structure P E  B  H  we have q     b h b b e   Since parameters in each product are independent  it follows that q    q    q and v    qq   We begin with adjustments to improve q    Bayes rule and some manipulation yields q      q      E QR  qr   q        E R  r E QR     r qr   qrr   q       E R    r   rr   We approximate qqrr using       qqr by       qr by       q by q    and replace qq by v    Rearranging terms yields the identity  v        r   rr   v     q   q         qr         r   rr    r qr      q     q      q      v     Notice that v  appears in the denominator of       We initially set this value to v    then iteratively solve for v    The values converge in a few iterations  We observe that replacing rr by zero has negligible effect on      as m    By also replacing q  by q  and qr  r by q   q    we obtain a simpler identity  v     v      q   q                  q   q        q     q      q      v     We again initialize by v    then iteratively solve for v    The approximations q  and v  may be preferred to q  and v  since r and rr are not required  Rates of convergence are summarized in Proposition   below  The proof of this result follows easily from Van Allen et al         and the development above  Proposition    Assume a discrete network satisfying     and let m    The query mean q remains constant while the variance qq approaches zero at rate m    The mean approximations have errors qj  q approaching zero at rate m  for j     and    and at the faster rate m    for j     and    All four variance approximations have relative errors  vj qq   qq approaching zero at rate m                Scaled Error q   q   q   q                   Scaled Error       Scaled Error  q    b  Diamond   m             a  NB   m              r qq    r qqr   qqrr E  Q  q    R             E R     r   rr       q   In trying to improve v    we began with the idea of replacing q  with q    This suggests an approximation v     q   q       which does help to reduce the under estimation problem  however  a greater improvement is obtained by further analysis of              The formula for q  in Table   follows from       Now recall that  under condition      r remains fixed while rr    as m    It follows that setting rr     in      will have negligible effect for large m  We thus obtain qr   q   q   r   leading to the simpler q  approximation   E  Q  q      e  e    v     q   q                               q   q   r   r   rr   r     r     rr        r     r     r      r  rr  rr  Scaled Error  If r      then set qr      Otherwise  substituting      for qrr and solving yields qr              HOOPER ET AL         UAI       q   q   q    c  NB   m        q   q   q    d  Diamond   m        Figure    Boxplots of scaled errors m qj  q    for j             m             and network structures NB and Diamond  Each boxplot shows variation in errors for a set of distinct queries              for NB and     for Diamond  Errors for q  and q  are nearly identical  Errors for q  are often much larger  Results for q  are not plotted since q   q     q   q         NUMERICAL RESULTS  We evaluated accuracy of approximations qj and vj using highly accurate empirical estimates of q and qq   These estimates q  and v  were obtained by simulating k       replicates of  from the posterior distribution  evaluating q   for each replicate  then calculating the sample mean and sample variance  Computational costs preclude using empirical variance estipaper R figures mates Users peterhooper Documents Research Doubling in practice  When m is large  asymptotic normality of q   implies that the distribution of v   qq is approximately    k  k with variance   k p Consequently v   qq varies over the interval        k for roughly     of samples  Since our variance approximations have relative errors of order m    it follows that k should be of order at least m  for v  to have substantially smaller relative error  When comparing approximate relative errors  vj  v    v  with k         variation in v  has a noticeable effect for m        see Figure   f   Our examples differ with respect to network structure  posterior distribution  and query  All variables are binary  All posterior distributions satisfy BDe constraints  e g   see Hooper        so all variables have the same effective sample size m  Hyperparameters are thus determined by m and the poste         HOOPER ET AL     E   all children of H  e varies over all combinations     for NB       for NB               Diamond network with   variables         all     distinct queries with one hypothesis variable           Scaled Relative Error                Scaled Relative Error           v   v   v   v   v   v   v                Scaled Relative Error                    Scaled Relative Error  v    b  Diamond   m           a  NB   m       v   v   v   v   v   v   v   v    d  Diamond   m        COMPUTATIONAL ISSUES                 Scaled Relative Error             Approximations for means are compared in Figure   and for variances in Figure    The errors and relative errors are multiplied by m in these figures to facilitate comparisons across a range of effective sample sizes  Boxplots for m            and     are shown  Plots for other values of m are similar  By Proposition    relative errors  vj  qq   qq should approach zero at rates cj  m  where cj depends implicitly on the network  E    D   and the query  This theory is supported by Figure   and additional plots  not shown  comparing the four methods for individual queries  Our results suggest that c   c  while c  and c  tend to be further from zero  Relative errors can be interpreted in terms of variances or standard deviations  If  vj  qq   qq   cj  m  then we have p r vj cj cj cj vj     and            qq m qq m  m          Scaled Relative Error           c  NB   m             UAI       v   v   v    e  NB   m        v   v   v   v   v    f  Diamond   m        Figure    Boxplots of relative errors m vj  v    v  for j                m                  and network structures NB and Diamond  Each boxplot shows variation among values for a set of distinct queries     for NB and     for Diamond  We observe that  relative errors tend to be larger for NB compared with Diamond  v  and v  tend to over estimate qq for NB and are more accurate than v    the three methods v    v   and v  have similar accuracy for Diamond  v  is less accurate than the other methods  The four methods appear to have paper R figures similar accuracy in  f   but these plots are mislead Users peterhooper Documents Research Doubling ing  Many of the Diamond queries have the property described following       where v    v    v    qq   We would therefore expect the Diamond results for m       to be similar to those for m        It appears that the variation among relative errors for m       is due in large part to variation in v    rior means E    D   Our examples are from three small networks  each with one vector E    D  and m                            Two nave Bayes networks  NB   and NB   with   and   features plus the root variable   H   root   Inference in Bayesian networks is in general an NPcomplete problem  Cooper         For instance  the complexity of the Variable Elimination  VE  Algorithm is O dr    where d is an upper bound on the number of values that a variable can take and r is an upper bound on the size of a factor generated by the VE Algorithm  Koller and Friedman         Network doubling uses essentially the same technique to calculate a variance as that used to evaluate a query  resulting in corresponding computational complexity  The doubled CPtables are larger  squared number of rows and columns   so the computational complexity of VE is increased to O d r    The delta method retains O dr   complexity  Van Allen et al          so is typically faster in large networks  see Table   below  In some cases  we can exploit the structure of the network or query to achieve a polynomial time inference algorithm  For poly tree Bayesian networks  i e  networks with at most one undirected path between any pair of nodes   there exist inference algorithms with linear time complexity  Reduced complexity is also available when the query can be expressed in terms of probabilities of hypothesis and evidence nodes conditioned on their Markov blanket  i e   the parents  the children and the parents of the children  Once again  we have a polynomial time inference algorithm  These techniques translate directly to efficient algorithms for computing all of the variance approximations in Table      UAI       HOOPER ET AL   Table    Timing results in seconds  Network NB   Diamond Alarm    Queries                      Delta                       Doubling                            Acknowledgements We are grateful for helpful comments from the anonymous reviewers  We acknowledge support provided by NSERC  iCORE  and the Alberta Ingenuity Centre for Machine Learning  
 We study the problem of learning Markov decision processes with finite state and action spaces when the transition probability distributions and loss functions are chosen adversarially and are allowed to change with time  We introduce an algorithm whose regret with respect to any policy in a comparison class grows as the square root of the number of rounds of the game  provided the transition probabilities satisfy a uniform mixing condition  Our approach is efficient as long as the comparison class is polynomial and we can compute expectations over sample paths for each policy  Designing an efficient algorithm with small regret for the general case remains an open problem      Notation  Let X be a finite state space and A be a finite action space  Let S be the space of probability distributions over set S  Define a policy  as a mapping from the state space to A      X  A   We use  a x  to denote the probability of choosing action a in state x under policy   A random action under policy  is denoted by  x   A transition probability kernel  or transition model  m is a mapping from the direct product of the state and action spaces to X   m   X  A  X   Let P    m  be the transition probability matrix of policy  under transition model m  A loss function is a bounded real valued function over state and action spaces  l   X  A  R  For a P vector v  define kvk    i  vi    For a real valued function f defined over X  A  define kf k     P maxxX aA  f  x  a    The inner product between two vectors v and w is denoted by hv  wi      Introduction  Consider the following game between a learner and an adversary  at round t  the learner chooses a policy t from a policy class   In response  the adversary chooses a transition model mt from a set of models M and a loss function lt   The learner takes action at  t    xt    moves to state xt    mt    xt   at   and suffers loss lt  xt   at    To simplify the discussion  we assume that the adversary is oblivious  i e  its choices do not depend on the previous choices of the learner  We assume that lt          In this paper  we study the full information version of the game  where the learner observes the transition model mt and the loss function lt at the end of round t  The game is shown in Figure    The objective of the learner is to suffer low loss over a period of T rounds  while the performance of the learner is measured using its regret with respect to the total loss he would have achieved had he followed the stationary policy in the comparison class  minimizing the total loss  Even Dar et al         prove a hardness result for MDP problems with adversarially chosen transition models  Their proof  however  seems to have gaps as it assumes that the learner chooses a deterministic policy before observing the state at each round  Note that an online learning algorithm only needs to choose an action at the current state and does not need to construct a complete deterministic policy at each round  Their hardness result applies to deterministic transition models  while we make a mixing assumption in our analysis  Thus  it is still an open problem whether it is possible to obtain a computationally efficient algorithm with a sublinear regret  Yu and Mannor      a b  study the same setting  but obtain only a regret bound that scales with the amount of variation in the transition models  This regret bound can grow linearly with time    Initial state  x  for t                do Learner chooses policy t Adversary chooses model mt and loss function lt Learner takes action at  t    xt   Learner suffers loss lt  xt   at   Update state xt    mt    xt   at   Learner observes mt and lt end for Figure    Online Markov Decision Processes  Even Dar et al         prove regret bounds for MDP problems with a fixed and known transition model and adversarially chosen loss functions  In this paper  we prove regret bounds for MDP problems with adversarially chosen transition models and loss functions  We are not aware of any earlier regret bound for this setting  Our approach is efficient as long as the comparison class is polynomial and we can compute expectations over sample paths for each policy  MDPs with changing transition kernels are good models for a wide range of problems  including dialogue systems  clinical trials  portfolio optimization  two player games such as poker  etc      Online MDP Problems  Let A be an online learning algorithm that generates a policy t at round t  Let xA t be the state at round t if we have followed the policies generated by algorithm A  Similarly  xt denotes the state if we have chosen the same policy  up to time t  Let l x      l x   x    The regret of algorithm A up to round T with respect to any policy    is defined by RT  A       T X t    t  xA t     lt  xA t   at     T X  lt  xt        t    where at   Note that the regret with respect to  is defined in terms of the sequence of states xt that would have been visited under policy   Our objective is to design an algorithm that achieves low regret with respect to any policy   In the absence of state variables  the problem reduces to a full information online learning problem  Cesa Bianchi and Lugosi         The difficulty with MDP problems is that  unlike the full information online learning problems  the choice of policy at each round changes the future states and losses  The main idea behind the design and the analysis of our algorithm is the following regret decomposition  RT  A       T X t    Let  lt  xA t   at     T X  BT  A     T X  t    t    CT  A       lt  xt t   t      T X t    T X t    lt  xA t   at     T X  lt  xt t   t     lt  xt t   t     T X  lt  xt             t    lt  xt t   t      t    T X  lt  xt        t    Notice that the choice of policies has no influence over future losses in CT  A     Thus  CT  A    can be bounded by a specific reduction to full information online learning algorithms  to be specified later   Also  notice that the competitor policy  does not appear in BT  A   In fact  BT  A  depends only on the algorithm A  We will show that if algorithm A and the class of models satisfy the following two smoothness assumptions  then BT  A  can be bounded by a sublinear term  Assumption A  Rarely Changing Policies Let t be the probability that algorithm A changes its policy at round t  There exists a constant D such  that for any    t  T   any sequence of models m            mt and loss functions l            lt   t  D  t      N   number of experts  T   number of rounds  Initialize wi       for each expert i  W    N   for t                do For any i  pi t   wi t   Wt    Draw It such that for any i  P  It   i    pi t   Choose the action suggested by expert It   The adversary chooses loss function ct   The learner suffers loss ct  It    For expert i  wi t   wi t  ect  i    P Wt   N i   wi t   end for Figure    The EWA Algorithm N   number p of experts  T   number of rounds     min  log N T         Initialize wi       for each expert i  W    N   for t                do For any i  pi t   wi t   Wt    With probability t   wIt   t   wIt   t  choose the previously selected expert  It   It  and with probability    t   choose It based on the distribution qt    p  t           pN t    Learner takes the action suggested by expert It   The adversary chooses loss function ct   The learner suffers loss ct  It    For all experts i  wi t   wi t       ct  i    PN Wt   i   wi t   end for Figure    The Shrinking Dartboard Algorithm  Assumption A  Uniform Mixing There exists a constant      such that for all distributions d and d over the state space  any deterministic policy   and any model m  M   kdP    m   d P    m k   e   kd  d k    As discussed by Neu et al          if Assumption A  holds for deterministic policies  then it holds for all policies       Full Information Algorithms We would like to have a full information online learning algorithm that rarely changes its policy  The first candidate that we consider is the well known Exponentially Weighted Average  EWA  algorithm  Vovk        Littlestone and Warmuth        shown in Figure    In our MDP problem  the EWA algorithm chooses a policy    according to distribution   t  X E  ls  xs                   qt     exp  s    The policies that this EWA algorithm generates most likely are different in consecutive rounds and thus  the EWA algorithm might change its policy frequently  However  a variant of EWA  called Shrinking Dartboard  SD   Geulen et al         and shown in Figure    satisfies Assumption A   Our algorithm  called SD MDP  is based on the SD algorithm and is shown in Figure    Notice that the algorithm needs to know the number of rounds  T   in advance      T   number p of rounds     min  log     T         For all policies                    w        for t                do For any   p t   w t   Wt    With probability t   wt   t   wt   t  choose the previous policy  t   t    while with probability    t   choose t based on the distribution qt    p  t           p   t    Learner takes the action at  t    xt   Adversary chooses transition model mt and loss function lt   Learner suffers loss lt  xt   at    Learner observes mt and lt   Update state  xt    mt    xt   at     For allP policies   w t   w t       E lt  xt       Wt    w t   end for Figure    SD MDP  The Shrinking Dartboard Algorithm for Markov Decision Processes  Consider a basic full information problem with N experts  Let RT  SD  i  be the regret of the SD algorithm with respect to expert i up to time T   We have the following results for the SD algorithm  Theorem    For any expert i              N    p RT  SD  i     T log N   log N    and also for any    t  T    P  Switch at time t    r  log N   T  Proof  The proof of the regret bound can be found in  Geulen et al         Theorem     The proof of the bound on the probability of switch is similar to the proof of Lemma   in  Geulen et al         and is as follows  As shown in  Geulen et al         Lemma     the probability of switch at time t is t    Wt   Wt   Wt   Thus  Wt       t  Wt    Because the loss function is bounded in         we have that Wt    N X i    wi t    N X i    wi t       ct  i    Thus     t       and thus   t     r  N X i    wi t               Wt     log N   T      Analysis of the SD MDP Algorithm The main result of this section is the following regret bound for the SD MDP algorithm  Theorem    Let the loss functions selected by the adversary be bounded in         and the transition models selected by the adversary satisfy Assumption A   Then  for any policy     p E  RT  SD MDP                 T log      log       In the rest of this section  we write A to denote the SD MDP algorithm  For the proof we use the regret decomposition      RT  A      BT  A    CT  A                Bounding E  CT  A     Lemma    For any policy       T   T X X p t  E  CT  A       E lt  xt   t    lt  xt        T log      log      t    t    Proof  Consider the following imaginary game between a learner and an adversary  we have a set of experts  policies                          At round t  the adversary chooses a loss vector ct           whose ith element determines the loss of expert  i at this round  The learner chooses a distribution over experts qt  defined by the SD algorithm   from which it draws an expert t   Next  the learner observes the loss function ct   From the regret bound for the SD algorithm  Theorem     it is guaranteed that for any expert   T X t    hct   qt i   T X t    p ct       T log      log       Next  we determine how the adversary h chooses ithe loss vector  At time t  the adversary chooses a i i i loss function lt and sets ct       E lt  xt        Noting that hct   qt i   E  lt  xt t   t    and ct       E  lt  xt      finishes the proof         Bounding E  BT  A   First  we prove the following two lemmas  Lemma    For any state distribution d  any transition model m  and any policies  and     kdP    m   dP      m k   k    k     Proof  Proof is easy and can be found in  Even Dar et al          Lemma      p Lemma    Let t be the probability of a policy switch at time t  Then  t  log    T    Proof  Proof is identical to the proof of Theorem    Lemma    We have that E  BT  A     E    T X t    lt  xA t   at     T X t       lt  xt t   t         p log   T    Proof  Let Ft                t    Notice that the choice of policies are independent of the state variables  We can write   T   T X X t A E  BT  A     E lt  xt   at    lt  xt   t   t     E  E         E     E      E     E     T X  t    X   t   xX  T X X  t   xX  T X X  t   xX  T X t    T X t    T X t          I xA  I xt t  x  lt  x  t  x   t  x   E  h      I xA  I xt t  x  lt  x  t  x   FT t  x     i  i   h  t F lt  x  t  x  E I xA  I T  xt  x  t  x   klt k E  h   I xA  I xt t  x  t  x   klt k kut  vt t k     kut  vt t k             FT  i                 i h     A t where us   E I xA is the is the distribution of x for s  t and v   E I F F s t T T  x  s  x  x  s s distribution of xs t for s  t   Let Et be the event of a policy switch at time t  From inequality ktk  t k    ktk  tk   k          kt   t k       t X  I Es      s tk    and Lemma    we get that h  i  E ktk  t k       r  log    k  T       Let Pt   P    mt    We have that         t  t E kut  vt t k    E ut  Pt   vt  t Pt        t  t t t   E ut  Pt   ut  Pt    ut  Pt   vt  t Pt        t  t t t   ut  Pt   vt  t Pt   E ut  Pt   ut  Pt      h i  E kt   t k     e   kut   vt  t k  h t  t  E kt   t k     e     ut  Pt   ut  Pt    i t t     ut  Pt   vt  t Pt    h i  E kt   t k     e   kt   t k     e   kut   vt  t k          t X  k   t X  k       r  h i ek  E ktk  t k     et  ku   v  t k   e  k   r  log    k   T  By      log         T       where we have used the fact that ku   v  t k       because the initial distributions are identical  By     and      we get that r T X p log      E  BT  A            log   T   T t    What makes the analysis possible is the fact that all policies mix no matter what transition model is played by the adversary  Proof of Theorem    The result is obvious by Lemmas   and    The next corollary extends the result of Theorem   to continuous policy spaces  Corollary    Let  be an arbitrary policy space  N  o  be the o covering number of space    k k      and C o  be an o cover  Assume that we run the SD MDP algorithm on C o   Then  under the same assumptions as in Theorem    for any policy     p E  RT  SD MDP                 T log N  o    log N  o     T o      Notice that FT contains only policies  which are independent of the state variables       hP i T  Proof  Let LT      E l  x     be the value of policy   Let u t  x    P  xt   x   First  t t t   we prove that the value function is Lipschitz with Lipschitz constant  T   The argument is similar to the argument in the proof of Lemma    For any   and       T   T X X      LT       LT         E lt  xt        lt  xt       t         T X t    T X t    t    ku   t  u   t k  klt k ku   t  u   t k     With an argument similar to the one in the proof of Lemma    we can show that ku   t  u   t k    k     k     Thus   LT       LT         T k     k      Given this and the fact that for any policy     there is a policy    C o  such that k    k    o  we get that p E  RT  SD MDP                 T log N  o    log N  o     T o   In particular if  is the space of all policies  N  o     A  o  A  X    so regret is no more than r  A   A       A  X   log   T o   E  RT  SD MDP               T  A  X   log o o p By the choice of o   T    we get that E  RT  SD MDP       O    T  A   X   log  A T      
 We consider the problem of controlling a Markov decision process  MDP  with a large state space  so as to minimize average cost  Since it is intractable to compete with the optimal policy for large scale problems  we pursue the more modest goal of competing with a lowdimensional family of policies  We use the dual linear programming formulation of the MDP average cost problem  in which the variable is a stationary distribution over state action pairs  and we consider a neighborhood of a low dimensional subset of the set of stationary distributions  defined in terms of state action features  as the comparison class  We propose two techniques  one based on stochastic convex optimization  and one based on constraint sampling  In both cases  we give bounds that show that the performance of our algorithms approaches the best achievable by any policy in the comparison class  Most importantly  these results depend on the size of the comparison class  but not on the size of the state space  Preliminary experiments show the effectiveness of the proposed algorithms in a queuing application      Introduction  We study the average loss Markov decision process problem  The problem is well understood when the state and action spaces are small  Bertsekas         Dynamic programming  DP  algorithms  such as value iteration  Bellman        and policy iteration  Howard         are standard techniques to compute the optimal policy  In large state space problems  exact DP is not feasible as the computational complexity scales quadratically with the number of states  A popular approach to large scale problems is to restrict the search to the linear span of a small number of features  The objective is to compete with the best solution within this comparison class  Two popular methods are Approximate Dynamic Programming  ADP  and Approximate Linear     Programming  ALP   This paper focuses on ALP  For a survey on theoretical results for ADP see  Bertsekas and Tsitsiklis        Sutton and Barto          Bertsekas        Vol     Chapter     and more recent papers  Sutton et al       b a  Maei et al                Our aim is to develop methods that find policies with performance guaranteed to be close to the best in the comparison class but with computational complexity that does not grow with the size of the state space  All prior work on ALP either scales badly or requires access to samples from a distribution that depends on the optimal policy  This paper proposes a new algorithm to solve the Approximate Linear Programming problem that is computationally efficient and does not require knowledge of the optimal policy  In particular  we introduce new proof techniques and tools for average cost MDP problems and use these techniques to derive a reduction to stochastic convex optimization with accompanying error bounds  We also propose a constraint sampling technique and obtain performance guarantees under an additional assumption on the choice of features        Notation  Let X and A be positive integers  Let X                  X  and A                  A  be state and action  spaces  respectively  Let S denote probability distributions over set S  A policy  is a map from the state space to A      X  A   We use  a x  to denote the probability of choosing action a  in state x under policy   A transition probability kernel  or transition kernel  P   X  A  X  maps from the direct product of the state and action spaces to X   Let P  denote the probability  transition kernel under policy   A loss function is a bounded real valued function over state and action spaces  l   X  A          Let Mi   and M  j denote ith row and jth column of matrix M P respectively  Let kvk  c   i ci  vi   and kvk c   maxi ci  vi   for a positive vector c  We use   and    to denote vectors with all elements equal to one and zero  respectively  We use  and  to denote  the minimum and the maximum  respectively  For vectors v and w  v  w means element wise inequality  i e  vi  wi for all i        Linear Programming Approach to Markov Decision Problems  Under certain assumptions  there exist a scalar  and a vector h  RX that satisfy the Bellman optimality equations for average loss problems        h  x    min l x  a    aA  X  x X       P x a  x h  x       The scalar  is called the optimal average loss  while the vector h is called a differential value function  The action that minimizes the right hand side of the above equation is the optimal action in state x and is denoted by a  x   The optimal policy is defined by   a  x  x       Given l and      P   the objective of the planner is to compute the optimal action in all states  or equivalently  to find the optimal policy  The MDP problem can also be stated in the LP formulation  Manne         max           h  s t   B     h   l   P h    where B        XAX is a binary matrix such that the ith column has A ones in rows      i    A  to iA  Let v be the stationary distribution under policy  and let   x  a    v  x  a x   We can write X     argmin   xX    argmin   v  x   X  X   a x l x  a   aA    x  a l x  a    argmin  l      x a X A  In fact  the dual of LP     has the form of min  l         RXA  s t                 P  B        The objective function   l  is the average loss under stationary distribution   The first two constraints ensure that  is a probability distribution over state action space  while the last constraint ensures that  is a stationary distribution  Given a solution   we can obtain a policy via P  a x     x  a   a A  x  a          Approximations for Large State Spaces  The LP formulations     and     are not practical for large scale problems as the number of variables and constraints grows linearly with the number of states  Schweitzer and Seidmann        propose approximate linear programming  ALP  formulations  These methods were later improved by de Farias and Van Roy      a b   Hauskrecht and Kveton         Guestrin et al          Petrik and Zilberstein         Desai et al          As noted by Desai et al          the prior work on ALP either requires access to samples from a distribution that depends on optimal policy or assumes the ability to solve an LP with as many constraints as states   See Section   for a more detailed discussion   Our objective is to design algorithms for very large MDPs that do not require knowledge of the optimal policy  In contrast to the aforementioned methods  which solve the primal ALPs  with value functions as variables   we work with the dual form      with stationary distributions as variables   Analogous     to ALPs  we control the complexity by limiting our search to a linear subspace defined by a small number of features  Let d be the number of features and  be a  XA   d matrix with features as  column vectors  By adding the constraint      we get min    l     s t                      P  B         If a stationary distribution   is known  it can be added to the linear span to get the ALP min       l           s t                                 P  B        Although      might not be a stationary distribution  it still defines a policy    a x    P      x  a     x a          a     x  a      x a              We denote the stationary distribution of this policy  which is only equal to      if  is in the feasible set        Problem definition  With the above notation  we can now be explicit about the problem we are solving  Definition    Efficient Large Scale Dual ALP   For an MDP specified by l and P   the efficient large scale dual ALP problem is to produce parameters b such n o   l  min  l    feasible for       O o   b        in time polynomial in d and   o  The model of computation allows access to arbitrary entries of   l  P       P    and    in unit time  Importantly  the computational complexity cannot scale with X and we do not assume any knowledge of the optimal policy  In fact  as we shall see  we solve a harder problem  which we define as follows  Definition    Expanded Efficient Large Scale Dual ALP   Let V   Rd  R  be some violation  function that represents how far     is from a valid stationary distribution  satisfying V         if  is a feasible point for the ALP      The expanded efficient large scale dual ALP problem is to    We use the notation  v    v    and  v     v          produce parameters b such that  l b         d  min  l   V        R   O o   o       in time polynomial in d and   o  under the same model of computation as in Definition    Note that the expanded problem is strictly more general as guarantee     implies guarantee      Also  many feature vectors  may not admit any feasible points  In this case  the dual ALP problem is trivial  but the expanded problem is still meaningful  Having access to arbitrary entries of the quantities in Definition   arises naturally in many situations  In many cases  entries of P   are easy to compute  For example  suppose that for any state x there are a small number of state action pairs  x  a  such that P  x  x  a       Consider Tetris  although the number of board configurations is large  each state has a small number of possible neighbors  Dynamics specified by graphical models with small connectivity also satisfy this constraint  Computing entries of P   is also feasible given reasonable features  If a feature i is a stationary distribution  then P  i   B  i   Otherwise  it is our prerogative to design sparse feature vectors  hence making the multiplication easy  We shall see an example of this setting later        Our Contributions  In this paper  we introduce an algorithm that solves the expanded efficient large scale dual ALP problem under a  standard  assumption that any policy converges quickly to its stationary distribution  Our algorithm take as input a constant S and an error tolerance o  and has access to the various products listed in Definition    Define                        kk  S   If no stationary distribution is known  we can simply choose        The algorithm is based on stochastic convex optimization  We prove that for any           after O   o    steps of gradient descent  the algorithm finds a vector b   such that  with probability at least       l  l  b      k       k     P  B         o o       O o log       holds for all     i e   we solve the expanded problem for V    equal to the L  error of the  violation  The second and third terms are zero for feasible points  points in the intersection of the feasible set of LP     and the span of the features   For points outside the feasible set  these terms measure the extent of constraint violations for the vector       which indicate how well stationary distributions can be represented by the chosen features   The above performance bound scales with   o that can be large when the feasible set is empty and o is very small  We propose a second approach and show that this dependence can be eliminated if we have extra information about the MDP  Our second approach is based on the constraint     sampling method that is already applied to large scale linear programming and MDP problems  de Farias and Van Roy        Calafiore and Campi        Campi and Garatti         We obtain performance bounds  but under the condition that a suitable function that controls the size of constraint violations is available  Our proof technique is different from previous work and gives stronger performance bounds  Our constraint sampling method takes two extra inputs  functions v  and v  that specify the amount of constraint violations that we tolerate at each state action pair  The algorithm samples O  d log     constraints and solves the sampled LP problem  Let e denote the solution of the sampled o  o  ALP and  denote the solution of the full ALP subject to constraints v  and v    We prove that with high probability   l e  l    O o   kv  k    kv  k          Related Work  de Farias and Van Roy      a  study the discounted version of the primal form      Let c  RX  be a vector with positive components and          be a discount factor  Let L   RX  RX be P the Bellman operator defined by  LJ  x    minaA  l x  a     x X P x a  x J x    for x  X   Let    RXd be a feature matrix  The exact and approximate LP problems are as follows  max c J    max c w    JRX  s t   wRd  LJ  J    s t   Lw  w    which can also be written as max c J    max c w    JRX  wRd  s t   x  a   l x  a    P x a    J  J x     s t         x  a   l x  a    P x a    w   w  x     The optimization problem on the RHS is an approximate LP with the choice of J   w  Let   P t   J  x    E t    l xt    xt    x    x be value of policy   J be the solution of LHS  and J  x     argminaA  l x  a    P x a    J  be the greedy policy with respect to J  Let   X be a probability  distribution and define             I  P       de Farias and Van Roy      a  prove that  for any J satisfying the constraints of the LHS of      kJJ  J k    Define u    maxx a  P  x    kJ  J k       J         P x a  x u x   u x   Let U    u  RX   u     u  span    u       Let      w be the solution of ALP  de Farias and Van Roy      a  show that for any u  U   kJ  w k  c    c u min kJ  wk   u      u w       This result has a number of limitations  First  solving ALP can be computationally expensive as the number of constraints is large  Second  it assumes that the feasible set of ALP is non empty  Finally  Inequality     implies that c   w   is an appropriate choice to obtain performance bounds  However  w itself is function of c and is not known before solving ALP  de Farias and Van Roy        propose a computationally efficient algorithm that is based on a constraint sampling technique  The idea is to sample a relatively small number of constraints and solve the resulting LP  Let N  Rd be a known set that contains w  solution of ALP   Let V V V c  x     c  x V  x     c V   and define the distribution  c  x  a     c  x  A  Let          P and o          Let  u    maxx x P x   x   x u x   u x  and        V     c V sup kJ  wk   V   D    c J wN    AD m      o         AD   log d log      o         Let S be a set of m random state action pairs sampled under V  c   Let w b be a solution of the  following sampled LP   max c w    wRd  s t   w  N    x  a   S  l x  a    P x a    w   w  x     de Farias and Van Roy        prove that with probability at least      we have kJ  wk b   c  kJ  w k  c   o kJ k  c    This result has a number of limitations  First  vector   c  that is used in the definition of D  depends on the optimal policy  but an optimal policy is what we want to compute in the first place  Second  we can no longer use Inequality     to obtain a performance bound  a bound on kJwb  J k  c    as w b does not necessarily satisfy all constraints of ALP   Desai et al         study a smoothed version of ALP  in which slack variables are introduced  that allow for some violation of the constraints  Let D  be a violation budget  The smoothed ALP  SALP  has the form of max c w    max c w   w s  s t   w  Lw   s   w s     c s     D   s             c s      s t  w  Lw   s  s        The ALP on RHS is equivalent to LHS with a specific choice of D    Let U    u  RX   u      be a set of weight vectors  Desai et al         prove that if w is a solution to above problem  then kJ  w k  c  inf kJ  wk   u w uU       c u      u   c u            The above bound improves     as U is larger than U and RHS in the above bound is smaller than RHS of      Further  they prove that if  is a distribution and we choose c         I P w    then  Jw  J            inf kJ  wk   u  w uU        u      u   c u             Similar methods are also proposed by Petrik and Zilberstein         One problem with this result is that c is defined in terms of w   which itself depends on c  Also  the smoothed ALP formulation uses  which is not known  Desai et al         also propose a computationally efficient algorithm  Let S be a set of S random states drawn under distribution   c   Let N   Rd be a known set  that contains the solution of SALP  The algorithm solves the following LP  max c w  w s  s t   X   s x         S xS  x  S   w  x    Lw  x    s x   s     w  N     Let w b be the solution of this problem  Desai et al         prove high probability bounds on the approximation error kJ  wk b   c   However  it is no longer clear if a performance bound on kJ  Jwb k  c can be obtained from this approximation bound   Next  we turn our attention to average cost ALP  which is a more challenging problem and is  also the focus of this paper  Let  be a distribution over states  u   X                          P   P              and L h   min  l   P h   de Farias and Van Roy        propose the  following optimization problem   min s    s           w s   s   s t   L w  w   s      s  u     s        Let  w   s     s     be the solution of this problem  Define the mixing time of policy  by    inf         t    X   t    P   l      t t  t t           Let    lim inf               Let  be the optimal policy when discount factor is       Let  w be the greedy policy with respect to w when discount factor is            P t   t   t      P   and  w     w   de Farias and Van Roy        prove that if           u   w             max D       min h  w w       u             w       where    max kI  P  k   u   D      w V    V   and V   L w  w   s       s   u   Similar results are obtained more recently by Veatch          An appropriate choice for vector  is     w   Unfortunately  w depends on   We should also note that solving      can be computationally expensive  de Farias and Van Roy        propose constraint sampling techniques similar to  de Farias and Van Roy         but no performance bounds are provided  Wang et al         study ALP     and show that there is a dual form for standard value function based algorithms  including on policy and off policy updating and policy improvement  They also study the convergence of these methods  but no performance bounds are shown      A Reduction to Stochastic Convex Optimization  In this section  we describe our algorithm as a reduction from Markov decision problems to stochastic convex optimization  The main idea is to convert the ALP     into an unconstrained optimization over  by adding a function of the constraint violations to the objective  then run stochastic gradient descent with unbiased estimated of the gradient  For a positive constant H  form the following convex cost function by adding a multiple of the total constraint violations to the objective of the LP      c     l          H k       k    H  P  B                  l          H k       k    H  P  B     X X    l          H     x  a     x a        H  P  B    x           x   x a   We justify using this surrogate loss as follows  Suppose we find a near optimal vector b such that b  min c     O o   We will prove c      that  b            and  b  P  B          Lemma   in Section     and  b  min c     O o      that l              are small and     b is close to b  by   Input  Constant S      number of rounds T   constant H  Let  be the Euclidean projection onto   Initialize        for t                  T do Sample  xt   at    q  and xt  q    Compute subgradient estimate gt       Update t       t  t gt    end forP bT   T  Tt   t   Return policy bT   Figure    The Stochastic Subgradient Method for Markov Decision Processes  As we will show  these two facts imply that with high probability  for any      l   l  b      k       k     P  B         o o       O o     which is to say that minimization of c   solves the extended efficient large scale ALP problem  Unfortunately  calculating the gradients of c   is O XA   Instead  we construct unbiased estimators and use stochastic gradient descent  Let T be the number of iterations of our algorithm  Let q  and q  be distributions over the state action and state space  respectively  we will later discuss how to choose them   Let   xt   at   t     T be i i d  samples from q  and  xt  t     T be i i d  samples from q    At round t  the algorithm estimates subgradient c   by  P  B   xt  at       xt  I    xt  at    xt  at           H s  P  B  gt      l   H   xt    q   xt   at   q   xt           This estimate is fed to the projected subgradient method  which in turn generates a vector t   P After T rounds  we average vectors  t  t     T and obtain the final solution bT   T t  T   Vector t        bT defines a policy  which in turn defines a stationary distribution bT    The algorithm is shown in Figure       Recall that  is the stationary distribution of policy   a x    P      x  a     x a           a     x  a      x a         With an abuse of notation  we use  to denote policy  as well             Analysis  In this section  we state and prove our main result  Theorem    We begin with a discussion of the assumptions we make then follow with the main theorem  We break the proof into two main ingredients  First  we demonstrate that a good approximation to the surrogate loss gives a feature vector that is almost a stationary distribution  this is Lemma    Second  we justify the use of unbiased gradients in Theorem   and Lemma    The section concludes with the proof  We make a mixing assumption on the MDP so that any policy quickly converges to its stationary distribution  Assumption A   Fast Mixing  for all distributions d and  d  For any policy   there exists a constant         such that  over the state space  kdP   d P  k   e      kd  d k     Define C     max   x a X A   x a      q   x  a   C    max xX   P  B    x    q   x   These constants appear in our performance bounds  So we would like to choose distributions q  and q  such that C  and C  are small  For example  if there is C      such that for any  x  a  and i   x a  i  C    XA  and each column of P has only N non zero elements  then we can simply  choose q  and q  to be uniform distributions  Then it is easy to see that  x a     C   q   x  a    P  B    x   C   N   A    q   x   As another example  if   i are exponential distributions and feature values at neighboring states are close to each other  then we can choose q  and q  to be appropriate exponential distributions so that  x a     q   x  a  and  P  B    x   q   x  are always bounded  Another example is when      B     C   and we have access to there exists a constant C      such that for any x  P  x   x P P   and can sample an efficient algorithm that computes Z     x a   x a    and Z    x B  x  from q   x  a      x a     Z  and q   x        Z   In what follows  we assume that such B  x    distributions q  and q  are known  We now state the main theorem  Theorem    Consider an expanded efficient large scale dual ALP problem  Suppose we apply the stochastic subgradient method  shown in Figure    to the problem  Let o          Let T     o  be the number of rounds and H     o be the constraints multiplier in subgradient estimate       Let bT be the output of the stochastic subgradient method after T rounds and let the learning rate be       P      T   S  G T    where G   d   H C    C     Define V        x a      x  a     x a         This condition requires that columns of  are close to their one step look ahead        and V        P  x   P  B    x          Then  for any           with probability at least      l  bT   min       l         V       V        O o    o        where constants hidden in the big O notation are polynomials in S  d  C    C    log      V      V            and   bT    Functions V  and V  are bounded by small constants for any set of normalized features  for any     V      k  k    kk       V        X   P  x            x  X  P  x  x     X   x a   X   x a          Sd    B  x          x             X  B  x  x                                                  S   Thus V  and V  can be very small given a carefully designed set of features  The output bT is a random vector as the algorithm is based on a stochastic convex optimization method  The above  theorem shows that with high probability the policy implied by this output is near optimal  p The optimal choice for o is o   V        V       where  is the minimizer of RHS of      and q not known in advance  Once we obtain bT   we can estimate V   bT   and V   bT   and use input o   V   bT     V   bT   in a second run of the algorithm  This implies that the error bound scales p like O  V        V        The next lemma  providing the first ingredient of the proof  relates the amount of constraint violation of a vector  to resulting stationary distribution    Lemma    Let u  RXA be a vector  Let N be the set of points  x  a  where u x  a      and S be complement of N   Assume X x a  u x  a        X   x a N   u x  a    o   u  P  B       o    Vector  u     k u   k  defines a policy  which in turn defines a stationary distribution u   We have  that  ku  uk     u   log   o    o   o      o        Proof  Let f   u  P  B   From u  P  B  X   x a S  such that  P  x  u x  a  P  B  x a  x      o   we get that for any x  X       X   x a N  u x  a  P  B  x a  x   f  x     f  x     o   Let h    u     k u   k    Let H    h  B  P       We write H    X x     X   x a S    X     o  x  h x  a  B  P   x a  x X   x a S  u x  a  B  P   x a  x  X   X  u x  a  B  P   x a  x   f  x       o  x  x a N   X X X     u x  a  B  P   x a  x    f  x        o x x  x a N   X X     o    u x  a    B  P   x a  x       o   x a N x     X        o   o o      u x  a        o     o     x a N        o   o    Vector h is almost a stationary distribution in the sense that h  B  P   Let kwk  S    P   x a S       o   o           w x  a    First  we have that kh  uk   h   u     o       u  u     o    S    o    Next we bound kh  hk    Let     h be the initial state distribution  We will show that as we  run policy h  equivalently  policy h    the state distribution converges to h and this vector is close     h to h  From       we have    P   h B   v    where v  is such that kv  k    o   o   Let M be a h X   XA  matrix that encodes policy h  M i  i  A      i iA    h  xi    Other entries of this matrix       are zero  We get that h P M h    h B   v   M h   h BM h   v  M h   h   v  M h    h where we used the fact that h BM h   h   Let      h P M which is the state action distribution  after running policy h for one step  Let v    v  M h P   v  P h and notice that as kv  k    o   o   we also have that kv  k    P h v       kv  k    o   o   Thus        P   h P   v    h B   v    v     By repeating this argument for k rounds  we get that  h  k   h    v    v         vk   M  and it is easy to see that  v    v         vk   M h       k  X i    kvi k   k  o   o     Thus  kk  hk   k  o   o    Now notice that k is the state action distribution after k rounds of policy h   Thus  by mixing assumption  kk  h k   ek   h    By the choice of k     h  log   o     we get that kh  hk     h  log   o    o   o     o    The second ingredient is the validity of using estimates of the subgradients  We assume access to estimates of the subgradient of a convex cost function  Error bounds can be obtained from results in the stochastic convex optimization literature  the following theorem  a high probability version of Lemma     of Flaxman et al         for stochastic convex optimization  is sufficient  Theorem    Let Z be a positive constant and Z be a bounded subset of Rd such that for any  z  Z  kzk  Z  Let  ft  t         T be a sequence of real valued convex cost functions defined over  Z  Let z    z            zT  Z be defined by z      and zt     Z  zt  ft    where Z is the Euclidean  projection onto Z       is a learning rate  and f            fT are unbiased subgradient estimates such  that E  ft  zt     f  zt   and kft k  F for some F      Then  for    Z  F T    for any            with probability at least       T T X X  ft  z   ZF T   ft  zt    min t    zZ  t    Proof  Let z   argminzZ  PT  t   ft  z   s         Z   T              Z  T   log   d log        d        and t   ft  ft  zt    Define function ht   Z  R by  ht  z    ft  z    zt   Notice that ht  zt     ft  zt     t   ft   By Theorem   of Zinkevich              we get that T X t    ht  zt     T X t    ht  z     T X t    ht  zt    min zZ  T X t     ht  z   ZF T    Thus  T X t    Let St    Pt   s    z zs  s    ft  zt     T X t    T X  ft  z    ZF T    z  zt  t   t    which is a self normalized sum  de la Pena et al          By Corollary      and Lemma E   of Abbasi Yadkori         we get that for any           with probability at least        v    u      t  u X Z  t   t    St     zt  z     log   d log         d s   s          Z  t          Z t    log   d log        d  Thus  T X t    ft  zt    min zZ  T X t     ft  z   ZF T    s         Z   T            Z  T       log   d log      d  Remark    Let BT denote RHS of       If all cost functions are equal to f   then by convexity of P f and an application of Jensens inequality  we obtain that f   Tt   zt  T    minzZ f  z   BT  T   As the next lemma shows  Theorem   can be applied in our problem to optimize cost function  c  Lemma    Under the same conditions as in Theorem    we have that for any           with probability at least       SG c bT    min c        T  s       S   T T           S  T     log   d log        d  Proof  We prove the lemma by showing that conditions of Theorem   are satisfied        We be   gin by calculating the subgradient and bounding its norm with a quantity independent of the number of states  If    x  a     x a         then      x  a     x a           Otherwise             x  a     x a         x a      Calculating   c     l    H   l   H  X   x a   X       x  a     x a        H  x a    I    x a   x a          H  X x  X x   x a     P  B    x     P  B    x s  P  B   x            where s z    I z     I z    is the sign function  Let  denote the plus or minus sign  the exact  sign does not matter here   Let G   k c  k  We have that  v v        u u uX uX d d X X X u u    G  Ht  x a  i     P  B  x a  x  x a  i    l    H t i    x  i     x a    x a   Thus  v v  u u d uX d uX X  u  G  t  l   i      H d   H t i    i     x a   X   P  B  x a  x x  v    u uX d X    u    x a  i    d      H     d   H d   Ht i           x a  i    x a   where we used l   i  klk k  i k       Next  we show that norm of the subgradient estimate is bounded by G    P  B    xt  at       xt  d   H C    C       H  kgt k  l    H q   xt   at   q   xt     Finally  we show that the subgradient estimate is unbiased  E  gt       l   H  X  q   x  a    x a    H   l   H  X   x a    I q   x  a      x a   x a         X x    q   x     P  B    x  q   x    s  P  B    x     x a    I    x a   x a          H  X   P  B    x s  P  B   x   x   x a      c     The result then follows from Theorem   and Remark          With both ingredients in place  we can prove our main result  Proof of Theorem    Let bT be the RHS of       Equation      implies that with high probability for any     l      bT     H V   bT     H V   bT    l          H V       H V       bT          From       we get that    def V   bT           S    H V       H V       bT     o   H   def V   bT           S    H V       H V       bT     o   H             Inequalities      and      and Lemma   give the following bound    l       bT   l    bT   log   o    o   o      o   b T        From      we also have  l      bT    l          H V       H V       bT    which  together with      and Lemma    gives the final result    l  l          H V       H V       bT     bT   log   o    o   o      o b T         l   H V       H V       bT     bT   log   o    o   o      o         log   V       V       V         V        Recall that bT   O H  T    Because H     o and T     o    we get that with high probability     for any      b l   l   o  V       V        O o   T  Lets compare Theorem   with results of de Farias and Van Roy         Their approach is to relate the original MDP to a perturbed version  and then analyze the corresponding ALP   See Section   for more details   Let  be a feature matrix that is used to estimate value functions  Recall that  is the average loss of the optimal policy and w is the average loss of the greedy policy with respect to value function w  Let h be the differential value function when the restart probability in the perturbed MDP is      For vector v and positive vector u  define the    In a perturbed MDP  the state process restarts with a certain probability to a restart distribution  Such perturbed MDPs are closely related to discounted MDPs        weighted maximum norm kvk u   maxx u x   v x    de Farias and Van Roy        prove that for  appropriate constants C  C      and weight vector u  w      C min h  w   w   u    C                 This bound has similarities to bound       tightness of both bounds depends on the quality of feature vectors in representing the relevant quantities  stationary distributions in      and value functions in        Once again  we emphasize that the algorithm proposed by de Farias and Van Roy        is computationally expensive and requires access to a distribution that depends on optimal policy      Sampling Constraints  In this section we describe our second algorithm for average cost MDP problems  Using the results on polytope constraint sampling  de Farias and Van Roy        Calafiore and Campi        Campi and Garatti         we reduce approximate the solution to the dual ALP with the solution to a smaller  sampled LP  Basically  de Farias and Van Roy        claim that given a set of affine constraints in Rd and some measure q over these constraints  if we sample k   O d log     o  constraints  then with probability at least      any point that satisfies all of these k sampled  constraints also satisfies    o of the original set of constraints under measure q  This result is  independent of the number of original constraints   Let L be a family of affine constraints indexed by i  constraint i is satisfied at point w  Rd if a i w   bi     Let I be the family of constraints by selecting k random constraints in L with  respect to measure q   Theorem    de Farias and Van Roy          Assume there exists a vector that satisfies all con    straints in L  For any  and o  if we take m   o d log    o   log    then  with probability      a set I of m i i d  random variables drawn from L with respect to distribution q satisfies sup  w iI a i w bi     q  j   a j w   bj        o    Our algorithm takes the following inputs  a positive constant S  a stationary distribution     a set                        kk  S   a distribution q  over the state action space  a distribution q  over the state space  and constraint violation functions v    X  A         and  v    X          We will consider two families of constraints   L        x  a     x a      v   x  a     x  a   X  A    n o n o  L     P  B         v  x    x  X  P  B         v  x    x  X         x     x        Input  Constant S      stationary distribution     distributions q  and q    constraint violation functions v  and v    number of samples k  and k    For i         let Ii be ki constraints sampled from Li under distribution qi   Let I be the set of vectors that satisfy all constraints in I  and I    Let e be the solution to LP  min l                   s t     I        Return policy e  Figure    The Constraint Sampling Method for Markov Decision Processes  Let  be the solution of min l                   s t     L      L          The constraint sampling algorithm is shown in Figure    We refer to      as the sampled ALP  while we refer to     as the full ALP        Analysis  We require Assumption A  as well as  Assumption A   Feasibility   There exists a vector that satisfies all constraints L  and L     Validity of this assumption depends on the choice of functions v  and v    Larger functions ensure that this assumption is satisfied  but as we show  this leads to larger error  The next two lemmas apply theorem   to constraints L  and L    respectively        Lemma    Let           and o           If we choose k    o   d log      log o      then with P e   SC  o    kv  k   probability at least         x a      x  a     x a         Proof  Applying theorem    we have that w p         q      x  a     x a    e  v   x  a       o     and thus  X  q   x  a I    x a    e   x a     v   x a     x a        o      Let L    P   x a   e    With probability            x  a     x a      L   X  e I     x  a     x a          x a     x a      X   x a       X  e I     x  a     x a          x a     x a    e I    x a     x a   X   x a      x a   X  e   x a    v   x a    e   x a    v   x a    e I    x a    SC  q   x  a I    x a    e   x a     v   x a      kv  k   e   x a    v   x a    e   x a    v   x a     x a     kv  k     kv  k    SC  o    kv  k     Lemma    Let           and o           If we choose k    probability at least         P  B  e      SC  o    kv  k     Proof  Applying theorem    we have that q  X x  Let L    P  x       o     d log     o     e  v   x      o    This yields    P  B    x  q   x I   P B   e v   x      x   e  P  B    x    Thus  with probability        X   o     e  P  B  e   x  I   P B    x    v   x   x X e  P  B    e   x  I   P B    x   v   x   x X e I   P B  e  v   x     kv  k    P  B    x    x x X  SC  q   x I   P B  e  v   x     kv  k    x  L        log      then with  x   SC  o    kv  k    where the last step follows from                   a  d      d      d      d      a   server   server   Figure    The  D queueing network  Customers arrive at queue   or   then are referred to queue   or     respectively  Server   can either process queue   or    and server   can only process queue   or     We are ready to prove the main result of this section  Let e denote the solution of the sampled  ALP   denote the solution of the full ALP       and e be the stationary distribution of the solution policy  Our goal is to compare l e and l     Theorem    Let o         and           Let o   SC  o   kv  k  and o   SC  o   kv  k    If we              sample constraints with k     o d log    o   log  and k    o d log o   log    then  with probability      l e  l      e  log   o    o   o      o        log    kv  k    kv  k   kv  k      kv  k   Proof  Let             By Lemmas   and    w p       e  P  B              o   Then by Lemma     P   x a   e   o and     x  a     x a       e     e  log   o    o   o      o   l e  l          e  l          Thus  We also have that l         l e  l             e  log   o    o   o      o  l      e  log   o    o   o      o         log    kv  k    kv  k   kv  k      kv  k   where the last step follows from Lemma             Experiments  In this section  we apply both algorithms to the four dimensional discrete time queueing network illustrated in Figure    This network has a relatively long history  see  e g  Rybko and Stolyar        and more recently de Farias and Van Roy      a   c f  section       There are four queues                  each with state            B  Since the cardinality of the state space is X        B     even a modest B results in huge state spaces  For time t  let Xt  X be the state and si t           i              denote whether queue i is being served  Server   only serves queue   or    server   only serves queue   or    and neither server can idle  Thus  s  t   s  t     and s  t   s  t      The dynamics are as follows  At each time t  the following random variables are sampled independently  A  t  Bernoulli a     A  t  Bernoulli a     and Di t  Bernoulli di  si t   for i               Using e            e  to denote the standard basis vectors  the dynamics are   Xt    Xt   A  t e    A  t e     D  t  e   e     D  t e    D  t  e   e     D  t e         i e  all four states are thresholded from below by   and above and Xt     max    min  B   Xt    by B   The loss function is the total queue size  l Xt       Xt       We compared our method against  two common heuristics  In the first  denoted LONGER  each server operates on the queue that is longer with ties broken uniformly at random  e g  if queue   and   had the same size  they are equally likely to be served   In the second  denoted LBFS  last buffer first served   the downstream queues always have priority  server   will serve queue   unless it has length    and server   will serve queue   unless it has length     These heuristics are common and have been used an benchmarks for queueing networks  e g  de Farias and Van Roy      a    We used a    a         d    d         and d    d         and buffer sizes B    B        B    B       as the parameters of the network   The asymmetric size was chosen because server   is the bottleneck and tend to have has longer queues  The first two features are the stationary distributions corresponding to two heuristics  We also included two types of non stationary distribution features  For every interval                                   and action A  we added a feature  with  x  a      if l x  a  is in the interval and a   A  To define the second type  consider the three intervals I             I              and I              For every   tuple of intervals  J    J    J    J      I    I    I      and action A  we created a feature  with  x  a      only if xi  Ji and a   A  Every feature was  normalized to sum to    In total  we had     features which is about a     reduction in dimension from the original problem        loss of running average  total constraint violations  average loss of the running average policy                                                                                                                                                                     x     x          x     Figure    The left plot is of the linear objective of the running average  i e  l bt   The center plot is the sum of the two constraint violations of bt   and the right plot is l bt  the average loss of the derived policy   The two horizontal lines correspond to the loss of the two heuristics  LONGER and LBFS        Stochastic Gradient Descent  We ran our stochastic gradient descent algorithm with I        sampled constraints and constraint gain H      Our learning rate began at     and halved every      iterations  The results of our algorithm are plotted in Figure      where bt denotes the running average of t   The left plot  is of the LP objective  l      bt    The middle plot is of the sum of the constraint violations       bt      P  B  bt   Thus  c bt   is a scaled sum of the first two plots  Finally  the       right plot is of the average losses  l bt and the two horizontal lines correspond to the loss of the two heuristics  LONGER and LBFS  The right plot demonstrates that  as predicted by our theory  minimizing the surrogate loss c   does lead to lower average losses  All previous algorithms  including de Farias and Van Roy      a   work with value functions  while our algorithm works with stationary distributions  Due to this difference  we cannot use the same feature vectors to make a direct comparison  The solution that we find in this different approximating set is slightly worse than the solution of de Farias and Van Roy      a         Constraint Sampling  For the constraint sampling algorithm  we sampled the simplex constraints uniformly with    different sample sizes                                                        and        Since XA              these sample sizes correspond to less that     The stationary constraints were  sampled in the same proportion  i e  A times fewer samples   Let a            aAN and b            bN be the indices of the sampled simplex and stationary constraints  respectively  Explicitly  the sampled       LP is  min   l            s t            ai         i              AN     P  B   bi  os   i              N  kk  M        where M and o are necessary to ensure the LP always has a feasible and bounded solution  This corresponds to setting v      and v    o  In particular  we used M     and o         Using differenc values of o did not have a large effect on the behavior of the algorithm  For each sample size  we sample the constraints  solve the LP  then simulate the average loss of the policy  We repeated this procedure    times for each sample size and plotted the mean with error bars corresponding to the variance across each sample size in Figure      Note the log scale on the x axis  The best loss corresponds to      sampled simplex constraints  or roughly     and is a marked improvement over the average loss found by the stochastic gradient descent method  However  changing the sample size by a factor of   in either direction is enough to obliterate this advantage  average loss produced by the constraint sampling algorithm with variance bars           average loss                                     number of simplex constraints sampled  Figure    Average loss with variance error bars of the constraint sampling algorithm run with a variety of sample sizes       First  we notice that the mean average loss is not monotonic  If we use too few constraints  then the sampled ALP does not reflect our original problem and we expect that the solution will make poor policies  On the other hand  if we sample too many constraints  then the LP is too restrictive and cannot adequately explore the feature space  To explain the increasing variance  recall that we have three families of constraints  the simplex constraints  the stationarity constraints  and the box constraints  i e      M    Only the simplex and stationarity constraints are sampled  For the  small sample sizes  the majority of the active constraints are the box constraint so   the minimizer  of the LP  is not very sensitive to the random sample  However  as the sample size grows  so does the number of active simplex and stationarity constraints  hence  the random constraints affect  to a greater degree and the variance increases      Conclusions  In this paper  we defined and solved the extended large scale efficient ALP problem  We proved that  under certain assumptions about the dynamics  the stochastic subgradient method produces a policy with average loss competitive to all     not just all  producing a stationary distri   bution  We demonstrated this algorithm on the Rybko Stoylar four dimensional queueing network  and recovered a policy better than two common heuristics and comparable to previous results on ALPs de Farias and Van Roy      a   A future direction is to find other interesting regularity conditions under which we can handle large scale MDP problems  We also plan to conduct more experiments with challenging large scale problems      Acknowledgements  We gratefully acknowledge the support of the NSF through grant CCF         and of the ARC through an Australian Research Council Australian Laureate Fellowship  FL             
