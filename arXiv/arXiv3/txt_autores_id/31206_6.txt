  very computationally efficient Monte Carlo algorithm for the calculation of Dempster Shafer belief is described  If Bel is the combination using Dempster s Rule of belief functions Bel           Belm then  for sub set b of the frame    Bel b  can be calcu lated in time linear in     and m  given that the weight of conflict is bounded   The algo rithm can also be used to improve the com plexity of the Shenoy Shafer algorithms on Markov trees  and be generalised to calculate Dempster Shafer Belief over other logics  A     INTRODUCTION  One of the major perceived problems with application of the Dempster Shafer Theory  Shafer      has been its apparent computational complexity e g     Kyburg        Bonissone       This is because the Dempster Shafer theory as usually implemented involves re peated application of Dempster s Rule of Combina tion  keeping a record at each stage of each subset of   with a non zero mass  For example the combination of m simple support functions can have as many as  q non zero masses where q is the minimum of m and      thus making the approach computationally infeasible for large m and      There have been a number of schemes to deal with this    Barnett      showed how calculation of Dempster Shafer belief in a very special case  when all the ev idence sets are either singletons or complements of singletons  belief could be calculated in linear time    Gordon and Shortliffe      extended this with an effi cient approximation to Dempster Shafer belief for hier archically related evidences  and it was shown in both   Shafer and Logan      and  Wilson      that the hi erarchical case could be dealt with exactly in a com putationally efficient manner  The Shafer Logan al gorithm was generalised to propagation of belief func tions in Markov Trees   Shafer and Shenoy      but   although this is a very important contribution  it still requires that the product space associated with the largest clique is small  a condition which will by no means always be satisfied  The hierarchical evidence algorithm in   Wilson      was generalised to arbitrary evidence sets  Wilson      and  because it calculates belief directly without first calculating the masses  it leads to very substantial increases in efficiency  see sec tion     However this algorithm appears to have com plexity worse than polynomial  which is not surprising since Dempster s Rule is    complete  Orponen         P rovan       This paper describes the Monte Carlo algorithm given in  Wilson      which also calculates belief directly  or  more accurately  it approximates belief up to arbi trary accuracy   This calculation has very low com plexity  showing that the general pessimism about the complexity of Dempster Shafer Theory is mis guided  The use of Monte Carlo algorithms for calcu lating Dempster Shafer belief has also been suggested in  P earl         Kampke      and   Kreinovich and Bar rett          THE MONTE CARLO ALGORITHM  Let Belt        Belm be belief functions on a finite frame    and let Bel  Belt Etl EtlBelm be their combination using Dempster s Rule  Using the model of   Dempster      Bel  is represented by a probability function P   on a finite set rl   and a compatibility function f    rl         where the meaning of f  is  for r E rl   if r is true then so is f   r      The mass function m  is given by  m  f  c     P  ci   and  for b S       Bel  b   P  f  c   s   b   that is   for c  E rl    I   P  c        r     b  Let n   nl X   X Slm and for c    c        c   define f  c    n    f  c     Define the  independent        nte C arlo Algorithm for Dempster Shafer Belief  A Mo  probability function  P  on f  by P   c         c m      n    P  c      Using   Dempster      it can be seen that Bel b   P  r c       blf c          where e g  P  r c        just means L  r        P  c    r c   can be viewed as a random set  Nguyen       The Monte Carlo algorithm just simulates the last equation  A large number  N  of trials are performed  For each trial  Randomly pick c  such that f c        a  For i           m randomly pick an element of f    i e  pick c  with probability P  c   Let c    c         em   b  If r c       then restart trial     If f c       b then trial succeeds  letT    else trial fails  let T             The proportion of trials that succeed converges to Bel  b   E T    P  r c       bll  c          Bel b   Var T    E T    E T      E T   E T      Bel b   Bel b    S  t Let f be the average value ofT over the N trials  i e   the proportion of trials that succeed  N T     Bel b  NVar TJ              N N   E f      and Var t      Therefore the variance  an    so also the standard de viation  for the estimate  T  of Bel b  can be made arbitrarily small independently of     and m  Let us say that the estimate t  of Bel b   has accuracy k  if   standard deviations ofT is less than or equal to k  Then f has accuracy k if N           Testing separately if r c       and if f  c       b wastes time  these tests can be combined within the same algorithm  where Xj denotes the jth element of     For each trial  repeat  pick c  with probability P  c        T      T    for j     to     if  f c      Xj then  T       if Xj      tf  b   since f c              exit trial   then T     end if end if next j until T           since r c   b   COMPUTATION TIME  Picking c  involves m random numbers so takes less than Am where A is constant  approximately the time it takes to generate a random number  with efficient storing of the P s   Testing if r c     Xj takes less than Bm for constant B  For a given trial there is a prob ability K   P  r c        that the repeat until loop will be entered a second time  The expected number of repeat until loops per trial is    K is a measure of the conflict of the evidences  Shafer      p     Thus the expected time the algorithm takes is less than   m A  Bl     and so the expected time to achieve accuracy k is less than       pm A   Bl     At least for the case where the Bel s are simple sup port functions  the condition f c    Xj can be tested more efficiently  under weak conditions this leads to expected time of less than       k   Am  Cl    for constant C  Wilson          EXPERIMENTAL RESULTS  The algorithm for the case where the Bel s are simple support functions has been implemented and tested using the language Modula   on a SUN      worksta tion  The results showed that the value of A is much bigger than the value of C in this implementation  A being roughly     seconds and C roughly       sec time taken to geerate a onds  A is essentially the   random number  and      seconds seems rather slow for that  This suggests that very substantial speed ups  of perhaps an order of magnitude or two  could be achieved by careful choice and use of the random number generator and the use of antithetic runs  so that the random number generator is only used once for several different data items   The results indicate that  unless the evidences are ex tremely conflicting  the Monte Carlo algorithm is prac tical for problems with large m and    For example  with K        m             and with      tri als  the calculation of the approximate value of Bel b  would be expected to take      seconds  The      trials mean that the standard deviation is less than        and so the confidence interval for the correct value of belief corresponding to   standard deviations would be roughly   b        b          If instead we did        trials this would take a little over   minutes  and give a standard deviation of        Extrapolating the figures  which seems unlikely to cause problems in             Wilson  this case  gives an approximate time of   minute for m              with      trials  and   minutes for m              Also in  Wilson      an exact algorithm for calcu lating belief is described  related to those described in  P rovan       which involves expressing the event f E  s   b as a boolean expression and then calculat ing the probability of this using the laws of boolean algebra  Again this avoids explicit calculation of the masses  The complexity for the simple support func tion case appears to be approximately of the form  l llogm   The usual approaches for calculating belief are mass based  they calculate the combined mass function and use this to calculate the appropriate belief  a good one of these is the fast Mobius transform in  Kennes and Smets        For large m and   this is of necessity very computationally expensive  since if q   min m        there can be as many as  q masses  For simplicity it is assumed that the calculation of belief then just does  q REAL multiplications  The speed of REAL multipli cation was tested on the same workstation and within the same language that the Exact and Monte Carlo al gorithms were tested and implemented on and it was found that it did just over     REAL multiplications per second  This gives the following results  m n    X    X    X    X    X    X                     MC   sees    sees    sees    sees    sees    sees  Mass based      sees   min   hour   day   month      years  Exact   sees    sees    sees   mins   mins   hours  The values for the Monte Carlo algorithm were based on doing      trials and the contradiction being      The figure of   hours for the Exact in the    case is a very rough upper bound derived from insufficient data  Details of the experiments and the full results and analysis are given in  Wilson    b       THE GENERALISED   or f  El          Em     the set  f  E   the logic doesn t have conjunction       i              m  if  For each trial     Randomly pick E such that f E  is not contradictory  a  For i              m randomly pick an element of ll   i e  pick E  with probability P  t    Let E               Em   b  If f c   is contradictory then restart trial     If b can be deduced from r c   then trial succeeds  let T     else trial fails  let T      Undecidability and semi decidability would clearly cause problems  in which case trials which went on for too long would have to be cut short  if T for these trials was given the value   then this would lead to a lower bound for Bel b   This technique of prematurely halting trials that take too long could be used to in crease the efficiency for other cases as well  at the cost of only finding lower and upper bounds for Bel b   The time this algorithm takes is then approximately  Am   R  where R is the average time it takes to  see if f c   is contradictory  and if f c   allows b to be deduced  Given that the weight of conflict of the ev idences is bounded this means that the complexity is proportional to that of proof in the logic  it is hard to see how any sensible uncertainty calculus could do bet ter than this  although the complexity for this Monte Carlo algorithm has a very large constant term if high accuracy is required   As Shafer points out  Shafer          can be a large product space  making the first algorithm impractical  The generalised algorithm can also be used to greatly improve the complexity of the algorithms for calcu lating Belief in Markov trees  Shafer and Shenoy       For each trial  propositions  i e  belief functions with a single focal element  must be propagated through the Markov tree  The complexity is then proportional to that of propagating propositions  rather than the whole belief functions  Some other propositional cases have been dealt with in  Wilson        ALGORITHM The algorithm can be generalised to deal with arbi trary logics  Wilson    a   Let L be the language of some logic  For each i  Bel  is now a function from L to        saying how much the evidence warrants belief in propositions in L and the compatibility function is a function f    ll  f   L  The combined compatibility function r is now defined by  f  El       Em      m     f  E    i l     DISCUSSION  There are two obvious drawbacks with the Monte Carlo algorithm   i  if very high accuracy is required then the Monte Carlo algorithm will require a large number of trials  quadratic in the reciprocal of accuracy  so giving a very high constant factor to the complexity   ii  when the evidence is highly conflicting the Monte Carlo algorithm loses some of its efficiency  I don t see   A Monte Carlo Algorithm for Dempster Shafer Belief  this as a great problem since an extremely high weight of conflict would suggest  except in exceptional circum stances  that Dempster s Rule is being applied when it is not valid  e g  updating a Bayesian prior with a Dempster Shafer belief function  see Wilson       I also argue there that  although Dempster s Rule has strong justifications for the combination of a finite number of simple support functions  the more general case has not been convincingly justified  the Monte Carlo algo rithm is guaranteed to give results in accordance with Dempster s Rule  but it remains to be seen if these are always sensible  It may be important to know which relatively small sets have relatively high beliefs  the Monte Carlo algo rithm can be easily applied to deal with this problem  Dempster s Rule makes particular independence as sumptions  using a single probability function on    By modifying step   of the algorithms the beliefs cor responding to other probability functions on n can be calculated   Acknowledgements I am currently supported by the ESPRIT basic re search action DRUMS         Most of the material in this paper was produced in the period Summer    Summer     when I was employed by the The Hotel and Catering Management  and Computing and Math ematical Sciences Departments of Oxford Polytechnic  Thanks also to Bills Triggs and Boatman for their help during this period  and more recently to Mike Clarke   
 We investigate the computational complexity of testing dominance and consistency in CP nets  Previously  the complexity of dominance has been determined for restricted classes in which the dependency graph of the CP net is acyclic  However  there are preferences of interest that define cyclic dependency graphs  these are modeled with general CP nets  In our main results  we show here that both dominance and consistency for general CP nets are PSPACE complete  We then consider the concept of strong dominance  dominance equivalence and dominance incomparability  and several notions of optimality  and identify the complexity of the corresponding decision problems  The reductions used in the proofs are from STRIPS planning  and thus reinforce the earlier established connections between both areas      Introduction The problems of eliciting  representing and computing with preferences over a multi attribute domain arise in many fields such as planning  design  and group decision making  However  in a multi attribute preference domain  such computations may be nontrivial  as we show here for the CP net representation  Natural questions that arise in a preference domain are  Is this item preferred to that one   and Is this set of preferences consistent  More formally  a set of preferences is consistent if and only if no item is preferred to itself  We assume that preferences are transitive  i e   if  is preferred to   and  is preferred to   then  is preferred to   An explicit representation of a preference ordering of elements  also called outcomes  of such multi variable domains is exponentially large in the number of attributes  Therefore  AI researchers have developed languages for representing preference orderings in a succinct way  The formalism of CP nets  Boutilier  Brafman  Hoos    Poole        is among the most popular ones  A CP net c      AI Access Foundation  All rights reserved    G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  provides a succinct representation of preference ordering on outcomes in terms of local preference statements of the form p   xi   x j   where xi   x j are values of a variable X and p is a logical condition  Informally  a preference statement p   xi   x j means that given p  xi is strictly preferred to x j ceteris paribus  that is  all other things being equal  The meaning of a CP net is given by a certain ordering relation  called dominance  on the set of outcomes  derived from such reading of preference statements  If one outcome dominates another  we say that the dominant one is preferred  Reasoning about the preference ordering  dominance relation  expressed by a CP net is far from easy  The key problems include dominance testing and consistency testing  In the first problem  given a CP net and two outcomes  and   we want to decide whether  dominates   The second problem asks whether there is a dominance cycle in the dominance ordering defined by an input CP net  that is  whether there is an outcome that dominates  is preferred to  itself  We study the computational complexity of these two problems  The results obtained prior to this work concerned only restricted classes of CP nets  all requiring that the graph of variable dependencies implied by preference statements in the CP net be acyclic  Under certain assumptions  the dominance testing problem is in NP and  under some additional assumptions  even in P  Domshlak   Brafman        Boutilier  Brafman  Domshlak  Hoos    Poole      a   We show that the complexity in the general case is PSPACE complete  and this holds even for the propositional case  by exhibiting in Section   a PSPACE hardness proof for dominance testing  We then turn to consistency testing  While acyclic CP nets are guaranteed to be consistent  this is not the case with general CP nets  Domshlak   Brafman        Brafman   Dimopoulos         In Section    we show that consistency testing is as hard as dominance testing  In the following two sections we study decision problems related to dominance and optimality in CP nets  First  we consider the complexity of deciding strict dominance  dominance equivalence and dominance incomparability of outcomes in a CP net  Then  we study the complexity of deciding the optimality of outcomes  and the existence of optimal outcomes  for several notions of optimality  To prove the hardness part of the results  we first establish the PSPACE hardness of some problems related to propositional STRIPS planning  We then show that these problems can be reduced to CP net dominance and consistency testing by exploiting connections between actions in STRIPS planning and preference statements in CP nets  The complexity results in this paper address CP nets whose dominance relation may contain cycles  Most earlier work has concentrated on the acyclic model  However  as argued earlier  for instance by Domshlak and Brafman         acyclic CP nets are not sufficiently expressive to capture human preferences on even some simple domains   Consider  for instance  a diner who has to choose either red or white wine  and either fish or meat  Given red wine  they prefer meat  and conversely  given meat they prefer red wine  On the other hand  given white wine  they prefer fish  and conversely  given fish they prefer white wine  This gives a consistent cyclic CP net  and there is no acyclic CP net giving rise to the same preferences on outcomes  So  such cyclicity of preference variables does not necessarily lead to a cyclic order on outcomes      We do not mean to say that cyclic CP nets are sufficient to capture all possible human preferences on simple domains  this is obviously not true  However  we note that every preference relation extends the preference relation induced by some CP net with possibly cyclic dependencies  Not only is this property no longer true when cyclic dependencies are precluded but  in the case of binary variables  the number of linear orders that extends some acyclic CP net is exponentially smaller than the number of all linear orders  Xia  Conitzer    Lang                T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS  We assume some familiarity with the complexity class PSPACE  We refer to Papadimitriou        for details  In particular  we later use the identities NPSPACE   PSPACE   coPSPACE  In several places  we will consider versions of decision problem  in which input instances are assumed to have some additional property  Such problems are usually formulated in the following way  Q  given R    We first note that Q  given R is not the same problem as Q and R  Let us recall the definition of a decision problem as presented by Ausiello et al          A decision problem is a pair P   hIP  YP i where IP is a set of strings  formally  a subset of    where  is a finite alphabet   The decision problem P   hIP  YP i reads as follows  given a string x  IP   decide whether x  YP   A problem hIP  YP i is in a complexity class C if the language YP   is in C  this does not depend on IP    A problem hIQ  YQ i is reducible to hIP  YP i if there is a polynomial time function F such that     for every x  IQ   F x   IP   and     for every x  IQ   x  YQ if and only if F x   YP   Thus  if P is the decision problem Q  given R  then IP is the set of all strings satisfying R  while YP is the set of all strings satisfying R  Q  For all such problems  it is granted that the input belongs to R  to solve them we do not have to check that the input string is indeed an element of R  Such problems Q  given R are widespread in the literature  However  in most cases  R is a very simple property  that can be checked in polynomial  and often linear  time  such as decide whether a graph possesses a Hamiltonian cycle  given that every vertex has a degree at most    Here  however  we will consider several problems Q  given R where R itself is not in the class P  unless the polynomial hierarchy collapses   However  as we said above  the complexity of recognizing whether a given string is in R does not matter  In other words  the complexity of Q  given R is the same  whether R can be recognized in unit time or is PSPACE complete  We will come back to this when the first such problem appears in the paper  cf  the proof of Proposition     In no case that we consider is the complexity of R greater than the complexity of Q  A part of this paper  up to Section    is an extended version of our earlier conference publication  Goldsmith  Lang  Truszczynski    Wilson         Sections   and   are entirely new      Generalized Propositional CP Nets Let V    x            xn   be a finite set of variables  For each variable x  V   we assume a finite domain Dx of values  An outcome is an n tuple  d            dn   of Dx       Dxn   In this paper  we focus on propositional variables  variables with binary domains  Let V be a finite set of propositional variables  For every x  V   we set Dx    x  x   thus  we overload the notation and write x both for the variable and for one of its values   We refer to x and x as literals  Given a literal l we write l to denote the dual literal to l  The focus on binary variables makes the presentation clearer and has no impact on our complexity results  We also note that in the case of binary domains  we often identify an outcome with the set of its values  literals   In fact  we also often identify such sets with the conjunctions of their elements  Sets  conjunctions  of literals corresponding to outcomes are consistent and complete  A conditional preference rule  sometimes  a preference rule or just a rule  over V is an expression p   l   l  where l is a literal of some atom x  V and p is a propositional formula over V that does not involve variable x     In the literature one often finds the following formulation  Q  even if R  which does not have exactly the same meaning as Q  given R  Specifically  when saying Q is NP complete  even if R  one means Q is NP complete  and Q  given R is NP complete as well         G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  In the rest of the paper  we need to refer to two different languages  a conditional preference language where for every  binary  variable x  the conditional preference table for x needs to specify a preferred value of x for every possible assignment of its parent variables  and a more general language where the tables may be incomplete  for some values of its parents  the preferred value of x may not be specified  and or locally inconsistent  for some values of its parents  the table may both contain the information that x is preferred and the information that x is preferred   We call these languages respectively CP nets and GCP nets  for generalized CP nets   Note that GCP nets are not new  as similar structures have been discussed before  Domshlak  Rossi  Venable    Walsh         The reason why we use this terminology  CP nets and GCP nets  is twofold  First  even if the assumptions of completeness and local consistency for CP nets are sometimes relaxed  most papers on CP nets do make them  Second  we could have used CP nets and locally consistent  complete CP nets instead of GCP nets and CP nets  but we felt our notation is simpler and more transparent  Definition    Generalized CP net  A generalized CP net C  for short  a GCP net  over V is a set of conditional preference rules  For x  V we define pC   x  and pC  x   usually written just  p   x  and p  x   as follows  pC   x  is equal to the disjunction of all p such that there exists a rule p   x   x in C  pC  x  is the disjunction of all p such that there exists a rule p   x   x in C  We define the associated directed graph GC  the dependency graph  over V to consist of all pairs  y  x  of variables such that y appears in either p   x  or p  x   In our complexity results we will also need the following representation of GCP nets  a GCPnet C is said to be in conjunctive form if C only contains rules p   l   l such that p is a  possibly empty  conjunction of literals  In this case all formulas p  x   p   x  are in disjunctive normal form  that is  a disjunction of conjunctions of literals  including   the empty conjunction of literals   GCP nets determine a transitive relation on outcomes  interpreted in terms of preference  A preference rule p   l   l represents the statement given that p holds  l is preferred to l ceteris paribus  Its intended meaning is as follows  If outcome  satisfies p and l  then  is preferred to the outcome  which differs from  only in that it assigns l to variable x  In this situation we say that there is an improving flip from  to  sanctioned by the rule p   l   l  Definition   If             m is a sequence of outcomes with m    and each next outcome in the sequence is obtained from the previous one by an improving flip  then we say that             m is an improving sequence from   to m for the GCP net  and that m dominates     written    m   Finally  a GCP net is consistent if there is no outcome  which is strictly preferred to itself  that is  such that     The main objective of the paper is to establish the complexity of the following two problems concerning the notion of dominance associated with GCP nets  sometimes under restrictions on the class of input GCP nets   Definition   GCP   DOMINANCE   given a GCP net C and two outcomes  and   decide whether    in C  that is  whether  dominates  in C  GCP   CONSISTENCY   given a GCP net C  decide whether C is consistent        T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS  GCP nets extend the notion of CP nets  Boutilier et al          There are two properties of GCP nets that are essential in linking the two notions  Definition   A GCP net C over V is locally consistent if for every x  V   the formula pC  x   pC   x  is unsatisfiable  It is locally complete if for every x  V   the formula pC  x   pC   x  is a tautology  Informally  local consistency means that there is no outcome in which both x is preferred over x and x is preferred over x  Local completeness means that  for every variable x  in every outcome either x is preferred over x or x is preferred over x  Definition    Propositional CP net  A CP net over the set V of  propositional  variables is a locally consistent and locally complete GCP net over V   It is not easy to decide whether a GCP net is actually a CP net  In fact  the task is coNPcomplete  Proposition   The problem of deciding  given a GCP net C  whether C is a CP net is coNPcomplete  Proof  Deciding whether a GCP net C is a CP net consists of checking local consistency and local completeness  Each of these tasks amounts to n validity tests  one for each variable   It follows that deciding whether a GCP net is a CP net is the intersection of  n problems from coNP  Hence  it is in coNP  itself  Hardness comes from the following reduction from UNSAT  To any propositional formula  we assign the CP net C    defined by its set of variables Var   z   where z   Var    and the following tables      for any variable x    z  pC    x      pC    x          z       z      pC    pC          z      There z   pC    x      moreover  pC    x   pC   For any variable x    z  we have pC      fore  C   is locally consistent  Now  for any variable x    z  we have pC    x   pC    x         Moreover  pC    z   pC    z      Thus  C   is locally complete if and only if  is unsatisfiable  It follows that C   is a CP net if and only if  is unsatisfiable     Many works on CP nets make use of explicit conditional preference tables that list every combination of values of parent variables  variables on which x depends  exactly once  each such combination designating either x or x as preferred   Clearly  CP nets in this restricted sense can be regarded as CP nets in our sense that  for every variable x  satisfy the following condition  if y            yk are all the atoms appearing in p   x  and p  x  then every complete and consistent conjunction of literals over  y            yn   appears as a disjunct in exactly one of p   x  and p  x      There are exceptions  Some are discussed for instance by Boutilier et al       a  in Section   of their paper         G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  Under this embedding  the concepts of dominance and consistency we introduced here for GCP nets generalize the ones considered for CP nets as defined by Boutilier et al       a   Problems CP   DOMINANCE and CP   CONSISTENCY are defined analogously to Definition    In the paper we are interested in the complexity of dominance and consistency problems for both GCPnets and CP nets  Therefore  the matter of the way in which these nets  especially CP nets  as for GCP nets there are no alternative proposals  are represented is important  Our representation of CP nets is often more compact than the one proposed by Boutilier et al       a   as the formulas p   x  and p  x  implied by the conditional preference tables can often be given equivalent  but exponentially smaller  disjunctive normal form representations  Thus  when defining a decision problem  it is critical to specify the way to represent its input instances  as the representation may affect the complexity of the problem  Unless stated otherwise  we assume that GCP nets  and thus  CP nets  are represented as a set of preference rules  as described in Definition    Therefore  the size of a GCP net is given by the total size of the formulas p  x   p   x   x  V   We now note a key property of consistent GCP nets  which we will use several times later in the paper  Proposition   If a GCP net C is consistent then it is locally consistent  Proof  If C is not locally consistent then there exists a variable x and an outcome  satisfying pC  x   pC   x   Then    can be shown by flipping x from its current value in  to the dual value and then flipping it back  since  satisfies pC  x   pC   x   and since pC  x   pC   x  does not involve any occurrences of x  both flips are allowed    Finally  we conclude this section with an example illustrating the notions discussed above  Example   Consider a GCP net C on variables V    x  y  with four rules  defined as follows  x   y   y  x   y   y  y   x   x  y   x   x  We have p   y    x  p  y    x  p   x    y and p  x    y  Therefore C is locally consistent and locally complete  and so is a CP net  There is a cycle of dominance between outcomes  x  y  x  y  x  y  x  y  x  y  and so C is inconsistent  This shows that consistency is a strictly stronger property than local consistency      Propositional STRIPS Planning In this section we derive some technical results on propositional STRIPS planning which form the basis of our complexity results in Sections   and    We establish the complexity of plan existence problems for propositional STRIPS planning under restrictions on input instances that make the problem of use in the studies of dominance and consistency in GCP nets  Let V be a finite set of variables  A state over V is a complete and consistent set of literals over V   which we often view as the conjunction of its members  A state is therefore equivalent to an outcome  defined in a CP nets context  Definition    Propositional STRIPS planning  By a propositional STRIPS instance we mean a tuple hV        ACTi  where    V is a finite set of propositional variables        T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS       is a state over V   called the initial state      is a state called the goal      ACT is a finite set of actions  where each action a  ACT is described by a consistent conjunction of literals pre a   a precondition  and a consistent conjunction of literals post a   a postcondition  or effect    An action a is executable in a state  if     pre a   The effect of a in state   denoted by eff  a     is the state  containing the same literals as  for all variables not mentioned in post a   and the literals of post a   We assume that an action can be applied to any state  but that it does not change the state if its preconditions do not hold  if      pre a   given that states are complete  this is equivalent to     pre a   then eff  a        This assumption has no influence as far as complexity results are concerned  The PROPOSITIONAL STRIPS PLAN EXISTENCE problem  or STRIPS PLAN for short  is to decide whether for a given propositional STRIPS instance hV        ACTi there is a finite sequence of actions leading from the initial state   to the final state   Each such sequence is a plan for hV        ACTi  A plan is irreducible if every one of its actions changes the state  We assume  without loss of generality  that for any action a  no literal in post a  appears also in pre a   otherwise we can omit the literal from post a  without changing the effect of the action  if post a  then becomes an empty conjunction  the action a can be omitted from ACT as it has no effect  We have the following result due to Bylander         Proposition    Bylander         STRIPS PLAN  is PSPACE complete   Typically  propositional STRIPS instances do not require that goals be states  Instead  goals are defined as consistent conjunctions of literals that do not need to be complete  In such a setting  a plan is a sequence of actions that leads from the start state to a state in which the goal holds  We restrict consideration to complete goals  This restriction has no effect on the complexity of the plan existence problem  it remains PSPACE complete under the goal completeness restriction  Lang             Acyclic STRIPS Definition    Acyclic sets of actions  A set of actions ACT  we use the same notation as in Definition    is acyclic if there is no state  such that hV      ACTi has a non empty irreducible plan  that is to say  if there are no non trivial directed cycles in the graph on states induced by ACT  We will now establish the complexity of the following problem  ACTION   SET ACYCLICITY    given a set ACT of actions  decide whether ACT is acyclic   Proposition   ACTION   SET ACYCLICITY is PSPACE complete     Note that in standard STRIPS the goal can be a partial state  This point is discussed just after Proposition       We emphasize that we allow negative literals in preconditions and goals  Some definitions of STRIPS do not allow this  This particular variant of STRIPS is sometimes called PSN  propositional STRIPS with negation  in the literature         G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  Proof  The argument for the membership in PSPACE is standard  we nevertheless give some details  We will omit such details for further proofs of membership in PSPACE  The following nondeterministic algorithm decides that ACT has a cycle  guess             repeat guess an action a  ACT       eff  a          until        This algorithm works in nondeterministic polynomial space  because we only need to store      and     which shows that ACTION   SET ACYCLICITY is in NPSPACE  and therefore in PSPACE  since NPSPACE   PSPACE  Thus  ACTION   SET ACYCLICITY is in coPSPACE  hence in PSPACE  since coPSPACE   PSPACE  We will now show that the complement of the ACTION   SET ACYCLICITY problem is PSPACEhard by reducing the ACYCLIC STRIPS PLAN problem to it  Let PE   hV        ACTi be an instance of the ACYCLIC STRIPS PLAN problem  In particular  we have that ACT is acyclic  Let a be a new action defined by pre a     and post a        It is easy to see that ACT   a  is not acyclic if and only if there exists a plan for PE  Thus  the PSPACEhardness of the complement of the ACTION   SET ACYCLICITY problem follows from Proposition    Consequently  the ACTION   SET ACYCLICITY problem is coPSPACE hard  Since PSPACE   coPSPACE  the ACTION   SET ACYCLICITY problem is PSPACE hard  as well    Next  we consider the STRIPS planning problem restricted to instances that have acyclic sets of actions  Formally  we consider the following problem  ACYCLIC STRIPS PLAN   Given a propositional STRIPS instance hV        ACTi such that ACT is acyclic and        decide whether there is a plan for hV        ACTi  This is the first of our problems of the form Q  given R that we encounter and it illustrates well the concerns we discussed at the end of the introduction  Here  R is the set of all propositional STRIPS instances hV        ACTi such that ACT is acyclic  and Q is the set of all such instances for which there is a plan for hV        ACTi  Checking whether a given propositional STRIPS instance is actually acyclic is itself PSPACE complete  this is what Proposition   states   but this does not matter when it comes to solving ACYCLIC STRIPS PLAN  when considering an instance of ACYCLIC STRIPS PLAN   we already know that it is acyclic  and this is reflected in the reduction below   Proposition   ACYCLIC STRIPS PLAN  is PSPACE complete   Proof  The argument for the membership in PSPACE is standard  cf  the proof of Proposition     To prove PSPACE hardness  we first exhibit a polynomial time reduction F from STRIPS PLAN  Let PE   hV        ACTi be an instance of STRIPS PLAN  The idea behind the reduction is to introduce a counter  so that each time an action is executed  the counter is incremented  The counter may count up to  n   where n    V    making use of n additional variables  The counter is initialized to       T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS     Once it reaches  n    it can no longer be incremented and no action can be executed  Hence  the set of actions in the resulting instance of STRIPS PLAN is acyclic  we are guaranteed to produce an instance of R  To describe the reduction  we write V as  x            xn    We define F PE    PE   hV           ACT  i as follows   V     x            xn   z            zn    where zi are new variables we will use to implement the counter          z       zn         z       zn    for each action a  ACT  we include in ACT  n actions ai      i  n  such that    pre ai     pre a   zi  zi        zn  for i  n      post ai     post a   zi  zi        zn   and   pre an     pre a   zn  for i   n   post an     post a   zn    Furthermore  we include in ACT  n actions bi      i  n  such that    pre bi     zi  zi        zn  for i  n      post bi     zi  zi        zn   and   pre bn     zn  for i   n   post bn     zn   We will denote states over V  by pairs    k   where  is a state over V and k is an integer     k   n     We view k as a compact representation of a state over variables z            zn   assuming that the binary representation of k is d        dn  with dn being the least significant digit   k represents the state which contains zi if di     and zi   otherwise  For instance  let V    x    x    x     Then we have V     x    x    x    z    z    z     and the state x   x   x   z   z   z  is denoted by  x   x   x        We note that the effect of ai or bi on state    k  is either void  or increments the counter  eff  ai      k          eff  a     k      if ai is executable in    k     k  otherwise  eff  bi      k            k      if bi is executable in    k     k  otherwise  Next  we remark that at most one ai and at most one bi are executable in a given state    k   More precisely   if k    n     then exactly one bi is executable in    k   denote by i k  the index such that bi k  is executable in    k   this index depends only on k   We also have that ai k  is executable in    k   provided that a is executable in    if k    n     then no ai and no bi is executable in    k         G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  Now we show that PE is acyclic  Assume  is an irreducible plan for hV          ACT  i  Let      k   If k    n     then  is empty  since any action in ACT  in any state either is nonexecutable or increments the counter  and an irreducible plan contains only actions whose effect is non void  If k    n     then no action of ACT  is executable in  and again  is empty  Thus  there exists no non empty irreducible plan for hV          ACT  i  and this holds for all    Therefore PE is acyclic  We now claim that there is a plan for PE if and only if there is a plan for PE   First  assume that there is a plan in PE  Let  be a shortest plan in PE and let m be its length  the number of actions used   We have m   n     since no state along  repeats  otherwise  shorter plans than  for PE would exist   Let                 m    be the sequence of states obtained by executing   Let a be the action used in the transition from k to k     Since k    n     because m   n    and k  m      there is exactly one i     i  n  such that the action ai applies at the state    k  over V    Replacing a with ai in  yields a plan that when started at         leads to  m   m       m   Appending that plan with appropriate actions bi to increment the counter to  n    yields a plan for PE   Conversely  if  is a plan for PE   the plan obtained from  by removing all actions of the form b j and replacing each action ai with a is a plan for PE  since ai has the same effect on V as a does  Thus  the claim follows      We emphasize that this reduction F from STRIPS PLAN to ACYCLIC STRIPS PLAN  or  equivalently  to STRIPS PLAN given ACTION   SET ACYCLICITY  works because it satisfies the following two conditions     for every instance PE of STRIPS PLAN  F PE  is an instance of ACYCLIC STRIPS PLAN  this holds because for every PE  F PE  is acyclic      for every PE of STRIPS PLAN  F PE  is a positive instance of ACYCLIC only if PE is a positive instance of STRIPS PLAN   STRIPS PLAN  if and      Mapping STRIPS Plans to Single Effect STRIPS Plans Versions of the STRIPS PLAN and ACYCLIC STRIPS PLAN problems that are important for us allow only actions with exactly one literal in their postconditions in their input propositional STRIPS instances  We call such actions single effect actions   We refer to the restricted problems as SE STRIPS PLAN and ACYCLIC SE STRIPS PLAN   respectively  To prove PSPACE hardness of both problems  we describe a mapping from STRIPS instances to single effect STRIPS instances   Consider an instance PE   hV        ACTi of the STRIPS PLAN problem  where ACT is not necessarily acyclic  For each action a  ACT we introduce a new variable xa   whose intuitive meaning is that action a is currently being executed  V We set X   aACT xa   That is  X is the conjunction of negative literals of all the additional V variables  In addition  for each a  ACT we set Xa   xa  bACT a  xb   We now define an instance PE   hV           S ACT i of the SE STRIPS PLAN problem as follows     Such actions are also called unary actions in the planning literature  We stick to the terminology single effect although it is less commonly used  simply because it is more explicit     PSPACE completeness of propositional STRIPS planning with single effect actions was proved already by Bylander         However  to deal with acyclicity we need to give a different reduction than the one used in that paper         T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS   Set of variables  V    V   xa   a  ACT    initial state         X   goal state       X   set of actions  S ACT     ai   a  ACT  i                post a         Let a be an action in ACT such that post a    l       lq   where l            lq are literals   For i              q  we define an action ai by setting  pre ai     pre a   X  li   post ai     xa   The role of ai is to enforce that Xa holds after ai is successfully applied  and in this way to enable starting the execution of a  provided that no action is currently being executed  that the ith effect of a is not already true  and that the precondition of a is true   For i   q               q  we define action ai by setting  pre ai     Xa   post ai     li   The role of ai is to make the ith effect of a true   Finally  we define a q   by setting  pre a q       Xa  l       lq   post a q       xa   Thus  a q   is designed so that X holds after a q   is successfully applied  that is  a q   closes the execution of a  thus allowing for the next action to be executed  Let  be a sequence of actions in ACT  We define S   to be the sequence of actions in S ACT  obtained by replacing each action a in  by a            a q     where q    post a    Now consider a sequence  of actions from S ACT   Remove from  every action ai such that i      post a        and replace actions of the form a  post a     by a  We denote the resulting sequence of actions from ACT by S     We note that S  S        The following properties then hold  Lemma   With the above definitions   i  if  is a plan for PE then S   is a plan for PE    ii  if  is an irreducible plan for PE then S    is an irreducible plan for PE   iii  ACT is acyclic if and only if S ACT  is acyclic  Proof   i  Let a  ACT be an action  let  be a state and let  be the state obtained from  by applying a  Let  be the V   state obtained by applying the sequence of actions ha            a q   i  where q    post a    to the state   X of PE   We will show that      X  We note that if for each i              q  state   X does not satisfy pre ai   then the sequence of actions ha            a q   i has no effect  so the state is still   X  For this to happen  either  doesnt satisfy pre a   or all of l            lq already hold in  so post a  holds in   In either case       and so      X        G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  Suppose now that for some i              q    does satisfy pre ai    Then the first such action causes xa and hence Xa to hold  After applying actions aq             a q   l       lq holds  and so post a  holds  After applying a q   both post a  and X hold  No other variable in V has changed  so      X  as required  Applying this result iteratively implies that if  is a plan for PE then S   is a plan for PE    ai   ii  Let  be an irreducible plan for PE   so that every action in  changes the state  which implies that every action in  is performed in a state where its precondition is true  We will show that S      When         S           too  and the assertion follows  is a plan for PE  We will assume that        j  Write the first action in  as a   where a  ACT  and let  be the maximal initial subsequence of  consisting of all actions of the form ai   We must have j   post a    since X holds in    by our assumption above  action a j does apply  and X is inconsistent with the precondition of ai for each i    post a    Also  pre a j   and l j hold in   and so  in   as well  Thus    satisfies pre a   and applying a changes the state  since l j holds in   and post a     l j   Let us denote by  the state resulting from applying a to     As we noted          Let  be the state resulting after applying  to     If  is the goal state  then X holds in    If  is not the goal state then        Let bi be the action in  directly following the last action in    By the definition of    a    b  After applying a j   Xa holds  so in  either Xa holds or X holds  Thus  Xb does not hold  as a    b  Since bi changes the state  i must be in              post b     so X holds in  in this case  too  Hence the last action in  is a q     where q    post a    Since the only variables in V which can be affected by actions ai are those that appear in the literals in post a  and since the action a q   can be executed  otherwise it would not belong to    it follows that      X  Applying this reasoning repeatedly  we show that applying S    to   yields   and that each action in S    changes the state  so S    is an irreducible plan for PE  which is non empty if and only if  is non empty   iii  Suppose ACT is not acyclic  so that there exists state  and a non empty irreducible plan  for PE   hV      ACTi  Then  by  i   S   is a plan for PE   hV      X    X  S ACT  i  Because  is non empty and irreducible  it changes some state  so S   also changes some state  and hence can be reduced to a non empty irreducible plan for PE   Therefore S ACT  is not acyclic  Conversely  suppose that S ACT  is not acyclic  Then there exists a state  and a non empty irreducible plan  for hV          S ACT i  We will first prove that X holds at some state obtained during the execution of this plan    By Suppose that X holds at no such state  and let a j be the first action in   We note that        our assumption  X does not hold either before or after applying a j   Therefore q      j   q  where q    post a    Since  is irreducible  a j changes the state  Thus  l j holds in  and l j holds in the state resulting from  after applying a j   By our assumption  Xa holds before and after applying a j   Thus  the next action  if there is one  must also be of the form ai for q      i   q  Repeating this argument implies that all actions in  are of the form ai where q      i   q  Since the set of literals in post a  is consistent  l j is never reset back to l j   Thus  the state resulting from  after applying  is different from    a contradiction  Thus  X holds at some state reached during the execution of   Let us consider one such state  It can be written as   X  for some state  over V   We can cyclically permute  to generate a non empty irreducible plan  for hV      X    X  S ACT i  By part  ii   S     is a non empty       T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS  irreducible plan for hV      ACTi  Therefore ACT is not acyclic      Proposition   SE STRIPS PLAN  and ACYCLIC SE STRIPS PLAN are PSPACE complete   Proof  Again  the argument for the membership in PSPACE is standard  PSPACE hardness of ACYCLIC SE STRIPS PLAN is shown by reduction from ACYCLIC STRIPS PLAN   The same construction shows that STRIPS PLAN is reducible to SE STRIPS PLAN  and thus SE STRIPS PLAN is PSPACE complete  Let us consider an instance PE   hV        ACTi of ACYCLIC STRIPS PLAN  We define PE    hV          S ACT i  which by Lemma   iii  is an instance of the ACYCLIC SE STRIPS PLAN problem  By Lemma   i  and  ii  there exists a plan for PE if and only if there exists a plan for PE   This implies that ACYCLIC SE STRIPS PLAN is PSPACE hard        Dominance The goal of this section is to prove that the GCP   DOMINANCE problem is PSPACE complete  and that the complexity does not go down even when we restrict the class of inputs to CP nets  We use the results on propositional STRIPS planning from Section   to prove that the general GCP DOMINANCE problem is PSPACE complete  We then show that the complexity does not change if we require the input GCP net to be locally consistent and locally complete  The similarities between dominance testing in CP nets and propositional STRIPS planning were first noted by Boutilier et al          They presented a reduction  discussed later in more detail by Boutilier et al       a   from the dominance problem to the plan existence problem for a class of propositional STRIPS planning specifications consisting of unary actions  actions with single effects   We prove our results for the GCP   DOMINANCE and GCP   CONSISTENCY problems by constructing a reduction in the other direction  This reduction is much more complex than the one used by Boutilier et al          due to the fact that CP nets impose more restrictions than STRIPS planning  Firstly  STRIPS planning allows multiple effects  but GCP nets only allow flips x   x or x   x that change the value of one variable  this is why we constructed the reduction from STRIPS planning to single effect STRIPS planning in the last section  Secondly  CP nets impose two more restrictions  local consistency and local completeness  which do not have natural counterparts in the context of STRIPS planning  For all dominance and consistency problems we consider  the membership in PSPACE can be demonstrated similarly to the membership proof of Proposition    namely by considering nondeterministic polynomial space algorithms consisting of repeatedly guessing appropriate improving flips and making use of the fact that PSPACE   NPSPACE   coPSPACE  Therefore  from now on we only provide arguments for the PSPACE hardness of problems we consider      Dominance for Generalized CP Nets We will prove that the GCP   DOMINANCE problem is PSPACE complete by a reduction from the problem SE STRIPS PLAN  which we now know to be PSPACE complete        G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON        M APPING S INGLE  E FFECT STRIPS P ROBLEMS P ROBLEMS  TO  GCP N ETS D OMINANCE  Let hV        ACTi be an instance of the SE STRIPS PLAN problem  For every action a  ACT we denote by la the unique literal in the postcondition of a  that is  post a    la   We denote by pre  a  the conjunction of all literals in pre a  different from la  we recall that by a convention we adopted earlier  pre  a  does not contain la    We then define ca to be the conditional preference rule pre  a    la   la and define M ACT  to be the GCP net C    ca   a  ACT   which is in conjunctive form  A sequence of states in a plan corresponds to an improving sequence from   to   which leads to the following result  Lemma   With the above notation   i  there is a non empty irreducible plan for hV        ACTi if and only if  dominates   in M ACT    ii  ACT is acyclic if and only if M ACT  is consistent  Proof  We first note the following equivalence  Let a be an action in ACT  and let  and  be different outcomes  or  in the STRIPS setting  states   The action a applied to  yields  if and only if the rule ca sanctions an improving flip from  to   This is because a applied to  yields  if and only if  satisfies pre a  and  and  differ only on literal la   with  satisfying la and  satisfying la   This is if and only if  satisfies pre  a  and  and  differ only on literal la   with  satisfying la   and  satisfying la   This  in turn  is equivalent to say that rule ca sanctions an improving flip from  to   Proof of  i   Suppose first that there exists a non empty irreducible plan a            am for hV        ACTi  Let                 m    be the corresponding sequence of outcomes  and  for each i              m  action ai   when applied in state i    yields different state i   By the above equivalence  for each i              m  cai sanctions an improving flip from i  to i   which implies that                 m is an improving flipping sequence in M ACT   and therefore  dominates   in M ACT   Conversely  suppose that  dominates   in M ACT   so that there exists an improving flipping sequence                 m with m     and m     For each i              m  let cai be an element of M ACT  which sanctions the improving flip from i  to i   Then  by the above equivalence  action ai   when applied to state i  yields i  which is different from i     and so a            am is a non empty irreducible plan for hV        ACTi  Proof of  ii   ACT is not acyclic if and only if there exists a state  and a non empty irreducible plan for hV      ACTi  By  i  this is if and only if there exists an outcome  which dominates itself in M ACT   which is if and only if M ACT  is not consistent     Theorem   The GCP   DOMINANCE problem is PSPACE complete  Moreover  this remains so under the restrictions that the GCP net is consistent and is in conjunctive form  Proof  PSPACE hardness is shown by reduction from ACYCLIC SE STRIPS PLAN  Proposition     Let hV        ACTi be an instance of the ACYCLIC SE STRIPS PLAN problem  By Lemma   ii   M ACT  is a consistent GCP net in conjunctive form  Since        imposed in the definition of       T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS  the problem ACYCLIC SE STRIPS PLAN   there is a plan for hV        ACTi if and only if there is a non empty irreducible plan for hV        ACTi  which  by Lemma   i   is if and only if  dominates   in C    Theorem   implies the PSPACE completeness of dominance in the more general conditional preference language introduced by Wilson      b   where the conditional preference rules are written in conjunctive form      Dominance in CP Nets In this section we show that GCP   DOMINANCE remains PSPACE complete under the restriction to locally consistent and locally complete GCP nets  that is  CP nets  We refer to this restriction of GCP   DOMINANCE as CP   DOMINANCE   Consistency of a GCP net implies local consistency  Proposition     Therefore  the reduction in the proof of Theorem    from ACYCLIC SE STRIPS PLAN to GCP   DOMINANCE restricted to consistent GCP nets  is also a reduction to GCP   DOMINANCE restricted to locally consistent GCP nets  PSPACE hardness of ACYCLIC SE STRIPS PLAN  Proposition    then implies that GCP DOMINANCE restricted to locally consistent GCP nets is PSPACE hard  and  in fact  PSPACEcomplete since membership in PSPACE is easily obtained with the usual line of argumentation  We will show PSPACE hardness for CP   DOMINANCE by a reduction from GCP   DOMINANCE for consistent GCP nets        M APPING L OCALLY C ONSISTENT GCP N ETS  TO  CP N ETS  Let C be a locally consistent GCP net  Let V    x            xn   be the set of variables of C  We define   We define a GCP net C over V    which we V    V   y            yn    where  y            yn    V      will show is a CP net  To this end  for every z  V  we will define conditional preference rules q   z    z   z and q  z    z   z to be included in C by specifying formulas q   z  and q  z   First  for each variable xi  V   we set q   xi     yi and q  xi     yi   Thus  xi depends only on yi   We also note that the formulas q   xi   and q  xi   satisfy local consistency and local completeness requirements  Next  for each variable yi      i  n  we define ei    x   y          xi   yi      xi    yi           xn  yn    fi    ei  p   xi   and fi   ei  p  xi    Finally  we define q   yi     fi     fi  xi   and q  yi     fi    fi   xi    Thus  yi depends on every variable in V  but itself  We note that by the local consistency of C  formulas fi   fi      i  n  are unsatisfiable  Consequently  formulas q   yi    q  yi       i  n  are unsatisfiable  Thus  C is locally consistent        G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  Finally  q   yi    q  yi   is equivalent to fi   xi  fi  xi   so is a tautology  Thus  C is locally complete and hence a CP net over V    Let  and  be outcomes over  x            xn   and  y            yn    respectively  By  we denote the outcome over V  obtained by concatenating n tuples  and   Conversely  every outcome for C can be written in this way  Let  be an outcome over V   We define  to be the outcome over  y            yn   obtained by replacing in  every component of the form xi with yi and every component xi with yi   Then for every i     i  n      ei   Let s be a sequence             m of outcomes over V   Define L s  to be the sequence of V  outcomes                                  m m   Further  let t be a sequence                 m of V  outcomes with      and m     Define L  t  to be the sequence obtained from t by projecting each element in t to V and iteratively removing elements in the sequence which are the same as their predecessor  until any two consecutive outcomes are different   Lemma   With the above definitions   i  if s is an improving sequence for C from  to  then L s  is an improving sequence for C from  to    ii  if t is an improving sequence from  to  then L  t  is an improving sequence from  to    iii  C is consistent if and only if C is consistent  Proof  Let e   ni    xi  yi    The definitions have been arranged so that the GCP net C and the CP net C have the following properties   a  If e does not hold in an outcome  over V    then every improving flip applicable to  changes the value of some variable xi or yi so that xi  yi holds after the flip  Indeed  let us assume that there is an improving flip from  to some outcome  over V    If the flip concerns a variable xi   then xi  yi holds in   Consequently  xi  yi holds in    Thus  let us assume that the flip concerns a variable yi   If ei holds in  then  since e does not  xi  yi holds in   Thus  xi  yi holds in    If ei does not hold in  then neither fi  nor fi does  Thus  if xi  xi   respectively  holds in   yi  yi   respectively  holds in    Since the flip concerns yi   it follows that xi  yi holds in     b  No improving flip from  changes any variable xi   Indeed  for any variable xi   since e holds in   xi  yi holds in   too  Thus  no improving flip changes xi    c  There is an improving flip in C that changes variable yi in an outcome  if and only if there is an improving flip for the GCP net C from outcome  that changes variable xi   After applying the improving flip  changing variable yi   to   there is exactly one improving flip possible  It changes xi and results in an outcome   where  is the outcome over V resulting from applying to  the improving flip changing the variable xi   To prove  c   let us first assume that yi holds in  and observe that in such case xi holds in   too  It follows that q   yi   holds in  if and only if p   xi   holds in   Consequently  changing yi in  is an improving flip in C if and only if changing xi in  is an improving flip in C  The argument in the case when yi holds in  is analogous  but involves q  yi   and p  xi     Thus  the first part of  c  follows  V        T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS  Let  be the outcome obtained by applying an improving flip to xi in   It follows that the improving flip changing the value of yi in  results in the outcome   In this outcome  by  a   an improving flip must concern x j or y j such that x j  y j holds after the flip  Since for every j    i  x j  y j holds in   the only improving flips in  concern either xi or yi   By the local consistency of C   yi cannot be flipped right back  Clearly  changing xi is an improving flip that can be applied to   By our discussion  it is the only improving flip applicable in  and it results in the outcome   This proves the second part of  c   Proof of  i   The assertion follows by iterative application of  c   Proof of  ii   Suppose that t is an improving sequence                 m of V   outcomes with      and m     Since e holds in      b  implies that the first flip changes some variable yi   and  c  implies that the second flip changes variable xi to make xi  yi hold again  Hence   can be written as   By  c  there is an improving flip in C from outcome  changing variable xi   that is  leading from  to   Iterating this process shows that L  t  is an improving sequence from  to   Proof of  iii   Suppose that C is inconsistent  Then there exists some outcome  and an improving sequence s in C from  to   By  i   L s  is an improving sequence from  to   proving that C is inconsistent  Conversely  suppose that C is inconsistent  so there exists an improving sequence t for C from some outcome to itself  By  a   any improving flip applied to an outcome in which e does not hold increases  by one  the number of i such that xi  yi holds  This implies that e must hold in some outcome in t  because t is not acyclic  Write this outcome as   We can cyclically permute t to form an improving sequence t  from  to itself  Part  ii  then implies that L  t    is an improving flipping sequence for C from  to itself  showing that C is inconsistent     Theorem   CP   DOMINANCE is PSPACE complete  This holds even if we restrict the CP nets to being consistent  Proof  We use a reduction from PSPACE hardness of the GCP   DOMINANCE problem when the GCP nets are restricted to being consistent  Theorem     Let C be a consistent  and hence locally consistent  GCP net over V   and let  and  be outcomes over V   Consider the CP net C over variables V  constructed above  Lemma   i  and  ii  imply that  dominates  in C if and only if  dominates  in C   Moreover  C is consistent by Lemma   iii   Consequently  the hardness part of the assertion follows    Note that PSPACE hardness obviously remains if we require input outcomes to be different  because the reduction for Theorem   uses a pair of different outcomes  Notice the huge complexity gap with the problem of deciding whether there exists a nondominated outcome  which is only NP complete  Domshlak et al                    Consistency of GCP Nets In this section we show that the from Sections   and     GCP   CONSISTENCY       problem is PSPACE complete  using results   G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  Theorem   GCP   CONSISTENCY is PSPACE complete  This holds even under the restriction to GCP nets in conjunctive form  Proof  PSPACE hardness is shown by reduction from ACTION   SET ACYCLICITY  We apply function S from Section     followed by M from Section      This maps instances of ACTION   SET ACYCLICITY to instances of GCP   CONSISTENCY in conjunctive form  By Lemma   iii  and Lemma    ii   an instance of ACTION   SET ACYCLICITY is acyclic if and only if the corresponding instance of GCP   CONSISTENCY is consistent  proving the result    We now show that consistency testing remains PSPACE complete for CP nets  GCP nets that are both locally consistent and locally complete   Theorem    CP   CONSISTENCY  is PSPACE complete   Proof  We use a reduction from GCP   CONSISTENCY under the restriction that the GCP net is in conjunctive form  Let C be a GCP net in conjunctive form  We define a CP net C as follows  Because C is in conjunctive form  local consistency can be decided in polynomial time  as it amounts to checking the consistency of a conjunction of conjunctions of literals  If C is not locally consistent we set C to be a predetermined inconsistent but locally consistent CP net  such as in the example in Section    Otherwise  C is locally consistent and for C we take the CP net we constructed in Section      The mapping from locally consistent GCP nets to CP nets  described in Section      preserves consistency  Lemma    iii    Since local inconsistency implies inconsistency  Proposition     we have that the GCP net C is consistent if and only if the CP net C is consistent  Thus  PSPACE hardness of the CP   CONSISTENCY problem follows from Theorem          Additional Problems Related to Dominance in GCP Nets Having proved our main results on consistency of and dominance in GCP nets  we move on to additional questions concerning the dominance relation  Before we state them  we introduce more terminology  Let  and  be outcomes in a GCP net C  We say that  and  are dominance equivalent in C  written  C   if      or  C  and  C   Next   and  are dominance incomparable in C if       C  and C   Finally   strictly dominates  if  C  and  C   Definition   We define the following decision problems  SELF   DOMINANCE   given a GCP net C and an outcome   decide whether  C   that is  whether  dominates itself in C  STRICT DOMINANCE   given a GCP net C and outcomes  and   decide whether  strictly dominates  in C  DOMINANCE EQUIVALENCE   given a GCP net C and outcomes  and   decide whether  and  are dominance equivalent in C  DOMINANCE INCOMPARABILITY   given a GCP net C and outcomes  and   decide whether  and  are dominance incomparable in C        T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS  When establishing the complexity of these problems  we will use polynomial time reductions from the problem GCP   DOMINANCE  Let H be a GCP net with the set of variables V    x            xn    and let  be an outcome  We define a GCP net G      H    with the set of variables W   V   y  by setting the conditions for flips on variables xi   i              n  and y as follows     if xi      p  G  xi     pH  xi    y  p G  xi     pH  xi    y    if xi      p  G  xi     pH  xi    y  p G  xi     pH  xi    y    p  G  y        p G  y      The mapping   can be computed in polynomial time  Moreover  one can check that if H is a locally consistent GCP net     H    is also locally consistent  Finally  if H is a CP net     H    is a CP net  as well  For every V  outcome   we let       y and      y  We note that every W  outcome is of the form   or    To explain the structure of the GCP net G  we point out that there is an improving flip in G from   into   if and only if there is an improving flip in H from  to   thus  G restricted to outcomes of the form   forms a copy of the GCP net H   Moreover  there is an improving flip in G from  into  if and only if  agrees with  on exactly one more variable xi than  does  Finally  an improving flip moves between outcomes of different type if and only if it transforms  to     or   to  for some       We now formalize some useful properties of the GCP net G      H     We use the notation introduced above  Lemma   For every V  outcome    G   and  if         G    in other words    dominates every other W  outcome   Proof  Consider any V  outcome       Then   y C   y since  given y  changing a literal to the form it has in  is an improving flip  By the definition  we also have   y C   y and   y G   y  as        It follows that  G   and   G  G     Thus  the assertion follows     Lemma   For arbitrary V  outcome  different from   the following statements are equivalent      H        G          G           G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  Proof  By Lemma      G     Thus  the conditions     and     are equivalent           Clearly  recall our discussion about the structure of G   if there is an improving flip from  to  in H  then there is an improving flip from   to   in G  Thus  if there is an improving sequence in H from  to   there is an improving sequence in G from   to              Let us assume   G     and let us consider an improving sequence of minimum length from   to     By the minimality  no internal element in such a sequence is     Thus  no internal element equals  either  as the only improving flip from  leads to      Since an improving flip from  to   requires that      all outcomes in the sequence are of the form     By dropping y from each outcome in this sequence  we get an improving flipping sequence from  to  in H  Thus   H     Lemma   Let H be consistent and let  and  be different V  outcomes  Then    G   if and only if  H   Proof  Suppose there exists an improving sequence from   to itself  There must be an outcome in the sequence of the form   y  otherwise  dropping y in every outcome yields an improving sequence from  to  in H  contradicting the consistency of H   To perform an improving flip from y to y we need  to hold  which implies that   appears in the sequence  Thus    G     By Lemma     H   Conversely  let us assume that  H   Again by Lemma      G     By Lemma      G     Thus    G       The next construction is similar  Let H be a GCP net on variables V    x            xn    and let  be an outcome  We define a GCP net F      H    as follows  As before  we set W   V   y  to be the set of variables of F  We define the conditions for flips on variables xi   i              n  and y as follows       p  G  xi     pH  xi    y     p G  xi     pH  xi    y     p  G  y        p G  y      Informally  outcomes of the form   form in F a copy of H  There are no improving flips between outcomes of the form    There is an improving flip from   to  and  for every       from  to     In particular  if F is consistent then    H    is consistent  The mapping   can be computed in polynomial time and we also have the following property  Lemma   Let  be a V  outcome different from   Then the following conditions are equivalent      H      strictly dominates  in F     and  are not dominance incomparable in F        T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS  Proof  If there exists an improving sequence from  to  then the first improving flip in the sequence changes  to     Moreover  there is an improving flip from   to  if and only if      Thus   F  if and only if  H   Since  F  all three conditions are equivalent     Proposition   The following problems are PSPACE complete  SELF   DOMINANCE  STRICT INANCE   DOMINANCE EQUIVALENCE   and DOMINANCE INCOMPARABILITY    DOM    Proof  For all four problems  membership is proven easily as for the problems in earlier sections  For the PSPACE hardness proofs  we use the problem CP   DOMINANCE in a version when we required that the input CP net be consistent and the two input outcomes different  The problem is PSPACE hard by Theorem    Let H be a consistent CP net on a set V of variables  and let  and  be two different V  outcomes  By Lemma     H  can be decided by deciding the problem DOMINANCE EQUIVALENCE for   and   in the GCP net    H     Thus  the PSPACE hardness of DOMINANCE EQUIVALENCE follows  Next  the equivalence of Lemma      G     H   which holds due to consistency of H  shows that the problem SELF   DOMINANCE is PSPACE hard  Finally  by Lemma     H  can be decided either by deciding the problem STRICT DOMI NANCE for outcomes  and  in    H     or by deciding the complement of the problem DOM INANCE INCOMPARABILITY for  and  in the GCP net    H     It follows that STRICT DOM INANCE and DOMINANCE INCOMPARABILITY  the latter by the fact that coPSPACE PSPACE  are PSPACE complete      Corollary   The problems SELF   DOMINANCE and DOMINANCE EQUIVALENCE are PSPACE complete under the restriction to CP nets  The problems STRICT DOMINANCE and DOMINANCE IN COMPARABILITY remain PSPACE complete under the restriction to consistent CP nets  Proof  Since in the proof of Proposition   we have that H is a CP net  the claim for the first two problems follows by our remarks that the mapping   preserves the property of being a CP net  For the last two problems  we observe that since H in the proof of Proposition   is assumed to be consistent  F      H    is consistent  too  Thus  it is also locally consistent and the mapping F to F  we used for the proof of Theorem   applies  In particular  F  is a consistent CP net and has the following properties  implied by Lemma         strictly dominates  in F if and only if  strictly dominates  in F      and  are dominance incomparable in F if and only if  and  are dominance incomparable in F    Since F  is a consistent CP net  the claim for the last two problems follows  too         For STRICT DOMINANCE  the result could have been also obtained as a simple corollary of Theorem    since in consistent GCP nets dominance is equivalent to strict dominance         G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON     Problems Concerning Optimality in GCP Nets The dominance relation C of a GCP net C determines a certain order relation  which gives rise to several notions of optimality  We will introduce them and study the complexity of corresponding decision problems  We first observe that the dominance equivalence relation is indeed an equivalence relation  reflexive  symmetric and transitive   Thus  it partitions the set of all outcomes into non empty equivalence classes  which we call dominance classes  We denote the dominance class of an outcome  in a GCP net C by   C   The relation C induces on the set of dominance classes a strict order relation  a relation that is irreflexive and transitive   Namely  we define   C Cdc   C if   C      C  equivalently    C   and  C   One can check that the definition of the relation Cdc on dominance classes is independent of the choice of representatives of the classes  Definition    Non dominated class  optimality in GCP nets  Let C be a GCP net  A dominance class   C is non dominated if it is maximal in the strict order Cdc  there is no dominance class   C such that   C Cdc   C    A dominance class is dominating if for every dominance class   C     C     C or   C Cdc   C   An outcome  is weakly non dominated if it belongs to a non dominated class  If  is weakly non dominated and is the only element in its dominance class  then  is non dominated  An outcome  is dominating if it belongs to a dominating class  An outcome  is strongly dominating if it is dominating and non dominated  Outcomes that are weakly non dominated  non dominated  dominating and strongly dominating capture some notions of optimality  In the context of CP nets  weakly non dominated and nondominated outcomes were proposed and studied before  Brafman   Dimopoulos         They were referred to as weakly and strongly optimal there  Similar notions of optimality were also studied earlier for the problem of defining winners in partial tournaments  Brandt  Fischer    Harrenstein         We will study here the complexity of problems to decide whether a given outcome is optimal and whether optimal outcomes exist  First  we note the following general properties  simple consequences of properties of finite strict orders   Lemma   Let C be a GCP net     There exist non dominated classes and so  weakly non dominated outcomes     Dominating outcomes and nondominated outcomes are weakly non dominated     A strongly dominating outcome is dominating and non dominated     The following conditions are equivalent   a  C has a unique non dominated class   b  C has a dominating outcome   c  weakly non dominated and dominating outcomes in C coincide  For consistent GCP nets only two different notions of optimality remain        T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS  Lemma   Let C be a consistent GCP net  Then     Each dominance class is a singleton  C is a strict order  and C and Cdc coincide  modulo the one to one and onto correspondence      C      If  is a weakly non dominated outcome   is non dominated  weakly non dominated and non dominated outcomes coincide     If  is a dominating outcome   is strongly dominating  strongly dominating and dominating outcomes coincide      Finally   is a unique  weakly  non dominated outcome if and only if  is strongly dominating  Next  we observe that all concepts of optimality we introduced are different  To this end  we will show GCP nets with a single non dominated class that is a singleton  with multiple non dominated classes  each being a singleton  with a single non dominated class that is not a singleton  and with multiple non dominated classes  each containing more than one element  We will also show a GCPnet with two non dominated classes  one of them a singleton and the other one consisting of several outcomes  Example   Consider the following GCP net C with two binary variables a and b   a   a   b   b This GCP net determines a strict preorder on the dominance classes  in which  ab  is the only maximal class  in fact  all dominance classes are singletons   Thus  ab is both non dominated and dominating and so  it is strongly dominating  Example   Consider the following GCP net C with two binary variables a and b b   a   a b   a   a a   b   b a   b   b This GCP net determines a strict preorder  in which  ab  and  ab  are two different non dominated classes  Thus  ab and ab are non dominated and there is no dominating outcome  Example   Consider a GCP net with variables a  b and c  defined as follows  a   b   b a   b   b b   a   a b   a   a ab   c   c        G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  There are two dominance classes  Sc    abc  abc  abc  abc  and Sc    abc  abc  abc  abc   Every outcome in Sc strictly dominates every outcome in Sc   therefore  Sc is the unique non dominated class and every outcome in Sc is dominating  Because Sc is not a singleton  there are no nondominated outcomes  and so  no strongly dominating outcome  either   Example   Let us remove from the GCP net of Example   the preference statement ab   c   c  Then Sc and Sc are still the two dominance classes  but now every outcome is Sc is incomparable with any outcome in Sc   Thus  Sc and Sc are both non dominated  Since there are two non dominated classes  there is no dominating outcome  Since each class has more than one element  there are no non dominated outcomes  All outcomes are weakly non dominated  though  Example   Let us modify the GCP net of Example   by changing the preference statement b   a   a into bc   a   a  The dominance relation  of this GCP net satisfies the following properties   i  the four outcomes in Sc dominate each other   ii  abc  abc  abc  abc   iii  any outcome in Sc dominates abc  and  a fortiori  abc   One can check that there are five dominance classes  Sc    abc    abc    abc  and  abc   Two of them are non dominated  Sc and  abc   Since there are two nondominated classes  there is no dominating outcome  On the other hand   abc  is a non dominated outcome  a unique one   We will consider the following decision problems corresponding to the notions of optimality we introduced  Definition    For a given GCP net C  WEAKLY NON   DOMINATED OUTCOME   given an outcome   decide whether  is weakly nondominated in C NON   DOMINATED OUTCOME   given an outcome   decide whether  is non dominated in C DOMINATING OUTCOME   given an outcome   decide whether  is dominating in C STRONGLY DOMINATING OUTCOME   given an outcome   decide whether  is strongly dominating in C EXISTENCE OF A NON   DOMINATED OUTCOME   decide whether C has a non dominated outcome EXISTENCE OF A DOMINATING OUTCOME   decide whether C has a dominating outcome EXISTENCE OF A STRONGLY DOMINATING OUTCOME   decide whether C has a strongly dominating outcome  In some of the hardness proofs  we will again use the reductions   and     described in the previous section  We note the following additional useful properties of the GCP net G      H     Lemma    For arbitrary V  outcome  different from   the following statements are equivalent       G        is weakly non dominated in G      is a dominating outcome in G        T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS  Proof  Since   is dominating in G  Lemma     weakly non dominated outcomes and dominating outcomes coincide  Lemma     It follows that the conditions         are equivalent to each other     Proposition   The following problems are PSPACE complete  WEAKLY NON   DOMINATED OUTCOME and DOMINATING OUTCOME   The result holds also for the problems restricted to CP nets  Proof  The membership is easy to prove by techniques similar to those we used earlier  For the PSPACE hardness proofs  we use reductions from CP   DOMINANCE for consistent CPnets  in the version where the two input outcomes are different   Let H be a CP net  and  and  two different V  outcomes  By Lemmas   and      H  can be decided by deciding either of the problems WEAKLY NON   DOMINATED OUTCOME and DOMINATING OUTCOME for the GCPnet G      H    and the outcome     We observed earlier  that if H is a CP net  then so is G      H     Thus  the second part of the assertion follows    Next  we will consider the problem STRONGLY DOMINATING OUTCOME  We will exploit the reduction F      H     which we discussed in the previous section  We observe the following property of F  Lemma    Let H be a GCP net and F      H     Then  is strongly dominating in F if and only if  is dominating in H  Proof  Let us assume that  is dominating in H  From the definition of F  it follows that for every V  outcome         F   and  F     Since   F     is dominating in F  Since there is no improving flip leading out of     is strongly dominating  Conversely  let us assume that  is strongly dominating in F and let  be a V  outcome different from   Let us consider an improving sequence from   to    All outcomes in the sequence other than the last one     are of the form     Moreover  the outcome directly preceding  is     Dropping y from every outcome in the segment of the sequence between   and   yields an improving sequence from  to  in H    We now have the following consequence of this result  Proposition   The problem STRONGLY stricted to CP nets   DOMINATING OUTCOME  is PSPACE complete  even if re   Proof  Let H be a CP net  over the set V of variables  and  an outcome  By Lemma     the problem DOMINATING OUTCOME can be decided by deciding the problem STRONGLY DOMINATING OUTCOME for F      H    and    Thus  the PSPACE hardness of STRONGLY DOMINATING OUTCOME follows by Proposition    The membership in PSPACE is  as in other cases  standard and is omitted  Since H is a CP net  it is locally consistent and so  F is locally consistent  too  As in the proof of Corollary   we use the mapping from GCP net F to CP net F  defined in Section      By Lemma     is a strongly dominating outcome in F if and only if  dominates every outcome of the form   which is if and only if  is a strongly dominating outcome in F    since any F   outcome is dominated by an outcome of the form   using the rules q   xi     yi and q  xi     yi    Therefore       G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  for F and  can be decided by deciding for F  and   Thus  the second part of the claim follows   STRONGLY DOMINATING OUTCOME NATING OUTCOME  STRONGLY DOMI    The problem NON   DOMINATED OUTCOME is easier  It is known to be in P for CP nets  Brafman   Dimopoulos         The result extends to GCP nets  Indeed  if H is a GCP net and  an outcome   is non dominated if and only if there is no improving flip that applies to   The latter holds if and only if for every variable x in H  if x  respectively  x  holds in   then p  x   respectively  p   x   does not hold in   Since the conditions can be checked in polynomial the claim holds and we have the following result  Proposition    The problem NON   DOMINATED  OUTCOME  for GCP nets is in P   Next  we will consider the problems concerning the existence of optimal outcomes  Let H be a GCP net on the set of variables V    x            xn    and let  and  be two different V  outcomes  For every i                 n  we define formulas i as follows  If xi    then i is the conjunction of all literals in   except that instead of xi we take xi   Similarly  if xi    then i is the conjunction of all literals in   except that instead of xi we take xi   Thus  i is the outcome that results in  when the literal in corresponding to xi is flipped into its dual  We now define a GCP net E      H      by taking W   V   y  as the set of variables of E and by defining the flipping conditions as follows       p  E  xi      pH  xi    y    y    i     pE  xi     pH  xi    y     p  E  y        p E  y      The GCP net    H      has the following properties  The outcomes of the form        y  form a copy of H  There is no improving flip for the outcome       y   Next  there is no improving flip into  from an outcome of the form    To see this  let us assume that such a flip exists and concerns a variable  say  xi   It follows that    i   By the definition of flipping conditions  an improving flip for  that involves xi is impossible  a contradiction  Thus  the only improving flip that leads to  originates in     We also have that for every outcome  other than  and    E    It follows from the fact that for every outcome  other than  and    has an improving flip  Indeed  for each such  there is a variable xi such that  i  xi is false in   and  ii  flipping the literal of xi to its dual does not lead to   that is   is not i     For even if    i for some i  then  because         there exists i    i such that  and  differ on xi   so that xi satisfies  i  and  ii    Thus  a flip on that variable is improving  As all improving flips between outcomes containing y result in one more variable xi assigned to true  thus having the same status as it has in    E  follows  Finally  we have  E   and  for every outcome  other than     E    This leads to the following property of E      H       Lemma    Let H be a GCP net and let  and  be two different outcomes  Then  H  if and only if    H      has a  strongly  dominating outcome        T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS  Proof   Only if  Based on our earlier remarks    E    Moreover  since  H   we have   E     In addition  for every  different from  and     E  E  E     Thus   is both dominating and strongly dominating  the latter follows from the fact that no improving flips lead out of      If  Let us assume that  is dominating  and so  the argument applies also when  is strongly dominating   Then there is an improving sequence from   to    Let us consider a shortest such sequence  Clearly    is the outcome just before  in that sequence  as we pointed out  no improving flip from an outcome of the form  to  is possible   Moreover  by the definition of    H      and the fact that we are considering a shortest sequence from   to    every outcome in the sequence between   and   is of the form     By dropping y from each of these outcomes  we get an improving sequence from  to      Proposition    The problem EXISTENCE OF DOMINATING OUTCOME and the problem EXISTENCE OF STRONGLY DOMINATING OUTCOME are PSPACE complete  even if restricted to CP nets  Proof  We show the hardness part only  as the membership part is straightforward  To prove hardness we notice that by Lemma     given a consistent CP net H and two outcomes  and    H  can be decided by deciding either of the problems EXISTENCE OF DOMINATING OUTCOME and EXISTENCE OF STRONGLY DOMINATING OUTCOME for    H       To prove the second part of the assertion  we note that if H is consistent  E      H      is consistent  too and so  the mapping from locally consistent GCP nets to CP nets applies  Let us denote the result of applying the mapping to E by E    Then  using the same argument as in the proof of Proposition    E has a  strongly  dominating outcome if and only if E  has a strongly dominating outcome  Thus  one can decide whether  H  in a consistent CP net H by deciding either of the problems EXISTENCE OF DOM INATING OUTCOME and EXISTENCE OF STRONGLY DOMINATING OUTCOME for E      We also note that the problem EXISTENCE standard complexity theory assumptions    OF NON   DOMINATED OUTCOME  Proposition    The problem EXISTENCE OF NON   DOMINATED  OUTCOME  is easier  under  is NP complete   Proof  We note that in the case of GCP nets in conjunctive form the problem is known to be NP hard  Domshlak et al                Thus  the problem is NP hard for GCP nets  The membership in the class NP follows from Proposition       If we restrict to consistent GCP nets  the situation simplifies  First  we recall  Lemma    that if a GCP net is consistent then weakly non dominated and non dominated outcomes coincide  and the same is true for dominating and strongly dominating outcomes  Moreover  for consistent GCP nets  non dominated outcomes exist  and so  the corresponding decision problem is trivially in P   Thus  for consistent GCP nets we will only consider problems DOMINATING OUTCOME and EXISTENCE OF DOMINATING OUTCOME   Proposition    The problems DOMINATING OUTCOME and COME restricted to consistent GCP nets are in coNP       EXISTENCE OF DOMINATING OUT    G OLDSMITH   L ANG   T RUSZCZY NSKI   W ILSON  Proof  Using Lemmas   and     is not a dominating outcome if and only if there exists an outcome      which is non dominated  Similarly  there is no dominating outcome in a consistent GCP net if and only if there are at least two non dominated outcomes  Thus  guessing non deterministically an outcome       and verifying that  is non dominated  is a non deterministic polynomial time algorithm deciding the complement of the problem DOMINATING OUTCOME  The argument for the other problem is similar    We do not know if the bounds in Proposition    are tight  that is  whether these two problems are coNP complete  We conjecture they are      Concluding Remarks We have shown that dominance and consistency testing in CP nets are both PSPACE complete  Also several related problems related to dominance and optimality in CP nets are PSPACE complete  too  The repeated use of reductions from planning problems confirms the importance of the structural similarity between STRIPS planning and reasoning with CP nets  This suggests that the welldeveloped field of planning algorithms for STRIPS representations  especially for unary operators  Brafman   Domshlak         could be useful for implementing algorithms for dominance and consistency in CP nets  Our theorems extend to CP nets with non binary domains  and to extensions and variations of CP nets  such as TCP nets  Brafman   Domshlak        Brafman  Domshlak    Shimony        that allow for explicit priority of some variables over others  and the more general language for conditional preferences  Wilson      a      b   where the conditional preference rules are written in conjunctive form  The complexity result for dominance is also relevant for the following constrained optimisation problem  given a CP net and a constraint satisfaction problem  CSP   find an optimal solution  a solution of the CSP which is not dominated by any other solution of the CSP   This is computationally complex  intuitively because a complete algorithm involves many dominance checks when the definition of dominance under constraints allows for dominance paths to go through outcomes violating the constraints  Boutilier  Brafman  Domshlak  Hoos    Poole      b    The problem of checking whether a given solution of a CSP is non dominated can be seen to be PSPACE complete by a reduction from CP dominance that uses a CSP that has exactly two solutions  Our results reinforce the need for work on finding special classes of problems where dominance and consistency can be tested efficiently  Domshlak   Brafman        Boutilier et al       a   and for incomplete methods for checking consistency and constrained optimisation  Wilson      a         Several open problems remain  We do not know the complexity of deciding whether the preference relation induced by a CP net is complete  We do not know whether dominance and consistency testing remain PSPACE complete when the number of parents in the dependency graph is bounded by a constant  We also do not know whether these two problems remain PSPACE complete for CP nets in conjunctive form  the reduction used to prove Theorems   and   yields CP nets that are not in conjunctive form   Two additional open problems are listed at the end of Section       With another possible definition  where going through outcomes violating the constraints is not allowed  Prestwich  Rossi  Venable    Walsh         dominance testing is not needed to check whether a given solution is non dominated         T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP N ETS  Acknowledgments Jerome Langs new address is  LAMSADE  Universite Paris Dauphine        Paris Cedex     France  The authors are grateful to the reviewers for their excellent comments  and to Pierre Marquis for helpful discussions  This work was supported in part by the NSF under Grants ITR          IIS         and KSEF      RDE      by the ANR Project ANR  BLAN     Preference Handling and Aggregation in Combinatorial Domains  by Science Foundation Ireland under Grants No     PI   C    and    IN I     and by Enterprise Ireland Ulysses travel grant FR           
  This paper examines the concept of a combi nation rule for belief functions  It is shown that two fairly simple and apparently reason able assumptions determine Dempster s rule  giving a new justification for it  Keywords  Dempster Shafer Theory  be lief functions  Dempster s rule  foundations of uncertain reasoning     INTRODUCTION  Dempster s rule is the cornerstone of Dempster Shafer Theory  the theory of uncertainty developed by Shafer    a  from the work of Dempster       The rule is used to combine the representations of a number of inde pendent evidences  to achieve a combined measure of belief  For the theory to be able give meaningful con clusions  it is essential that Dempster s rule is con vincingly justified  The rule and its justifications have been criticised from many angles  a common criticism being that it can be hard to know when evidences are independent  and indeed  what  independence  means here  In this paper an axiomatic approach to the combina tion of belief functions is taken  The concept of a com bination rule is formulated precisely  and assumptions are made which determine a unique rule  Dempster s rule  A benefit of this approach is that it makes the in dependence or irrelevance assumptions explicit  Since the assumptions are arguably reasonable this gives a justification of the rule  This justification is quite dif ferent from previous justifications of the complete rule  though it is related to the justification in  Wilson        c  of Dempster s rule for a collection of simple sup port functions  In section    the mathematical framework is intro duced  in section    the concept of a combination rule is defined  section   discusses Dempster s rule and some  of the problems with previous justifications of the rule  section   defines Bayesian conditioning  used for rep resenting one of the assumptions  the assumptions on rules of combination are defined and discussed in sec tion    and the main result of the paper  that they determine Dempster s rule  is given     SOURCE STRUCTURESAND BELIEF FUNCTIONS  In this section the basic concepts are introduced  The mathematical framework is essentially that of  Demp ster      with different notation  and minor differences  but some fundamental issues are considered in greater detail       SOME BASIC CONCEPTS  We will be interested in sets of propositions and con sidering measures of belief over these  Definition  Frame  A frame is defined to be a finite set  Without loss of generality  it will be assumed that frames are subsets of the set of natural numbers   IN  The intended interpretation of a frame is a set of mu tually exclusive and exhaustive propositions  Then the set of subsets of a frame    written as    is a boolean algebra of propositions    Frame  is an abbreviation for Shafer s term  frame of discernment   Shafer    a    Dempster         and  Shafer      allow frames to be infinite  however the results here only apply to finite frames of discernment    Actually any other infinite set would do  this is just to ensure that the collection of all multiple source structures  defined later  is a set         Wilson  Definition    Additive  Probability Function  Let   be a frame  P is said to be a probability function over   if P is a function from   to         such that  i  P O       and  ii   additivity  for all A  B    such that An B      P A U B    P A    P B   We are interested in the propositions in    for frame e  Dempster  in his key paper  Dempster      consid ers a situation where we have a probability function over a related frame   representing Bayesian beliefs  Definition  Source Structure  A source structure Sover frame e is a triple     P  I   where n and e are frames  known as the underlying frame and frame of interest respectively  P  known as the underlying probability function  is a proba bility function over    and compatibility function I  Dempster s multi valued mapping  is a function from   to    Furthermore  for w E    if I w          then P w        The interpretation ofSis as follows  The set of propo sitions we are interested in is    but we have no un certain information directly about e  Instead we have a subjective additive measure of belief P over    and a logical connection between the frames given by I  we know that  for w E    if w is true  then I w  is also true  Here it is assumed that P is made with knowl edge of I  The reason for the last condition in the definition is that if w is true then I w  is true  however  if I w         then  since   is the contradictory proposition  w cannot be true  so must be assigned zero probability  Since it is frame e that we are interested in  we need to extend our uncertain information about   to    Associated with the source structure Sis a belief func tion and mass function over e  see  Shafer    a  for the definitions of these terms  defined  for X  e by m   X    L P w  wE l J w  X  Be    X     L  P w    wE l J w    X  Bel  is the extension of the uncertain information given by P  via the compatibility function I  to the frame e  It is viewed as a subjective measure of belief over e  and is generally non additive   work  and focuses on belief functions  the lower prob abilities in Dempster s framework   The relationship between Dempster s and Shafer s frameworks is fairly straight forward  but for clarity the connection will be described here  Although this paper deals primarily with source structures  and justifies Dempster s rule within Dempster s framework  these results also ap ply to Shafer s framework  using the correspondence between the two  Proposition  Function Bel                 is a belief function if and only if there exists a source structure S over e with Bel    Bel  Each belieffunction has a unique associated mass func tion  and vice versa  The focal elements of a belief function are the subsets of the frame which have non zero mass  Let us define the focal elements of a source structureS      P  I  over e to be the subsets A of e such that I w    A for some wE   such that P w  O  It can easily be seen that the set of focal elements of S is the same as the set of focal elements of Bel   From any belief function Bel  one can generate a source structure by letting   be a set in     correspondence with the set of focal elements  and defining the under lying probability function and compatibility function in the obvious way  Though the underlying frame may be more abstract than the frame of interest  the natural occurrences of belief functions generally seem to have an intrinsic un derlying frame  Even Shafer  who in his book does away with the underlying frame  uses a Dempster type framework in later work  for example in his random codes justification of Dempster s rule       It is assumed here that all the source structures we are interested in combining are over the same frame  This is not really a restriction since if they are over different frames  we can take a common refinement e of all the frames  see  Shafer    a  chapter      All the source structures can then be re expressed as source structures over e  and we can proceed as before          THE CONNECTION BETWEEN SOURCE STRUCTURES AND BELIEF FUNCTIONS  In his book  a mathematical theory of of evidence  Shafer    a   Shafer re interprets Dempster s frame See also  Dempster spaces  in  Hajek et al          EXTENSION TO DIFFERENT FRAMES OF INTEREST  COMBINATION RULES  Crucial to Shafer s and Dempster s theories is combi nation of belief functions source structures  The idea is that the body of evidence is broken up into small   intuitively  independent pieces  the impact of each individual piece of evidence is represented by a belief function  and the impact of the whole body of evidence   The Assumptions Behind Demp ster s Rule  is calculated by combining these belief functions using Dempster s rule  Informally  a combination rule is a mapping which takes a collection of source structures and gives a source structure  which is intended to represent the combined effect of the collection  the combined mea sures of belief in propositions of interest can then be calculated  If possible we would like to make natural assumptions that determine a uniquely sensible com bination rule       COMBINING SOURCE STRUCTURES  First a collection of source structures must be for mally represented  This is done using a multiple source structure  Definition  Jl ultiple Source Structures  A multiple source structure s over frame   is defined to be a function with finite domain    J  C IN  which maps each i E    J  to a source structure over    we write s i  as the triple  Qi  Pi  I    There are some collections of source structures that give inconsistent information  This leads to the fol lowing definition  which is justified in section    Definition  Combinable  Multiple source structures  over some frame  is said to be combinable if there exist Wi En   for each i E    J   with P  w   f O and niE I I  If w   f    Definition  Combination Rule   i  The Underlying Fl ame  Let us interpret element w E n as meaning that Wi is true for all i E  p  n is exhaustive  since each Qi is exhaustive  and every combination is considered  the elements of n are mutually exclusive since any two different ws differ in at least one co ordinate i  and the elements of Of are mutually exclusive  Therefore we can use n as the underlying frame for the combina tion   Some of the elements of the product space may well be known to be impossible  using the compatibil ity functions  so a smaller underlying frame could be used  but this makes essentially no difference    ii  The Combined Compatibility Function  For w E n  if w is true  then Wi  E nn is true for each i E   which implies It  w   is true for each i  so nie  P It is true  since intersection of sets in  e corre sponds to conjunction of propositions   Assuming we have no other information about dependencies between underlying frames  this is the strongest proposition we can deduce from w  Thus compatibility functions It generate compatibility function I  on n   iii  The Combined Underlying Probability Function  This is the hard part of the combination rule so it is convenient to consider this part on its own  defining a C rule to be the third component of a combination rule  Definition  C rule  Let C be the set of all combinable multiple source structures  over any frames   A combination rule II is defined to be a function with domain C such that  for sEC over frame    II s  is a source structure over e             THE DIFFERENT COMPONENTS OF A COMBINATION RULE  A C rule  r is defined to be a function  with domain the set of all combinable multiple source structures  which acts on a combinable multiple source structure s over some frame   and produces an additive probability function over Q   We write the probability function  r   s   as       It turns out that there are easy  natural choices for two of the three components of a combination rule    the two logical components      Definition  In this section Dempster s rule is expressed within the framework of this paper  and previous justifications are discussed   For multiple source structure  s    i  ns is defined to be niE P  n   An element w of n is a function with domain    J  such that w i  E Of  The element w  i  will usually be written w    ii  The compatibility function I  is given by P w  niE P  lf  w        DEMPSTER S RULE OF COMBINATION    This refers to the rule described in  Shafer    a  and the combination rule in  Dempster       not the amended non normalised version of the rule  suggested in  Smets       which is sometimes  confusingly  also referred to as  Dempster s rule               Wilson  DEMPSTER S COMBINATION RULE AND C RULE  Definition  the Dempster C rule  The Dempster C rule  rDs is defined as follows  For combinable multiple source structure s  and w E     if P w      then  riJ  w       else  r b  w      K  IT P  wi    iE fJ  where K is a constant  i e   independent of w  chosen such that  riJ  n       as it must for  rvs to be a probability function   Definition  the Dempster Combination Rule  The Dempster Combination Rule acts on multi ple source structure s to give source structure       rns J    It is easy to see that this is the combination rule used in  Dempster      and corresponds to  Dempster s rule  in  Shafer    a   Justification of Dempster s rule therefore amounts to justifying the Dempster C rule  rDS In section   the Dempster C rule is justified by consid ering a set of constraints and assumptions on C rules that determine a unique C rule       DISCUSSION OF JUSTIFICATIONS OF DEMPSTER S RULE  Dempster s explanation of his rule in  Dempster      amounts to assuming independence  so that for any w E     the propositions represented by Wj for i E   J  are considered to be independent  thus generating the product probability function P w     TiiE fJ Pi wi   for w E n  If I    w  is empty then w cannot be true  so P is then conditioned on the set  w   J  w      leading to Dempster s rule  This two stage process  of firstly assuming indepen dence  and then conditioning on I  w  being non empty  needs to be justified  The information given by J is a dependence between Wi for i E       so they clearly should not be assumed to be independent if this dependence is known  Ruspini s justification  Ruspini      also appears not to deal satisfactorily with this crucial point  A major weakness of a mathematical theory of evidence is that the numerical measures of belief are not given a clear interpretation  and Dempster s rule is not prop erly justified  This is rectified in  Shafer      with his random codes canonical examples   Shafer s Random Codes Canonical Examples  Here the underlying frame n is a set of codes  An agent randomly picks a particular code w with chance P w  and this code is used to encode a true statement  which is represented by a subset of some frame e  We know the set of codes and the chances of each being picked  but not the particular code picked  so when we receive the encoded message we decode it with each code w  E n in turn to yield a message J w    which is a subset of e for each w    This situation corresponds to a source structure     P  J  over e  This leads to the desired two stage process  for if there are a number of agents picking codes stochastically in dependently and encoding true  but possibly different  messages then the probability distributions are  at this stage  independent  Then if we receive all their mes sages and decode them we may find certain combina tions of codes are incompatible  leading to the second  conditioning  stage  To use Shafer s theory to represent a piece of evidence  we choose the random codes canonical example  and associated source structure  that is most closely anal ogous to that piece of evidence  Two pieces of ev idences are considered to be independent if we can satisfactorily compare them to the picking of indepen dent random codes  However  in practice  it will often be very hard to say whether our evidences are analo gous to random codes canonical examples  and judging whether these random codes are independent may also be very hard  especially if the comparison is a rather vague one   Shafer s justification applies only when the underlying probability function has meaning independently of the compatibility function  that is  when the compatibil ity function is transitory  Shafer       see also  Wilson    b  for some discussion of this point   Many occur rences of belief functions are not of this form  The justification given in this paper opens up the possibil ity of justifying Dempster s rule for other cases  The Non Normalised Version of Dempster s Rule  The non normalised version of Dempster s rule  Smets          is simpler mathematically so it is less hard to find mathematical assumptions that determine it  However  whether these assumptions are reasonable or not is another matter  Smets considers that the un normalised rule applies when the frame is interpreted as a set of mutually exclusive propositions which are not known to be exhaustive  Such a frame can be rep resented by a conventional frame  by adding an extra element representing the proposition which is true if and only if all the other propositions  represented by   ther criticisms of this justification are given in the various comments on  Shafer    a    b   and in  Levi         The Assumptions Behind Demp ster s Rule  other elements of the frame  are false  thus restoring exhaustivity  Therefore Smets  non exhaustive frames are unnecessary  and are restrictive   Smets also attempts to justify  the normalised  Demp ster s rule using the unnormalised rule by  closed world conditioning   Smets       i e   combining the belief functions as if the frame was not known to be exhaus tive  and then conditioning on the frame being exhaus tive after all  This suffers from a similar problem to that faced by Dempster s justification  see above dis cussion   and seems very unsatisfactory  if we know that the frame is exhaustive then this information should be taken into account at the beginning  and then Smets  justification does not apply  pretending temporarily that the frame is not exhaustive is per verse and liable to lead to unreliable results  See also  Dubois and Prade      Hajek      Klawonn and Schwecke               Definition  Product Subsets  Let s be a multiple source structure  A is said to be a product subset of n  with respect to s  if A  TiiEtfJ Ai for some   iAi  Of  iE  P    Note that such a representation  if it exists  is unique  For product subset A of    and i E  P   we will write Ai as the projection of A into Of  The following is a straight forward extension of the Bayesian conditioning of a source structure  Definition  Bayesian Conditioning of a Multiple Source Structure  Let s be a multiple source structure and let A be a product subset of Q  such that Pt A   iO for all iE  lj    Then the multiple source structure s is defined as follows  s has domain    J  and  for i E    J   s i     s  i     BAYESIAN CONDITIONING    In this section Bayesian conditioning of source structures  is defined  these are used to simply express assumption  A  in section    Definition  Bayesian Conditioning of a Probability Function  Let P be an  additive  probability function over set n  and let A  n be such that P A  iO  Then the probability function P  over A  is defined by P f    P f  P A   for  r   A   This is used for conditioning on certain evidence A  Note that if A is considered to be certain  and n is a frame  then A is also a frame  Definition  Bayesian Conditioning of a Source Structure  Let S   Q  P  I  be a source structure over frame   and let A  n  representing certain evidence  be such that P A  iO  Then s is defined to be the source structure  A  P  I   where I is I restricted to A  This should be uncontroversial  given that the judge ment of the underlying epistemic probability P is made with knowledge of the compatibility function  Incidentally if  for source structure S       P  I  over   and A     we let A    wEn   I w   A  then S corresponds to geometric conditioning by A  Shafer    b  Suppes and Zanotti        This is not closely related to Bayesian updating of a belieffunction  Kyburg      Jatfray       CONSTRAINTS AND ASSUMPTIONS ON C RULES  In this section we introduce two clearly natural con straints on C rules  and two arguably reasonable as sumptions  It is shown that together these determine a unique C rule  which turns out to be Dempster s C rule  hence justifying Dempster s rule  Constraint  Respecting Contradictions  C rule  r is said to respect contradictions if for any combinable multiple source structure s and w E n  if P w      then  r  w      If I    w         then w cannot be true since w true im plies J  w  true  and   represents the contradictory proposition  Therefore any sensible C rule must re spect contradictions  Constraint  Respecting Zero Probabilities  C rule  r is said to respect zero probabilities if for any combinable multiple source structure s and w E     if Pi  w       for some iE    J   then      w       If Pf   w       for some i then w  is considered impossi ble  since frames are finite   so  since w is the conjunc tion of the propositions w   w should clearly have zero  probability   Note that if we missed out the condition that the mul tiple source structure had to be  combinable  in these two constraints and in the definition of a C rule then these two constraints are inconsistent  for any C rule  r and any multiple source structure s which is not com binable  if  r respects contradictions and zero proba         Wilson  bilities then  r  w      for any wE   l   which is incon sistent with  r  being a probability function  Definition  Let s be a multiple source structure  k E   J   and I E   lt  Then  Ei is defined to be  wE n   w k   I   and   Ei is defined to be  wE n   w k fl   i e     l  Ei  The set Ei is the cylindrical extension in n of I  E   lt   and can be thought of as expressing the event that variable k takes the value l  Definition  Assumption  A   C rule  r is said to satisfy assumption  A  if  r respects zero probabilities and  for any combinable multiple source structure s  for any k E   J   I E   lt such that  r      f   where         EL    r  a    ra  Note that since  r respects zero probabilities  if  r      f  then Pk    k f   so s is defined  In fact it can be shown that if  r satisfies assumption  A  then it satisfies a more general form of the assump tion where     is allowed to be an arbitrary product subset of n  Assumption  A  can be thought of as postulating that Bayesian conditioning commutes with source structure combination  Bayesian conditioning by   Ei can be viewed  roughly speaking  as omitting the lth focal element from the kth Belief function  and scaling up the other masses   Assumption  A  amounts to saying that it should not make any difference whether we omit that focal ele ment before  or after  combination  Definition  Assumption  B   Let s be a combinable multiple source structure such that  for some kE   J   lOti    and lOti    foriE   J     k   and l   w f  for wE n  Then for IE   lt   The notation hides the simplicity of this assumption  The multiple source structures referred to are of a very simple kind  one of the component source structures has just two elements in its underlying space  and so leads to a belief function with at most two focal ele mens  and all the other component source structures give belief functions with just one focal element  so  they can be viewed as just propositions  i e   certain evidences  furthermore there is no conflict in the evi dences  In terms of belief functions this is the situation where we are conditioning a belief function with two masses by a subset of e  Assumption  B  is just that adding all the other cer tain sources does not change the probabilities of com ponent k  The rationale behind this assumption is that the certain evidences are not in conflict with the in formation summarised by the kth source structure  so why should they change the probabilities  Theorem   rDS is the unique C rule respecting contradictions  zero probabilities and satisfying  A  and  B   This means that Dempster s rule of combination uniquely satisfies our constraints and assumptions  hence justifying it  Sketch of Proof  Unfortunately the proof of this theorem is far too long to be included here  To give the reader some idea of the structure of the proof  it will be briefly sketched  It can easily be checked that  rDS satisfies the con straints and assumptions  Conversely  let  r be an ar bitrary C rule satisfying the constraints and assump tions  First  it is shown that  r satisfies a more gen eral form of  A   where     is allowed to be an arbi trary product subset of n  This is then applied to the case of        w  w    where w and w  differ in only one co ordinate  In conjunction with assumption  B  this enables us to show that  when the denominators are non zero   r  w   r  w      r bs w   r bs w       A source structure over e is said to be discounted if e is a focal element of it  and a multiple source structure is said to be discounted if each of the source structures of which it is composed is discounted  It is then shown that  for any discounted multiple source structure s   r     r b   using the last result repeatedly  The theorem is then proved by taking an arbitrary combinable multiple source structure t  discounting it to form s  see  Shafer    a   and using the more general form of assumption  A  again to relate  rt and  r   r bs     DISCUSSION  Both assumptions  A  and  B  seem fairly reasonable   A  appears to be an attractive property of a C rule  but is a rather strong one  and it is not currently clear to me in which situations it should hold  it is conceiv able that there are other reasonable seeming principles   The Assump tions Behind Dempster s Rule  with which it is sometimes in conflict   Further work should attempt to clarify exactly when both assump tions are reasonable  There are cases where Dempster s rule can seem un intuitive  for example  I argued in  Wilson    b  that Dempster s rule is unreasonable at least for some in stances of Bayesian belief functions  and there has been much criticism of certain examples of the use of the rule e g    Pearl    a    b  Walley      Voorbraak      Zadeh        If it does turn out that there are certain types of belief functions where assumption  A  or  B  is not reason able  then the above theorem  as it stands  is not use ful  However  an examination of its proof reveals that only two operations on belief functions source struc tures are used Bayesian conditioning  i e  omitting focal elements and scaling the others up  and discount ing  i e  adding a focal element equal to the frame e  and scaling the others down   This means that the proof could be used to justify Dempster s rule for any sub class of belief functions source structures  for which  A  and  B  may be more reasonable  which is closed under these operations  for example the set of simple support functions or the set of consonant sup port functions  Also  for the same reason  the proof could be used to justify Dempster s rule for collections of belief functions multiple source structures s such that l  w   j    for all w E f    if  A  and  B  were con sidered reasonable here  It might also be interesting to investigate alternatives to  B   which give different values for      Ek  than those given in  B   The proof of the theorem can be modified to show that there is at most one C rule satisfying the constraints and assumptions  though of course it will not be the Dempster C rule  Acknowledgements  I am very grateful to an anonymous referee for pointing out a minor error  This work was supported by a SERC postdoctoral fel lowship  based at Queen Mary and Westfield College  Thanks also to Oxford Brookes University for use of their facilities  
 We take a general approach to uncertainty on product spaces  and give sufficient condi tions for the independence structures of un certainty measures to satisfy graphoid prop erties  Since these conditions are arguably more intuitive than some of the graphoid properties  they can be viewed as explana tions why probability and certain other for malisms generate graphoids  The conditions include a sufficient condition for the Inter section property which can still apply even if there is a strong logical relationship between the variables  We indicate how these results can be used to produce theories of qualita tive conditional probability which are semi graphoids and graphoids  Keywords  Graphoids  Conditional Inde pendence  Qualitative P robability   This means that then independence assumptions can be propagated using the graphoid inference rules  and can be represented and propagated using the  both directed and undirected  graphical methods of  Pearl        In section   we define independence structures  semi graphoids and graphoids  GCPPs are defined in sec tion    with examples and we show how they give rise to independence structures  Section   considers the Intersection property  In the literature it seems to be generally assumed that this only holds for probability distributions which are always non zero  we show here that it holds much more generally  a sufficient con dition being a connectivity property on the non zero values of the probability  exactly the same condition is sufficient for other GCPPs to satisfy Intersection  Section   considers different sufficient conditions for GCPPs to give rise to semi graphoids  These are use ful for constructing uncertainty calculi which generate graphoids and might also be used for showing that an uncertainty calculus gives rise to a graphoid      INTRODUCTION  The importance of the qualitative features of prob abilistic reasoning has often been emphasised in the recent AI literature  especially by Judea Pearl  An i mportan t qualitative aspect of probability is given by the graphoid properties  defined in  Pearl       see also  Dawid      Smith       which sum up many of the properties of probabilistic conditional independence  In this paper we look at the reasons why probabil ity obeys these properties  with an eye to generating other uncertainty theories which share much of the same structure as probability  but represent different types of information  perhaps of a more qualitative nature   A fairly general family of uncertainty calculi on prod  uct spaces is introduced  which we call Generalised Conditional Probability on Product Spaces  GCPP   and define two different types of conditional indepen dence for GCPPs  We show that under simple  and apparently fairly weak  conditions  conditional inde pendence for GCPPs satisfies the graphoid properties   In section   we consider another view of GCPPs  as qualitative conditional probabilities  This view allows graphoids to be constructed from qualitative compar ative judgements of probability  Section   briefly con siders computation of GCPPs  and section   highlights some areas for further study      INDEPENDENCE STRUCTURES  Let U be a finite set  An independence structure I on  U is defined to be a set of triples  X  Z  Y  where X  Y and Z are disjoint  subsets of U  We write I X  Z  Y  for  X  Z  Y  E I  For disjoint subsets X  Y  U  their  union  XU Y  will be written  XY   U is intended to be a set of variables  and I X  Z  Y  is intended to mean that variables X are independent of variables Y given we know the values of the variables  z    A collection A of sets is sa id to be disjoint if for ea ch E A  X n Y     X  Y           Wilson  The Graphoid Properties of Independence Structures  I X  Z       Trivial Independence   If I X Z  Y  then I  Y  Z  X    Symmetry   If I X Z  YW  then I X  Z  Y   If I X Z  YW  then I X ZY  W    Decomposition    Contraction   If I X ZY  W  and I X  ZW  Y  then I X   Z  YW   Intersection  where W  X  Y  Z are arbitrary disjoint subsets of U  so  for example  I satisfies symmetry if and only if the above property holds for all disjoint X  Y and Z   If an  independence structure satisfies all these proper ties then it is said to be a graphoid  if it satisfies the first five  i e  all except Intersection  then it is said to be a semi graphoid  As we shall see in section    prob abilistic conditional independence is a semi graphoid  and in certain situations a graphoid  The definitions given here for semi graphoid and graphoid differ from that given in  Pearl       in that we require Trivial Independence to hold  However  our definition seems to be what Pearl intended   it is not implied by other properties  consider the empty inde pendence structure  and it is satisfied by probabilis tic conditional independence so it is a  rather triv ial  counter example to the Completeness Conjecture   Pearl       also without Trivial Independence  Markov boundaries don t necessarily exist  consider the empty independence structure again  which makes Theorem   of  Pearl      incorrect  The intersection of a family of  semi  graphoids is a  semi  graphoid  Hence  for any independence struc ture I  there is a unique smallest  semi  graphoid con taining I   GENERALISED CONDITIONAL PROBABILITY ON PROD UCT SPACES  GCPPs   Uncertainty measures are usually defined on boolean algebras  However  for our purposes of studying in dependence structures generated by the uncertainty measure  a different domain is natural       The set u is defined to be   Weak Union   If I X ZY  W  and I X Z  Y  then I X Z  YW      disjoint X  Y  U  an element of XY may be written xy for x EX  y E Y  For disj oi nt X  Y  U we define XIY to be the set of all pairs  xly   x EX  y E Y   The set    is defined to be a singleton  T   An element  xiT of X I will usually be abbreviated to x  we are identifying Xl l  with X    THE B ASIC DEFINITIONS  U    X         Xn  is said to be a set of variables if associated with each variable X  E U is a finite set of values X   For X U define X to be Ilx E X   For  X   See  for example  the sentence before Theorem    p   of  Pearl        Fatal counter examples are given in  Studeny        u  X YCU XnY    X IY   A GCPP p over set of variables U is defined to be a function p  u    D for some set D containing differ ent distinguished elements   an d oo such that for any disjoint X  Y  U and x EX   i  p x      if and only if for all y E Y  and  p xy          ii  for any y E Y  p xlv    oo if and only if p y       GCPPs will be viewed as measures of uncertainty  p xly  may be thought of as some sort of measure of how plausible it is that the composite variable X takes the value x   given that Y takes the value y  The assignment p x IY      is intended to mean that  c is impossible given y  i e   X cannot take the value     if Y takes the value y  The inclusion of element oo in D is not strictly necessary  it is u sed as a notational convenience  and can be read as  undefined   We re quire  i  because  x is possible if and only if there is some value of Y which is possible when X takes the value x  We require  ii  because  if y is impossible then conditioning on y doesn t make much sense  Note that no structure on Dis assumed  for example  we do not assume that D     oo   IR  or even that D has an ordering on it  The definition implies that p T          oo and any X U and x EX  p J        oo    that  for  Definition  For GCPP p over U and disjoint X  Y  U define pXIY to be p restricted to X IY and define px to be pXI   For Z  U and z E Z define Pz   U   Z      D by  for disjoint X  Y  U   Z  x E X  y E Y  Pz  xly    p xlyz   For disjoint X  Y  U   Z  p  JY is defined to be Pz restricted to X IY  and P  is defined to be p      GCPP p over U is said to be a full GCPP over U if for every Z  U and z E Z such that p z         p  is a GCPP over U   Z  The function P z may be thought of as p conditioned on Z   z  It turns out that  for GCPP p  p is a full GCPP if and only if for all disjoint X  Y  U  x E X  y E Y    p xly             p xy      and p y               EXAMPLES OF GCPPS  A probability function over set of variables U is de fined to be a function p  u           u       such that        Generating Graphoids from Generalised Conditional Probability  for xJy E U    i  P xJy   ii  if P y       P xJy  P x     LweU X P xw            P y        P xy  P y   and  iii   oo  The definition implies that P is a full GCPP over U and P T       The latter follows since P T  is equal  by definition  to P TJT  so by  i  above  P T      if and only if P T    oo  which implies that P  T  is neither   or oo  We can now apply  ii  to get P T    P TIT    P T  P T  which implies that P T      as re quired  For any  finite  set of variables U  there is a one to one correspondence between probability functions P over U and probability functions f on U  i e   functions f U          such that LuEuf u       P restricted to U is a probability function on U  and conversely  a probability function f on U extends uniquely to a probability function over U using  i    ii  and  iii    A Dempster possibility function over set of variables U         u       such is defined to be a function      u that for xJy E U    i   r xly    oo        r y      ii  if  r y        r xly     r xy   r y   and  iii   r x    maxwEU X  r xw    include a maximal element in the range of OCFs be cause he desired belief change to be reversible  Spohn      p      Kappa f u nct ion x  can be transformed into a Demp ster possibility function  r  by  r       oo          e        oon and  r   l J         J   otherwise  for   J E U   where   oo is taken to be    For   J   E U   x           x              r           r        This means that  for our purposes  kappa functions can be viewed as s p eci al cases of Dempster p ossibili ty functions   Shafer s pla usi bil ity functions  Shafer       also give full GCPPs  their dual functions  belief functions  and ne ce ssi ty functions  the dual of possibility functions  do not give GCPPs  since  for these  a value of   means a lack of e vid ence   rather than  impossible        INDEPENDENCE STRUCTURES OF GCPPS         Again  the definition implies that  r is a full GCP P over U and  r T       De mps ter possibility functions are essentially Zadeh s possibility measures  Zadeh     Dubois and P r ade       and consonant plausibility fun c tions  Shafer       the definition of conditional possibil i ty is obtained from Dempster s rule  and is not the one most commonly used  partly because it means that the range of  T cannot be viewed as an ordinal scale  and most justifications of possibility theory require this   A special case of Demps ter possibility functions are consistency functions over U  where  T only takes the values      and oo   r xjy      is then int ended to mean that  given that y is the true value of variables Y  it is possible that x is the true value of variables X  Every full GCPP p over U gives rise to a consistency function p  over U defined  for     E U    by p   l      p  I J  if p  l J      or oo  and p  l J      otherwise  Consistency functions appear in the theory of relational databases  Fagin       and also in  Shafer et a         A kappa function over set of variables U is defined to be a function K  U                    oo oon    where oo n is different from the other elements  such that for xiy E U    i  K  xly    DOn            y    oo   ii  if K y    oo  K xjy    x  xy    K y   and  i ii   x  x    minwEU X K xw   The definition impli es that K  T      and      is a full GCPP over U  however  the labelling of the elements in the range of    is confusing  the zero of D in the definition of a GCPP is oo and the element meaning  undefined  is oo n not oo     Kappa f unctions are based on Spohn s Ordinal Conditional Fu nctions  Spohn       An important difference is that the range of kappa f unctions has a maximum element oo  Spohn did not  For GCPP p over set of variab les U  independence structures Ip a n d I are defined as follows  Let X  Y and Z be disjoint subsets of U   Ip X  Z  Y  if and only if p  I jyz    p J Iz  for all X  y EY  z E Z such that p yz        x  E  I X  Z  Y  if and only if p  z Jyz    p xly z  for all x EX  y y  E Y  z E Z s  ch that p  yz   P   and  p y z  O  For set S and functions g  h  S    D write g     h if they are equal when they are both defined  i e   if for all s ES   g s    h s  or g s    oo or h s    oo    This gives a simpler way of expressing  the two independence structures  For disjoint  subsets X  Y and Z of U   Ip X  Z  Y  if and only if for ally E Y  p jiZ  and  I X Z Y  if and XIZ XIZ py Py        only i ffor all y y        pXIZ   EY       To underst and the definitions  first consider the case when Z       Then Ip X  Z  Y  if the degree of plau sibility of x  p x   does not change when we condition by any  possible  value y of Y  Thus our uncertainty about variable X does not change by learning the value of variable Y I X  Z  Y  holds if the degree of plau sibility of x gi ven y  p xiy  does not depend on the choice of  possible  value y of Y  The same  emarks apply for general Z  except that  now we must consider the degrees of plau sibilit y conditional on each value z of Z  We shall see in s ectio n   that for any GCPP p  lp satisfies Trivial Independence and Contraction  and  if Ip   I   it satisfies Decomposition and Weak Union al so          Wilson  If GCPP p is non zero on U then  trivially  U connected  and so satisfies Intersection   THE INTERSECTION PROPERTY     I   This is the only one of the graphoid properties that does not hold for all probability functions   Pe ar l      p    appears to suggest that it only holds for proba bility functions which are every where non zero  This turns out not to be the case  and we will see that a sufficient condition for Intersection to hold sometimes allows very strong logical dependencies between the variables  Set n is said t o be connected under relation R  n X n if the smallest equivalence relation on   containing R is the relation   X    Let p be a GCPP over set of variables U and let Y  W and Z be disjoint subsets of U  For z E Z   define  YW t z    yw E YW   p ywz          We say that  Y  W  is p  Z connected  if for all z E      YW t    is connected under the relation R defined by              y   y or w   w    yw R y  w   For GCPP p over set of variables U  we say that U if p connected if for all disjoint subsets Y  W  Z of U  the pair  Y  W  is p  Z connected  Note that these properties only depend on the set of elements of U for which pis zero  that is  those which are known to be impossible   The above concepts are not quite as obscure as they appear at first sight   YW   is the set of yw which are not known to be impossible when we know that Z   z  If we label Y as Y         Ym and W as W         Wn then YW   Y x W can be viewed as the squares of am x n chess board  Then y Wj R Yi Wj  iff i   i  or j   j   i e   iff the rook chesspiece could move between the squares  i j  and  i  j    Let N  be the set of squares corresponding to  YW t    We there fore have that  Y  W  is p  Z connected iff for all z  it is possible to move between any two elements of N  us ing a sequence of rook moves  where each intermediate square is also in Nz   Let pbe a GCPP over a set of variables U   i  For disjoint subsets X  Y  Z and W of U  suppose that  Y  W  is p  Z connected and also that    x  ZY  W  and I  x  ZW  Y   Then holds    ii  If GCPP p over set of variables U is such that U if p connec ted then satisfies Intersection     I  similar concept is important in in  Moral and Wilson  it guar antees the convergence of Markov Chain Monte Carlo algo rithms  and in ilson  it is relevant to the justification of Dempster s rule   Interestingly  a  very  Dempster Shafer theory    W                   Milan Studeny has found a similar result  for the of probability functions    case  p  Example Let U    S  H   H    Variable S ranges over shoe sizes  and the correct value is the shoe size of the  un known  next person to walk into my office  H  and H  both take integer values between   and       The cor rect value of H   is the height in millimetres rounded down of this unknown person and the correct value of H  is their height to the nearest millimetre  Let P be a Bayesian probability function on U  rep resenting our Bayesian beliefs about the variables  As described above  P extends uniquely to a GCPP over U  Now  P ij      unless i   j or i   j     where ij means H    i and H    j  Despite the very strong logical relationship between H  and H     HI   H    is P    connected  and so if we considered S to be logically independent of  H    H    in the sense that P sh h       if and only if P s      or P h h        then U would be P connected  This implies that Ip     If  by the results of the next section  would satisfy the Intersection axiom  and so would be a graphoid  In any case  given knowledge of height to the near est millimetre  one will learn almost nothing more about shoe size by learning height in millimetres rounded down  so one might be tempted to make the subjective conditional independence judgement  p  S    H     HI      Alternatively  if one did not know the precise meaning of H  and H   but had the values of the variables for previous visitors then it would take a vast amount of data to detect a de pendency   Similarly one might be tempted to say  p  S    HI    H     But these lead  using the last proposition  to Ip  S       H   H    which is certainly unreasonable since there is a definite dependency be tween shoe size and height   This example illustrates how careful one must be with subjective independence judgements for Bayesian probability  or any other GCPP   It also seems to sug gest that GCPPs  with definition cannot represent  approximate independence          Proposition    I X  Z  YW   IS     SUFFICIENT CONDITIONS FOR GCPPS TO GENERATE GRAPHOIDS  Here we consider simple sufficient conditions on GCPPs for its associated independence structures t o satisfy semi graphoid properties  Since probability functions  Dempster possibility functions  kappa func tions and consistency functions sat isf y these proper ties with the exception of the conditions of proposi tion   v   these could be v iewed as explanations for why they are semi graphoids    Generating Graphoids from Generalised Conditional Probability       CONDITIONAL COHERENCE  It is easy to see that for any GCPP p  Ip  I  i e   if Ip X  Z Y  holds then I  X  Z Y  must hold  Fur thermore if p is inten de d to represent a generalised form of probability then a natural constraint is that Ip   I  T his is because a sufficient condition for Ip   I is the apparently reasonable condition  Conditional Coherence   For any disjoint subsets  EX and z E Z  if for all y y  EY   p xiyz    p xly z   i e   p xlyz  does not vary with y  then p xlz    p xlyz   i e   p xlz  is e qu al to that constant v alue   We say t hat p is weakly conditional coherent if Ip   I  X Y Z of U   x  Consider conditioning on a fixed z E Z  The idea behind conditional coherence is that if the degree of plausibility of x given y  i e  Pz xiy   is not dependent on which value y of Y we use  then one might expect that the degree of plausibility of x   i e    Pz x   wou ld be equal to that constant value  The conditionals and marginal then cohere in a particular sense   Conditional Coherence is a re stri cted version of the S an d wich Prin cip le  Pearl      IJAR       Pl ausi bi l ity  belief functions and upper  lower probability func tions have good reason not to obey conditional coher ence  see e g    W ilson      Chunhai and Arasta       It is satisfied by probability and Dempster possibility functions and hence by con si stency and kappa func ti ons  Proposition    For any GCPP p over set of variables U the indepen dence s tru cture IP satisfies Trivial Independence and Contraction  and lp satisfies Decomposition if and only if it satisfies Weak Union  Now suppose tha t p is weakly conditional coherent  We then have   i  lp s at is fies Decomposition and Weak Union  Therefore if Ip satisfies Symmetry then it is a semi  graphoid    ii  If U is p connected then Ip satisfies Intersection  Therefore if lp satisfies Symmetry then it is a graphoid  Perhaps the only one of t he grap hoid properties  with the exc epti on of Trivial Independence  which imme diately seems natural is Symmetry  Surprisingly  it seems to be harde r to find natural sufficient conditions on p for Ip t o satisfy Symmetry  The follo wi ng result gives a fairly strong condition  Proposition    Suppose that p is a full GCPP over U such that for all Z  U and z E Z  there exists a fu ncti on o  R    D for some R  D x D such that        i  for al l xly E  U Z    Pz xy    p   xly   pz Y   and  ii  if a o b   c   a and  a   f    t hen b    c   Then Ip satisfies Sy mme tr y       DETERMINIST IC RELATIONS BETWEEN JOINTS  CONDITIONALS AND MARGIN ALS  If I is an independence structure let its reflection IR be defined by IR Y  Z X         I X  Z  Y   Clearly   I satisfies Symmetry if and only if I   I R  Let I    In JR be the symmetric part of I  so that for disjoint subsets X Y  Z of U  I   X  Z Y  if and only if I X  Z  Y  and I Y  Z X   Proposition    Suppose p is a GCPP o ve r set of variables U    i  If p Tix    p T  for all X  U and x EX such that p x   f    t h en  I R s ati sfies Trivial In depen  d en ce   ii  If for all disjoint W Y X  U there exists a func tion M su ch that for al l x E X  M p  YY    p   then  I R satisfies De com positi on    iii  If for all disjoint W Y X  U t here exists a fun c tion C s u ch that for all x EX  C p  YY    p  YIY then  I R satisfei s Weak Union   iv  If p i s a full GCPP and for all disjoint W Y X  U there exists a function J such that  a  for all x E X  J p  YIY  p      p  YY and  b  J g  h    J g  h  whe n for all wand y   g wYi     g    wly  or h y        for functions g  g   WIY    D and h  Y   D wit h  g h  and  g  h  in the domain of J  The n  I R sati s fies Contraction    v  If p is non zero on U and for all disjoint W  Y  X  U there exists a function S s uch that for all x EX  S p  YIY  pIW    p  YY then  I R satisfies Inter  section   The functi on Min  ii  can be thoughtofas a marginal isation op e r ator  and C in  ii i  as a conditioning op erator  J in  iv  gives a way of calculating the joint  dist r ib u tion from the conditional and marginal distri butions  condition  b  in  iv  can be omitted if p is non  ze ro on U  J is e ssen tia lly just pointwise multipli cation for probability and Dempster possibility  The existence of M in  ii  means that for each x  the joint distribution of Px on WY determines the marginal distribution  of Px on Y   To see how this condition is used   suppose I WY    X  and p x   p x    f    then p  YY   p   I Y sop   p which leads to I Y    X   Similar considerations apply to  iii    iv  and  v   Pr op osi ti on   implies that if p is a full GCPP  satisfy ing the conditions of  i    ii    iii  and  iv  above  an d   Wilson           is symmetric then  I   is a semi graphoid  further  more if U is p connected then  J is a graphoid   The result also leads to a way of constructing a semi graphoid from a GCPP even if I  and are not sym metric   I  Proposition   If GCPP p over U is weakly conditional coherent  and satisfies the conditions of  i    ii    iii  and  iv  of proposition   then is a semi graphoid  If  in ad   If  dition  p satisfies the conditions of  v  then graphoid      If  IS  a  associated SQCPPs  The correspondences between GCPPs and SQCPPs mean that the sufficient con ditions for independence structures to satisfy the graphoid properties given in section   can be trans lated into sufficient conditions for SQCPPs  and hence QCPPs  to generate independence structures with those properties   If consistent  is a full SQCPP  i e  for all xly E u              xy    and y   j       a sufficient condi I  xly  tion for I to satisfy Symmetry is the following  cross multiplication  condition  For all disjoint X  Y  Z       U  x E X  y E Y  z E  if xz    and xyzlyz xzlz then xyzlxz  yziz  where we have trivially extended  to elements  r yly of XYIY  for disjoint X  Y       U   by placing xyiy in the same  e quivalence c las s as xly   QUALITATIVE COND ITIONAL PROBABILITY     Another way of viewing GCPPs is as Qualitative Con ditional Probabilities on Product spaces  QCPPs   A Symmetric QCPP  abbreviated to SQCPP  over set of variables U is an equivalence relation on u U  O oo  satisfying  for disjoint X  Y       U and x EX   i   p x      if and only if for ally E Y   p xy        We will ofte n want to cons truct a QCPP from a num ber of different types of information   p y        i  some qualitative probability relationships we ex pect always to hold  such as  perhaps    xl   for all xiy E U    and  ii  for any  y  E Y   p xly   oo if   is said to be consistent if   f   and only if  oo   Independence s t ru c tures I   and      on U are defined analogously to the definitions for GCPPs  Ir    X Z  Y            xlyz  xly for all x  y  z such that yz     and I X  Z  Y            xlyz  xly  z for all x  y  y   z such that yz f    f  y  z  The framework of consistent SQCPPs is essentially equivalent to the framework of GCPPs  For any GCPP p over U we can define a consistent SQCPP P over U  by first ext endi ng p toW U O  oo  by defining p O      and p oo    oo  and then  for      E U   U  O oo   defining t J P            p t      p    We then have Ir     p   and I      I     Ip   p  Constructing QCPPs  p  Conversely  for consistent SQCPP  we can define  GCPP Pr     by letting D    uu O  oo     calling the equivalence classes of   and oo by   and oo res pec ti v e ly and  for xly E U   defining Pr     xly    d          d   xly  We have I     Ir     and I      I  Also  for any SQCPP   we have  p      Relation  is said to be a QCPP over set of variables U i f it is a reflexive transitive relation on U  U     oo   such that its symmetric part   the intersection of  and   is a SQCPP over U   QCPPs might be thought of as probability fun cti on s without the numbers  a statement such as xlz  yiz could mean  given z  value y i s at least as prob able as  x    QCPPs generate independence s tru ctures via their      ii  some desirable properties of       such as the above sufficient condition for Symmetry of Ir      and other conditions that imply graphoid properties   iii  an agent s comparative probability judgements  e g   statements of the form xlz       x or xlz  yjz   iv  an agent s conditional independence judgements  The obvious way to at t e mpting to construct a QCPP  for a particular si tu a t io n is to treat  i  and  iii  as sets of axioms and  ii  and  iv  as sets of inference rules  and generate the QCPP from these  H owever  there is a technical problem  because of the condi tions yz     the conditional independence assump tions cannot quite be viewed as se ts of inference rules  We can solve this by requiring that the user gives  ex plicitly or implicitly  a II the values u of U such that u     that is  the set of all u which are considered im possible   the key point here is that the application of the rules must not lead to any more zero values of U  The conditional independence assumptions can now be viewed as inference rules  since they are now closed un der intersection  For the same reason  we re q uire the properties in  ii  also to be closed under intersection  once the zeros of U are determined  Naturally  if we have included in  ii  properties which imply that Ir     is a semi graphoid  then we can pr opa gate conditional independence assumptions using the semi graphoid prope rt ie s  or using the graphical meth od s described in  Pearl         Generating Graphoids from Generalised Conditional Probability  COMPUTATION OF GCPPS     Here we only consider computation of values of the joint distribution of a GCPP  leaving other aspects of computation for future work   I  Let be an independence structure on U  For X  E U  and W  U   Xi  the set B  W is said to be an   Markov boundary of X  with respect to W if B is minimal such that I  Xi   B  W  B    If I satisfies Trivial Independence then there is at least one   Markov boundary of X  with respect toW  and if I satisfies Weak Union and Intersection then there is at most one   Proposition          to explore  as would its relationship with comparative probability  Walley and Fine       T here may well also be connections between GCPPs and the framework of  Shenoy      which uses a pro duct definition of inde pendence  Acknowledgements  Thanks to Serafin Moral and Luis de Campos for some useful discussions  and to Milan Studeny and the ref erees for their helpful comments  The author is sup ported by a SERC postdoctoral fellowship  I am also grateful for the use of the computing facilities of the school of Computing and Mathematical Sciences  Ox ford Brookes University  
     I I  Rules  Belief Functions and Default Logic   I  Nic Wilson Department of Computer Science Queen Mary and Westfield College Mile End Rd   London El  NS  UK  I  A b st ra ct  I  This paper describes a natural framework for rules   bas ed on belief functions   which includes a repre s entation of numerical rules  default rules and rules allowing and rules not allowing contraposition  In particular it jus tifies th e us e of th e Demps ter Sh afer Th eory for repres enting a particular class of rules   Belief calculated being a lower probability given cer tain independence assumptions on an underlying s pace  It s h ows h ow a belief function framework can be generalised to other logics  including a general Monte Car lo algorithm for calculating belief  and h ow a version of Reiter s Default Logic can be s een as a limiting case of a belief function formalism   I I I I      Rules used by people are often not completely re liable so any attempt to represent them must cope with the conclusion of the rule sometimes being in correct  Numerical approaches do this by giving some kind of weighting to the conclusion of an un certain rule  non monotonic reasoning  a symbolic approach  ensures that these rules are defeasible  so that their conclusions could later be retracted if nec essary  There has been little work done  however  on relat ing numerical and symbolic techniques  an exception being the work of Adams  Adams      further devel oped by Geffner and Pearl  Geffner      Pearl      where a  logic is produced from probability theory  by tending the probabilities to    This paper shows how a belief function approach can represent numerical rules  both those allowing con traposition and those not allowing contra position  and how default rules may be viewed as the lim iting case of such rules  when the certainty of the rule tends to    This allows the integration of the Dempster Shafer Theory  DST   Shafer      and Re iter s Default Logic  Reiter       hence enhancing the understanding of both   I I I I I I I I I  Introduct ion     This research was carried out as part of t he ESPRlT basic research action DRUMS         Section   deals with the representation of numeri cal rules within DST      giving an interpretation of the type of rule that DST typically represents      presents a belief function framework that allows the theory to be generalised to other logics  and     shows how the framework can be applied to include rules which don t allow contraposition  Section   deals with the representation of default rules within the framework      reformulates Reiter s Default Logic and defines a modified extension  equivalent to Lukaszewicz s       shows how the belief function framework can be turned into a logic and     shows how to represent default rules within this logic  Sec tion   indicates how priorities between rules can be represented  and Section   suggests how numerical and default rules could be used together within the framework     Numerical Rule s  Expert Systems like MYCIN  Buchanan and Short liffe      use uncertain rules of the form If a then c    a    where a  is the some measure of how reliable the rule is  There are many ways of interpreting such a rule  We consider a natural interpretation which leads to the standard Dempster Shafer representa tion of rules      J u stifyin g  DST Representation of Rules  The standard way of representing the rules If a  then c     a     for i             m  with the Dempster Shafer Theory is  for each rule to produce a simple support function with mass a   allocated to the ma terial implication a     c  and the remaining mass     o  allocated to the tautology  and then to com bine these simple support functions by repeated ap plication of Dempster s Rule  Pearl has criticised this representation for its behaviour under chaining and reasoning by cases  However it turns out that this DST approach represents a very natural type of rule  The uncertain rule may in fact be an approximation to the certain rule n  a    c where n is an unknown antecedent or one too complicated to be easily ex pressed by the expert but which they judge to be true with probability o  After all        uncertainty measures characterise invisible facts  i e   exceptions not covered in the formulas   Pearl      p    Since n    a    c is logically equivalent to n        c        a  such a rule allows contraposition  an d since it s also logically equivalent to n     a    c    this rule may also be interpreted In a proportion o o f worlds  or situations   we know th e material implication a    c is true         If we represent such a rule by a simple support func tion  as described above  Belief is just the probability that we know a    c to be true  so that it s a lower probability for a    c  Similarly if we have a num ber of such rules  a     c   i            m    represent them as simple support functions and combine these with Dempster s Rule  Belief is a lower probability  given certain independence assumptions on the n s  Consider now the typical Reasoning by Cases situa tion  we re given two rules If a then c   Ql  and If   a then c    Q   which we ll interpret as uncertain material implications n     a    c and n       a    c with Pr n     Q   i         Pearl argues that any reasonable measure of belief should obey the Sandwich Principle  deducing from those two rules that belief in c should be between Q  and Q   the Dempster Shafer approach however gives that Bel c    Ql Q   But it is clear why the Sandwich Principle is violated for this approach  knowing either a or   a increases our knowledge and hence our belief  In worlds where n       n  is true  c may be always false if a is always false  in the event   n    n   c may be always false if a is always true  and in the event  m       n  there is no constraint on c so c may again always be false  Only in the event n     n  can we be sure that c is true  so making the assumption of independence of n  and n   which is reasonable without contrary knowledge  we get Bel  c    the probability that we re in a world where we know c to be true  is Q a   This type of rule can also be chained  n     a    b and n     b    c with Pr n     a    i         leads to  n     n      a    c  and again assuming independence of n  and n  this gives Pr n      n     a a   If we now learn that a is true we get Pr c   a  a  and so Bel c    a  a   the result given by application of Dempster s Rule  Of course the assumption of independence of the n s will not always be valid if correlations between the rules are known they should be  and can be  incor porated  The Dempster Shafer approach is thus a natural  formally justified as well as a computationally ef ficient way  see  Wilson      and section      to rep resent If Then rules   logic of knowledge   A natural way to extend Demp ster s multi valued mapping  Dempster      is as fol lows  We have a mutually exclusive and exhaustive set   with a probability function P on it  and we re inter ested in the truth of formulae in L  where L is the language of some logic  With each    E n is associ ated a set K     L   the set of all formulae known to be true given that    is true  For a formula dE L  Bel d  is defined to be the probability that we know d to be true i e   Bel d     L  P            d     L  P         K d  Justifying Dempster s Rule for general belief func tions is problematic  so we restrict ourselves to the combination of a finite number of simple support functions and  to justify this  use the Sources of Ev idence framework  based on Shafer s random sources canonical example  Shafer       see  Wilson      for details   Suppose we have distinct propositions n   i            m   not in L  and for each we have a prior probability Q   Suppose also that we know that if n  is true  some evidence Evd  is also true  where Evd  is a statement about the logic  it might for example be that the material implication a     c  is true  as in section       If n  is not true we know nothing about the truth of Evd   We also allow there to be a set of facts W which are known certainly to be true  n  may  as the name of the framework suggests  rep resent the event that a source of evidence  which tells us evidence Evd   is reliable  Alternatively n  may be an unknown antecedent of a rule  as described in the last section  or ni may just be some event for which  when it occurs  we are sure that the evidence Evd  is true  Let  lu be the elementary event     n  A      n   iEu  if   and let n be the mutually exclusive and exhaus tive set of elementary events   T u   u            m       Take some probability function P on       For example there is the problem of the collaps ing of the Belief Plausibility interval  e g    Pearl  We will be interested in extending DST to other log      see also Shafer s presentations of his random ics  see  Saffioti      for other work on this  and see codes canonical example  with discussion  Shafer   Ruspini      for a justification of DST using a modal   a    b       The Sources of Evidence Framework  I I I I I I I I I I I I I I I I I I I        I I I I I I I I I I I I I I I I I I I  If we think ofP as saying  for each       the probability that we are in a world in which f    is true  then Bel d  is the probability that we are in a world in which we know d is true  Bel maybe viewed as a lower probability given the probability function P on the underlying space n  It is argued in  Wilson      that  in the absence of correlation information on the n s  certain assump tions  A  and  B  are entirely reasonable   A  is roughly that  since an unreliable source rule doesn t give us any information  it shouldn t affect the prob abilities  an example of the application of this as sumption is given below in        B  is that  if the sources are not contradictory  i e         n  is not known to be impossible  then we take  for each i  P n   to be a   its prior value  These assumptions determine a unique probability function P   given by pDS c        where and       jk p     k  p       if K   inconsistent  otherwise  p       I  K consistent   IT a  ITC   a      iEo  i    This is  in fact  the probability function that leads to Dempster Shafer belief when each Evd  is that some proposition p  is true  the belief as defined above will be the same as that calculated by using Dempster s Rule to combine simple support functions with mass a  attributed to the proposition p   i            m  Since Belief  as defined here  is just  randomised logic  the calculation of Belief inherits its compu tational efficiency from that of the underlying logic  Bel  d  can be calculated  using the following Monte Carlo algorithm  For each trial     i  Pick u with probability P q     ii  If K     d then trial succeeds else trial fails  The proportion of successful trials then converges to Bel d   Given that P    u   is not too hard to calculate  the calculation takes time proportional to the time it takes to check if d E Kt   but with a fairly large constant term corresponding to the number of trials needed to get reasonable accuracy  With the probability function P   P    step  i  can be performed very easily  Since any sensible measure  of belief should collapse to the logic for the extreme case  its computational efficiency cannot hope to be better than that of the underlying logic  Thus the calculation of Dempster Shafer belief is as fast  up to a constant  as the calculation of a measure of belief could possibly be  In particular it is shown in  Wil son      that  up to arbitrary accuracy  Dempster Shafer Belief on a mutually exclusive and exhaustive frame of discernment can be calculated in time ap proximately linear in the number of evidences and size of the frame of discernment      Rules Not Allowing Contraposition  Some rules do not allow contraposition  For exam ple the rule Typically males don t have long beards seems reasonable  and even mildly informative  but on meeting someone with a long beard  it would be unreasonable to deduce that they were female  In order to represent rules not allowing contraposition  inference rules such as a   c will be used which  like the rules used in many Expert Systems  given a  al low the deduction of c  but given   c  do not allow   a to be deduced  Suppose we have a set of rules If a  then c     a   for i              m   a  s and c s closed wffs in first order logic  for which we do not wish to allow contraposition  Let I     a    c    i            m  where the  certain  inference rule a  I c  means  if we know a  we can deduce c   and let I      a  I c    ieu    For some set U of closed wffs and set of inference rules J we define Th   U  to be the logical closure of U when all the inference rules in J are added to the logic i e   the set of formulae obtained by applying all the inference rules in J repeatedly to U  so that Th   U  is the smallest set r such that  i  r   u   ii  Th r    r and  iii  if a I c E J and a E r then c E r   where Th r  means the logical closure of r within first order logic   Abbreviate Th     W  to Th   W   To represent this set of rules within the sources of evidence framework we make the ith evidence be that the inference rule ai I c  is added to the logic  so that whenever a  is known  c  may be deduced   To be precise  we set K   to be Tht   W   This includes the uncertain material implications  described in      as a special case  make  for all i  the ith inference rule equal T I  a    c             I  Example     Default Rules  Whilst attempting to deduce information about our acquaintance Nixon we learn that he is a quaker and a republican  so that W    quaker republican   Two rules  If quaker then pacifist   Ca    and  a    are also If republican then   pacifist   known  To represent these we take Evd  to be that the first rule is correct and that the corresponding inference rule quaker  pacifist should be added to the logic  and similarly for Evd   Thus if n  then  if at any time we learn quaker  we will deduce pacifist  This gives  An alternative to rules with numerical uncertainty are default rules in the absence of information in dicating that the circumstances are exceptional  the rule is fired  though the consequence of the rule may later have to be retracted  if it s discovered that cir cumstances are in fact exceptional   Ke   Th W    Th   quaker  republican   K l    Th W U  pacifist   K      Th W u    pacifist   K l      Th W U  pacifist      pacifist     Since K      is inconsistent  P nt   n   must be    In order to come up with a probability function P we make certain independence assumptions  Knowing only about one rule  the first  we would obviously take P n     a    adding an unreliable second rule doesn t give us any information so shouldn t change this probability i e   we make the assumption that P n l   n     O t  Symmetrically we make the as sumption P n j  nt    a    Both these assumptions are instances of assumption  A  mentioned above in      Only in worlds when n       n  is true  when u        do we know pacifist  and only in worlds   nl    n    u        do we know   pacifist  so      Default Logic  Reiter s Default Logic  Reiter      is a logic for rea soning with default rules  A default rule is a rule of the form  If we know a then deduce c  as long as b is consistent   or a   b I c for short  Let     D  W  be a closed default theory where L is the language of a first order logic  W  L  a set of closed wffs  are the facts and D is the set of default rules a    b                    m  c          where a   b  and c  are closed formulae  for each i  It turns out that Reiter s default logic can be ex pressed in terms of inference rules  Let I    a  I c    i              m   The behaviour of the defaults in D will be mimicked by use of the corresponding infer ence rule in I  Let S    Th Y W    y   l      m    S contains all the sets of formulae produced by applying different subsets of the inference rules to W  For some K E S an inference rule a  I c  may have been applied even though b  is inconsistent  i e     b  e K   in which case the inference rule was not behaving like the corresponding Default rule  Then we say that K is A inconsistent  Formally this prop erty can be defined as follows   I I I I I I I I I I  K E S is  consistent if and only if there exists a          m  with K   Th Y W  and K    b  for all i e     I  In default logic the extensions are intended to be the different possible completions  using the default rules  of an incomplete set of facts about the world   I   y  If the reliabilities of the two rules are the same then Bel pac ifist    Be l   pacif iat    If  on the other hand  the first rule is very reliable  but the second isn t so reliable then Bel pacifist  will be close to   and Bel   pacifist  will be close to     I  Theorem     Th Y W  where  E is an extension if and only if E i     i     b  fl  E        This shows that extensions are  consistent sets in  S  In fact we have  If E is an extension of A then E is a maximal  consistent set in S   Theorem     E is said to be  M extension of if E is a maximal     consistent set inS  Definition   an       I I I I I        I I I I I I I I I I I I I I I I I I I  M extensions are formed by applying  S many in ference rules as possible without contradicting  consistency  Theorem   showed that extensions are always M extensions   to infinity  and given      there exists N  such that for all z   N  and for all p E L Bel g  z  p  Bel g  z  p   Let  be a closed normal default theory  Then E is an extension of Ll if and only if E is a maximally  consistent set in S   Theorem     Thus for closed normal default theories E is an ex tension if and only if E is an M extension  M extensions have for general closed default theories the nice properties extensions only have for closed normal default theories  Theorem    M extension   Every closed default theory has an  Let Ll    D  W        D   W  be closed default theories with D  D   If E is an M extension of  then there exists an M extension E  of  with E      E   Theorem    Semi monotonicity    We ca n also define  an M default proof  in a n obvious way  which is complete  that is for any dosed wff p there is an M default proof of p if and only if p E E for some M extension E  It might be suggested that any M extension of a de fault theory which is not an extension is not a sen sible completion of one s knowledge  this however is not the case e g   there are apparently coherent default theories that allow no extension  see  Wil son      for an example  and also for proofs of the above results  but which  by Theorem    allow M extensions  M extensions turn out to be the modified extensions defined in  Lukaszewicz       also see  Besnard             The Sources of Evidence Framework as  a Logic  To turn the Sources of Evidence Framework into a Logic  we tend the reliabilities of the sources  the a is  to    B extensions are the sets of formulae whose belief can be made to tend to    We can consider Bel as a function Bel g  p  where        at  a         am  is the vector consisting of the re liabilities of all the sources  To use the Sources of Evidence framework to pro duce a logic we require that for any closed wff p  Bel p  tends to either   or    A B extension is then the set of formulae whose belief tends to    Formally E is a B extension if and only if for i             m there exist monotonic functions ai       oo          with ai   z  tending to   as z tends         if pEE if p E       Example Co ntinued  In the case of Nixon we have   B extensions  When the reliability of the first rule tends to   much faster than that of the second rule we get that Bel pacit ist  tends to    and Bel   paci f ist  tends to   so K l    Th W U  pacifist   is a B extension  Similarly K      Th W U    pacifist   is a B extension  Theorem  for some  u       If E is a B extension then E     Ku  If we don t have information about correlations be tween the sources we can reasonably make assump tions  A  and  B  giving p   p     Theorem    With P   P     E is a B extension if and only if E   K   for some u maximal with K   consistent      Representation of Default Rules in Sources of Evidence Framework  Default rules will be represented in the Sources of Evidence framework by treating them rather like nu merical rules with a high  but unknown  certainty  roughly speaking we make the ith evidence Evd  be that the inference rule ai I Ci is a correct rule  as we did in      and take the limit as the reliabilities of the sources  that is  the certainties of the rules  tend to    to produce the B extensions  In the example we found that the B extensions were just the same as Reiter s extensions  This was no co incidence  when the probability function pDS on   is used the B extensions are exactly the M extensions of the default theory        Closed Normal Default Theories  Let be a closed normal default theory  We want the ith evidence to be that the inference rule ai I Ci    is a correct rule  so  formally  we set Kt    Th  W      and also set P   P   Theorem    Let be a closed normal default the  ory  With the above representation of Closed Nor mal Default rules within the Sources of Evidence framework        E is a B extension     E is an M extension of     is an extension of           E  condition  b   P    To represent the consistency     we have to be a little trickier  We first  add new distinct symbols q         qm to the alphabet of the language to get a new language  L    We want the statement of the ith source to be that  inferences rules a    q   q    c     b     q  are correct rules  The idea is that knowing a  will enable us to deduce c  unless  known  in which case we will    b  is  get an inconsistency since we ll know both q  and   q    To be precise we let  be the theorems of  K  L   when the inference rules  Ju  a   q              C   q        b           q     aE  W  K  n  L   so that  Ku  mentioning some q    is  u       rule   and  P n l     n    sumption to make  This gives P n                 A n    tent  P n  A   n    o   since P n    is still an intuitive  since  K l         P   nl A n    as  is inconsis       ol a   P   nlA  n        ol  l o    and Bel     flies    o   Bel  fli es         al o   Here Th   penguin  bird    flies   is the only B       extension      Combining Numerical and Default Rules It has been shown how the Sources of Evidence  to     default  rules  The next step is to combine both  within this framework   Suppose that our knowledge includes both default  K rTh     W   wffs in L so let Ku  be  rules and numerical rules  F irst we represent both  as evidences  which add an inference rule to the logic   in the sources of evidence framework  which includes the contrapositioning rules as a special case    This gives the following result   Theorem     framework can represent either numerical rules  or   stripped of all formulae  K  In this case we specify P nl   reflect that pref  taking the limit as the reliabilities of the rules tend  are added to the logic  that is   We re only interested in the  P that  should be unaffected by the addition of a second        General Closed Default Theories We again set P  on the probability function erence   I  W ith the representation of Default  rules in the Sources of Evidence framework described above  for any set of closed wffs E  We  first consider only the default rules  and produce the  B extensions  Then we add the other rules sources to get a belief function in each B extension   E is a B extension if and only if E is an M extension   BEL  d  is defined to be the minimum   value of Bel d  over the extensions  a rather conser     Expressing Preferences between Rules  mum value of Bel d   If BEL  d  is high this gives  Suppose we have two rules      flies     al    and  Co    and two facts   If penguin then  If bird then  flie      W   penguin  bird    Expressing these as inference rules gives  as in the Nixon example   For dE  L   vative measure  BEL  d  is defined to be the maxi  at least some reason for believing d  there is some combination of default rules which if correct lend  high support to d   extensions could also be a useful measure   Another way of looking at this is to consider Bel  as a function of the unknown  but high reliabilities  g   BEL  d  is then info  Bel g  p       and BEL  d  is sup   Bel g  p   g      alJ      am       ansmg  from  Some average of Bel over the  assumptions  P n    nl    a    P n l     n       o    But since we know that penguins are a subclass of  Concluding Comments  We have given counter arguments to some of Pearl s criticisms of the use of belief functions to represent  rules and argued that the Dempster Shafer Theory  is a natural way to represent a type of If Then rule   birds it seems that the first rule should override the  If it is known that the rules are correlated  then the  get Bel    flies   which should not be changed  and so a more general belief function approach such  The preference of some sets of rules over others are  allow the dependencies between the rules to be in  second  if we only knew about the first rule we would        on learning the second rule   represented by making some different assumptions  independence assumptions may well not be justified   as the sources of evidence framework is needed to  corporated in the underlying probability function P   I I I I I I I I I I I I I I I I I I        I I I I I I I I I I  Dependencies must also be used  as described in sec tion    to represent dominance of certain rules  or chains of rules  over others  We have shown that this belief function approach also enables the repre sentation of default rules  Another very natural type of rule H a then c     a  related to that described in     is where  again  there is an unknown antecedent n with n    a   c  but instead of knowing the prior probability of n  we know the conditional probability P   n l a    a  With a number of such rules we can take  as before  Belief as a lower probability  tend the a s to   and see which Beliefs tend to    This is effectively the approach taken by Adams  Geffner and Pearl  It would be interesting to explore whether progress can be made by making independence assumptions on the n  s  as we did for the type of rule described in       It is clear that there is no single correct way of rep resenting numerical If Then rules  Future research in this area should attempt to clarify wha t different types of numerical rule there are  a nd to represent them within a single framework   I I I I I I I  Pearl  Judea      Reasoning with Belief Functions  An Analysis of Compatibility  Technical Report R      Computer Science Department  UCLA  Los Angeles  CA              November        e i  Ruspini  E  H       Epist m c Logics  Probability and the Calculus of Evidence   Conf  on AI  Proc   lOth Inti  Joint   IJCAI      Milan            Saffioti  A       A Hybrid Framework for Representing Uncertain Knowledge   AI  Proc    th Nat    Con  on   AAAI      Boston  USA   Shafer  G        A Mat hematical Theory of Evidence   Princeton University Press  Princeton  NJ    Shafer  G      a  Belief Functions and Parametric Mod els  with discussion    tical Society   Journal of th e R oyal Statis  series B      No               Shafer  G      b  Lindley s paradox  with discussion    Journal of the American Statistical Ass oc iation    Vol    No                 Shafer  G       Probability Judgment in Artificial Intelli gence and Expert Systems   Statistical  Science  Vol     No            Acknowledgements  Reiter  R       A Logic for Default Reasoning   I am greatly indebted to Mike Clarke  for numerous use ful and interesting discussions  and without whom this paper could not have been written  I have also enjoyed many productive conversations with Mike Hopkins and my colleagues on the DRUMS project   I I  Morgan Kaufmann Publishers Inc        Chapter    in particular           ed  J  Hintikka and  Default Logic   Springer Verlag   Berlin  Heidelberg  r  ew York  Buchanan  B  G   and Shortliffe  E  H        expert systems   Rule based  Reading  Mass   Addison Wesley   Dempster  A  P       Upper and Lower Probabilities In  Ann  Math  Statistics             Geffner  H       Default Reasoning  Causal and Con ditional Theories  PhD thesis  Computer Science duced by a Multi valued Mapping   Department  UCLA  Los Angeles  CA  November       Lukacewicz  Proc        Considerations on Default Logic   Non Monotonic Reasoning Workshop  New  also        Computational Intelligence     pp      Pearl  Judea      Probabilistic Reasoning in Intelli gent Systems  Networks of Plausible Inference  Paltz NY  pp          Research Report no      June       Dept  of Com puting and Mathematical Sciences  Oxford Polytech  Logic  Research Report  Dept   P  Suppes  Amsterdam  North Holland  Besnard  Philippe       and Generalisation of the Dempster Shafer Theory   W ilson  Nic      Rules  Belief Functions and Default  Adams  E       Probability and the logic of conditionals   Aspects of inductive logic              pp         Wilson  Nic      Justification  Computational Efficiency  nic   
 In this paper  we develop a qualitative theory of influence diagrams that can be used to model and solve sequential decision making tasks when only qualitative  or imprecise  information is available  Our approach is based on an orderof magnitude approximation of both probabilities and utilities and allows for specifying partially ordered preferences via sets of utility values  We also propose a dedicated variable elimination algorithm that can be applied for solving order of magnitude influence diagrams      INTRODUCTION  Influence diagrams have been widely used for the past three decades as a graphical model to formulate and solve decision problems under uncertainty  The standard formulation of an influence diagram consists of two types of information  qualitative information that defines the structure of the problem eg  the set of  discrete  chance variables describing the set of possible world configurations  the set of available decisions  as well as the dependencies between the variables  and quantitative information  also known as the parametric structure  that  together with the qualitative information  defines the model  The parametric structure is composed of the conditional probability distributions as well as the utility functions describing the decision makers preferences  In general  the solution to an influence diagram depends on both types of information  Quite often  however  we may have precise knowledge of the qualitative information but only very rough  or imprecise  estimates of the quantitative parameters  In such cases  the standard solution techniques cannot be applied directly  unless the missing information is accounted for  In this paper  we propose a qualitative theory for influence  This work was supported in part by the Science Foundation Ireland under grant no     PI I      Nic Wilson Cork Constraint Computation Centre University College Cork  Ireland n wilson  c ucc ie  diagrams in which such partially specified sequential decision problems can be modeled and solved  In particular  we introduce the order of magnitude influence diagram model that uses an order of magnitude representation of the probabilities and utilities  The model allows the decision maker to specify partially ordered preferences via finite sets of utility values  In this case  there will typically not be a unique maximal value of the expected utility  but rather a set of them  To compute this set and also the corresponding decision policy we propose a dedicated variable elimination algorithm that performs efficient operations on sets of utility values  Numerical experiments on selected classes of influence diagrams show that as the quantitative information becomes more precise  the qualitative decision process becomes closer to the standard one  The paper is organized as follows  Section   gives background on influence diagrams  In Section   we present the order of magnitude calculus as a representation framework for imprecise probabilities and utilities  Sections   and   describe the main operations over sets of order ofmagnitude values and introduce the order of magnitude influence diagram model  In Section   we present the results of our empirical evaluation  Section   overviews related work  while Section   provides concluding remarks      INFLUENCE DIAGRAMS  An influence diagram is defined by a tuple hX  D  U  Gi  where X    X            Xn   is a set of oval shaped nodes labeled by the chance variables which specify the uncertain decision environment  D    D            Dm   is a set of rectangle shaped nodes labeled by the decision variables which specify the possible decisions to be made in the domain  U    U            Ur   are diamond shaped nodes labeled by the utility functions which represent the preferences of the decision maker  and G is a directed acyclic graph containing all the nodes X  D  U  As in belief networks  each chance variable Xi  X is associated with a conditional probability table  CPT  Pi   P  Xi  pa Xi     where pa Xi    X  D    Xi   are the parents of Xi in   The wildcatter could do a seismic test that will help determine the geological structure of the site  The test results can show a closed reflection pattern  indication of significant oil   an open pattern  indication of some oil   or a diffuse pattern  almost no hope of oil   The probabilistic knowledge consists of the CPTs P  O  and P  S O  T    while the utility function is the sum of U   T   and U   D  O   The optimal policy is to perform the seismic test and to drill only if the test results show an open or a closed pattern  The maximum expected utility of this policy is         Figure    The oil wildcatter influence diagram  G  Similarly  each decision variable Dk  D has a parent set pa Dk    X  D    Dk   in G  denoting the variables whose values will be known at the time of the decision and may affect directly the decision  Non forgetting is typically assumed for an influence diagram  meaning that a decision node and its parents are parents to all subsequent decisions  Finally  each utility node Uj  U is associated with a utility function that depends only on the parents pa Uj   of Uj   The decision variables in an influence diagram are typically assumed to be temporally ordered  Let D    D         Dm be the order in which the decisions are to be made  The chance variables can be partitioned into a collection of disjoint sets I    I            Im   For each k  where     k   m  Ik is the set of chance variables that are observed between Dk and Dk     I  is the set of initial evidence variables that are observed before the first D    Im is the set of chance variables left unobserved when the last decision Dm is made  This induces a partial order  over X  D  as follows  I   D   I       Dm  Im      A decision policy  or strategy  for an influence diagram is a list of decision rules                 m   consisting of one rule for each decision variable  A decision rule for the decision Dk  D is a mapping k   pa Dk    Dk   where for a set S  X  D  S is the Cartesian product of the individual domains of the variables in S  Solving an influence diagram is to find the optimal decision policy that maximizes the expected utility  The maximum expected utility  MEU  is equal to    r n X X X X Y  Pi  max    Uj      max I   D   Im   Dm  Im  i    j    Example   For illustration  consider the influence diagram displayed in Figure   which is based on the classic oil wildcatter decision problem      An oil wildcatter must decide either to drill or not to drill for oil at a specific site   Variable Elimination Several exact methods have been proposed over the past decades for solving influence diagrams using local computations                        These methods adapted classical variable elimination techniques  which compute a type of marginalization over a combination of local functions  in order to handle the multiple types ofPinformation  probabilities and utilities   marginalization   and max  and combination   for probabilities    for utilities  Pinvolved in influence diagrams  Since the alternation of and max in Eq    does not commute in general  it prevents the solution technique from eliminating variables in any ordering  Therefore  the computation dictated by Eq    must be performed along a legal elimination ordering that respects   namely the reverse of the elimination ordering is some extension of  to a total order             FOUNDATIONS  Our approach towards a qualitative theory for influence diagrams is based on the qualitative decision theory proposed by Wilson       Wilsons theory defines a set of abstract quantities called extended reals  denoted by R   that are used to represent qualitative probabilities and utilities  Each extended real is a rational function p q where p and q are polynomials in o with coefficients in the rationals  where o is a very small but unknown quantity so that the extended reals can be used to represent information up to o precision  For example  quantities such as  o and o might be used for qualitative probabilities likely and unlikely respectively  and o  for a high utility  These quantities can then be combined using standard arithmetic operations between polynomials for computing expected qualitative utilities  The resulting utilities are then compared among each other by means of a total order on R that is defined in            ORDER OF MAGNITUDE CALCULUS  Rather than using extended reals explicitly  we adopt a simpler calculus that allows us to reason about the order of magnitude of the extended reals       We start with the definition of an order of magnitude value that represents a qualitative probability or utility value  D EFINITION   An order of magnitude value is a pair   h  ni  where           is called the sign and n  Z is called the order of magnitude  respectively  Intuitively  for each integer n we have an element h   ni meaning of order on   and an element h  ni meaning of order on   Moreover  if we add something of order on to something of order on then the result can be of order om   for any m  n  To ensure closure of the calculus under addition  we therefore add the element h  ni representing this set of possibilities  In the following  we also define O    h  ni   n  Z               h  i   O    h  ni   n  Z      and O     h   ni   n  Z       The element h  i will sometimes be written as    element h    i as    and element h   i as     Standard arithmetic operations such as multiplication    and addition     follow from the semantics of the order ofmagnitude values      and are defined next  D EFINITION    multiplication  Let a  b  O be such that a   h  mi and b   h  ni  We define a  b   h    m   ni  where    n   n       for n  Z     and  is the natural multiplication of signs  namely it is the commutative operation on         such that                      and                   This multiplication is associative and commutative  and a  O  a        and a      a  respectively  Furthermore  for b  O   O   we define b  to be the multiplicative inverse of b  namely h  mi    h  mi for          Given a  O  we define a b   a  b    D EFINITION    addition  Let a  b  O be such that a   h  mi and b   h  ni  We define a   b to be      h  mi if m   n      h  ni if m   n      h    mi if m   n  where                  and otherwise         Addition is associative and commutative  and a       a  a  O  For a  b  O  let b      b and a  b   a  b   Clearly  we can write h  mi   h  mi  where                and        We also have the distributivity  a  b  c  O   a   b   c   a  c   b  c       ORDERING ON SETS OF ORDER OF MAGNITUDE VALUES  We will use the following ordering over the elements of O  which is slightly stronger than that defined in       D EFINITION    ordering  Let a  b  O be such that a   h  mi and b   h  ni  We define the binary relation   on O by a   b if and only if either           and      and m  n  or          and     and m  n  or          and      or         and     and m  n  or         and     and m  n  Given a  b  O  if a   b then we say that a dominates b  For A  B  O  we say that A   B if every element of  B is dominated by some element of A  so that A contains as least as large elements as B   namely if for all b  B there exists a  A with a   b  As usual  we write a  b if and only if a   b and it is not the case that b   a  It is easy to see that   is a partial order on O and the following monotonicity property holds  P ROPOSITION   Let a  b  c  O  If a   b then a   c   b   c  and if a   b and c  O  then a  c   b  c  Any finite set of order of magnitude values can therefore be represented by its maximal elements with respect to    D EFINITION    maximal set  Given a finite set A  O  we define the maximal set of A  denoted by max   A   to be the set consisting of the undominated elements in A  namely max   A     a  A   b  A such that b  a       OPERATIONS ON SETS OF ORDER OF MAGNITUDE VALUES  We introduce now the main operations that can be performed over partially ordered finite sets of order ofmagnitude values  In particular  we extend the addition     and multiplication    operations from singleton to sets of order of magnitude values as well as define a maximization operation over such sets       ADDITION  MULTIPLICATION AND MAXIMIZATION  Given two finite sets A  B  O and q  O    we define the summation and multiplication operations as A   B    a   b   a  A  b  B  and q  A    q  a   a  A   respectively  The maximization operation is defined by max A  B    max   A  B   In order to use the order of magnitude calculus to define a qualitative version of influence diagrams we need to be sure that each of     and max is commutative and associative  and also to give sufficient conditions such that the following distributivity properties hold  q  q    q   O  and A  B  C  O D  q   A   B     q  A     q  B  D   q    q     A    q   A     q   A  D  max A  B    C   max A  C    max B  C  It is easy to see that     and max are commutative and associative  and the distributivity properties  D   and  D   hold as well  Unfortunately  the distributivity property  D   does not always hold for sets of order of magnitude values  To give a simple example  let q    h    i  q    h    i and let A    h   i  h   i   Then   q    q     A yields the set    h   i  h   i   whereas  q   A     q   A  is equal to  h   i  h   i  h   i   This property does however hold for convex sets  as we will show next   We can show now that any finite subset of O is in fact equivalent with a set of order of magnitude values containing one or two elements  namely        T HEOREM   Let A be any finite subset of O  Then either A   a  for some a  O  or  m  n  Z with m   n and           such that A   h  mi  h  ni    CONVEX SETS AND CONVEX CLOSURE  Based on Definition    every element of a finite set A  O is dominated by some maximal element in A  We can therefore define an equivalence relation between finite sets of order of magnitude values  as follows  D EFINITION    relation   Given two finite sets A  B  O  we say that A is  equivalent with B  denoted by A  B  if and only if A   B and B   A  Clearly   is an equivalence relation  namely it is reflexive  symmetric and transitive  We then have that  P ROPOSITION   Let A  B  C  O be finite sets and let q  O    The following properties hold      A  B if and only if max   A    max   B       if A  B then A   C  B   C and q  A  q  B  We introduce next the notions of convex sets and convex closure of sets of order of magnitude values  D EFINITION   A set A  O is said to be convex if q    q   O  with q    q       and a  b  A  we have that  q   a     q   b   A  The convex closure C A  of a set AP O is defined to consist of every element of k the form i    qi  ai    where k is an arbitrary natural Pk number  each ai  A  each qi  O  and i   qi      Consider two elements h  mi and h  ni in O  where we can assume without loss of generality that m  n  Any convex combination of these two elements is of the form h  li where l   m  n  and if l   n then      if l   n then      or       This implies that the convex combination of a finite number of non zero elements is finite  since every element a in the convex combination has its order restricted to be within a finite range   and so  in particular can be represented by its maximal set  In fact  this property holds even if we allow the zero element h  i  We can define now the following equivalence relation  D EFINITION    relation   Given the finite sets A  B  O  we say that A is  equivalent with B  denoted by A  B  if and only if C A   C B   Therefore  two sets of order of magnitude values are considered equivalent if  for every convex combination of elements of one  there is a convex combination of elements of the other which is at least as good  P ROPOSITION   Let A  B  C  O be finite sets and let q  O    The following properties hold      A  B if and only if max   C A     max   C B        if A  B then A   C  B   C  q  A  q  B  and A  C  B  C        OPERATIONS ON EQUIVALENT SETS OF ORDER OF MAGNITUDE VALUES  Theorem   allows us to efficiently perform the required operations  ie  summation  multiplication and maximization  on sets of order of magnitude values  We assume that the subsets O are either singleton sets or are of the form  h  mi  h  ni   where m   n  We need to ensure that the outputs are of this form as well  For a given a  O  we use the notation  a  and a to denote the sign and the order of magnitude of a  respectively  Multiplication Given A  O of the required form  and q  O    we need to generate a set A that is  equivalent with q  A  Write q as h   li  If A    h  mi  then q  A is just equal to the singleton set  h  l   mi   Otherwise  A is of the form  h  mi  h  ni   where m   n  Then q  A equals  h  l   mi  h  l   ni   which is of the required form  since l   m   l   n  Maximization Given the sets A    A            Ak  O  each of them having the required form  we want to compute a set A that is  equivalent to max A            Ak    Let A   A       Ak and  for            we define m and n as follows  if there exists no element a  A with  a     then we say that m and n are both undefined  otherwise we have that m   min l   h  li  A  and n   max l   h  li  A   respectively  The set A is computed as follows      if m  and m are both undefined  there are only negative elements  then A    h  n i       if m  is defined and either m   m or m is undefined then A    h   m  i       if m    m  and both are defined  then A    h  m i  h   m  i       if m  is undefined  no positive elements  and either n  n or n is undefined then A    h  m i  h  n i    and     if m  is undefined  there are no positive elements  and n   n then A    h  m i  h  n i   Summation Given the sets A    A            Ak  O of required form as before  we want to compute a set A that is  equivalent to  A         Ak    We can write Ai as  ai   bi   where if ai    bi then  ai      and ai   bi   Then   A         Ak     a  b  where a   a         ak and b   b         bk   We can write b more explicitly as h b   bi where b   min b            bk    and  b      if and only if all bi with minimum bi have  bi        else  b     if all bi with minimum bi have  bi       else  b      Similarly for a  If  a      then  a  b  reduces to a singleton because a   b    Example   Consider the sets A     h   i  h   i  and A     h   i  h   i   To generate A  max A    A     we first compute m     and n      and then we have that A    h   i  h   i  which corresponds to the extreme points of the input sets  Similarly  we can compute the set A   A    A    as  h   i  h   i        DISTRIBUTIVITY PROPERTIES REVISITED  In summary  we can show now that all three distributivity properties hold with respect to the  equivalence relation between finite sets of order of magnitude values  T HEOREM   q  q    q   O  and A  B  C  O finite sets we have that   D   q   A   B    q  A     q  B   and  D    q    q     A   q   A     q   A   and  D   max A  B    C  max A  C    max B  C       ORDER OF MAGNITUDE INFLUENCE DIAGRAMS  In this section  we introduce a new qualitative version of the influence diagram model based on an order of magnitude representation of the probabilities and utilities       Table    Optimal policies sets for order of magnitude influence diagrams corresponding to the oil wildcatter problem  decision rule Test  Drill   S closed  T yes S open  T yes S diffuse  T yes S closed  T no S open  T no S diffuse  T no order of magnitude MEU  o        yes no  yes yes no yes yes yes h    i  OOM ID o        o          yes  no   yes  no  yes  yes  no  yes  yes  no   yes  no   yes  no  yes  yes  no  yes  yes  no  yes  yes  no  h    i  h   i  h   i   THE QUALITATIVE DECISION MODEL  An order of magnitude influence diagram  OOM ID  is a qualitative counterpart of the standard influence diagram graphical model  The graphical structure of an OOM ID is identical to that of a standard ID  namely it is a directed acyclic graph containing chance nodes  circles  for the random discrete variables X  decision nodes  rectangles  for the decision variables D  and utility nodes  diamonds  for the local utility functions U of the decision maker  The directed arcs in the OOM ID represent the same dependencies between the variables as in the standard model  Each chance node Xi  X is associated with a conditional probability distribution Pio that maps every configuration of its scope to a positive order of magnitude probability value  namely Pio   Xi pa Xi    O    The utility functions Ujo  U represent partially ordered preferences which are expressed by finite sets of order of magnitude values  namely Ujo   Qj   O   where Qj is the scope of Uj   Solving an order of magnitude influence diagram is to find the optimal policy                 mQ  that maximizes Pr the n order of magnitude expected utility i   Pio  j   Ujo   We define the optimal policies set of an order of magnitude influence diagram to be the set of all policies having the same maximum order of magnitude expected utility       Figure    Order of magnitude probability and utility functions corresponding to the oil wildcatter influence diagram   AN EXAMPLE  Figure   displays the order of magnitude probability and utility functions of an OOM ID corresponding to the oil  wildcatter decision problem from Example    For our purpose  we used an extension of Spohns mapping from the original probability distributions and utility functions to their corresponding order of magnitude approximation          Specifically  given a small positive o      the order of magnitude approximation of a probability value p         is h   ki such that k  Z and ok     p  ok   while the order of magnitude approximation of a positive utility value u     is h   ki such that ok  u   o k     the case of negative utilities is symmetric   For example  if we consider o       then the probability P  S   closed O   dry  T   yes         is mapped to h    i  while the utilities U   O   dry  D   yes       and U   O   soaking  D   yes        are mapped to h   i and h    i  respectively  Table   shows the optimal policies sets  including the maximum order of magnitude expected utility  obtained for the order of magnitude influence diagrams corresponding to o                      When o        we can see that there are two optimal policies having the same maximum order of magnitude expected utility  namely    for T   yes  and    for T   no   Therefore  if the seismic test is performed  T   yes  then drilling is to be done only if the test results show an open or closed pattern  Otherwise  T   no   the wildcatter will drill regardless of the test results  Ties like these at the decision variables are expected given that the order of magnitude probabilities and utilities represent abstractions of the real values  The expected utilities of   and   in the original influence diagram are         and        respectively   Algorithm    ELIM OOM ID  When o         we also see that both drilling options are equally possible if the seismic test is performed and the test results show a diffuse pattern  In this case  there are four optimal policies having the same maximum order ofmagnitude expected utility  Finally  when o          we can see that all decision options are possible and the corresponding optimal policies set contains     policies  The explanation is that the order of magnitude influence diagram contains in this case only trivial order of magnitude values such as h    i  h   i and h   i  respectively   Data  An OOM ID hX  D  U  Gi  bucket structure along a legal elimination ordering of the variables o Result  An optimal policy     top down phase for p   t downto   do let p             j   and p             k   be the probability and utility components in buckets p  if Yp is a chance variable then P Q p  Yp ji   i Q P P p   p     Yp    ji   i      kj   j                else if Yp is a decision variable then P if p    then p  maxYp kj   j else Q p  maxYp ji   i Q P p   maxYp    ji   i      kj   j                 VARIABLE ELIMINATION        Theorem   ensures the soundness and correctness of a variable elimination procedure using the summation      multiplication    and maximization  max  operations over partially ordered sets of order of magnitude values  for solving order of magnitude influence diagrams  Therefore  a variable elimination algorithm that computes the optimal policy of an order of magnitude influence diagram  and also the maximum order of magnitude expected utility  is described by Algorithm    The algorithm  called ELIM OOM ID  is based on Dechters bucket elimination framework for standard influence diagrams     and uses a bucket structure constructed along a legal elimination ordering o   Y          Yt of the variables in X  D  The bucket data structure  called buckets  associates each bucket with a single variable  The bucket of Yp contains all input probability and utility functions whose highest variable is Yp   The algorithm processes each bucket  top down from the last to the first  by a variable elimination procedure that computes new probability  denoted by   and utility  denoted by   components which are then placed in corresponding lower buckets  lines       The p of a chance bucket is generated by multiplying all probability components and eliminating by summation the bucket variable  The p of a chance bucket is computed as the average utility of the bucket  normalized by the buckets compiled p   For a decision variable  we compute the p and p components in a similar manner and eliminate the bucket variable by maximization  In this case  the product of the probability components in the bucket is a constant when viewed as a function of the buckets decision variable         and therefore  the compiled p is a constant as well  In the second  bottom up phase  the algorithm computes an optimal policy  The decision buckets are processed in reverse order  from the first variable to the last  Each decision rule is generated by taking the argument of the maximization operator applied over the combination of the probability and utility components in the respective bucket  for each configuration of the variables in the buckets scope  ie  the union of the scopes of all functions in that bucket minus the      place each p and p in the bucket of the highest index variable in its scope    bottom up phase for p     to t do if Yp is a decision variable then P Q p  arg maxYp    ji   i      kj   j        p      return                 bucket variable Yp    T HEOREM    complexity  Given an OOM ID with n variables  algorithm ELIM OOM ID is time and space O n   k wo    where wo is the treewidth of the legal elimination ordering o and k bounds the domain size of the variables      EXPERIMENTS  In this section  we evaluate empirically the quality of the decision policies obtained for order of magnitude influence diagrams  All experiments were carried out on a    GHz quad core processor with  GB of RAM  Methodology We experimented with random influence diagrams described by the parameters hnc   nd   k  p  r  ai  where nc is the number of chance variables  nd is the number of decision variables  k is the maximum domain size  p is the number of parents in the graph for each variable  r is the number of root nodes and a is the arity of the utility functions  The structure of the influence diagram is created by randomly picking nc   nd  r variables out of nc   nd and  for each  selecting p parents from their preceding variables  relative to some ordering  whilst ensuring that the decision variables are connected by a directed path  A single utility node with a parents picked randomly from the chance and decision nodes is then added to the graph  We generated two classes of random problems with parameters hn               i and having either positive utilities only or mixed  positive and negative  utilities  They are   denoted by P   hn               i and M   hn               i  respectively  In each case      of the chance nodes were assigned extreme CPTs which were populated with numbers drawn uniformly at random between     and       whilst ensuring that the table is normalized  The remaining CPTs were randomly filled using a uniform distribution between   and    For class P   the utilities are of the form   u   where u is an integer uniformly distributed between   and    For class M   the utilities are of the form    u or   u   where u is between   and    as before  and we have an equal number of positive and negative utility values  Each influence diagram instance was then converted into a corresponding order of magnitude influence diagram using the mapping of the probabilities and utilities described in Section      for some o      Intuitively  the smaller o is  the coarser the order of magnitude approximation of the exact probability and utility values  ie  more information is lost    Results Figure   displays the distribution of the relative errors med  top  and max  bottom  obtained on orderof magnitude influence diagrams derived from class P  ie  positive utilities   as a function of the problem size  given by the number of variables   for o                      Each data point and corresponding error bar represents the   th   median and   th percentiles obtained over    random problem instances generated for the respective problem size  We can see that med is the smallest  less than      for o        However  as o decreases  the loss of information due to the order of magnitude abstraction increases and the corresponding relative errors med increase significantly  Notice that the best policy max derived from the order of magnitude influence diagram was almost identical to that of the corresponding standard influence diagram  for all o  ie  the error max is virtually zero   Figure   shows the distribution of med  top  and max  bottom  obtained on order of magnitude influence diagrams from class M  ie  mixed utilities   The pattern of the results is similar to that from the previous case  However  in this case  the errors span over two or three orders of  relative error of OOM median policy                                                           variables                 variables                  relative error of OOM best policy      Measures of Performance To measure how close the decision policies derived from the optimal policy set of an order of magnitude influence diagram are to the optimal policy of the corresponding standard influence diagram  we use two relative errors  defined as follows  Let I be an influence diagram and let Io be the corresponding order ofmagnitude approximation  for some o value  We sample s different policies  uniformly at random  from the optimal policies set of Io   and for each sampled policy we compute its expected utility in I  Let med be a policy corresponding to the median expected utility vmed amongst the samples  We define the relative error med     v  vmed   v   where v is the maximum expected utility of the optimal policy in I  Similarly  we define max     v  vmax   v   where max is the best policy having the highest expected utility vmax amongst the samples                                                         Figure    Results for class P influence diagrams  We show the distribution of the relative errors med  top  and max  bottom  for o                        of samples s        magnitude  especially for o        and        This is because the sampled policy space includes policies which are quite different from each other and  although they have the same maximum order of magnitude expected utility  their expected utility in the corresponding standard influence diagram is significantly different  For this reason  we looked in more detail at the distribution of the expected utility values of     policies sampled uniformly at random from the optimal policies set of a class M OOM ID instance with    variables  for o                      As expected  we observed that the smallest sample variance is obtained for o        For o        and o          the samples are spread out even more from the mean  and the variance of the expected utility is significantly larger  This explains the large variations of the relative errors med and max   especially for smaller o values  eg  o        and o               RELATED WORK  Several extensions of the standard influence diagram model have been proposed in recent years to deal with imprecise probabilistic and utility information  Garcia and Sabbadin     introduced possibilistic influence diagrams to model and solve decision making problems under qualitative uncertainty in the framework of possibilistic theory  Pralet et al     considered a generalized influence diagram system   attribute utility allowing trade offs   relative error of OOM median policy               
