 The Bayesian Logic  BLOG  language was recently developed for defining first order probability models over worlds with unknown numbers of objects  It handles important problems in AI  including data association and population estimation  This paper extends BLOG by adopting generative processes over function spaces  known as nonparametrics in the Bayesian literature  We introduce syntax for reasoning about arbitrary collections of objects  and their properties  in an intuitive manner  By exploiting exchangeability  distributions over unknown objects and their attributes are cast as Dirichlet processes  which resolve difficulties in model selection and inference caused by varying numbers of objects  We demonstrate these concepts with application to citation matching      Introduction  Probabilistic first order logic has played a prominent role in recent attempts to develop more expressive models in artificial intelligence                           Among these  the Bayesian logic  BLOG  approach      stands out for its ability to handle unknown numbers of objects and data association in a coherent fashion  and it does not assume unique names and domain closure  A BLOG model specifies a probability distribution over possible worlds of a typed  first order language  That is  it defines a probabilistic model over objects and their attributes  A model structure corresponds to a possible world  which is obtained by extending each object type and interpreting each function symbol  Objects can either be guaranteed  meaning the extension of a type is fixed  or they can be generated from a distribution  For example  in the aircraft tracking domain      the times and radar blips are known  and the number of unknown aircraft may vary in possible worlds  BLOG as a case study provides a strong argument for Bayesian hierarchical methodology as a basis for probabilistic first order logic   BLOG specifies a prior over the number of objects  In many domains  however  it is unreasonable for the user to suggest such a proper  data independent prior  An investigation of this issue was the seed that grew into our proposal for Nonparametric Bayesian Logic  or NP BLOG  a language which extends the original framework developed in       NP BLOG is distinguished by its ability to handle object attributes that are generated by unbounded sets of objects  It also permits arbitrary collections of attributes drawn from unbounded sets  We extend the BLOG language by adopting Bayesian nonparametrics  which are probabilistic models with infinitely many parameters      The statistics community has long stressed the need for models that avoid commiting to restrictive assumptions regarding the underlying population  Nonparametric models specify distributions over function spaces  a natural fit with Bayesian methods  since they can be incorporated as prior information and then implemented at the inference level via Bayes theorem  In this paper  we recognize that Bayesian nonparametric methods have an important role to play in first order probabilistic inference as well  We start with a simple example that introduces some concepts necessary to understanding the main points of the paper  Consider a variation of the problem explored in       You have just gone to the candy store and have bought a box of Smarties  or M Ms   and you would like to discover how many colours there are  while avoiding the temptation to eat them    Even though there is an infinite number of colours to choose from  the candies are coloured from a finite set  Due to the manufacturing process  Smarties may be slightly discoloured  You would like to discover the unknown  true  set of colours by randomly picking Smarties from the box and observing their colours  After a certain number of draws  you would like to answer questions such as  How many different colours are in the box  Do two Smarties have the same colour  What is the probability that the first candy you select from a new box is a colour you have never seen before  The graphical representation of the BLOG model is shown in Fig   a  The number of Smarties of different colours  n Smartie   is chosen from a Poisson distribution with   it is unreasonable to expect a domain expert to implement nonparametrics considering the degree of effort required to grasp these abstract notions  We show that Bayesian nonparametrics lead to sophisticated representations that can be easier to implement than their parametric counterparts  We formulate a language that allows one to specify nonparametric models in an intuitive manner  while hiding complicated implementation details from the user  Sec    formalizes our proposed language extension as a set of rules that map code to a nonparametric generative process  We emphasize that NP BLOG is an extension to the BLOG language  so it retains all the functionality specified in        Figure     a  The BLOG and  b  NP BLOG graphical models for counting Smarties  The latter implements a Dirichlet process mixture  The shaded nodes are observations  mean Smartie   A colour for each Smartie s is drawn from the distribution HColourDist   Then  for every draw d  zSmartieDrawn  d  is drawn uniformly from the set of Smarties             n Smartie    Finally  we sample the observed  noisy colour of each draw conditioned on zSmartieDrawn  d  and the true colours of the Smarties  The NP BLOG model for the same setting is shown in Fig   b  The true colours of an infinite sequence of Smarties s are sampled from HColourDist   Smartie is a distribution over the choice of coloured Smarties  and is sampled from a uniform Dirichlet distribution with parameter Smartie   Once the Smarties and their colours are generated  the true Smartie for draw d  represented by the indicator zSmartieDrawn  d    s  is sampled from the distribution of Smarties Smartie   The last step is to sample the observed colour  which remains the same as in the BLOG model  One advantage of the NP BLOG model is that it determines a posterior over the number of Smarties colours without having to specify a prior over n Smartie   This is important since this prior is difficult to specify in many domains  A more significant advantage is that NP BLOG explicitly models a distribution over the collection of Smarties  This is not an improvement in expressiveness  one can always reverse engineer a parametric model given a target nonparametric model in a specific setting  Rather  nonparametrics facilitate the resolution of queries on unbounded sets  such as the colours of Smarties  This plays a key role in making inference tractable in sophisticated models with object properties that are themselves unbounded collections of objects  This is the case with the citation matching model in Sec       in which publications have collections of authors  The skeptic might still say  despite these advantages  that  We focus on an important class of nonparametric methods  the Dirichlet process  DP   because it handles distributions over unbounded sets of objects as long as the objects themselves are infinitely exchangeable  a notion formalized in Sec       The nonparametric nature of DPs makes them suitable for solving model selection problems that arise in the face of identity uncertainty and unknown numbers of objects  Understanding the Dirichlet process is integral to understanding NP BLOG  so we devote a section to it  Sec      shows how DPs can characterize collections of objects  Models based on DPs have been shown to be capable of solving a variety of difficult tasks  such as topic document retrieval          Provided the necessary expert knowledge  our approach can attack these applications  and others  We conduct a citation matching experiment in Sec     demonstrating accurate and efficient probabilistic inference in a real world problem      Dirichlet processes  A Dirichlet process G     H  DP    H   with parameter  and base measure H  is the unique probability measure defined G on the space of all probability measures    B   where  is the sample space  satisfying  G           G K    Dirichlet H           H K       for every measurable partition             K of   The base measure H defines the expectation of each partition  and  is a precision parameter  One can consider the DP as a generalization of the Dirichlet distribution to infinite spaces  In Sec       we formalize exchangeability of unknown objects  In order to explain the connection between exchangeability and the DP  it is instructive to construct DPs with the Polya urn scheme      Consider an urn with balls of K possible colours  in which the probability of the first ball being colour k is given by Hk   We draw a ball from the urn  observe its colour     then return it to the urn along with another ball of the same colour  We then make another draw  observing its colour with probability p     k        Hk       k         After N observations  the colour of the next ball is distributed as Hk   P  N      k   N      N  PN   i   k     N  i          The marginal P    N   of this process  obtained by applying the chain rule to successive predictive distributions  can be shown to satisfy the infinite mixture representation   Z K PN Y    k  i P    N     DP H  d       k i   M   k    where the k are multinomial success rates of each colour k  This result  a manifestation of de Finettis theorem  establishes the existence and uniqueness of the DP prior for the Polya urn scheme      In the Polya urn setting  observations i are infinitely exchangeable and independently distributed given the measure G  Thus  what we have established here in a somewhat cursory fashion is the appropriateness of the DP for the case when the observations i are infinitely exchangeable  Analogously  if the urn allows for infinitely many colours  then for any measurable interval  of  we have p N        N      N   X H      i      N    N i    The first term in this expansion corresponds to prior knowledge and the second term corresponds to the empirical distribution  Larger values of  indicate more confidence in the prior H  Note that  as N increases  most of the colours will be repeated  Asymptotically  one ends up sampling colours from a possibly large but finite set of colours  achieving a clustering effect  Nonetheless  there is always some probability of generating a new cluster  DPs are essential building blocks in our formulation of nonparametric first order logic  In the literature  these blocks are used to construct more flexible models  such as DP mixtures and hierarchical or nested DPs          Since observations are provably discrete  DP mixtures add an additional layer xi  P  xi  i   in order to model continuous draws xi from discrete mixture components i   In the Polya urn scheme  G is integrated out and the i s are sampled directly from H  Most algorithms for sampling DPs are based on this scheme              In the hierarchies constructed by our language  however  we rely on an explicit representation of the measure G since it is not clear we can always integrate it out  even when the measures are conjugate  This compels us to use the stick breaking construction       which establishes that i i d  sequences wk  Beta      and k  H can be used Pto construct the equivalent empirical distribution G   k   k  k    Qk  where the stick breaking weights k   wk j       wj   and can be shown to sum to unity  We abbreviate the sampling of the weights as k  Stick    This shows that G is an infinite sum of discrete values  The DP mixture due to the stick breaking construction is i   H  H zi            Stick   xi   i   zi  p xi  zi          where zi   k indicates that sample xi belongs to component k  The Smarties model  Fig   b  is in fact an example of a DP mixture  where the unbounded set of colours is   By grounding on the support of the observations  the true number of colours K is finite  At the same time  the DP mixture is open about seeing new colours as new Smarties are drawn  In NP BLOG  the unknown objects are the mixture components  NP BLOG semantics  Sec     define arbitrary hierarchies of Dirichlet process mixtures  By the stick breaking construction      every random variable xi has a countable set of ancestors  the unknown objects   hence DP mixtures preserve the well definedness of BLOG models  To infer the hidden variables of our models  we employ the efficient blocked Gibbs sampling algorithm developed in     as one of the steps in the overall Gibbs sampler  One complication in inference stems from the fact that a product of Dirichlets is difficult to simulate  Teh et al       provide a solution using an auxiliary variable sampling scheme      Syntax and semantics  This section formalizes the NP BLOG language by specifying a procedure that takes a set of statements L in the language and returns a model   A model comprises a set of types  function symbols  and a distribution over possible worlds      We underline that our language retains all the functionality of BLOG  Unknown objects must be infinitely exchangeable  but this trivially the case in BLOG  Sec      elaborates on this  We illustrate the concepts introduced in this section with an application to citation matching  Even though our citation matching model doesnt touch upon all the interesting aspects of NP BLOG  the reader will hopefully find it instrumental in understanding the semantics      Citation matching One of the main challenges in developing an automated citation matching system is the resolution of identity uncertainty  for each citation  we would like to recover its true title and authors  For instance  the following citations from the CiteSeer database probably refer to the same paper  Kozierok  Robin  and Maes  Pattie  A Learning Interface Agent for Meeting Scheduling  Proceedings of the      International Workshop on Intelligent user Interfaces  ACM Press  NY  R  Kozierok and P  Maes  A learning interface agent for scheduling meetings  In W  D  Gray  W  E  Heey  and D  Murray  editors  Proc  of the Internation al Workshop on Intelligent User Interfaces  Orlando FL  New York        ACM Press   Even after assuming the title and author strings have been segmented into separate fields  an open research problem itself    citation matching still exhibits serious challenges  two different strings may refer to the same author  e g  J F G  de Freitas and Nando de Freitas  and  conversely  the same string may refer to different authors  e g  David Lowe in vision and David Lowe in quantum field theory        type Author  type Pub  type Citation     guaranteed Citation      Author  NumAuthorsDist        Pub  NumPubsDist       Name a   NameDist       Title p   TitleDist       NumAuthors p   NumAuthorsDist       RefAuthor p  i  if Less i  NumAuthors p   then  Uniform Author a      RefPub c   Uniform Pub p      CitedTitle c   TitleStrDist Title RefPub c        CitedName c  i  if Less i  NumAuthors RefPub c    then  NameStrDist Name RefAuthor RefPub c   i      Figure    BLOG model for citation matching          type Author  type Pub     type Citation  type AuthorMention     guaranteed Citation  guaranteed AuthorMention     Name a   NameDist       Title p   TitleDist       CitedTitle c   TitleStrDist Title RefPub c        RefAuthor u   PubAuthorsDist RefPub CitedIn u        CitedName u   NameStrDist Name RefAuthor u      Figure    NP BLOG model for citation matching  There are a number of different approaches to this problem  Pasula et al  incorporate unknown objects and identity uncertainty into a probabilistic relational model       Wellner et al  resolve identity uncertainty by computing the optimal graph partition in a conditional random field       We elaborate on the BLOG model presented in      in order to contrast it with the one we propose  The BLOG model is shown in Fig    with cosmetic modifications and the function declaration statements omitted  The BLOG model describes a generative sampling process  Line   declares the object types  and line   declares that the citations are guaranteed  hence are not generated by a number statement   Lines   and   are number statements  and lines      are dependency statements  their combination defines a generative process  The process starts by choosing a certain number of authors and publications from their respective prior distributions  Then it samples author names  publication titles and the number of authors per publication  For each author string i in a citation  we choose the referring author from the set of authors  Finally  the properties of the citation objects are chosen  For example  generating an interpretation of CitedTitle c  for citation c requires values for RefPub c  and estimates of publication titles  TitleStrDist s  can be interpreted as a measure that adds noise in the form of perturbations to input string s  The NP BLOG model in Fig    follows a similar generative approach  the key differences being that it samples collections of unknown objects from DPs  and it allows for uncertainty in the order of authors in publications  But what do we gain by implementing nonparametrics  The advan   Figure    Three representations of lines     in Fig     as an NP BLOG program  as a generative process  and as a graphical model  Darker  hatched nodes are fixed or generated from other lines and shaded nodes are observed  Note the similarity between the graphical model and Fig   b  Lines     describe a DP mixture     over the publications p  where the base measure is TitleDist   Title is the hidden distribution over publication objects  the indicators are the true publications zRefPub  c  corresponding to the citations c  and the continuous observations are the titles CitedTitle  c   tage lies in the ability to capture sophisticated models of unbounded sets of objects in a high level fashion  and the relative ease of conducting inference  since nonparametrics can deal gracefully with the problem of model selection  One can view a model such as the automatic citation matcher from three perspectives  it is a set of statements in the language that comprise a program  from a statisticians point of view  the model is a process that samples the defined random variables  and from the perspective of machine learning  it is a graphical model  Fig    interprets lines     of Fig    in three different ways  The semantics  as we will see  formally unify all three perspectives  Both BLOG and NP BLOG can answer the following queries  Is the referring publication of citation c the same as the referring publication of citation d  How many authors are there in the given citation database  What are the names of the authors of the publication referenced by citation c  How many publications contain the author a  where a is one of the authors in the publication referenced by citation c  And what are the titles of those publications  However  only NP BLOG can easily answer the following query  what group of researchers do we expect to be authors in a future  unseen publication      Objects and function symbols This section is largely devoted to defining notation so that we can properly elaborate on NP BLOG semantics in Sections     to      The notation as it appears in these sections makes the connection with both first order logic and the Dirichlet process mixture presented in Sec       The set of objects of a type  is called the extension of    and is denoted by      In BLOG  extensions associated with unknown  non guaranteed  types can vary over possible worlds   so we sometimes write       The size of     is denoted by n       Note that objects may be unknown even if there is a fixed number of them  Guaranteed objects are present in all possible worlds  We denote  to be the set of possible worlds for model   A model introduces a set of function symbols indexed by the objects  For conciseness  we treat predicates as Boolean functions and constants as zero ary functions  For example  the citation matching model  Fig     has the function symbols Name and CitedTitle  among others  so there is a Name a  for every author a and CitedTitle c  for every citation c  By assigning numbers to objects as they are generated  we can consider logical variables a and c to be indices on the set of natural numbers  Since BLOG is a typed language  the range of interpretations of a function symbol f is specified by its type signature  For example  the interpretation of RefAuthor u   for each u   AuthorMention                   n AuthorMention    takes a value on the range  Author   Likewise  Title p  ranges over the set of strings  String   Figures   and   omit function declaration statements  which specify type signatures  Nonetheless  this should not prevent the reader from deducing the type signatures of the functions via the statements that generate them  Nonparametric priors define distributions over probability measures  so we need function symbols that uniformly refer to them  Letting X and Y be object domains  e g  X    Author    we define MD  X   Y  to be the set of conditional probability densities p x  X   y  Y  following the class of parameterizations D  We can extend this logic  denoting MD  MD  X   Y    Z  to be the set of probability measures p d  D   z  Z  over the choice of parameterizations d  D  conditioned on Z  And so on  For peace of mind  we assume each class of distributions D is defined on a measurable  field and the densities are integrable over the range of the sample space  Note that Y or Z  but not X   may be a Cartesian product over sets of objects  BLOG does not allow return types that are tuples of objects  so we restrict distributions of objects accordingly  One can extend the above reasoning to accommodate distributions over multiple unknown objects by adopting slightly more general notation involving products of sets of objects  We assign symbols to all the functions defined in the language L   For instance  the range of NameDist in Fig    is M  String   for some specified parameterization class  Since NameDist is not generated in another line  it must be fixed over all possible worlds  For each publication p  the interpretation of symbol PubAuthorsDist p  is assigned a value on the space MMultinomial   Author    That is  the   Even though the DP imposes a distribution over an infinite set of unknown objects  n     is still finite since it refers to the estimated number of objects in   n    corresponds to the random variables of the DP mixture  as explained in Sec        function symbol refers to a distribution over author objects  How the model chooses the success rate parameters for this multinomial distribution  given that it is not on the left side of any generating statement  is the subject of Sec       NP BLOG integrates first order logic with Bayesian nonparametric methods  but we have left out one piece of the puzzle  how to specify distributions such as NameDist  or classes of distributions  This is an important design decision  but an implementation level detail  so we postpone it to future work  For the time being  one can think of parameterizations as object classes in a programming language such as Java that generate samples of the appropriate type  We point out that there is already an established language for constructing hierarchical Bayesian models  BUGS       The truth of any first order sentence is determined by a possible world in the corresponding language  A possible world    consists of an extension     for each type  and an interpretation for each function symbol f   Sec      details how NP BLOG specifies a distribution over        Dependency statements for known objects The dependency statement is the key ingredient in the specification of a generative process  We have already seen several examples of dependency statements  and we formalize them here  It is well explained in       but we need to extend the definition in the context of nonparametrics  In BLOG  a dependency statement looks like f  x            xL    g t            tN          where f is a function symbol  x            xL is a tuple of logical variables representing arguments to the function  g is a probability density conditioned on the arguments t            tN   which are terms or formulae in the language L in which the logical variables x            xL may appear  The dependency statement carries out a generative process  For an example  lets look at the dependency statement on line    of Fig     Following the rules of semantics       line    generates assignments for random variables CitedTitle  c   for c              n Citation   from probability density g conditioned on values for zRefPub  c  and Title  p   for all p              n Pub   As in       we use square brackets to index random variables  instead of the statistics convention of using subscripts  In NP BLOG  the probability density g is itself a function symbol  and the dependency statement is given by f  x            xL    g t            tM   tM              tM  N        where f and g are function symbols  and t            tM  N are formulae of the language as in      For this to be a valid statement  g t            tM   must be defined on the range M X   Y   where X is the range of f  x            xL   and Y is the domain of the input arguments within the curly braces  The first M terms inside the parentheses are evaluated in possible world   and their resulting values determine the   choice of measure g  The terms inside the curly braces are evaluated in  and the resulting values are passed to distribution g t            tM    When all the logical variables x            xL refer to guaranteed objects  the semantics of the dependency statement are given by       The curly brace notation is used to disambiguate the two roles of input argument variables  The arguments inside parentheses are indices to function symbols  e g  the c in RefPub c  in Fig      whereas the arguments inside curly braces serve as input to probability densities  e g  the term inside the curly braces in TitleStrDist Title RefPub c      This new notation is necessary when a distribution takes both types of arguments  We dont have such an example in citation matching  so we borrow one from an NP BLOG model in the aircraft tracking domain   State a  t  if t     then  InitState   else  StateTransDist a  State a  t        The state of the aircraft a at time t is an R Vector object which stores the aircrafts position and velocity in space  When t      the state is generated from the transition distribution of aircraft a given the state at the previous time step  StateTransDist a  corresponds to a measure on the space M  R Vector     R Vector    For example  in line   of Fig     the citation objects are guaranteed  Following the rules of semantics  line   defines a random variable CitedTitle  c  corresponding to the interpretation of function symbol CitedTitle c  for every citation c  Given assignments to TitleStrDist   zRefPub  c   we use z to be consistent with the notation of the semantics used in this paper  although it makes no difference in BLOG  and Title  p  for all p   Pub   assignments that are either observed or generated from other statements  the dependency statement defines the generative process CitedTitle  c   TitleStrDist  Title  p   s t  p   zRefPub  c   BLOG allows for contingencies in dependency statements  These can be subsumed within our formal framework by defining a new measure   c  t    P i  ci  i  ti     ti             where    is the indicator function  ci is the condition i which must be satisfied in order to sample from the density i   c and t are the complete sets of terms and conditions  and the summation is over the number of clauses  Infinite contingencies and their connection to graphical models are discussed in           Exchangeability and unknown objects Unknown objects are precisely those which are not guaranteed  In this section  we formalize some important properties of generated objects in BLOG  We adopt the notion of exchangeability     to objects in probabilistic first order logic  We start with some standard definitions     In which aircraft in flight appear as blips on a radar screen  and the objectives are to infer the number of aircraft and their flight paths and to resolve identity uncertainty  arising because a blip might not represent any aircraft or  conversely  an aircraft might produce multiple detections        Definition    The random variables x            xN are  finitely  exchangeable under probability density function p if p satisfies p x            xN     p x              x N     for all permutations  on             N        When n is finite  the concept of exchangeability is intuitive  the ordering is irrelevant since possible worlds are equally likely  The next definition extends exchangeability to unbounded sequences of random variables  Definition    The random variables x    x          are infinitely exchangeable if every finite subset is finitely exchangeable      Exchangeability is useful for reasoning about distributions over properties on sets of objects in BLOG  From Definitions   and    we have the following result  Proposition    It is possible to define g in the dependency statements     and     such that the sequence of objects x            xL is finitely exchangeable if and only if the terms t            tM  N do not contain any statements referring to a particular xl   For example  the distribution of hair colours of two people  Eric and Mike  is not exchangeable given evidence that Eric is the father of Mike  What about sequences of objects such as time  As long as we do not set the predecessor function beforehand  any sequence is legally exchangeable  In this paper  models are restricted to infinitely exchangeable unknown objects  We can interpret this presupposition this way  if we reorder a sequence of objects  then their probability remains the same  If we add another object to the sequence at some arbitrary position  both the original and new sequence with one more object are exchangeable  We can then appeal to de Finettis theorem      and hence the Dirichlet process  Therefore  the order of unknown objects is not important  and we can reason about set of objects rather than sequences  While there are many domains in which one would like to infer the presence of objects that are not infinitely exchangeable  this constraint leaves us open to modeling a wide range of interesting domains  Unknown or non guaranteed objects are assigned non rigid designators  a symbol in different possible worlds does not necessarily refer to the same object  and so it does not make sense to assign it a rigid label  This consideration imposes a constraint  we can only refer to a publication p via a guaranteed object  such as a citation c that refers to it  While we cannot form a query that addresses a specific unknown object  or a subset of unknown objects  we can pose questions about publications using existential and universal quantifiers  resolved using Skolemization  for instance   We could ask  for instance  how many publications have three or more authors      Dependency statements for unknown objects Sec      formalized the notion of type extensions and function symbols in NP BLOG programs  Sec      served up the preliminaries of syntax and semantics in dependency   statements  The remaining step to complete the full prescription of the semantics as a mapping from the language L to a distribution over possible worlds  This is accomplished by constructing a Bayesian hierarchical model over random variables    n     such that the set of random variables  is in one to one correspondence with the set of function interpretations  n refers to the sizes of the type extensions  and  is a set of auxiliary random variables such R that p   n   d   p   n   One might wonder why we dont dispense of function symbols entirely and instead describe everything using random variables  as in       The principal reason is to establish the connection with firstorder logic  Also  we want to make it clear that some random variables do not map to any individual in the domain  What follows is a procedural definition of the semantics  We now define distributions over the random variables  and their mapping to the symbols of the first order logic  In order to define the rules of semantics  we collect dependency and number statements according to their input argument types  If the collection of statements includes a number statement  then the rules of semantics are given in       Otherwise  we describe how the objects and their properties are implicitly drawn from a DP  Consider a set of K dependency statements such that the generated functions f            fK all require a single input of type   and    can vary over possible worlds   We denote x to be the logical variable that ranges over      The output types of the fk s are not important   The K dependency statements look like f   x   g   t              t  M    t  M               t  M   N                   fK  x   gK  tK          tK MK  tK MK          tK MK NK   where Mk and Nk are the number of input arguments to gk    and gk     respectively  and tk i is a formula in the language in which x may appear  As in BLOG  each fk  x  is associated with a random variable fk  x   The random variables g            gK   including all those implicated in the terms  must have been generated by other lines or are observed  Overloading the notation  we define the terms tk i to be random variables that depend deterministically on other generated or observed random variables  The set of statements     defines the generative process   Stick        fk  x   gk  tk          tk Mk   tk Mk          tk Mk Nk        for k              K  x                where  is the userdefined DP concentration parameter and  is a multinomial distribution such that each success rate parameter  x determines the probability of choosing a particular object x  NP BLOG infers a distribution  over objects of type  following the condition of infinite exchangeability  For example  applying rules       to line   of Fig     we get Author  Stick Author   Name  a   NameDist   for a                If an object type does not have any dependency or number statements  then no distribution over its extension is introduced  e g  strings in the citation matching model   The implementation of the DP brings about an important subtlety  if x takes on a possibly infinite different set of values  how do we recover the true number of objects n     The idea is to introduce a bijection from the subset of positive natural numbers that consists only of active objects to the set             n      An object is active in possible world  if and only if at least one random variable is assigned to that object in   In the above example  n Author  is the number of author objects that are mentioned in the citations  Of course  in practice we do not sample an infinite series of random variables Name  a   If we declare a function symbol f with a return type  ranging over a set of unknown objects  then there exists the default generating process zf  x             We use zf  x  instead of f  x  to show that the random variables are the indicators of the DP mixture      For example  each zRefPub  c  in line   in Fig    is independently drawn from the distribution of publications Pub   We can view line   as constructing a portion of the hierarchical model  as shown in Fig     The number of publications n Pub  is set to the number of different values assigned to zRefPub  c   NP BLOG allows for the definition of a symbol f that corresponds to a multinomial distribution over      so its range is MMultinomial        It exhibits the default prior f  x   Dirichlet f            analogous to       f is a user defined scalar  We define nf  x  to be the true number of objects associated with collection f  x   This is useful for modeling collections of objects such as the authors of a publication  Applying rules          to the statements in Fig    involving publication objects  we arrive at the generative process Pub  Stick Pub   Title  p   TitleDist   for p              n Pub  PubAuthorsDist  p   Dirichlet PubAuthorsDist Author     Most of the corresponding graphical model is shown in Fig     Only the PubAuthorsDist  p s are missing  and they are shown in Fig     The true number of authors nPubAuthorsDist  p  in publication p comes from the support of all random variables that refer to it  and n Pub  is determined by nPubAuthorsDist   While this paper focuses on the Dirichlet process  our framework allows for other classes of nonparametric distributions  One example can be found in the aircraft tracking domain from Sec       in which the generation of aircraft transition tables might be specified with the statement StateTransDist a   StateTransPrior    In both cases      and       one can override the defaults by including appropriate dependency statements for f   in   Num  citations Num  papers Phrase matching RPM MCMC CRF Seg  N      NP BLOG  Figure    The white nodes are the portion of the graphical model generated in lines   and   of Fig     See Fig    for an explanation of the darkened nodes  which case we get f  x   g   following rule      For example  lines   and   in Fig    specify the generative process for the author mention objects  zRefAuthor  u   PubAuthorsDist  p  CitedName  u   NameStrDist  Name  a     s t  p   zRefPub  c   c   CitedIn  u   a   zRefAuthor  u   Fig    shows the equivalent graphical model  The generative process       is a stick breaking construction over the unknown objects and their attributes  When the objects x range over the set of natural numbers        is equivalent to the Dirichlet process G  DP     H        H K          P where G   x    x  f   x         fK  x    and H k is the base measure over the assignments to fk   defined by gk conditioned on the terms tk             tk Mk  Nk   Since BLOG is a typed  free language  we need to allow for the null assignment to f  x  when it is implicitly drawn from  in       We permit the clause f  x   if cond then null         which defines f  x    null  cond          cond    This statement is necessary to take care of the situation when an objects source can be of different types  as in the aircraft tracking domain with false alarms       Next  we briefly describe how to extend the rules of semantics to functions with multiple input arguments  Lets consider the case of two inputs with an additional logical variable y      Handling an additional input argument associated with known  guaranteed  objects is easy  We just duplicate       for every instance of y in the guaranteed type extension  This is equivalent to adding a finite series of plates in the graphical model  Otherwise  we assume the unknown objects are drawn independently  That is            Multiple unknown objects as input does cause some superficial complications with the interpretation of       as a DP  principally because we need to define new notation for products of measures over different types   Face Reinforce  Reason  Constraint                                                                                                                  Table    Citation matching results for the Phrase Matching      RPM       CRF Seg      and NP BLOG models  Performance is measured by counting the number of publication clusters that are recovered perfectly  The NP BLOG column reports an average over      samples  The DP determines an implicit distribution of unknown  infinitely exchangeable objects according to their properties  That is  the DP distinguishes unknown objects solely by their attributes  However  this is not always desirable  for instance  despite being unable to differentiate the individual pieces  we know a chess board always has eight black pawns  This is precisely why we retain the original number statement syntax of BLOG which allows the user to specify a prior over the number of unknown objects  independent of their properties  In the future  we would like to experiment with priors that straddle these two extremes  This could possibly be accomplished by setting a prior on the Dirichlet concentration parameter    By tracing the rules of semantics  one should see that only thing the citation matching model does not generate is values for CitedIn u   Therefore  they must be observed  We can also provide observations from any number of object attributes  such as CitedTitle c  and CitedName u   which would result in unsupervised learning  By modifying the set of evidence  one can also achieve supervised or semisupervised learning  Moreover  the language can capture both generative and discriminative models  depending whether or not the observations are generated  To summarize  the rules given by            combined with the number statement       construct a distribution p   z  n    such that the set of auxiliary variables is             z  is in one to one correspondence with the interpretations of the function symbols  the n are the sizes of the      and an assignment to    z  n  completely determines the possible world     The rules of semantics assemble models that are arbitrary hierarchies of DPs      Experiment  The purpose of this experiment is to show that the NPBLOG language we have described realizes probabilistic inference on a real world problem  We simulate the citation matching model in Fig    on the CiteSeer data set      which consists of manually segmented citations from four research areas in AI  We use Markov Chain Monte Carlo  MCMC  to simulate possible worlds from the model posterior given evidence in the form of cited authors and titles  Sec    briefly describes   There is much future work on this topic  An important direction is the development of efficient  flexible and on line inference methods for hierarchies of Dirichlet processes   Figure    Estimated  solid blue  and true  dashed red line  number of publications for the Face and Reasoning data   Acknowledgements This paper wouldnt have happened without the help of Brian Milch  Special thanks to Gareth Peters and Mike Klaas for their assistance  and to the reviewers for their time and effort in providing us with constructive comments   
