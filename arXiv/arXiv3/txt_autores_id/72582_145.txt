  incomplete on each step  still efficiently computes optimal actions in a timely manner   We consider the problem of efficiently learning optimal control policies and value functions over large state spaces in an online setting in which estimates must be available after each interaction with the world  This paper develops an explicitly model based approach extending the Dyna architecture to linear function approximation  Dynastyle planning proceeds by generating imaginary experience from the world model and then applying model free reinforcement learning algorithms to the imagined state transitions  Our main results are to prove that linear Dyna style planning converges to a unique solution independent of the generating distribution  under natural conditions  In the policy evaluation setting  we prove that the limit point is the least squares  LSTD  solution  An implication of our results is that prioritized sweeping can be soundly extended to the linear approximation case  backing up to preceding features rather than to preceding states  We introduce two versions of prioritized sweeping with linear Dyna and briefly illustrate their performance empirically on the Mountain Car and Boyan Chain problems   The Dyna architecture  Sutton       provides an effective and flexible approach to incremental planning while maintaining responsiveness  There are two ideas underlying the Dyna architecture  One is that planning  acting  and learning are all continual  operating as fast as they can without waiting for each other  In practice  on conventional computers  each time step is shared between planning  acting  and learning  with proportions that can be set arbitrarily according to available resources and required response times   Online learning and planning  Efficient decision making when interacting with an incompletely known world can be thought of as an online learning and planning problem  Each interaction provides additional information that can be used to learn a better model of the worlds dynamics  and because this change could result in a different action being best  given the model   the planning process should be repeated to take this into account  However  planning is inherently a complex process  on large problems it not possible to repeat it on every time step without greatly slowing down the response time of the system  Some form of incremental planning is required that  though  The second idea underlying the Dyna architecture is that learning and planning are similar in a radical sense  Planning in the Dyna architecture consists of using the model to generate imaginary experience and then processing the transitions of the imaginary experience by model free reinforcement learning algorithms as if they had actually occurred  This can be shown  under various conditions  to produce exactly the same results as dynamic programming methods in the limit of infinite imaginary experience  The original papers on the Dyna architecture and most subsequent extensions  e g   Singh       Peng   Williams       Moore   Atkeson       Kuvayev   Sutton       assumed a Markov environment with a tabular representation of states  This table lookup representation limits the applicability of the methods to relatively small problems  Reinforcement learning has been combined with function approximation to make it applicable to vastly larger problems than could be addressed with a tabular approach  The most popular form of function approximation is linear function approximation  in which states or state action pairs are first mapped to feature vectors  which are then mapped in a linear way  with learned parameters  to value or next state estimates  Linear methods have been used in many of the successful large scale applications of reinforcement learning  e g   Silver  Sutton   Muller       Schaeffer  Hlynka   Jussila        Linear function approximation is also simple  easy to understand  and possesses some of the strongest convergence and performance guarantees among function approximation methods  It is   natural then to consider extending Dyna for use with linear function approximation  as we do in this paper  There has been little previous work addressing planning with linear function approximation in an online setting  Paduraru        treated this case  focusing mainly on sampling stochastic models of a cascading linear form  but also briefly discussing deterministic linear models  Degris  Sigaud and Wuillemin        developed a version of Dyna based on approximations in the form of dynamic Bayes networks and decision trees  Their system  SPITI  included online learning and planning based on an incremental version of structured value iteration  Boutilier  Dearden   Goldszmidt        Singh        developed a version of Dyna for variable resolution but still tabular models  Others have proposed linear least squares methods for policy evaluation that are efficient in the amount of data used  Bradtke   Barto       Boyan             Geramifard  Bowling   Sutton        These methods can be interpreted as forming and then planning with a linear model of the worlds dynamics  but so far their extensions to the control case have not been well suited to online use  Lagoudakis   Parr       Peters  Vijayakumar   Schaal       Bowling  Geramifard    Wingate        whereas our linear Dyna methods are naturally adapted to this case  We discuss more specifically the relationship of our work to LSTD methods in a later section  Finally  Atkeson        and others have explored linear  learned models with off line planning methods suited to low dimensional continuous systems      Notation  We use the standard framework for reinforcement learning with linear function approximation  Sutton   Barto        in which experience consists of the time indexed stream s    a    r    s    a    r    s           where st  S is a state  at  A is an action  and rt  R is a reward  The actions are selected by a learning agent  and the states and rewards are selected by a stationary environment  The agent does not have access to the states directly but only through a corresponding feature vector t  Rn    st    The n agent selects actions P according to a policy     R  A         such that aA    a         An important step towards finding a good policy is to estimate the value function for a given policy  policy evaluation   The value function is approximated as a linear function with parameter vector   Rn       X    s   V   s    E  t  rt   s    s   t    where           In this paper we consider policies that are greedy or   greedy with respect to the approximate statevalue function   Algorithm     Linear Dyna for policy evaluation  with random sampling and gradient descent model learning Obtain initial     F  b For each time step  Take action a according to the policy  Receive r          r            F  F       F    b  b    r  b    temp    Repeat p times  planning   Generate a sample  from some distribution     F  r  b         r              temp     Theory for policy evaluation  The natural place to begin a study of Dyna style planning is with the policy evaluation problem of estimating a statevalue function from a linear model of the world  The model consists of a forward transition matrix F  Rn  Rn  incorporating both environment and policy  and an expected reward vector b  Rn   constructed such that F  and b   can be used as estimates of the feature vector and reward that follow   A Dyna algorithm for policy evaluation goes through a sequence of planning steps  on each of which a starting feature vector  is generated according to a probability distribution   and then a next feature vector     F  and next reward r   b   are generated from the model  Given this imaginary experience  a conventional modelfree update is performed  for example  according to the linear TD    algorithm  Sutton              r                   or according to the residual gradient algorithm  Baird              r                         where      is a step size parameter  A complete algorithm using TD     including learning of the model  is given in Algorithm         Convergence and fixed point  There are two salient theoretical questions about the Dyna planning iterations     and      Under what conditions on  and F do they converge  and What do they converge to  Both of these questions turn out to have interesting answers  First  note that the convergence of     is in question in part because it is known that linear TD    may diverge if the distribution of starting states during training does not match the distribution created by the normal dynamics of   the system  that is  if TD    is used off policy  This suggests that the sampling distribution used here    might have to be strongly constrained in order for the iteration to be stable  On the other hand  the data here is from the model  and the model is not a general system  it is deterministic  and linear  This special case could be much better behaved  In fact  convergence of linear Dyna style policy evaluation  with either the TD    or residual gradient iterations  is not affected by   but only by F   as long as  exercises all directions in the full n dimensional vector space  Moreover  not only is the fact of convergence unaffected by   but so is the value converged to  In fact  we show below that convergence is to a deterministic fixed point  a value of  such that the iterations     and     leave it unchanged not just in expected value  but for every individual  that could be generated by   The only way this could be true is if the TD error  the first expression in parentheses in each iteration  were exactly zero  that is  if       r                 b     F          b   F           And the only way that this can be true for all  is for the expression in parenthesis above to be zero     Before verifying the conditions of this result  let us rewrite     in terms of the matrix G   I  F   k      k   k  b  k   k   F  I k  k    b    F    I      k   k sk        I  F      b        where    Rn is P arbitrary  AssumeP that  i  the step size   sequence satisfies k   k     k   k       ii  r F        iii   k   are uniformly   bounded   i i d  random variables  and that  iv  C   E k   is non singular  k Then the parameter vector k converges with probability one to  I  F      b     k   k  b  k  k  Gk  k  Here sk is defined by the last equation       assuming that the inverse exists  Note that this expression for the fixed point does not depend on   as promised  If I  F   is nonsingular  then there might be no fixed point  This could happen for example if F were an expansion  or more generally if the limit  F   were not zero  These cases correspond to world models that say the feature vectors diverge to infinity over time  Failure to converge in these cases should not be considered a problem for the Dyna iterations as planning algorithms  these are cases in which the planning problem is ill posed  If the feature vectors diverge  then so too may the rewards  in which case the true values given the model are infinite  No real finite Markov decision process could behave in this way  It remains to show the conditions on F under which the iterations converge to the fixed point if one exists  We prove next that under the TD    iteration      convergence is guaranteed if the numerical radius of F is less than one   and    k     k   k  b  k   k  F k  k  k  k      b   F       which immediately implies that   Theorem      Convergence of linear TD    Dyna for policy evaluation   Consider the TD    iteration with a nonnegative step size sequence  k     Proof  The idea of the proof is to view the algorithm as a stochastic gradient descent method  In particular  we apply Proposition     of  Bertsekas   Tsitsiklis               then that under the residual gradient iteration      convergence is guaranteed for any F as long as the fixed point exists  That F s numerical radius be less than   is a stronger condition than nonsingularity of I  F     but it is similar in that both conditions pertain to the matrix trending toward expansion when multiplied by itself   The model is deterministic because it generates the expectation of the next feature vector  the system itself may be stochastic    The numerical radius of a real valued square matrix A is defined by r A    maxkxk     xT Ax   The cited proposition requires the definition of a potential function J   and will allow us to conclude that limk J k       with probability one  Let   us choose J         E  b  k     F k    k      Note that by our i i d  assumptions on the features  J   is welldefined  We need to check four conditions  because the step size conditions are automatically satisfied    i  The nonnegativity of the potential function   ii  The Lipschitz continuity of J     iii  The pseudo gradient property of the expected update direction  and  iv  The boundedness of the  expected  magnitude of the update  more precisely that E ksk k    k  O kJ k  k      Nonnegativity is satisfied by definition and the boundedness condition  iv  is satisfied thanks to the boundedness of the features  Let us show now that the pseudo gradient property  iii  is satisfied  This condition requires the demonstration of a positive constant c such that ckJ k  k    J k    E  sk  k           Define sk   E  sk  k     Cb  CG  k   A simple calculation gives J k     Gsk   Hence kJ k  k           s  k G Gsk and  J k    sk   sk Gsk   Therefore           is equivalent to c sk G Gsk  sk Gsk   In order to make this true with a sufficiently small c  it suffices to show that   s  Gs     holds for any non zero vector s  An elementary reasoning shows that this is equivalent to     G   G    being positive definite  which in turn is equivalent to r F       showing that  iii  is satisfied  Hence  we have verified all the assumptions of the cited proposition and can therefore we conclude that limk J k       with probability one  Plugging in the expression of J k    we get limt  CbCG  k        Because C and G are invertible  this latter follows from r F        it follows that the limit of k exists and limk k    G     b    I  F      b   verges with probability one to  I  F      b  assuming that  I  F     is non singular  Proof  As all the conditions of Proposition     of  Bertsekas   Tsitsiklis       are trivially satisfied with the choice J     E  J   k     we can conclude that k converges w p   to the minimizer of J    In the previous theorem we have seen that the minimizer of J   is indeed     I  F      b  finishing the proof       Convergence to the LSTD solution  Several extensions of this result are possible  First  the requirement of i i d  sampling can be considerably relaxed  With an essentially unchanged proof  it is possible to show that the theorem remains true if the feature vectors are generated by a Markov process given that they satisfy appropriate ergodicity conditions  Moreover  building on a result by Delyon         one can show that the result continues to hold even if the sequence of features is generated in an algorithmic manner  again provided that some ergodicity conditions are met  PKThe major assumption then is that C   limK   K k   k   k exists and is nonsingular  Further  because there is no noise to reject  there is noP need to decay the step sizes towards zero  the condi tion k   k      in the proofs is used to filter out noise   In particular  we conjecture that sufficiently small constant step sizes would work as well  for a result of this type see Proposition     by Bertsekas   Tsitsiklis         So far we have discussed the convergence of planning given a model  but we have said nothing about the relationship of the model to data  or about the quality of the resultant solution  Suppose the model were the best linear fit to a finite dataset of observed feature vector to feature vector transitions with accompanying rewards  In this case we can show that the fixed point of the Dyna updates is the least squares temporal difference solution  This is the solution for which the mean TD    update is zero and is also the solution found by the LSTD    algorithm  Barto   Bradtke         On the other hand the requirement on the numerical radius of F seems to be necessary for the convergence of the TD    iteration  By studying the ODE associated with      we see that it is stable if and only if CG is a positive stable matrix  i e   iff all its eigenvalues have positive real part   From this it seems necessary to require that G is positive stable  However  to ensure that CG is positive stable the strictly stronger condition that G   G  is positive definite must be satisfied  This latter condition is equivalent to r F        Proof  It suffices to show that the respective solution sets of the equations  We turn now to consider the convergence of Dyna planning using the residual gradient Dyna iteration      This update rule can be derived by taking the gradient of J   k      b  k     k    k    w r t    Thus  as an immediate consequence of Proposition     of  Bertsekas   Tsitsiklis       we get the following result  Theorem      Convergence of residual gradient Dyna for policy evaluation   Assume that k is updated according to k     k   k  b  k   k  F k  k  k   k  F k    where    Rn is arbitrary  Assume that the non negative step size sequence  k   satisfies the summability condition  i  of Theorem     and that  k   are uniformly bounded i i d  random variables  Then the parameter vector k con   Theorem      Given a training dataset of feature  reward  next state feature triples D        r                 n   rn    n    let F  bPbe the least squares model built on D  Assume that n C   k   k   k has full rank  Then the solution     is the same as the LSTD solution on this training set         n X  k  rk     k        k          k          b    F    I        are the same  This is because the LSTD parameter vectors are obtained by solving the first equation and the TD    Dyna solutions are derived from the second equation  Pn Pn Let D   k   k   k      and r   k   k rk   A standard calculation shows that F      C   D  and b   C   r   Plugging in C  D into     and factoring out  shows that any solution of     also satisfies       r    D  C          If we multiply both sides of     by C   from the left we get      Hence any solution of     is also a solution of      Because all the steps of the above derivation are reversible  we get that the reverse statement holds as well    Algorithm     Linear Dyna with PWMA prioritized sweeping  policy evaluation  Obtain initial     F  b For each time step  Take action a according to the policy  Receive r      r                 F  F       F    b  b    r  b    For all i such that  i        For all j such that F ij       Put j on the PQueue with priority  F ij  i   Repeat p times while PQueue is not empty  i  pop the PQueue   b i      F ei   i   i    i     For all j such that F ij       Put j on the queue with priority  F ij           Algorithm     Linear Dyna with MG prioritized sweeping  policy evaluation  Obtain initial     F  b For each time step  Take action a according to the policy  Receive r      r                 F  F       F    b  b    r  b    For all i such that  i        Put i on the PQueue with priority   i   Repeat p times while PQueue is not empty  i  pop the PQueue For all j such that F ij         b j      F ej   j   j    j     Put j on the PQueue with priority         Linear prioritized sweeping  We have shown that the convergence and fixed point of policy evaluation by linear Dyna are not affected by the way the starting feature vectors are chosen  This opens the possibility of selecting them cleverly so as to speed the convergence of the planning process  One natural ideathe idea behind prioritized sweepingis to work backwards from states that have changed in value to the states that lead into them  The lead in states are given priority for being updated because an update there is likely to change the states value  because they lead to a state that has changed in value   If a lead in state is updated and its value is changed  then its lead in states are in turn given priority for updating  and so on  In the table lookup context in which this idea was developed  Moore   Atkeson       Peng       see also Wingate   Seppi        there could be many states preceding each changed state  but only one could be updated at a time  The states waiting to be updated were kept in a queue  prioritized by the size of their likely effect on the value function  As high priority states were popped off the queue and updated  it would sometimes give rise to highly efficient sweeps of updates across the state space  this is what gave rise to the name prioritized sweeping  With function approximation it is not possible to identify and work backwards from individual states  but alternatively one could work backwards feature by feature  If there has just been a large change in  i   the component of the parameter vector corresponding to the ith feature  then one can look backwards through the model to find the features j whose components  j  are likely to have changed as a result  These are the features j for which the elements F ij of F are large  One can then preferentially construct  starting feature vectors  that have non zero entries at these j components  In our algorithms we choose the starting vectors to be the unit basis vectors ej   all of whose components are zero except the jth  which is     Our theoretical results assure us that this cannot affect the result of convergence   Using unit basis vectors is very efficient computationally  as the vector matrix multiplication F  is reduced to pulling out a single column of F   There are two tabular prioritized sweeping algorithms in the literature  The first  due simultaneously to Peng and Williams        and to Moore and Atkeson         which we call PWMA prioritized sweeping  adds the predecessors of every state encountered in real experience to the priority queue whether or not the value of the encountered state was significantly changed  The second form of prioritized sweeping  due to McMahan and Gordon         and which we call MG prioritized sweeping  puts each encountered state on the queue  but not its predecessors  For McMahan and Gordon this resulted in a more efficient planner  A complete specification of our feature by feature versions of these two forms of prioritized sweeping are given above  with TD    updates and gradient descent model learning  as Algorithms   and    These algorithms differ slightly from previous prioritized sweeping algorithms in that they update the value function from the real experiences and not just from model generated experience  With function approximation  real experience is always more informative than model generated experience  which will be distorted by the function approximator  We found this to be a significant effect in our empirical experiments  Section       Algorithm    Linear Dyna with MG prioritized sweeping and TD    updates  control  Obtain initial     F  b For each time step       a  arg maxa b   or   greedy  a     Fa  Take action a  receive r      r                 Fa  Fa       Fa    ba  ba    r  b  a   For all i such that  i        Put i on the PQueue with priority   i   Repeat p times while PQueue is not empty  i  pop the PQueue ij For all j s t  there   exists an a s t  F   a         maxa ba  j     Fa ej   j   j    j     Put j on the PQueue with priority            Theory for Control  We now turn to the full case of control  in which separate models Fa   ba are learned and are then available for each action a  These are constructed such that Fa  and b  a  can be used as estimates of the feature vector and reward that follow  if action a is taken  A linear Dyna algorithm for the control case goes through a sequence of planning steps on each of which a starting feature vector  and an action a are chosen  and then a next feature vector     Fa  and next reward r   ba  are generated from the model  Given this imaginary experience  a conventional model free update is performed  The simplest case is to again apply      A complete algorithm including prioritized sweeping is given in Algorithm    The theory for the control case is less clear than for policy evaluation  The main issue is the stability of the mixture of the forward model matrices  The corollary below is stated for an i i d  sequence of features  but by the remark after Theorem     it can be readily extended to the case where the policy to be evaluated is used to generate the trajectories  Corollary      Convergence of linear TD    Dyna with action models   Consider the Dyna recursion     with the modification that in each step  instead of F k   we use F k   k   where  is a policy mapping feature vectors to actions and  Fa   is a collection of forward model matrices  Similarly  b  k is replaced by b   k   k   As before  assume that k is an unspecified i i d  process  Let  F  b    be the least squares   model of   F   harg minG E kGk  iF k   k k   and b     arg minu E  u  k  b  If the numerical radius  k   k   of F is bounded by one  then the conclusions of Theo       N                   N                                                                                                                      Figure    The general Boyan Chain problem  rem     hold  the parameter vector k converges with probability one to  I  F      b  Proof  The proof is immediate from equation     the normal     for F   which states that E F k     E F k   k   k k   and once we observe that  in the proof of  Theorem       F appears only in expressions of the form E F k   k   As in the case of policy evaluation  there is a corresponding corollary for the residual gradient iteration  with an immediate proof  These corollaries say that  for any policy with a corresponding model that is stable  the Dyna recursion can be used to compute its value function  Thus we can perform a form of policy iterationcontinually computing an approximation to the value function for the greedy policy      Empirical results  In this section we illustrate the empirical behavior of the four Dyna algorithms and make comparisons to model free methods using variations of two standard test problems  Boyan Chain and Mountain Car  Our Boyan Chain environment is an extension of that by Boyan              from    to    states  and from   to    features  Geramifard  Bowling   Sutton        Figure   depicts this environment in the general form  Each episode starts at state N      and terminates in state    For all states s      there is an equal probability of transitioning to states s    or s    with a reward of    From states   and    there are deterministic transitions to states   and   with respective rewards of   and    Our Mountain Car environment is exactly as described by Sutton        Sutton   Barto        re implemented in Matlab  An underpowered car must be driven to the top of a hill by rocking back and forth in a valley  The state variables are a pair  position velocity  initialized to            at the beginning of each episode  The reward is   per time step  There are three discrete actions  accelerate  reverse  and coast   We used a value function representation based on tile coding feature vectors exactly as in Suttons        experiments  with    tilings over the combined  position  velocity  pair  and with the tiles hashed down to        features  In the policy evaluation experiments with this domain  the policy was to accelerate in         Boyan chain         Mountain Car  x     Dyna Random TD        Loss  Dyna Random Dyna PWMA  Loss         TD         Dyna MG  Dyna PWMA     Dyna MG                      Episode                              Episode             Figure    Performance of policy evaluation methods on the Boyan Chain and Mountain Car environments the direction of the current velocity  and we added noise to the domain that switched the selected action to a random action with     probability  Complete code for our test problems as standard RL Glue environments is available from the RL Library hosted at the University of Alberta  In all experiments  the step size parameter  took the form      t     NN   t       in which t is the episode number and the pair  N        was selected based on empirically finding the best combination out of                 and N                     separately for each algorithm and domain  All methods observed the same trajectories in policy evaluation  All graphs are averages of    runs  error bars indicate standard errors in the means  Other parameter settings were                and       We performed policy evaluation experiments with four algorithms  Dyna Random  Dyna PWMA  Dyna MG  as in Algorithms      and model free TD     In the case of the Dyna Random algorithm  the starting feature vectors in planning were chosen to be unit basis vectors with the   in a random location  Figure   shows the policy evaluation performance of the four methods in the Boyan Chain and Mountain Car environments  For the Boyan Chain domain  the loss was the root mean squared error of the learned value function compared to the exact analytical value  averaged over all states  In the Mountain Car domain  the states are visited very non uniformly  and a more sophisticated measure is needed  Note that all of the methods drive  toward an asymptotic value in which the expected TD    update is zero  we can use the distance from this as a loss measure  Specifically  we evaluated each learned value function by freezing it and then running a fixed set of         episodes with it while running the TD    algorithm  but not allowing  to actually change   The norm of the sum of the  attempted  update vectors was then computed and used as the loss  In practice  this measure can be computed very efficiently as   A   b     in the notation of  LSTD     see Bradtke   Barto        In the Boyan Chain environment  the Dyna algorithms generally learned more rapidly than model free TD     DynaMG was initially slower than the other algorithms  then caught up and surpassed them  The relatively poor early performance of Dyna MG was actually due to its being a better planning method  After few episodes the model tends to be of very high variance  and so therefore is the best value function estimate given it  We tested this hypothesis by running the Dyna methods starting with a fixed  well learned model  in this case Dyna MG was the best of all the methods from the beginning  All of these data are for one step of planning for each real step of interaction with the world  p       In preliminary experiments with larger values of p  up to p       we found further improvements in learning rate of the Dyna algorithms over TD     and again Dyna MG was best  The results for Mountain Car are less clear  Dyna MG quickly does significantly better than TD     but the other Dyna algorithms lag initially and never surpass TD     Note that  for any value of p  Dyna MG does many more  updates than the other two Dyna algorithms  because these updates are in an inner loop  cf  Algorithms   and     Even so  because of its other efficiencies Dyna MG tended to run faster overall in our implementation  Obviously  there is a lot more interesting empirical work that could be done here  We performed one Mountain Car experiment with DynaMG as a control algorithm  Algorithm     comparing it with model free Sarsa  i e   Algorithm   with p       The results are shown in Figure    As before  Dyna MG showed a distinct advantage over the model free method in terms of learning rate  There was no clear advantage for either method in the second half of the experiment  We note that  asymptotically  model free methods are never worse than model based methods  and are often better because the model does not converge exactly to the true system because               Return       Dyna MG           Sarsa                           Episode           Figure    Control performance on Mountain Car  Conclusion  In this paper we have taken important steps toward establishing the theoretical and algorithmic foundations of Dyna style planning with linear function approximation  We have established that Dyna style planning with familiar reinforcement learning update rules converges under weak conditions corresponding roughly  in some cases  to the existence of a finite solution to the planning problem  and that convergence is to a unique least squares solution independent of the distribution used to generate hypothetical experience  These results make possible our second main contribution  the introduction of algorithms that extend prioritized sweeping to linear function approximation  with correctness guarantees  Our empirical results illustrate the use of these algorithms and their potential for accelerating reinforcement learning  Overall  our results support the conclusion that Dyna style planning may be a practical and competitive approach to achieving rapid  online control in stochastic sequential decision problems with large state spaces  Acknowledgements  of structural modeling assumptions   The case we treat herelinear models and value functions with one step TD methodsis a rare case in which asymptotic performance of model based and model free methods should be identical   The benefit of models  and of planning generally  is in rapid adaptation to new problems and situations   The authors gratefully acknowledge the substantial contributions of Cosmin Paduraru and Mark Ring to the early stages of this work  This research was supported by iCORE  NSERC and Alberta Ingenuity   These empirical results are not extensive and in some cases are preliminary  but they nevertheless illustrate some of the potential of linear Dyna methods  The results on the Boyan Chain domain show that Dyna style planning can result in a significant improvement in learning speed over modelfree methods  In addition  we can see trends that have been observed in the tabular case re occurring here with linear function approximation  In particular  prioritized sweeping can result in more efficient learning than simply updating features at random  and the MG version of prioritized sweeping seems to be better than the PWMA version   Atkeson  C          Using local trajectory optimizers to speed up global optimization in dynamic programming  Advances in Neural Information Processing Systems             Baird  L  C          Residual algorithms  Reinforcement learning with function approximation  In Proceedings of the Twelfth International Conference on Machine Learning  pp        Bertsekas  Dimitri P   Tsitsiklis  J          Neuro Dynamic Programming  Athena Scientific        Boutilier  C   Dearden  R   Goldszmidt  M          Stochastic dynamic programming with factored representations  Artificial Intelligence             Bowling  M   Geramifard  A   Wingate  D          Sigma point policy iteration  In Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems  Boyan  J  A          Least squares temporal difference learning  In Proceedings of the Sixteenth International Conference on Machine Learning        Boyan  J  A          Technical update  Least squares temporal difference learning  Machine Learning              Bradtke  S   Barto  A  G          Linear least squares al   Finally  we would like to note that we have done extensive experimental work  not reported here  attempting to adapt least squares methods such as LSTD to online control domains  in particular to the Mountain Car problem  A major difficulty with these methods is that they place equal weight on all past data whereas  in a control setting  the policy changes and older data becomes less relevant and may even be misleading  Although we have tried a variety of forgetting strategies  it is not easy to obtain online control performance with these methods that is superior to modelfree methods  One reason we consider the Dyna approach to be promising is that no special changes are required for this case  it seems to adapt much more naturally and effectively to the online control setting   
