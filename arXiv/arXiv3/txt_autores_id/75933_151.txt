 Compiling graphical models has recently been under intense investigation  especially for probabilistic modeling and processing  We present here a novel data structure for compiling weighted graphical models  in particular  probabilistic models   called AND OR Multi Valued Decision Diagram  AOMDD   This is a generalization of our previous work on constraint networks  to weighted models  The AOMDD is based on the frameworks of AND OR search spaces for graphical models  and Ordered Binary Decision Diagrams  OBDD   The AOMDD is a canonical representation of a graphical model  and its size and compilation time are bounded exponentially by the treewidth of the graph  rather than pathwidth as is known for OBDDs  We discuss a Variable Elimination schedule for compilation  and present the general APPLY algorithm that combines two weighted AOMDDs  and also present a search based method for compilation method  The preliminary experimental evaluation is quite encouraging  showing the potential of the AOMDD data structure      Introduction  We present here an extension of AND OR Multi Valued Decision Diagrams  AOMDDs       to general weighted graphical models  including Bayesian networks  influence diagrams and Markov random fields  The work on AOMDDs is based on two existing frameworks      AND OR search spaces for graphical models and     decision diagrams  DD   AND OR search spaces     have proven to be a unifying framework for various classes of search algorithms for graphical models  The main characteristic is the exploitation of independencies between variables during search  which can provide exponential speedups over traditional search methods that can  be viewed as traversing an OR structure  The AND nodes capture problem decomposition into independent subproblems  and the OR nodes represent branching according to variable values  Decision diagrams are widely used in many areas of research  especially in software and hardware verification      A BDD represents a Boolean function by a directed acyclic graph with two sink nodes  labeled   and     and every internal node is labeled with a variable and has exactly two children  low for   and high for    A BDD is ordered if variables are encountered in the same order along every path  A BDD is reduced if all isomorphic nodes  i e   with the same label and identical children  are merged  and all redundant nodes  i e   whose low and high children are identical  are eliminated  The result is the celebrated reduced ordered binary decision diagram  or OBDD      AOMDDs combine the two ideas  in order to create a decision diagram that has an AND OR structure  thus exploiting problem decomposition  As a detail  the number of values is also increased from two to any constant  but this is less significant for the algorithms  A decision diagram offers a compilation of a problem  It typically requires an extended offline effort in order to be able to support polynomial  in its size  or constant time online queries  The benefit of moving from OR structure to AND OR is in a lower complexity of the algorithms and size of the compiled structure  It typically moves from being bounded exponentially in pathwidth pw   which is characteristic to chain decompositions or linear structures  to being exponentially bounded in treewidth w   which is characteristic of tree structures  it always holds that w  pw and pw  w  log n   Our contributions in this paper are as follows      We formally describe the extension of AND OR multi valued decision diagram  AOMDD  to weighted graphical models      We describe the extension to weighted models of the APPLY operator that combines two AOMDDs by an operation  The output of APPLY is still bounded by the product of the sizes of the inputs      We present two compilation   MATEESCU   DECHTER A                  B                  f ABC                   C                  A  A  B  B  C     C      a  Table        A  B  C          C        C      b  Ordered tree  B  C  C        A  B  C   c  Isomorphic nodes  B  C  B  C         d  Redundant nodes  C         e  OBDD  Figure    Boolean function representations A f  AC  f  AB  f  ABE   B  C  A   h  A   B   C  AB   E   C  ABE   C   C  AC   D   C   BCD   A  h  AB   AB bucket B AB  f  BCD   ABE E  D   a  Model  h  BC    b  VE execution  bucket A  A  h  AB   bucket E  AB  ABC bucket C BC  BCD bucket D  algorithms for AOMDDs  One is based on the repeated application of APPLY along a Variable Elimination schedule  The other is based on search  Both schemes are exponential in the treewidth of the model      We provide encouraging preliminary experimental evaluation of the search based compilation method      We discuss how AOMDDs relate to various earlier and recent works  providing a unifying perspective for all these methods  The structure of the paper is as follows  Section   provides preliminaries  Section   gives an overview of AND OR search space  Section   describes the AOMDD for constraint networks  the Variable Elimination schedule for compilation and the APPLY operator  and a search based compilation scheme  Section   contains the main contribution  the extension of AOMDDs to weighted models  and a discussion of their canonical form and the extensions of the compilation schedule and APPLY operator  Section   provides experimental evaluation and section   concludes   Preliminaries  In this section we describe graphical models  Binary Decision Diagrams  OBDDs  and Variable Elimination  D EFINITION    graphical model  A graphical model R is a   tuple  R   hX  D  F  i  where      X    X            Xn   is a set of variables      D    D            Dn   is the set of their respective finite domains of values      F    f            fr   is a set of discrete realvalued functions  each defined over a subset of variables Si  X  called its scope  Q and P sometimes denoted by scope fi        i fi    i fi   i fi    i fi   is a combination operator    The graphical model represents the combination of all its functions  ri   fi   A reasoning task is    Examples of graphical models include Bayesian networks  constraint networks  influence diagrams  Markov networks    c  Bucket tree  Figure    Variable Elimination     based on a projection  elimination  operator    and is defined by  Z  ri   fi           Zt ri   fi   where Zi  X   The combination operator can be defined axiomatically        D EFINITION    universal equivalent graphical model  Given a graphical model R   hX  D  F    i the universal equivalent model of R is u R    hX  D  F     fi F  fi    i  Two graphical models are equivalent if they represent the same set of solutions  Namely  if they have the same universal model  D EFINITION    primal graph  The primal graph of a graphical model is an undirected graph that has variables as its vertices and an edge connects any two variables that appear in the scope of the same function  A pseudo tree resembles the tree rearrangements       D EFINITION    pseudo tree  A pseudo tree of a graph G    X  E  is a rooted tree T having the same set of nodes X  such that every arc in E is a back arc in T  i e   it connects nodes on the same path from root   D EFINITION    induced graph  induced width  treewidth  pathwidth  An ordered graph is a pair  G  d   where G is an undirected graph  and d    X         Xn   is an ordering of the nodes  The width of a node in an ordered graph is the number of neighbors that precede it in the ordering  The width of an ordering d  denoted by w d   is the maximum width over all nodes  The induced width of an ordered graph  w  d   is the width of the induced ordered graph obtained as follows  for each node  from last to first in d  its preceding neighbors are connected in a clique  The induced width of a graph  w   is the minimal induced width over all orderings  The induced width is also equal to the treewidth of a graph  The pathwidth pw of a graph is the treewidth over the restricted class of orderings that correspond to chain decompositions       Binary Decision Diagrams  Decision diagrams are widely used in many areas of research to represent decision processes  In particular  they        MATEESCU   DECHTER A  A  A  A      f  AC               f  AB   B  B  B  B  f  ABE   B  A   B  f  BCD   E   AB  E  E     C  AB   D  BC   D   a  Graphical model  C         C  E        D  D                  b  Pseudo tree    C  E        D                C  E        D  D                       D  D  D                  c  Search tree     E  C       C    E      C       E          C          E    D  D  D  D                           C        d  Context minimal graph  Figure    AND OR search space can be used to represent functions  Due to the fundamental importance of Boolean functions  a lot of effort has been dedicated to the study of Binary Decision Diagrams  BDDs   which are extensively used in software and hardware verification                 A BDD is a representation of a Boolean function  Given B           a Boolean function f   Bn  B  has n arguments  X         Xn   which are Boolean variables  and takes Boolean values  A Boolean function can be represented by a table  see Figure   a    but this is exponential in n  and so is the binary tree representation in Figure   b   The goal is to have a compact representation  that also supports efficient operations between functions  OBDDs     provide such a framework by imposing the same order to the variables along each path in the binary tree  and then applying the following two reduction rules exhaustively      isomorphism  merge nodes that have the same label and the same respective children  see Figure   c        redundancy  eliminate nodes whose low  zero  and high  one  edges point to the same node  and connect the parent of removed node directly to the child of removed node  see Figure   d    The resulting OBDD is shown in Figure   e        Variable Elimination  VE      AND OR Search Space  The AND OR search space     is a recently introduced unifying framework for advanced algorithmic schemes for graphical models  Its main virtue consists in exploiting independencies between variables during search  which can provide exponential speedups over traditional search methods oblivious to problem structure       AND OR Search Trees  Given a graphical model M   hX  D  Fi  its primal graph G and a pseudo tree T of G  the associated AND OR search tree  ST  R   has alternating levels of OR and AND nodes  The OR nodes are labeled Xi and correspond to the variables  The AND nodes are labeled hXi   xi i and correspond to the value assignments in the domains of the variables  The structure of the AND OR search tree is based on the underlying pseudo tree T   The root of the AND OR search tree is an OR node labeled with the root of T   The children of an OR node Xi are AND nodes labeled with assignments hXi   xi i  that are consistent with the assignments along the path from the root  The children of an AND node hXi   xi i are OR nodes labeled with the children of variable Xi in the pseudo tree T   The AND OR search tree can be traversed by a depth first search algorithm  thus using linear space  It was already shown               that   Variable elimination  VE         is a well known algorithm for inference in graphical models  Consider a graphical model R   hX  D  Fi and an elimination ordering d    X    X            Xn    Xn is eliminated first  X  last   Each function placed in the bucket of its latest variable in d  Buckets are processed from Xn to X  by eliminating the bucket variable  the functions residing in the bucket are combined together  and the bucket variable is projected out  and placing the resulting function  also called message  in the bucket of its latest variable in d  Figure   a  shows a graphical model and   b  the execution of VE   T HEOREM   Given a graphical model M and a pseudo tree T of depth m  the size of the AND OR search tree based on T is O n k m    where k bounds the domains of variables  A graphical model having treewidth w has a pseudo tree of depth at most w log n  therefore it has an  AND OR search tree of size O n k w log n     VE execution defines a bucket tree  by linking the bucket of each Xi to the destination bucket of its message  called the parent bucket   A node in the bucket tree has a bucket variable  a collection of functions  and a scope  the union of the scopes of its functions   If the nodes of the bucket tree are replaced by their respective bucket variables  we obtain a pseudo tree  see Figure   c  and   b     The AND OR search tree may contain nodes that root identical conditioned subproblems  These nodes are said to be unifiable  When unifiable nodes are merged  the search space becomes a graph  Its size becomes smaller at the expense of using additional memory by the search algorithm  The depth first search algorithm can therefore be modified to cache previously computed results  and retrieve       AND OR Search Graphs   MATEESCU   DECHTER       A  A  P A       B  C  B  P B   A     P B   A        E     D E P E   A   B     A  E  E  P E   A   B        B  D     D  P D   B   C    P C   A     C     P E   A   B           C  C     D P E   A   B             C  C  P D   B   C    P D   B   C    P C   A    P C   A     P D   B   C    P D   B   C    P C   A    P C   A     P D   B   C    P D   B   C    P C   A    P C   A     P D   B   C    P C   A                          Figure    Arc weights for probabilistic networks      A    B          E       Weighted AND OR Search Graphs     C       E       D       Figure    Meta node them when the same nodes are encountered again  Some unifiable nodes can be identified based on their contexts  We can define graph based contexts for both OR nodes and AND nodes  just by expressing the set of ancestor variables in T that completely determine a conditioned subproblem  However  it can be shown that using caching based on OR contexts makes caching based on AND contexts redundant  so we will only use OR caching  Given a pseudo tree T of an AND OR search space  the context of an OR node X  denoted by context X     X        Xp    is the set of ancestors of X in T ordered descendingly  that are connected in the primal graph to X or to descendants of X  It is easy to verify that the context of X separates the subproblem below X from the rest of the network  The context minimal AND OR graph is obtained by merging all the context unifiable OR nodes  It was shown that         T HEOREM   Given a graphical model M  its primal graph G and a pseudo tree T   the size of the context min imal AND OR search graph based on T is O n k wT  G      where wT  G  is the induced width of G over the depth first traversal of T   and k bounds the domain size  Figure   a  shows the primal graph of a graphical model defined by the functions f            f    which are assumed to be strictly positive  i e   every assignment is valid   Figure   b  shows a pseudo tree for the graph  The dotted lines are edges in the primal graph  and back arcs in the pseudo tree  The OR context of each node is shown in square brackets  Figure   c  shows the AND OR search tree and   d  shows the context minimal AND OR graph   In some cases  e g  constraint networks   the functions of the graphical model take binary values    and    or true and false   In this case  an AND OR search graph expresses the consistency  valid or not  of each assignment  and can associate this value with its leaves  In more general cases  which are the focus of this paper  the functions of the graphical model take  positive  real values  called weights  For example  in Bayesian networks the weights express the conditional probability  In the more general case of weighted models  it is useful to associate weights to the internal OR AND arcs in the AND OR graph  to maintain the global function decomposition and facilitate the merging of nodes  D EFINITION    buckets relative to a backbone tree  Given a graphical model R   hX  D  F  i and a backbone tree T   the bucket of Xi relative to T   denoted by BT  Xi    is the set of functions whose scopes contain Xi and are included in pathT  Xi    which is the set of variables from the root to Xi in T   Namely  BT  Xi      f  F  Xi  scope f    scope f    pathT  Xi     D EFINITION    OR AND weights  Given an AND OR tree ST  R   of a graphical model R  the weight w n m   Xi   xi   of arc  n  m  where Xi labels n and xi labels m  is the combination  e g  product  of all the functions in BT  Xi   assigned by values along the path to m  m   Formally  w n m   Xi   xi     f BT  Xi   f  asgn m   scope f      Figure   shows a belief network  a DFS tree that drives its weighted AND OR search tree  and a portion of the AND OR search tree with the appropriate weights on the arcs expressed symbolically  In this case the bucket of E contains the function P  E A  B   and the bucket of C contains two functions  P  C A  and P  D B  C   Note that P  D B  C  belongs neither to the bucket of B nor to the bucket of D  but it is contained in the bucket of C  which is the last variable in its scope to be instantiated in a path from the root of the pseudo tree         MATEESCU   DECHTER A D  A  G     B     B  C  B  F  E  A  H  C     F  D  E  G  H  C    B       C       C       C       F          F       F       F           a  Primal graph and pseudo tree D  m   A     m           C             C       F  C          D             F       F    B  F        G  E                         m   G       H             D  E  G        C       D          C                   H          A B           F  B     F       A        G                 C    H          C  C     H    C  B  F    G       F  F     B  A  B    B    E  D    H    H  A  A  C          m     B  G     c  Final AOMDD  message m     H       m   F  B  A              m   C        C D  G     B    B    E    A     B  B    C       A  A    B    D    D  F  D  D  D  D  D     C            m   C      C     E     C  D               C   D  C                m                               C   C      C                 E  C            H         C   A       H  B  G    E     A  G  G  F  F  G  F G  G     G       E  F  F  F    B  G  G  E  m   A     B  H  m   A  A  E  E  E     C   m   A    D  D  D  m   m            m   G     H          F            G  C   C   F  H  H  H         d  OBDD equivalent to m    b  Variable Elimination based compilation  Figure    Execution of VE with AOMDDs     AND OR Multi Valued Decision Diagram for Constraint Networks  Constraint networks have only binary valued functions  In      we presented a compilation scheme for AOMDDs for constraint networks based on the Variable Elimination schedule  For completeness  we only provide below the main ideas for constraint networks  and then present the current contribution extending the AOMDD for weighted graphical models  The context minimal graph is a data structure that is equivalent to the given graphical model  in the sense that it represents the same set of solutions  and any query on the graphical model can be answered by inspecting the context minimal graph  Our goal is to shrink the context minimal graph even further  by identifying mergeable nodes beyond those based on context  Redundant nodes can also be identified and removed  Suppose we are given an AND OR search graph  it could also be a tree initially   The reduction rules of OBDDs are also applicable to it  if we maintain the semantics  In particular  we have to detail the treatment of AND nodes and OR nodes  If we consider only reduction by isomorphism  then the AND OR graph can be processed by ignoring the AND or OR attributes of the nodes  If we consider reduction by redundancy  then it is useful to group each OR node  together with its AND children into a meta node  D EFINITION    meta node  A nonterminal meta node v in an AND OR search graph consists of an OR node labeled var v    Xi and its ki AND children labeled hXi   xi  i          hXi   xiki i that correspond to its value assignments  We will sometimes abbreviate hXi   xij i  by xij   Each AND node labeled xij points to a list of child meta nodes  u childrenj   Consider the pseudo tree in Figure   b   An example of meta node corresponding to variable A is given in Figure    assuming three values  That is just a portion of an AND OR graph  where redundant meta nodes were removed  For A      the child meta node has variable B  For A      B is irrelevant so the corresponding meta node was removed  and there is an AND arc pointing to E and C  For A      both B and C are irrelevant  This example did not take into account possible weights on the OR AND arcs       Compiling AOMDDs by Variable Elimination  Consider the network defined by X    A  B          H   DA           DH          and the constraints   denotes XOR   C    F H  C    AH  C    ABG  C    F G  C    BF   C    AE  C    CE  C    CD  C    B  C  The constraint graph is shown in Figure   a     MATEESCU   DECHTER Algorithm     APPLY  v     w            wm      AOMDDs f with nodes vi and g with nodes wj   based on compatible pseudo trees T    T  that can be embedded in T   var v    is an ancestor of all var w             var wm   in T   var wi    var wj   are not ancestor descendant in T   output   AOMDD v      w          wm    based on T   if H   v    w            wm      null then return H   v    w            wm   if  any of v    w            wm is    then return   if  v       then return   if  m      then return v  create new nonterminal meta node u var u   var v     call it Xi   with domain Di    x            xki     for j    to ki do u childrenj      children of j th AND node of u if    m      and  var v      var w      Xi     then temp Children  w   childrenj input                              else      group nodes from v   childrenj  temp Children in several  v     w            wr   for each  v     w            wr   do y  APPLY v     w            wr   if  y      then u childrenj     break                     temp Children   w            wm    where F has two children  G and H  Some of the nodes corresponding to F have two outgoing edges for value    The processing continues in the same manner The final output of the algorithm  which coincides with m    is shown in Figure   c   The OBDD based on the same ordering d is shown in Fig    d   Notice that the AOMDD has    nonterminal nodes and    edges  while the OBDD has    nonterminal nodes and    edges  We present the APPLY algorithm for combining AOMDDs for constraints  It was shown in      that the complexity of the APPLY is at most quadratic in the input  In      it was shown that the time and space complexity of the VE based compilation scheme is exponential in the treewidth of the model       Compiling AOMDDs by AND OR Search  else u childrenj  u childrenj   y          if  u children            u childrenki   then return u children       if  H   var u   u children            u childrenki      null  then return H   var u   u children            u childrenki                Let H   v    w            wm     u    Let H   var u   u children            u childrenki     u    return u  Consider the ordering d    A  B  C  D  E  F  G  H   The pseudo tree induced by d is given in Fig    a   Figure   b  shows the execution of VE with AOMDDs along ordering d  Initially  the constraints C  through C  are represented as AOMDDs and placed in the bucket of their latest variable in d  Each original constraint is represented by an AOMDD based on a chain  For bi valued variables  they are OBDDs  for multiple valued they are MDDs  multivalued decision diagrams   Note that we depict metanodes  one OR node and its two AND children  that appear inside each larger square node  The dotted edge corresponds to the   value  the low edge in OBDDs   the solid edge to the   value  the high edge   We have some redundancy in our notation  keeping both AND value nodes and arc types  doted arcs from   and solid arcs from     The VE scheduling is used to process the buckets in reverse order of d  A bucket is processed by joining all the AOMDDs inside it  using the APPLY operator  described further   However  the step of eliminating the bucket variable will be omitted because we want to generate the full AOMDD  In our example  the messages m    C     C  and m    C     C  are still based on chains  so they are still OBDDs  Note that they still contain the variables H and G  which have not been eliminated  However  the message m    C     m     m  is not an OBDD anymore  We can see that it follows the structure of the pseudo tree   We describe here a search based approach for compiling an AOMDD  Theorem   ensures that the context minimal  CM  graph can be traversed by AND OR search in time  and space O n k wT  G     When full caching is used  the trace of AND OR search  i e   the AND OR graph traversed by the algorithm  is a subset of the CM graph  if pruning techniques are used  some portions of the CM graph may not be traversed   When the AND OR search algorithm terminates  its trace is an AND OR graph that expresses the original graphical model  We can therefore apply the reduction rules  isomorphism and redundancy  to the trace of the AND OR search in a single bottom up pass  that has complexity linear in the size of the trace  In fact  the reduction rules can be included in the depth first AND OR search algorithm itself  whenever the entire subgraph of a meta node has been visited  the algorithm can check for isomorphism between the current node and metanodes of the same variable  and also check redundancy  before the search retracts to the parent meta node  The end result will be the AOMDD of the original graphical model  The time and space complexity of this scheme is bounded in the worst case by that of exploring the CM graph  which is given in Theorem    i e   exponential in the treewidth of the model   In Section   we provide preliminary evaluation of the search based compilation      AND OR Multi Valued Decision Diagram for Weighted Graphs  We will now describe an extension of AOMDDs to weighted graphical models  which include probabilistic graphical models  The functions defining the model can in this case take arbitrary positive real values  The AND OR search space is well defined for such graphical models  and in particular the context minimal graph is a decision diagram that represents the same function as the model  The        MATEESCU   DECHTER A  M  A                        B  A  A  C  B f M A B                                      M                  B                  M  A B  C M                  M  M  C g M B C                                                  B  B  B  B                                                     C  C  C  C  C  C  C  C                                                      a  Graphical model                                                                                                    A  A              B  B  B  B                               C                           C  C  C                                                               c  AND OR context       minimal graph   b  AND OR search tree  Figure    Weighted graphical model M       M  M                         A  A  A     A A                          B  B  B  B  B  B  B  B                                  C  C                                                    C                                      C                                   C                                     C          a  Normalizing level of C                                     b  Promoting constants                B                                 C                B        C                               C                 C                            c  AOMDD  Figure    Normalizing values bottom up reduction rules  merge isomorphic nodes and reduce redundant nodes  are also well defined for weighted models  if we operate with meta nodes   and guaranteed to produce equivalent decision diagrams  For example  isomorphic nodes should have the same variable  the same sets of children  and the same weights on their respective ORAND arcs  If we start with the AND OR tree and apply the isomorphism rule exhaustively  we are guaranteed to obtain a graph at least as compact as the context minimal graph  This is because OR nodes that have the same context also represent isomorphic meta nodes when the isomorphic rule was applied exhaustively to all the levels below  However  the property of being a canonical representation of a function is lost in the case of weighted graphs  if we only use the usual reduction rules  Figure   a  shows a weighted graphical model  defined by two  cost  functions  f  M  A  B  and g M  B  C   Assuming the order  M  A  B  C   Figure   b  shows the AND OR search tree  The arcs are labeled with function values  and the leaves show the value of the corresponding full assignment  which is the product of numbers on the arcs of the path   We can see that either value of M    or    gives rise to the same function  because the leaves in the two subtrees have the same values   Therefore  the universal model of the conditioned subproblem for M     is identical to that for M      However  the two subtrees can not be identified as representing the same function by the usual reduction rules  because of different weights on the arcs  Figure   c  shows the context minimal graph  which  has a compact representation of each subtree  but does not share any of their parts  In these figures we do not show the contours of meta nodes  to reduce clutter  What we would like in this case is to have a method of recognizing that the left and right subtrees corresponding to M     and M     represent the same function  We do this by normalizing the weights in each level  and processing bottom up by promoting the normalization constant  In Figure   a  the weights on the OR AND arcs of level C have been normalized  and the normalization constant was promoted up to the OR node value  In Figure   b  the normalization constants are promoted upwards again by multiplication into the OR AND weights  This process does not change the value of each full assignment  and therefore produces equivalent graphs  We can see now that some of the C level  meta  nodes are mergeable  Continuing this process gives the final AOMDD for the weighted model  in Figure   c   D EFINITION    weighted AOMDD  A weighted AOMDD is an AND OR graph  with meta nodes   where for each OR node  the emanating OR AND arcs have an associated weight  such that their sum is    and the root meta node has a weight  the resulting normalization constant   The terminal nodes are just   and    The following theorem ensures the that the weighted AOMDD is a canonical representation  T HEOREM   Given two equivalent weighted graphical   MATEESCU   DECHTER Network cpcs   cpcs    cpcs   b cpcs   b c    c    s    s                                    EA  EA  EA  EA  bm       bm       mm          mm           Class  CPCS  ISCAS  GRID  LINKAGE  PRIMULA   n  d                                                                                                                                                                                          e                                                               w   h                                                                                                                                                                                      Zeros                                                                                                                          Time  sec                                                                                                                                 cm                                                                                                                                                            aomdd                                                                                                                                                  Ratio                                                                                                       Table    Results for experiments with    belief networks from   problem classes  models that accept a common pseudo tree T   normalizing arc values together with exhaustive application of reduction rules yields the same AND OR graphs  The proof is omitted here for space reasons  We only mention that the proof is by structural induction bottom up over the layers of the AND OR graph  The APPLY algorithm needs minimal modifications now to operate on weighted AOMDDs  The hash function H    which hashes meta nodes  has to take as extra arguments the weights of the meta node  Similarly  when checking redundancy in line     the weights should also be equal for the node to be redundant  and their common value has to be promoted by multiplication  When checking isomorphism in line     the corresponding weights are checked via the hash function H    The same VE schedule can now be used to compile an AOMDD for a weighted graphical model      Experimental Evaluation  Our experimental evaluation is in preliminary stages  but the results we have are already encouraging  We ran the search based compile algorithm  by recording the trace of the AND OR search  and then reducing the resulting AND OR graph bottom up  In these results we only applied the reduction by isomorphism and still kept the redundant meta nodes  Table   shows the results for    belief networks from   problem classes  medical diagnosis  CPCS   digital circuits  ISCAS   deterministic grid networks  GRID   genetic linkage analysis  LINKAGE  as well as relational belief networks  PRIMULA   For each network we chose randomly e variables and set their values as evidence  For each  query we recorded the compilation time in seconds  the number of OR nodes in the context minimal graph explored   cm  and the size of the resulting AOMDD   aomdd   In addition  we also computed the compression ratio of the AOMDD structure as ratio    cm  aomdd  We also report the number of variables  n   domain size  d   induced width  w    pseudo tree depth  h   as well as the percentage of zero probability tuples  zeros      for each test instance  We see that in a few cases the compression ratio is significant  e g   cpcs   b         s            Our future work will include the reduction rule by redundancy  as well as the compilation algorithm by Variable Elimination schedule      Conclusion and Discussion  We presented the new data structure of weighted AOMDD  as a target for compilation of weighted graphical models  It is based on AND OR search spaces and Binary Decision Diagrams  We argue that the AOMDD has an intuitive structure  and can easily be incorporated into other already existing algorithm  e g   join tree clustering   We provide two compilation methods  one based on Variable Elimination and the other based on search  both being time and space exponential in the treewidth of the graphical model  The preliminary experimental evaluation is quite encouraging  and shows the potential of the new AOMDD data structure  Compiling graphical models into weighted AOMDDs also extends decision diagrams for the computation of semiring valuations       from linear variable ordering into treebased partial ordering  This provides an improvement        MATEESCU   DECHTER  of the complexity guarantees to exponential in treewidth  rather than pathwidth  There are various lines of related research  We only mention here  deterministic decomposable negation normal form  d DNNF       case factor diagrams       compilation of CSPs into tree driven automata       and the recent work on compilation          We think that our framework using AND OR search graphs has a unifying quality that helps make connections among seemingly different compilation techniques  The approach of compiling graphical models into AOMDDs may seem to go against the current trend in model checking  which moves away from BDD based algorithms into CSP SAT based approaches  However  algorithms that are search based and compiled data structures such as BDDs differ primarily by their choices of time vs memory  When we move from regular OR search space to an AND OR search space the spectrum of algorithms available is improved for all time vs memory decisions  We believe that the AND OR search space clarifies the available choices and helps guide the user into making an informed selection of the algorithm that would fit best the particular query asked  the specific input function and the computational resources   Acknowledgments We thank Radu Marinescu for his help with the experimental evaluation  This research was supported in part by the NSF grant IIS           
 In this paper we compare search and inference in graphical models through the new framework of AND OR search  Specifically  we compare Variable Elimination  VE  and memoryintensive AND OR Search  AO  and place algorithms such as graph based backjumping and no good and good learning  as well as Recursive Conditioning     and Value Elimination     within the AND OR search framework      work  We show that there is no principled difference between memory intensive search restricted to fixed variable ordering and inference beyond     different direction of exploring a common search space  top down for search vs  bottom up for inference      different assumption of control strategy  depth first for search and breadth first for inference   We also show that those differences have no practical effect  except under the presence of determinism  Our analysis assumes a fixed variable ordering  Some of the conclusions may not hold for dynamic variable ordering  Section   provides background  Section   compares VE with AO search  Section   addresses the effect of advanced algorithmic schemes and section   concludes   INTRODUCTION    It is convenient to classify algorithms that solve reasoning problems of graphical models as either search  e g   depth first  branch and bound  or inference  e g   variable elimination  join tree clustering   Search is time exponential in the number of variables  yet it can be accomplished in linear memory  Inference exploits the models graph structure and can be accomplished in time and space exponential in the problems tree width  When the tree width is big  inference must be augmented with search to reduce the memory requirements  In the past three decades search methods were enhanced with structure exploiting methods  These improvements often require substantial memory  making the distinction between search and inference fuzzy  Recently  claims regarding the superiority of memory intensive search over inference or vice versa are made      Our aim is to clarify this relationship and to create cross fertilization using the strengths of both schemes  In this paper we compare search and inference in graphical models through the new framework of AND OR search  recently introduced       Specifically  we compare Variable Elimination  VE  against memory intensive AND OR Search  AO   and place algorithms such as graph based backjumping  no good and good learning  and look ahead schemes      as well as Recursive Conditioning     and Value Elimination     within the AND OR search frame        BACKGROUND GRAPHICAL MODELS  A graphical model is defined by a collection of functions  over a set of variables  conveying probabilistic or deterministic information  whose structure is captured by a graph  D EFINITION      graphical models  A graphical model is a   tuple M hX  D  F  i  where     X  X            Xn   is a set of variables     D  D            Dn   is a set of finite domains of values     F   f            fr   is a set of real valued functions  The scope of function fi   denoted scope f  X  is the set of arguments of fi    Q i  P i fi    i fi   i fi     i fi   is a combination operator  The graphical model represents the combination of all its functions  namely the set  ri   fi   When the combination operator is irrelevant we denote M by hX  D  F i  D EFINITION      primal graph  The primal graph of a graphical model is an undirected graph that has variables as its vertices and edges connecting any two variables that appear in the scope of the same function  Two central graphical models are belief networks and constraint networks  A belief network B   hX  D  P i is defined over a directed acyclic graph G    X  E  and its   A  A   P A  h  A   B   P B A  h  AB  h  AB   E   P E AB   C   P C A  h  BC   bucket A  A A  AB bucket B B  E  C  D   a   D   AB  AB  ABC bucket C  ABE bucket E  BC  P D BC    b        AND OR SEARCH SPACE  BCD bucket D   c   Figure    Execution of Variable Elimination functions Pi denotes conditional probability tables  CPTs   Pi    P  Xi   pai     where pai is the set of parent nodes pointing to Xi in G  Common tasks are finding the posterior probability of some variables given the evidence  or finding the most probable assignment to all the variables given the evidence  A constraint network R   hX  D  Ci has a set of constraints C    C         Ct   as its functions  Each constraint is a pair Ci    Si   Ri    where Si  X is the scope of the relation Ri defined over Si   denoting the allowed combination of values  Common tasks are finding a solution and counting the number of solutions  We assume that the domains of functions include a zero element     Combining  e g   multiplying  anything with   yields a    The   value expresses inconsistent tuples  This is a primary concept in constraint networks but can also be defined relative to a graphical model as follows  Each function fi expresses an implicit flat constraint which is a relation Ri over its scope that includes all and only the consistent tuples  namely those that are not mapped to    In this paper  a constraint network refers also to the flat constraints that can be extracted from any graphical model  When all the assignments are consistent we say that the graphical model is strictly positive  A partial assignment is consistent if none of its functions evaluate to zero  A solution is a consistent assignment to all the variables  We assume the usual definitions of induced graphs  induced width  tree width and path width              d    A  B  E  C  D   The buckets are processed from D to A     Figure  c shows the bucket tree   INFERENCE BY VARIABLE ELIMINATION  Variable elimination algorithms        are characteristic of inference methods  Consider a graphical model G   hX  D  F i and an ordering d    X    X            Xn    The ordering d dictates an elimination order for VE  from last to first  All functions in F that contain Xi and do not contain any Xj   j   i  are placed in the bucket of Xi   Buckets are processed from Xn to X  by eliminating the bucket variable  combining all functions and removing the variable by a marginalization  and placing the resulting function  also called message  in the bucket of its latest variable in d  This VE procedure also constructs a bucket tree  by linking each bucket Xi to the one where the resulting function generated in bucket Xi is placed  which is called the parent of Xi   Example     Figure  a shows a belief network  Figure b shows the execution of Variable Elimination along ordering  The usual way to do search consists of instantiating variables in turn  we only consider fixed variable ordering   In the simplest case this defines a search tree  whose nodes represent states in the space of partial assignments  A depth first search  DFS  algorithm searching this space could run in linear space  If more memory is available  then some of the traversed nodes can be cached  and retrieved when similar nodes are encountered  The traditional search space does not capture the structure of the underlying graphical model  Introducing AN D nodes into the search space can capture the structure decomposing the problem into independent subproblems by conditioning on values           Since the size of the AND OR tree may be exponentially smaller than the traditional search tree  any algorithm searching the AND OR space enjoys a better computational bound  For more details see          A classical algorithm that explores the AND OR search space is Recursive Conditioning      Given a graphical model M  its AND OR search space is based on a pseudo tree       D EFINITION      pseudo tree  Given an undirected graph G    V  E   a directed rooted tree T    V  E     defined on all its nodes is called pseudo tree if any arc of G which is not included in E   is a back arc  namely it connects a node to an ancestor in T          AND OR Search Tree  Given a graphical model M   hX  D  F i  its primal graph G and a pseudo tree T of G  the associated AND OR search tree  denoted ST  M   has alternating levels of AND and OR nodes  The OR nodes are labeled Xi and correspond to the variables  The AND nodes are labeled hXi   xi i and correspond to the values in the domains of the variables  The structure of the AND OR search tree is based on the underlying backbone pseudo tree T   The root of the AND OR search tree is an OR node labeled with the root of T   The children of an OR node Xi are AND nodes labeled with assignments hXi   xi i that are consistent with the assignments along the path from the root  path xi      hX    x  i  hX    x  i          hXi    xi  i   Consistency is well defined for constraint networks  For probabilistic networks it is defined relative to the underlying flat constraint network derived from the belief network  The children of an AND node hXi   xi i are OR nodes labeled with the children of variable Xi in the pseudo tree T   Arc labeling The arcs from Xi to hXi   xi i are labeled with the appropriate combined values of the functions in F that   This is a non standard graphical representation  reversing the top down bucket processing described in earlier papers    B              B  B  B  B    E  E  A  A  A  C D         C     E           C     E                 C  E         E  C    E    C  E    C  E  C     D  D  D  D  D  D  D  D                                           a     C                    D  D  D  D         b   Figure    AND OR search tree contain Xi and have their scopes fully assigned  When the pseudo tree is a chain  the AND OR search tree coincides with the regular OR search tree  Example     Consider again the belief network in Figure  a  Figure  a shows a pseudo tree of its primal graph  together with the back arcs  dotted lines   Figure  b shows the AND OR search tree based on the pseudo tree  for binary        valued variables assuming positive functions  Arc labels are not included   Figure    Context minimal AND OR search graph Based on earlier work by          it can be shown that  T HEOREM      size of minimal context graphs  Given a graphical model M  a pseudo tree T and w the induced width of G along the depth first traversal of T      The size of CMT  M  is O n  k w    when k bounds the domain size     The context minimal AND OR search graph  relative to all pseudo trees  is bounded exponentially by the treewidth  while the context minimal OR search graph is bounded exponentially by the path width   Based on earlier work by                it can be shown that  T HEOREM     Given a graphical model M and a pseudo tree T   the size of the AND OR search tree ST is O n  exp m   where m is the depth of T   A graphical model that has a tree width w has an AND OR search tree whose size is O n  exp w  log n    D EFINITION      backtrack free  An AND OR search tree of a graphical model is backtrack free iff all nodes that do not root a consistent solution are pruned         AND OR Search Graph  The AND OR search tree may contain nodes that root identical subtrees  These are called unifiable  When unifiable nodes are merged  the search space becomes a graph  Its size becomes smaller at the expense of using additional memory when being traversed  When all unifiable nodes are merged  a computational intensive task  we get the unique minimal AND OR graph  Some unifiable nodes can be identified based on their contexts     or conflict sets      The context of an AND node hXi   xi i is defined as the set of ancestors of Xi in the pseudo tree  including Xi   that are connected  in the induced primal graph  to descendants of Xi   It is easy to verify that the context of Xi dseparates      the subproblem below Xi from the rest of the network  The context minimal AND OR graph denoted CMT  M   is obtained by merging all the context unifiable AND nodes  When the graphical model is strictly positive  it yields the full context minimal graph  The backtrack free context minimal graph  BF  CMT   is the context minimal graph where all inconsistent subtrees are pruned  Example     Figure   shows the full context minimal graph of the problem and pseudo tree from Figure     Value function A task over a graphical model  e g   belief updating  counting  induces a value function for each node in the AND OR space  The algorithmic task is to compute the value of the root  This can be done recursively from leaves to root by any traversal scheme  When an AO traversal of the search space uses full caching based on context it actually traverses the context minimal  CMT   graph  It is this context minimal graph that allows comparing the execution of AO search against VE      VE VS  AO SEARCH  VEs execution is uniquely defined by a bucket tree  and since every bucket tree corresponds to a pseudo tree  and a pseudo tree uniquely defines the context minimal AND OR search graph  we can compare both schemes on this common search space  Furthermore  we choose the contextminimal AND OR search graph  CM  because algorithms that traverse the full CM need memory which is comparable to that used by VE  namely  space exponential in the tree width of their pseudo bucket trees  Algorithm AO denotes any traversal of the CM search graph  AO DF is a depth first traversal and AO BF is a breadth first traversal  We will compare VE and AO via the portion of this graph that they generate and by the order of node generation  The tasks value computation performed during search adds only a constant factor  We distinguish graphical models with or without determinism  namely  graphical models that have inconsistencies vs  those that have none  We compare brute force versions of VE and AO  as well as versions enhanced by various known features  We assume that the task requires the examination of all solutions  e g  belief updating  counting solutions     D   A  h   D   D  bucket D  D    D  C   h  CD   CD  D    C    bucket C  C    C    C    CD  B  C  B   P D BC  h  BC   BCD  bucket B  B C D   BC  A   P A  P B A  P C A  h  AB   ABC  D  E   P E AB   ABE   b    a   B C D   B C D   B C D   B C D   B    B    B C D   B C D   B    bucket A  AB  E  B C D   B   A B C   bucket E   c   A B C   A B C   A   E A B   E A B   E    A B C   E A B   A B C   A B C   A    A    E A B   E    E    A B C   A    E A B   E A B   A B C   E A B   E A B   E    Figure    Variable Elimination Figure    Context minimal AND OR search space      VE VS  AO WITH NO DETERMINISM  We start with the simplest case in which the graphical model contains no determinism and the bucket tree  pseudo tree  is a chain         OR Search Spaces  Figure  a shows a Bayesian network  Lets consider the ordering d    D  C  B  A  E  which has the tree width w d    w      Figure  b shows the bucket chain and a schematic application of VE along this ordering  the bucket of E is processed first  and the bucket of D last   The buckets include the initial CPTs and the functions that are generated and sent  as messages  during the processing  Figure  c shows the bucket tree  If we use the chain bucket tree as pseudo tree for the AND OR search along d  we get the full CM graph given in Figure    Since this is an OR space  we can eliminate the OR levels as shown  Each level of the graph corresponds to one variable  The edges should be labeled with the product of the values of the functions that have just been instantiated on the current path  We note on the arc just the assignment to the relevant variables  e g   B  denotes B       For example  the edges between C and B in the search graph are labeled with the function valuation on  BCD   namely P  D B  C   where for each individual edge this function is instantiated as dictated on the arcs  AO DF computes the value  e g   updated belief  of the root node by generating and traversing the context minimal graph in a depth first manner and accumulating the partial value  e g   probabilities  using combination  products  and marginalization  summation   The first two paths generated by AO DF are  D    C    B    A    E    and  D    C    B    A    E     which allow the first accumulation of value h   A  B      P  E   A  B      P  E   A  B     AO DF subsequently generates the two paths  D    C    B    A    E    and  D    C    B    A    E    and accumulates the next partial value h   A  B      P  E   A  B      P  E   A  B     Subsequently it computes the summation h   B  C      P  A     P  B   A     P  C   A     h   A  B      P  A     P  B   A     P  C   A     h   A  B     Notice that due to caching each arc is gener   ated and traversed just once  in each direction   For example when the partial path  D    C    B    is created  it is recognized  via context  that the subtree below was already explored and its compiled value will be reused  In contrast  VE generates the full context minimal graph by layers  from the bottom of the search graph up  in a manner that can be viewed as dynamic programming or as breadthfirst search on the explicated search graph  VEs execution can be viewed as first generating all the edges between E and A  in some order   and then all the edges between A and B  in some order   and so on up to the top  We can see that there are   edges between E and A  They correspond to the   tuples in the bucket of E  the function on  ABE    There are   edges between A and B  corresponding to the   tuples in the bucket of A  And there are   edges between B and C  corresponding to the   tuples in the bucket of B  Similarly    edges between C and D correspond to the   tuples in the bucket of C  and   edges between D and the rood correspond to the   tuples in the bucket of D  Since the computation is performed from bottom to top  the nodes of A store the result of eliminating E  namely the function h   AB  resulting by summing out E   There are   nodes labeled with A  corresponding to the   tuples in the message sent by VE from bucket of E to bucket of A  the message on  AB    And so on  each level of nodes corresponds to the number of tuples in the message sent on the separator  the common variables  between two buckets         AND OR Graphs  The above correspondence between Variable Elimination and AND OR search is also maintained in non chain pseudo bucket trees We refer again to the example in Figures      and   and assume belief updating  The bucket tree in Figure  c has the same structure as the pseudo tree in Figure  a  We will show that VE traverses the AND OR search graph in Figure   bottom up  while AO DF traverses the same graph in depth first manner  top down  AO DF first sums h   A    B      P  E   A    B      P  E   A    B    and then goes depth first to h   B    C      P  D   B    C      P  D   B    C    and h   B    C      P  D   B    C      P  D   B    C     Then it computes   h   A    B       P  C   A     h   B    C        P  C   A     h   B    C      All the computation of AO DF is precisely the same as the one performed in the buckets of VE  Namely  h  is computed in the bucket of D and placed in the bucket of C  h  is computed in the bucket of C and placed in the bucket of B  h  is computed in the bucket of E and also placed in the bucket of B and so on  as shown in Figure  b  All this corresponds to traversing the AND OR graph from leaves to root  Thus  both algorithms traverse the same graph  only the control strategy is different  We can generalize both the OR and AND OR examples  T HEOREM      VE and AO DF are identical  Given a graphical model having no determinism  and given the same bucket pseudo tree VE applied to the bucket tree is a  breadth first  bottom up search that will explore all the full CM search graph  while AO DF is a depth first top down search that explores  and records  the full CM graph as well  Breadth first on AND OR  Since one important difference between AO search and VE is the order by which they explore the search space  top down vs  bottom up  we wish to remove this distinction and consider a VE like algorithm that goes top down  One obvious choice is breadth first search  yielding AO BF  That is  in Figure   we can process the layer of variable A first  then B  then E and C  and then D  General breadth first or best first search of AND OR graphs for computing the optimal cost solution subtrees are well defined procedures  The process involves expanding all solution subtrees in layers of depth  Whenever a new node is generated and added to the search frontier the value of all relevant partial solution subtrees are updated  A well known Best first version of AND OR spaces is the AO  algorithm       Algorithm AO BF can be viewed as a topdown inference algorithm  We can now extend the comparison to AO BF  Proposition    VE and AO BF are identical  Given a graphical model with no determinism and a bucket pseudo tree  VE and AO BF explore the same full CM graph  one bottom up  VE  and the other top down  both perform identical value computation  Terminology for algorithms comparison  Let A and B be two algorithms over graphical models  whose performance is determined by an underlying bucket pseudo tree  D EFINITION      comparison of algorithms  We say that     algorithms A and B are identical if for every graphical model and when given the same bucket tree they traverse an identical search space  Namely  every node is explored by A iff it is explored by B     A is weakly better than B if there exists a graphical model and a bucket tree  for which A explores a strict subset of the nodes explored by B     A is better than B if A is weakly  better than B but B is not weakly better than A     The relation of weakly better defines a partial order between algorithms  A and B are not comparable if they are not comparable w r t to the weakly better partial order  Clearly  any two algorithms for graphical models are either    identical     one is better than the other  or    they are not comparable  We can now summarize our observations so far using the new terminology  T HEOREM     For a graphical model having no determinism AO DF  AO BF and VE are identical  Note that our terminology captures the time complexity but may not capture the space complexity  as we show next         Space Complexity  To make the complete correspondence between VE and AO search  we can look not only at the computational effort  but also at the space required  Both VE and AO search traverse the context minimal graph  but they may require different amounts of memory to do so  So  we can distinguish between the portion of the graph that is traversed and the portion that should be recorded and maintained  If the whole graph is recorded  then the space is O n  exp w     which we will call the base case  VE can forget layers Sometimes  the task to be solved can allow VE to use less space by deallocating the memory for messages that are not necessary anymore  Forgetting previously traversed layers of the graph is a well known property of dynamic programming  In such a case  the space complexity for VE becomes O dBT  exp w    where dBT is the depth of the bucket tree  assuming constant degree in the bucket tree   In most cases  the above bound is not tight  If the bucket tree is a chain  then dBT   n  but forgetting layers yields an O n  improvement over the base case  AO DF cannot take advantage of this property of VE  It is easy to construct examples where the bucket tree is a chain  for which VE requires O n  less space than AO DF  AO dead caches The straightforward way of caching is to have a table for each variable  recording its context  However  some tables might never get cache hits  We call these dead caches  In the AND OR search graph  dead caches appear at nodes that have only one incoming arc  AO search needs to record only nodes that are likely to have additional incoming arcs  and these nodes can be determined by inspection from the pseudo tree  Namely  if the context of a node includes that of its parent  then AO need not store anything for that node  because it would be a dead cache  In some cases  VE can also take advantage of dead caches  If the dead caches appear along a chain in the pseudo tree  then avoiding the storage of dead caches in AO corresponds to collapsing the subsumed neighboring buckets in the bucket tree  This results in having cache tables of   Figure    CM graphs with determinism  a  AO  b  VE the size of the separators  rather than the cliques  The time savings are within a constant factor from the complexity of solving the largest clique  but the space complexity can be reduced from exponential in the size of the maximal cique to exponential in the maximal separator  However  if the variables having dead caches form connected components that are subtrees  rather than chains  in the pseudo tree  then the space savings of AO cannot be achieved by VE  Consider the following example  Example     Let variables  X            Xn   be divided in   three sets  A    X            X n     B    X n               X  n             X    There are two cliques on and C    X  n n      A  B and A  C defined by all possibile binary functions over variables in those respective cliques  The input is therefore O n     Consider the bucket tree  pseudo tree  defined by the ordering d    X            Xn    where Xn is eliminated first by VE  In this pseudo tree  all the caches are dead  and as a result the AO search graph coincides with the AO search tree  Therefore  AO can solve the problem using space O   n      VE can collapse some neighboring buckets  for variables in B and C   but needs to store at least one message on the variables in A  which yields space complexity O exp  n      In this example  AO and VE have the same time complexity  but AO uses space linear in the number of variables while VE needs space exponential in the number of variables  and exponential in the input too   The above observation is similar to the known properties of depth first vs  breadth first search in general  When the search space is close to a tree  the benefit from the inherent memory use of breadth first search is nil       VE VS  AO WITH DETERMINISM  When the graphical model contains determinism the AND OR trees and graphs are dependant not only on the primal graph but also on the  flat  constraints  namely on the consistency and inconsistency of certain relationships  no good tuples  in each relation  In such cases AO and VE  may explore different portions of the context minimal graphs because the order of variables plays a central role  dictating where the determinism reveals itself  Example     Lets consider a problem on four variables  A  B  C  D  each having the domain               and the  constraints A   B  B   C and C   D  The primal graph of the problem is a chain  Lets consider the natural ordering from A to D  which gives rise to a chain pseudo tree  and bucket tree  rooted at A  Figure  a shows the full CM graph with determinism generated by AO search  and Figure  b the graph generated and traversed by VE in reverse order  The thick lines and the white nodes are the ones traversed  The dotted lines and the black nodes are not explored  when VE is executed from D  the constraint between D and C implies that C     is pruned  and therefore not further explored   Note that the intersection of the graphs explored by both algorithms is the backtrack free AND OR context graph  corresponding to the unique solution  A   B   C   D     As we saw in the example  AO and VE explore different parts of the inconsistent portion of the full CM  Therefore  in the presence of determinism  AO DF and AO BF are both un comparable to VE  as they differ in the direction they explore the CM space  T HEOREM     Given a graphical model with determinism  then AO DF and AO BF are identical and both are uncomparable to VE  This observation is in contrast with claims of superiority of one scheme or the other      at least for the case when variable ordering is fixed and no advanced constraint propagation schemes are used and assuming no exploitation of context independence      ALGORITHMIC ADVANCES AND THEIR EFFECT  So far we compared brute force VE to brute force AO search  We will now consider the impact of some enhancements on this relationship  Clearly  both VE and AO explore the portion of the context minimal graph that is backtrack free  Thus they can differ only on the portion that is included in full CM and not in the backtrack free CM  Indeed  constraint propagation  backjumping and nogood recording just reduce the exploration of that portion of the graph that is inconsistent  Here we compare those schemes against bare VE and against VE augmented with similar enhancements whenever relevant       VE VS  AO WITH LOOK AHEAD  In the presence of determinism AO DF and AO BF can naturally accommodate look ahead schemes which may avoid parts of the context minimal search graph using some level of constraint propagation  It is easy to compare AO BF against AO DF when both use the same look ahead because the notion of constraint propagation as look ahead is well defined for search and because both algorithms explore the search space top down  Not surprisingly when   both algorithms have the same level of look ahead propagation  they explore an identical search space      We can also augment VE with look ahead constraint propagation  e g   unit resolution  arc consistency   yielding VELAH as follows  Once VE LAH processes a single bucket  it then applies constraint propagation as dictated by the look ahead propagation scheme  bottom up   then continues with the next bucket applied over the modified set of functions and so on  We can show that  T HEOREM     Given a graphical model with determinism and given a look ahead propagation scheme  LAH     AO DF LAH and AO BF LAH are identical     VE and VE LAH are each un comparable with each of AO DF LAH and AO BF LAH       GRAPH BASED NO GOOD LEARNING  AO search can be augmented with no good learning      Graph based no good learning means recording that some nodes are inconsistent based on their context  This is automatically accomplished when we explore the CM graph which actually amounts to recording no goods and goods by their contexts  Therefore AO DF is identical to AO BF and both already exploit no goods  we get that  AO NG denotes AO with graph based no good learning   T HEOREM     For every graphical model the relationship between AO NG and VE is the same as the relationship between AO  Depth first or breadth first  and VE  Combined no goods and look ahead  No goods that are generated during search can also participate in the constraint propagation of the look ahead and strengthen the ability to prune the search space further  The graphical model itself is modified during search and this affects the rest of the look ahead  It is interesting to note that AO BF is not able to simulate the same pruned search space as AODF in this case because of its breadth first manner  While AO DF can discover deep no goods due to its depth first nature  AO BF has no access to such deep no goods and cannot use them within a constraint propagation scheme in shallower levels  However  even when AO exploits nogoods within its look ahead propagation scheme  VE and AO remain un comparable  Any example that does not allow effective no good learning can illustrate this  Example     Consider a constraint problem over n variables  Variables X            Xn  have the domain                n        made of n   integer values and a special  value  Between any pair of the n    variables there is a not equal constraint between the integers and equality between stars  There is an additional variable Xn which has a constraint with each variable  whose values are consistent only with the  of the other n   variables  Clearly if the ordering is d    X            Xn    Xn    AO may search                                          a                                               b             c    d   Figure    GBJ vs  AND OR search all the exponential search space over the first n    variables  the inconsistent portion  before it reaches the  of the n  th variable  On the other hand  if we apply VE starting from the n  th variable  we will reveal the only solution immediately  No constraint propagation  nor nogood learning can help any AO search in this case  T HEOREM     Given a graphical model with determinism and a particular look ahead propagation scheme LAH     AO DF LAH NG is better than AO BF LAH NG     VE and AO DF LAH NG are not comparable       GRAPH BASED BACKJUMPING  Backjumping algorithms     are backtracking search applied to the OR space  which uses the problem structure to jump back from a dead end as far back as possible  In graph based backjumping  GBJ  each variable maintains a graph based induced ancestor set which ensures that no solutions are missed by jumping back to its deepest variable  DFS orderings  If the ordering of the OR space is a DFS ordering of the primal graph  it is known     that all the backjumps are from a variable to its DFS parent  This means that naive AO DF automatically incorporates GBJ jumping back character  Pseudo tree orderings  In the case of pseudo tree orderings that are not DFS trees  there is a slight difference between OR GBJ and AO DF and GBJ may sometime perform deeper backjumps than those implicitly done by AO  Figure  a shows a probabilistic model   b a pseudo tree and  c a chain driving the OR search  top down   If a deadend is encountered at variable    GBJ retreats to    see  c   while naive AO DF retreats to    the pseudo tree parent  When the deadend is encountered at    both algorithms backtrack to   and then to    Therefore  in such cases  augmenting AO with GBJ can provide additional pruning on top of the AND OR structure  In other words  GBJ on OR space along a pseudo tree is never stronger than GBJ on AND OR and it is sometimes weaker  GBJ can be applied using an arbitrary order d for the OR space  The ordering d can be used to generate a pseudo tree  In this case  however  to mimic GBJ on d  the AO   A  A  A B  AB  ABE  B  C  AC  BCD  D  BC  A D DF  A  AB  AC  ABE  BCD  DF  C A  D  B  D  B  F  E E F   a    b   C  C  D  A  F  E   c   Figure    RC d trees and AND OR pseudo trees traversal will be controlled by d  In Figure  d we show an arbitrary order d                            which generates the pseudo tree in  b  When AO search reaches    it goes in a breadth first manner to    according to d  It can be shown that GBJ in order d on OR space corresponds to the GBJbased AND OR search based on the associated pseudo tree  All the backjumps have a one to one correspondence  Since VE is not comparable with AO DF  it is also uncomparable with AO DF GBJ  Note that backjumping is not relevant to AO BF or VE  In summary  T HEOREM        When the pseudo tree is a DFS tree AO DF is identical to AO DF GBJ  This is also true when the AND OR search tree is explored  rather than the CMgraph      AO DF GBJ is superior to AO DF for general pseudo trees     VE is not comparable to AO DF GBJ       RECURSIVE CONDITIONING AND VALUE ELIMINATION  Recursive Conditioning  RC      defined for belief networks is based on the divide and conquer paradigm  RC instantiates variables with the purpose of breaking the network into independent subproblems  on which it can recurs using the same technique  The computation is driven by a data structure called dtree  which is a full binary tree  the leaves of which correspond to the network CPTs  It can be shown that RC explores an AND OR space  Consider the example in Figure    which shows   a  a belief network   b  and  c   two dtrees and the corresponding pseudo trees for the AND OR search  It can also be shown that the context of the nodes in RC is identical to that in AND OR and therefore equivalent caching schemes can be used   We show that there is no principled difference between memory intensive search with fixed variable ordering and inference beyond     different direction of exploring a common search space  top down for search vs  bottomup for inference      different assumption of control strategy  depth first for search and breadth first for inference   We also show that those differences occur only in the presence of determinism  We show the relationship between algorithms such as graph based backjumping and no good learning      as well as Recursive Conditioning     and Value Elimination     within the AND OR search space  AND OR search spaces can also accommodate dynamic variable and value ordering which can affect algorithmic efficiency significantly  Variable Elimination and general inference methods however require static variable ordering  This issue will be addressed in future work  Acknowledgments This work was supported in part by the NSF grant IIS        and the MURI ONR award N                 
 Inspired by the recently introduced framework of AND OR search spaces for graphical models  we propose to augment Multi Valued Decision Diagrams  MDD  with AND nodes  in order to capture function decomposition structure and to extend these compiled data structures to general weighted graphical models  e g   probabilistic models   We present the AND OR Multi Valued Decision Diagram  AOMDD  which compiles a graphical model into a canonical form that supports polynomial  e g   solution counting  belief updating  or constant time  e g  equivalence of graphical models  queries  We provide two algorithms for compiling the AOMDD of a graphical model  The first is search based  and works by applying reduction rules to the trace of the memory intensive AND OR search algorithm  The second is inference based and uses a Bucket Elimination schedule to combine the AOMDDs of the input functions via the the APPLY operator  For both algorithms  the compilation time and the size of the AOMDD are  in the worst case  exponential in the treewidth of the graphical model  rather than pathwidth as is known for ordered binary decision diagrams  OBDDs   We introduce the concept of semantic treewidth  which helps explain why the size of a decision diagram is often much smaller than the worst case bound  We provide an experimental evaluation that demonstrates the potential of AOMDDs      Introduction The paper extends decision diagrams into AND OR multi valued decision diagrams  AOMDDs  and shows how graphical models can be compiled into these data structures  The work presented in this paper is based on two existing frameworks      AND OR search spaces for graphical models and     decision diagrams      AND OR Search Spaces AND OR search spaces  Dechter   Mateescu      a      b        have proven to be a unifying framework for various classes of search algorithms for graphical models  The main characteristic is the exploitation of independencies between variables during search  which can provide exponential speedups over traditional search methods that can be viewed as traversing an OR structure  The c      AI Access Foundation  All rights reserved    M ATEESCU   D ECHTER   M ARINESCU  AND nodes capture problem decomposition into independent subproblems  and the OR nodes represent branching according to variable values  AND OR spaces can accommodate dynamic variable ordering  however most of the current work focuses on static decomposition  Examples of AND OR search trees and graphs will appear later  for example in Figures   and    The AND OR search space idea was originally developed for heuristic search  Nilsson         In the context of graphical models  AND OR search  Dechter   Mateescu        was also inspired by search advances introduced sporadically in the past three decades for constraint satisfaction and more recently for probabilistic inference and for optimization tasks  Specifically  it resembles the pseudo tree rearrangement  Freuder   Quinn               that was adapted subsequently for distributed constraint satisfaction by Collin  Dechter  and Katz              and more recently by Modi  Shen  Tambe  and Yokoo         and was also shown to be related to graph based backjumping  Dechter         This work was extended by Bayardo and Miranker        and Bayardo and Schrag        and more recently applied to optimization tasks by Larrosa  Meseguer  and Sanchez         Another version that can be viewed as exploring the AND OR graphs was presented recently for constraint satisfaction  Terrioux   Jegou      b  and for optimization  Terrioux   Jegou      a   Similar principles were introduced recently for probabilistic inference  in algorithm Recursive Conditioning  Darwiche        as well as in Value Elimination  Bacchus  Dalmao    Pitassi      b      a   and are currently at the core of the most advanced SAT solvers  Sang  Bacchus  Beame  Kautz    Pitassi             Decision Diagrams Decision diagrams are widely used in many areas of research  especially in software and hardware verification  Clarke  Grumberg    Peled        McMillan         A BDD represents a Boolean function by a directed acyclic graph with two terminal nodes  labeled   and     and every internal node is labeled with a variable and has exactly two children  low for   and high for    If isomorphic nodes were not merged  we would have the full search tree  also called Shannon tree  which is the usual full tree explored by a backtracking algorithm  The tree is ordered if variables are encountered in the same order along every branch  It can then be compressed by merging isomorphic nodes  i e   with the same label and identical children   and by eliminating redundant nodes  i e   whose low and high children are identical   The result is the celebrated reduced ordered binary decision diagram  or OBDD for short  introduced by Bryant         However  the underlying structure is OR  because the initial Shannon tree is an OR tree  If AND OR search trees are reduced by node merging and redundant nodes elimination we get a compact search graph that can be viewed as a BDD representation augmented with AND nodes      Knowledge Compilation for Graphical Models In this paper we combine the two ideas  creating a decision diagram that has an AND OR structure  thus exploiting problem decomposition  As a detail  the number of values is also increased from two to any constant  In the context of constraint networks  decision diagrams can be used to represent the whole set of solutions  facilitating solutions count  solution enumeration and queries on equivalence of constraint networks  The benefit of moving from OR structure to AND OR is in a lower complexity of the algorithms and size of the compiled structure  It typically moves from being bounded exponentially in pathwidth pw   which is characteristic to chain decompositions or linear structures  to being exponentially bounded in treewidth w   which is characteristic of tree       AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  structures  Bodlaender   Gilbert         it always holds that w  pw and pw  w  log n  where n is the number of variables of the model   In both cases  the compactness result achieved in practice is often far smaller than what the bounds suggest  A decision diagram offers a compilation of a propositional knowledge base  An extension of the OBDDs was provided by Algebraic Decision Diagrams  ADD   Bahar  Frohm  Gaona  Hachtel  Macii  Pardo    Somenzi         where the terminal nodes are not just   or    but take values from an arbitrary finite domain  The knowledge compilation approach has become an important research direction in automated reasoning in the past decade  Selman   Kautz        Darwiche   Marquis        Cadoli   Donini         Typically  a knowledge representation language is compiled into a compact data structure that allows fast responses to various queries  Accordingly  the computational effort can be divided between an offline and an online phase where most of the work is pushed offline  Compilation can also be used to generate compact building blocks to be used by online algorithms multiple times  Macro operators compiled during or prior to search can be viewed in this light  Korf   Felner         while in graphical models the building blocks are the functions whose compact compiled representations can be used effectively across many tasks  As one example  consider product configuration tasks and imagine a user that chooses sequential options to configure a product  In a naive system  the user would be allowed to choose any valid option at the current level based only on the initial constraints  until either the product is configured  or else  when a dead end is encountered  the system would backtrack to some previous state and continue from there  This would in fact be a search through the space of possible partial configurations  Needless to say  it would be very unpractical  and would offer the user no guarantee of finishing in a limited time  A system based on compilation would actually build the backtrack free search space in the offline phase  and represent it in a compact manner  In the online phase  only valid partial configurations  i e   that can be extended to a full valid configuration  are allowed  and depending on the query type  response time guarantees can be offered in terms of the size of the compiled structure  Numerous other examples  such as diagnosis and planning problems  can be formulated as graphical models and could benefit from compilation  Palacios  Bonet  Darwiche    Geffner        Huang   Darwiche      a   In diagnosis  compilation can facilitate fast detection of possible faults or explanations for some unusual behavior  Planning problems can also be formulated as graphical models  and a compilation would allow swift adjustments according to changes in the environment  Probabilistic models are one of the most used types of graphical models  and the basic query is to compute conditional probabilities of some variables given the evidence  A compact compilation of a probabilistic model would allow fast response to queries that incorporate evidence acquired in time  For example  two of the most important tasks for Bayesian networks are computing the probability of the evidence  and computing the maximum probable explanation  MPE   If some of the model variables become assigned  evidence   these tasks can be performed in time linear in the compilation size  which in practice is in many cases smaller than the upper bound based on the treewidth or pathwidth of the graph  Formal verification is another example where compilation is heavily used to compare equivalence of circuit design  or to check the behavior of a circuit  Binary Decision Diagram  BDD   Bryant        is arguably the most widely known and used compiled structure  The contributions made in this paper to knowledge compilation in general and to decision diagrams in particular are the following     We formally describe the AND OR Multi Valued Decision Diagram  AOMDD  and prove it to be a canonical representation for constraint networks  given a pseudo tree        M ATEESCU   D ECHTER   M ARINESCU     We extend the AOMDD to general weighted graphical models     We give a compilation algorithm based on AND OR search  that saves the trace of a memory intensive search and then reduces it in one bottom up pass     We present the APPLY operator that combines two AOMDDs and show that its complexity is at most quadratic in the input  but never worse than exponential in the treewidth     We give a scheduling order for building the AOMDD of a graphical model starting with the AOMDDs of its functions which is based on a Variable Elimination algorithm  This guarantees that the complexity is at most exponential in the induced width  treewidth  along the ordering     We show how AOMDDs relate to various earlier and recent compilation frameworks  providing a unifying perspective for all these methods     We introduce the semantic treewidth  which helps explain why compiled decision diagrams are often much smaller than the worst case bound     We provide an experimental evaluation of the new data structure  The structure of the paper is as follows  Section   provides preliminary definitions  a description of binary decision diagrams and the Bucket Elimination algorithm  Section   gives an overview of AND OR search spaces  Section   introduces the AOMDD and discusses its properties  Section   describes a search based algorithm for compiling the AOMDD  Section   presents a compilation algorithm based on a Bucket Elimination schedule and the APPLY operation  Section   proves that the AOMDD is a canonical representation for constraint networks given a pseudo tree  and Section   extends the AOMDD to weighted graphical models and proves their canonicity  Section   ties the canonicity to the new concept of semantic treewidth  Section    provides an experimental evaluation  Section    presents related work and Section    concludes the paper  All the proofs appear in an appendix      Preliminaries Notations A reasoning problem is defined in terms of a set of variables taking values from finite domains and a set of functions defined over these variables  We denote variables or subsets of variables by uppercase letters  e g   X  Y         and values of variables by lower case letters  e g   x  y          Sets are usually denoted by bold letters  for example X    X            Xn   is a set of variables  An assignment  X    x            Xn   xn   can be abbreviated as x    hX    x  i          hXn   xn i  or x    x            xn    For a subset of variables Y  DY denotes the Cartesian product of the domains of variables in Y  The projection of an assignment x    x            xn   over a subset Y is denoted by xY or x Y   We will also denote by Y   y  or y for short  the assignment of values to variables in Y from their respective domains  We denote functions by letters f   g  h etc   and the scope  set of arguments  of the function f by scope f        Graphical Models D EFINITION    graphical model  A graphical model M is a   tuple  M   hX  D  F  i  where        AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS     X    X            Xn   is a finite set of variables     D    D            Dn   is the set of their respective finite domains of values     F    f            fr   is a set of positive real valued discrete functions  i e   their domains can be listed   each defined over a subset of variables Si  X  called its scope  and denoted by scope fi    Q P     is a combination operator   e g               product  sum  join   that can take as input two  or more  real valued discrete functions  and produce another real valued discrete function  The graphical model represents the combination of all its functions  ri   fi   Several examples of graphical models appear later  for example  Figure   shows a constraint network and Figure   shows a belief network  In order to define the equivalence of graphical models  it is useful to introduce the notion of universal graphical model that is defined by a single function  D EFINITION    universal equivalent graphical model  Given a graphical model M   hX  D  F    i the universal equivalent model of M is u M    hX  D  F     fi F  fi    i  Two graphical models are equivalent if they represent the same function  Namely  if they have the same universal model  D EFINITION    weight of a full and a partial assignment  Given a graphical model M   hX  D  Fi  the weight of a full assignment x    x            xn   is defined by w x    f F f  x scope f      Given a subset of variables Y  X  the weight of a partial assignment y is the combination of all the functions whose scopes are included in Y  denoted by FY   evaluated at the assigned values  Namely  w y    f FY f  y scope f      Consistency For most graphical models  the range of the functions has a special zero value   that is absorbing relative to the combination operator  e g   multiplication   Combining anything with   yields a    The   value expresses the notion of inconsistent assignments  It is a primary concept in constraint networks but can also be defined relative to other graphical models that have a   element  D EFINITION    consistent partial assignment  solution  Given a graphical model having a   element  a partial assignment is consistent if its cost is non zero  A solution is a consistent assignment to all the variables  D EFINITION    primal graph  The primal graph of a graphical model is an undirected graph that has variables as its vertices and an edge connects any two variables that appear in the scope of the same function  The primal graph captures the structure of the knowledge expressed by the graphical model  In particular  graph separation indicates independency of sets of variables given some assignments to other variables  All of the advanced algorithms for graphical models exploit the graphical structure  by using a heuristically good elimination order  a tree decomposition or some similar method  We will use the concept of pseudo tree  which resembles the tree rearrangements introduced by Freuder and Quinn            The combination operator can also be defined axiomatically  Shenoy                M ATEESCU   D ECHTER   M ARINESCU  E  A  A  D  E  B  D F  B  G  F  C  G C   a  Graph coloring problem   b  Constraint graph  Figure    Constraint network D EFINITION    pseudo tree  A pseudo tree of a graph G    X  E  is a rooted tree T having the same set of nodes X  such that every arc in E is a backarc in T  A path in a rooted tree starts at the root and ends at one leaf  Two nodes can be connected by a backarc only if there exists a path that contains both   We use the common concepts and parameters from graph theory  that characterize the connectivity of the graph  and how close it is to a tree or to a chain  The induced width of a graphical model governs the complexity of solving it by Bucket Elimination  Dechter         and was also shown to bound the AND OR search graph when memory is used to cache solved subproblems  Dechter   Mateescu         D EFINITION    induced graph  induced width  treewidth  pathwidth  An ordered graph is a pair  G  d   where G     X            Xn    E  is an undirected graph  and d    X            Xn   is an ordering of the nodes  The width of a node in an ordered graph is the number of neighbors that precede it in the ordering  The width of an ordering d  denoted w d   is the maximum width over all nodes  The induced width of an ordered graph  w  d   is the width of the induced ordered graph obtained as follows  for each node  from last to first in d  its preceding neighbors are connected in a clique  The induced width of a graph  w   is the minimal induced width over all orderings  The induced width is also equal to the treewidth of a graph  The pathwidth pw of a graph is the treewidth over the restricted class of orderings that correspond to chain decompositions  Various reasoning tasks  or queries can be defined over graphical models  Those can be defined formally using marginalization operators such as projection  summation and minimization  However  since our goal is to present a compilation of a graphical model which is independent of the queries that can be posed on it  we will discuss tasks in an informal manner only  For more information see the work of Kask  Dechter  Larrosa  and Dechter         Throughout the paper  we will use two examples of graphical models  constraint networks and belief networks  In the case of constraint networks  the functions can be understood as relations  In other words  the functions  also called constraints  can take only two values          or  f alse  true   A   value indicates that the corresponding assignment to the variables is inconsistent  not allowed   and a   value indicates consistency  Belief networks are an example of the more general case of graphical models  also called weighted graphical models   The functions in this case are conditional probability tables  so the values of a function are real numbers in the interval                AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  Example   Figure   a  shows a graph coloring problem that can be modeled by a constraint network  Given a map of regions  the problem is to color each region by one of the given colors  red  green  blue   such that neighboring regions have different colors  The variables of the problems are the regions  and each one has the domain  red  green  blue   The constraints are the relation different between neighboring regions  Figure   b  shows the constraint graph  and a solution  A red  B blue  C green  D green  E blue  F blue  G red  is given in Figure   a   A more detailed example will be given later in Example    Propositional Satisfiability A special case of a CSP is propositional satisfiability  SAT   A formula  in conjunctive normal form  CNF  is a conjunction of clauses             t   where a clause is a disjunction of literals  propositions or their negations   For example      P  Q  R  is a clause  where P   Q and R are propositions  and P   Q and R are literals  The SAT problem is to decide whether a given CNF theory has a model  i e   a truth assignment to its propositions that does not violate any clause  Propositional satisfiability  SAT  can be defined as a CSP  where propositions correspond to variables  domains are         and constraints are represented by clauses  for example the clause  A  B  is a relation over its propositional variables that allows all tuples over  A  B  except  A      B       Cost Networks An immediate extension of constraint networks are cost networks where the set of functions are real valued cost functions  and the primary task is optimization  Also  GAI nets  generalized additive independence  Fishburn        can be used to represent utility functions  An example of cost functions will appear in Figure     D EFINITION P   cost network  combinatorial optimization  A cost network is a   tuple  hX  D  C  i  where X is a set of variables X    X            Xn    associated with a set of discrete valued domains  D    D            Dn    and a set of cost functions C    C            Cr    Each Ci is a real valued function defined on a subset of variables Si  X  The combination operator  is P   The reasoning problem is to find a minimum cost solution   Belief Networks  Pearl        provide a formalism for reasoning about partial beliefs under conditions of uncertainty  They are defined by a directed acyclic graph over vertices representing random variables of interest  e g   the temperature of a device  the gender of a patient  a feature of an object  the occurrence of an event   The arcs signify the existence of direct causal influences between linked variables quantified by conditional probabilities that are attached to each cluster of parentschild vertices in the network  Q D EFINITION    belief networks  A belief network  BN  is a graphical model P   hX  D  PG   i  where X    X            Xn   is a set of variables over domains D    D            Dn    Given a directed acyclic graph G over X as nodes  PG    P            Pn    where Pi    P  Xi   pa  Xi       are conditional probability tables  CPTs for short  associated with each Xi   where pa Xi   are the parents of Xi in the Qacyclic graph G  A belief network represents a probability distribution over X  P  x            xn     ni   P  xi  xpa Xi      An evidence set e is an instantiated subset of variables  When formulated as a graphical model  functions in F denote conditional probability tables and the scopes of these functions are determined by the directed acyclic graph G  each function Q fi ranges over variable Xi and its parents in G  The combination operator is product       The primal graph of a belief network  viewed as an undirected model  is called a moral graph  It connects any two variables appearing in the same CPT        M ATEESCU   D ECHTER   M ARINESCU  A Season  Sprinkler B  Watering D  A  C Rain  B  F Wetness  D  G Slippery  C  F  G   a  Directed acyclic graph   b  Moral graph  Figure    Belief network Example   Figure   a  gives an example of a belief network over   variables  and Figure   b  shows its moral graph   The example expresses the causal relationship between variables Season  A   The configuration of an automatic sprinkler system  B   The amount of rain expected  C   The amount of manual watering necessary  D   The wetness of the pavement  F   and Whether or not the pavement is slippery  G   The belief network expresses the probability distribution P  A  B  C  D  F  G    P  A   P  B A   P  C A   P  D B  A   P  F  C  B   P  G F    Another example of a belief network and CPTs appears in Figure    The two most popular tasks for belief networks are defined below  D EFINITION     belief updating  most probable explanation  MPE   Given a belief network and evidence e  the belief updating task is to compute the posterior marginal probability of variable Xi   conditioned on the evidence  Namely  X  Bel Xi   xi     P  Xi   xi   e      n Y  P  xk   e xpak       x       xi   xi        xn   E e Xi  xi   k    where  is a normalization constant  The most probable explanation  MPE  task is to find a complete assignment which agrees with the evidence  and which has the highest probability among all such assignments  Namely  to find an assignment  xo            xon   such that P  xo            xon     maxx       xn  n Y  P  xk   e xpak     k        Binary Decision Diagrams Review Decision diagrams are widely used in many areas of research to represent decision processes  In particular  they can be used to represent functions  Due to the fundamental importance of Boolean functions  a lot of effort has been dedicated to the study of Binary Decision Diagrams  BDDs   which are extensively used in software and hardware verification  Clarke et al         McMillan         The earliest work on BDDs is due to Lee         who introduced the binary decision program  that can be understood as a linear representation of a BDD  e g   a depth first search ordering of the nodes   where each node is a branching instruction indicating the address of the next instruction for both the   and the   value of the test variable  Akers        presented the actual graphical       AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  A                  B                  C                  f ABC                   A  A  B  C  C     C      a  Table     B  B        B        B  C         b  Unordered tree  C        C        C            c  Ordered tree  Figure    Boolean function representations representation and further developed the BDD idea  However  it was Bryant        that introduced what is now called the Ordered Binary Decision Diagram  OBDD   He restricted the order of variables along any path of the diagram  and presented algorithms  most importantly the apply procedure  that combines two OBDDs by an operation  that have time complexity at most quadratic in the sizes of the input diagrams  OBDDs are fundamental for applications with large binary functions  especially because in many practical cases they provide very compact representations  A BDD is a representation of a Boolean function  Given B           a Boolean function f   Bn  B  has n arguments  X         Xn   which are Boolean variables  and takes Boolean values  Example   Figure   a  shows a table representation of a Boolean function of three variables  This explicit representation is the most straightforward  but also the most costly due to its exponential requirements  The same function can also be represented by a binary tree  shown in Figure   b   that has the same exponential size in the number of variables  The internal round nodes represent the variables  the solid edges are the    or high  value  and the dotted edges are the    or low  value  The leaf square nodes show the value of the function for each assignment along a path  The tree shown in   b  is unordered  because variables do not appear in the same order along each path  In building an OBDD  the first condition is to have variables appear in the same order  A B C  along every path from root to leaves  Figure   c  shows an ordered binary tree for our function  Once an order is imposed  there are two reduction rules that transform a decision diagram into an equivalent one      isomorphism  merge nodes that have the same label and the same children      redundancy  eliminate nodes whose low and high edges point to the same node  and connect parent of removed node directly to child of removed node  Applying the two reduction rules exhaustively yields a reduced OBDD  sometimes denoted rOBDD  We will just use OBDD and assume that it is completely reduced  Example   Figure   a  shows the binary tree from Figure   c  after the isomorphic terminal nodes  leaves  have been merged  The highlighted nodes  labeled with C  are also isomorphic  and Figure   b  shows the result after they are merged  Now  the highlighted nodes labeled with C and B are redundant  and removing them gives the OBDD in Figure   c       Bucket Elimination Review Bucket Elimination  BE   Dechter        is a well known variable elimination algorithm for inference in graphical models  We will describe it using the terminology for constraint networks  but BE       M ATEESCU   D ECHTER   M ARINESCU  A  A  B  B  C  C  C        A  B  C  B  C  C      a  Isomorphic nodes  B  C         b  Redundant nodes      c  OBDD  Figure    Reduction rules A   A C  AC  C  AB  C  ABE   B  C  C  BCD   h  A   B   C  AB   E   C  ABE   A  h  AB   h  AB   AB bucket B AB  ABE  C  E  D   a  Constraint network  D   C  AC   h  BC   bucket A  A  bucket E  AB  ABC bucket C BC  BCD bucket D  C   BCD    b  BE execution   c  Bucket tree  Figure    Bucket Elimination can also be applied to any graphical model  Consider a constraint network R   hX  D  Ci and an ordering d    X    X            Xn    The ordering d dictates an elimination order for BE  from last to first  Each variable is associated with a bucket  Each constraint from C is placed in the bucket of its latest variable in d  Buckets are processed from Xn to X  by eliminating the bucket variable  the constraints residing in the bucket are joined together  and the bucket variable is projected out  and placing the resulting constraint  also called message  in the bucket of its latest variable in d  After its execution  BE renders the network backtrack free  and a solution can be produced by assigning variables along d  BE can also produce the solutions count if marginalization is done by summation  rather than projection  over the functional representation of the constraints  and join is substituted by multiplication  BE also constructs a bucket tree  by linking the bucket of each Xi to the destination bucket of its message  called the parent bucket   A node in the bucket tree typically has a bucket variable  a collection of constraints  and a scope  the union of the scopes of its constraints   If the nodes of the bucket tree are replaced by their respective bucket variables  it is easy to see that we obtain a pseudo tree  Example   Figure   a  shows a network with four constraints  Figure  b  shows the execution of Bucket Elimination along d    A  B  E  C  D   The buckets are processed from D to A   Figure   c  shows the bucket tree  The pseudo tree corresponding to the order d is given in Fig    a      The representation in Figure   reverses the top down bucket processing described in earlier papers  Dechter                AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  Procedure GeneratePseudoTree G  d             input   graph G    X  E   order d    X            Xn   output   Pseudo tree T Make X  the root of T Condition on X   eliminate X  and its incident edges from G   Let G            Gp be the resulting connected components of G for i     to p do Ti   GeneratePseudoTree  Gi   d Gi   Make root of Ti a child of X     return T      Orderings and Pseudo Trees Given an ordering d  the structural information captured in the primal graph through the scopes of the functions F    f            fr   can be used to create the unique pseudo tree that corresponds to d  Mateescu   Dechter         This is precisely the bucket tree  or elimination tree   that is created by BE  when variables are processed in reverse d   The same pseudo tree can be created by conditioning on the primal graph  and processing variables in the order d  as described in Procedure GeneratePseudoTree  In the following  d Gi is the restriction of the order d to the nodes of the graph Gi       Overview of AND OR Search Space for Graphical Models The AND OR search space is a recently introduced  Dechter   Mateescu      a      b        unifying framework for advanced algorithmic schemes for graphical models  Its main virtue consists in exploiting independencies between variables during search  which can provide exponential speedups over traditional search methods oblivious to problem structure  Since AND OR MDDs are based on AND OR search spaces we need to provide a comprehensive overview for the sake of completeness      AND OR Search Trees The AND OR search tree is guided by a pseudo tree of the primal graph  The idea is to exploit the problem decomposition into independent subproblems during search  Assigning a value to a variable  also known as conditioning   is equivalent in graph terms to removing that variable  and its incident edges  from the primal graph  A partial assignment can therefore lead to the decomposition of the residual primal graph into independent components  each of which can be searched  or solved  separately  The pseudo tree captures precisely all these decompositions given an order of variable instantiation  D EFINITION     AND OR search tree of a graphical model  Given a graphical model M   hX  D  Fi  its primal graph G and a pseudo tree T of G  the associated AND OR search tree has alternating levels of OR and AND nodes  The OR nodes are labeled Xi and correspond to variables  The AND nodes are labeled hXi   xi i  or simply xi   and correspond to value assignments  The structure of the AND OR search tree is based on T   The root is an OR node labeled with the root of T   The children of an OR node Xi are AND nodes labeled with assignments hXi   xi i that        M ATEESCU   D ECHTER   M ARINESCU  A  A  B     B  B    E  E     C  D    C       E        D       a  Pseudo tree         C  E        D  D                   C  E        D  D                 C       D  D  D                  b  Search tree  Figure    AND OR search tree are consistent with the assignments along the path from the root  The children of an AND node hXi   xi i are OR nodes labeled with the children of variable Xi in the pseudo tree T   Example   Figure   shows an example of an AND OR search tree for the graphical model given in Figure   a   assuming all tuples are consistent  and variables are binary valued  When some tuples are inconsistent  some of the paths in the tree do not exist  Figure   a  gives the pseudo tree that guides the search  from top to bottom  as indicated by the arrows  The dotted arcs are backarcs from the primal graph  Figure   b  shows the AND OR search tree  with the alternating levels of OR  circle  and AND  square  nodes  and having the structure indicated by the pseudo tree  The AND OR search tree can be traversed by a depth first search algorithm  thus using linear space  It was already shown  Freuder   Quinn        Bayardo   Miranker        Darwiche        Dechter   Mateescu      a        that  T HEOREM   Given a graphical model M over n variables  and a pseudo tree T of depth m  the size of the AND OR search tree based on T is O n k m    where k bounds the domains of variables  A graphical model of treewidth w has a pseudo tree of depth at most w log n  therefore it has an  AND OR search tree of size O n k w log n    The AND OR search tree expresses the set of all possible assignments to the problem variables  all solutions   The difference from the traditional OR search space is that a solution is no longer a path from root to a leaf  but rather a tree  defined as follows  D EFINITION     solution tree  A solution tree of an AND OR search tree contains the root node  For every OR node  it contains one of its child nodes and for each of its AND nodes it contains all its child nodes  and all its leaf nodes are consistent      AND OR Search Graph The AND OR search tree may contain nodes that root identical subproblems  These nodes are said to be unifiable  When unifiable nodes are merged  the search space becomes a graph  Its size becomes smaller at the expense of using additional memory by the search algorithm  The depth first search algorithm can therefore be modified to cache previously computed results  and retrieve them when the same nodes are encountered again  The notion of unifiable nodes is defined formally next         AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  D EFINITION     minimal AND OR graph  isomorphism  Two AND OR search graphs G and G  are isomorphic if there exists a one to one mapping  from the vertices of G to the vertices of G  such that for any vertex v  if  v    v     then v and v   root identical subgraphs relative to   An AND OR graph is called minimal if all its isomorphic subgraphs are merged  Isomorphic nodes  that root isomorphic subgraphs  are also said to be unifiable  It was shown by Dechter and Mateescu        that  T HEOREM   A graphical model M has a unique minimal AND OR search graph relative to a pseudo tree T   The minimal AND OR graph of a graphical model G relative to a pseudo tree T is denoted by MT  G   Note that the definition of minimality used in the work of Dechter and Mateescu        is based only on isomorphism reduction  We will extend it here by also including the elimination of redundant nodes  The previous theorem only shows that given an AND OR graph  the merge operator has a fixed point  which is the minimal AND OR graph  We will show in this paper that the AOMDD is a canonical representation  namely that any two equivalent graphical models can be represented by the same unique AOMDD given that they accept the same pseudo tree  and the AOMDD is minimal in terms of number of nodes  Some unifiable nodes can be identified based on their contexts  We can define graph based contexts for both OR nodes and AND nodes  just by expressing the set of ancestor variables in T that completely determine a conditioned subproblem  However  it can be shown that using caching based on OR contexts makes caching based on AND contexts redundant and vice versa  so we will only use OR caching  Any value assignment to the context of X separates the subproblem below X from the rest of the network  D EFINITION     OR context  Given a pseudo tree T of an AND OR search space  context X     X        Xp   is the set of ancestors of X in T   ordered descendingly  that are connected in the primal graph to X or to descendants of X  D EFINITION     context unifiable OR nodes  Given an AND OR search graph  two OR nodes n  and n  are context unifiable if they have the same variable label X and the assignments of their contexts is identical  Namely  if   is the partial assignment of variables along the path to n    and   is the partial assignment of variables along the path to n    then their restriction to the context of X is the same     context X       context X    The depth first search algorithm that traverses the AND OR search tree  can be modified to traverse a graph  if enough memory is available  We could allocate a cache table for each variable X  the scope of the table being context X   The size of the cache table for X is therefore the product of the domains of variables in its context  For each variable X  and for each possible assignment to its context  the corresponding conditioned subproblem is solved only once and the computed value is saved in the cache table  and whenever the same context assignment is encountered again  the value of the subproblem is retrieved from the cache table  Such an algorithm traverses what is called the context minimal AND OR graph  D EFINITION     context minimal AND OR graph  The context minimal AND OR graph is obtained from the AND OR search tree by merging all the context unifiable OR nodes        M ATEESCU   D ECHTER   M ARINESCU  R  F  G  B      C  A J  K   C   H   C   L   CK   A   CH   N   CKL   B   CHA   O   CKLN   P   CKO   H E C  D  L  E   CHAB   R   HAB   J   CHAE   F   AR   D   CEJ   G   AF   M   CD   K M N P O   a  Primal graph   b  Pseudo tree C        K  H  K              L  L  L  L  H        A     A     A  A                                                  N  N  N  N  N  N  N  N  B  B  B  B  B  B  B  B                                                  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O     E        E  E  E        E  E  E        E  E  E  E        E  E  E  E     E           R  R  R        R  R  R  R     R                                                                                      P  P  P  P  P  P  P                                                                                P J  J  J  J  J  J  J  J  J  J  J  J  J  J  J  J                                                                                                   D  D  D  D                   D  D  D  F  F  F  F                   D                   G  G  G  G                  M  M  M  M                    c  Context minimal graph  Figure    AND OR search graph It was already shown  Bayardo   Miranker        Dechter   Mateescu      a        that  T HEOREM   Given a graphical model M  its primal graph G and a pseudo tree T   the size of the context minimal AND OR search graph based on T   and therefore the size of its minimal AND OR  search graph  is O n k wT  G     where wT  G  is the induced width of G over the depth first traversal of T   and k bounds the domain size   Example   Lets look at the impact of caching on the size of the search space by examining a larger example  Figure   a  shows a graphical model with binary variables and Figure   b  a pseudo tree that drives the AND OR search  The context of each node is given in square brackets  The context minimal graph is given in Figure   c   Note that it is far smaller than the AND OR search tree  which has          AND nodes at the level of M alone  because M is at depth   in the pseudo tree   The shaded rectangles show the size of each cache table  equal to the number of OR nodes that appear in each one  A cache entry is useful whenever there are more than one incoming edges into the OR node  Incidentally  the caches that are not useful  namely OR nodes with only one incoming arc   are called dead caches  Darwiche         and can be determined based only on the pseudo       AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  tree inspection  therefore a cache table need not be allocated for them  The context minimal graph can also explain the execution of BE along the same pseudo tree  or  equivalently  along its depth first traversal order   The buckets are the shaded rectangles  and the processing is done bottom up  The number of possible assignments to each bucket equals the number of AND nodes that appear in it  The message scope is identical to the context of the bucket variable  and the message itself is identical to the corresponding cache table  For more details on the relationship between AND OR search and BE see the work of Mateescu and Dechter             Weighted AND OR Graphs In the previous subsections we described the structure of the AND OR trees and graphs  In order to use them to solve a reasoning task  we need to define a way of using the input function values during the traversal of an AND OR graph  This is realized by placing weights  or costs  on the OR to AND arcs  dictated by the function values  Only the functions that are relevant contribute to an OR to AND arc weight  and this is captured by the buckets relative to the pseudo tree  D EFINITION     buckets relative to a pseudo tree  Given a graphical model M   hX  D  F  i and a pseudo tree T   the bucket of Xi relative to T   denoted BT  Xi    is the set of functions whose scopes contain Xi and are included in pathT  Xi    which is the set of variables from the root to Xi in T   Namely  BT  Xi      f  F Xi  scope f    scope f    pathT  Xi      A function belongs to the bucket of a variable Xi iff its scope has just been fully instantiated when Xi was assigned  Combining the values of all functions in the bucket  for the current assignment  gives the weight of the OR to AND arc  D EFINITION     OR to AND weights  Given an AND OR graph of a graphical model M  the weight w n m   Xi   xi   of arc  n  m  where Xi labels n and xi labels m  is the combination of all the functions in BT  Xi   assigned by values along the current path to the AND node m  m   Formally  w n m   Xi   xi     f BT  Xi   f  asgn m   scope f      D EFINITION     weight of a solution tree  Given a weighted AND OR graph of a graphical model M  and given a solution tree t having the OR to AND set of arcs arcs t   the weight of t is defined by w t    earcs t  w e   Example   We start with the more straightforward case of constraint networks  Since functions only take values   or    and the combination is by product  join of relations   it follows that any ORto AND arc can only have a weight of   or    An example is given in Figure    Figure   a  shows a constraint graph    b  a pseudo tree for it  and   c  the four relations that define the constraint problem  Figure   d  shows the AND OR tree that can be traversed by a depth first search algorithm that only checks the consistency of the input functions  i e   no constraint propagation is used   Similar to the OBDD representation  the OR to AND arcs with a weight of   are denoted by dotted lines  and the tree is not unfolded below them  since it will not contain any solution  The arcs with a weight of   are drawn with solid lines        M ATEESCU   D ECHTER   M ARINESCU  A B C  A  D  F  B  E   a  Constraint graph A                  B                  B                  C RABC                                  C                  C  E  D  F   b  Pseudo tree  D RBCD                                  A                  B                  E RABE                                  A                  E                  F RAEF                                   c  Relations A              B  B               C  C  E         C  E  C  E  E                                                                                                  D  D  D  F                                         F  D  F                                    D  D  F  F                                                      d  AND OR tree  Figure    AND OR search tree for constraint networks Example   Figure   shows a weighted AND OR tree for a belief network  Figure   a  shows the directed acyclic graph  and the dotted arc BC added by moralization  Figure   b  shows the pseudo tree  and   c  shows the conditional probability tables  Figure   d  shows the weighted AND OR tree  As we did for constraint networks  we can move from weighted AND OR search trees to weighted AND OR search graphs by merging unifiable nodes  In this case the arc labels should be also considered when determining unifiable subgraphs  This can yield context minimal weighted AND OR search graphs and minimal weighted AND OR search graphs      AND OR Multi Valued Decision Diagrams  AOMDDs  In this section we begin describing the contributions of this paper  The context minimal AND OR graph  Definition     offers an effective way of identifying some unifiable nodes during the execution of the search algorithm  Namely  context unifiable nodes are discovered based only on their paths from the root  without actually solving their corresponding subproblems  However  merging based on context is not complete  which means that there may still exist unifiable nodes in the search graph that do not have identical contexts  Moreover  some of the nodes in the context       AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  P A   A  A      A  P B   A   P A         A      P C   A  A      B          B          P D   B C   B B  B          C  E E  C D  D   a  Belief network  C          P E   A B  D                D                C          C           b  Pseudo tree  A          B          E                E                 c  CPTs  A                B  B           C        E                       D           D                                   D              C     E                       D  D              E  C                E                                  C                       D        D                    D                     d  Weighted AND OR tree  Figure    Weighted AND OR search tree for belief networks minimal AND OR graph may be redundant  for example when the set of solutions rooted at variable Xi is not dependant on the specific value assigned to Xi  this situation is not detectable based on context   This is sometimes termed as interchangeable values or symmetrical values  As overviewed earlier  Dechter and Mateescu            a  defined the complete minimal AND OR graph which is an AND OR graph whose unifiable nodes are all merged  and Dechter and Mateescu        also proved the canonicity for non weighted graphical models  In this paper we propose to augment the minimal AND OR search graph with removing redundant variables as is common in OBDD representation as well as adopt notational conventions common in this community  This yields a data structure that we call AND OR BDD  that exploits decomposition by using AND nodes  We present the extension over multi valued variables yielding AND OR MDD or AOMDD and define them for general weighted graphical models  Subsequently we present two algorithms for compiling the canonical AOMDD of a graphical model  the first is search based  and uses the memory intensive AND OR graph search to generate the context minimal AND OR graph  and then reduces it bottom up by applying reduction rules  the second is inferencebased  and uses a Bucket Elimination schedule to combine the AOMDDs of initial functions by APPLY operations  similar to the apply for OBDDs   As we will show  both approaches have the same worst case complexity as the AND OR graph search with context based caching  and also the same complexity as Bucket Elimination  namely time and space exponential in the treewidth of the  problem  O n k w    The benefit of each of these generation schemes will be discussed         M ATEESCU   D ECHTER   M ARINESCU  A  A      a  OBDD          k   b  MDD  Figure     Decision diagram nodes  OR  A  A                     a  AOBDD     k                  b  AOMDD  Figure     Decision diagram nodes  AND OR      From AND OR Search Graphs to Decision Diagrams An AND OR search graph G of a graphical model M   hX  D  F  i represents the set of all possible assignments to the problem variables  all solutions and their costs   In this sense  G can be viewed as representing the function f   fi F fi that defines the universal equivalent graphical model u M   Definition     For each full assignment x    x            xn    if x is a solution expressed by the tree tx   then f  x    w tx     earcs tx   w e   Definition      otherwise f  x       the assignment is inconsistent   The solution tree tx of a consistent assignment x can be read from G in linear time by following the assignments from the root  If x is inconsistent  then a dead end is encountered in G when attempting to read the solution tree tx   and f  x       Therefore  G can be viewed as a decision diagram that determines the values of f for every complete assignment x  We will now see how we can process an AND OR search graph by reduction rules similar to the case of OBDDs  in order to obtain a representation of minimal size  In the case of OBDDs  a node is labeled with a variable name  for example A  and the low  dotted line  and high  solid line  outgoing arcs capture the restriction of the function to the assignments A     or A      To determine the value of the function  one needs to follow either one or the other  but not both  of the outgoing arcs from A  see Figure    a    The straightforward extension of OBDDs to multi valued variables  multi valued decision diagrams  or MDDs  was presented by Srinivasan  Kam  Malik  and Brayton         and the node structure that they use is given in Figure    b   Each outgoing arc is associated with one of the k values of variable A  In this paper we generalize the OBDD and MDD representations demonstrated in Figures    a  and    b  by allowing each outgoing arc to be an AND arc  An AND arc connects a node to a set of nodes  and captures the decomposition of the problem into independent components  The number of AND arcs emanating from a node is two in the case of AOBDDs  Figure    a    or the domain size of the variable in the general case  Figure    b    For a given node A  each of its k AND arcs can connect it to possibly different number of nodes  depending on how the problem decomposes based on each particular assignment of A  The AND arcs are depicted by a shaded sector that connects the outgoing lines corresponding to the independent components         AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  A                    k             a  Nonterminal meta node         b  Terminal meta node     c  Terminal meta node    Figure     Meta nodes We define the AND OR Decision Diagram representation based on AND OR search graphs  We find that it is useful to maintain the semantics of Figure    especially when we need to express the redundancy of nodes  and therefore we introduce the meta node data structure  which defines small portions of any AND OR graph  based on an OR node and its AND children  D EFINITION     meta node  A meta node u in an AND OR search graph can be either      a terminal node labeled with   or    or     a nonterminal node  that consists of an OR node labeled X  therefore var u    X  and its k AND children labeled x            xk that correspond to the value assignments of X  Each AND node labeled xi stores a list of pointers to child meta nodes  denoted by u childreni   In the case of weighted graphical models  the AND node xi also stores the OR toAND arc weight w X  xi    The rectangle in Figure    a  is a meta node for variable A  that has a domain of size k  Note that this is very similar to Figure     with the small difference that the information about the value of A that corresponds to each outgoing AND arc is now stored in the AND nodes of the meta node  We are not showing the weights in that figure  A larger example of an AND OR graph with meta nodes appears later in Figure     The terminal meta nodes play the role of the terminal nodes in OBDDs  The terminal metanode    shown in Figure    b   indicates inconsistent assignments  while the terminal meta node    shown in figure    c  indicates consistent ones  Any AND OR search graph can now be viewed as a diagram of meta nodes  simply by grouping OR nodes with their AND children  and adding the terminal meta nodes appropriately  Once we have defined the meta nodes  it is easier to see when a variable is redundant with respect to the outcome of the function based on the current partial assignment  A variable is redundant if any of its assignments leads to the same set of solutions  D EFINITION     redundant meta node  Given a weighted AND OR search graph G represented with meta nodes  a meta node u with var u    X and  D X     k is redundant iff   a  u children            u childrenk and  b  w X  x              w X  xk    An AND OR graph G  that contains a redundant meta node u  can be transformed into an equivalent graph G   by replacing any incoming arc into u with its common list of children u children    absorbing the common weight w X  x    by combination into the weight of the parent meta node corresponding to the incoming arc  and then removing u and its outgoing arcs from G  The value X   x  is picked here arbitrarily  because they are all isomorphic  If u is the root of the       M ATEESCU   D ECHTER   M ARINESCU  Procedure RedundancyReduction   AND OR graph G  redundant meta node u  with var u    X  List of meta node parents of u  denoted by P arents u   output   Reduced AND OR graph G after the elimination of u    if P arents u  is empty then   return independent AND OR graphs rooted by meta nodes in u children    and constant w X  x    input    forall v  P arents u   assume var v     Y   do   forall i               D Y     do   if u  v childreni then   v childreni  v childreni    u    v childreni  v childreni  u children    w Y  yi    w Y  yi    w X  x      remove u    return reduced AND OR graph G  Procedure IsomorphismReduction   AND OR graph G  isomorphic meta nodes u and v  List of meta node parents of u  denoted by P arents u   output   Reduced AND OR graph G after the merging of u and v  forall p  P arents u  do if u  p childreni then p childreni  p childreni    u  p childreni  p childreni   v   input             remove u   return reduced AND OR graph G  graph  then the common weight w X  x    has to be stored separately as a constant  Procedure RedundancyReduction formalizes the redundancy elimination  D EFINITION     isomorphic meta nodes  Given a weighted AND OR search graph G represented with meta nodes  two meta nodes u and v having var u    var v    X and  D X     k are isomorphic iff   a  u childreni   v childreni i              k  and  b  wu  X  xi     wv  X  xi   i              k    where wu   wv are the weights of u and v   Procedure IsomorphismReduction formalizes the process of merging isomorphic metanodes  Naturally  the AND OR graph obtained by merging isomorphic meta nodes is equivalent to the original one  We can now define the AND OR Multi Valued Decision Diagram  D EFINITION     AOMDD  An AND OR Multi Valued Decision Diagram  AOMDD  is a weighted AND OR search graph that is completely reduced by isomorphic merging and redundancy removal  namely      it contains no isomorphic meta nodes  and     it contains no redundant meta nodes         AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  A  A      k     B  c     d        k  z            z  k       y  c       d  y   b  After eliminating the B meta node   a  Fragment of an AOMDD  Figure     Redundancy reduction A    A      k           k  B    C     d        C         B     k     e             k  C      k         k     y  d   a  Fragment of an AOMDD     k  e       y   b  After merging the isomorphic C meta nodes  Figure     Isomorphism reduction Example    Figure    shows an example of applying the redundancy reduction rule to a portion of an AOMDD  On the left side  in Figure    a   the meta node of variable B is redundant  we dont show the weights of the OR to AND arcs  to avoid cluttering the figure   Any of the values             k  of B will lead to the same set of meta nodes  c  d          y   which are coupled in an AND arc  Therefore  the meta node of B can be eliminated  The result is shown in Figure    b   where the meta nodes  c  d          y  and z are coupled in an AND arc outgoing from A      In Figure    we show an example of applying the isomorphism reduction rule  In this case  the meta nodes labeled with C in Figure    a  are isomorphic  again  we omit the weights   The result of merging them is shown in Figure    b   Examples of AOMDDs appear in Figures        and     Note that if the weight on an OR toAND arc is zero  then the descendant is the terminal meta node    Namely  the current path is a dead end  cannot be extended to a solution  and is therefore linked directly to        Using AND OR Search to Generate AOMDDs In Section     we described how we can transform an AND OR graph into an AOMDD by applying reduction rules  In Section     we describe the explicit algorithm that takes as input a graphi      M ATEESCU   D ECHTER   M ARINESCU  cal model  performs AND OR search with context based caching to obtain the context minimal AND OR graph  and in Section     we give the procedure that applies the reduction rules bottom up to obtain the AOMDD      Algorithm AND OR S EARCH  AOMDD Algorithm    called AND OR S EARCH  AOMDD  compiles a graphical model into an AOMDD  A memory intensive  with context based caching  AND OR search is used to create the context minimal AND OR graph  see Definition      The input to AND OR S EARCH  AOMDD is a graphical model M and a pseudo tree T   that also defines the OR context of each variable  Each variable Xi has an associated cache table  whose scope is the context of Xi in T   This ensures that the trace of the search is the context minimal AND OR graph  A list denoted by LXi  see line      is used for each variable Xi to save pointers to meta nodes labeled with Xi   These lists are used by the procedure that performs the bottom up reduction  per layers of the AND OR graph  one layer contains all the nodes labeled with one given variable   The fringe of the search is maintained on a stack called OPEN  The current node  either OR or AND node  is denoted by n  its parent by p  and the current path by n   The children of the current node are denoted by successors n   For each node n  the Boolean attribute consistent n  indicates if the current path can be extended to a solution  This information is useful for pruning the search space  The algorithm is based on two mutually recursive steps  Forward  beginning at line    and Backtrack  beginning at line      which call each other  or themselves  until the search terminates  In the forward phase  the AND OR graph is expanded top down  The two types of nodes  AND and OR  are treated differently according to their semantics  Before an OR node is expanded  the cache table of its variable is checked  line     If the entry is not null  a link is created to the already existing OR node that roots the graph equivalent to the current subproblem  Otherwise  the OR node is expanded by generating its AND descendants  The OR to AND weight  see Definition     is computed in line     Each value xi of Xi is checked for consistency  line      The least expensive check is to verify that the OR to AND weight is non zero  However  the deterministic  inconsistent  assignments in M can be extracted to form a constraint network  Any level of constraint propagation can be performed in this step  e g   look ahead  arc consistency  path consistency  i consistency etc    The computational overhead can increase  in the hope of pruning the search space more aggressively  We should note that constraint propagation is not crucial for the algorithm  and the complexity guarantees are maintained even if only the simple weight check is performed  The consistent AND nodes are added to the list of successors of n  line      while the inconsistent ones are linked to the terminal   meta node  line      An AND node n labeled with hXi   xi i is expanded  line     based on the structure of the pseudo tree  If Xi is a leaf in T   then n is linked to the terminal   meta node  line      Otherwise  an OR node is created for each child of Xi in T  line      The forward step continues as long as the current node is not a dead end and still has unevaluated successors  The backtrack phase is triggered when a node has an empty set of successors  line      Note that  as each successor is processed  it is removed from the set of successors in line     When the backtrack reaches the root  line      the search is complete  the context minimal AND OR graph is generated  and the Procedure B OTTOM U P R EDUCTION is called  When the backtrack step processes an OR node  line      it saves a pointer to it in cache  and also adds a pointer to the corresponding meta node to the list LXi   The consistent attribute of        AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  Algorithm    AND OR S EARCH   AOMDD input   M   hX  D  Fi  pseudo tree T rooted at X    parents pai  OR context  for every variable Xi   output   AOMDD of M    forall Xi  X do Initialize context based cache table CacheXi  pai   with null entries     Create new OR node t  labeled with Xi   consistent t   true  push t on top of OPEN   while OPEN     do   n  top OPEN   remove n from OPEN    Forward   successors n      if n is an OR node labeled with Xi then    OR expand if CacheXi  asgn n   pai       null then   Connect parent of n to CacheXi  asgn n   pai         Use the cached pointer                                else forall xi  Di do Create new AN D node t  labeled with hXi   xi i w X  xi     f  asgn n   pai    f BT  Xi    if hXi   xi i is consistent with n then consistent t   true add t to successors n  else consistent t   f alse make terminal   the only child of t                           if n is an AND node labeled with hXi   xi i then if childrenT  Xi       then make terminal   the only child of n else forall Y  childrenT  Xi   do Create new OR node t  labeled with Y consistent t   f alse add t to successors n                      Add successors n  to top of OPEN while successors n      do let p be the parent of n if n is an OR node labeled with Xi then if Xi    X  then Call BottomUpReduction procedure                        Constraint Propagation     AND expand     Backtrack     Search is complete    begin reduction to AOMDD  Cache asgn n   pai     n Add meta node of n to the list LXi consistent p   consistent p   consistent n  if consistent p     f alse then remove successors p  from OPEN successors p            if n is an AND node labeled with hXi   xi i then consistent p   consistent p   consistent n           remove n from successors p  np          Save in cache     Check if p is dead end   M ATEESCU   D ECHTER   M ARINESCU  Procedure BottomUpReduction   A graphical model M   hX  D  Fi  a pseudo tree T of the primal graph  rooted at X    Context minimal AND OR graph  and lists LXi of meta nodes for each level Xi   output   AOMDD of M  Let d    X            Xn   be the depth first traversal ordering of T for i  n down to   do Let H be a hash table  initially empty forall meta nodes n in LXi do if H Xi   n children            n childrenki   wn  Xi   x             wn  Xki   xki    returns a meta node p then merge n with p in the AND OR graph input                              else if n is redundant then eliminate n from the AND OR graph combine its weight with that of the parent else hash n into the table H  H Xi   n children            n childrenki   wn  Xi   x             wn  Xki   xki     n     return reduced AND OR graph  the AND parent p is updated by conjunction with consistent n   If the AND parent p becomes inconsistent  it is not necessary to check its remaining OR successors  line      When the backtrack step processes an AND node  line      the consistent attribute of the OR parent p is updated by disjunction with consistent n   The AND OR search algorithm usually maintains a value for each node  corresponding to a task that is solved  We did not include values in our description because an AOMDD is just an equivalent representation of the original graphical model M  Any task over M can be solved by a traversal of the AOMDD  It is however up to the user to include more information in the meta nodes  e g   number of solutions for a subproblem       Reducing the Context Minimal AND OR Graph to an AOMDD Procedure BottomUpReduction processes the variables bottom up relative to the pseudo tree T   We use the depth first traversal ordering of T  line     but any other bottom up ordering is as good  The outer for loop  starting at line    goes through each level of the context minimal AND OR graph  where a level contains all the OR and AND nodes labeled with the same variable  in other words it contains all the meta nodes of that variable   For efficiency  and to ensure the complexity guarantees that we will prove  a hash table  initially empty  is used for each level  The inner for loop  starting at line    goes through all the metanodes of a level  that are also saved  or pointers to them are saved  in the list LXi   For each new meta node n in the list LXi   in line   the hash table H is checked to verify if a node isomorphic with n already exists  If the hash table H already contains a node p corresponding to the hash key  Xi   n children            n childrenki   wn  Xi   x             wn  Xki   xki     then p and n are isomorphic and should be merged  Otherwise  if the new meta node n is redundant  then it is eliminated from the AND OR graph  If none of the previous two conditions is met  then the new meta node n is hashed into table H         AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  A D  G  C  B  F  E  A  H  B C D   a   F E  G  H   b   Figure      a  Constraint graph for C    C            C     where C    F  H  C    A  H  C    A  B  G  C    F  G  C    B  F   C    A  E  C    C  E  C    C  D  C    B  C   b  Pseudo tree  bucket tree  for ordering d    A  B  C  D  E  F  G  H   Proposition   The output of Procedure BottomUpReduction is the AOMDD of M along the pseudo tree T   namely the resulting AND OR graph is completely reduced  Note that we explicated Procedure BottomUpReduction separately only for clarity  In practice  it can actually be included in Algorithm AND OR S EARCH  AOMDD  and the reduction rules can be applied whenever the search backtracks  We can maintain a hash table for each variable  during the AND OR search  to store pointers to meta nodes  When the search backtracks out of an OR node  it can already check the redundancy of that meta node  and also look up in the hash table to check for isomorphism  Therefore  the reduction of the AND OR graph can be done during the AND OR search  and the output will be the AOMDD of M  From Theorem   and Proposition   we can conclude  T HEOREM   Given a graphical model M and a pseudo tree T of its primal graph G  the AOMDD  of M corresponding to T has size bounded by O n k wT  G    and it can be computed by Algorithm  AND OR S EARCH  AOMDD in time O n k wT  G     where wT  G  is the induced width of G over the depth first traversal of T   and k bounds the domain size      Using Bucket Elimination to Generate AOMDDs In this section we propose to use a Bucket Elimination  BE  type algorithm to guide the compilation of a graphical model into an AOMDD  The idea is to express the graphical model functions as AOMDDs  and then combine them with APPLY operations based on a BE schedule  The APPLY is very similar to that from OBDDs  Bryant         but it is adapted to AND OR search graphs  It takes as input two functions represented as AOMDDs based on the same pseudo tree  and outputs the combination of initial functions  also represented as an AOMDD based on the same pseudo tree  We will describe it in detail in Section      We will start with an example based on constraint networks  This is easier to understand because the weights on the arcs are all   or    and therefore are depicted in the figures by solid and dashed lines  respectively  Example    Consider the network defined by X    A  B          H   DA           DH          and the constraints  where  denotes XOR   C    F H  C    AH  C    ABG  C    F G        M ATEESCU   D ECHTER   M ARINESCU  m   A  m  A  A          B          C    C                   C       A     B  B        F  C    B          F          F    B  F              B  C D    D  G  E                      m   G       H             D  E     m   F  H    G  m   m   A       B  C     C    A           D          C  C          F  B           A           G                    H       B  F    G       F  F     F       A  B    B    E  D       B  A     H     H       F    C                m   C   C     E     C  D             E  D     C                   G              C  A       C   C                           B  H          A       H     G  E  C         A     G    G  F  F    B  G  H  m   m      B       C   m   A  A     E  E        C   m   A    D  D  D  m   m            m   C      H       F     F            G  C   C   H  Figure     Execution of BE with AOMDDs A    A     B    B       B     B  C C    C       C       C       F       F       F       C        D     D  E  D    D       E       G       G       H       C  C  F     D  D  E  F  H    D  E  F  F  F     G  G  G  G  H     D     H      a   G      b   Figure      a  The final AOMDD   b  The OBDD corresponding to d C    B  F   C    A  E  C    C  E  C    C  D  C    B  C  The constraint graph is shown in Figure    a   Consider the ordering d    A  B  C  D  E  F  G  H   The pseudo tree  or bucket tree  induced by d is given in Fig     b   Figure    shows the execution of BE with AOMDDs along ordering d  Initially  the constraints C  through C  are represented as AOMDDs and placed in the bucket of their latest variable in d  The scope of any original constraint always appears on a        AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  Algorithm    BE AOMDD   Graphical model M   hX  D  Fi  where X    X            Xn    F    f            fr     order d    X            Xn   output   AOMDD representing iF fi   T   GeneratePseudoTree G  d     for i    to r do    place functions in buckets   place Gfaomdd in the bucket of its latest variable in d i input    for i  n down to   do message Xi    G aomdd     while bucket Xi       do   pick Gfaomdd from bucket Xi               process buckets    initialize with AOMDD of        combine AOMDDs in bucket of Xi  bucket Xi    bucket Xi      Gfaomdd    message Xi    APPLY message Xi    Gfaomdd   add message Xi   to the bucket of the parent of Xi in T     return message X     path from root to a leaf in the pseudo tree  Therefore  each original constraint is represented by an AOMDD based on a chain  i e   there is no branching into independent components at any point   The chain is just the scope of the constraint  ordered according to d  For bi valued variables  the original constraints are represented by OBDDs  for multiple valued variables they are MDDs  Note that we depict meta nodes  one OR node and its two AND children  that appear inside each gray node  The dotted edge corresponds to the   value  the low edge in OBDDs   the solid edge to the   value  the high edge   We have some redundancy in our notation  keeping both AND value nodes and arc types  dotted arcs from   and solid arcs from     The BE scheduling is used to process the buckets in reverse order of d  A bucket is processed by joining all the AOMDDs inside it  using the APPLY operator  However  the step of elimination of the bucket variable is omitted because we want to generate the full AOMDD  In our example  the messages m    C     C  and m    C     C  are still based on chains  therefore they are OBDDs  Note that they contain the variables H and G  which have not been eliminated  However  the message m    C     m     m  is not an OBDD anymore  We can see that it follows the structure of the pseudo tree  where F has two children  G and H  Some of the nodes corresponding to F have two outgoing edges for value    The processing continues in the same manner  The final output of the algorithm  which coincides with m    is shown in Figure    a   The OBDD based on the same ordering d is shown in Fig     b   Notice that the AOMDD has    nonterminal nodes and    edges  while the OBDD has    nonterminal nodes and    edges      Algorithm BE AOMDD Algorithm    called BE AOMDD  creates the AOMDD of a graphical model by using a BE schedule for APPLY operations  Given an order d of the variables  first a pseudo tree is created based on   the primal graph  Each initial function fi is then represented as an AOMDD  denoted by Gfaomdd i and placed in its bucket  To obtain the AOMDD of a function  the scope of the function is ordered according to d  a search tree  based on a chain  that represents fi is generated  and then reduced by Procedure BottomUpReduction  The algorithm proceeds exactly like BE  with the only difference that the combination of functions is realized by the APPLY algorithm  and variables are not       M ATEESCU   D ECHTER   M ARINESCU  eliminated but carried over to the destination bucket  The messages between buckets are initialized with the dummy AOMDD of    denoted by G aomdd   which is neutral for combination  In order to create the compilation of a graphical model based on AND OR graphs  it is necessary to traverse the AND OR graph top down and bottom up  This is similar to the inward and outward message passing in a tree decomposition  Note that BE AOMDD describes the bottom up traversal explicitly  while the top down phase is actually performed by the APPLY operation  When two AOMDDs are combined  after the top chain portion of their pseudo tree is processed  the remaining independent branches are attached only if they participate in the newly restricted set of solutions  This amounts to an exchange of information between the independent branches  which is equivalent to the top down phase      The AOMDD APPLY Operation We will now describe how to combine two AOMDDs  The APPLY operator takes as input two AOMDDs representing functions f  and f  and returns an AOMDD representing f   f    In OBDDs the apply operator combines two input diagrams based on the same variable ordering  Likewise  in order to combine two AOMDDs we assume that their pseudo trees are identical  This condition is satisfied by any two AOMDDs in the same bucket of BE AOMDD  However  we present here a version of APPLY that is more general  by relaxing the previous condition from identical to compatible pseudo trees  Namely  there should be a pseudo tree in which both can be embedded  In general  a pseudo tree induces a strict partial order between the variables where a parent node always precedes its child nodes  D EFINITION     compatible pseudo trees  A strict partial order d     X       over a set X is consistent with a strict partial order d     Y       over a set Y  if for all x    x   X  Y  if x     x  then x     x    Two partial orders d  and d  are compatible iff there exists a partial order d that is consistent with both  Two pseudo trees are compatible iff the partial orders induced via the parent child relationship  are compatible  For simplicity  we focus on a more restricted notion of compatibility  which is sufficient when using a BE like schedule for the APPLY operator to combine the input AOMDDs  as described in Section     The APPLY algorithm that we will present can be extended to the more general notion of compatibility  D EFINITION     strictly compatible pseudo trees  A pseudo tree T  having the set of nodes X  can be embedded in a pseudo tree T having the set of nodes X if X   X and T  can be obtained from T by deleting each node in X   X  and connecting its parent to each of its descendents  Two pseudo trees T  and T  are strictly compatible if there exists T such that both T  and T  can be embedded in T   Algorithm APPLY  algorithm    takes as input one node from Gfaomdd and a list of nodes from Ggaomdd   Initially  the node from Gfaomdd is its root node  and the list of nodes from Ggaomdd is in fact also made of just one node  which is its root  We will sometimes identify an AOMDD by its root node  The pseudo trees Tf and Tg are strictly compatible  having a target pseudo tree T   The list of nodes from Ggaomdd always has a special property  there is no node in it that can be the ancestor in T of another  we refer to the variable of the meta node   Therefore  the list z            zm       AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  APPLY  v    z            zm   input   AOMDDs Gfaomdd with nodes vi and Ggaomdd with nodes zj   based on strictly compatible pseudo trees Tf   Tg that can be embedded in T   var v    is an ancestor of all var z             var zm   in T   var zi   and var zj   are not in ancestor descendant relation in T   i    j  output   v    z          zm    based on T   if H   v    z            zm      null then return H   v    z            zm       is in cache if  any of v    z            zm is    then return   if  v       then return   if  m      then return v     nothing to combine create new nonterminal meta node u var u   var v     call it Xi   with domain Di    x            xki     for j    to ki do u childrenj      children of the j th AND node of u    assign weight from v  wu  Xi   xj    wv   Xi   xj   if    m      and  var v      var z      Xi     then temp Children  z   childrenj    combine input weights wu  Xi   xj    wv   Xi   xj    wz   Xi   xj    Algorithm                                       else                  group nodes from v   childrenj  temp Children in several  v     z             z r   for each  v     z             z r   do y  APPLY v     z             z r   if  y      then u childrenj     break         temp Children   z            zm    else u childrenj  u childrenj   y             if  u children            u childrenki   and  wu  Xi   x              wu  Xi   xki    then promote wu  Xi   x    to parent return u children     redundancy         if  H   Xi   u children            u childrenki   wu  Xi   x             wu  Xki   xki       null  then return H   Xi   u children            u childrenki   wu  Xi   x             wu  Xki   xki       isomorphism     Let H   v    z            zm     u    Let H   Xi   u children            u childrenki   w u  Xi   x             w u  Xki   xki      u    return u     add u to H     add u to H   from g expresses a decomposition with respect to T   so all those nodes appear on different branches  We will employ the usual techniques from OBDDs to make the operation efficient  First  if one of the arguments is    then we can safely return    Second  a hash table H  is used to store the nodes that have already been processed  based on the nodes  v    z            zr    Therefore  we never need to make multiple recursive calls on the same arguments  Third  a hash table H  is used to detect isomorphic nodes  This is typically split in separate tables for each variable  If at the end of the recursion  before returning a value  we discover that a meta node with the same variable  the same children and the same weights has already been created  then we dont need to store it and we simply return the existing node  And fourth  if at the end of the recursion we discover that we created a redundant node  all the children are the same and all the weights are the same   then we dont store it  and return instead one of its identical lists of children  and promote the common weight        M ATEESCU   D ECHTER   M ARINESCU  A                  B                  C                  f ABC                   A                  A B C  A  A        C  A         A   B   A   B      B  B  D  D     D    D  C  A          A  A B  B     B   A     B          g ABC                   A          D                  B     B  A   B                     B      B  A B   B         C    B      A B      A         D  B        B       D  B              Figure     Example of APPLY operation Note that v  is always an ancestor of all z            zm in T   We consider a variable in T to be an ancestor of itself  A few self explaining checks are performed in lines      Line   is specific for multiplication  and needs to be changed for other combination operations  The algorithm creates a new meta node u  whose variable is var v      Xi  recall that var v    is highest  closest to root  in T among v    z            zm   Then  for each possible value of Xi   line    it starts building its list of children  One of the important steps happens in line     There are two lists of meta nodes  one from each original AOMDD f and g  and we will refer only to their variables  as they appear in T   Each of these lists has the important property mentioned above  that its nodes are not ancestors of each other  The union of the two lists is grouped into maximal sets of nodes  such that the highest node in each set is an ancestor of all the others  It follows that the root node in each set belongs to one of the original AOMDD  say v   is from f   and the others  say z             z r are from g  As an example  suppose T is the pseudo tree from Fig     b   and the two lists are  C  G  H  from f and  E  F   from g  The grouping from line    will create  C  E  and  F   G  H   Sometimes  it may be the case that a newly created group contains only one node  This means there is nothing more to join in recursive calls  so the algorithm will return  via line    the single node  From there on  only one of the input AOMDDs is traversed  and this is important for the complexity of APPLY  discussed below  Example    Figure    shows the result of combining two Boolean functions by an AND operation  or product   The input functions f and g are represented by AOMDDs based on chain pseudo trees  while the results is based on the pseudo tree that expresses the decomposition after variables A and B are instantiated  The APPLY operator performs a depth first traversal of the two input AOMDDs  and generates the resulting AOMDD based on the output pseudo tree  Similar to the case of OBDDs  a function or an AOMDD can be identified by its root meta node  In this example the input meta nodes have labels  A    A    B    B    etc    The output meta node labeled by A  B  is       AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  the root of a diagram that represents the function obtained by combining the functions rooted by A  and B        Complexity of APPLY and BE AOMDD We now provide a characterization of the complexity of APPLY  based on different criteria  The following propositions are inspired by the results that govern OBDD apply complexity  but are adapted for pseudo tree orderings  An AOMDD along a pseudo tree can be regarded as a union of regular MDDs  each restricted to a full path from root to a leaf in the pseudo tree  Let T be such a path in T   Based on the definition of strictly compatible pseudo trees  T has corresponding paths Tf in Tf and Tg in Tg   The MDDs from f and g corresponding to Tf and Tg can be combined using the regular MDD apply  This process can be repeated for every path T   The resulting MDDs  one for each path in T need to be synchronized on their common parts  on the intersection of the paths   The algorithm we proposed does all this processing at once  in a depth first search traversal over the inputs  Based on our construction  we can give a first characterization of the complexity of AOMDD APPLY as being governed by the complexity of MDD apply  Proposition   Let             l be the set of paths in T enumerated from left to right and let Gfi and Ggi be the MDDs restricted to path i   then the size of the output of AOMDD apply by P P is bounded i     G i    n  max  G i     G i    The time complexity is also bounded by i     G i     G  G i g g g i f i f f n  maxi  Gfi     Ggi    A second characterization of the complexity can be given  similar to the MDD case  in terms of total number of nodes of the inputs  Proposition   Given two AOMDDs Gfaomdd and Ggaomdd based on strictly compatible pseudo trees  the size of the output of APPLY is at most O   Gfaomdd      Ggaomdd     We can further detail the previous proposition as follows  Given AOMDDs Gfaomdd and Ggaomdd   based on compatible pseudo trees Tf and Tg and the common pseudo tree T   we define the intersection pseudo tree Tf g as being obtained from T by the following two steps      mark all the subtrees whose nodes belong to either Tf or Tg but not to both  the leaves of each subtree should be leaves in T        remove the subtrees marked in step     from T   Steps     and     are applied just once  that is  not recursively   The part of AOMDD Gfaomdd corresponding to the variables in Tf g is denoted by Gff g   and similarly for Ggaomdd it is denoted by Ggf g   Proposition   The time complexity of  Gfaomdd      Ggaomdd      APPLY  and the size of the output are O  Gff g     Ggf g      We now turn to the complexity of the BE AOMDD algorithm  Each bucket has an associated bucket pseudo tree  The top chain of the bucket pseudo tree for variable Xi contains all and only the variables in context Xi    For any other variables that appear in the bucket pseudo tree  their associated buckets have already been processed  The original functions that belong to the bucket of Xi have their scope included in context Xi    and therefore their associated AOMDDs are based       M ATEESCU   D ECHTER   M ARINESCU  on chains  Any other functions that appear in bucket of Xi are messages received from independent branches below  Therefore  any two functions in the bucket of Xi only share variables in the context Xi    which forms the top chain of the bucket pseudo tree  We can therefore characterize the complexity of APPLY in terms of treewidth  or context size of a bucket variable  Proposition   Given two AOMDDs in the same bucket of BE AOMDD  the time and space complexity of the APPLY between them is at most exponential in the context size of the bucket variable  namely the number of the variables in the top chain of the bucket pseudo tree   We can now bound the complexity of BE AOMDD and the output size  T HEOREM   The space complexity of BE AOMDD and the size of the output AOMDD are  O n k w    where n is the number of variables  k is the maximum domain size and w is the treewidth  of the bucket tree  The time complexity is bounded by O r k w    where r is the number of initial functions      AOMDDs Are Canonical Representations It is well known that OBDDs are canonical representations of Boolean functions given an ordering of the variables  Bryant         namely a strict ordering of any CNF specification of the same Boolean function will yield an identical OBDD  and this property extends to MDDs  Srinivasan et al          The linear ordering of the variables defines a chain pseudo tree that captures the structure of the OBDD or MDD  In the case of AOBDDs and AOMDDs  the canonicity is with respect to a pseudo tree  transitioning from total orders  that correspond to a linear ordering  to partial orders  that correspond to a pseudo tree ordering   On the one hand we gain the ability to have a more compact compiled structure  but on the other hand canonicity is no longer with respect to all equivalent graphical models  but only relative to those graphical models that are consistent with the pseudo tree that is used  Specifically  if we start from a strict ordering we can generate a chain AOMDD that will be canonical relative to all equivalent graphical models  If however we want to exploit additional decomposition we can use a partial ordering captured by a pseudo tree and create a more compact AOMDD  This AOMDD however is canonical relative to those equivalent graphical models that can accept the same pseudo tree that guided the AOMDD  In general  AOMDD can be viewed as a more flexible framework for compilation that allows both partial and total orderings  Canonicity is restricted to a subset of graphical models whose primal graph agrees with the partial order but it is relevant to a larger set of orderings which are consistent with the pseudo tree  In the following subsection we discuss the canonicity of AOMDD for constraint networks  The case of general weighted graphical models is discussed in Section        AOMDDs for Constraint Networks Are Canonical Representations The case of constraint networks is more straightforward  because the weights on the OR to AND arcs can only be   or    We will show that any equivalent constraint networks  that admit the same pseudo tree T   have the same AOMDD based on T   We start with a proposition that will help prove the main theorem  Proposition   Let f be a function  not always zero  defined by a constraint network over X  Given a partition  X            Xm   of the set of variables X  namely  Xi  Xj      i    j  and X         AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  i i m i   X    if f   f          fm and f   g          gm   such that scope fi     scope gi     X for all i              m   then fi   gi for all i              m   Namely  if f can be decomposed over the given partition  then the decomposition is unique   We are now ready to show that AOMDDs for constraint networks are canonical representations given a pseudo tree  T HEOREM    AOMDDs are canonical for a given pseudo tree  Given a constraint network  and a pseudo tree T of its constraint graph  there is a unique  up to isomorphism  AOMDD that represents it  and it has the minimal number of meta nodes  A constraint network is defined by its relations  or functions   There exist equivalent constraint networks that are defined by different sets of functions  even having different scope signatures  However  equivalent constraint networks define the same function  and we can ask if the AOMDD of different equivalent constraint networks is the same  The following corollary can be derived immediately from Theorem    Corollary   Two equivalent constraint networks that admit the same pseudo tree T have the same AOMDD based on T       Canonical AOMDDs for Weighted Graphical Models Theorem   ensures that the AOMDD is canonical for constraint networks  namely for functions that can only take the values   or    The proof relied on the fact that the OR to AND weights can only be   or    and on Proposition   that ensured the unique decomposition of a function defined by a constraint network  In this section we turn to general weighted graphical models  We can first observe that Proposition   is no longer valid for general functions  This is because the valid solutions  having strictly positive weight  can have their weight decomposed in more than one way into a product of positive weights  Therefore we raise the issue of recognizing nodes that root AND OR graphs that represent the same universal function  even though the graphical representation is different  We will see that the AOMDD for a weighted graphical model is not unique under the current definitions  but we can slightly modify them to obtain canonicity again  We have to note that canonicity of AOMDDs for weighted graphical models  e g   belief networks  is far less crucial than in the case of OBDDs that are used in formal verification  Even more than that  sometimes it may be useful not to eliminate the redundant nodes  in order to maintain a simpler semantics of the AND OR graph that represents the model  The loss of canonicity of AOMDD for weighted graphical models can happen because of the weights on the OR to AND arcs  and we suggest a possible way of re enforcing it if a more compact and canonical representation is needed  Example    Figure    shows a weighted graphical model  defined by two  cost  functions  f  M  A  B  and g M  B  C   Assuming the order  M A B C   Figure    shows the AND OR search tree on the left  The arcs are labeled with function values  and the leaves show the value of the corresponding full assignment  which is the product of numbers on the arcs of the path   We can       M ATEESCU   D ECHTER   M ARINESCU  A  M                  M A  M  B B C  C  A                  B f M A B                                      M                  B                  C g M B C                                      Figure     Weighted graphical model M             A  A  A  A        B     B                        C  C                        C                                        B            C         B         M  C  B           C  C                                                                         B                               C                             B       C        B     C                                   C  C                                         Figure     AND OR search tree and context minimal graph  see that either value of M    or    gives rise to the same function  because the leaves in the two subtrees have the same values   However  the two subtrees can not be identified as representing the same function by the usual reduction rules  The right part of the figure shows the context minimal graph  which has a compact representation of each subtree  but does not share any of their parts  What we would like in this case is to have a method of recognizing that the left and right subtrees corresponding to M     and M     represent the same function  We can do this by normalizing the values in each level  and processing bottom up  In Figure    left  the values on the OR to AND arcs have been normalized  for each OR variable  and the normalization constant was promoted up to the OR value  In Figure    right  the normalization constants are promoted upwards again by multiplication  This process does not change the value of each full assignment  and therefore produces equivalent graphs  We can see already that some of the nodes labeled by C can now be merged  producing the graph in Figure    on the left  Continuing the same process we obtain the AOMDD for the weighted graph  shown in Figure    on the right  We can define the AOMDD of a weighted graphical model as follows  D EFINITION     AOMDD of weighted graphical model  The AOMDD of a weighted graphical model is an AND OR graph  with meta nodes  such that      for each meta node  its weights sum to        the root meta node has a constant associated with it      it is completely reduced  namely it has no isomorphic meta nodes  and no redundant meta nodes         AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  M M        A  A        B     B  B                                                                                                                                                       B                           C          B        C            B           C      A  B         C     A    B          C  C                    C        C                                      Figure     Normalizing values bottom up      M         M            A     A A        B           B                     B              B                  C                                   B      B                            C                                       C                   C                         Figure     AOMDD for the weighted graph The procedure of transforming a weighted AND OR graph into an AOMDD is very similar to Procedure B OTTOM U P R EDUCTION from Section    The only difference is that when a new layer is processed  first the meta node weights are normalized and promoted to the parent  and then the procedure continues as usual with the reduction rules  T HEOREM   Given two equivalent weighted graphical models that accept a common pseudo tree T   normalizing arc values together with exhaustive application of reduction rules yields the same AND OR graph  which is the AOMDD based on T   Finite Precision Arithmetic The implementation of the algorithm described in this section may prove to be challenging on machines that used finite precision arithmetic  Since the weights are real valued  the repeated normalization may lead to precision errors  One possible approach  which we also used in our experiments  is to define some  tolerance  for some user defined sufficiently small   and consider the weights to be equal if they are within  of each other      Semantic Treewidth A graphical model M represents a universal function F   fi   The function F may be represented by different graphical models  Given a particular pseudo tree T   that captures some of the structural information of F   we are interested in all the graphical models that accept T as a pseudo tree  namely their primal graphs only contain edges that are backarcs in T   Since the size of the AOMDD for F based on T is bounded in the worst case by the induced width of the graphical model along T   we define the semantic treewidth to be        M ATEESCU   D ECHTER   M ARINESCU    A        B C  C  o B  o        A        o o  C D  A  o  D  B     o  A B                          o o   a  The two solutions  A C                                  A D                                          D B C                          B D                                  C D                           b  First model  A  A B          B  C  B C          D  C D           c  Second model  Figure     The   queen problem D EFINITION     semantic treewidth  The semantic treewidth of a graphical model M relative to a pseudo tree T denoted by swT  M   is the smallest treewidth taken over all models R that are equivalent to M  and accept the pseudo tree T   Formally  it is defined by swT  M    minR u R  u M  wT  R   where u M  is the universal function of M  and wT  R  is the induced width of R along T   The semantic treewidth of a graphical model  M  is the minimal semantic treewidth over all the pseudo trees that can express its universal function  Computing the semantic treewidth can be shown to be NP hard   T HEOREM   Computing the semantic treewidth of a graphical model M is NP hard  Theorem   shows that computing the semantic treewidth is hard  and it is likely that the actual complexity is even higher  However  the semantic treewidth can explain why sometimes the minimal AND OR graph or OBDD are much smaller than the exponential in treewidth or pathwidth upper bounds  In many cases  there could be a huge disparity between the treewidth of M and the semantic treewidth along T   Example    Figure    a  shows the two solutions of the   queen problem  The problem is expressed by a complete graph of treewidth    given in Figure    b   Figure    c  shows an equivalent problem  i e   that has the same set of solutions   which has treewidth    The semantic treewidth of the   queen problem is    Based on the fact that an AOMDD is a canonical representation of the universal function of a graphical model  we can conclude that the size of the AOMDD is bounded exponentially by the semantic treewidth along the pseudo tree  rather than the treewidth of the given graphical model representation  Proposition   The size of the AOMDD of a graphical model M is bounded by O n k swT  M     where n is the number of variables  k is the maximum domain size and swT  M  is the semantic treewidth of M along the pseudo tree T      We thank David Eppstein for the proof         AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  A A  B  B  C  C  B  M  D  D  C  N  D  P         a  OBDD representation   b  Primal graph with hidden variables M  N and P    Figure     The parity function Example    Consider a constraint network on n variables such that every two variables are constrained by equality  X   Y    One graph representation is a complete graph  another is a chain and another is a tree  If the problem is specified as a complete graph  and if we use a linear order  the OBDD will have a linear size because there exists a representation having a pathwidth of    rather than n   While the semantic treewidth can yield a much better upper bound on the AOMDD  it can also be a very bad bound  It is well known that the parity function on n variables has a very compact  chain like OBDD representation  Yet  the only constraint network representation of a parity function is the function itself  namely a complete graph on all the variables   whose treewidth and semantic treewidth is its number of variables  n  The OBDD representation of the parity function suggests that the addition of hidden variables can simplify its presentation  We show an example in Figure     On the left side  in Figure    a  we have the OBDD representation of the parity function for four binary variables  A graphical model would represent this function by a complete graph on the four variables  However  we could add the extra variables M  N and P in Figure    b   sometimes called hidden variables  that can help decompose the model  In this case M can form a constraint together with A and B such that M represents the parity of A and B  namely M     if A  B      where  is the parity  XOR  operator  Similarly  N would capture the parity of M and C  and P would capture the parity of N and D  and would also give the parity of the initial four variables  The two structures are surprisingly similar  It would be interesting to study further the connection between hidden variables and compact AOBDDs  but we leave this for future work       Experimental Evaluation Our experimental evaluation is in preliminary stages  but the results we have are already encouraging  We ran the search based compile algorithm  by recording the trace of the AND OR search  and then reducing the resulting AND OR graph bottom up  In these results we only applied the reduction by isomorphism and still kept the redundant meta nodes  We implemented our algorithms in C   and ran all experiments on a    GHz Intel Core   Duo with  GB of RAM  running Windows         M ATEESCU   D ECHTER   M ARINESCU       Benchmarks We tested the performance of the search based compilation algorithm on random Bayesian networks  instances from the Bayesian Network Repository and a subset of networks from the UAI   Inference Evaluation Dataset  Random Bayesian Networks The random Bayesian networks were generated using parameters  n  k  c  p   where n is the number of variables  k is the domain size  c is the number of conditional probability tables  CPTs  and p is the number of parents in each CPT  The structure of the network was created by randomly picking c variables out of n and  for each  randomly picking p parents from their preceding variables  relative to some ordering  The remaining n  c variables are called root nodes  The entries of each probability table were generated randomly using a uniform distribution  and the table was then normalized  It is also possible to control the amount of determinism in the network by forcing a percentage det of the CPTs to have only   and   entries  Bayesian Network Repository The Bayesian Network Repository  contains a collection of belief networks extracted from various real life domains which are often used for benchmarking probabilistic inference algorithms  UAI   Inference Evaluation Dataset The UAI      Inference Evaluation Dataset  contains a collection of random as well as real world belief networks that were used during the first UAI      Inference Evaluation contest  For our purpose we selected a subset of networks which were derived from the ISCAS   digital circuits benchmark   ISCAS   circuits are a common benchmark used in formal verification and diagnosis  Each of these circuits was converted into a Bayesian network by removing flip flops and buffers in a standard way  creating a deterministic conditional probability table for each gate  and putting uniform distributions on the input signals       Algorithms We consider two search based compilation algorithms  denoted by AOMDD BCP and AOMDDSAT  respectively  that reduce the context minimal AND OR graph explored via isomorphism  while exploiting the determinism  if any  present in the network  The approach we take for handling the determinism is based on unit resolution over a CNF encoding  i e   propositional clauses  of the zero probability tuples of the CPTs  The idea of using unit resolution during search for Bayesian networks was first explored by Allen and Darwiche         AOMDD BCP is conservative and applies only unit resolution at each node in the search graph  whereas AOMDD SAT is more aggressive and detects inconsistency by running a full SAT solver  We used the zChaff SAT solver  Moskewicz  Madigan  Zhao  Zhang    Malik        for both unit resolution as well as full satisfiability  For comparison  we also ran an OR version of AOMDD BCP  called MDD BCP  For reference we also report results obtained with the ACE  compiler  ACE compiles a Bayesian network into an Arithmetic Circuit  AC  and then uses the AC to answer multiple queries with respect to the network  An arithmetic circuit is a representation that is equivalent to AND OR graphs  Mateescu   Dechter         Each time ACE compiler is invoked  it uses one of two algorithms as the basis for compilation  First  if an elimination order can be generated for the network having              http   www cs huji ac il compbio Repository  http   ssli ee washington edu bilmes uai  InferenceEvaluation Available at  http   www fm vslib cz kes asic iscas  Available at  http   reasoning cs ucla edu ace        AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  Network   w   h    n  k   ACE  nodes time  MDD w  BCP AOMDD w  BCP AOMDD w  SAT  meta  cm OR  time  meta  cm OR  time  meta  cm OR  time Bayesian Network Repository alarm                                                                            cpcs                                                                         cpcs                                                                       cpcs   b                                  diabetes                                  hailfinder                                                               mildew                                                                               mm                                                                         munin                                    munin                                    munin                                    pathfinder                                                                                              pigs                                                                                 water                                                                                            UAI   Evaluation Dataset BN                                                                             BN                                                                                       BN                                                                             BN                                                                                 BN                                                                                               BN                                                                                      BN                                                                                    BN                                                                                      BN                                                                                      BN                                                                                BN                                                                                    BN                                                                                              BN                                                                                      BN                                                                                       BN                                                                          BN                                                                         Positive Random Bayesian Networks  n     k    p    c     r                                                                          r                                                                          r                                                                          r                                                                    r                                                                           r                                                                           r                                                                          r                                                                          r                                                                    r                                                                     Deterministic Random Bayesian Networks  n      k    p    c     and det       of the CPTs containing only   and   entries r   d                                                                            r   d                                                                             r   d                                                                                  r   d                                                                             r   d                                                                                  r   d                                                                                  r   d                                                                                  r   d                                                                            r   d                                                                             r   d                                                                               Table    Results for experiments with    Bayesian networks from   problem classes  w   treewidth  h   depth of pseudo tree  n   number of variables  k   domain size  time given in seconds  bold types highlight the best results across rows         M ATEESCU   D ECHTER   M ARINESCU  sufficiently small induced width  then tabular variable elimination will be used as the basis  This algorithm is similar to the one discussed by Chavira and Darwiche         but uses tables to represent factors rather than ADDs  If the induced width is large  then logical model counting will be used as the basis  Tabular variable elimination is typically efficient when width is small but cannot handle networks when the width is larger  Logical model counting  on the other hand  incurs more overhead than tabular variable elimination  but can handle many networks having larger treewidth  Both tabular variable elimination and logical model counting produce ACs that exploit local structure  leading to efficient online inference  When logical model counting is invoked  it proceeds by encoding the Bayesian network into a CNF  Chavira   Darwiche        Chavira  Darwiche    Jaeger         simplifying the CNF  compiling the CNF into a d DNNF  and then extracting the AC from the compiled d DNNF  A dtree over the CNF clauses drives the compilation step  In all our experiments we report the compilation time in seconds  time   the number of OR nodes in the context minimal graph explored   cm   the number of meta nodes of the resulting AOMDD   meta   as well as the size of the AC compiled by ACE   nodes   For each network we specify the number of variables  n   domain size  k   induced width  w   and pseudo tree depth  h   A   stands for exceeding the  GB memory limit by the respective algorithm  The best performance points are highlighted       Evaluation on Bayesian Networks Table   reports the results obtained for experiments with    Bayesian networks  The AOMDD compilers as well as ACE used the min fill heuristic  Kjaerulff        to construct the guiding pseudo tree and dtree  respectively         BAYESIAN N ETWORKS R EPOSITORY We see that ACE is overall the fastest compiler on this domain  outperforming both AOMDD BCP and AOMDD SAT with up to several orders of magnitude  e g   mildew  pigs   However  the diagrams compiled by ACE and AOMDD BCP  resp  AOMDD SAT  are comparable in size  In some cases  AOMDD BCP and AOMDD SAT were able to compile much smaller diagrams than ACE  For example  the diagram produced by AOMDD BCP for the mildew network is    times smaller than the one compiled by ACE  In principle the output produced by ACE and AOMDD should be similar if both are guided by the same pseudo tree dtree  Our scheme should be viewed as a compilation alternative which     extends decision diagrams and     mimics traces of search properties that may make this representation accessible  The OR compiler MDD BCP was able to compile only   out of the    test instances  but their sizes were far larger than those produced by AOMDD BCP  For instance  on the pathfinder network  AOMDD BCP outputs a decision diagram almost   orders of magnitude smaller than MDD BCP         UAI   DATASET For each of the UAI   Dataset instances we picked randomly    variables and instantiated as evidence  We see that ACE is the best performing compiler on this dataset  AOMDD BCP is competitive with ACE in terms of compile time only on   out the    test instances  AOMDD SAT is able to compile the smallest diagrams for   networks only  e g   BN     BN     BN     BN     BN     BN      As before  the difference in size between the compiled data structures produces by MDD BCP and AOMDD BCP is up to   orders of magnitude in favor of the latter        AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS         R ANDOM N ETWORKS The problem instances denoted by r     through r      were generated from a class of random belief networks with parameters  n       k      p      c        Similarly  the instances denoted by r   d     through r   d      belong to a class with parameters  n        k      p      c        In the latter case  det       of the CPTs are deterministic  namely they contain only   and   probability tuples  These test instances were compiled without any evidence  We see that on this domain AOMDD BCP AOMDD SAT were able to compile the smallest diagrams  which were on average about   times smaller than those produced by ACE  However  ACE was again the fastest compiler  Notice that the OR compiler MDD BCP ran out of memory in all test cases       The Impact of Variable Ordering As theory dictates  the AOMDD size is influenced by the quality of the guiding pseudo tree  In addition to the min fill heuristic we also considered the hypergraph heuristic which constructs the pseudo tree by recursively decomposing the dual hypergraph associated with the graphical model  This idea was also explored by Darwiche        for constructing dtrees that guide ACE  Since both the min fill and hypergraph partitioning heuristics are randomized  namely ties are broken randomly   the size of the AOMDD guided by the resulting pseudo tree may vary significantly from one run to the next  Figure    displays the AOMDD size using hypergraph and min fill based pseudo trees for   networks selected from Table    over    independent runs  We also record the average induced width and depth obtained for the pseudo trees  see the header of each plot in Figure      We see that the two heuristics do not dominate each other  namely the variance in output size is quite significant in both cases       Memory Usage Table   shows the memory usage  in MBytes  of ACE  AOMDD BCP and AOMDD SAT  respectively  on the Bayesian networks from Table    We see that in some cases the AOMDD based compilers require far less memory than ACE  For example  on the mildew network  both AOMDDBCP and AOMDD SAT use about    MB of memory to compile the AND OR decision diagram  while ACE requires as much as     MB of memory  Moreover  the compiled AOMDD has in this case about one order of magnitude fewer nodes than that constructed by ACE  When comparing the two AND OR search based compilers  we observe that on networks with a significant amount of determinism  such as those from the UAI   Evaluation dataset  AOMDD SAT uses on average two times less memory than AOMDD BCP  The most dramatic savings in memory usage due to the aggressive constraint propagation employed by AOMDD SAT compared with AOMDD BCP can be seen on the BN    network  In this case  the difference in memory usage between AOMDD SAT and AOMDD BCP is about   orders of magnitude in favor of the former       Related Work The related work can be viewed along two directions      the work related to the AND OR search idea for graphical models and     the work related to compilation for graphical models that exploits problem structure  An extensive discussion for     was provided in the previous work of Dechter and Mateescu         Since this is not the focus of the paper  we just mention that the AND OR idea was origi      M ATEESCU   D ECHTER   M ARINESCU  Figure     Effect of variable ordering         AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  Network  ACE AOMDD w  BCP AOMDD w  SAT  nodes memory  MB   nodes memory  MB   nodes memory  MB  Bayesian Network Repository alarm                                  cpcs                                            cpcs                                          cpcs   b                  diabetes                  hailfinder                                      mildew                                              mm                                           munin                   munin                   munin            n a pathfinder                                        pigs                                               water                                            UAI   Evaluation Dataset BN          n a                              BN           n a                                   BN          n a                              BN           n a                                BN          n a                            BN          n a                           BN          n a                         BN          n a                           BN          n a                           BN          n a                         BN          n a                         BN          n a                             BN          n a                           BN          n a                           BN          n a                             BN          n a                           Positive Random Bayesian Networks with parameters  n     k    p    c     r                                             r                                             r                                             r                                           r                                              r                                              r                                             r                                             r                                           r                                            Deterministic Random Bayesian Networks with parameters  n      k    p    c     r   d                                             r   d                                               r   d                                                 r   d                                               r   d                                                 r   d                                                 r   d                                                 r   d                                              r   d                                              r   d                                                 Table    Memory usage in MBytes of ACE  AOMDD BCP and AOMDD SAT on the    Bayesian networks from Table    Bold types highlight the best performance across rows  The n a indicates that the respective memory usage statistic was not available from ACEs output         M ATEESCU   D ECHTER   M ARINESCU  nally developed for heuristic search  Nilsson         As mentioned in the introduction  the AND OR search for graphical models is based on a pseudo tree that spans the graph of the model  similar to the tree rearrangement of Freuder and Quinn               The idea was adapted for distributed constraint satisfaction by Collin et al               and more recently by Modi et al          and was also shown to be related to graph based backjumping  Dechter         This work was extended by Bayardo and Miranker         Bayardo and Schrag        and more recently applied to optimization tasks by Larrosa et al          Another version that can be viewed as exploring the AND OR graphs was presented recently for constraint satisfaction  Terrioux   Jegou      b  and for optimization  Terrioux   Jegou      a   Similar principles were introduced recently for probabilistic inference  in algorithm Recursive Conditioning  Darwiche        as well as in Value Elimination  Bacchus et al       b      a   and are currently at the core of the most advanced SAT solvers  Sang et al          For direction      there are various lines of related research  The formal verification literature  beginning with the work of Bryant        contains a very large number of papers dedicated to the study of BDDs  However  BDDs are in fact OR structures  the underlying pseudo tree is a chain  and do not take advantage of the problem decomposition in an explicit way  The complexity bounds for OBDDs are based on pathwidth rather than treewidth  As noted earlier  the work of Bertacco and Damiani        on Disjoint Support Decomposition  DSD  is related to AND OR BDDs in various ways  The main common aspect is that both approaches show how structure decomposition can be exploited in a BDD like representation  DSD is focused on Boolean functions and can exploit more refined structural information that is inherent to Boolean functions  In contrast  AND OR BDDs assume only the structure conveyed in the constraint graph  and are therefore more broadly applicable to any constraint expression and also to graphical models in general  They allow a simpler and higher level exposition that yields graphbased bounds on the overall size of the generated AOMDD  The full relationship between these two formalisms should be studied further  McMillan        introduced the BDD trees  along with the operations for combining them  For  w circuits of bounded tree width  BDD trees have a linear space upper bound of O  g  w     where  g  is the size of the circuit g  typically linear in the number of variables  and w is the treewidth  This bound hides some very large constants to claim the linear dependence on  g  when w is bounded  However  McMillan maintains that when the input function is a CNF expression BDD trees have the same bounds as AND OR BDDs  namely they are exponential in the treewidth only  To sketch just a short comparison between McMillans BDD trees and AOMMDs  consider an example where we have a simple pseudo tree with root   left child  and right child   Each of these nodes may stand for a set of variables  In BDD trees  the assignments to  are grouped into equivalence classes according to the cofactors generated by them on the remaining  and   For example assignments   and   are equivalent if they generate the same function on  and   The node  can be represented by a BDD whose leaves are the cofactors  The same is done for   The node  is then represented by a matrix of BDDs  where each column corresponds to a cofactor of  and each line to a cofactor of   By contrast  an AOMDD represents the node  as a BDD whose leaves are the cofactors  the number of distinct functions on  and   and then each cofactor is the root of a decomposition  an AND node  between  and   Moreover  the representations of   as descendants of different cofactor of   are shared as much as possible and the same goes for   This is only a high level description  that becomes slightly more complicated when redundant nodes are eliminated  but the idea remains the same        AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  The AND OR structure restricted to propositional theories is very similar to deterministic decomposable negation normal form  d DNNF   Darwiche   Marquis        Darwiche         More recently  Huang and Darwiche      b  used the trace of the DPLL algorithm to generate an OBDD  and compared with the typical formal verification approach of combining the OBDDs of the input function according to some schedule  The structures that were investigated in that case are still OR  This idea is extended in our present work by the AND OR search compilation algorithm  McAllester  Collins  and Pereira        introduced the case factor diagrams  CFD   which subsume Markov random fields of bounded tree width and probabilistic context free grammars  PCFG   CFDs are very much related to the AND OR graphs  The CFDs target the minimal representation  by exploiting decomposition  similar to AND nodes  but also by exploiting context sensitive information and allowing dynamic ordering of variables based on context  CFDs do not eliminate the redundant nodes  and part of the cause is that they use zero suppression  There is no claim about CFDs being canonical forms  and also no description of how to combine two CFDs  There are numerous variants of decision diagrams that are designed to represent integer valued or real valued functions  For a comprehensive view we refer the reader to the survey of Drechsler and Sieling         Algebraic decision diagrams  ADDs   Bahar et al         provide a compilation for general real valued rather than Boolean functions  Their main drawback is that their size increases very fast if the number of terminals becomes large  There are several approaches that try to alleviate this problem  However the structure that they capture is still OR  and they do not exploit decomposition  Some alternatives introduce edge values  or weights  that enable more subgraph sharing  Edge valued binary decision diagrams  EVBDDs   Lai   Sastry        use additive weights  and when multiplicative weights are also allowed they are called factored EVBDDs  FEVBDDs   Tafertshofer   Pedram         Another type of BDDs called K BMDs  Drechsler  Becker    Ruppertz        also use integer weights  both additive and multiplicative in parallel  ADDs have also been extended to affine ADDs  Sanner   McAllester         through affine transformations that can achieve more compression  The result was shown to be beneficial for probabilistic inference algorithms  such as tree clustering  but they still do not exploit the AND structure  More recently  independently and in parallel to our work on AND OR graphs  Dechter   Mateescu      a      b   Fargier and Vilarem        and Fargier and Marquis              proposed the compilation of CSPs into tree driven automata  which have many similarities to our work  Their main focus is the transition from linear automata to tree automata  similar to that from OR to AND OR   and the possible savings for tree structured networks and hyper trees of constraints due to decomposition  Their compilation approach is guided by a tree decomposition while ours is guided by a variable elimination based algorithms  And it is well known that Bucket Elimination and cluster tree decomposition are in principle the same  Dechter   Pearl         Wilson        extended OBDDs to semi ring BDDs  The semi ring treatment is restricted to the OR search spaces  but allows dynamic variable ordering  It is otherwise very similar in aim and scope to our AOMDD  When restricting the AOMDD to OR graphs only  the two are closely related  except that we express BDDs using the Shenoy Shafer axiomatization that is centered on the two operation of combination and marginalization rather then on the semi ring formulation  Minimality in the formulation of Wilson        is more general allowing merging nodes having different values and therefore it can capture symmetries  called interchangeability   Another framework very similar to AOMDDs  that we became aware of only recently  is Probabilistic Decision Graphs  PDG  of Jaeger         This work preceded most of the relevant work       M ATEESCU   D ECHTER   M ARINESCU  we discussed above  Fargier   Vilarem        Wilson        and went somewhat unnoticed  perhaps due to notational and cultural differences  It is however similar in motivation  framework and proposed algorithms  We believe our AND OR framework is more accessible  We define the framework over multi valued domains  provide greater details in algorithms and complexity analysis  make an explicit connection with search frameworks  fully address the issues of canonicity as well as provide an empirical demonstration  In particular  the claim of canonicity for PDGs is similar to the one we make for AOMDDs of weighted models  in that it is relative to the trees  or forests  that can represent the given probability distribution  There is another line of research by Drechsler and his group  e g  Zuzek  Drechsler    Thornton         who use AND OR graphs for Boolean function representation  that may seem similar to our approach  However  the semantics and purpose of their AND OR graphs are different  They are constructed based on the technique of recursive learning and are used to perform Boolean reasoning  i e  to explore the logic consequences of a given assumption based on the structure of the circuit  especially to derive sets of implicants  The meaning of AND and OR in their case is related to the meaning of the gates functions  while in our case the meaning is not related to the semantic of the functions  The AND OR enumeration tree that results from a circuit according to Zuzek et al         is not related to the AND OR decomposition that we discuss       Conclusion We propose the AND OR multi valued decision diagram  AOMDD   which emerges from the study of AND OR search spaces for graphical models  Dechter   Mateescu      a      b  Mateescu   Dechter        Dechter   Mateescu        and ordered binary decision diagrams  OBDDs   Bryant         This data structure can be used to compile any graphical model  Graphical models algorithms that are search based and compiled data structures such as BDDs differ primarily by their choices of time vs  memory  When we move from regular OR search space to an AND OR search space the spectrum of algorithms available is improved for all time vs  memory decisions  We believe that the AND OR search space clarifies the available choices and helps guide the user into making an informed selection of the algorithm that would fit best the particular query asked  the specific input function and the available computational resources  The contribution of our work is      We formally describe the AOMDD and prove that it is a canonical representation of a constraint network      We extend the AOMDD to general weighted graphical models      We give a compilation algorithm based on AND OR search  that saves the trace of a memory intensive search  the context minimal AND OR graph   and then reduces it in one bottom up pass      We describe the APPLY operator that combines two AOMDDs by an operation and show that its complexity is quadratic in the input  but never worse than exponential in the treewidth      We give a scheduling order for building the AOMDD of a graphical model starting with the AOMDDs of its functions which is based on a Variable Elimination algorithm  This guarantees that the complexity is at most exponential in the induced width  treewidth  along the ordering      We show how AOMDDs relate to various earlier and recent compilation frameworks  providing a unifying perspective for all these methods      We introduce the semantic treewidth  which helps explain why compiled decision diagrams are often much smaller than the worst case bound  Finally      we provide a preliminary empirical demonstration of the power of the current scheme         AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  Acknowledgments This work was done while Robert Mateescu and Radu Marinescu were at the University of California  Irvine  The authors would like to thank the anonymous reviewers for their constructive suggestions to improve the paper  David Eppstein for a useful discussion of complexity issues  and Lars Otten and Natasha Flerova for comments on the final version of the manuscript  This work was supported by the NSF grants IIS         and IIS          and the initial part by the Radcliffe fellowship            through the partner program   with Harvard undergraduate student John Cobb   Appendix Proof of Proposition   Consider the level of variable Xi   and the meta nodes in the list LXi   After one pass through the meta nodes in LXi  the inner for loop   there can be no two meta nodes at the level of Xi in the AND OR graph that are isomorphic  because they would have been merged in line    Also  during the same pass through the meta nodes in LXi all the redundant meta nodes in LXi are eliminated in line    Processing the meta nodes in the level of Xi will not create new redundant or isomorphic meta nodes in the levels that have been processed before  It follows that the resulting AND OR graph is completely reduced    Proof of Theorem   The bound on the size follows directly from Theorem    The AOMDD size can only be smaller than  the size of the context minimal AND OR graph  which is bounded by O n k wT  G     To prove the time bound  we have to rely on the use of the hash table  and the assumption that an efficient implementation allows an access time that is constant  The time bound of AND OR S EARCH  AOMDD  is O n k wT  G     from Theorem    because it takes time linear in the output  we assume here that no constraint propagation is performed during search   Procedure B OTTOM U P R EDUCTION  procedure    takes time linear in the size of the context minimal AND OR graph  Therefore  the AOMDD  can be computed in time O n k wT  G     and the result is the same for the algorithm that performs the reduction during the search    Proof of Proposition   The complexity of OBDD  and MDD  apply is known to be quadratic in the input  Namely  the number of nodes in the output is at most the product of number of nodes in the input  Therefore  the number of nodes that can appear along one path in the output AOMDD can be at most the product of the number of nodes in each input  along the same path   Gfi     Ggi    Summing over all the paths in T gives the result    Proof of Proposition   The argument is identical to the case of MDDs  The recursive calls in APPLY lead to combinations of one node from Gfaomdd and one node from Ggaomdd  rather than a list of nodes   The number of total possible such combinations is O   Gfaomdd      Ggaomdd       Proof of Proposition   The recursive calls of APPLY can generate one meta node in the output for each combination of       M ATEESCU   D ECHTER   M ARINESCU  nodes from Gff g and Ggf g   Lets look at combinations of nodes from Gff g and Ggaomdd   Ggf g   The meta nodes from Ggaomdd   Ggf g that can participate in such combinations  lets call this set A  are only those from levels  of variables  right below Tf g   This is because of the mechanics of the recursive calls in APPLY  Whenever a node from f that belongs to Gff g is combined with a node from g that belongs to A  line    of APPLY expands the node from f   and the node  or nodes  from A remain the same  This will happen until there are no more nodes from f that can be combined with the node  or nodes  from A  and at that point APPLY will simply copy the remaining portion of its output from Ggaomdd   The size of A is therefore proportional to   Ggf g    because it is the layer of metanodes immediately below Ggf g    A similar argument is valid for the symmetrical case  And there are no combinations between nodes in Ggaomdd   Ggf g and Ggaomdd   Ggf g   The bound follows from all these arguments    Proof of Proposition   The APPLY operation works by constructing the output AOMDD from root to leaves  It first creates a meta node for the root variable  and then recursively creates its children metanodes by using APPLY on the corresponding children of the input  The worst case that can happen is when the output is not reduced at all  and a recursive call is made for each possible descendant  This corresponds to an unfolding of the full AND OR search tree based on the context variables  which is exponential in the context size  When the APPLY finishes the context variables  and arrives at the first branching in the bucket pseudo tree  the remaining branches are independent  Similar to the case of OBDDs  where one function occupies a single place in memory  the APPLY can simply create a link to the corresponding branches of the inputs  this is what happens in line   in the APPLY algorithm   Therefore  the time and space complexity is at most exponential in the context size    Proof of Theorem   The space complexity is governed by that of BE  Since an AOMDD never requires more space than  that of a full exponential table  or a tree   it follows that BE AOMDD only needs space O n k w    The size of the output AOMDD is also bounded  per layers  by the number of assignments to the context of that layer  namely  by the size of the context minimal AND OR graph   Therefore   because context size is bounded by treewidth  it follows that the output has size O n k w    The time complexity follows from Proposition    and from the fact that the number of functions in a bucket cannot exceed r  the original number of functions    Proof of Proposition   It suffices to prove the proposition for m      The general result can then be obtained by induction  It is essential that the function is defined by a constraint network  i e   the values are only   or     and that the function takes value   at least for one assignment  The value   denotes consistent assignments  solutions   while   denotes inconsistent assignments  Suppose f   f  f    Lets denote by x a full assignment to X  and by x  and x  the projection of x over X  and X    respectively  We can write x   x  x   concatenation of partial assignments   It follows that f  x    f   x     f   x     Therefore  if f  x       it must be that f   x        and f   x         We claim that for any x    f   x        only if there exists some x  such that f  x  x         Suppose by contradiction that there exist some x  such that f   x        and f  x  x        for any other x    Since f is not always zero         AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  it follows that f  is not always zero  and therefore there must be some x  for which f   x         This leads to a contradiction  and therefore the functions f  and f  are uniquely defined by f     Proof of Theorem   The proof is by structural induction over the depth of the pseudo tree T   It follows the canonicity proofs for OBDDs  Bryant        and MDDs  Srinivasan et al          but extends them from linear orderings to tree orderings that capture function decomposition according to the pseudo tree T   The depth of T   along each of its paths from root to a leaf  is actually the size of the dependency set  or the set of variables on which the value of the function depends  Remember that the AOMDD is an AND OR graph that is completely reduced  We will use the word function  denoted by f   to refer to the universal relation  or its characteristic function  defined by the constraint network  Assume the depth of T is    This means that the function does not depend on any variable  and must be one of the constants   or    Suppose the function is the constant    Then  it must be that the AOMDD does not contain the terminal meta node    since all the nodes must be reachable along some path  and it would mean that the function can also evaluate to    Suppose the AOMDD contains a nonterminal meta node  say labeled with X  where X can take k different values  It must be that all the k children meta nodes of X are the terminal meta node    If there are more than one terminal    then the AOMDD is not completely reduced  If there is only one    it follows that the meta node labeled with X is redundant  Therefore  from all the above  it follows that the AOMDD representing the constant   is made of only the terminal    This is unique  and contains the smallest number of nodes  A similar argument applies for the constant    Now  suppose that the statement of the theorem holds for any constraint network that admits a pseudo tree of depth strictly smaller than p  and that we have a constraint network with a pseudo tree of depth equal to p  where p      Let X be the root of T   having domain  x            xk    We denote by fi   where i              k   the functions defined by the restricted constraint network for X   xi   namely fi   f  X xi   Let Y            Ym be the children of X in T   Suppose we have two AOMDDs of f   denoted by G and G     We will show that these two AND OR graphs are isomorphic  The functions fi can be decomposed according to the pseudo tree T when the root X is removed  This can in fact be a forest of independent pseudo trees  they do not share any variables   rooted by Y            Ym   Based on Proposition    there is a unique decomposition fi   fiY          fiYm   for all Y i              k   Based on the induction hypothesis  each of the function fi j has a unique AOMDD  In the AND OR graphs G and G     if we look at the subgraphs descending from X   xi   they both are completely reduced and define the same function  fi   therefore there exists an isomorphic mapping i between them  Let v be the root metanode of G and v   the root of G     We claim that G and G   are isomorphic according to the following mapping      v  if u   v   u    i  u   if u is in the subgraph rooted by hX  xi i  To prove this  we have to show that  is well defined  and that it is an isomorphic mapping  If a meta node u in G is contained in both subgraphs rooted by hX  xi i and hX  xj i  Then the AND OR graphs rooted by i  u  and j  u  are isomorphic to the one rooted at u  and therefore to each other  Since G   is completely reduced  it does not contain isomorphic subgraphs  and therefore i  u    j  u   Therefore  is well defined  We can now show that  is a bijection  To show that it is one to one  assume two distinct metanodes u  and u  in G  with  u       u     Then  the subgraphs rooted by u  and u  are isomorphic       M ATEESCU   D ECHTER   M ARINESCU  to the subgraph rooted by  u     and therefore to each other  Since G is completely reduced  it must be that u    u    The fact that  is onto and is an isomorphic mapping follows from its definition and from the fact that each i is onto and the only new node is the root meta node  Since the AOMDDs only contain one root meta node  more than one root would lead to the conclusion that the root meta nodes are isomorphic and should be merged   we conclude that G and G   are isomorphic  Finally  we can show that among all the AND OR graphs representing f   the AOMDD has minimal number of meta nodes  Suppose G is an AND OR graph that represents f   with minimal number of meta nodes  but without being an AOMDD  Namely  it is not completely reduced  Any reduction rule would transform G into an AND OR graph with smaller number of meta nodes  leading to a contradiction  Therefore  G must be the unique AOMDD that represents f     Proof of Corollary   The proof of Theorem   did not rely on the scopes that define the constraint network  As long as the network admits the decomposition induced by the pseudo tree T   the universal function defined by the constraint network will always have the same AOMDD  and therefore any constraint network equivalent to it that admits T will also have the same AOMDD    Proof of Theorem   The constant that is associated with the root is actually the sum of the weights of all solutions  This can be derived from the definition of the weighted AOMDD  The weights of each meta node are normalized  they sum to     therefore the values computed for each OR node by AND OR search is always    when the task is computing P the sum of all solution weights   Therefore  the constant of the weighted AOMDD is always x w x  regardless of the graphical model  We will prove that weighted AOMDDs are canonical for functions that are normalized  Assume we have two different weighted AOMDDs  denoted by G   and G     for the same normalized function f   Let the root variable be A  with the domain  a            ak    Let x denote a full assignment to all the variables  Similar to the above argument for the root constant  P because all the meta nodes have normalized weights  it follows that w   A  a      w   A  a      x A a  f  x   The superscript of w  and w  indicates the AOMDD  and the summation is over all possible assignments restricted to A   a    It follows that the root meta nodes are identical  For each value of the root variable  the restricted functions represented in G   and G   are identical  and we will recursively apply the same argument as above  However  for the proof to be complete  we have to discuss the case when a restricted function is decomposed into independent functions  according to the pseudo tree  Suppose there are two independent components  rooted by B and C  If one of them is the   function  it follows that the entire function is    We will prove that the meta nodes of B in G   and G   are identical  If B has only one value b  extendable to a solution  its weight must be   in both meta nodes  so the meta nodes are identical  If B has more than one value  suppose without loss of generality that the weights are different for the first value b    and w   B  b      w   B  b           Since f       there must be a value C   c  such that B   b    C   c  can be extended to a full solution  The sum of the weights of all these possible extensions is X f  x    w   B  b     w   C  c      w   B  b     w   C  c         x B b   C c         AND OR M ULTI  VALUED D ECISION D IAGRAMS  AOMDD S   FOR G RAPHICAL M ODELS  From Equations   and   and the fact that the weight are non zero  it follows that w   C  c      w   C  c           From Equation    the fact that B has more than one value and the fact that the weights of B are normalized  it follows that there should be a value b  such that w   B  b      w   B  b           From Equations   and    it follows that w   B  b     w   C  c      w   B  b     w   C  c           However  both sides of P the Equation   represent the sum of weights of all solutions when B   b    C   c    namely x B b   C c  f  x   leading to a contradiction  Therefore  it must be that Equation   is false  Continuing the same argument for all values of B  it follows that the metanodes of B are identical  and similarly the meta nodes of C are identical  If the decomposition has more than two components  the same argument applies  when B is the first component and C is a meta variable that combines all the other components    Proof of Theorem   Consider the well known NP complete problem of    COLORING  Given a graph G  is there a   coloring of G  Namely  can we color its vertices using only three colors  such that any two adjacent vertices have different colors  We will reduce    COLORING to the problem of computing the semantic treewidth of a graphical model  Let H be a graph that is   colorable  and has a nontrivial semantic treewidth  It is easy to build examples for H  If G is   colorable  then G  H is also   colorable and will have a non trivial semantic treewidth  because adding G will not simplify the task of describing the colorings of H  However  if G is not   colorable  then G  H is also not   colorable  and will have a semantic treewidth of zero    Proof of Proposition   Since AOMDDs are canonical representations of graphical models  it follows that the graphical model for which the actual semantic treewidth is realized will have the same AOMDD as M  and therefore the AOMDD is bounded exponentially in the semantic treewidth     
 The paper investigates parameterized approximate message passing schemes that are based on bounded inference and are inspired by Pearls belief propagation algorithm  BP   We start with the bounded inference mini clustering algorithm and then move to the iterative scheme called Iterative Join Graph Propagation  IJGP   that combines both iteration and bounded inference  Algorithm IJGP belongs to the class of Generalized Belief Propagation algorithms  a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini clustering and belief propagation  as well as a number of other stateof the art algorithms on several classes of networks  We also provide insight into the accuracy of iterative BP and IJGP by relating these algorithms to well known classes of constraint propagation schemes      Introduction Probabilistic inference is the principal task in Bayesian networks and is known to be an NP hard problem  Cooper        Roth         Most of the commonly used exact algorithms such as jointree clustering  Lauritzen   Spiegelhalter        Jensen  Lauritzen    Olesen        or variableelimination  Dechter              Zhang  Qi    Poole         and more recently search schemes  Darwiche        Bacchus  Dalmao    Pitassi        Dechter   Mateescu        exploit the network structure  While significant advances were made in the last decade in exact algorithms  many real life problems are too big and too hard  especially when their structure is dense  since they are time and space exponential in the treewidth of the graph  Approximate algorithms are therefore necessary for many practical problems  although approximation within given error bounds is also NP hard  Dagum   Luby        Roth          c      AI Access Foundation  All rights reserved    M ATEESCU   K ASK   G OGATE   D ECHTER  The paper focuses on two classes of approximation algorithms for the task of belief updating  Both are inspired by Pearls belief propagation algorithm  Pearl         which is known to be exact for trees  As a distributed algorithm  Pearls belief propagation can also be applied iteratively to networks that contain cycles  yielding Iterative Belief Propagation  IBP   also known as loopy belief propagation  When the networks contain cycles  IBP is no longer guaranteed to be exact  but in many cases it provides very good approximations upon convergence  Some notable success cases are those of IBP for coding networks  McEliece  MacKay    Cheng        McEliece   Yildirim         and a version of IBP called survey propagation for some classes of satisfiability problems  Mezard  Parisi    Zecchina        Braunstein  Mezard    Zecchina         Although the performance of belief propagation is far from being well understood in general  one of the more promising avenues towards characterizing its behavior came from analogies with statistical physics  It was shown by Yedidia  Freeman  and Weiss              that belief propagation can only converge to a stationary point of an approximate free energy of the system  called Bethe free energy  Moreover  the Bethe approximation is computed over pairs of variables as terms  and is therefore the simplest version of the more general Kikuchi        cluster variational method  which is computed over clusters of variables  This observation inspired the class of Generalized Belief Propagation  GBP  algorithms  that work by passing messages between clusters of variables  As mentioned by Yedidia et al          there are many GBP algorithms that correspond to the same Kikuchi approximation  A version based on region graphs  called canonical by the authors  was presented by Yedidia et al                      Our algorithm Iterative Join Graph Propagation is a member of the GBP class  although it will not be described in the language of region graphs  Our approach is very similar to and was independently developed from that of McEliece and Yildirim         For more information on BP state of the art research see the recent survey by Koller         We will first present the mini clustering scheme which is an anytime bounded inference scheme that generalizes the mini bucket idea  It can be viewed as a belief propagation algorithm over a tree obtained by a relaxation of the networks structure  using the technique of variable duplication   We will subsequently present Iterative Join Graph Propagation  IJGP  that sends messages between clusters that are allowed to form a cyclic structure  Through these two schemes we investigate      the quality of bounded inference as an anytime scheme  using mini clustering       the virtues of iterating messages in belief propagation type algorithms  and the result of combining bounded inference with iterative message passing  in IJGP   In the background section    we overview the Tree Decomposition scheme that forms the basis for the rest of the paper  By relaxing two requirements of the tree decomposition  that of connectedness  via mini clustering  and that of tree structure  by allowing cycles in the underlying graph   we combine bounded inference and iterative message passing with the basic tree decomposition scheme  as elaborated in subsequent sections  In Section   we present the partitioning based anytime algorithm called Mini Clustering  MC   which is a generalization of the Mini Buckets algorithm  Dechter   Rish         It is a messagepassing algorithm guided by a user adjustable parameter called i bound  offering a flexible tradeoff between accuracy and efficiency in anytime style  in general the higher the i bound  the better the accuracy   MC algorithm operates on a tree decomposition  and similar to Pearls belief propagation algorithm  Pearl        it converges in two passes  up and down the tree  Our contribution beyond other works in this area  Dechter   Rish        Dechter  Kask    Larrosa        is in      Extending the partition based approximation for belief updating from mini buckets to general treedecompositions  thus allowing the computation of the updated beliefs for all the variables at once        J OIN  G RAPH P ROPAGATION A LGORITHMS  This extension is similar to the one proposed by Dechter et al          but replaces optimization with probabilistic inference      Providing empirical evaluation that demonstrates the effectiveness of the idea of tree decomposition combined with partition based approximation for belief updating  Section   introduces the Iterative Join Graph Propagation  IJGP  algorithm  It operates on a general join graph decomposition that may contain cycles  It also provides a user adjustable i bound parameter that defines the maximum cluster size of the graph  and hence bounds the complexity   therefore it is both anytime and iterative  While the algorithm IBP is typically presented as a generalization of Pearls Belief Propagation algorithm  we show that IBP can be viewed as IJGP with the smallest i bound  We also provide insight into IJGPs behavior in Section    Zero beliefs are variable value pairs that have zero conditional probability given the evidence  We show that      if a value of a variable is assessed as having zero belief in any iteration of IJGP  it remains a zero belief in all subsequent iterations      IJGP converges in a finite number of iterations relative to its set of zero beliefs  and  most importantly     that the set of zero beliefs decided by any of the iterative belief propagation methods is sound  Namely any zero belief determined by IJGP corresponds to a true zero conditional probability relative to the given probability distribution expressed by the Bayesian network  Empirical results on various classes of problems are included in Section    shedding light on the performance of IJGP i   We see that it is often superior  or otherwise comparable  to other state of the art algorithms  The paper is based in part on earlier conference papers by Dechter  Kask  and Mateescu         Mateescu  Dechter  and Kask        and Dechter and Mateescu             Background In this section we provide background for exact and approximate probabilistic inference algorithms that form the basis of our work  While we present our algorithms in the context of directed probabilistic networks  they are applicable to any graphical model  including Markov networks      Preliminaries Notations  A reasoning problem is defined in terms of a set of variables taking values on finite domains and a set of functions defined over these variables  We denote variables or subsets of variables by uppercase letters  e g   X  Y  Z  S  R        and values of variables by lower case letters  e g   x  y  z  s   An assignment  X    x            Xn   xn   can be abbreviated as x    x            xn    For a subset of variables S  DS denotes the Cartesian product of the domains of variables in S  xS is the projection of x    x            xn   over a subset S  We denote functions by letters f   g  h  etc   and the scope  set of arguments  of the function f by scope f    D EFINITION    graphical model   Kask  Dechter  Larrosa    Dechter        A graphical model M is a   tuple  M   hX  D  Fi  where  X    X            Xn   is a finite set of variables  D    D            Dn   is the set of their respective finite domains of values  F    f            fr   is a set of positive real valued discrete functions  each defined over a subset of variables Si  X  called its scope  and denoted by scope f P i    A graphical model typically has an associated combination   operator    e g            product  sum   The graphical model represents the combination    The combination operator can also be defined axiomatically  Shenoy                M ATEESCU   K ASK   G OGATE   D ECHTER  of all its functions  ri   fi   A graphical model has an associated primal graph that captures the structural information of the model  D EFINITION    primal graph  dual graph  The primal graph of a graphical model is an undirected graph that has variables as its vertices and an edge connects any two vertices whose corresponding variables appear in the scope of the same function  A dual graph of a graphical model has a one to one mapping between its vertices and functions of the graphical model  Two vertices in the dual graph are connected if the corresponding functions in the graphical model share a variable  We denote the primal graph by G    X  E   where X is the set of variables and E is the set of edges  D EFINITION    belief networks  A belief  or Bayesian  network is a graphical model B   hX  D  G  P i  where G    X  E  is a directed acyclic graph over variables X and P    pi    where pi    p Xi   pa  Xi       are conditional probability tables  CPTs  associated with each variable Xi and pa Xi     scope pi   Xi   is the set of parents of Xi in G  Given a subset of variables S  we will write P  s  as the probability P  S   s   where s  DS   A belief network represents a probability distribution over X  P  x             xn     ni   P  xi  xpa Xi      An evidence set e is an instantiated subset of variables  The primal graph of a belief network is called a moral graph  It can be obtained by connecting the parents of each vertex in G and removing the directionality of the edges  Equivalently  it connects any two variables appearing in the same family  a variable and its parents in the CPT   Two common queries in Bayesian networks are Belief Updating  BU  and Most Probable Explanation  MPE   D EFINITION    belief network queries  The Belief Updating  BU  task is to find the posterior probability of each single variable given some evidence e  that is to compute P  Xi  e   The Most Probable Explanation  MPE  task is to find a complete assignment to all the variables having maximum probability given the evidence  that is to compute argmaxX i pi       Tree Decomposition Schemes Tree decomposition is at the heart of most general schemes for solving a wide range of automated reasoning problems  such as constraint satisfaction and probabilistic inference  It is the basis for many well known algorithms  such as join tree clustering and bucket elimination  In our presentation we will follow the terminology of Gottlob  Leone  and Scarcello        and Kask et al          D EFINITION    tree decomposition  cluster tree  Let B   hX  D  G  P i be a belief network  A tree decomposition for B is a triple hT    i  where T    V  E  is a tree  and  and  are labeling functions which associate with each vertex v  V two sets   v   X and  v   P satisfying     For each function pi  P   there is exactly one vertex v  V such that pi   v   and scope pi     v      For each variable Xi  X  the set  v  V  Xi   v   induces a connected subtree of T   This is also called the running intersection  or connectedness  property  We will often refer to a node and its functions as a cluster and use the term tree decomposition and cluster tree interchangeably        J OIN  G RAPH P ROPAGATION A LGORITHMS  D EFINITION    treewidth  separator  eliminator  Let D   hT    i be a tree decomposition of a belief network B  The treewidth  Arnborg        of D is maxvV   v       The treewidth of B is the minimum treewidth over all its tree decompositions  Given two adjacent vertices u and v of a tree decomposition  the separator of u and v is defined as sep u  v     u    v   and the eliminator of u with respect to v is elim u  v     u    v   The separator width of D is max u v   sep u  v    The minimum treewidth of a graph G can be shown to be identical to a related parameter called induced width  Dechter   Pearl         Join tree and cluster tree elimination  CTE  In both Bayesian network and constraint satisfaction communities  the most used tree decomposition method is join tree decomposition  Lauritzen   Spiegelhalter        Dechter   Pearl         introduced based on relational database concepts  Maier         Such decompositions can be generated by embedding the networks moral graph G into a chordal graph  often using a triangulation algorithm and using its maximal cliques as nodes in the join tree  The triangulation algorithm assembles a join tree by connecting the maximal cliques in the chordal graph in a tree  Subsequently  every CPT pi is placed in one clique containing its scope  Using the previous terminology  a join tree decomposition of a belief network B   hX  D  G  P i is   a tree T    V  E   where V is the set of cliques of a chordal graph G that contains G  and E is a set of edges that form a tree between cliques  satisfying the running intersection property  Maier         Such a join tree satisfies the properties of tree decomposition and is therefore a cluster tree  Kask et al          In this paper  we will use the terms tree decomposition and join tree decomposition interchangeably  There are a few variants for processing join trees for belief updating  e g   Jensen et al         Shafer   Shenoy         We adopt here the version from Kask et al          called cluster treeelimination  CTE   that is applicable to tree decompositions in general and is geared towards space savings  It is a message passing algorithm  for the task of belief updating  messages are computed by summation over the eliminator between the two clusters of the product of functions in the originating cluster  The algorithm  denoted CTE BU  see Figure     pays a special attention to the processing of observed variables since the presence of evidence is a central component in belief updating  When a cluster sends a message to a neighbor  the algorithm operates on all the functions in the cluster except the message from that particular neighbor  The message contains a single combined function and individual functions that do not share variables with the relevant eliminator  All the non individual functions are combined in a product and summed over the eliminator  Example   Figure  a describes a belief network and Figure  b a join tree decomposition for it  Figure  c shows the trace of running CTE BU with evidence G   ge   where h u v  is a message that cluster u sends to cluster v  T HEOREM    complexity of CTE BU   Dechter et al         Kask et al         Given a Bayesian network B   hX  D  G  P i and a tree decomposition hT    i of B  the time complexity of CTE BU is O deg   n   N    dw      and the space complexity is O N  dsep    where deg is the maximum degree of a node in the tree decomposition  n is the number of variables  N is the number of nodes in the tree decomposition  d is the maximum domain size of a variable  w is the treewidth and sep is the maximum separator size         M ATEESCU   K ASK   G OGATE   D ECHTER  Algorithm CTE for Belief Updating  CTE BU  Input  A tree decomposition hT    i  T    V  E  for B   hX  D  G  P i  Evidence variables var e   Output  An augmented tree whose nodes are clusters containing the original CPTs and the messages received from neighbors  P  Xi   e   Xi  X  Denote by H u v  the message from vertex u to v  nev  u  the neighbors of u in T excluding v  cluster u     u    H v u    v  u   E   clusterv  u    cluster u  excluding message from v to u    Compute messages  For every node u in T   once u has received messages from all nev  u   compute message to node v     Process observed variables  Assign relevant evidence to all pi   u     Compute the combined function  X  h u v     Y  f  elim u v  f A  where A is the set of functions in clusterv  u  whose scope intersects elim u  v   Add h u v  to H u v  and add all the individual functions in clusterv  u   A Send H u v  to node v   Compute P  Xi   e   For every Xi  X let u be a vertex in T such that Xi   u   Compute P  Xi   e    P Q  u  Xi     f cluster u  f    Figure    Algorithm Cluster Tree Elimination for Belief Updating  CTE BU   A              A  B  C           p a    p b   a    p c   a  b       ABC BC  B    C  D            B   C   D   F            p d   b   p  f   c  d      BCDF  E  BF             B  E   F            p  e   b  f                E   F   G           p  g   e  f        F G   a   BEF EF      b   EFG  h            b   c        b   c      a  h           p   a    p  b   a    p  c   a   b   p   d   b    p   f   c   d    h          b   f    d f  h          b   f       h            b   f      e  p  d   b   h            e   f      p   e   b   f    h          b   f    c  d    p   f   c  d    h            b   c    p   e   b   f    h           e   f    b  h           e   f     p   G   g e   e   f     c   Figure     a  A belief network   b  A join tree decomposition   c  Execution of CTE BU      Partition Based Mini Clustering The time  and especially the space complexity  of CTE BU renders the algorithm infeasible for problems with large treewidth  We now introduce Mini Clustering  a partition based anytime algorithm which computes bounds or approximate values on P  Xi   e  for every variable Xi         J OIN  G RAPH P ROPAGATION A LGORITHMS  Procedure MC for Belief Updating  MC BU i      Compute the combined mini functions  Make an  i  size mini cluster partitioning of clusterv  u    mc             mc p    P Q h  u v    elim u v  f mc    f Q hi u v    maxelim u v  f mc i  f i              p add  hi u v   i              p  to H u v    Send H u v  to v  Compute upper bounds P  Xi   e  on P  Xi   e   For every Xi  X let u  V be a cluster such that Xi   u   Make  i  mini clusters from cluster u    mc             mc p    Compute P  Xi   e    P Q Qp Q    u Xi f mc    f      k   max u Xi f mc k  f     Figure    Procedure Mini Clustering for Belief Updating  MC BU       Mini Clustering Algorithm Combining all the functions of a cluster into a product has a complexity exponential in its number of variables  which is upper bounded by the induced width  Similar to the mini bucket scheme  Dechter         rather than performing this expensive exact computation  we partition the cluster into p mini clusters mc             mc p   each having at most Pi variables  Q where i is an accuracy parameter  Instead of computing by CTE BU h u v    elim u v  f  u  f   we can divide the functions of  u  mc k   k              p   and rewrite h u v    P into p mini clusters Qp Q P Q f   mc k  f   By migrating the summation operator into elim u v  elim u v  f  u  Q P k   fQ p each mini cluster  yielding k   elim u v  f mc k  f   we get an upper bound on h u v    The resulting algorithm is called MC BU i   Consequently  the combined functions are approximated via mini clusters  as follows  Suppose u  V has received messages from all its neighbors other than v  the message from v is ignored even if received   The functions in clusterv  u  that are to be combined are partitioned into mini clusters  mc             mc p    each one containing at most i variables  Each mini cluster is processed by summation over the eliminator  and the resulting combined functions as well as all the individual functions are sent to v  It was shown by Dechter and Rish        that the upper bound can be improved by using the maximization operator max rather than the summation operator sum on some mini buckets  Similarly  lower bounds can be generated by replacing sum with min  minimization  for some mini buckets  Alternatively  we can replace sum by a mean operator  taking the sum and dividing by the number of elements in the sum   in this case deriving an approximation of the joint belief instead of a strict upper bound  Algorithm MC BU for upper bounds can be obtained from CTE BU by replacing step   of the main loop and the final part of computing the upper bounds on the joint belief by the procedure given in Figure    In the implementation we used for the experiments reported here  the partitioning was done in a greedy brute force manner  We ordered the functions according to their sizes  number of variables   breaking ties arbitrarily  The largest function was placed in a mini cluster by itself  Then  we picked the largest remaining function and probed the mini clusters in the order of their creation        M ATEESCU   K ASK   G OGATE   D ECHTER     ABC  BC  H         h          b  c      p  a    p  b   a    p c   a  b  a            h  H          b      p  d   b   h          b  f   d  f  h        c     max p   f   c  d   d  f     BCDF             h  H          BF     BEF  EF  c  d  h         f      max p  f   c  d   c d               b  f       p e   b  f    h        e  f    H           h  H           h          e  f       p  e   b  f    h        b   h         f    H               b      p  d   b   h          b  c   e  b              h   e  f      p G   g e   e  f    EFG  Figure    Execution of MC BU for i      trying to find one that together with the new function would have no more than i variables  A new mini cluster was created whenever the existing ones could not accommodate the new function  Example   Figure   shows the trace of running MC BU    on the problem in Figure    First  evidence G   ge is assigned in all CPTs  There are no individual functions to be sent from cluster   to cluster    Cluster   contains only   variables         A  B  C   therefore it is not partitioned  P p a   p b a   p c a  b  is computed and the message The combined function h        b  c    a   H         h       b  c   is sent to node    Now  node   can send its message to node    Again  there are no individual functions  Cluster   contains   variables         B  C  D  F    and a partitioning is necessary  MC BU    can choose P mc       p d b   h       b  c   and mc       p f  c  d      The combined functions h       b    c d p d b   h       b  c  and h       f     maxc d p f  c  d  are computed and the message H         h        b   h        f    is sent to node    The algorithm continues until every node has received messages from all its neighbors  An upper bound on p a  G   ge   can now be computed by choosing cluster    which contains variable A  It doesnt need partitionP ing  so the algorithm just computes b c p a   p b a   p c a  b   h        b   h        c   Notice that unlike CTE BU which processes   variables in cluster    MC BU    never processes more than   variables at a time  It was already shown that  T HEOREM    Dechter   Rish        Given a Bayesian network B   hX  D  G  P i and the evidence e  the algorithm MC BU i  computes an upper bound on the joint probability P  Xi   e  of each variable Xi  and each of its values  and the evidence e  T HEOREM    complexity of MC BU i    Dechter et al         Given a Bayesian network B   hX  D  G  P i and a tree decomposition hT    i of B  the time and space complexity of MC BU i  is O n  hw  di    where n is the number of variables  d is the maximum domain size of a variable and hw   maxuT   f  P  scope f     u         which bounds the number of mini clusters        J OIN  G RAPH P ROPAGATION A LGORITHMS                                                                                                                                                                                                                                                                                    Figure    Node duplication semantics of MC   a  trace of MC BU      b  trace of CTE BU  Semantics of Mini Clustering The mini bucket scheme was shown to have the semantics of relaxation via node duplication  Kask   Dechter        Choi  Chavira    Darwiche         We extend it to mini clustering by showing how it can apply as is to messages that flow in one direction  inward  from leaves to root   as follows  Given a tree decomposition D  where CTE BU computes a function h u v   the message that cluster u sends to cluster v   MC BU i  partitions cluster u into p mini clusters u            up   which are processed independently and then the resulting functions h ui  v  are sent to v  Instead consider a different decomposition D    which is just like D  with the exception that  a  instead of u  it has clusters u            up   all of which are children of v  and each variable appearing in more than a single mini cluster becomes a new variable   b  each child w of u  in D  is a child of uk  in D     such that h w u   in D  is assigned to uk  in D    during the partitioning  Note that D  is not a legal tree decomposition relative to the original variables since it violates the connectedness property  the mini clusters u            up contain variables elim u  v  but the path between the nodes u            up  this path goes through v  does not  However  it is a legal tree decomposition relative to the new variables  It is straightforward to see that H u v  computed by MC BU i  on D is the same as  h ui  v   i              p  computed by CTE BU on D  in the direction from leaves to root  If we want to capture the semantics of the outward messages from root to leaves  we need to generate a different relaxed decomposition  D     because MC  as defined  allows a different partitioning in the up and down streams of the same cluster  We could of course stick with the decomposition in D  and use CTE in both directions which would lead to another variant of mini clustering  Example   Figure   a  shows a trace of the bottom up phase of MC BU    on the network in Figure    Figure   b  shows a trace of the bottom up phase of CTE BU algorithm on a problem obtained from the problem in Figure   by splitting nodes D  into D  and D     and F  into F   and F       The MC BU algorithm computes an upper bound P  Xi   e  on the joint probability P  Xi   e   However  deriving a bound on the conditional probability P  Xi  e  is not easy when the exact       M ATEESCU   K ASK   G OGATE   D ECHTER  Random Bayesian N    K   P   C                    Avg abs error                       ev    ev     ev     ev                                                                               Number of iterations  Figure    Convergence of IBP     variables  evidence from      variables   value of P  e  is not available  If we just try to divide  multiply  P  Xi   e  by a constant  the result is not P necessarily an upper bound on P  Xi  e   It is easy to show that normalization  P  xi   e   xi Di P  xi   e   with the mean operator is identical to normalization of MC BU output when applying the summation operator in all the mini clusters  MC BU i  is an improvement over the Mini Bucket algorithm MB i   in that it allows the computation of P  Xi   e  for all variables with a single run  whereas MB i  computes P  Xi   e  for just one variable  with a single run  When computing P  Xi   e  for each variable  MB i  has to be run n times  once for each variable  an algorithm we call nMB i   It was demonstrated by Mateescu et al         that MC BU i  has up to linear speed up over nMB i   For a given i  the accuracy of MC BU i  can be shown to be not worse than that of nMB i       Experimental Evaluation of Mini Clustering The work of Mateescu et al         and Kask        provides an empirical evaluation of MC BU that reveals the impact of the accuracy parameter on its quality of approximation and compares with Iterative Belief Propagation and a Gibbs sampling scheme  We will include here only a subset of these experiments which will provide the essence of our results  Additional empirical evaluation of MC BU will be given when comparing against IJGP later in this paper  We tested the performance of MC BU i  on random Noisy OR networks  random coding networks  general random networks  grid networks  and three benchmark CPCS files with         and     variables respectively  these are belief networks for medicine  derived from the Computer based Patient Case Simulation system  known to be hard for belief updating   On each type of network we ran Iterative Belief Propagation  IBP    set to run at most    iterations  Gibbs Sampling  GS  and MC BU i   with i from   to the treewidth w to capture the anytime behavior of MC BU i   The random networks were generated using parameters  N K C P   where N is the number of variables  K is their domain size  we used only K     C is the number of conditional probability tables and P is the number of parents in each conditional probability table  The parents in each table are picked randomly given a topological ordering  and the conditional probability tables are filled       J OIN  G RAPH P ROPAGATION A LGORITHMS     e         NHD max  IBP  MC BU     MC BU     MC BU                        mean                          N     P       instances Abs  Error max     E       E       E       E       E       E       E       E       E     mean    E       E       E       E       E       E       E       E       E       E       E       E     Rel  Error  max     E       E       E       E       E       E       E       E       E     mean    E       E       E       E       E       E       E       E       E       E       E       E     Time max                                                         mean                                                                          Table    Performance on Noisy OR networks  w       Normalized Hamming Distance  absolute error  relative error and time   randomly  The grid networks have the structure of a square  with edges directed to form a diagonal flow  all parallel edges have the same direction   They were generated by specifying N  a square integer  and K  we used K     We also varied the number of evidence nodes  denoted by  e  in the tables  The parameter values are reported in each table  For all the problems  Gibbs sampling performed consistently poorly so we only include part of its results here  In our experiments we focused on the approximation power of MC BU i   We compared two versions of the algorithm  In the first version  for every cluster  we used the max operator in all its mini clusters  except for one of them that was processed by summation  In the second version  we used the operator mean in all the mini clusters  We investigated this second version of the algorithm for two reasons      we compare MC BU i  with IBP and Gibbs sampling  both of which are also approximation algorithms  so it would not be possible to compare with a bounding scheme      we observed in our experiments that  although the bounds improve as the i bound increases  the quality of bounds computed by MC BU i  was still poor  with upper bounds being greater than   in many cases   Notice that we need to maintain the sum operator for at least one of the mini clusters  The mean operator simply performs summation and divides by the number of elements in the sum  For example  if A  B  C are binary variables  taking values   and     and f  A  B  C  is the aggregated function of one mini cluster  and elim    A  B   then computing the message h C  by the mean P operator gives      A B      f  A  B  C   We computed the exact solution and used three different measures of accuracy     Normalized Hamming Distance  NHD    we picked the most likely value for each variable for the approximate and for the exact  took the ratio between the number of disagreements and the total number of variables  and averaged over the number of problems that we ran for each class     Absolute Error  Abs  Error    is the absolute value of the difference between the approximate and the exact  averaged over all values  for each variable   all variables and all problems     Relative Error  Rel  Error    is the absolute value of the difference between the approximate and the exact  divided by the exact  averaged over all values  for each variable   all variables and all problems  For coding networks     Wexler and Meek        compared the upper lower bounding properties of the mini bucket on computing probability of evidence  Rollon and Dechter        further investigated heuristic schemes for mini bucket partitioning         M ATEESCU   K ASK   G OGATE   D ECHTER      e         N     P       instances Abs  Error  NHD max  mean                                      IBP                                MC BU     MC BU     MC BU     MC BU      MC BU      max     E       E       E       E       E       E       E       E       E       E       E       E       E         mean    E       E       E       E       E       E       E       E       E       E       E       E       E       E       E       E         Rel  Error max  Time  mean    E       E       E       E       E       E       E       E       E       E       E       E       E       E       E       E            E       E       E       E       E       E       E       E       E       E       E       E       E         max  mean                                                                                                                                                                                                         Table    Performance on Noisy OR networks  w       Normalized Hamming Distance  absolute error  relative error and time  Noisy OR networks  N     P    evid     w         instances  Noisy OR networks  N     P    evid     w         instances   e     e    MC IBP Gibbs Sampling  MC IBP Gibbs Sampling   e    Absolute error  Absolute error   e     e     e     e     e     e     e     e     e                                                                 i bound  i bound  Figure    Absolute error for Noisy OR networks  we report only one measure  Bit Error Rate  BER   In terms of the measures defined above  BER is the normalized Hamming distance between the approximate  computed by an algorithm  and the actual input  which in the case of coding networks may be different from the solution given by exact algorithms   so we denote them differently to make this semantic distinction  We also report the time taken by each algorithm  For reported metrics  time  error  etc   provided in the Tables  we give both mean and max values  In Figure   we show that IBP converges after about   iterations  So  while in our experiments we report its time for    iterations  its time is even better when sophisticated termination is used  These results are typical of all runs         J OIN  G RAPH P ROPAGATION A LGORITHMS  Random networks  N     P    k    evid    w         instances  Random networks  N     P    k    evid     w         instances              MC Gibbs Sampling IBP              MC Gibbs Sampling IBP              Absolute error  Absolute error                                                                                                 i bound            i bound  Figure    Absolute error for random networks  BER         max mean  IBP GS MC BU    MC BU    MC BU    MC BU                                                                               IBP GS MC BU    MC BU    MC BU    MC BU    MC BU                                                                                                                 max mean max mean max mean N      P       instances  w                                                                                                                                                                                                                            N      P       instances  w                                                                                                                                                                                                                                                                         max mean  Time                                                                                                                                                                                                                                       Table    Bit Error Rate  BER  for coding networks   Random Noisy OR networks results are summarized in Tables   and    and Figure    For NHD  both IBP and MC BU gave perfect results  For the other measures  we noticed that IBP is more accurate when there is no evidence by about an order of magnitude  However  as evidence is added  IBPs accuracy decreases  while MC BUs increases and they give similar results  We see that MC BU gets better as the accuracy parameter i increases  which shows its anytime behavior  General random networks results are summarized in Figure    They are similar to those for random Noisy OR networks  Again  IBP has the best result only when the number of evidence variables is small  It is remarkable how quickly MC BU surpasses the performance of IBP as evidence is added  for more  see the results of Mateescu et al          Random coding networks results are given in Table   and Figure    The instances fall within the class of linear block codes    is the channel noise level   It is known that IBP is very accurate for this class  Indeed  these are the only problems that we experimented with where IBP outperformed MC BU throughout  The anytime behavior of MC BU can again be seen in the variation of numbers in each column and more vividly in Figure          M ATEESCU   K ASK   G OGATE   D ECHTER  Coding networks  N      P    sigma      w         instances  Coding networks  N      P    sigma      w         instances              MC IBP         MC IBP               Bit Error Rate  Bit Error Rate                                                                                                            i bound  i bound  Figure    Bit Error Rate  BER  for coding networks  Grid   x    evid     w         instances  Grid   x    evid     w         instances           MC IBP        MC IBP      Time  seconds   Absolute error                                                                                           i bound                         i bound  Figure     Grid   x    absolute error and time  Grid networks results are given in Figure     We notice that IBP is more accurate for no evidence and MC BU is better as more evidence is added  The same behavior was consistently manifested for smaller grid networks that we experimented with  from  x  up to   x     CPCS networks results We also tested on three CPCS benchmark files  The results are given in Figure     It is interesting to notice that the MC BU scheme scales up to fairly large networks  like the real life example of CPCS     induced width      IBP is again more accurate when there is no evidence  but is surpassed by MC BU when evidence is added  However  whereas MC BU is competitive with IBP time wise when i bound is small  its runtime grows rapidly as i bound increases  For more details on all these benchmarks see the results of Mateescu et al          Summary Our results show that  as expected  IBP is superior to all other approximations for coding networks  However  for random Noisy OR  general random  grid networks and the CPCS networks  in the presence of evidence  the mini clustering scheme is often superior even in its weakest form  The empirical results are particularly encouraging as we use an un optimized scheme that exploits a universal principle applicable to many reasoning tasks         J OIN  G RAPH P ROPAGATION A LGORITHMS  CPCS      evid    w        instance  CPCS      evid     w        instance             MC IBP  MC IBP        Absolute error  Absolute error                                                                                                i bound                         i bound  Figure     Absolute error for CPCS         Join Graph Decomposition and Propagation In this section we introduce algorithm Iterative Join Graph Propagation  IJGP  which  like miniclustering  is designed to benefit from bounded inference  but also exploit iterative message passing as used by IBP  Algorithm IJGP can be viewed as an iterative version of mini clustering  improving the quality of approximation  especially for low i bounds  Given a cluster of the decomposition  mini clustering can potentially create a different partitioning for every message sent to a neighbor  This dynamic partitioning can happen because the incoming message from each neighbor has to be excluded when realizing the partitioning  so a different set of functions are split into mini clusters for every message to a neighbor  We can define a version of mini clustering where for every cluster we create a unique static partition into mini clusters such that every incoming message can be included into one of the mini clusters  This version of MC can be extended into IJGP by introducing some links between mini clusters of the same cluster  and carefully limiting the interaction between the resulting nodes in order to eliminate over counting  Algorithm IJGP works on a general join graph that may contain cycles  The cluster size of the graph is user adjustable via the i bound  providing the anytime nature   and the cycles in the graph allow the iterative application of message passing  In Subsection     we introduce join graphs and discuss their properties  In Subsection     we describe the IJGP algorithm itself      Join Graphs D EFINITION    join graph decomposition  A join graph decomposition for a belief network B   hX  D  G  P i is a triple D   hJG    i  where JG    V  E  is a graph  and  and  are labeling functions which associate with each vertex v  V two sets   v   X and  v   P such that     For each pi  P   there is exactly one vertex v  V such that pi   v   and scope pi     v       connectedness  For each variable Xi  X  the set  v  V  Xi   v   induces a connected subgraph of JG  The connectedness requirement is also called the running intersection property         M ATEESCU   K ASK   G OGATE   D ECHTER                     A  C             A       B              C             B  a            b   Figure     An edge labeled decomposition  We will often refer to a node in V and its CPT functions as a cluster  and use the term joingraph decomposition and cluster graph interchangeably  Clearly  a join tree decomposition or a cluster tree is the special case when the join graph D is a tree  It is clear that one of the problems of message propagation over cyclic join graphs is overcounting  To reduce this problem we devise a scheme  which avoids cyclicity with respect to any single variable  The algorithm works on edge labeled join graphs  D EFINITION    minimal edge labeled join graph decompositions  An edge labeled join graph decomposition for B   hX  D  G  P i is a four tuple D   hJG      i  where JG    V  E  is a graph   and  associate with each vertex v  V the sets  v   X and  v   P and  associates with each edge  v  u   E the set   v  u    X such that     For each function pi  P   there is exactly one vertex v  V such that pi   v   and scope pi     v       edge connectedness  For each edge  u  v     u  v     u    v   such that Xi  X  any two clusters containing Xi can be connected by a path whose every edge label includes Xi   Finally  an edge labeled join graph is minimal if no variable can be deleted from any label while still satisfying the edge connectedness property  D EFINITION    separator  eliminator of edge labeled join graphs  Given two adjacent vertices u and v of JG  the separator of u and v is defined as sep u  v      u  v    and the eliminator of u with respect to v is elim u  v     u     u  v    The separator width is max u v   sep u  v    Edge labeled join graphs can be made label minimal by deleting variables from the labels while maintaining connectedness  if an edge label becomes empty  the edge can be deleted   It is easy to see that  Proposition   A minimal edge labeled join graph does not contain any cycle relative to any single variable  That is  any two clusters containing the same variable are connected by exactly one path labeled with that variable  Notice that every minimal edge labeled join graph is edge minimal  no edge can be deleted   but not vice versa     Note that a node may be associated with an empty set of CPTs         J OIN  G RAPH P ROPAGATION A LGORITHMS  Example   The example in Figure   a shows an edge minimal join graph which contains a cycle relative to variable    with edges labeled with separators  Notice however that if we remove variable   from the label of one edge we will have no cycles  relative to single variables  while the connectedness property is still maintained  The mini clustering approximation presented in the previous section works by relaxing the jointree requirement of exact inference into a collection of join trees having smaller cluster size  It introduces some independencies in the original problem via node duplication and applies exact inference on the relaxed model requiring only two message passings  For the class of IJGP algorithms we take a different route  We choose to relax the tree structure requirement and use join graphs which do not introduce any new independencies  and apply iterative message passing on the resulting cyclic structure  Indeed  it can be shown that any join graph of a belief network is an I map  independency map  Pearl        of the underlying probability distribution relative to node separation  Since we plan to use minimally edge labeled join graphs to address over counting problems  the question is what kind of independencies are captured by such graphs  D EFINITION     edge separation in edge labeled join graphs  Let D   hJG      i  JG    V  E  be an edge labeled decomposition of a Bayesian network B   hX  D  G  P i  Let NW   NY  V be two sets of nodes  and EZ  E be a set of edges in JG  Let W  Y  Z be their corresponding sets of variables  W   vNW  v   Z   eEZ  e    We say that EZ edge separates NW and NY in D if there is no path between NW and NY in the JG graph whose edges in EZ are removed  In this case we also say that W is separated from Y given Z in D  and write hW  Z Y iD   Edgeseparation in a regular join graph is defined relative to its separators  T HEOREM   Any edge labeled join graph decomposition D   hJG      i of a belief network B   hX  D  G  P i is an I map of P relative to edge separation  Namely  any edge separation in D corresponds to conditional independence in P   Proof  Let M G be the moral graph of BN   Since M G is an I map of P   it is enough to prove that JG is an I map of M G  Let NW and NY be disjoint sets of nodes and NZ be a set of edges in JG  and W  Z  Y be their corresponding sets of variables in M G  We will prove  hNW  NZ  NY iJG   hW  Z Y iM G by contradiction  Since the sets W  Z  Y may not be disjoint  we will actually prove that hW  Z Z Y  ZiM G holds  this being equivalent to hW  Z Y iM G   Supposing hW  Z Z Y  ZiM G is false  then there exists a path                    n       n in M G that goes from some variable       W  Z to some variable    n  Y  Z without intersecting variables in Z  Let Nv be the set of all nodes in JG that contain variable v  X  and let us consider the set of nodes  S   ni   Ni  NZ We argue that S forms a connected sub graph in JG  First  the running intersection property ensures that every Ni   i              n  remains connected in JG after removing the nodes in NZ  otherwise  it must be that there was a path between the two disconnected parts in the original JG  which implies that a i is part of Z  which is a contradiction   Second  the fact that  i   i      i         M ATEESCU   K ASK   G OGATE   D ECHTER             n     is an edge in the moral graph M G implies that there is a conditional probability table  CPT  on both i and i     i              n     and perhaps other variables   From property   of the definition of the join graph  it follows that for all i              n    there exists a node in JG that contains both i and i     This proves the existence of a path in the mutilated join graph  JG with NZ pulled out  from a node in NW containing      to the node containing both   and    N  is connected   then from that node to the one containing both   and    N  is connected   and so on until we reach a node in NY containing    n   This shows that hNW  NZ  NY iJG is false  concluding the proof by contradiction    Interestingly however  deleting variables from edge labels or removing edges from edge labeled join graphs whose clusters are fixed will not increase the independencies captured by edge labeled join graphs  That is  Proposition   Any two  edge labeled  join graphs defined on the same set of clusters  sharing  V        express exactly the same set of independencies relative to edge separation  and this set of independencies is identical to the one expressed by node separation in the primal graph of the join graph  Proof  This follows by looking at the primal graph of the join graph  obtained by connecting any two nodes in a cluster by an arc over the original variables as nodes  and observing that any edgeseparation in a join graph corresponds to a node separation in the primal graph and vice versa    Hence  the issue of minimizing computational over counting due to cycles appears to be unrelated to the problem of maximizing independencies via minimal I mapness  Nevertheless  to avoid over counting as much as possible  we still prefer join graphs that minimize cycles relative to each variable  That is  we prefer minimal edge labeled join graphs  Relationship with region graphs There is a strong relationship between our join graphs and the region graphs of Yedidia et al                      Their approach was inspired by advances in statistical physics  when it was realized that computing the partition function is essentially the same combinatorial problem that expresses probabilistic reasoning  As a result  variational methods from physics could have counterparts in reasoning algorithms  It was proved by Yedidia et al               that belief propagation on loopy networks can only converge  when it does so  to stationary points of the Bethe free energy  The Bethe approximation is only the simplest case of the more general Kikuchi        cluster variational method  The idea is to group the variables together in clusters and perform exact computation in each cluster  One key question is then how to aggregate the results  and how to account for the variables that are shared between clusters  Again  the idea that everything should be counted exactly once is very important  This led to the proposal of region graphs  Yedidia et al               and the associated counting numbers for regions  They are given as a possible canonical version of graphs that can support Generalized Belief Propagation  GBP  algorithms  The join graphs accomplish the same thing  The edge labeled join graphs can be described as region graphs where the regions are the clusters and the labels on the edges  The tree ness condition with respect to every variable ensures that there is no over counting  A very similar approach to ours  which is also based on join graphs appeared independently by McEliece and Yildirim         and it is based on an information theoretic perspective         J OIN  G RAPH P ROPAGATION A LGORITHMS  Algorithm Iterative Join Graph Propagation  IJGP  Input An arc labeled join graph decomposition hJG      i  JG    V  E  for B   hX  D  G  P i  Evidence variables var e   Output An augmented graph whose nodes are clusters containing the original CPTs and the messages received from neighbors  Approximations of P  Xi  e   Xi  X  Denote by h u v  the message from vertex u to v  nev  u  the neighbors of u in JG excluding v  cluster u     u    h v u    v  u   E   clusterv  u    cluster u  excluding message from v to u   One iteration of IJGP  For every node u in JG in some topological order d and back  do    Process observed variables  Assign relevant evidence to all pi   u   u      u   var e   u  V    Compute individual functions  Include in H u v  each function in clusterv  u  whose scope does not contain variables in elim u  v   Denote by A the remaining functions  P Q    Compute and send to v the combined function  h u v    elim u v  f A f   Send h u v  and the individual functions H u v  to node v  Endfor  Compute an approximation of P  Xi  e   For every Xi  X let u be P a vertex in JG Q such that Xi   u   Compute P  Xi   e      u  Xi     f cluster u  f    Figure     Algorithm Iterative Join Graph Propagation  IJGP       Algorithm IJGP Applying CTE iteratively to minimal edge labeled join graphs yields our algorithm Iterative JoinGraph Propagation  IJGP  described in Figure     One iteration of the algorithm applies messagepassing in a topological order over the join graph  forward and back  When node u sends a message  or messages  to a neighbor node v it operates on all the CPTs in its cluster and on all the messages sent from its neighbors excluding the ones received from v  First  all individual functions that share no variables with the eliminator are collected and sent to v  All the rest of the functions are combined in a product and summed over the eliminator between u and v  Based on the results by Lauritzen and Spiegelhalter        and Larrosa  Kask  and Dechter        it can be shown that  T HEOREM      If IJGP is applied to a join tree decomposition it reduces to join tree clustering  and therefore it is guaranteed to compute the exact beliefs in one iteration      The time complexity of one iteration of IJGP is O deg   n   N    dw      and its space complexity is O N  d    where deg is the maximum degree of a node in the join graph  n is the number of variables  N is the number of nodes in the graph decomposition  d is the maximum domain size  w is the maximum cluster size and  is the maximum label size  For proof  see the properties of CTE presented by Kask et al                 M ATEESCU   K ASK   G OGATE   D ECHTER     A   B  C  a   A  AB     A A  B  b      ABC    AB  A  A    AB  ABC  c   Figure     a  A belief network  b  A dual join graph with singleton labels  c  A dual join graph which is a join tree   The special case of Iterative Belief Propagation Iterative belief propagation  IBP  is an iterative application of Pearls algorithm that was defined for poly trees  Pearl         to any Bayesian network  We will describe IBP as an instance of join graph propagation over a dual join graph  D EFINITION     dual graphs  dual join graphs  Given a set of functions F    f            fl   over scopes S            Sl   the dual graph of F is a graph DG    V  E  L  that associates a node with each function  namely V   F and an edge connects any two nodes whose functions scope share a variable  E     fi   fj   Si  Sj       L is a set of labels for the edges  each edge being labeled by the shared variables of its nodes  L    lij   Si  Sj   i  j   E   A dual join graph is an edgelabeled edge subgraph of DG that satisfies the connectedness property  A minimal dual join graph is a dual join graph for which none of the edge labels can be further reduced while maintaining the connectedness property  Interestingly  there may be many minimal dual join graphs of the same dual graph  We will define Iterative Belief Propagation on any dual join graph  Each node sends a message over an edge whose scope is identical to the label on that edge  Since Pearls algorithm sends messages whose scopes are singleton variables only  we highlight minimal singleton label dual join graphs  Proposition   Any Bayesian network has a minimal dual join graph where each edge is labeled by a single variable  Proof  Consider a topological ordering of the nodes in the acyclic directed graph of the Bayesian network d   X            Xn   We define the following dual join graph  Every node in the dual graph D  associated with pi is connected to node pj   j   i if Xj  pa Xi    We label the edge between pj and pi by variable Xj   namely lij    Xj    It is easy to see that the resulting edge labeled subgraph of the dual graph satisfies connectedness   Take the original acyclic graph G and add to each node its CPT family  namely all the other parents that precede it in the ordering  Since G already satisfies connectedness so is the minimal graph generated   The resulting labeled graph is a dual graph with singleton labels     Example   Consider the belief network on   variables A  B  C with CPTs   P  C A  B     P  B A  and   P  A   given in Figure   a  Figure   b shows a dual graph with singleton labels on the edges  Figure   c shows a dual graph which is a join tree  on which belief propagation can solve the problem exactly in one iteration  two passes up and down the tree          J OIN  G RAPH P ROPAGATION A LGORITHMS  Algorithm Iterative Belief Propagation  IBP  Input  An edge labeled dual join graph DG    V  E  L  for a Bayesian network B   hX  D  G  P i  Evidence e  Output  An augmented graph whose nodes include the original CPTs and the messages received from neighbors  Approximations of P  Xi  e   Xi  X  Approximations of P  Fi  e   Fi  B  Denote by  hvu the message from u to v  ne u  the neighbors of u in V   nev  u    ne u    v   luv the label of  u  v   E  elim u  v    scope u   scope v    One iteration of IBP For every node u in DJ in a topological order and back  do     Process observed variables Assign evidence variables to the each pi and remove them from the labeled edges     Compute and send to v the function  X Y hvu    pu  hui   elim u v    hu i  inev  u    Endfor  Compute approximations of P  Fi  e   P  Xi  e   For every Xi QX let u be the vertex of family Fi in DJ  P  Fi   e      hu  une i  hui    pu   P i P  Xi   e     scope u  Xi   P  Fi   e    Figure     Algorithm Iterative Belief Propagation  IBP   For completeness  we present algorithm IBP  which is a special case of IJGP  in Figure     It is easy to see that one iteration of IBP is time and space linear in the size of the belief network  It can be shown that when IBP is applied to a minimal singleton labeled dual graph it coincides with Pearls belief propagation applied directly to the acyclic graph representation  Also  when the dual join graph is a tree IBP converges after one iteration  two passes  up and down the tree  to the exact beliefs      Bounded Join Graph Decompositions Since we want to control the complexity of join graph algorithms  we will define it on decompositions having bounded cluster size  If the number of variables in a cluster is bounded by i  the time and space complexity of processing one cluster is exponential in i  Given a join graph decomposition D   hJG      i  the accuracy and complexity of the  iterative  join graph propagation algorithm depends on two different width parameters  defined next  D EFINITION     external and internal widths  Given an edge labeled join graph decomposition D   hJG      i of a network B   hX  D  G  P i  the internal width of D is maxvV   v    while the external width of D is the treewidth of JG as a graph  Using this terminology we can now state our target decomposition more clearly  Given a graph G  and a bounding parameter i we wish to find a join graph decomposition D of G whose internal width is bounded by i and whose external width is minimized  The bound i controls the complexity of join graph processing while the external width provides some measure of its accuracy and speed of convergence  because it measures how close the join graph is to a join tree        M ATEESCU   K ASK   G OGATE   D ECHTER  Algorithm Join Graph Structuring i     Apply procedure schematic mini bucket i      Associate each resulting mini bucket with a node in the join graph  the variables of the nodes are those appearing in the mini bucket  the original functions are those in the minibucket     Keep the edges created by the procedure  called out edges  and label them by the regular separator     Connect the mini bucket clusters belonging to the same bucket in a chain by in edges labeled by the single variable of the bucket   Figure     Algorithm Join Graph Structuring i   Procedure Schematic Mini Bucket i     Order the variables from X  to Xn minimizing  heuristically  induced width  and associate a bucket for each variable     Place each CPT in the bucket of the highest index variable in its scope     For j   n to   do  Partition the functions in bucket Xj   into mini buckets having at most i variables  For each mini bucket mb create a new scope function  message  f where scope f      X X  mb    Xi   and place scope f  in the bucket of its highest variable  Maintain an edge between mb and the mini bucket  created later  of f    Figure     Procedure Schematic Mini Bucket i   We can consider two classes of algorithms  One class is partition based  It starts from a given tree decomposition and then partitions the clusters until the decomposition has clusters bounded by i  An alternative approach is grouping based  It starts from a minimal dual graph based join graph decomposition  where each cluster contains a single CPT  and groups clusters into larger clusters as long as the resulting clusters do not exceed the given bound  In both methods one should attempt to reduce the external width of the generated graph decomposition  Our partition based approach inspired by the mini bucket idea  Dechter   Rish        is as follows  Given a bound i  algorithm Join Graph Structuring i  applies the procedure Schematic MiniBucket i   described in Figure     The procedure only traces the scopes of the functions that would be generated by the full mini bucket procedure  avoiding actual computation  The procedure ends with a collection of mini bucket trees  each rooted in the mini bucket of the first variable  Each of these trees is minimally edge labeled  Then  in edges labeled with only one variable are introduced  and they are added only to obtain the running intersection property between branches of these trees  Proposition   Algorithm Join Graph Structuring i  generates a minimal edge labeled join graph decomposition having bound i  Proof  The construction of the join graph specifies the vertices and edges of the join graph  as well as the variable and function labels of each vertex  We need to demonstrate that    the connectedness property holds  and    that edge labels are minimal         J OIN  G RAPH P ROPAGATION A LGORITHMS  G   GFE   GFE  P G F E  EF  E   EBF    EF   EBF  P E B F   P F C D   F   FCD    BF   BF F  FCD  BF  CD  D   DB    CD   P D B   CDB CB  C   CAB   CB   P C A B   B CAB BA  B   BA    AB   A   A    A    B   P B A   BA A P A   A   b    a   Figure     Join graph decompositions  Connectedness property specifies that for any   vertices u and v  if vertices u and v contain variable X  then there must be a path u  w            wm   v between u and v such that every vertex on this path contains variable X  There are two cases here     u and v correspond to   mini buckets in the same bucket  or    u and v correspond to mini buckets in different buckets  In case   we have   further cases   a  variable X is being eliminated in this bucket  or  b  variable X is not eliminated in this bucket  In case  a  each mini bucket must contain X and all mini buckets of the bucket are connected as a chain  so the connectedness property holds  In case  b  vertexes u and v connect to their  respectively  parents  who in turn connect to their parents  etc  until a bucket in the scheme where variable X is eliminated  All nodes along this chain connect variable X  so the connectedness property holds  Case   resolves like case  b  To show that edge labels are minimal  we need to prove that there are no cycles with respect to edge labels  If there is a cycle with respect to variable X  then it must involve at least one in edge  edge connecting two mini buckets in the same bucket   This means variable X must be the variable being eliminated in the bucket of this in edge  That means variable X is not contained in any of the parents of the mini buckets of this bucket  Therefore  in order for the cycle to exist  another in edge down the bucket tree from this bucket must contain X  However  this is impossible as this would imply that variable X is eliminated twice     Example   Figure   a shows the trace of procedure schematic mini bucket    applied to the problem described in Figure  a  The decomposition in Figure   b is created by the algorithm graph structuring  The only cluster partitioned is that of F into two scopes  FCD  and  BF   connected by an in edge labeled with F  A range of edge labeled join graphs is shown in Figure     On the left side we have a graph with smaller clusters  but more cycles  This is the type of graph IBP works on  On the right side we have a tree decomposition  which has no cycles at the expense of bigger clusters  In between  there could be a number of join graphs where maximum cluster size can be traded for number of cycles  Intuitively  the graphs on the left present less complexity for join graph algorithms because the cluster size is smaller  but they are also likely to be less accurate  The graphs on the right side       M ATEESCU   K ASK   G OGATE   D ECHTER  A  A  A  C  ABC  AB ABDE  BC  BE  C  A  A  ABC  AB  C  BCE  C BC  BC  ABDE  ABCDE  DE  CE  CDE  C DE  CE  CE  CDEF  CDEF  CDEF  CDEF F  FGH  H  FGH  H  FGI  GH  GI  F  H  GHIJ  H  FGH H F  F  F FG  ABCDE  BCE  C DE  BCE  FGI  GI  F F  GH  GHIJ  FGI  GH  GI  GHI GHIJ  FGHI  GHIJ  more accuracy less complexity  Figure     Join graphs  are computationally more complex  because of the larger cluster size  but they are likely to be more accurate      The Inference Power of IJGP The question we address in this subsection is why propagating the messages iteratively should help  Why is IJGP upon convergence superior to IJGP with one iteration and superior to MC  One clue can be provided when considering deterministic constraint networks which can be viewed as extreme probabilistic networks  It is known that constraint propagation algorithms  which are analogous to the messages sent by belief propagation  are guaranteed to converge and are guaranteed to improve with iteration  The propagation scheme of IJGP works similar to constraint propagation relative to the flat network abstraction of the probability distribution  where all non zero entries are normalized to a positive constant   and propagation is guaranteed to be more accurate for that abstraction at least  In the following we will shed some light on the IJGPs behavior by making connections with the well known concept of arc consistency from constraint networks  Dechter         We show that   a  if a variable value pair is assessed as having a zero belief  it remains as zero belief in subsequent iterations   b  that any variable value zero beliefs computed by IJGP are correct   c  in terms of zero non zero beliefs  IJGP converges in finite time  We have also empirically investigated the hypothesis that if a variable value pair is assessed by IBP or IJGP as having a positive but very close to zero belief  then it is very likely to be correct  Although the experimental results shown in this paper do not contradict this hypothesis  some examples in more recent experiments by Dechter  Bidyuk  Mateescu  and Rollon        invalidate it         J OIN  G RAPH P ROPAGATION A LGORITHMS        IJGP  AND  A RC  C ONSISTENCY  For any belief network we can define a constraint network that captures the assignments having strictly positive probability  We will show a correspondence between IJGP applied to the belief network and an arc consistency algorithm applied to the constraint network  Since arc consistency algorithms are well understood  this correspondence not only proves the target claims  but may provide additional insight into the behavior of IJGP  It justifies the iterative application of belief propagation  and it also illuminates its distance from being complete  D EFINITION     constraint satisfaction problem  A Constraint Satisfaction Problem  CSP  is a triple hX  D  Ci  where X    X            Xn   is a set of variables associated with a set of discretevalued domains D    D            Dn   and a set of constraints C    C            Cm    Each constraint Ci is a pair hSi   Ri i where Ri is a relation Ri  DSi defined on a subset of variables Si  X and DSi is a Cartesian product of the domains of variables Si   The relation Ri denotes all compatible tuples of DSi allowed by the constraint  A projection operator  creates a new relation  Sj  Ri      x x  DSj and y  y  DSi  Sj and xy  Ri    where Sj  Si   Constraints can be combined with the join operator    resulting in a new relation  Ri   Rj    x Si  x   Ri and Sj  x   Rj    A solution is an assignment of values to all the variables x    x            xn    xi  Di   such that Ci  C  xSi  Ri   The constraint network represents its set of solutions   i Ci   Given a belief network B  we define a flattening of the Bayesian network into a constraint network called f lat B   where all the zero entries in a probability table are removed from the corresponding relation  The network f lat B  is defined over the same set of variables and has the same set of domain values as B  D EFINITION     flat network  Given a Bayesian network B   hX  D  G  P i  the flat network f lat B  is a constraint network  where the set of variables is X  and for every Xi  X and its CPT P  Xi  pa Xi     B we define a constraint RFi over the family of Xi   Fi    Xi    pa Xi   as follows  for every assignment x    xi   xpa Xi     to Fi    xi   xpa Xi      RFi iff P  xi  xpa Xi          T HEOREM   Given a belief network B   hX  D  G  P i  where X    X            Xn    for any tuple x    x            xn    PB  x       x  sol f lat B    where sol f lat B   is the set of solutions of the flat constraint network  Proof  PB  x       ni   P  xi  xpa Xi          i              n   P  xi  xpa Xi          i              n    xi   xpa Xi      RFi  x  sol f lat B      Constraint propagation is a class of polynomial time algorithms that are at the center of constraint processing techniques  They were investigated extensively in the past three decades and the most well known versions are arc   path   and i consistency  Dechter               D EFINITION     arc consistency   Mackworth        Given a binary constraint network  X  D  C   the network is arc consistent iff for every binary constraint Rij  C  every value v  Di has a value u  Dj s t   v  u   Rij          M ATEESCU   K ASK   G OGATE   D ECHTER  Note that arc consistency is defined for binary networks  namely the relations involve at most two variables  When a binary constraint network is not arc consistent  there are algorithms that can process it and enforce arc consistency  The algorithms remove values from the domains of the variables that violate arc consistency until an arc consistent network is generated  There are several versions of improved performance arc consistency algorithms  however we will consider a non optimal distributed version  which we call distributed arc consistency  D EFINITION     distributed arc consistency algorithm  The algorithm distributed arcconsistency is a message passing algorithm over a constraint network  Each node is a variable  and maintains a current set of viable values Di   Let ne i  be the set of neighbors of Xi in the constraint graph  Every node Xi sends a message to any node Xj  ne i   which consists of the values in Xj s domain that are consistent with the current Di   relative to the constraint Rji that they share  Namely  the message that Xi sends to Xj   denoted by Dij   is  Dij  j  Rji   Di         Di  Di    kne i  Dki         and in addition node i computes   Clearly the algorithm can be synchronized into iterations  where in each iteration every node computes its current domain based on all the messages received so far from its neighbors  Eq      and sends a new message to each neighbor  Eq      Alternatively  Equations   and   can be combined  The message Xi sends to Xj is  Dij  j  Rji   Di  kne i  Dki         Next we will define a join graph decomposition for the flat constraint network so that we can establish a correspondence between the join graph decomposition of a Bayesian network B and the join graph decomposition of its flat network f lat B   Note that for constraint networks  the edge labeling  can be ignored  D EFINITION     join graph decomposition of the flat network  Given a join graph decomposition D   hJG      i of a Bayesian network B  the join graph decomposition Df lat   hJG    f lat i of the flat constraint network f lat B  has the same underlying graph structure JG    V  E  as D  the same variable labeling of the clusters   and the mapping f lat maps each cluster to relations corresponding to CPTs  namely Ri  f lat  v  iff CPT pi   v   The distributed arc consistency algorithm of Definition    can be applied to the join graph decomposition of the flat network  In this case  the nodes that exchange messages are the clusters  namely the elements of the set V of JG   The domain of a cluster of V is the set of tuples of the join of the original relations in the cluster  namely the domain of cluster u is   Rf lat  u  R   The constraints are binary  and involve clusters of V that are neighbors  For two clusters u and v  their corresponding values tu and tv  which are tuples representing full assignments to the variables in the cluster  belong to the relation Ruv  i e    tu   tv    Ru v   if the projections over the separator  or labeling   between u and v are identical  namely   u v   tu     u v   tv         J OIN  G RAPH P ROPAGATION A LGORITHMS  We define below the algorithm relational distributed arc consistency  RDAC   that applies distributed arc consistency to any join graph decomposition of a constraint network  We call it relational to emphasize that the nodes exchanging messages are in fact relations over the original problem variables  rather than simple variables as is the case for arc consistency algorithms  D EFINITION     relational distributed arc consistency algorithm  RDAC over a join graph  Given a join graph decomposition of a constraint network  let Ri and Rj be the relations of two clusters  Ri and Rj are the joins of the respective constraints in each cluster   having the scopes Si and Sj   such that Si  Sj      The message Ri sends to Rj denoted h i j  is defined by  h i j   Si Sj  Ri         where ne i     j Si  Sj      is the set of relations  clusters  that share a variable with Ri   Each cluster updates its current relation according to  Ri  Ri     kne i  h k i          Algorithm RDAC iterates until there is no change  Equations   and   can be combined  just like in Equation    The message that Ri sends to Rj becomes  h i j   Si Sj  Ri     kne i  h k i           To establish the correspondence with IJGP  we define the algorithm IJGP RDAC that applies RDAC in the same order of computation  schedule of processing  as IJGP  D EFINITION     IJGP RDAC algorithm  Given the Bayesian network B   hX  D  G  P i  let Df lat   hJG    f lat   i be any join graph decomposition of the flat network f lat B   The algorithm IJGP RDAC is applied to the decomposition Df lat of f lat B   and can be described as IJGP applied to D  with the following modifications  Q    Instead of   we use    P    Instead of   we use      At end end  we update the domains of variables by  Di  Di  Xi    vne u  h v u        R u  R         where u is the cluster containing Xi   Note that in algorithm IJGP RDAC  we could first merge all constraints in each cluster u into a single constraint Ru   R u  R  From our construction  IJGP RDAC enforces arc consistency over the join graph decomposition of the flat network  When the join graph Df lat is a join tree  IJGP RDAC solves the problem namely it finds all the solutions of the constraint network         M ATEESCU   K ASK   G OGATE   D ECHTER  Proposition   Given the join graph decomposition Df lat   hJG    f lat   i  JG    V  E   of the flat constraint network f lat B   corresponding to a given join graph decomposition D of a Bayesian network B   hX  D  G  P i  the algorithm IJGP RDAC applied to Df lat enforces arcconsistency over the join graph Df lat   Proof  IJGP RDAC applied to the join graph decomposition Df lat   hJG    f lat   i  JG    V  E   is equivalent to applying RDAC of Definition    to a constraint network that has vertices V as its variables and   R u  R u  V   as its relations    Following the properties of convergence of arc consistency  we can show that  Proposition   Algorithm IJGP RDAC converges in O m  r  iterations  where m is the number of edges in the join graph and r is the maximum size of a separator Dsep u v  between two clusters  Proof  This follows from the fact messages  which are relations  between clusters in IJGP RDAC change monotonically  as tuples are only successively removed from relations on separators  Since the size of each relation on a separator is bounded by r and there are m edges  no more than O mr  iterations will be needed    In the following we will establish an equivalence between IJGP and IJGP RDAC in terms of zero probabilities  Proposition   When IJGP and IJGP RDAC are applied in the same order of computation  the messages computed by IJGP are identical to those computed by IJGP RDAC in terms of zero   nonzero probabilities  That is  h u v   x       in IJGP iff x  h u v  in IJGP RDAC  Proof  The proof is by induction  The base case is trivially true since messages h in IJGP are initialized to a uniform distribution and messages h in IJGP RDAC are initialized to complete relations  The induction step  Suppose that hIJGP  u v  is the message sent from u to v by IJGP  We will show IJGP RDAC IJGP RDAC that if hIJGP where h u v  is the message sent by IJGP u v   x        then x  h u v  RDAC from u to v  Assume that the claim holds for all messages received by u from its neighbors  Let f  clusterv  u  in IJGP and Rf be the corresponding relation in IJGP RDAC  and P Q t be an asIJGP signment of values to variables in elim u  v   We have h u v   x        elim u v  f f  x       Q  t  f f  x  t        t  f  f  x  t        t  f  scope Rf    x  t   Rf  t  elim u v    Rf IJGP RDAC IJGP RDAC      x  h u v  scope Rf    x  t    h u v   Next we will show that IJGP computing marginal probability P  Xi   xi       is equivalent to IJGP RDAC removing xi from the domain of variable Xi   Proposition   IJGP computes P  Xi   xi       iff IJGP RDAC decides that xi   Di   Proof  According to Proposition   messages computed by IJGP and IJGP RDAC are identical in terms of zero probabilities  Let f  cluster u  in IJGP and Rf be the corresponding relation in IJGP RDAC  and t be an assignment of values to variables in  u  Xi   We will show that when IJGP computes P  Xi   xi        upon convergence   then IJGP RDAC computes xi   Di   We       J OIN  G RAPH P ROPAGATION A LGORITHMS  Q Q P have P  Xi   xi     f f  xi   t       t  f  f  xi   t       f f  xi        t  X Xi t  Rf   scope Rf    xi   t    Rf  t   xi   t      Rf Rf  xi   t    xi   Di  Xi   Rf Rf  xi   t    xi   Di   Since arc consistency is sound  so is the decision of zero probabilities    Next we will show that P  Xi   xi       computed by IJGP is sound  T HEOREM   Whenever IJGP finds P  Xi   xi        then the probability P  Xi   expressed by the Bayesian network conditioned on the evidence is   as well  Proof  According to Proposition    whenever IJGP finds P  Xi   xi        the value xi is removed from the domain Di by IJGP RDAC  therefore value xi  Di is a no good of the network f lat B   and from Theorem   it follows that PB  Xi   xi          In the following we will show that the time it takes IJGP to find all P  Xi   xi       is bounded  Proposition   IJGP finds all P  Xi   xi       in finite time  that is  there exists a number k  such that no P  Xi   xi       will be found after k iterations  Proof  This follows from the fact that the number of iterations it takes for IJGP to compute P  Xi   xi       is exactly the same number of iterations IJGP RDAC takes to remove xi from the domain Di  Proposition   and Proposition     and the fact the IJGP RDAC runtime is bounded  Proposition       Previous results also imply that IJGP is monotonic with respect to zeros  Proposition    Whenever IJGP finds P  Xi   xi        it stays   during all subsequent iterations  Proof  Since we know that relations in IJGP RDAC are monotonically decreasing as the algorithm progresses  it follows from the equivalence of IJGP RDAC and IJGP  Proposition    that IJGP is monotonic with respect to zeros           A F INITE P RECISION P ROBLEM On finite precision machines there is the danger that an underflow can be interpreted as a zero value  We provide here a warning that an implementation of belief propagation should not allow the creation of zero values by underflow  We show an example in Figure    where IBPs messages converge in the limit  i e   in an infinite number of iterations   but they do not stabilize in any finite number of iterations  If all the nodes Hk are set to value    the belief for any of the Xi variables as a function of iteration is given in the table in Figure     After about     iterations  the finite precision of our computer is not able to represent the value for Bel Xi       and this appears to be zero  yielding the final updated belief              when in fact the true updated belief should be            Notice that             cannot be regarded as a legitimate fixed point for IBP  Namely  if we would initialize IBP with the values              then the algorithm would maintain them  appearing to have a fixed point  but initializing IBP with zero values cannot be expected to be correct  When we        M ATEESCU   K ASK   G OGATE   D ECHTER  X   Prior for Xi  H   H   X   X   Xi  P   Xi                           H  Hk Xi  CPT for Hk  Xj  P   Hk   Xi   Xj                                                        iter  Bel Xi      Bel Xi      Bel Xi                                                                                              e                      e                      True belief           Figure     Example of a finite precision problem  initialize with zeros we forcibly introduce determinism in the model  and IBP will always maintain it afterwards  However  this example does not contradict our theory because  mathematically  Bel Xi      never becomes a true zero  and IBP never reaches a quiescent state  The example shows that a close to zero belief network can be arbitrarily inaccurate  In this case the inaccuracy seems to be due to the initial prior belief which are so different from the posterior ones        ACCURACY OF IBP ACROSS B ELIEF D ISTRIBUTION We present an empirical evaluation of the accuracy of IBPs prediction for the range of belief distribution from   to    These results also extend to IJGP  In the previous section  we proved that zero values inferred by IBP are correct  and we wanted to test the hypothesis that this property extends to   small beliefs  namely  that are very close to zero   That is  if IBP infers a posterior belief close to zero  then it is likely to be correct  The results presented in this paper seem to support the hypothesis  however new experiments by Dechter et al         show that it is not true in general  We do not have yet a good characterization of the cases when the hypothesis is confirmed  To test this hypothesis  we computed the absolute error of IBP per intervals of         For a given interval  a  b   where    a   b     we use measures inspired from information retrieval  Recall Absolute Error and Precision Absolute Error  Recall is the absolute error averaged over all the exact posterior beliefs that fall into the interval  a  b   For Precision  the average is taken over all the approximate posterior belief values computed by IBP to be in the interval  a  b   Intuitively  Recall  a b   indicates how far the belief computed by IBP is from the exact  when the exact is in  a  b   Precision  a b   indicates how far the exact is from IBPs prediction  when the value computed by IBP is in  a  b   Our experiments show that the two measures are strongly correlated  We also show the histograms of distribution of belief for each interval  for the exact and for IBP  which are also strongly correlated  The results are given in Figures    and     The left Y axis corresponds to the histograms  the bars   the right Y axis corresponds to the absolute error  the lines   We present results for two classes of problems  coding networks and grid network  All problems have binary variables  so the graphs are symmetric about     and we only show the interval           The number of variables  number of iterations and induced width w  are reported for each graph         J OIN  G RAPH P ROPAGATION A LGORITHMS  Recall Abs  Error  noise         noise              Absolute Error                                                                                         Precision Abs  Error                                                                                                                                                                                                                                                  IBP Histogram            Percentage  Exact Histogram      noise         Figure     Coding  N           instances  w      Recall Abs  Error  evidence      evidence                                Absolute Error                                                                                                                                                                                                                                                          Precision Abs  Error        IBP Histogram             Percentage  Exact Histogram      evidence       Figure       x   grids      instances  w      Coding networks IBP is famously known to have impressive performance on coding networks  We tested on linear block codes  with    nodes per layer and   parent nodes  Figure    shows the results for three different values of channel noise           and      For noise      all the beliefs computed by IBP are extreme  The Recall and Precision are very small  of the order of        So  in this case  all the beliefs are very small    small  and IBP is able to infer them correctly  resulting in almost perfect accuracy  IBP is indeed perfect in this case for the bit error rate   When the noise is increased  the Recall and Precision tend to get closer to a bell shape  indicating higher error for values close to     and smaller error for extreme values  The histograms also show that less belief values are extreme as the noise is increased  so all these factors account for an overall decrease in accuracy as the channel noise increases  These networks are examples with a large number of   small probabilities and IBP is able to infer them correctly  absolute error is small   Grid networks We present results for grid networks in Figure     Contrary to the case of coding networks  the histograms show higher concentration of beliefs around      However  the accuracy is still very good for beliefs close to zero  The absolute error peaks close to   and maintains a plateau  as evidence is increased  indicating less accuracy for IBP      Experimental Evaluation As we anticipated in the summary of Section    and as can be clearly seen now by the structuring of a bounded join graph  there is a close relationship between the mini clustering algorithm MC i        M ATEESCU   K ASK   G OGATE   D ECHTER  IBP  it           MC   evid                                                                                                       Absolute error IJGP i   i                                                                                                                                                    IBP i                                                                                                                                                                                                                              Relative error IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                              KL distance IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                     Time IJGP i                                                                   i                                                                   i                                                                                                                                   Table    Random networks  N     K    C     P        instances  w       and IJGP i   In particular  one iteration of IJGP i  is similar to MC i   MC sends messages up and down along the clusters that form a set of trees  IJGP has additional connections that allow more interaction between the mini clusters of the same cluster  Since this is a cyclic structure  iterating is facilitated  with its virtues and drawbacks s In our evaluation of IJGP i   we focus on two different aspects   a  the sensitivity of parametric IJGP i  to its i bound and to the number of iterations   b  a comparison of IJGP i  with publicly available state of the art approximation schemes      Effect of i bound and Number of Iterations We tested the performance of IJGP i  on random networks  on M by M grids  on the two benchmark CPCS files with    and     variables  respectively and on coding networks  On each type of networks  we ran IBP  MC i  and IJGP i   while giving IBP and IJGP i  the same number of iterations  We use the partitioning method described in Section     to construct a join graph  To determine the order of message computation  we recursively pick an edge  u v   such that node u has the fewest incoming messages missing  For each network except coding  we compute the exact solution and compare the accuracy using the absolute and relative error  as before  as well as the KL  Kullback Leibler  distance Pexact  X   a   log Pexact  X   a  Papproximation  X   a   averaged over all values  all variables and all problems  For coding networks we report the Bit Error Rate  BER  computed as described in Section      We also report the time taken by each algorithm  The random networks were generated using parameters  N K C P   where N is the number of variables  K is their domain size  C is the number of conditional probability tables  CPTs  and P is the number of parents in each CPT  Parents in each CPT are picked randomly and each CPT is filled randomly  In grid networks  N is a square number and each CPT is filled randomly  In each problem class  we also tested different numbers of evidence variables  As before  the coding networks are from the class of linear block codes  where  is the channel noise level  Note that we are limited to relatively small and sparse problem instances because our evaluation measures are based on comparing against exact figures  Random networks results for networks having N     K    C    and P   are given in Table   and in Figures    and     For IJGP i  and MC i  we report   different values of i bound           For IBP and IJGP i  we report results for   different numbers of iterations            We report results       J OIN  G RAPH P ROPAGATION A LGORITHMS  Random networks  N     K    P    evid    w            IJGP   it IJGP   it IJGP   it IJGP   it IJGP    it IJGP    it IJGP    it MC IBP   it IBP   it IBP   it IBP   it IBP    it         KL distance                                                                    i bound   a  Performance vs  i bound  Random networks  N     K    P    evid    w            IBP IJGP    IJGP             KL distance                                                            Number of iterations   b  Convergence with iterations   Figure     Random networks  KL distance  for   different numbers of evidence            From Table   and Figure   a we see that IJGP i  is always better than IBP  except when i   and number of iterations is     sometimes by an order of magnitude  in terms of absolute error  relative error and KL distance  IBP rarely changes after   iterations  whereas IJGP i s solution can be improved with more iterations  up to         As theory predicted  the accuracy of IJGP i  for one iteration is about the same as that of MC i   But IJGP i  improves as the number of iterations increases  and is eventually better than MC i  by as much as an order of magnitude  although it clearly takes more time  especially when the i bound is large         M ATEESCU   K ASK   G OGATE   D ECHTER  Random networks  N     K    P    evid    w         IJPG   it IJGP   it IJGP   it IJGP   it IJGP    it IJGP    it IJGP    it MC IBP   it IBP    it  Time  seconds                                                                  i bound  Figure     Random networks  Time  IBP  it           MC   evid                                                                                                       Absolute error IJGP i   i                                                                                                                                                    IBP i                                                                                                                                                                                                                              Relative error IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                              KL distance IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                     Time IJGP i                                                                   i                                                                   i                                                                                                                                   Table     x  grid  K        instances  w       Figure   a shows a comparison of all algorithms with different numbers of iterations  using the KL distance  Because the network structure changes with different i bounds  we do not necessarily see monotonic improvement of IJGP with i bound for a given number of iterations  as is the case with MC   Figure   b shows how IJGP converges with more iterations to a smaller KL distance than IBP  As expected  the time taken by IJGP  and MC  varies exponentially with the i bound  see Figure      Grid networks results with networks of N     K        instances are very similar to those of random networks  They are reported in Table   and in Figure     where we can see the impact of having evidence    and   evidence variables  on the algorithms  IJGP at convergence gives the best performance in both cases  while IBPs performance deteriorates with more evidence and is surpassed by MC with i bound   or larger  CPCS networks results with CPCS   and CPCS    are given in Table   and Figure     and are even more pronounced than those of random and grid networks  When evidence is added  IJGP i  is more accurate than MC i   which is more accurate than IBP  as can be seen in Figure   a  Coding networks results are given in Table    We tested on large networks of     variables  with treewidth w      with IJGP and IBP set to run    iterations  this is more than enough to ensure       J OIN  G RAPH P ROPAGATION A LGORITHMS  Grid network  N     K    evid    w           IJGP   it IJGP   it IJGP   it IJGP   it IJGP    it MC IBP   it IBP   it IBP   it IBP   it IBP    it         KL distance                                                                    i bound   a  Performance vs  i bound  Grid network  N     K    evid    w      e   IJGP    iterations  at convergence   e    KL distance   e     e     e     e     e                                         i bound   b  Fine granularity for KL   Figure     Grid  x   KL distance  convergence   IBP is known to be very accurate for this class of problems and it is indeed better than MC  However we notice that IJGP converges to slightly smaller BER than IBP even for small values of the i bound  Both the coding network and CPCS    show the scalability of IJGP for large size problems  Notice that here the anytime behavior of IJGP is not clear  In summary  we see that IJGP is almost always superior to both IBP and MC i  and is sometimes more accurate by several orders of magnitude  One should note that IBP cannot be improved with more time  while MC i  requires a large i bound for many hard and large networks to achieve reasonable accuracy  There is no question that the iterative application of IJGP is instrumental to its success  In fact  IJGP    in isolation appears to be the most cost effective variant         M ATEESCU   K ASK   G OGATE   D ECHTER  IBP  it            MC          MC   evid  Absolute error IJGP i   i    Relative error IJGP i    IBP i    i                                                                                                                                                                             KL distance IJGP i    IBP i    CPCS                                                                                                                                                                                                    Time IBP  i                               e                       e                                                                                      e                                                                                               e                            i                                                                      e                                                                                           i    IJGP i    i                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          e      e                    CPCS                                                                                                                                                                                                                                                                                                                                                                                                                             e                      e      e              e                                           e                                                                                                                                                                                                                                               Table    CPCS      instances  w      CPCS       instances  w             IJGP MC      IJGP MC      IJGP MC      IJGP MC      IJGP MC      IJGP MC                                                                                                     IJGP         MC          Bit Error Rate i bound                                                                                                                                                                                                                                                                                                       Time                                                                                                                                                      IBP                                                                            Table    Coding networks  N      P        instances     iterations  w           Comparing IJGP with Other Algorithms In this section we provide a comparison of IJGP with state of the art publicly available schemes  The comparison is based on a recent evaluation of algorithms performed at the Uncertainty in AI      conference    We will present results on solving the belief updating task  also called the task of computing posterior node marginals   MAR   We first give a brief overview of the schemes that we experimented and compared with     EDBP   Edge Deletion for Belief Propagation    Complete results are available at http   graphmod ics uci edu uai   Evaluation Report         J OIN  G RAPH P ROPAGATION A LGORITHMS  CPCS     evid     w          IJGP   it IJGP    it IJGP    it MC IBP   it IBP    it IBP    it             KL distance                                                                                 i bound   a  Performance vs  i bound  CPCS     evid     w      e   IJGP    iterations  at convergence   e    KL distance   e     e     e     e                                          i bound   b  Fine granularity for KL   Figure     CPCS     KL distance  EDBP  Choi   Darwiche      a      b  is an approximation algorithm for Belief Updating  It solves exactly a simplified version of the original problem  obtained by deleting some of the edges of the problem graph  Edges to be deleted are selected based on two criteria  quality of approximation and complexity of computation  tree width reduction   Information loss from lost dependencies is compensated for by introducing auxiliary network parameters  This method corresponds to Iterative Belief Propagation  IBP  when enough edges are deleted to yield a poly tree  and corresponds to generalized BP otherwise     TLSBP   A truncated Loop series Belief propagation algorithm        M ATEESCU   K ASK   G OGATE   D ECHTER  TLSBP is based on the loop series expansion formula of Chertkov and Chernyak        which specifies a series of terms that need to be added to the solution output by BP so that the exact solution can be recovered  This series is basically a sum over all so called generalized loops in the graph  Unfortunately  because the number of these generalized loops can be prohibitively large  the series is of little value  The idea in TLSBP is to truncate the series by decomposing all generalized loops into simple and smaller loops  thus limiting the number of loops to be summed  In our evaluation  we used an implementation of TLSBP available from the work of Gomez  Mooji  and Kappen         The implementation can handle binary networks only     EPIS   Evidence Pre propagation Importance Sampling EPIS  Yuan   Druzdzel        is an importance sampling algorithm for Belief Updating  It is well known that sampling algorithms perform poorly when presented with unlikely evidence  However  when samples are weighted by an importance function  good approximation can be obtained  This algorithm computes an approximate importance function using loopy belief propagation and   cutoff heuristic  We used an implementation of EPIS available from the authors  The implementation works on Bayesian networks only     IJGP   Iterative Join Graph Propagation In the evaluation  IJGP i  was first run with i    until convergence  then with i    until convergence  etc  until i  treewidth  when i bound treewidth  the join graph becomes a join tree and IJGP becomes exact   As preprocessing  the algorithm performed SAT based variable domain pruning by converting zero probabilities in the problem to a SAT problem and performing singleton consistency enforcement  Because the problem size may reduce substantially  in some cases  this preprocessing step may have a significant impact on the time complexity of IJGP  amortized over the increasing i bound  However  for a given i bound  this step improves the accuracy of IJGP only marginally     SampleSearch SampleSearch  Gogate   Dechter        is a specialized importance sampling scheme for graphical models that contain zero probabilities in their CPTs  On such graphical models  importance sampling suffers from the rejection problem in that it generates a large number of samples which have zero weight  SampleSearch circumvents the rejection problem by sampling from the backtrack free search space in which every assignment  sample  is guaranteed to have non zero weight  The backtrack free search space is constructed on the fly by interleaving sampling with backtracking style search  Namely  when a sample is supposed to be rejected because its weight is zero  the algorithm continues instead with systematic backtracking search  until a non zero weight sample is found  For the evaluation version  the importance distribution of SampleSearch was constructed from the output of IJGP with i bound of    For more information on how the importance distribution is constructed from the output of IJGP  see the work by Gogate         The evaluation was conducted on the following benchmarks  see footnote   for details      UAI   MPE   from UAI        instances  Bayesian networks     instances were used      UAI   PE   from UAI        instances  Bayesian networks     instances were used         J OIN  G RAPH P ROPAGATION A LGORITHMS  IJGP EDBP TLSBP EPIS SampleSearch  WCSPs BN O Grids Linkage Promedas UAI   MPE UAI   PE Relational                                  Table    Scope of our experimental study  Score vs KL distance   Score vs KL distance          Score                                                        KL distance  Figure     Score as a function of KL distance     Relational Bayesian networks   constructed from the Primula tool      instances  binary variables  large networks with large tree width  but with high levels of determinism     instances were used      Linkage networks      instances  tree width        Markov networks    instances were used      Grids   from   x   to   x        instances  treewidth           BN O networks   Two layer Noisy OR Bayesian networks     instances  binary variables  up to    variables  treewidth           WCSPs   Weighted CSPs     instances  Markov networks     instances were used      Promedas   real world medical diagnosis      instances  tree width       Markov networks     instances were used    Table   shows the scope of our experimental study  A indicates that the solver was able to  handle the benchmark type and therefore evaluated on it while a lack of a indicates otherwise  We measure the performance of the algorithms in terms of a KL distance based score  Formally  the score of a solver on a problem instance is equal to   avgkld where avgkld is the average KL distance between the exact marginal  which was computed using the UCLA Ace solver  see Chavira   Darwiche        and the approximate marginal output by the solver  If a solver does not output a solution  we consider its KL distance to be   A score lies between   and    with   indicating that the solver outputs exact solution while   indicating that the solver either does not output a solution or has infinite average KL distance  Figure    shows the score as a function of KL distance        M ATEESCU   K ASK   G OGATE   D ECHTER  In Figures       we report the results of experiments with each of the problem sets  Each solver has a timeout of    minutes on each problem instance  when solving a problem  each solver periodically outputs the best solution found so far  Using this  we can compute  for each solver  at any point in time  the total sum of its scores over all problem instances in a particular set  called SumScore t   On the horizontal axis  we have the time and on the vertical axis  the SumScore t   The higher the curve of a solver is  the better  the higher the score   In summary  we see that IJGP shows the best performance on the first four classes of networks  UAI MPE  UAI PE  Relational and Linkage   it is tied with other algorithms on two classes  Grid and BN O   and is surpassed by EDBP on the last two classes  WCSPs and Promedas   EPIS and SampleSearch  which are importance sampling schemes  are often inferior to IJGP and EDBP  In theory  the accuracy of these importance sampling schemes should improve with time  However  the rate of improvement is often unknown in practice  On the hard benchmarks that we evaluated on  we found that this rate is quite small and therefore the improvement cannot be discerned from the Figures  We discuss the results in detail below  As mentioned earlier  TLSBP works only on binary networks  i e   two variables per function  and therefore it was not evaluated on WCSPs  Linkage  UAI   MPE and UAI   PE benchmarks  The UAI MPE and UAI PE instances were used in the UAI      evaluation of exact solvers  for details see the report by Bilmes   Dechter         Exact marginals are available on    UAI MPE instances and    UAI PE instances  The results for UAI MPE and UAI PE instances are shown in Figures    and    respectively  IJGP is the best performing scheme on both benchmark sets reaching a SumScore very close to the maximum possible value in both cases after about   minutes of CPU time  EDBP and SampleSearch are second best in both cases  Relational network instances are generated by grounding the relational Bayesian networks using the Primula tool  Chavira  Darwiche    Jaeger         Exact marginals are available only on    out of the submitted     instances  From Figure     we observe that IJGPs SumScore steadily increases with time and reaches a value very close to the maximum possible score of    after about    minutes of CPU time  SampleSearch is the second best performing scheme  EDBP  TLSBP and EPIS perform quite poorly on these instances reaching the SumScore of        and    respectively after    minutes of CPU time  The Linkage instances are generated by converting linkage analysis data into a Markov network using the Superlink tool  Fishelson   Geiger         Exact marginals are available only on   out of the    instances  The results are shown in Figure     After about one minute of CPU time  IJGPs SumScore is close to   which remains steady thereafter while EDBP only reaches a SumScore of   in    minutes  SampleSearch is the second best performing scheme while EDBP is third best  The results on Grid networks are shown in Figure     The sink node of the grid is the evidence node  The deterministic ratio p is a parameter specifying the fraction of nodes that are deterministic  that is  whose values are determined given the values of their parents  The evaluation benchmark set consists of    instances having p           and     with exact marginals available on    instances only  EPIS  IJGP  SampleSearch and EDBP are in a close tie on this network  while TLSBP has the lowest performance  While hard to see  EPIS is just slightly the best performing scheme  IJGP is the second best followed by SampleSearch and EDBP  On this instances IJGPs SumScore increases steadily with time  The results on BN O instances appear in Figure     This is again a very close tie  in this case of all five algorithms  IJGP has a minuscule decrease of SumScore with time from       to       Although in general an improvement in accuracy is expected for IJGP with higher i bound  it is not       J OIN  G RAPH P ROPAGATION A LGORITHMS  Approximate Mar Problem Set uai   mpe         Sum Score                                                            Time in minutes SampleSearch  IJGP  EDBP  EPIS  Figure     Results on UAI MPE networks  TLSBP is not plotted because it cannot handle UAIMPE benchmarks   Approximate Mar Problem Set uai   pe      Sum Score                                                           Time in minutes SampleSearch  IJGP  EDBP  EPIS  Figure     Results on UAI PE networks  TLSBP is not plotted because it cannot handle UAI PE benchmarks         M ATEESCU   K ASK   G OGATE   D ECHTER  Approximate Mar Problem Set Relational         Sum Score                                                                      Time in minutes SampleSearch IJGP  EDBP TLSBP  EPIS  Figure     Results on relational networks   Approximate Mar Problem Set Linkage       Sum Score                                               Time in minutes SampleSearch  IJGP  EDBP  Figure     Results on Linkage networks  EPIS and TLSBP are not plotted because they cannot handle Linkage networks         J OIN  G RAPH P ROPAGATION A LGORITHMS  Approximate Mar Problem Set Grids      Sum Score                                                                      Time in minutes SampleSearch IJGP  EDBP TLSBP  EPIS  Figure     Results on Grid networks   Approximate Mar Problem Set bn o        Sum Score                                                Time in minutes SampleSearch IJGP  EDBP TLSBP  EPIS  Figure     Results on BN O networks  All solvers except IJGP quickly converge to the maximum possible score of    and are therefore indistinguishable in the Figure         M ATEESCU   K ASK   G OGATE   D ECHTER  Approximate Mar Problem Set WCSPs           Sum Score                                                         Time in minutes SampleSearch  IJGP  EDBP  Figure     Results on WCSPs networks  EPIS and TLSBP are not plotted because they cannot handle WCSPs   Approximate Mar Problem Set Promedas           Sum Score                                                            Time in minutes SampleSearch  IJGP  EDBP  TLSBP  Figure     Results on Promedas networks  EPIS is not plotted because it cannot handle Promedas benchmarks  which are Markov networks         J OIN  G RAPH P ROPAGATION A LGORITHMS  guaranteed  and this is an example when it does not happen  The other solvers reach the maximum possible SumScore of     or very close to it  after about   minutes of CPU time  The WCSP benchmark set has    instances  However we used only the    instances for which exact marginals are available  Therefore the maximum SumScore that an algorithm can reach is     The results are shown in Figure     EDBP reaches a SumScore of    after almost   minutes of CPU time while IJGP reaches a SumScore of    after about   minutes  The SumScores of both IJGP and EDBP remain unchanged in the interval from   to    minutes  After looking at the raw results  we found that IJGPs score was zero on   instances out of     This was because the singleton consistency component implemented via the SAT solver did not finish in    minutes on these instances  Although the singleton consistency step generally helps to reduce the practical time complexity of IJGP on most instances  it adversely affects it on these WCSP instances  The Promedas instances are Noisy OR binary Bayesian networks  Pearl         These instances are characterized by extreme marginals  Namely  for a given variable  the marginals are of the form           where   is a very small positive constant  Exact marginals are available only on    out of the submitted     instances  On these structured problems  see Figure      we see that EDBP is the best performing scheme reaching a SumScore very close to    after about   minutes of CPU time while TLSBP and IJGP are able to reach a SumScore of about    in    minutes      Related Work There are numerous lines of research devoted to the study of belief propagation algorithms  or message passing schemes in general  Throughout the paper we have mentioned and compared with other related work  especially in the experimental evaluation section  We give here a short summary of the developments in belief propagation and present some related schemes that were not mentioned before  For additional information see also the recent review by Koller         About a decade ago  Iterative Belief Propagation  Pearl        received a lot of interest from the information theory and coding community  It was realized that two of the best error correcting decoding algorithms were actually performing belief propagation in networks with cycles  The LDPC code  low density parity check  introduced long time ago by Gallager         is now considered one of the most powerful and promising schemes that often performs impressively close to Shannons limit  Turbo codes  Berrou  Glavieux    Thitimajshima        are also very efficient in practice and can be understood as an instance of belief propagation  McEliece et al          A considerable progress towards understanding the behavior and performance of BP was made through concepts from statistical physics  Yedidia et al         showed that IBP is strongly related to the Bethe Peierls approximation of variational  Gibbs  free energy in factor graphs  The Bethe approximation is a particular case of the more general Kikuchi        approximation  Generalized Belief Propagation  Yedidia et al         is an application of the Kikuchi approximation that works with clusters of variables  on structures called region graphs  Another algorithm that employs the region based approach is Cluster Variation Method  CVM   Pelizzola         These algorithms focus on selecting a good region graph structure to account for the over counting  and over overcounting  etc   of evidence  We view generalized belief propagation more broadly as any belief propagation over nodes which are clusters of functions  Within this view IJGP  and GBP as defined by Yedidia et al          as well as CVM  are special realizations of generalized belief propagation  Belief Propagation on Partially Ordered Sets  PBP   McEliece   Yildirim        is also a generalized form of Belief Propagation that minimizes the Bethe Kikuchi variational free energy  and        M ATEESCU   K ASK   G OGATE   D ECHTER  that works as a message passing algorithm on data structures called partially ordered sets  which has junction graphs and factor graphs as examples  There is one to one correspondence between fixed points of PBP and stationary points of the free energy  PBP includes as special cases many other variants of belief propagation  As we noted before  IJGP is basically the same as PBP  Expectation Propagation  EP   Minka        is a an iterative approximation algorithm for computing posterior belief in Bayesian networks  It combines assumed density filtering  ADF   an extension of the Kalman filter  used to approximate belief states using expectations  such as mean and variance   with IBP  and iterates until these expectations are consistent throughout the network  TreeEP  Minka   Qi        deals with cyclic problem by reducing the problem graph to a tree subgraph and approximating the remaining edges  The relationship between EP and GBP is discussed by Welling  Minka  and Teh         Survey Propagation  SP   Braunstein et al         solves hard satisfiable  SAT  problems using a message passing algorithm on a factor graph consisting of variable and clause nodes  SP is inspired by an algorithm called Warning Propagation  WP  and by BP  WP can determine if a tree problem is SAT  and if it is then it can provide a solution  BP can compute the number of satisfying assignments for a tree problem  as well as the fraction of the assignments where a variable is true  These two algorithms are used as heuristics to define the SP algorithm  that is shown to be more efficient than either of them on arbitrary networks  SP is still a heuristic algorithm with no guarantee of convergence  SP was inspired by the new concept of cavity method in statistical physics  and can be interpreted as BP where variables can not only take the values true or false  but also the extra dont care value  For a more detailed treatment see the book by Mezard and Montanari             Conclusion In this paper we investigated a family of approximation algorithms for Bayesian networks  that could also be extended to general graphical models  We started with bounded inference algorithms and proposed Mini Clustering  MC  scheme as a generalization of Mini Buckets to arbitrary tree decompositions  Its power lies in being an anytime algorithm governed by a user adjustable i bound parameter  MC can start with small i bound and keep increasing it as long as it is given more time  and its accuracy usually improves with more time  If enough time is given to it  it is guaranteed to become exact  One of its virtues is that it can also produce upper and lower bounds  a route not explored in this paper  Inspired by the success of iterative belief propagation  IBP   we extended MC into an iterative message passing algorithm called Iterative Join Graph Propagation  IJGP   IJGP operates on general join graphs that can contain cycles  but it is sill governed by an i bound parameter  Unlike IBP  IJGP is guaranteed to become exact if given enough time  We also make connections with well understood consistency enforcing algorithms for constraint satisfaction  giving strong support for iterating messages  and giving insight into the performance of IJGP  IBP   We show that      if a value of a variable is assessed as having zero belief in any iteration of IJGP  then it remains a zero belief in all subsequent iterations      IJGP converges in a finite number of iterations relative to its set of zero beliefs  and  most importantly     that the set of zero beliefs decided by any of the iterative belief propagation methods is sound  Namely any zero belief determined by IJGP corresponds to a true zero conditional probability relative to the given probability distribution expressed by the Bayesian network         J OIN  G RAPH P ROPAGATION A LGORITHMS  Our experimental evaluation of IJGP  IBP and MC is provided  and IJGP emerges as one of the most powerful approximate algorithms for belief updating in Bayesian networks   
