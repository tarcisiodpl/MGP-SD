  the  Most successful Bayesian network  BN  ap plications to date have been built through knowledge elicitation from experts   This is  difficult and time consuming  which has lead to recent interest in automated methods for learning BNs from data  We present a case  study in the construction of a BN in an intel ligent tutoring application  specifically dec imal misconceptions   We describe the BN  construction using expert elicitation and then  domain   Liz Sonenberg  Vicki Steinlell  common human difficulties  ifying  and  being  unable to identify the  influences been  combining between  much   e g     for  in  Hence  recent  time  constructing BNs   Spirtes et al           in  and  spec  experts  causal direction of  variables   interest  mated methods  probabilities   there  has  in  auto  from  data  Wallace and Korb         Beckerman and Geiger           Most evaluation of  these automated methods is done by taking an exist ing BN model  generating data from it that is given to the automated learner  the learned BN is compared to the original   investigate how certain existing automated  While there have been attempts to combine knowl  knowledge discovery methods might support  edge elicitation from experts and automated knowl  the BN knowledge engineering process   edge discovery methods  e g   Heckerman et a          Onisko et al           there is as yet no established methodology  Kennett et al            Appropriate eval     INTRODUCTION  Bayesian networks have become a popular AI represen tation for reasoning under uncertainty  with successful applications in  medical  diagnosis  planning  moni toring  vision  information retrieval and intelligent tu toring  Conati et al         Mayo and Mitrovic        VanLehn and Niu         Most successful applications to date have been built through knowledge elicita tion from experts   time consuming  In general  this is difficult and   Druzdzel and van der Gaag           with problems involving incomplete knowledge of  School of Computer Sci  and Soft  Eng   Monash Uni versity  VIC       Australia  annnfllcsse monash edu au  sity  t Department of Computer Science  The Univer of Melbourne  Parkville  VIC       Australia   bonehfllstudents cs mu oz au  tAs for A  Nicholson  tawll csse monash edu au  Department of Science and Mathematics Educa tion  The University of Melbourne  VIC       A ustralia   uation  in particular  is an open question  most auto mated methods use some sort of statistical measure of how well the BN model fits the data whereas elicited models are assessed in part by how well their predic tions on particular test scenarios meet expert expecta tions  When both data and expert knowledge about a domain is available  it is not simply a question of using the automated methods to validate the expert elicited BN  or using the expert to choose between learned networks or complete those not fully specified   Net  works built using the different methods may be very different  The question then becomes how to resolve such differences such that the resultant BN model is acceptable to the expert client and hence deployable  In this paper  we present a case study in the construc tion of a BN model in the intelligent tutoring system  ITS   Section     We describe the initial network con struction using expert elicitation  together with a pre liminary evaluation  Section      We then apply au  k staceyfllunimelb edu au   tomated knowledge discovery methods to each main    Department of Information Systems  The Uni versity of Melbourne  Parkville  VIC       Australia   task in the construction process   Section           we  LizSstaff dis unimelb edu au   we perform simple parameter learning based on fre   liAs for K  Stacey   v  steinleflledfac unimelb edu  au  apply a classification method to student test data        NICHOLSON ET AL   UA        quency counts to the expert BN structures and     we apply an existing BN learning program  In each case       for that item type by that student class other than the  we compare the performance of the resultant network  combinations seen above  We note that the fine mis conception classifications have been  grouped  by the  with the expert elicited networks  providing an insight  experts into a coarse classification  into how elicitation and knowledge discovery might b e  decimals are larger numbers   S  shorter is larger   A  combined in the B N knowledge engineering process      L  think longer   apparent expert  and UN  other   The LU  SU and AU fine classifications correspond to students who on their answers on Type   and   items behave like others in  THE ITS DOMAIN     their coarse classification  however they don t b ehave  D ecimal notation is widely used in our society   Our  testing  Stacey and Steinle        of      students has indicated that less     of Year    students  age about    years  understand the notation well enough to reliably judge the relative size of decimals    age about    years  have mastered this important Expertise grows only very slowly through  out the intervening years under normal instruction in our schools  and so an intelligent tutoring ap proach to this important topic is of interest   Stu  dents  understanding of decimal notation has been mapped using a short test  the Decimal Comparison Test  DCT   where the student is asked to choose the larger number from each of    pairs of decimals  Stacey and Steinle          The pairs of decimals are carefully chosen so that from the patterns of responses  students   mis understanding can be diagnosed as be longing to one of a number of categories  These cate gories have been identified manually  based on exten sive research  Stacey and Steinle           may be students behaving consistently according to an unknown misconception  or students who are not following any consistent interpretation   On  the other hand  more than     o f Grade   students concept   like them on the other item types  These and the UNs  For most stu  Table    Responses experts expect from students with different misconceptions Expert Class  ATE AMO MIS AU LWH  LZE LRV  LU  SDF SRN  su UN             H H L H L  L L L H  H  H               H H  L H H H H H  Item type                         H H H L  L  L  L  H H  H  L L L  L  H  H  L  H  L                          H H  H H L  H  H H  H H L  H  H  L  L  L  dents  there is consistency in their responses to similar test items and some children display the same miscon ception over long periods of time  Most are based on  false analogies  which are sometimes embellished by isolated learned facts  such as a zero in the tenths column makes a number small   For example  many younger students think     is smaller than      be cause there are   parts  of unspecified size  for these students  in the first number and    parts  also of unspecified size  in the second  However  these stu dents  LWH in Table    get many items right  e g        compared with       with the same erroneous thinking  Students in the SRN class  Table    choose      as greater than      but for the wrong reason  as they draw an analogy between fractions and decimals and use knowledge that     is greater than        See  Stacey and Steinle        for a detailed discussion of these responses and categories of students   Table    shows the rules the experts originally used to classify students based on their response to     types of DCT  test items  H   High correctness  e g    or   out of     L     have  developed  an  i ntelligent  tutoring  sys  tem based on computer games for this decimals  A bout a dozen misconceptions have been identified using the DCT and interviews   We  Low number correct  e g    or     out of     with      indicating that any performance level is observable  domain Mclntosh et al           The overall architec  ture of the system is shown in Figure     The com  puter game genre was chosen to provide children with an experience different from  but complementary to  normal classroom instruction and to appeal across the target age range  Grade   and above    Each game  focuses on one aspect of decimal numeration  thinly disguised by a story line   It is possible for a stu  dent to be good at one game or the diagnostic test  but not good at another  emerging knowledge is often compartmentalised    In the  Hidden Numbers  game students are con fronted with two decimal numbers with digits hidden be hind closed doors  the task is to find which number is the larger by opening as few doors as possible  The game  Fly ing Photographer  requires students to place a number on a number line  prompting students to think differently about decimal numbers  The  Number Between  game is also played on a number line  but particularly focuses on the density of the decimal numbers  students have to type in a number between a given pair   Decimaliens  is a classic shooting game  designed to link various representations of the value of digits in a decimal number    NICHOLSON ET AL        UAI      The simple expert rules classification described above  the conditional probability tables associated with each  makes quite arbitrary decisions about borderline cases   network node  For our purposes we consider there to  The use of a BN to model the uncertainty allows it to  be an additional task     the evaluation of the network   make more informed decisions in these cases  Using a  While in theory these tasks can be performed sequen  BN also provides a framework for integrating student  tially  in practice the knowledge engineering process  responses from the computer games with DCT infor  iterates over these task until the resultant network is  mation  The BN is initialised with a generic student  considered  acceptable   In this section we describe  model  with the options of individualising with class  the elicitation of the decimal misconception BN from  room or online DCT results  The BN is used to update  the education domain experts   an ongoing assessment of the student s understanding  to predict which item types that student might be ex pected to get right or wrong  and  using sensitivity analysis  to identify which evidence would most im prove the misconception diagnosis  The development of the BN is described below  The system controller module uses the information       BN VARIABLES  Student misconceptions are represented on two levels  by two variables  The coarseClass node can take the values L  S  A  and UN  whereas the f ineClass node  incorporating all the misconception types identified by the experts  can take the    values shown in column  provided by the BN  together with the student s previ    of Table    Note that the experts consider the clas  ous responses  to select which item type to present to  sifications to be mutually exclusive  If that were not  the student next  and to decide when to present help  the case  then two variables would not be sufficient   or change to a new game   This architecture allows  flexibility in combining the teaching  sequencing tac tic   that is  whether easy items are presented before harder ones  harder first  or alternating easy  hard   coverage of all item types  and items which will most improve the diagnosis   More detailed descriptions of  both the architecture and the item selection algorithm are given in  Stacey et al          The ITS shown in  rather we would require a Boolean variable for each of the classifications  Each DCT item type is made a variable in the BN  rep resenting student performance on test items of those types  student test answers are entered as evidence for these nodes  The following alternatives were consid ered for the possible values of the item type nodes   Figure    including the four computer games  has     Suppose the test contains N items of a given type   been fully implemented  The game interfaces are cur  One possible set of values for the BN item type node is  dren  with deployment and assessment in the class               N   representing the nu m ber of the items the student answered correctly  The number of items may  room environment to take place over the next year   vary for the different types  and for the particular test  rently being assessed for usability on individual chil  set given to the students  but it is not difficult to adapt the BN  Note that the more values for each node  the more complex the overall model  if N were large  e g         this model may lead to complexity problems      The item type node may be given the values  High  Medium  Low   reflecting an aggregated assessment of the student s answers for that item type  For example  if   such items were presented    or   correct would be considered low    or   would be medium  while   or     would be High  For types with   items  medium  encompasses only   correct  while for types with only   itemsl the medium value is omitted completely  This Figure    Intelligent Tutoring System Architecture     EXPERT ELICITATION  It is generally accepted that building a BN for a particular application domain involves three tasks  Druzdzel and van der Gaag              identification of the important variables  and their values      iden  reflects the expert rules classification described above       BN STRUCTURE  The experts considered the coarse classification to be a strictly deterministic combination of the fine classifi cation  hence the coarseClass node was made a child of the fineClass node  For example  a student was considered an L if and only if it was one of an LWH   tification and representation of the relationships be tween variables in the network structure  and     pa  LZE  LRV or LU   rameterisation of the network  that is determining  The type nodes are observation nodes  where entering   UA        N ICHOLSON ET AL        evidence for a type node should update the posterior  hundred students from Grades  probability of a student having a particular misconcep  then pre processed to give each student s results in  tion   This diagnostic reasoning is typically reflected  terms of the        test item types   and      These were were the               in a BN structure where the class  or  cause  is the  number of items of these type   to   respectively  The  parent of the  effect   i e  evidence  node  Therefore  particular form of the pre processing depends on the  an arc was added from the subclass node to each of  item type values used  with the  the type nodes  No connections were added between  ues  a student s results might be         whereas with        N type node val  any of the type nodes  reflecting the experts  intuition  the H M L values  the same students results would be  that a student s answers for different item types are  represented as HHLMHH   independent  given the subclassification   The expert rule classifications were used to generate  A part of the expert elicited BN structure implemented  the priors for the sub classifications   in the ITS is shown in Figure     This network fragment  of the test item types take the form of P Type  shows the coarseClass node  values L S A UN   the  ValuejClassification  detailed misconception fineClass node     values    the domain description  the experts expect particu  X       All the CPTs     As we have seen from  the item type nodes used for the DCT  plus additional  lar classes of students to get certain item types cor  nodes for some games  These additional nodes are not  rect  and others wrong   described in this paper but are included to illustrate  model the natural deviations from such  rules   where  the complexity of the full network   The balded nodes  students make a careless error  that is  they apply  are those in the restricted network used subsequently  their own logic but do not carry it through   in this paper for evaluation and experimentation   ample  students who are thinking according to the  However we do need to  For ex  LWH misconception are predicted to get all of Type   correct  ability of          items  If however that there is a prob  of a careless mistake on any one item   the probability of a score of   is           and the prob  ability of other scores follows the binomial distribu  tion  the full vector for P Type JSubclass LWH  is                                  When  the  item  type   to two decimal places    values  H M L  are  used   the numbers are accumulated to give the vector                  for H  M and L  We note that the ex perts consider that this mistake probability is consid erably less than       say  of the order of        Much more difficult than handling the careless errors in the well understood behaviour of the specific known misconceptions  is to model situations where the ex perts do not know how a student will behave  This was the case where the experts specified     for the classifi cations LU  SU  AU and UN in Table    We modelled the expert not knowing what such a student would do on the particular item type in the BN by using Figure    Fragment of the expert elicited BN currently implemented  Bold nodes are those discussed here                i e   that a student will get each item correct  with  the binomial distribution to produce the CPTs  We ran experiments with different probabilities for       BN PARAMETERS  The education experts had collected data that con sisted of the test results and the expert rule classifi cation on a    item DCT for over two thousand five   An indication as to the meaning of these additional nodes is as follows  The  HN  nodes relate to the Hidden Numbers game  with evidence entered for the number of doors opened before an answer was g iven  and a measure of the  goodness of order  in opening doors  The root node for the Hidden Number game subnet reflects a player s game ability   in this case door opening  efficiency    a single careless mistake   pcm             and          with the CPTs calculated in this manner  to investi gate the effect of this parameter on the behaviour of the system  These number were chosen to give a com bined probability for HIGH  for   items  of             and     respectively  numbers that our experts thought were reasonable  Results are described in Section            BN EVALUATION P ROCESS  During the expert elicitation process we performed the following three basic types of evaluation   First was   NICHOLSON ET AL        Case based evaluation  where the experts  play  with the net  imitating the response of a student with cer tain misconceptions and review the posterior distribu tions on the net   Depending on the BN parameters   it was often the case that while the incorporation of the evidence for the     item types from the DCT test  data greatly increased the BN s belief for a particu lar misconception  the expert classification was not the BN classification with the highest posterior  because it started with a low prior  We found that it was useful to the experts if we also provided the ratio by which each classification belief had changed  although the highest posterior is used in all evaluations   During the use of the BN in the full ITS  see Figure     each  Table      UAI      Expert rule vs expert elicited BN classifica  tion  Type node states   N  pcm O Il  lwh lze lrv lu sdf srn su ate amo mis au un lwh                           lze                          lrv                          lu                          sdf                          srn                           su                            ate                            amo                          mis                         au                          un                                   time student answers are entered  the posteriors  for the fine classification are updated and in turn be come the new priors for that node  in this way  the network adapts to the individual student over a range of games and item types over time  This adaptive as pect allows the system to identify students with mis conceptions that are fairly infrequent in the overall population  This motivated our Adaptiveness evalua  tion  where the experts imitate repeated responses of a student  update the priors after every test and enter another expected test result  This detection of classi fications over repetitive testing built up the confidence of the experts in the adaptive use of the BN  Next  we undertook Comparison evaluation between th e classifications of th e BN compared to the expert  rules on the DCT data    As well as a comparison  grid  see next subsection   we provided the experts with details of the records where the BN classification differed from that of the expert rules   This output  proved to be very useful for the expert in order to understand the way the net was working and to build their confidence in the net   have had on the overall behaviour  The iterative pro cess halts when the exp erts feel the behaviour of the BN is satisfactory        RESULTS  Table   is an example of the comparison grids for the fine classification that were produced during the com parison evaluation phase  Similar grids were produced for the coarse classification  Each row corresponds to the expert rules classification  while each column cor responds to the BN classification  using the highest posterior  each entry in the grid shows how many stu dents had a particular combination of classifications from the two methods  The grid diagonals show those students for whom the two classifications are in agree ment  while the  desirable  changes are shown in ital  ics  and undesirable changes are shown in bold  Note  that we use the term  match    rather than saying that the BN classification was  correct   because the expert  Finally  we undertook a Prediction evaluation which considers the prediction of student performance on in dividual item type nodes rather than direct miscon ception diagnosis  We enter a student s answers for     of the   item type nodes  then predict their answer for the remruning one  this is repeated for each item type  The number of correct predictions gives a meas ure of the accuracy of each model  using a score of   for a correct prediction  using the highest posterior  and   for an incorrect prediction  We also look at the pre dicted probability for the actual student answer  Both measures are averaged over all students   we modified the BN structure or the CPTs  This way we were aware of the implications this change may     We performed these four types of evaluation every time  We note that this evaluation is similar to the compar ison used in  van der Gaag et al            This evaluation method was suggested by an anony mous reviewer  the analysis of results using these predic tion measures is preliminary due to time constraints   ule classification is not necessarily ideal   r  Further assessment of these results by the experts re vealed that when the BN classification does not match the expert rules classification  the misconception with the second highest posterior often did match  The ex perts then assessed whether differences in the BN s  classification from the expert rules classification were in some way desirable or undesirable  depending on how the BN classification would be used  They came up with the following general principles which provided some general comparison measures      it is desirable for expert rule classified LUs to be re classified as an other ofthe specific Ls  similarly for A Us and SUs  and it was desirable for Us to be re classified as an ything  else   because this is dealing with borderline cases that  the expert rule really can t say much about       it is undesirable for   a   specific classifications  i e  not  those involving any kind of   U    to change  because   UAI      Table  NICHOLSON ET AL   Coarse classification grid  expert rules vs ex      pert elicited BN  varying item type values  pcm                      H M L and    N        N s       A  s  L UN      A s L UN      A s L UN  A L UN                                                                                                                                                    fo                                               and  H MJL L A UN S                    fa                                                                                                                                                                                      Table    Fine classification summary  com p arison  ious models compared to the expert rules Method  Type values  Expert BN    N    N H M L  Avg Avg Avg Avg                 N H M L  Avg Avg  EBN learned     DCT   N H M L   N H M L  CaMML constr   CaMML uncons                                                                                                     HfMJL SNOB  Match               Des  change                                                                                      var  Uncles  change                                                                               ond  the expert elicited BN structure and parameters the experts are confident about these classifications   reflects both the experts  good understanding for the  and  b  for any classification to change to UN  be  known fine classifications  and their poor understand  cause this is in some sense throwing away information  ing of the behaviour of  U  students  LU  SU  AU    e g  LU to UN loses information about the    like   and UN   Finally  as discussed earlier  some classes are  behaviour of the students    broken down into fine classifications more than others   Table     shows the coarse classification comparison  grids obtained when varying the probability of a care  resulting in lower priors  so the more common classifi cations  such as ATE and UN  tend to draw in others   less mistake   pcm            and       and the item  O N vs H M L   Each grid is accompanied  Closer inspection also shows that some  undesirable   type values  changes are reasonable  For example a student answer  by the percentages for match  desirable and undesir  ing        is classified as an ATE by both the expert  able change   rule and the H M L BN  since one mistake on any  As the probability decreases  the total  number of matches with expert classifications goes up   item is considered  high   However the   N BN  for  due to more UN students being in agreement  how ever more L S and A students no longer match  which  pcm       classifies the student as UN  since the com bined probability of   careless mistakes  one on each  is considered  undesirable   see above   In effect  the  item type  is very low   definition of A S  and L becomes more stringent as the probability of a careless error decreases  so more move out of A  Land S into UN  and less move from UN into A  L and S  T here are also shifts between undesirable classification differences  for example the   A students who the BN classifies as L  values    N  pcm         in  fact  highly offensive to our experts    shift to the also generally undesirable UN   pcm         It is not possible for reasons of space to present the full set of results for the fine classifications  Table      Set    shows a summary of the expert BN fine clas sification  varying the type values and probability of careless mistake  in terms of percentage of matches  i e  on the grid diagonal   desirable changes and un desirable changes  We can see that matches are higher for H M L than the corresponding   N run  desirable  We believe that the differences between the BN and  changes are lower  while there is no consistent trend  the expert rule classifications are due to the follow  for undesirable changes  The undesirable change per  ing factors  First  the expert rules give priority to the  centages are quite low  especially considering that we  type   and type    results  whereas the BN model gives   item types  An example of a student with answers         who the expert  know some of these can be considered quite justified   equal weighting to all  Table    Set  this is  Section     shows the two prediction measures  see       averaged over all predicted item types  for  rule classifies as AU due to the  High  result for item  all students  Both measures show the H M L model  with a fairly high chance of a careless mistake          ing O N run  Both measures show the probability of  says this student looks like an LHW  as the  a careless error effects the results for the   N models   only difference is the answer for item type    while for  but only the predicted probability shows an effect on  types   and    ignoring the other answers     The BN  pcm                   the BN classifies the student as UN  Sec   giving better prediction results than the correspond  the H M   model results         NICHOLSON ET AL   Table    Accuracy of various models predicting stu dent item type answers  Method  Expert BN  Type values    N H M L                                Avg Avg                                           O N H M L  Avg               N  H M L EBN learned CaMML constr  CaMML  uncons   Avg Pred  Accuracy    N  H M L  Avg Avg  Avg  Avg Pred   Prob                                                                           Overall it is clear that the expert elicited network per forms a good classification of students misconceptions  and captures well the different uncertainties in the ex perts domain knowledge  In addition  its performance is quite robust to changes in parameters such as the probability of careless mistakes or the granularity of the evidence nodes     KNOWLEDGE DISCOVERY  The next stage of the project involved the application of certain automated methods for knowledge discovery to the domain data       CLASSIFICATION  The first aspect investigated was the classification of decimal misconceptions  We applied the SNOB clas sification program Wallace and Dowe         based on the information theoretic Minimum Message Length  MML   SNOB was run on the data from      stu dents on    DCT items  each being a binary value as to whether the student got the item correct or incor rect  with a variety of initial guesses for the number of classes                  All five classifications were very similar  we present here results from the model with the lowest MML estimate    initial classes   Us ing the most probable class for each member  we con structed a grid comparing the SNOB classification with the expert rule classification  Of the    classes produced by SNOB  we were able to identify   that corresponded closely to the expert classifications  i e  had most members on the grid diagonal   Two classes were not found  LRV and SU   Of the other   classes    were mainly combinations of the AU and UN classi fications  while the other   were mainly UNs  SNOB was unable to classify    students         The per centages of match  desirable and undesirable change are shown in Table    set    row     They are compa rable with the expert BN   N and only slightly worse than the expert BN H M L results   UAI      SNOB was then run on the pre processed data consist ing of student answers on the   item types  values   N and H M     The comparison results for this run were not particularly good  For O N type values  SNOB found only   classes     students      not classi fied   corresponding roughly to some of the most pop ulous expert classes  WH  SDF  SRN  ATE and UN  and subsuming the other expert classes  For H M   type values  SNOB found   classes     students       not classified   corresponding roughly to   of the most populous expert classes   WH  SDF  SRN  ATE  UN   plus a class that combined MIS with UN  In this case  ZEs were all grouped with ATEs  as were AMOs  The match results are shown in Table    set    rows   and     Clearly  summarising the results of    DCT into types gives relatively poor performance  it is proposed that this is because many pairs of the classes are dis tinguished by student behaviour on just one item type  and SNOB might consider these differences to be noise within one class     The overall good performance of the classification method shows that automated knowledge discovery methods may be useful in assisting expert identify suit able values for classification type variables       PARAMETER S  Our next investigation was to learn the parameters for the expert elicited network structure  The data was randomly divided into five         splits for training and testing  the training data was used to parame terise the expert BN structures using the Netica BN software s parameter learning feature   while the test data was given to the resultant BN for classification  The match results  averaged over the   splits  for the fine classification comparison of the expert BN struc tures  with the different type values    N and H M L  with learned parameters are shown in Table    set     with corresponding prediction results  also averaged over the   splits  given shown in Table    set     The average prediction probabilities for the BN with learned parameters are better than for the expert BNs for the   N type values        compared to         the other prediction results show no significant difference  The match percentages for both BNs with learned pa rameters are significantly higher than for all the ex pert BNs with elicited parameters  with varying pcm   while both the desirable and undesirable changes are lower  most of the difference is due to the reduction in desirable changes  Within the learned parameter re sults  the match percentage is significantly higher for H M   than   N  while the changes are lower  In both cases  the percentage of undesirable changes is lower than the desirable change  Clearly  learning the pa ww norsys com   NICHOLSON ET AL   UAI           rameters from data  if it is available  gives results that  The undesirable changes include quite a few shifts from  are much closer to the expert rule classification  The  one specific classification to another  which is particu  trade off is that the network no longer makes changes  larly bad as far as our experts are concerned  for exam  to the various  U  classifications  i e  it doesn t shift  ple  several of the networks do not identify the SDF  LUs  SUs  AUs and UNs into other classifications that  and MIS classifications  instead grouping them with  may be more useful in a teaching context  However it  ATE  We also note that the variation between the re  does mean that expert input into the knowledge en  sults for each data set     was much higher than for  gineering process can be reduced  with the parameter  the variation when learning parameters for the expert  learning on an elicited structure giving a BN model  BN structure  This no doubt reflects the difference be  that can be used in the ITS   tween the network structure learned for the different       tween the complexity of the learned network structures  splits  However we did not find a clear correlation be STRUCTURE  and their classification performance  Our third investigation involved the application of Causal MM   CaMM    Wallace and Korb  learn network structure          to  In order to compare the  learned structure with that of the expert elicited BN  we decided to use the pre processed     type data  each  program was given the student data for   variables  the fine classification variable and the     item types    with both the   N values and the H M   values  The same     random         splits of the data were used  for training and testing  The training data was given as input to the structural learning algorithm  and then  used to parameterise the result networks using Netica s parameter learning method   In seeking to improve automated discovery of struc ture by exploiting expert domain knowledge  experts could provide constraints to guide the search and could manually select for further investigation those alterna tive structures which were best interpretable in terms of the domain concepts      CON CLUSION S  This work began with the recognition that we had ac cess to a novel combination of data and information which could enable the developments and compara tive studies reported above  a domain where student  We ran CaMM  once for each split  a  without any  misconceptions abound  involvement of experts with a  constraints and  b  with the ordering constraint that  detailed understanding of the basis of the misconcep  the classification should be an ancestor of each of  tions  and how they relate to domain specific activities   This constraint reflects the gen  and an extensi ve data set of student behaviour on test  the type nodes   eral known causal structure   Each run produced a  items in the domain   The work reported here falls  slightly different network structure  with some hav  into three components   a  the development  by expert  ing the fineClass node as a root  some not   elicitation  of a Bayesian network designed to be the  One  fairly typical network with the ordering constraint con   engine  of an adaptive tutoring system  an elicitation  tained   arcs from the class node to type nodes  with  strongly informed by the experts  detailed understand  one type node also being a root node  only two type  ing of patterns in the data set   b  a study of learn  nodes leaves  and    arcs between type nodes   ing techniques applied to the same data set   looking  The  arcs nodes ratio of the learned structures varies from  at learning of classification  structure  and parameters      to      while the number of parameters varies from    which could be compared against the experts  net  about     to          the structures produced for the  work  and  c  some reflection  based on the experience  H M L data seem simpler using these measures  but  of working with the experts and the automated tools   this is not statistically significant   as to how elicitation and knowledge discovery might be  The percentage match results comparing the CaMML BN classifications  constrained and unconstrained  O  N and H M L  are also shown in Table    sets   and      with the prediction results shown in Table    sets   and     The prediction results for both   N and  combined in the BN knowledge engineering process  A first  important  observation is that the automated techniques were able to yield networks which gave quantitative results comparable to the results from the BN elicited from the experts  This level of matching  H M   are similar to those of the fully elicited expert  provides a form of  validation  of the learning tech  BNs   niques and suggests that automated methods can re  The match percentages are similar to those of  the fully elicited expert BN for the   N  however the  duce the input required from domain experts  It also  undesirable change percentages are higher  while the  supports the reciprocal conclusion regarding the valid  desirable change percentages are lower  For H M     ity of manual construction when there is enough expert  the match results are higher than for the expert BN  knowledge but no available data set  In addition  we  compared to the highest of         with fewer  have seen that the use of automated techniques can          desirable and undesirable changes   provide opportunities to explore the implications of   NI CHOLSON ET AL        mo delli n g choices and to get a feel for design tradeoffs   some examples of this were reported above in both  the initial eli cit ation stage  and the discovery stage  e g    N vs  H M L     UA        networks and decision theory  Int  Journal of AI in  Education  to appear           Mcintosh et  al            J    Mcintosh    S t acey   De s ig ning  Tromp  C     and Lightfoot   D             G iven that elicited BN was based on the expert knowl  K    constructivist computer games for teaching about     J  B    edge that had been accumulated over a period of time  decimal numbers    through much analysis and investigation  how useful  t or   Mathematics Education Beyond        Proc  of  is an automated approach in domains where such de  the   rd A nnual Conf  of the Mathematics Educa  t ailed  validation  knowledge is not available   Our  experience suggests that a hybrid of expert and au  tomated approaches is feas ib le  these methods in a situation  We plan to apply    st u d ent  work on alge  bra  where we have data on student behaviour  but do not have detailed prior exp ert analysis of the data   knowledge the preliminary work undertaken by Elise Dettman  an d thank Brent Boerlage for his assistance with Netica and the anonymous reviewers for their helpful s ugg e s t io ns    modeling  C    Gertn er   A     Van On line stu  for coached problem solving using  B ayesian Networks   In UM     Proc  of the  th  Int  Conf  on User Modeling  p ages           Druzdzel and van der Gaag         Druzdzel   M   and  van der Gaag  L             Building probabilistic net works  Where do the numbers come from  G u est  editors int ro duct ion    IEEE Trans  on Knowledge  and Data Engineering                     He cker man and G ei ger         Heckerman   and             Learning B ayesian networks  A  P   In  and Hanks  S     ed i t ors   UAI     Proc   of the    th Conf  on Uncertainty in A rtificial Intel ligence  pages          San Francisco   and Chickering  D             L earning  Bayesian net  works  the combination of knowledge and statisti In de Mantras  L  and Poole  D     edi  tors  UAI     Proc  of the    th Conf  on Uncer tainty in Artificial Intelligence  pages           San Francisco   K en ne t t et al          Ken n e tt   N ichol son   A              Bayesian networks   R    Korb   K     and  Seabreeze prediction using  In PAKDD       Proc  of the   th Pacific Asia Conf  on Knowledge Discovery and Data Mining  p ages  M ayo and             Dr uzd zel   M   and  rameters from small data sets  application of Noisy  Working Notes of the Workshop on   Bayesian and Causal networks  from inference to data mining      th European Conf  on Artificial in telligence   ECAI          S pir t e s et al          Spirtes              P     G lymo ur   C     and  Causation  Prediction and            Mitrovic                 St acey et al      Stacey   K     Sonenberg  L     Nicholson  A     Boneh  T     and Steinle  V             Modelling teaching for concep  tual change using a Bayesian network   Submitted  to Int  Journal of AI in Education   Stacey and Steinle         Stacey  K  and Steinle               V   A longitudinal study of childen s thin k  ing about decimals  a preliminary analy s i s   In Za slavsky         editor  Proc  of the   rd Conf  of the  Int  Group for the Psychology of Mathematics Edu  van der Gaag et al          van der Gaag  L   Renooij    S     Ale m an    B     and Taal  B            Evaluation of  a probabilistic model for staging of oesophageal car cinoma  In Medical Infobahn for Europe  Proc  of MIE     and GMDS      pages          Amster  dam   OS Press    Heckerman et al          Hecker man   D   Geiger  D      cal data   O ni sko   A    cation  volume    pages           Haifa  PME   D   unification for discret e and gaussian domains  Besnard   al           Wasyluk  H           Learning Bayesian network p a  Search  Number     in Lecture Notes in Statistics   Lehn  K     and Druzdzel  M              D    Onisko et  S pringe r Verlag    Conati et al          Conati   Geiger   tion Research Group of Australasia   pages              Scheines  R   
  the expeeted number of actions required to reach the goal   We describe a method for time critical de cision making involving sequential tasks and stochastic processes  The method employs several iterative refinement routines for solv ing different aspects of the decision mak ing problem  This paper concentrates on the meta level control problem of delibera tion scheduling  allocating computational re sources to these routines  We provide dif ferent models corresponding to optimization problems that capture the different circum stances and computational strategies for de cision making under time constraints  We consider precursor models in which all deci sion making is performed prior to execution and recurrent models in which decision mak ing is performed in parallel with execution  accounting for the states observed during ex ecution and anticipating future states  We describe algorithms for precursor and recur rent models and provide the results of our empirical investigations to date   We represent goals of aehievement in terms of an opti mal sequential decision making problem in which there is a reward function specially formulated for a partie ular goal  For the goal of achieving p as quiekly as possible  the reward is   for all states satisfying p and    otherwise  The optimization problem is to find a policy  a mapping from states to actions  maximiz ing the expected discounted cumulative reward with respect to the underlying stochastic process and the specially formulated reward function  In our formula tion  a policy is nothing more than a conditional plan for achieving goals quickly on average   Introduction  We are interested in solving sequential decision making problems given a model of the underlying dynamical system specified as a stochastic automaton  i e   a set of states  actions  and a transition matrix which we assume is sparse     In the following  we refer to the specified automaton as the system automaton  Our approach builds on the theoretical work in operations research and the decision sciences for posing and solv ing sequential decision making problems  but it draws its power from the goal directed perspective of artifi cial intelligence  Achieving a goal corresponds to per forming a sequence of actions in order to reach a state satisfying a given proposition  In general  the shorter the sequence of actions the better  Because the state transitions are governed by a stochastic proeess  we cannot guarantee the length of a sequenee achieving a given goal  Instead  we are interested in minimizing  Instead of generating an optimal policy for the sys tem automaton  which would be impractical for an automaton with a large state space  we formulate a simpler or restrictert stochastic automaton and then search for an optimal policy in this restricted automa ton  At all times  the system maintains a restricted au tomaton  The restricted automaton and correspond ing policy are improved as time permits by successive refinement  This approach was inspired by the work of Drummond and Bresina  Drummond and Bresina        on anytime synthetic projeC tion  The state space for the restricted automaton corre sponds to a subset of the states of the system au tomaton  this subset is called the envelope of the re stricted automaton  and a special state OUT that rep resents being in some state outside of the envelope  For states in the envelope  the transition funetion of the restricted automaton is the same as in the system automaton  The pseudo state OUT is a sink  i e   all actions result in transitions back to OUT  and  for a given action and state in the envelope  the probability of making a transition to OUT is one minus the sum of the probabilities of making a transition to the same or some other state in the envelope  There are two basic types of operations on the re stricted automaton  The first is called envelope al teration and serves to increase or decrease the num ber of states in the restricted automaton  The second is called policy generation and determines a policy for        Dean et al   b  i   ii   Figure    Stochastic process and a restricted version  the system automaton using the restricted automaton  Note that  while the policy is constructed using the re stricted automaton  it is a complete policy and applies to all of the states in the system automaton  For states outside of the envelope  the policy is defined by a set of reflexes that implement some default behavior for the agent  In this paper  deliberation scheduling refers to the problem of allocating processor time to envelope alteration and policy generation  There are several different methods for envelope al teration  In the first method  we simply search for a  new  path or trajectory from the initial state to a state satisfying the goal and add the states traversed in this path to the state space for the restricted automa ton  This method need not make use of the current restricted automaton  A second class of methods op erates by finding the first state outside the envelope that the agent is most likely to transition to using its current policy  given that it leaves the set of states corresponding the current envelope  There are several variations on this  add the state  add the state and the n next most likely states  add all of the states in a path from the state to a state satisfying the goal  add all of the states in a path from the state to a state back in the current envelope  Finally  there are methods that prune states from the current envelope on the grounds that the agent is unlikely to end up in those states and therefore need not consider them in formulating a policy  Figure   i shows an example system automaton con sisting of five states  Suppose that the initial state is    and state   satisfies the goal  The path         goes from the initial state to a state satisfying the goal and the corresponding envelope is            Fig ure   ii shows the restricted automaton for that en velope  Let  r  x  be the action specified by the pol icy  r to be taken in state x  the optimal policy for the restricted automaton shown in Figure l ii is de fined by  r         r        r       a on the states of the envelope and the reflexes by  r OUT  b  i e    ifX                    r  X    b     All of our current policy generation techniques are based on iterative algorithms such as value iteration  Bellman        and policy iteration  Howard         In this paper  we use the latter  These techniques can be interrupted at any point to return a policy whose  value improves in expectation on each iteration  Each iteration of policy iteration takes    IE    where E is the envelope or set of states for the restricted automa ton  The total number of iterations until no further improvement is possible varies but is guaranteed to be polynomial in lEI  This paper is primarily concerned with how to allocate computational resources to enve lope alteration and policy generation  In the following  we consider several different models  In the simpler models called precursor deliberation models  we assume that the agent has one opportu nity to generate a policy and that  having generated a policy  the agent must use that policy thereafter  Precursor deliberation models include    a deadline is given in advance  specifying when to stop deliberating and start acting according to the generated policy    the agent is given an unlimited amount of time to respond  with a  linear cost of delay There are also more complicated precursor deliberation models  which we do not address in this paper  such as the following two models  in which a trigger event occurs  indicating that the agent must begin following its policy immediately with no further refinement     the trigger event can occur at any time in a fixed interval with a uniform distribution    the trigger event is governed by a more compli cated distribution  e g   a normal distribution cen tered on an expected time In more complicated models  called recurrent deliberation models  we assume that the agent period ically replans  Recurrent deliberation models include    the agent performs further envelope alteration and policy generation if and only if it  falls out  of the envelope  defined by the current restricted automaton    the agent performs further envelope alteration and policy generation periodically  tailoring the restricted automaton and its corresponding pol icy to states expected to occur in the near future The rest of this paper assumes some familiarity with basic methods for sequential decision making in stochastic domains  A companion paper  Dean et al         provides additional details regarding algo rithms for precursor deliberation models  In this pa per  we dispense with the mathematical preliminaries  and concentrate on conveying basic ideas and empir ical results  A complete description of our approach including relevant background material is available in a forthcoming technical report     Deliberation Scheduling  In the previous section  we sketched an algorithm that generates policies  Each policy  r has some value with   Deliberation Scheduling for Time Critical Sequential Decision Making       respect to an initial state x   this value is denoted V   x   and corresponds to the expected cumulative reward that results from executing the policy starting in x  Given a stochastic process and reward function  V   x   is well defined for any policy  r and state x   We are assuming that  in time critical applications  it is impractical to compute V    x   for a given policy and initial state and  more importantly  that it is im practical to compute the optimal policy for the entire system automaton  In order to control complexity  in generating a pol icy  our algorithm considers only a subset of the state space of the stochastic process  The algorithm starts with an initial policy and a restricted state space  or envelope   extends that envelope  and then computes a new policy  We would like it to be the case that the new policy  r  is an improvement over  or at the very least no worse than   the old policy  r in the sense that V r   xo   V    xo         In general  however  we cannot guarantee that the pol icy will improve without extending the state space to be the entire space of the system automaton  which results in computational problems  The best that we can hope for is that the algorithm improves in expecta tion  Suppose that the initial envelope is just the ini tial state and the initial policy is determined entirely by the reflexes  The difference Vrr  xo   V r xo  is a random variable  where  r is the reflex policy and  r  is the computed policy  We would like it to be the case that E V r  x    V r x         where the expectation is taken over start states and goals drawn from some fixed distribution  Although it is possible to construct system automata for which even this improvement in expectation is impossible  we believe most moderately benign navigational environments  for instance  are well behaved in this respect  Our algorithm computes its own estimate of the value of policies by using a smaller and computationally more tractable stochastic process  Ideally  we wo uld like to show that there is a strong correllation be tween the estimate that our algorithm uses and the value of the policy as defined above with respect to the complete stochastic process  but for the time be ing we show empirically that our algorithm provides policies whose values increase over time  Our basic algorithm consists of two stages  envelope alteration  EA  followed by policy generation  PG   The algorithm takes as input an envelope and a policy and generates as output a new envelope and policy  We also assume that the algorithm has access to the state transition matrix for the stochastic process  In general  we assume that the algorithm is applied in the manner of iterative refinement  with more than one invocation of the algorithm  We will also treat en velope alteration and policy generation as separate  so we east the overall process of poliey formation in terms of some number of rounds of envelope alteration fol lowed by poliey generation  resulting in a sequenee of  Figure    Sequenee of restrieted automata and associ ated paths through state space polieies  Figure   depicts a sequenee of automata gen erated by iterative refinement along with the associ ated paths through state spaee traversed in extending the envelope  Envelope alteration can be further classified in terms of three basic operations on the envelope  trajectory planning  envelope extension  and envelope pruning  Trajectory planning eonsists of searching for some path from an initial state to a state satisfying the goal  En velope extension consists of adding states to the enve lope  Envelope pruning involves removing states from the envelope and is generally used only in recurrent deliberation models  Let   r   represent the policy after the ith round and let  tEA  be the time spent in the ith round of envelope alteration  We say that poliey generation is inflexi ble if the ith round of poliey generation is always run to completion on IEil   Policy generation is itself an  iterative algorithm that improves an initial policy by estimating the value of policies with respect to the re stricted stochastic  process mentioned earlier  W hen run to eompletion  policy generation continues to iter ate until it finds a policy that it cannot improve with respect to its estimate of value  The time spent on the ith round of policy generation tpa  depends on the size of the state space IEil   In the following  we present a number of deeision mod els  Note that for each instance of the problems that we eonsider  there is a large number of possible deci sion models  Our seleetion of which decision models to investigate is guided by our interest in providing some insight into the problems of time critical deeision mak ing and our antieipation of the combinatorial problems involved in deliberation scheduling     Precursor Deliberation  In this section we consider the first precursor deliberation model  in which there is a fixed deadline known in advance  It is straightforward to extend this to model    where the agent is given an unlimited re sponse time with a Linear eost of delay  models    and   are more eomplicated and and are not eonsidered in this paper              Dean et al   The Model  Let troT be the total amount of time from the current time until the deadline  If there are k rounds of enve lope alteration and policy generation  then we have tEA    tpa     tEAk  tpak  trOT Case    Single round  inflexible policy genera tion In the simplest case  policy generation does not  inform envelope alteration and so we might as well do all of the envelope alteration before policy generation  and tEA    tpa    troT Here is what we need in order to schedule time for EA  and PG      the expected value  taken over randomly chosen pairs of initial states and goals  of the improve ment of the value of the initial state  given a fixed amount of time allocated to envelope alteration  E V r   xo   V ro xo itEA Ji    the expected size of the envelope given the time allocated to the first round of envelope alteration  E IE IItEA    and    the expected time required for policy generation  given the size of the envelope after the first round of envelope alteration  E  tpa IIE I   Note that  because policy generation is itself an iterative refinement algorithm  we can interrupt it at any point and obtain a policy  for instance  when policy generation takes longer than pre dicted by the above expectation  Each of          and     can be determined empm cally  and  at least in principle  the optimal allocation to envelope alteration and policy generation can be determined  Case II  Multiple rounds  inflexible policy gen eration Assume that policy generation can prof  itably inform envelope alteration  i e   the policy after round i provides guidance in extending the environ ment during round i     In this case  we also have k rounds and tEA   tpa    tEAk  tpak  troT  Informally  let the fringe states for a given envelope and policy correspond to those states outside the enve lope  that can be reached with some probability greater than zero in a single step by following the policy start ing from some state within the envelope  Let the most likely falling out state with respect to a given envelope and policy correspond to that fringe state that is most likely to be reached by following the policy starting in the initial state  We might consider a very simple method of envelope alteration in which we just add the most likely falling out state and then the next most likely and so on  Suppose that adding each additional state takes a fixed amount of time  Let  E V r   xo   V r     xo IIE       m  IE I   m   n  denote the expected improvement after the ith round of envelope alteration and policy generation given that  there are n states added to the m states already in the envelope after round i      Again  the expectations described above can be ob tained empirically  Coupled with the sort of expecta tions described for Case I  e g   E  tPa IIE I      one could  in principle  determine the optimal number of rounds k and the allocation to tEA  and tpa  for    j  k   In practice  we use slightly different statis tics and heuristic methods for deliberation scheduling to avoid the combinq torics  Case III  Single round  flexible policy genera tion Actually  this case is simpler in concept than  Case I  assuming that we can compile the following statistics  Case IV  Multiple round  flexible policy gener ation Again  with  tdditional statistics  e g    E V r  xo  V r     xo IIE       m  IE I   m n  tpa      this case is not much more difficult than Case II       Algorithms and Experimental Results  Our initial experiments are based on stochastic au tomata with up to several thousand states  automata were chosen to be small enough that we can still compute the optimal policy using exact techniques for comparison  but large enough to exercise our ap proach  The domain  mobile robot path planning  was chosen so that it would be easy to understand the poli cies generated by our algorithms  For the experiments reported here  there were     locations that the robot might find itself in and four possible orientations re sulting in     states  These locations are arranged on a grid representing the layout of the fourth floor of the Brown University Computer Science department  The robot is given a tasK to navigate from some starting location to some target location  The robot has five ac tions  stay  go forward  turn right  turn left  and turn about  The stay action succeeds with probability one  the other actions succeed with probability      except in the case of sinks corresponding to locations that are difficult or impossible to get out of  In the mobile robot domain  a sink might correspond to a stairwell that the robot could fall into  The reward function for the sequential des  ision problem associated with a given initial and target location assigns   to the four states corresponding to the target location and    to all other states  We gathered a variety of statistics on how extend ing the envelope increases value  The statistics that proved most useful corresponded to the expected im provement in value for different numbers of states added t o the envelope  Instead of conditioning just on the size of the envelope prior to alteration we found it necessary to condition on both the size of the envelope and the estimated value of the current policy  i e   the   Deliberation Scheduling for Time Critical Sequential Decision Making  value of the optimal policy computed by policy itera tion on the restricted automaton   At run time  we use the size of the automaton and the estimated value of the current policy to index into a table of performance profiles giving expected improvement as a function of number of states added to the envelope  Figure   de picts some representative functions for different ranges of the value of the current policy        Value       s oo           r      s   t                    r   hCXi ie                  n      t        t        t      J          i          l  t        t    r        t     j                            i             t     j            fl  L             t      t                                        Time  secoo ds   Figure    Comparison of the greedy algorithm with standard  inflexible  policy iteration and interruptable  flexible  policy iteration                              Figure    Expected improvement as a function of the number of states n added to initial envelope of size m In general  computing the optimal deliberation sched ule for the multiple round precursor deliberation mod els described above is computationally complex  We have experimented with a number of simple  greedy and myopic scheduling strategies  we report on one such strategy here  Using the mobile robot domain  we generated         data points to compute statistics of the sort shown in Figure   plus estimates of the time required for one round of envelope alteration followed by policy gen eration given the size of the envelope  the number of states added  and value of the current policy  We use the following simple greedy strategy for choosing the number of states to add to the envelope on each round  For each round of envelope alteration followed by pol icy generation  we use the statistics to determine the number of states which  added to the envelope  max imizes the ratio of performance improvement to the time required for computation  Figure   compares the greedy algorithm with the standard  inflexible  pol icy iteration on the complete automaton and with an interruptable  flexible  version of policy iteration on the complete automaton  The data for Figure   was determined from one representative run of the three algorithms on a particular initial state and goal  In another paper   Dean et al         we present results for the average improvement of the start state under the policy available at time t as a function of time         Recurrent Deliberation The Model  In recurrent deliberation models  the agent has to re peatedly decide how to allocate time to deliberation  taking into account new information obtained during execution  In this section  we consider a particular  model for recurrent deliberation in which the agent al locates time to deliberation only at prescribed times  We assume that the agent has separate deliberation and execution modules that run in parallel and com municate by message passing  the deliberation module sends policies to the execution module and the execu tion module sends observed states to the deliberation module  We also assume that the agent correctly iden tifies its current state  in the extended version of this paper  we consider the case in which there is uncer tainty in observation  We call the model considered in this section the dis crete  weakly coupled  recurrent deliberation model  It is discrete because each tick of the clock corresponds to exactly one state transition  recurrent because the exe cution module gets a new policy from the deliberation module periodically  weakly coupled in that the two modules communicate by having the execution mod ule send the deliberation module the current state and the deliberation module send the execution module the latest policy  In this section  we consider the case in which communication between the two modules occurs exactly once every n ticks  at times n   n   n         the deliberation module sends off the policy generated in the last n ticks  receies the current state from the ex ecution module  and begins deliberating on the next policy  In the next section  we present an algorithm for the case where the interval between communications is allowed to vary  In the recurrent models  it is often necessary to remove states from the envelope in order to lower the compu tational costs of generating policies from the restricted automata  For instance  in the mobile robot domain  it may be appropriafe to remove states corresponding to portions of a path the robot has already traversed if there is little chance of returning to those states  In general  there are many more possible strategies for deploying envelope alteration and policy generation in recurrent models than in the case of precursor mod els  Figure   shows a typical sequence of changes to the envelope corresponding to the state space for the restricted automaton  The current state is indicated        Dean et al   Find path to the goal   Extend the envelope  Extend and then prune the envelope      Find path back to the  interval  the execution module is given a new policy       and the deliberation module is given the current state x   It is possible that x  is not included in the enve lope for       if the reflexes do not drive the robot inside the envelope then the agent s behavior throughout the next n tick interval will be determined entirely by the reflexes  Figure   shows a possible run depicting inter vals in which the system is executing reflexively and intervals in which it is using the c urrent policy  for this example  we assume reflexes that enable an agent to remain in the same state indefinitely   Let  n  x  r  x   be the probability of ending up in x  starting from x and following  r for n steps  Suppose that we are given a set of strategies  F  F      As Extend and then prune the envelope is usual in such combinatorial problems with indefi nite horizons  we adopt a myopic decision model  In particular  we assume that  at the beginning of each n tick interval  we are planning to follow the current policy  r for n steps   follow the policy F  r  generated Figure    Typical sequence of changes to the envelope by some strategy F attempting to improve on  r for the next n steps  and thereafter follow the optimal policy intervals during which the system is executing reflexively  r   If we assume that it is impossible to get to a goal    n  n n  n state in the next  n steps  the expected value of using strategy F is given by falls outofthe envelo  n l n curre nt state happens tt be contaied in the new envelo Z  l i  L  n  x  r x   L n  x  F   r  x  V      x          I  falls out of the envelope again     r  ctuTent state is not in the new envelope     current state is in the new envelope  Figure    Recurrent deliberation by   and the goal state is indicated by D  To cope with the attendant combinatorics  we raise the level of abstraction and assume that we are given a small set of strategies that have been determined empirically to improve policies significantly in vari ous circumstances  Each strategy corresponds to some fixed schedule for allocating processor time to envelope alteration and policy generation routines  Strategies would be tuned to a particular n tick deliberation cy cle  One strategy might be to use a particular pruning algorithm to remove a specified number of states and then use whatever remains of the n ticks to generate a new policy  In this regime  deliberation scheduling consists of choosing which strategy to use at the begin ning of each n tick interval  In this section  we ignore the time spent in deliberation scheduling  in the next section  we will arrange it so that the time spent in deliberation scheduling is negligible  Before we get into the details of our decision model  consider some complications that arise in recurrent deliberation problems  At any given moment  the agent is exec uting a polic y  call it  r  defined on the cur rent envelope and augmented with a set of reflexes for states falling outside the envelope  The agent begins exec uting  r in state x  At the end of the c urrent n tick  x EX  i O          x EX  l  where            i  a discounting factor  controlling the degree of influence of future results on the current decision  Extending the above model to account for the possi bility of getting to the goal state in the next  n steps is straightforward  computing a good estimate of v     is not  however  We might use the value of some pol icy other than  r   but then we risk choosing strategies that are optimized to support a particular suboptimal policy when in fact  the agent should be able to do much better  In general  it is difficult to estimate the value of prospects beyond any given limited horizon for sequential decision problems of indefinite duration  In the next section  we consider one possible practical expedient that appears to have heuristic merit       Algorithms and Experimental Results  In this section  we present a method for solving recurrent deliberation problems of indefinite duration using statistical estimates of the value of a variety of deliberation strategies  We deviate from the decision model described in the previous sec tion in one addi tional important way  we allow variable length inter vals for deliberation  Although fixed length facilitate exposition  it is much easier to collect useful statistical estimates of the utility of deliberation strategies if the deliberation interval is allowed to vary  For the remainder of this section  a deliberation strat egy is just a particular sequence of invocations of enve lope alteration and policy generation routines  Delib    Deliberation Scheduling for Time Critical Sequential Decision Making  eration strategies are parameterized according to at tributes of the policy such as the estimated value of policies and the size of the envelopes  The function EIV  F V    IE  l  provides an estimate of the expected improvement from using the strategy F assuming that the estimated value of the current policy and the size of the corresponding envelope fall within the speci fied ranges  This function is implemented as a table in which each entry is indexed by a strategy F and a set of ranges  e g      minV   maxV      miniE  I maxiE             online deliberation scheduling   We determine the EIV function off line by gathering statistics for F running on a wide variety of policies  The ranges are established so that  for values within the specified ranges the expected improvements have low variance  At run time  the deliberation scheduler computes an estimate of the current policy V    deter mines the size IE    I of the corresponding envelope and chooses the strategy F maximizing EIV  F  V    IE       To avoid complicating the online decision making  we have adopted the following expedient which allows us to keep our one step lookahead model  We modify the transition probabilities for the restricted automaton so that there is always a non zero probability of getting back into the envelope having fallen out of it  Exactly what this probability should be is somewhat eompli cated  The particular value chosen will determine just how concerned the agent will be with the prospect of falling out of the envelope  In fact  the value is depen dent on the actual strategies chosen by deliberation scheduling which  in our particular case  depends on EIV and this value of falling back in  We might pos sibly resolve the circularity by solving a large and very complicated set of simultaneous equations  instead  we have found that in practice it is not difficult to find a value that works reasonably well   To build a table of estimates of function EIV off line  we begin by gathering data on the performance of strategies ranging over possible initial states  goals  and policies  For a particular strategy F   initial state x  and policy  r  we run F on  r  determine the elapsed number of steps k  and compute estimated improve ment in value   The experimental results for the recurrent model were obtained on the mobile robot domain with      possi ble locations and hence      states  The actions avail able to the agent were the same as those used to obtain the precursor model results  The transition probabil ities were also the same  except that the domain no longer contained sinks        Yi     Yk  x  k x            x  VF  r  x     V   x    where the first term corresponds to the value of using  r for the first k steps and F   r  there after and the second term corresponds to the case in which we do no deliberation whatsoever and use  r forever  As in the model described in the previous section  we assume that the goal cannot be reached in the next k steps  again it is simple to extend the analysis to the case in which the goal may be reached in less than k steps  Given data of the sort described above  we build the table for EIV  F V    IE   I   by appropriately dividing the data into subsets with low variance  One unresolved problem with this approach is exactly how we are to compute V     x   Recall that  r is only a partial policy defined on a subset of X augmented with a set of reflexes to handle states outside the cur rent envelope  In estimating the value of a policy  we are really interested in estimating the value of the aug mented partial policy  If the reflexes kept the agent in the same place indefinitely  then as long as there was some nonzero probability of falling out of the envelope with a given policy starting in a given state the actual value of the policy in that state would be            Of course  this is an extremely pessimistic estimate for the long term value of a particular policy since in the recurrent model the agent will periodically compute a new policy based on where it is in the state space  The problem is that we cannot directly account for these subsequent policies without extending the horizon of the myopic decision model and absorbing the associ ated computational costs in offline data gathering and       We used a set of    hand crafted strategies  which were combinations of envelope optimization  a  and the following types of envelope alteration     findpath  FP   if the agent s current state X cur is not in the envelope  find a path from Xcur to a goal state  and add this path to the envelope    robustify  R N    we used the following heuris tic to extend the envelope  find the N most lkely fringe states that the agent would fall out of the envelope into  and add them to the envelope    prune  P N    of the states that have a worse value than the current state  remove the N least likely to be reached using the current policy  Each of the strategies used began with findpath and ended with optimization  Between the first and last phases  robustification  pruning and optimization were used in different combinations  with the number of states to be added or deleted E                    exam ples of the strategies we used are  FP R     a    FP P     a    FP P     R     o    FP R      P          FP R     a P         We collected statistics over about      runs generat ing         data points for strategy execution  The start goal pairs were  chosen uniformly at random and we ran the simulated robot in parallel with the plan ner until the goal was reached  The planner executed the following loop  choose one of the    strategies uni formly  at random  execute that strategy  and then pass the new policy to the simulated robot  We found the following conditioning variables to be significant  the envelope size  lEI  the value of the current state V    the  fatness  of the envelope  the ratio of envelope        Dean et al   size to fringe size   and the Manhattan distance  M  between the start and goal locations  We then build a lookup table of expected improvement in value over the time the strategy takes to compute   V      k  as a function of E  V     the fatness  M and the strategy s  To test our algorithm  we took    pairs of start and goal states  chosen uniformly at random from pairs of Manhattan distance less than one third of the diameter of the world  For each pair we ran the simulated robot in parallel with the following deliberation mechanisms   recurrent deliberation with strategies chosen us ing statistical estimates of EIV  LOOKUP   dynamic programming policy iteration over the entire domain  with a new policy given to the robot after each iteration  ITER  and only after it has been optimized   wHOLE   The average number of steps taken by LOOKUP  and WHOLE were        and     respectively  ITER  W hile the improvement obtained using the recurrent deliberation algorithm is only small it is statistically significant  These preliminary results were obtained when there were still bugs in the implementation  how ever  since we have determined that the strategies are in fact being pessimistic  we expect to obtain further performance improvement using LOOKUP  Recall also that we are still working in the comparatively small domain necessary to be able to compute the optimal policy over the whole domain  for larger domains  ITER and WHOLE are computationally infeasible     Related Work and Conclusions  Our primary interest is in applying the sequential de cision making techniques of Bellman  Bellman        and Howard  Howard        in time critical applica tions  Our initial motivation for this research arose in attempting to put the anytime synthetic projec   tion work of Drummond and Bresina  Drummond and Bresina        on more secure theoretical foundations  The approach described in this paper represents a particular instance of time dependent planning  Dean and Boddy        and borrows from  among others  Horvitz   Horvitz        approach to flexible compu tation  Hansson and Mayer s BPS  Bayesian Problem Solver   Hansson and Mayer        supports general state space search with decision theoretic control of in ference  it may be that BPS could be used as the basis for envelope alteration  Boddy  Boddy        describes solutions to related problems involving dynamic pro gramming  For an overview of resource bounded de cision making methods  see chapter   of the text by Dean and Wellman  Dean and Wellman         We have presented an approach to coping with un certainty and time pressure in decision making  The approach lends itself to a variety of online computa tional strategies  a few of which are described in this paper  Our algorithms exploit both the goal directed   state space search methods of artificial intelligence and the dynamic programming  stochastic decision making methods of operations research  Our empirical results demonstrate that it is possible to obtain high perfor mance policies for large stochastic processes in a man ner suitable for time critical decision making  Acknowledgements  Tom Dean s work was supported in part by a Na tional Science Foundation Presidential Young Investi gator Award IRI          by the Advanced Research Projects Agency of the DoD monitored by the Air Force under Contract No  F         C       and by the National Science foundation in conjunction with the Advanced Research Projects Agency of the DoD under Contract No  IRI          Leslie Kaelbling s work was supported in part by a National Science Foundation National Young Investigator Award IRI        and in part by ONR Contract N              ARPA Order       Thanks also to Moises Lejter for his input during the development and implementa tion of the recurrent deliberation model  
  The trajectory of a robot is monitored in a restricted dynamic environment using light beam sensor data  We have a Dynamic Belief Network  DBN   based on a discrete model of the domain  which provides discrete mon itoring analogous to conventional quantita tive filter techniques  Sensor observations are added to the basic DBN in the form of specific evidence  However  sensor data is often par tially or totally incorrect  We show how the basic DBN  which infers only an impossible combination of evidence  may be modified to handle specific types of incorrect data which may occur in the domain  We then present an extension to the DBN  the addition of an invalidating node  which models the status of the sensor as working or defective  This node provides a qualitative explanation of incon sistent data  it is caused by a defective sen sor  The connection of successive instances of the invalidating node models the status of a sensor over time  allowing the DBN to handle both persistent and intermittent faults      IN T RODUCTION  A robot vehicle is to be monitored as it executes a se quence of tasks against a schedule  People and other robots cross its path  so the schedule is not strictly adhered to  On occasions the robot fails  it is late arriving at its next port of call  or it turns left in stead of right  In the current application we use data from a simple sensor  a light beam sensor  which sig nals when some object crosses it  Other sensors are available also for conventional  quantitative  control and will be incorporated later into our framework for discrete probabilistic monitoring  The conventional q uantitative approach to such a tracking problem is to use a controller such as a Kalman Filter  Bar Shalom and Fortmann         which is based on the cycle  predict state  measure   i e  sense   update state estimate  Such quantitative methods are inadequate for handling the gross changes that are the focus of our work  as they are restricted to reporting ever larger covariances  Light beam sen sors provide coarse  comparatively sparse data about movement in an environment  which are not suited to a conventional quantitative treatment  A symbolic rep resentation of change is more informative  as we apply probabilistic reasoning techniques to monitoring gross changes  Belief Networks  Pearl        integrate a mechanism for inference under uncertainty with a secure Bayesian foundation  Belief networks have been been used in various applications  such as medical diagnosis  Spiegelhalter et al         and model based vision  Levitt et al          which initially were more static  i e  essentially the nodes and links do not change over time  Such approaches involve determining the struc ture of the network  supplying the prior probabilities for root nodes and conditional probabilities for other nodes  adding or retracting evidence about nodes  re peating the inference algorithm for each change in evi dence  There has also been work on the dynamic con struction of belief networks  Breese         C harniak and Goldman         but the desired output is still a single static network  Only recently have a few re searchers used belief networks in dynamic domains  where the world changes and the focus is reasoning over time  Such dynamic applications include robot navigation and map learning based on temporal belief networks  Dean and Wellman        and monitoring diabetes  Andreassen et al          For such applica tions the network grows over time  as the state of each domain variable at different times is represented by a series of nodes  These dynamic networks are Marko vian  which constrains the state space to some extent  however it is also crucial to limit the history being maintained in the network  We have developed such a dynamic belief network for discrete monitoring us ing light beam sensor data  Nicholson        which we briefly describe in Section    Sensor data may be noisy or incorrect  In Section   we review how conventional quantitative methods val idate sensor data and reject incorrect data  then de         Nicholson and Brady  scribe the types of incorrect data which may occur in the domain  In Section   we show how the basic DBN  which infers only an impossible combination of evidence  may be modified to handle  and implicitly re ject  specific types of incorrect data  We then present an extension to the DBN in Section   which provides a qualitative explanation of inconsistent data being caused by a defective sensor  allowing us to model ei ther intermittent or persistent faults          THE DOMAIN THE DISCRETE SPATIAL AND TEMPORAL MODEL  The environment  a laboratory in which a robot ve hicle roams  is divided into regions by the light beam sensors  Without significant loss of generality  we re strict attention initially to rectangular regions  We monitor moving objects  which may be robots or peo ple  An object s position is given by the region it is believed to be in  Each light beam sensor provides data about a light beam sensor crossing  BC   the direction of the crossing  and the hegin and tend time points for the time interval over which it occurred  The tempo ral representation is a time line divided at irregular intervals by the tbegin tend time points  The time in tervals between observation data  during which there is no change in the world state  are labelled T   T       T   T  l  etc   We also refer to successive intervals as T and T l   The discrete trajectory for an object is a sequence of region time interval pairs  R  T   The heading of an obj ect in a given region indicates from which direction it entered  i e  one of N  S  E  W   We also assume that we have a model of the object s mobility  the tendency of an object to move  This is a function over time of the speed of the object  the spatial layout  the type of object  and so on  and gives us the probability that it will move at time instant t  which can be projected onto the time interval T       THE DBN MODEL  This discrete spatial and temporal model of the do main may be represented by the discrete valued nodes in the DBN  For now we make the reasonable prac tical assumption that the environment is closed and that the number of objects  N  in the environment  is known and fixed  the more general case is not sig nificantly more complex but involves dynamic mod ification of the network structure  The spatial lay out of regions and sensors is fixed and known  let us suppose that we have M regions and P sensors  Ta ble     gives a summary of the types of nodes  their states  and their function in the network  The world nodes are those that represent the world state space  object position  OBJ   heading  HEAD  and mobility  MOTION   and region occupancy information   R   Nodes are time stamped with the time interval  T   over which they apply and during which the world  Table Node UHJ fl    HEAD  T  MOTION  T   RAT       DBN Node Types   States and l  unctton r   r   rM          Position of object i at timeT u hN hs hE hw Heading of object i  u   unknown   stat  aoye  Mobility of object i  for stationary  stat short            n  Region occupancy   number only  for i  region j contains i objects  HC Otl  i   BC ACT   nc dir  dir   crossing data from sensor i  nc indicates no crossing  dir   dir   the possible directions  nc  dir   dir  both Actual crossings of LB i  both represents objects crossing in both directions  does not change  Observation nodes are those rep resenting the sensor crossings  a node for the crossing data provided by the sensor  BC OBS   and a node representing the actual physical crossing of the whole sensor which occurred  BC ACT   We make this dis tinction between actual and observed because objects may cross a sensor in both directions during the ob servation data time interval  T BC  however the sensors only detect a single directional crossing  The proba bility distribution  PD  for BC OBS   is   P BC OBS  dirl I BC ACT    dirl  P BC OBS  dir  IBC ACT       dir   P BC OBS   dir  IBC ACT    both  P BC OBS  dir  IBC ACT   both   P BC OBS  nc IBC ACT  nc                               During any given time interval T when nothing has changed  there will be N object position  heading  and mobility nodes  and M region nodes  If there are P light beam sensors  a sensor crossing will generate P actual crossing  BC ACT  and observed signal  BC OBS  nodes             ITTTl L   J   L LJ  Figure     a  General expansion of the network   b  Example scenario used throughout this paper  The dynamic construction of a network combines the world model  movement of objects between regions  and the observation model  the light beam sensor data which is generated  as the network grows over time  see Figure l a    The network expansion and inference   Sensor Validation Using Dynamic Belief Networks  algorithm is                       since they are non linear   Make new instiUlces of world nodes  OBJ  HEAD  MOTION  R  for the T l interV l  Connect old  T   nd new  T l  world nodes  Create new observation nodes  BC ACT  BC OBS   Connect world iUld observation nodes  Add da ta  as evidence for obs  nodes  BC OBS   Run inference algorithm to update beliefs   If step   is omitted then the predictions made by the network corresponds to a prediction of the position of an object dependent only on its previous position and its mobility  If the sensor crossing data is added    evidence  then the inference gives an updated estimate of the object position at time interval T    and may also change beliefs about any node in the network  including those before time intervalT  The DBN is multiply connected  requiring compli cated inference algorithms  such as conditioning or clustering  Pearl         The problem of inference for such networks is NP hard  Cooper         however im proved algorithms such as  Jensen et al         have made inference in carefully structured networks feasi ble  Andreassen et al          The DBN as described gives us an inference engine which infers alternative world models  the position of object in regions  with associated probabilities  from both the model of object motion  the prior probabilities for the objects  mobil ity  and the the sensor crossing data  the observation model    Nicholson        provides more details  The example scenario  shown in Figure   b  used throughout this paper is a linear arrangement of   re gions    light beam sensors containing one object  The methods described in this paper also apply to multiple objects and other divisions of the environment  includ ing a grid of sensors   A previous paper  Nicholson and Brady        shows how the DBN may be extended to maintain a limited history of the movement of the object  This provides a solution to the data association problem  DAP   that of deciding which object has given rise to which ob servation  Quantitative solutions to the DAP include certain techniques for handling observations which do not fall within the validation regions  One method is to discard them as  clutter   which is sometime called a false alarm  Bar Shalom and Fortmann         An alternative is to initiate a new track  and hence filter  for such an observation and discontinue it after a cer tain time if no further data supports this hypothesis of a new object  In some quantitative methods track continuation  Bar Shalom and Fortmann      p       is done to handle missing data  If the validation region is empty  the track is extrapolated  If a predetermined number of subsequent validation regions in a row are also empty  the track is dropped       Incorrect light beam sensor data may be classified fol ows        a sensor crossing is signaled but in fact never took place  a fa se positive  This corre sponds to clutter  noise or general false alarms in quantitative methods       Wrong Direction Data  a beam is broken  but the signaled direction of crossing is incorrect  The sensor data is inaccurate  rather than completely wrong  the sensor is certainly malfunctioning       Missing Data  an object moves from one region to another but no sensor crossing data is regis tered  a false negative  This corresponds directly to missed detection in quantitative methods       Wrong Time Data  a sensor crossing does oc cur  and the direction is correct  however the time stamp is incorrect   INCORRECT DATA Quantitative Methods  Quantitative approaches to tracking and sensor vali dation involve a noise model  random perturbations which usually have only a small effect  For the usual case of unbiased estimators  a Gaussian model is ad equate and is optimal for white noise   If the estima tors are biased  there are models for  coloured  noise   These are used for continuous variables  If variables values are discrete a Poisson distribution is used in stead of a Gaussian  To handle gross errors of the sort that are the focus here  a number of different tech niques have been proposed  A threshold called a vali dation gate may be applied to the Gaussian  for exam ple    standard deviations  corresponding to         Alternatively robust statistics  Huber        Durrant W hyte        may be used  where  for example  the error is a linear combination of two Gaussians  Fi nally non parametric statistics have been developed  but they are more difficult to compute and analyse  as     Ghost Data         Incorrect Data for the Domain  Suppose we know that the object is in region R  at timeT  The observation BC OBS  of either direction of crossing must be ghost data  However if we know the object is in region Rt and the next data received is BC OBS    dir   then this may be either ghost data or wrong direction data  Obviously we are not always able to determine immediately that data is in correct  this may depend on the combination of data received  Suppose that we do not know the where abouts of the only object in the environment and that we receive two pieces of data  BC OBS    dirt and BC OBS    dir   Received together  the two obser vations are not mutually compatible  they are incon sistent  one must be a ghost crossing  or they both might be   If they are observed sequentially  there             Nicholson and Brady  may have been some missing crossings  or again one or both are ghost data  We want to represent these as possible but competing alternatives  and to allow subsequent data to support a particular alternative  In this paper we do not deal with the possibility that both ghost and wrong direction data could be caused by an object which the system does not know about  we assume that all initialisation information is correct and that no new objects appear  The main point to be noted for both ghost and wrong direction data is that there is an observation node with evidence in the DBN which directly represents the incorrect data  We have based the DBN on the assumption that the time frames are determined by the sensor data which corresponds to a change of state  i e  an object has moved between regions  Missing data means that an abject has moved undetected to another region  In some situations we can model this missing data within the existing DBN expansion and inference algorithm  Suppose  for example  there is a missing crossing for sensor LB   and an observation is received for another sensor  LB   While adding the received observation  BC OBS    ciir   we create a negative data node far the first sensor  BC OBS    DC  which represents the missing crossing  although with incorrect time stamp   However if nothing has changed  the network has not been expanded  and there is not even an incorrect DC signal recorded  If the object that made the unde tected movement generates the next positive observa tion  then there will never be a BC OBS added with evidence nc that actually represents the wrong read ing  If the region the object has moved into undetected is otherwise unoccupied this may cause a subsequent detected sensor signal that would be considered ghost data  or wrong direction data  The higher level rea soning and additional expansion of the DBN which is required to handle this missing data is given in  Nichol son         If the time stamp is incorrect but the temporal order of the observation data nodes added to the network is correct  then wrong time data will only affect the system s temporal reasoning  for example comparing against schedules and predictions  If the error in the time stamp is wrong to the extent the order of the BC nodes is wrong  this will generate problems of missing data and ghost crossings  Such incorrect ordering of data cannot be handled within the network and is not considered in this paper      HANDLING INCORRECT DATA WI THIN THE BASIC DBN  The basic DBN does not handle inconsistent data  it finds the evidence impossible and rejects it  We can modify the existing DBN to provide a mechanism for handling certain kinds of inconsistent data        MODIFYING THE PD FOR BC OBS  The first three types of incorrect data which we iden tified above involve a discrepancy between the sensor crossing data received by the DBN controller  and the crossing which actually took place  We have already modeled the distinction between the crossing which took place and the data received by creating the two types of sensor crossing node  BC ACT and BC OBS  The modification to the existing DBN involves chang ing the probability distribution for the BC OBS node  Instead of using binary values  we represent the uncer tainty in the network itself  as the PD entries for each BC OBSr become  P BC OBS dir IBC ACT dir   conft P BC OBS dir IBC ACT dir      con J    P BC OBS nciBC ACT dir      conh     ok wrong miss   P BC OBS dir IBC ACT dir      conft    P BC OBS dir IBC ACT dir   con J P BC OBS nciBC ACT dir      con t     wrong ok miss   P BC OBS dir IBC ACT nc     con      P BC OBS dir jBC ACT nc   l conh    P BC OBS ncjBC ACT nc  con    ghost ghost ok  The confidence in the observation is given by some value based on a model of the sensor s performance and is empirically obtainable  conft is the confidence in the positive sensor data  con   is the confidence in the negative sensor data  or    con   is the probability of ghost data   We have modeled positive data being ghost or wrong direction data as being equiprobable   this need not be the case and can be replaced by any alternative plausible values  Likewise for negative data  although the equiprobable direction of the actual crossing seems intuitively reasonable       RESULTS FOR UNINITIALISED EXAMPLE  We now show the results from the modified DBN for the example environment  with the position of the ob ject at To unknown  The sensor observations made are as follows  Crossing  To Tt  to to T a to T  to  Tt T  T  T   BC OBS  nc  BC OBSt dir  nc  nc  DC  nc  nc  nc  BC OBS  nc dir  dir  dir   Table   and Figure   shows the beliefs inferred by the DBN after each new observation is received and the network expanded  Each row of example diagrams shows the updated beliefs for the position of the object at some timeT  The observations are shown between the appropriate rows  Each column corresponds to the belief at some timeT for the position of the object over time  i e  shows the inferred trajectory  We make the   Sensor Validation Using Dynamic Belief Networks  Table    Beliefs inferred by the modified DBN for inconsistent observations  Initial position unknown   conf                                                                                                                                                                                                                                                                                                                                                                                                                                      BC OBSt Tt  BC OBSa T   correct ghost ghost correct wrong direction ghost ghost wrong direction ghost ghost The first two alternatives have the same probabilities and are considered the most likely  i e  approaching     probability    Beliefs during Ta  The additional BC OBSa Ta  crossing  from Ra to   acts as support for the BC OBS  T    observation being correct  belief BC ACT  T      dir            and belief BC ACTt Tl    nc            i e  BC OBSt To  was probably ghost data  Beliefs during T    The additional BC OBSa T   crossing is further support for the alternative that the first observation was ghost data and second correct  belief BC ACT  Tt  nc          and beliet BC ACTa T    nc            The DBN has inferred that the object is probably initially in Rt  belief OBJt To  r                    RESULTS FOR INITIALISED EXAMPLE  DATA N   U       DAT  Ul   A   I I           c   I   I   I I I  I   I I   I I    I                   Figure    Object position beliefs for inconsistent ob servations from Table    The belief that the object is in a region is indicated by the intensity of shading   following observations on these results  Beliefs during T     alternatives are being main tained explicitly  all equally probable  Beliefs during T   The DBN is nearly certain that the OBJ moved R  to R   The initial beliefs  i e  the Oth instance  have been revised  indicating that the OBJ was very likely to have been in R   If the data was ghost data  considered unlikely   there is a small chance that the object started in R    Ra or  There is also the alternative that the crossing occurred but in the opposite direction  Hence the belief for R   ghost plus wrong direction alternatives  is larger than R  and R   ghost only   Beliefs during T    The system now maintains the alternatives   J J  Figure                             I                   Object position beliefs with successive BC  INV nodes unconnected   Figure   shows the beliefs inferred with the object initially in R   with no observations added  column      then for the   alternative observations shown  We make the following observations on these results  No observations  The object may stay in R  or move into Rz  Sensors LB   and LB  should generate a no crossing signal  because there are no object in the re gions they separate  however the DBN infers a small probability of a ghost crossing signal  The beliefs in ferred for the signal BC OBSt are a combination of the possible nc or dir   plus possible incorrect data from the sensor  hence the predicted observation probabili ties differ from the actual crossings predicted  Observation A  For the BC OBS    dir  crossing data  the DBN correctly infers that this might be cor rect data  i e  BC ACTt   dirt  or ghost data  i e  BC ACT    nc               Nicholson and Brady Because there was no object in R  at To  the BC ACT    dir  crossing must be icorrct data  it may be either ghost data  or wrong duectlon data   Observation B   BC ACT f   BC INV T   The DBN infers that BC ACTa must be nc  implicitly rejecting the observation as incorrect data   Observation C        Our model includes observations as specific evidence for a variable  the BC OBS node  One possible al ternative would be to model the uncertainty in the accuracy of the observation by using virtual evidence  Pearl         which is given as a likelihood ratio of the states of the BC OBS node  If the data was for a dir  crossing of sensor LB    then the spe cific evidence using the existing scheme would be   t evidenc e BC OBS     dir    The corresponding virtual evidence for takes the form dir   DIR   IC  i e  conf     con         con      This use of virtual evidence provides the same results as modifying the PD for BC OBS  Since the BC OBS evidence is the physical output of a sensor  we prefer to enter it as specific evidence and model the difference between  he observation from the sensor and the actual crossmg within the DBN itself     EXPLAINING BAD DATA AS A DEFECTIVE SENSOR  The modification to the DBN described in the pre vious section provides a mechanism for handling  by implicitly rejecting  certain inconsistent data  It rep resents adequately the underlying assumptions about the data uncertainty  which are that the observed sen sor crossing might not match the actual sensor crossing that took place  However it does not provide an ex planation of why the observed sensor data might not reflect the actual crossing  We want to represent the most usual source of incorrect data  namely a defective sensor       BC OBS T   Using Virtual Evidence  THE INVALIDATING NODE  We adapt an idea that has been used in other re search areas  that of a moderating or invalidating con dition  In the social sciences and psychology  the term  moderator  is used for an alternative variable that  messes up  or  moderates  the relationship between other variables  Zedeck        Wermuth        Wer muth         A similar idea has been used in expert system research  in  Andersen et al         such nodes are called  invalidators   Of course  this idea is also fa miliar to the AI community  Winston  Winston        described the notion of a censor  which acts as an  un less  condition  if a BC ACT occurs  then BC OBS will be generated unless the sensor is defective   Figure    Adding the invalidating node  the DBN  BC INV   to  We add a node  BC INV  the invalidating node  which has two states   work  det  short for  working  and  defective   It is connected as a predecessor of BC OBS  see Figure     The PD for BC OBS for ghost data  wrong direction data  and missing data is given by     P BC OBS dir  IBC ACT dir  BC INV vort      P BC OBS dir l BC ACT dir  BC INV vort    P BC OBS nc I BC ACT  nc BC INV vort         P BC OBS dir  I BC ACT dirl BC INV def        P BC OBS nc I BC ACT dir  BC INV def        P BC OBS dir  I BC ACT dir  BC INV def        P BC OBS nc I BC ACT dir  BC INV def        P BC OBS dirl I BC ACT nc BC INV def        P BC OBS dir  I BC ACT nc BC INV def         The question then arises  what are the prior probabil ities for BC INV  We explicitly represent how likely it is that the sensor is working correctly by the prior probabilities for BC INV  which can be obtained from empirical data  con  is now explicitly the confidence that the sensor is working  P BC INV vort I     conf P BC INV def I      conf      RESULTS FOR SENSOR STATUS  The inference algorithm was run for the same set of al ternative observations  A  B and C   on the DBN with the BC INV nodes added  again the object is initially in R  and con          The additional beliefs inferrd for the BC INV nodes having state def are shown m Figure   under the appropriate sensor  in row labelled P def    For cases A and B  the DBN infers that all nc observations for sensors LB  and LBa are correct  i e  BC INV  wort  because there were no objects in adjacent regions to move across these sensors  In case C the DBN infers correctly that sensor LBa must be dfective  i e  BC INVa   def   there is a small pos sibility that the nc observation for sensor LBt may be incorrect  if there is missing data       MODELING SENSOR STATU S OVER TIME   Sensor Validation Using Dynamic Belief Networks  P BC OBS     dir  IBC ACT dir  BC INV def  x P BC OBS dir IBC ACT dir  BC INV de f  x P BC OBS nciBC ACT nc BC INVdef  x      BC OOS T I   Figure      Modeling sensor status over time   The invalidating node provides the explicit representa tion of the cause of incorrect data   a defective sensor  However  there is no connection between successive BC INV nodes  which means no correlation between the working status of a sensor at different times  If the DBN infers that a sensor is defective at some time T because the dat a received has been wrong  it should also effect the interpretation put on subsequent  and possibly earlier  data from that sensor  To provide such a model of the sensor  we assume that at the ini tial time T  all BC I NV   T    nodes have some prior such as described above  At each time step  a copy is made of all the BC INV nodes  whether or not any data is received for that sensor   and each is connected to its success or  see Figure     The PO for each BC INV T l  is then given by   A persistent fault may be modeled by X equals    but without the need to change the probability distribu tion for BC OBS  An example of a persistent fault is the incorrect wiring of the sensor so that the crossing direction is wrong each signal  In practice  a controller will request confirmation of the status of the sensor  or receive information that it has been repaired  In this case  BC INV time of report  will h av e no prede cessors and the prior will reflect the confidence in the status report  Results from the DBN with the BC INV T  to BC INV  T l  connection for the same set of observations are shown in Figure    conf         d         X      Since X      once sensors LB   case B  and LB   case C  has been identified as definitely defective  the DBN infers that the probability it was defective initially  BC INV  To    def  is                                    iI  I I  I lffl   I I  JI                         d  where d is a degradation factor and X is related to the consistency of the fault  The degradation factor dis the probability that a sen sor which has been working during the previous time interval has begun to respond defectively  It is based on a model of the expected degradation of the sensor and is a function of the time between sensor readings         Ot                    I  i    I I    f   I I   f   I        Tt l I l  do    P BC INV T I   vork IBC INV T       vork       IP BC INV T I   def I BC INV T   vork       d P BC INV T l   def I BC INV T    def      X P BC INV T l   vork IBC INV T       def  X           PERSISTENT AND INTERMITTENT FAULTS  There are two general models for a defective sensor  an intermittent fault  which means that not every signal from the sensor is incorrect  a persistent fault  that manifests itself for each observation  One method for modeling an intermittent fault is to make the variable X strictly positive  However if the DBN infers from the data that  i  BC INV T     def  and  ii  BC INV Ti I   work then the fault detected during T  cannot be passed on to T    An alterna tive is to have X     all the time  i e  once a sensor is known to be defective it remains defective  and change the PD for BC OBS so that if a defective sensor can still produce correct data   Figure    Object position beliefs inferred by DBN with successive invalidating nodes connected       MODELING DIFFERENT DEFECTS  The current BC  INV node  with only two states  does not allow the explanation to distinguish between types of defects  We can increase the BC INV states to  work  def ghost  def dir  def miaa   for ghost data  wrong direction data and missing data respec Details of and results for these additional tively  states  as well as results for various combinations of con f   d and X may be found in  Nicholson             CONCLUSIONS  The basic DBN provides discrete tracking of objects based on light beam sensor data  in a method which is analogous to quantitative filter techniques  In this paper we have described a solution to the problem of incorrect or noisy data  By changing the PD for the BC OBS node to contain values other than   or    the DBN is able to handle inconsistent data  rather than simply inferring a contradiction in the evidence  The addition of a node which models the status of the sensor as working or defective  as another parent of the BC OBS node  provides an explanation of the incorrect data as being caused by a defective sensor              Nicholson and Brady  The connection of the successi ve instances of the inval idating node models the status of a sensor over time  allowing the DBN to handle both persistent and in termittent faults  We have shown that a combination of AI techniques   discrete representation and reason ing with uncertainty   can provide a solution to a real world problem  i e incorrect sensor data  Moreover  the solution is in some ways more intuitive than equiv alent conventional quantitative methods  Acknowledgements  
