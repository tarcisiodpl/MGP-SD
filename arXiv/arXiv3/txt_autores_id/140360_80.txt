 Value of information analyses provide a straightforward means for selecting the best next observation to make  and for determining whether it is better to gather additional information or to act immediately  Determining the next best test to perform  given a state of uncertainty about the world  requires a consideration of the value of making all possible sequences of observations  In practice  decision analysts and expert system designers have avoided the intractability of exact computation of the value of information by relying on a myopic approximation  Myopic analyses are based on the assumption that only one additional test will be performed  even when there is an opportunity to make a large number of observations  We present a nonmyopic approximation for value of information that bypasses the traditional myopic analyses by exploiting the statistical properties of large samples      INTRODUCTION  A person faced with a decision usually has the opportunity to gather additional information about the state of the world before taking action  Decisiontheoretic methods for determining the value of gathering additional information date back to the earliest literature on the principle of maximum expected utility  MEU   These methods form an integral part of many probabilistic expert systems  such as Gorrys congestive heart failure program  Gorry and Barnett        and Pathfinder  Heckerman et al         Heckerman et al          an expert system that assists pathologists with the diagnosis of lymph node diseases  To decide whether or not to perform a test  an expert   From the Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence  Los Angeles  CA  pages         Morgan Kaufmann  July        Blackford Middleton Section on Medical Informatics Division of General Internal Medicine Stanford University Medical Center Stanford  California        system computes the value of information of that test  The system recommends that the test be performed if and only if the value of information exceeds the cost of the test   In most decision contexts  a decision maker has the option to perform several tests  and can decide which test to perform after seeing the results of all previous tests  Thus  an expert system should consider the value of all possible sequences of tests  Such an analysis is intractable  because the number of sequences grows exponentially with the number of tests  Builders of expert systems have avoided the intractability of complete value of information analyses by implementing myopic or greedy value of information analyses  In such analyses  a system determines the next best test by computing value of information based on the assumption that the decision maker will act immediately after seeing the results of the single test  Gorry et al         Heckerman et al          In this paper  we present an approximate nonmyopic analysis  The analysis avoids the traditional myopic assumption by making use of the statistical properties of large samples      VALUE OF INFORMATION COMPUTATIONS FOR DIAGNOSIS  We discuss myopic and nonmyopic value of information computations in terms of the simple model for diagnosis under uncertainty represented by the influence diagram in Figure    In this model  the chance node H represents a mutually exclusive and exhaustive set of possible hypotheses  and the decision node D represents a mutually exclusive and exhaustive set of possible alternatives  The value node U represents the utility of the decision maker  which depends on the outcome of H and the decision D  The chance nodes E    E            En are observable pieces of evi  This prescription for action assumes that the delta property holds  See Section      D U H  E   En   E   such that we should take action D if and only if the probability of H exceeds p   This threshold is the probability of H at which the decision maker is indifferent between acting and not acting  That is  p is the point where acting and not acting have equal utility  or p U  H  D        p  U  H  D        p U  H  D        p  U  H  D  In Equation    U  H  D  is the decision makers utility for the situation where H occurs and action D is taken  U  H  D  is the utility when H occurs and action D is not taken  and so on  Solving Equation   for p   we obtain C p       C  B where C is the cost of the decision  E   C   U  H  D   U  H  D        and B is the benefit of the decision Figure    An influence diagram representation of the problem of diagnosis under uncertainty  The decisionmakers utility  rounded rectangular node  U   depends on a hypothesis  oval node  H  and a decision  square node  D   The variables Ei are pieces of evidence or tests about the true state of H   B   U  H  D   U  H  D        If the decision maker has observed pieces of evidence E    E            Em   then the decision maker should choose action D if and only if p H E          Em     p   In terms of the odds formulation  he should act if and only if O H E            Em     dence or tests about the true state of H  This model is identical to that for Pathfinder  Heckerman          p    p       We make several simplifying assumptions  First  we assume that H is a binary chance variable and D is a binary decision variable  We use H and H to denote the two outcomes of H  and D and D to denote the two outcomes of D  For definiteness  we assume that the decision maker chooses D  as opposed to D   when H occurs  Second  we assume that each piece of evidence  E    E            En   is binary  Finally  we assume that each piece of evidence is conditionally independent of all other evidence  given H and H  In Section    we relax these assumptions   The weight of evidence  wi   is defined as the log of the likelihood ratio  ln i   Mapping likelihood ratios into weights of evidence allows us to update the probability of H through the addition of the weights of evidence  Referring to Equations   and    we can rewrite the threshold probability condition in terms of the loglikelihood ratio where wi   ln i   The decision maker should choose action D if and only if  Using the assumption of conditional independence of evidence  we can calculate the posterior probability of the hypothesis by multiplying together all of the p Ei  H  p H  likelihood ratios  p E   with the prior odds  p H    i  H   In this expression  W  is the decision threshold in terms of weights of evidence   p H Ei           Em   p E   H  p Em  H  p H        P  H Ei           Em   p E   H  p Em  H  p H   We can write this equation more compactly in odds form as m Y O H Ei           Em     O H  i     i    where i is the likelihood ratio  p Ei  H  p Ei  H     Because D and H are binary  it follows from the MEU principle that there exists a threshold probability p    W    m X i       wi  ln  p  ln O H    W     p       MYOPIC ANALYSIS  Let us assume that the user of a diagnostic system has instantiated zero or more pieces of evidence in the influence diagram shown in Figure    We can propagate the effects of these instantiations to the uninstantiated nodes  and remove the instantiated nodes from the influence diagram  This removal leaves an influence diagram of the same form as that shown in Figure    To simplify our notation  we continue to refer to the remaining pieces of evidence as E    E            En   also  we use p H  to refer to the probability of the hypothesis H  given the instantiated evidence    The decision maker now considers whether he should observe another piece of evidence before acting  A myopic procedure for identifying such evidence computes  for each piece of evidence  the expected utility of the decision maker under the assumption that the decision maker will act after observing only that piece of evidence  In addition  the procedure computes his expected utility if he does not observe any evidence before making his decision  If  for each piece of evidence  the expected utility given that evidence is less than the expected utility given no evidence  then the decision maker acts immediately in accordance with Equation    Otherwise  the decision maker observes the piece of evidence with the highest expected utility  then  the myopic procedure repeats this computation to identify additional evidence for observation  Because the myopic procedure allows for the gathering of additional evidence  the procedure is inconsistent with its own assumptions  We return to this observation in the next section  In the remainder of this section  we examine the computation of expected utilities and introduce notation  Let EU  E  CE   denote the expected utility of the decision maker who will observe E at cost CE   and then act  Let CE E  CE   be the certain equivalent of this situation  That is  U  CE E  CE      EU  E  CE         or  CE E  CE     U    EU  E  CE        where U    is the decision makers utility function  a monotonic increasing function that maps the value of an outcome  e g   in dollars  to the decision makers utility for that outcome  Similarly  let EU       denote the expected utility of the decision maker if he acts immediately  and let CE      denote the certain equivalent of this situation  Thus  in the myopic procedure  a decision maker should observe the piece of evidence E for which the quantity CE E  CE    CE             is maximum  provided it is greater than    In this paper  to simplify the discussion  we assume that the delta property holds   The delta property states that an increase in value of all outcomes in a lottery by an amount   increases the certain equivalent of that lottery by    Howard         Under this assumption  we obtain CE E  CE     CE E      CE        where CE E     is the certain equivalent of observing E at no cost  Therefore  we have CE E  CE    CE        V I E   CE          The primary result of this researchthat we can use the central limit theorem to make tractable an approximate nonmyopic analysisis unaffected by this assumption   where V I E    CE E      CE             is the value of information of observing E  The quantity V I E  represents the largest amount that the decision maker would be willing to pay to observe E  When we compare Expression    with Equation     we see that  in the myopic procedure  a decision maker should observe the piece of evidence E  if any  for which the quantity V I E   CE  N V I E       is maximum and positive  We call N V I E  the net value of information of observing E  The decision maker usually specifies directly the cost of observing evidence  In contrast  we can compute V I E  from the decision makers utilities and probabilities  Specifically  from Equations   and     we have V I E    U    EU  E       U    EU        To simplify notation  we use the abbreviations EU  E      EU  E  and EU        EU    Thus  we obtain V I E    U    EU  E    U    EU          The computation of EU    is straightforward  We have  p H U  H  D    p H U  H  D       p H   p  EU        p H U  H  D    p H U  H  D      p H    p      by definition of p   To compute EU  E   let us assume that E is defined such that the observation of E increases the probability of H  If p H E    p and p H E    p   then V I E       because the decision maker will not change his decision if he observes E  Similarly  if p H E    p and p H E    p   then V I E       Thus  we need only to consider the case where p H E    p and p H E    p   Let us consider separately the cases H and H  We have EU  E H         p E H U  H  D    p E H U  H  D  and EU  E H         p E H U  H  D    p E H U  H  D  where EU  E H  and EU  E H  are the expected utilities of observing E  given H and H  respectively  To obtain the expected utility of observing E  we average these two quantities EU  E    p H EU  E H    p H EU  E H       To compute V I E   we combine Equations         and       Other names for V I E  include the value of perfect information of E and the value of clairvoyance on E       NONMYOPIC ANALYSIS  As we mentioned in the previous section  the myopic procedure for identifying cost effective observations includes the incorrect assumption that the decision maker will act after observing only one piece of evidence  This myopic assumption can affect the diagnostic accuracy of an expert system because information gathering might be halted even though there exists some set of features whose value of information is greater that the cost of its observation  For example  a myopic analysis may indicate that no feature is cost effective for observation  yet the value of information for one or more feature pairs  were they computed  could exceed the cost of their observation  There has been little investigation of the accuracy of myopic analyses  In one analysis  Kalagnanam and Henrion        showed that a myopic policy is optimal  when the decision makers utility function U    is linear  and the relationship between hypotheses and evidence is deterministic  In an empirical study  Gorry        demonstrated that the use of a myopic analysis does not diminish significantly the diagnostic accuracy of an expert system for congenital heart disease  In a correct identification of cost effective evidence  we should take into account the fact that the decision maker may observe more than one piece of evidence before acting  This computation must consider all possible ordered sequences of evidence observation  and is  therefore  intractable  Let us consider  however  the following nonmyopic approximation for identifying features that are cost effective to observe  Again  we assume that the delta property holds  First  under the myopic assumption  we compute the net value of information for each piece of evidence  If there is at least one piece of evidence that has a positive net value of information  then we identify for observation the piece of evidence with the highest net value of information  Otherwise  we arrange the pieces of evidence in descending order of their net values of information  Let us label the pieces of evidence E    E            En   such that N V I Ei     N V I Ej    if and only if i   j  Next  we compute the net value of information of each subsequence of E    E            En   That is  for m               n  we compute the difference between the value of information for observing E    E            Em   and the cost of observing this sequence of evidence  If any such net value of information is greater than    then we identify E  as a piece of evidence that is cost effective to observe  Once the decision maker has observed E    we repeat the entire computation described in this section  This approach does not consider all possible test sequences  but it does overcome one limitation of the myopic analysis  In particular  the method can iden   tify sets of features that are cost effective for observation  even when the observation of each feature alone is not cost effective      VALUE OF INFORMATION FOR A SUBSET OF EVIDENCE  As in the myopic analysis  we assume that the decision maker can specify the cost of observing a set of evidence  In this section  we show how we can compute the value of information for a set of evidence from the decision makers utilities and probabilities  As in the previous section  let us suppose that the decision maker has the option to observe a particular subset of evidence  E    E            Em   before acting  There are  m possible instantiations of the evidence in this set  corresponding to the observation of Ei or Ei for every i  Let E denote an arbitrary instantiation  and let ED and ED denote the set of instantiations E such that p H E    p and p H E   p   respectively  The computation of the value of information for the observation of the set  E    E            Em   parallels the myopic computation  In particular  we have EU  E        Em     p H EU  E        Em  H   p H EU  E        Em  H         where  and  EU  E        Em  H    P  EED p E H  U  H  D   P  EED p E H  U  H  D   EU  E        Em  H    P  EED p E H  U  H  D   P  EED p E H  U  H  D               To obtain V I E   we combine Equations         and     When m is small  we can compute directly the sums in Equations    and     When m is large  we can compute these sums using an approximation that involves the central limit theorem as follows  First we express the sums in terms of weights of evidence  We have X p E H    p W   W   H       EED  X  p E H    p W   W   H         p E H        p W   W   H         p E H        p W   W   H         EED  X  EED  X  EED   where W and W  are defined in Equation    The term p W   W   H   for example  is the probability that the sum of the weight of evidence from the observation of E    E            Em exceeds W    That is  p W   W   H  is the probability that the decision maker will take action D after observing the evidence  given that H is true   p W H   Next  let us consider the weight of evidence for one piece of evidence  We have wi  p wi  H   p wi  H   p Ei  H  ln p E i  H   p Ei  H   p Ei  H   p Ei  H  ln p E i  H   p Ei  H   p Ei  H   W   To simplify notation  we let p Ei  H     and p Ei  H      The expectation and variance of w  given H and H  are then        EV  w H     ln         ln                                  V ar w H         ln  EV  w H     ln                 ln                                  V ar w H         ln   Now  we take advantage of the additive property of weights of evidence  The central limit theorem states that the sum of independent random variables approaches a normal distribution when the number of variables becomes large  Furthermore  the expectation and variance of the sum is just the sum of the expectations and variances of the individual random variables  respectively  Because we have assumed that evidence variables are independent  given H or H  the expected value of the sum of the weights of evidence for E    E            Em is EV  W  H     m X i    EV  wi  H         m X i    V ar wi  H         Thus  p W  H   the probability distribution over W   is m X  p W  H   N    i    EV  wi  H    m X i    The expression for H is similar   V ar wi  H    Figure    The probability that the total weight of evidence will exceed the threshold weight is the area under the normal curve above the threshold weight W   shaded region   Finally  given the distributions for H and H  we evaluate Equations    through    using an estimate or table of the cumulative normal distribution  We have Z   t     p W   W   H     e   dt         W  where    EV  W  H  and    V ar W  H   The probability that the weight will exceed W  corresponds to the shaded area in Figure    Again  the expression for H is similar  In this analysis  we assume that no probability  p Ei  H  or p Ei  H   is equal to   or    Thus  all expected values and variances are finite  We relax this assumption in the next section      RELAXATION OF THE ASSUMPTIONS  We can relax the assumption that evidence is twovalued with little effort  In particular  we can extend easily the odds likelihood inference rule  Equation    and its logarithmic transform  to include multiplevalued evidential variables  In addition  the computation of means and variances for multiple valued evidential variables  see Equations    through     is straightforward  In addition  we can relax the assumption that no probability is equal to   or    For example  let us suppose that  The variance of the sum of the weights is V ar W  H     W            p Ej  H         p Ej  H             p Ei  H            p Ei  H        i                 n  i    j  i                 n  i    j   Using Equations    through     we obtain EV  wj  H        V ar wj  H      EV  wj  H      V ar wj  H        a   Therefore  although the computation of p W   W   H  is straightforward  we cannot compute p W   W   H  as described in the previous section  Instead  we compute p W   W   H   by considering separately the cases Ej and Ej   We have  H  G     G       G  j  p W   W   H    p Ej  H  p W   W   HEj     p Ej  H  p W   W   HEj         If Ej is observed  W      and p W   W   HEj        Consequently  Equation    becomes  H   b   p W   W   H    p Ej  H  p W   W   HEj     p Ej  H   We compute p W   W   HEj   as described in Equations    through     replacing EV  wj  H  with wj in the summation of Equation     and V ar wj  H  with   in the summation of Equation     The other terms in the summations remain the same  because we have assumed that evidence variables are independent  given H or H  This approach generalizes easily to multiplevalued evidence variables and to cases where more than one probability is equal to   or    We can extend our analysis to special cases of conditional dependence among evidence variables  For example  Figure   shows a schematic of the belief network for Pathfinder  In this model  there are groups of dependent evidence  where each group is conditionally independent of all other groups  We can apply our analysis to this model by using a clustering technique described by Pearl  Pearl         pp            As in the previous section  suppose we want to compute the value of information for the set of evidence S    E    E            Em    For each group of dependent features Gk   we cluster those variables in the intersection of S and Gk into a single variable  Then  we average out all variables in the belief network that are not in S  What remains is a set of clustered variables that are conditionally independent  given H and H  We can now apply our analysisgeneralized to multiple valued variablesto this model  There are special classes of dependent distributions for which the central limit theorem is valid  We can use this fact to extend our analysis to other cases of dependent evidence  For example  the central limit theorem applies to distributions that form a Markov chain  provided the transition probabilities in the chain are not correlated  Billingsley         Thus  we can extend our analysis to belief networks of the form shown in Figure    We can generalize the value of information analysis even further  if we use the Markov extension in combination with the clustering approach described in the previous paragraph   Gk Ei  E i    E i    Figure    A schematic belief network for Pathfinder   a  The features in Pathfinder can be arranged into groups of evidence variables G    G          Gj   The variables within each group are dependent  but the groups are conditionally independent  given the disease variable H   b  A detailed view of the evidence variables Ei   Ei     and Ei   within group Gk    H  E   E   E     En  Figure    A conditional Markov chain  The evidence variables form a Markov chain conditioned on the variable H  We can extend our analysis involving the central limit theorem to this case    It is difficult for us to extend the analysis to include multiple valued hypotheses and decisions  The algebra becomes more complex  because the simple p model for action no longer applies  There is  however  the opportunity for applying our technique to more complex problems  In particular  we can abstract a given decision problem into one involving a binary hypothesis and decision variable  For example  we can abstract the problem of determining which of n diseases is present in a patient into one of determining whether the disease is benign or malignant  In doing so  we ignore details of the decision makers preferences  and we introduce dependencies among evidence variables  Nonetheless  the benefits of a nonmyopic analysis may outweigh these drawbacks in some domains      SUMMARY AND CONCLUSIONS  We presented work on the use of the central limit theorem to compute the value of information for sets of tests  Our technique provides a nonmyopic  yet tractable alternative to traditional myopic analyses for determining the next best piece of evidence to observe  Our approach is limited to information acquisition decisions for problems involving     specific classes of dependencies among evidence variables  and     binary hypothesis and action variables  Additional research  however  may help to relax these restrictions  For now  we pose the nonmyopic methodology as a new specialcase tool for identifying cost effective observations  We hope to see empirical comparisons of the relative accuracy of the nonmyopic analysis with that of traditional myopic analyses  We expect that the results of such evaluations will be sensitive to the details of the application areas   Acknowledgments This work was supported by the National Cancer Institute under Grant RO CA        A   and by the Agency for Health Care Policy and Research under Grant T HS        
  that we developed  the dynamic network tool that we designed and implemented to aid knowledge engineering  We present several techniques for knowledge engineering of large belief networks  BNs  based  for  CPCS  and the Bayesian network algorithms that we  employed for this large  complex network   on the our experiences with a network derived from a large medical knowledge base  The noisy MAX  a generalization of the noisy OR gate  is  used to model causal independence in a BN with  knowledge engineering or evaluation  challenging  The  multivalued variables  We describe the use of leak  development of CPCS necessitated the implementation of algorithms that could make inference in BNs of this size  probabilities  to  enforce  the  closed world  assumption in our model  We present Netview  a  more efficient  An example of this is a generalization of the  visualization tool based on causal independence and the use of leak probabilities  The Netview  noisy OR gate  Cooper       Peng and Reggia       Pearl  software  model causal independence  The  allows  knowledge  engineers  to  dynamically view subnetworks for knowledge engineering  and it provides version control for editing a BN  Netview generates sub networks in which leak probabilities are dynamically updated     The CPCS is one of the largest BNs in use at the current time  and its sheer size makes most tasks  such as        that is commonly used in binary valued networks to  CPCS BN contains nodes  that are multivalued  for example  disease nodes may have four values  absent  mild  moderate  severe   consequently we use a generalization of the noisy OR gate called the  noisy MAX  The specification of a complete conditional m with sm values and n predecessors requires the assessment of  sm  l IJ I s   to reflect the missing portions of the network   probability matrix for a node  INTRODUCTION  probabilities  where  s is  the  number  of  values  predecessor i  for a binary network this reduces to  of   n   In    Given the relative maturity of algorithm development in the  contrast  the causal independence assumption in the form of  Bayesian reasoning community  attention is now starting to focus on applying these algorithms to real world  a noisy gate reduces this assessment task to  probabilities  thereby simplifying knowledge  applications   and greatly reducing storage requirements   The Quick Medical Reference Decision  Ln sm  l s   dcquisition   QMR DT  project seeks to develop practical decision analytic methods for large knowledge based  To aid in the editing and refinement of the  systems  The first stage of the project converted the  have developed a network visualization tool we named  Theoretic  Internist   knowledge base  Miller  Pople et al          QMR  s predecessor  into a binary  two layered belief  CPCS BN  we  Netview  The Netview tool provides dynamic views of the  BN  and can generate subnetworks by taking advantage of        Shwe   the noisy MAX and leak assumptions  For inference  the  Middleton et al          In the second stage of the QMR DT  network  or subnetworks generated by Netview  are sent to the IDEAL package  Srinivas and Breese       for  network  BN   Middleton  Shwe et al   project we are creating a multilayer belief network with mu tiva ued variables  and developing efficient inference algorithms for the network  This paper will concentrate on the knowledge engineering issues we faced when building a large multilayered BN  To create a large multilevel  multivalued BN we took advantage of a rich knowledge base  the Computer based  inference  Netview is a flexible tool which can be used in any BN that uses noisy gates  and is described in section       Like the Internist     derived B N  the CPCS BN uses leak probabilities  Henrion       to represent the chance of an event occurring when all of its modeled causes are absent   We discuss our use of leak probabilities   and the  Patient Case Simulation system  CPCS PM   developed over two years by R  Parker and R Miller  Parker and  modifications to the leak probabilities required by the  Miller       in the mid     s as an experimental extension  dynamic network tool  in section       of the Internist   knowledge base  This paper makes contributions both in knowledge engineering and in algorithm development and implementation for large BNs  We describe the CPCS BN     KNOWLEDGE BASE TO BELIEF NETWORK  The CPCS PM system is a knowledge base and simulation program designed to create patient scenarios in the medical   Knowledge Engineering for Large Belief Networks  sub domain of hepatobiliary disease  and then evaluate medical students as they managed the simulated patient s problem  Unlike that of its predecessor lnternist    the CPCS PM knowledge base models the pathophysiology of diseases th e intermediate states causally linked between diseases and manifestations  The original CPCS PM system was developed in FranzLisp  Diseases and intermediate pathophysiological states  IPSs  were represented as Lisp frames  Minsky                 and were mapped to probability values  as described in next section  Frequency weights from the CPCS PM were mapped to probability values based on previous work in probabilistic interpretations of Internist   frequencies   the     To construct the BN we converted the CPCS PM knowledge base to CommonLisp and then parsed it to create nodes  We r e pr esented diseases and IPSs as four levels of severity in the CPCS BN absent  mild  moderate  and severe  Predisposing factors of a disease or IPS node were represented as that node s predecessors  and findings and symptoms of a disease or IPS node as the successors for that node  In addition to the findings  CPCS contained causal links between disease and IPS frames  we converted these links into arcs in the BN  Frequency weigh t s  Shwe  Middleton et al        from the CPCS PM ranged from   to  We generated the CPCS BN automatically  we did manual consistency checking using domain knowledge to edit the network  Because the CPCS PM knowledge base was not designed with probabilistic interpretations in mind  we had to make numerous minor corrections to remove artifactual nodes  to make node values consistent and to confinn that only mutually exclusive values were contained within a node  The resultant network has     nodes and     arcs  Figure     A t o tal of seventy four of the nodes in the network are predisposing factors and required prior p robab ilities the remaining nodes required leak probabilities assessed for each of their values  We thus had to assess over     probabilities to specify the network fully   Figure I  A small portion of the CPCS BN displayed in the Netview visualization program  The node ascending cholangitis in the third row shown in inverse has been selected by the user            Pradhan  Provan  Middleton  and Henrion  While the CPCS PM knowledge base is derived from the Internist   knowledge base it has been significantly     augmented by inclusion of the IPS states  and multivalued     representations of both diseases and manifestations of disesase  The original  QMR BN transformation of the  Internist   knowledge base used only binary valued disease and manifestation nodes  While conceptually simple  this     approach does not adequately reflect the potential variation in presentation of disease manifestations  or the severity of           diseases   Figure     A node x with  predecessors D i n   NOISY OR  ordered states  The noisy OR is a simplified  BN representation that  respectively  or disease and manifestation variables respectively  Peng and Reggia          variable  variables or predecessors  X  the  variables  and is typically described for a  which are interpreted as either cause and effect variables  that has  n  cause          P X     VI    If there are multiple  manifestation  variables  j          l   Consider an effect  D    Dn The noisy OR can be used when   tf having  probabilities required to calculate  probability matrix  The noisy OR is defined over a set of two level network partitioned into two sets of variables   E              The  shaded area represents the  requires far fewer parameters than the full conditional binary valued     q  GENERALIZATION OF THE NOISY OR          Aj   P Xi  xi I VI     II l pii   each D has  i D eV   an activation probability p  being sufficient to produce the effect in the absence of all other causes  and       the  probability of each cause being sufficient is independent of          the presence of other causes  Henrion  In this paper  we define variables using upper case letters   X is ilx  If variable X is present it is denoted using the letter x   letters   The domain of possible values for variable  if it is absent  it is denoted using x   The activation of a variable  X  by a predecessor variable D   is independent of the value of D   assumption   X is  conditional probability given by other  Under the noisy or  activated when D   P   is active  with a    P x I d    dk      In  words  this activation probability deh otes the  probability when inactive   where Pij is the link probability on the arc from D  to  lJi is active and all other predecessors are  For a two level noisy OR network  we define a set V of cause or disease variables  and a set W of effect or  manifestation variables  If there is a set VI of V of predecessors of  X  e  W that are present  then since the D   X will be false when all D  are false   P X X I VI   II P D   d   i D eV   P X  X I Vj      BN  application  since we need to accommodate n ary variables   For example  CPCS BN disease and IPSs can take on values such as absent  mild  moderate  and severe       NOISY  MAX  Consider a generalization of the noisy OR situation in which each variable is allowed to have a finite discrete state space  rather  than just a binary state s pace   This  generalization was first proposed by  Henrion         but he  did not describe the algorithmic details  In developing this generalization  we assume that we have a set  V of  predecessor variables D        Dn  Consider first the case where we have a variable  in VI are independent   Xi  The simple noisy OR is insufficient for the CPCS  and values that variables can take on using lower case  for  then we obtain  X with a subset  present  with the predecessors indexed by  V  of V that are  i j   q       The variable domains in CPCS BN are all partially ordered   for example   absent  mild  moderate  severe   and it turns out that such a partial ordering is necessary for all variable domains  For the remainder of our work we assume that all variables have ordered domains  We now want to compute  II      P    i D eV   The value xis given by In other words   From this  it is simple to derive  domain values  P X xiVI  l  II l p    i D eV   x    max i j           q   Henrion         X takes on as its value the maximum of the of  its predecessors   predecessors are all independent   given  that  the   Knowledge Engineering for Large Belief Networks       V    This computation of P X   xI can be viewed as setting up a hypercube of dimensions i x j x   x q and summing   the cells  each of which contains a probability value  fljkq P jkq  As an example of the derivation of the general formula  we consider the case of two predecessors D  and    j  If these variables take on values i j respectively  then the  probability P X  xI V      where  x   max i  j     For  example  Figure   graphically depicts the conditional  probability matrix for D  and Dj  b oth of which have  ordered states             If x    then consists of the shaded squares of the grid   P X   D  Dj   Figure    Explicit representation  of the leak probability as a cause of X   In this multivalued noisy MA X situation  the probabilities  that are being calculated in these hypercubes are cumulative  probabilities  that is   P X s  xI D s  d     For notational  convenience  we define the cumulative probability for a variable  X that has a s ingle predecessor D with maximum d as   domain value   f x I d   P X s  xI D s  d    Under the generalized noisy OR assumption  for a given variable  X  with a set of  which each D   q  D         Dq d   we know that  predecessors  has maximum value  for     LEAKS  As in any other knowledge representation scheme  the BN representation suffers from incompleteness  in that it  typically cannot model every possible case  A leak variable  can be used to enforce the closed world assumption  Reiter        The leak variable represents the set of causes that are not modeled explicitly   A leak probability is assigned as  the probability that the effect will occur in the absence of  of the cause s  D        D n that are explicitly modeled  If the leak variable is explicitly modeled  then it can be  any  treated like any other cause and can be depicted as such     IJ  P xld    i D eV   In this representation  the leak node is always  assumed to be  on   that is If  Note that using this transformation  we can define the  probability assigned to        Figure  i D E  P L true          the leak L with value l is factored into the calculation of  the unconditional probability for variable X  then we obtain  X taking on a particular value Xk    P X s  x    P L s  l   TI  p P D  s  d       P      i D V  Explicitly representing leak nodes in the  The unconditional probability assigned to a variable can be  derived in an analogous fashion  First  we nee d to derive  D   assuming no        this is given  the unconditional probability of variable predecessors  As described in  Provan by  P X    x    P L s      IJ p P D  s  d        P   P x I V     i D eV   P XS x     conditioning parents  The Netview knowledge engineering  tool  described in section    facilitates the maintenance and  editing of leak probabilities  Netview stores leak values  separately  as if they were explicit nodes  and then inserts  the leak values into the appropriate probability exporting a network for inference in IDEAL      The unconditional probability is given by  IJ p P D   S d    l p       i D EV  The unconditional probability assigned to X taking on a particular value x is   CPCS BN would  almost double the size of the network  so leaks are implicitly represented in the probability tables of a node s       tables when  TOOLS FOR KNOWLEDGE ENGINEERING NETVIEW   A TOOL FOR NETWORK  VISUALIZATION AND EDITING  Verification and refinement of the structure of the CPCS BN is an important part of the QMR  Df project for two reasons  First  because the  CPCS BN was generated from a pre  existing knowledge base  Second  the effect of model structure on network performance and accuracy is an important aspect of the QMR  Df project   During the knowledge engineering process  it became  V    Using this approach  the value P xI can be computed in time proportional to the number of predecessors in V   This generalized noisy MAX has been implemented in IDEAL   obvious  that  available tools were not suitable for  visualizing and editing a network the size of  Figure       CPCS BN  In particular  most tools only permit a static        Pradhan  Provan  Middleton  and Henrion  view of the network  a limitation that made editing the  disease   and so on  A node may have any number of semantic labels  Semantic labeling is a useful technique for  CPCS network very hard   filtering nodes to focus attention during knowledge by allowing knowledge engineers to focus on portions of  engineering  It is possible  say  to focus only on  gastric  findings and diseases when dealing with the appropriate  the network  The program is implemented in Macintosh  domain expert  In the future we will also use the semantic  Netview was created to help knowledge engineering efforts  CommonLISP       The main features of Netview are o  dynamic network layout  o  semantic labeling of nodes  o  version control    subnetwork generation and dynamic  o  labels in the dynamic layout algorithm to improve the appearance of subnetwork views  It is useful to keep track of modifications while editing the  BN  To facilitate this  Netview includes basic version  leak modification  control to store deleted and added arcs and nodes and  leak value editing  changes to probability tables  Arc and node additions and removals between versions are displayed through the use of  Because of the causal independence assumptions implied by the use of the noisy MAX and noisy    different colors   R gates   knowledge engineers are can select smaller parts of the       CPCS BN for viewing  Netview allows the user to view all ancestors  all predecessors  or all ancestors and predecessors of selected nodes  For example  in Figure l while the node  ascending cholangitis is selected  inverse  color   we can use Netview s ability to show all successors and predecessors of the selected node or nodes  resulting in the subnetwork view shown in Figure     Other options include viewing nodes  Markov blanket  and immediate successors or predecessors  Netview uses a dynamic layout algorithm to display the selected nodes  The knowledge engineer is able to move rapidly between views by selecting nodes and choosing viewing options  or by retrieving previously saved views  Quickly viewing a node s predecessors allows rapid assessment of leak probabilities  In addition to subnetwork selection  Netview allows semantic labeling of nodes  and filtering based on semantic labels  For example  nodes in CPCS BN are labeled  lab finding    symptom    sign    disease    IPS    liver        The  SUBNETWORK GENERATION AND DYNAMIC LEAK MODIFICATION  Subnetwork generation Netview  program  is  used  only  for  network  visualization and editing  Netview saves files in IDEAL format for inference  Because of the size of the CPCS BN it  is not always desirable to send the entire network to  IDEAL  for inference  If we are only interested in verifying a small set of diseases we can generate a subnetwork including only those diseases of interest and their associated findings  IPSs and predisposing factors  When we run test cases  against a subnetwork we don t require the system to compute the posterior probabilities of diseases that we are not interested in         Dynamic leak modification Subnetworks we select from the full  CPCS  BN using  Netview can be exported to IDEAL for inference  It is  possible to select subsets of the larger CPCS  BN for  i n f e r e n ce  leaks    due  to  the  presence  of  Figure    A subnetwork of the CPCS BN displayed in Netview  This view comprises all predecessors and successors of the node ascending cholangitis    Knowledge Engineering for Large Belief Networks  Figure    Subnetwork creation  Node D is removed from the network  the value of the leak node updated to p based on the probability of D           p is       When a subnetwork is saved Netview updates the leak probabilities to take into account the missing diseases  In the CPCS BN the node hepatomegaly has parents shown in figure    The leak probability for hepatomegaly is therefore calculated based on this set of predecessors  In figure   a subnetwork was selected based on the ancestors and predecessors of the disease ascending cholangitis  Conse quently  the only parent of hepatomegaly in the subnetwork is ascending cholangitis  its other parents are not included  The transformation of leak probabilities required during subnetwork creation is shown in Figure    The leak probability must be updated from p to p   This updating is done in order to preserve the total probability mass  If the value I of L is updated to a value l  for new leak L   we can compute the updated leak node probability as  BN  and we are exploring how much the network performance changes when the assumption is relaxed        Information metrics  When subnetworks are created some information is lost as parts of the network are excluded  A future area of research is to use Netview to calculate the information Joss of a subnetwork based on information metrics  Provan        and to compare differences in posterior probability between the complete network and the subnetwork which has been selected      RELATED WORK  The generalization of the noisy OR was first proposed in  Henrion        and the derivation and implementation p    P L    lA D     d   described here follow that original proposal  Two related   P L    l P D     d   generalizations are described in  Srinivas       and  Diez        The generalization of the noisy OR by Srinivas is   p P D     d    l  p    different to this proposal  and is intended for a different application  This generalization is for circuits  or other such If we want to combine a set of Q nodes into a leak node  devices  which can be either functional or non functional  where each node d  in Q has link probability  then the new In the case of medicine  findings can take on values such as leak node probability is given by   absent  mild  moderate  severe   in which case the binary generalization of Srinivas is insufficient to deal with p   P L   lAD     d AADq    dq  arbitrary n ary variables  The noisy MAX generalization in D i ez       is virtually identical to the one described here    P L       fl P Dt     d   and we have derived our noisy MAX independent of that in i D eQ  Diez        Also  the proposal in  Diez       is described  P L      n p P D    d    l p     within the context of learning models for OR gates  and its i D eQ application to inference in Bayesian networks is not directly apparent  We prove in  Provan       that if the network is To our knowledge  there is no other tool which allows hierarchical and there are no arcs between nodes at the dynamic selection of subsets of Bayesian networks  There same level of the hierarchy  then the leak updating is sound  are several graphical tools for creating Bayesian networks  that is  the probability assigned to X given the new set of including IDEAL Edit  Ergo  Hugin  Andersen  Olesen et al  predecessors is the same as the probability assigned to X        and Demos  Morgan  Henrion et al         But these with the original predecessors  This proof holds if the tools do not provide dynamic network layout and do not subnetwork consists of a Markov blanket of a node  all have features aimed at knowledge engineering large BNs  predecessors and successors of a node  or all successors of a node  The assumption for the proofs holds for the CPCS   CONCLUSION    Figure    Parents of the node hepatomegaly   In this paper we have presented several methods for representing  and a software tool for managing  large BNs based on our experience with the CPCS BN  The noisy MAX is a generalization of the noisy OR gate for multivalued        Pradhan  Provan  Middleton  and Henrion  variables which reduces the complexity of the knowledge acquisition task and storage requirements for a network  Leak probabilities are used in the CPCS BN to model causes other than those explicitly modeled in the network   Middleton  B   Shwe  M  A   et al          Probabilistic diagnosis using a reformulation of the Internist   QMR knowledge base     Evaluation of diagnostic performance  Methods of Information in Medicine              Based on the causal independence assumptions of the noisy MAX  and the use of leak probabilities we have developed Netview  a tool for visualizing BNs based on the dynamic layout of subnetworks  and which also provides basic version control for editing networks  The creation of subnetworks allows for more efficient knowledge engineering  and for easier verification of the B N  We describe a technique for updating leak probabilities based on the excluded parents of a node in subnetworks   Miller  R  A   Pople  H  E  J   et al          Internist    An experimental computer based diagnostic consultant for general internal medicine  N Engl J Med                Recent advances in creating BNs from pre existing data or knowledge bases will result in networks that are larger and more complex than those created manually  We believe that the techniques described in this paper are important to facilitate the management and verification of such networks  Acknowledgments  This work was supported by NSF Grant Project IRI         and by computing resources provided by the Stanford University CAMIS project  which is funded under grant number LM      from the National Library of Medicine of the National Institutes of Health  The authors would like to thank K  C  Chang and R  Fung for their graph layout algorithm on which NetView s layout method is based  and R Miller for access to the CPCS knowledge base  
