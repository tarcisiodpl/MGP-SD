 We describe an expert system  Maies  under development for analysing forensic identification problems involving DNA mixture traces using quantitative peak area information  Peak area information is represented by conditional Gaussian distributions  and inference based on exact junction tree propagation ascertains whether individuals  whose profiles have been measured  have contributed to the mixture  The system can also be used to predict DNA profiles of unknown contributors by separating the mixture into its individual components  The use of the system is illustrated with an application to a real world example  The system implements a novel MAP  maximum a posteriori  search algorithm that is briefly described      Introduction  Probabilistic expert systems  PES  for evaluating DNA evidence were introduced in      This paper is concerned with describing the current status of a computer software system called Maies  Mixture Analysis in Expert Systems  that analyses mixed traces where several individuals may have contributed to a DNA sample left at a scene of crime  In     it was shown how to construct a PES using information about which alleles were present in the mixture  and we refer to this article for a general description of the problem and for genetic background information   A brief summary to genetic terminology is given in Appendix A   The results of a DNA analysis are usually represented as an electropherogram  EPG  measuring responses in relative fluorescence units  RFU  and the alleles in the mixture correspond to peaks with a given height and area around each allele  see Figure    The band intensity around each allele in the relative fluorescence  Julia Mortera Dipartimento di Economia Universita Roma Tre Via Ostiense            Roma  Italy   units represented  for example  through their peak areas  contains important information about the composition of the mixture   Figure    An electropherogram  EPG  of marker VWA from a mixture  Peaks represent alleles at        and    and the areas and height of the peaks express the quantities of each  Since the peak around allelic position    is the highest this indicates that the    allele is likely to be a homozygote or a shared allele between two heterozygotes  This image is supplied courtesy of LGC Limited        The main focus of the present paper is to describe the current status of a computer package called Maies  which automatically builds Bayesian network models for mixture traces based on conditional Gaussian distributions     for the peak areas  given the composition of the true DNA mixtures  Currently the program only considers a DNA mixture from exactly two contributors  which seems to be the most common scenario in forensic casework      and the program ignores other important complications such as stutter  dropout alleles  etc    We distinguish two types of calculations that Maies can perform  One type is evidential calculation  in which a suspect with known genotype is held and we want to determine the likelihood ratio for the hypothesis that the suspect has contributed to the mixture vs  the hypothesis that the contributor is a randomly chosen individual  We distinguish two cases  the other contributor could be a victim with a known genotype or a contaminator with an unknown genotype  possibly without a direct relation to the crime  This could be a laboratory contamination or any other source of contamination from an unknown contributor  The other type calculation that Maies can perform is the separation of profiles  i e  identifying the genotype of each of the possibly unknown contributors to the mixture  the evidential calculation playing a secondary role  Both types of calculation are illustrated in      of DNA in a mixture sample  The model is idealized in that it ignores complicating artefacts such as stutter  drop out alleles and so on  and assumes that the mixture is made up of DNA from two people  who we refer to as p  and p   Typically  prior to amplification in a laboratory  a DNA mixture sample will contain an unknown number of cells from p  and a further unknown number of cells from p   Hence there is an unknown common fraction  or proportion  across the markers of the amount of DNA from p   that we denote by   In an ideal amplification apparatus  during each amplification cycle the proportion of alleles of each allelic type would be preserved without error  We model departures from this ideal as random variation using the Gaussian distributions  and we introduce an additional variance term to represent other measurement error  represented by       Previous related work includes that of     and     who respectively developed numerical methods known as Linear Mixture Analysis  LMA  and Least Square Deconvolution  LSD  for separating mixture profiles using peak area information  Both methods are based on least squares heuristics that assume the mixture proportion of the contributors DNA in the sample is constant across markers  A computer program has been written     for estimating the proportion of the individual contributions in two person mixtures and to rank the genotype combinations based on minimizing a residual sum of squares  More recently      describes PENDULUM  a computer package to automate guidelines in     and      None of the methods described above utilizing peak area information are probabilistic in nature  nor do they use information about allele frequency  In contrast  the methodology proposed in      combines a model using the gene frequencies with a model describing variability in scaled peak areas to calculate likelihood ratios and study their sensitivity to assumptions about the mixture proportions   The post amplification proportions of alleles for each marker are represented in the peak area information  which we include in the analysis through the relative peak weight  The  absolute  peak weight wa of an allele with repeat number a is defined by scaling the peak area with the repeat number as wa   aa   where a is the peak area around allele a  Multiplying the area with the repeat number is a crude way of correcting for the fact that alleles with a high repeat number tend to be less amplified than alleles with a low repeat number  For issues concerning heterozygous imbalance see        The plan of the rest of the paper is as follows  In the following section we describe the mathematical assumptions underlying the Bayesian networks that Maies generates for analysing two person DNA mixtures  We then describe the components that Maies uses to build up the networks  This is followed by a description of a simple MAP search algorithm  implemented in Maies for separation of profiles  We then illustrate the use of Maies on a real life example  and then summarize future work required to make Maies into a tool for routine casework   To avoid the arbitrariness in scaling used to measure the areas  we consider the observed relative peak weight ra   obtained by scaling with the total peak weight as X ra   wa  w    w    wa       The mathematical model  Our PES is a probabilistic model for relating the preamplification and post amplification relative amounts  We further assume that  The peak weight for an allele is approximately proportional to the amount of DNA of that allelic type   The peak weight for an allele possessed by both contributors is the sum of the corresponding weights for the two contributors   a  so that then  P  a ra        For the relative peak weight  denoted by the random variable Ra   we assume a Gaussian error distribution Ra  N  a   a          a    n    a        na           where  is the proportion  or fraction  of DNA in the  i  mixture originating from the first contributor  na is the number of alleles with repeat number a possessed  i  by person i  Note that na            and hence a            We assume an error variance for a  of the form a           a     a               for U S  Caucasians for the analysis in    of data taken from          where  and  are variance factors for the contributions to the variation from the amplification and measurement processes   Note that if a     then a       and Ra  N           The interpretation of this is that if there are no alleles of type a in the mixture prior to amplification  then any detected post amplification can be ascribed to measurement error  Similarly if a       which means that for the given marker all alleles are of type a in the mixture before amplification  then Ra  N           that is post amplification all alleles for the given marker are of type a  up to measurement error  However the Bayesian networks that Maies constructs uses the variance structure a       a              The reason is that we need to consider the correlation between weights due to the fact that they must add up to unity  It turns out  perhaps surprisingly  that the P likelihood obtained using     when conditioned on a ra     has precisely the form as would be obtained using the likelihood based on     used in our model if we ignore measurement error by setting         and   the likelihoods are P very close numerically for small    This constraint a ra     is imposed when we enter the complete set of observed peak weights as evidence in our networks  The proof  too long for this paper  may be found in       In our example in    we used           and             corresponding approximately to a standard deviation for the observed relative weight of about p                       for a       substituted into      These parameter values imply that when amplifying DNA from one heterozygous individual  for which a         an ra value at two standard deviations from the mean would give a value of                  for the ratio of the minor to the major peak area  this is about the limit of variability in peak imbalance that has been reported in the literature       and suggests that our chosen parameter values are perhaps conservative  In general the variance factors may depend on the marker and on the amount of DNA analysed  but for simplicity we use the values above   Our PES model is robust to small changes in these parameter estimates   Finally  we assume known gene frequencies of single STR alleles  in particular we use those reported in          Maies  The basic form of our Bayesian network models is fairly straightforward  but the networks can grow large when modelling the ten or so markers typical in a mixture problem   In the example in    the network has     nodes   One way to manage this complexity is to use object oriented Bayesian network software  as we describe in detail in       Here we describe Maies  a purpose built program that  after entering peak area information and available genetic information  if available  about the potential contributors  automatically constructs a single conditionalGaussian Bayesian network on which the probability calculations are performed  Maies implements the local propagation scheme of       Peak areas are automatically converted to normalized weights and entered as evidence in the relevant nodes by the program  An example of a network generated for a single marker with two alleles observed in the mixture is shown in Figure    The figure illustrates the repetitive modular structure that makes it possible for Maies to create the much larger Bayesian networks required to analyse mixtures on several markers  We now describe these various structures and how they interrelate  working from top to bottom in Figure    u mg  u pg  smg  u gt  spg  vmg  sgt  vpg  u mg  vgt p    s   p gt  u pg  u gt  p    v   target  p gt  jointgt p     p     p  x  p       inmix    p     p  x  x inmix     inmix      weight    weight  x weight    weightobs  x weightobs  p  frac   weightobs  sym  Figure    The structure of a Bayesian network generated by MAIES for a single marker  in which two allele peaks    and    were observed      The first term in the variance structure in     can be seen as a second order approximation to a more sophisticated model based on gamma distributions for the absolute scaled peak weights to be discussed elsewhere     This dataset has an observed allele    of the marker D    As none of the     subjects in      had this allele  we chose to use               as its frequency         Top level people  Maies currently models mixtures only for DNA from two individuals  Thus it sets up nodes for four individuals who are paired up  prefixed by s  for suspect   v  for victim   and u  and u  representing two unspecified persons from the population  Corresponding to each of these individuals is a triple of nodes representing their genotype  gt  on the marker  and the individuals paternal  pg  and maternal  mg  genes  The probability tables associated with the maternal and paternal genes contain the allele frequencies of the observed alleles  whilst the conditional probability table associated with the genotype node is the logical combination of the maternal and paternal gene            Repeat number nodes  On the level below the allele counting nodes are the repeat number nodes  labelled   inmix     inmix  and x inmix   These are  yes no  binary valued nodes representing whether or not the particular alleles are present in the mixture  thus for example allele   is present in the mixture if either of the allele counting nodes p    or p    takes a non zero value  For the node x inmix  the x refers to all of the alleles in the marker that are not observed  When using repeat number information as evidence the repeat number nodes present in the mixture will be given the value yes and x inmix  will be given the value no   Actual contributors to the mixture  The genotypes on the marker of the two individuals p  and p  whose DNA is in the mixture are the nodes labelled p gt and p gt  Node p gt has incoming arrows from nodes u gt  sgt and a  yes no  valued binary node labelled p    s   The function of this latter node is to set the genotype of node p gt to be that of sgt if p    s  takes the value yes  otherwise to set the genotype of node p gt to be that of u gt  An equivalent relationship holds between the genotype nodes p gt  vgt  u gt and p    v   Uniform priors are placed on the nodes p    s  and p    v   The node labelled target represents the four possible combinations of values of the two nodes p    s  and p    v   with a conditional probability table of zeros and ones representing the logical identities  The marginal posterior distribution of this node is used to calculate likelihood ratios in evidential calculations  The network also has a node representing the joint genotypes of individuals p  and p   which is labelled jointgt  with incoming arrows from p gt and p gt  the  quite big  conditional probability table associated with this node has entries that are either of zero or one  The most likely configuration of the marginal distribution in all joint genotype nodes across all markers is required for separating the mixture  and is found using the MAP search algorithm described in           i   These nodes model the na variables introduced in       Allele counting nodes  On the level below the genotype nodes for p  and p  is a set of nodes representing the number of alleles  taking the value of      or    of a certain type in each individual  Thus  for example  the node p    counts the number of alleles of repeat number   in the genotype of individual p  for the given marker  this value only depends upon the genotype of the individual p  and hence there is an arrow from p gt to p           True and observed weight nodes  These nodes are represented by the elliptical shapes  The nodes   weight    weight and x weight represent the true relative peak weights r    r  and rx respectively of the alleles      and x in the amplified DNA sample  Each true weight node is given a conditionalGaussian distribution as in      where the fraction  of DNA from p  in the mixture is modelled in the network by a discrete distribution in the node labelled p  frac  The variance is taken to be    a   The nodes   weightobs    weightobs and x weightobs represent the measured weights  The observed weight is given a conditional Gaussian distribution with mean the true weight  and measurement variance      hence leading to the variance      When using peak area information as evidence the nodes representing the observed weights will have their values set to the relative peak weights  The sym node is only used for separating a mixture of two unknown contributors  to break the symmetry between p  and p   see             Networks with more than one marker  The network displayed in Figure   generated by Maies is for a single marker  for mixture problems involving several markers the structure is similar but more complex because the number of nodes grows with the number of markers  In such a network the nodes shaded in Figure   occur only once  The unshaded nodes are replicated once for each marker  with each node having text in their labels to identify the marker that the allele or genotype nodes refer to  There will also be extra repeat number  allele counting and allele weight nodes in each marker having more than two observed alleles in the mixture  extending the pattern for the one marker network in the obvious manner       A simple MAP search algorithm  It is well known that the most likely configuration of a set of discrete variables is not necessarily the same as found by picking the most likely states in the individual marginals of the variables  see for example        The basis of the MAP search algorithm in Maies is to assume that this is close  Specifically  after entering and propagating evidence  one finds the individual marginals of the set of MAP variables M of interest  There are then two variants of the MAP algorithm  batch and sequential  In the batch variant  one finds for some reasonable number n  say n         the top n most likely configurations of the joint probability given by the product of the individual marginals of MAP variables  This is done by constructing a disconnected graph in the MAP variables  and using the efficient algorithm of       These configurations are stored in an list  c    c            cn   ordered by decreasing probability according to the independence graph  Now returning to the original Bayesian network  one propagates the available evidence E and finds the normalization constant P  E  and stores this in rp  say  short for remainder probability   One then processes the  c    c            cn   configurations as additional evidence in the original Bayesian network and finds from the normalization constant each of their probabilities P  ci   E   After processing each configuration  one keeps track of the highest probability configuration found and its probability  bp  One also subtracts p ci   E  from rp  so that if it ever happens bp   rp then the MAP has been found  If one stores all of the probabilities p ci   E   i              k for all of the configurations that have been processed  then perhaps the second  third etc  most likely configurations may Pk be identified if their probabilities exceed P  E   i   P  ci   E   The sequential variant proceeds similarly  the difference is that the candidates c    c           are generated one at a time as required  The following is pseudo code for the sequential variant for finding the MAP   the second  third  fourth etc   most likely configurations      A criminal case example  Our example is taken from Appendix B of     and illustrates the use of the amelogenin marker in the analysis of DNA mixtures when the individual contributors are of opposite sex  Peak area analysis of the amelogenin marker in DNA recovered from a condom used in a rape attack indicated an approximate     ratio for the amount of female to male DNA contributing to the mixture  Peak area information was available on six other markers  the information is shown in Table    we shall refer to this as the Clayton data   Further examples are illustrated in      and        Table    Clayton data of     showing mixture composition  peak areas and relative weights together with the DNA profiles of both victim  v  and suspect  s   For the marker D   the allele designation in brackets is as given in     using the labelling convention of      Marker  Alleles  Amelogenin  X Y                                                                 D  D    D    FGA THO VWA   Initialize  i   j      bp      and rp   P  E    While bp   rp do   Find ci and P  ci   E    If p ci   E    bp set bp   P  ci   E  and j   i   Set rp    rp  P  ci   E  and i    i       cj is the MAP configuration  For purely discrete networks  this algorithm does not appear to be as efficient as that described in       However it is neither clear that the latter can be applied to finding the MAP of a set of discrete variables in a conditional Gaussian network  nor that it could identify                      Peak area                                                                                                    Relative weight                                                                                                                                                     s  v  X Y  X                                                                  Evidential calculation  One possible use of the system to the Clayton data would be to compare the two hypotheses   H    the suspect and victim both contributed to the mixture  H    the victim and an unknown contributor contributed to the mixture   In a courtroom setting  the null hypothesis  H    would be a prosecutions case  whilst H  would represent the defences case   It is standard procedure in court for likelihood ratios of these hypotheses to be reported    of genotypes of the two contributors to the mixture  but other less likely but also plausible combinations  Maies achieves this with the MAP search algorithm described in      To do this calculation in Maies  evidence is entered in the observed relative peak area nodes  the repeat number nodes  and information on the suspect and victim genotypes  After propagating the evidence the marginal on the target is examined  This has the following values  taken from Maies    For separating a mixture  we may or may not have genetic information about one of the contributors  For our rape example  suppose that we have the genotype of the victim  Then using Maies we may enter as evidence the victims genotype  the relative peak areas and the repeat number information  We also select the value yes in the p    v  node  We then select the set of joint genotype nodes  and perform the MAP search  Maies returns two configuration  the most likely having posterior probability           with the genotype p  matching our suspect profile in Table    The second most likely combination has a posterior probability of             and differs from the true profile in the marker FGA where a homozygous          genotype is predicted  All remaining possible genotype combinations have a total probability mass of less than           u    u  v and u s and u s and v                                                                                   From this the likelihood ratio of H  to H  is calculated to be P  s and v   E  P  s and u   E                 where E denotes the complete set of evidence   Note that because we have placed uniform priors on the nodes p    s  and p    v   then P  E   s and v  P  E   s and u    P  s and v   E  P  s and u   E    It may be that only DNA from a suspect is available  but not from a victim  In such a situation we could use Maies to compare the following two hypotheses   H    the suspect and an unknown contributor contributed to the mixture  H    two unknown contributors contributed to the mixture Again in a courtroom setting these could represent prosecution and defence cases respectively  The calculation proceeds as before  but with the victim profile omitted from the evidence  This time the marginal on the target node is given by u    u  v and u s and u s and v                                                                                 and the likelihood ratio of H  vs  H  is given by P  s and u   E  P  u  and u    E                     Mixture separation calculations  The other type of calculation that may be performed with Maies is that of separating a two person mixture into genotypes of the contributors  The output from such a decomposition could be used to find a match in a DNA database search  For such a search it is useful to have not just the most likely combination  The second possibility is that no genotypic information is available on either contributor to the mixture  To do this calculation  evidence on the observed relative peak areas and the repeat number information is entered  To overcome the symmetry in the network between p  and p   we enter evidence on the sym node that p  frac is       Then  selecting the joint genotype nodes as before  we perform a MAP search  The result is shown in Table    All markers are correctly identified  Note in particular that the genotypes for the marker THO are identified correctly  In     this was only possible to do so after the victims profile was taken into account  Table    Most likely genotype combination of both contributors for Clayton data  The victim  here p   and male suspect  p   is correctly identified on every marker  The final column indicates the marginal probabilities for the genotype pairs on individual markers  with the figure in parenthesis the product of these marginals  Marker Amelogenin D  D   D   FGA THO VWA joint  Genotype Genotype p  p  XX XY                                                                               Posterior probability                                                                            For this example  the Maies MAP search algorithm identifies the next three most likely combinations         Posterior density              sumed correct  Such methods could also be useful for calibrating the variance parameters    and      We are pursuing ways that this could be accomplished using an EM estimation algorithm   Bayesian methods for estimating the variance parameters could also be developed   Nevertheless  despite these many issues  we feel that the present framework provides a sound foundation in which these and other matters can be be addressed and incorporated into Maies         Acknowledgements                                           Proportion of DNA from the major contributor   Figure    Posterior distribution of mixture proportion from Clayton data using no genotypic information  of genotypes  these have probabilities of                     and           respectively   The network has     discrete nodes and    continuous nodes  The total state space of the   joint genotype nodes is approximately             Identifying the    most likely combinations of these nodes took approximately    seconds on a    GHz laptop with    Kb memory   Finally for this example  Figure   shows the posterior distribution of the mixture proportion  the peak at around      corresponds to a mixture ratio of         in line with the approximate     estimated in          Discussion  We have described a software system  Maies  for analysing DNA mixtures using peak area information  yielding a coherent way of predicting genotypes of unknown contributors and assessing evidence for particular individuals having contributed to the mixture  and applied it to a real life example  A simple MAP search algorithm allows a set of most plausible genotypes to be generated  perhaps for use in a DNA database search for a suspect  There are a number of issues that would need addressing before the system could be used in routine analysis of casework  for example  complications such as more than two potential contributors  multiple traces  indirect genotypic evidence  stutter  etc  In addition  preliminary investigations seem to indicate that the variance factor depends critically on the total amount of DNA available for analysis  As this necessarily is varying from case to case  a calibration study should be performed to take this properly into account  Methods for diagnostic checking and validation of the model should be developed based upon comparing observed weights to those predicted when genotypes are as   This research was supported by a Research Interchange Grant from the Leverhulme Trust  We are indebted to participants in the above grant and to Sue Pope and Niels Morling for constructive discussions  We thank Caryn Saunders for supplying the EPG image used in Figure     
  OHB  UK  While on the face of it these two approaches appear quite different  I will argue that model search meth ods based upon maximizing a local log score can be  It is often stated in papers tackling the task  expressed  of selecting a Bayesian network structure from data that there are these two distinct approaches    i   as  equivalent search methods employing lo  cal conditional independence tests  The plan of the paper is as follows  The next section  Apply conditional indepen  dence tests   when testing for the presence  introduces notation together with some theoretical re  or otherwise of  sults  Section  edges   ii  Search the  model     states the assumptions made in later  Section   considers learning structure from  sections   space using a scoring metric   a known distribution  which is equivalent to learning  Here I argue that for complete data and a  from an infinite data set  Section   considers the more  given node ordering this division is largely a  realistic case of inferring model structure from finite  myth  by showing that cross entropy methods  data  from both a classical and a Bayesian perspec  for checking conditional independence are  tive   mathematically identical to methods based upon discriminating between models by their overall goodness of fit logarithmic scores     Keywords Bayesian networks   structural learning   conditional independence test  scoring metric  cross entropy      tion of a Bayesian network  for a recent monograph   X     In this paper I consider learning Bayesian network structures on a finite set of discrete variables  under the restrictions of complete data and a given node ordering  The following quote  Cheng et al          is  typical of statements made in articles either introduc ing a novel algorithm or reviewing current algorithms for learning Bayesian networks  Generally  these algorithms can be grouped one category of algo  rithms uses heuristic searching methods to construct a model and then evaluates it using a scoring method       The other category of algorithms constructs Bayesian networks by analyzing dependency relationships between nodes   I will assume that readers are familiar with the no see Cowell et al   Introduction   into two categories   Notation and background results              Xn   of          n  X  I consider a finite set      finite  discrete random variables  X   X         Xn                  n  denotes some in  taking values in the state space  xj  Xi   If A  V  dex set  then XA will denote the subset of variables   X a     a E  A   and will take values in  Where convenient a variable its index  v   X    XA      XaEAXa   may be referred to by  Particular configurations will be denoted  using lower case letters  for example  or XA E XA   x    x              Xn    In this paper I consider search algorithms constrained by a given node ordering  without loss of generality I will take the node ordering to be   X         X     Let   n denote the set of directed acyclic graphs  dags  on  X   such that  Xi  For g E  n let  can be a parent of  P   Xj  only if i   j   denote the set of distributions di  rected Markov with respect to g  This means that for  any  P   E  P    the probability mass function factorizes       COWELL  as     Pg x   where v in g   Xpa v g   IT Pg Xv I Xpa v g     Assumptions made in learning a network        vEV  I shall make the following assumptions for the remain  denotes the set of parents of the vertex  ing sections      I will be looking for good predictive models  se  P X  and Q X  be two probability distributions X  The Kullback Leibler divergence between P and Q is defined to be  Let  lected according to a log scoring rule  and choos  over  K P  Q  It takes  a     p X  log   q X    Ep     ing the simplest model among equally good pre dictive models     p x  L xp x  log q x          The dataset is complete  and there are no latent  xE  variables   non negative value measuring the similarity  or closeness of the distribution Q to that of  P   vanish    The node ordering is given  and without loss of  ing if and only if the distributions are identical   generality is  It was shown by Cowell         see also Cowell et al           that for a given graph g E Yn the distribu  tion P  E P  which minimizes  distribution  P X   K P  P    UAI          for some fixed  assigns to every vertex v E V    XI              Xn   There are no logical constraints between the vari ous conditional probability tables to be estimated   Assumption   emphasizes that I am not looking to Let A  Band C be disjoint index subsets of V  and let  P  X   be some distribution over  entropy  X   Then the cross  of X A and XB is defined to be  construct causal models from data  but simply seeking good predictive models  The log scoring rule is unique  in that it is  for multi attribute variables  the only proper scoring rule whose value is determined solely by the probability attached to the outcome that actually  occurs  Bernardo        The final part of Assumption whilst the cross entropy of  Xc   XA  and  XB  conditional on     is Occam s razor  without it I could choose the satu  rated model  that is  the complete graph  which would fit the data perfectly   or conditional cross entropy  is defined as  Scoring based search methods  usually try and balance these two aspects   by penal izing a model s predictive score with some measure of the model s complexity   as a way of reducing over fitting  Assumption   is made for simplicity  to avoid approxi  XA is conditionally independent of Xa given Xc under the distribution P  written as XAlLPXB I Xc  if and only if p XA  XB I Xc  p XA I Xc  p Xa I Xc   Dawid        The notation  to account for the pattern of missing data  It also im  llp will be abbreviated to lL when the distribution  depending upon the node and its parents in the dag    We say     P under consideration is clear from the context  Note that if XAllPXB I Xc  then Hp  XA  XB I Xc       and vice versa   For g E   n  any distribution  P   E P  has the directed  Markov property  that is  any node is conditionally in  dependent of its non descendants given its parents in  g   plies that the logarithmic score of a dag decomposes additively into functions  one function for each node  thus making local search possible by enabling indepen dent optimizations of each node s parent set  Assumption   implies that the dag I obtain might not exhibit all of the conditional independence properties of the data  but only those consistent with the order ing  Assumption  XvllX nd v g  I X pa v g  Note  mations being made to handle missing data  or having  in  particular  that  Hpg  Xu  X nd v g  I X pa v g            states that I am assuming local meta  independence of the conditional probabilities associ    ated with the families of any given graph considered this  implies   Dawid and Lauritzen        These conditional prob abilities will be taken as parameters to be estimated    UAI         Learning networks from a known distribution   In this section  I assume that the joint distribution P X  is known  this is equivalent to recovering P X  from its maximum likelihood estimate  MLE  for the saturated model in the limit of an infinite amount of data drawn from P X   The task is to find the sim plest model g E  n such that P  X  P X               COWELL  sets R  to find the largest such set for which the cross entropy vanishes  In practice  this is not usually possi ble because the search space is too large  Thus heuris tic searches are normally applied  usually based upon evaluating     with S  singleton sets  An example of such a search is     Set Xpa i     independence tests       Hp X   R  I X pa i   XJ           L  t P x  r  Xpa  i  XJ    p x    I Xpa   i     p  I Xpa i    r I Xpa i      log  p x  I r  Xpa i        r   p  p  X I Xpa              X  s  Xpa i    log  p x  I s  Xpa i      X I Xpa  t     p  Xpa i     Y     Model selection via Kullback Leibler  Given a graph g E Yn  the distribution directed Markov with respect to g  that is  factorizes as      which has minimum Kullback Leibler divergence from P X  obeys     for each node in the graph  In prin ciple  one could perform an exhaustive search over all possible graphs g E Yn  finding their closest matching distributions P  X  in terms of Kullback Leibler di vergence from P X   selecting those graphs for which the Kullback Leibler divergence vanishes  and select ing among these graphs the one having the fewest num ber of edges  Consider a graph g E  n  and it associated distribu tion P  X  which satisfies      The Kullback Leibler divergence is given by       vanishes  and conversely  Hence     forms the ba sis of a conditional independence test  Note that if X ll R  I Xpa i   then for any subset S  C R  it is also true that X ll S  I Xpa i   and  L  t P         When     hold s the conditional cross entropy       Hp X   S  I Xpa i    Xpa il  divergence      X   Select S  E R  such that Hp X  S  I Xpa i   is maximized  RemoveS  from R  and add it to Xpa i   This is similar to the  thickening and thinning  algo rithm of Cheng et al          More generally  S  could represent a restricted set of subsets of R   not just sin gleton sets       The minimal set Xpa i  may then be taken as the set of parents of node xi in the sought for graph  If found for each X   the joint distribution will factorize as      Let us write R       X         X  t    Xpa i  and Xr  X         Xi   Then using the identity P X  I R   Xpa i  P R  I Xpa i    P X   R  I Xpa i   we have  L  t P   X   r   Xpa i   log        Xt       X  t    Xpa i  such that X ll Y I Xpa i     Y  is TRUE do   which is equivalent to the independence statement        WHILE    Y E  n P X   II P X  I Xt       X      i l The goal of the model search using conditional inde pendence tests is to find for each node X  a minimal set Xpa i    X         Xi t   such that    and R      WHILE X ll R  I Xpa i  is FALSE do  Model selection via conditional  Given the ordering  X        Xn   the joint distribu tion P X  may be factorized as           will vanish also  and conversely   In principle  one could perform an exhaustive search over all possible       Note that     decomposes into a sum of terms  one for each node  where the g dependence of the term on each node depends upon the family in g of that node  In fact for the same graph g  the ith term in the summation     is identical to the cross entropy expression      Thus  an exhaustive search based upon conditional independence is equivalent to an exhaustive  search which minimizes Kullback Leibler divergence        UAI      COWELL  Suppose in a stepwise search algorithm that g is our  current model and a candidate model g  differs from  F X   Xpa i g   S    X  for which Xpa i g        Xpa i g  Then the difference in Kullback Leibler divergence of the  L  gin one node  two models is found f rom       to be  Hp X   X pa i g     Xp a i g  I Xpa i g          with  S       fi x  Xpa i g  s    l  l    g i   which is  Xi Xpa  i g   Si  and the conditional cross entropy  Xpa i g    Xpa i g        Thus choos  P x  I Xpa i g    where  og     X   p    x   IXpa i g    s    p   X  I Xpa i g       p x    Xpa i g   fi xpa i g        etc   Then a search heuristic would employ a decision rule  which on the basis of the value of  would either       accept or reject the hypothesis that  X JlS  IXpa i   ing the g  differing from g by one or more edges which  and if the latter  decide which among the candidate  minimizes  algorithm   maximizes      is equivalent to choosing g       g which K P  P     After adding parents to X  until  no further decrease in Kullback Leibler divergence is  possible  on adding yet further nodes as parents   one could thin the parents of node  for which  l    g  g    X    remains zero   by removing nodes  More generally  a decision criterion in the search al    gorithm which moves to a model g from a submodel g  based upon the consideration of a set of pos sible sets  S   and their associated conditional independence tests  could be used to give the same result  or move  based  S   to add to Xpa i  for the next iteration of the search Two common decision heuristics are   i   if the value of       is below a fixed threshold value  E  then accept the conditional independence   ii  perform  a classical significance test  using the null hypothesis of conditional independence  under which a suitable multiple        the multiplier being twice the number  X  of observations  will have a  distri b u tion for some  suitable k  Each of these has a counterpart in search heuristics based upon a log score   upon the consideration of the same S  and the changes       in Kullback Leibler divergence  because the numeri  Let us write n xA  for the marginal count of the num ber of cases in the dataset for which XA  For  cal quantities entering into the decision process upon which the decision is based are identical in the two search frameworks  Put another way  for every search heuristic based upon using conditional independence tests  there is an equiv  alent search heuristic based upon using changes in Kullback Leibler divergence  and vice versa   Model selection via maximum likelihood   XA  g E     Q    the log likelihood of the data decomposes as  logL p       L IT i  There  Xi Xpa i      n x   Xpa i g   log p   x  IXpa i g       is no fundamental difference between the two ap  proaches  only a difference in interpretation      which yields the MLE    Pg X  I Xpa i g    Learning networks from finite data       Suppose  as in Section  Xpa i g    independence tests  The directed Markov property  and the completeness  of the data  allows conditional independence tests to be performed locally on each node  The conditional in  dependence tests employ MLEs  However due to sam  pling variability the tests are not sharp  so typically the requirement of the exact vanishing of  conditional   cross entropy expressions is relaxed  model  with node  suppose that g is  X   from the data one obtains the ML Es  our current  Xpa i g   and F X   Xpa i g    having parents  Furthermore  suppose that for some node or set of nodes  S   E  X  l   Xpa i g   n x  Xpa  i g   n  Xpa i g            one evaluates  the MLEs        Xpa i g        that g is our current  model and g  differs from g in one node  Model selection via conditional  Thus for example      X   for which  Then the difference in the log  likelihoods of the two models evaluated at their MLEs  is given by log  L pg   L pg     L         Xi Xpa  i gl   log  n x   Xpa i g     X  n x   Xpa i g    n xpa i g     n x   Xpa i g     n Xpa i g             Thus one could decide to move from g to g  in the model search if this quantity is positive  However  this  will generally be the case with finite data  because the  larger model will fit the data better by virtue of having  extra parameters  hence the significance of the better   COWELL  UAI      fit needs assessing  One simple heuristic is to set a threshold e such that if the change is greater than e the difference is taken to be significant   to do this we must first normalize      by the total number of cases N    L x n x  in the dataset  Doing this yields   N  log  L fv  L pg       L  Xi XpA  i  g       p x  I Xpa i g       p x   Xpa i g    log   A  p  X  IXpa i g        where p represents the  marginal of the  MLE of the saturated model  This is identical to      with Si    Xpa i g     Xpa i g   A more formal approach would be based on hypoth esis testing  Note that twice the value of      is the difference in the deviances of the two models  which under the assumption that the larger model is true  and that the smaller model is also true  will have a X distribution with k equal to the difference in the degrees of freedom of the two nested models  Thus we perform the same test  and obtain the same result  as the formal conditional independence test described at the end of Section      Alternatively  one could penal ize the deviance by some function of the number of parameters  for example by using the Akaike Informa tion Criterion  Akaike       which penalizes the more complex model by twice the number of extra parame ters   More generally  because of the equality of     and     it follows as in the last paragraph of Section      that for every search heuristic based upon testing for con ditional independence  there is an equivalent search heuristic based upon using changes in log maximum likelihood  and vice versa  There is no fundamental difference between the two approaches  only a differ ence in interpretation       The Bayesian approach   Many belief network search algorithms using a scor ing metric tend to employ the Bayesian formalism  with the score being the log marginal likelihood  The advantages are that for smaller data sets  where the asymptotic distribution results required for the tests in Section     and Section     may not apply  although exact classical tests are available  see Chapter   of Lau ritzen          the results tend to be more robust and  furthermore  generally less sensitive to the presence of zeroes in marginal counts  The Bayesian approach requires a prior on the space of graphical structures   usually this is taken to be uniform  but there are other alternatives  Beckerman        For each graphical structure a prior on the      probability parameters is also required   usually these are taken to be locally independent Dirichlet priors  Under these assumptions and complete data the mar ginal likelihood may be evaluated explicitly and de composes into a product of terms  one for each node  An early and important paper is Cooper and Ber skovits         who gave an explicit formula for the marginal likelihood under these conditions  A common feature of the analyses given in Section     and Section     is that the global scores factorize into local contributions from each node  and  moreover  that in comparing two similar graphs their score dif ference is identical to quantities which arise when test ing conditional independence using cross entropy mea sures  I shall now show that a similar circumstance arises in a Bayesian approach when globally indepen dent priors are employed  The key feature is that global independence is preserved under updating with complete data  Cowell et al         Thus suppose each node v of a graph g E  n has an associated  vector  parameterization   of the condi tional probability table of v  and a globally indepen dent prior distribution over the parameters            v E V   Global independence means that the prior measure factorizes as d r        flv d r      Under these conditions the marginal likelihood of the graph g in the light of complete data D is  L g    p Dig    J Pg Dig    d rg B     II   V  II Pg Xv IXpa v   er xv Xp v  d l g      Bv Xu  Xpa v         From      we see that the marginal likelihood factor izes into terms  one for each node and it parents  As before  let g  be a graph identical to graph g except for a difference in the parent set of the Xi  Then g  will require a different parameterization and associated prior   see Cowell         Beckerman et al         for alternative strategies for doing this for Dirichlet pri ors   but we may take for every node other than Xi the same local parameterization and contribution to the prior as for the graph g  that is  for Xv     Xi   e    e  Pg  Xv IXpa v g   e     Pg Xv IX pa v g e  andd r   Z    d       z     If  furthermore  we take uni   form priors over the alternative graphical structures  ie  P g    P g     then after suitable cancellations we obtain the ratio of posterior probabilities given in              COWELL  p g  I D    p D I g   p g I D    p D I g   UA         fe  fL xpa i g   Pg   xi I Xpa i g    e  t x  Xpa   g   d rg  B      for flx  xpa   Dl Pg Xi IXpa i g   Bf n x  x  a   gJ d r   f     The decision of a local score driven search to stay with           Conclusions  graph g or move to graph g  would depend upon the         use      in a Markov chain Monte Carlo  value of this ratio  Madigan and Raftery  Under the conditions of complete data and given node   the logarithm of    ordering I have shown that conditional independence  based graphical model search procedure  which they  tests for searching for Bayesian networks are equiva  apply to model selection and model averaging  see also  lent to local log scoring metrics   they are two ways  Madigan and York           of interpreting the same numerical quantities   I am not aware of papers applying Bayesian tests of  conditional independence to Bayesian network model  selection  hence there is not a direct comparison I can make of        to results extant in the current literature    This is not to say there are none  however Bayesian methods   based on comparison of posterior proba bilities   for testing for independence in contingency  It is  possible to relax the node ordering constraint by con sidering arc reversals in addition to arc removals and additions  then the change in score  which will be local to a pair of nodes  will be a combination of the terms which would be considered using conditional indepen dence tests   However  in the latter case  one would  have the extra option of deciding if the conditional in  tables do exist  see for example Jeffreys            Good  dependence properties associated with each of the two            conditional independence searching can be more re             See also the discussion in Madigan and Raftery  However  a formal Bayesian approach would  consider the following two hypotheses   pendence tests were combined into a single test  then  p Xi I Xpa i g    e   p Xpa i g   I   i g    d r ef   d r    i g      p Xi  Xpa i g   IBH   d r BH   p Xi IXpa i g   Bf p Xpa i g   I  i g     H   d r Bf d r    i g      model exhibiting conditional independence  and  is a parameterization common to the two hypothe Then  from the  possibly equal  priors  P Ho   P H   and the data D  posterior probabilities P  Ho I D  and P H  ID  are evaluated and compared  It is left to the reader to verify that this leads to       and  Thus if one were to do model search based upon lo  cal conditional independence tests  then one should use        the two procedures would again be equivalent under the same decision rules   
Propagation using Chain Event Graphs Peter A  Thwaites  Statisti s Dept  University of Warwi k Coventry UK CV   AL  Jim Q  Smith  Statisti s Dept University of Warwi k Coventry UK CV   AL  Abstra t  A Chain Event Graph  CEG  is a graphi al model whi h is designed to embody onditional independen ies in problems whose state spa es are highly asymmetri and do not admit a natural produ t stru ture  In this paper we present a probability propagation algorithm whi h uses the topology of the CEG to build a transporter CEG  Intriguingly  the transporter CEG is dire tly analogous to the triangulated Bayesian Network  BN  in the more onventional jun tion tree propagation algorithms used with BNs  The propagation method uses fa torization formulae also analogous to  but di erent from  the ones using potentials on liques and separators of the BN  It appears that the methods will be typi ally more eient than the BN algorithms when applied to ontexts where there is signi ant asymmetry present    INTRODUCTION Based on an event tree  a Chain Event Graph  CEG  is a more expressive alternative to a dis rete Bayesian Network  BN   embodying olle tions of onditional independen e statements in its topology  In Anderson and Smith        it is shown not only how asymmetries in a problem s sample spa e an be represented expli itly through the topology of its CEG  but also how it an express a mu h wider range of types of onditional independen e statement not simultaneously expressible through a single BN  As with the BN  the CEG of an hypothesised model an be interrogated using natural language before the graph is embellished with probabilities  In Thwaites and Smith        and Ri omagno and Smith        we demonstrate how the CEG an also be used to represent and analyse various ausal hypotheses  In this paper we ontinue the development of CEGs by demonstrating how the  Robert G  Cowell  Cass Business S hool City University London EC Y  TZ  graph provides a useful stru ture for fast probability propagation in asymmetri models  It has been noted that the CEG is an espe ially powerful framework for inferen e when a probability model is highly asymmetri and eli ited through a des ription of how situations unfold  Although theoreti ally a BN an be used in this ontext  the lique probability tables are then very sparse and ontain many zeros or repeated probabilities  This impedes fast propagation algorithms and has led to the development of many ontext spe i variants of BNs  Boutilier et al       M Allester et al       Poole and Zhang       Salmeron et al        often based on trees within liques  These developments provoke the question as to whether a single tree might be used for propagation instead of the BN  Now obviously the event tree itself expresses no onditional independen ies in its topology and these independen ies are the building blo ks of urrent propagation algorithms  However  unlike the event tree  the CEG expresses a fairly omprehensive olle tion of onditional independen ies  In this paper we demonstrate the surprising fa t that there is a dire t analogue between a distribution on a BN expressed as a produ t of potentials supported by a graph of liques and separators  and propagation algorithms on CEGs using the distributions on the hildren of the CEG s non leaf nodes and marginal likelihoods on the verti es themselves  This enables us to develop fast propagation algorithms that use a single graph  the transporter CEG   analogous to a triangulated BN   as its framework  This framework is highly e ient for asymmetri  non produ t spa e ontexts  and in parti ular does not involve propagating zeros in sparse but large probability tables  nor ontinually repeating the same al ulations  whi h would be the ase if we were to use the BN as a framework in this sort of environment with a naive BN propagation algorithm  In the next se tion we formally de ne the transporter CEG C  T   of a hypothesised probability tree T   In   se tion   we present an algorithm analogous to that of Cowell and Dawid        for a BN where onditional probability tables asso iated with the hildren of a given vertex of the CEG take the role of liques  and vertex probabilies take the role of separators  In se tion   we demonstrate the e ien y of this algorithm with a simple example     PROBABILITY TREES AND CHAIN EVENT GRAPHS Probability trees  Shafer        and their ontrol analogues de ision trees  have been found to be a very natural and expressive framework for probability and deision problems  and they provide an ex ellent framework for des ribing sample spa e asymmetry and inhomogeneity in a given ontext  see for example Fren h and Insua          We start with an event tree T with vertex set V  T   and  dire ted  edge set E  T    Hen eforth all the tree s non leaf verti es fvg situations  and denote this set of verti es S  T     V  T    We an onvert an event tree into a probability tree by spe ifying a transition matrix from its verti es V  T    where the absorbing states orrespond to the leaf verti es  Transition probabilities from a situation are zero exept for transitions to one of that situation s hildren  This makes the transition matrix upper triangular  Su h a matrix would look like the one in Table   whi h shows part of the matrix for the problem des ribed in Example    Note that ea h transition probability an be identi ed by an edge on the tree   Table    Part of the transition matrix for Example   v  v  v  v         v  v  v  v  v   v   v   v   v                                                                                                                   v                                         One way of seeing onditional independen e statements on a BN is as identities in ertain ve tors of onditional probabilties   expli itly those probability ve tors asso iated with di erent an estor on gurations but the same parent on guration of a variable in the BN  Ri omagno and Smith        There is a large lass of models where the probabilities in some of the rows of the transition matrix an be identitifed with ea h other  The CEG is a topologi al representation of this lass of models  and the transporter CEG de ned below is a subgraph of the CEG  Let T  vi    i        be the unique subtrees whose roots are the situations vi   and whi h ontain all verti es after vi in T   Say v  and v  are in the same position w if      the trees T  v    and T  v    are topologi ally identi al     there is a map between T  v    and T  v    su h that the edges in T  v    are annotated  under that map  by the same  possibly unknown  probabilities as the orresponding edges in T  v     It is easily he ked that the set W  T   of positions w partitions S  T    Furthermore  somewhat more subtlely  if v    v    w and vij   V  T  vi     then the vertex sets of T  vi   i        are mapped on to ea h other by this map  and vij   wj i        for some position wj  providing vij is not a leaf vertex in either subtree   For details of this property see Anderson and Smith         We now draw a new graph to depi t both the sample spa e of T and ertain onditional independen e statements  The transporter CEG C  T   is a dire ted graph whose verti es W  C  T    are W  T     fw  g  There is an edge  e   E  C  T     from w  to w     w  for ea h situation v    w  whi h is a hild of a xed representative v    w  for some v    S  T    and an edge from w  to w  for ea h leaf node v   V  T   whi h is a hild of some xed representative v    w  for some v    S  T    The transporter CEG  hen eforth labelled simply as C   is the subgraph of a CEG  de ned in Anderson and Smith         where all undire ted edges in the CEG are omitted  The relationship between the transporter CEG and the CEG is dire tly analogous to the relationship between a triangulated BN and the original BN  Certain onditional independen e statements that an be lost through onditioning are simply forgotten so that an homogeneous propagation algorithm an be onstru ted on the basis of the enduring onditional independen ies  Unlike the BN  this CEG an have many edges between two verti es and always has a single sink vertex w    Although typi ally having many fewer verti es than T   it retains a depi tion of the sample spa e stru ture of T   Thus it is easy to he k that the set of root to leaf paths of the tree  representing the set of all possible unfoldings of the history of a unit  are in one to one orresponden e with the set of root to sink paths on the transporter CEG  The CEG  onstru tion pro ess is illustrated in Example    Example    Consider the tree in Figure    whi h has    atoms  root to leaf paths   Note that as the subtrees rooted in the verti es fv i g are the same  and those rooted in fv i g are the same  the distribution on the tree an be stored using   onditional tables whi h ontain       free  probabilities  Our transporter CEG  Figure    is produ ed by ombining the verti es fv i g into one position w    the ver    ti es fv i g into one position w    the verti es fv i g into one position w    and all leaf verti es into a single sinknode w    The full CEG for our example is simple   it has no undire ted edges  and is identi al to the transporter CEG C   For a simple CEG  all the onditional independen ies inherent in the problem are onveyed by the transporter CEG  vinf                           v                           v             v                                                         v                      v                                 v    v                      v            v              v    Figure    Tree for Example            w                                                     w              winf                           w                              w                                       w                                            w                                  v                vinf   su h a way  the onditional independen ies embodied in the problem  and in our transporter CEG  annot be e iently oded in a BN without introdu ing tables with many zeros  Consequently  even in this very simple example we have e ien y gains in storing this distribution over using a saturated model  a BN  or a tree   w                 Figure    Transporter CEG for Example   Figure   shows the probabilities of rea hing ea h position w  the event rea hing w  denoted   w   is the union of all root to sink paths passing through w   It also shows ea h edge probability  e  w  j w          e w  w     j   w    where   e w  w     is the union of all root to sink paths utilising the edge e w  w       The problem represented by the tree in Figure   is asymmetri in that not all the root to leaf paths are of the same length  and also in the lo al stru ture asso iated with its verti es  We do not know whether the verti es fv i g are related in any ontextual way to the verti es fv i g or fv i g  and hen e we annot obviously de ne variables on the sigma algebra of the tree to allow us to represent the problem as a BN  Even supposing we were able to represent the problem in  A SIMPLE PROPAGATION ALGORITHM THE FRAMEWORK  To spe ify the joint distribution of all random variables measurable with respe t to a CEG we simply need to spe ify the ve tor of onditional probability mass fun tions asso iated with ea h of its positions  The rst step of our propagation algorithm is analogous to the triangulation step for a BN  whi h allows us to retain all onditional independen e properties at the ost of a possible loss of e ien y  To do this we ignore onditional independen e statements oded by the undire ted edges of the CEG and work only with the subgraph onsisting of its positions  together with its dire ted edges  but not its undire ted edges   our transporter CEG C   For ea h position w   W   W  C  nfw  g we store a ve tor of probabilites   w    f e  w  j w  j e w  w      E  w g where E  w    E  C   is the set of all edges emanating from w    w  is of ourse a onditional probability distribution  We let X  w  be the random variable taking values on f              n E  w  g  where n E  w   is the number of edges emanating from w  whose probability mass fun tion is given by the omponents of   w  taken in order  The positions w   W take the role of the liques in a triangulated BN  whilst the ve tors f  w  j w   W g are analogous to the lique probability tables  We an now spe ify the probability    of every atom    a root to sink path of C   of length n      as a fun tion of f  w  j w   W g and C   If       w    w      e      w              e   n     w    then nY             e   i  i    where   e   i  is a omponent of the probability ve tor   w   i         i   n     It follows that the distribution of any random variable measurable with respe t to C an be al ulated from f  w  j w   W g       COMPATIBLE OBSERVATIONS  Re all that propagation algorithms for BNs based on triangulation are only designed to propagate information that an be expressed in the form     fXj   Aj g for some subsets fAj g of the sample spa es of fXj g the vertex variables of the BN  Propagating information about the value of some general fun tion of the vertex variables using lo al message passing is not generally possible  be ause onditioning on the values of su h a fun tion an destroy the onditional independen ies on whi h the lo al steps of the propagation algorithm depend for their validity  In the same way the types of observation we an efiently propagate using C and f  w  j w   W g needs to be onstrained  In general an observation an be identi ed with a subset   of the set of all root to sink paths f g  The most obvious onstraining assumption on    and the one we will hen eforth make in this paper  about what we might learn is that our observation   an be identi ed with having learned that fX  w    A w g for some subsets fA w g of the sample spa es of the position random variables fX  w g  Call su h a set C ompatible  Note that   is C ompatible if and only if there exists possibly empty subsets fE   w  j w   W g su h that O A       f  j e    E   w  for some w   W  for ea h edge e  on the path   in C g So we an identify aS ompatible observation with the set of edges E    w W E   w    E  C    We note that the set of ompatible observations is large and in parti ular when the CEG is expressible as a BN ontains all sets of the form O A  de ned above  Example         MESSAGE PASSING FROM COMPATIBLE OBSERVATIONS ON A CEG  The message passing algorithm is a fun tion from the original probabilities f  w  j w   W g to revised probabilities on the same graph f    w  j w   W g onditional on the observation    Note that on e edgeprobabilities have been revised  the resulting graph may not be a minimal CEG  in that we may have verti es within the graph whi h are the roots of identi al sub graphs   It is possible  although unne essary for information propagating purposes  to add a further algorithm step to produ e a minimal CEG if this is required  This step ensures that any verti es that are equivalent are ombined into a single position  Messages are passed from the terminal edges ba kwards through the transporter CEG along neighbouring edges until rea hing the root in a olle t step giving a new pair f   w     w  j w   W g  We then move forward from the root produ ing revised f    w  j w   W g  Let W      denote the set of positions all of whose outgoing edges terminate in w  in C      For any edge e w  w    su h that w   W       set the potential  e  w  j w      if e w  w       E    and  e  w  j w     e  w  j w  if e w  w      E    Let the emphasis    w     X  e E  w    e  w   j w   Say that w  and ea h of these positions is a ommodated   Consider      f  j e    fe  w    w     e   w    w     e   w    w     e   w    w     e   w    w     e   w    w     e    w    w     e    w    w     e    w    w     e    w    w   gg This orresponds to all the root to sink paths in the subgraph of C given in Figure    w   w      For any position w all of whose hildren are a ommodated  and edge e w  w     set the potential  e  w  j w      if e w  w        E    and  e  w  j w         e  w j w    w   if e w  w      E    Let the emphasis  X   w     e  w  j w  e E  w   Say that w is a ommodated     Repeat step   until all w   W are a ommodated  This ompletes the olle t steps     For all w   W   set      w      if    w          w       w  if    w         w   w  w   winf  where    w    f e  w  j w  j e w  w      E  w g  Clearly we have that  w   w   Figure    Subgraph for event   in Example       e  w   j w      if e w  w       E         w  j w     e  w j w  if e w  w      E e    w       A proof of these results is given in the appendix  Note that as we move forward through the graph the updated probabilities of   w    w  subpaths will be of the form        w  j w       and we get        w      Y  i       e  wi    X    f  w   w g        w  j w     Steps      and   give us the graph in Figure    Step   gives us the CEG in Figure    note that our CEG is again simple  and also minimal without the need for the additional step previously mentioned         w            x                               x  w      x  w                   x        w                                 winf  x                             x      w        w         Figure    Potentials and emphases added                      w                                               w                                            w   w                     winf                                                     Also note that at the ost of some omputation  we an perform inferen e on the redu ed graph C  whose edges E  C    are just the edges e in E  C   with nonzero probabilities     e   and whose verti es W  C    are the w   W  C   for whi h   w        The non zero edge and vertex probabilities of C then simply map on to their orresponding edge and vertex probabilities in C    Note that  unlike for the BN  any non trivial C ompatible observation stri tly redu es the number of edges in the edge set after this operation  A pseudo  ode version of our algorithm is provided below  Let C  W  C    E  C    be a transporter CEG with edges in E  C   having labels ei   i               ne   su h that i   j   ei   ej  ei does not lie downstream of ej on any w    w  path   and positions in W  C  nfw  g having labels wi   i                  mw   su h that i   j   wi   wj   To update the edge probabilities on C following observation of an event    do      Set A         Set B         Set i         Repeat  a  Sele t ei  b  If ei   E    add ei to A otherwise  set    ei         Set i   i     Until i   ne         Set   w            Set j   mw     Repeat  a  Sele t wj  b  Repeat  i  Sele t e wj   wj      E  wj     A  ii  Set  e  wj  j wj      e  wj  j wj     wj     iii  Add e wj   wj    to B Until E  wj     A   B P     Set   wj     e E wj    e  wj  j wj    d  Set j   j   Until j     w j w      For ea h e w  w      E    set   e  w  j w     e     w      Return f  e g      w   n     i    Example           Q    e  i         j                    e   i    n  i   Q      w  i  i   Y  n     j wi    From the de nition of a ommodation  the order of these operations  like the perfe t order used to update a triangulated BN  depends only on the toplogy of C   so it an be set up beforehand                      w    w      e      w              e   n     w    is given by the invarian e formula   w   Figure    Updated CEG C  Note that  in analogy with equation     of Cowell and Dawid         the onditional probability of any atom     A CLOSER LOOK AT OUR EXAMPLE Consider the CEG in Figure   and let the    edges be labelled ei in the same order as the f i g thereon  In   Examples   to   we showed how to reate and use a Transporter CEG without on erning ourselves with a ontext  We now add that ontext and suppose that this CEG represents a Treatment regime for a serious medi al ondition  and the edges arry the meanings given in Table    Table    Edge des riptors Edge e  e  e  e  e  e    e  e    e  e   e   e     e   e   e   e    Des ription Not riti al   Treatment pres ribed I Liver failure   Treatment       II Liver   Kidney failure   Treatment       II Responds to I   Full re overy No response to I   Surgery pres ribed III Responds to II   Surgery       III No response to II   Surgery       IV Re overy   Lifetime monitoring Re overy   Lifetime medi ation Death in surgery Survives surgery IV   Treatment       V Re overy   Lifetime on treatment V No response to V   Dies  As alluded to in se tion    it is not possible to represent this regime e iently as a BN  nor yet as a ontextspe i BN  given that the asymmetry of the problem does not just lie in it having asymmetri sample spa e stru tures  By equating the des riptions of edges e  and e     edges e   and e     and edges e     e   and e     we an however approximate the problem with a   variable BN  where X  Diagnosis and initial treatment an take values orresponding to the out omes fNot riti al  Liver failure  Liver   Kidney failureg  X   nd treatment to fNone  III  IVg  X   rd treatment to fNone  Vg  and X  Response to fDeath  Partial reovery  Full re overyg  The BN for this approximation to the model is given in Figure    X   X   X   X   Figure    BN for our example To store the model using a CEG requires    ells   orresponding to the    edges   but in this BN    ells    for the lique fX    X  g and    for fX   X    X  g      of whi h are storing the value zero  The event   in our example orresponds to the observation that a patient was not diagnosed with Liver and Kidney failure  and is still alive  Propagation of this event enables a pra titioner to establish prob   ability distributions for the possible histories of our patient  Note that it is only the fa t that we an des ribe   in su h a simple manner that has allowed us to approximate the problem with the BN in Figure    Propagating of the event   using a simple Jun tion Tree algorithm on the liques of the BN takes a minimum of    operations  Propagation on the CEG using our algorithm requires    operations   orresponding to    ba kward edges    ba kward verti es and    forward edges   So even in this simple example  using the CEG is more e ient than the BN  The e ien y here is due mainly to the fa t that the lique probability tables ontain many zeros  This is re e ted in the CEG by the w    w  paths not all having the same length  It is this form of asymmetry in a model that ontext spe i BNs do not ope with adequately  and why CEGs are a better stru ture for use with this type of problem  The problems in whi h the algorithm des ribed above are most e ient are when the CEG stru ture is known to be simple  To store the probability tables for the CEG requires only N     W  C        E  C         E  C    ells  In this ase the olle t step involves only N al ulations and the topology of the CEG is valid so that in parti ular the original probability table stru ture an be preserved  The potential produ t ne essitates only a single distribute step whi h again only involves at most N al ulations  For large trees with mu h of the type of subtree symmetry dis ussed above the propagation is extremely fast  It is worth qui kly looking at a very simple example arising from model sele tion in graphi al or partition model problems  an area urrently attra ting some interest  Consider a model with random variables X          Xn  where X  with M        n     n    possible states  determines whi h pair of binary variables from fX         Xn g are dependent  all other variables from fX          Xn g being independent of ea h other and of the pair determined  The CEG of this model has at most M       n  edges and     M n positions  whereas the BN is a single lique requiring M    n   ells for storage  As the number of operations required for propagation on both the BN and the CEG is of the same order of magnitude as the number of ells required for storage  it is lear that the CEG is far more e ient in this example    DISCUSSION There are several advantages of this method over the oding of this type of problem through a BN  Firstly  the al ulated probabilities an be proje ted ba k on to the edges of the eli ited tree  so that the onsequen es of inferen es given di erent types of information an be immediately appre iated by the pra ti    tioner  Se ondly  the a ommmodation of data in the form of a ompatible observation is mu h more general than the a ommodation of subsets of observations from a predetermined set of random variables  so the CEG provides a more exible framework for propagation  parti ularly when data is ontingently ensored  Thirdly  there are e ien y gains as outlined above  We intend to show how great these gains an be for very large problems in a later paper  Note also that  as is the ase with the triangulation step in BN based algorithms  there are faster algorithms  Thwaites       than the one des ribed above  although they lose some of this algorithm s generality  Our algorithms are urrently being oded by Cowell within freely available software  and will be available shortly  Of ourse BNs provide a simpler representation of more symmetri problems and should always be preferred when the three ontingen ies are not satisi ed  The CEG does not provide a universal improvement over the BN for propagation  In parti ular in problems when the underlying BN is de omposable but the CEG is not simple the BN propagation an be mu h more e ient  But in highly asymmetri problems  the CEG should de nitely be a rst hoi e  It should be noted that it is also possible to de ne a dynami analogue of the CEG  and our investigation of these suggests that a time sli ed CEG  analogous to a time sli ed BN  will be an ideal vehi le for a dynami updating algorithm  We hope to report on these developments in the near future  APPENDIX We laim that     e  w   j w         e w  w     j      w    e  w j w   if e w  w      E    w      if e w  w       E     Proof   For a CEG C   and C ompatible event    let T be the tree asso iated with C   T  the tree asso iated with C    and T    the subtree of T ontaining only those root to leaf paths in    T    di ers from T  in that the former retains the edge probabilities from T   Consider a position w   C  w   C    orresponding to a set of verti es fvi g   T   Then the subtrees rooted in ea h vi are identi al both in topology and in their edge probabilities  If there is a subpath   w    w  whi h is not part of a w    w  path in    ie    w    w  exists in C   but not in C    then there will exist a subset of fvi g whi h does not exist in T   or T       We split the set fvi g into   fvi gi I fvi gi J  verti es existing in T  verti es not existing in T  Be ause   is C ompatible  the subtrees in T    rooted in ea h vi   fvi gi I are also identi al both in topology and in their edge probabilities that they retain from T   Suppose there exists an edge e w  w    in C   then for ea h vi   fvi g  there exists an edge e vi   vi    in T orresponding to this edge  Note that      w      vi     e w  w        e  v    i  j  i I  J       e vi   vi      i I  J vi      e  w  w   j   i   I   J  and sin e the subtrees in T    rooted in ea h vi   fvi gi I are identi al  we also have       j   vi           j   vj            e vi   vi     j   vi              e vj   vj     j   vj    for i  j   I      j   vi    is the sum of the probabilities of all the   vi   vleaf   subpaths in T      and         e vi   vi     j   vi    is the sum of the probabilities of all the   vi   e vi   vi     vi    vleaf   subpaths in T     So     e  w  j w        e w  w     j      w   e w  w                w           w             S     i I  J    vi      e vi   vi           i I  J   vi     S   an expression evaluated on T   sin e   vi       e vj   vj         for i    j P   J         vi      e vi   vi       i I  P i I  J         vi    But       vi  P    for vi   fvi gi J   so this equals        vi      e vi   vi      i I  P     vi    P    i  I      e v   v      j   vi         vi      i I P     ij   i v         vi    i i I P           e vj   vj    j   vj    i I      vi    P        j   vj    i I      vi    for any vj   fvi gi I         e vj   vj     j   vj           j   vj    for any vj   fvi gi I Turning our attention to the terms in the algorithm  we laim that   w        j   vi    and  e  w  j w            e vi   vi     j   vi     vi   fvi gi I   for all w  e w  w      C    where fvi gi I is the set of verti es in T    orresponding to w  We prove this by indu tion    part of the proje t Chain Event Graphs  Semanti s  Step     and Inferen e   Consider positions w   W       Then    w       X e X e   e  w   j w      e  vleaf  j vi    X e   e  w   j w   Referen es     P  E  Anderson and J  Q  Smith  Conditional independen e and Chain Event Graphs  Arti ial Intelligen e                      C  Boutilier  N  Friedman  M  Goldszmidt  and D  Koller  Context spe i independen e in Bayesian Networks  In Pro eedings of the   th  in T     for any vi   fvi gi I       j   vi    Step     Suppose w is su h that all of its outgoing edges terminate in positions fw  g for whi h       w         j   vi     Then    w       X  e X e  X   e  w   j w      e  v    j vi       j   vi      i  e   e  w   j w    w     for any vi   fvi gi I X       e vi   vi     j   vi        j   vi     e  But   vi        e vi   vi         vi   in a tree  so this equals  X      e vi   vi        vi    j   vi    e      X       j    vi      e  vi   vi      vi             e vi   vi       vi   j   vi    e X          e vi   vi     j   vi    e          vi   j   vi          j   vi     Hen e   e  w  j w     e  w  j w    w       e  vi  j vi       j   vi     for any vi   fvi gi I       e vi   vi     j   vi        j   vi                      e vi   vi     j   vi    We now ombine our two results to give          e vj   vj     j   vj       e  w  j w         j   vj      w j w     e     w      A knowledgements  This work has been partly funded by the EPSRC as  Conferen e on Un ertainty in Arti ial Intelligen e  pages          Portland  Oregon            R  G  Cowell and A  P  Dawid  Fast retra tion of eviden e in a probabilisti expert system  Statisti s and Computing                    S  Fren h and D  R  Insua  Statisti al De ision Theory  Arnold           D  M Allester  M  Collins  and F  Periera  Case fa tor diagrams for stru tured probabilisti modeling  In Pro eedings of the   th Conferen e on Un ertainty in Arti ial Intelligen e  pages                    D  Poole and N  L  Zhang  Exploiting ontextual independen e in probabilisti inferen e  Journal of Arti ial Intelligen e Resear h                       E  M  Ri omagno and J  Q  Smith  The ausal manipulation and Bayesian estimation of Chain Event Graphs  Resear h Report        CRiSM           E  M  Ri omagno and J  Q  Smith  The geometry of ausal probability trees that are algebrai ally onstrained  In L  Pronzato and A  Zhigljavsky  editors  Optimal Design and Related Areas in Optimization and Statisti s  hapter    pages           Springer Verlag           A  Salmeron  A  Cano  and S  Moral  Importan e sampling in Bayesian Networks using probability trees  Computational Statisti s and Data Analysis                        G  Shafer  The Art of Causal Conje ture  MIT Press            P  A  Thwaites  Chain Event Graphs  Theory and appli ation  PhD thesis  University of Warwi k            P  A  Thwaites and J  Q  Smith  Evaluating ausal e e ts using Chain Event Graphs  In Pro eedings of the  rd European Workshop on Probabilisti Graphi al Models  pages          Prague           
