 The paper investigates parameterized approximate message passing schemes that are based on bounded inference and are inspired by Pearls belief propagation algorithm  BP   We start with the bounded inference mini clustering algorithm and then move to the iterative scheme called Iterative Join Graph Propagation  IJGP   that combines both iteration and bounded inference  Algorithm IJGP belongs to the class of Generalized Belief Propagation algorithms  a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini clustering and belief propagation  as well as a number of other stateof the art algorithms on several classes of networks  We also provide insight into the accuracy of iterative BP and IJGP by relating these algorithms to well known classes of constraint propagation schemes      Introduction Probabilistic inference is the principal task in Bayesian networks and is known to be an NP hard problem  Cooper        Roth         Most of the commonly used exact algorithms such as jointree clustering  Lauritzen   Spiegelhalter        Jensen  Lauritzen    Olesen        or variableelimination  Dechter              Zhang  Qi    Poole         and more recently search schemes  Darwiche        Bacchus  Dalmao    Pitassi        Dechter   Mateescu        exploit the network structure  While significant advances were made in the last decade in exact algorithms  many real life problems are too big and too hard  especially when their structure is dense  since they are time and space exponential in the treewidth of the graph  Approximate algorithms are therefore necessary for many practical problems  although approximation within given error bounds is also NP hard  Dagum   Luby        Roth          c      AI Access Foundation  All rights reserved    M ATEESCU   K ASK   G OGATE   D ECHTER  The paper focuses on two classes of approximation algorithms for the task of belief updating  Both are inspired by Pearls belief propagation algorithm  Pearl         which is known to be exact for trees  As a distributed algorithm  Pearls belief propagation can also be applied iteratively to networks that contain cycles  yielding Iterative Belief Propagation  IBP   also known as loopy belief propagation  When the networks contain cycles  IBP is no longer guaranteed to be exact  but in many cases it provides very good approximations upon convergence  Some notable success cases are those of IBP for coding networks  McEliece  MacKay    Cheng        McEliece   Yildirim         and a version of IBP called survey propagation for some classes of satisfiability problems  Mezard  Parisi    Zecchina        Braunstein  Mezard    Zecchina         Although the performance of belief propagation is far from being well understood in general  one of the more promising avenues towards characterizing its behavior came from analogies with statistical physics  It was shown by Yedidia  Freeman  and Weiss              that belief propagation can only converge to a stationary point of an approximate free energy of the system  called Bethe free energy  Moreover  the Bethe approximation is computed over pairs of variables as terms  and is therefore the simplest version of the more general Kikuchi        cluster variational method  which is computed over clusters of variables  This observation inspired the class of Generalized Belief Propagation  GBP  algorithms  that work by passing messages between clusters of variables  As mentioned by Yedidia et al          there are many GBP algorithms that correspond to the same Kikuchi approximation  A version based on region graphs  called canonical by the authors  was presented by Yedidia et al                      Our algorithm Iterative Join Graph Propagation is a member of the GBP class  although it will not be described in the language of region graphs  Our approach is very similar to and was independently developed from that of McEliece and Yildirim         For more information on BP state of the art research see the recent survey by Koller         We will first present the mini clustering scheme which is an anytime bounded inference scheme that generalizes the mini bucket idea  It can be viewed as a belief propagation algorithm over a tree obtained by a relaxation of the networks structure  using the technique of variable duplication   We will subsequently present Iterative Join Graph Propagation  IJGP  that sends messages between clusters that are allowed to form a cyclic structure  Through these two schemes we investigate      the quality of bounded inference as an anytime scheme  using mini clustering       the virtues of iterating messages in belief propagation type algorithms  and the result of combining bounded inference with iterative message passing  in IJGP   In the background section    we overview the Tree Decomposition scheme that forms the basis for the rest of the paper  By relaxing two requirements of the tree decomposition  that of connectedness  via mini clustering  and that of tree structure  by allowing cycles in the underlying graph   we combine bounded inference and iterative message passing with the basic tree decomposition scheme  as elaborated in subsequent sections  In Section   we present the partitioning based anytime algorithm called Mini Clustering  MC   which is a generalization of the Mini Buckets algorithm  Dechter   Rish         It is a messagepassing algorithm guided by a user adjustable parameter called i bound  offering a flexible tradeoff between accuracy and efficiency in anytime style  in general the higher the i bound  the better the accuracy   MC algorithm operates on a tree decomposition  and similar to Pearls belief propagation algorithm  Pearl        it converges in two passes  up and down the tree  Our contribution beyond other works in this area  Dechter   Rish        Dechter  Kask    Larrosa        is in      Extending the partition based approximation for belief updating from mini buckets to general treedecompositions  thus allowing the computation of the updated beliefs for all the variables at once        J OIN  G RAPH P ROPAGATION A LGORITHMS  This extension is similar to the one proposed by Dechter et al          but replaces optimization with probabilistic inference      Providing empirical evaluation that demonstrates the effectiveness of the idea of tree decomposition combined with partition based approximation for belief updating  Section   introduces the Iterative Join Graph Propagation  IJGP  algorithm  It operates on a general join graph decomposition that may contain cycles  It also provides a user adjustable i bound parameter that defines the maximum cluster size of the graph  and hence bounds the complexity   therefore it is both anytime and iterative  While the algorithm IBP is typically presented as a generalization of Pearls Belief Propagation algorithm  we show that IBP can be viewed as IJGP with the smallest i bound  We also provide insight into IJGPs behavior in Section    Zero beliefs are variable value pairs that have zero conditional probability given the evidence  We show that      if a value of a variable is assessed as having zero belief in any iteration of IJGP  it remains a zero belief in all subsequent iterations      IJGP converges in a finite number of iterations relative to its set of zero beliefs  and  most importantly     that the set of zero beliefs decided by any of the iterative belief propagation methods is sound  Namely any zero belief determined by IJGP corresponds to a true zero conditional probability relative to the given probability distribution expressed by the Bayesian network  Empirical results on various classes of problems are included in Section    shedding light on the performance of IJGP i   We see that it is often superior  or otherwise comparable  to other state of the art algorithms  The paper is based in part on earlier conference papers by Dechter  Kask  and Mateescu         Mateescu  Dechter  and Kask        and Dechter and Mateescu             Background In this section we provide background for exact and approximate probabilistic inference algorithms that form the basis of our work  While we present our algorithms in the context of directed probabilistic networks  they are applicable to any graphical model  including Markov networks      Preliminaries Notations  A reasoning problem is defined in terms of a set of variables taking values on finite domains and a set of functions defined over these variables  We denote variables or subsets of variables by uppercase letters  e g   X  Y  Z  S  R        and values of variables by lower case letters  e g   x  y  z  s   An assignment  X    x            Xn   xn   can be abbreviated as x    x            xn    For a subset of variables S  DS denotes the Cartesian product of the domains of variables in S  xS is the projection of x    x            xn   over a subset S  We denote functions by letters f   g  h  etc   and the scope  set of arguments  of the function f by scope f    D EFINITION    graphical model   Kask  Dechter  Larrosa    Dechter        A graphical model M is a   tuple  M   hX  D  Fi  where  X    X            Xn   is a finite set of variables  D    D            Dn   is the set of their respective finite domains of values  F    f            fr   is a set of positive real valued discrete functions  each defined over a subset of variables Si  X  called its scope  and denoted by scope f P i    A graphical model typically has an associated combination   operator    e g            product  sum   The graphical model represents the combination    The combination operator can also be defined axiomatically  Shenoy                M ATEESCU   K ASK   G OGATE   D ECHTER  of all its functions  ri   fi   A graphical model has an associated primal graph that captures the structural information of the model  D EFINITION    primal graph  dual graph  The primal graph of a graphical model is an undirected graph that has variables as its vertices and an edge connects any two vertices whose corresponding variables appear in the scope of the same function  A dual graph of a graphical model has a one to one mapping between its vertices and functions of the graphical model  Two vertices in the dual graph are connected if the corresponding functions in the graphical model share a variable  We denote the primal graph by G    X  E   where X is the set of variables and E is the set of edges  D EFINITION    belief networks  A belief  or Bayesian  network is a graphical model B   hX  D  G  P i  where G    X  E  is a directed acyclic graph over variables X and P    pi    where pi    p Xi   pa  Xi       are conditional probability tables  CPTs  associated with each variable Xi and pa Xi     scope pi   Xi   is the set of parents of Xi in G  Given a subset of variables S  we will write P  s  as the probability P  S   s   where s  DS   A belief network represents a probability distribution over X  P  x             xn     ni   P  xi  xpa Xi      An evidence set e is an instantiated subset of variables  The primal graph of a belief network is called a moral graph  It can be obtained by connecting the parents of each vertex in G and removing the directionality of the edges  Equivalently  it connects any two variables appearing in the same family  a variable and its parents in the CPT   Two common queries in Bayesian networks are Belief Updating  BU  and Most Probable Explanation  MPE   D EFINITION    belief network queries  The Belief Updating  BU  task is to find the posterior probability of each single variable given some evidence e  that is to compute P  Xi  e   The Most Probable Explanation  MPE  task is to find a complete assignment to all the variables having maximum probability given the evidence  that is to compute argmaxX i pi       Tree Decomposition Schemes Tree decomposition is at the heart of most general schemes for solving a wide range of automated reasoning problems  such as constraint satisfaction and probabilistic inference  It is the basis for many well known algorithms  such as join tree clustering and bucket elimination  In our presentation we will follow the terminology of Gottlob  Leone  and Scarcello        and Kask et al          D EFINITION    tree decomposition  cluster tree  Let B   hX  D  G  P i be a belief network  A tree decomposition for B is a triple hT    i  where T    V  E  is a tree  and  and  are labeling functions which associate with each vertex v  V two sets   v   X and  v   P satisfying     For each function pi  P   there is exactly one vertex v  V such that pi   v   and scope pi     v      For each variable Xi  X  the set  v  V  Xi   v   induces a connected subtree of T   This is also called the running intersection  or connectedness  property  We will often refer to a node and its functions as a cluster and use the term tree decomposition and cluster tree interchangeably        J OIN  G RAPH P ROPAGATION A LGORITHMS  D EFINITION    treewidth  separator  eliminator  Let D   hT    i be a tree decomposition of a belief network B  The treewidth  Arnborg        of D is maxvV   v       The treewidth of B is the minimum treewidth over all its tree decompositions  Given two adjacent vertices u and v of a tree decomposition  the separator of u and v is defined as sep u  v     u    v   and the eliminator of u with respect to v is elim u  v     u    v   The separator width of D is max u v   sep u  v    The minimum treewidth of a graph G can be shown to be identical to a related parameter called induced width  Dechter   Pearl         Join tree and cluster tree elimination  CTE  In both Bayesian network and constraint satisfaction communities  the most used tree decomposition method is join tree decomposition  Lauritzen   Spiegelhalter        Dechter   Pearl         introduced based on relational database concepts  Maier         Such decompositions can be generated by embedding the networks moral graph G into a chordal graph  often using a triangulation algorithm and using its maximal cliques as nodes in the join tree  The triangulation algorithm assembles a join tree by connecting the maximal cliques in the chordal graph in a tree  Subsequently  every CPT pi is placed in one clique containing its scope  Using the previous terminology  a join tree decomposition of a belief network B   hX  D  G  P i is   a tree T    V  E   where V is the set of cliques of a chordal graph G that contains G  and E is a set of edges that form a tree between cliques  satisfying the running intersection property  Maier         Such a join tree satisfies the properties of tree decomposition and is therefore a cluster tree  Kask et al          In this paper  we will use the terms tree decomposition and join tree decomposition interchangeably  There are a few variants for processing join trees for belief updating  e g   Jensen et al         Shafer   Shenoy         We adopt here the version from Kask et al          called cluster treeelimination  CTE   that is applicable to tree decompositions in general and is geared towards space savings  It is a message passing algorithm  for the task of belief updating  messages are computed by summation over the eliminator between the two clusters of the product of functions in the originating cluster  The algorithm  denoted CTE BU  see Figure     pays a special attention to the processing of observed variables since the presence of evidence is a central component in belief updating  When a cluster sends a message to a neighbor  the algorithm operates on all the functions in the cluster except the message from that particular neighbor  The message contains a single combined function and individual functions that do not share variables with the relevant eliminator  All the non individual functions are combined in a product and summed over the eliminator  Example   Figure  a describes a belief network and Figure  b a join tree decomposition for it  Figure  c shows the trace of running CTE BU with evidence G   ge   where h u v  is a message that cluster u sends to cluster v  T HEOREM    complexity of CTE BU   Dechter et al         Kask et al         Given a Bayesian network B   hX  D  G  P i and a tree decomposition hT    i of B  the time complexity of CTE BU is O deg   n   N    dw      and the space complexity is O N  dsep    where deg is the maximum degree of a node in the tree decomposition  n is the number of variables  N is the number of nodes in the tree decomposition  d is the maximum domain size of a variable  w is the treewidth and sep is the maximum separator size         M ATEESCU   K ASK   G OGATE   D ECHTER  Algorithm CTE for Belief Updating  CTE BU  Input  A tree decomposition hT    i  T    V  E  for B   hX  D  G  P i  Evidence variables var e   Output  An augmented tree whose nodes are clusters containing the original CPTs and the messages received from neighbors  P  Xi   e   Xi  X  Denote by H u v  the message from vertex u to v  nev  u  the neighbors of u in T excluding v  cluster u     u    H v u    v  u   E   clusterv  u    cluster u  excluding message from v to u    Compute messages  For every node u in T   once u has received messages from all nev  u   compute message to node v     Process observed variables  Assign relevant evidence to all pi   u     Compute the combined function  X  h u v     Y  f  elim u v  f A  where A is the set of functions in clusterv  u  whose scope intersects elim u  v   Add h u v  to H u v  and add all the individual functions in clusterv  u   A Send H u v  to node v   Compute P  Xi   e   For every Xi  X let u be a vertex in T such that Xi   u   Compute P  Xi   e    P Q  u  Xi     f cluster u  f    Figure    Algorithm Cluster Tree Elimination for Belief Updating  CTE BU   A              A  B  C           p a    p b   a    p c   a  b       ABC BC  B    C  D            B   C   D   F            p d   b   p  f   c  d      BCDF  E  BF             B  E   F            p  e   b  f                E   F   G           p  g   e  f        F G   a   BEF EF      b   EFG  h            b   c        b   c      a  h           p   a    p  b   a    p  c   a   b   p   d   b    p   f   c   d    h          b   f    d f  h          b   f       h            b   f      e  p  d   b   h            e   f      p   e   b   f    h          b   f    c  d    p   f   c  d    h            b   c    p   e   b   f    h           e   f    b  h           e   f     p   G   g e   e   f     c   Figure     a  A belief network   b  A join tree decomposition   c  Execution of CTE BU      Partition Based Mini Clustering The time  and especially the space complexity  of CTE BU renders the algorithm infeasible for problems with large treewidth  We now introduce Mini Clustering  a partition based anytime algorithm which computes bounds or approximate values on P  Xi   e  for every variable Xi         J OIN  G RAPH P ROPAGATION A LGORITHMS  Procedure MC for Belief Updating  MC BU i      Compute the combined mini functions  Make an  i  size mini cluster partitioning of clusterv  u    mc             mc p    P Q h  u v    elim u v  f mc    f Q hi u v    maxelim u v  f mc i  f i              p add  hi u v   i              p  to H u v    Send H u v  to v  Compute upper bounds P  Xi   e  on P  Xi   e   For every Xi  X let u  V be a cluster such that Xi   u   Make  i  mini clusters from cluster u    mc             mc p    Compute P  Xi   e    P Q Qp Q    u Xi f mc    f      k   max u Xi f mc k  f     Figure    Procedure Mini Clustering for Belief Updating  MC BU       Mini Clustering Algorithm Combining all the functions of a cluster into a product has a complexity exponential in its number of variables  which is upper bounded by the induced width  Similar to the mini bucket scheme  Dechter         rather than performing this expensive exact computation  we partition the cluster into p mini clusters mc             mc p   each having at most Pi variables  Q where i is an accuracy parameter  Instead of computing by CTE BU h u v    elim u v  f  u  f   we can divide the functions of  u  mc k   k              p   and rewrite h u v    P into p mini clusters Qp Q P Q f   mc k  f   By migrating the summation operator into elim u v  elim u v  f  u  Q P k   fQ p each mini cluster  yielding k   elim u v  f mc k  f   we get an upper bound on h u v    The resulting algorithm is called MC BU i   Consequently  the combined functions are approximated via mini clusters  as follows  Suppose u  V has received messages from all its neighbors other than v  the message from v is ignored even if received   The functions in clusterv  u  that are to be combined are partitioned into mini clusters  mc             mc p    each one containing at most i variables  Each mini cluster is processed by summation over the eliminator  and the resulting combined functions as well as all the individual functions are sent to v  It was shown by Dechter and Rish        that the upper bound can be improved by using the maximization operator max rather than the summation operator sum on some mini buckets  Similarly  lower bounds can be generated by replacing sum with min  minimization  for some mini buckets  Alternatively  we can replace sum by a mean operator  taking the sum and dividing by the number of elements in the sum   in this case deriving an approximation of the joint belief instead of a strict upper bound  Algorithm MC BU for upper bounds can be obtained from CTE BU by replacing step   of the main loop and the final part of computing the upper bounds on the joint belief by the procedure given in Figure    In the implementation we used for the experiments reported here  the partitioning was done in a greedy brute force manner  We ordered the functions according to their sizes  number of variables   breaking ties arbitrarily  The largest function was placed in a mini cluster by itself  Then  we picked the largest remaining function and probed the mini clusters in the order of their creation        M ATEESCU   K ASK   G OGATE   D ECHTER     ABC  BC  H         h          b  c      p  a    p  b   a    p c   a  b  a            h  H          b      p  d   b   h          b  f   d  f  h        c     max p   f   c  d   d  f     BCDF             h  H          BF     BEF  EF  c  d  h         f      max p  f   c  d   c d               b  f       p e   b  f    h        e  f    H           h  H           h          e  f       p  e   b  f    h        b   h         f    H               b      p  d   b   h          b  c   e  b              h   e  f      p G   g e   e  f    EFG  Figure    Execution of MC BU for i      trying to find one that together with the new function would have no more than i variables  A new mini cluster was created whenever the existing ones could not accommodate the new function  Example   Figure   shows the trace of running MC BU    on the problem in Figure    First  evidence G   ge is assigned in all CPTs  There are no individual functions to be sent from cluster   to cluster    Cluster   contains only   variables         A  B  C   therefore it is not partitioned  P p a   p b a   p c a  b  is computed and the message The combined function h        b  c    a   H         h       b  c   is sent to node    Now  node   can send its message to node    Again  there are no individual functions  Cluster   contains   variables         B  C  D  F    and a partitioning is necessary  MC BU    can choose P mc       p d b   h       b  c   and mc       p f  c  d      The combined functions h       b    c d p d b   h       b  c  and h       f     maxc d p f  c  d  are computed and the message H         h        b   h        f    is sent to node    The algorithm continues until every node has received messages from all its neighbors  An upper bound on p a  G   ge   can now be computed by choosing cluster    which contains variable A  It doesnt need partitionP ing  so the algorithm just computes b c p a   p b a   p c a  b   h        b   h        c   Notice that unlike CTE BU which processes   variables in cluster    MC BU    never processes more than   variables at a time  It was already shown that  T HEOREM    Dechter   Rish        Given a Bayesian network B   hX  D  G  P i and the evidence e  the algorithm MC BU i  computes an upper bound on the joint probability P  Xi   e  of each variable Xi  and each of its values  and the evidence e  T HEOREM    complexity of MC BU i    Dechter et al         Given a Bayesian network B   hX  D  G  P i and a tree decomposition hT    i of B  the time and space complexity of MC BU i  is O n  hw  di    where n is the number of variables  d is the maximum domain size of a variable and hw   maxuT   f  P  scope f     u         which bounds the number of mini clusters        J OIN  G RAPH P ROPAGATION A LGORITHMS                                                                                                                                                                                                                                                                                    Figure    Node duplication semantics of MC   a  trace of MC BU      b  trace of CTE BU  Semantics of Mini Clustering The mini bucket scheme was shown to have the semantics of relaxation via node duplication  Kask   Dechter        Choi  Chavira    Darwiche         We extend it to mini clustering by showing how it can apply as is to messages that flow in one direction  inward  from leaves to root   as follows  Given a tree decomposition D  where CTE BU computes a function h u v   the message that cluster u sends to cluster v   MC BU i  partitions cluster u into p mini clusters u            up   which are processed independently and then the resulting functions h ui  v  are sent to v  Instead consider a different decomposition D    which is just like D  with the exception that  a  instead of u  it has clusters u            up   all of which are children of v  and each variable appearing in more than a single mini cluster becomes a new variable   b  each child w of u  in D  is a child of uk  in D     such that h w u   in D  is assigned to uk  in D    during the partitioning  Note that D  is not a legal tree decomposition relative to the original variables since it violates the connectedness property  the mini clusters u            up contain variables elim u  v  but the path between the nodes u            up  this path goes through v  does not  However  it is a legal tree decomposition relative to the new variables  It is straightforward to see that H u v  computed by MC BU i  on D is the same as  h ui  v   i              p  computed by CTE BU on D  in the direction from leaves to root  If we want to capture the semantics of the outward messages from root to leaves  we need to generate a different relaxed decomposition  D     because MC  as defined  allows a different partitioning in the up and down streams of the same cluster  We could of course stick with the decomposition in D  and use CTE in both directions which would lead to another variant of mini clustering  Example   Figure   a  shows a trace of the bottom up phase of MC BU    on the network in Figure    Figure   b  shows a trace of the bottom up phase of CTE BU algorithm on a problem obtained from the problem in Figure   by splitting nodes D  into D  and D     and F  into F   and F       The MC BU algorithm computes an upper bound P  Xi   e  on the joint probability P  Xi   e   However  deriving a bound on the conditional probability P  Xi  e  is not easy when the exact       M ATEESCU   K ASK   G OGATE   D ECHTER  Random Bayesian N    K   P   C                    Avg abs error                       ev    ev     ev     ev                                                                               Number of iterations  Figure    Convergence of IBP     variables  evidence from      variables   value of P  e  is not available  If we just try to divide  multiply  P  Xi   e  by a constant  the result is not P necessarily an upper bound on P  Xi  e   It is easy to show that normalization  P  xi   e   xi Di P  xi   e   with the mean operator is identical to normalization of MC BU output when applying the summation operator in all the mini clusters  MC BU i  is an improvement over the Mini Bucket algorithm MB i   in that it allows the computation of P  Xi   e  for all variables with a single run  whereas MB i  computes P  Xi   e  for just one variable  with a single run  When computing P  Xi   e  for each variable  MB i  has to be run n times  once for each variable  an algorithm we call nMB i   It was demonstrated by Mateescu et al         that MC BU i  has up to linear speed up over nMB i   For a given i  the accuracy of MC BU i  can be shown to be not worse than that of nMB i       Experimental Evaluation of Mini Clustering The work of Mateescu et al         and Kask        provides an empirical evaluation of MC BU that reveals the impact of the accuracy parameter on its quality of approximation and compares with Iterative Belief Propagation and a Gibbs sampling scheme  We will include here only a subset of these experiments which will provide the essence of our results  Additional empirical evaluation of MC BU will be given when comparing against IJGP later in this paper  We tested the performance of MC BU i  on random Noisy OR networks  random coding networks  general random networks  grid networks  and three benchmark CPCS files with         and     variables respectively  these are belief networks for medicine  derived from the Computer based Patient Case Simulation system  known to be hard for belief updating   On each type of network we ran Iterative Belief Propagation  IBP    set to run at most    iterations  Gibbs Sampling  GS  and MC BU i   with i from   to the treewidth w to capture the anytime behavior of MC BU i   The random networks were generated using parameters  N K C P   where N is the number of variables  K is their domain size  we used only K     C is the number of conditional probability tables and P is the number of parents in each conditional probability table  The parents in each table are picked randomly given a topological ordering  and the conditional probability tables are filled       J OIN  G RAPH P ROPAGATION A LGORITHMS     e         NHD max  IBP  MC BU     MC BU     MC BU                        mean                          N     P       instances Abs  Error max     E       E       E       E       E       E       E       E       E     mean    E       E       E       E       E       E       E       E       E       E       E       E     Rel  Error  max     E       E       E       E       E       E       E       E       E     mean    E       E       E       E       E       E       E       E       E       E       E       E     Time max                                                         mean                                                                          Table    Performance on Noisy OR networks  w       Normalized Hamming Distance  absolute error  relative error and time   randomly  The grid networks have the structure of a square  with edges directed to form a diagonal flow  all parallel edges have the same direction   They were generated by specifying N  a square integer  and K  we used K     We also varied the number of evidence nodes  denoted by  e  in the tables  The parameter values are reported in each table  For all the problems  Gibbs sampling performed consistently poorly so we only include part of its results here  In our experiments we focused on the approximation power of MC BU i   We compared two versions of the algorithm  In the first version  for every cluster  we used the max operator in all its mini clusters  except for one of them that was processed by summation  In the second version  we used the operator mean in all the mini clusters  We investigated this second version of the algorithm for two reasons      we compare MC BU i  with IBP and Gibbs sampling  both of which are also approximation algorithms  so it would not be possible to compare with a bounding scheme      we observed in our experiments that  although the bounds improve as the i bound increases  the quality of bounds computed by MC BU i  was still poor  with upper bounds being greater than   in many cases   Notice that we need to maintain the sum operator for at least one of the mini clusters  The mean operator simply performs summation and divides by the number of elements in the sum  For example  if A  B  C are binary variables  taking values   and     and f  A  B  C  is the aggregated function of one mini cluster  and elim    A  B   then computing the message h C  by the mean P operator gives      A B      f  A  B  C   We computed the exact solution and used three different measures of accuracy     Normalized Hamming Distance  NHD    we picked the most likely value for each variable for the approximate and for the exact  took the ratio between the number of disagreements and the total number of variables  and averaged over the number of problems that we ran for each class     Absolute Error  Abs  Error    is the absolute value of the difference between the approximate and the exact  averaged over all values  for each variable   all variables and all problems     Relative Error  Rel  Error    is the absolute value of the difference between the approximate and the exact  divided by the exact  averaged over all values  for each variable   all variables and all problems  For coding networks     Wexler and Meek        compared the upper lower bounding properties of the mini bucket on computing probability of evidence  Rollon and Dechter        further investigated heuristic schemes for mini bucket partitioning         M ATEESCU   K ASK   G OGATE   D ECHTER      e         N     P       instances Abs  Error  NHD max  mean                                      IBP                                MC BU     MC BU     MC BU     MC BU      MC BU      max     E       E       E       E       E       E       E       E       E       E       E       E       E         mean    E       E       E       E       E       E       E       E       E       E       E       E       E       E       E       E         Rel  Error max  Time  mean    E       E       E       E       E       E       E       E       E       E       E       E       E       E       E       E            E       E       E       E       E       E       E       E       E       E       E       E       E         max  mean                                                                                                                                                                                                         Table    Performance on Noisy OR networks  w       Normalized Hamming Distance  absolute error  relative error and time  Noisy OR networks  N     P    evid     w         instances  Noisy OR networks  N     P    evid     w         instances   e     e    MC IBP Gibbs Sampling  MC IBP Gibbs Sampling   e    Absolute error  Absolute error   e     e     e     e     e     e     e     e     e                                                                 i bound  i bound  Figure    Absolute error for Noisy OR networks  we report only one measure  Bit Error Rate  BER   In terms of the measures defined above  BER is the normalized Hamming distance between the approximate  computed by an algorithm  and the actual input  which in the case of coding networks may be different from the solution given by exact algorithms   so we denote them differently to make this semantic distinction  We also report the time taken by each algorithm  For reported metrics  time  error  etc   provided in the Tables  we give both mean and max values  In Figure   we show that IBP converges after about   iterations  So  while in our experiments we report its time for    iterations  its time is even better when sophisticated termination is used  These results are typical of all runs         J OIN  G RAPH P ROPAGATION A LGORITHMS  Random networks  N     P    k    evid    w         instances  Random networks  N     P    k    evid     w         instances              MC Gibbs Sampling IBP              MC Gibbs Sampling IBP              Absolute error  Absolute error                                                                                                 i bound            i bound  Figure    Absolute error for random networks  BER         max mean  IBP GS MC BU    MC BU    MC BU    MC BU                                                                               IBP GS MC BU    MC BU    MC BU    MC BU    MC BU                                                                                                                 max mean max mean max mean N      P       instances  w                                                                                                                                                                                                                            N      P       instances  w                                                                                                                                                                                                                                                                         max mean  Time                                                                                                                                                                                                                                       Table    Bit Error Rate  BER  for coding networks   Random Noisy OR networks results are summarized in Tables   and    and Figure    For NHD  both IBP and MC BU gave perfect results  For the other measures  we noticed that IBP is more accurate when there is no evidence by about an order of magnitude  However  as evidence is added  IBPs accuracy decreases  while MC BUs increases and they give similar results  We see that MC BU gets better as the accuracy parameter i increases  which shows its anytime behavior  General random networks results are summarized in Figure    They are similar to those for random Noisy OR networks  Again  IBP has the best result only when the number of evidence variables is small  It is remarkable how quickly MC BU surpasses the performance of IBP as evidence is added  for more  see the results of Mateescu et al          Random coding networks results are given in Table   and Figure    The instances fall within the class of linear block codes    is the channel noise level   It is known that IBP is very accurate for this class  Indeed  these are the only problems that we experimented with where IBP outperformed MC BU throughout  The anytime behavior of MC BU can again be seen in the variation of numbers in each column and more vividly in Figure          M ATEESCU   K ASK   G OGATE   D ECHTER  Coding networks  N      P    sigma      w         instances  Coding networks  N      P    sigma      w         instances              MC IBP         MC IBP               Bit Error Rate  Bit Error Rate                                                                                                            i bound  i bound  Figure    Bit Error Rate  BER  for coding networks  Grid   x    evid     w         instances  Grid   x    evid     w         instances           MC IBP        MC IBP      Time  seconds   Absolute error                                                                                           i bound                         i bound  Figure     Grid   x    absolute error and time  Grid networks results are given in Figure     We notice that IBP is more accurate for no evidence and MC BU is better as more evidence is added  The same behavior was consistently manifested for smaller grid networks that we experimented with  from  x  up to   x     CPCS networks results We also tested on three CPCS benchmark files  The results are given in Figure     It is interesting to notice that the MC BU scheme scales up to fairly large networks  like the real life example of CPCS     induced width      IBP is again more accurate when there is no evidence  but is surpassed by MC BU when evidence is added  However  whereas MC BU is competitive with IBP time wise when i bound is small  its runtime grows rapidly as i bound increases  For more details on all these benchmarks see the results of Mateescu et al          Summary Our results show that  as expected  IBP is superior to all other approximations for coding networks  However  for random Noisy OR  general random  grid networks and the CPCS networks  in the presence of evidence  the mini clustering scheme is often superior even in its weakest form  The empirical results are particularly encouraging as we use an un optimized scheme that exploits a universal principle applicable to many reasoning tasks         J OIN  G RAPH P ROPAGATION A LGORITHMS  CPCS      evid    w        instance  CPCS      evid     w        instance             MC IBP  MC IBP        Absolute error  Absolute error                                                                                                i bound                         i bound  Figure     Absolute error for CPCS         Join Graph Decomposition and Propagation In this section we introduce algorithm Iterative Join Graph Propagation  IJGP  which  like miniclustering  is designed to benefit from bounded inference  but also exploit iterative message passing as used by IBP  Algorithm IJGP can be viewed as an iterative version of mini clustering  improving the quality of approximation  especially for low i bounds  Given a cluster of the decomposition  mini clustering can potentially create a different partitioning for every message sent to a neighbor  This dynamic partitioning can happen because the incoming message from each neighbor has to be excluded when realizing the partitioning  so a different set of functions are split into mini clusters for every message to a neighbor  We can define a version of mini clustering where for every cluster we create a unique static partition into mini clusters such that every incoming message can be included into one of the mini clusters  This version of MC can be extended into IJGP by introducing some links between mini clusters of the same cluster  and carefully limiting the interaction between the resulting nodes in order to eliminate over counting  Algorithm IJGP works on a general join graph that may contain cycles  The cluster size of the graph is user adjustable via the i bound  providing the anytime nature   and the cycles in the graph allow the iterative application of message passing  In Subsection     we introduce join graphs and discuss their properties  In Subsection     we describe the IJGP algorithm itself      Join Graphs D EFINITION    join graph decomposition  A join graph decomposition for a belief network B   hX  D  G  P i is a triple D   hJG    i  where JG    V  E  is a graph  and  and  are labeling functions which associate with each vertex v  V two sets   v   X and  v   P such that     For each pi  P   there is exactly one vertex v  V such that pi   v   and scope pi     v       connectedness  For each variable Xi  X  the set  v  V  Xi   v   induces a connected subgraph of JG  The connectedness requirement is also called the running intersection property         M ATEESCU   K ASK   G OGATE   D ECHTER                     A  C             A       B              C             B  a            b   Figure     An edge labeled decomposition  We will often refer to a node in V and its CPT functions as a cluster  and use the term joingraph decomposition and cluster graph interchangeably  Clearly  a join tree decomposition or a cluster tree is the special case when the join graph D is a tree  It is clear that one of the problems of message propagation over cyclic join graphs is overcounting  To reduce this problem we devise a scheme  which avoids cyclicity with respect to any single variable  The algorithm works on edge labeled join graphs  D EFINITION    minimal edge labeled join graph decompositions  An edge labeled join graph decomposition for B   hX  D  G  P i is a four tuple D   hJG      i  where JG    V  E  is a graph   and  associate with each vertex v  V the sets  v   X and  v   P and  associates with each edge  v  u   E the set   v  u    X such that     For each function pi  P   there is exactly one vertex v  V such that pi   v   and scope pi     v       edge connectedness  For each edge  u  v     u  v     u    v   such that Xi  X  any two clusters containing Xi can be connected by a path whose every edge label includes Xi   Finally  an edge labeled join graph is minimal if no variable can be deleted from any label while still satisfying the edge connectedness property  D EFINITION    separator  eliminator of edge labeled join graphs  Given two adjacent vertices u and v of JG  the separator of u and v is defined as sep u  v      u  v    and the eliminator of u with respect to v is elim u  v     u     u  v    The separator width is max u v   sep u  v    Edge labeled join graphs can be made label minimal by deleting variables from the labels while maintaining connectedness  if an edge label becomes empty  the edge can be deleted   It is easy to see that  Proposition   A minimal edge labeled join graph does not contain any cycle relative to any single variable  That is  any two clusters containing the same variable are connected by exactly one path labeled with that variable  Notice that every minimal edge labeled join graph is edge minimal  no edge can be deleted   but not vice versa     Note that a node may be associated with an empty set of CPTs         J OIN  G RAPH P ROPAGATION A LGORITHMS  Example   The example in Figure   a shows an edge minimal join graph which contains a cycle relative to variable    with edges labeled with separators  Notice however that if we remove variable   from the label of one edge we will have no cycles  relative to single variables  while the connectedness property is still maintained  The mini clustering approximation presented in the previous section works by relaxing the jointree requirement of exact inference into a collection of join trees having smaller cluster size  It introduces some independencies in the original problem via node duplication and applies exact inference on the relaxed model requiring only two message passings  For the class of IJGP algorithms we take a different route  We choose to relax the tree structure requirement and use join graphs which do not introduce any new independencies  and apply iterative message passing on the resulting cyclic structure  Indeed  it can be shown that any join graph of a belief network is an I map  independency map  Pearl        of the underlying probability distribution relative to node separation  Since we plan to use minimally edge labeled join graphs to address over counting problems  the question is what kind of independencies are captured by such graphs  D EFINITION     edge separation in edge labeled join graphs  Let D   hJG      i  JG    V  E  be an edge labeled decomposition of a Bayesian network B   hX  D  G  P i  Let NW   NY  V be two sets of nodes  and EZ  E be a set of edges in JG  Let W  Y  Z be their corresponding sets of variables  W   vNW  v   Z   eEZ  e    We say that EZ edge separates NW and NY in D if there is no path between NW and NY in the JG graph whose edges in EZ are removed  In this case we also say that W is separated from Y given Z in D  and write hW  Z Y iD   Edgeseparation in a regular join graph is defined relative to its separators  T HEOREM   Any edge labeled join graph decomposition D   hJG      i of a belief network B   hX  D  G  P i is an I map of P relative to edge separation  Namely  any edge separation in D corresponds to conditional independence in P   Proof  Let M G be the moral graph of BN   Since M G is an I map of P   it is enough to prove that JG is an I map of M G  Let NW and NY be disjoint sets of nodes and NZ be a set of edges in JG  and W  Z  Y be their corresponding sets of variables in M G  We will prove  hNW  NZ  NY iJG   hW  Z Y iM G by contradiction  Since the sets W  Z  Y may not be disjoint  we will actually prove that hW  Z Z Y  ZiM G holds  this being equivalent to hW  Z Y iM G   Supposing hW  Z Z Y  ZiM G is false  then there exists a path                    n       n in M G that goes from some variable       W  Z to some variable    n  Y  Z without intersecting variables in Z  Let Nv be the set of all nodes in JG that contain variable v  X  and let us consider the set of nodes  S   ni   Ni  NZ We argue that S forms a connected sub graph in JG  First  the running intersection property ensures that every Ni   i              n  remains connected in JG after removing the nodes in NZ  otherwise  it must be that there was a path between the two disconnected parts in the original JG  which implies that a i is part of Z  which is a contradiction   Second  the fact that  i   i      i         M ATEESCU   K ASK   G OGATE   D ECHTER             n     is an edge in the moral graph M G implies that there is a conditional probability table  CPT  on both i and i     i              n     and perhaps other variables   From property   of the definition of the join graph  it follows that for all i              n    there exists a node in JG that contains both i and i     This proves the existence of a path in the mutilated join graph  JG with NZ pulled out  from a node in NW containing      to the node containing both   and    N  is connected   then from that node to the one containing both   and    N  is connected   and so on until we reach a node in NY containing    n   This shows that hNW  NZ  NY iJG is false  concluding the proof by contradiction    Interestingly however  deleting variables from edge labels or removing edges from edge labeled join graphs whose clusters are fixed will not increase the independencies captured by edge labeled join graphs  That is  Proposition   Any two  edge labeled  join graphs defined on the same set of clusters  sharing  V        express exactly the same set of independencies relative to edge separation  and this set of independencies is identical to the one expressed by node separation in the primal graph of the join graph  Proof  This follows by looking at the primal graph of the join graph  obtained by connecting any two nodes in a cluster by an arc over the original variables as nodes  and observing that any edgeseparation in a join graph corresponds to a node separation in the primal graph and vice versa    Hence  the issue of minimizing computational over counting due to cycles appears to be unrelated to the problem of maximizing independencies via minimal I mapness  Nevertheless  to avoid over counting as much as possible  we still prefer join graphs that minimize cycles relative to each variable  That is  we prefer minimal edge labeled join graphs  Relationship with region graphs There is a strong relationship between our join graphs and the region graphs of Yedidia et al                      Their approach was inspired by advances in statistical physics  when it was realized that computing the partition function is essentially the same combinatorial problem that expresses probabilistic reasoning  As a result  variational methods from physics could have counterparts in reasoning algorithms  It was proved by Yedidia et al               that belief propagation on loopy networks can only converge  when it does so  to stationary points of the Bethe free energy  The Bethe approximation is only the simplest case of the more general Kikuchi        cluster variational method  The idea is to group the variables together in clusters and perform exact computation in each cluster  One key question is then how to aggregate the results  and how to account for the variables that are shared between clusters  Again  the idea that everything should be counted exactly once is very important  This led to the proposal of region graphs  Yedidia et al               and the associated counting numbers for regions  They are given as a possible canonical version of graphs that can support Generalized Belief Propagation  GBP  algorithms  The join graphs accomplish the same thing  The edge labeled join graphs can be described as region graphs where the regions are the clusters and the labels on the edges  The tree ness condition with respect to every variable ensures that there is no over counting  A very similar approach to ours  which is also based on join graphs appeared independently by McEliece and Yildirim         and it is based on an information theoretic perspective         J OIN  G RAPH P ROPAGATION A LGORITHMS  Algorithm Iterative Join Graph Propagation  IJGP  Input An arc labeled join graph decomposition hJG      i  JG    V  E  for B   hX  D  G  P i  Evidence variables var e   Output An augmented graph whose nodes are clusters containing the original CPTs and the messages received from neighbors  Approximations of P  Xi  e   Xi  X  Denote by h u v  the message from vertex u to v  nev  u  the neighbors of u in JG excluding v  cluster u     u    h v u    v  u   E   clusterv  u    cluster u  excluding message from v to u   One iteration of IJGP  For every node u in JG in some topological order d and back  do    Process observed variables  Assign relevant evidence to all pi   u   u      u   var e   u  V    Compute individual functions  Include in H u v  each function in clusterv  u  whose scope does not contain variables in elim u  v   Denote by A the remaining functions  P Q    Compute and send to v the combined function  h u v    elim u v  f A f   Send h u v  and the individual functions H u v  to node v  Endfor  Compute an approximation of P  Xi  e   For every Xi  X let u be P a vertex in JG Q such that Xi   u   Compute P  Xi   e      u  Xi     f cluster u  f    Figure     Algorithm Iterative Join Graph Propagation  IJGP       Algorithm IJGP Applying CTE iteratively to minimal edge labeled join graphs yields our algorithm Iterative JoinGraph Propagation  IJGP  described in Figure     One iteration of the algorithm applies messagepassing in a topological order over the join graph  forward and back  When node u sends a message  or messages  to a neighbor node v it operates on all the CPTs in its cluster and on all the messages sent from its neighbors excluding the ones received from v  First  all individual functions that share no variables with the eliminator are collected and sent to v  All the rest of the functions are combined in a product and summed over the eliminator between u and v  Based on the results by Lauritzen and Spiegelhalter        and Larrosa  Kask  and Dechter        it can be shown that  T HEOREM      If IJGP is applied to a join tree decomposition it reduces to join tree clustering  and therefore it is guaranteed to compute the exact beliefs in one iteration      The time complexity of one iteration of IJGP is O deg   n   N    dw      and its space complexity is O N  d    where deg is the maximum degree of a node in the join graph  n is the number of variables  N is the number of nodes in the graph decomposition  d is the maximum domain size  w is the maximum cluster size and  is the maximum label size  For proof  see the properties of CTE presented by Kask et al                 M ATEESCU   K ASK   G OGATE   D ECHTER     A   B  C  a   A  AB     A A  B  b      ABC    AB  A  A    AB  ABC  c   Figure     a  A belief network  b  A dual join graph with singleton labels  c  A dual join graph which is a join tree   The special case of Iterative Belief Propagation Iterative belief propagation  IBP  is an iterative application of Pearls algorithm that was defined for poly trees  Pearl         to any Bayesian network  We will describe IBP as an instance of join graph propagation over a dual join graph  D EFINITION     dual graphs  dual join graphs  Given a set of functions F    f            fl   over scopes S            Sl   the dual graph of F is a graph DG    V  E  L  that associates a node with each function  namely V   F and an edge connects any two nodes whose functions scope share a variable  E     fi   fj   Si  Sj       L is a set of labels for the edges  each edge being labeled by the shared variables of its nodes  L    lij   Si  Sj   i  j   E   A dual join graph is an edgelabeled edge subgraph of DG that satisfies the connectedness property  A minimal dual join graph is a dual join graph for which none of the edge labels can be further reduced while maintaining the connectedness property  Interestingly  there may be many minimal dual join graphs of the same dual graph  We will define Iterative Belief Propagation on any dual join graph  Each node sends a message over an edge whose scope is identical to the label on that edge  Since Pearls algorithm sends messages whose scopes are singleton variables only  we highlight minimal singleton label dual join graphs  Proposition   Any Bayesian network has a minimal dual join graph where each edge is labeled by a single variable  Proof  Consider a topological ordering of the nodes in the acyclic directed graph of the Bayesian network d   X            Xn   We define the following dual join graph  Every node in the dual graph D  associated with pi is connected to node pj   j   i if Xj  pa Xi    We label the edge between pj and pi by variable Xj   namely lij    Xj    It is easy to see that the resulting edge labeled subgraph of the dual graph satisfies connectedness   Take the original acyclic graph G and add to each node its CPT family  namely all the other parents that precede it in the ordering  Since G already satisfies connectedness so is the minimal graph generated   The resulting labeled graph is a dual graph with singleton labels     Example   Consider the belief network on   variables A  B  C with CPTs   P  C A  B     P  B A  and   P  A   given in Figure   a  Figure   b shows a dual graph with singleton labels on the edges  Figure   c shows a dual graph which is a join tree  on which belief propagation can solve the problem exactly in one iteration  two passes up and down the tree          J OIN  G RAPH P ROPAGATION A LGORITHMS  Algorithm Iterative Belief Propagation  IBP  Input  An edge labeled dual join graph DG    V  E  L  for a Bayesian network B   hX  D  G  P i  Evidence e  Output  An augmented graph whose nodes include the original CPTs and the messages received from neighbors  Approximations of P  Xi  e   Xi  X  Approximations of P  Fi  e   Fi  B  Denote by  hvu the message from u to v  ne u  the neighbors of u in V   nev  u    ne u    v   luv the label of  u  v   E  elim u  v    scope u   scope v    One iteration of IBP For every node u in DJ in a topological order and back  do     Process observed variables Assign evidence variables to the each pi and remove them from the labeled edges     Compute and send to v the function  X Y hvu    pu  hui   elim u v    hu i  inev  u    Endfor  Compute approximations of P  Fi  e   P  Xi  e   For every Xi QX let u be the vertex of family Fi in DJ  P  Fi   e      hu  une i  hui    pu   P i P  Xi   e     scope u  Xi   P  Fi   e    Figure     Algorithm Iterative Belief Propagation  IBP   For completeness  we present algorithm IBP  which is a special case of IJGP  in Figure     It is easy to see that one iteration of IBP is time and space linear in the size of the belief network  It can be shown that when IBP is applied to a minimal singleton labeled dual graph it coincides with Pearls belief propagation applied directly to the acyclic graph representation  Also  when the dual join graph is a tree IBP converges after one iteration  two passes  up and down the tree  to the exact beliefs      Bounded Join Graph Decompositions Since we want to control the complexity of join graph algorithms  we will define it on decompositions having bounded cluster size  If the number of variables in a cluster is bounded by i  the time and space complexity of processing one cluster is exponential in i  Given a join graph decomposition D   hJG      i  the accuracy and complexity of the  iterative  join graph propagation algorithm depends on two different width parameters  defined next  D EFINITION     external and internal widths  Given an edge labeled join graph decomposition D   hJG      i of a network B   hX  D  G  P i  the internal width of D is maxvV   v    while the external width of D is the treewidth of JG as a graph  Using this terminology we can now state our target decomposition more clearly  Given a graph G  and a bounding parameter i we wish to find a join graph decomposition D of G whose internal width is bounded by i and whose external width is minimized  The bound i controls the complexity of join graph processing while the external width provides some measure of its accuracy and speed of convergence  because it measures how close the join graph is to a join tree        M ATEESCU   K ASK   G OGATE   D ECHTER  Algorithm Join Graph Structuring i     Apply procedure schematic mini bucket i      Associate each resulting mini bucket with a node in the join graph  the variables of the nodes are those appearing in the mini bucket  the original functions are those in the minibucket     Keep the edges created by the procedure  called out edges  and label them by the regular separator     Connect the mini bucket clusters belonging to the same bucket in a chain by in edges labeled by the single variable of the bucket   Figure     Algorithm Join Graph Structuring i   Procedure Schematic Mini Bucket i     Order the variables from X  to Xn minimizing  heuristically  induced width  and associate a bucket for each variable     Place each CPT in the bucket of the highest index variable in its scope     For j   n to   do  Partition the functions in bucket Xj   into mini buckets having at most i variables  For each mini bucket mb create a new scope function  message  f where scope f      X X  mb    Xi   and place scope f  in the bucket of its highest variable  Maintain an edge between mb and the mini bucket  created later  of f    Figure     Procedure Schematic Mini Bucket i   We can consider two classes of algorithms  One class is partition based  It starts from a given tree decomposition and then partitions the clusters until the decomposition has clusters bounded by i  An alternative approach is grouping based  It starts from a minimal dual graph based join graph decomposition  where each cluster contains a single CPT  and groups clusters into larger clusters as long as the resulting clusters do not exceed the given bound  In both methods one should attempt to reduce the external width of the generated graph decomposition  Our partition based approach inspired by the mini bucket idea  Dechter   Rish        is as follows  Given a bound i  algorithm Join Graph Structuring i  applies the procedure Schematic MiniBucket i   described in Figure     The procedure only traces the scopes of the functions that would be generated by the full mini bucket procedure  avoiding actual computation  The procedure ends with a collection of mini bucket trees  each rooted in the mini bucket of the first variable  Each of these trees is minimally edge labeled  Then  in edges labeled with only one variable are introduced  and they are added only to obtain the running intersection property between branches of these trees  Proposition   Algorithm Join Graph Structuring i  generates a minimal edge labeled join graph decomposition having bound i  Proof  The construction of the join graph specifies the vertices and edges of the join graph  as well as the variable and function labels of each vertex  We need to demonstrate that    the connectedness property holds  and    that edge labels are minimal         J OIN  G RAPH P ROPAGATION A LGORITHMS  G   GFE   GFE  P G F E  EF  E   EBF    EF   EBF  P E B F   P F C D   F   FCD    BF   BF F  FCD  BF  CD  D   DB    CD   P D B   CDB CB  C   CAB   CB   P C A B   B CAB BA  B   BA    AB   A   A    A    B   P B A   BA A P A   A   b    a   Figure     Join graph decompositions  Connectedness property specifies that for any   vertices u and v  if vertices u and v contain variable X  then there must be a path u  w            wm   v between u and v such that every vertex on this path contains variable X  There are two cases here     u and v correspond to   mini buckets in the same bucket  or    u and v correspond to mini buckets in different buckets  In case   we have   further cases   a  variable X is being eliminated in this bucket  or  b  variable X is not eliminated in this bucket  In case  a  each mini bucket must contain X and all mini buckets of the bucket are connected as a chain  so the connectedness property holds  In case  b  vertexes u and v connect to their  respectively  parents  who in turn connect to their parents  etc  until a bucket in the scheme where variable X is eliminated  All nodes along this chain connect variable X  so the connectedness property holds  Case   resolves like case  b  To show that edge labels are minimal  we need to prove that there are no cycles with respect to edge labels  If there is a cycle with respect to variable X  then it must involve at least one in edge  edge connecting two mini buckets in the same bucket   This means variable X must be the variable being eliminated in the bucket of this in edge  That means variable X is not contained in any of the parents of the mini buckets of this bucket  Therefore  in order for the cycle to exist  another in edge down the bucket tree from this bucket must contain X  However  this is impossible as this would imply that variable X is eliminated twice     Example   Figure   a shows the trace of procedure schematic mini bucket    applied to the problem described in Figure  a  The decomposition in Figure   b is created by the algorithm graph structuring  The only cluster partitioned is that of F into two scopes  FCD  and  BF   connected by an in edge labeled with F  A range of edge labeled join graphs is shown in Figure     On the left side we have a graph with smaller clusters  but more cycles  This is the type of graph IBP works on  On the right side we have a tree decomposition  which has no cycles at the expense of bigger clusters  In between  there could be a number of join graphs where maximum cluster size can be traded for number of cycles  Intuitively  the graphs on the left present less complexity for join graph algorithms because the cluster size is smaller  but they are also likely to be less accurate  The graphs on the right side       M ATEESCU   K ASK   G OGATE   D ECHTER  A  A  A  C  ABC  AB ABDE  BC  BE  C  A  A  ABC  AB  C  BCE  C BC  BC  ABDE  ABCDE  DE  CE  CDE  C DE  CE  CE  CDEF  CDEF  CDEF  CDEF F  FGH  H  FGH  H  FGI  GH  GI  F  H  GHIJ  H  FGH H F  F  F FG  ABCDE  BCE  C DE  BCE  FGI  GI  F F  GH  GHIJ  FGI  GH  GI  GHI GHIJ  FGHI  GHIJ  more accuracy less complexity  Figure     Join graphs  are computationally more complex  because of the larger cluster size  but they are likely to be more accurate      The Inference Power of IJGP The question we address in this subsection is why propagating the messages iteratively should help  Why is IJGP upon convergence superior to IJGP with one iteration and superior to MC  One clue can be provided when considering deterministic constraint networks which can be viewed as extreme probabilistic networks  It is known that constraint propagation algorithms  which are analogous to the messages sent by belief propagation  are guaranteed to converge and are guaranteed to improve with iteration  The propagation scheme of IJGP works similar to constraint propagation relative to the flat network abstraction of the probability distribution  where all non zero entries are normalized to a positive constant   and propagation is guaranteed to be more accurate for that abstraction at least  In the following we will shed some light on the IJGPs behavior by making connections with the well known concept of arc consistency from constraint networks  Dechter         We show that   a  if a variable value pair is assessed as having a zero belief  it remains as zero belief in subsequent iterations   b  that any variable value zero beliefs computed by IJGP are correct   c  in terms of zero non zero beliefs  IJGP converges in finite time  We have also empirically investigated the hypothesis that if a variable value pair is assessed by IBP or IJGP as having a positive but very close to zero belief  then it is very likely to be correct  Although the experimental results shown in this paper do not contradict this hypothesis  some examples in more recent experiments by Dechter  Bidyuk  Mateescu  and Rollon        invalidate it         J OIN  G RAPH P ROPAGATION A LGORITHMS        IJGP  AND  A RC  C ONSISTENCY  For any belief network we can define a constraint network that captures the assignments having strictly positive probability  We will show a correspondence between IJGP applied to the belief network and an arc consistency algorithm applied to the constraint network  Since arc consistency algorithms are well understood  this correspondence not only proves the target claims  but may provide additional insight into the behavior of IJGP  It justifies the iterative application of belief propagation  and it also illuminates its distance from being complete  D EFINITION     constraint satisfaction problem  A Constraint Satisfaction Problem  CSP  is a triple hX  D  Ci  where X    X            Xn   is a set of variables associated with a set of discretevalued domains D    D            Dn   and a set of constraints C    C            Cm    Each constraint Ci is a pair hSi   Ri i where Ri is a relation Ri  DSi defined on a subset of variables Si  X and DSi is a Cartesian product of the domains of variables Si   The relation Ri denotes all compatible tuples of DSi allowed by the constraint  A projection operator  creates a new relation  Sj  Ri      x x  DSj and y  y  DSi  Sj and xy  Ri    where Sj  Si   Constraints can be combined with the join operator    resulting in a new relation  Ri   Rj    x Si  x   Ri and Sj  x   Rj    A solution is an assignment of values to all the variables x    x            xn    xi  Di   such that Ci  C  xSi  Ri   The constraint network represents its set of solutions   i Ci   Given a belief network B  we define a flattening of the Bayesian network into a constraint network called f lat B   where all the zero entries in a probability table are removed from the corresponding relation  The network f lat B  is defined over the same set of variables and has the same set of domain values as B  D EFINITION     flat network  Given a Bayesian network B   hX  D  G  P i  the flat network f lat B  is a constraint network  where the set of variables is X  and for every Xi  X and its CPT P  Xi  pa Xi     B we define a constraint RFi over the family of Xi   Fi    Xi    pa Xi   as follows  for every assignment x    xi   xpa Xi     to Fi    xi   xpa Xi      RFi iff P  xi  xpa Xi          T HEOREM   Given a belief network B   hX  D  G  P i  where X    X            Xn    for any tuple x    x            xn    PB  x       x  sol f lat B    where sol f lat B   is the set of solutions of the flat constraint network  Proof  PB  x       ni   P  xi  xpa Xi          i              n   P  xi  xpa Xi          i              n    xi   xpa Xi      RFi  x  sol f lat B      Constraint propagation is a class of polynomial time algorithms that are at the center of constraint processing techniques  They were investigated extensively in the past three decades and the most well known versions are arc   path   and i consistency  Dechter               D EFINITION     arc consistency   Mackworth        Given a binary constraint network  X  D  C   the network is arc consistent iff for every binary constraint Rij  C  every value v  Di has a value u  Dj s t   v  u   Rij          M ATEESCU   K ASK   G OGATE   D ECHTER  Note that arc consistency is defined for binary networks  namely the relations involve at most two variables  When a binary constraint network is not arc consistent  there are algorithms that can process it and enforce arc consistency  The algorithms remove values from the domains of the variables that violate arc consistency until an arc consistent network is generated  There are several versions of improved performance arc consistency algorithms  however we will consider a non optimal distributed version  which we call distributed arc consistency  D EFINITION     distributed arc consistency algorithm  The algorithm distributed arcconsistency is a message passing algorithm over a constraint network  Each node is a variable  and maintains a current set of viable values Di   Let ne i  be the set of neighbors of Xi in the constraint graph  Every node Xi sends a message to any node Xj  ne i   which consists of the values in Xj s domain that are consistent with the current Di   relative to the constraint Rji that they share  Namely  the message that Xi sends to Xj   denoted by Dij   is  Dij  j  Rji   Di         Di  Di    kne i  Dki         and in addition node i computes   Clearly the algorithm can be synchronized into iterations  where in each iteration every node computes its current domain based on all the messages received so far from its neighbors  Eq      and sends a new message to each neighbor  Eq      Alternatively  Equations   and   can be combined  The message Xi sends to Xj is  Dij  j  Rji   Di  kne i  Dki         Next we will define a join graph decomposition for the flat constraint network so that we can establish a correspondence between the join graph decomposition of a Bayesian network B and the join graph decomposition of its flat network f lat B   Note that for constraint networks  the edge labeling  can be ignored  D EFINITION     join graph decomposition of the flat network  Given a join graph decomposition D   hJG      i of a Bayesian network B  the join graph decomposition Df lat   hJG    f lat i of the flat constraint network f lat B  has the same underlying graph structure JG    V  E  as D  the same variable labeling of the clusters   and the mapping f lat maps each cluster to relations corresponding to CPTs  namely Ri  f lat  v  iff CPT pi   v   The distributed arc consistency algorithm of Definition    can be applied to the join graph decomposition of the flat network  In this case  the nodes that exchange messages are the clusters  namely the elements of the set V of JG   The domain of a cluster of V is the set of tuples of the join of the original relations in the cluster  namely the domain of cluster u is   Rf lat  u  R   The constraints are binary  and involve clusters of V that are neighbors  For two clusters u and v  their corresponding values tu and tv  which are tuples representing full assignments to the variables in the cluster  belong to the relation Ruv  i e    tu   tv    Ru v   if the projections over the separator  or labeling   between u and v are identical  namely   u v   tu     u v   tv         J OIN  G RAPH P ROPAGATION A LGORITHMS  We define below the algorithm relational distributed arc consistency  RDAC   that applies distributed arc consistency to any join graph decomposition of a constraint network  We call it relational to emphasize that the nodes exchanging messages are in fact relations over the original problem variables  rather than simple variables as is the case for arc consistency algorithms  D EFINITION     relational distributed arc consistency algorithm  RDAC over a join graph  Given a join graph decomposition of a constraint network  let Ri and Rj be the relations of two clusters  Ri and Rj are the joins of the respective constraints in each cluster   having the scopes Si and Sj   such that Si  Sj      The message Ri sends to Rj denoted h i j  is defined by  h i j   Si Sj  Ri         where ne i     j Si  Sj      is the set of relations  clusters  that share a variable with Ri   Each cluster updates its current relation according to  Ri  Ri     kne i  h k i          Algorithm RDAC iterates until there is no change  Equations   and   can be combined  just like in Equation    The message that Ri sends to Rj becomes  h i j   Si Sj  Ri     kne i  h k i           To establish the correspondence with IJGP  we define the algorithm IJGP RDAC that applies RDAC in the same order of computation  schedule of processing  as IJGP  D EFINITION     IJGP RDAC algorithm  Given the Bayesian network B   hX  D  G  P i  let Df lat   hJG    f lat   i be any join graph decomposition of the flat network f lat B   The algorithm IJGP RDAC is applied to the decomposition Df lat of f lat B   and can be described as IJGP applied to D  with the following modifications  Q    Instead of   we use    P    Instead of   we use      At end end  we update the domains of variables by  Di  Di  Xi    vne u  h v u        R u  R         where u is the cluster containing Xi   Note that in algorithm IJGP RDAC  we could first merge all constraints in each cluster u into a single constraint Ru   R u  R  From our construction  IJGP RDAC enforces arc consistency over the join graph decomposition of the flat network  When the join graph Df lat is a join tree  IJGP RDAC solves the problem namely it finds all the solutions of the constraint network         M ATEESCU   K ASK   G OGATE   D ECHTER  Proposition   Given the join graph decomposition Df lat   hJG    f lat   i  JG    V  E   of the flat constraint network f lat B   corresponding to a given join graph decomposition D of a Bayesian network B   hX  D  G  P i  the algorithm IJGP RDAC applied to Df lat enforces arcconsistency over the join graph Df lat   Proof  IJGP RDAC applied to the join graph decomposition Df lat   hJG    f lat   i  JG    V  E   is equivalent to applying RDAC of Definition    to a constraint network that has vertices V as its variables and   R u  R u  V   as its relations    Following the properties of convergence of arc consistency  we can show that  Proposition   Algorithm IJGP RDAC converges in O m  r  iterations  where m is the number of edges in the join graph and r is the maximum size of a separator Dsep u v  between two clusters  Proof  This follows from the fact messages  which are relations  between clusters in IJGP RDAC change monotonically  as tuples are only successively removed from relations on separators  Since the size of each relation on a separator is bounded by r and there are m edges  no more than O mr  iterations will be needed    In the following we will establish an equivalence between IJGP and IJGP RDAC in terms of zero probabilities  Proposition   When IJGP and IJGP RDAC are applied in the same order of computation  the messages computed by IJGP are identical to those computed by IJGP RDAC in terms of zero   nonzero probabilities  That is  h u v   x       in IJGP iff x  h u v  in IJGP RDAC  Proof  The proof is by induction  The base case is trivially true since messages h in IJGP are initialized to a uniform distribution and messages h in IJGP RDAC are initialized to complete relations  The induction step  Suppose that hIJGP  u v  is the message sent from u to v by IJGP  We will show IJGP RDAC IJGP RDAC that if hIJGP where h u v  is the message sent by IJGP u v   x        then x  h u v  RDAC from u to v  Assume that the claim holds for all messages received by u from its neighbors  Let f  clusterv  u  in IJGP and Rf be the corresponding relation in IJGP RDAC  and P Q t be an asIJGP signment of values to variables in elim u  v   We have h u v   x        elim u v  f f  x       Q  t  f f  x  t        t  f  f  x  t        t  f  scope Rf    x  t   Rf  t  elim u v    Rf IJGP RDAC IJGP RDAC      x  h u v  scope Rf    x  t    h u v   Next we will show that IJGP computing marginal probability P  Xi   xi       is equivalent to IJGP RDAC removing xi from the domain of variable Xi   Proposition   IJGP computes P  Xi   xi       iff IJGP RDAC decides that xi   Di   Proof  According to Proposition   messages computed by IJGP and IJGP RDAC are identical in terms of zero probabilities  Let f  cluster u  in IJGP and Rf be the corresponding relation in IJGP RDAC  and t be an assignment of values to variables in  u  Xi   We will show that when IJGP computes P  Xi   xi        upon convergence   then IJGP RDAC computes xi   Di   We       J OIN  G RAPH P ROPAGATION A LGORITHMS  Q Q P have P  Xi   xi     f f  xi   t       t  f  f  xi   t       f f  xi        t  X Xi t  Rf   scope Rf    xi   t    Rf  t   xi   t      Rf Rf  xi   t    xi   Di  Xi   Rf Rf  xi   t    xi   Di   Since arc consistency is sound  so is the decision of zero probabilities    Next we will show that P  Xi   xi       computed by IJGP is sound  T HEOREM   Whenever IJGP finds P  Xi   xi        then the probability P  Xi   expressed by the Bayesian network conditioned on the evidence is   as well  Proof  According to Proposition    whenever IJGP finds P  Xi   xi        the value xi is removed from the domain Di by IJGP RDAC  therefore value xi  Di is a no good of the network f lat B   and from Theorem   it follows that PB  Xi   xi          In the following we will show that the time it takes IJGP to find all P  Xi   xi       is bounded  Proposition   IJGP finds all P  Xi   xi       in finite time  that is  there exists a number k  such that no P  Xi   xi       will be found after k iterations  Proof  This follows from the fact that the number of iterations it takes for IJGP to compute P  Xi   xi       is exactly the same number of iterations IJGP RDAC takes to remove xi from the domain Di  Proposition   and Proposition     and the fact the IJGP RDAC runtime is bounded  Proposition       Previous results also imply that IJGP is monotonic with respect to zeros  Proposition    Whenever IJGP finds P  Xi   xi        it stays   during all subsequent iterations  Proof  Since we know that relations in IJGP RDAC are monotonically decreasing as the algorithm progresses  it follows from the equivalence of IJGP RDAC and IJGP  Proposition    that IJGP is monotonic with respect to zeros           A F INITE P RECISION P ROBLEM On finite precision machines there is the danger that an underflow can be interpreted as a zero value  We provide here a warning that an implementation of belief propagation should not allow the creation of zero values by underflow  We show an example in Figure    where IBPs messages converge in the limit  i e   in an infinite number of iterations   but they do not stabilize in any finite number of iterations  If all the nodes Hk are set to value    the belief for any of the Xi variables as a function of iteration is given in the table in Figure     After about     iterations  the finite precision of our computer is not able to represent the value for Bel Xi       and this appears to be zero  yielding the final updated belief              when in fact the true updated belief should be            Notice that             cannot be regarded as a legitimate fixed point for IBP  Namely  if we would initialize IBP with the values              then the algorithm would maintain them  appearing to have a fixed point  but initializing IBP with zero values cannot be expected to be correct  When we        M ATEESCU   K ASK   G OGATE   D ECHTER  X   Prior for Xi  H   H   X   X   Xi  P   Xi                           H  Hk Xi  CPT for Hk  Xj  P   Hk   Xi   Xj                                                        iter  Bel Xi      Bel Xi      Bel Xi                                                                                              e                      e                      True belief           Figure     Example of a finite precision problem  initialize with zeros we forcibly introduce determinism in the model  and IBP will always maintain it afterwards  However  this example does not contradict our theory because  mathematically  Bel Xi      never becomes a true zero  and IBP never reaches a quiescent state  The example shows that a close to zero belief network can be arbitrarily inaccurate  In this case the inaccuracy seems to be due to the initial prior belief which are so different from the posterior ones        ACCURACY OF IBP ACROSS B ELIEF D ISTRIBUTION We present an empirical evaluation of the accuracy of IBPs prediction for the range of belief distribution from   to    These results also extend to IJGP  In the previous section  we proved that zero values inferred by IBP are correct  and we wanted to test the hypothesis that this property extends to   small beliefs  namely  that are very close to zero   That is  if IBP infers a posterior belief close to zero  then it is likely to be correct  The results presented in this paper seem to support the hypothesis  however new experiments by Dechter et al         show that it is not true in general  We do not have yet a good characterization of the cases when the hypothesis is confirmed  To test this hypothesis  we computed the absolute error of IBP per intervals of         For a given interval  a  b   where    a   b     we use measures inspired from information retrieval  Recall Absolute Error and Precision Absolute Error  Recall is the absolute error averaged over all the exact posterior beliefs that fall into the interval  a  b   For Precision  the average is taken over all the approximate posterior belief values computed by IBP to be in the interval  a  b   Intuitively  Recall  a b   indicates how far the belief computed by IBP is from the exact  when the exact is in  a  b   Precision  a b   indicates how far the exact is from IBPs prediction  when the value computed by IBP is in  a  b   Our experiments show that the two measures are strongly correlated  We also show the histograms of distribution of belief for each interval  for the exact and for IBP  which are also strongly correlated  The results are given in Figures    and     The left Y axis corresponds to the histograms  the bars   the right Y axis corresponds to the absolute error  the lines   We present results for two classes of problems  coding networks and grid network  All problems have binary variables  so the graphs are symmetric about     and we only show the interval           The number of variables  number of iterations and induced width w  are reported for each graph         J OIN  G RAPH P ROPAGATION A LGORITHMS  Recall Abs  Error  noise         noise              Absolute Error                                                                                         Precision Abs  Error                                                                                                                                                                                                                                                  IBP Histogram            Percentage  Exact Histogram      noise         Figure     Coding  N           instances  w      Recall Abs  Error  evidence      evidence                                Absolute Error                                                                                                                                                                                                                                                          Precision Abs  Error        IBP Histogram             Percentage  Exact Histogram      evidence       Figure       x   grids      instances  w      Coding networks IBP is famously known to have impressive performance on coding networks  We tested on linear block codes  with    nodes per layer and   parent nodes  Figure    shows the results for three different values of channel noise           and      For noise      all the beliefs computed by IBP are extreme  The Recall and Precision are very small  of the order of        So  in this case  all the beliefs are very small    small  and IBP is able to infer them correctly  resulting in almost perfect accuracy  IBP is indeed perfect in this case for the bit error rate   When the noise is increased  the Recall and Precision tend to get closer to a bell shape  indicating higher error for values close to     and smaller error for extreme values  The histograms also show that less belief values are extreme as the noise is increased  so all these factors account for an overall decrease in accuracy as the channel noise increases  These networks are examples with a large number of   small probabilities and IBP is able to infer them correctly  absolute error is small   Grid networks We present results for grid networks in Figure     Contrary to the case of coding networks  the histograms show higher concentration of beliefs around      However  the accuracy is still very good for beliefs close to zero  The absolute error peaks close to   and maintains a plateau  as evidence is increased  indicating less accuracy for IBP      Experimental Evaluation As we anticipated in the summary of Section    and as can be clearly seen now by the structuring of a bounded join graph  there is a close relationship between the mini clustering algorithm MC i        M ATEESCU   K ASK   G OGATE   D ECHTER  IBP  it           MC   evid                                                                                                       Absolute error IJGP i   i                                                                                                                                                    IBP i                                                                                                                                                                                                                              Relative error IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                              KL distance IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                     Time IJGP i                                                                   i                                                                   i                                                                                                                                   Table    Random networks  N     K    C     P        instances  w       and IJGP i   In particular  one iteration of IJGP i  is similar to MC i   MC sends messages up and down along the clusters that form a set of trees  IJGP has additional connections that allow more interaction between the mini clusters of the same cluster  Since this is a cyclic structure  iterating is facilitated  with its virtues and drawbacks s In our evaluation of IJGP i   we focus on two different aspects   a  the sensitivity of parametric IJGP i  to its i bound and to the number of iterations   b  a comparison of IJGP i  with publicly available state of the art approximation schemes      Effect of i bound and Number of Iterations We tested the performance of IJGP i  on random networks  on M by M grids  on the two benchmark CPCS files with    and     variables  respectively and on coding networks  On each type of networks  we ran IBP  MC i  and IJGP i   while giving IBP and IJGP i  the same number of iterations  We use the partitioning method described in Section     to construct a join graph  To determine the order of message computation  we recursively pick an edge  u v   such that node u has the fewest incoming messages missing  For each network except coding  we compute the exact solution and compare the accuracy using the absolute and relative error  as before  as well as the KL  Kullback Leibler  distance Pexact  X   a   log Pexact  X   a  Papproximation  X   a   averaged over all values  all variables and all problems  For coding networks we report the Bit Error Rate  BER  computed as described in Section      We also report the time taken by each algorithm  The random networks were generated using parameters  N K C P   where N is the number of variables  K is their domain size  C is the number of conditional probability tables  CPTs  and P is the number of parents in each CPT  Parents in each CPT are picked randomly and each CPT is filled randomly  In grid networks  N is a square number and each CPT is filled randomly  In each problem class  we also tested different numbers of evidence variables  As before  the coding networks are from the class of linear block codes  where  is the channel noise level  Note that we are limited to relatively small and sparse problem instances because our evaluation measures are based on comparing against exact figures  Random networks results for networks having N     K    C    and P   are given in Table   and in Figures    and     For IJGP i  and MC i  we report   different values of i bound           For IBP and IJGP i  we report results for   different numbers of iterations            We report results       J OIN  G RAPH P ROPAGATION A LGORITHMS  Random networks  N     K    P    evid    w            IJGP   it IJGP   it IJGP   it IJGP   it IJGP    it IJGP    it IJGP    it MC IBP   it IBP   it IBP   it IBP   it IBP    it         KL distance                                                                    i bound   a  Performance vs  i bound  Random networks  N     K    P    evid    w            IBP IJGP    IJGP             KL distance                                                            Number of iterations   b  Convergence with iterations   Figure     Random networks  KL distance  for   different numbers of evidence            From Table   and Figure   a we see that IJGP i  is always better than IBP  except when i   and number of iterations is     sometimes by an order of magnitude  in terms of absolute error  relative error and KL distance  IBP rarely changes after   iterations  whereas IJGP i s solution can be improved with more iterations  up to         As theory predicted  the accuracy of IJGP i  for one iteration is about the same as that of MC i   But IJGP i  improves as the number of iterations increases  and is eventually better than MC i  by as much as an order of magnitude  although it clearly takes more time  especially when the i bound is large         M ATEESCU   K ASK   G OGATE   D ECHTER  Random networks  N     K    P    evid    w         IJPG   it IJGP   it IJGP   it IJGP   it IJGP    it IJGP    it IJGP    it MC IBP   it IBP    it  Time  seconds                                                                  i bound  Figure     Random networks  Time  IBP  it           MC   evid                                                                                                       Absolute error IJGP i   i                                                                                                                                                    IBP i                                                                                                                                                                                                                              Relative error IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                              KL distance IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                     Time IJGP i                                                                   i                                                                   i                                                                                                                                   Table     x  grid  K        instances  w       Figure   a shows a comparison of all algorithms with different numbers of iterations  using the KL distance  Because the network structure changes with different i bounds  we do not necessarily see monotonic improvement of IJGP with i bound for a given number of iterations  as is the case with MC   Figure   b shows how IJGP converges with more iterations to a smaller KL distance than IBP  As expected  the time taken by IJGP  and MC  varies exponentially with the i bound  see Figure      Grid networks results with networks of N     K        instances are very similar to those of random networks  They are reported in Table   and in Figure     where we can see the impact of having evidence    and   evidence variables  on the algorithms  IJGP at convergence gives the best performance in both cases  while IBPs performance deteriorates with more evidence and is surpassed by MC with i bound   or larger  CPCS networks results with CPCS   and CPCS    are given in Table   and Figure     and are even more pronounced than those of random and grid networks  When evidence is added  IJGP i  is more accurate than MC i   which is more accurate than IBP  as can be seen in Figure   a  Coding networks results are given in Table    We tested on large networks of     variables  with treewidth w      with IJGP and IBP set to run    iterations  this is more than enough to ensure       J OIN  G RAPH P ROPAGATION A LGORITHMS  Grid network  N     K    evid    w           IJGP   it IJGP   it IJGP   it IJGP   it IJGP    it MC IBP   it IBP   it IBP   it IBP   it IBP    it         KL distance                                                                    i bound   a  Performance vs  i bound  Grid network  N     K    evid    w      e   IJGP    iterations  at convergence   e    KL distance   e     e     e     e     e                                         i bound   b  Fine granularity for KL   Figure     Grid  x   KL distance  convergence   IBP is known to be very accurate for this class of problems and it is indeed better than MC  However we notice that IJGP converges to slightly smaller BER than IBP even for small values of the i bound  Both the coding network and CPCS    show the scalability of IJGP for large size problems  Notice that here the anytime behavior of IJGP is not clear  In summary  we see that IJGP is almost always superior to both IBP and MC i  and is sometimes more accurate by several orders of magnitude  One should note that IBP cannot be improved with more time  while MC i  requires a large i bound for many hard and large networks to achieve reasonable accuracy  There is no question that the iterative application of IJGP is instrumental to its success  In fact  IJGP    in isolation appears to be the most cost effective variant         M ATEESCU   K ASK   G OGATE   D ECHTER  IBP  it            MC          MC   evid  Absolute error IJGP i   i    Relative error IJGP i    IBP i    i                                                                                                                                                                             KL distance IJGP i    IBP i    CPCS                                                                                                                                                                                                    Time IBP  i                               e                       e                                                                                      e                                                                                               e                            i                                                                      e                                                                                           i    IJGP i    i                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          e      e                    CPCS                                                                                                                                                                                                                                                                                                                                                                                                                             e                      e      e              e                                           e                                                                                                                                                                                                                                               Table    CPCS      instances  w      CPCS       instances  w             IJGP MC      IJGP MC      IJGP MC      IJGP MC      IJGP MC      IJGP MC                                                                                                     IJGP         MC          Bit Error Rate i bound                                                                                                                                                                                                                                                                                                       Time                                                                                                                                                      IBP                                                                            Table    Coding networks  N      P        instances     iterations  w           Comparing IJGP with Other Algorithms In this section we provide a comparison of IJGP with state of the art publicly available schemes  The comparison is based on a recent evaluation of algorithms performed at the Uncertainty in AI      conference    We will present results on solving the belief updating task  also called the task of computing posterior node marginals   MAR   We first give a brief overview of the schemes that we experimented and compared with     EDBP   Edge Deletion for Belief Propagation    Complete results are available at http   graphmod ics uci edu uai   Evaluation Report         J OIN  G RAPH P ROPAGATION A LGORITHMS  CPCS     evid     w          IJGP   it IJGP    it IJGP    it MC IBP   it IBP    it IBP    it             KL distance                                                                                 i bound   a  Performance vs  i bound  CPCS     evid     w      e   IJGP    iterations  at convergence   e    KL distance   e     e     e     e                                          i bound   b  Fine granularity for KL   Figure     CPCS     KL distance  EDBP  Choi   Darwiche      a      b  is an approximation algorithm for Belief Updating  It solves exactly a simplified version of the original problem  obtained by deleting some of the edges of the problem graph  Edges to be deleted are selected based on two criteria  quality of approximation and complexity of computation  tree width reduction   Information loss from lost dependencies is compensated for by introducing auxiliary network parameters  This method corresponds to Iterative Belief Propagation  IBP  when enough edges are deleted to yield a poly tree  and corresponds to generalized BP otherwise     TLSBP   A truncated Loop series Belief propagation algorithm        M ATEESCU   K ASK   G OGATE   D ECHTER  TLSBP is based on the loop series expansion formula of Chertkov and Chernyak        which specifies a series of terms that need to be added to the solution output by BP so that the exact solution can be recovered  This series is basically a sum over all so called generalized loops in the graph  Unfortunately  because the number of these generalized loops can be prohibitively large  the series is of little value  The idea in TLSBP is to truncate the series by decomposing all generalized loops into simple and smaller loops  thus limiting the number of loops to be summed  In our evaluation  we used an implementation of TLSBP available from the work of Gomez  Mooji  and Kappen         The implementation can handle binary networks only     EPIS   Evidence Pre propagation Importance Sampling EPIS  Yuan   Druzdzel        is an importance sampling algorithm for Belief Updating  It is well known that sampling algorithms perform poorly when presented with unlikely evidence  However  when samples are weighted by an importance function  good approximation can be obtained  This algorithm computes an approximate importance function using loopy belief propagation and   cutoff heuristic  We used an implementation of EPIS available from the authors  The implementation works on Bayesian networks only     IJGP   Iterative Join Graph Propagation In the evaluation  IJGP i  was first run with i    until convergence  then with i    until convergence  etc  until i  treewidth  when i bound treewidth  the join graph becomes a join tree and IJGP becomes exact   As preprocessing  the algorithm performed SAT based variable domain pruning by converting zero probabilities in the problem to a SAT problem and performing singleton consistency enforcement  Because the problem size may reduce substantially  in some cases  this preprocessing step may have a significant impact on the time complexity of IJGP  amortized over the increasing i bound  However  for a given i bound  this step improves the accuracy of IJGP only marginally     SampleSearch SampleSearch  Gogate   Dechter        is a specialized importance sampling scheme for graphical models that contain zero probabilities in their CPTs  On such graphical models  importance sampling suffers from the rejection problem in that it generates a large number of samples which have zero weight  SampleSearch circumvents the rejection problem by sampling from the backtrack free search space in which every assignment  sample  is guaranteed to have non zero weight  The backtrack free search space is constructed on the fly by interleaving sampling with backtracking style search  Namely  when a sample is supposed to be rejected because its weight is zero  the algorithm continues instead with systematic backtracking search  until a non zero weight sample is found  For the evaluation version  the importance distribution of SampleSearch was constructed from the output of IJGP with i bound of    For more information on how the importance distribution is constructed from the output of IJGP  see the work by Gogate         The evaluation was conducted on the following benchmarks  see footnote   for details      UAI   MPE   from UAI        instances  Bayesian networks     instances were used      UAI   PE   from UAI        instances  Bayesian networks     instances were used         J OIN  G RAPH P ROPAGATION A LGORITHMS  IJGP EDBP TLSBP EPIS SampleSearch  WCSPs BN O Grids Linkage Promedas UAI   MPE UAI   PE Relational                                  Table    Scope of our experimental study  Score vs KL distance   Score vs KL distance          Score                                                        KL distance  Figure     Score as a function of KL distance     Relational Bayesian networks   constructed from the Primula tool      instances  binary variables  large networks with large tree width  but with high levels of determinism     instances were used      Linkage networks      instances  tree width        Markov networks    instances were used      Grids   from   x   to   x        instances  treewidth           BN O networks   Two layer Noisy OR Bayesian networks     instances  binary variables  up to    variables  treewidth           WCSPs   Weighted CSPs     instances  Markov networks     instances were used      Promedas   real world medical diagnosis      instances  tree width       Markov networks     instances were used    Table   shows the scope of our experimental study  A indicates that the solver was able to  handle the benchmark type and therefore evaluated on it while a lack of a indicates otherwise  We measure the performance of the algorithms in terms of a KL distance based score  Formally  the score of a solver on a problem instance is equal to   avgkld where avgkld is the average KL distance between the exact marginal  which was computed using the UCLA Ace solver  see Chavira   Darwiche        and the approximate marginal output by the solver  If a solver does not output a solution  we consider its KL distance to be   A score lies between   and    with   indicating that the solver outputs exact solution while   indicating that the solver either does not output a solution or has infinite average KL distance  Figure    shows the score as a function of KL distance        M ATEESCU   K ASK   G OGATE   D ECHTER  In Figures       we report the results of experiments with each of the problem sets  Each solver has a timeout of    minutes on each problem instance  when solving a problem  each solver periodically outputs the best solution found so far  Using this  we can compute  for each solver  at any point in time  the total sum of its scores over all problem instances in a particular set  called SumScore t   On the horizontal axis  we have the time and on the vertical axis  the SumScore t   The higher the curve of a solver is  the better  the higher the score   In summary  we see that IJGP shows the best performance on the first four classes of networks  UAI MPE  UAI PE  Relational and Linkage   it is tied with other algorithms on two classes  Grid and BN O   and is surpassed by EDBP on the last two classes  WCSPs and Promedas   EPIS and SampleSearch  which are importance sampling schemes  are often inferior to IJGP and EDBP  In theory  the accuracy of these importance sampling schemes should improve with time  However  the rate of improvement is often unknown in practice  On the hard benchmarks that we evaluated on  we found that this rate is quite small and therefore the improvement cannot be discerned from the Figures  We discuss the results in detail below  As mentioned earlier  TLSBP works only on binary networks  i e   two variables per function  and therefore it was not evaluated on WCSPs  Linkage  UAI   MPE and UAI   PE benchmarks  The UAI MPE and UAI PE instances were used in the UAI      evaluation of exact solvers  for details see the report by Bilmes   Dechter         Exact marginals are available on    UAI MPE instances and    UAI PE instances  The results for UAI MPE and UAI PE instances are shown in Figures    and    respectively  IJGP is the best performing scheme on both benchmark sets reaching a SumScore very close to the maximum possible value in both cases after about   minutes of CPU time  EDBP and SampleSearch are second best in both cases  Relational network instances are generated by grounding the relational Bayesian networks using the Primula tool  Chavira  Darwiche    Jaeger         Exact marginals are available only on    out of the submitted     instances  From Figure     we observe that IJGPs SumScore steadily increases with time and reaches a value very close to the maximum possible score of    after about    minutes of CPU time  SampleSearch is the second best performing scheme  EDBP  TLSBP and EPIS perform quite poorly on these instances reaching the SumScore of        and    respectively after    minutes of CPU time  The Linkage instances are generated by converting linkage analysis data into a Markov network using the Superlink tool  Fishelson   Geiger         Exact marginals are available only on   out of the    instances  The results are shown in Figure     After about one minute of CPU time  IJGPs SumScore is close to   which remains steady thereafter while EDBP only reaches a SumScore of   in    minutes  SampleSearch is the second best performing scheme while EDBP is third best  The results on Grid networks are shown in Figure     The sink node of the grid is the evidence node  The deterministic ratio p is a parameter specifying the fraction of nodes that are deterministic  that is  whose values are determined given the values of their parents  The evaluation benchmark set consists of    instances having p           and     with exact marginals available on    instances only  EPIS  IJGP  SampleSearch and EDBP are in a close tie on this network  while TLSBP has the lowest performance  While hard to see  EPIS is just slightly the best performing scheme  IJGP is the second best followed by SampleSearch and EDBP  On this instances IJGPs SumScore increases steadily with time  The results on BN O instances appear in Figure     This is again a very close tie  in this case of all five algorithms  IJGP has a minuscule decrease of SumScore with time from       to       Although in general an improvement in accuracy is expected for IJGP with higher i bound  it is not       J OIN  G RAPH P ROPAGATION A LGORITHMS  Approximate Mar Problem Set uai   mpe         Sum Score                                                            Time in minutes SampleSearch  IJGP  EDBP  EPIS  Figure     Results on UAI MPE networks  TLSBP is not plotted because it cannot handle UAIMPE benchmarks   Approximate Mar Problem Set uai   pe      Sum Score                                                           Time in minutes SampleSearch  IJGP  EDBP  EPIS  Figure     Results on UAI PE networks  TLSBP is not plotted because it cannot handle UAI PE benchmarks         M ATEESCU   K ASK   G OGATE   D ECHTER  Approximate Mar Problem Set Relational         Sum Score                                                                      Time in minutes SampleSearch IJGP  EDBP TLSBP  EPIS  Figure     Results on relational networks   Approximate Mar Problem Set Linkage       Sum Score                                               Time in minutes SampleSearch  IJGP  EDBP  Figure     Results on Linkage networks  EPIS and TLSBP are not plotted because they cannot handle Linkage networks         J OIN  G RAPH P ROPAGATION A LGORITHMS  Approximate Mar Problem Set Grids      Sum Score                                                                      Time in minutes SampleSearch IJGP  EDBP TLSBP  EPIS  Figure     Results on Grid networks   Approximate Mar Problem Set bn o        Sum Score                                                Time in minutes SampleSearch IJGP  EDBP TLSBP  EPIS  Figure     Results on BN O networks  All solvers except IJGP quickly converge to the maximum possible score of    and are therefore indistinguishable in the Figure         M ATEESCU   K ASK   G OGATE   D ECHTER  Approximate Mar Problem Set WCSPs           Sum Score                                                         Time in minutes SampleSearch  IJGP  EDBP  Figure     Results on WCSPs networks  EPIS and TLSBP are not plotted because they cannot handle WCSPs   Approximate Mar Problem Set Promedas           Sum Score                                                            Time in minutes SampleSearch  IJGP  EDBP  TLSBP  Figure     Results on Promedas networks  EPIS is not plotted because it cannot handle Promedas benchmarks  which are Markov networks         J OIN  G RAPH P ROPAGATION A LGORITHMS  guaranteed  and this is an example when it does not happen  The other solvers reach the maximum possible SumScore of     or very close to it  after about   minutes of CPU time  The WCSP benchmark set has    instances  However we used only the    instances for which exact marginals are available  Therefore the maximum SumScore that an algorithm can reach is     The results are shown in Figure     EDBP reaches a SumScore of    after almost   minutes of CPU time while IJGP reaches a SumScore of    after about   minutes  The SumScores of both IJGP and EDBP remain unchanged in the interval from   to    minutes  After looking at the raw results  we found that IJGPs score was zero on   instances out of     This was because the singleton consistency component implemented via the SAT solver did not finish in    minutes on these instances  Although the singleton consistency step generally helps to reduce the practical time complexity of IJGP on most instances  it adversely affects it on these WCSP instances  The Promedas instances are Noisy OR binary Bayesian networks  Pearl         These instances are characterized by extreme marginals  Namely  for a given variable  the marginals are of the form           where   is a very small positive constant  Exact marginals are available only on    out of the submitted     instances  On these structured problems  see Figure      we see that EDBP is the best performing scheme reaching a SumScore very close to    after about   minutes of CPU time while TLSBP and IJGP are able to reach a SumScore of about    in    minutes      Related Work There are numerous lines of research devoted to the study of belief propagation algorithms  or message passing schemes in general  Throughout the paper we have mentioned and compared with other related work  especially in the experimental evaluation section  We give here a short summary of the developments in belief propagation and present some related schemes that were not mentioned before  For additional information see also the recent review by Koller         About a decade ago  Iterative Belief Propagation  Pearl        received a lot of interest from the information theory and coding community  It was realized that two of the best error correcting decoding algorithms were actually performing belief propagation in networks with cycles  The LDPC code  low density parity check  introduced long time ago by Gallager         is now considered one of the most powerful and promising schemes that often performs impressively close to Shannons limit  Turbo codes  Berrou  Glavieux    Thitimajshima        are also very efficient in practice and can be understood as an instance of belief propagation  McEliece et al          A considerable progress towards understanding the behavior and performance of BP was made through concepts from statistical physics  Yedidia et al         showed that IBP is strongly related to the Bethe Peierls approximation of variational  Gibbs  free energy in factor graphs  The Bethe approximation is a particular case of the more general Kikuchi        approximation  Generalized Belief Propagation  Yedidia et al         is an application of the Kikuchi approximation that works with clusters of variables  on structures called region graphs  Another algorithm that employs the region based approach is Cluster Variation Method  CVM   Pelizzola         These algorithms focus on selecting a good region graph structure to account for the over counting  and over overcounting  etc   of evidence  We view generalized belief propagation more broadly as any belief propagation over nodes which are clusters of functions  Within this view IJGP  and GBP as defined by Yedidia et al          as well as CVM  are special realizations of generalized belief propagation  Belief Propagation on Partially Ordered Sets  PBP   McEliece   Yildirim        is also a generalized form of Belief Propagation that minimizes the Bethe Kikuchi variational free energy  and        M ATEESCU   K ASK   G OGATE   D ECHTER  that works as a message passing algorithm on data structures called partially ordered sets  which has junction graphs and factor graphs as examples  There is one to one correspondence between fixed points of PBP and stationary points of the free energy  PBP includes as special cases many other variants of belief propagation  As we noted before  IJGP is basically the same as PBP  Expectation Propagation  EP   Minka        is a an iterative approximation algorithm for computing posterior belief in Bayesian networks  It combines assumed density filtering  ADF   an extension of the Kalman filter  used to approximate belief states using expectations  such as mean and variance   with IBP  and iterates until these expectations are consistent throughout the network  TreeEP  Minka   Qi        deals with cyclic problem by reducing the problem graph to a tree subgraph and approximating the remaining edges  The relationship between EP and GBP is discussed by Welling  Minka  and Teh         Survey Propagation  SP   Braunstein et al         solves hard satisfiable  SAT  problems using a message passing algorithm on a factor graph consisting of variable and clause nodes  SP is inspired by an algorithm called Warning Propagation  WP  and by BP  WP can determine if a tree problem is SAT  and if it is then it can provide a solution  BP can compute the number of satisfying assignments for a tree problem  as well as the fraction of the assignments where a variable is true  These two algorithms are used as heuristics to define the SP algorithm  that is shown to be more efficient than either of them on arbitrary networks  SP is still a heuristic algorithm with no guarantee of convergence  SP was inspired by the new concept of cavity method in statistical physics  and can be interpreted as BP where variables can not only take the values true or false  but also the extra dont care value  For a more detailed treatment see the book by Mezard and Montanari             Conclusion In this paper we investigated a family of approximation algorithms for Bayesian networks  that could also be extended to general graphical models  We started with bounded inference algorithms and proposed Mini Clustering  MC  scheme as a generalization of Mini Buckets to arbitrary tree decompositions  Its power lies in being an anytime algorithm governed by a user adjustable i bound parameter  MC can start with small i bound and keep increasing it as long as it is given more time  and its accuracy usually improves with more time  If enough time is given to it  it is guaranteed to become exact  One of its virtues is that it can also produce upper and lower bounds  a route not explored in this paper  Inspired by the success of iterative belief propagation  IBP   we extended MC into an iterative message passing algorithm called Iterative Join Graph Propagation  IJGP   IJGP operates on general join graphs that can contain cycles  but it is sill governed by an i bound parameter  Unlike IBP  IJGP is guaranteed to become exact if given enough time  We also make connections with well understood consistency enforcing algorithms for constraint satisfaction  giving strong support for iterating messages  and giving insight into the performance of IJGP  IBP   We show that      if a value of a variable is assessed as having zero belief in any iteration of IJGP  then it remains a zero belief in all subsequent iterations      IJGP converges in a finite number of iterations relative to its set of zero beliefs  and  most importantly     that the set of zero beliefs decided by any of the iterative belief propagation methods is sound  Namely any zero belief determined by IJGP corresponds to a true zero conditional probability relative to the given probability distribution expressed by the Bayesian network         J OIN  G RAPH P ROPAGATION A LGORITHMS  Our experimental evaluation of IJGP  IBP and MC is provided  and IJGP emerges as one of the most powerful approximate algorithms for belief updating in Bayesian networks   
 A major limitation of exact inference algorithms for probabilistic graphical models is their extensive memory usage  which often puts real world problems out of their reach  In this paper we show how we can extend inference algorithms  particularly Bucket Elimination  a special case of cluster  join  tree decomposition  to utilize disk memory  We provide the underlying ideas and show promising empirical results of exactly solving large problems not solvable before      Introduction  Exact inference algorithms for graphical models broadly fall into two categories     Inference based algorithms  e g  Bucket Elimination  BE   Cluster Tree Elimination  CTE    and    Search based  e g  best  rst  depth  rst branch and bound                Inference based algorithms are time exponential in the induced width and also require space exponential in the induced width  While brute force search algorithms can work in linear space  e g   depth  rst search   more advanced search schemes that use and or search spaces require memory exponential in the induced width as well  Consequently  both classes of algorithms are feasible only when the induced width  or treewidth  is small due to memory limitations  Not surprisingly  various algorithms have been proposed that can work with bounded memory at the expense of additional time                In this paper we aim to push the boundary of memory intensive algorithms further  allowing a more e ective tradeo  or in some cases eliminating the compromise altogether  To do so  we extend the memory available to the algorithm to include external  disk  memory  In comparison to main virtual memory  external mem   ory is seemingly unlimited  So an algorithm that e ectively utilizes external memory should  in principle  be able to tackle problems with very large induced widths  However  the additional space does not come without cost  as access of disk memory is typically orders of magnitude slower than main memory  Nonetheless  it has been demonstrated in the context of  A   heuristic search that algorithms can be designed to mitigate such e ects         yielding powerful schemes that can be applied to previously unsolvable problems  To make the realm of problems solvable by inference algorithms using external memory more concrete  consider a Bayesian network  BN  comprised of binary variables  k      and having induced width  w        The largest table in this model has                entries  Assuming that a double precision  oating point number requires   bytes  this problem requires about  MB of memory and easily  ts into main memory  However  if each variable in the model is ternary  k      rather than binary  the largest table requires about  GB of memory  While   GB is more main memory than most computers have  such a problem would  t comfortably into external disk  which can exceed several Tera bytes in size  In the remainder of this paper  we describe how a speci c inference based algorithm  BE      can be modi ed to use external memory  We demonstrate the performance of this new algorithm  named Bucket Elimination with External Memory  BEEM   for computing the probability of evidence on a class of large networks for which exact computations had not previously been made  and otherwise show that it is faster then some of the best algorithms that trade space for time      Background  In this section we present some necessary preliminaries on graphical models and Bucket Elimination   Definition    Graphical Model   A Graphical model R is a   tuple R   hX  D  F  i  where       X    X         Xn   is a set of variables     D    D         Dn   is the set of their respective  nite domains of values     F    f         fr   is a set of real valued functions de ned over a subset of variables Si  X   The scope of function fi   denoted scope fi    is its set of arguments  Si   Q P    i fi    i fi   i fi     i fi   is a combination operator  The graphical model represents the combination of all its functions  ri   fi   The primal graph of a graphical model associates a node with each variable and connect any two nodes whose variables appear in the same scope  Definition    Induced Width   An ordered graph is a pair  G  d   where G is an undirected graph and d   X           X n  is an ordering of the nodes  X i  means the ith node in the ordering   The width of a node is the number of the node s neighbors that precede it in the ordering  The width of an ordering d is the maximum width over all nodes  The induced width of an ordered graph w   d  is the width obtained when nodes are processed from last to  rst  such that when node X i  is processed  all of its preceding neighbors  X j  for j   i  are connected  The induced width of a graph  w    is the minimal induced width over all possible orderings  Bucket Elimination  BE  is a special case of clus   ter tree elimination in which the tree structure upon which messages are passed is determined by the variable elimination order used      In BE terminology  the nodes of the tree structure are referred to as buckets and each bucket is associated with a variable to be eliminated  Each bucket contains a set of functions  either the original functions  e g  Conditional Probability Tables  CPTs  in a BN   or functions generated by the algorithm  Each bucket is processed by BE in two steps  First  all functions in the bucket are combined  by multiplication in the case of BNs    Then the variable associated with the bucket is eliminated from the combined function  by summation in case of Belief Updating in BNs   The function resulting from the combination and elimination steps is then passed to the parent of the current bucket  Processing occurs in this fashion  from the leaves of the tree to the root  one node  bucket  at a time as illustrated in Figure    It is important to note that the bucket tree induces  a partial order on the variables in which child nodes  variables  are processed prior to their parents   Formal de nitions of BE data structures are given next for completeness  A formal description of the BE algorithm is also presented in Figure     Definition     Bucket Let Bx         Bxn be a set of buckets  one for each variable and let d be ordering of these variables  Each bucket Bxi contains those functions in F whose latest variable in d is Xi   Definition    Bucket Tree Let G d be the induced graph along an ordering d of a reasoning problem whose primal graph is G  The vertices of the buckettree are the n buckets  which are denoted by their respective variables  Each vertex BX points to BY  or  BY is the parent of BX   if Y is the latest neighbor of X that appear before X in G d   The degree of bucket B   denoted degB   is the number of neighbors of bucket B in the bucket tree  Definition    Input Output Functions Given a directed bucket tree T   for any bucket B   the output function of B is the function that B sends to its parent  and the input functions of B are functions that B receives from its children   Figure    Illustration of BE for the query P  a e      given ordering d    A  C  B  E  D   Bucket D is processed  rst  The CPT P  d a  b  is placed in bucket D and a function fD  a  b  is generated and passed to bucket B  Next bucket E is processed fE  b  c  is sent to bucket B  Then bucket B is processed by multiplying P  b a   fD  a  b  and fE  b  c  and eliminating variable B  Processing on the bucket tree continues in this manner until the query can be computed      Bucket Elimination with External Memory  Processing a bucket in a bucket tree involves two operations     Combining functions in the bucket  and    Eliminating the bucket s variable    In many problems  the function resulting from these operations is too large to  t into memory  For example  assume we are computing a function fXp   by eliminating a variable Xp from a function h  Normal BE  as de ned in Figure    cannot operate if the function h does not  t into main memory in its entirety  Since computers have orders of magnitude more disk space than main memory  a straightforward modi cation of BE would be to divide large tables  such as h  into blocks that  t into main memory and store these blocks to disk    In the remainder of this paper  we assume that functions take a tabular form    Algorithm Bucket Elimination for P e  A problem description P    X  D  F    evidence variables var e   and an ordering of the variables d    X           X n     Output  Probability of Evidence p e   Input    Initialize  Partition the functions in F into buckets denoted BX         BXn   where initially BXi contains all input functions whose highest variable is Xi  ignore  instantiated variables    During the algorithm s execution BXi   f    f         fj  Backward  For p  n downto    process BXi    Generate the function Q fXp by     Combination  fp    f BX f p P    Elimination  fXp   Xp fp   Add fXp to the bucket of the largest index variable in the scope of fXp   Q   Return  p e     f BX   f  Figure    The Bucket Elimination Algorithm     The function fXp can then be computed by loading required blocks from hard disk  In an extreme case  one could compute fXp one entry at a time  each time loading the relevant entries of the input function h from disk  while saving entries of fXp as they are computed  The performance of this naive  entry by entry  algorithm would be extremely poor because   While main memory has bandwidth  data transfer rate  of a few GB second  disk memory typically has sequential transfer rate of     MB second  and much worse non sequential transfer rate   Main memory has   seek time  since it allows random access   while disk memory has a seek time of about    ms  As a result  this naive algorithm would spend most of its time waiting for table entries to be loaded and then saving them  i e  in disk I O   rather than performing actual computations  The time spent on disk I O has a linear component that depends on block size and a  xed component given by the seek time  In most applications the  xed component dominates  suggesting that the primary goal in designing a disk based adaptation of any algorithm is to minimize the number of reads writes from hard disk  The challenge of minimizing disk reads writes is further compounded in a multi threaded environment  Since individual entries of a function table are independent  function blocks can be processed in parallel  Thus  while one thread is waiting for data to be loaded from disk  other threads can carry out computation on their assigned blocks  This parallelism o ers the potential for algorithmic speed up  while at the same  time introducing a new scheduling challenge  Speci cally  one now has to schedule disk I O so as to minimize the amount of time that each thread waits for data to be loaded saved  In this paper  we address the two  potentially con icting goals of minimizing reads writes and limiting thread starvation  by decomposing the challenge into two tasks     Function table indexing  and    Blocksize computation  The block size computation task involves dividing the function tables into blocks that are as large as possible  The function table indexing task involves arranging the entries within a table  and block  so as to minimize the number of reads writes to disk  Both of these tasks must be addressed within the constraints imposed by the bucket tree  The following two subsections describe our approach to these two design aspects       Function Table Indexing  When processing a table  all of the table s entries are ordered  assigned an index consistent with that order  and then processed one by one  For example  if f  X    X    is a table of ternary variables  the entries are ordered as            index               index               index               index     etc  The ordering of variables in the scope of a function thus dictates where an entry is located within that function s table  Since we are considering functions  i e  fXp   that are broken into blocks because they do not  t into memory  the ordering of variables can also impact the number of reads writes to disk  In the following section  we illustrate how the ordering of variables within a scope can impact the performance of our algorithm  We then show how some of these ine ciencies are remedied by the scope ordering imposed by the bucket tree structure  Since processing a bucket and generating its output function involves two steps   combination and elimination   we analyze the ordering constraints imposed by these two steps         Ordering Constraints due to Elimination  Assume we are computing a function f  X    X     by eliminating variable Y from the function h Y  X    X     as shown in Figure     Furthermore  assume that we are computing entry    corresponding to value combination           of f   Since f  X       X         P Y h Y  X       X        we need entries           from table h  corresponding to argument combinations                                        respectively   As mentioned in the previous section  data is loaded in predetermined size blocks  each stored as a separate  le  If the blocks of h were only   entries in size    to compute f  X       X       are at indices          Under this ordering  no redundant loading unloading of blocks of h are required  The monotonicity of h relative to f can thus be achieved by the following two ordering constraints  Proposition     ing X    Let f be obtained from h by eliminat      If X is the last variable in the scope of h  and    If the order of the remaining variables in the scope of h agrees with the order of the variables in the scope of f   then h is monotone relative to f   Proof  The  rst condition guarantees that the entries Figure    Function Table Indexing Example the required entries             would reside in di erent blocks and three block load operations would be required  When processing the next entry of f  corresponding to value combination            we then require entries            which also reside in di erent blocks  Worse yet  since only one block per table is kept in memory at a time  the blocks of h containing entries           were in memory when entry   of f was computed  but then unloaded to make room for subsequent blocks  Thus  many unnecessary block loads unloads are performed due to the poor ordering of variables in the scope  Our goal is to minimize the number of times any block is loaded  More generally  computing a function f by eliminating a variable from function h involves maintaining two lists of table entries  First  we enumerate the entries of f and then we identify the entries of h required by each entry of f   A condition su cient to guarantee that every block is loaded no more than once is that the enumeration of entries of h be monotonically organized  This condition was not satis ed in the previous example  since to compute entries   and   of f we needed entries           and           of h  respectively   Definition     monotone computation of h relative to f     If f is obtained from h by eliminating X  f   P x h   we say that h is monotone relative to f i      the computation of a single entry of f requires only consecutive entries of h     the computation of successive entries in f requires successive collections of consecutive entries in h   Clearly the non monotoncity stems from the fact that the order of variables in the scopes of f and h do not agree  By rearranging the scope of h to   X    X    Y    we can make the enumeration of h monotonic wrt f   In particular  the entries of h needed to compute f  X       X       are at indices          and the entries needed  of the input function h needed for the computation of any entry of the output function f are consecutive in the table of h  The second condition ensures the second requirement of monotonicity  It turns out that the constraints for monotonicity can be satis ed over all buckets  simultaneously  In fact  the topological  partial ordering dictated by the bucket tree  from leaves to root  can be shown to guarantee monotonicity   Given a directed bucket tree T   where V are the bucket variables and E are the directed edges  any partial order along the tree and  in particular the order of bucket elimination execution  yields monotonic processing of buckets relative to elimination   Proposition      V  E    Since the variables in the scope of each function correspond to nodes in the bucket tree that have yet to be processed  we can infer the following   The partial order along the buckettree yields a strict ordering within each function scope  Proposition     Algorithm Scope Ordering takes as input a buckettree and orders the scopes of functions in all buckets  according to the topological ordering of the given bucket tree  It works by traversing the bucket tree from the root in a breadth  rst manner and at each bucket ordering the scopes of the input functions wrt the bucket s output function         Ordering Constraints due to Combination  Our discussion thus far has focused on the elimination step  During the combination step  a new set of ordering constraints are needed to ensure monotonicity  To illustrate  consider the following example  Assume we are computing a function f  X    X    X     by combining and eliminating variable Y from functions   h   X    X    Y    h   X    X    Y    h   X    X    Y    Further assume that the domain size of all variables is    From the previous section  we know that Y must be the last variable in the scope of the combined function  Thus  when computing f  X       X       X        we need to access entries h   X       X       Y             corresponding to indices        and    in table h    The next entry of f  X       X       X        requires entries h   X       X       Y             corresponding to indices       and     Finally  the entry f  X       X       X       requires entries h   X       X       Y             corresponding to indices        and     This enumeration of h  is nonmontonic  even though it is consistent with the constraints imposed by elimination  The problem occurs because the scope of f has a  gap  with respect to the scope of h    Speci cally  f has a variable X  between variables X  and X  that is not in the scope of h    In this example  there is no ordering of the variables that will avoid such a  gap   In many situations  such gaps occur due to ordering constraints imposed by the elimination order        Block Computation  As discussed earlier  we assume that all tables of intermediate functions are handled  i e  loaded  computed  saved  as blocks  Clearly  loading saving one table entry at a time is ine cient  Intuitively  then  we should divide function tables into blocks that are as large as possible  However  block size is limited by several factors  First  our algorithm is operating in a shared memory setup and each thread requires memory to operate  In addition  processing a bucket requires enough space in memory for the output function block and the blocks of each input function  Our goal is thus to determine how to divide function tables into blocks that minimize unutilized memory  To simplify this problem we make the following design assumptions     We assume the original functions occupy little space and can be stored in memory at all times       We assume that each thread uses the same amount of memory and that the memory allocated to a thread remains  xed throughout the algorithm s execution     We assume that each bucket s output function table is broken into equally sized blocks  and    The removal of  gaps  is untreated at this point and we are currently exploring a variety of approaches for dealing with this issue    This assumption is for simplicity  The original function could also be broken into blocks and stored on the disk      When computing a block of an output function  a thread requires enough space in memory for the output function block and the necessary blocks from each input P function  Q That is  to compute a block of fXp   Xp f BX f   we assume that p the needed blocks of f  BXp are in memory  The  rst two assumptions imply that the amount of memory available to each thread  denoted by M pt  is M pt    M  O  m  where M is the total memory available  O is the memory occupied by the original functions and m is the number of threads  It is worth noting that the third design restriction does not imply that all block sizes are the same size  Under the above design restrictions  a workable and simple upper bound on block size of all the functions residing in a bucket BX is M pt degBX   i e  allocate the memory equally among the output function of a block and the output functions of its children in the bucket tree  Since each block is used twice   once as an output block and once as input block   and the degree of the buckets operating on a block may  and most likely will  be di erent  we need to coordinate this upper bound between adjacent buckets  To illustrate this issue  consider the function fu sent by bucket u to bucket v in Figure    When function fu is computed  its bucket imposes an upper bound on its block size as M pt    since its degree is    However  when fu is used by parent bucket  v   its block size is bounded by M pt    since bucket v  s degree is    Therefore  setting the block size of fu to M pt   equally among all the bucket s function block would violate the fourth design restriction when bucket v is processed  The limitations on block sizes can be captured more formally by the following set of simultaneous constraints  Given a bucket tree and the root node of the bucket tree  the block size of each bucket must satisfy X M pt    i   bi   bj for each bucket i     jC Xi    where bi is the block size for bucket i  C Xi   is the set of children of bucket i and  i is the unutilized space in the computation of blocks for bucket i  With the block size constraints in place  the problem of minimizing the amount of unutilized memory can be formalized as   b          b n     arg min b       bn    n X i    s t  Eqn     bi       i     i   i        Algorithm Block Size Computation A bucket tree  with buckets BXi      BXn along ordering d  Output  A set of block sizes  denoted bi   for each output function fXi of bucket BXi   Input    Do i     to n  in decreasing order of degree in the  Figure    Block Size Computation for Function fu  bucket tree      Let BX  Bi   let I be the indices of child variables of X in the bucket tree whose block size was not yet determined and J those indices whose block size was already determined   where M pt in Eqn   is a constant and the children of each node are governed by the underlying bucket tree structure  Eqn    is a standard constraint satisfaction problem  In Figure   we provide a greedy algorithm that provides a feasible  though possibly suboptimal  solution to this problem  The algorithm computes block sizes starting with the buckets of highest degree  since they are the most constrained  and continues processing buckets in decreasing order of degree  At each bucket the remaining memory  i e  the memory not already allocated to functions in that bucket  is divided equally between the undetermined functions in that bucket  Proposition    The complexity of the Block Size Computation algorithm is O n  log n    where n is the number of variables   It is worth noting that many di erent block sizes can satisfy the constraints in Eqn   depending on which variable is chosen as the root of the tree  This is why we require the bucket tree structure to be  xed in advance  It is also worth noting that bi   the block size for bucket i  is not a continuous variable  rather it is some multiple of the operating system s cluster size  However  in practice we have found this relaxation to be non problematic       The BEEM Algorithm  We have developed a new algorithm  called BEEM  that incorporates the design ideas and algorithms presented in this section  The basic outline of the BEEM algorithm is given in Figure    There is a   to   mapping between blocks and  les  A  le name is a concatenation of the bucket s variable  that generated the function that the block belongs to  and the block index  wrt the function table   For example  if the function generated by bucket BX is split into   blocks  the    les that contain the data will be named  X      X      X      X      X     When a particular block is needed  the program looks for and loads saves a  le with the appropriate name  A block is an array of double precision  oating point numbers  and entire block  le can be loaded saved with a single command      For each undetermined function fXj for j  I compute its block size as P bj   M pt   kJ  bk  degBX   I   Figure    Algorithm for computing block size A basic step of the algorithm is computing a function table block  First we pick the block to compute  from an eligible bucket  Computation      A block is computed by enumerating all entries as described in Computation    In particular  we determine the indices of the entries in each input table and output table as described in the Function Table Indexing section  Based on these indices  we can then determine which block from each input table is needed to carry out the computation  Our scope ordering heuristic guarantees that  for any function f  X         Xk   Y    where Y is the variable being eliminated  given any assignment of values to X         Xk   all entries of the table corresponding to all di erent assignments to Y reside within the same block  This implies that when a thread computes an entry in the table of a bucket s output function  it needs exactly one block from each of the input tables  see Computation   d     Algorithm BEEM  given in Figure    is correct in terms of computing P  e   It performs the  same amount of work  O nkw    as regular BE  where n is the number of variables  k is variable domain size and w is the induced width  Proposition        Experimental Evaluation  To evaluate the BEEM algorithm  we compared its performance with that of two other algorithms on computing the probability of evidence on probabilistic networks  Bayesian and Markov   In our comparison  we used a set of problems  called linkage pedigree problems  derived from genetics  These problems were used   Algorithm BEEM for P e  A problem description P    X  D  F    variables var e   A variable ordering d    X         Xn   and its corresponding bucket tree  T    X  E   The number of threads m and main memory size M   Input   Evidence  Output   P e    In the following  let  fX be the function computed by bucket BX    X  i  denote block i of fX and s X i   e X i    be the index of the  rst  last  element in block  X  i    nbX denote the number of blocks in fX and ncX denote the number of blocks in fX that are computed   rl X i  be the list of threads using block  X  i   Whenever a thread is removed from rl X i  and rl X i     as a result  block  X  i  will be deleted  Also  whenever a thread is added to rl X i  and rl X i     before  block  X  i  will be loaded  from a disk  le named  X  i    Preprocessing      Order the variables in the scope of each function as guided by the bucket tree     Compute table block sizes nbX using Algorithm in Fig    This determines values s X i    e X i       Initialize  set all ncX      rl X i       Computation  in parallel  on m threads  execute      Select a block to compute  from a bucket that has no non computed children  resulting in a bottom up computation order  Let BY be a bucket in the bucket tree such that ncY   nbY and for j  C Y    ncj   nbj   and i be index of a block in fY not yet computed  Set ncY   ncY      mark  X  i  as being computed     Enumerate all entries of block  X  i   for k   s Y i  to e Y i     a  Let A be the assignment of values to arguments of fY corresponding to k   b  j  C Y    let ij k be the index of the block  in fj   corresponding to assignment A   Y        c  if j  ij k    ij k   meaning a current block held in memory for this child is changing   i  remove this thread from rl j ij k      ii  add this thread to rl j ij k      d  compute entry k of block  Y  i  using the product sum rule and input blocks  j  ij k       save block  Y  i  to a disk  le named  Y  i    Return  P e  computed at root of the bucket tree   Figure    The BEEM algorithm for P e   in the solver competition held at the UAI      conference    Many of these problems were not solved in that competition and  in addition  we also consider a class of problems   type   linkage   with high induced width and large numbers of variables  The two algorithms we used for comparison were     VEC  Variable Elimination and Conditioning       and ACE     Both VEC and ACE participated in the UAI     solver competition  In their class  exact solvers for probabilistic networks  solving the P e  problem  VEC ACE were the two best solvers at the UAI      competition  so a comparison with them seems warranted  A brief description of these algorithms follows in the next section  Our experiments were carried out on a PC with an Intel quad core processor  using   X  TB hard disks in RAID   con guration  a total of  TB of disk space   BEEM was con gured so that m     worker threads  As a space saving measure  we deleted all input functions to a bucket after its output function was computed  All algorithms were given  GB of RAM  main  memory and both VEC and BEEM were given the same variable orderings       VEC  VEC is an algorithm that uses conditioning to produce sub problems with induced widths small enough to be solved by an elimination algorithm    A basic outline of VEC is as follows    As a pre processing step  VEC reduces variable domains by converting all   probabilities to a SAT problem F and checks for each assignment X   a whether  F and X   a  is consistent  Inconsistent assignments are pruned from the domain of X    Repeatedly  remove conditioning variables from the problem until the remaining problem  ts within  GB of main memory   Enumerate all value combinations of the conditioning variables  For each assignment  solve the remaining problem using variable elimination  Combine conditioned subproblem solutions to yield a solution to the entire problem       ACE  ACE is a software package for performing exact inference on Bayesian networks developed in the Au   For a report on the results of the competition  see http   graphmod ics uci edu uai   Evaluation Report    see http   graphmod ics uci edu group Software for more detail on VEC   tomated Reasoning Group at UCLA    ACE operates by compiling a Bayesian network into an Arithmetic Circuit  AC  and then using this AC to execute queries  Compiling into an AC occurs by  rst encoding the Bayesian network into Conjunctive Normal Form  CNF  and then extracting the AC from the factored CNF      Encoding a network in this way e ciently exploits determinism  allowing ACE to answer queries on large networks in the UAI    solver competition       Results  Preliminary results from running the three algorithms on a single class of problems are shown in Tables   and    In these tables N indicates the number of variables  w  is an upper bound on the induced width  determined experimentally using several min  ll orderings  and K is the maximum domain size  The run time is presented in hh mm ss format and      h  indicates the algorithm failed to compute p e  in    hours  while  OOM  indicates the algorithm exceeded the allotted   GB of RAM  Table   contains results on pedigree problems with a few hundred to a thousand variables  Table   contains results from a set of problems with several thousand variables  On both sets of problems  we observe a few interesting phenomena  First  if a problem has w  small enough that the problem  ts into memory  all three algorithms compute p e  very rapidly  In such situations  VEC and ACE may actually outperform BEEM because of the overhead associated with multithreading  However  only BEEM and VEC are capable of solving problems that do not  t into RAM  In such situations  we see that the cost associated with reading and writing to hard disk is far less than the cost of conditioning  Finally  BEEM successfully computed p E  for problems                          and    for which an exact solution is not known      Conclusions  We proposed an extension of the Bucket Elimination algorithm that utilizes external disk space for storing intermediate function tables  Extending the BE algorithm in this manner and also parallelizing computation is a non trivial matter  In this paper we identi ed and addressed a number of key issues  including the decomposition of functions into appropriately sized blocks and processing to minimize access to hard disk  While the performance of our algorithm is not fully optimized  it has shown very promising results on a class of large probabilistic networks  The algorithm demonstrates improved scalability  allowing exact computa   http   reasoning cs ucla edu   tion of p e  on problems not before solved by a generalpurpose algorithm  To better understand its performance  we plan to run BEEM on several additional classes of problems  In addition to further improving the table decomposition and computation schemes  we also plan to extend BEEM for belief updating on variables other than the root and to handle more general tree decompositions  As illustrated in this paper  such modi cations will inevitably impact the way in which tables are decomposed and processed  Acknowledgements  This work was supported in part by the NSF under award number IIS         and by the NIH grant R  HG            
