  This paper addresses the tradeoff s which need to be considered in reasoning using probabilistic network representations  such as Influence Diagrams  IDs   In particular  we examine the tradeoff s entailed in using Tem poral Influence Diagrams  TIDs  which ad equately capture the temporal evolution of a dynamic system without prohibitive data and computational requirements  Three ap proaches for TID construction which make different tradeoff s are examined      tailor ing the network at each time interval to the data available  rather then just copying the original Bayes Network for all time intervals       modeling the evolution of a parsimonious subset of variables  rather than all variables   and     model selection approaches  which seek to minimize some measure of the predic tive accuracy of the model without introduc ing too many parameters  which might cause  overfitting  of the model  Methods of evalu ating the accuracy  efficiency of the tradeoff s are proposed     INTRODUCTION  This paper examines tradeoff s which need to be con sidered for reasoning with Probabilistic Networks such as Influence Diagrams  IDs            For large net works  both data acquisition and network evaluation are expensive processes  and some means of controlling network size is often necessary  In particular  model ing time varying systems with Temporal Influence Di agrams  TIDs  or Temporal Bayes Networks  TBNs  often requires large networks  especially if several time slices are modeled  We examine three methods of lim iting network size  and examine the tradeoff s entailed in each of these methods  Some formal techniques for characterizing such tradeoff s are introduced   This work was supported by NSF grant  IRI          and NLM grant  BLR   ROl LM        Sl   The main network type examined  the TBN  has been used to model a variety of dynamic processes  includ ing applications for planning and control          and medicine  e g       VPnet       and ABDO        In such applications  the static system structure is mod eled using a Bayes Network  BN  or influence diagram  ID   and the temporal evolution of the system is mod eled using a time series process  connecting nodes in the BN over different time intervals using  temporal arcs   In other words  if BN   BN       BNk are a tem poral sequence of Bayesian networks  called a tempo ral BN or TBN   these systems address a method of defining the interconnections among these temporally indexed BNs  The sequence of Bayesian networks  which evolve according to the stochastic dynamic pro cess  together with a corresponding sequence of man agement decisions and values derived from the deci sions defines the temporal influence diagram  In almost all of these approaches  a Markov assump tion is made  due primarily to the entailed well known theoretical properties and relative computational fea sibility  However  this simple form of temporal de pendence is violated by many real world processes  Higher order Markov processes can be embedded in the TBN or TID to capture longer term stochastic pro cesses  but at the expense of adding more temporal arcs  thereby increasing data requirements and com putational demands of network evaluation    Similarly  other temporal processes  such as dynamic linear mod els  DLM        can be embedded into temporal BNs or IDs              Some difficulties which arise in large  complicated do mains   e g  for domains in which large TIDs are con structed                   include    Given that exact network evaluation is NP hard      and the approximation task is also NP hard      limiting the size of networks is often the only way to ensure computational feasibility  Hence  during model construction  one needs to trade off    Modeling time series processes other then first order Markov processes can be computationally infeasible for large systems         Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams      a utility maximizing model for parsimony  and computational feasibility   It is difficult to evaluate time series processes for models which contain many variables  In addi tion  the data collection storage requirements for large models can be prohibitive  Due to certain conditional dependencies among variables  it may make more sense to model the temporal evolution of only the subset of variables which are in fact evolving  and use these processes to drive the changes in the dependent variables   This paper addresses the tradeoff s inherent in con structing TIDs which adequately capture the tempo ral evolution of the system without prohibitive data and computational requirements  Three approaches for TID construction which make different tradeoff s are introduced      knowledge base construction ap proaches  which tailor the network at each time inter val to the data available  rather then just copying the original Bayes Network for all time intervals            domain specific time series approaches  which model the evolution of a parsimonious subset of variables  rather than all variables   and     model selection ap proaches  which seek to minimize some measure of the predictive accuracy of the model without introducing too many parameters  which might cause  overfitting  of the model  The second and third approaches are the main contribution of this paper  the second approach is a new analysis of TIDs  and the third approach is the first application tb probabilistic networks of trad ing predictive accuracy for model parsimony  The tradeoff s made by these parsimonious approaches are quantified using various methods  and illustrated using a medical diagnosis example  In addition  some Bayesian approaches to model selection are also exam ined     TEMPORAL BAYESIAN NETWORKS       Static Model Structure  We characterize a BN or TID model M using the pair         where   refers to the graphical structure of the model and IJ refers to the set of parameters associated with    such as conditional probability distributions assigned to arcs in g  The qualitative structure Q V  A  consists of a directed acyclic graph  DAG  of vertices V and arcs A  where A  V X V  Each vertex corresponds to a discrete random variable t J with finite domain O p  Arcs in the BN represent the dependence relationships among the variables  Arcs into chance nodes represent probabilis tic dependence and are called conditioning arcs  The absence of an arc from node i to j indicates that the associated variable t Ji is conditionally independent of variable t Ji given t Ji  s direct predecessors in the DAG      Q V A   For a static model  i e  a single time slice  the quanti tative parameter set IJ consists of the conditional prob ability distributions necessary to define the joint dis tribution P  rh  t              Jn  The required distributions are given by P t J  for every node t J with no incoming arcs  and by the P t Ji     Ji  for the nodes t Ji  t Ji joined by an arc in the DAG  Note that the structure g unam biguously defines the parameter set   which is neces sary to specify the joint distribution P t      t            t Jn   and the structure g of a BN is implicit in the para metric description       Example  Acute Abdominal Pain Model  Provan and Clarke          have developed an ID model for the diagnosis and treatment of acute abdominal pain  AAP   A common cause of acute abdominal pain is appendicitis  and in many cases a clear diagnosis of appendicitis is difficult  since other diseases such as Non Specific Abdominal Pain  NSAP  can present similar signs and symptoms  findings   Figure    Influence diagram for diagnosis and treat ment of acute abdominal pain  In this model  a BN models the physiology of the system  and decision and value nodes represent the actions taken and corresponding utilities of such ac tions respectively  Figure   presents an example of the type of network created for the diagnosis of AAP for a single time slice  In this figure  chance  deci sion and value nodes are represented diagrammati cally by ovals  rectangles and diamonds respectively  For example  the chance node for Inflammation  In fiamm  is conditionally dependent on the chance nodes for Perforated Appendix  Perf App  and Appendicial Obstruction  A Obs   Some possible diseases studied in the model are Appendicitis  App  and NSAP  In this single time slice ID there is one decision node d and one value node V  The shaded nodes in this di agram represent observable variables X  e g  Absent Bowel Sounds  ABS   Right Lower Quadrant Tender ness  RLQ T   Nausea  N   Vomiting  V   etc       Dynamic Model Structure  A temporal BN  or ID  consists of a sequence of BNs  IDs  indexed by time  e g                 t  such that       Provan  temporal arcs connect gi with gi  with the direction of the arcs going from ito j if i   j   A temporal arc Ar t  connecting networks for time slices t    and t is a subset of the inter network edge set Ain t t   given by  Figure    TID for patient X over   time intervals  with new findings of anorexia  A  and muscular guarding  G  in the second time interval  as shown by shaded n des                                      a   B  Ia E V t         E V t    a      E V t     x V t     Inter lemp lral arc  If we index the BN by time  i e  gt    V t   A t    then the full temporal network over N time slices  which may be intervals or points   is given by gN    VN   AN     where N  U V t    vN  AN  t O     N  U A t   t O  lntratemporal arc  and u  N  U Ar t    t l  Each temporal arc connects a pair of vertices The temporal node set connected over time slices t    and t is given by  VP t     Vi t     u Vj t IAr t       c  Vi t      X  vj t     Example  Temporal Model for Diagnosis  Temporal reasoning for AAP is important due to the difficulty of diagnosis and treatment based on data from just a single time slice  Appendicitis progresses over a course of hours to days  and one might be tempted to wait until the complex of signs and symp toms is highly characteristic of appendicitis before re moving the appendix  However  the inflamed appendix may perforate during the observation period  causing a more generalized infection and raising the risk of death from about   in     cases to about   in          Thus  the tradeoff is between the possibility of an unneces sary operation on someone whose findings are simi lar to early appendicitis and a perforation in someone whose appendicitis is allowed to progress  Given that data over time can greatly simplify the di agnostic process  a TID is used for this domain  As an example  consider a simple situation in which   temporal intervals are modeled for the AAP domain as shown in Figure    Dashed lines indicate the arcs joining nodes from two different time slices       Parametric Specification  The probability distributions to be specified for a TID can be classified into two types      time series process distributions for temporal arcs Ain t t    and     distri butions for the network for each time slice  g   g        For example if Figure   represents graphs for two time  This notation is adapted from        slices  g  and g   then a sample of temporal arc distri butions includes  P V   IV      P RLQ   IRLQ      P ABS   IABD      P App    App      P Perf App    Perf  App      etc  A sample of distribu tions within a single time slice includes  P V     lnflamm      P RLQ    Inflamm      P ABS     P App    App      P Inflamm     Perit      NSAP         TID CONSTRUCTION FROM KNOWLEDGE BASES  TIDs  or TBNs  are typically constructed  e g        by replicating the ID for the initial time slice over the succeeding N    time slices  i e  gi  i               N are all the same   and then joining the networks over suc cessive time slices using a Markov assumption  This approach is relatively inflexible  as it does not allow the network to be altered over time  In addition  if the network s size changes over time  many redundant variables will be present as the stati   network is repli cated for future time slices  since the first network will need to incorporate all potentially relevant structure over future time slices  A recent approach to reduce  static  Bayes network complexity  tailoring networks to data           of fers the potential to improve network evaluation costs for such networks  This approach entails construct ing a knowledge base  KB  for the domain in ques tion  Given a particular set   of observations  this approach does not construct a network corresponding to the entire KB  but instead tailors a model to the observations   from the KB  This approach has been extended to the construction   Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams  of TIDs in       where a first order Markov assumption was made in defining the temporal arcs Ar t   For example  Figure   represents a TID for two time slices  Note that even though the KB for the acute abdominal pain domain covers over    findings     intermediate disease states and   diseases       the network  h for the first time slice is significantly simpler  and covers only   findings    intermediate disease states and   diseases   Evaluating this smaller network can be done much more efficiently  Note also that this approach can model how the findings change over time during the evolution of the underlying disease by altering the Q  s over time  For example  in Figure    the network  h for the second time slice introduces variables not contained in h  representing findings present in time slice   but absent in time slice      For complex domains like the diagnosis of AAP  the re duction in network size afforded by the automatic net work construction approach improves computational efficiency  but not enough to allow modeling complex time series processes like higher order Markov pro cesses  Some other techniques  such as the one dis cussed below  are also necessary     DOMAIN SPECIFIC TIME SERIES MODELS  In this section we propose two new domain specific heuristics for cases in which even tailoring the net work to the observations does not produce an easily evaluated BN or ID  For TIDs  a promising heuristic is to model the temporal evolution of only a subset of variables  Two different models for which variables should evolve are possible   driving  variables or ob servable variables  These are discussed below       Parsimonious Modeling of System Temporal Evolution  Driving Variables  This approach entails a domain dependent identification of the system variables which are actually evolving  driving changes in other system variables  To this effect  we partition the system vari ables tjJ into a set V of dynamic or evolving variables and a set S of variables  which are either constant or whose changes are due to some x E V  For complete ness  we assume a set  Y of variables which are inde pendent of the variables x E V  Under this partition  we have t J   V U S U  Y   Using this partition  an appropriate stochastic process is associated with each x E V  In a TID  this is repre sented by  an appropriate set of temporal arcs for each such stochastic process  This partition should be made to trade off model ac curacy for computational efficiency  In some domains    A network like this is constructed for a particular case in which   findings are presented       there may be techniques to govern which variables may be modeled as static  and which must be dynamic  In other domains  in order to make the appropriate tradeoffs  heuristics must be used  Section   presents some ways to formally evaluate the tradeoffs which are made  Observed Variables  This approach seeks to model the observables  findings  X which are the evidence of the internal evolution of the system  Typically  when one is monitoring the system  there exists data  over time  for these variables  However  if these variables are not the ones that are driving the process under study  then one is estimating the values of the driving variables V from the observables  using the model to relate the two classes of variables   We now present an example of the modeling of acute abdominal pain to demonstrate this temporal arc se lection process       Example  Acute Abdominal Pain Model  The AAP model has three variable types  observ able  intermediate  latent  and disease variables  de noted X  V  W respectively  The current method for modeling AAP over time is to use a TID in which a semi Markov process governs the evolution of all sys tem variables        This entails defining a large num ber of temporal arcs  Figure   shows a simple situa tion in which   temporal intervals are modeled  For just a first order Markov assumption  the large num ber of temporal arcs in Figure   is immediately obvi ous  Model evaluation is consequently very expensive  More importantly  the true temporal processes for this domain are not adequately captured by this first order Markov assumption       Hence  a higher order Markov model is required to capture this more com plex system evolution  However  this should be em bedded without introducing significantly more tempo ral arcs  with their entailed data and computational resource requirements   One solution is to model the evolution of a subset of the variables  Two approaches to developing a model in which only a subset of the variables evolve over time include  Driving Variables  This approach models the un derlying physiology which is driving the evolution of the system  Consider the set of causal relationships  App     A Obs    Inflamm    V  If we assume that once a case of appendicitis is initi ated by an appendicial obstruction  A Obs   and that the obstruction does not change  then the only vari able which changes is the inflammation  Vomiting  V  changes in response to the degree of inflammation    In this semi Markov model  data is available to es timate transition distributions for the findings variables  and transition distributions are estimated based on expert opinion for the remaining variables  Dr  J R  Clarke is the surgeon providing the expert opinion        Provan  A similar analysis can be done to identify the set of variables  D which drive the system evolution  This partitions the variables into static S and dynamic  D variables  tf  t    S U  D t   The set V t  consists of latent variables  as shown in Figure    The findings   Figure    Reduced version TID for patient X over   time intervals with temporal arcs for both findings  observable variables  and latent variables      lnter tomporol    Figure    Reduced version TID for patient X over   time intervals  with just disease and latent variables evolving temporally  lnln tomporolac                           lomporot     lnln tomporot        MODEL SELECTION APPROACHES       which also change over time  are conditionally depen dent on the x E  D t   and are an observable reflection of the internal physiological changes over time  The drawback to this approach is that  D t  consists of latent variables  for many of which detailed tem poral physiological models do not exist  For example  insufficient information about the progressive inflam mation of the appendix is known to create a parame terized model  nor can direct measures of the degree of inflammation be made  except possibly using white blood count  WBC   instead  this process is typically inferred from the findings which accompany it  Observable variables  This approach models the observables  findings  X which are the evidence of the internal evolution of the system  A large body of data exists for these variables       as data collection is sim plest for these variables  An example of such a network is shown in Figure     The drawback to this approach is that the observables X t  may not necessarily predict the underlying dis eases W t  as reliably as the latent variables V t   if the latent variables V t  are assumed to be static   A sec ond drawback is that there are relatively more observ able than finding variables  so this approach is more computationally expensive than using latent variables alone    A more accurate model might include both latent and finding variables as dynamic variables   Specifying Tradeoffs  If the topology and probabilities of the TID are changed  then some measure of how the changes affect the predictive accuracy of the output  or of the  qual ity  of the decision making provided by the network  needs to be computed  In this analysis  we assume that there is some utility function which is used as a measure the network  accuracy  or  decision quality  for different models    The effectiveness of any decision rule is measured us ing a loss function L        This is interpreted as mea suring the loss L        associated with taking action   when the world state is parameterized by    Given a prior probability estimate  r    of the world state    the risk function provides a measure of the expected loss under varying values of the observable variables X  and is given by R         E L      X    The Bayes risk of a decision rule    with respect to a prior distribution     on the entire parameter space e  is defined as r  r       E     R          This averages over the risk functions given all priors that can be assigned to    A decision rule    is preferred to a rule    if r  r        r  r       A Bayes rule is a decision rule which minimizes r  r      and is thus optimal  This paper proposes a variety of techniques for ana We use a loss function  to maintain consistency with much of the decision theory literature  e g        utility and loss  for the purposes of this paper  are duals to each other  Hence  one seeks either to maximize the expected utility  or minimize the expected loss    Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams  lyzing the tradeoffs made during model selection  in cluding risk based as well as purely probabilistic cri teria  Although a utility measure is desired  proba bilistic criteria can be used in a variety of situations  e g            Many probabilistic criteria are simpler to compute  and do not require a prior distribution         For example  in the medical example described earlier  the utility function measures the utility of the treat ment given what disease is actually present  Thus an unnecessary appendectomy will have low utility  and a necessary appendectomy will have relatively high util ity  So if the loss L         associated with decision    under parameter set    is less than that under model     i e  L           L          this means that model    allows you to provide better treatment under the same decision rule    than    For the purposes of   medical treatment one needs to determine if the dif ference is significant  In addition to the loss function  one may want to trade off decreased model utility for increased computational efficiency  The model parameter penalty g    to be in troduced in equation   can be used to provide a mea sure of computational expense based on the number of model parameters  Alternatively  one can incorporate into the loss function a computation penalty function        which measures the computational resources nec essary to evaluate a model with parameters denoted by         the quality of the model for prediction  The selection procedure criteria can be defined using the following equation                        g  Y   where f       is a measure of predictive error  and g  Y  is a penalty for the number of model parame ters  One widely studied approach is to choose some In E f that jointly minimizes the sum of predictive error  and parametric penalty  setting the predictive error measure to be the sum of squared error  SSE        arg min SSE y   l lu II        rer where II    is a pre specified constant  Ill is the num ber of nonzero components o   and SSE     r   J   In the right hand side of equation    the first term de notes the predictive error  and the second term is a penalty function on the number of parameters in the model  Hence this equation can capture a wide vari eties of approaches which trade off predictive accuracy and model size  For example  for known u   the Akaike Information Criterion  AIC      is the special case of In when II      and the BIC approach      is the special case of In when II    ogn  A third approach  called the risk inflation  RI  approach       is defined with respect to a  correct  model parameter set     e g  as determined by an oracle   The risk inflation measure RI  Y   is In    Rl  Y   Statistical Etimation Model Selection Approaches  Consider the case where there are p total observable parameters  predictors   fh          Bp   of which some subset q   p is to be selected to estimate a latent variable y  Possible measures of predictive accuracy based on a parameter estimate   are  Sum of Squared Error  SSE  Log likelihood Predictive Risk          log       E           Corresponding to the p parameters  we introduce a set of p indicator variables given by                      p   where  i i is defined as follows  Is             if B  is to be estimated by   otherwise   Definer as the set of all p tuples  i    can be thought of as an indicator vector denoting which parameters are considered in a model  and r as the set of all possible models over    The model selection procedure then consists of select ing some   E f and then estimating   by   y   Various criteria for this process have been used to compute  Details for computing   r are given in               E l        sup       E J     rl   R      R      r   The selection procedure with smallest risk inflation will be minimax with respect to the ratio function RI          The risk inflation criterion calibrates the risk of a model selection estimator against the risk of an ideal model selection procedure       Bayesian Model Selection Approaches  The Bayesian approach to model selection is based on computing the posterior probabilities of the alternative models  given the observations  Two Bayesian analy ses of the model selection process applied to BNs have been published recently  One method focuses on av eraging over all possible BN models to select a model with improved predictive ability           Since the space of all possible models is potentially enormous  two approximation techniques are proposed      use Markov chain Monte Carlo simulation to directly ap proximate the model selection process       and     select a subset of the set of all models by excluding all models which receive less support from the data then their simpler  in terms of number of parameters  counterparts       Both studies indicate that model averaging improves predictive performance    u  denotes the variance of the random predictive error in the estimation process        Provan  Given a large model space  as denoted by r   the model space pruning heuristics proposed in      can be crucial to the model selection process  given no prior knowledge about alternative models  In contrast  here we present model selection techniques which are useful when a small set of alternative models is being consid ered  i e  the entire model spae is not considered    A second approach examines BN structure purely from the viewpoint of predictive accuracy      This ap proach computes a logarithmic score for alternative models  ignoring the number of parameters in the model  Given a discrete random variable y whose value is to be estimated from a model denoted by the pa rameter set    the scoring rule used is  logP yj    This approach is thus a restriction of equation   to the case where            logP yj    and g l       In addition  this selection process is sequential  in that scores are summed over a set of M cases  if S m    logPm yj   is the score on the mth case  the total score for a particular model is given by  as follows  The AIC criterion selected the  t order Markov model  The BIC and Risk Inflation criteria selected the observable parameter model  The BIC with II     criterion selected the  nd order Markov model by a narrow margin over the canonical model  without a penalty for model size this criterion sug gests that a  nd order model may actually best fit this data  In contrast  imposing a penalty for model size on this BIC test selects a simpler model  indicating that the cost of adding parameters for the  nd order model outweighs the increased predictive accuracy  given the chosen penalty II   Although this analysis is informative  further analysis is clearly necessary  The selection of the observable parameter model over the driving parameter model may be due to the availability of better data for the observable parameters than the latent parameters     Further  this analysis needs to be done for a large nm ber of cases  however  this pilot study has shown the promise of these model evaluation criteria   M  S         logPm YI     m l This approach allows monitoring of the performance of models as new data becomes available  by updating the score S   facilitating model adaptation over time  Several related scoring rules are also analyzed in         EVALUATING TRADEOFFS  We now present results from a simple AAP pilot study which applies these different model selection criteria to a set of models  In the following  we assume we know the true state of the world  as represented by the canonical model      The goal is to compare to     alternative models  i  i            k  where the models differ by the time series process parameters for tem poral arcs Aint   t    The BN model analysed is a network consisting of   copies of the BN portion of the ID presented in Figure    joined together by temporal arcs based on four temporal models        t order Markov        nd  order Markov      driving parameters      observable parameters   The canonical model was assumed to be the   t order Markov model  even though the long term nature of the disease evolution may violate this     order Markov assumption  This choice was made because this model has been studied most carefully to date  Measures for these models were computed using four different criteria       AIC      BIC      Risk Inflation      BIC with II         Due to space limitations  the full details of this pi lot analysis are omitted  A summary of the results is  Data for this AAP domain was briefly discussed in         This last criterion is similar to the Bayesian criterion presented in          RELATED LITERATURE  The methods of analyzing networks presented here are orthogonal to the approach proposed by Goldman and Breese       Goldman and Breese describe methods of integrating model construction and evaluation dur ing the process of automated network construction  The main thrust of the work presented here is exam ining alternative network structures  However  some of the model selection criteria examined here can be used during automated network construction to pro vide scoring rules for whether nodes and or arcs should be added to a partially constructed network  Of the work in temporal probabilistic networks  the most closely associated work is that of Dagum et al           The system proposed in      is primarily in terested in the statistical process underlying temporal Bayesian networks  To this end  the paper focuses on computing inter temporal conditional dependence relations  in other words  if BN   BN       BN c are a temporal sequence of Bayesian networks  Dagum et al  address a method of defining the interconnections among these temporally indexed BNs  In     an addi tive BN approximation model is proposed  Parameter estimation is done using the Kullback Liebler measure  which is a restriction of Equation   to g         Related issues of tradeoffs in belief network construc tion are discussed in      These dynamic network refor mulation techniques can be used to identify the opti mal resources devoted to network evaluation  and may help define the computation resource measures intro duced in Section      These techniques may also be pertinent to facilitating the network construction ap proaches discussed here      Many latent parameters are rough subjective esti mates  Further data collection and analysis is planned to rectify this problem    Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams     CONCLUSIONS       T  Dean and K  Kanazawa  A Model for Reasoning about Persistence and Causation  Computational In telligence                           T  Dean and M  Wellman  Planning and Control  Mor gan Kaufmann             E I  George and D P  Foster  The Risk Inflation Cri terion for Multiple Regression  Technical Report     University of Chicago  Graduate School of Business             R  Goldman and J  Breese  Integrating Model Con struction and Evaluation  In Proc  Con   Uncertainty in Artificial Intelligence             H  Keshavan  guest editor   PAMI special issue on Model Construction from Databases  IEEE Transac  This paper has proposed several approaches for con structing parsimonious TIDs for systems which evolve over time  where the state of the system during any time interval is modeled using a Bayesian network  Possible approaches to modeling the dynamic struc ture of the system have been examined  and the trade offs entailed in adopting particular approaches quan tified using a variety of metrics  As an example  these techniques are applied to the medical management of acute abdominal pain  In addition  this paper has proposed methods for se lecting models with better predictive accuracy  and for trading off predictive accuracy for simpler models  Es pecially for complex domains such as temporal reason ing  limiting network size without compromising pre dictive accuracy too much can play an important role in ensuring computational tractability   tions on Pattern Analysis and Machine Intelligence               Dr  J R  Clarke provided the medical expertise necessary for this research  Selecting a parsimonious subset of parameters for time series modeling was suggested to me by G  Rutledge  This work has also been influenced by discussions with D  Foster and M  Mintz  and by the anonymous referees  Acknowledgements               
  A new probabilistic network construction system  DYNASTY  is proposed for diagnos tic reasoning given variables whose probabil ities change over time  Diagnostic reason ing is formulated as a sequential stochastic process  and is modeled using influence dia grams  Given a set   of observations  DY NASTY creates an influence diagram in or der to devise the best action given    Sensi tivity analyses are conducted to determine if the best network has been created  given the uncertainty in network parameters and topol ogy  DYNASTY uses an equivalence class ap proach to provide decision thresholds for the sensitivity analysis  This equivalence class approach to diagnostic reasoning differenti ates diagnoses only if the required actions are different  A set of network topology updat ing algorithms are proposed for dynamically updating the network when necessary     INTRODUCTION  The development of graphical representations for prob abilistic models  e g  belief networks  Pearl         influence diagrams  Howard and Matheson        Shachter        Shachter         has enabled effi cient probabilistic models to be developed for many tasks  such as diagnostic reasoning  Pearl        Heck erman and Horvitz         natural language analy sis Goldman and Charniak         etc  These represen tations  by specifying the causal relationships among variables in a causal graph  and not all possible rela tionships   facilitate efficient inference  A great deal of the recent research in automated probabilistic reason ing has focused on developing more efficient and more general algorithms for causal probabilistic models  and on methods for incrementally constructing belief net works  However  the application of these techniques and rep   resentations to complex diagnostic tasks  such as med ical diagnosis  have oversimplified such tasks  A com mon simplification made in many current approaches is modeling the diagnostic process as a single stage  static process  This is inadequate  as diagnostic rea soning is a sequential  dynamic process in which feed back is important  Provan and Poole        point out the necessity of considering this complete process  and in particular  the effects of feedback  This paper exteJ Jds existing diagnostic models to incor porate the dynamic and sequential nature of diagnos tic reasoning  It proposes techniques for constructing sequential belief networks  and of dynamically updat ing such networks  Many existing techniques for con structing belief networks  e g   Goldman and Char niak        Heckerman and Horvitz         model the process for one instant of time   For certain tasks this is adequate  but for tasks in which the probabilistic re lationships among variables changes over time  it can be difficult to know when the best model has been con structed  This sometimes produces incorrect answers due to the selection of incorrect probabilities and or causal relationships  Hence  both the diagnosis and the decision taken given this diagnosis may hinge on whether the best model has been constructed  given the data at a particular time t  Sensitivity analyses may be used to test how the data at different times af fects the best decision  If the sensitivity analyses show that a better decision would be made under an alter native model  then the model needs to be updated  It is these sensitivity analyses and model updating tech niques that are of interest here  Criteria are proposed to determine when network topology revisions are nec essary given time varying probabilistic and causal re lationships  These criteria are based on examining the equivalence of outcomes  e g  treatments for dis eases   Algorithms for conducting the necessary revi sions are outlined  including refinement and coarsening techniques  Chang and Fung         and other network  This is true even for systems in which the mode   can be constructed incrementally  e g   Goldman and Char niak                Provan  revision algorithms  Pearl        Srinivas and Breese          This approach makes dynamic network updating pos sible  and formalizes the sequential nature of diagnos tic reasoning  e g  to allow feedback into the network   The explicit introduction of utilities into diagnostic models  allows a more realistic formalization of the diagnostic process  In addition  it is expected that the techniques developed for diagnostic reasoning m ty be applied to other domains  where appropriate     Figure    Change over time of likelihood ratio for the occurrence of Right Lower Quadrant pain given a di agnosis of appendicitis P  symptoms  appendicitis  P  symptoms    appendicttis   DYNAMICS OF DIAGNOSTIC REASONING UNDER UNCERTAINTY  Treating a diagnostic task as being time independent can lead to incorrect results in certain domains  Con sider medical diagnosis  and in particular the diagnosis of abdominal pain  Constructing a model for the ob servation of abdominal pain should not be done for a single time interval  since  as noted in  Schwartz et a           many symptoms take on different meanings as diseases evolve over time  both in terms of their inter relationships and the diseases indicated by the particular symptoms  In a possible case of appendici tis  the initial symptoms include non specific abdom inal pain  which could be confused with many other ailments   and are often accompanied soon thereafter by gastrointestinal distress and possibly by anorexia and fever  This pain subsequently becomes localized to the right lower quadrant  RLQ  of the abdomen  which then provides a strong indication of appendici tis  along with a high white blood count   If the ap pendix ruptures  then there are several more symp toms  however  a perforated appendix leads to serious internal complications   Given the evolution of a dis ease such as appendicitis  the probabilities assigned to network nodes  and even the topology of the network itself  must change over time  For example  Figure   shows how the likelihood ratio for the diagnosis of appendicitis might change over time  Clearly  in the initial stages of appendicitis  many other diagnoses are equally likely given the symptoms  A second aspect of this dynamic nature of  diagnostic  reasoning is the need for modeling the temporal order of observations  In some cases the temporal sequence of observations  as opposed to just an unordered list of the set of observations  can provide strong cues for a diagnosis  For example  if a woman has abdominal pain  noting whether this pain is immediately followed by gastrointestinal distress could help identify a pos  Utility considerations have been ignored in most formal models of diagnostic reasoning  except for approaches such as  Heckerman and Horvitz          Most diagnostic procedures attempt to avoid perfora tion and its resulting complications   Time  sible case of appendicitis  whereas the absence of such immediate distress would make the presence of a gono horreal cyst in the right fallopian tub   more likely  A second example is the diagnosis of a car which has trouble starting  The sequence of events leading to the inability to start can help identify the problem  Thus  the inability to start only on mornings after it has rained may indicate that moisture is getting under the distributor cap  A third aspect is the ability to incorporate the effects of feedback  Feedback can alter not only the proba bility assignments to a network  but also the topol ogy of the network  For example  consider a network constructed for a case of RLQ abdominal distress  If simple stomach upset is diagnosed  and a treatment of Diovol is administered  the persistence of RLQ ab dominal distress will provide feedback to the system that the diagnosis may be incorrect  and the network topology and or probabilities may need to be updated  This paper proposes extensions to existing network construction techniques to model diagnostic reasoning as a sequential  dynamic process using the formalism of influence diagrams  This proposal is not intended to be a full temporal calculus based on Bayesian networks  as discussed in  Kanazawa         for example  Instead  it attempts to build simple networks which will real istically model the dynamics of diagnostic reasoning without necessitating the complicated  and computa tionally costly  construction and solution of temporal Bayesian networks     SYSTEM ARCHITECTURE  There are many existing systems and theories for model construction  Examples of such n  twork con struction frameworks include the proposal of Lf hmann         and examples of such systems include Q   R DT  Shwe and Cooper        and FRAIL   Goldman and Charniak         In each of these proposals  the   Dynamic Network Updating Techniques for Diagnostic Reasoning  goal is to construct a model which completely charac terizes the data  However  this goal conflicts with the need for efficient performance of implemented systems  Solving Bayesian network models is NP hard  Cooper         so the networks constructed must be as small as possible to ensure efficiency  The proposal presented in this paper trades off  to some extent  completeness and accuracy for efficiency  as is done in many other systems  such as  Heckerman and Horvitz           Figure    NASTY       Sensitivity Analysis  The remainder of the paper discusses the algorithms used to create an influence diagram from the KB  and for dynamically altering this influence diagram    The appropriate balance of resources between meta analysis of model construction and model solution has been studied by  Horvitz et al         Breese and Horvitz         As an example  the QMR DT network represents diseases       manifestations and        disease manifestation arcs  Heckerman and Horvitz                 Heuristics  I  Select  Best  Diagnosis  The KB for DYNASTY consists of a network of nodes and arcs  Nodes represent state variables  and arcs exist between pairs of nodes related causally and or temporally   Within the general model construction framework  such as that described in Lehmann           there is al ways uncertainty in choosing the correct model  That uncertainty may be due to uncertainty in the instru ments used to record data  to noise  or to the rela tionship between data from observations and causes for the observations  e g  the diseases causing the ob served symptoms   This paper examines the uncer tainty arising from relating observations and causes  and in particular the temporal uncertainty of this re lationship   l  Influence Diagram Model  l  Like several existing network construction methods  e g  QMR DT  FRAIL    we start with a Knowl edge Base  KB  containing     causal rules  and     a set of conditional probability tables  From this KB a network is constructed to solve a given task   Typically  the complete KB for a given domain is quite large   and given a set   of observations  it is necessary to construct a network containing only the data related to    and not the entire KB    DY  m  KB  A new system architecture proposed to model dynamic reasoning tasks is depicted in Figure    This system is called DYNASTY  for DYnamic Network Analysis of System TopologY   Associated with the network are probability tables for the conditional probabilities for the network  such as those required for the construction of a Bayesian net work  In addition  utility values are stored for decision making   Network construction methods     MODEL CONSTRUCTION HEURISTICS       Time Dependence  As noted earlier  diagnostic tasks whose characteristics change over time have not been modeled in earlier ap proaches  The approach taken in DYNASTY is to dis cretize the possible times from which the observations could have occurred  Call  D   the network  consisting of causes and intermediate causes observations  which would need to be constructed at time t   In full gener ality  the networks at different times are different  and they can each be quite large for complicated tasks  To fully model a diagnostic task  an influence diagram  ID  containing sub networks for each time I  would need to be constructed  given a set   of observations  This is shown in Figure     Figure    Most general influence diagram for solving a stochastic diagnostic task             Provan  DYNASTY attempts to solve a simplified task  it cre ates a network for particular time tj  and then con ducts a sensitivity analysis to determine if the action taken is affected by the choice of time tj  The ID which would be constructed is shown in Figure    Figure    Simplified influence diagram for solving a stochastic diagnostic task  However  these observations may actually be indica tive of the early stages of appendicitis  To make sure that a possible case of appendicitis might be diag nosed  the ID shown in Figure   must be constructed  This ID bears little relation to the ID shown in Fig ure    The possible treatments include      emetic  for Figure    More complex influence diagram for abdom inal pain example  Example    Consider the time course of a possible case of appendicitis  Early in the course of appen dicitis  the symptoms could appear to be a simple up set stomach  Figure   shows the notation necessary Figure    Notation for constructing Abdominal Pain Influence Diagram OBSERVATIONS or  P  N F     HYPOTHESES     anorexia nausea    fever abdominal pain    LLQ     LLQ pain  RLQ     RLQ pain  A US FP GC                  appendicitis upset stomach food poisoning  gonohorreal cyst  to construct IDs for this task  If the observations are nausea and general abdominal pain  then the simple ID shown in Figure   may be constructed  This is an easy influence diagram to construct and solve  Given an ID  food poisoning       Diovol  for simple upset stomach       removal of appendix  for appendicitis   or     treat ment or removal of gonohorreal cyst  This example shows how  given a set of observations  uncertainty in the time course of possible diseases may require entirely different IDs     There are a number of heuristics used in DYNASTY for network construction  One heuristic is the use of temporal orderings for probability assignments  This heuristic is best demonstrated by an example  Con sider the diagnosis of a car which infrequently has problems starting  The two diagnoses under consid eration are a distributor cap problem  DC  or an al ternator problem  ALT   The weather  Vi   may affect the diagnosis  as wet conditions can cause condensa tion under a distributor       J   thereby causing the fail ure of the car to start  ST   Other possible causes of the problems in starting  e g  the alternator may be faulty and not recharging the battery  are not affected by weather conditions  A simple Bayes network for this problem is shown in Figure    Knowledge of the  Figure    Simple influence diagram for abdominal pain example Figure    Bayesian network model for determining the cause of the failure of a car to start  such as this  the possible treatments are the adminis tration of an emetic  for food poisoning  or Diovol  for simple upset stomach    history of the correlation between weather conditions   Dynamic Network Updating Techniques for Diagnostic Reasoning  and success in starting the car can significantly affect the probabilities assigned to the network  For exam ple  if the car only gives trouble starting in wet con ditions  then the problem is most likely DC  if the car gives trouble with equal probability in both wet and dry conditions  then the problem is most likely ALT  In fact  trouble in a single instance when the weather is dry will lead to the assignment of a low probabil ity to P DCIST  W   In this case  the history of the problem is crucial to the probability assignment  Hence  the history heuristic is the use of temporal his tory  whenever possible  in selecting the probabilities  from the probability tables  to be assigned to the net work in consideration  The temporal history is com puted simply by tracing the history for a node in the KB  using revised Truth Maintenance algorithms for computing the justifications for a node in a depen dency network  McAllester         The history heuris tic also uses triggers to guide probability assignments  For example  finding a single instance when the car won t start in dry conditions is a tri to the assign ment of a low probability to P DCIST  W        Sequential Diagnostic Process  The ID framework also allows diagnostic reasoning to be formulated as a sequential diagnostic process  Us ing a result of Tatman and Shachter          an ID can model a sequential process using dynamic program ming  provided that the value function Vis separable  In terms of IDs  a value node is separable if it can be represented as the sum or product of multiple sub value nodes  Value node separability has been exploited in the de sign of a sequential process for image understanding  Levitt et al          In a similar manner  value node separability is used to model the sequential nature of diagnostic reasoning  In brief  the decision nodes in a DYNASTY ID are called treatments  which may be tests to determine more observations  or actual treat ments for hypothesized diseases  In the former case  given an ID shown in Figure    the test T can deter mine a new observation     creating a new ID with another decisit t  node T   e g  another test or a treat ment  and another value node V   In this manner  the sequential nature of tests  or treatments  providing feedback to the diagnostic process can be modeled           MODEL UPDATING Overview  In a problem for which probabilities are temporally dependent  the sensitivity of the computed decisions  Please refer to  Provan         forthcoming    for more details  The presentation here is brief due to space limitations   to the temporally dependent probabilities must be tested  This provides a threshold for determining when a better model is warranted  This may require new probability values  corresponding to a new time t    or a new network topology corresponding to time t   This sensitivity analysis model updating in DY NASTY occurs in two stages  F irst  a sensitivity analysis is conducted to determine if data from time t  pro vides a better model than the data from time t   Sensitivity Analysis  If the network model needs to be updated  then some of the following processes may need to be invoked   Model Updating  I   New probability values are assigned and propagated to compute a new network equi librium state     Network topology is altered     A new model is built for a different time t   These processes are now discussed in greater detail       Equivalence Class Sensitivity Analysis  Given the construction of an ID model at time t  a deci sion  with accompanying diagnosis  of maximal utility is computed  For example  in the car diagnosis ex ample  the diagnosis might be DC  and the decision REPLACE DC  This decision would maximise the re quirement of ensuring that the car no longer has trou ble starting  In the process of computing this best decision  the next best decision for a different equivalence class is also recorded  In the car example  this is REPLACE ALT  If there is uncertainty concerning which proba bilities are correct  then the sensitivity of the decision to this uncertainty must be determined  This is for malised in terms of equivalence classes of decisions as follows         Analysis of Equivalence Class es  The equivalence class approach to diagnosis  as origi nally formulated in  Provan and Poole         is sum marised here  The rationale is that there is no point in distinguishing between decision equivalent diagnoses  i e  diagnoses for which the decision taken  e g  ad ministration of drugs to a patient  are the same  as far as the decision maker is concerned decision equivalent diagnoses should be considered as the same diagnosis  The aim of diagnostic reasoning is to provide a treat ment for a set of observations  From an equivalence class point of view  this reduces to refining the set of use equivalent possibilities  i e  one does not care about distinct diagnoses  but distinct treatments  and their associated distinct equivalence classes   Thus              Provan  use equivalence induces a partition on the set of diag noses  where each partition corresponds to a possible distinct decision  Let T be the set of all treatments  or decisions     Let V be the set of all possible diagnoses  Definition     The possible treatment space P is a subset of Vx T   D  T  E P means that T is a possible treatment given that the diagnosis is D E V   P induces an equivalence relation on the set of diag noses  This will be called strong equivalence with re spect to P  The idea is that equivalent diagnoses have the same set of possible treatments    and D  are to P  written P if and only if  D   Definition     Two diagnoses strongly equivalent with respect  D     P  Dz  if V T  P   E  T    D  T   E   D   T   E         Equivalence Class Decision making  We assume we have a measure p D T  of the utility of treatment T given diagnosis D  We can define the pos sible treatment space as the set of diagnoses with the same utility   In this case   strong use equivalence  means having the same utility for each treatment  Let V be the set of use diagnoses  For D E V  every logical model of D has the same utility measure  The following proposition about the expected value   T   of treatment T was proven in  Provan and Poole          T     L p D  T  p D   X       DEV  Under this approach to diagnostic reasoning  diagnoses are selected such that the expected utility of the treat ment is maximised  That is  the goal is to compute  Yi such that the expected value of the treatment given by equation   is maximised  Consider an ID in which the variables are denoted by X   x        xn   such that any diagnosis D consists of a subset of variables X  C X which are not func tioning normally  cf   de J leer et a          Pearl        Provan and Poole        for a further descrip tion of such diagnostic models   Then equation   can  By a treatment we mean a total prescription of what to do  i e   we do not conjoin different treatments   the conjunction would be one treatment   A treatment may be a test to distinguish abnormalities  the administration of drugs  replacement of circuit components  etc    ther types of equivalences  e g  weak equivalence  are also distinguished in  Provan and Poole         such cases are not discussed here due to space limitations   Formally  the treatment in the possible treatment space would be a pair  T  v  where  D   T  v   E P if I  D  T    v   be rewritten in terms of these variables as f TJ   L LJI  x T xp x         DEV Dl x  where JI  x  T  is the value of true in D   p D T  such  that  x is  The notion behind the sensitivity analysis is as fol lows  consider a model constructed at time t  such that decision T  is the optimal treatment  Call f  the expected utility for decision T   If the probabilities of certain variables are time dependent  then these new probabilities need to be substituted into the model to check if the decision would change  Note that differ ent diagnoses may be computed  but if the decision is unchanged  then  under this use equivalent approach  no network updating is necessary  For network updat ing to be necessary  the threshold f  must be exceeded by the expected utility of another treatment Tj given probabilities for timet   i e      f Tj        L L p x T xp x   DEV DFX     f    This provides a precise bound on when the treatment changes  When the threshold is exceeded  then net work alterations may be necessary  These updating methods are now summarised       Model Updating Techniques  There are several types of model updating operations  of which two of the most important are      probability value updating  and     network topology updating  These are discussed in turn         Probability Value Updating  This is the simple case of network upd ati n g  If no changes to the network topology are required when the model is updated from time t to t   then the re quired alterations to the probability values are made  and these values are propagated to obtain a new net work equilibrium state  For example  during the early stages of appendicitis diagnosis  probability values may need to be updated given changes in location of abdominal pain  Possible changes in probability assignments are shown in Figure   b   c          Network Topology Updating  Consider the onset of an entirely new set of symptoms in the observation of a patient with a possible case of the later stages of appendicitis  These are shown in F igure    If we started with the model in Figure    we see that the topology of the network needs to be altered    Dynamic Network Updating Techniques for Diagnostic Reasoning  vides a set of constraints on how M x  must be altered  In an analogous manner  constraints can be defined for the coarsening of the values of the state space of variable x  flx  where multiple val ues of Wx E rlx are combined into a single value w E C wx  The coarsening operation is defined similarly  Chang and Fung         The coarsening oper ation may lose information during the process of node aggregation  i e  the network proba bility assignments may be altered   Using the equivalence class approach  such information loss is acceptable if the equivalence class does not change  Otherwise  approximations may need to be used  Chang and Fung          F igure    Early stages of the diagnosis of appendicitis   a    b    c   If changes to the network topology are required when the model is updated from time t to t   then one of several algorithms may be used  These algorithms in clude  Refinement coarsening operations  Chang and Fung        are used to split merge network nodes respectively  Consider a network refinement necessary to include new al ternatives  For example  in abdominal diagnosis  the construction of a network which models only lower abdominal pain may need to be refined to differentiate right lower quadrant  RLQ  and left lower quadrant  LLQ  pain  Hence  a node mod eling lower abdominal pain needs to be split into nodes for RLQ and LLQ  cf  Figures   a   b    Or in the car diagnosis example  the single node for weather may need to be split into nodes for wet weather and mixed  wet and dry  weather  The network changes made for the refine ment coarsening operations are local  and do not involve all nodes in the network  This is for malised as follows  If x is a state node  then we call lix the predecessors of x in the network  and I x the successors of x in the network  The Markov boundary of x is the minimal set of nodes which  shield  x from the rest of the network  The Markov boundary M x  of node x consists of lix U I x U liE   Hence  ensuring the joint prob ability distribution of M x  is unaffected by the refinement coarsening or x ensures that the rest of the network will be unaffected as well  For example  it is shown in  Chang and Fung        that in a refinement of the values of the state space of variable x  flx  each value Wx E rlx is refined into multiple values w E R wx  For each value Wx E rlx which is refined into a value w E R wx    Refinement  coarsening  p I x lwx  liE   p Wx  lix          p I xlw  liE  p w  lix       wER wx   must be satisfied for all values of lix  This pro   Instead of splitting and or merging existing nodes  completely new nodes may need to be added to  or particular nodes deleted from  the network  In such cases a va riety of other algorithms are invoked  such as the reduction and clustering algorithms present in the IDEAL system algorithm library  Srinivas and Breese         In network addition  the KB is consulted to determine which nodes must be added based on causal relationships  Network Re instantiation It may turn out that the network created is inappropriate for the diag nostic task  For example  a simple network may be created which cannot be appropriately aug mented to model a more complicated case   In such a situation  a completely new network is con structed from the KB  Network additions       Implementation  The KB is implemented in Common Lisp  Extended Justification based TMS  e g   McAllester         data structures and algorithms are used for determining relevant nodes to instantiate given a set of observa tions  The inti uence diagrams are implemented using the IDEAL system  Srinivas and Breese         It is hoped that the TraumAID system   Yeb her et a          will be used as a test bed for this system  TraumAID is a decision support tool for the manage ment of multiple trauma  Trauma management in cludes both diagnosis and treatment  and this diagnos tic tool achieves these features using two modules      a rule based reasoner which models the relationships between clinical evidence and diagnostic therapeutic goals  and     a planner which manages the achieve ment of multiple goals  TraumAID is an excellent sys tem on which to test the theoretical results because  unlike most similar systems  it already contains a no   f radical changes must be made to an initial network  it can be computationally cheaper to create a new net work from scratch than to alter the original network using coarsening refinement operations              Provan  tion of sequential action and change  key elements of the proposed theory of diagnostic reasoning  Further  efficient incremental management of action and change is necessary for trauma management     This paper has described a proposed dynamic network construction system which can build models for prob lems with temporally dependent probabilities  Heuris tics are used to identify the best possible model  and to test the sensitivity of this model to probability val ues over time  Given the network updating capabil ities of DYNASTY  the full diagnostic cycle  which includes feedback from the decisions made  can be in corporated into the network  In addition  the ability to refine coarsen the network enables different levels of granularity  i e  the coarseness of the description of the system being modeled  to be examined during the diagnostic process  Most other approaches to diagnos tic reasoning  e g   de Kleer et a           have no way of dynamically altering the granularity of the system description  Future work includes testing the feasibility of the al gorithms in DYNASTY on real world problems  and extending and optimising these algorithms  The KB for the TraumAID system is the first set of real data for which such tests are proposed  ACKNOWLEDGEMENTS  The comments of the anonymous reviewers have led to improvements in the paper    Breese  and Horvitz        J  Breese and E  Horvitz  Ideal Reformulation of Belief Networks  In Proc  Con   Un certainty in Artificial Intelligence  pages              Fung   K   Chang and R  Fung   Re finement and Coarsening of Bayesian Networks  In         Proc  Conf  Uncertainty pages                 in Artificial Intelligence    Cooper         G F  Cooper  The Computational Com plexity of Probabilistic Inference Using Belief Net works  Artificial Intelligence   Kleer et al                                J  de Kleer  A  Mackworth  and R   Reiter  Characterizing Diagnoses  pages                 In Proc  AAAI   and Charniak        R  Goldman and E  Char niak  Dynamic Construction of Belief Networks  In Proc  Conf  Uncertainty in Artificial Intelligence                   Heckerman  and Horvitz   Horvitz   et a     erman             Strategic Decisions  K  Kanazawa  Logic and Time Nets for  Probabilistic Inference  In Proc  AAAI    Lehmann         H P  Lehmann   A         Decision Analytic  Model for Using Scientific Data  In M  Henrion  R  Shachter  L  Kana   and J  Lemmer  editors  Un certainty in Artificial Intelligence    pages North Holland          Levitt  et a                      T  Levitt  J M  Agosta  and T  Bin  ford  Model Based Influence Diagrams for Machine Vision  In M  Henrion  R  Shachter  L  Kana   and J  Lemmer  editors  Uncertainty in Artificial Intelligence     North Holland    McAllester                D  McAllester  Truth Maintenance  In  Proc  AAAI  pages   Pearl         J  Pearl                    Probabilistic Reasoning in Intelli  gent Systems  Morgan Kaufmann           Provan         forthcoming    G M  Provan  A Decision Theoretic Approach to Diagnostic Reasoning         forthcoming     Provan and Poole         G  M  Provan and D  Poole  A Utility Based Analysis of Consistency Based Diagno  sis  In Proc  Conf  on Knowledge Representation  pages                  Schwartz  et al         S  Schwartz  J  Baron  and J  Clarke  A Causal Bayesian Model for the Diagno sis of Appendicitis  In L  Kana  and Lemmer J   edi         D   Heckerman  and  E   Problem Formulation as the Reduction of          Shachter         R  Shachter  Evaluating Influence Dia grams  Operati ons Research                      Shachter         R  Shachter  Probabilistic Inference and  Influence Diagrams   Operati ons Research                         Shwe and  Cooper          M  Shwe and G F  Cooper  An  Empirical Analysis of Likelihood Weighting Simula tion on a Large  Multiply Connected Belief Network  In Proc  Conf  Uncertainty in Artificial Intelligence  pages                  Srinivas  and Breese          S   Srinivas  and  J   Breese   IDEAL  A Software Package for Analysis of Influence Diagrams   In Proc  Conf  Uncertainly in Artificial                   Tatman and Shachter        Dynamic  J  Tatman and R  Shachter   Programming  and  Influence  Diagrams   IEEE Trans  Systems  Man and Cybernetics                        a Decision Model  In Proc  Conf  Uncertainty in Ar tificial Intelligence  pages                Horvitz   Kanazawa         Intelligence  pages   Goldman  pages  and Matheson        R A  Howard and J E  Matheson  Influence diagrams  In R  Howard and J  Matheson  editors  T he Principles and Applications of  tors  Proc  Conf  Uncertainty in Artificial Intelligence  pages                 
 Qualitative and infinitesimal probability schemes are consistent with the axioms of probability theory  but avoid the need for precise numerical probabilities  U sing qualitative probabilities could substantially reduce the effort for knowledge engineering and improve the robustness of results  We examine experimentally how well infinitesimal probabilities  the kappa calculus of Goldszmidt and Pearl  perform a diagnostic task troubleshooting a car that will not start   by comparison with a conventional numerical belief network  We found the infinitesimal scheme to be as good as the numerical scheme in identifying the true fault  The performance of the infinitesimal scheme worsens significantly for prior fault probabilities greater than       These results suggest that infinitesimal probability methods may be of substantial practical value for machine diagnosis with small prior fault probabilities  Keywords  Bayesian probabilities  kappa probabilities  diagnosis   networks  qualitative calculus  i nfinitesimal    BACKGROUND AND GOALS  Bayesian and decision theoretic methods have long  been criticized for an excessive need for quantification  They require many numerical probabilities and  Brendan Del Faverol  Gillian Sanders    oepartment of Engineering Economic Systems  Stanford University  CA        Section on Medical Informatics Stanford University  CA        utilities that are difficult to assess and are liable to judgmental biases  Some people claim that since human thinking is inherently qualitative  it is incompatible with quantitative schemes  These criticisms have fueled interest in alternative formalisms for reasoning and decision making under uncertainty that are intended to be easier to use and more compatible with human cognition  Among these alternative schemes are  various generalizations of decision theory  Edwards         Dempster Shafer belief functions  Shafer        generalizations of logic  including default and non monotonic logics  Ginsberg         fuzzy logic  Zadeh         possibility theory  Dubois and Prade         and fuzzy probabilities  If   however  our goal is simply to provide a qualitative basis for reasoning and decision making under uncertainty  there is no need to abandon Bayesian decision theory  The axioms of decision theory  indeed  assume only the ability to make qualitative judgments   that is  to order events by probability or outcomes by desirability  The quantification of probabilities and utilities can be based on purely qualitative judgments  Furthermore  several schemes have been developed that are purely qualitative  but are consistent with the axioms of decision theory  One such scheme is qualitative probabilities  originated by Wellman        Henrion   Druzdzel       Wellman   Henrion         A second approach to qualitative probabilities is the kappa calculus  Goldszmidt and Pearl         which represents all probabilities in a Bayesian belief network by e K  where is an integral power of E  The K  calculus is  K   Henrion  Prova n Del Favero  and Sanders          consistent with the axioms of probability where E       Events are ranked according to K  Events with larger K are assumed to be negligible relative to events with smaller K  The calculus provides a plausible set of events  those with the smallest  most probable  consistent with the observed findings  The calculus is sometimes called  qualitative probability   To avoid  confusion with other qualitative probability schemes  we call this representation Pearl  infinitesimal probabilities          has extended this scheme to handle  similar difficulties  Much current research on qualitative simulation is directed towards integrating quantitative information to resolve ambiguities  and the resultant combinatorial explosions of the search space   In this paper  we report the results of an initial experimental  study  comparing  the  diagnostic  performance on a specific belief network using     the K  calculus or infinitesimal probabilities  and       qualitative utilities to support decision making   numerical probabilities  Our goal is to examine how  The K calculus or infinitesimal probabilities can be  approximation to the numerical representation   well  the  infinitesimal  scheme performs as  an We  looked at in two ways   a  as providing a scheme for  start with a fully assessed numerical representation   non monotonic reasoning whose semantics are firmly  convert this into a kappa representation using finite e  grounded in probability and decision theory  or  b  as  values  and perform inference on a set of test cases   providing a simplification of belief networks with  We first explain the mappings we used to obtain  numerical probabilities  In this paper  we are focus on  infinitesimal  the second view  and examine the performance of  probabilities  and how we mapped back from the  infinitesimal probabilities as an approximation to numerical probabilities   or  K values  from  the  numerical  pos terior K values into probabilities for comparison of  From this perspective   performance  Then  we describe the experimental  proponents of infinitesimal probabilities may claim  design  including the sample network  the set of test  four possible advantages over traditional numerical  cases   belief networks   probabilities  the epsilon values used in mapping  and     It may be easier to express beliefs by partitioning  and  our  variations  of  the  prior  fault  the number of findings observations per case   The  number of sets of relative  infinitesimal scheme provides a set of the most  plausibility  that is values  than by assigning  plausible diagnoses for each case  In the results  we  events into a small  each event a precise numerical probabilities      Results from reasoning with infinitesimal  compare these plausible sets with the posterior probabilities for the diagnoses produced by the  probabilities are more robust and therefore more  numerical  trustworthy since they are based on less specific  implications of these results for the application of the  inputs   K calculus as a practical representation   scheme   Finally   we  discuss  the     Reasoning with infinitesimal probabilities is easier to understand and explain     Inference methods with infinitesimal probabilities  can be computationally more efficient   Initial analysis of the computational complexity of          suggests that  in general  it is of the same order as reasoning with numerical probabilities  that is NP hard   Cooper           There  may  be modest  computational savings from doing arithmetic with small integers instead of floating point numbers  Most  research  on  qualitative probabilities has  concentrated on developing the formalisms and efficient algorithms   AND INFINITESIMAL PROBABILITIES  Hitherto  these claims have been largely untested  reasoning infinitesimal probabilities Darwiche    MAPPINGS BETWEEN NUMERICAL  There has been little concerted  effort to demonstrate their application to real tasks and to evaluate their practicality  Initial studies of QPNs  Henrion and Druzdzel         Druzdzel and Henrion        Druzdzel        suggest that they are often  inconclusive for nontrivial cases  For example  QPNs give vacuous results in any case with conflicting evidence  Studies of qualitative simulation have found  In order to be a b le to apply  the  K  calculus to  probabilistic reasoning on a belief network with finite probabilities  we need to provide a mapping from probabilities into kappa values  In order to compare the results we need to map the kappa results back again into probabilities  Strictly  the K calculus is only valid as E   tO   We use an approximation for finite  values of E  For a finite E  the K calculus partitions the  real interval        into regions identified by integers   based on the smallest power of in the polynomial  This mapping is illustrated in Figure    More specifically  consider the real       interval I  which is the interval used by probability theory  and a discretized representation of I  which we call      is a set of non negative integers which the  calculus uses to represent probability measures in the interval I   We  wish to explore the mappings f  I   tS  i e   from numerical to infinitesimal probability  and g  S    t I   Nume rical and Qualitative Probabilistic Reasoning   i e   from infinitesimal to  numerical probability    Note that there is information loss in the mapping f  since it is not injective  Moreover  the mapping g is not surjective   Definition           K  map   Spohn       The mapping f  from probability measures to  K  values takes a  probability  r and a threshold probability e and  outputs a K  value K  e S such that    APPLICATION DOMAIN  WHY YOUR  CAR DOES NOT START The task is to troubleshoot why a car is not starting  given evidence on the status of the lights  battery  fuel   fan belt  and so on  Figure   shows the Bayesian belief  network displaying the causal and conditional independence relations   We are grateful to David  Heckerman for providing the original belief network and to Paul Dagum for lending us his expertise as a Figure   shows an example of a mapping for            car mechanic in adjusting some of the probabilities   All variables are binary  present or absent   except for battery charge which has three values  high  low  none   The initial network contains fully quantified  numerical conditional probability distributions for  Kappa  C X   each influence and prior probabilities for each fault  source variable       common  effect  Effects of multiple causes of a are combined  with noisy ORs   generalized where necessary  There are nine explicitly identified faults in this model  spark plugs bad distributor bad fuel line bad fuel pump bad gas tank empty                    Prd lability p X        starter bad battery bad  Figure    An example mapping giving kappa as a  fan belt loose  function of probability  for        alternator bad  Figure     Bayesian network representing the car diagnosis domain  Leak events represent all the  potential causes of a fault other than those shown explicitly  The number in each origin fault of a leak node represents its prior probability in the original network  The numbers attached to each influence arrow represent causal strengths  that is the probability that the successor is broken given that the predecessor is broken  and all other predecessors are normal         He nrion  Prova n Del Favero  and Sanders    We also identified three leaks  Each leak event represents all possible causes of an event that are not explicitly identified above  The probability of a leak is the probability that its associated effect will be observed even though none of its identified causes are present   from    to       Table   shows the mean and range of  the resulting prior odds we used   Table    The minimum  mean  and maximum prior fault probabilities  The top line shows the original  engine start other  The  network with larger probabilities  To do this  we multiplied the prior odds by an odds factor ranging  probabilities  Those below are derived by multiplying  engine tum over other  the odds of each prior by the odds factor and  charging system other  converting back to probabilities   leaky noisy  or model assigns a probability to each  leak  to handle the fact that the network is inevitably incomplete  In our adjusted network  the probability  Odds  of each leak was substantially smaller than the sum of  factor  the probabilities of the identified causes for each event   Minimum  Mean  Maximum                                                                                                                                                                                                There are    observable findings in the model  listed here in non decreasing order of expense to test      engine start    gas gauge    engine tum over     lights    radio    fan belt    battery age    distributor        spark plugs     alternator  Note that there are four findings that are also enumerated faults  namely fan belt  alternator  spark      Test Cases and quantity of evidence  plugs  and distributor   We expected that the performance of both numerical    EXPERIMENTAL DESIGN  function of the quantity of evidence  We also wished  We wish to investigate the effects of three factors on  relative  the diagnos tic probabilities   performance  and infinitesimal schemes would improve as a  of  inf initesimal  to examine the effect of the quantity of evidence on the performance  of  the  two  schemes   Accordingly  we needed a representative set of test cases with varying numbers of findings    a  The choice of the value of E on the mapping between numerical and infinitesimal probabilities    b  The range of prior fault probabilities  c  The quantity of evidence in the test cases  We have already discussed factor  a   Here  we will discuss our choice of each of these factors  and the conduct of the experiment   We generated a set of     test cases  in the following manner  For each of twelve faults  nine identified faults plus three leaks   we identified the most likely  modal  value for each of the ten observable findings  For each fault  we created a base  case  consisting of all  findings at their modal value  In four cases  the fault is itself a finding  which we omitted from the base test  case  since including the true fault as observed in the test case would be trivial  We then generated a second case for each fault by omitting the most expensive observation from the base case   Further cases were      Range of prior fault probabilities  generated by omitting the next most expensive  The numbers in Figure  finding that the engine does not start  In this way  we created a series of ten cases for eight faults  and nine    are the original prior fault  probabilities  To examine the effect of the magnitude of the priors on the relative performance of the infinitesimal calculus  we created versions of the  observation in tum   In all cases  we retained the   Numerical and Qualitative Probabilistic Reasoning  cases for the four faults that are observable  resulting in a total of     test cases in all             faults are clearly identifiable  having probabilities at  least an order of magnitude greater than those of all other faults  We found that this approach  as expected   gave very similar results to the exact IC calculus  Computation  inference using CNETS    To obtain results for the numerical probabilistic  scheme  we employed IDEAL  Srinivas and Breese          using the clustering algorithm from the I DEAL library  We applied each of the     test cases to the network using each of the six sets of priors  performing a total of     run  For each run we computed the posterior probability for each of the twelve faults resulting in      probabilities     RESULTS Our first goal was to examine the effect of E values on the performance of the infinitesimal probability scheme  We then selected the value of E that gave the best results and examined the effect of varying the  quantity of evidence on the performance of both  numerical and infinitesimal schemes   We also converted the original numerical probabilities into K values  using the three values e                     resulting in a total of      additional runs  We ran       calculus developed at  we might expect it to perform better for small  where  each case using CNETS  a full implementation of the K the  Rockwell  Palo Alto  Laboratory  Darwiche         producing posterior K values for each fault  For each run  we computed the  plausible set  that is the subset of faults with the minimal K value  Definition   Plausible Set      Consider a set  V    v  v    vm representing m possible hypotheses    Let  vmin       minvj by the minimum    value  J  probability interval           as shown in Figure    and  larger e  To investigate this we analyzed an initial set of    test cases usin g E values of                                 Figure   shows a graph of average probability against e  It is interested to note that the average score  identical fore      and e  To compare the infinitesimal scheme with the numerical one  we converted K values of diagnoses back to probabilities as follows  De fi n it ion     original probabilities  with less information lost   Accordingly  we might expect it to do better with  is identical for E        and E  Cll V   j vj  vminl   Pro b a bility  setV  v  v        vm r e presenting assigned a  the approximation will be mere exact  On the other  hand  a larger E provides rnore partitions to the  score assigned to the true diagnosis for these cases   The plausible set is given by  hypotheses   Since the kappa calculus is only strictly correct as E        consequently  it provides a finer discretization of the  in which each hypothesis has been assigned a value   Effect of E values  score   m  For              and also       Overall  there is an  improvement in performance with increasing E up to       Accordingly  we selected E         for use in our  remaining experiments   a  p o s s i b   e  in which each hypothes is has been     value  the corresponding probability  distribution is given by  ifvj vmax                                   I           s                   a              r                         otherwise  That is  the probability ni    n is assigned to the true faults if it is in the plausible set of size n  Otherwise  we assigned p                                                As an additional test  we also ran IDEAL using the  exact algorithm  but using fault probabilities mapped to O OlK for the values obtained from the mapping using the full set of K values   subset of    test cases   We applied this to a  In the results  the plausible  Figure    Effect of E o n the score  probability  assigned to the true fault  by the infinitesimal scheme   Henrion  Provan  Del Favero  and Sanders           Effect of Number of Findin gs on the  Plausible set                        As the quantity of evidence increases  we should expect the performance of both numerical and infinitesimal schemes to improve   Accordingly  we  classified the cases by the number of findings  Figure   graphs the average size of the plausible set  number of  Gl  I I          Cl     e  Cl          plausible faults  identified by the infinitesimal scheme as a function of the number of findings  These results summarize all    cases fore            As expected  the  average size of the plausible set of faults decreases with the number of findings  from   faults with   finding to      faults for    findings  With   findings   o                  Number of findings  this scheme provides almost complete specificity that is  the plausible set usually consists of just a single diagnosis        Ill  Figure    The probability assigned to the true fault for each scheme as a function of number of findings              Ill      Ill  I       liloo  What is  perhaps  surpnsmg is how closely the performance of the infinitesimal scheme tracks the performance of the numerical scheme           N           Indeed the  infinitesimal scheme appears to perform better than the numerical scheme for intermediate numbers of     findings  but this difference is not significant   Since  the infinitesimal representation is derived from the numerical one  we could not expect it to do better  on average  Note that  even with all ten findings  both schemes  o             r                Number of fi nd i ngs         average about     probability for the true diagnosis  This relatively poor performance arises because of the limited scope of the network  which does not provide the means to differentiate among several classes of  Figure    The average size of the plausible set  as a function of the number of findings in each case       Comparing the performance of infinitesimal and numerical schemes  Next  we compare how the number of findings affects the diagnostic performance for the infinitesimal and numerical schemes  Figure   graphs the performance in terms of the average probability each assigns to the true fault  as a function of the number of findings  For both schemes  as expected  the average probability assigned to the true fault increases with increasing evidence  from about      with   finding  to about       with    findings   fault       The magnitude of priors and the performance of infinitesimal  probabilities The infinitesimal probability scheme appears to perform very well relative to numerical probabilities for the original car network  in which the prior fault probabilities are very small  on average          To  examine if it performs equally well for larger priors  we multiplied the prior odds by five odds factors  as shown in Table      Figure   shows the average  probability assigned to the true diagnosis as a function of the average priors   Interestingly  the two schemes  are almost indistinguishable up to an average fault prior         Above that  the performance of the infinitesimal probability drops off sharply   that is  for average priors of       and         These results   Numerical and Qualitative Probabilistic Reasoning  confirm our ex pect ation that infinitesimal works well for small priors  but not so well for large pr i ors          Cl              c       ordering of diagnosis  A third  would be to evaluate  even more  the quality of decisions will be less rather than more sensitive to these differences in representation        While these findings are encouraging for the practical usefulness of infinitesimal pr oba b il ities  we should   CI c      a   them  Another way would be to compare the rank the quality of decisions based on the diagnosi s  In general  scoring rules based on ranks of diagnosis or                  remember that these initial results are on a single domain  This car model dom ain is simple  with few       loops and short chains   This kind of experiment  should be conducted on a wide range of types of network to see how far these initial results will hold        up  In the introduction  we distinguished view infinitesimal  o              o oo          o oo       Aver age prior fault probability Figure    Comparison of the average performance of infinitesimal and numerical probability schemes as a function of prior fault probabilities   probabilities   as  an   a   approach  of to  nonmonotonic reasoning  from view  b   as an approximation to numerical probabilities   We  reiterate that this paper  we focus on  b   and we are not attempting to evaluate its use as an approach to nonmonotonic logic   Conclusions about the former  have limited relevance to the latter  Infinitesimal pro babi lit ies are quite appealing as an alternative to numerical probab il ities  They should be    CONCLUSIONS  significantly easier to eli ci t from experts  Inference  We find these initial results very encouraging in terms  of the diagnostic performance of the infinitesimal probability scheme  For this example domain  we  found the best performance occurs using E     to      Performance for E      was slightly worse     may be more effjcient  And resulting inferences should be somewhat more robust to changes in probabilities  Some questions that need further investigation include      Performance of the infinitesimal scheme relative to the numerical  scheme  does  not  appear  to  Does the best choice of E vary with the domain   vary  significantly with the quantity of evi dence  The performance using infinitesimal probability is not  Does these results hold for larger networks  with more complex structures   noticeably worse than the numerical probabilities for prior fault probabilities up to about       For larger average fault probabilities  the relative perform ance of  Can this infinitesimal approximation be extended to utilities and decision making   infinitesimal probabilities starts to drop off sharply   This findings suggests that infinitesimal probabilities  Can we obtain a clearer analytic characterization  are more likely to be reliable for diagnosis tasks with  of when performance  very small prior fault probabilities  such as most machine and electronic devices  They may also work for some med ical domains  as long as the priors are less than  disease       we have used is very simple   In  addition   engineering  The mapping from K values back to probabilities that More sophistic ated  mappings are pos sible  making use of higher values   We should also point out that the scoring methods that we have used to evaluate performan ce have been based on posterior probability of the true diagnosis  which is perhaps the most exacting way to compare  will be or won t be  reliable  we  methods  need practical knowledge for  eliciting  infinitesimal  probabilities  We an ticipate that  in the long run  the  best p r actical tools will quantitative methods   combine qualitative and        Henrion  Provan  Del Favero  and Sanders  Intelligence Conference  Acknowledgments  M  Goldszmidt and J  Pearl  causal relations   This work was supported by the National Science Institute for Decision Systems Research  We would like to thank David Beckerman for use of the car  pages         Vermont        entropy approach to nonmonotonic reasoning   refining some of the probabilities   M   Shacter   The Logic of Conditionals   G F  Cooper  The Computational Complexity of Probabilistic Inference Using Belief Networks   Artificial Intelligence                    Darwiche  A symbolic generalization of probability theory  Ph D  dissertation  Computer Science Dept   Stanford University  Palo Alto  CA        M      Goldzmidt   CNETS   A  computational environment for generalized causal networks         this volume    M  Druzdzel and M  Henrion  Efficient reasoning in  Proceedings of the American Association for Artificial Intelligence Conference  pages          Washington D C         J  Druzdzel  Probabilistic Reasoning in Decision Support Systems  From Computation to Common Sense  PhD thesis  Department of Engineering and qualitative probabilistic networks  In  M   Public  Policy   Carnegie  Mellon  University   Pittsburgh  Pa         H  Prade  Possibility Theory  an Approach to Computerized Processing of Uncertainty  Plenum  D  Dubois and  Press  NY         Utility Theories  Measurements and Applications  Kluwer Academic        H  A  Geffner  Default Reasoning  Causal and Conditional  W  Edwards   Theories  M   MIT Press   Henrion   and  Cambridge  MA        M   Druzdzel   Qualitative  propagation and scenario based explanation of probabilistic reasoning   In M  Henrion and R  S h achter   editors   Intelligence       Uncertainty in Artificial  Elsevier  Science  B V    North  Holland         M   Hendon    Search based methods to bound  diagnostic probabilities in very large belief nets    M   In Proceedings of Conf on Uncertainty and Artificial Intelligence        Ginsberg  Readings in Nonmonotonic Reasoning  Morgan Kaufmann  San Mateo  CA         M  Goldszmidt and J  Pearl  System Z   A formalism for reasoning with variable strength defaults  In  Proceedings of American Association for Artificial  editors   Uncertainty in Artificial  Intelhgence  pages          Elsevier Science B V   D  Reidel   Dordrecht  Netherlands         D arwiche  IEEE Transactions on Pattern Analysis and Machine Intelligence                      He nrion  An Introduction to Algorithms for Inference in Belief Networks  In M  Henrion and R   
 This paper proposes a novel  algorithm independent approach to optimizing belief network inference  Rather than designing op timizations on an algorithm by algorithm ba sis  we argue that one should use an unop timized algorithm to generate a Q DAG  a compiled graphical representation of the be lief network  and then optimize the Q DAG and its evaluator instead  We present a set of Q DAG optimizations that supplant opti mizations designed for traditional inference algorithms  including zero compression  net work pruning and caching  We show that our Q DAG optimizations require time linear in the Q DAG size  and significantly simplify the process of designing algorithms for opti mizing belief network inference     Introduction  Query DAGs  Q DAGs  have been introduced recently to allow the cost effective implementation of belief network inference on multiple software and hardware platforms         According to the Q DAG approach  belief network inference is decomposed into two steps as shown in Figure    The first step takes place off line and results in the generation of a Q DAG that can answer a number of pre specified probabilistic queries  The second step takes place on line and involves the evaluation of a Q DAG to compute answers to proba bilistic queries in the context of some given evidence  A Q DAG evaluator is a very simple piece of software  which allows one to implement it cost effectively on multiple software and hardware platforms  Our initial discussion of Q DAGs has focused on three key points   a  Q DAGs can be generated using modi fied versions of standard belief network algorithms   b  the time and space complexity of Q DAG generation  Olr llne  t Q J AGr J I  On line  Figure    The Query DAG framework   is the same as the time complexity of the underlying belief network algorithm  and  c  a Q DAG evaluator is a very simple piece of software         Our own experience  however  has revealed another important property of Q DAGs that was not origi nally intended but that seems to be as crucial as the multiple platform feature  In a nutshell  when us ing a belief network algorithm to generate a Q DAG  one need not worry about optimizing the algorithm using techniques such as computation caching  zero compression  and network pruning            Similar  if not better  efficiency can be expected by simply using an unoptimized version of the algorithm to generate a Q DAG and then optimizing inference at the Q DAG level  This involves reducing the Q DAG before evalu ating it and implementing an optimized Q DAG eval uator  The same Q DAG evaluator can be used with any generation algorithm and optimizations at the Q DAG level seem to be much simpler to understand and implement since they deal with graphically represented arithmetic expressions  without having to invoke prob    Optimizing Belief Network Inference using Query DAGs  ability or belief network theory  Therefore  the merits of this alternative approach are many  but most im portantly  the Q DAG alternative is systematic  sim ple and accessible to a bigger class of algorithms and developers  The rest of this paper is structured as follows  First  we will review Q DAGs and their semantics in Sec tion    Next  we will present complete pseudocode for an optimized Q DAG evaluator in Section   and dis cuss its computational complexity  In Section    we present techniques and pseudocode for reducing the size of a Q DAG and discuss its computational com plexity  Section   is then dedicated to how Q DAG reduction and evaluation account for many of the stan dard optimization techniques that one seeks to realize in belief network algorithms  Moreover  we will argue in this section that the Q DAG approach is not just an alternative  but a better alternative according to a number of measures that we shall also discuss  We fi nally close in Section   with some concluding remarks   Query DAGs     We will review Q DAGs using an example  Consider the belief network in Figure   a  and suppose we are interested in queries of the form Pr b I c     Figure   c  depicts a Q DAG for answering such queries  which is essentially a parameterized arithmetic expression where the value of parameters depend on the evidence obtained  This Q DAG will actually answer queries of the form Pr b  c   but we can use normalization to compute Pr b I c     This Q DAG was generated us ing the join tree algorithm which builds a join tree as shown in Figure   b   Details of this generation pro cess can be found in         A number of observations are in order this Q DAG      It has two leaf nodes labeled  B  ON  and  B  OFF   These are called query nodes be cause their values represent answers to queries Pr  ON e  and Pr  OFF   e  where e is the available evidence  It has two root nodes labeled   C    N  and  C  OFF   These are called Evidence Specific Nodes  ESNs  since their values depend on the evidence collected about variable C on line   According to the semantics of Q DAGs  the value of evidence specific node  V  v   is l if variable V is ob served to have value v or is unknown  and   otherwise  Once the values of ESNs are determined  we evaluate the remaining nodes of a Q DAG using numeric multi plication and addition  The numbers that get assigned       to query nodes as a result of this evaluation are the an swers to queries represented by these nodes  For example  suppose that the evidence we have is C ON  Then the value of ESN  C  ON  is set to   and the value of ESN  C  OFF  is set to    The Q DAG in Figure   c  is then evaluated as given in Fig ure   a   thus leading to Pr  ON  C ON          and Pr  OFF  C ON             If the evidence we have is C OFF  however  then  C  ON  is set to   and  C  OFF  is set to    The Q DAG in Figure   c  will then be evaluated as given in Fig ure   b   thus leading to Pr  ON  C OFF           and Pr  OFF  O OFF            If we have no evidence about variable C  the value of C is unknown   both evidence specific nodes  C  ON  and  C  OFF  will then be set to   and the remaining nodes will be evaluated accordingly  Formally  a probabilistic Q DAG is a directed acyclic graph  Each root node in a Q DAG is either a nu meric node  Num  which is labeled with a number p in         or an evidence specific node  Esn  which is la beled with a pair  V  v   where V is a variable and v is a value of the variable  Each non root node is either a multiplication n ode     which is labeled with a   or an addition node  EB  which is labeled with a   In the rest of this paper  we assume the following func tions for manipulating Q DAGs  Children n   the chil dren of node n  Parents   n    the parents of node n  Type  n    the type of node n  which is either    EB  Esn or Num  Label n   the probability associated with a numeric node n  Value n   the value of a Q DAG node n  Given Values for evidence specific nodes  the Value of nodes can be determined as follows  The Value of a numeric nodes is its Label  the Value of an addi tion node is the addition of its parents  Values  the Value of a multiplication node is the multiplication of its parents  Values  We will also use N to represent the number of nodes in a Q DAG and  to represent the number of edges     A Q DAG Evaluator  We now discuss an optimized Q DAG evaluator that initializes the probabilities  values  of Q DAG nodes and updates them as evidence changes  Evidence in this case is the setting of an evidence specific node to either   or   according to Q DAG semantics described in        and reviewed in Section    There are two high level procedures for implementing the evaluator  the goal of which is to keep the function Value updated  This function assigns a probability Value n  to each node n so that if n is a query node        Darwiche and Provan  a   b   Pr AN      sf     f   s   n  c      A        A  C       ON    ON  OFF      A   B O   N ON       OFF      C  QN  ON      OFF      C OFF  I     A  SoON  BOFF  ON                    OFF                    C ON    C OFF   Figure     a  A belief network   b  a corresponding join tree  and  c  a Q DAG generated using the join tree    b C OFF   a  C ON  Figure    Q DAG evaluation  labeled with  V v   and if Value n    p  then we must have Pr v e    p  The two procedures are      initialize qdag  Figure     computes probabil  ities of nodes under the assumption that no evi dence is collected  all evidence specific nodes are set to         set evidence  Figure     sets the value of an evidence specific node n to v and updates the function Value accordingly   value is     And if n is a numeric node  then its initial value is simply the label associated with it  The ini tialization algorithm takes     time and it computes the prior probability of each query node    Procedure set evidence works by incrementally up dating the probabilistic values of nodes  Specifically  suppose that the value of node n changes from Vt to v  and consider a child m of n    If m is an addition node  then its value will change by the amount v  v   which is also the change that node n has undergone  Therefore  we can update the value of node m by simply adding v  v  to its previous value  We must also update the children of m recursively     One would use these procedures by first calling initialize qdag to initialize the Q DAG  As ev idence becomes available  corresponding calls to set evidence are made  Procedure initialize qdag starts by initializing the probability of each query node  To initialize the prob ability of a node n  one first initializes the probabili ties of its parents recursively and then combines these probabilities depending on the type of node n  The boundary conditions occur when n is a root node  Esn or Num   If n is an evidence specific node  its initial    If m is a multiplication node  and if v  f     we can    When no evidence is available  all evidence specific nodes have the value      Note that initialization can be done off line since it does not depend on any evidence  Therefore  it should not  in principle  be part of the Q DAG evaluator but part of the Q DAG compiler         Optimizing Belief Network Inference using Query DAGs  initialize qdag   for every query node n do initialize prob n  initialize prob n  unless Value  n  is initialized do for every node m in Pare nt s   n  do initialize prob m  case Type n   set evidence  n  NewValue   OldValue    Value n  Value n    NewValue propagate change  n  OldValue  NewValue   propagate change n  Old Value  New Value  unless   dValue  New Value do for each node m in Children n  do  Num   Value n     Label n  Esn   Value  n          Value n     value of mul node n  EB  Va ue n     value of add node n     dChildValue    Value m   if Type m  EB then Value m        Value m   Old Value  New Value  else if Old Value   then Value m      value of mul node m  else Value m        value of mul node n  Value     for all m in Paren ts  n  do  Value    Value  Value m  Value  Value m   OldValue  NewValue Old ChildValue  Value  m    return  propagate change   m   value of addition node n   Value      Figure    An optimized Q DAG evaluator   Parents n  do Value    Value  Value  m  return Value  for all m in  Figure    Initializing a Q DAG  l  update the value of node m by simply multiplying v jv  by its previous value  If Vt         however  we cannot incrementally update the value of m   Instead  we have to recompute its value by mul tiplying the values of its parent nodes  which is what function value of mul node does  Procedure set evidence takes     time in the worst case  but its average performance is better than linear since it will only visit those nodes that change their values      Simplifying the Q DAG  One may reduce a Q DAG by eliminating some of its nodes and arcs while maintaining its ability to answer probabilistic queries correctly  The motivation behind this simplification or reduction is twofold  faster eval uation of Q DAGs and less space to store them  Inter estingly enough  we have observed that a few  simple reduction techniques tend in certain cases to subsume optimization techniques that have been influential in practical implementations of belief network systems  Therefore  reducing Q DAGs can be very important practically  This section is structured as follows  First  we start by discussing three simple reduction techniques in the form of rew rite rules  Next  we provide pseudocode that implements these reductions and discuss their computational complexity  Finally  the implications of these reductions on optimizing belief network infer       x x x x x      x    Ql     pi  Ql  pl  Fig ure  Ql     pi  Q   pl  h   d   I  Ql  Ql  I     Ql  Q         Ql    Ql  Q      Q      Q DAG reduction techniques   ence are discussed at length in Section    The goal of Q DAG reduction is to reduce the size of a Q DAG while maintaining the arithmetic expression it represents   Definition    Two Q DAGs are equivalent iff they have the same set of evidence specific nodes  the same set of query nodes  and they agree on the values of query nodes for all possible values of evidence specific nodes   Figure   shows three basic reduction operations on Q DAGs  Identity elimination eliminates a numeric node if it is an identity element of its child  Fig ures   a  and   b    Numeric reduction replaces a multiplication or addition node with a numeric node if all its parents are numeric nodes  Figure   c    Zero compression replaces a multiplication node by a nu meric node if one of its parents is a numeric node with value zero  Figure   d    Identity elimination is implemented by the straight forward procedures eliminate identity zero and eliminate identity one in Figure    each of which takes O N  time         Darwiche and Provan  eliminate identity zero    n do Type n    Num  zero compression    for each node if  for each node and  then for every node  Label n      m in Chi dren n   Type m    I ll then Parents m   if      n  do  Value n      then Type n     Num Label n       Parents n        if do  Parents m     n   eliminate identity one   for each node if  Type n      n do Num  and  then for every node  Label n      m in Children n   Figure    Pseudocode for zero compression  do  Type m      then Parents m     Parents m     n   if  Figure    Pseudocode for identity elimination  numeric reduction    initialize queue q  for every node  n  Type n  Num   add n to queue q    InDegree n     I Parents n  I I ll  JnDegree n     I Parents n  I  while queue q is not empty do  n     Optimization using Q DAGs  do  case  get  compression as depicted in Figure   d  because ev ery multiplication node that has a zero parent will also have the value zero  and  therefore  will be con verted to a numeric node  The time complexity of zero compression is O N    from queue q  m in Children n  do InDegree m    InDegree m  if In Degree m      then Type m     Num Labe  m     Value m  Parents m       add m to queue q  for each  Figure    Pseudocode for numeric reduction  Numeric reduction is implemented by procedure in Figure    which maintains a queue of numeric nodes in the Q DAG and a counter for each addition multiplication node to count its par ents  For each  numeric  node on the queue  the pro cedure processes the node by decrementing the coun ters of its children  If any of these counters reaches zero  that means all parents of the corresponding nodes are numeric and  therefore  it can be reduced into a numeric node  W hen the reduction is per formed  the node is added to the queue  which al lows the possible reduction of its children  Procedure numeric reduction takes      time  numeric reduction  Zero compression is implemented by the procedure in Figure    Note here that if any Q DAG node attains the value zero after call ing procedure initialize qdag  it will maintain this value under any further evidence   Procedure zero compression is complete with respect to zerozero compression    Accommodating evidence entails changing the values of some evidence specific nodes from   to    This cannot increase the value of any Q DAG node   The main proposal in this paper is as follows  Instead of implementing an optimized algorithm for belief net work inference  use an unoptimized version ofthe algo rithm to generate a Q DAG  reduce the Q DAG using the procedures in Figures      and evaluate it using the procedures in Figure    The benefits of the Q DAG approach are      the Q DAG evaluator can be easily and cost effectively implemented on various software and hardware plat forms      Q DAG reduction and evaluation are algorithm independent      Q DAG reduction sub sumes the technique of zero compression  and some forms of network pruning      the Q DAG evaluator implements a sophisticated scheme for computation caching  which is simpler and more refined than any of the caching schemes that are typically implemented in algorithms based on message passing      the Q DAG evaluator handles the retraction of evidence with mini mal computations  while most caching mechanisms we are aware of seem to have difficulties in handling this kind of evidence efficiently  The first two points above are self evident and will not be discussed further  We focus in the rest of this section on the last three points  explaining them in detail and supporting them by examples       Zero Compression  Zero compression is an optimization technique that is typically implemented in algorithms based on join trees      Zero compression is designed to take advan tage of conditional probability tables which contain zero entries  implying some logical or functional rela tionship between network variables  During initializa tion of a join tree  each zero conditional probability is multiplied into some clique entry  which causes the corresponding entry in the clique to be zero as well    Optimizing Belief Network Inference using Query DAGs  s     f          o  t  c         N t   c          FF           A P a  ONI     r   sl    OFF             I  A  P B ONia   I f C ONib  ON ON OFF  BoOFF     B  OFF                     A   P a  ON     A  ON OFF  I  P B ONia          b    a   Figure     Pruning Node C      dl P A  ON B b   Figure     Zero compression at the Q DAG level   After performing some message passing to propagate evidence  some of these zeros will propagate through out the entire join tree  As more evidence is ob tained and propagated  computational resources are expended adding and multiplying clique entries by ze ros  Zero compression  as presented in      addresses this wasteful propagation by visiting entries in cliques to identify and annihilate the zero entries  The annihi lation step should restructure the internals of cliques to exclude zero entries from subsequent message passes  The same optimization can also be implemented in the context of other algorithms  but the details would dif fer  W hat is common  however  among different algorithms is that one can save computationally  sometimes signif  icantly  by avoiding multiplications by zeros whenever possible  As we demonstrate now  this zero compres sion optimization is subsumed by our Q DAG zero compression technique that we discussed in Section    Consider the Q DAG in Figure    Suppose that Pr C   OFF I A   ON           and Pr B   ON I A   OFF      instead  The resulting join tree will then be as given in Figure lO a  where each clique has a zero entry  The technique of zero compression aims at factoring out these entries so they do not enter into further computations when propagating messages  Alternatively  one could use a join tree algorithm that does not incorporate zero compression to generate the Q DAG shown in Figure lO b   One would then ini tialize the Q DAG to discover that some nodes will attain the value zero  Procedure zero compression can then be applied to generate the Q DAG in Fig ure lO c   which could further be reduced using Pro cedure eliminate identity zero leading to the Q DAG in Figure lO d   Therefore  one need not worry about implementing zero compression in the chosen   a  Original Q DA G   b  Roducod Q DAG  Figure     Reducing a Q DAG  belief network algorithm  one can rely on Q DAG re duction to achieve the same result as illustrated above       Network Pruning  Pruning is the process of deleting irrelevant parts of a belief network before invoking inference  Consider the network in Figure ll a  for an example  where B is an evidence variable and A is a query variable  One can prune node C from the network  leading to the network in Figure ll b   Any query of the form Pr a I b  will have the same value with respect to either network  but working with the smaller network is clearly preferred  Now  if we generate a Q DAG for the network in Fig ure ll a  using the polytree algorithm  we obtain the one in Figure    a   On the other hand  if we generate a Q DAG for the network in Figure ll b   we obtain the one in Figure    b   which is smaller as expected  The key observation  however  is that the optimized Q DAG in Figure    b  can be obtained from the un optimized one in Figure    a  using Q DAG reduction  In particular  the nodes enclosed in dotted lines can be collapsed using numeric reduction into a single node with value    Identity elimination can then remove the resulting node  leading to the optimized Q DAG in Figure    b          Darwiche and Provan  A   Pr A   ON  A Pr B ONIA  I rC ONIB   ON OFF  o       B ON OFF  A ON                I  B ON OFF  b   Pr B       fr C ONIB                              Global Retraction  P  C  ON B  b            B ON         B OFF    H  singly connected after pruning  thereby  reducing the complexity of inference  But using Q DAG reduction  we still have to generate a Q DAG using the multiply connected network        a  DP iQd Q DAO  P V I e   Figure     Block diagram of the join tree algorithm                Global    Update  Figure     Pruning Node A  P C  ON s  tl            Dynamic Evidence   b   RcdliCIId Q DAG  Figure     Reducing a Q DAG  To consider another example  suppose that we are in terested in computing Pr C   ON I b  in the net work of Figure    a   One can prune node A from the network  leading to the network in Figure    b  where the priors of node B are computed as follows  Pr b      a Pr b I a Pr a   Any query of the form Pr c I b  will have the same value with respect to ei ther network  but working with the smaller network is clearly referred  If we generate a Q DAG for the network in Fig ure    a  using the polytree algorithm  we obtain the one in Figure    a   If we generate a Q DAG for the network in Figure    b   we obtain the one m Fig ure    b   Note  however  that the Q DAG m Fig ure    b  can be obtained from the Q DAG m Fig ure    a  using numeric reduction  Therefore  some forms of network pruning are a by product of Q DAG reduction and  hence  one can de cide to ignore them at the algorithmic level and expect that their effect will be realized if Q DAG reduction is utilized  There are two caveats  however  First  it is not clear whether all forms of network pruning will be subsumed by Q DAG reduction  Second  Q DAG reduction will not reduce the computational complex ity of inference  although network pruning may  For example  a multiply connected network may become  The proper handling of dynamic evidence is an es sential property of practical belief network inference  Inference with dynamic evidence is typically imple mented with a computation caching scheme that at tempts to maximize the reuse of previous computa tions to conduct new ones  Unfortunately  computa tion caching is non trivial  typically undocumented  and its details vary from one algorithm to another  The main objective of this section is to show that a Q DAG framework allows us to handle dynamic evidence in a simple  uniform and sophisticated manner  Using this framework  we can  a  ignore dynamic evidence at the algorithmic level   b  use the algorithm to generate a Q DAG  and  c  handle dynamic evidence at the Q DAG level  But before we substantiate these claims  we review how dynamic evidence is typically handled in the join tree algorithm            Figure     which is borrowed from      depicts the over all control of the join tree algorithm  There are two important points to notice about this figure  First  the introduction of evidence leads to invalidating cer tain computations  which leads to an inconsistent join tree  The goal is then to recover this consistency  val idate probabilities  by doing the least amount of work possible  Second  there is a distinction between evi dence update and evidence retraction  in that evidence retraction requires more work to accommodate  We apply evidence update to variable V if its current value is unknown but evidence suggests a new value v of V  We apply evidence retraction to variable V if it        Optimizing Belief Network Inference using Query DAGs  has a current value v  but evidence retracts this value or suggests a different value v   In the join tree algo rithm  evidence update requires recomputing certain messages which are passed between cliques  Moreover  the messages to be recomputed are decided upon by certain flags that indicate the validity of messages as evidence is collected  Evidence retraction requires in addition the te initialization of certain clique poten tials  Details of these operations are beyond the scope of this paper  but see     for a relatively comprehen sive discussion  The metric we use for determining how well a system handles dynamic evidence is the amount of work needed to update probabilities   what one finds in other frameworks  Specifically  in ev idence update  the condition OldValue     will never be satisfied when calling the procedure set evidence and procedure value of mul node will never be in voked  This follows since no node will increase its value given that no evidence specific node has increased its value  in evidence update  the value of an evidence specific node will never change from   to     The pro cedure value of mul node may only be invoked in case of evidence retraction  which is the only extra work needed to handle evidence retraction versus evi dence update   First  we need to define evidence update and retraction formally in the context of a Q DAG framework      Definition   Evidence update occurs when each evidence specific node either maintains its value or changes its value from   to    Evidence retraction occurs when some evidence specific node changes its value from   to     Given the Q DAG semantics of Section    evidence up date occurs if and only if each variable either maintains its observed value or changes from unknown to ob served  On the other hand  evidence retraction occurs if and only if some observed variable either becomes unknown or changes its observed value to a different one  Dynamic evidence is handled in the Q DAG frame work as follows  Both evidence update and retraction are handled using the same procedure set evidence given in Figure    As far as simplicity is concerned  the pseudocode in Figure   speaks for itself  As far as uniformity is concerned  this code is independent of the algorithm used to generate the Q DAG  therefore  it can be used with any Q DAG generation algorithm  As far as efficiency is concerned  we have three points to make  First  set evidence takes     time in the worst case but does much better on average since it only visits nodes that change their values  Second  the caching scheme implied by set evidence is more refined than schemes based on message passing  Note that each message pass involves a number of arithmetic operations which correspond to some Q DAG nodes  If a message becomes invalid  all of these operations must be re applied although some of them may not lead to new values  In a Q DAG framework  only nodes that change their values are re evaluated  therefore leading to a more refined caching scheme  This level of refine ment is missed in message passing algorithms since caching is done at the message level  not at the arith metic operation level  Our final point is regarding the minor difference between evidence update and retrac tion in the Q DAG framework  which is contrary to  Conclusion  The message of this paper is simple  instead of opti mizing belief network algorithms   a  use plain  unopti mized versions of the algorithms to generate a Q DAG   b  reduce the Q DAG according to the procedures given in Figures      and  c  evaluate the Q DAG us ing the procedures given in Figure    This proposed alternative is cost effective  uniform  relatively simple  and optimized as compared to the standard approach   
  that we developed  the dynamic network tool that we designed and implemented to aid knowledge engineering  We present several techniques for knowledge engineering of large belief networks  BNs  based  for  CPCS  and the Bayesian network algorithms that we  employed for this large  complex network   on the our experiences with a network derived from a large medical knowledge base  The noisy MAX  a generalization of the noisy OR gate  is  used to model causal independence in a BN with  knowledge engineering or evaluation  challenging  The  multivalued variables  We describe the use of leak  development of CPCS necessitated the implementation of algorithms that could make inference in BNs of this size  probabilities  to  enforce  the  closed world  assumption in our model  We present Netview  a  more efficient  An example of this is a generalization of the  visualization tool based on causal independence and the use of leak probabilities  The Netview  noisy OR gate  Cooper       Peng and Reggia       Pearl  software  model causal independence  The  allows  knowledge  engineers  to  dynamically view subnetworks for knowledge engineering  and it provides version control for editing a BN  Netview generates sub networks in which leak probabilities are dynamically updated     The CPCS is one of the largest BNs in use at the current time  and its sheer size makes most tasks  such as        that is commonly used in binary valued networks to  CPCS BN contains nodes  that are multivalued  for example  disease nodes may have four values  absent  mild  moderate  severe   consequently we use a generalization of the noisy OR gate called the  noisy MAX  The specification of a complete conditional m with sm values and n predecessors requires the assessment of  sm  l IJ I s   to reflect the missing portions of the network   probability matrix for a node  INTRODUCTION  probabilities  where  s is  the  number  of  values  predecessor i  for a binary network this reduces to  of   n   In    Given the relative maturity of algorithm development in the  contrast  the causal independence assumption in the form of  Bayesian reasoning community  attention is now starting to focus on applying these algorithms to real world  a noisy gate reduces this assessment task to  probabilities  thereby simplifying knowledge  applications   and greatly reducing storage requirements   The Quick Medical Reference Decision  Ln sm  l s   dcquisition   QMR DT  project seeks to develop practical decision analytic methods for large knowledge based  To aid in the editing and refinement of the  systems  The first stage of the project converted the  have developed a network visualization tool we named  Theoretic  Internist   knowledge base  Miller  Pople et al          QMR  s predecessor  into a binary  two layered belief  CPCS BN  we  Netview  The Netview tool provides dynamic views of the  BN  and can generate subnetworks by taking advantage of        Shwe   the noisy MAX and leak assumptions  For inference  the  Middleton et al          In the second stage of the QMR DT  network  or subnetworks generated by Netview  are sent to the IDEAL package  Srinivas and Breese       for  network  BN   Middleton  Shwe et al   project we are creating a multilayer belief network with mu tiva ued variables  and developing efficient inference algorithms for the network  This paper will concentrate on the knowledge engineering issues we faced when building a large multilayered BN  To create a large multilevel  multivalued BN we took advantage of a rich knowledge base  the Computer based  inference  Netview is a flexible tool which can be used in any BN that uses noisy gates  and is described in section       Like the Internist     derived B N  the CPCS BN uses leak probabilities  Henrion       to represent the chance of an event occurring when all of its modeled causes are absent   We discuss our use of leak probabilities   and the  Patient Case Simulation system  CPCS PM   developed over two years by R  Parker and R Miller  Parker and  modifications to the leak probabilities required by the  Miller       in the mid     s as an experimental extension  dynamic network tool  in section       of the Internist   knowledge base  This paper makes contributions both in knowledge engineering and in algorithm development and implementation for large BNs  We describe the CPCS BN     KNOWLEDGE BASE TO BELIEF NETWORK  The CPCS PM system is a knowledge base and simulation program designed to create patient scenarios in the medical   Knowledge Engineering for Large Belief Networks  sub domain of hepatobiliary disease  and then evaluate medical students as they managed the simulated patient s problem  Unlike that of its predecessor lnternist    the CPCS PM knowledge base models the pathophysiology of diseases th e intermediate states causally linked between diseases and manifestations  The original CPCS PM system was developed in FranzLisp  Diseases and intermediate pathophysiological states  IPSs  were represented as Lisp frames  Minsky                 and were mapped to probability values  as described in next section  Frequency weights from the CPCS PM were mapped to probability values based on previous work in probabilistic interpretations of Internist   frequencies   the     To construct the BN we converted the CPCS PM knowledge base to CommonLisp and then parsed it to create nodes  We r e pr esented diseases and IPSs as four levels of severity in the CPCS BN absent  mild  moderate  and severe  Predisposing factors of a disease or IPS node were represented as that node s predecessors  and findings and symptoms of a disease or IPS node as the successors for that node  In addition to the findings  CPCS contained causal links between disease and IPS frames  we converted these links into arcs in the BN  Frequency weigh t s  Shwe  Middleton et al        from the CPCS PM ranged from   to  We generated the CPCS BN automatically  we did manual consistency checking using domain knowledge to edit the network  Because the CPCS PM knowledge base was not designed with probabilistic interpretations in mind  we had to make numerous minor corrections to remove artifactual nodes  to make node values consistent and to confinn that only mutually exclusive values were contained within a node  The resultant network has     nodes and     arcs  Figure     A t o tal of seventy four of the nodes in the network are predisposing factors and required prior p robab ilities the remaining nodes required leak probabilities assessed for each of their values  We thus had to assess over     probabilities to specify the network fully   Figure I  A small portion of the CPCS BN displayed in the Netview visualization program  The node ascending cholangitis in the third row shown in inverse has been selected by the user            Pradhan  Provan  Middleton  and Henrion  While the CPCS PM knowledge base is derived from the Internist   knowledge base it has been significantly     augmented by inclusion of the IPS states  and multivalued     representations of both diseases and manifestations of disesase  The original  QMR BN transformation of the  Internist   knowledge base used only binary valued disease and manifestation nodes  While conceptually simple  this     approach does not adequately reflect the potential variation in presentation of disease manifestations  or the severity of           diseases   Figure     A node x with  predecessors D i n   NOISY OR  ordered states  The noisy OR is a simplified  BN representation that  respectively  or disease and manifestation variables respectively  Peng and Reggia          variable  variables or predecessors  X  the  variables  and is typically described for a  which are interpreted as either cause and effect variables  that has  n  cause          P X     VI    If there are multiple  manifestation  variables  j          l   Consider an effect  D    Dn The noisy OR can be used when   tf having  probabilities required to calculate  probability matrix  The noisy OR is defined over a set of two level network partitioned into two sets of variables   E              The  shaded area represents the  requires far fewer parameters than the full conditional binary valued     q  GENERALIZATION OF THE NOISY OR          Aj   P Xi  xi I VI     II l pii   each D has  i D eV   an activation probability p  being sufficient to produce the effect in the absence of all other causes  and       the  probability of each cause being sufficient is independent of          the presence of other causes  Henrion  In this paper  we define variables using upper case letters   X is ilx  If variable X is present it is denoted using the letter x   letters   The domain of possible values for variable  if it is absent  it is denoted using x   The activation of a variable  X  by a predecessor variable D   is independent of the value of D   assumption   X is  conditional probability given by other  Under the noisy or  activated when D   P   is active  with a    P x I d    dk      In  words  this activation probability deh otes the  probability when inactive   where Pij is the link probability on the arc from D  to  lJi is active and all other predecessors are  For a two level noisy OR network  we define a set V of cause or disease variables  and a set W of effect or  manifestation variables  If there is a set VI of V of predecessors of  X  e  W that are present  then since the D   X will be false when all D  are false   P X X I VI   II P D   d   i D eV   P X  X I Vj      BN  application  since we need to accommodate n ary variables   For example  CPCS BN disease and IPSs can take on values such as absent  mild  moderate  and severe       NOISY  MAX  Consider a generalization of the noisy OR situation in which each variable is allowed to have a finite discrete state space  rather  than just a binary state s pace   This  generalization was first proposed by  Henrion         but he  did not describe the algorithmic details  In developing this generalization  we assume that we have a set  V of  predecessor variables D        Dn  Consider first the case where we have a variable  in VI are independent   Xi  The simple noisy OR is insufficient for the CPCS  and values that variables can take on using lower case  for  then we obtain  X with a subset  present  with the predecessors indexed by  V  of V that are  i j   q       The variable domains in CPCS BN are all partially ordered   for example   absent  mild  moderate  severe   and it turns out that such a partial ordering is necessary for all variable domains  For the remainder of our work we assume that all variables have ordered domains  We now want to compute  II      P    i D eV   The value xis given by In other words   From this  it is simple to derive  domain values  P X xiVI  l  II l p    i D eV   x    max i j           q   Henrion         X takes on as its value the maximum of the of  its predecessors   predecessors are all independent   given  that  the   Knowledge Engineering for Large Belief Networks       V    This computation of P X   xI can be viewed as setting up a hypercube of dimensions i x j x   x q and summing   the cells  each of which contains a probability value  fljkq P jkq  As an example of the derivation of the general formula  we consider the case of two predecessors D  and    j  If these variables take on values i j respectively  then the  probability P X  xI V      where  x   max i  j     For  example  Figure   graphically depicts the conditional  probability matrix for D  and Dj  b oth of which have  ordered states             If x    then consists of the shaded squares of the grid   P X   D  Dj   Figure    Explicit representation  of the leak probability as a cause of X   In this multivalued noisy MA X situation  the probabilities  that are being calculated in these hypercubes are cumulative  probabilities  that is   P X s  xI D s  d     For notational  convenience  we define the cumulative probability for a variable  X that has a s ingle predecessor D with maximum d as   domain value   f x I d   P X s  xI D s  d    Under the generalized noisy OR assumption  for a given variable  X  with a set of  which each D   q  D         Dq d   we know that  predecessors  has maximum value  for     LEAKS  As in any other knowledge representation scheme  the BN representation suffers from incompleteness  in that it  typically cannot model every possible case  A leak variable  can be used to enforce the closed world assumption  Reiter        The leak variable represents the set of causes that are not modeled explicitly   A leak probability is assigned as  the probability that the effect will occur in the absence of  of the cause s  D        D n that are explicitly modeled  If the leak variable is explicitly modeled  then it can be  any  treated like any other cause and can be depicted as such     IJ  P xld    i D eV   In this representation  the leak node is always  assumed to be  on   that is If  Note that using this transformation  we can define the  probability assigned to        Figure  i D E  P L true          the leak L with value l is factored into the calculation of  the unconditional probability for variable X  then we obtain  X taking on a particular value Xk    P X s  x    P L s  l   TI  p P D  s  d       P      i D V  Explicitly representing leak nodes in the  The unconditional probability assigned to a variable can be  derived in an analogous fashion  First  we nee d to derive  D   assuming no        this is given  the unconditional probability of variable predecessors  As described in  Provan by  P X    x    P L s      IJ p P D  s  d        P   P x I V     i D eV   P XS x     conditioning parents  The Netview knowledge engineering  tool  described in section    facilitates the maintenance and  editing of leak probabilities  Netview stores leak values  separately  as if they were explicit nodes  and then inserts  the leak values into the appropriate probability exporting a network for inference in IDEAL      The unconditional probability is given by  IJ p P D   S d    l p       i D EV  The unconditional probability assigned to X taking on a particular value x is   CPCS BN would  almost double the size of the network  so leaks are implicitly represented in the probability tables of a node s       tables when  TOOLS FOR KNOWLEDGE ENGINEERING NETVIEW   A TOOL FOR NETWORK  VISUALIZATION AND EDITING  Verification and refinement of the structure of the CPCS BN is an important part of the QMR  Df project for two reasons  First  because the  CPCS BN was generated from a pre  existing knowledge base  Second  the effect of model structure on network performance and accuracy is an important aspect of the QMR  Df project   During the knowledge engineering process  it became  V    Using this approach  the value P xI can be computed in time proportional to the number of predecessors in V   This generalized noisy MAX has been implemented in IDEAL   obvious  that  available tools were not suitable for  visualizing and editing a network the size of  Figure       CPCS BN  In particular  most tools only permit a static        Pradhan  Provan  Middleton  and Henrion  view of the network  a limitation that made editing the  disease   and so on  A node may have any number of semantic labels  Semantic labeling is a useful technique for  CPCS network very hard   filtering nodes to focus attention during knowledge by allowing knowledge engineers to focus on portions of  engineering  It is possible  say  to focus only on  gastric  findings and diseases when dealing with the appropriate  the network  The program is implemented in Macintosh  domain expert  In the future we will also use the semantic  Netview was created to help knowledge engineering efforts  CommonLISP       The main features of Netview are o  dynamic network layout  o  semantic labeling of nodes  o  version control    subnetwork generation and dynamic  o  labels in the dynamic layout algorithm to improve the appearance of subnetwork views  It is useful to keep track of modifications while editing the  BN  To facilitate this  Netview includes basic version  leak modification  control to store deleted and added arcs and nodes and  leak value editing  changes to probability tables  Arc and node additions and removals between versions are displayed through the use of  Because of the causal independence assumptions implied by the use of the noisy MAX and noisy    different colors   R gates   knowledge engineers are can select smaller parts of the       CPCS BN for viewing  Netview allows the user to view all ancestors  all predecessors  or all ancestors and predecessors of selected nodes  For example  in Figure l while the node  ascending cholangitis is selected  inverse  color   we can use Netview s ability to show all successors and predecessors of the selected node or nodes  resulting in the subnetwork view shown in Figure     Other options include viewing nodes  Markov blanket  and immediate successors or predecessors  Netview uses a dynamic layout algorithm to display the selected nodes  The knowledge engineer is able to move rapidly between views by selecting nodes and choosing viewing options  or by retrieving previously saved views  Quickly viewing a node s predecessors allows rapid assessment of leak probabilities  In addition to subnetwork selection  Netview allows semantic labeling of nodes  and filtering based on semantic labels  For example  nodes in CPCS BN are labeled  lab finding    symptom    sign    disease    IPS    liver        The  SUBNETWORK GENERATION AND DYNAMIC LEAK MODIFICATION  Subnetwork generation Netview  program  is  used  only  for  network  visualization and editing  Netview saves files in IDEAL format for inference  Because of the size of the CPCS BN it  is not always desirable to send the entire network to  IDEAL  for inference  If we are only interested in verifying a small set of diseases we can generate a subnetwork including only those diseases of interest and their associated findings  IPSs and predisposing factors  When we run test cases  against a subnetwork we don t require the system to compute the posterior probabilities of diseases that we are not interested in         Dynamic leak modification Subnetworks we select from the full  CPCS  BN using  Netview can be exported to IDEAL for inference  It is  possible to select subsets of the larger CPCS  BN for  i n f e r e n ce  leaks    due  to  the  presence  of  Figure    A subnetwork of the CPCS BN displayed in Netview  This view comprises all predecessors and successors of the node ascending cholangitis    Knowledge Engineering for Large Belief Networks  Figure    Subnetwork creation  Node D is removed from the network  the value of the leak node updated to p based on the probability of D           p is       When a subnetwork is saved Netview updates the leak probabilities to take into account the missing diseases  In the CPCS BN the node hepatomegaly has parents shown in figure    The leak probability for hepatomegaly is therefore calculated based on this set of predecessors  In figure   a subnetwork was selected based on the ancestors and predecessors of the disease ascending cholangitis  Conse quently  the only parent of hepatomegaly in the subnetwork is ascending cholangitis  its other parents are not included  The transformation of leak probabilities required during subnetwork creation is shown in Figure    The leak probability must be updated from p to p   This updating is done in order to preserve the total probability mass  If the value I of L is updated to a value l  for new leak L   we can compute the updated leak node probability as  BN  and we are exploring how much the network performance changes when the assumption is relaxed        Information metrics  When subnetworks are created some information is lost as parts of the network are excluded  A future area of research is to use Netview to calculate the information Joss of a subnetwork based on information metrics  Provan        and to compare differences in posterior probability between the complete network and the subnetwork which has been selected      RELATED WORK  The generalization of the noisy OR was first proposed in  Henrion        and the derivation and implementation p    P L    lA D     d   described here follow that original proposal  Two related   P L    l P D     d   generalizations are described in  Srinivas       and  Diez        The generalization of the noisy OR by Srinivas is   p P D     d    l  p    different to this proposal  and is intended for a different application  This generalization is for circuits  or other such If we want to combine a set of Q nodes into a leak node  devices  which can be either functional or non functional  where each node d  in Q has link probability  then the new In the case of medicine  findings can take on values such as leak node probability is given by   absent  mild  moderate  severe   in which case the binary generalization of Srinivas is insufficient to deal with p   P L   lAD     d AADq    dq  arbitrary n ary variables  The noisy MAX generalization in D i ez       is virtually identical to the one described here    P L       fl P Dt     d   and we have derived our noisy MAX independent of that in i D eQ  Diez        Also  the proposal in  Diez       is described  P L      n p P D    d    l p     within the context of learning models for OR gates  and its i D eQ application to inference in Bayesian networks is not directly apparent  We prove in  Provan       that if the network is To our knowledge  there is no other tool which allows hierarchical and there are no arcs between nodes at the dynamic selection of subsets of Bayesian networks  There same level of the hierarchy  then the leak updating is sound  are several graphical tools for creating Bayesian networks  that is  the probability assigned to X given the new set of including IDEAL Edit  Ergo  Hugin  Andersen  Olesen et al  predecessors is the same as the probability assigned to X        and Demos  Morgan  Henrion et al         But these with the original predecessors  This proof holds if the tools do not provide dynamic network layout and do not subnetwork consists of a Markov blanket of a node  all have features aimed at knowledge engineering large BNs  predecessors and successors of a node  or all successors of a node  The assumption for the proofs holds for the CPCS   CONCLUSION    Figure    Parents of the node hepatomegaly   In this paper we have presented several methods for representing  and a software tool for managing  large BNs based on our experience with the CPCS BN  The noisy MAX is a generalization of the noisy OR gate for multivalued        Pradhan  Provan  Middleton  and Henrion  variables which reduces the complexity of the knowledge acquisition task and storage requirements for a network  Leak probabilities are used in the CPCS BN to model causes other than those explicitly modeled in the network   Middleton  B   Shwe  M  A   et al          Probabilistic diagnosis using a reformulation of the Internist   QMR knowledge base     Evaluation of diagnostic performance  Methods of Information in Medicine              Based on the causal independence assumptions of the noisy MAX  and the use of leak probabilities we have developed Netview  a tool for visualizing BNs based on the dynamic layout of subnetworks  and which also provides basic version control for editing networks  The creation of subnetworks allows for more efficient knowledge engineering  and for easier verification of the B N  We describe a technique for updating leak probabilities based on the excluded parents of a node in subnetworks   Miller  R  A   Pople  H  E  J   et al          Internist    An experimental computer based diagnostic consultant for general internal medicine  N Engl J Med                Recent advances in creating BNs from pre existing data or knowledge bases will result in networks that are larger and more complex than those created manually  We believe that the techniques described in this paper are important to facilitate the management and verification of such networks  Acknowledgments  This work was supported by NSF Grant Project IRI         and by computing resources provided by the Stanford University CAMIS project  which is funded under grant number LM      from the National Library of Medicine of the National Institutes of Health  The authors would like to thank K  C  Chang and R  Fung for their graph layout algorithm on which NetView s layout method is based  and R Miller for access to the CPCS knowledge base  
  Within diagnostic reasoning there have been a numbr of proposed definitions of a diagnosis  and of an opti mal or most likely diagnosis  These include most prob able posterior hypothesis  most probable interpretation  most probable covering hypothesis  etc  Most of these approaches assume that the most likely diagnosis must be computed  and that a definition of what should be computed can be made a priori  independent of wht   the diagnosis is used for  We argue that the dtagnosttc problem  as currently pos ed  is incomplete  it does  ot consider how the diagnosiS IS to be used  or the utility associated with the treatment of the abnormalities  In this paper we analyse several well known  d efinitions f diagnosis  showing that the different defimt  s of opti mal diagnosis have different qualitative meamngs  even given the same input data  We argue that the most ap propriate definition of  optimal  diagnosis needs to ake into account the utility of outcomes and what the diag nosis is used for     INTRODUCTION  Within diagnostic reasoning there have been a number of proposals of what constitutes a diagnosis  an so pre    sumably  what constitutes an optimal or mot likely di agnosis  These include most probable posterior hpoth esis  Pearl        most probable interpretation lPearl         most probable provable hypothesis  Reiter        de Kleer and Williams        de Kleer et al          most probable covering hypothesis  Reggia et al         Peng and Reggia      a  Peng and Reggia       b   Un like earlier logic based diagnoses that consider what can be proven about a faulty device  Genesreth         the papers have considered that the quest   at i s a di agnosis   is important to answer  The mtitlOn IS ht it is important to characterise the set of  logical possibil ities  for a diagnosis  presumably to be able to compare them  Most of these approaches assume that the most likely diagnosis must be computed  and that a definition of the what should be computed can be made a priori  independently of what the diagnosis is used for   This author was supported by NSERC gra nt OGP         tThe author completed this research with the support of NSERC grant A     to A K  Mackworth   Once the sets of hypotheses considered as diagnoses are determined one of the ways we may want to compare competing diagnoses to give us the most likely diagnosis is by using probability  de Kleer and Williams        Neufeld and Poole        de Kleer and Williams        Pearl         In computing the probability of A given B  p AIB   Bayesian analysis tells us that the B should be all of the available evidence  Kyburg        Pearl         but does not give us any hints as to what A should be  This paper asks the question of what combination of hypotheses A should be in order to be most usefl  In this paper we study six approaches to dianos tic reasoning  and their associated notions of optal ity based on probability theory  or another uncertamty calculus   Each approach considers a conjunction  f hy potheses as a most likely diagnosis  We call the slX ap proaches     most likely single fault hypothesis     most likely posterior hypothesis     most likely interpretation     probability of provability     covering explanations  and    utility based explanation  We contrast the first five approaches to diagnostic rea soning with a classic utility based approach to diagnostic reasoning  Ledley and Lusted           In analysing these proposals  we show that the differ ent definitions of optimal diagnosis have different qual itative  and quantitative  results  even given he sae input data  Moreover  we argue that the diagnostic problem  as currently posed  is incomplete  it does  ot consider how the diagnosis is to be used  or the utility associated with either the diagnosis or the treatment of the abnormalities  We argue that the most appropri ate definition of  optimal  diagnosis should be based on what the diagnosis is used for  The point of this paper is to question current approaches to formalising diagnos tic reasoning  and hopefully focus attention on crucial questions not being studied  The remainder of the paper is organised as follows  Section   introduces the notation and discusses what the diagnostic problem should entail  Section   formally defines the six approaches to diagnosis studied in this paper  Section   shows examples of how the diagnoses       produced by the different approaches are qualitatively different  Section   discusses the proposals  evaluating their strengths and weaknesses  Finally  Section   draws a few conclusions     Figure    A complete diagnostic cycle  I  WHAT IS THE DIAGNOSTIC  I  PROBLEM       Notation  We call E the knowledge used to compute a diagnosis  E can be broken down into a set F of facts which are unchanging from instance to instance  e g  F can be a model of a circuit which is being diagnosed   and a set   of observations concerning a particular instance  We call H    h        hm  the set of hypotheses under con sideration given E  T    t        tl  is the set of possible treatments  This diagnostic problem can be formalised in either probabilistic or logical terminology  In terms of logi cal terminology  we use the Theorist framework of hy pothetical reasoning  Poole et al         Poole         a formalism well suited to the task as the paradigms can be naturally represented in the simple formal framework  Theorist  Poole        is defined as follows  The user pro videsF  a set of closed formulae  called the facts  and H  a set of open formulae  called the possible hypothe ses   A scenario is a set DUF where D is a conjunction of hypotheses D     i hi for some i            m  such that D U F is consistent  An explanation of formula g is a scenario that logically implies g  An extension is the set of logical consequences of a maximal  with respect to set inclusion  scenario  For a given treatment r s    T  we define a utility function u E  D  r    Using a standard decision theoretic approach  e g   Berger          the goal of diagnostic reasoning can be defined as choosing r to maximise u E  D  r     If there are probability distributions de fined over E and D  then the maximum expected utility     u E  D  r    is required  For example  if for diagno sis i  i           k  the utility associated with treating diagnosis Di is u E  Di  r    ai  and diagnosis Di oc curs with probability p Di   the goal is to choose r to maximise Ei ai p Di    In computing expected utility values  influence diagrams  Howard and Matheson        Shachter        can be used to find the treatment with highest utility         Diagnostic Problem  A complete diagnostic cycle consists of reasoning from evidence E to hypothesised diagnosis H  and then ad ministering a treatment T for the diagnosis  or abnor malities   This is shown in Figure    In most current formal theories of diagnosis  e g   Reiter        Peng and Reggia      a  de Kleer and Williams        Pearl        Pople          the treat ment phase is not considered at all  and diagnoses are defined with respect to the evidence hypothesis phase only  These approaches ignore utility considerations to tally  It is interesting to note that in one of the earli est analyses of medical diagnostic reasoning  Ledley and  I  Lusted        described a method of implementing the full diagnostic cycle  We argue that diagnostic reasoning must take into ac count the complete cycle  and should consider utility maximisation  In using a utility based approach  the definition of a diagnosis is strongly influenced by how the diagnosis is to be used  There are a number of possible uses of a diagnosis  including     to find out the thing  or things  that is wrong  that is  through testing  to determine the exact state of the world with respect to the symptoms observed     to give a plausible account  an explanation  for the symptoms  that is  to give a description of what is wrong that is understandable to people     to enable a decision as to how to fix something  that is  to maximise the utility derived from the diagnos tic process through the treatment of the abnormal ities     to fix the symptoms  in some cases we may be happy to fix the symptoms without caring about what is really wrong  The first of these may be carried out by someone who is trying to determine what errors occur so that they can be prevented  for example  by redesigning some compo nents   The second and the third are both activities that an ideal doctor must undertake  As well as giving optimal treatment they also have to be able to give a ra tionale for the treatment  The fourth may be something that has to be done in an emergency  for example reduc ing the temperature of a patient with fever  or restoring power in a failed power station  While each of these may seem reasonable  we will see that different goals will lead to different evaluation cri teria  For example  the best decision may involve av eraging over many cases  which may not be conducive to a good explanation  Also there may be no point in finding the exact causes for a problem if further refine ment of the problem will not improve the outcome  In computing explanations  the most coherent explanation may not contain all the evidence or hypotheses  It is important to keep these different goals in mind when considering different proposals         DESCRIPTION OF PROPOSALS Most likely single fault hypothesis  In this approach  the hypotheses considered are the unit hypotheses  h        hm  A single fault diagnosis is defined as a conjunction of hypotheses  only one of  I I I I I I I I I I I I I I I I   I I I I I I I I I I I I I I I I I I I     which is true  Hence  if hypothesis hi is the proposed single fault hypothesis  the single fault diagnosis Hi is  ii    i        hi      h      hi      i i  ij  For example  if an electrical circuit contains m components  a single fault diagnosis is that component i is faulty and all other components are functioning normally  The most likely diagnosis is defined as the diagnosis with the highest probability  p HiiE   over the set i E           m                    Most likely posterior hypothesis  As in the single fault approach  the hypotheses consid ered are the unit hypotheses  h         hm  The most likely diagnosis is defined as the hypothesis with the highest posterior probability p hi IE   This is the approach taken in MEDAS  Ben Bassat et al          INTERNIST  Pople         and by Pearl         for example  Pearl frames his analysis within Bayesian networks  This entails defining a causal graph of the problem  where the nodes represent random variables  or propositions  and directed edges represent direct causal influences between random variables       Most likely interpretation  This approach entails considering the set I    h       Ik  of interpretations  the set of truth assignments for the propositions in H  The interpretation which is most likely given the evidence must be determined  Pearl        defines this optimal interpretation as the interpre tation which has the highest posterior probability given the evidence  p IdE   where there is no Ij such that p IdE    p Ij IE   Reiter and Mackworth        ad vocate considering all visual interpretations for a given image formalised as a set of logical clauses  Note that all interpretations need not be computed in order to find the most likely one  Pearl              Probability of provability  In this case we use a logical axiomatisation of the prob lem  as well as a probabilistic model of the domain  The logical model is used to prove the logical possibili ties of the observations    which constitute the set of hypotheses in which one is interested   and then the probabilistic model is used to compute the likelihood of these  This corresponds to finding the most likely consistency based diagnosis  Reiter        Poole      b  de Kleer et al          In terms of normal and abnormal function of system components  a consistency based diagnosis is defined as  Definition      Consistency Based Diagnosis   A consistency based diagnosis is a minimal set of abnormalities such that the observations are consistent with all other components acting normally  Reiter         This approach entails axiomatising the problem as a logical theory   which consists of a set E of clauses rep resenting   and F  From E a set r of clauses logically entailed by E can be derived  This set can be defined as a minimal disjunct of maximal conjuncts of elements of  H that follow from E    i e      F  hi          hj  V  h         l h   V    V  hm         hn   Each conjunct is defined to be a diagnosis   In order to compute the probabilities of the elements of r  a probability distribution  or some other measure  must be assigned to E  Then the measure assigned to the  Yi E r must be computed  One method of such a computation is provided by the ATMS  as described in  de Kleer and Williams        or  Provan        Laskey and Lehner          An assump tion can be assigned to each clause Ei E E to symboli cally represent the measure assigned to Ei  The ATMS then computes the set of assumptions assigned to each literal consistent with E  Consequently  the assumption set associated with each  Yi can be computed  Assigning a measure to each assumption enables the measure for each  Yi to be derived  The causal relationship in this approach has been de scribed by Pearl        as evidential  as the direction of causality proceeds from evidence to cause       Covering explanations  In this case the goal is to abduce a causal explanation of the observations  Definition      Abductive Diagnosis  Given a causal axiomatisation of the system  an abductive di agnosis is a minimal set of hypotheses which  together with the background knowledge implies the observations    Poole et al         Poole      a   As in the provability approach  a logical axiomatisation is required  More formally  the hypotheses considered are the minimal conjunct of elements of H that imply   from F  cf   Poole          F      hi         hi  V hk         h  V   V hm         hn         Each conjunct is an explanation or diagnosis  Abduction is used to compute a causal explanation for the observations  Note that the background knowledge F must be axiomatised differently in this ap proach and the previous one  Poole        Poole      aj  Another method of describing this approach using graph theory is that of a  causal bipartite graph  Reg gia et al          In such a  graph the bipartite edge sets consist of cause nodes and observation nodes  with di rected edges from cause nodes to observation nodes  A node covering of causes for a given set of observation nodes is required  A probabilistic analysis of this approach can be found in  Peng and Reggia      a  Peng and Reggia      b  and in  Neufeld and Poole         Peng and Reggia      b  and Neufeld and Poole        describe a method of com puting the best diagnoses by evaluating the best partial diagnoses only   See  for example   Reiter          de Kleer and Williams        or  de Kleer et al         for details   For the purposes of this paper it is irrelevant what type of measure or combination function is used  What is of interest here is the measure assigned to r  and what is contained in r       a  Pearl  Pearl        describes this u a causal approach  as the direction of causality proceeds from cause to evi dence   I  c      Utility hued explanation  To compute utilities  we can use the definition of a Bayesian network  as done in the first two approaches described in the paper  augmented with a set of deci sion nodes  a value node and utility values  an influence diagram  Shachter          This approach makes no commitment as to which set of hypotheses constitute a diagnosis  unlike the logic based approaches  approaches   and     Bayesian net works can also be used to compute the probability of any conjunction of hypotheses  by creating a new node that represents the conjunction of the hypotheses of the nodes it is influenced by  Dependent on the utilities of a given problem instance  different combinations of hypotheses will be considered  In general  utility can be assigned on a problem by prob lem basis  Obviously  dependent on the utility func tion chosen  this approach can end up computing differ ent combinations of hypotheses than for the  diagnosis  from the first five approaches  The utility based approach is influenced by work in computer vision  planning and in cognitive science in which a high level description of the task influences both the objective function to be evaluated and the method of solving the task  For example  in computer vision  model based object recognition systems use a descrip tion of the object to speed recognition of the object by looking for only image primitives which will most likely constitute a part of the object  Chin and Dyer         We argue that the problem representation must be de pendent on the nature of the diagnosis required  Thus  if distinguishing among diagnoses does not affect the de cision made  there is no point to computing the different diagnoses  or an optimal diagnosis  Or  if the utility value is indifferent to particular liver diseases or heart diseases  the problem can be reformulated  e g  all liver diseases considered as a  group   and all heart diseases considered as another  group     As another example  consider a computer which has four main circuit boards  each of which can be divided into groups of components  If the computer goes down due to hardware failure  then the optimal diagnosis for a situation in which the computer must be fixed as soon as possible is to identify which circuit boards need to be replaced  If there is no time pressure  the optimal diagnosis may be to identify which group of components  or which specific components  must be replaced  given the high cost of replacing entire boards   I  b  This approach computes not the most probable conjunc tion of hypotheses  or diagnosis   but the conjunction of hypotheses that are most useful for the treatment phase of diagnosis   d  Figure        A  I  circuit  ARE THEY REALLY ALL THE SAME   In this section we demonstrate that the approaches do indeed give different answers   Example     Consider the analogue circuit of figure    In this figure a  b  c and d are gates that are faulty if they are closed  i e   they are supposed to be open  but they can be shorted   Let A be the proposition that is true if a is shorted  and B be the proposition that is true if b is shorted  etc  Suppose we have the knowledge that the gates break independently and we have the priors  p A  p B  p C      p D                               P   Ps   P     P    P     P   Ps  pg   PlO   Pu    P     Pts  P     Pl     p A  B Ac DIE  p AABACA   DIE  p A  BA     c DIE  p AABA     cA    DIE  p A      B  cADIE  p AA     BACA   DIE  p A      B      c DIE  p AA     BA     CA  DIE  p     AABA CADIE  p   A ABACA   DIE  p   AA BA   CADIE  p     AABA     CA    DIE  p   A      B  c DIE  p    AA      BACA   DIE  p     A      BA     c DIE  p    AA   BA     cA    DIE     I I I I I I  Suppose we observe that there is a current flowing through the circuit  Let E be the proposition that rep resents this evidence  The diagnoses computed by each of the approaches are now examined  We first give the interpretations as the the probabilities of all interpreta tions serves as a generator for all the probabilities  The    possible interpretations are  Po  Pt    I                                                                                                                                     There are a few things to notice about the probabilities of the interpretations   I I I I I I I I I   I I I I I I I I I I I I I I I I I I I         All of the other posterior probabilities can be gen  erated from the Pi In particular  for any formula we have p wiE     w     All of the interpretations that are not possible have probability zero  The last   interpretations are not possible given the circuit  and so must have proba bility zero  Note that there was no need to do any logical pruning before the probability phase  The logical axiomatisation of the provability and cover ing cases     and    was not to remove impossible in terpretations  as in  Ledley and Lusted         but to determine what it is that we are getting the prob ability of  The possible single fault diagnoses are obtained by saying that all of the interpretations except P   Pu  P s and P   are impossible  There is only one possible single fault diagnosis  a is shorted  This does not depend on any prior probabilities  ex cept for the knowledge that the probability is zero for all of the impossible diagnoses  Posterior  When computing the most likely posterior  we compare p AIE  p BjE  P  CjE  p DIE   p AjE  p B A OlE  p B    DjE   p IIE   I w is true in I   Single fault   Numerically  the probabilities are                              The most likely diagnosis is that a is shorted  For the covering hypothesis case we also need a logical axiomatisation  such as  Covering   AE BI CE BI DE When E is observed  we get the same comparisons as the previous case  and the same most likely diag nosis  Utility based  The final case is where we have to take utilities into account  The next three examples show how this can interact with the diagnoses  Example     Suppose that we can fix up the gates of example     independently  In this case the only relevant probabilities are the posteriors of each of the hypotheses  We denote byF   the treatment of fixing gate x  Con sider the following utility values  u E  D    r               ifF   E r    XED       ifF   E r    X D   otherwise    Po  Pl   P    Ps   P    P    P    P  Po  Pl   P    Ps   Ps   P    Plo  Pll In this case  since p BIE  is greater than       we ex   Po  Pl   P    Ps   Ps   P    P     P s pect to gain from fixing gate b  but do not gain from Po  P    P    P    Ps   P o  P     Pl  fixing any of the other gates  Thus it is only worthwhile fixing gate b  Numerically  the posterior probabilities are  If one believes that the aim of the diagnosis is to fix all of the problems  then this is a peculiar thing to do  p AjE           Based on the logical analysis  it cannot be the case that p BjE          only b is shorted  p CIE          Example     Suppose there is a heavy penalty for not p DIE          fixing the circuit by replacing a particular gate  as shown in the following utility function  The most likely faulty component is b  ifF   E r    XE D       Interpretation  The highest probability is P   which     ifF   E r   X D   indicates that the most likely interpretation is that u E  D    TD          ifF    r    XED   b and c are shorted  and a and d are not shorted     ifF   rf  r    X rf  D   Provability  In the fourth case we need to axiomatise In this case  all gates will be replaced in our example  the circuit  All we needed to compute was the posterior probabilities EAV BA CVD    for the individual hypotheses  Example     Suppose we can fix up gates b and c in and find the probabilities of the resulting diagnoses  dependently  but that the ways to fix up gates a and p AIE    Po  Pl   P    Ps   P    P  d interact in a complex way  In this case the relevant probabilities are  P    P  p B    CjE    Po  Pl   Ps   Ps p AjE            p B A DIE  p BjE           Po  P    Ps   Plo p CIE           Note that this diagnosis just coincidentally corresponds p A    DIE           to the most likely posterior hypothesis  in that they both have b broken   If we changed the priors slightly to make p AA  DjE          p C          the most likely interpretation becomes the one p     AADIE          with just A true  but the most likely posterior hypothesis is p   AA  DIE          unchanged           which is not what any of the diagnOiiell computed  Note that in this case the probabilities that are needed are not determined by what can explain the observations  but rather what is needed for the treatment  It might coincidentally happen that two approaches compute the best qualitative diagnosis  but this is not true in general  It is  however  not coincidence that the provability and covering approaches produce the same answer  It can be shown that  under certain reasonable assumptions about how the knowledge is represented  the propositional versions of the abductive and the de ductive systems  Poole        are identical  This is not  however  necessarily true for the non propositional ver sions  The difference arises because of the level of detail of the diagnoses  The more specific the diagnoses  the less likely it is  The abductive systems require the level of detail specific enough to imply the observations  In the deductive system  the level of detail is prescribed  not by the observation  but by the knowledge base  Poole      a   ANALYSIS OF PROPOSALS     Each of the proposals has good and bad points  some of which are now discussed       Single fault Diagnosis  The main problem with the most likely single fault hy pothesis is that it may be wrong  The real diagnosis may be a combination of faults  Another problem is that the single fault definition de pends on the representation used  At one level of ab straction a single fault may be a very complicated com bination of faults at another level of detail  For example  at one level of abstraction a problem may be that one power supply is broken  At another level of detail there may be a number of pwblerns with the one power supply       Most Likely Posterior  There are a number of problems with the  most likely posterior hypothesis  approach     It is the weakest statement about the world  Thus  if a pigeon is a type of bird  one must have p pigeoniE    p birdiE      High  possibly irrelevant  priors may dominate the relevant hypotheses     Groups of hypotheses are not considered  This ap proach cannot compute diagnoses which consist of sets of hypotheses  Multiple hypothesis diagnoses can be determined by heuristics only  e g  as is done in INTERNIST  Pople          In contrast  the in terpretation  consistency and covering methods can compute multiple hypothesis diagnoses based on the underlying theory of the method  Note that this does not imply we cannot diagnose systems with multiple faults  but rather that we do not consider conjunctions in diagnoses  Note that we can add hypotheses which are con junctions of hypotheses  but these will always be less likely that the original hypotheses         Most Likely Interpretation  In the most likely interpretation approach       there can be an exponential number of interpretations  and so for any reasonable sized problem we do not want to de rive all interpretations  as in  Reiter and Mackworth          Many interpretations will be highly unlikely  and computing them will be a waste of computational resources  However  it is not necessary to enumerate all interpretations  Pearl        de Kleer and Williams         Another drawback of this approach is that the most likely interpretation does not let us be agnostic about any proposition  we have to give a truth value to every hypothesis  no matter how related to the observations  Changing the space of hypotheses can change the prob ability of the most likely composite  and even what the most likely composite is  Pearl        p       Even more importantly  for building large knowledge bases adding  irrelevant  hypotheses can also change the most likely interpretation  If we imagine a random variable that is probabilistica lly independent of the other variables  then the most likely qualitative conjunct will remain the same  we will end up with the product of the most likely prior of the irrelevant hypothesis and the most likely in terpretation  If however the new random variable is not independent  then just by imagining different scenarios we can change the qualitative diagnosis  as the following example shows  Example     Consider a  patient who has symptoms which suggest either the flu or yellow fever  the flu be ing the more likely diagnosis  Now  suppose we add a variable that represents where the person was at some particular time two weeks previously  We partition the world so that the different places all have equal probabili ties  e g    Africa  may be one area  and  above the most northerly fioor tile in their office  may be another   If these new variables are independent of the disease vari able  the most likely interpretation will consist of the most likely values for each variable  Simply by assuming that it is much more likely that the patient has yellow fever if she was in Africa  the most likely composite hy pothesis may be that the patient has Yellow fever and was in Africa   One other problem with finding the most likely in terpretation is that the most likely interpretation may have very low probability  Peng and Reggia      c  first pointed this out and suggested that rather than finding the most likely interpretation  we should find a set of interpretations that covers some percentage of all of the cases  Note that the phase of removing interpretations inconsis tent with the knowledge  i e   those interpretations that are not models of the logical constraints  is unnecessary because p hi E      if h  is inconsistent with evidence E   This example is different from the example given by Pearl        p        in that we are adding a new hypothesis rather than changing a hypothesis  Just by imagining a new hy pothesis  and making no new observations  we can change the value of the most likely interpretation   I I I I I I I I I I I I I I I I I I I   I I I I I I I I I I I I I I I I I I I          Covering Explanations  Approaches   and   are discussed together because propositional versions of these approaches have been shown to produce the same results under certain assump tions  Poole         These differ from the interpretation approach in enabling us to be agnostic about the value of some variables which are irrelevant to the goal       Consistency and Covering Approaches  These logic based approaches are limited to what is prov able within the logical theory T  Hence  they are sensi tive to the particular logical description  If an observa tion is not provable  then nothing can be said about it   Similarly  for the Bayesian approach  if the appropri ate conditional probabilities are present for an observa tion  no easure c be assigned to the observation   In contrast  m the utility based approach considered here what is computed depends on the goal of the diagnosis   Both the consistency and covering approaches rely on making assumptions about the system under examina tion  Pearl        has shown that this approach has two major flaws      independencies among events with unknwn probabilities cannot be represented  and     domam knowledge describing defeasible conditional sen tences cannot be represented  Thus  these approaches are limited to problems in which the domain knowledge can be defined in categorical terms  e  g  strict taxo nomic hierarchies  deterministic systems  electronic cir cuits but not medical diagnosis or default reasoning   etc  If these approaches are misapplied to certain domains   non intuitive results can be obtained    Pearl        argues that instead of making assump tiOns about the hypotheses  examining the logical con sequenc s of these assumptions  and then assigning a probability measure over the assumptions a probability measure should be assigned over the logical clauses  E and the probabilities assigned to interpretations be ex a mined  as  done in approach     If not all clauses are as Igned a weight  then probability bounds  specifically the mner and outer measure  can be obtained for the mea sure assigned to the hpotheses  For example  Nilsson        presents a techmque which can accept an incom plete prbabilistic specification  and computes ranges for the reqmred measures for hypotheses  This paper argues that it is not clear that the approach suggested by Pearl  i e  that the most likely interpretation must be evalu ated  is always the best one  However  because these logic based approaches seem to be appealing for a number of reasons  it may be impor tant to determine subcases of non categorical domains in which paradoxes do not arise       Utility based diagnoses  The utility based approach assumes that utilities are domain dependent  which implies that diagnoses must be domain dependent as well  In contrast  the other approaches assume that the definition of diagnosis is domain independent  In addition  preferences used to define best diagnoses cannot be assigned in a domain independent manner  Doyle and Wellman        have proven that there exists no universally valid preference  set  in other words  preferences are consistent only within particular domains  Furthermore  Doyle      j argues for a decision theoretic  and not merely probabilistic definition of consistency and rationality in decision making  and therefore diagnosis   Te util it  based approa h is considered the proper oe m declSion theory and m several areas of cognitive science  For example  in visual recognition it has been shown  how several low level  and quite primitive  pre attentive processes are used to focus attention on the ost salient features of a scene  Treisman         and simple features are used to guide scene interpretation  Rosefeld         In some cases  results from these pre attentive processes are used directly to initiate action  For example  in the forest if a deer sees motion close by  the presence of motion in the visual field can be detected pre attentively   it will start running without identifying the cause of the motion  as it might get eaten if it spent the time trying to identify the cause of the motion  If the deer is not in danger  motion can trigger a closer scrutiny for the cause of the motion  One of the problems of the utility based approach is that there is nothing in a diagnosis unless there is a goal and utilities  There is no   value free  definition of a diagnosis  Whether this is desirable or not is left up to te reader  There is also the problem of being able to g ve someone an understandable explanation to answer the question  what is wrong       CONCLUSIONS  This paper has analysed several definitions of what a diansis  an  so  presumably  what a most likely diag  osis Is  showmg them to be mutually incompatible  It IS not clear that one definition is correct over all domains and situations  and for all possible uses that there could be for the diagnosis  Instead  we argue that the notion of most likely diagnosis cannot be defined a priori  but is defined based on what the diagnosis is to be used for and o n the uitlity of the outcomes of treating the abnor Oali ties  In other words  there may be no a priori ontological definition of optimal diagnosis  it is epistemological and situation dependent  
