 Computing the probability of evidence even with known error bounds is NP hard  In this paper we address this hard problem by settling on an easier problem  We propose an approximation which provides high confidence lower bounds on probability of evidence but does not have any guarantees in terms of relative or absolute error  Our proposed approximation is a randomized importance sampling scheme that uses the Markov inequality  However  a straight forward application of the Markov inequality may lead to poor lower bounds  We therefore propose several heuristic measures to improve its performance in practice  Empirical evaluation of our scheme with stateof the art lower bounding schemes reveals the promise of our approach      Introduction  Computing the probability of evidence even with known error bounds is NP hard  Dagum and Luby         In this paper we address this hard problem by proposing an approximation that gives high confidence lower bounds on the probability of evidence but does not have any guarantees of relative or absolute error  Previous work on bounding the probability of evidence comprises of deterministic approximations  Dechter and Rish        Leisink and Kappen        Bidyuk and Dechter      a  and sampling based randomized approximations  Cheng        Dagum and Luby         An approximation algorithm for computing the lower bound is deterministic if it always outputs an approximation that is a lower bound  On the other hand  an approximation algorithm is randomized if the approximation fails with some probability       The work in this paper falls under the class of randomized approximations   Randomized approximations  Cheng        Dagum and Luby        use known inequalities such as the Chebyshev and the Hoeffding inequalities  Hoeffding        for lower  and upper  bounding the probability of evidence  The Chebyshev and Hoeffding inequalities provide bounds on how the sample mean of N independently and identically distributed random variables deviates from the actual mean  The main idea in  Cheng        Dagum and Luby        is to express the problem of computing the probability of evidence as the problem of computing the mean  or expected value  of independent random variables and then use the mean over the sampled random variables to bound the deviation from the actual mean  The problem with these previous approaches is that the number of samples required to guarantee high confidence lower  or upper  bounds is inversely proportional to the probability of evidence  or the actual mean   Therefore  if the probability of evidence is arbitrarily small  e g            a large number of samples  approximately        are required to guarantee the correctness of the bounds  We alleviate this problem  which arises from the dependence of the Hoeffding and Chebyshev inequalities on the number of samples N  by using the Markov inequality which is independent of N  Recently  the Markov inequality was used to lower bound the number of solutions of a Satisfiability formula  Gomes et al         showing good empirical results  We adapt this scheme to compute lower bounds on probability of evidence and extend it in several ways  First  we show how importance sampling can be used to obtain lower bounds using the Markov inequality  Second  we address the difficulty associated with the approach presented in  Gomes et al         in that with the increase in number of samples the lower bound is likely to decrease by proposing several parametric heuristic methods  Third  we show how the probability of evidence of belief networks with zero probabilities can be efficiently estimated by using the Markov inequality in conjunction with a recently proposed SampleSearch scheme  Gogate and Dechter         Finally  we provide empirical results demonstrating the potential of our new scheme by        GOGATE ET AL   comparing against state of the art bounding schemes such as bound propagation  Leisink and Kappen        and its improvements  Bidyuk and Dechter      b   The rest of the paper is organized as follows  In section    we discuss preliminaries and related work  In section    we present our lower bounding scheme and propose various heuristics to improve it  In section    we describe how the SampleSearch scheme can be used within our lower bounding scheme  Experimental results are presented in section   and we end with a summary in section        Preliminaries and Previous work  We represent sets by bold capital letters and members of a set by capital letters  An assignment of a value to a variable is denoted by a small letter while bold small letters indicate an assignment to a set of variables  Definition     belief networks  A belief network  BN  is a graphical model P   hZ  D  Pi  where Z    Z            Zn   is a set of random variables over multi valued domains D    D            Dn    Given a directed acyclic graph G over Z  P    Pi    where Pi   P Zi  pa Zi    are conditional probability tables  CPTs  associated with each Zi   The set pa Zi   is the set of parents of the variable Zi in G  A belief network represents a probability distribution over Z  P Z    ni   P Zi  pa Zi     An evidence set E   e is an instantiated subset of variables  Definition    Probability of Evidence   Given a belief network P and evidence E   e  the probability of evidence P E   e  is defined as  n  P e       P Z j  pa Z j    E e       Z E j    To compute the probability of evidence by importance sampling  we use the substitution  n  f  x    P z  e      P Z j  pa Z j    E e   j    X   Z E       For the rest of the paper  assume M   P e  and f  x    nj   P Z j  pa Z j    E e   Several choices are available for the proposal distribution Q x  ranging from the prior distribution as in likelihood weighting to more sophisticated alternatives such as IJGP Sampling  Gogate and Dechter        and EPIS BN  Yuan and Druzdzel        where the output of belief propagation is used to compute the proposal distribution  As in prior work  Cheng and Druzdzel         we assume that the proposal distribution is expressed in a factored product form dictated by the belief network  Q X    ni   Qi  Xi  X            Xi      ni   Qi  Xi  Yi    where Yi   X            Xi     Qi  Xi  Yi     Q Xi  X            Xi    and  Yi     c for some constant c  When Q is given in a product form   we can generate a full sample from Q as follows  For i     to n  sample Xi   xi from the conditional distribution Q Xi  X    x            Xi    xi    and set Xi   xi   This is often referred to as an ordered Monte Carlo sampler       Related Work   Dagum and Luby        provide an upper bound on the number of samples N required to guarantee that for any b computed using Equation   ap         the estimate M proximates M with relative error  with probability at least       Formally  b  M               Pr M        M       The specific bound on N that the authors derive is  The notation h Z  E e stands for a function h over Z   E with the assignment E   e       N  Importance sampling  Rubinstein        is a simulation technique commonly used to evaluate the following sum  M   xX f  x  for some real function f   The idea is to generate samples x            xN from a proposal distribution Q  satisfying f  x       Q x       and then estimate M as follows   b    M N  N   w xi     where  i    w xi      f  xi   Q xi        ln M          These bounds were later improved by  Cheng        to yield   Computing Probability of Evidence Using Importance Sampling  f  x  f  x  Q x    EQ     M    f  x     Q x  Q x  xX xX  N            w is often referred to as the sample weight  It is known b   M  Rubinstein         that the expected value E M         ln M        ln                 In both these bounds  Equations   and    N is inversely proportional to M and therefore when M is small  a large number of samples are required to achieve an acceptable confidence level               A bound on N is required because  Dagum and Luby        Cheng        use Chebyshev and Hoeffding inequalities which depend on N for correctness  Instead  we could use the Markov inequality which is independent of N and still achieve high confidence lower bounds  The independence from N allows us to use even a single sample to derive lower bounds  The only caveat is that our proposed method does not have any guarantees in terms of relative error    We describe our method in the next section    GOGATE ET AL      Markov Inequality to lower bound P e   Definition    Markov Inequality   For any random variable X and k      Pr X  kE X     k  Gomes et al         show how the Markov inequality can be used to obtain probabilistic lower bounds on the number of solutions of a satisfiability constraint satisfaction problem  Using the same approach  we present a small modification of importance sampling for obtaining lower bounds on the probability of evidence  see Algorithm     The algorithm generates k independent samples from a proposal  x  distribution Q and returns the minimum fQ x   minCount in Algorithm    over the k samples  Algorithm   Markov LB   f   Q  k                           minCount   for i     to k do Generate a sample xi from Q f  xi   IF minCount    Q xi   THEN minCount   end for Return minCount  f  xi    Q xi    T HEOREM    Lower Bound   With probability of at least       k   Markov LB returns a lower bound on M   P e  Proof  Consider an arbitrary sample xi   It is clear from the discussion in section   that the expected value of f  xi   Q xi   is equal to M  Therefore  by the f  xi   Pr   Q x i   Markov inequality  we have   M         Since  the generated k samples are independent  the f  xi   k probability Pr minki    Q x i     M       and therefore i  f  x     Pr minki      Q x i      M        k    The problem with Algorithm   is that unless the variance of f  x  Q x  is very small  we expect the lower bound to decrease with increase in the number of samples k  In practice  given a required confidence of     k   one can decrease  as k is increased  Note that each sample in Algorithm   provides a lower bound with probability             We can replace the sample by any procedure that provides a lower bound with probability            and therefore in the following we propose several heuristic methods to compute a lower bound with probability                            Using the maximum over N samples  We can even use the maximum instead of the average over the N i i d samples and still achieve a confidence of         Given a set of N independent events such that each event occurs with probability             the probability that all events occur is           N   Consequently  we can prove that  Proposition    Given N i i d  samples generated from Q   xi   N Pr maxNi     fQ x i       M               Therefore  by setting         N          i e                     N    and recording the maximum value of f  xi    Q xi   over the N samples  we can achieve a lower bound on M with confidence           Again the problem with this method is that increasing the number of samples increases  and consequently the lower bound decreases  However  when the variance of the random variables f  xi   Q xi   is large  the maximum value is likely to be larger than the sample average  Another approach to utilize the maximum over the N samples is to use the martingale inequalities       Using the martingale Inequalities  In this subsection  we show how the martingale theory can be used to obtain lower bounds on P e   Definition    Martingale   A sequence of random variables X            XN is a martingale with respect to another sequence Z            ZN defined on a common probability space  iff E Xi  Z            Zi      Xi  for all i  Given i i d  samples  x            xN   generated from Q  note i  p f  x   that the sequence             N   where  p   i   MQ xi   forms a martingale as shown below   E  p  x           x p     E  p      p  f  x     p         E  MQ x p    x           x  Because   we  have   x            x p     E  p    p  as required  The expected value E         and for such martingales which have a mean of     Breiman        provides the following extension of the Markov inequality  Pr maxN i   i        Using Average over N samples  One obvious way is to use the importance sampling estimab Because E M  b   M  by Markov inequality M  b  is a tor M  lower bound of M with confidence         As the number of samples increases  the average becomes more stable and is likely to increase the minimum value over the k iterations of Algorithm     f  x p    x           x p    M  Q x p   f  x p    p   E   x           x p    M  Q x p                and therefore  i  f  x j            j  MQ x  j    Pr  maxN i          From Inequality    we can see that f  x j     i   i N maxi       j     Q x j      is a lower bound on M with a        GOGATE ET AL   confidence of           In general one could use any randomly selected permutation of the samples  x            xN   and apply inequality    Another related extension of Markov inequality for martingales deals with the order statistics of the sample  Let f  x      MQ x        N        f  x   f  x    MQ x               MQ x N    be the order statistics of the sample  Using martingale theory   Kaplan        proved that the random variable  f  x N j       i     maxN i     j    N  i  M  Q x N j        satisfies the inequality Pr             Therefore  f  x N j       i  Pr  maxN i     j    From maxNi        M  Q x N j        Inequality       f  x N j      ij     Q x N j      N     i      N  i  we is          can a            see  that  lower  bound  i  on M with a confidence of           To summarize in this section  we have proposed four heuristic ways to improve Algorithm       The average method      The max method and     The martingale random permutation method and     The martingale order statistics method      In the following example  we show how constraints can be extracted from the CPTs   Overcoming Rejection  Using SampleSearch with Markov LB  One problem with importance sampling based algorithms is the so called rejection problem and in this section we discuss how to alleviate this problem in MarkovLB by using the recently proposed SampleSearch scheme  Gogate and Dechter              The rejection problem has been largely ignored in the importance sampling community except the work on adaptive importance sampling techniques  Hernandez et al         Cheng and Druzdzel        Yuan and Druzdzel         In  Gogate and Dechter         we initiated a new approach of reducing the amount of rejection by using constraint processing methods  The main idea is to express the zero probabilities in the belief network using constraints  Definition    constraint network   A constraint network  CN  is defined by a   tuple  R   hZ  D  Ci  where Z is a set of variables Z    Z            Zn    associated with a set of discrete valued domains  D    D            Dn    and a set of constraints C    C           Cr    Each constraint Ci is a relation RSi defined on a subset of variables Si  Z  The relation denotes all compatible tuples of the cartesian product of the domains of Si   A solution is an assignment of values to all variables z    Z    z            Zn   zn    zi  Di   such that z belongs to the natural join of all constraints i e  z  RS          RSr   The constraint satisfaction problem  CSP  is to determine if a constraint network has a solution  and if so  to find one  When we write R z   we mean that z satisfies all the constraints in R   Rejection Problem  Given a positive belief network that expresses the probability distribution P Z    ni   P Zi  Z            Zi    and an empty evidence set  all full samples generated by the ordered Monte Carlo sampler along the ordering Z            Zn are guaranteed to have a non zero weight  However  in presence of both zero probabilities and evidence the ordered Monte Carlo sampler may generate samples which have zero weight because the sample may conflict with the evidence and zero probabilities  Formally  if the proposal distribution Q is such that the probability of sampling an assignment from the set  x  f  x       is substantially larger than the probability of sampling an assignment from the set  x  f  x        a large number of samples generated from Q will have zero weight  In fact  in the extreme case if no positive weight samples are generated  the lower bound reported by the Markov LB scheme will be trivially zero   Figure    An example Belief Network  Example    Figure   presents a belief network over   binary variables  The CPTs associated with C and G have zero probabilities  The constraint extracted from the CPT of C is the relation RA C                            while the CPT of G yields the constraint relation RD F G                                               Namely  each   tuple in a CPT corresponds to a no good  and therefore does not appear in the corresponding relation  Our importance sampling scheme called IJGP Sampling  Gogate and Dechter        uses constraint propagation to reduce rejection  Given a partial sample  x            x p    constraint propagation prunes values in the domains of future variables Xp             Xn which are inconsistent with  x            x p    However  we observed recently that when a substantial number of zero probabilities are present or when   GOGATE ET AL  there are many evidence variables  the level of constraint propagation achieved by IJGP is not effective and often few no consistent samples will be generated  Therefore in  Gogate and Dechter         we proposed a more aggressive approach that searches explicitly for a non zero weight sample yielding the SampleSearch scheme  Algorithm   SampleSearch Input  The proposal distribution Q x    ni   Qi  xi  x           xi     hard constraints R that represent zeros in f  x  Output  A sample x    x           xn   satisfying all constraints in R    i      Di   Di  copy domains   Qi  Xi     Qi  Xi    copy distribution   x       while    i  n do    if Di is not empty then    Sample Xi   xi from Qi and remove it from Di    if  x           xi   violates any constraints in R then    set Qi  xi  x           xi        and normalize Qi    Goto Step      end if    x   x  xi   i   i      Di   Di   Qi  Xi  x           xi      Qi  Xi  x           xi        else     x   x xi        set Qi   Xi    xi   x           xi        and normalize      set i   i        end if     end while     Return x       The SampleSearch scheme  An ordered Monte Carlo sampler samples variables in the order hX            Xn i from the proposal distribution Q and rejects a partial or full sample  x            xi   if it violates any constraints in R  R models zero probabilities in f    Upon rejecting a  partial or full  sample  the sampler starts sampling anew from the first variable in the ordering  Instead  we propose the following modification  We can set Qi  Xi   xi  x            xi         to reflect that  x            xi   is not consistent   normalize Qi and re sample Xi from the normalized distribution  The newly sampled value may be consistent in which case we can proceed to variable Xi   or it may be inconsistent  If we repeat the process we may reach a point where Qi  Xi  x            xi    is   for all values of Xi   In this case   x            xi    is inconsistent and therefore we need to change the distribution at Xi  by setting Qi   Xi    xi   x            xi         normalize and resample Xi    We can repeat this process until a globally consistent full sample that satisfies all constraints in R is generated  By construction  this process always yields a globally consistent full sample  Our proposed SampleSearch scheme is described as Algorithm    It is a depth first backtracking search  DFS  over the state space of consistent partial assignments searching for a solution to a constraint satisfaction problem R  whose value selection is guided by Q        It can be proved that SampleSearch generates independently and identically distributed samples from the backtrack free distribution which we define below  Definition    Backtrack free distribution    Given a distribution Q X    Ni   Qi  Xi  X            Xi     an ordering O   hx            xn i and a set of constraints R  the backtrack free distribution QR is the distribution  n  QR  x     QRi  xi  x           xi           i    where QRi  xi  x            xi    is given by  QRi  xi  x           xi       Qi  xi  x           xi       xi Bi Qi  xi  x           xi           where Bi    xi  Di   x            xi    xi   can not be extended to a solution of R  and xi    Bi   Note that by definition  f  x       QR  x      and vice versa   T HEOREM     Gogate and Dechter        SampleSearch generates independently and identically distributed samples from the backtrack free distribution  Given that the backtrack free distribution is the sampling distribution of SampleSearch  we can use SampleSearch within the importance sampling framework as follows  Let  x            xN   be a set of i i d samples generated by SampleSearch  Then we can estimate M   xX f  x  as  N i b      f  x   M N i   QR  xi          Although SampleSearch was described using the naive backtracking algorithm  in principle we can integrate any systematic CSP SAT solver that employs advanced search schemes with sampling through our SampleSearch scheme  Since the current implementations of SAT solvers are very efficient  we represent the zero probabilities in the belief network using cnf  SAT  expressions and use Minisat  Sorensson and Een        as our SAT solver  Computing QR  x  given a sample x  From Definition    we notice that to compute the components QRi  xi  x            xi    for a sample x    x            xn    we have to determine the set Bi    xi  Di   x            xi    xi   can not be extended to a solution of R   The set Bi can be determined by checking for each xi  Di if the partial assignment  x            xi    xi   can be extended to a solution of R  To speed up this checking at each branch point  we use the Minisat SAT solver  Sorensson and Een         Minisat should be invoked a maximum of O n   d      times where n is the number of variables and d is the maximum domain size  In  Gogate and Dechter         we found that the SampleSearch based importance sampling scheme outperforms all competing approaches when a substantial number of zero        GOGATE ET AL   probabilities are present in the belief network  Therefore  we employ SampleSearch as a sampling technique within Markov LB when a substantial number of zero probabilities are present  It should be obvious that when Samplei Search is used  we should use QfR x x i   as a random variable instead of         f  xi   Q xi    P c         cq   e   q    C   for a polynomial number of partially instantiated tuples of subset C  resulting in  h  k  i    i    L P e    P ci   e     PBP  ci         ciq   e         L  c        c   e  is obtained using where lower bound PBP q   bound propagation  Although bound propagation bounds marginal probabilities  it can be used to bound any joint probability P z  as follows   in the Markov LB scheme   Empirical Evaluation Competing Algorithms  L L PBP  z     PBP  zi  z         zi           i  Markov LB with SampleSearch and IJGP sampling  The performance of importance sampling based algorithms is highly dependent on the proposal distribution  Cheng and Druzdzel        Yuan and Druzdzel         It was shown that computing the proposal distribution from the output of a Generalized Belief Propagation scheme of Iterative Join Graph Propagation  IJGP  yields better empirical performance than other available choices  Gogate and Dechter         Therefore  we use the output of IJGP to compute the proposal distribution Q  The complexity of IJGP is time and space exponential in its i bound  a parameter that bounds cluster sizes  We use a i bound of   in all our experiments  The preprocessing time for computing the proposal distribution using IJGP  i      was negligible      seconds for the hardest instances   We experimented with four versions of Markov LB  a  Markov LB as given in Algorithm     b  Markov LB with the average heuristic   c  Markov LB with the martingale random permutation heuristic and  d  Markov LB with the martingale order statistics heuristic  In all our experiments  we set      and k     which gives us a correctness confidence of                on our lower bounds  Finally  we set N       for the heuristic methods  Also note that when the belief network is positive we use IJGP sampling but when the belief network has zero probabilities  we use SampleSearch whose initial proposal distribution Q is computed from the output of IJGP  Bound Propagation with Cut set Conditioning We also experimented with the state of the art any time bounding scheme that combines sampling based cut set conditioning and bound propagation  Leisink and Kappen        and which is a part of Any Time Bounds framework for bounding posterior marginals  Bidyuk and Dechter      a   Given a subset of variables C  X E  we can compute P e  exactly as follows  k  P e     P ci   e         i    The lower bound on P e  is obtained by computing P ci   e  for h high probability tuples of C  selected through sampling  and bounding the remaining probability mass by computing a lower bound PL  c         cq   e  on  L  z  z        z where lower bound PBP i   i    is computed directly by bound propagation  We use here the same variant of bound propagation described in  Bidyuk and Dechter      b  that is used by the AnyTime Bounds framework  The lower bound obtained by Eq     can be improved by exploring a larger number of tuples h  After generating h tuples by sampling  we can stop the computation at any time after bounding p   k out of k partially instantiated tuples and produce the result   In our experiments we run the bound propagation with cutset conditioning scheme until convergence or until a stipulated time bound has expired  Finally  we should note that the bound propagation with cut set conditioning scheme provides deterministic lower and upper bounds on P e  while our Markov LB scheme provides only a lower bound and it may fail with a probability               Evaluation Criteria We experimented with six sets of benchmark belief networks  a  Alarm networks  b  CPCS networks   c  Randomly generated belief networks   d  Linkage networks   e  Grid networks and  f  Two layered deterministic networks  Note that only linkage  grid and deterministic networks have zero probabilities  On each network instance  we compare log relative error between the exact probability of evidence and the lower bound reported by the competing techniques  Formally  if Pexact is the actual probability of evidence and Papp is the approximate probability of evidence  we compute the logrelative error as follows     Abs   log Pexact    log Papp     log Pexact          Note that the exact P e  for most instances is available from the UAI competition web site     The exact P e  for the two layered deterministic networks was computed using AND OR search  Dechter and Mateescu         We compute the log relative error instead of the usual relative error because when the probability of evidence is   http   ssli ee washington edu bilmes uai  InferenceEvaluation    GOGATE ET AL   Table    Results on various benchmarks  The columns Min  Avg  Per and Ord give the log relative error  for the minimum  the average  the martingale random permutation and the martingale order statistics heuristics respectively  The last two columns provide log relative error  and time for the bound propagation with cut set conditioning scheme  In the first column N is the number of variables  D is the maximum domain size and E is the number of evidence variables  Time is in seconds  The column best LB reports the best lower bound reported by all competing scheme whose log relative error is highlighted in each row    N  D   E   Alarm                                                        CPCS                                                                                         Random                                                     Exact P e   IJGP sampling Markov LB Bound Min Avg Per Ord Propagation     Time  Time  Best LB     E       E       E       E       E                                                                                                                                                                                                              E       E       E       E       E        E       E       E       E       E       E       E       E                                                                                                                                                                                                                                                                                                                                E       E       E       E       E       E       E       E                                                                                                                                                 SampleSearch Markov LB Exact Min Avg Per Ord P e      Time     E       E       E       E       E                                                         Bound Propagation  Time   N  D   E   Grid                 E                                                      E                                                      E                                                      E                                                      E                                                      E                                      Linkage                E                                                       E                                                        E                                                         E                                                         E                                                       E                                                       E                                                        E                                                          E                                       Two layered                 E                                                       E                                                      E                                                      E                                                       E                                           E       E       E       E       E    Best LB                               E       E       E       E       E       E                                                      E       E       E        E        E        E       E        E        E                                  E       E       E       E       E     extremely small the relative error between the exact and the approximate probability of evidence will be arbitrarily close to   and we would need a large number of digits to determine the best performing competing scheme       Results  Our results are summarized in Table    We see that our new strategy of Markov LB scales well with problem size and provides good quality high confidence lower bounds       on most problems  It clearly outperforms the bound propagation with cut set conditioning scheme  We discuss the results in detail below  The Alarm networks The Alarm networks are one of the earliest belief networks designed by medical experts for monitoring patients in intensive care  The evidence in these networks was set at random  These networks have between         binary nodes  We can see that Markov LB is slightly superior to the bound propagation based scheme accuracy wise  but is far more efficient time wise  Among the different versions of Markov LB  the average heuristic performs better than the martingale heuristics  The minimum heuristic is the worst performing heuristic  The CPCS networks The CPCS networks are derived from the Computer based Patient Case Simulation system  Pradhan et al          The nodes of CPCS networks correspond to diseases and findings and conditional probabilities describe their correlations  The CPCS   b and CPCS   b networks have     and     variables respectively  We report results on the two networks with          and    randomly selected evidence nodes  We see that the lower bounds reported by the bound propagation based scheme are slightly better than Markov LB on the CPCS   b networks but they take far more time  However  on the CPCS   b networks  Markov LB has better performance than the bound propagation based scheme  The martingale heuristics  the random permutation and the order statistics  have better performance than the average heuristic  Again  the minimum heuristic has the worst performance  Note that we stop each algorithm after   mins of run time if the algorithm has not terminated by itself  Random networks The random networks are randomly generated graphs available from the UAI competition website  The evidence nodes are generated at random  The networks have between    and    nodes and the maximum domain size is     We see that Markov LB is better than the bound propagation based scheme on all random networks  The random permutation and the ordered statistics martingale heuristics are slightly better than the average heuristic on most instances  Grid Networks The Grid instances are also available from the UAI competition web site  All nodes in the Grid are binary and evidence nodes are selected at random  The Grid networks have substantial amount of determinism and therefore we employ the SampleSearch based importance sampling scheme to compute the lower bound  Here  we stop each algorithm after    minutes if it has not terminated by itself  We notice that the performance of MarkovLB is significantly better than the bound propagation based scheme on all instances  Linkage networks The linkage instances are generated by converting a Pedigree to a Bayesian network  Fishelson and Geiger         These networks have be         GOGATE ET AL   tween          nodes with a maximum domain size of    and are much larger than the Alarm  the CPCS  and the random networks  On these networks  we ran each algorithm until termination or until a time bound of  hr expired  The Linkage instances have a large number of zero probabilities which makes them hard for traditional importance sampling based schemes because of the rejection problem  Therefore  in all our experiments on linkage instances we used the SampleSearch based importance sampling scheme  On Linkage instances  IJGP sampling did not return a single non zero weight sample  not shown in Table    in more than one hour of run time yielding a lower bound of    We see that the bound propagation based scheme yields inferior lower bounds as compared to the SampleSearch based Markov LB scheme  However  we notice that the log relative error is significantly higher for Markov LB on the linkage instances than the Alarm  the CPCS  and the random instances  We suspect that this is because the quality of the proposal distribution computed from the output of IJGP is not as good on the linkage instances as compared to other instances  Finally  we notice that the average and the martingale heuristics perform better than the min heuristic on all instances with the martingale order statistics heuristic being the best performing heuristic   using the SampleSearch scheme  Our experimental results on a range of benchmarks show that our new lower bounding scheme outperforms the state of the art bound propagation scheme and provides high confidence good quality lower bounds on most instances   Deterministic two layered networks Our final domain is that of completely deterministic two layered networks  Here  the first layer is a set of root nodes connected to a second layer of leaf nodes  The CPTs of the root node are such that each value in the domain is equally likely while the CPTs associated with the leaf nodes are deterministic i e  each CPT entry is either one or a zero  All nodes are binary  The evidence set is all the leaf nodes instantiated to a value  We experimented with   randomly generated      variable two layered networks each with     leaf nodes which are set to true  evidence   We employ the SampleSearch based importance sampling scheme for these networks because these instances have zero probabilities  We see that the average and the martingale heuristics are the best performing heuristics while the min heuristic performs the worst  The SampleSearch based Markov LB scheme shows significantly better performance than the bound propagation based scheme    Dechter and Mateescu        Dechter  R  and Mateescu  R          Mixtures of deterministic probabilistic networks and their and or search space  In UAI      Conclusion and Summary  In this paper  we proposed a randomized approximation algorithm  Markov LB for computing high confidence lower bounds on probability of evidence  Markov LB is based on importance sampling and the Markov inequality  A straight forward application of the Markov inequality may lead to poor lower bounds and therefore we suggest various heuristic measures to improve Markov LBs performance  We also show how the performance of Markov LB can be improved further on belief networks with zero probabilities by  ACKNOWLEDGEMENTS This work was supported in part by the NSF under award numbers IIS         and IIS           
 The paper introduces AND OR importance sampling for probabilistic graphical models  In contrast to importance sampling  AND OR importance sampling caches samples in the AND OR space and then extracts a new sample mean from the stored samples  We prove that AND OR importance sampling may have lower variance than importance sampling  thereby providing a theoretical justification for preferring it over importance sampling  Our empirical evaluation demonstrates that AND OR importance sampling is far more accurate than importance sampling in many cases      Introduction  Many problems in graphical models such as computing the probability of evidence in Bayesian networks  solution counting in constraint networks and computing the partition function in Markov random fields are summation problems  defined as a sum of a function over a domain  Because these problems are NP hard  sampling based techniques are often used to approximate the sum  The focus of the current paper is on importance sampling  The main idea in importance sampling  Geweke        Rubinstein        is to transform the summation problem to that of computing a weighted average over the domain by using a special distribution called the proposal  or importance  distribution  Importance sampling then generates samples from the proposal distribution and approximates the true average over the domain by an average over the samples  often referred to as the sample average  The sample average is simply a ratio of the sum of sample weights and the number of samples  and it can be computed in a memory less fashion since it requires keeping only these two quantities in memory  The main idea in this paper is to equip importance sampling with memoization or caching in order to exploit conditional  independencies that exist in the graphical model  Specifically  we cache the samples on an AND OR tree or graph  Dechter and Mateescu        which respects the structure of the graphical model and then compute a new weighted average over that AND OR structure  yielding  as we show  an unbiased estimator that has a smaller variance than the importance sampling estimator  Similar to AND OR search  Dechter and Mateescu         our new AND OR importance sampling scheme recursively combines samples that are cached in independent components yielding an increase in the effective sample size which is part of the reason that its estimates have lower variance  We present a detailed experimental evaluation comparing importance sampling with AND OR importance sampling on Bayesian network benchmarks  We observe that the latter outperforms the former on most benchmarks and in some cases quite significantly  The rest of the paper is organized as follows  In the next section  we describe preliminaries on graphical models  importance sampling and AND OR search spaces  In sections      and   we formally describe AND OR importance sampling and prove that its sample mean has lower variance than conventional importance sampling  Experimental results are described in section   and we conclude with a discussion of related work and summary in section       Preliminaries We represent sets by bold capital letters and members of a set by capital letters  An assignment of a value to a variable is denoted by a small letter while bold small letters indicate an assignment to a set of variables  Definition      belief networks   A belief network  BN  is a graphical model R    X  D  P   where X    X            Xn   is a set of random variables over multi valued domains D    D            Dn    Given a directed acyclic graph G over X  P    Pi    where Pi   P Xi  pa Xi    are conditional probability tables  CPTs  associated with each Xi   pa Xi   is the set of parents of the variable Xi in G  A belief network represents a probability distribution over X  P X         AND  C  AND     OR  B   A   B  OR     E  D  B  AB       CBA  C  F  G  E  EAB    DBC  D   a         OR  D  D        C  E  AND  AND     C  OR               b            B C  OR AND  B    AND  A     A  OR  B  A  OR  A        D  D                 C  E        D  D           C  E  C  E  E     C  E  C  E        AND        OR  D  D              D  D              D  D              D  D        E        D  D        AND                     c             d   Figure     a  Bayesian Network   b  Pseudo tree  c  AND OR tree  d  AND OR search graph ni   P Xi  pa Xi     An evidence set E   e is an instantiated subset of variables The moral graph  or primal graph  of a belief network is the undirected graph obtained by connecting the parent nodes and removing direction  Definition      Probability of Evidence   Given a belief network R and evidence E   e  the probability of evidence P E   e  is defined as  n  P e       P X j  pa X j    E e       X E j    The notation h X  E e stands for a function h over X   E with the assignment E   e       AND OR search spaces  We can compute probability of evidence by search  by accumulating probabilities over the search space of instantiated variables  In the simplest case  this process defines an OR search tree  whose nodes represent partial variable assignments  This search space does not capture the structure of the underlying graphical model  To remedy this problem   Dechter and Mateescu        introduced the notion of AND OR search space  Given a bayesian network R    X  D  P   its AND OR search space is driven by a pseudo tree defined below  Definition      Pseudo Tree   Given an undirected graph G    V  E   a directed rooted tree T    V  E  defined on all its nodes is called pseudo tree if any arc of G which is not included in E is a back arc  namely it connects a node to an ancestor in T   Definition      Labeled AND OR tree   Given a graphical model R   hX  D  Pi  its primal graph G and a backbone pseudo tree T of G  the associated AND OR search tree  has alternating levels of AND and OR nodes  The OR nodes are labeled Xi and correspond to the variables  The AND nodes are labeled hXi   xi i and correspond to the value assignments in the domains of the variables  The structure of the AND OR search tree is based on the underlying backbone tree T   The root of the AND OR search tree is an OR node labeled by the root of T   Each OR arc  emanating from an OR node to an AND node is associated with a label which can be derived from the CPTs of the bayesian network   Dechter and Mateescu         Each OR node and AND node is also associated with a value that is used for computing the quantity of interest  Semantically  the OR states represent alternative assignments  whereas the AND states represent problem decomposition into independent subproblems  all of which need be solved  When the pseudo tree is a chain  the AND OR search tree coincides with the regular OR search tree  The probability of evidence can be computed from a labeled AND OR tree by recursively computing the value of all nodes from leaves to the root  Dechter and Mateescu         Example      Figure   a  shows a bayesian network over seven variables with domains of         F and G are evidence nodes  Figure   c  shows the AND OR search tree for the bayesian network based on the Pseudo tree in Figure   b   Note that because F and G are instantiated  the search space has only   variables      Computing Probability of Evidence Using Importance Sampling Importance sampling  Rubinstein        is a simulation technique commonly used to evaluate the sum  M   xX f  x  for some real function f   The idea is to generate samples x            xN from a proposal distribution Q  satisfying f  x       Q x       and then estimate M as follows  M     xX  f  x     f  x   f  x    Q x  Q x    EQ   Q x          xX  N i b      w xi     where w xi     f  x   M N i   Q xi         w is often referred to as the sample weight  It is known that b   M  Rubinstein         the expected value E M  To compute the probability of evidence by importance sampling  we use the substitution  n  f  x     P X j  pa X j    E e       j    Several choices are available for the proposal distribution Q x  ranging from the prior distribution as in likelihood weighting to more sophisticated alternatives such as   IJGP Sampling  Gogate and Dechter        and EPIS BN  Yuan and Druzdzel        where the output of belief propagation is used to compute the proposal distribution  As in prior work  Cheng and Druzdzel         we assume that the proposal distribution is expressed in a factored product form  Q X    ni   Qi  Xi  X            Xi      ni   Qi  Xi  Yi    where Yi   X            Xi     Qi  Xi  Yi     Q Xi  X            Xi    and  Yi     c for some constant c  We can generate a full sample from Q as follows  For i     to n  sample Xi   xi from the conditional distribution Q Xi  X    x            Xi    xi    and set Xi   xi       AND OR importance sampling  We first discuss computing expectation by parts  which forms the backbone of AND OR importance sampling  We then present the AND OR importance sampling scheme formally and derive its properties       Estimating Expectation by Parts  In Equation    the expectation of a multi variable function is computed by summing over the entire domain  This method is clearly inefficient because it does not take into account the decomposition of the multi variable function as we illustrate below  Consider the tree graphical model given in Figure   a   Let A   a and B   b be the evidence variables  Let Q ZXY     Q Z Q X Z Q Y  Z  be the proposal distribution  For simplicity  let us assume that f  Z    P Z   f  XZ    P Z X P A   a X  and f  Y Z    P Z Y  P B   b Y    We can express probability of evidence P a  b  as  f  Z  f  XZ  f  Y Z  Q Z Q X Z Q Y  Z  Q Z Q X Z Q Y  Z  XY Z     f  Z  f  XZ  f  Y Z   E Q Z Q X Z Q Y  Z   P a  b          We can decompose the expectation in Equation   into smaller components as follows   Z  f  Z Q Z  Q Z    X  f  XZ Q X Z  Q X Z       Y  f  Y Z Q Y  Z  Q Y  Z            f  Z  f  XZ  f  Y Z  P a  b     E  Z E  Z Q Z  Q X Z  Q Y  Z  Z Q Z             f  XZ  f  Y Z  f  Z  E  Z E  Z P a  b    E Q Z  Q X Z  Q Y  Z     gX   Z   j    Nj  f  xi   Z   j I xi   Z   j  Q xi   Z   j     f  yi   Z   j I yi   Z   j  Q yi   Z   j   i   N i         From Equation    we can now derive the following unbiased estimator for P a  b     N     N j f  Z   j gX   Z   j gY   Z   j  Q Z   j  j              Importance sampling on the other hand would estimate P a  b  as follows      P a  b    N             N    where I xi   Z   j   or I yi   Z   j   is an indicator function which is   iff the tuple  xi   Z   j    or  yi   Z   j    is generated in any of the N samples and   otherwise        By definition  Equation   can be written as     Assume that we are given samples  z    x    y              zN   xN   yN   generated from Q decomposed according to Figure   a   For simplicity  let        be the domain of Z and let Z     and Z     be sampled h N  andi f  XZ   Z N  times respectively  We can approximate E Q X Z  h i f  Y Z  and E Q Y  Z   Z by gX   Z   j  and gY   Z   j  defined below     P a  b   The quantities in the two brackets in Equation   are  by definition  conditional expectations of a function over X and Y respectively given Z  Therefore  Equation   can be written as     Importance sampling ignores the decomposition of expectation while approximating it by the sample average  Our new algorithm estimates the true expectation by decomposing it into several conditional expectations and then approximating each by an appropriate weighted average over the samples  Since computing expectation by parts is less complex than computing expectation by summing over the domain  we expect that approximating it by parts will be easier as well  We next illustrate how to estimate expectation by parts on our example Bayesian network given in Figure   a      gY   Z   j    Nj    P a  b      We will refer to Equations of the form   as expectation by parts borrowing from similar terms such as integration and summation by parts  If the domain size of all variables is d      for example  computing expectation using Equation   would require summing over d             terms while computing the same expectation by parts would require summing over d   d     d                      terms  Therefore  exactly computing expectation by parts is clearly more efficient     Nj     f  Z   j    N j Q Z   j   j   N  f  xi   Z   j  f  yi   Z   j    Q xi  Z   j Q Y i  Z   j  I xi   yi   Z   j         i    where I xi   yi   Z   j  is an indicator function which is   iff the tuple  xi   yi   Z   j  is generated in any of the N samples and   otherwise    Z P Z  Z  Z    Z              P X Z   X    X    X    Z                   P Y Z   Y    Y    Y    Z                   Z                   Z                   P B Y   B    Y  X P A X   A    A    X              X              X         A  B       B    Y              Y              Y              Evidence A    B                Sample    Z  X  Y                                                  X  X  Y        Y                                        Q XYZ  uniform distribution   a                          P  Z                  Q  Z                                                      P  X       Z      P  A       X                          Q  X       Z            b    c   Figure     a  Bayesian Network  its CPTs   b  Proposal Distribution and Samples  c  AND OR sample tree Equation    which is an unbiased estimator of expectation by parts given in Equation   provides another rationale for preferring it over the usual importance sampling estimator given by Equation     In particular in Equation     we estimate two functions defined over the random variables X Z   z and Y  Z   z respectively from the generated samples  In importance sampling  on the other hand  we estimate a function over the joint random variable XY  Z   z using the generated samples  Because the samples for X Z   z and Y  Z   z are considered independently in Equation     N j samples drawn over the joint random variable XY  Z   z in Equation    correspond to a larger set N j  N j   N  j of virtual samples  We know that  Rubinstein        the variance  and therefore the mean squared error  of an unbiased estimator decreases with an increase in the effective sample size  Consequently  our new estimation technique will have lower error than the conventional approach  In the following subsection  we discuss how the AND OR structure can be used for estimating expectation by parts yielding the AND OR importance sampling scheme       Computing Sample Mean in AND OR space  In this subsection  we formalize the ideas of estimating expectation by parts on a general AND OR tree starting with some required definitions  We define the notion of an AND OR sample tree which is restricted to the generated samples and which will be used to compute the AND OR sample mean  The labels on this AND OR tree are set to account for the importance weights  Definition      Arc Labeled AND OR Sample Tree   Given a a graphical model R   hX  D  Pi  a pseudo tree T  V  E    a proposal distribution Q   ni   Q Xi  Anc Xi    such that Anc Xi   is a subset of all ancestors of Xi in T   a sequence of assignments  samples  S and a complete AND OR search tree T   an AND OR sample tree SAOT is constructed from T by removing all edges and corresponding nodes which are not in S i e  they are not sampled  The Arc label for an OR node Xi to an AND node Xi   xi in SAOT is a pair hw   i where  P Xi  xi  anc xi     w   Q X is called the weight of the arc  i  xi  anc xi    anc xi   is the assignment of values to all variables  from the node Xi to the root node of SAO and P Xi   xi   anc xi    is the product of all functions in R that mention Xi but do not mention any variable ordered below it in T given  Xi   xi   anc xi        is the frequency of the arc  Namely  it is equal to the number of times the assignment  Xi   xi   anc xi    is sampled  Example      Consider again the Bayesian network given in Figure   a   Assume that the proposal distribution Q XY Z  is uniform  Figure   b  shows four hypothetical random samples drawn from Q  Figure   c  shows the AND OR sample tree over the four samples  Each arc from an OR node to an AND node in the AND OR sample tree is labeled with appropriate frequencies and weights according to Definition      Figure   c  shows the derivation of arc weights for two arcs  The main virtue of arranging the samples on an AND OR sample tree is that we can exploit the independencies to define the AND OR sample mean  Definition      AND OR Sample Mean   Given a AND OR sample tree with arcs labeled according to Definition      the value of a node is defined recursively as follows  The value of leaf AND nodes is   and the value of leaf OR nodes is    Let C n  denote the child nodes and v n  denotes the value of node n  If n is a AND node then  v n    n C n  v n   and if n is a OR node then v n     n C n     n  n  w n  n  v n    n C n    n  n    The AND OR sample mean is the value of the root node  We can show that the value of an OR node is equal to an unbiased estimate of the conditional expectation of the variable at the OR node given an assignment from the root to the parent of the OR node  Since all variables  except the evidence variables are unassigned at the root node  the value of the root node equals the AND OR sample mean which is an unbiased estimate of probability of evidence  Formally  T HEOREM      The AND OR sample mean is an unbiased estimate of probability of evidence  Example      The calculations involved in computing the sample mean on the AND OR sample tree on our example                                                 Z                                                                                                                               X X Y                                                                                                         Y  constant domain size   the time complexity of computing AND OR sample mean is O nN   same as importance sampling  and its space complexity is O nN   the space complexity of importance sampling is constant      Variance Reduction              Figure    Computation of Values of OR and AND nodes in a AND OR sample tree  The value of root node is equal to the AND OR sample mean Bayesian network given in Figure   are shown in Figure    Each AND node and OR node in Figure   is marked with a value that is computed recursively using definition      The value of OR nodes X and Y given Z   j         is equal to gX   Z   j  and gY   Z   j  respectively defined in Equation    The value of the root node is equal to the AND OR sample mean which is equal to the sample mean computed by parts in Equation     Algorithm   AND OR Importance Sampling Input  an ordering O    X            Xn   a Bayesian network BN and a proposal distribution Q Output  Estimate of Probability of Evidence    Generate samples x            xN from Q along O     Build a AND OR sample tree SAOT for the samples x            xN along the ordering O     Initialize all labeling functions hw   i on each arc from an Ornode n to an And node n using Definition         FOR all leaf nodes i of SAOT do    IF And node v i     ELSE v i       For every node n from leaves to the root do    Let C n  denote the child nodes of node n    IF n   hX  xi is a AND node  then v n    n C n  v n      ELSE if n   X is a OR node then n C n     n  n  w n  n  v n    v n      n C n    n  n       Return v root node   We now have the necessary definitions to formally present the AND OR importance sampling scheme  see Algorithm     In Steps      the algorithm generates samples from Q and stores them on an AND OR sample tree  The algorithm then computes the AND OR sample mean over the AND OR sample tree recursively from leaves to the root in Steps       We can show that the value v n  of a node in the AND OR sample tree stores the sample average of the subproblem rooted at n  subject to the current variable instantiation along the path from the root to n  If n is the root  then v n  is the AND OR sample mean which is our AND OR estimator of probability of evidence  Finally  we summarize the complexity of computing AND OR sample mean in the following theorem  T HEOREM      Given N samples and n variables  with  In this section  we prove that the AND OR sample mean may have lower variance than the sample mean computed using importance sampling  Equation     T HEOREM      Variance Reduction   Variance of AND OR sample mean is less than or equal to the variance of importance sampling sample mean  Proof  The details of the proof are quite complicated and therefore we only provide the intuitions involved  As noted earlier the guiding principle of AND OR sample mean is to take advantage of conditional independence in the graphical model  Let us assume that we have three random variables X  Y and Z with the following relationship  X and Y are independent of each other given Z  similar to our example Bayesian network   The expression for variance derived here can be used in an induction step  induction is carried on the nodes of the pseudo tree  to prove the theorem  In this case  importance sampling generates samples   x    y    z              xN   yN   zN    along the order hZ  X  Yi and estimates the mean as follows  Ni   xi yi zi      N Without loss of generality  let  z    z    be the domain of Z and let these values be sampled N  and N  times respectively  We can rewrite Equation    as follows    IS  XYZ      IS  XYZ       N      N j zj  j    Ni   xi yi I z j   xi   yi   Nj        where I z j   xi   yi   is an indicator function which is   iff the partial assignment  z j   xi   yi   is generated in any of the N samples and   otherwise  AND OR sample mean is defined as   AO  XYZ       N      Njz j  j       Ni   xi I z j  xi   Nj      Ni   yi I z j  yi   Nj           where I x j   zi    and similarly I y j   zi    is an indicator function which equals   when one of the N samples contains the tuple  x j   zi    and similarly  y j   zi     and is   otherwise  By simple algebraic manipulations  we can prove that the variance of estimator  IS  XYZ  is given by  Var  IS  XYZ             z j Q zj    j      X z j   V  Y z j             Y z j   V  X z j    V  X z j  V  Y zj    N  XYZ  N           Similarly  the variance of AND OR sample mean is given by  Var  AO  XYZ             z j Q zj    j      X z j   V  Y z j      V  X z j  V  Y zj        N     Y z j   V  X z j      N  XYZ Nj          where   X z j   and V  X z j   are the conditional mean and variance respectively of X given Z   z j   Similarly    Y z j   and V  Y z j   are the conditional mean and variance respectively of Y given Z   z j   From Equations    and     if N j     for all j  then we can see that the Var  AO  XYZ     Var  IS  XYZ    However if N j      Var  AO  XYZ     Var  IS  XYZ    This proves that the variance of AND OR sample mean is less than or equal to the variance of conventional sample mean on this special case  As noted earlier using this case in induction over the nodes of a general pseudo tree completes the proof      Estimation in AND OR graphs  Next  we describe a more powerful algorithm for estimating mean in AND OR space by moving from AND OR trees to AND OR graphs as presented in  Dechter and Mateescu         An AND OR tree may contain nodes that root identical subtrees  When such unifiable nodes are merged  the tree becomes a graph and its size becomes smaller  Some unifiable nodes can be identified using contexts defined below  Definition      Context   Given a belief network and the corresponding AND OR search tree SAOT relative to a pseudo tree T   the context of any AND node hXi   xi i  SAOT   denoted by context Xi    is defined as the set of ancestors of Xi in T   that are connected to Xi and descendants of Xi   The context minimal AND OR graph is obtained by merging all the context unifiable AND nodes  The size of the largest context is bounded by the tree width w of the pseudo tree  Dechter and Mateescu         Therefore  the time and space complexity of a search algorithm traversing the context minimal AND OR graph is O exp w     Example      For illustration  consider the contextminimal graph in Figure   e  of the pseudo tree from Figure   c   Its size is far smaller that that of the AND OR tree from Figure   c      nodes vs     nodes   The contexts of the nodes can be read from the pseudo tree in Figure   b  as follows  context A     A   context B     B A   context C     C B A   context D     D C B  and context E     E A B   The main idea in AND OR graph estimation is to store all samples on an AND OR graph instead of an AND OR tree   Similar to an AND OR sample tree  we can define an identical notion of an AND OR sample graph  Definition       Arc labeled AND OR sample graph   Given a complete AND OR graph G and a set of samples S   an AND OR sample graph SAOG is obtained by removing all nodes and arcs not in S from G   The labels on SAOG are set similar to that of an AND OR sample tree  see Definition       Example      The bold edges and nodes in Figure   c  define an AND OR sample tree  The bold edges and nodes in Figure   d  define an AND OR sample graph corresponding to the same samples that define the AND OR sample tree in Figure   c   The algorithm for computing the sample mean on AND OR sample graphs is identical to the algorithm for AND ORtree  Steps      of Algorithm     The main reason in moving from trees to graphs is that the variance of the sample mean computed on an AND OR sample graph can be even smaller than that computed on an AND OR sample tree  More formally  T HEOREM      Let V  AOG    V  AOT   and V  IS   be the variance of AND OR sample mean on an AND OR sample graph  variance of AND OR sample mean on an AND OR sample tree and variance of sample mean of importance sampling respectively  Then given the same set of input samples  V  AOG    V  AOT    V  IS   We omit the proof due to lack of space  T HEOREM      Complexity of computing AND OR graph sample mean   Given a graphical model with n variables  a psuedo tree with treewidth w and N samples  the time complexity of AND OR graph sampling is O nNw   while its space complexity is O nN      Experimental Evaluation     Competing Algorithms The performance of importance sampling based algorithms is highly dependent on the proposal distribution  Cheng and Druzdzel         It was shown that computing the proposal distribution from the output of a Generalized Belief Propagation scheme of Iterative Join Graph Propagation  IJGP  yields better empirical performance than other available choices  Gogate and Dechter         Therefore  we use the output of IJGP to compute the proposal distribution Q  The complexity of IJGP is time and space exponential in its i bound  a parameter that bounds cluster sizes  We use a i bound of   in all our experiments  We experimented with three sampling algorithms for benchmarks which do not have determinism   a   pure  IJGP sampling   b  AND OR tree IJGP sampling and  c  AND OR graph IJGP sampling  Note that the underlying scheme for generating the samples is identical in all the       e       e        e       e        e       e        e     e        e       e                Grid Networks All Grid instances have      binary nodes and between      evidence nodes  From Figures   a  and   b   we can see that AND OR graph SampleSearch and AND OR tree SampleSearch are substantially better than pure SampleSearch  Linkage Networks The linkage instances are generated by converting a Pedigree to a Bayesian network  Fishelson and Geiger         These networks have between          nodes with a maximum domain size of     Note that it is hard to compute exact probability of evidence in these networks  Fishelson and Geiger         We observe from Figures   a   b   c  and  d  that AND ORgraph SampleSearch is substantially more accurate than AND OR tree SampleSearch which in turn is substantially more accurate than pure SampleSearch  Notice the logscale in Figures    a   d  which means that there is an order of magnitude difference between the errors  Our results suggest that AND OR graph and tree estimators yield far better performance than conventional estimators especially on problems in which the proposal distribution is a bad approximation of the posterior distribution                    Random BN       n    d     E        e        e       e        e     e        e       e        e       e        e       e                                         Time in Seconds IJGP Sampling AND OR Graph IJGP Sampling AND OR Tree IJGP Sampling Exact   b   Figure    Random Networks Grids BN     n      d    E       e     P e      e     e       e     e     e                                                      Time in Seconds SampleSearch AND OR Graph SampleSearch AND OR Tree SampleSearch Exact   a  Grids BN      n      d    E       P e   Random Networks From Figures   a  and   b   we see that AND OR graph sampling is better than AND OR tree sampling which in turn is better than pure IJGP sampling  However there is not much difference in the error because the proposal distribution seems to be a very good approximation of the posterior        a       Our results are presented in Figures      Each Figure shows approximate probability of evidence as a function of time  The bold line in each Figure indicates the exact probability of evidence  The reader can visualize the error from the distance between the approximate curves and the exact line  For lack of space  we show only part of our results  Each Figure shows the number of variables n  the maximum domain size d and the number of evidence nodes  E  for the respective benchmark       IJGP Sampling AND OR Graph IJGP Sampling AND OR Tree IJGP Sampling Exact  Results  We experimented with three sets of benchmark belief networks  a  Random networks   b  Linkage networks and  c  Grid networks  Note that only linkage and grid networks have zero probabilities on which we use SampleSearch The exact P e  for most instances is available from the UAI      competition web site       Time in Seconds  P e          Random BN      n    d     E     P e   methods  What changes is the method of accumulating the samples and deriving the estimates  On benchmarks which have zero probabilities or determinism  we use the SampleSearch scheme introduced by  Gogate and Dechter        to overcome the rejection problem  We experiment with the following versions of SampleSearch on deterministic networks   a  pure SampleSearch   b  AND OR tree SampleSearch and  c  AND OR graph SampleSearch      e     e       e     e       e     e       e     e       e     e                                                      Time in Seconds SampleSearch AND OR Graph SampleSearch AND OR Tree SampleSearch Exact   b   Figure    Grid Networks    Related Work and Summary The work presented in this paper is related to the work by  Hernndez and Moral        Kjrulff        Dawid et al         who perform sampling based inference on a junction tree  The main idea in these papers is to perform message passing on a junction tree by substituting messages which are too hard to compute exactly by their sampling based approx    Log P e   linkage instance n     d     E                                                                                                             Time in Seconds SampleSearch AND OR Graph SampleSearch AND OR Tree SampleSearch Exact   a   Log P e   linkage instance n      d     E                                                                                                         Time in Seconds SampleSearch AND OR Graph SampleSearch AND OR Tree SampleSearch Exact  linkage instance n      d     E       This work was supported in part by the NSF under award numbers IIS          IIS         and IIS         and the NIH grant R   HG                       Log P e   To summarize  the paper introduces a new sampling based estimation technique called AND OR importance sampling  The main idea of our new scheme is to derive statistics on the generated samples by using an AND OR tree or graph that takes advantage of the independencies present in the graphical model  We proved that the sample mean computed on an AND OR tree or graph may have smaller variance than the sample mean computed using the conventional approach  Our experimental evaluation is preliminary but quite promising showing that on most instances AND OR sample mean has lower error than importance sampling and sometimes by significant margins  Acknowledgements   b                        
 Many representation schemes combining firstorder logic and probability have been proposed in recent years  Progress in unifying logical and probabilistic inference has been slower  Existing methods are mainly variants of lifted variable elimination and belief propagation  neither of which take logical structure into account  We propose the first method that has the full power of both graphical model inference and first order theorem proving  in finite domains with Herbrand interpretations   We first define probabilistic theorem proving  their generalization  as the problem of computing the probability of a logical formula given the probabilities or weights of a set of formulas  We then show how this can be reduced to the problem of lifted weighted model counting  and develop an efficient algorithm for the latter  We prove the correctness of this algorithm  investigate its properties  and show how it generalizes previous approaches  Experiments show that it greatly outperforms lifted variable elimination when logical structure is present  Finally  we propose an algorithm for approximate probabilistic theorem proving  and show that it can greatly outperform lifted belief propagation      INTRODUCTION  Unifying first order logic and probability enables uncertain reasoning over domains with complex relational structure  and is a long standing goal of AI  Proposals go back to at least Nilsson       with substantial progress within the UAI community starting in the     s  e g                 and added impetus from the new field of statistical relational learning starting in the     s       Many well developed representations now exist  e g                 but the state of inference is less advanced  For the most part  inference is still carried out by converting models to propositional form  e g   Bayesian networks  and then applying standard propositional algorithms  This typically incurs an exponen   tial blowup in the time and space cost of inference  and forgoes one of the chief attractions of first order logic  the ability to perform lifted inference  i e   reason over large domains in time independent of the number of objects they contain  using techniques like resolution theorem proving       In recent years  progress in lifted probabilistic inference has picked up  An algorithm for lifted variable elimination was proposed by Poole      and extended by de Salvo Braz      and others  Lifted belief propagation was introduced by Singla and Domingos      and extended by others  e g              These algorithms often yield impressive efficiency gains compared to propositionalization  but still fall well short of the capabilities of first order theorem proving  because they ignore logical structure  treating potentials as black boxes  This paper proposes the first full blown probabilistic theorem prover  capable of exploiting both lifting and logical structure  and having standard theorem proving and standard graphical model inference as special cases  Our solution is obtained by reducing probabilistic theorem proving  PTP  to lifted weighted model counting  We first do the corresponding reduction for the propositional case  extending previous work by Darwiche     and Sang et al        see also       We then lift this approach to the firstorder level  and refine it in several ways  We show that our algorithm can be exponentially more efficient than firstorder variable elimination  and is never less efficient  up to constants   For domains where exact inference is not feasible  we propose a sampling based approximate version of our algorithm  Finally  we report experiments in which PTP greatly outperforms first order variable elimination and belief propagation  and discuss future research directions      LOGIC AND THEOREM PROVING  We begin with a brief review of propositional logic  firstorder logic and theorem proving       The simplest formulas in propositional logic are atoms  individual symbols representing propositions that may be true of false in a given world  More complex formulas are recursively built up from atoms and the logical connectives   negation     conjunction     disjunction     implication  and    Algorithm   TP KB K  query Q  KQ  K   Q  return SAT CNF KQ      equivalence   For example  A   B  C  is true iff A is false or B and C are true  A knowledge base  KB  is a set of logical formulas  The fundamental problem in logic is determining entailment  and algorithms that do this are called theorem provers  A knowledge base K entails a query formula Q iff Q is true in all worlds where all the formulas in K are true  a world being an assignment of truth values to all atoms  A world is a model of a KB iff the KB is true in it  Theorem provers typically first convert K and Q to conjunctive normal form  CNF   A CNF formula is a conjunction of clauses  each of which is a disjunction of literals  each of which is an atom or its negation  For example  the CNF of A   B  C  is  A  B    A  C   A unit clause consists of a single literal  Entailment can then be computed by adding Q to K and determining whether the resulting KB KQ is satisfiable  i e   whether there exists a world where all clauses in KQ are true  If not  KQ is unsatisfiable  and K entails Q  Algorithm   shows this basic theorem proving schema  CNF K  converts K to CNF  and SAT C  returns True if C is satisfiable and False otherwise  The earliest theorem prover is the Davis Putnam algorithm  henceforth called DP       It makes use of the resolution rule  if a KB contains the clauses A          An and B          Bm   where the as and bs represent literals  and some literal Ai is the negation of some literal Bj   then the clause A          Ai   Ai           An  B          Bj   Bj           Bm can be added to the KB  For each atom A in the KB  DP resolves every pair of clauses C    C  in KB such that C  contains A and C  contains A  adds the result to the KB  and deletes C  and C    If at some point the empty clause is derived  the KB is unsatisfiable  and the query formula  previously negated and added to the KB  is therefore proven  As Dechter      points out  DP is in fact just the variable elimination algorithm for the special case of     potentials  Modern propositional theorem provers use the DPLL algorithm      a variant of DP that replaces the elimination step with a splitting step  instead of eliminating all clauses containing the chosen atom A  resolve all clauses in the KB with A  simplify and recurse  and do the same with A  If both recursions fail  the KB is unsatisfiable  DPLL has linear space complexity  compared to exponential for DavisPutnam  DPLL is the basis of the algorithms in this paper  First order logic inherits all the features of propositional logic  and in addition allows atoms to have internal structure  An atom is now a predicate symbol  representing a relation in the domain of interest  followed by a parenthesized list of variables and or constants  representing objects  For example  Friends Anna  x  is an atom  A ground atom has only constants as arguments  First order logic has two additional connectives    universal quan   tification  and   existential quantification   For example  x Friends Anna  x  means that Anna is friends with everyone  and x Friends Anna  x  means that Anna has at least one friend  In this paper  we assume that domains are finite  and therefore function free  and that there is a oneto one mapping between constants and objects in the domain  Herbrand interpretations   As long as the domain is finite  first order theorem proving can be carried out by propositionalization  creating atoms from all possible combinations of predicates and constants  and applying a propositional theorem prover  However  this is potentially very inefficient  A more sophisticated alternative is first order resolution       which proceeds by resolving pairs of clauses and adding the result to the KB until the empty clause is derived  Two first order clauses can be resolved if they contain complementary literals that unify  i e   there is a substitution of variables by constants or other variables that makes the two literals identical up to the negation sign  Conversion to CNF is carried out as before  with the additional step of removing all existential quantifiers by a process called skolemization  First order logic allows knowledge to be expressed vastly more concisely than propositional logic  For example  the rules of chess can be stated in a few pages in first order logic  but require hundreds of thousands in propositional logic  Probabilistic logical languages extend this power to uncertain domains  The goal of this paper is to similarly extend the power of first order theorem proving      PROBLEM DEFINITION  Following Nilsson       we define probabilistic theorem proving as the problem of determining the probability of an arbitrary query formula Q given a set of logical formulas Fi and their probabilities P  Fi    For the problem to be well defined  the probabilities must be consistent  and Nilsson      provides a method for verifying consistency  Probabilities estimated by maximum likelihood from an observed world are guaranteed to be consistent       In general  a set of formula probabilities does not specify a complete joint distribution over the atoms appearing in them  but one can be obtained by making the maximum entropy assumption  the distribution contains no information beyond that specified by the formula probabilities       Finding the maximum entropy distribution given a set of formula probabilities is equivalent to learning a maximum likelihood loglinear model whose features are the formulas  many algorithms for this purpose are available  iterative scaling  gradient descent  etc         We call a set of formulas and their probabilities together with the maximum entropy assumption a probabilistic knowledge base  PKB   Equivalently  a PKB can be directly defined as a log linear model with the formulas as features and the corresponding weights or potential values  Potentials are the most convenient form  since they allow determinism      probabilities  without recourse to infinity  If x   LWSAT  Lifted  TP    PTP   LWMC  LMC  MPE   WSAT ted igh e W Counting  PI   WMC  TP     SAT MC Figure    Inference problems addressed in this paper  TPo and TP  is propositional and first order theorem proving respectively  PI is probabilistic inference  computing marginals   MPE is computing the most probable explanation  SAT is satisfiability  MC is model counting  W is weighted and L is lifted  A   B means A can be reduced to B   is a world and i  x  is the potential corresponding to formula Fi   by convention  and without loss of generality  we let i  x      if Fi is true  and i  x    i    if the formula is false  Hard formulas have i     and soft formulas have i      In order to compactly subsume standard probabilistic models  we interpret a universally quantified formula as a set of features  one for each grounding of the formula  as in Markov logic       A PKB   Fi   i    thus represents the joint distribution   Y ni  x  P  X   x           Z i i where ni  x  is the number of false groundings of Fi in x  and Z is a normalization constant  the partition function   We can now define PTP succinctly as follows  Probabilistic theorem proving  PTP  Input  Probabilistic KB K and query formula Q Output  P  Q K  If all formulas are hard  a PKB reduces to a standard logical KB  Determining whether a KB K logically entails a query Q is equivalent to determining whether P  Q K            Graphical models are easily converted into equivalent PKBs      Conditioning on evidence is done by adding the corresponding hard ground atoms to the PKB  and the conditional marginal of an atom is computed by issuing the atom as the query  Thus PTP has both logical theorem proving and inference in graphical models as special cases  In this paper we solve PTP by reducing it to lifted weighted model counting  Model counting is the problem of determining the number of worlds that satisfy a KB  Weighted model counting can be defined as follows      Assign a weight to each literal  and let the weight of a world be the product of the weights of the literals that are true in it  Then weighted model counting is the problem of determining the sum of the weights of the worlds that satisfy a KB   Algorithm   WCNF PKB K  for all  Fi   i    K s t  i     do K  K    Fi  Ai           Fi   i    C  CNF K  for all Ai literals do WAi  i for all other literals L do WL    return  C  W    Algorithm   PTP PKB K  query Q  KQ  K    Q      return WMC WCNF KQ    WMC WCNF K    addressed by this paper  Generality increases in the direction of the arrows  We first propose an algorithm for propositional weighted model counting and then lift it to first order  The resulting algorithm is applicable to all the problems in the diagram   Weighted SAT MPE inference requires replacing sums with maxes and an additional traceback step  but we do not pursue this here  cf  Park       and de Salvo Braz      on the lifted case       PROPOSITIONAL CASE  This section generalizes the Bayesian network inference techniques in Darwiche     and Sang et al       to arbitrary propositional PKBs  evidence  and query formulas  Although this is of value in its own right  its main purpose is to lay the groundwork for the first order case  The probability of a formula is the sum of the probabilities of the worlds that satisfy it  Thus to compute the probability of a formula Q given a PKB K it suffices to compute the partition function of K with and without Q added as a hard formula  P Q Z K    Q       x  Q  x  i i  x  P  Q K      Z K  Z K      where  Q  x  is the indicator function    if Q is true in x and   otherwise   In turn  the computation of partition functions can be reduced to weighted model counting using the procedure in Algorithm    This replaces each soft formula Fi in K by a corresponding hard formula Fi  Ai   where Ai is a new atom  and assigns to every Ai literal a weight of i  the value of the potential i when Fi is false   Theorem   Z K    WMC WCNF K     Weighted model counting  WMC  Input  CNF C and set of literal weights W Output  Sum of weights of worlds that satisfy C  Proof  If a world violates any of the hard clauses in K or any of the Fi  Ai equivalences  it does not satisfy C and is therefore not counted  The weight of each remaining world x is the product of the weights of the literals that are true in x  By the Fi  Ai equivalences and the weights assigned Q by WCNF K   this is i i  x    Recall that i  x      if Fi is true in x and i  x    i otherwise   Thus xs weight is the unnormalized probability of x under K  Summing these yields the partition function Z K      Figure   depicts graphically the set of inference problems  Equation   and Theorem   lead to Algorithm   for PTP    Algorithm   WMC CNF C  weights W    Algorithm   LWMC CNF C  substs  S  weights W       Base case if all clauses Q in C are satisfied then return AA C   WA   WA   if C has an empty unsatisfied clause then return      Decomposition step if C can be partitioned into CNFs C            Ck sharing no atoms then Qk return i   WMC Ci   W      Splitting step Choose an atom A return WA WMC C A  W     WA WMC C A  W       Lifted base case if all clauses Q in C are satisfied then return AA C   WA   WA  nA  S  if C has an empty unsatisfied clause then return      Lifted decomposition step if there exists a lifted decomposition  C              C  m            Ck   Q k        Ck mk   of C under S then return i    LWMC Ci     S  W   mi    Lifted splitting step Choose an atom A      l  Let  A S           A S   be a lifted split of A for C under S  Pl   Compare with Algorithm     WMC C  W   can be any weighted model counting algorithm      Most model counters are variations of Relsat  itself an extension of DPLL      Relsat splits on atoms until the CNF is decomposed into sub CNFs sharing no atoms  and recurses on each subCNF  This decomposition is crucial to the efficiency of the algorithm  In this paper we use a weighted version of Relsat  shown in Algorithm    A C  is the set of atoms that appear in C  C A denotes the CNF obtained by resolving each clause in C with A  which results in removing A from all clauses it appears in  and setting to Satisfied all clauses in which A is true  Notice that  unlike in DPLL  satisfied clauses cannot simply be deleted  because we need to keep track of which atoms they are over for model counting purposes  However  they can be ignored in the decomposition step  since they introduce no dependencies  The atom to split on in the splitting step can be chosen using various heuristics       Theorem   Algorithm WMC C W   correctly computes the weighted model count of CNF C under literal weights W   Proof sketch  If all clauses in C are satisfied  all assignments to the atoms Q in C satisfy it  and the corresponding total weight is AA C   WA   WA    If C has an empty unsatisfied clause  it is unsatisfiable given the truth assignment so far  and the corresponding weighted count is    If two CNFs share no atoms  the WMC of their conjunction is the product of the WMCs of the individual CNFs  Splitting on an atom produces two disjoint sets of worlds  and the total WMC is therefore the sum of the WMCs of the two sets  weighted by the corresponding literals weight        FIRST ORDER CASE  We now lift PTP to the first order level  We consider first the case of PKBs without existential quantifiers  Algorithms   and   remain essentially unchanged  except that formulas  literals and CNF conversion are now first order  In particular  for Theorem   to remain true  each new atom Ai in Algorithm   must now consist of a new predicate symbol followed by a parenthesized list of the variables and constants in the corresponding formula Fi   The proof of the first order version of the theorem then follows by propositionalization  Lifting Algorithm   is the focus of the rest of this section   fi LWMC C j   Sj   W   return i   ni WAti WA where ni   ti   fi   j and Sj are as in Proposition    We begin with some necessary definitions  A substitution constraint is an expression of the form x   y or x    y  where x is a variable and y is either a variable or a constant   Much richer substitution constraint languages are possible  but we adopt the simplest one that allows PTP to subsume both standard function free theorem proving and first order variable elimination   Two literals are unifiable under a set of substitution constraints S if there exists at least one ground literal consistent with S that is an instance of both  up to the negation sign  A  C  S  pair  where C is a first order CNF whose variables have been standardized apart and S is a set of substitution constraints  represents the ground CNF obtained by replacing each clause in C with the conjunction of its groundings that are consistent with the constraints in S  For example  using upper case for constants and lower case for variables  if C   R B  C    R x  y   S y  z   and S    x   y  z    B    C  S  represents the ground CNF R B  C  R B  B S B  C   R C  C S C  C    Clauses with equality substitution constraints can be abbreviated in the obvious way  e g   T x  y  z  with x   y and z   C can be abbreviated as T x  x  C    We lift the base case  decomposition step  and splitting step of Algorithm   in turn  The result is shown in Algorithm    In addition to the first order CNF C and weights on firstorder literals W   LWMC takes as an argument an initially empty set of substitution constraints S which  similar to logical theorem proving  is extended along each branch of the inference as the algorithm progresses       LIFTING THE BASE CASE  The base case changes only by raising each first order atom As sum of weights to nA  S   the number of groundings of A compatible with the constraints in S  This is necessary and sufficient since each atom A has nA  S  groundings  and all ground atoms are independent because the CNF is satisfied irrespective of their truth values  Note that nA  S  is the number of groundings of A consistent with S that can be formed using all the constants in the original CNF         LIFTING THE DECOMPOSITION STEP  Clearly  if C can be decomposed into two or more CNFs such that no two CNFs share any unifiable literals  a lifted decomposition of C is possible  i e   a decomposition of C into first order CNFs on which LWMC can be called recursively   But the symmetry of the first order representation can be further exploited  For example  if the CNF C can be decomposed into k CNFs C            Ck sharing no unifiable literals and such that for all i  j  Ci is identical to Cj up to a renaming of the variables and constants   then LWMC C     LWMC C    k   We formalize these conditions below  Definition   The set of first order CNFs  C              C  m            Ck             Ck mk   is called a lifted decomposition of CNF C under substitution constraints S if  given S  it satisfies the following three properties   i  C   C            Ck mk    ii  no two Ci j s share any unifiable literals   iii  for all i  j  j     such that j    j     Ci j is identical to Ci j   Proposition   If  C              C  m            Ck             Ck mk   is a lifted decomposition of C under S  then k Y LWMC C  S  W      LWMC Ci     S  W   mi       i    Rules for identifying lifted decompositions can be derived in a straightforward manner from the inversion argument in de Salvo Braz      and the power rule in Jha et al        An example of such a rule is given in the definition and proposition below  Note that this rule is more general than de Salvo Brazs inversion elimination       Definition   A set of variables X    x            xm   is called a decomposer of a CNF C if it has the following three properties   i  X is the union of all variables appearing as the same argument of a predicate R in C   ii  every xi in X appears in all atoms of a clause in C   iii  if xi and xj appear as arguments of a predicate R    they must appear as the same argument of R     R  may or may not be the same as R   For example   x    x    is a decomposer of the CNF  R x    S x    x       R x     T x    x      Given a decomposer  x            xm   and a CNF C  it is easy to see that for every pair of substitutions of the form SX    x    X          xm   X  and SY    x    Y          xm   Y   with X    Y  the CNFs CX and CY obtained by applying SX and SY to C do not share any unifiable literals  A decomposer thus yields a lifted decomposition  Given a CNF  a decomposer can be found in linear time  When there are no substitution constraints on the variables in the decomposer  as in the example above  all CNFs formed by substituting the variables in the decomposer with a constant are identical  Thus  k     in Equation   and m  equals the number of constants  objects  in the   Henceforth  when we say that two clauses are identical  we mean that they are identical up to a renaming of constants and variables   PKB  However  when there are substitution constraints  the CNFs may not be identical  For example  given the CNF  R x     S x    x       R x     T x    x     and substitution constraints  x     A  x     B   the CNF formed by substituting  x    A  x    B  is not identical to the CNF formed by substituting  x    C  x    C   Intuitively  if all the clauses in the CNF have the same set of groundings relative to the decomposer  then any two CNFs formed by substituting the variables in the decomposer with any two  valid  distinct constants will be identical  Thus  we need to split the CNF into disjoint CNFs that have identical groundings relative to the decomposer  We can achieve this by considering all possible combinations of the substitution constraints  For instance  we can decompose our example CNF into the following four CNFs  each of which has an identical set of groundings relative to x  and x   for readability  we do not standardize variables apart and show the constraints separately for each CNF        R x     S x    x       R x     T x    x       x     A  x     B  x     A  x     B        R x     S x    x       R x     T x    x       x     A  x    B  x     A  x    B        R x     S x    x       R x     T x    x       x    A  x    A  x     B  x     B   and      R x     S x    x       R x     T x    x       x    A  x    B  x    A  x    B   Notice that the fourth CNF has no valid groundings and can be removed  In general  a CNF can be partitioned into subsets of identical but disjoint CNFs using constraint satisfaction techniques  as in Kisynski and Poole       In summary  Proposition   Let X    x            xt   be a decomposer of C  Let   X              X  m              Xk             Xk mk    be a partition of the constants in the domain and let C      CX              CX  m            CXk             CXk mk   be a partition of C such that  i  for all i  j  j   such that j    j     CXi j is identical to CXi j    and  ii  CXi mi is a CNF formed by substituting each variable in X by the constant Xi mi   Then C   is a lifted decomposition of C under S       LIFTING THE SPLITTING STEP  Splitting on a non ground atom means splitting on all groundings of it consistent with the current substitution constraints S  Naively  if the atom has c groundings consistent with S this will lead to a sum of  c recursive calls to LWMC  one for each possible truth assignment to the c ground atoms  However  in general these calls will have repeated structure and can be replaced by a much smaller number  The lifted splitting step exploits this  We introduce some notation and definitions  Let A S denote a truth assignment to the groundings of atom A consistent with substitution constraints S  and let A S denote the set of all possible such assignments  Let C A S denote the CNF formed by removing A from all clauses it appears in  and setting to Satisfied all ground clauses that are satisfied because of A S   This can be done in a lifted manner by updating the substitution constraints asso    ciated with each clause  For instance  consider the clause R x   S x  y  and substitution constraint  x    A   and suppose the domain is  A  B  C   i e   these are all the constants appearing in the PKB   Given the assignment R A    False  R B    True  R C    False and ignoring satisfied clauses  the clause becomes S x  y  and the constraint set becomes  x    A  x    B   R x  is removed from the clause because all of its groundings are instantiated  The constraint x    B is added because the assignment R B    True satisfies all groundings in which x   B        l   Definition   The partition  A S           A S   of A S is called a lifted split of atom A for CNF C under substitu i  tion constraints S if every part A S satisfies the following  i   two properties   i  all truth assignments in A S have the  i   same number of true atoms   ii  for all pairs j   k  A S   C j is identical to C k         l   Proposition   If  A S           A S   is a lifted split of A for C under S  then l X fi ni WAti WA LWMC C j   Sj   W   LWMC C  S  W     i    i    i   where ni    A S    j  A S   ti and fi are the number of true and false atoms in j   respectively  and Sj is S augmented with the substitution constraints required to form C j   Again  we can derive rules for identifying a lifted split by using the counting arguments in de Salvo Braz      and the generalized binomial rule in Jha et al        We omit the details for lack of space  In the worst case  lifted splitting defaults to splitting on a ground atom  In most inference problems  the PKB will contain many hard ground unit clauses  the evidence   Splitting on the corresponding ground atoms then reduces to a single recursive call to LWMC for each atom  In general  the atom to split on in Algorithm   should be chosen with the goal of yielding lifted decompositions in the recursive calls  for example  using lifted versions of the propositional heuristics        Notice that the lifting schemes used for decomposition and splitting in Algorithm   by no means exhaust the space of possible probabilistic lifting rules  For example  Jha et al       and Milch et al       contain examples of other lifting rules  Searching for new probabilistic lifted inference rules  and positive and negative theoretical results about what can be lifted  looks like a fertile area for future research  The theorem below follows from Theorem   and the arguments above  Theorem   Algorithm LWMC C    W   correctly computes the weighted model count of CNF C under literal weights W        EXTENSIONS  Although most probabilistic logical languages do not include existential quantification  handling it in PTP is desirable for the sake of logical completeness  This is compli   cated by the fact that skolemization is not sound for model counting  skolemization will not change satisfiability but can change the model count   and so cannot be applied  The result of conversion to CNF is now a conjunction of clauses with universally and or existentially quantified variables  e g    xy  R x  y   S y      uvw T u  v  w     Algorithm   now needs to be able to handle clauses of this form  If no universal quantifier appears nested inside an existential one  this is straightforward  since in this case an existentially quantified clause is just a compact representation of a longer one  For example  if the domain is  A  B  C   the unit clause xy R x  y  represents the clause x  R x  A   R x  B   R x  C    The decomposition and splitting steps in Algorithm   are both easily extended to handle such clauses without loss of lifting  and the base case does not change   However  if universals appear inside existentials  a first order clause now corresponds to a disjunction of conjunctions of propositional clauses  For example  if the domain is  A  B   xy  R x  y   S x  y   represents  R A  A S A  A   R A  B S A  B   R B  A  S B  A     R B  B   S B  B    Whether these cases can be handled without loss of lifting remains an open question  Several optimizations of the basic LWMC procedure in Algorithm   can be readily ported from the algorithms PTP generalizes  These optimizations can tremendously improve the performance of LWMC  Unit Propagation When LWMC splits on atom A  the clauses in the current CNF are resolved with the unit clauses A and A  This results in deleting false atoms  which may produce new unit clauses  The idea in unit propagation is to in turn resolve all clauses in the new CNF with all the new unit clauses  and continue to do this until no further unit resolutions are possible  This often produces a much smaller CNF  and is a key component of DPLL that can also be used in LWMC  Other techniques used in propositional inference that can be ported to LWMC include pure literals  clause learning  clause indexing  and random restarts             Caching Memoization In a typical inference  LWMC will be called many times on the same subproblems  The solutions of these can be cached when they are computed  and reused when they are encountered again   Notice that a cache hit only requires identity up to renaming of variables and constants   This can greatly reduce the time complexity of LWMC  but at the cost of increased space complexity  If the results of all calls to LWMC are cached  full caching   in the worst case LWMC will use exponential space  In practice  we can limit the cache size to the available memory and heuristically prune elements from it when it is full  Thus  by changing the cache size  LWMC can explore various time space tradeoffs  Caching in LWMC corresponds to both caching in model counting      and recursive conditioning     and to memoization of common subproofs in theorem proving       Knowledge Based Model Construction KBMC first uses logical inference to select the subset of the PKB that is rel    evant to the query  and then propositionalizes the result and performs standard probabilistic inference on it       A similar effect can be obtained in PTP by noticing that in Equation   factors that are common to Z K    Q       and Z K  cancel out and do not need to be computed  Thus we can modify Algorithm   as follows   i  simplify the PKB by unit propagation starting from evidence atoms  etc    ii  drop from the PKB all formulas that have no path of unifiable literals to the query   iii  pass to LWMC only the remaining formulas  with an initial S containing the substitutions required for the unifications along the connecting path s        THEORETICAL PROPERTIES  We now theoretically compare the efficiency of PTP and first order variable elimination  FOVE            Theorem   PTP can be exponentially more efficient than FOVE  Proof sketch  We provide a constructive proof  Consider the hard CNF  R   x     R   x    x     R   x    x       R   x     R   x    x     R   x    x       R   x      Neither counting elimination      nor inversion elimination      is applicable here  and therefore the complexity of FOVE will be the same as that of  propositional  variable elimination  i e   exponential in the treewidth  The treewidth of the CNF is polynomial in the domain size  number of constants   and therefore variable elimination and by extension FOVE will require exponential time  On the other hand  PTP will solve this problem in polynomial time  Since R   x    is a unit clause  PTP will remove the first clause because it is satisfied  clause deletion   It will then remove R  from the second clause  unit propagation   yielding the hard CNF R   x    x     R   x    x     PTP will solve this reduced CNF by first running lifted decomposition  x  is a decomposer  followed by two lifted splits over R   A  x    and R   A  x     Thus  the overall time complexity of PTP is polynomial in the domain size    Theorem   LWMC with full caching has the same worstcase time and space complexity as FOVE  The proof of Theorem   is a little involved  The main insight for this result comes from previous work on recursive conditioning     and AND OR search       Specifically  these papers show that the worst case time and space complexity of propositional WMC with caching  and without unit propagation and clause deletion  is the same as that of variable elimination  VE   exponential in the treewidth   Specifically the authors show that both WMC and VE are graph traversal schemes that operate by traversing the same graph in a top down and bottom up manner respectively  Lifting can be seen as a way of compressing this graph via Propositions   and    specifically by aggregating nodes in the graph that behave similarly  Since FOVE is a lifted analog of VE  it traverses the compressed lifted graph in a bottom up manner while LWMC with caching traverses it in a top down manner  assuming that they use the same  rules for lifting  Since the lifting rules used by LWMC are at least as general as FOVE  its worst case time and space complexity is the same as FOVE  De Salvo Brazs FOVE      and lifted BP      completely shatter the PKB in advance  This may be wasteful because many of those splits may not be necessary  Like Poole      and Ng et al        LWMC splits only as needed       DISCUSSION  PTP yields new algorithms for several of the inference problems in Figure    For example  ignoring weights and replacing products by conjunctions and sums by disjunctions in Algorithm   yields a lifted version of DPLL for first order theorem proving  cf        Of the standard methods for inference in graphical models  propositional PTP is most similar to recursive conditioning     and AND OR search      with context sensitive decomposition and caching  but applies to arbitrary PKBs  not just Bayesian networks  Also  PTP effectively performs formula based inference      when it splits on one of the auxiliary atoms introduced by Algorithm    PTP realizes some of the benefits of lazy inference for relational models      by keeping in lifted form what lazy inference would leave as default      APPROXIMATE INFERENCE  LWMC lends itself readily to Monte Carlo approximation  by replacing the sum in the splitting step with a random choice of one of its terms  calling the algorithm many times  and averaging the results  This yields the first lifted sampling algorithm  We first apply this importance sampling approach      to WMC  yielding the MC WMC algorithm  The two algorithms differ only in the last line  Let Q A C  W   denote the importance or proposal distribution over A given the current CNF C and literal weights W   Then WA we return Q A C W   MC WMC C A  W   with probability WA Q A C  W    or Q A C W   MC WMC C A  W   otherwise  By importance sampling theory      and by the law of total expectation  it is easy to show that   Theorem   If Q A C  W   satisfies WMC C A  W        Q A C  W       for all atoms A and its true and false assignments  then the expected value of the quantity output by MC WMC C  W   equals WMC C  W    In other words  MC WMC C  W   yields an unbiased estimate of WMC C  W    An estimate of WMC C  W   is obtained by running MC WMC C  W   multiple times and averaging the results  By linearity of expectation  the running average is also unbiased  It is well known that the accuracy of the estimate is inversely proportional to its variance       The variance can be reduced by either running MC WMC more times or by choosing Q that is as close as possible to the posterior    Objs           Clause PTP FOVE PTP FOVE PTP FOVE Size                               X        X      X       X       X      X      X       X     X      X  distribution P  or both   Thus  for MC WMC to be effective in practice  at each point  given the current CNF C  we should select Q A C  W   that is as close as possible to the marginal probability distribution of A w r t  C and W   The following simple procedure can be used to construct the proposal distribution Q  Let A be an atom that needs to be sampled and  abusing notation  let o    A            An   be an ordering of its ground atoms  we select the ordering randomly   Given a truth assignment to the previous i    atoms  let ni t and ni f denote the number of ground clauses that are satisfied by assigning Ai to true and false respectively  Then  we use Q Ai  A            Ai    C  W     ni t WA   ni t WA   ni f WA    Thus we perform only a one step look ahead for constructing Q  In future  we envision using more sophisticated heuristics  MC WMC suffers from the rejection problem       it may return a zero  We can solve this problem by either backtracking when a sample is rejected or by generating samples from the backtrack free distribution       Next  we present a lifted version of MC WMC  which is obtained by replacing the  last line of the  lifted splitting step in LWMC by the following lifted sampling step  return  fi t ni WAi WA  i  Q A S    MC LWMC C j   Sj   W    where ni   ti   fi   j and Sj are as in Proposition    In the lifted sampling step  we construct a distribution Q  i  over the lifted split and sample an element A S from it  Then we weigh the sampled element w r t  Q and call the al i  gorithm recursively on the CNF conditioned on j  A S   Notice that A is a first order atom and the distribution  i  Q A S   is defined in a lifted manner  However  seman i   tically  each A S represents all of groundings of A and  i   therefore given a ground assignment j  A S   the prob i   ability of sampling j is QG  j     Q A S   ni   Thus  ignoring the decomposition step  MC LWMC is equivalent to MC WMC that uses QG to sample all the groundings of A  In the decomposition step  given a set of identical and disjoint CNFs  we simply sample just one of the CNFs and raise our estimate to the appropriate count  The correctness of this step follows from the fact that the expected value of the product of k identical and independent random variables R            Rk equals E R   k   and  R   k is an unbiased estimate of E R   k where R  is a random sample of R    Therefore  the following theorem immediately follows from Theorem     i   Theorem   If Q A S   satisfies WMC C j   Sj   W      i  Q A S     i  A S        for all elements of the lifted split of A for C under S  then MC LWMC C  S  W   yields an unbiased estimate of WMC C  W    MC LWMC has smaller variance than MC WMC and is therefore likely to have higher accuracy  The smaller variance is due to the smaller time complexity of MC LWMC   Table    Impact of increasing the number of objects and clause size on the time complexity of FOVE and PTP  Time is in seconds  X indicates that the algorithm ran out of memory  which in turn is due to the decomposition step  Recall that we group identical and independent CNFs  sample just one CNF from the group  and raise the estimate by the number of members in the group  Thus  for each lifted decomposition of size mi      we have a factor of mi speedup  Therefore  given a specific time bound  the estimate returned by MC LWMC will be based on a larger sample size  or more runs  than the one returned by MC WMC          EXPERIMENTS EXACT INFERENCE  In this subsection  we compare the performance of PTP and FOVE on randomly generated and link prediction PKBs  We implemented PTP in C   and ran all our experiments on a Linux machine with a      GHz Intel Xeon processor and  GB of RAM  We used a constraint solver based on forward checking to implement the substitution constraints  We used the following heuristics for splitting  At any point  we prefer an atom which yields the smallest number of recursive calls to LWMC  i e   an atom that yields maximum lifting   We break ties by selecting an atom that appears in the largest number of ground clauses  this number can be computed using the constraint solver  If it is the same for two or more atoms  we break ties randomly  Random PKBs with Varying Clause Size In the first set of experiments  we show that PTPs advantage relative to FOVE increases with clause length  In order to compare the performance in a controlled setting  we generated random PKBs parameterized by five integer constants  n  m  s  e and c  where n is the number of predicates  m is the number of clauses  s is the number of literals in each clause  e is the number of evidence atoms  and c is the number of constants in the domain  The PKB is generated as follows  All predicates are unary  We generate m clauses by randomly selecting s predicates and negating each with probability      We then choose e ground atoms as evidence  each of which is set to either True or False with equal probability  We set n   m       varied s from   to   in increments of   and c from    to     and set e   c     Table   shows the impact of increasing the number of objects c and the clause size s on the time complexity of FOVE and PTP  The results are averaged over    PKBs  We can see that PTP always dominates FOVE  When the PKB has small clauses  PTP is only slightly better than FOVE  However  when the clauses are large  PTP is substantially better than FOVE  which runs   Negative log likelihood  Time  seconds                                        PTP FOVE                    Lifted BP MC SAT MC WMC MC LWMC                                              Percentage of evidence objects  a  Negative log likelihood  Time  seconds           PTP FOVE                                                   Link Prediction We experimented with a simple PKB consisting of two clauses  GoodProf x   GoodStudent y   Advises x  y   FutureProf y  and Coauthor x  y   Advises x  y   The PKB has two types of objects  professors  x  and students  y   Given data on a subset of papers and goodness of professors and students  the task is to be predict who advises whom and who is likely to be a professor in the future  We evaluated the performance of FOVE and PTP along two dimensions   i  the number of objects and  ii  the amount of evidence  We varied the number of objects from    to      and the number of evidence atoms from     to      Figure   a  shows the impact of increasing the number of evidence atoms on the performance of the two algorithms on a link prediction PKB with     objects  FOVE runs out of memory  typically after around    minutes of run time  after the percentage of evidence atoms rises above      PTP solves all the problems and is also much faster than FOVE  notice the log scale on the y axis   Figure   b  shows the impact of increasing the number of objects on a link prediction PKB with     of the atoms set as observed  We can see that FOVE is unable to solve any problems after the number of objects is increased beyond     because it runs out of memory  PTP  on the other hand  solves all                         Lifted BP MC SAT MC WMC MC LWMC                             Time  minutes   b    b   out of memory on all the instances  typically after around    minutes of run time  When large clauses are present  unit propagation is very effective and causes a large amount of pruning  Because of this  PTP is much faster than FOVE             Number of objects  time complexity of FOVE and PTP in the link prediction domain  The number of objects in the domain is       b  Impact of increasing the number of objects on the time complexity of FOVE and PTP in the link prediction domain  with     of the atoms set as evidence                      Figure     a  Impact of increasing the amount of evidence on the      Time  minutes   a   Figure    Negative log likelihood of the data as a function of time for lifted BP  MC SAT  MC WMC and MC LWMC on  a  the entity resolution  Cora  and  b  the collective classification domains   problems in less than    s       APPROXIMATE INFERENCE  In this subsection  we compare the performance of MCLWMC  MC WMC  lifted belief propagation       and MC SAT      on two domains  We used the entity resolution  Cora  and collective classification datasets and Markov logic networks used in Singla and Domingos      and Poon and Domingos      respectively  The Cora dataset contains      citations to     different research papers  The inference task here is to detect duplicate citations  authors  titles and venues  The collective classification dataset consists of about      query atoms  Since computing the exact posterior marginals is infeasible in these domains  we used the following evaluation method  We partitioned the data into two equal sized sets  evidence set and test set  We then computed the probability of each ground atom in the test set given all atoms in the evidence set using the four inference algorithms  We measure the error using negative log likelihood of the data according to the inference algorithms  the negative log likelihood is a sampling approximation of the K L divergence to the datagenerating distribution  shifted by its entropy   The results  averaged over    runs  are shown in Figures   a  and   b   The figures show how the log likelihood of the data varies with time for the four inference algorithms used  We see that MC LWMC has the lowest negative loglikelihood of all algorithms by a large margin  It significantly dominates MC WMC in about   minutes of run time and is substantially superior to both lifted BP and MC SAT    notice the log scale   This shows the advantages of approximate PTP over lifted BP and ground inference      CONCLUSION  Probabilistic theorem proving  PTP  combines theorem proving and probabilistic inference  This paper proposed an algorithm for PTP based on reducing it to lifted weighted model counting  and showed both theoretically and empirically that it has significant advantages compared to previous lifted probabilistic inference algorithms  An implementation of PTP will be available in the Alchemy system       Directions for future research include  extension of PTP to infinite  non Herbrand first order logic  new lifted inference rules  theoretical analysis of liftability  porting to PTP more speedup techniques from logical and probabilistic inference  lifted splitting heuristics  better handling of existentials  variational PTP algorithms  better importance distributions  approximate lifting  answering multiple queries simultaneously  applications  etc  Acknowledgements This research was partly funded by ARO grant W   NF            AFRL contract FA        C      DARPA contracts FA                FA        D       HR        C       HR        C      and NBCH D        NSF grants IIS         and IIS          and ONR grant N                 The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of ARO  DARPA  NSF  ONR  or the U S  Government   
 The paper investigates parameterized approximate message passing schemes that are based on bounded inference and are inspired by Pearls belief propagation algorithm  BP   We start with the bounded inference mini clustering algorithm and then move to the iterative scheme called Iterative Join Graph Propagation  IJGP   that combines both iteration and bounded inference  Algorithm IJGP belongs to the class of Generalized Belief Propagation algorithms  a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini clustering and belief propagation  as well as a number of other stateof the art algorithms on several classes of networks  We also provide insight into the accuracy of iterative BP and IJGP by relating these algorithms to well known classes of constraint propagation schemes      Introduction Probabilistic inference is the principal task in Bayesian networks and is known to be an NP hard problem  Cooper        Roth         Most of the commonly used exact algorithms such as jointree clustering  Lauritzen   Spiegelhalter        Jensen  Lauritzen    Olesen        or variableelimination  Dechter              Zhang  Qi    Poole         and more recently search schemes  Darwiche        Bacchus  Dalmao    Pitassi        Dechter   Mateescu        exploit the network structure  While significant advances were made in the last decade in exact algorithms  many real life problems are too big and too hard  especially when their structure is dense  since they are time and space exponential in the treewidth of the graph  Approximate algorithms are therefore necessary for many practical problems  although approximation within given error bounds is also NP hard  Dagum   Luby        Roth          c      AI Access Foundation  All rights reserved    M ATEESCU   K ASK   G OGATE   D ECHTER  The paper focuses on two classes of approximation algorithms for the task of belief updating  Both are inspired by Pearls belief propagation algorithm  Pearl         which is known to be exact for trees  As a distributed algorithm  Pearls belief propagation can also be applied iteratively to networks that contain cycles  yielding Iterative Belief Propagation  IBP   also known as loopy belief propagation  When the networks contain cycles  IBP is no longer guaranteed to be exact  but in many cases it provides very good approximations upon convergence  Some notable success cases are those of IBP for coding networks  McEliece  MacKay    Cheng        McEliece   Yildirim         and a version of IBP called survey propagation for some classes of satisfiability problems  Mezard  Parisi    Zecchina        Braunstein  Mezard    Zecchina         Although the performance of belief propagation is far from being well understood in general  one of the more promising avenues towards characterizing its behavior came from analogies with statistical physics  It was shown by Yedidia  Freeman  and Weiss              that belief propagation can only converge to a stationary point of an approximate free energy of the system  called Bethe free energy  Moreover  the Bethe approximation is computed over pairs of variables as terms  and is therefore the simplest version of the more general Kikuchi        cluster variational method  which is computed over clusters of variables  This observation inspired the class of Generalized Belief Propagation  GBP  algorithms  that work by passing messages between clusters of variables  As mentioned by Yedidia et al          there are many GBP algorithms that correspond to the same Kikuchi approximation  A version based on region graphs  called canonical by the authors  was presented by Yedidia et al                      Our algorithm Iterative Join Graph Propagation is a member of the GBP class  although it will not be described in the language of region graphs  Our approach is very similar to and was independently developed from that of McEliece and Yildirim         For more information on BP state of the art research see the recent survey by Koller         We will first present the mini clustering scheme which is an anytime bounded inference scheme that generalizes the mini bucket idea  It can be viewed as a belief propagation algorithm over a tree obtained by a relaxation of the networks structure  using the technique of variable duplication   We will subsequently present Iterative Join Graph Propagation  IJGP  that sends messages between clusters that are allowed to form a cyclic structure  Through these two schemes we investigate      the quality of bounded inference as an anytime scheme  using mini clustering       the virtues of iterating messages in belief propagation type algorithms  and the result of combining bounded inference with iterative message passing  in IJGP   In the background section    we overview the Tree Decomposition scheme that forms the basis for the rest of the paper  By relaxing two requirements of the tree decomposition  that of connectedness  via mini clustering  and that of tree structure  by allowing cycles in the underlying graph   we combine bounded inference and iterative message passing with the basic tree decomposition scheme  as elaborated in subsequent sections  In Section   we present the partitioning based anytime algorithm called Mini Clustering  MC   which is a generalization of the Mini Buckets algorithm  Dechter   Rish         It is a messagepassing algorithm guided by a user adjustable parameter called i bound  offering a flexible tradeoff between accuracy and efficiency in anytime style  in general the higher the i bound  the better the accuracy   MC algorithm operates on a tree decomposition  and similar to Pearls belief propagation algorithm  Pearl        it converges in two passes  up and down the tree  Our contribution beyond other works in this area  Dechter   Rish        Dechter  Kask    Larrosa        is in      Extending the partition based approximation for belief updating from mini buckets to general treedecompositions  thus allowing the computation of the updated beliefs for all the variables at once        J OIN  G RAPH P ROPAGATION A LGORITHMS  This extension is similar to the one proposed by Dechter et al          but replaces optimization with probabilistic inference      Providing empirical evaluation that demonstrates the effectiveness of the idea of tree decomposition combined with partition based approximation for belief updating  Section   introduces the Iterative Join Graph Propagation  IJGP  algorithm  It operates on a general join graph decomposition that may contain cycles  It also provides a user adjustable i bound parameter that defines the maximum cluster size of the graph  and hence bounds the complexity   therefore it is both anytime and iterative  While the algorithm IBP is typically presented as a generalization of Pearls Belief Propagation algorithm  we show that IBP can be viewed as IJGP with the smallest i bound  We also provide insight into IJGPs behavior in Section    Zero beliefs are variable value pairs that have zero conditional probability given the evidence  We show that      if a value of a variable is assessed as having zero belief in any iteration of IJGP  it remains a zero belief in all subsequent iterations      IJGP converges in a finite number of iterations relative to its set of zero beliefs  and  most importantly     that the set of zero beliefs decided by any of the iterative belief propagation methods is sound  Namely any zero belief determined by IJGP corresponds to a true zero conditional probability relative to the given probability distribution expressed by the Bayesian network  Empirical results on various classes of problems are included in Section    shedding light on the performance of IJGP i   We see that it is often superior  or otherwise comparable  to other state of the art algorithms  The paper is based in part on earlier conference papers by Dechter  Kask  and Mateescu         Mateescu  Dechter  and Kask        and Dechter and Mateescu             Background In this section we provide background for exact and approximate probabilistic inference algorithms that form the basis of our work  While we present our algorithms in the context of directed probabilistic networks  they are applicable to any graphical model  including Markov networks      Preliminaries Notations  A reasoning problem is defined in terms of a set of variables taking values on finite domains and a set of functions defined over these variables  We denote variables or subsets of variables by uppercase letters  e g   X  Y  Z  S  R        and values of variables by lower case letters  e g   x  y  z  s   An assignment  X    x            Xn   xn   can be abbreviated as x    x            xn    For a subset of variables S  DS denotes the Cartesian product of the domains of variables in S  xS is the projection of x    x            xn   over a subset S  We denote functions by letters f   g  h  etc   and the scope  set of arguments  of the function f by scope f    D EFINITION    graphical model   Kask  Dechter  Larrosa    Dechter        A graphical model M is a   tuple  M   hX  D  Fi  where  X    X            Xn   is a finite set of variables  D    D            Dn   is the set of their respective finite domains of values  F    f            fr   is a set of positive real valued discrete functions  each defined over a subset of variables Si  X  called its scope  and denoted by scope f P i    A graphical model typically has an associated combination   operator    e g            product  sum   The graphical model represents the combination    The combination operator can also be defined axiomatically  Shenoy                M ATEESCU   K ASK   G OGATE   D ECHTER  of all its functions  ri   fi   A graphical model has an associated primal graph that captures the structural information of the model  D EFINITION    primal graph  dual graph  The primal graph of a graphical model is an undirected graph that has variables as its vertices and an edge connects any two vertices whose corresponding variables appear in the scope of the same function  A dual graph of a graphical model has a one to one mapping between its vertices and functions of the graphical model  Two vertices in the dual graph are connected if the corresponding functions in the graphical model share a variable  We denote the primal graph by G    X  E   where X is the set of variables and E is the set of edges  D EFINITION    belief networks  A belief  or Bayesian  network is a graphical model B   hX  D  G  P i  where G    X  E  is a directed acyclic graph over variables X and P    pi    where pi    p Xi   pa  Xi       are conditional probability tables  CPTs  associated with each variable Xi and pa Xi     scope pi   Xi   is the set of parents of Xi in G  Given a subset of variables S  we will write P  s  as the probability P  S   s   where s  DS   A belief network represents a probability distribution over X  P  x             xn     ni   P  xi  xpa Xi      An evidence set e is an instantiated subset of variables  The primal graph of a belief network is called a moral graph  It can be obtained by connecting the parents of each vertex in G and removing the directionality of the edges  Equivalently  it connects any two variables appearing in the same family  a variable and its parents in the CPT   Two common queries in Bayesian networks are Belief Updating  BU  and Most Probable Explanation  MPE   D EFINITION    belief network queries  The Belief Updating  BU  task is to find the posterior probability of each single variable given some evidence e  that is to compute P  Xi  e   The Most Probable Explanation  MPE  task is to find a complete assignment to all the variables having maximum probability given the evidence  that is to compute argmaxX i pi       Tree Decomposition Schemes Tree decomposition is at the heart of most general schemes for solving a wide range of automated reasoning problems  such as constraint satisfaction and probabilistic inference  It is the basis for many well known algorithms  such as join tree clustering and bucket elimination  In our presentation we will follow the terminology of Gottlob  Leone  and Scarcello        and Kask et al          D EFINITION    tree decomposition  cluster tree  Let B   hX  D  G  P i be a belief network  A tree decomposition for B is a triple hT    i  where T    V  E  is a tree  and  and  are labeling functions which associate with each vertex v  V two sets   v   X and  v   P satisfying     For each function pi  P   there is exactly one vertex v  V such that pi   v   and scope pi     v      For each variable Xi  X  the set  v  V  Xi   v   induces a connected subtree of T   This is also called the running intersection  or connectedness  property  We will often refer to a node and its functions as a cluster and use the term tree decomposition and cluster tree interchangeably        J OIN  G RAPH P ROPAGATION A LGORITHMS  D EFINITION    treewidth  separator  eliminator  Let D   hT    i be a tree decomposition of a belief network B  The treewidth  Arnborg        of D is maxvV   v       The treewidth of B is the minimum treewidth over all its tree decompositions  Given two adjacent vertices u and v of a tree decomposition  the separator of u and v is defined as sep u  v     u    v   and the eliminator of u with respect to v is elim u  v     u    v   The separator width of D is max u v   sep u  v    The minimum treewidth of a graph G can be shown to be identical to a related parameter called induced width  Dechter   Pearl         Join tree and cluster tree elimination  CTE  In both Bayesian network and constraint satisfaction communities  the most used tree decomposition method is join tree decomposition  Lauritzen   Spiegelhalter        Dechter   Pearl         introduced based on relational database concepts  Maier         Such decompositions can be generated by embedding the networks moral graph G into a chordal graph  often using a triangulation algorithm and using its maximal cliques as nodes in the join tree  The triangulation algorithm assembles a join tree by connecting the maximal cliques in the chordal graph in a tree  Subsequently  every CPT pi is placed in one clique containing its scope  Using the previous terminology  a join tree decomposition of a belief network B   hX  D  G  P i is   a tree T    V  E   where V is the set of cliques of a chordal graph G that contains G  and E is a set of edges that form a tree between cliques  satisfying the running intersection property  Maier         Such a join tree satisfies the properties of tree decomposition and is therefore a cluster tree  Kask et al          In this paper  we will use the terms tree decomposition and join tree decomposition interchangeably  There are a few variants for processing join trees for belief updating  e g   Jensen et al         Shafer   Shenoy         We adopt here the version from Kask et al          called cluster treeelimination  CTE   that is applicable to tree decompositions in general and is geared towards space savings  It is a message passing algorithm  for the task of belief updating  messages are computed by summation over the eliminator between the two clusters of the product of functions in the originating cluster  The algorithm  denoted CTE BU  see Figure     pays a special attention to the processing of observed variables since the presence of evidence is a central component in belief updating  When a cluster sends a message to a neighbor  the algorithm operates on all the functions in the cluster except the message from that particular neighbor  The message contains a single combined function and individual functions that do not share variables with the relevant eliminator  All the non individual functions are combined in a product and summed over the eliminator  Example   Figure  a describes a belief network and Figure  b a join tree decomposition for it  Figure  c shows the trace of running CTE BU with evidence G   ge   where h u v  is a message that cluster u sends to cluster v  T HEOREM    complexity of CTE BU   Dechter et al         Kask et al         Given a Bayesian network B   hX  D  G  P i and a tree decomposition hT    i of B  the time complexity of CTE BU is O deg   n   N    dw      and the space complexity is O N  dsep    where deg is the maximum degree of a node in the tree decomposition  n is the number of variables  N is the number of nodes in the tree decomposition  d is the maximum domain size of a variable  w is the treewidth and sep is the maximum separator size         M ATEESCU   K ASK   G OGATE   D ECHTER  Algorithm CTE for Belief Updating  CTE BU  Input  A tree decomposition hT    i  T    V  E  for B   hX  D  G  P i  Evidence variables var e   Output  An augmented tree whose nodes are clusters containing the original CPTs and the messages received from neighbors  P  Xi   e   Xi  X  Denote by H u v  the message from vertex u to v  nev  u  the neighbors of u in T excluding v  cluster u     u    H v u    v  u   E   clusterv  u    cluster u  excluding message from v to u    Compute messages  For every node u in T   once u has received messages from all nev  u   compute message to node v     Process observed variables  Assign relevant evidence to all pi   u     Compute the combined function  X  h u v     Y  f  elim u v  f A  where A is the set of functions in clusterv  u  whose scope intersects elim u  v   Add h u v  to H u v  and add all the individual functions in clusterv  u   A Send H u v  to node v   Compute P  Xi   e   For every Xi  X let u be a vertex in T such that Xi   u   Compute P  Xi   e    P Q  u  Xi     f cluster u  f    Figure    Algorithm Cluster Tree Elimination for Belief Updating  CTE BU   A              A  B  C           p a    p b   a    p c   a  b       ABC BC  B    C  D            B   C   D   F            p d   b   p  f   c  d      BCDF  E  BF             B  E   F            p  e   b  f                E   F   G           p  g   e  f        F G   a   BEF EF      b   EFG  h            b   c        b   c      a  h           p   a    p  b   a    p  c   a   b   p   d   b    p   f   c   d    h          b   f    d f  h          b   f       h            b   f      e  p  d   b   h            e   f      p   e   b   f    h          b   f    c  d    p   f   c  d    h            b   c    p   e   b   f    h           e   f    b  h           e   f     p   G   g e   e   f     c   Figure     a  A belief network   b  A join tree decomposition   c  Execution of CTE BU      Partition Based Mini Clustering The time  and especially the space complexity  of CTE BU renders the algorithm infeasible for problems with large treewidth  We now introduce Mini Clustering  a partition based anytime algorithm which computes bounds or approximate values on P  Xi   e  for every variable Xi         J OIN  G RAPH P ROPAGATION A LGORITHMS  Procedure MC for Belief Updating  MC BU i      Compute the combined mini functions  Make an  i  size mini cluster partitioning of clusterv  u    mc             mc p    P Q h  u v    elim u v  f mc    f Q hi u v    maxelim u v  f mc i  f i              p add  hi u v   i              p  to H u v    Send H u v  to v  Compute upper bounds P  Xi   e  on P  Xi   e   For every Xi  X let u  V be a cluster such that Xi   u   Make  i  mini clusters from cluster u    mc             mc p    Compute P  Xi   e    P Q Qp Q    u Xi f mc    f      k   max u Xi f mc k  f     Figure    Procedure Mini Clustering for Belief Updating  MC BU       Mini Clustering Algorithm Combining all the functions of a cluster into a product has a complexity exponential in its number of variables  which is upper bounded by the induced width  Similar to the mini bucket scheme  Dechter         rather than performing this expensive exact computation  we partition the cluster into p mini clusters mc             mc p   each having at most Pi variables  Q where i is an accuracy parameter  Instead of computing by CTE BU h u v    elim u v  f  u  f   we can divide the functions of  u  mc k   k              p   and rewrite h u v    P into p mini clusters Qp Q P Q f   mc k  f   By migrating the summation operator into elim u v  elim u v  f  u  Q P k   fQ p each mini cluster  yielding k   elim u v  f mc k  f   we get an upper bound on h u v    The resulting algorithm is called MC BU i   Consequently  the combined functions are approximated via mini clusters  as follows  Suppose u  V has received messages from all its neighbors other than v  the message from v is ignored even if received   The functions in clusterv  u  that are to be combined are partitioned into mini clusters  mc             mc p    each one containing at most i variables  Each mini cluster is processed by summation over the eliminator  and the resulting combined functions as well as all the individual functions are sent to v  It was shown by Dechter and Rish        that the upper bound can be improved by using the maximization operator max rather than the summation operator sum on some mini buckets  Similarly  lower bounds can be generated by replacing sum with min  minimization  for some mini buckets  Alternatively  we can replace sum by a mean operator  taking the sum and dividing by the number of elements in the sum   in this case deriving an approximation of the joint belief instead of a strict upper bound  Algorithm MC BU for upper bounds can be obtained from CTE BU by replacing step   of the main loop and the final part of computing the upper bounds on the joint belief by the procedure given in Figure    In the implementation we used for the experiments reported here  the partitioning was done in a greedy brute force manner  We ordered the functions according to their sizes  number of variables   breaking ties arbitrarily  The largest function was placed in a mini cluster by itself  Then  we picked the largest remaining function and probed the mini clusters in the order of their creation        M ATEESCU   K ASK   G OGATE   D ECHTER     ABC  BC  H         h          b  c      p  a    p  b   a    p c   a  b  a            h  H          b      p  d   b   h          b  f   d  f  h        c     max p   f   c  d   d  f     BCDF             h  H          BF     BEF  EF  c  d  h         f      max p  f   c  d   c d               b  f       p e   b  f    h        e  f    H           h  H           h          e  f       p  e   b  f    h        b   h         f    H               b      p  d   b   h          b  c   e  b              h   e  f      p G   g e   e  f    EFG  Figure    Execution of MC BU for i      trying to find one that together with the new function would have no more than i variables  A new mini cluster was created whenever the existing ones could not accommodate the new function  Example   Figure   shows the trace of running MC BU    on the problem in Figure    First  evidence G   ge is assigned in all CPTs  There are no individual functions to be sent from cluster   to cluster    Cluster   contains only   variables         A  B  C   therefore it is not partitioned  P p a   p b a   p c a  b  is computed and the message The combined function h        b  c    a   H         h       b  c   is sent to node    Now  node   can send its message to node    Again  there are no individual functions  Cluster   contains   variables         B  C  D  F    and a partitioning is necessary  MC BU    can choose P mc       p d b   h       b  c   and mc       p f  c  d      The combined functions h       b    c d p d b   h       b  c  and h       f     maxc d p f  c  d  are computed and the message H         h        b   h        f    is sent to node    The algorithm continues until every node has received messages from all its neighbors  An upper bound on p a  G   ge   can now be computed by choosing cluster    which contains variable A  It doesnt need partitionP ing  so the algorithm just computes b c p a   p b a   p c a  b   h        b   h        c   Notice that unlike CTE BU which processes   variables in cluster    MC BU    never processes more than   variables at a time  It was already shown that  T HEOREM    Dechter   Rish        Given a Bayesian network B   hX  D  G  P i and the evidence e  the algorithm MC BU i  computes an upper bound on the joint probability P  Xi   e  of each variable Xi  and each of its values  and the evidence e  T HEOREM    complexity of MC BU i    Dechter et al         Given a Bayesian network B   hX  D  G  P i and a tree decomposition hT    i of B  the time and space complexity of MC BU i  is O n  hw  di    where n is the number of variables  d is the maximum domain size of a variable and hw   maxuT   f  P  scope f     u         which bounds the number of mini clusters        J OIN  G RAPH P ROPAGATION A LGORITHMS                                                                                                                                                                                                                                                                                    Figure    Node duplication semantics of MC   a  trace of MC BU      b  trace of CTE BU  Semantics of Mini Clustering The mini bucket scheme was shown to have the semantics of relaxation via node duplication  Kask   Dechter        Choi  Chavira    Darwiche         We extend it to mini clustering by showing how it can apply as is to messages that flow in one direction  inward  from leaves to root   as follows  Given a tree decomposition D  where CTE BU computes a function h u v   the message that cluster u sends to cluster v   MC BU i  partitions cluster u into p mini clusters u            up   which are processed independently and then the resulting functions h ui  v  are sent to v  Instead consider a different decomposition D    which is just like D  with the exception that  a  instead of u  it has clusters u            up   all of which are children of v  and each variable appearing in more than a single mini cluster becomes a new variable   b  each child w of u  in D  is a child of uk  in D     such that h w u   in D  is assigned to uk  in D    during the partitioning  Note that D  is not a legal tree decomposition relative to the original variables since it violates the connectedness property  the mini clusters u            up contain variables elim u  v  but the path between the nodes u            up  this path goes through v  does not  However  it is a legal tree decomposition relative to the new variables  It is straightforward to see that H u v  computed by MC BU i  on D is the same as  h ui  v   i              p  computed by CTE BU on D  in the direction from leaves to root  If we want to capture the semantics of the outward messages from root to leaves  we need to generate a different relaxed decomposition  D     because MC  as defined  allows a different partitioning in the up and down streams of the same cluster  We could of course stick with the decomposition in D  and use CTE in both directions which would lead to another variant of mini clustering  Example   Figure   a  shows a trace of the bottom up phase of MC BU    on the network in Figure    Figure   b  shows a trace of the bottom up phase of CTE BU algorithm on a problem obtained from the problem in Figure   by splitting nodes D  into D  and D     and F  into F   and F       The MC BU algorithm computes an upper bound P  Xi   e  on the joint probability P  Xi   e   However  deriving a bound on the conditional probability P  Xi  e  is not easy when the exact       M ATEESCU   K ASK   G OGATE   D ECHTER  Random Bayesian N    K   P   C                    Avg abs error                       ev    ev     ev     ev                                                                               Number of iterations  Figure    Convergence of IBP     variables  evidence from      variables   value of P  e  is not available  If we just try to divide  multiply  P  Xi   e  by a constant  the result is not P necessarily an upper bound on P  Xi  e   It is easy to show that normalization  P  xi   e   xi Di P  xi   e   with the mean operator is identical to normalization of MC BU output when applying the summation operator in all the mini clusters  MC BU i  is an improvement over the Mini Bucket algorithm MB i   in that it allows the computation of P  Xi   e  for all variables with a single run  whereas MB i  computes P  Xi   e  for just one variable  with a single run  When computing P  Xi   e  for each variable  MB i  has to be run n times  once for each variable  an algorithm we call nMB i   It was demonstrated by Mateescu et al         that MC BU i  has up to linear speed up over nMB i   For a given i  the accuracy of MC BU i  can be shown to be not worse than that of nMB i       Experimental Evaluation of Mini Clustering The work of Mateescu et al         and Kask        provides an empirical evaluation of MC BU that reveals the impact of the accuracy parameter on its quality of approximation and compares with Iterative Belief Propagation and a Gibbs sampling scheme  We will include here only a subset of these experiments which will provide the essence of our results  Additional empirical evaluation of MC BU will be given when comparing against IJGP later in this paper  We tested the performance of MC BU i  on random Noisy OR networks  random coding networks  general random networks  grid networks  and three benchmark CPCS files with         and     variables respectively  these are belief networks for medicine  derived from the Computer based Patient Case Simulation system  known to be hard for belief updating   On each type of network we ran Iterative Belief Propagation  IBP    set to run at most    iterations  Gibbs Sampling  GS  and MC BU i   with i from   to the treewidth w to capture the anytime behavior of MC BU i   The random networks were generated using parameters  N K C P   where N is the number of variables  K is their domain size  we used only K     C is the number of conditional probability tables and P is the number of parents in each conditional probability table  The parents in each table are picked randomly given a topological ordering  and the conditional probability tables are filled       J OIN  G RAPH P ROPAGATION A LGORITHMS     e         NHD max  IBP  MC BU     MC BU     MC BU                        mean                          N     P       instances Abs  Error max     E       E       E       E       E       E       E       E       E     mean    E       E       E       E       E       E       E       E       E       E       E       E     Rel  Error  max     E       E       E       E       E       E       E       E       E     mean    E       E       E       E       E       E       E       E       E       E       E       E     Time max                                                         mean                                                                          Table    Performance on Noisy OR networks  w       Normalized Hamming Distance  absolute error  relative error and time   randomly  The grid networks have the structure of a square  with edges directed to form a diagonal flow  all parallel edges have the same direction   They were generated by specifying N  a square integer  and K  we used K     We also varied the number of evidence nodes  denoted by  e  in the tables  The parameter values are reported in each table  For all the problems  Gibbs sampling performed consistently poorly so we only include part of its results here  In our experiments we focused on the approximation power of MC BU i   We compared two versions of the algorithm  In the first version  for every cluster  we used the max operator in all its mini clusters  except for one of them that was processed by summation  In the second version  we used the operator mean in all the mini clusters  We investigated this second version of the algorithm for two reasons      we compare MC BU i  with IBP and Gibbs sampling  both of which are also approximation algorithms  so it would not be possible to compare with a bounding scheme      we observed in our experiments that  although the bounds improve as the i bound increases  the quality of bounds computed by MC BU i  was still poor  with upper bounds being greater than   in many cases   Notice that we need to maintain the sum operator for at least one of the mini clusters  The mean operator simply performs summation and divides by the number of elements in the sum  For example  if A  B  C are binary variables  taking values   and     and f  A  B  C  is the aggregated function of one mini cluster  and elim    A  B   then computing the message h C  by the mean P operator gives      A B      f  A  B  C   We computed the exact solution and used three different measures of accuracy     Normalized Hamming Distance  NHD    we picked the most likely value for each variable for the approximate and for the exact  took the ratio between the number of disagreements and the total number of variables  and averaged over the number of problems that we ran for each class     Absolute Error  Abs  Error    is the absolute value of the difference between the approximate and the exact  averaged over all values  for each variable   all variables and all problems     Relative Error  Rel  Error    is the absolute value of the difference between the approximate and the exact  divided by the exact  averaged over all values  for each variable   all variables and all problems  For coding networks     Wexler and Meek        compared the upper lower bounding properties of the mini bucket on computing probability of evidence  Rollon and Dechter        further investigated heuristic schemes for mini bucket partitioning         M ATEESCU   K ASK   G OGATE   D ECHTER      e         N     P       instances Abs  Error  NHD max  mean                                      IBP                                MC BU     MC BU     MC BU     MC BU      MC BU      max     E       E       E       E       E       E       E       E       E       E       E       E       E         mean    E       E       E       E       E       E       E       E       E       E       E       E       E       E       E       E         Rel  Error max  Time  mean    E       E       E       E       E       E       E       E       E       E       E       E       E       E       E       E            E       E       E       E       E       E       E       E       E       E       E       E       E         max  mean                                                                                                                                                                                                         Table    Performance on Noisy OR networks  w       Normalized Hamming Distance  absolute error  relative error and time  Noisy OR networks  N     P    evid     w         instances  Noisy OR networks  N     P    evid     w         instances   e     e    MC IBP Gibbs Sampling  MC IBP Gibbs Sampling   e    Absolute error  Absolute error   e     e     e     e     e     e     e     e     e                                                                 i bound  i bound  Figure    Absolute error for Noisy OR networks  we report only one measure  Bit Error Rate  BER   In terms of the measures defined above  BER is the normalized Hamming distance between the approximate  computed by an algorithm  and the actual input  which in the case of coding networks may be different from the solution given by exact algorithms   so we denote them differently to make this semantic distinction  We also report the time taken by each algorithm  For reported metrics  time  error  etc   provided in the Tables  we give both mean and max values  In Figure   we show that IBP converges after about   iterations  So  while in our experiments we report its time for    iterations  its time is even better when sophisticated termination is used  These results are typical of all runs         J OIN  G RAPH P ROPAGATION A LGORITHMS  Random networks  N     P    k    evid    w         instances  Random networks  N     P    k    evid     w         instances              MC Gibbs Sampling IBP              MC Gibbs Sampling IBP              Absolute error  Absolute error                                                                                                 i bound            i bound  Figure    Absolute error for random networks  BER         max mean  IBP GS MC BU    MC BU    MC BU    MC BU                                                                               IBP GS MC BU    MC BU    MC BU    MC BU    MC BU                                                                                                                 max mean max mean max mean N      P       instances  w                                                                                                                                                                                                                            N      P       instances  w                                                                                                                                                                                                                                                                         max mean  Time                                                                                                                                                                                                                                       Table    Bit Error Rate  BER  for coding networks   Random Noisy OR networks results are summarized in Tables   and    and Figure    For NHD  both IBP and MC BU gave perfect results  For the other measures  we noticed that IBP is more accurate when there is no evidence by about an order of magnitude  However  as evidence is added  IBPs accuracy decreases  while MC BUs increases and they give similar results  We see that MC BU gets better as the accuracy parameter i increases  which shows its anytime behavior  General random networks results are summarized in Figure    They are similar to those for random Noisy OR networks  Again  IBP has the best result only when the number of evidence variables is small  It is remarkable how quickly MC BU surpasses the performance of IBP as evidence is added  for more  see the results of Mateescu et al          Random coding networks results are given in Table   and Figure    The instances fall within the class of linear block codes    is the channel noise level   It is known that IBP is very accurate for this class  Indeed  these are the only problems that we experimented with where IBP outperformed MC BU throughout  The anytime behavior of MC BU can again be seen in the variation of numbers in each column and more vividly in Figure          M ATEESCU   K ASK   G OGATE   D ECHTER  Coding networks  N      P    sigma      w         instances  Coding networks  N      P    sigma      w         instances              MC IBP         MC IBP               Bit Error Rate  Bit Error Rate                                                                                                            i bound  i bound  Figure    Bit Error Rate  BER  for coding networks  Grid   x    evid     w         instances  Grid   x    evid     w         instances           MC IBP        MC IBP      Time  seconds   Absolute error                                                                                           i bound                         i bound  Figure     Grid   x    absolute error and time  Grid networks results are given in Figure     We notice that IBP is more accurate for no evidence and MC BU is better as more evidence is added  The same behavior was consistently manifested for smaller grid networks that we experimented with  from  x  up to   x     CPCS networks results We also tested on three CPCS benchmark files  The results are given in Figure     It is interesting to notice that the MC BU scheme scales up to fairly large networks  like the real life example of CPCS     induced width      IBP is again more accurate when there is no evidence  but is surpassed by MC BU when evidence is added  However  whereas MC BU is competitive with IBP time wise when i bound is small  its runtime grows rapidly as i bound increases  For more details on all these benchmarks see the results of Mateescu et al          Summary Our results show that  as expected  IBP is superior to all other approximations for coding networks  However  for random Noisy OR  general random  grid networks and the CPCS networks  in the presence of evidence  the mini clustering scheme is often superior even in its weakest form  The empirical results are particularly encouraging as we use an un optimized scheme that exploits a universal principle applicable to many reasoning tasks         J OIN  G RAPH P ROPAGATION A LGORITHMS  CPCS      evid    w        instance  CPCS      evid     w        instance             MC IBP  MC IBP        Absolute error  Absolute error                                                                                                i bound                         i bound  Figure     Absolute error for CPCS         Join Graph Decomposition and Propagation In this section we introduce algorithm Iterative Join Graph Propagation  IJGP  which  like miniclustering  is designed to benefit from bounded inference  but also exploit iterative message passing as used by IBP  Algorithm IJGP can be viewed as an iterative version of mini clustering  improving the quality of approximation  especially for low i bounds  Given a cluster of the decomposition  mini clustering can potentially create a different partitioning for every message sent to a neighbor  This dynamic partitioning can happen because the incoming message from each neighbor has to be excluded when realizing the partitioning  so a different set of functions are split into mini clusters for every message to a neighbor  We can define a version of mini clustering where for every cluster we create a unique static partition into mini clusters such that every incoming message can be included into one of the mini clusters  This version of MC can be extended into IJGP by introducing some links between mini clusters of the same cluster  and carefully limiting the interaction between the resulting nodes in order to eliminate over counting  Algorithm IJGP works on a general join graph that may contain cycles  The cluster size of the graph is user adjustable via the i bound  providing the anytime nature   and the cycles in the graph allow the iterative application of message passing  In Subsection     we introduce join graphs and discuss their properties  In Subsection     we describe the IJGP algorithm itself      Join Graphs D EFINITION    join graph decomposition  A join graph decomposition for a belief network B   hX  D  G  P i is a triple D   hJG    i  where JG    V  E  is a graph  and  and  are labeling functions which associate with each vertex v  V two sets   v   X and  v   P such that     For each pi  P   there is exactly one vertex v  V such that pi   v   and scope pi     v       connectedness  For each variable Xi  X  the set  v  V  Xi   v   induces a connected subgraph of JG  The connectedness requirement is also called the running intersection property         M ATEESCU   K ASK   G OGATE   D ECHTER                     A  C             A       B              C             B  a            b   Figure     An edge labeled decomposition  We will often refer to a node in V and its CPT functions as a cluster  and use the term joingraph decomposition and cluster graph interchangeably  Clearly  a join tree decomposition or a cluster tree is the special case when the join graph D is a tree  It is clear that one of the problems of message propagation over cyclic join graphs is overcounting  To reduce this problem we devise a scheme  which avoids cyclicity with respect to any single variable  The algorithm works on edge labeled join graphs  D EFINITION    minimal edge labeled join graph decompositions  An edge labeled join graph decomposition for B   hX  D  G  P i is a four tuple D   hJG      i  where JG    V  E  is a graph   and  associate with each vertex v  V the sets  v   X and  v   P and  associates with each edge  v  u   E the set   v  u    X such that     For each function pi  P   there is exactly one vertex v  V such that pi   v   and scope pi     v       edge connectedness  For each edge  u  v     u  v     u    v   such that Xi  X  any two clusters containing Xi can be connected by a path whose every edge label includes Xi   Finally  an edge labeled join graph is minimal if no variable can be deleted from any label while still satisfying the edge connectedness property  D EFINITION    separator  eliminator of edge labeled join graphs  Given two adjacent vertices u and v of JG  the separator of u and v is defined as sep u  v      u  v    and the eliminator of u with respect to v is elim u  v     u     u  v    The separator width is max u v   sep u  v    Edge labeled join graphs can be made label minimal by deleting variables from the labels while maintaining connectedness  if an edge label becomes empty  the edge can be deleted   It is easy to see that  Proposition   A minimal edge labeled join graph does not contain any cycle relative to any single variable  That is  any two clusters containing the same variable are connected by exactly one path labeled with that variable  Notice that every minimal edge labeled join graph is edge minimal  no edge can be deleted   but not vice versa     Note that a node may be associated with an empty set of CPTs         J OIN  G RAPH P ROPAGATION A LGORITHMS  Example   The example in Figure   a shows an edge minimal join graph which contains a cycle relative to variable    with edges labeled with separators  Notice however that if we remove variable   from the label of one edge we will have no cycles  relative to single variables  while the connectedness property is still maintained  The mini clustering approximation presented in the previous section works by relaxing the jointree requirement of exact inference into a collection of join trees having smaller cluster size  It introduces some independencies in the original problem via node duplication and applies exact inference on the relaxed model requiring only two message passings  For the class of IJGP algorithms we take a different route  We choose to relax the tree structure requirement and use join graphs which do not introduce any new independencies  and apply iterative message passing on the resulting cyclic structure  Indeed  it can be shown that any join graph of a belief network is an I map  independency map  Pearl        of the underlying probability distribution relative to node separation  Since we plan to use minimally edge labeled join graphs to address over counting problems  the question is what kind of independencies are captured by such graphs  D EFINITION     edge separation in edge labeled join graphs  Let D   hJG      i  JG    V  E  be an edge labeled decomposition of a Bayesian network B   hX  D  G  P i  Let NW   NY  V be two sets of nodes  and EZ  E be a set of edges in JG  Let W  Y  Z be their corresponding sets of variables  W   vNW  v   Z   eEZ  e    We say that EZ edge separates NW and NY in D if there is no path between NW and NY in the JG graph whose edges in EZ are removed  In this case we also say that W is separated from Y given Z in D  and write hW  Z Y iD   Edgeseparation in a regular join graph is defined relative to its separators  T HEOREM   Any edge labeled join graph decomposition D   hJG      i of a belief network B   hX  D  G  P i is an I map of P relative to edge separation  Namely  any edge separation in D corresponds to conditional independence in P   Proof  Let M G be the moral graph of BN   Since M G is an I map of P   it is enough to prove that JG is an I map of M G  Let NW and NY be disjoint sets of nodes and NZ be a set of edges in JG  and W  Z  Y be their corresponding sets of variables in M G  We will prove  hNW  NZ  NY iJG   hW  Z Y iM G by contradiction  Since the sets W  Z  Y may not be disjoint  we will actually prove that hW  Z Z Y  ZiM G holds  this being equivalent to hW  Z Y iM G   Supposing hW  Z Z Y  ZiM G is false  then there exists a path                    n       n in M G that goes from some variable       W  Z to some variable    n  Y  Z without intersecting variables in Z  Let Nv be the set of all nodes in JG that contain variable v  X  and let us consider the set of nodes  S   ni   Ni  NZ We argue that S forms a connected sub graph in JG  First  the running intersection property ensures that every Ni   i              n  remains connected in JG after removing the nodes in NZ  otherwise  it must be that there was a path between the two disconnected parts in the original JG  which implies that a i is part of Z  which is a contradiction   Second  the fact that  i   i      i         M ATEESCU   K ASK   G OGATE   D ECHTER             n     is an edge in the moral graph M G implies that there is a conditional probability table  CPT  on both i and i     i              n     and perhaps other variables   From property   of the definition of the join graph  it follows that for all i              n    there exists a node in JG that contains both i and i     This proves the existence of a path in the mutilated join graph  JG with NZ pulled out  from a node in NW containing      to the node containing both   and    N  is connected   then from that node to the one containing both   and    N  is connected   and so on until we reach a node in NY containing    n   This shows that hNW  NZ  NY iJG is false  concluding the proof by contradiction    Interestingly however  deleting variables from edge labels or removing edges from edge labeled join graphs whose clusters are fixed will not increase the independencies captured by edge labeled join graphs  That is  Proposition   Any two  edge labeled  join graphs defined on the same set of clusters  sharing  V        express exactly the same set of independencies relative to edge separation  and this set of independencies is identical to the one expressed by node separation in the primal graph of the join graph  Proof  This follows by looking at the primal graph of the join graph  obtained by connecting any two nodes in a cluster by an arc over the original variables as nodes  and observing that any edgeseparation in a join graph corresponds to a node separation in the primal graph and vice versa    Hence  the issue of minimizing computational over counting due to cycles appears to be unrelated to the problem of maximizing independencies via minimal I mapness  Nevertheless  to avoid over counting as much as possible  we still prefer join graphs that minimize cycles relative to each variable  That is  we prefer minimal edge labeled join graphs  Relationship with region graphs There is a strong relationship between our join graphs and the region graphs of Yedidia et al                      Their approach was inspired by advances in statistical physics  when it was realized that computing the partition function is essentially the same combinatorial problem that expresses probabilistic reasoning  As a result  variational methods from physics could have counterparts in reasoning algorithms  It was proved by Yedidia et al               that belief propagation on loopy networks can only converge  when it does so  to stationary points of the Bethe free energy  The Bethe approximation is only the simplest case of the more general Kikuchi        cluster variational method  The idea is to group the variables together in clusters and perform exact computation in each cluster  One key question is then how to aggregate the results  and how to account for the variables that are shared between clusters  Again  the idea that everything should be counted exactly once is very important  This led to the proposal of region graphs  Yedidia et al               and the associated counting numbers for regions  They are given as a possible canonical version of graphs that can support Generalized Belief Propagation  GBP  algorithms  The join graphs accomplish the same thing  The edge labeled join graphs can be described as region graphs where the regions are the clusters and the labels on the edges  The tree ness condition with respect to every variable ensures that there is no over counting  A very similar approach to ours  which is also based on join graphs appeared independently by McEliece and Yildirim         and it is based on an information theoretic perspective         J OIN  G RAPH P ROPAGATION A LGORITHMS  Algorithm Iterative Join Graph Propagation  IJGP  Input An arc labeled join graph decomposition hJG      i  JG    V  E  for B   hX  D  G  P i  Evidence variables var e   Output An augmented graph whose nodes are clusters containing the original CPTs and the messages received from neighbors  Approximations of P  Xi  e   Xi  X  Denote by h u v  the message from vertex u to v  nev  u  the neighbors of u in JG excluding v  cluster u     u    h v u    v  u   E   clusterv  u    cluster u  excluding message from v to u   One iteration of IJGP  For every node u in JG in some topological order d and back  do    Process observed variables  Assign relevant evidence to all pi   u   u      u   var e   u  V    Compute individual functions  Include in H u v  each function in clusterv  u  whose scope does not contain variables in elim u  v   Denote by A the remaining functions  P Q    Compute and send to v the combined function  h u v    elim u v  f A f   Send h u v  and the individual functions H u v  to node v  Endfor  Compute an approximation of P  Xi  e   For every Xi  X let u be P a vertex in JG Q such that Xi   u   Compute P  Xi   e      u  Xi     f cluster u  f    Figure     Algorithm Iterative Join Graph Propagation  IJGP       Algorithm IJGP Applying CTE iteratively to minimal edge labeled join graphs yields our algorithm Iterative JoinGraph Propagation  IJGP  described in Figure     One iteration of the algorithm applies messagepassing in a topological order over the join graph  forward and back  When node u sends a message  or messages  to a neighbor node v it operates on all the CPTs in its cluster and on all the messages sent from its neighbors excluding the ones received from v  First  all individual functions that share no variables with the eliminator are collected and sent to v  All the rest of the functions are combined in a product and summed over the eliminator between u and v  Based on the results by Lauritzen and Spiegelhalter        and Larrosa  Kask  and Dechter        it can be shown that  T HEOREM      If IJGP is applied to a join tree decomposition it reduces to join tree clustering  and therefore it is guaranteed to compute the exact beliefs in one iteration      The time complexity of one iteration of IJGP is O deg   n   N    dw      and its space complexity is O N  d    where deg is the maximum degree of a node in the join graph  n is the number of variables  N is the number of nodes in the graph decomposition  d is the maximum domain size  w is the maximum cluster size and  is the maximum label size  For proof  see the properties of CTE presented by Kask et al                 M ATEESCU   K ASK   G OGATE   D ECHTER     A   B  C  a   A  AB     A A  B  b      ABC    AB  A  A    AB  ABC  c   Figure     a  A belief network  b  A dual join graph with singleton labels  c  A dual join graph which is a join tree   The special case of Iterative Belief Propagation Iterative belief propagation  IBP  is an iterative application of Pearls algorithm that was defined for poly trees  Pearl         to any Bayesian network  We will describe IBP as an instance of join graph propagation over a dual join graph  D EFINITION     dual graphs  dual join graphs  Given a set of functions F    f            fl   over scopes S            Sl   the dual graph of F is a graph DG    V  E  L  that associates a node with each function  namely V   F and an edge connects any two nodes whose functions scope share a variable  E     fi   fj   Si  Sj       L is a set of labels for the edges  each edge being labeled by the shared variables of its nodes  L    lij   Si  Sj   i  j   E   A dual join graph is an edgelabeled edge subgraph of DG that satisfies the connectedness property  A minimal dual join graph is a dual join graph for which none of the edge labels can be further reduced while maintaining the connectedness property  Interestingly  there may be many minimal dual join graphs of the same dual graph  We will define Iterative Belief Propagation on any dual join graph  Each node sends a message over an edge whose scope is identical to the label on that edge  Since Pearls algorithm sends messages whose scopes are singleton variables only  we highlight minimal singleton label dual join graphs  Proposition   Any Bayesian network has a minimal dual join graph where each edge is labeled by a single variable  Proof  Consider a topological ordering of the nodes in the acyclic directed graph of the Bayesian network d   X            Xn   We define the following dual join graph  Every node in the dual graph D  associated with pi is connected to node pj   j   i if Xj  pa Xi    We label the edge between pj and pi by variable Xj   namely lij    Xj    It is easy to see that the resulting edge labeled subgraph of the dual graph satisfies connectedness   Take the original acyclic graph G and add to each node its CPT family  namely all the other parents that precede it in the ordering  Since G already satisfies connectedness so is the minimal graph generated   The resulting labeled graph is a dual graph with singleton labels     Example   Consider the belief network on   variables A  B  C with CPTs   P  C A  B     P  B A  and   P  A   given in Figure   a  Figure   b shows a dual graph with singleton labels on the edges  Figure   c shows a dual graph which is a join tree  on which belief propagation can solve the problem exactly in one iteration  two passes up and down the tree          J OIN  G RAPH P ROPAGATION A LGORITHMS  Algorithm Iterative Belief Propagation  IBP  Input  An edge labeled dual join graph DG    V  E  L  for a Bayesian network B   hX  D  G  P i  Evidence e  Output  An augmented graph whose nodes include the original CPTs and the messages received from neighbors  Approximations of P  Xi  e   Xi  X  Approximations of P  Fi  e   Fi  B  Denote by  hvu the message from u to v  ne u  the neighbors of u in V   nev  u    ne u    v   luv the label of  u  v   E  elim u  v    scope u   scope v    One iteration of IBP For every node u in DJ in a topological order and back  do     Process observed variables Assign evidence variables to the each pi and remove them from the labeled edges     Compute and send to v the function  X Y hvu    pu  hui   elim u v    hu i  inev  u    Endfor  Compute approximations of P  Fi  e   P  Xi  e   For every Xi QX let u be the vertex of family Fi in DJ  P  Fi   e      hu  une i  hui    pu   P i P  Xi   e     scope u  Xi   P  Fi   e    Figure     Algorithm Iterative Belief Propagation  IBP   For completeness  we present algorithm IBP  which is a special case of IJGP  in Figure     It is easy to see that one iteration of IBP is time and space linear in the size of the belief network  It can be shown that when IBP is applied to a minimal singleton labeled dual graph it coincides with Pearls belief propagation applied directly to the acyclic graph representation  Also  when the dual join graph is a tree IBP converges after one iteration  two passes  up and down the tree  to the exact beliefs      Bounded Join Graph Decompositions Since we want to control the complexity of join graph algorithms  we will define it on decompositions having bounded cluster size  If the number of variables in a cluster is bounded by i  the time and space complexity of processing one cluster is exponential in i  Given a join graph decomposition D   hJG      i  the accuracy and complexity of the  iterative  join graph propagation algorithm depends on two different width parameters  defined next  D EFINITION     external and internal widths  Given an edge labeled join graph decomposition D   hJG      i of a network B   hX  D  G  P i  the internal width of D is maxvV   v    while the external width of D is the treewidth of JG as a graph  Using this terminology we can now state our target decomposition more clearly  Given a graph G  and a bounding parameter i we wish to find a join graph decomposition D of G whose internal width is bounded by i and whose external width is minimized  The bound i controls the complexity of join graph processing while the external width provides some measure of its accuracy and speed of convergence  because it measures how close the join graph is to a join tree        M ATEESCU   K ASK   G OGATE   D ECHTER  Algorithm Join Graph Structuring i     Apply procedure schematic mini bucket i      Associate each resulting mini bucket with a node in the join graph  the variables of the nodes are those appearing in the mini bucket  the original functions are those in the minibucket     Keep the edges created by the procedure  called out edges  and label them by the regular separator     Connect the mini bucket clusters belonging to the same bucket in a chain by in edges labeled by the single variable of the bucket   Figure     Algorithm Join Graph Structuring i   Procedure Schematic Mini Bucket i     Order the variables from X  to Xn minimizing  heuristically  induced width  and associate a bucket for each variable     Place each CPT in the bucket of the highest index variable in its scope     For j   n to   do  Partition the functions in bucket Xj   into mini buckets having at most i variables  For each mini bucket mb create a new scope function  message  f where scope f      X X  mb    Xi   and place scope f  in the bucket of its highest variable  Maintain an edge between mb and the mini bucket  created later  of f    Figure     Procedure Schematic Mini Bucket i   We can consider two classes of algorithms  One class is partition based  It starts from a given tree decomposition and then partitions the clusters until the decomposition has clusters bounded by i  An alternative approach is grouping based  It starts from a minimal dual graph based join graph decomposition  where each cluster contains a single CPT  and groups clusters into larger clusters as long as the resulting clusters do not exceed the given bound  In both methods one should attempt to reduce the external width of the generated graph decomposition  Our partition based approach inspired by the mini bucket idea  Dechter   Rish        is as follows  Given a bound i  algorithm Join Graph Structuring i  applies the procedure Schematic MiniBucket i   described in Figure     The procedure only traces the scopes of the functions that would be generated by the full mini bucket procedure  avoiding actual computation  The procedure ends with a collection of mini bucket trees  each rooted in the mini bucket of the first variable  Each of these trees is minimally edge labeled  Then  in edges labeled with only one variable are introduced  and they are added only to obtain the running intersection property between branches of these trees  Proposition   Algorithm Join Graph Structuring i  generates a minimal edge labeled join graph decomposition having bound i  Proof  The construction of the join graph specifies the vertices and edges of the join graph  as well as the variable and function labels of each vertex  We need to demonstrate that    the connectedness property holds  and    that edge labels are minimal         J OIN  G RAPH P ROPAGATION A LGORITHMS  G   GFE   GFE  P G F E  EF  E   EBF    EF   EBF  P E B F   P F C D   F   FCD    BF   BF F  FCD  BF  CD  D   DB    CD   P D B   CDB CB  C   CAB   CB   P C A B   B CAB BA  B   BA    AB   A   A    A    B   P B A   BA A P A   A   b    a   Figure     Join graph decompositions  Connectedness property specifies that for any   vertices u and v  if vertices u and v contain variable X  then there must be a path u  w            wm   v between u and v such that every vertex on this path contains variable X  There are two cases here     u and v correspond to   mini buckets in the same bucket  or    u and v correspond to mini buckets in different buckets  In case   we have   further cases   a  variable X is being eliminated in this bucket  or  b  variable X is not eliminated in this bucket  In case  a  each mini bucket must contain X and all mini buckets of the bucket are connected as a chain  so the connectedness property holds  In case  b  vertexes u and v connect to their  respectively  parents  who in turn connect to their parents  etc  until a bucket in the scheme where variable X is eliminated  All nodes along this chain connect variable X  so the connectedness property holds  Case   resolves like case  b  To show that edge labels are minimal  we need to prove that there are no cycles with respect to edge labels  If there is a cycle with respect to variable X  then it must involve at least one in edge  edge connecting two mini buckets in the same bucket   This means variable X must be the variable being eliminated in the bucket of this in edge  That means variable X is not contained in any of the parents of the mini buckets of this bucket  Therefore  in order for the cycle to exist  another in edge down the bucket tree from this bucket must contain X  However  this is impossible as this would imply that variable X is eliminated twice     Example   Figure   a shows the trace of procedure schematic mini bucket    applied to the problem described in Figure  a  The decomposition in Figure   b is created by the algorithm graph structuring  The only cluster partitioned is that of F into two scopes  FCD  and  BF   connected by an in edge labeled with F  A range of edge labeled join graphs is shown in Figure     On the left side we have a graph with smaller clusters  but more cycles  This is the type of graph IBP works on  On the right side we have a tree decomposition  which has no cycles at the expense of bigger clusters  In between  there could be a number of join graphs where maximum cluster size can be traded for number of cycles  Intuitively  the graphs on the left present less complexity for join graph algorithms because the cluster size is smaller  but they are also likely to be less accurate  The graphs on the right side       M ATEESCU   K ASK   G OGATE   D ECHTER  A  A  A  C  ABC  AB ABDE  BC  BE  C  A  A  ABC  AB  C  BCE  C BC  BC  ABDE  ABCDE  DE  CE  CDE  C DE  CE  CE  CDEF  CDEF  CDEF  CDEF F  FGH  H  FGH  H  FGI  GH  GI  F  H  GHIJ  H  FGH H F  F  F FG  ABCDE  BCE  C DE  BCE  FGI  GI  F F  GH  GHIJ  FGI  GH  GI  GHI GHIJ  FGHI  GHIJ  more accuracy less complexity  Figure     Join graphs  are computationally more complex  because of the larger cluster size  but they are likely to be more accurate      The Inference Power of IJGP The question we address in this subsection is why propagating the messages iteratively should help  Why is IJGP upon convergence superior to IJGP with one iteration and superior to MC  One clue can be provided when considering deterministic constraint networks which can be viewed as extreme probabilistic networks  It is known that constraint propagation algorithms  which are analogous to the messages sent by belief propagation  are guaranteed to converge and are guaranteed to improve with iteration  The propagation scheme of IJGP works similar to constraint propagation relative to the flat network abstraction of the probability distribution  where all non zero entries are normalized to a positive constant   and propagation is guaranteed to be more accurate for that abstraction at least  In the following we will shed some light on the IJGPs behavior by making connections with the well known concept of arc consistency from constraint networks  Dechter         We show that   a  if a variable value pair is assessed as having a zero belief  it remains as zero belief in subsequent iterations   b  that any variable value zero beliefs computed by IJGP are correct   c  in terms of zero non zero beliefs  IJGP converges in finite time  We have also empirically investigated the hypothesis that if a variable value pair is assessed by IBP or IJGP as having a positive but very close to zero belief  then it is very likely to be correct  Although the experimental results shown in this paper do not contradict this hypothesis  some examples in more recent experiments by Dechter  Bidyuk  Mateescu  and Rollon        invalidate it         J OIN  G RAPH P ROPAGATION A LGORITHMS        IJGP  AND  A RC  C ONSISTENCY  For any belief network we can define a constraint network that captures the assignments having strictly positive probability  We will show a correspondence between IJGP applied to the belief network and an arc consistency algorithm applied to the constraint network  Since arc consistency algorithms are well understood  this correspondence not only proves the target claims  but may provide additional insight into the behavior of IJGP  It justifies the iterative application of belief propagation  and it also illuminates its distance from being complete  D EFINITION     constraint satisfaction problem  A Constraint Satisfaction Problem  CSP  is a triple hX  D  Ci  where X    X            Xn   is a set of variables associated with a set of discretevalued domains D    D            Dn   and a set of constraints C    C            Cm    Each constraint Ci is a pair hSi   Ri i where Ri is a relation Ri  DSi defined on a subset of variables Si  X and DSi is a Cartesian product of the domains of variables Si   The relation Ri denotes all compatible tuples of DSi allowed by the constraint  A projection operator  creates a new relation  Sj  Ri      x x  DSj and y  y  DSi  Sj and xy  Ri    where Sj  Si   Constraints can be combined with the join operator    resulting in a new relation  Ri   Rj    x Si  x   Ri and Sj  x   Rj    A solution is an assignment of values to all the variables x    x            xn    xi  Di   such that Ci  C  xSi  Ri   The constraint network represents its set of solutions   i Ci   Given a belief network B  we define a flattening of the Bayesian network into a constraint network called f lat B   where all the zero entries in a probability table are removed from the corresponding relation  The network f lat B  is defined over the same set of variables and has the same set of domain values as B  D EFINITION     flat network  Given a Bayesian network B   hX  D  G  P i  the flat network f lat B  is a constraint network  where the set of variables is X  and for every Xi  X and its CPT P  Xi  pa Xi     B we define a constraint RFi over the family of Xi   Fi    Xi    pa Xi   as follows  for every assignment x    xi   xpa Xi     to Fi    xi   xpa Xi      RFi iff P  xi  xpa Xi          T HEOREM   Given a belief network B   hX  D  G  P i  where X    X            Xn    for any tuple x    x            xn    PB  x       x  sol f lat B    where sol f lat B   is the set of solutions of the flat constraint network  Proof  PB  x       ni   P  xi  xpa Xi          i              n   P  xi  xpa Xi          i              n    xi   xpa Xi      RFi  x  sol f lat B      Constraint propagation is a class of polynomial time algorithms that are at the center of constraint processing techniques  They were investigated extensively in the past three decades and the most well known versions are arc   path   and i consistency  Dechter               D EFINITION     arc consistency   Mackworth        Given a binary constraint network  X  D  C   the network is arc consistent iff for every binary constraint Rij  C  every value v  Di has a value u  Dj s t   v  u   Rij          M ATEESCU   K ASK   G OGATE   D ECHTER  Note that arc consistency is defined for binary networks  namely the relations involve at most two variables  When a binary constraint network is not arc consistent  there are algorithms that can process it and enforce arc consistency  The algorithms remove values from the domains of the variables that violate arc consistency until an arc consistent network is generated  There are several versions of improved performance arc consistency algorithms  however we will consider a non optimal distributed version  which we call distributed arc consistency  D EFINITION     distributed arc consistency algorithm  The algorithm distributed arcconsistency is a message passing algorithm over a constraint network  Each node is a variable  and maintains a current set of viable values Di   Let ne i  be the set of neighbors of Xi in the constraint graph  Every node Xi sends a message to any node Xj  ne i   which consists of the values in Xj s domain that are consistent with the current Di   relative to the constraint Rji that they share  Namely  the message that Xi sends to Xj   denoted by Dij   is  Dij  j  Rji   Di         Di  Di    kne i  Dki         and in addition node i computes   Clearly the algorithm can be synchronized into iterations  where in each iteration every node computes its current domain based on all the messages received so far from its neighbors  Eq      and sends a new message to each neighbor  Eq      Alternatively  Equations   and   can be combined  The message Xi sends to Xj is  Dij  j  Rji   Di  kne i  Dki         Next we will define a join graph decomposition for the flat constraint network so that we can establish a correspondence between the join graph decomposition of a Bayesian network B and the join graph decomposition of its flat network f lat B   Note that for constraint networks  the edge labeling  can be ignored  D EFINITION     join graph decomposition of the flat network  Given a join graph decomposition D   hJG      i of a Bayesian network B  the join graph decomposition Df lat   hJG    f lat i of the flat constraint network f lat B  has the same underlying graph structure JG    V  E  as D  the same variable labeling of the clusters   and the mapping f lat maps each cluster to relations corresponding to CPTs  namely Ri  f lat  v  iff CPT pi   v   The distributed arc consistency algorithm of Definition    can be applied to the join graph decomposition of the flat network  In this case  the nodes that exchange messages are the clusters  namely the elements of the set V of JG   The domain of a cluster of V is the set of tuples of the join of the original relations in the cluster  namely the domain of cluster u is   Rf lat  u  R   The constraints are binary  and involve clusters of V that are neighbors  For two clusters u and v  their corresponding values tu and tv  which are tuples representing full assignments to the variables in the cluster  belong to the relation Ruv  i e    tu   tv    Ru v   if the projections over the separator  or labeling   between u and v are identical  namely   u v   tu     u v   tv         J OIN  G RAPH P ROPAGATION A LGORITHMS  We define below the algorithm relational distributed arc consistency  RDAC   that applies distributed arc consistency to any join graph decomposition of a constraint network  We call it relational to emphasize that the nodes exchanging messages are in fact relations over the original problem variables  rather than simple variables as is the case for arc consistency algorithms  D EFINITION     relational distributed arc consistency algorithm  RDAC over a join graph  Given a join graph decomposition of a constraint network  let Ri and Rj be the relations of two clusters  Ri and Rj are the joins of the respective constraints in each cluster   having the scopes Si and Sj   such that Si  Sj      The message Ri sends to Rj denoted h i j  is defined by  h i j   Si Sj  Ri         where ne i     j Si  Sj      is the set of relations  clusters  that share a variable with Ri   Each cluster updates its current relation according to  Ri  Ri     kne i  h k i          Algorithm RDAC iterates until there is no change  Equations   and   can be combined  just like in Equation    The message that Ri sends to Rj becomes  h i j   Si Sj  Ri     kne i  h k i           To establish the correspondence with IJGP  we define the algorithm IJGP RDAC that applies RDAC in the same order of computation  schedule of processing  as IJGP  D EFINITION     IJGP RDAC algorithm  Given the Bayesian network B   hX  D  G  P i  let Df lat   hJG    f lat   i be any join graph decomposition of the flat network f lat B   The algorithm IJGP RDAC is applied to the decomposition Df lat of f lat B   and can be described as IJGP applied to D  with the following modifications  Q    Instead of   we use    P    Instead of   we use      At end end  we update the domains of variables by  Di  Di  Xi    vne u  h v u        R u  R         where u is the cluster containing Xi   Note that in algorithm IJGP RDAC  we could first merge all constraints in each cluster u into a single constraint Ru   R u  R  From our construction  IJGP RDAC enforces arc consistency over the join graph decomposition of the flat network  When the join graph Df lat is a join tree  IJGP RDAC solves the problem namely it finds all the solutions of the constraint network         M ATEESCU   K ASK   G OGATE   D ECHTER  Proposition   Given the join graph decomposition Df lat   hJG    f lat   i  JG    V  E   of the flat constraint network f lat B   corresponding to a given join graph decomposition D of a Bayesian network B   hX  D  G  P i  the algorithm IJGP RDAC applied to Df lat enforces arcconsistency over the join graph Df lat   Proof  IJGP RDAC applied to the join graph decomposition Df lat   hJG    f lat   i  JG    V  E   is equivalent to applying RDAC of Definition    to a constraint network that has vertices V as its variables and   R u  R u  V   as its relations    Following the properties of convergence of arc consistency  we can show that  Proposition   Algorithm IJGP RDAC converges in O m  r  iterations  where m is the number of edges in the join graph and r is the maximum size of a separator Dsep u v  between two clusters  Proof  This follows from the fact messages  which are relations  between clusters in IJGP RDAC change monotonically  as tuples are only successively removed from relations on separators  Since the size of each relation on a separator is bounded by r and there are m edges  no more than O mr  iterations will be needed    In the following we will establish an equivalence between IJGP and IJGP RDAC in terms of zero probabilities  Proposition   When IJGP and IJGP RDAC are applied in the same order of computation  the messages computed by IJGP are identical to those computed by IJGP RDAC in terms of zero   nonzero probabilities  That is  h u v   x       in IJGP iff x  h u v  in IJGP RDAC  Proof  The proof is by induction  The base case is trivially true since messages h in IJGP are initialized to a uniform distribution and messages h in IJGP RDAC are initialized to complete relations  The induction step  Suppose that hIJGP  u v  is the message sent from u to v by IJGP  We will show IJGP RDAC IJGP RDAC that if hIJGP where h u v  is the message sent by IJGP u v   x        then x  h u v  RDAC from u to v  Assume that the claim holds for all messages received by u from its neighbors  Let f  clusterv  u  in IJGP and Rf be the corresponding relation in IJGP RDAC  and P Q t be an asIJGP signment of values to variables in elim u  v   We have h u v   x        elim u v  f f  x       Q  t  f f  x  t        t  f  f  x  t        t  f  scope Rf    x  t   Rf  t  elim u v    Rf IJGP RDAC IJGP RDAC      x  h u v  scope Rf    x  t    h u v   Next we will show that IJGP computing marginal probability P  Xi   xi       is equivalent to IJGP RDAC removing xi from the domain of variable Xi   Proposition   IJGP computes P  Xi   xi       iff IJGP RDAC decides that xi   Di   Proof  According to Proposition   messages computed by IJGP and IJGP RDAC are identical in terms of zero probabilities  Let f  cluster u  in IJGP and Rf be the corresponding relation in IJGP RDAC  and t be an assignment of values to variables in  u  Xi   We will show that when IJGP computes P  Xi   xi        upon convergence   then IJGP RDAC computes xi   Di   We       J OIN  G RAPH P ROPAGATION A LGORITHMS  Q Q P have P  Xi   xi     f f  xi   t       t  f  f  xi   t       f f  xi        t  X Xi t  Rf   scope Rf    xi   t    Rf  t   xi   t      Rf Rf  xi   t    xi   Di  Xi   Rf Rf  xi   t    xi   Di   Since arc consistency is sound  so is the decision of zero probabilities    Next we will show that P  Xi   xi       computed by IJGP is sound  T HEOREM   Whenever IJGP finds P  Xi   xi        then the probability P  Xi   expressed by the Bayesian network conditioned on the evidence is   as well  Proof  According to Proposition    whenever IJGP finds P  Xi   xi        the value xi is removed from the domain Di by IJGP RDAC  therefore value xi  Di is a no good of the network f lat B   and from Theorem   it follows that PB  Xi   xi          In the following we will show that the time it takes IJGP to find all P  Xi   xi       is bounded  Proposition   IJGP finds all P  Xi   xi       in finite time  that is  there exists a number k  such that no P  Xi   xi       will be found after k iterations  Proof  This follows from the fact that the number of iterations it takes for IJGP to compute P  Xi   xi       is exactly the same number of iterations IJGP RDAC takes to remove xi from the domain Di  Proposition   and Proposition     and the fact the IJGP RDAC runtime is bounded  Proposition       Previous results also imply that IJGP is monotonic with respect to zeros  Proposition    Whenever IJGP finds P  Xi   xi        it stays   during all subsequent iterations  Proof  Since we know that relations in IJGP RDAC are monotonically decreasing as the algorithm progresses  it follows from the equivalence of IJGP RDAC and IJGP  Proposition    that IJGP is monotonic with respect to zeros           A F INITE P RECISION P ROBLEM On finite precision machines there is the danger that an underflow can be interpreted as a zero value  We provide here a warning that an implementation of belief propagation should not allow the creation of zero values by underflow  We show an example in Figure    where IBPs messages converge in the limit  i e   in an infinite number of iterations   but they do not stabilize in any finite number of iterations  If all the nodes Hk are set to value    the belief for any of the Xi variables as a function of iteration is given in the table in Figure     After about     iterations  the finite precision of our computer is not able to represent the value for Bel Xi       and this appears to be zero  yielding the final updated belief              when in fact the true updated belief should be            Notice that             cannot be regarded as a legitimate fixed point for IBP  Namely  if we would initialize IBP with the values              then the algorithm would maintain them  appearing to have a fixed point  but initializing IBP with zero values cannot be expected to be correct  When we        M ATEESCU   K ASK   G OGATE   D ECHTER  X   Prior for Xi  H   H   X   X   Xi  P   Xi                           H  Hk Xi  CPT for Hk  Xj  P   Hk   Xi   Xj                                                        iter  Bel Xi      Bel Xi      Bel Xi                                                                                              e                      e                      True belief           Figure     Example of a finite precision problem  initialize with zeros we forcibly introduce determinism in the model  and IBP will always maintain it afterwards  However  this example does not contradict our theory because  mathematically  Bel Xi      never becomes a true zero  and IBP never reaches a quiescent state  The example shows that a close to zero belief network can be arbitrarily inaccurate  In this case the inaccuracy seems to be due to the initial prior belief which are so different from the posterior ones        ACCURACY OF IBP ACROSS B ELIEF D ISTRIBUTION We present an empirical evaluation of the accuracy of IBPs prediction for the range of belief distribution from   to    These results also extend to IJGP  In the previous section  we proved that zero values inferred by IBP are correct  and we wanted to test the hypothesis that this property extends to   small beliefs  namely  that are very close to zero   That is  if IBP infers a posterior belief close to zero  then it is likely to be correct  The results presented in this paper seem to support the hypothesis  however new experiments by Dechter et al         show that it is not true in general  We do not have yet a good characterization of the cases when the hypothesis is confirmed  To test this hypothesis  we computed the absolute error of IBP per intervals of         For a given interval  a  b   where    a   b     we use measures inspired from information retrieval  Recall Absolute Error and Precision Absolute Error  Recall is the absolute error averaged over all the exact posterior beliefs that fall into the interval  a  b   For Precision  the average is taken over all the approximate posterior belief values computed by IBP to be in the interval  a  b   Intuitively  Recall  a b   indicates how far the belief computed by IBP is from the exact  when the exact is in  a  b   Precision  a b   indicates how far the exact is from IBPs prediction  when the value computed by IBP is in  a  b   Our experiments show that the two measures are strongly correlated  We also show the histograms of distribution of belief for each interval  for the exact and for IBP  which are also strongly correlated  The results are given in Figures    and     The left Y axis corresponds to the histograms  the bars   the right Y axis corresponds to the absolute error  the lines   We present results for two classes of problems  coding networks and grid network  All problems have binary variables  so the graphs are symmetric about     and we only show the interval           The number of variables  number of iterations and induced width w  are reported for each graph         J OIN  G RAPH P ROPAGATION A LGORITHMS  Recall Abs  Error  noise         noise              Absolute Error                                                                                         Precision Abs  Error                                                                                                                                                                                                                                                  IBP Histogram            Percentage  Exact Histogram      noise         Figure     Coding  N           instances  w      Recall Abs  Error  evidence      evidence                                Absolute Error                                                                                                                                                                                                                                                          Precision Abs  Error        IBP Histogram             Percentage  Exact Histogram      evidence       Figure       x   grids      instances  w      Coding networks IBP is famously known to have impressive performance on coding networks  We tested on linear block codes  with    nodes per layer and   parent nodes  Figure    shows the results for three different values of channel noise           and      For noise      all the beliefs computed by IBP are extreme  The Recall and Precision are very small  of the order of        So  in this case  all the beliefs are very small    small  and IBP is able to infer them correctly  resulting in almost perfect accuracy  IBP is indeed perfect in this case for the bit error rate   When the noise is increased  the Recall and Precision tend to get closer to a bell shape  indicating higher error for values close to     and smaller error for extreme values  The histograms also show that less belief values are extreme as the noise is increased  so all these factors account for an overall decrease in accuracy as the channel noise increases  These networks are examples with a large number of   small probabilities and IBP is able to infer them correctly  absolute error is small   Grid networks We present results for grid networks in Figure     Contrary to the case of coding networks  the histograms show higher concentration of beliefs around      However  the accuracy is still very good for beliefs close to zero  The absolute error peaks close to   and maintains a plateau  as evidence is increased  indicating less accuracy for IBP      Experimental Evaluation As we anticipated in the summary of Section    and as can be clearly seen now by the structuring of a bounded join graph  there is a close relationship between the mini clustering algorithm MC i        M ATEESCU   K ASK   G OGATE   D ECHTER  IBP  it           MC   evid                                                                                                       Absolute error IJGP i   i                                                                                                                                                    IBP i                                                                                                                                                                                                                              Relative error IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                              KL distance IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                     Time IJGP i                                                                   i                                                                   i                                                                                                                                   Table    Random networks  N     K    C     P        instances  w       and IJGP i   In particular  one iteration of IJGP i  is similar to MC i   MC sends messages up and down along the clusters that form a set of trees  IJGP has additional connections that allow more interaction between the mini clusters of the same cluster  Since this is a cyclic structure  iterating is facilitated  with its virtues and drawbacks s In our evaluation of IJGP i   we focus on two different aspects   a  the sensitivity of parametric IJGP i  to its i bound and to the number of iterations   b  a comparison of IJGP i  with publicly available state of the art approximation schemes      Effect of i bound and Number of Iterations We tested the performance of IJGP i  on random networks  on M by M grids  on the two benchmark CPCS files with    and     variables  respectively and on coding networks  On each type of networks  we ran IBP  MC i  and IJGP i   while giving IBP and IJGP i  the same number of iterations  We use the partitioning method described in Section     to construct a join graph  To determine the order of message computation  we recursively pick an edge  u v   such that node u has the fewest incoming messages missing  For each network except coding  we compute the exact solution and compare the accuracy using the absolute and relative error  as before  as well as the KL  Kullback Leibler  distance Pexact  X   a   log Pexact  X   a  Papproximation  X   a   averaged over all values  all variables and all problems  For coding networks we report the Bit Error Rate  BER  computed as described in Section      We also report the time taken by each algorithm  The random networks were generated using parameters  N K C P   where N is the number of variables  K is their domain size  C is the number of conditional probability tables  CPTs  and P is the number of parents in each CPT  Parents in each CPT are picked randomly and each CPT is filled randomly  In grid networks  N is a square number and each CPT is filled randomly  In each problem class  we also tested different numbers of evidence variables  As before  the coding networks are from the class of linear block codes  where  is the channel noise level  Note that we are limited to relatively small and sparse problem instances because our evaluation measures are based on comparing against exact figures  Random networks results for networks having N     K    C    and P   are given in Table   and in Figures    and     For IJGP i  and MC i  we report   different values of i bound           For IBP and IJGP i  we report results for   different numbers of iterations            We report results       J OIN  G RAPH P ROPAGATION A LGORITHMS  Random networks  N     K    P    evid    w            IJGP   it IJGP   it IJGP   it IJGP   it IJGP    it IJGP    it IJGP    it MC IBP   it IBP   it IBP   it IBP   it IBP    it         KL distance                                                                    i bound   a  Performance vs  i bound  Random networks  N     K    P    evid    w            IBP IJGP    IJGP             KL distance                                                            Number of iterations   b  Convergence with iterations   Figure     Random networks  KL distance  for   different numbers of evidence            From Table   and Figure   a we see that IJGP i  is always better than IBP  except when i   and number of iterations is     sometimes by an order of magnitude  in terms of absolute error  relative error and KL distance  IBP rarely changes after   iterations  whereas IJGP i s solution can be improved with more iterations  up to         As theory predicted  the accuracy of IJGP i  for one iteration is about the same as that of MC i   But IJGP i  improves as the number of iterations increases  and is eventually better than MC i  by as much as an order of magnitude  although it clearly takes more time  especially when the i bound is large         M ATEESCU   K ASK   G OGATE   D ECHTER  Random networks  N     K    P    evid    w         IJPG   it IJGP   it IJGP   it IJGP   it IJGP    it IJGP    it IJGP    it MC IBP   it IBP    it  Time  seconds                                                                  i bound  Figure     Random networks  Time  IBP  it           MC   evid                                                                                                       Absolute error IJGP i   i                                                                                                                                                    IBP i                                                                                                                                                                                                                              Relative error IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                              KL distance IJGP i                                                                            i                                                                            IBP i                                                                                                                                                                                                                     Time IJGP i                                                                   i                                                                   i                                                                                                                                   Table     x  grid  K        instances  w       Figure   a shows a comparison of all algorithms with different numbers of iterations  using the KL distance  Because the network structure changes with different i bounds  we do not necessarily see monotonic improvement of IJGP with i bound for a given number of iterations  as is the case with MC   Figure   b shows how IJGP converges with more iterations to a smaller KL distance than IBP  As expected  the time taken by IJGP  and MC  varies exponentially with the i bound  see Figure      Grid networks results with networks of N     K        instances are very similar to those of random networks  They are reported in Table   and in Figure     where we can see the impact of having evidence    and   evidence variables  on the algorithms  IJGP at convergence gives the best performance in both cases  while IBPs performance deteriorates with more evidence and is surpassed by MC with i bound   or larger  CPCS networks results with CPCS   and CPCS    are given in Table   and Figure     and are even more pronounced than those of random and grid networks  When evidence is added  IJGP i  is more accurate than MC i   which is more accurate than IBP  as can be seen in Figure   a  Coding networks results are given in Table    We tested on large networks of     variables  with treewidth w      with IJGP and IBP set to run    iterations  this is more than enough to ensure       J OIN  G RAPH P ROPAGATION A LGORITHMS  Grid network  N     K    evid    w           IJGP   it IJGP   it IJGP   it IJGP   it IJGP    it MC IBP   it IBP   it IBP   it IBP   it IBP    it         KL distance                                                                    i bound   a  Performance vs  i bound  Grid network  N     K    evid    w      e   IJGP    iterations  at convergence   e    KL distance   e     e     e     e     e                                         i bound   b  Fine granularity for KL   Figure     Grid  x   KL distance  convergence   IBP is known to be very accurate for this class of problems and it is indeed better than MC  However we notice that IJGP converges to slightly smaller BER than IBP even for small values of the i bound  Both the coding network and CPCS    show the scalability of IJGP for large size problems  Notice that here the anytime behavior of IJGP is not clear  In summary  we see that IJGP is almost always superior to both IBP and MC i  and is sometimes more accurate by several orders of magnitude  One should note that IBP cannot be improved with more time  while MC i  requires a large i bound for many hard and large networks to achieve reasonable accuracy  There is no question that the iterative application of IJGP is instrumental to its success  In fact  IJGP    in isolation appears to be the most cost effective variant         M ATEESCU   K ASK   G OGATE   D ECHTER  IBP  it            MC          MC   evid  Absolute error IJGP i   i    Relative error IJGP i    IBP i    i                                                                                                                                                                             KL distance IJGP i    IBP i    CPCS                                                                                                                                                                                                    Time IBP  i                               e                       e                                                                                      e                                                                                               e                            i                                                                      e                                                                                           i    IJGP i    i                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          e      e                    CPCS                                                                                                                                                                                                                                                                                                                                                                                                                             e                      e      e              e                                           e                                                                                                                                                                                                                                               Table    CPCS      instances  w      CPCS       instances  w             IJGP MC      IJGP MC      IJGP MC      IJGP MC      IJGP MC      IJGP MC                                                                                                     IJGP         MC          Bit Error Rate i bound                                                                                                                                                                                                                                                                                                       Time                                                                                                                                                      IBP                                                                            Table    Coding networks  N      P        instances     iterations  w           Comparing IJGP with Other Algorithms In this section we provide a comparison of IJGP with state of the art publicly available schemes  The comparison is based on a recent evaluation of algorithms performed at the Uncertainty in AI      conference    We will present results on solving the belief updating task  also called the task of computing posterior node marginals   MAR   We first give a brief overview of the schemes that we experimented and compared with     EDBP   Edge Deletion for Belief Propagation    Complete results are available at http   graphmod ics uci edu uai   Evaluation Report         J OIN  G RAPH P ROPAGATION A LGORITHMS  CPCS     evid     w          IJGP   it IJGP    it IJGP    it MC IBP   it IBP    it IBP    it             KL distance                                                                                 i bound   a  Performance vs  i bound  CPCS     evid     w      e   IJGP    iterations  at convergence   e    KL distance   e     e     e     e                                          i bound   b  Fine granularity for KL   Figure     CPCS     KL distance  EDBP  Choi   Darwiche      a      b  is an approximation algorithm for Belief Updating  It solves exactly a simplified version of the original problem  obtained by deleting some of the edges of the problem graph  Edges to be deleted are selected based on two criteria  quality of approximation and complexity of computation  tree width reduction   Information loss from lost dependencies is compensated for by introducing auxiliary network parameters  This method corresponds to Iterative Belief Propagation  IBP  when enough edges are deleted to yield a poly tree  and corresponds to generalized BP otherwise     TLSBP   A truncated Loop series Belief propagation algorithm        M ATEESCU   K ASK   G OGATE   D ECHTER  TLSBP is based on the loop series expansion formula of Chertkov and Chernyak        which specifies a series of terms that need to be added to the solution output by BP so that the exact solution can be recovered  This series is basically a sum over all so called generalized loops in the graph  Unfortunately  because the number of these generalized loops can be prohibitively large  the series is of little value  The idea in TLSBP is to truncate the series by decomposing all generalized loops into simple and smaller loops  thus limiting the number of loops to be summed  In our evaluation  we used an implementation of TLSBP available from the work of Gomez  Mooji  and Kappen         The implementation can handle binary networks only     EPIS   Evidence Pre propagation Importance Sampling EPIS  Yuan   Druzdzel        is an importance sampling algorithm for Belief Updating  It is well known that sampling algorithms perform poorly when presented with unlikely evidence  However  when samples are weighted by an importance function  good approximation can be obtained  This algorithm computes an approximate importance function using loopy belief propagation and   cutoff heuristic  We used an implementation of EPIS available from the authors  The implementation works on Bayesian networks only     IJGP   Iterative Join Graph Propagation In the evaluation  IJGP i  was first run with i    until convergence  then with i    until convergence  etc  until i  treewidth  when i bound treewidth  the join graph becomes a join tree and IJGP becomes exact   As preprocessing  the algorithm performed SAT based variable domain pruning by converting zero probabilities in the problem to a SAT problem and performing singleton consistency enforcement  Because the problem size may reduce substantially  in some cases  this preprocessing step may have a significant impact on the time complexity of IJGP  amortized over the increasing i bound  However  for a given i bound  this step improves the accuracy of IJGP only marginally     SampleSearch SampleSearch  Gogate   Dechter        is a specialized importance sampling scheme for graphical models that contain zero probabilities in their CPTs  On such graphical models  importance sampling suffers from the rejection problem in that it generates a large number of samples which have zero weight  SampleSearch circumvents the rejection problem by sampling from the backtrack free search space in which every assignment  sample  is guaranteed to have non zero weight  The backtrack free search space is constructed on the fly by interleaving sampling with backtracking style search  Namely  when a sample is supposed to be rejected because its weight is zero  the algorithm continues instead with systematic backtracking search  until a non zero weight sample is found  For the evaluation version  the importance distribution of SampleSearch was constructed from the output of IJGP with i bound of    For more information on how the importance distribution is constructed from the output of IJGP  see the work by Gogate         The evaluation was conducted on the following benchmarks  see footnote   for details      UAI   MPE   from UAI        instances  Bayesian networks     instances were used      UAI   PE   from UAI        instances  Bayesian networks     instances were used         J OIN  G RAPH P ROPAGATION A LGORITHMS  IJGP EDBP TLSBP EPIS SampleSearch  WCSPs BN O Grids Linkage Promedas UAI   MPE UAI   PE Relational                                  Table    Scope of our experimental study  Score vs KL distance   Score vs KL distance          Score                                                        KL distance  Figure     Score as a function of KL distance     Relational Bayesian networks   constructed from the Primula tool      instances  binary variables  large networks with large tree width  but with high levels of determinism     instances were used      Linkage networks      instances  tree width        Markov networks    instances were used      Grids   from   x   to   x        instances  treewidth           BN O networks   Two layer Noisy OR Bayesian networks     instances  binary variables  up to    variables  treewidth           WCSPs   Weighted CSPs     instances  Markov networks     instances were used      Promedas   real world medical diagnosis      instances  tree width       Markov networks     instances were used    Table   shows the scope of our experimental study  A indicates that the solver was able to  handle the benchmark type and therefore evaluated on it while a lack of a indicates otherwise  We measure the performance of the algorithms in terms of a KL distance based score  Formally  the score of a solver on a problem instance is equal to   avgkld where avgkld is the average KL distance between the exact marginal  which was computed using the UCLA Ace solver  see Chavira   Darwiche        and the approximate marginal output by the solver  If a solver does not output a solution  we consider its KL distance to be   A score lies between   and    with   indicating that the solver outputs exact solution while   indicating that the solver either does not output a solution or has infinite average KL distance  Figure    shows the score as a function of KL distance        M ATEESCU   K ASK   G OGATE   D ECHTER  In Figures       we report the results of experiments with each of the problem sets  Each solver has a timeout of    minutes on each problem instance  when solving a problem  each solver periodically outputs the best solution found so far  Using this  we can compute  for each solver  at any point in time  the total sum of its scores over all problem instances in a particular set  called SumScore t   On the horizontal axis  we have the time and on the vertical axis  the SumScore t   The higher the curve of a solver is  the better  the higher the score   In summary  we see that IJGP shows the best performance on the first four classes of networks  UAI MPE  UAI PE  Relational and Linkage   it is tied with other algorithms on two classes  Grid and BN O   and is surpassed by EDBP on the last two classes  WCSPs and Promedas   EPIS and SampleSearch  which are importance sampling schemes  are often inferior to IJGP and EDBP  In theory  the accuracy of these importance sampling schemes should improve with time  However  the rate of improvement is often unknown in practice  On the hard benchmarks that we evaluated on  we found that this rate is quite small and therefore the improvement cannot be discerned from the Figures  We discuss the results in detail below  As mentioned earlier  TLSBP works only on binary networks  i e   two variables per function  and therefore it was not evaluated on WCSPs  Linkage  UAI   MPE and UAI   PE benchmarks  The UAI MPE and UAI PE instances were used in the UAI      evaluation of exact solvers  for details see the report by Bilmes   Dechter         Exact marginals are available on    UAI MPE instances and    UAI PE instances  The results for UAI MPE and UAI PE instances are shown in Figures    and    respectively  IJGP is the best performing scheme on both benchmark sets reaching a SumScore very close to the maximum possible value in both cases after about   minutes of CPU time  EDBP and SampleSearch are second best in both cases  Relational network instances are generated by grounding the relational Bayesian networks using the Primula tool  Chavira  Darwiche    Jaeger         Exact marginals are available only on    out of the submitted     instances  From Figure     we observe that IJGPs SumScore steadily increases with time and reaches a value very close to the maximum possible score of    after about    minutes of CPU time  SampleSearch is the second best performing scheme  EDBP  TLSBP and EPIS perform quite poorly on these instances reaching the SumScore of        and    respectively after    minutes of CPU time  The Linkage instances are generated by converting linkage analysis data into a Markov network using the Superlink tool  Fishelson   Geiger         Exact marginals are available only on   out of the    instances  The results are shown in Figure     After about one minute of CPU time  IJGPs SumScore is close to   which remains steady thereafter while EDBP only reaches a SumScore of   in    minutes  SampleSearch is the second best performing scheme while EDBP is third best  The results on Grid networks are shown in Figure     The sink node of the grid is the evidence node  The deterministic ratio p is a parameter specifying the fraction of nodes that are deterministic  that is  whose values are determined given the values of their parents  The evaluation benchmark set consists of    instances having p           and     with exact marginals available on    instances only  EPIS  IJGP  SampleSearch and EDBP are in a close tie on this network  while TLSBP has the lowest performance  While hard to see  EPIS is just slightly the best performing scheme  IJGP is the second best followed by SampleSearch and EDBP  On this instances IJGPs SumScore increases steadily with time  The results on BN O instances appear in Figure     This is again a very close tie  in this case of all five algorithms  IJGP has a minuscule decrease of SumScore with time from       to       Although in general an improvement in accuracy is expected for IJGP with higher i bound  it is not       J OIN  G RAPH P ROPAGATION A LGORITHMS  Approximate Mar Problem Set uai   mpe         Sum Score                                                            Time in minutes SampleSearch  IJGP  EDBP  EPIS  Figure     Results on UAI MPE networks  TLSBP is not plotted because it cannot handle UAIMPE benchmarks   Approximate Mar Problem Set uai   pe      Sum Score                                                           Time in minutes SampleSearch  IJGP  EDBP  EPIS  Figure     Results on UAI PE networks  TLSBP is not plotted because it cannot handle UAI PE benchmarks         M ATEESCU   K ASK   G OGATE   D ECHTER  Approximate Mar Problem Set Relational         Sum Score                                                                      Time in minutes SampleSearch IJGP  EDBP TLSBP  EPIS  Figure     Results on relational networks   Approximate Mar Problem Set Linkage       Sum Score                                               Time in minutes SampleSearch  IJGP  EDBP  Figure     Results on Linkage networks  EPIS and TLSBP are not plotted because they cannot handle Linkage networks         J OIN  G RAPH P ROPAGATION A LGORITHMS  Approximate Mar Problem Set Grids      Sum Score                                                                      Time in minutes SampleSearch IJGP  EDBP TLSBP  EPIS  Figure     Results on Grid networks   Approximate Mar Problem Set bn o        Sum Score                                                Time in minutes SampleSearch IJGP  EDBP TLSBP  EPIS  Figure     Results on BN O networks  All solvers except IJGP quickly converge to the maximum possible score of    and are therefore indistinguishable in the Figure         M ATEESCU   K ASK   G OGATE   D ECHTER  Approximate Mar Problem Set WCSPs           Sum Score                                                         Time in minutes SampleSearch  IJGP  EDBP  Figure     Results on WCSPs networks  EPIS and TLSBP are not plotted because they cannot handle WCSPs   Approximate Mar Problem Set Promedas           Sum Score                                                            Time in minutes SampleSearch  IJGP  EDBP  TLSBP  Figure     Results on Promedas networks  EPIS is not plotted because it cannot handle Promedas benchmarks  which are Markov networks         J OIN  G RAPH P ROPAGATION A LGORITHMS  guaranteed  and this is an example when it does not happen  The other solvers reach the maximum possible SumScore of     or very close to it  after about   minutes of CPU time  The WCSP benchmark set has    instances  However we used only the    instances for which exact marginals are available  Therefore the maximum SumScore that an algorithm can reach is     The results are shown in Figure     EDBP reaches a SumScore of    after almost   minutes of CPU time while IJGP reaches a SumScore of    after about   minutes  The SumScores of both IJGP and EDBP remain unchanged in the interval from   to    minutes  After looking at the raw results  we found that IJGPs score was zero on   instances out of     This was because the singleton consistency component implemented via the SAT solver did not finish in    minutes on these instances  Although the singleton consistency step generally helps to reduce the practical time complexity of IJGP on most instances  it adversely affects it on these WCSP instances  The Promedas instances are Noisy OR binary Bayesian networks  Pearl         These instances are characterized by extreme marginals  Namely  for a given variable  the marginals are of the form           where   is a very small positive constant  Exact marginals are available only on    out of the submitted     instances  On these structured problems  see Figure      we see that EDBP is the best performing scheme reaching a SumScore very close to    after about   minutes of CPU time while TLSBP and IJGP are able to reach a SumScore of about    in    minutes      Related Work There are numerous lines of research devoted to the study of belief propagation algorithms  or message passing schemes in general  Throughout the paper we have mentioned and compared with other related work  especially in the experimental evaluation section  We give here a short summary of the developments in belief propagation and present some related schemes that were not mentioned before  For additional information see also the recent review by Koller         About a decade ago  Iterative Belief Propagation  Pearl        received a lot of interest from the information theory and coding community  It was realized that two of the best error correcting decoding algorithms were actually performing belief propagation in networks with cycles  The LDPC code  low density parity check  introduced long time ago by Gallager         is now considered one of the most powerful and promising schemes that often performs impressively close to Shannons limit  Turbo codes  Berrou  Glavieux    Thitimajshima        are also very efficient in practice and can be understood as an instance of belief propagation  McEliece et al          A considerable progress towards understanding the behavior and performance of BP was made through concepts from statistical physics  Yedidia et al         showed that IBP is strongly related to the Bethe Peierls approximation of variational  Gibbs  free energy in factor graphs  The Bethe approximation is a particular case of the more general Kikuchi        approximation  Generalized Belief Propagation  Yedidia et al         is an application of the Kikuchi approximation that works with clusters of variables  on structures called region graphs  Another algorithm that employs the region based approach is Cluster Variation Method  CVM   Pelizzola         These algorithms focus on selecting a good region graph structure to account for the over counting  and over overcounting  etc   of evidence  We view generalized belief propagation more broadly as any belief propagation over nodes which are clusters of functions  Within this view IJGP  and GBP as defined by Yedidia et al          as well as CVM  are special realizations of generalized belief propagation  Belief Propagation on Partially Ordered Sets  PBP   McEliece   Yildirim        is also a generalized form of Belief Propagation that minimizes the Bethe Kikuchi variational free energy  and        M ATEESCU   K ASK   G OGATE   D ECHTER  that works as a message passing algorithm on data structures called partially ordered sets  which has junction graphs and factor graphs as examples  There is one to one correspondence between fixed points of PBP and stationary points of the free energy  PBP includes as special cases many other variants of belief propagation  As we noted before  IJGP is basically the same as PBP  Expectation Propagation  EP   Minka        is a an iterative approximation algorithm for computing posterior belief in Bayesian networks  It combines assumed density filtering  ADF   an extension of the Kalman filter  used to approximate belief states using expectations  such as mean and variance   with IBP  and iterates until these expectations are consistent throughout the network  TreeEP  Minka   Qi        deals with cyclic problem by reducing the problem graph to a tree subgraph and approximating the remaining edges  The relationship between EP and GBP is discussed by Welling  Minka  and Teh         Survey Propagation  SP   Braunstein et al         solves hard satisfiable  SAT  problems using a message passing algorithm on a factor graph consisting of variable and clause nodes  SP is inspired by an algorithm called Warning Propagation  WP  and by BP  WP can determine if a tree problem is SAT  and if it is then it can provide a solution  BP can compute the number of satisfying assignments for a tree problem  as well as the fraction of the assignments where a variable is true  These two algorithms are used as heuristics to define the SP algorithm  that is shown to be more efficient than either of them on arbitrary networks  SP is still a heuristic algorithm with no guarantee of convergence  SP was inspired by the new concept of cavity method in statistical physics  and can be interpreted as BP where variables can not only take the values true or false  but also the extra dont care value  For a more detailed treatment see the book by Mezard and Montanari             Conclusion In this paper we investigated a family of approximation algorithms for Bayesian networks  that could also be extended to general graphical models  We started with bounded inference algorithms and proposed Mini Clustering  MC  scheme as a generalization of Mini Buckets to arbitrary tree decompositions  Its power lies in being an anytime algorithm governed by a user adjustable i bound parameter  MC can start with small i bound and keep increasing it as long as it is given more time  and its accuracy usually improves with more time  If enough time is given to it  it is guaranteed to become exact  One of its virtues is that it can also produce upper and lower bounds  a route not explored in this paper  Inspired by the success of iterative belief propagation  IBP   we extended MC into an iterative message passing algorithm called Iterative Join Graph Propagation  IJGP   IJGP operates on general join graphs that can contain cycles  but it is sill governed by an i bound parameter  Unlike IBP  IJGP is guaranteed to become exact if given enough time  We also make connections with well understood consistency enforcing algorithms for constraint satisfaction  giving strong support for iterating messages  and giving insight into the performance of IJGP  IBP   We show that      if a value of a variable is assessed as having zero belief in any iteration of IJGP  then it remains a zero belief in all subsequent iterations      IJGP converges in a finite number of iterations relative to its set of zero beliefs  and  most importantly     that the set of zero beliefs decided by any of the iterative belief propagation methods is sound  Namely any zero belief determined by IJGP corresponds to a true zero conditional probability relative to the given probability distribution expressed by the Bayesian network         J OIN  G RAPH P ROPAGATION A LGORITHMS  Our experimental evaluation of IJGP  IBP and MC is provided  and IJGP emerges as one of the most powerful approximate algorithms for belief updating in Bayesian networks   
 In this paper  we consider Hybrid Mixed Networks  HMN  which are Hybrid Bayesian Networks that allow discrete deterministic information to be modeled explicitly in the form of constraints  We present two approximate inference algorithms for HMNs that integrate and adjust well known algorithmic principles such as Generalized Belief Propagation  Rao Blackwellised Importance Sampling and Constraint Propagation to address the complexity of modeling and reasoning in HMNs  We demonstrate the performance of our approximate inference algorithms on randomly generated HMNs     INTRODUCTION In this paper  we present and evaluate approximate inference algorithms for Hybrid Mixed Networks which are Hybrid Bayesian Networks that contain discrete deterministic information in the form of constraints  Our work is motivated by a real world application of reasoning about cartravel activity of individuals  This application was modeled using Dynamic Bayesian Networks and requires expressing discrete and continuous variables as well as deterministic discrete constraints  The two popular approximate inference algorithms used for inference in Dynamic Bayesian Networks  DBN  are RaoBlackwellised Particle Filtering  Doucet et al         and Expectation Propagation  Heskes and Zoeter         These algorithms use Importance Sampling  exact inference and Generalized Belief Propagation  GBP  on a Bayesian Network  which is a basic structural component of a DBN  We seek to extend these algorithms to our application in which the basic structural component is a Hybrid Mixed Network  HMN   We show how exact inference algorithms like join tree clustering and a parameterized GBP algorithm called It   erative Join Graph Propagation  IJGP  can be extended to HMNs in a straightforward way  However  extending Rao Blackwellised Importance Sampling algorithms  RBSampling  to HMNs using the straightforward way results in poor performance  This is because in HMNs every sample that violates a constraint will receive zero weight and will be rejected  To remedy this problem  we suggest a new Importance Sampling algorithm called IJGP RB Sampling which uses the output of IJGP as an importance function and we view it as the main contribution of this paper  We performed experiments on random HMNs to compare how IJGP  pure RB Sampling and IJGP RB Sampling perform relative to each other in terms of accuracy when given the same amount of time  Our empirical results suggest that IJGP RB Sampling is always better than pure RBSampling and dominates IJGP as the constraint tightness increases  The rest of the paper is organized as follows  Section   defines HMNs and presents some preliminaries  In the two subsequent sections  we describe how to extend join tree clustering and Iterative Join Graph Propagation to HMNs  We follow by describing IJGP RB Sampling for performing effective Rao Blackwellised Importance Sampling in HMNs  We then present empirical results on random HMNs and conclude with a discussion of related work and summary     PRELIMINARIES AND DEFINITIONS A graphical model is defined by a collection of functions  over a set of variables  conveying probabilistic or deterministic information  whose structure is captured by a graph  D EFINITION     A graphical model is a triplet  X  D  F  where    X    x    x            xn   is a finite set of variables     D    D            Dn   is a set of domains of values in which Di is a domain of Xi and    F    F    F            Fm   is a set of real valued functions  The scope of functions fi denoted as scope  fi    X  is the set of arguments of fi     D EFINITION     The primal graph of a graphical model is an undirected graph that has variables as its vertices and an edge connects any two variables that appear in the scope of the same function  Two graphical models of interest in this paper are Hybrid Bayesian Networks and Constraint Networks  A Hybrid Bayesian Network  HBN   Lauritzen        B    X  D  P  is defined over a directed acyclic graph G    X  E  and its functions Pi    P xi  pai    where pai is the set of parent nodes of xi   X is the set of variables partitioned into discrete  and continuous  variables  S i e  X      The graph structure G is restricted in that continuous variables cannot have discrete variables as their child nodes  The conditional distribution of continuous variables are given by a linear Gaussian model  P xi  I   i  Z   z    N  i     i   z   i   xi   where Z and I are the set of continuous and discrete parents of xi respectively and N     is a multi variate normal distribution  The network represents a joint distribution over all its variables given by a product of all its CPDs  A Constraint network  Dechter        R    X  D C   has C    C   C           Cm   as its functions also called as constraints  Each constraint Ci is a relation Ri defined over a subset of the variables Si  X and denotes the combination of values that can be assigned simultaneously  A Solution is an assignment of values to all the variables such that no constraint is violated  The primary query is to decide if the Constraint Network is consistent  whether it has a solution  and if so find one or all solutions  Using the Mixed Network framework  Dechter and Mateescu        for augmenting Bayesian Networks with constraints  we can extend HBNs to include discrete constraints  yielding Hybrid Mixed Networks  HMNs   Formally  D EFINITION      Hybrid Mixed Network  Given a HBN B    X  D  P  that expresses the joint probability PB and given a Constraint Network R      D  C  that expresses a set of solutions   a Hybrid Mixed Network  HMN  based on B and R denoted by M B  R  is created from B and R as follows  The discrete variables  and their domains are shared and the relationships include the CPDs in P and the constraints in C  We assume that the Constraint Network is always consistent and so the HMN expresses the conditional probability PM X   PM  x    PB  x x    i f x   and   otherwise  Example     Figure   shows a HBN and a Constraint Network yielding a HMN over variables  A B C D F G  where D and G are continuous variables  drawn as squares  and the rest are discrete  drawn as circles   D EFINITION      Graph Decomposition  Given a graphical model  X  D  F   a graph decomposition is a triplet  GD      where GD V  E  is a graph and  and  are  Figure    Example HMN consisting of a HBN and a Constraint Network labeling functions which associate with each vertex v  V two sets   v   X and  v   F such that      For each function fi  F  there is exactly one vertex v  V such that fi   v   and scope  fi     v       For each variable xi  X  the set of  v  V  xi   v   induces a connected subgraph of G  called the running intersection property   The width of a graph decomposition is w   max  v       A join tree decomposition is a graph decomposition in which the graph is a tree while join graph decompositions  JG i   are graph decompositions in which the width is bounded by i  Example     Figure   showing  a  primal graph   b  joingraph decomposition and  c  join tree decomposition of the example HMN shown in Figure    Another relevant notion is that of w cutset  Given a graphical model  X  D  F    the w cutset is the set of variables X   X whose removal from the graphical model yields a graphical model whose treewidth is bounded by w  This paper focuses on the problem of computing the posterior marginal distribution  or beliefs  at each variable given evidence i e  P xi  e   This problem is known to be NP hard and so we resort to approximations     EXACT INFERENCE IN HMNs In this section  we extend a class of exact inference algorithms based on join tree clustering  Lauritzen        from HBNs to HMNs  This algorithm will serve as a basis for the Generalized Belief Propagation scheme described in section   which will be investigated empirically as a standalone scheme and also as a component in our Importance Sampling scheme described in section    The join tree clustering algorithm for HMNs can be derived in a straightforward way by incorporating ideas from  Lauritzen        and we describe it here for completeness sake  see Figure     The exact inference algorithm in  Lauritzen        works by first forming a join tree    Figure    Graph decompositions of HMN in Figure    Algorithm Join tree clustering hmn  Input  A Hybrid Mixed Network MN    X  D  P C  and Evidence e  Output  A join tree decomposition containing the original functions and the messages     Instantiate Evidence    Create a special join tree decomposition     GD V  E           Select a strong root  using a method from Lauritzen  Lauritzen            Let  e            ek   be a DFS ordering of edges from the strong root  of      Call Message Passing    e            ek       Call Message Passing    ek           e      to form a single function PC  Here  multiplication can be performed on the functions in P  and C  separately using the operators in  Lauritzen        and  Dechter        respectively to compute a single probabilistic function P and a single constraint relation C  These two functions P and C can be multiplied by deleting all tuples in P that are not present in C to form the required function PC  We comment on two major technical points for the algorithm given in Figure    Firstly  to be sound the join treeclustering algorithm must satisfy the strong root property as required by HBNs  Lauritzen          Procedure Message Passing  Input  A graph decomposition  GD V  E       for a HMN and an ordering of edges P    e            ek   where ei  E   Output  An graph decomposition containing new messages and functions   for j     to k do    Compute messages  Let e j    u  v  Compute m u  w v   the message that vertex u sends to vertex v  w N  w m u  v    w f cl u   f   m u v  f A usep u v   where cl u     u    m vk   u   vk   u   E     Send message m v j   v  to v   Figure    Join tree clustering for HMNs decomposition and then passing messages between individual cliques of a join tree decomposition  A message from node Ni to N j is constructed by first multiplying all messages and functions in Ni excluding the message from N j and then marginalizing the product over the separator between Ni and NN j   The operators of marginalization  and multiplication required for message passing on a join tree decomposition of a HMN can be constructed in a straightforward way by combining the operators in  Lauritzen        and  Dechter        that work on HBNs and constraint relations respectively  We will now briefly comment on how the multiplication operator can be derived  Let us assume we want to multiply a collection of probabilistic functions P  and a set of constraint relations C   which consist of discrete tuples allowed by the constraint   D EFINITION      Strong Root  Given a join treedecomposition  GD V  E        a node r  V is a strong root iff for all neighboring nodes c and d with c closer to r than d  we have that sep c  d    or d   c   where  is the set of continuous variables and  is the set of discrete variables  A sufficient condition to ensure that there is at least one strong root in a join tree decomposition is to use an ordering for triangulation in which all continuous variables are ordered before discrete variables  We call such join treedecompositions as special join tree decompositions  see Figure   c    Finally  because Gaussian nodes can be processed in polynomial time  the complexity of processing each clique is exponential only in the number of discrete variables in the clique  We capture this property using the definition of adjusted width  D EFINITION     Given a join graph decomposition  GD       the adjusted width of a join graph decomposition is w   max   v         The adjusted treewidth of join tree decomposition is equal to its adjusted width  It is straight forward to show that  Lauritzen         T HEOREM     Given a HMN MN X  D  P C  and evidence e  algorithm Join tree clustering hmn is sound  For discrete variables  the marginal at each clique computed by multiplying the messages and functions in each clique is   exact while for continuous variables the marginal is exact in the sense that it has the correct first and second moments as the exact marginal  time complexity of Join treeT HEOREM     The clustering hmn is O      c     d w       Here c is the maximum number of continuous variables in the clique of a join tree decomposition  d is the maximum domain size of the discrete variables   is the set of discrete variables in the HMN and w is the adjusted treewidth of the special join tree decomposition used     ITERATIVE JOIN GRAPH PROPAGATION In this section  we extend an approximate inference algorithm called Iterative Join Graph Propagation  IJGP  to HMNs  IJGP i   Dechter et al         is parameterized Generalized Belief Propagation algorithm which operates on a join graph decomposition having less than i     variables in each clique  The complexity of IJGP i  is bounded exponentially by i  also called the i bound  This algorithm was defined for discrete Bayesian Networks in  Dechter et al          IJGP i  can be extended to HMNs in a straight forward way by iteratively applying the message passing procedure given in Figure   to a join graph decomposition until a maximum number of iterations is performed or until the algorithm converges  An important technical difference between the extension of IJGP i  to HMNs and the original IJGP i  algorithm  Dechter et al         is that i stands for adjustedwidth rather than width  T HEOREM     The complexity of IJGP i  when applied to HMN is O       n   d i   c      where    is the number of discrete variables  d is the maximum domain size of the discrete variables  i is the adjusted i bound  n is the number of nodes in a join graph and  c   is the maximum number of continuous variables in any clique of the join treedecomposition used      RAO BLACKWELLISED IMPORTANCE SAMPLING  In this section  we propose an effective Importance Sampling for HMNs  We will first review Importance Sampling algorithms for computing posterior distribution and then review w cutset sampling which is a special version of the Rao Blackwellisation concept  Subsequently  we discuss how an Importance Sampling algorithm would run into problems when hard constraints are present  We end the section by presenting an algorithm called IJGP RBSampling that remedies these problems by using Iterative  Join Graph Propagation to generate an effective proposal distribution  Sampling methods are used for approximate inference in Bayesian Networks and are useful in cases when the distribution is hard to compute analytically using exact inference  The virtue of sampling schemes is that they are guaranteed to yield the correct posterior distribution when they converge and they use only linear space  An important class of sampling algorithms is Importance Sampling for Bayesian Networks  The idea here is that since we cannot sample from the true posterior P X e   while it is NP hard to compute   we will sample from an approximation Q X  such that the ratio w     P X   x e  Q X   x   is known up to a normalizing constant   We can then compute the fj required posterior marginal as P xi   x e     j f j  x   w fj where f j  x  is the sample that agrees with xi   x and w fj   are the normalized importance weights computed as w w j   k wk   Ideally  the proposal distribution should have the following properties      It is easy to sample from     It allows easy evaluation of the value Q X   x  for each sample so that the weights can be computed in a cost effective manner and     If P X       then Q X        The last property ensures that Importance Sampling converges to the true posterior in limit of convergence  Geweke         It is well known that any sampling scheme over multidimensional space can be assisted by Rao Blackwellised sampling  namely by sampling over a subspace  We now describe w cutset sampling which is a special version of Rao Blackwellised  RB  sampling  w cutset sampling  Bidyuk and Dechter        is a method that combines exact inference and sampling and provides a systematic scheme for sampling from a subset of variables  The idea is that given an assignment to a set of variables it might be possible to compute the remaining distribution analytically  More formally  in w cutset sampling we partition the set of variables X into two subsets X   X   X  such that the treewidth of the graphical model when X  is removed is bounded by w  Each w cutset sample consists of an assignment of values to X    x  and a belief state P X   x     The variables in the set X  are sampled and the remaining X  variables are solved exactly using exact algorithms like join tree clustering  We can straightforwardly adapt w cutset Importance Sampling to Hybrid Bayesian Networks  HBNs   Since exact inference is polynomial if all nodes are Gaussian  w cutset sampling in HBNs can be done by sampling only a subset of the discrete variables  Lerner         Extending this idea to HMNs  suggests that we sample the discrete variables using a suitable proposal distribution and discard all samples that violate one or more constraints  This method can be inefficient  For example  if we use the prior as the proposal distribution  as in Likelihood weighting  and   This  is usually called Biased Importance Sampling   Algorithm IJGP RB Sampling  Input  A Hybrid Mixed Network MN X  D  P C  and Evidence e  Integer i  k  w and N   Output  Estimate of P X e    Perform Iterative Join graph propagation on MN with i bound i and number of iterations k  Let us call its output    Partition the Variables of HMN into X  and X  such that the adjusted treewidth of a special  join tree decomposition of X  is bounded by w   Create a bucket tree BT  V    from  such that V contains only variables in X     For i     to N do  Figure    An ordered Buckets structure for the join graphdecomposition in Figure    m x y  is the message sent by node x to node y  the prior is such that solutions to the constraint portion are highly unlikely  a large number of samples will be rejected  because P X i      for a sample X i and so weight would be     On the other hand  if we want to make the sample rejection rate zero we would have to use a proposal distribution Q such that all samples from Q are solutions of the constraint portion  One way to find this proposal distribution is to make the Constraint Network backtrack free  perhaps using adaptive consistency  Dechter         along an ordering of variables and then sample along a reverse ordering  However  adaptive consistency can be costly unless the treewidth of the constraint portion is small  Thus on one hand  zero rejection rate implies using a costly inference procedure and on the other hand  sampling from a proposal distribution that ignores the constraints may result in a high rejection rate  We propose to exploit the middle ground between the two extremes by combining the Constraint Network and the Bayesian Network into a single approximate distribution Q using IJGP i   By using IJGP i  we are likely to reduce the rejection rate because it applies constraint propagation in the form of relational i consistency  Dechter and Mateescu          namely it removes many inconsistent tuples  Dechter         Note that the output of IJGP i  can be used to generate a proposal distribution because as shown in  Dechter and Mateescu        P X e      implies that Q X e      where Q X e  is the distribution of IJGP i   We now describe a method to generate samples from the output of IJGP i   Here  given an ordering    hx            x j i of the discrete variables to be sampled  we first compute an approximate marginal denoted by Q x    from the output of IJGP i  and then sample x  from Q x     Then  we set the sampled value x    a  as evidence  run IJGP i   compute the marginal Q x   x    a    and sample x  from this marginal  The above process is repeated until all variables are sampled  The method is inefficient however  requiring O  X     exp i   N  d  time for generating all samples     si   Generate a sample from BT along the order d of BT for the set of variables X       Use join tree clustering to compute the distribution on X  by setting evidence as e  X    si   Lets call it ri      Reject the sample if ri is not a solution     Compute the importance weights wi of si    Normalize the importance weights wi    Output the samples  si   ri   and the normalized weights wi  Figure    IJGP RB Sampling for Hybrid Mixed Networks  where X  are the sampled variables  N is the number of samples  i is the i bound used and d is the maximum domain size  Instead  we use a simplified method in which IJGP i  is applied just once yielding a time complexity of O exp i    N   X     d  to generate all samples  The simplified method uses a special data structure of ordered buckets  Given a collection of functions and messages as the output of IJGP i  and an ordering    hx            x j i of the discrete variables to be sampled  we construct the ordered buckets structure as follows  We associate a bucket with each variable xi in  and consider only those functions and messages  F whose scope is included in  x j           x     We then start processing from i  j to   putting all functions in F that mention xi in the bucket of xi   Once the ordered buckets structure is created  we sample along the order from i     to j  The construction procedure guarantees that when we sample a variable xi from its bucket  all variables ordered before xi are instantiated and there is only a single un instantiated variable in each function in the bucket of xi   So  the time complexity to sample each bucket is bounded by O d  yielding a time complexity of O N   X     d  to generate all samples  An example ordered buckets structure for the join graph decomposition in Figure   b  is given in Figure    We now describe how to compute the weight of each sample  According to Rao Blackwellised Importance Sampling theory  Geweke        Doucet et al          the weight of each sample X k over variables X  is given by wk   P   Q  such that P   Q      P X k  e  Q X k  e   where  is a constant  We can determine the quantity P X k   e  using join tree clustering while we can compute Q X k   e   up to a normalizing constant  from the ordered buckets struc    ture described above by multiplying individual probabilities  Now since Q e       we have required   P X k  e  Q X k  e       P X k  e    Q X k  e   as  An important advantage of using IJGP i  in addition to constraint propagation is that it may yield good approximation to the true posterior thereby proving to be an ideal candidate for proposal distribution  The integration of the ideas expressed above into a formal algorithm called IJGPRB sampling is given in Figure    The algorithm first runs IJGP i  for k iterations to generate an approximation to the true posterior  Then  it partitions the variables X into two sets X  and X  such that the treewidth of the special jointree decomposition of X  is bounded by w using a method proposed in  Bidyuk and Dechter         It then creates an ordered bucket structure over X  from the output of IJGP i  and performs Importance Sampling using the ordered buckets structure as described above  We conclude that  T HEOREM     The complexity of IJGP RB Sampling i w  is O  N  d w     c                  n   d i   c       where  is the set of discrete variables  d is the maximum domain size  i is the adjusted i bound  w is the adjusted w cutset  n is the number of nodes in the join graph and  c   is the maximum number of continuous variables in the clique of the join graph decomposition     EXPERIMENTAL EVALUATION We tested the performance of IJGP i   pure RBSampling and IJGP RB Sampling i w  on randomly generated HMNs  We used a parametric model  N    N    K C   C    P  T   where N  is the number of discrete variables  N  is the number of Gaussian Variables  K is the domain size for each discrete variable  C  is the number of constraints allowed and T is the tightness or the number of forbidden tuples in each constraint  C  is the number of conditional probability distributions  CPDs  and P is the number of parents in each CPD  Parents in each CPD are picked randomly and each CPD is filled randomly  Note that each Gaussian CPDs was assigned a mean and variance randomly chosen in the range         Also no Gaussian variables have discrete children in our random problems  The constraint portion is generated according to Model B  Smith         In Model B  for a given N  and K  we select C  constraints uniformly at random from the available N N      binary constraints and then for each constraint we select exactly T tuples  called as constraint tightness  as no goods  or forbidden  from the available K   tuples  We generated two classes of problems  a  a    variable set with parameters                        T   and T was varied with values      and   and  b  a     variable set with parameters                         T   and T was varied with values      and    In each problem class      of the vari   ables were randomly selected as evidence variables  Each algorithm was given the same amount of time for computing approximate posterior Beliefs  For the    variableset  we let each algorithm run for   s while for the    variable set we let each algorithm run for    s  The choice of these time bounds was arbitrary  Also for each IJGPRB Sampling i w  algorithm instance IJGP i  is run for    iterations only  For each network  we compute the exact solution using the join tree clustering algorithm and compare the accuracy of algorithms using     Absolute error   the absolute value of the difference between the approximate and the exact  averaged over all values  all variables and all problems     Relative error   the absolute value of the difference between the approximate and the exact  divided by the exact  averaged over all values  all variables and all problems     KL distance   Pe  xi    log Pe  xi   Pa  xi    averaged over all values  all variables and all problems where Pe and Pa are the exact and approximate probability values for variable xi respectively  For IJGP i   we experimented with i bounds of      and   while for IJGP RB Sampling  i  w   we experimented with i bound and w of      and   each  We also experimented with a w cutset Importance Sampling algorithm  or pure RB Sampling  with w being set to         and    Thus  we have a total of    algorithms in our experimental setup  We tabulate the results using a  x  matrix for each combination of the problem set  value of tightness T and accuracy scheme  KL distance  relative and approximate error   The rows of the matrix are labeled from w   to w   in increments of   corresponding to the w values used while the columns are labeled from i     to i     corresponding to the i bound used  Note that the column vector i     gives the results for w cutset sampling while the row vector w     gives results for IJGP i   except for i     when it gives results for w cutset sampling   The rest of the matrix contains results for IJGP RB Sampling for different values of i and w  see Tables   and         EXPERIMENTS ON THE    VARIABLE SET Results on the    variable set are given in Table    The results are averaged over     instances each  Here  we see that IJGP i  has slightly better accuracy than IJGP RBSampling when the problem tightness is low  T       see Figure    Table     However  as we increase the tightness to  T      the performance of IJGP i  is worse than IJGPRB Sampling  see Figure    Table     As expected the performance of w cutset sampling improves as w is increased  However IJGP RB sampling shows only a slight improvement in accuracy with increase in w  The accuracy of wcutset sampling is always worse than IJGP i  and IJGPRB Sampling and also it deteriorates more rapidly as the tightness is increased  see Table       Table    Table showing absolute error  relative error and K L distance for    variable set  w   w   w   w    i                                    Relative Error i   i                                                                    i                                    i                                    Absolute Error i   i                                                                    i                                    i                                    K L distance i   i                                                                    i                                    w   w   w   w                                                                                                                                                                                                                                                                                                                                                                                                              w   w   w   w                                                                                                                                                                                                                                                                                                                                                                                                           T           Figure    Figure comparing relative error of IJGP and IJGP RB Sampling  i w  for T   for    variable set  Figure    Figure comparing relative error of IJGP and IJGP RB Sampling  i w  for T   for     variable set  Figure    Figure comparing relative error of IJGP and IJGP RB Sampling  i w  for T   for    variable set  Figure    Figure comparing relative error of IJGP and IJGP RB Sampling  i w  for T   for     variable set      EXPERIMENTS ON THE     VARIABLE SET    RELATED WORK AND SUMMARY  Results on the     variable set are given in Table    The results are averaged over     instances each  Here  we see that unlike the    variable set  IJGP i  has comparable accuracy to IJGP RB Sampling when the tightness is low  T       see Figure   and Table     However  as we increase tightness  T       the accuracy of IJGP i  is considerably worse than IJGP RB Sampling  see Figures      and Table     Also RB Sampling is significantly worse than IJGP RB Sampling for various values of w  see Table      A Mixed Network framework for representing deterministic and uncertain information was presented in  Larkin and Dechter        Dechter and Mateescu         These previous works also describe exact inference algorithms for Mixed Networks with the restriction that all variables should be discrete  Our work goes beyond these previous works in that we describe approximate inference algorithms for the Mixed Network framework and allow continuous Gaussian nodes    Table    Table showing absolute error  relative error and K L distance for     variable set  w   w   w   w    i                                    Relative Error i   i                                                                    i                                    i                                    Absolute Error i   i                                                                    i                                    i                                    K L distance i   i                                                                    i                                    w   w   w   w                                                                                                                                                                                                                                                                                                                                                                                                                w   w   w   w                                                                                                                                                                                                                                                                                                                                                                                                                T           A class of approximate inference algorithms called IJGP i  described in  Dechter et al         handles only discrete variables  In our work  we extend IJGP i  to include Gaussian variables and discrete constraints  Importance Sampling is a commonly used algorithm for sampling in Bayesian Networks  Geweke         A main step in Importance Sampling is choosing a proposal distribution that is as close as possible to the target distribution  We show how a bounded inference procedure like IJGP i  can be used to select a good proposal distribution  The main algorithmic contribution of this paper is presenting a class of Rao Blackwellised Importance Sampling algorithms  IJGP RB Sampling for HMNs which integrates a Generalized Belief Propagation component with a RaoBlackwellised Importance Sampling scheme for effective sampling in presence of constraints  Our experimental results are preliminary but very encouraging  Our results on randomly generated HMNs show that IJGP RB Sampling is almost always superior to pure wcutset sampling  RB Sampling  which does not use IJGP as a importance function  Our results also show that IJGPRB Sampling has better accuracy than IJGP when the problem tightness is high or when the number of solutions to the constraint portion of HMNs is low  ACKNOWLEDGEMENTS This work was supported in part by the NSF under award numbers                  and by NSF grant IIS           Dechter        Dechter  R          Constraint Processing  Morgan Kaufmann   Dechter et al         Dechter  R   Kask  K   and Mateescu  R          Iterative join graph propagation  In UAI     pages         Morgan Kaufmann   Dechter and Mateescu        Dechter  R  and Mateescu  R          A simple insight into iterative belief propagations success  UAI        Dechter and Mateescu        Dechter  R  and Mateescu  R          Mixtures of deterministic probabilistic networks and their and or search space  In Proceedings of the   th Annual Conference on Uncertainty in Artificial Intelligence  UAI       Doucet et al         Doucet  A   de Freitas  N   Murphy  K  P   and Russell  S  J          Rao blackwellised particle filtering for dynamic bayesian networks  In UAI       Geweke        Geweke  J          Bayesian inference in econometric models using monte carlo integration  Econometrica                 Heskes and Zoeter        Heskes  T  and Zoeter  O          Expectation propagation for approximate inference in dynamic bayesian networks  In UAI        Larkin and Dechter        Larkin  D  and Dechter  R          Bayesian inference in the presence of determinism  In AI STATS        
 Inference in graphical models consists of repeatedly multiplying and summing out potentials  It is generally intractable because the derived potentials obtained in this way can be exponentially large  Approximate inference techniques such as belief propagation and variational methods combat this by simplifying the derived potentials  typically by dropping variables from them  We propose an alternate method for simplifying potentials  quantizing their values  Quantization causes different states of a potential to have the same value  and therefore introduces contextspecific independencies that can be exploited to represent the potential more compactly  We use algebraic decision diagrams  ADDs  to do this efficiently  We apply quantization and ADD reduction to variable elimination and junction tree propagation  yielding a family of bounded approximate inference schemes  Our experimental tests show that our new schemes significantly outperform state of the art approaches on many benchmark instances      INTRODUCTION  Many widely used approximate inference algorithms such as mini bucket elimination  Dechter and Rish        and the generalized mean field algorithm  Xing et al         are essentially scope based approximations  The approximation is invoked when either the factors of the posterior distribution or intermediate functions generated during the execution of a variable elimination algorithm are too large to fit in memory or too time consuming to compute  Since the time and memory cost of processing a function is exponential in its scope size  in this paper  we consider only discrete graphical models   these schemes reduce complexity by approximating a large scope function by several small scope functions  For instance  the generalized mean field algorithm approximates each component Pi of the posterior distribution by a tractable component Qi defined over a subset  of the scope of Pi such that the KL divergence between Qi and Pi is minimized  The reasons for the popularity of the scope based approach are obvious  it is a very natural and simple idea  it is easy to implement and its complexity can be easily controlled  In this paper  we propose a fundamentally different but complementary class of range based approximations  the main idea is to quantize a function by mapping a number of distinct values in its range to a single value  When the number of distinct values in the range is reduced  the function becomes more compressible and the time required to manipulate it may decrease substantially  Unfortunately  if we represent functions using tables  namely if we store a real number for every possible configuration of all variables appearing in the functions scope  quantization will be useless because we will not reduce the representation size  In other words  we need structured representations to take advantage of quantization  Many structured representations have been proposed in literature such as confactors  Poole and Zhang         sparse representations  Larkin and Dechter         algebraic decision diagrams  ADDs   Chavira and Darwiche         arithmetic circuits  Darwiche         AND OR multi valued decision diagrams  Mateescu et al         and formula based representations  Gogate and Domingos         When a function has a large number of similar values  as a result of quantization or not   the size and compute time of these representations can be exponentially smaller than the tabular representation  Although one can use any of these structured representations or combinations to compactly represent a quantized function  in this paper we propose to use ADDs  Bahar et al          ADDs are canonical representations of functions  and have many efficient manipulation algorithms  In particular  all inference operations  multiplication  maximization  and elimination can be efficiently implemented using standard ADD operations  Another advantage of ADDs is that there is a large literature on them  This has led to the wide availability of many efficient open source software implementations  e g   CUDD Somenzi          which can be leveraged to efficiently and quickly implement the ideas presented in this paper    Quantization is a general principle that can be applied to a variety of probabilistic inference algorithms  In this paper  we apply it to two standard algorithms  bucket  or variable  elimination  Dechter        and the junction tree algorithm  Lauritzen and Spiegelhalter         yielding approximate  anytime and coarse to fine versions of these schemes  Just like mini bucket elimination  Dechter and Rish        and related iterative algorithms such as expectation propagation  Minka        and generalized belief propagation  Yedidia et al          one can view our new schemes as running exact inference on a simplified version of the graphical model  All approximate schemes proposed to date define a simplified model as a low treewidth model   However  treewidth is an overly strong condition for determining feasibility of exact inference  Chavira and Darwiche         For example  algorithms such as ADD VE  Chavira and Darwiche        and formula decomposition and conditioning  Gogate and Domingos        can solve problems having large treewidth by taking advantage of context specific independence  or identical potential values   Boutilier et al         and determinism  Quantization artificially introduces context specific independence and thus enables us to define a new class of approximations that take advantage of the efficiency and power of the aforementioned schemes by simplifying the graphical model in a much finer manner  We present experimental results on four classes of benchmark problems  Ising models  logistics planning instances  networks for medical diagnosis and coding networks  Our experiments show that schemes that utilize quantization and ADD reduction significantly outperform state of theart bounding and approximate inference approaches when the graphical model has a large number of similar probability values or local structure such as determinism and context specific independence  When the network does not have these properties  our algorithms are slightly inferior to the best performing state of the art scheme but superior to other state of the art approaches  The rest of the paper is organized as follows  Section   describes background  Section   presents quantization  Section   presents approximate inference schemes based on quantization  Experimental results are presented in Section   and we conclude in Section        PRELIMINARIES       MARKOV NETWORKS  For simplicity  we focus on Markov networks defined over bi valued variables  Our approach can be easily applied    The only exception we are aware of is the recent work of  Lowd and Domingos         who compile an arithmetic circuit  which are structured representations similar to ADDs  from dependent samples generated from the posterior distribution  Our approach is very different  and empirically seems to yield much greater speedups  although to date there is no head to head comparison in the same domains because an implementation of the Lowd and Domingos scheme is not available    to multi valued variables  and other graphical models such as Bayesian networks and Markov logic  Domingos and Lowd         Let X    X            Xn   be a set of bivalued  Boolean  variables taking values from the domain         or  False True    A Markov network denoted by M  is a pair  X  F  where X is a set of variables and F    F            Fm   is a collection of potentials or realvalued Boolean functions of the form       k  R    Each potential Fi is defined over a subset of variables  denoted by V  Fi    X  also called its scope  The set of values in the range of Fi is denoted by R Fi    A Markov network represents the following probability distribution  Pr x     m   Y Fi  xV  Fi     Z i         where x is a     truth assignment to all variables X  X  xV  F is the projection of x on the scope of Fi and Pi   Q m Z   x i   Fi  xV  Fi     is the normalization constant  also called the partition function  In this paper  we will focus on the approximating the partition function Z and the marginal distribution P  Xi   xi   at each variable Xi   Our approach can be easily extended to other problems such as computing the most probable explanation  MPE       ALGEBRAIC DECISION DIAGRAMS An algebraic decision diagram  ADD  is an efficient graph representation of a real valued Boolean function  It is a directed acyclic graph  DAG  in which each leaf node is labeled by a real value and each non leaf decision node is labeled by a variable  Each decision node has two outgoing arcs corresponding to the true and false assignments of the corresponding variable  ADDs enforce a strict variable ordering from the root to the leaf node and impose the following three constraints on the DAG   i  no two arcs emanating from a decision node can point to the same node   ii  if two decision nodes have the same variable label  then they cannot have  both  the same true child node and the same false child node and  iii  no two leaf nodes are labeled by the same real value  ADDs that do not satisfy these constraints are referred to as unreduced ADDs  while those that do are called reduced ADDs   An unreduced ADD can be reduced by merging isomorphic subgraphs and eliminating any nodes whose two children are isomorphic  for details  see Bahar et al           ADDs are canonical representations of real valued Boolean functions  namely  two functions will have the same ADD  under the same variable ordering  iff they are the same  Figure   shows a real valued Boolean function and its corresponding ADD  All inference operations  including sum  product  elimination  etc   can be efficiently implemented using ADDs  their complexity is polynomial in the size of the corresponding ADDs  Unfortunately  the time and memory constants involved in using ADDs are much larger than those involved   A                  B                  C Value                                                  A  B  C            a         b   Figure     a  A real valued Boolean function and  b  its ADD representation  Bold edges in the ADD correspond to true assignments and dashed edges correspond to false assignments  Leaf nodes correspond to the real values in the range of the function  A  B     A  C          a   B             b   Figure     a  An unreduced ADD obtained by applying quantization                              to the ADD given in Figure   b  and  b  Reduced ADD obtained from the ADD of  a    in using tables  Because of this  ADD based elimination  and multiplication  may be more expensive  both timewise and memory wise  even when they perform fewer numeric operations than table based elimination  and multiplication   However  when a function has a substantial amount of context specific independence  the ADD operations can be significantly faster      QUANTIZATION  Quantization is the process of replacing a range of real numbers by a single number  Formally  a quantization function denoted by Q  is a many to one mapping from a set T to a set Q of real numbers  where  T    Q   Let F be a real valued Boolean function  Q be a set of real numbers and Q be a quantization function from R F   to Q  We say that a function FQ is a quantization of F w r t  Q if FQ is constructed from F by replacing each value w in the range of F by Q w   Quantization may reduce the size of the ADD of a function  but it will never increase it  Formally  Proposition    Let FQ denote the quantization of F w r t  Q  Then  the ADD of FQ is smaller than or equal to the ADD of F   Figure   demonstrates the effect of quantization on the size of the ADD given in Figure   b   As mentioned in the introduction  the main problem in approximate inference is to find a small bounded function that approximates a large intractable function such that the approximation error is minimized  Assuming that we represent the function using ADDs and approximate using quantizations  we can formalize this problem as follows   Quantization Problem  Given a function F   an integer constant k and an error measure D  e g   KL divergence  mean squared error  etc    find a  optimal  quantization FQ of F such that   Size Constraint  The size of the ADD of FQ is less than or equal to k   Error Constraint  There does not exist a quantization FQ of F such that the size of the ADD of FQ is less than or equal to k and D F  FQ     D F  FQ    Unfortunately  finding an optimal quantization is extremely hard because the quantization problem is a multi objective constrained optimization problem  Therefore  we propose the following three heuristics  Our first heuristic optimizes for error and solves the following relaxation  given an integer l and an ADD F  F represents a function F   having t leaves  find an ADD FQ  that represents the quantization FQ of F   having l leaves such that D F  FQ   is minimized  This problem can be solved in O lt  time using dynamic programming and matrix searching  see Wu        for details   Given l  the relaxation optimizes FQ in terms of the error measure D while disregarding the size of the ADD of FQ  although since l   t  FQ will be smaller than F    To use this heuristic for solving the quantization problem  we have to determine the value of l that will yield an ADD having less than k     nodes  To find l  we use binary search  We call this heuristic the min error heuristic  Our second heuristic solves the following relaxation  given an integer l and an ADD F having t leaves  find an ADD FQ having l leaves such that there does not exist an ADD FQ that has l leaves but fewer nodes than FQ  FQ is a quantization of F    Unfortunately  this relaxation is much harder to solve than the relaxation that optimizes the error  Therefore  we use the following  heuristic  technique to solve it  As before  we perform a binary search over l starting with l   t    At each search point  we select a leaf node and merge it with another leaf that shares the largest number of parents with it  ties broken by the relative difference between the leaf values   When two leaves having the same parent are merged  the parent will point to the same leaf node in the new  unreduced  ADD and will be deleted when the ADD is reduced  Notice that the heuristic ignores the error measure D  except when breaking ties  and reduces the ADD size by merging as fewer leaves as possible  Therefore  we call this heuristic the min merge heuristic  In practice  we can run both heuristics in parallel  compute the error between the original function and the quantized function obtained using each heuristic  and choose the quantized function having the smallest error  We call this heuristic the min error merge heuristic  We will evaluate the performance of both the heuristics as well as the combination in the experimental section  Note that when approximations without bounding guarantees are   Algorithm    ABQ k  Input  A Markov network M and a size bound k Output  An estimate of the partition function of M begin Heuristically select a variable ordering o    X            Xn    Express each potential of M as an ADD     Create Buckets Let BXi be the bucket of Xi   Put each ADD in the bucket of its highest ordered variable  Z   for i   n downto   do repeat    Process the Bucket of Xi if BXi contains only one ADD   then P    Xi    Put  in the bucket of its highest ordered variable  If  has no variables then Z   Z   Delete   from BXi else Heuristically select   and   from BXi           Delete   and   from BXi   if the size of  is greater than k then    Quantization step q   ADD formed by repeatedly quantizing and reducing  until its size is less than k  Put q in BXi   else Put  in BXi    end  until BXi is empty return Z  desired  we assign the median value of the merged leaves to the new leaf  When upper  or lower  bounds are desired  we assign the maximum  or the minimum  value instead      APPROXIMATION BY QUANTIZATION  In this section  we apply quantization and ADD reduction to two standard inference algorithms   i  bucket or variable elimination  Dechter         and  ii  junction tree propagation  Lauritzen and Spiegelhalter         Applying quantization and ADD reduction to the former yields a one pass algorithm for computing the partition function similar to mini bucket elimination  Dechter and Rish         and applying it to the latter yields an iterative algorithm that can compute posterior marginal distribution at each variable  similar to expectation propagation  Minka              ONE PASS APPROXIMATION BY QUANTIZATION  ABQ   Before describing our algorithm  we give background on bucket elimination  Bucket elimination  BE   Dechter        is an exact algorithm for computing the partition function  The algorithm maintains a database of valid functions that is partitioned into buckets  one for each variable   Given an ordering o of variables  the algorithm partitions the potentials of a Markov network by putting each potential in the bucket of the highest ordered variable in its scope  The algorithm operates by eliminating variables one by one  along o  A variable X is eliminated by computing a product of all the functions in its bucket  and then summing out X from this product  This creates a new function  whose scope is the union of the scopes of all functions that mention X  minus  X   The algorithm then deletes the functions involving X  namely the bucket of X  from the database of valid functions  adds the newly created function to it and continues  The function  a real number  created by eliminating the last bucket equals the partition function  It is known that the time and space complexity of BE is exponential in the treewidth of the Markov network  BE assumes tabular representation of functions  It can be easily extended to use ADDs yielding the ADD BE algorithm  first presented in  Chavira and Darwiche         In ADD BE  we represent all functions using ADDs and use ADD operators for elimination and multiplication  Unfortunately  just like BE  it is an exact algorithm and is therefore not scalable to interesting real world applications  We propose to make ADD BE practical by quantizing large ADDs generated during its execution  Algorithm   describes the proposed scheme  The algorithm takes as input a Markov network M and a size bound k and outputs an estimate of the partition function  It is essentially a standard ADD based bucket elimination algorithm except for the quantization step  Here  given an ADD whose size is greater than k  we repeatedly merge its leaf nodes using the heuristics described in the previous section  until its size is smaller than k  Note that when k    the algorithm runs full bucket elimination and is equivalent to the ADD BE algorithm of  Chavira and Darwiche         Thus  ABQ represents an anytime  anyspace bounded approximation of ADD BE  controlled by the size bound k  We mention an important technical detail which can positively impact both the complexity and accuracy of ABQ  Notice that after quantizing an ADD  some variables may become irrelevant  for example  variable C is irrelevant to the ADD of Figure   b  because it does not appear in any of its internal nodes   Thus  instead of adding the quantized ADD to the current bucket  we can safely transfer it to the bucket of its highest ordered relevant variable  Note that variables may also become irrelevant when we multiply two ADDs or eliminate the bucket variable from the ADD  Obviously  we can use the same approach in these cases too and transfer the newly generated ADD to the bucket of its highest ordered relevant variable  The time and space complexity of Algorithm   is summarized in the following theorem  Theorem    The time complexity of ABQ k  is O mk     where m is the number of potentials and k is the size bound  Its space complexity is O max mk  k       Algorithm   can be easily extended to yield an upper   Algorithm    IABQ k   Procedure send message u  v  k   Input  A Markov network M and a ADD size bound k Output  A set of junction tree cliques containing potentials and messages received from neighbors begin Construct a junction tree for M Let  e            el   be an ordering of edges of the junction tree for message passing from leaves to the root repeat for i     to l do Let ei    ui   vi   send message ui   vi   k  for i   l downto   do Let ei    ui   vi   send message vi   ui   k   Input  Cliques u and v of a junction tree and a constant k Output  v with the old message  ADD  from u replaced by a new message begin Let  u             u k   be a heuristic ordering of the ADDs currently in the clique u except the message received from v u v     for i     to k do u v   u v  u i if the size of u v is greater than k then    Quantization step u v   ADD formed by repeatedly quantizing and reducing u v until its size is smaller than k Let sep u  P v    clique u   clique v  u v   clique u  sep u v  u v Replace the old message from u in v with u v  until convergence or timeout end end   lower  bound on the partition function  All we have to do is ensure that the quantization function Q x  used by ABQ is an upper  lower  approximation  namely w FQ  w   F  w   w FQ  w   F  w    Trivially  a quantization function that replaces each value in the interval by the maximum  minimum  value is an upper  lower  approximation  Formally  Theorem    If all quantizations in Algorithm ABQ k  use a quantization function Q satisfying w FQ  w   F  w   then the output of ABQ k  is an upper bound on the partition function  On the other hand  if Q satisfies w FQ  w   F  w   then ABQ k  yields a lower bound on the partition function       ITERATIVE APPROXIMATION BY QUANTIZATION  IABQ   In this section  we will show how to approximate the junction tree algorithm  Lauritzen and Spiegelhalter        using quantization and ADD reduction  The junction tree algorithm is a message passing algorithm over a modified graph called the junction tree  which is obtained by clustering together variables of a Markov network until the network becomes a tree  The clusters are also called cliques  Each clique is associated with a subset of potentials such that the scope of each potential is covered by the variables in the cliques  The message passing works as follows  First  we designate an arbitrary cluster as the root and send messages in two passes  from the leaves to the root  inward pass  and then from the root to the leaves  outward pass   The message that a clique u sends to its neighbor v is constructed as follows  In clique u  we multiply all the potentials associated with u  with all the messages received from its neighbors except v  and then eliminate all variables that appear in u but not in v  The time and space complexity of the junction tree algorithm is exponential in the maximum cluster size of the junction tree used  We can construct an approximate version of the junctiontree algorithm using quantization and ADD reduction in  a straight forward manner  Algorithm   describes our approach  The algorithm first constructs a junction tree for the Markov network and then sends messages along its edges using the send message procedure  In the send message procedure  we send a message from a clique u to clique v by multiplying all ADDs corresponding to the messages  except the one received from v  and potentials  Just as in ABQ  if the size of the product ADD is larger than k  we recursively apply quantization and ADD reduction until its size is smaller than or equal to k  Since  the message propagation is performed on a tree  the algorithm will always converge in two passes  assuming that the quantization heuristics do not change between passes   IABQ belongs to the class of sum product expectation propagation  EP  algorithms  see Minka        and Koller and Friedman         Chapter     which perform inference by sending approximate messages  In practice  we can further improve the accuracy of IABQ by performing belief update propagation instead of sum product propagation  Belief update IABQ constructs the message from clique u to clique v by first multiplying  and quantizing if necessary  all the incoming messages  including the one received from v   Then  it projects the resulting factor on sep u  v  and divides it by the message v u received from v  thus unlike sum product IABQ  belief update IABQ requires the division operation   Belief update IABQ is not guaranteed to converge in two passes and may not converge at all  However  as we shall see in the experimental section  when it does converge  it often converges very quickly  in       iterations  and yields highly accurate estimates  IABQ yields a new class of bounded EP algorithms  Existing bounded EP algorithms use treewidth to determine feasibility of inference  In particular  in the junction tree algorithm  the message between u and v corresponds to a  local  fully connected  clique  graphical model over the separator sep u  v   Existing EP algorithms ensure tractability by sending bounded treewidth messages  achieved by introducing new conditional independencies between the sepa    rator variables   IABQ  on the other hand  can create messages having substantially larger treewidth than existing EP algorithms  This is because it uses quantization and ADDs to introduce context specific independencies between the separator variables      EXPERIMENTS  In this section  we compare the performance of ABQ and IABQ with other algorithms from the literature  We also evaluate the impact of various quantization heuristics on accuracy  We experimented with instances from four benchmark domains   i  logistics planning  Sang et al           ii  linear block coding   iii  Promedas Bayesian networks for medical diagnosis  Wemmenhove et al         and  iv  Ising models  We implemented our algorithms in C    We ran our experiments on a Linux machine with a      GHz Intel Xeon quad core processor and    GB of RAM  We gave each algorithm a memory limit of  GB and  unless otherwise specified  a time limit of   hours  We used the CUDD package  Somenzi        to implement ADDs  We used the minfill ordering heuristic for constructing the junction tree in IABQ and for eliminating variables in ABQ       EXPERIMENTS EVALUATING THE BOUNDING POWER OF ABQ  When exact results are not available  evaluating the capability of approximate schemes is problematic because the quality of the approximation  namely how close the approximation is to the exact  cannot be assessed  To allow some comparison on large  hard instances  we evaluate the upper bounding power of ABQ  and compare it with three algorithms from literature  mini bucket elimination  MBE   Dechter and Rish        Rollon and Dechter         Treereweighted Belief Propagation  TRW   Wainwright et al         and Box propagation  BoxProp   Mooij and Kappen         For a fair comparison  we also compare with our own ADD based implementation of mini bucket elimination  ADD MBE   ADD MBE represents all messages and potentials in the MBE algorithm using ADDs instead of tables  both ADD MBE and ABQ use the same variable ordering   BoxProp was derived for bounding posterior probabilities and therefore Z is obtained by applying the chain rule to individual bounds on posteriors  We experimented with anytime versions of MBE  ADD MBE and ABQ  Namely  we start with a crude size bound  k      and increase it progressively by multiplying it by    until the algorithm runs out of memory or time  Recall that in ABQ  k bounds the size of the ADD  In MBE  it bounds the size of the new functions created by the algorithm  The results in this subsection were obtained using the min error merge heuristic described in Section    we compare the impact of heuristics on accuracy in the next subsection   Note that almost all the instances that we consider in this subsection are quite hard and the exact value of their par   Instance   n d m w   ABQ MBE BoxProp TRW ADD MBE Z Z Z Z Z      Logistics planning log                    e     e     e      e     e                             log                      e     e     X X  e                  log                      e     e    X X  e                  log                      e     e     X X  e                 log                      e      e     X X  e                  Medical Diagnosis  Promedas networks or chain                       e          e     e                                  or chain                       e     e     e     e     e                            or chain                       e     e     e     e     e                          or chain                     e     e          e     e                             Coding networks BN                     e     e     e     e     e                               BN                     e     e     e     e     e                             BN                     e     e     e     e     e                               BN                     e     e     e     e     e                                 BN                     e     e     e     e     e                                 Ising models   x                    e       e       e       e       e                                     x                    e       e       e       e       e                                     x                     e       e       e       e       e                                       x                     e       e       e       e       e                                     Table    Table showing the upper bound on the partition function and the log relative difference  for ABQ  MBE and BoxProp  TRW and ADD MBE  Each algorithm was given a time limit of   hours and a memory limit of   GB  The best performing scheme is highlighted by bold in each row  X indicates that the algorithm did not return a value   tition function is not known  except for the logistics instances   Table   shows the results  The first column shows the instance name  The second column shows various statistics for the instance such as the number of variables  n   the domain size  d   the number of potentials  p  and the upper bound on treewidth obtained using the minfill ordering heuristic  w   Columns            and   show the upper bound on the partition function computed by ABQ  MBE  BoxProp  TRW and ADD MBE respectively  For each scheme  we also report the relative difference   defined below  between the log of the best known upper bound UBest and the log of the upper bound U output by the current scheme     log U    log UBest   log UBest         The log relative difference provides a quantitative measure for assessing the relative approximation quality of the bounding schemes  smaller is better   Logistics planning instances Our first domain is that of logistics planning  the networks are available from  Sang   et al           Given prior probabilities on actions and facts  the task is to compute the probability of evidence  From Table    we can see that ABQ significantly outperforms MBE  ADD MBE  TRW and BoxProp on all instances  the logrelative difference is quite large   ADD MBE is much superior to MBE on most instances  This is because the domain has a large amount of determinism and identical probability values which ADD MBE exploits effectively  ADD MBE is worse than ABQ suggesting that quantization based approximations are much better in terms of accuracy than MBE based approximations  Medical Diganosis  Promedas networks Our second domain is that of noisy OR medical diagnosis networks generated by the Promedas expert system for internal medicine  Wemmenhove et al          The global architecture of the diagnostic model in Promedas is similar to the QMR DT medical diagnosis networks  Shwe et al          Each network can be specified using a two layer bipartite graph in which the top layer consists of diseases and the bottom layer consists of symptoms  If a disease causes a symptom  there is an edge from the disease to the symptom  The networks are available from UAI      evaluation website  Darwiche et al          From Table    we can see that ABQ is superior to MBE  ADD MBE  TRW and BoxProp on all instances  notice that for all instances the log relative difference between ADD based schemes and others is quite large   Coding networks Our third domain is random coding networks from the class of linear block codes  Kask and Dechter         the networks are available from the UAI      evaluation website  Darwiche et al           From Table    we can see that ABQ outperforms MBE  TRW and BoxProp on all instances  except BN      On this network  MBE is slightly better than ABQ  because of the overhead of ADDs   On all other networks  ABQ is slightly superior to MBE  ADD MBE is worse than ABQ on all instances  Again  our results on the coding networks clearly demonstrate that quantization with ADD reduction is a better approximation strategy than MBE  Ising models Our last domain is that of Ising models which are n  n pair wise grid networks  They are specified using potentials defined over each edge and each node  Each node potential is given by        where  is drawn uniformly between   and    The edge potentials are either              or their mirror image              where  is drawn uniformly between   and    is called the coupling strength   We use        to generate our networks  From Table    we can see that ABQ outperforms BoxProp  TRW and ADD MBE on these models  However  it is slightly inferior to MBE  notice that the log relative difference between ABQ and MBE is very small   Intuitively  ABQ should do well when the graphical model contains many similar or identical probability values in each potential  Ising models are interesting in this respect because they represent the worst possible case for ABQ  with no determinism or context specific structure at  Instance  min error merge min error min merge Z Z Z    Logistics planning log       e        e        e          log       e        e        e                    log       e        e        e                    log      e        e       e              log       e         e         e                      Medical Diganosis  Promedas networks or chain         e        e        e                   or chain         e       e        e                   or chain             e        e              or chain         e        e        e          Coding networks BN      e     e     e              BN         e        e        e              BN         e       e        e             BN         e        e        e        e            BN         e        e        e                   Ising models   x       e          e          e                   x       e          e          e                        x       e          e          e                  x       e          e          e                  Table    Table showing the impact of the three quantization heuristics   i  min error merge   ii  min error and  iii  min merge on the upper bound output by ABQ  For each heuristic  we also report the log relative error    all  Remarkably  ABQ still outperforms BoxProp  TRW and ADD MBE on these models  In our initial experiments it also outperformed MBE  but it does slightly worse than the latest version  which is the one reported in Table    MBE employs sophisticated partitioning heuristics  Rollon and Dechter        that could also be incorporated into ABQ  and many other optimizations characteristic of a mature system  its good performance relative to ABQ is likely due to these improvements  rather than to the basic algorithm  However  there is in general a tradeoff in using ADDs versus tables  as shown by the ADD MBE results  ADDs can be exponentially smaller and faster by taking advantage of context specific independence and determinism  but ADDs incur higher overhead than tables  so the latter may be preferable when there is no structure to exploit  Overall  we see that that ABQ always outperforms TRW  BoxProp and ADD MBE  and outperforms MBE on all domains except Ising models  ABQs advantage increases with the the amount of  approximate or exact  contextspecific independence and determinism in the domain  but ABQ still does quite well even when these are absent    IJGP IABQ Gibbs                                                  Time  seconds   Average KL Divergence  IJGP IABQ Gibbs                                                Time  seconds   b   Figure    KL divergence vs  Time plots for IJGP  Gibbs sampling and IABQ for  a  logistics planning instance log   and  b     x    Ising model        EXPERIMENTS EVALUATING THE QUANTIZATION HEURISTICS  In this subsection  we evaluate the performance of the three quantization heuristics described in Section    Table   shows the results  We can see that the min error merge heuristic performs the best overall  The min merge heuristic is only slightly inferior to the min error merge heuristic  The min error heuristic is inferior to the min merge heuristic except on the promedas networks  The promedas networks have many similar probability values  approximate context specific independence  which the min error heuristic exploits quite effectively  On the other hand  the Ising models represent the worst possible case for the min error heuristic because the intermediate potentials generated during ABQs execution have almost no similar probability values       IABQ K      IABQ K       IABQ K                                                                   Figure    KL divergence vs  Number of iterations for IABQ  Ising model   x      log    logistics planning instance      Number of iterations   a       Average KL Divergence  Average KL Divergence  log    logistics planning instance      EXPERIMENTS EVALUATING THE ACCURACY OF IABQ  In this subsection  we evaluate the accuracy of belief update IABQ for computing posterior marginals  We compare IABQ with Iterative Join Graph propagation  IJGP   Mateescu et al          a state of the art generalized belief propagation scheme  IJGP won   out of the   marginal estimation categories at the      UAI approximate evaluation challenge  Elidan and Globerson          As a baseline  we  compare with Gibbs sampling  Geman and Geman         We ran both IJGP and IABQ as anytime algorithms  Both algorithms take as input a size parameter which determine their complexity  We vary this parameter starting with its lowest possible value  progressively increasing it until the algorithm runs out of memory or time  We ran each algorithm for   hour and gave each algorithm a memory limit of  GB  Both IJGP and IABQ may or may not converge to a fixed point  Therefore  we ran each for    iterations or until convergence  whichever was earlier  Convergence is detected by comparing the absolute difference between messages at the current and previous iteration  We measure performance using the KL divergence  Let P  Xi   and Q Xi   denote the exact and approximate marginals of variable Xi   Then  the average KL divergence is defined as        X X P  xi   KL P  A    P  xi   log  X  Q xi   x Xi X  i  For brevity  we only describe our results for two sample instances   a  a logistics planning instance  and  b  a    x    Ising model  Average KL divergence vs  time plots for these instances are given in Figure    Our results are consistent with the empirical evidence in the previous subsection  Specifically  when the graphical model has many identical or similar probability values  IABQ dominates IJGP  e g   on the log   instance   However  when the graphical model does not have these properties  IJGP is slightly better than IABQ because of the overhead of ADDs  Figure   shows the impact of increasing the number of iterations on the accuracy of IABQ for different values of the size bound parameter k  We can see that IABQ converges to its fixed point in about       iterations  Its accuracy typically increases with k and with the number of iterations  This shows that the belief update IABQ performs better than sum product IABQ  sum product IABQ is equivalent to running just one iteration of belief update IABQ      CONCLUSION The most challenging problem in approximate inference is how to approximate a large function that is computation    ally infeasible by a collection of tractable functions  The paper proposes to solve this problem using quantization  Quantization replaces a number of values in the range of a function by a single value  and thus artificially introduces context specific independence  Conventional tabular representations of functions are inadequate at exploiting this structure  We therefore proposed to use structured representations such as algebraic decision diagrams  ADDs   We showed how quantization can be applied to two standard algorithms in probabilistic inference  variable elimination and junction tree propagation  yielding two new schemes   i  A one pass algorithm that can be used to approximate and bound the partition function and  ii  An iterative algorithm that can be used for approximating posterior marginals  Our new approximate schemes significantly enhance the class of approximations considered by existing algorithms  which constrain their approximations to have low treewidth  By imposing context specific independencies between variables via quantization  our new algorithms construct structured approximations in the high treewidth space  Our empirical evaluation demonstrates that schemes that employ quantization often yield more accurate results than schemes that do not  Thus approximation by quantization is a promising approach for future investigations  Acknowledgements This research was partly funded by ARO grant W   NF            AFRL contract FA        C      DARPA contracts FA                FA        D       HR        C       HR        C      and NBCH D        NSF grants IIS         and IIS          and ONR grant N                 The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of ARO  DARPA  NSF  ONR  or the U S  Government   
 Computing the probability of a formula given the probabilities or weights associated with other formulas is a natural extension of logical inference to the probabilistic setting  Surprisingly  this problem has received little attention in the literature to date  particularly considering that it includes many standard inference problems as special cases  In this paper  we propose two algorithms for this problem  formula decomposition and conditioning  which is an exact method  and formula importance sampling  which is an approximate method  The latter is  to our knowledge  the first application of model counting to approximate probabilistic inference  Unlike conventional variable based algorithms  our algorithms work in the dual realm of logical formulas  Theoretically  we show that our algorithms can greatly improve efficiency by exploiting the structural information in the formulas  Empirically  we show that they are indeed quite powerful  often achieving substantial performance gains over state of the art schemes     Introduction The standard task in the field of automated reasoning is to determine whether a set of logical formulas  the knowledge base KB  entails a query formula Q   The formulas could be propositional or first order  in this paper we focus on the propositional case   Logics lack of a representation for uncertainty severely hinders its ability to model real applications  and thus many methods for adding probability to it have been proposed  One of the earliest is Nilssons probabilistic logic  Nilsson         which attaches probabilities to the formulas in the KB and uses these to compute the probability of the query formula  One problem with this approach is that the formula probabilities may be inconsistent  yielding no solution  but consistency can be verified  and enforced  Nilsson         Another problem is that in general a set of formula probabilities does not completely specify a distribution  but this is naturally solved by assuming the maximum entropy distribution consistent with the specified probabilities  Nilsson        Pietra et al          A more serious problem is the lack of efficient inference procedures for probabilistic logic  This contrasts with the large literature on inference for graphical models  which always specify unique and consistent distributions  Pearl         However  the representational flexibility and compactness of logic is highly desirable  particularly for modeling complex domains  This issue has gained prominence in the field of statistical relational learning  SRL   Getoor and Taskar         which seeks to learn models with both logical and probabilistic aspects  For example  Markov logic represents knowledge as a set of weighted formulas  which define a log linear model  Domingos and Lowd         Formulas with probabilities with the maximum entropy assumption and weighted formulas are equivalent  the problem of converting the former to the latter is equivalent to the problem of learning the maximum likelihood weights  Pietra et al          In this paper we assume weighted formulas  but our algorithms are applicable to formulas with probabilities by first performing this conversion  Another reason to seek efficient inference procedures for probabilistic logic is that inference in graphical models can be reduced to it  Park         Standard inference schemes for graphical models such as junction trees  Lauritzen and Spiegelhalter        and bucket elimination  Dechter        have complexity exponential in the treewidth of the model  making them impractical for complex domains  However  treewidth can be overcome by exploiting structural properties like determinism  Chavira and Darwiche        and context specific independence  Boutilier         Several highly efficient algorithms accomplish this by encoding a graphical models as sets of weighted formulas and applying logical inference techniques to them  Sang et al         Chavira and Darwiche         All of these algorithms are variable based  in that they explore the search space defined by truth assignments to the   variables  In this paper  we propose a new class of algorithms that explore the search space defined by truth assignments to arbitrary formulas  including but not necessarily those contained in the original specification  Our formula based schemes generalize variable based schemes because a variable is a special case of a formula  namely a unit clause  For deriving exact answers  we propose to exhaustively search the space of truth assignments to formulas  yielding the formula decomposition and conditioning  FDC  scheme  FDC performs AND OR search  Dechter and Mateescu        or recursive conditioning  Darwiche         with and without caching  over the space of formulas  utilizing several Boolean constraint propagation and pruning techniques  Even with these techniques  large complex domains will still generally require approximate inference  For this  we propose to compute an importance distribution over the formulas  yielding formula importance sampling  FIS   Each sample in FIS is a truth assignment to a set of formulas  To compute the importance weight of each such sampled assignment  we need to know its model count  or number of solutions   These model counts can either be computed exactly  if it is feasible  or approximately using the recently introduced approximate model counters such as SampleCount  Gomes et al         and SampleSearch  Gogate and Dechter      b   To the best of our knowledge  this is the first work that harnesses the power of model counting for approximate probabilistic inference  We prove that if the model counts can be computed accurately  formula importance sampling will have smaller variance than variablebased importance sampling and thus should be preferred  We present experimental results on three classes of benchmark problems  random Markov networks  QMR DT networks from the medical diagnosis domain and Markov logic networks  Our experiments show that as the number of variables in the formulas increases  formula based schemes not only dominate their variable based counterparts but also state of the art exact algorithms such as ACE  Chavira and Darwiche        and approximate schemes such as MC SAT  Poon and Domingos        and Gibbs sampling  Geman and Geman         The rest of the paper is organized as follows  Section   describes background  Section   presents formula decomposition and conditioning  Section   presents formula importance sampling  Experimental results are presented in Section   and we conclude in Section       Background     Notation Let X    X            Xn   be a set of propositional variables that can be assigned values from the set        or  False True   Let F be a propositional formula over X   A model or a solution of F is a     truth assignment to all variables in X such that F evaluates to True  We will assume throughout that F is in CNF  namely it is a conjunction of clauses  a clause being a disjunction of literals  A literal is a variable Xi or its negation Xi   A unit clause is a clause with one literal  Propositional Satisfiability or SAT is the decision problem of determining whether F has any models  This is the canonical NP complete problem  Model Counting is the problem of determining the number of models of F   it is a  P complete problem  We will denote formulas by letters F   G  and H  the set of solutions of F by Sol F   and its number of solutions by   F    Variables are denoted by letters X and Y   We denote sets by bold capital letters e g   X  Y etc  Given a set X    X            Xn   of variables  x denotes a truth assignment  x            xn    where Xi is assigned the value xi   Clauses are denoted by the letters C  R  S and T   Discrete functions are denoted by small Greek letters  e g      etc  The variables involved in a function   namely the scope of  is denoted by V     Similarly  the variables of a clause C are denoted by V  C   Given an assignment x to a superset X of Y  xY denotes the restriction of x to Y  The expected value of a random P variable X with respect to a distribution Q is EP Q  X    xX xQ x   The variance of x is V arQ  X    xX  x  EQ  X    Q x    In this paper  we advocate using a collection of weighted propositional formulas instead of the conventional tabular representations to encode the potentials in Markov random fields  MRFs  or conditional probability tables in Bayesian networks  Specifically  we will use the following representation  which we call as propositional MRF or PropMRF in short  A PropMRF is a Markov logic network  Richardson and Domingos        in which all formulas are propositional  It is known that any discrete Markov random field or a Bayesian network can be encoded as a PropMRF  Park        Sang et al         Chavira and Darwiche          D EFINITION    Propositional MRFs   A propositional MRF  PropMRF   denoted by M is a triple  X  C  R  where X is a set of n Boolean variables  C     C    w              Cm   wm    is a set of m soft  weighted  clauses and R    R            Rp   is a set of p hard clauses  Each soft clause is a pair  Ci   wi   where Ci is a clause and wi is a real number  We will denote by FM   R       Rp   the CNF formula defined by the hard clauses of M  The primal graph of a PropMRF has variables as its vertices and an edge between any two nodes that are involved in the same hard or soft clause  We can associate a discrete function i with each soft clause  Ci   wi    defined as follows    exp wi   If x evaluates Ci to True i  xV  Ci         Otherwise   The probability distribution associated with M is given by  PM  x        Qm    ZM  i       i  xV  i      If x  Sol FM   Otherwise       where ZM is the normalization constant  often referred to as the partition function  ZM is given by  m Y  X  ZM    i  xV  i      Note that if M has no soft clauses  then ZM equals the number of models of the formula FM   Thus  model counting is a special case of computing ZM   We will focus on the query of finding the probability of a CNF formula G  denoted by P  G   By definition  X PM  x  P  G    xSol FM G     Z     X     m Y  i  xV  i           xSol FM G  i      Because From Equations   and    we get P  G    ZZM M computing P  G  is equivalent to computing a ratio of two partition functions  in the sequel  we will present formulabased algorithms for computing ZM only     Exact Formula based Inference We first explain how to perform inference by variablebased conditioning and then show how it can be generalized via formula based conditioning  Consider the expression for ZM  See Equation     Given assignments Xj and Xj   we can express ZM as  ZM  X     m Y  i  xV  i      xSol FM Xj   i       X  m Y  i  xV  i           xSol FM Xj   i      ZMXj   ZMXj  Figure    An example PropMRF   AVBVCVDVE  w    AVBVCVFVG w    DVEVH w    FVGVJ w   Left arcs are True arcs and right arcs are False arcs  exp w  w        DVEVH w    FVGVJ w    A   DVEVH w    FVGVJ w   A  BVCVDVE  w    BVCVFVG w    Decompose  DVEVH w     FVGVJ w    Figure    Figure demonstrating the simplification steps after con        xSol FM G  i    X  weight w  w  w  w   ditioning on variable A for the PropMRFgiven in Figure     i  xV  i      If we add all the clauses of G to the hard clauses of M yielding another PropMRF M   then the partition function ZM of M is given by  Z M  Clause ABC DE ABC F G DEH F GJ       xSol FM   i    m Y  ClauseID S  S  S  S        where MX and MX are PropMRFs obtained by adding X and X to the set of hard clauses of M respectively  Then  one can perform conditioning to compute ZMXj and ZMXj   recursively for each PropMRF until all variables  in X have been instantiated  Conditioning by itself is not that useful  For example  if the PropMRF has no hard clauses  then conditioning would perform  n summations  However  one can augment it with various simplification schemes such as Boolean constraint propagation  and utilize problem decomposition  yielding powerful schemes in practice  These and other ideas form the backbone of many state of the art schemes such as ACE  Chavira and Darwiche        and Cachet  Sang et al          To simplify a PropMRF  we can apply any parsimonious operators   operators which do not change its partition function  In particular  we can remove all clauses which evaluate to True from the set of hard clauses  These clauses are redundant  Examples of operations that aid in identifying such hard clauses are unit propagation  resolution and subsumption elimination  For example  given a hard clause A  the hard clause A  B is redundant and can be removed because it is subsumed within A  Similarly  A could be removed after unit propagation  because it always evaluates to True  We can simplify the soft clauses based on the hard clauses by removing all soft clauses which evaluate to either True or False  multiplying the partition function with an appropriate constant to account for their removal  For example  given a hard clause A  the soft clause A  B having weight w is always satisfied and can be removed  by multiplying the partition function by exp w     Another advancement that we can use is problem decomposition  Darwiche        Dechter and Mateescu         The idea here is that if the soft and hard clauses of a PropMRF can be partitioned into k     sets such that any two clauses in any of the k sets have no variables in com  Note that if we remove a variable that is not a unit clause from all the hard and soft clauses  then we have to multiply the partition function by       AVBVCVDVE  w    AVBVCVFVG w    DVEVH w    FVGVJ w   Left arcs are True arcs and right arcs are False arcs  Algorithm    Formula Decomposition and Conditioning  FDC  Input  A PropMRF M Output  ZM begin w         Simplify begin Simplify the hard and soft clauses  Add the weights of all soft clauses which evaluate to True to w  Remove all soft clauses which evaluate to either True or False from M  Update w to account for variables completely removed from all formulas  if FM has an empty clause then return   if FM has only unit clauses then return exp w   AVBVC   DVE  w    FVG w    A   B   C  DVEVH w    FVGVJ w     DVEVH w    FVGVJ w   A VBVC  Decompose Decompose  DVEVH w     FVGVH w    A VBVC   DVE w    DVEVH w   DVE  DVE   FVG w    FVGVJ w   FVG   H w   D E  FVG   J w   F G  end    Decompose begin if the primal graph of M is decomposable into k components then Let M    M         Mk be the PropMRFs corresponding to the k components  return exp w   F DC M            F DC Mk    Figure    Search space of Formula Decomposition and Conditioning for an example PropMRF mon  then the partition function equals the product of the partition functions of the k PropMRFs induced by each set  The following example demonstrates simplification and decomposition on an example PropMRF  E XAMPLE    Consider the PropMRF shown in Figure    After conditioning on A and simplifying using Boolean constraint propagation  we get two PropMRFs shown under the True  left  and false  right  branches of A in Figure    The PropMRF at the True branch contains only two soft clauses which have no variables in common  Thus  they could be decomposed into two PropMRFs as shown  The contribution to the partition function due to the True branch of A is then simply a product of the partition functions of the two PropMRFs and exp w    w          Our main observation is that we can condition on arbitrary formulas instead of variables  Formally  given an arbitrary formula Hj   we can express ZM as  ZM  X     m Y  i  xV  i      xSol FM Hj   i       X  m Y  i  xV  i           xSol FM Hj   i      ZMHj   ZMHj       When combined with Boolean constraint propagation and problem decomposition  this seemingly simple idea is quite powerful because it can yield a smaller search space  as we demonstrate in the following example  In some cases  these reductions could be significant  E XAMPLE    Consider again the PropMRF shown in Figure    The first two clauses share a sub clause A  B  C  If we condition first on A  B  C  we get the search space shown in Figure    which has only   leaf nodes  One can  end    Condition begin Heuristically choose a formula R to condition on  Add hard clauses logically equivalent to R and R to M yielding MR and MR respectively  return exp w    F DC MR     F DC MR    end end  verify that if we condition only on the variables instead of arbitrary formulas  the best ordering scheme will explore    leaf nodes   This search space is not shown because of lack of space  It can be worked out using Figure     Algorithm Formula Decomposition and Conditioning  FDC  is presented as Algorithm    It takes as input a PropMRF M  The first step is the simplification step in which we reduce the size of the hard and the soft clauses using techniques such as unit propagation  resolution and subsumption elimination  In the decomposition step  Step     we decompose M into independent PropMRFs if its primal graph is decomposable  Each of them are then solved independently  Note that this is a very important step and is the primary reason for efficiency of techniques such as recursive conditioning  Darwiche        and AND OR search  Dechter and Mateescu         In fact  the whole idea in performing simplification and heuristic conditioning is to split the PropMRF into several PropMRFs that can be solved independently  Finally  in the conditioning step  we heuristically select a formula R to condition on and then recurse on the true and the false assignments to R    We summarize the dominance of FDC over VDC  where VDC is same as FDC except that we condition only on unit clauses in Step    in the following proposition  P ROPOSITION    Given a PropMRF M  let SM F and SM V be the number of nodes in the smallest search space explored by FDC and VDC respectively  Then SM F  SM V   Sometimes  this inequality can be strict  Improvements We consider two important improvements  First  note that if we are not careful  the algorithm as presented may yield a super exponential search space  For example  if we condition on a set of arbitrary formulas  none of which simplify the PropMRF  we may end up conditioning on a super exponential number of formulas  Trivially  to guarantee at least an exponential search space in the size of the clausal specification  the formula selected for conditioning must reduce simplify at least one soft clause or at least one hard clause  Second  we can augment FDC with component caching and clause learning as in Cachet  Sang et al         and use w cutset conditioning  Dechter        in a straight forward manner  We omit the details      Related work FDC generalizes variable based conditioning schemes such as recursive conditioning  Darwiche         AND OR search  Dechter and Mateescu        and value elimination  Bacchus et al         because all we have to do is restrict our conditioning to unit clauses  FDC also generalizes weighted model counting  WMC  approaches such as ACE  Chavira and Darwiche        and Cachet  Sang et al          These weighted model counting approaches introduce additional Boolean variables to model each soft clause  Conditioning on these Boolean variables is equivalent to conditioning on the soft clauses present in the PropMRF  Thus  FDC can simulate WMC by restricting its conditioning to not only the unit clauses but also the soft clauses already present in the PropMRF  Finally  FDC is related to streamlined constraint reasoning  SCR  approach of  Gomes and Sellmann         The idea in SCR is to add a set of streamlining formulas to the input formula in order to cut down the size of its solution space in a controlled manner  The goal of streamlining is solving a Boolean Satisfiability  or a Constraint Satisfaction  problem while FDC uses  streamlined  formulas for weighted model counting     Formula Importance Sampling In this section  we generalize conventional variable based importance sampling to formula importance sampling and show that our generalization yields new sampling schemes having smaller variance  We first present background on variable based importance sampling       Variable Based Importance Sampling Importance sampling  Rubinstein        is a general scheme which can be used to approximate any quantity such as ZM which can be expressed as a sum of a function over a domain  The main idea is to use an importance distribution Q  which satisfies PM  x       Q x      and express ZM as follows  ZM       X  m Y  Q x  Q x  xSol F   i   Q     I x  m i   i  xV  i     EQ Q x  i  xV  i            where I x  is an indicator function which is   if x is a solution of FM and   otherwise  Given N independent and identical  i i d   samples  x              x N     drawn from Q  we can estimate ZM usbN   defined below  ing Z  i   i  Qm N   X I x   j   j  xV  j     b ZN   N i   Q x i           bN     ZM   It is known  Rubinstein        that EQ  Z namely it is unbiased  The mean squared error  MSE  of bN is given by  Z Q i h I x  m i   i  xV  i     V arQ Q x  bN     M SE Z      N Thus  we can reduce the mean squared error by either reducing the variance  given in the numerator  or by increasing the number of samples N  or both        Formula based Importance Sampling Importance sampling can be extended to the space of clauses  or formulas  in a straight forward manner  Let H    H            Hr   be a set of arbitrary formulas over the variables X of M  and let h    h            hr   be a truth assignment to all the clauses in H  Let H be such that every consistent truth assignment h evaluates all soft clauses to either True or False  Note that this condition is critical  Trivially  if H equals the set of soft clauses  then the condition is satisfied  Let Fh be the formula corresponding to conjunction  H    h          Hr   hr   and let xh  Sol Fh    Given a function   let xh V    be the restriction of xh to the scope of   Then  given an importance distribution U  H   we can rewrite ZM as  Q X   Fh  FM    m i   i  xh V  i     U  h  ZM   U  h  hH Qm       Fh  FM    i   i  xh V  i            EU U  h    Algorithm    Formula Importance Sampling  FIS       Variance Reduction  Input  A PropMRFM and an importance distribution U  H  over a set of clauses H    H            Hr   Output  An unbiased estimate of ZM begin e      N     Z repeat qb      Backtrack free probability is stored here   G   FM and h    for i     to  H  do Let G    G  Hi and G    G  Hi if G  and G  have a solution  Checked using a SAT solver  then Sample hi from U  Hi  h  h   h  hi qb   qb  U  Hi   hi  h  G   G   Hi   hi   else if G  is Satisfiable then h   h   Hi      G   G   Hi      else h   h   Hi      G   G   Hi       eN output by Algorithm   is likely to have The estimate Z bN given in Equation     smaller mean squared error than Z In particular  given a variable based importance distribution Q X   we can always construct a formula based importance distribution U  H  from Q X   such that the variance eN is smaller than that of Z bN   Define  of Z X Q xh        U  h     end  w   sum of weights of soft clauses satisfied by h s   Estimate of model counts of G e Z e   s  exp w  qb Z N  N    until timeout e   Z N e Z e return Z  Given N samples h              h N   generated from U  H   we eN   where  can estimate ZM as Z Qm N   X   Fh i   FM    j   j  xh i   V  j     e ZN   N i   U  h i           There are two issues that need to be addressed in order to use Equation    for any practical purposes  First  the importance distribution U  h  may suffer from the rejection problem  Gogate and Dechter      a  in that we may generate truth assignments  to clauses  which are inconsistent  namely their model count is zero  Note that this could happen even if there are no hard clauses in M because the formula combinations considered may be inconsistent  Fortunately  if we ensure that U  h      whenever h is inconsistent  namely make U  h  backtrack free  Gogate and Dechter      a   we can avoid this problem altogether  Algorithm   outlines a procedure for constructing such a distribution using a complete SAT solver  for example Minisat  Sorensson and Een          Second  computing   Fh j   FM   exactly may be too time consuming  In such cases  we can use state of the art approximate counting techniques such as ApproxCount  Wei and Selman         SampleCount  Gomes et al         and SampleSearch  Gogate and Dechter      b    xh Sol Fh FM    Intuitively  each sample from U  H  given by Equation    is heavy in the sense that it corresponds to   FM  Fh   samples from Q xh    Because of this larger sample size  eN is smaller than that of Z bN  assuming the variance of Z that   FM  Fh   can be computed efficiently   The only caveat is that generating samples from U  H  is more expensive  Formally  the proof is provided in the extended version of the paper available online   T HEOREM    Given a PropMRF M  a proposal distribution Q X  defined over the variables of M  a set of formulas H    H            Hr   and a distribution U  H  defined as eN is less than or equal to in Equation     the variance of Z bN   that of Z  We can easily integrate FIS with other variance reduction schemes such as Rao Blackwellisation  Casella and Robert        and AND OR sampling  Gogate and Dechter         These combinations can lead to interesting time versus variance tradeoffs  We leave these improvements for future work  We describe how U  H  can be constructed in practice in the next section     Experiments     Exact Inference We compared Formula Decomposition and Conditioning  FDC  against Variable Decomposition and Conditioning  VDC   variable elimination  VE   Dechter        and ACE  Chavira and Darwiche         which internally uses the C D compiler  Darwiche         for computing the partition function on benchmark problems from three domains   a  Random networks   b  medical diagnosis networks and  c  Relational networks  ACE  FDC and VDC use the same clausal representation while VE uses tabular representation  Note that the domains are deliberately chosen to elucidate the properties of FDC  in particular  to verify our intuition that as size of the clauses increases  FDC is likely to dominate VDC  We implemented FDC and VDC on top of RELSAT  Roberto J  Bayardo Jr  and Pehoushek         which is a SAT model counting algorithm  As mentioned earlier  after conditioning on a formula  we use various Boolean   Problem Random                                                                                                 QMRDT                                                                                                    FS fs      fs      fs      fs      Cora Cora  Cora   w  FDC  VDC  ACE  VE                                                                                                                               X X                                                                   X X X                                                        X X X            X X      X X X      X X X                                                                                                                                                                                                                            X                                                                                        X X      X X X X X X X                                                                                                                      X X X                                  X       X       X  Table    Average runtime in seconds of the four algorithms used in our study over    random instances for each problem  We gave each solver a time bound of   hrs and a memory bound of  GB  X indicates that either the memory or time bound was exceeded  The second column gives the average treewidth   propagation  pruning techniques such as unit propagation  clause learning  subsumption elimination and resolution  Also  similar to Cachet  Sang et al          we use component caching and similar to w cutset conditioning  Dechter         we invoke bucket elimination at a node if the treewidth of the  remaining  PropMRF at the node is less than     Since FDC is a DPLL style backtracking search scheme  its performance is highly dependent upon a good branching heuristic  that selects the next clause to condition on   In our implementation  we used a simple dynamic heuristic of conditioning on the largest sub clause  unit clause in case of VDC  that is common to most hard and soft clauses  ties broken arbitrarily  The main intuition for this heuristic is that branching on the largest common sub clause would cause the most propagation  yielding the most reduction in the search space size  We also tried a few other heuristics  both static and dynamic  such as  i  conditioning on a subclause C  and its negation  that causes the most unit propa   gations  but one has to perform unit propagations for each candidate clause  which can be quite expensive in practice   ii  graph partitioning heuristics based on the min fill  mindegree and hmetis orderings  these heuristics are used by solvers such as ACE  Chavira and Darwiche        and AND OR search  Dechter and Mateescu        and  iii  Entropy based heuristics  The results for these heuristics show a similar trend as the results for the heuristic used in our experiments  with the latter performing better on an average  We leave the development of sophisticated formulaordering heuristics for future work  Table   shows the results  For each problem  we generated    random instances  For each instance  we set    of randomly chosen variables as evidence  Each row shows the average time in seconds for each problem        Random networks Our first domain is that of random networks  The networks are generated using the model  n  m  s   where n is the number of  Boolean  variables  m is the number of weighted clauses and s is the size of each weighted clause  Given n variables X    X            Xn    each clause Ci  for i     to m  is generated by randomly selecting s  distinct  random variables from X and negating each with probability      For our experiments  we set n   m and experimented with three values for n and m  n  m                s was varied from   to   in increments of    A random problem  n  m  s  is designated as n  m  s in Table    We see that FDC dominates VDC as s increases  ACE is often inferior to FDC and often inferior to VDC  As expected  variable elimination which does not take advantage of the structure of the formulas is the fastest scheme when the treewidth is small but is unable to solve any problems having treewidth greater than           Medical Diagnosis Our second domain is a version of QMR DT medical diagnosis networks  Shwe et al         as used in Cachet  Sang et al          Each problem can be specified using a two layer bipartite graph in which the top layer consists of diseases and the bottom layer consists of symptoms  If a disease causes a symptom  there is an edge from the disease to the symptom  We have a weighted unit clause for each disease and a weighted clause for each symptom  which is simply a logical OR of the diseases that cause it  in  Sang et al          this clause was hard  We attach an arbitrary weight to it to make the problem harder   For our experiments  we varied the numbers of diseases and symptoms from    to     For each symptom  we varied the number of diseases that can cause it from   to    in increments of    The diseases for each symptom are chosen randomly  A QMR DT problem  d  f  s  is designated as d  f  s in Table    We can see that as the size of the clauses increases    FDC performs better than VDC  FDC also dominates ACE as the problem size increases        Relational networks Our final domain is that of relational networks  We experimented with the Friends and Smokers networks and the Entity resolution networks  In the friends and smokers networks  FS   we have three first order predicates smokes x   which indicates whether a person smokes  cancer x   which indicates whether a person has cancer  and f riends x  y   which indicates who are friends of whom  The probabilistic model is defined by assigning weights to two logical constraints  f riends x  y   smokes x   smokes y  and smokes x   cancer x   Given a domain for x and y  a PropMRF can be generated from these two logical constraints by considering all possible groundings of each predicate  We experimented with different domain sizes for x and y ranging from    to     From Table    we can see that the time required by FDC is almost the same as VDC  This is because the size of the clauses is small       ACE dominates both FDC and VDC  Entity resolution is the problem of determining which observations correspond to the same entity  In our experiments  we consider the problem of matching citations of scientific papers  We used the CORA Markov logic network given in the Alchemy tutorial  Kok et al          This MLN has ten predicates such as Author bib  author   T itle bib  title   SameAuthor author  author   SameT itle title  title  etc  and clauses ranging from size   to    The clauses express relationship such as  if two fields have high similarity  then they are  probably  the same  if two records are the same  their fields are the same  and vice versa  etc  We experimented with domain sizes of   and   for each of the   first order variables present in the domain  The problems are denoted as cora  and cora  respectively  From Table    we can see that FDC is the only algorithm capable of solving the largest instance      Approximate Inference We compared Formula importance sampling  FIS  against Variable importance sampling  VIS  and stateof the art schemes such as MC SAT  Poon and Domingos        and Gibbs sampling available in Alchemy  Kok et al         on the three domains described above  For both VIS and FIS  we chose to construct the importance distribution Q from the output of a Belief propagation scheme  BP   because BP was shown to yield a better importance function than other approaches in previous studies  Yuan and Druzdzel        Gogate and Dechter         We describe next  how the method described in  Gogate  and Dechter        can be adapted to construct an importance distribution over formulas  Here  we first run BP  or Generalized Belief Propagation  Yedidia et al          over a factor  or region  graph in which the nodes are the variables and the factors are the hard and the soft clauses  Let  C            Cm   be an ordering over the soft clauses  Given a truth assignment to the first i    soft clauses ci     c            ci     we compute U  Ci  ci    as follows  We first simplify the formula F   FM  Fci    possibly deriving new unit clauses  Let Ci be the marginal distribution at the factor corresponding to the clause Ci in the output of BP  Then  U  Ci  ci    is given by  X U  Ci   T rue ci          IF Ci  y Ci  y  yCi  where IF Ci  y      if y evaluates Ci to True but does not violate any unit clause in F   and   otherwise  Note that the importance distribution Q over the variables is a special case of the scheme described above in which we construct a distribution over all the unit clauses  We implemented Algorithm   as follows  Notice that the algorithm requires a SAT solver and a model counter  We used Minisat  Sorensson and Een        as our SAT solver  For model counting  we use the RELSAT model counter whenever exact counting was feasible  and the approximate solver SampleSearch  Gogate and Dechter      b  whenever it wasnt  We measure the performance of the sampling schemes using the sum Kullback Leibler divergence  KLD  between the exact and the approximate posterior marginals for each variable given evidence  Time versus sum KLD plots for two representative problems from each domain are shown in Figures      and    We can clearly see that as the size of the clauses increases  FIS outperforms VIS  MC SAT and Gibbs sampling     Summary and Conclusion In this paper  we introduced a new formula based approach for performing exact and approximate inference in graphical models  Formula based inference is attractive because   a  it generalizes standard variable based inference   b  it yields several new efficient algorithms that are not possible by reasoning just over the variables and  c  it fits naturally within the recent research efforts in combining logical and probabilistic Artificial Intelligence  Our empirical evaluation shows that formula based approach is especially suitable for domains having large clauses  Such clauses are one of the main reasons for using logic instead of tables for representing potentials or    Exact counting was invoked if the number of variables was less than      which was the case for most networks that we experimented with  except the relational benchmarks                       Sum KLD  Sum KLD                                             e      e      e                                               Time in seconds MC SAT Gibbs sampling                        Time in seconds  FormulaIS VariableIS  MC SAT Gibbs sampling   a  Random problem  n       m       s       FormulaIS VariableIS   a  QMR DT problem  d       f       s                           Sum KLD     Sum KLD                                                       e      e      e                                          Time in seconds MC SAT Gibbs sampling                             Time in seconds  FormulaIS VariableIS   b  Random problem  n       m       s            MC SAT Gibbs sampling  FormulaIS VariableIS   b  QMR DT problem  d       f       s        Figure    Time versus Sum KLD plots for   Random instances   Figure    Time versus Sum KLD plots for   QMR DT networks   CPTs in graphical models  In particular  conventional tabular representations require space exponential in the number of variables in the scope of the potential  while if the potential can be summarized using a constant number of clauses  we only require linear space  Since an efficient inference scheme is one of the main bottleneck in learning PropMRFs having large clauses  we believe that our formula based approach to inference can lead to new structure and weight learning schemes that learn large weighted clauses from data   D        NSF grants IIS         and IIS          and ONR grant N                 The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of ARO  DARPA  NSF  ONR  or the United States Government   Our work can be extended in several ways  In particular  we envision formula based versions of various inference schemes such as variable elimination  belief propagation and Markov Chain Monte Carlo  MCMC  sampling  One of these schemes  namely formula elimination trivially follows from this work  as it is known that conditioning works along the reverse direction of elimination  Dechter         Also  we envision the development of lifted versions of all the formula based schemes proposed in this paper  Acknowledgements This research was partly funded by ARO grant W   NF           AFRL contract FA        C       DARPA contracts FA                FA        D       HR        C       HR        C      and NBCH   
  of our approach on a complex dynamic domain of a persons transportation routines   This paper describes a general framework called Hybrid Dynamic Mixed Networks  HDMNs  which are Hybrid Dynamic Bayesian Networks that allow representation of discrete deterministic information in the form of constraints  We propose approximate inference algorithms that integrate and adjust well known algorithmic principles such as Generalized Belief Propagation  Rao Blackwellised Particle Filtering and Constraint Propagation to address the complexity of modeling and reasoning in HDMNs  We use this framework to model a persons travel activity over time and to predict destination and routes given the current location  We present a preliminary empirical evaluation demonstrating the effectiveness of our modeling framework and algorithms using several variants of the activity model   Focusing on algorithmic issues  the most popular approximate query processing algorithms for dynamic networks are Expectation propagation EP   Heskes and Zoeter        and Rao Blackwellised Particle Filtering  RBPF   Doucet et al          We therefore extend these algorithms to accommodate and exploit discrete constraints in the presence of continuous probabilistic functions  Extending Expectation Propagation to handle constraints is easy  extension to continuous variables is a little more intricate but still straightforward  The presence of constraints introduces a principles challenge for Sequential Importance Sampling algorithms  however  Indeed the main algorithmic contribution of this paper in presenting a class of Rao Blackwellised Particle Filtering algorithm  IJGP RBPF for HDMNs which integrates a Generalized Belief Propagation component with a Rao Blackwellised Particle Filtering scheme     INTRODUCTION Modeling sequential real life domains often requires the ability to represent both probabilistic and deterministic information  Hybrid Dynamic Bayesian Networks  HDBNs  were recently proposed for modeling such phenomena  Lerner         In essence  these are factored representation of Markov processes that allow discrete and continuous variables  Since they are designed to express uncertain information they represent constraints as probabilistic entities which may have negative computational consequences  To address this problem  Dechter and Mateescu        Larkin and Dechter        introduced the framework of Mixed Networks  In this paper we extend the Mixed Networks framework to dynamic environments  allow continuous Gaussian variables  yielding Hybrid Dynamic Mixed Networks  HDMN   We address the algorithmic issues that emerge from this extension and demonstrate the potential  Our motivation for developing HDMNs as a modeling framework is a range of problems in the transportation literature that depend upon reliable estimates of the prevailing demand for travel over various time scales  At one end of this range  there is a pressing need for accurate and complete estimation of the global origins and destinations  O D  matrix at any given time for an entire urban area  Such estimates are used in both urban planning applications  Sherali et al         and integrated traffic control systems based upon dynamic traffic assignment techniques  Peeta and Zilaskopoulos         Even the most advanced techniques  however  are hamstrung by their reliance upon out dated  pencil and paper travel surveys and sparsely distributed detectors in the transportation system  We view the increasing proliferation of powerful mobile computing devices as an opportunity to remedy this situation  If even a small sample of the traveling public agreed to collect their travel data and make that data publicly available  transportation management systems could significantly improve their operational efficiency  At the   other end of the spectrum  personal traffic assistants running on the mobile devices could help travelers replan their travel when the routes they typically use are impacted by failures in the system arising from accidents or natural disasters  A common starting point for these problems is to develop an efficient formulation for learning and inferring individual traveler routines like travelers destination and his route to destination from raw data points  The rest of the paper is organized as follows  In the next section  we discuss preliminaries and introduce our modeling framework  We then describe two approximate inference algorithms for processing HDMN queries  an Expectation Propagation type and a Particle Filtering type  Subsequently  we describe the transportation modeling approach and present preliminary empirical results on how effectively a model is learnt and how accurately its predictions are given several models and a few variants of the relevant algorithms  We view the contribution of this paper in addressing a complex and highly relevant real life domain using a general framework and domain independent algorithms  thus allowing systematic study of modeling  learning and inference in a non trivial setting     PRELIMINARIES AND DEFINITIONS Hybrid Bayesian Networks  HBN   Lauritzen        are graphical models defined by a tuple B    X  G  P   where X is the set of variables partitioned into discrete and continuS ous ones X      respectively  G is a directed acyclic graph whose nodes corresponds to the variables  P    P         Pn   is a set of conditional probability distributions  CPDs   Given variable xi and its parents in the graph pa xi    Pi   P xi  pa xi     The graph structure G is restricted in that continuous variables cannot have discrete variables as their child nodes  The conditional distribution of continuous variables are given by a linear Gaussian model  P xi  I   i  Z   z    N  i     i   z   i  xi   where Z and I are the set of continuous and discrete parents of xi   respectively and N     is a multi variate normal distribution  The network represents a joint distribution over all its variables given by a product of all its CPDs  A Constraint Network  Dechter        is a graphical model R    X  D C   where X    x            xn   is the set of variables  D    D            Dn   is their respective discrete domains and C    C   C           Cm   is the set of constraints  Each constraint Ci is a relation Ri defined over a subset of the variables Si  X and denotes the combination of values that can be assigned simultaneously  A Solution is an assignment of values to all the variables such that no constraint is violated  The primary query is to decide if the constraint network is consistent and if so find one or all solutions   The recently proposed Mixed Network framework  Dechter and Mateescu        for augmenting Bayesian Networks with constraints  can immediately be applied to HBNs yielding the Hybrid Mixed Networks  HMNs   Formally  given a HBN B    X  G  P  that expresses the joint probability PB and given a constraint network R    X  D C  that expresses a set of solutions   an HMN is a pair M    B   R    The discrete variables and their domains are shared by B and R and the relationships are those expressed in P and C  We assume that R is consistent  The mixed network M    B   R   represents the conditional probability PM  x    PB  x x    i f x   and   otherwise  Dynamic Bayesian Networks are Markov models whose state space and transition functions are expressed in a factored form using Bayesian Networks  They are defined by a prior P X    and a state transition function P Xt    Xt    Hybrid Dynamic Bayesian Networks  HDBNs  allow continuous variables while Hybrid Dynamic Mixed Networks  HDMNs  also permit explicit discrete constraints  D EFINITION     A Hybrid Dynamic Mixed Network  HDMN  is a pair  M    M    defined over a set of variables X    x         xn    where M  is an HMN defined over X representing P X     M is a   slice network defining the stochastic process P Xt    Xt    The   time slice Hybrid    Mixed network    THMN  is an HMN defined over X    X      such that X and X are identical to X  The acyclic graph   of the probabilistic portion is restricted so that nodes in X are root nodes and have no CPDs associated with them  The constraints are defined the usual way  The   THMN      represents a conditional distribution P X  X    The semantics of any dynamic network can be understood by unrolling the network to T time slices  Namely  T P X  t     P X     t   P Xt  Xt    where each probabilistic component can be factored in the usual way  yielding a regular HMN over T copies of the state variables  The most common task over Dynamic Probabilistic Networks is filtering and prediction Filtering is the task of determining the belief state P Xt  e  t   where Xt is the set of variables at time t and e  t are the observations accumulated at time slices   to t  Filtering can be accomplished in principle by unrolling the dynamic model and using any stateof the art exact or approximate reasoning algorithm  The join tree clustering algorithm is the most commonly used algorithm for exact inference in Bayesian networks  It partitions the CPDs and constraints into clusters that interact in a tree like manner  the join tree  and applies messagepassing between clusters  The complexity of the algorithm is exponential in a parameter called treewidth  which is the maximum number of discrete variables in a cluster  However  the stochastic nature of Dynamic Networks restricts the applicability of join tree clustering considerably  In the discrete case the temporal structure implies   tree width which equals to the number of state variables that are connected with the next time slice  thus making the factored representation ineffective  Even worse  when both continuous and discrete variables are present the effective treewidth is O T   when T is the number of time slices  thus making exact inference infeasible  Therefore the applicable approximate inference algorithms for Hybrid Dynamic Networks are either sampling based such as Particle Filtering or propagation based such as Expectation Propagation  In the next two sections  we will extend these algorithms to HDMNs     EXPECTATION PROPAGATION In this section we extend an approximate inference algorithm called Expectation Propagation  EP   Heskes and Zoeter        from HDBNs to HDMNs  The idea in EP  forward pass  is to perform Belief Propagation by passing messages between slices t and t     along the ordering t     to T   EP can be thought of as an extension of Generalized Belief Propagation  GBP  to HDBNs  Heskes and Zoeter         For simplicity of exposition  we will extend a GBP algorithm called Iterative Join graph propagation  Dechter et al         to HDMNs and call our technique IJGP i  S where S denotes that the process is sequential  The extension is rather straightforward and can be easily derived by integrating the results in  Murphy        Dechter et al         Lauritzen        Larkin and Dechter         IJGP  Dechter et al         is a Generalized Belief Propagation algorithm which performs message passing on a join graph  A join graph is collection of cliques or clusters such that the interaction between the clusters is captured by a graph  Each clique in a join graph contains a subset of variables from the graphical model  IJGP i  is a parameterized algorithm which operates on a join graph which has less than i     discrete variables in each clique  The complexity of IJGP i  is bounded exponentially by i   also called the i bound  In the message passing step of IJGP i   a message is sent between any two nodes that are neighbors of each other in the join graph  A message sent by node Ni to N j is constructed by multiplying all the functions and messages in a node  except the message received from N j   and marginalizing on the common variables between N j and Ni  see  Dechter et al           IJGP i  can be easily adapted to HDMNs  which we call IJGP i  S  and we describe some technical details here rather than a complete derivation due to lack of space  Note that because we are performing online inference  we need to construct the join graph used by IJGP i  S in an online manner rather than recomputing the join graph every time new evidence arrives  Murphy  Murphy        describes a method to compute a join tree in an online manner by pasting together join trees of individual time slices using  special cliques called the interface   Dechter et al         describe a method to compute join graphs from join trees  The two methods can be combined in a straightforward way to come up with an online procedure for constructing a join graph  In this procedure  we split the interface into smaller cliques such that the new cliques have less than i     variables  This construction procedure is shown in Figure    Message passing is then performed in a sequential way as follows  At each time slice t  we perform message passing over nodes in t and the interface of t with t    and t      shown by the ovals in Figure     The new functions computed in the interface of t with t     are then used by t      when we perform message passing in t      Three important technical issues remain to be discussed  First  message passing requires the operations of multiplication and marginalization to be performed on functions in each node  These operators can be constructed for HDMNs in a straightforward way by combining the operators by  Lauritzen        and  Larkin and Dechter        that work on HBNs and discrete mixed networks respectively  We will now briefly comment on how the multiplication operator can be derived  Let us assume we want to multiply a collection of probabilistic functions P  and a set of constraint relations C   which consist of only discrete tuples allowed by the constraint  to form a single function PC  Here  multiplication can be performed on the functions in P  and C  separately using the operators in  Lauritzen        and  Dechter        respectively to compute a single probabilistic function P and a single constraint relation C  These two functions P and C can be multiplied by deleting all tuples in P that are not present in C to form the required function PC  Second  because IJGP i  S constructs join graphs sequentially  the maximum i bound for IJGP i  S is bounded by the treewidth of the time slice and its interfaces and not the treewidth of the entire HDMN model  see Figure      Figure    Schematic illustration of the Procedure used for creating join graphs and join trees of HDMNs   Algorithm IJGP RBPF  Input  A Hybrid Dynamic Mixed Network  X  D  G  P C   T and a observation sequence e  T Integer N  w and i   Output  P XT  e  T    For t     to T do  Sequential Importance Sampling step     Generalized Belief Propagation step Use IJGP i  to compute the proposal distribution app    Rao Blackwellisation step Partition the Variables Xt into Rt and Zt such that the treewidth of a join tree of Zt is w     Sampling step For i     to N do  a  Generate a Rti from app    b  reject sample if rti is not a solution  i i     c  Compute the importance weights wti of Rti   ci      Normalize the importance weights to form w t  Selection step   Resample N samples from Rbti according to the normalized importance weights ci to obtain new N random samples  w t   Exact step   for i     to N do i   e   Rbi and Use join tree clustering to compute the distribution on Zti given Zt  t t d i R   t   Figure    IJGP RBPF for HDMNs Third  IJGP i  guarantees that the computations will be exact if i is equal to the treewidth  This is not true for IJGP i S in general as shown in  Lerner         It can be proved that  T HEOREM     The complexity of IJGP i  S is O    t     n   d i  t     T   where  t   is the number of discrete variables in time slice t  d is the maximum domain size of the discrete variables  i is the i bound used  n is the number of nodes in a join graph of the time slice  t is the maximum number of continuous variables in the clique of the join graph used and T is the number of time slices     RAO BLACKWELLISED PARTICLE FILTERING In this section  we will extend the Rao Blackwellised Particle filtering algorithm  Doucet et al         from HDBNs to HDMNs  Before  we present this extension  we will briefly review Particle Filtering and Rao Blackwellised Particle Filtering  RBPF  for HDBNs  Particle filtering uses a weighted set of samples or particles to approximate the filtering distribution  Thus  given a set of particles Xt            XtN approximately distributed according to the target filtering distribution P Xt   M e  t    the filtering distribution is given by P Xt   M e  t       N Ni    Xti   M  where  is the Dirac delta function  Since we cannot sample from P Xt   M e  t   directly  Parti   cle filtering uses an appropriate  importance  proposal distribution Q X  to sample from  The particle filter starts by generating N particles according to an initial proposal distribution Q X   e     At each step  it generates the next state i for each particle X i by sampling from Q X i Xt   t    Xt   e  t    t It then computes the weight of each particle based given by wt   P X  Q X  to compute a weighted distribution and then re samples from the weighted distribution to obtain a set of un biased or un weighted particles  Particle filtering often shows poor performance in highdimensional spaces and its performance can be improved by sampling from a sub space by using the RaoBlackwellisation  RB  theorem  and the particle filtering is called Rao Blackwellised Particle Filtering  RBPF    Specifically  the state Xt is divided into two sets  Rt and Zt such that only variables in set Rt are sampled  from a proposal distribution Q Rt     while the distribution on Zt is computed analytically given each sample on Rt  assuming that P Zt  Rt   e  t   Rt    is tractable   The complexity of RBPF is proportional to the complexity of exact inference step i e  computing P Zt  Rt   e  t   Rt    for each sample Rtk   w cutset  Bidyuk and Dechter        is a parameterized way to select Rt such that the complexity of computing P Zt  Rt   e  t   Rt    is bounded exponentially by w  Below  we use the w cutset idea to perform RBPF in HDMNs  Since exact inference can be done in polynomial time if a HDBN contains only continuous variables  a straightforward application of RBPF to HDBNs involves sampling only the discrete variables in each time slice and exactly inferring the continuous variables  Lerner         Extending this idea to HDMNs  suggests that in each time slice t we sample the discrete variables and discard all particles that violate the constraints in the time slice  Let us assume that we select a proposal distribution Q that is a good approximation of the probabilistic filtering distribution but ignores the constraint portion  The extension described above can be inefficient because if the proposal distribution Q is such that it makes non solutions to the constraint portion highly probable  most samples from Q will be rejected  because these samples Rti will have P Rti       and so the weight will be zero   Thus  on one extreme sampling only from the Bayesian Network portion of each time slice may lead to potentially high rejection rate  On the other extreme  if we want to make the sample rejection rate zero we would have to use a proposal distribution Q  such that all samples from this distribution are solutions  One way to find this proposal distribution is to make the constraint network backtrack free  using adaptive consistency  Dechter        or exact constraint propagation  along an ordering of variables and then sample along the reverse ordering  Another approach is to use join tree clustering which combines probabilistic and deterministic information and then sample from the join    tree  However  both join tree clustering and adaptiveconsistency are time and space exponential in treewidth and so they are costly when the treewidth is large  Thus on one hand  zero rejection rate implies using a potentially costly inference procedure while on the other hand sampling from a proposal distribution that ignores constraints may result in a high rejection rate  We propose to exploit the middle ground between the two extremes by combining the constraint network and the Bayesian Network into a single approximate distribution app using IJGP i  which is a bounded inference procedure  Note that because IJGP i  has polynomial time complexity for constant i  we would not eliminate the samplerejection rate completely  However  by using IJGP i  we are more likely to reduce the rejection rate because IJGP i  also achieves Constraint Propagation and it is well known that Constraint Propagation removes many inconsistent tuples thereby reducing the chance of sampling a nonsolution   Dechter         Another important advantage of using IJGP i  is that it yields very good approximations to the true posterior  Dechter et al         thereby proving to be an ideal candidate for proposal distribution  Note that IJGP i  can be used as a proposal distribution because it can be proved using results from  Dechter and Mateescu        that IJGP i  includes all supports of P Xt  e  t   Xt     i e  P Xt  e  t   Xt        implies that the output of IJGP i  viz  Q      Note that IJGP i  we use here is different from the algorithm IJGP i  S that we described in the previous section  This is because in our RBPF procedure  we need to comk  e   pute an approximation to the distribution P Rt  Rt    t k given the sample Rt  on variables Rt  and evidence e  t   IJGP i  as used in our RBPF procedure works on HMNs and can be derived using the results in  Dechter et al         Lauritzen        Larkin and Dechter         For lack of space we do not describe the details of this algorithm  see a companion paper  Gogate and Dechter        for details   The integration of the ideas described above into a formal algorithm called IJGP RBPF is given in Figure    It uses the same template as in  Doucet et al         and the only step different in IJGP RBPF from the original template is the implementation of the Sequential Importance Sampling step  SIS   SIS is divided into three steps      In the Generalized Belief Propagation step of SIS  we first perform Belief Propagation using IJGP i  to form an approximation of the posterior  say app   as described above      In the Rao Blackwellisation step  we first partition the variables in a  THMN into two sets Rt and Zt using a method due to  Bidyuk and Dechter         This method  Bidyuk and Dechter        removes minimal variables Rt from Xt such that the treewidth of the remain   Figure    Car Travel Activity model of an individual ing network Zt is bounded by w      In the sampling step  the variables Rt are sampled from app   To generate a sample from app   we use a special data structure of ordered buckets which is described in a companion paper  Gogate and Dechter         Importance weights are computed as usual  Doucet et al          Finally  the exact step computes a distribution on Zt using join tree clustering for HMNs  see a companion paper  Gogate and Dechter        for details on join treeclustering for HMNs   It can be proved that  T HEOREM     The complexity of IJGP RBPF i w  is O  NR  d w            n    d i           T   where    is the number of discrete variables  d is the maximum domain size of the discrete variables  i is the adjusted i bound  w is defined by w cutset  n is the number of nodes in a joingraph   is the number of continuous variables in a  THMN  NR is the number of samples actually drawn and T is the number of time slices      THE TRANSPORTATION MODEL  In this section  we describe the application of HDMNs to a real world problem of inferring car travel activity of individuals  The major query in our HDMN model is to predict where a traveler is likely to go and what his her route to the destination is likely to be  given the current location of the travelers car  This application was described in  Liao et al         in a different context for detecting abnormal behavior in Alzheimers patients and they use a Abstract Hierarchical Markov Models  AHMM  for reasoning about this problem  The novelty in our approach is not only a more general modeling framework and approximate inference algorithms but also a domain independent implementation which allows an expert to add and test variants of the model  Figure   shows a HDMN model for modeling the car travel activity of individuals  Note that the directed links express the probabilistic relationships while the undirected  bold    edges express the constraints  We consider the roads as a Graph G V  E  where the vertices V correspond to intersections while the edges E correspond to segments of roads between intersections  The variables in the model are as follows  The variables dt and wt represent the information about time of day and dayof week respectively  dt is a discrete variable and has four values  morning  a f ternoon  evening  night  while the variable wt has two values  weekend  weekday   Variable gt represents the persons next goal  e g  his office  home etc   We consider a location where the person spends significant amount of time as a proxy for a goal  Liao et al          These locations are determined through a preprocessing step by noting the locations in which the dwell time is greater than a threshold     minutes   Once such locations are determined  we cluster those that are in close proximity to simplify the goal set  A goal can be thought of as a set of edges E   E in our graph representation  The route level rt represents the route taken by the person to move from one goal to other  We arbitrarily set the number of values it can take to  gt      The persons location lt and velocity vt are estimated from GPS reading yt   ft is a counter  essentially goal duration  that governs goal switching  The Location lt is represented in the form of a two tuple  a  w  where a    s    s    a  E and s    s   V is an edge of the map G V  E  and w is a Gaussian whose mean is equal to the distance between the persons current position on a and one of the intersections in a  The probabilistic dependencies in the model are straightforward and can be found by tracing the arrows  see Figure     The constraints in the model are as follows  We assume that a person switches his goal from one time slice to another when he is near a goal or moving away from a goal but not when he is on a goal location  We also allow a forced switch of goals when a specified maximum time that he is supposed to spend at a goal is reached  This is modeled by using a constant D  These assumptions of switching between goals is modeled using the following constraints between the current location  the current goal  the next goal and the switching counters     If lt    gt  and Ft      Then Ft   D      If lt    gt  and Ft      Then Ft   Ft          If lt     gt  and Ft      Then Ft     and     If lt     gt  and Ft      Then Ft          If Ft      and Ft     Then gt is given by P gt  gt         If Ft     and Ft     Then gt is same as gt        If Ft      and Ft     gt is same as gt  and     If Ft      and Ft     gt is given by P gt  gt        EXPERIMENTAL RESULTS The test data consists of a log of GPS readings collected by one of the authors  The test data was collected over a six month period at intervals of     seconds each  The data consist of the current time  date  location and veloc   ity of the persons travel  The location is given as latitude and longitude pairs  The data was first divided into individual routes taken by the person and the HDMN model was learned using the Monte Carlo version of the EM algorithm  Liao et al         Levine and Casella         We used the first three months data as our training set while the remaining data was used as a test set  TIGER Line files available from the US Census Bureau formed the graph on which the data was snapped  As specified earlier our aim is two fold   a  Finding the destination or goal of a person given the current location and  b  Finding the route taken by the person towards the destination or goal  To compare our inference and learning algorithms  we use three HDMN models  Model   is the model shown in Figure    Model   is the model given in Figure   with the variables wt and dt removed from each time slice  Model  is the base model which tracks the person without any high level information and is constructed from Figure   by removing the variables wt   dt   ft   gt and rt from each timeslice  We used   inference algorithms  Since EM learning uses inference as a sub step  we have   different learning algorithms  We call these algorithms as IJGP S     IJGPS    and IJGP RBPF     N  and IJGP RBPF     N  respectively  Note that the algorithm IJGP S i   described in Section    uses i as the i bound  IJGP RBPF i w N   described in Section    uses i as the i bound for IJGP i   w as the w cutset bound and N is the number of particles at each time slice  Three values of N were used           and      For EM learning  N was      Experiments were run on a Pentium       GHz machine with  G of RAM  Note that for Model    we only use IJGP RBPF      and IJGP    S because the maximum i bound in this model is bounded by    see section         FINDING DESTINATION OR GOAL OF A PERSON The results for goal prediction with various combinations of models  learning and inference algorithms are shown in Tables      and    We define prediction accuracy as the number of goals predicted correctly  Learning was performed offline  Our slowest learning algorithm based on GBP RBPF      used almost   days of CPU time for Model    and almost   days for Model  significantly less than the period over which the data was collected  The column Time in Tables      and   shows the time for inference algorithms in seconds while the other entries indicate the accuracy for each combination of inference and learning algorithms  In terms of which model yields the best accuracy  we can see that Model   achieves the highest prediction accuracy   of     while Model   and Model   achieve prediction accuracies of     and     respectively or lower  For Model    to verify which algorithm yields the best learned model we see that IJGP RBPF      and IJGP   S yield an accuracy of     and     respectively while for Model    we see that the average accuracy of IJGPRBPF      and IJGP    S was     and     respectively  From these two results  we can see that IJGP RBPF      and IJGP    S are the best performing learning algorithms  For Model   and Model    to verify which algorithm yields the best accuracy given a learned model  we see that IJGP    S is the most cost effective alternative in terms time versus accuracy while IJGP RBPF yields the best accuracy  Table    Goal prediction  Model   N                          Inference IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP    S IJGP    S Average  Time                                          LEARNING IJGP RBPF IJGP S                                                                                                                                                    FINDING THE ROUTE TAKEN BY THE PERSON To see how our models predict a persons route  we use the following method  We first run our inference algorithm on the learned model and predict the route that the person is likely to take  We then super impose this route on the actual route taken by the person  We then count the number of roads that were not taken by the person but were in the predicted route i e  the false positives  and also compute the number of roads that were taken by the person but were not in the actual route i e  the false negatives  The two measures are reported in Table   for the best performing learning models in each category  viz GBP RBPF      for Model   and Model   and GBP RBPF      for Model    As we can see Model   and Model   have the best route prediction accuracy  given by low false positives and false negatives    Table    Goal Prediction Model   N              Inference IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP    S Average  Time                     LEARNING IJGP RBPF      IJGP    S                                       RELATED WORK  Liao et al         and  Patterson et al         describe a model based on AHMEM  Bui        and Hierarchical Markov Models  HMMs  respectively for inferring highlevel behavior from GPS data  Our model goes beyond their model by representing two new variables day of week and time of day which improves the accuracy in our model by about     A mixed network framework for representing deterministic and uncertain information was presented in  Dechter and Larkin        Larkin and Dechter        Dechter and Mateescu         These previous works also describe exact inference algorithms for mixed networks with the restriction that all variables should be discrete  Our work goes beyond these previous works in that we describe approximate inference algorithms for the mixed network framework  allow continuous Gaussian nodes with certain restrictions in the mixed network framework and model discrete time stochastic processes  The approximate inference algorithms called IJGP i  described in  Dechter et al         handled only discrete variables  In our work  we extend this algorithm to include Gaussian variables and discrete constraints  We also develop a sequential version of this algorithm for dynamic models  Particle Filtering is a very attractive research area  Doucet et al          Particle Filtering in HDMNs can be inefficient if non solutions of constraint portion have high probability of being sampled  We show how to alleviate this difficulty by performing IJGP i  before sampling  This algorithm IJGP RBPF yields the best performance in our settings and might prove to be useful in applications in which particle filtering is preferred   Table    Goal Prediction  Model   N                          Inference IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP    S IJGP    S Average  Time                                            LEARNING IJGP RBPF IJGP S                                                                                                                                             Table    False positives  FP  and False negatives for routes taken by a person  FN  N                   INFERENCE IJGP    IJGP    IJGP RBPF      IJGP RBPF      IJGP RBPF      IJGP RBPF       Model  FP FN                                      Model  FP FN                                      Model  FP FN                       CONCLUSION AND FUTURE WORK In this paper  we introduced a new modeling framework called HDMNs  a representation that handles discrete timestochastic processes  deterministic and probabilistic information on both continuous and discrete variables in a systematic way  We also propose a GBP based algorithm called IJGP i  S for approximate inference in this framework  The main algorithmic contribution of this paper is presenting a class of Rao Blackwellised particle filtering algorithm  IJGP RBPF for HDMNs which integrates a generalized belief propagation component with a RaoBlackwellised Particle Filtering scheme for effective sampling in the presence of constraints  Another contribution of this paper is addressing a complex and highly relevant real life domain using a general framework and domain independent algorithms  Directions for future work include relaxing the restrictions made on dependencies between discrete and continuous variables and developing an efficient EM algorithm   ACKNOWLEDGEMENTS The first and third author were supported in part by National Science Foundation under award numbers         and          The second author was supported in part by the NSF grant IIS           
