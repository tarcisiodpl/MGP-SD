 We define the notion of compiling a Bayesian network with evidence and provide a specific approach for evidencebased compilation  which makes use of logical processing  The approach is practical and advantageous in a number of application areasincluding maximum likelihood estimation  sensitivity analysis  and MAP computationsand we provide specific empirical results in the domain of genetic linkage analysis  We also show that the approach is applicable for networks that do not contain determinism  and show that it empirically subsumes the performance of the quickscore algorithm when applied to noisyor networks      INTRODUCTION  It is wellknown that exploiting evidence can make inference in a Bayesian network more tractable  Two of the most common techniques are removing leaf nodes that are not part of the evidence or query  Shachter        and removing edges outgoing from observed nodes  Shachter         These preprocessing steps  which we call classical pruning  can significantly reduce the connectivity of the network  making accessible many queries that would be inaccessible otherwise  Although classical pruning can be very effective  one can identify situations where it does not exploit the full power of evidence  especially when the network contains local structure  The investigation in this paper serves to spotlight the power of evidence by discussing the extent to which it can be exploited computationally  and by introducing the notion of compiling a Bayesian network with evidence  Traditionally  one incurs a compilation cost to prepare for answering a large number of queries over different evidence  amortizing the cost over the queries  But  when evidence is fixed  this benefit may seem illusory at first  We will show  however  that compiling with evidence is often more tractable than compiling without evidence and that it can be very practical  First  the evidence may be fixed on only a subset of the variables  leaving room for posing queries with respect to other variables  this happens in MAP computations   Second  one may be interested in estimating the value of network parameters which will maximize the probability of given evidence  this happens  for example  in genetic linkage analysis   In this case  one may want to use iterative algorithms such as EM or gradient ascent  Dempster et al           which pose many network queries with respect to the given evidence but different network parameter values  A similar application appears in sensitivity analysis  where the goal is to search for some network parameters that satisfy a given constraint  Our approach to compiling with evidence is based on an approach to compiling a network without evidence into an arithmetic circuit  Darwiche        Darwiche         see Figure    The inputs to the circuit correspond to evidence indicators  for recording evidence  and network parameters and the output to the probability of recorded evidence under the given values of parameters  Given evidence  we will then compile an arithmetic circuit that is hardwired for that evidence and  hence  will only be good for computing queries with respect to that evidence  The particular compilation approach we adopt reduces the problem to one of logical inference  which we argue is a natural setting for exploiting the interaction between evidence and network local structure  We apply the compilation approach to genetic linkage analysis  where we provide experimental results showing order of magnitude improvement over state of the art systems for certain benchmarks  We also show that the proposed approach subsumes empirically the quickscore algorithm  Heckerman         This paper is organized as follows  Section   defines      A  B    Compile  a       a     a     a           C              c  a b                   c  a b c  a b             c  a b        b     b          c  c  a b                  b    c    c  a b              b          c           c  a b          c  a b             c  a b        c  a b        c  a b       c  a b        Figure    A Bayesian network and a corresponding AC  a semantics for compiling with evidence and describes areas where it applies  Section   describes our specific approach to compiling with evidence  We illustrate some of the reasons the approach works and apply it to genetic linkage analysis in Section    In Section    we show that the approach empirically subsumes the quickscore algorithm and applies to networks without determinism  Finally  Section   presents some concluding remarks      COMPILING WITH EVIDENCE  This section defines a semantics for compiling a Bayesian network with evidence and explains some areas where such compilation can provide significant advantage       Semantics  We begin by reviewing compilation without evidence as described in  Darwiche        and  Darwiche         We view each network as a multilinear function  MLF   which contains two types of variables  an evidence indicator x for each value x of each variable X and a parameter variable x u for each network parameter  The MLF contains a term for each instantiation of the network variables  and the term is the product of all indicators and parameters that are consistent with the instantiation  For example  consider the network in Figure    where A and B have two values  and C has three values  The corresponding MLF involves twentythree variables and contains twelve terms as follows  a  b  c  a  b  c   a   b  a  b  c  a  b  c   a   b  a  b  c  a  b  c   a   b  a  b  c  a  b  c   a   b  a  b  c  a  b  c   a   b  a  b  c  a  b  c   a   b     a  b  c  a  b  c   a   b      a  b  c  a  b  c   a   b      a  b  c  a  b  c   a   b      a  b  c  a  b  c   a   b      a  b  c  a  b  c   a   b      a  b  c  a  b  c   a   b       To compute the probability of evidence e  we evaluate the MLF after setting indicators that contradict e to   and other indicators to    For example  to compute Pr a    b     we evaluate the above MLF after setting a    b  to   and a    b    c    c    c  to    Setting indicators has the effect of excluding those terms that are incompatible with the evidence  Computing answers to other probabilistic queries  such as posterior marginals on network variables or families  can be obtained from the partial derivatives of the MLF  see  Darwiche        for details  As is clear from the above description  the MLF has an exponential size  Yet  one may be able to factor this function and represent it more compactly  An arithmetic circuit  AC  is a rooted DAG  where each leaf represents a realvalued variable or constant and each internal node represents the product or sum of its children  Figure   depicts an AC  If we can factor the network MLF efficiently using an arithmetic circuit  then inference can be done in time linear in the size of the circuit  since the value and  first  partial derivatives of an arithmetic circuit can all be computed simultaneously in time linear in the circuit size  Darwiche         In an AC representing a network MLF  each leaf represents an indicator or parameter  An effective method of producing an AC is given in  Darwiche         and  Park   Darwiche      b  shows that the AC is a generalization of the jointree   The MLF above and its corresponding AC are capable of answering queries with respect to any evidence  However  if we are willing to commit to specific evidence e  then we can instead work with a much simpler MLF  For evidence e    a    b     the above MLF can be reduced as follows  c  a  b  c   a   b    c  a  b  c   a   b    c  a  b  c   a   b          The AC is a generalization of Jointree in the sense that a Jointree embeds an AC with extra syntactic restrictions  Moreover  the two passes in jointree inference correspond to circuit evaluation and differentiation      a     b       c  a b             c     c  a b        c       c  a b        c     Figure    An AC that incorporates evidence  In general  one obtains the instantiated MLF for a given network and evidence by removing each term from the MLF that contradicts the evidence  An instantiated MLF  and hence its corresponding AC  is therefore capable of answering only queries where e is given  such as Pr e   Pr X e  for network variable X  and Pr e  m  for additional evidence m  Figure   depicts the instantiated AC  which contrasts with the AC in Figure    Although we have lost the ability to apply arbitrary evidence  compiling with evidence can be much more efficient than compiling without  Moreover  the instantiated AC still captures information that is critical to many inference tasks and does so in a way that provides significant advantage to an approach not based on compilation  We provide some examples next       Applications  Genetic linkage analysis can model genetic information for a population of related individuals  a pedigree  using a Bayesian network  Fishelson   Geiger         Some network parameters             n represent recombination factors  and the goal is to search for the recombination factors which maximize the probability of given evidence e  argmax        n Pr e             n    The procedure amounts to ordering genes on a chromosome and determining the distance between them  Typically  one solves this problem by posing Pr e             n   using particular values of recombination factors  parameters   and then repeating multiple times for different values  Our results demonstrate that  on some benchmarks  compilation can significantly improve on superlink       which is a state oftheart system for the task  Sensitivity analysis involves searching for parameter changes that satisfy certain constraints  For example  an expert may decide that Pr A   a   e  must be greater than Pr A   a   e  for some specific evidence e  Our goal is to identify minimal parameter changes that satisfy this constraint  The problem can be solved relatively efficiently for a single parameter change  or multiple parameter changes within the same CPT  Chan   Darwiche         For multiple parameters spread over multiple CPTs  the solution involves numerical    http   bioinfo cs technion ac il superlink   methods that pose multiple probabilistic queries under evidence e  but with different values for network parameters  In this case  compiling the network with given evidence is quite practical  as the work done during compilation can be amortized over the many different queries  MAP is the problem of computing the most likely instantiation of a set of variables M given evidence e  Computing MAP can utilize compilation with evidence in a way that is similar to that of genetic linkage and sensitivity analysis  but instead of adjusting parameters  we adjust indicators  Both exact and approximate algorithms for computing MAP involve obtaining initial evidence e and then repeatedly computing Pr e  m  for different instantiations m of a subset of the MAP variables  Park   Darwiche      a   We can therefore compile an AC with evidence e and then evaluate it for different values associated with indicators of variables M      IMPLEMENTATION  We now describe the technique used in the experimental results to compile a network with evidence into an AC  The approach for compiling a network without evidence into an AC has been described in  Darwiche         and is based on encoding the network MLF into a set of logical clauses  CNF   factoring the CNF  and then extracting an AC from the factored CNF  The details of factoring the CNF and extracting the AC are not critical here  so we will refer the reader to  Darwiche        for details  We will however review how a realvalued MLF can be encoded semantically into a propositional theory  and show how the network MLF can be encoded using a CNF  This is needed for explaining how evidence is exploited during compilation  To illustrate the encoding scheme  consider the MLF f   a   ad   abd   abcd over realvalued variables a  b  c  d  The basic idea is to specify this MLF using a propositional theory that has exactly four models  one for each term in f   Specifically  the propositional theory f   Va   Vb  Vd     Vc  Vb   over Boolean variables Va   Vb   Vc   Vd has exactly four models and encodes f as follows  Model          Va true true true true  Vb false false true true  Vc false false false true  Vd false true true true  encoded term a ad abd abcd  That is  model  encodes term t since  Vj     true iff term t contains the realvalued variable j  The encoding described above is semantic  that is  it describes the theory f which encodes a multilinear   function by describing its models  We specify these theories using a CNF that has one Boolean variable V for each indicator variable   and one Boolean variable V for each parameter variable   For brevity though  we will abuse notation and simply write  and  instead of V and V   CNF clauses fall into three sets  First  for each network variable X with domain x    x            xn   we have  Indicator clauses   x   x          xn xi  xj   for i   j These clauses ensure that exactly one of Xs indicator variables appears in each term of the MLF  The second two sets of clauses correspond to network parameters  In particular  for each parameter xn  x   x       xn    we have  IP clause   x   x          xn  xn  x   x       xn  PI clauses   xn  x   x       xn   xi   for each i The models of this CNF are in onetoone correspondence with the terms of the MLF  In particular  each model of the CNF will correspond to a unique network variable instantiation  and will set to true only those indicator and parameter variables which are compatible with that instantiation  The encoding we use in our experiments is a bit more sophisticated than described above  Chavira   Darwiche         but the above encoding will suffice to make our points below  The encoding as discussed does not include information about evidence  Recall that to incorporate evidence e  we need to exclude MLF terms that contradict e  It is quite easy to do so in the current framework  Consider the network in Figure    its MLF      and the evidence  a    b     Assume that we have generated the CNF  for this network  Our goal becomes excluding from  models corresponding to terms that contradict the evidence  We can easily do so by adding the following unit clauses to the CNF  a  and b    In general  to incorporate evidence x    x           xn   we add unit clauses x    x           xn   Moreover  it is easy to incorporate more general types of evidence  For example  we can incorporate the assertion c  or  a  and b    by including the clauses c   a  and c   b    In our implementation  we simplify the constructed CNF together with evidence by running unit resolution and then removing subsumed clauses  We then invoke our compiler which factors the CNF using a version of the recursive conditioning algorithm  Darwiche         This algorithm makes repeated use of conditioning to decompose the CNF into disconnected CNFs that are compiled independently  Moreover  the algorithm runs unit resolution after each conditioning to simplify the CNF further  This process of decomposition becomes much more effective given the initial  evidence injected into the CNF  which helps to simplify the CNF considerably  Some of the benefit is obtained immediately from the initial preprocessing of the CNF  Other benefits  however  are obtained during the compilation process itself since conditioning sets the value of variables  which together with the injected evidence can lead to even more simplification of the CNFs and  hence  better decomposition  We will see examples of this behavior in the following section      THE POWER OF EVIDENCE  Consider the Original Evidence portion of Table    which contains a set of Bayesian networks corresponding to pedigrees in the domain of genetic linkage analysis  Each network has been classically pruned for specific evidence  yet they still have very connected topologies  as shown by the cluster sizes obtained using a minfill variable ordering heuristic   None of these networks could be compiled without evidence  yet the table lists data on successful compilations for most of these networks once evidence is introduced  despite the large cluster sizes   In particular  the table shows the offline time  which includes preprocessing and compiling   size of AC  and online inference time for computing Pr e   Note that online inference may be repeated for new recombination values  without reincurring the offline cost  Our current implementation uses only unit resolution and removal of subsumed clauses during its simplification of the CNF before compiling  However  based on the amount of determinism in these networks  more advanced logical techniques can be utilized  We therefore augmented the given evidence with some additional evidence learned by the domain specific Lange and Goradia algorithm  Lange   Goradia         It should be noted that this additional evidence can be inferred by standard logical techniques applied to the initial evidence and network determinism  and could therefore be made domain independent  By using this additional  inferred  evidence  we can see in the Learned Evidence portion of Table   that all these networks compile in a reasonable amount of time  and that online inference is faster  Since the additional learned evidence may apply to internal  non leaf  nodes  one may use this evidence to empower classical pruning  Indeed the table lists the adjusted cluster sizes for these networks after having applied classical pruning using the additional evidence  The learned evidence makes many of these networks accessible to classical    We are reporting here normalized cluster sizes  log   of the number of instantiations of a given cluster     Experiments in this paper ran on a    GHz Pentium M processor with  GB of memory    Table    EA results  NET ea  ea  ea  ea  ea  ea  ea  ea  ea  ea   ea    MAX CLUST                                                             Original Evidence OFFLINE AC SEC EDGES                                                                                                                            n a n a n a n a n a n a  Pr e  SEC                                         n a n a n a  MAX CLUST                                                         Learned Evidence OFFLINE AC SEC EDGES                                                                                                                                                                 Table    EE results with full preprocessing  NET ee   ee   ee   ee   ee    MAX CLUST                           OFFLINE SEC                                  AC EDGES                                                      Pr e  SEC                           superlink SEC                                       inference algorithms  but three of the networks still pose problems for classical techniques  It is worth putting the results in perspective by comparing to state of the art results in genetic linkage analysis obtained with superlink  This system uses a combination of variable elimination and conditioning  along with many domain specific preprocessing rules and a sophisticated search for a variable ordering  All superlink timings we report include preprocessing and computing answers to two Pr e  queries  where on difficult networks  the majority of the time is spent doing inference  Until the latest release  the networks in Table   were considered very challenging  with EA   taking over    hours  The newest version of superlink       includes enhancements that preprocess and perform the two Pr e  queries in   seconds on even the most difficult of these networks  If we allow ourselves to use further simplification techniques  which include some simplifications from superlink     and also some rules to detect variable equivalence  we obtain the results shown in the Full Preprocessing portion of Table    Here  offline time takes about    seconds on the hardest network and online inference is very fast  More dramatic are the results reported in Table   on five networks from the challenging superlink     data sets  On these networks  the compilation approach was able to improve on superlinks performance as reported in  Fishelson et al           On four of these networks  offline time is shorter than the superlink time  Once compiled  the generated ACs can repeatedly compute Pr e  extremely efficiently compared to superlink  Because one of the main tasks of genetic linkage analysis is to do maximum likelihood estima   Pr e  SEC                                                         MAX CLUST                                                         Full Preprocessing OFFLINE AC SEC EDGES                                                                                                                                       Pr e  SEC                                                         tion over many iterations  the ability to perform online inference quickly is critical  Note that we can differentiate these circuits and  hence  obtain marginals over families in about      times as long as it takes to evaluate Pr e   This allows us to run the EM algorithm  which requires marginals over families to perform each iteration  When comparing these timings with the time it would take to re run superlink for the same purpose  one sees the significant benefit of compiling with evidence  Suppose for example that we have    parameters we want to estimate and that EM or gradient ascent takes    iterations to converge  For network ee    we would perform     queries in about     seconds using AC  whereas running superlink to compute those values would require days       Examples  We now demonstrate how combining evidence with local structure can make the inference process more efficient  These gains cannot be obtained using classical pruning  although some can be obtained using more sophisticated schemes  e g    Larkin   Dechter          The first example uses the network in Figure    where A and B have two states and C has three states  Let the CPT for the variable C contain all zeros except for the four lines below  A a  a  a  a   B b  b  b  b   C c  c  c  c   P r C A  B                   Suppose we know that C   c    From this information  we can logically infer A   a  and B   b    In fact  this information can be obtained by preprocessing the CNF encoding of the network using unit resolution  The learned evidence can be added to the CNF  or it could be used to empower classical pruning  Now assume that we have evidence  c     Because A and B are binary  there would normally be four possible configurations of A and B  However  given the   CPT parameterization  we can rule out both  a    b    and  a    b     leaving only two configurations  This conclusion can again be obtained by applying unit resolution to our CNF encoding  However  in this case  the inferred information cannot be expressed in terms of classical pruning  Furthermore  it cannot be expressed using a more advanced form of simplification  where variable states known to never occur are removed form the variables domain  since every state of A and B is valid  The learned multi variable evidence is  however  easily written in the language of CNF  and can be utilized in further simplifications during the compilation process  One question is how often situations like the above occur in real networks  The examples actually derive from real networks in the domain of genetic linkage analysis  where variables A and B represent a persons genotype  for example a persons blood type  and C represents the phenotype  the observed portion of the genotype   The example then shows one way the genotype can be mapped to the phenotype  Take the simplified case where there are only two blood types    and    Then the four possible genotype combinations are                     although frequently     and     cannot be differentiated  so there are only three phenotypes  The example models this situation by mapping two configurations of A and B to the same value for C  Furthermore  in this domain  evidence is typically on phenotype variables  which translates to evidence on C in our model  The third example from genetic linkage analysis involves four variables  child C with parents A  B  and S  The variable C in this case is not the phenotype  but the genotype in a child which is inherited from one of the parents genes  A B  based on the value of S  We assume that all four variables are binary and that the portion of the table with S   s  is as follows   S s  s  s  s   A a  a  a  a   B b  b  b  b   C c  c  c  c   P r C A  B                   The point of this example is to illustrate how compilation can utilize evidence even when preprocessing cannot  This type of gain is one of the factors that allows us to successfully compile a network whose treewidth remains high after preprocessing  Compiling repeatedly conditions to decompose the CNF  Let us consider the case where we are given evidence  c     and during compilation  we condition on S   s    Assuming a proper encoding of the network into CNF  combining the evidence with the value for S allows us to infer a      In general  the variables are multivalued  and this discussion also applies in this case   Table    Friends and Smokers Results  DOM SIZE                                MAX CLUST                                         OFFLINE SEC                                                               AC EDGES                                                                     ONLINE SEC                                                         which unit resolution can use to achieve further gains  Conditioning on S   s  yields a similar conclusion for b    In this case  the full power of evidence on C is realized only when combined with conditioning  which takes place during the compilation step  We close this section by quickly examining one more set of networks   Richardson   Domingos        discusses a relational probabilistic model involving an imaginary domain of people and relations on the people including which smoke  which have cancer  and who are friends of whom  There are also logical constraints on the model  such as the constraint that if a persons friend smokes  then the person also smokes  We worked with a slight variation on this model  and each network in Table   represents an instance for a different number of people  For a given network  some nodes represent ground relations and others represent logical constraints  The key point is that  in the absence of evidence  we could only compile the first two networks listed  However  when we commit to evidence asserting that the logical constraints are true  the networks become relatively easy  the hardest requiring     seconds to compile and      seconds for online inference  Online inference involves asserting evidence e on some of the relations and computing Pr e  and marginals on all remaining relations      THE QUICKSCORE ALGORITHM  We illustrate two points in this section  First  our compiling approach empirically subsumes the quickscore algorithm  a dedicated algorithm for twolevel noisy or networks  Second  networks which do not contain determinism  and hence may not look amenable to exploiting evidence as described earlier  can be transformed to make them amenable to these techniques  We start by considering twolevel noisyor networks as given in Figure   a   Here  each di represents a dis    d   d   f        d        f   dn  fm   a  d   d        d   dn  c     c     c     c     c          c  m  cn m  a     a     a     a     a          a  m  an m  f        f   fm   b   Figure     a  A disease feature network   b  the network with determinism introduced   ease  which occurs in a patient with probability pi   and each fj represents a feature  e g   test result   which we may observe to be negative or positive in the patient  We assume a noisyor relationship between a specific feature fj and the diseases di that may causes it  That is  if di is not present  then di will not cause fj   Otherwise  di will cause fj with probability pi j   We wish to compute a marginal for each disease given evidence on features  Standard inference has a worst case time complexity that is exponential in the number n of diseases  However   Heckerman        showed that computing such marginals can be done in time exponential only in the size m  of the set F   of features known to be positive  The argument makes several appeals to the independence relationships created by the noisyor assumptions and by the network structure  It culminates with the definition of the quickscore algorithm  which iterates over the power set of F   and computes   a marginal on a single disease in time  nm  m    where m is the number of negative findings  It is wellknown that the semantics of the noisyor relationships allows us to transform the network in Figure   a  into the network in Figure   b   Here  each edge from di to fj in the original network is replaced with two nodes  ci j and ai j   and three edges  Each introduced ci j is a binary root such that Pr ci j     pi j   and each introduced ai j represents whether disease di causes feature fj   Therefore  ai j represents a conjunction of di and ci j   and each feature is a disjunction of its parents  This disjunction can be represented very compactly in CNF  even when there are a large number of parents  Although the network in Figure   a  typically does not possess determinism  the transformed network possesses an abundance in  the form of introduced conjunctions and disjunctions  leading one to wonder whether combining this determinism with evidence in the manner proposed would duplicate quickscores performance  To test this hypothesis  we chose different values for m    and for each  we constructed ten experiments  each designed to be similar to the experiments on the proprietary network used to demonstrate quickscore  For each experiment  we generated a random network containing     diseases and      features  For each feature  we chose    possible causes uniformly from the set of diseases  We then chose each pi and each pi j uniformly from the open interval         In addition  we generated evidence by setting to positive m  features chosen uniformly from the set of features and setting the remaining features to negative   In this way  the experiment utilizes its own randomly generated network and its own randomly generated evidence  Finally  we compiled and evaluated the network with the evidence  yielding a marginal over each disease  Each of the experiments produced a network for which minfill computed a maximum cluster size between     and      Because the set of evidence variables is the same as the set of leaves in the network  classical pruning would have no effect on this cluster size  For each value of m    Table   shows results  averaged over the ten experiments  The most important observation is that the approach to compiling with evidence empirically subsumes quickscore  Indeed  quickscore is exponential in m  even in the best case  whereas compiling was sometimes fast  even for large m    For example  the minimum compile times for m       and m       were   s and    s  respectively  Furthermore  quickscore computes a marginal only for a single disease  whereas the described method computes marginals over all     diseases simultaneously  The transformation to introduce determinism applies not only to the types of networks on which quickscore runs  but to any network involving noisyor relationships  There are similar transformations for other types of local structure  including noisyor with leak  noisymax  Dez   Galan         and contextspecific independence  Boutilier et al           Consider a final example involving a family containing binary variable C with binary parents A and B  Suppose that given A   a    C becomes independent of B  yet this is not true for A   a    In this case  we introduce auxiliary variable S with three states between A B and C  Ss CPT is deterministic and sets S to s  when A   a    to s  for parent state  a    b     and to s  for parent state  a    b     Moreover  the CPT for C becomes  We could have left some features F   out of the evidence  In this case  classical pruning would suffice to remove nodes F   from the network      Table    Averaged diagnosis results  TRUE FEATURES                                     S s  s  s   OFFLINE SEC                                                                                      AC EDGES                                                                                                         ONLINE SEC                                                                   P r c   S  P r c   a    b      P r c   a    b    P r c   a    b    P r c   a    b     Given evidence A   a    our logicbased strategy can infer both the value of S and the independence of C from B  This technique allows for more efficient decomposition during the compilation process  even though the original network contains no determinism      CONCLUSION  We discussed the exploitation of evidence in probabilistic inference and highlighted the extent to which it can render inference tractable  We proposed a particular notion and approach for compiling networks with evidence  and discussed a number of practical applications to maximum likelihood estimation  sensitivity analysis and MAP computations  We presented several empirical results illustrating the power of proposed approach  and showed in particular how it empirically appears to subsume the performance of the quickscore algorithm   
 We formulate in this paper the mini bucket algorithm for approximate inference in terms of exact inference on an approximate model produced by splitting nodes in a Bayesian network  The new formulation leads to a number of theoretical and practical implications  First  we show that branchand bound search algorithms that use minibucket bounds may operate in a drastically reduced search space  Second  we show that the proposed formulation inspires new minibucket heuristics and allows us to analyze existing heuristics from a new perspective  Finally  we show that this new formulation allows mini bucket approximations to benefit from recent advances in exact inference  allowing one to significantly increase the reach of these approximations      INTRODUCTION  Probabilistic reasoning tasks in Bayesian networks are typically NPhard  and approximation algorithms are often sought to address this apparent intractability  One approach to approximate inference is based on mini buckets  a scheme that has been successfully employed by branch and bound algorithms for computing MPEs  Most Probable Explanations   Dechter   Rish        Marinescu  Kask    Dechter         Roughly speaking  mini buckets is a greedy approach to approximate inference that applies the variable elimination algorithm to a problem  but only as long as computational resources allow it  When time and space constraints keep us from progressing  a mini buckets approach will heuristically ignore certain problem dependencies  permitting the process of variable elimination to continue  Zhang   Poole        Dechter         Mini buckets will therefore give rise to a family  of approximations that  in particular  are guaranteed to produce upper bounds on the value we seek  and further whose quality depends on the heuristic used to ignore dependencies  In this paper  we make explicit in the most fundamental terms the dependencies that mini bucket approximations ignore  In particular  we reformulate the mini bucket approximation using exact inference on an approximate model  produced by removing dependencies from the original model  We refer to this process of removing dependencies as node splitting  and show that any mini bucket heuristic can be formulated as a node splitting heuristic  This perspective on mini buckets has a number of implications  both theoretical and practical  First  it shows how one can significantly reduce the search space of brand and bound algorithms that make use of mini bucket approximations for generating upper bounds  Second  it provides a new basis for designing mini bucket heuristics  a process which is now reduced to specifying an approximate model that results from node splitting  We will indeed propose a new heuristic and compare it to an existing heuristic  which we reformulate in terms of node splitting  Third  it allows one to embed the mini bucket approximation in the context of any exact inference algorithmfor example  ones that exploits local structure  Chavira   Darwiche       which could speed up the process of generating mini bucket bounds  without affecting the quality of the approximation  We will illustrate this ability in some of the experiments we present later  This paper is organized as follows  In Section    we review the MPE task  as well as algorithms for finding MPEs  In Section    we define node splitting operations for Bayesian networks  and show in Section   how mini bucket elimination is subsumed by splitting nodes  In Section    we examine mini buckets as a node splitting strategy  and introduce a new strategy based on jointrees  In Section    we consider branchand bound search for finding MPEs  and show how       CHOI ET AL   we can exploit node splitting to improve the efficiency of search  In Section    we provide empirical support for the claims in Section    and conclude in Section    Proofs and other results appear in the Appendix      MOST PROBABLE EXPLANATION  We will ground our discussions in this paper using the problem of computing MPEs  which we define formally next  Let N be a Bayesian network with variables X  inducing distribution Pr   The most probable explanation  MPE  for evidence e is then defined as  MPE  N  e   def     arg max Pr  x   xe  where x  e means that instantiations x and e are compatible  they agree on every common variable  Note that the MPE solution may not be unique  in which case MPE  N  e  denotes a set of MPEs  One can also define the MPE probability  MPE p  N  e   def     max Pr  x   xe  A number of approaches have been proposed to tackle the MPE problem  when a Bayesian network has a high treewidth  These include methods based on local search  Park        Hutter  Hoos    Stutzle        and max product belief propagation  e g   Pearl        Weiss         including generalizations  e g   Yedidia  Freeman    Weiss        Dechter  Kask    Mateescu        and related methods  Wainwright  Jaakkola    Willsky        Kolmogorov   Wainwright          Although these approaches have been successful themselves  and can provide high quality approximations  they are in general non optimal  An approach based on systematic search can be used to identify provably optimal MPE solutions  although the efficiency of a search depends heavily on the problem formulation as well as the accompanying heuristics  In particular  it is quite common also to use branchand bound search algorithms for computing MPEs and their probability  e g   Marinescu et al         Marinescu   Dechter         The use of these search algorithms  however  requires the computation of an upper bound on the MPE probability to help in pruning the search space  The mini buckets method is the state of the art for computing such bounds  Dechter   Rish         In fact  the success of mini buckets is most apparent in this context of computing MPEs  which is the reason we will use this application to drive our theoretical analysis and empirical results   X  X  Figure    When we split a variable X  left   we create a clone X that inherits some of the children  right       SPLITTING NODES  We will define in this section a method for approximating Bayesian networks by splitting nodes  An operation that creates a clone X of some node X  where the clone inherits some of the children of X  see Figure    Definition   Let X be a node in a Bayesian network N with children Y  We say that node X is split according to children Z  Y when it results in a network that is obtained from N as follows   The edges outgoing from node X to its children Z are removed   A new root node X with a uniform prior is added to the network with nodes Z as its children  A special case of node splitting is edge deletion  where a node is split according to a single child  i e   splitting also generalizes edge deletion as defined in Choi   Darwiche      a      b   Definition   Let X  Y be an edge in a Bayesian network N   We say that node X is split along an edge X  Y when the node X is split according to child Y   The following case of node splitting will be the basis of a splitting strategy that yields a special class of minibucket approximations with implications in search  Definition   Let X be a node in a Bayesian network N   We say that node X is fully split when X is split along every outgoing edge X  Y   Thus  when we fully split a node X  we create one clone for each of its outgoing edges  Figure   illustrates an example of a network where two nodes have been split  Node C has been split according to children  D  E   and Node A has been split along the edge A  D  A network N   which results from splitting nodes in network N has some interesting properties  To explicate these properties  however  we need to introduce a function which  given an instantiation x of variables in network N   gives us an instantiation of clones in N   that agrees with the values given to variables in x    CHOI ET AL       Algorithm   ve N  e   returns MPE p  N  e      i       S   f e   f e is a CPT  incorporating e  of N      while S contains variables do    ii      X  a variable appearing in S    Si  all factors Y in S that contain X    fi  max f X  Figure    A Bayesian network N  left  and an approximation N    right  found by splitting C according to  D  E   and splitting A according to D  Definition   Let N be a Bayesian network  and let N   be the result of splitting nodes in N   If x is an  instantiation of variables in N   then let  x be the compatible instantiation of the corresponding clones in N     For example  in the split network in Figure    an instantiation x    A   a    B   b    C   c    D   d    E   e     is compatible with instantiation  x    A   a    C   c     Moreover  x is not compatible with  A   a    C   c     To see the effect that splitting a node can have on a network  consider a simple two node network A  B with binary variables  where a        b   a        and b   a        After splitting A according to B  we have  x a  b   a  b   a  b   a  b       Pr  x                       x a  a  b  a  a  b  a  a  b  a  a  b  a  a  b  a  a  b  a  a  b  a  a  b            Pr  x                                            x a  a  b  a  a  b  a  a  b  a  a  b  a  a  b  a  a  b  a  a  b  a  a  b         Pr  x                                            where     A         We see that whenever A  and its clone A  are set to the same value  we can recover the original probabilities Pr  x  after splitting  by using Pr    x     This includes the value of the MPE in N   which may no longer be the largest value of Pr    x     This intuition yields the key property of split networks  Theorem   Let N be a Bayesian network  and let N   be the result of splitting nodes in N   We then have  MPE  N  e   MPE  N     e   e    Q  Here     network N      p  CC  p   C   where C is the set of clones in  That is  the MPE probability with respect to a split network provides an upper bound on the MPE probability with respect to the original network  We note that the probability of evidence is also upper bounded in the split network  see Theorem   in the Appendix   f Si     S  S  Si   fi      return product of factors in S  Algorithm   mbe N  e   returns an upper bound on MPE p  N  e     Identical to Algorithm    except for Line         Si  some factors in S that contain X  The following corollary shows that splitting degrades the quality of approximations monotonically  Corollary   Let network N  be obtained by splitting nodes in network N    which is obtained by splitting nodes in network N    We then have MPE p  N    e       MPE p  N    e   e        MPE  N   e  e       p          where       and  e     e  are as defined by Theorem        MINI BUCKET ELIMINATION  We discuss in this section the relationship between the approximations returned by split networks and those computed by the mini buckets algorithms  Dechter   Rish         In particular  we show that every minibuckets heuristic corresponds precisely to a node splitting strategy  where exact inference on the resulting split network yields the approximations computed by mini buckets  Our discussion here will be restricted to computing MPEs  yet the correspondence extends to probability of evidence as well  We start first by a review of the mini buckets method  which is a relaxed version of the variable elimination method given in Algorithm    Zhang   Poole        Dechter         According to this algorithm  variable elimination starts with a set of factors corresponding to the CPTs of a given Bayesian network  It then iterates over the variables appearing in factors  eliminating them one at a time  In particular  to eliminate a variable X  the method multiplies all factors that contain X and then max out X from the result  The bottleneck of this algorithm is the step where the factors containing X are multiplied  as the resulting       CHOI ET AL                      Figure    An execution trace of ve on N  left  and mbe on N  right   The network is defined in Figure     factor may be too big for the computational resources available  The mini bucket method deals with this difficulty by making a simple change to the variable elimination algorithm  also known as the bucket elimination algorithm    This change concerns Line   in which all factors containing variable X are selected  In minibuckets  given in Algorithm    Dechter   Rish         one chooses only a subset of these factors in order to control the size of their product  Which particular set of factors is chosen depends on the specific heuristic used  Yet  regardless of the heuristic used  the answer obtained by the mini buckets method is guaranteed to be an upper bound on the correct answer   One should note here that the simple change from all to some on Line   implies the following  The number of iterations performed by Algorithm   is exactly the number of network variables  since each iteration will eliminate a network variable  However  Algorithm   may only partially eliminate a variable in a given iteration  and may take multiple iterations to eliminate it completely  To help us visualize the computations performed by Algorithms   and    consider their execution trace  Definition   Given an instance of ve or mbe run on a given network N   we define its execution trace T as a labeled DAG which adds  for each iteration i   a node i  labeled by the factor set Si   and  directed edges j  i  for all factors fj  Si   each labeled by the corresponding factor fj      More precisely  bucket elimination is a particular implementation of variable elimination in which one uses a list of buckets to manage the set of factors during the elimination process  Although the use of such buckets is important for the complexity of the algorithm  we ignore them here as the use of buckets is orthogonal to our discussion    This is also true for versions of the algorithm that compute the probability of evidence   Figure   depicts traces of both algorithms on the network in Figure    left   Variable elimination  whose trace is shown on the left  eliminates variables from A to E  and performs five iterations corresponding to the network variables  Mini buckets  however  performs seven iterations in this case  as it takes two iterations to eliminate variable A and two iterations to eliminate variable C  Note that an execution trace becomes a rooted tree after reversing the direction of all edges  Given an execution trace T   we can visually identify all of the network CPTs used to construct any factor in Algorithms   and    For mini buckets  we also want to identify a subtrace of T   but one that covers only those network CPTs that are relevant to a particular attempt at eliminating variable X at iteration i  A CPT is not relevant to iteration i if X is eliminated from it in a later iteration  or if X has already been eliminated from it in some previous iteration  Given a trace T   we thus define the subtrace Ti relevant to an iteration i as the nodes and edges of T that are reachable from node i  including itself   but only by walking up edges j  i  and only those edges labeled with factors fj mentioning variable X  For example  in Figure    right   the subtrace Ti for iteration i     is the chain          In the same trace  the subtrace Ti for iteration i     is the chain          Given a subtrace Ti   we can identify only those CPTs that are relevant to a partial elimination of X  but further  the set of variables those CPTs belong to  Definition   Let i be an iteration of mbe where we eliminate variable X  and let Ti be the subtrace of T that is relevant to iteration i  The basis B of an iteration i is a set of variables where Y  B iff   Y  U  Sj for some node j of Ti   and  X   Y    U  where Y  U are CPTs in N   For example  in Figure    right   the basis of iteration i     is  D  E   since C is eliminated from the CPTs of D and E at iteration    Given this notion  we can show how to construct a network with split nodes  that corresponds to a particular execution of the mini bucket method  In particular  exact variable elimination in N   will be able to mimic mini bucket elimination in N   with the same computational complexity  This is given in Algorithm   which returns both a network N   and an ordering    of the variables in N    this includes the variables in original network N and their clones in N      Figure   shows a trace corresponding to a split network  and the associated variable order    CHOI ET AL  Algorithm   split mbe N  e   returns a split network N   and variable ordering      corresponding to a run of mbe N  e      N    N    for each iteration i of mbe N  e  do    X  as chosen on Line   of mbe    Si  as chosen on Line   of mbe    B  basis of iteration i    if X  B then        i   X    else    split node X in N   according to children B         i   clone X of X resulting from split     return network N   and ordering                       NODE SPLITTING STRATEGIES  Given the correspondences in the previous section  every mini bucket heuristic can now be interpreted as a node splitting strategy  Consider for example the mini bucket heuristic given in  Dechter   Rish         which is a greedy strategy for bounding the size of the factors created by mbe  This heuristic works as follows  given a bound on the size of the largest factor   A particular variable order is chosen and followed by the heuristic   When processing variable X  the heuristic will pick a maximal set of factors Si whose product will be a factor of size within the given bound   The above process is repeated in consecutive iterations and for the same variable X until variable X is eliminated from all factors   Once X is completely eliminated  the heuristic picks up the next variable in the order and the process continues         Figure    An execution trace of mbe on N  left  and ve on N    right   For simplicity  we ignore the priors of clone variables in N     Networks are defined in Figure     We now have our basic correspondence between minibuckets and node splitting  Theorem   Let N be a Bayesian network  e be some evidence  and let N   and    be the results of split mbe N  e   We then have   mbe N  e    MPE  N     e   e    Q  p  where    CC  C  and C are the clone variables in N     Moreover  variable elimination on network N   using the variable order    has the same time and space complexity of the corresponding run mbe N  e   Note that the ordering    returned by Algorithm   may not be the most efficient ordering to use when running exact variable elimination in a split network  there  may be another variable order where ve N     e   e   produces smaller intermediate factors than mbe N  e   Indeed  we need not restrict ourselves to variable elimination when performing inference on the split network  as any exact algorithm suffices for this purpose  This property can have significant practical implications  a point we highlight in Section   where we exploit recent advances in exact inference algorithms   This heuristic tries then to minimize the number of instances where a proper subset of factors is selected in Line   of Algorithm    and can be interpreted as a heuristic to minimize the number of clones introduced into an approximation N     In particular  the heuristic does not try to minimize the number of split variables  We now introduce a new node splitting strategy based on fully splitting nodes  where a variable is split along every outgoing edge  The strategy is also a greedy algorithm  which attempts to fully split the variable that contributes most to the difficulty of running a jointree algorithm in the approximate network N     This process is repeated until the network is sufficiently simplified  In particular  the method starts by building a jointree of the original network  It then picks a variable whose removal from the jointree will introduce the largest reduction in the sizes of the cluster and separator tables  Once a variable is chosen  it is fully split  One can obtain a jointree for the split network by simply modifying the existing jointree  which can then be used to choose the next variable to split on   In our empirical evaluation  we go further and construct a new jointree for the simpler network  and choose the next variable to split from it  This process is repeated until the largest jointree cluster is within our bound  We now have two strategies for splitting nodes in a network  The first is based on the classical mini bucket heuristic that tries to minimize the number of clones    In particular  one can simply adjust the separators and clusters without changing the structure of the jointree        CHOI ET AL   Algorithm   split bnb  z and q   are global variables      q  MPE p  N     z   z       if q   q then    if z is a complete instantiation then    q   q    else    pick some X   Z    for each value x of variable X do    z  z   X   x     split bnb       z  z   X   x   and the second one is based on reducing the size of jointree tables and tries to minimize the number of split variables  Recall that Corollary   tells us that the quality of the MPE bound given by a split network degrades monotonically with further splits  As we shall see in Section    and empirically in Section    it may sometimes be more important to minimize the number of split variables  rather than the number of clones  in the context of branch and bound search      SEARCHING FOR MPES  When computing the MPE is too difficult for traditional inference algorithms  we can employ systematic search methods to identify provably optimal solutions  Suppose now that we are given network N and evidence e  and that we want to compute MPE p  N  e  using depth first branch and bound search  We want then to select some network N   using a node splitting heuristic from the previous section to allow for exact inference in N    say  by the jointree algorithm   Theorem   gives us the upper bound  MPE p  N  e   MPE p  N     e   e    Moreover  one can easily show that if z is a complete variable instantiation x of N   we then have  MPE p  N  x    MPE p  N     x   x    see Lemma    These two properties form the basis of our proposed search algorithm  split bnb  which is summarized in Algorithm    Throughout the search  we keep track of two global variables  First  z is a partial assignment of variables in the original network that may be extended to produce an MPE solution in MPE  N  e   Second  q   is a lower bound on the MPE probability that is the largest probability of a complete instantiation so far encountered  The search is initiated after setting z to e and q   to      we use evidence e as the base instantiation   and     as a trivial lower bound  Upon completion of the search  we have the optimal MPE probability q     MPE p  N  e   At each search node  we compute a bound on the best completion of z by performing exact inference in the approximate network N     If the resulting upper bound q is greater than the current lower bound q     then we must continue the search  since it is possible that z can provide us with a better solution than what we have already found  In this case  if z is already a complete instantiation  it is easy to show that q is equal to Pr  z   by Lemma    in the Appendix  and that we have found a new best candidate solution q     If z is not a complete instantiation  we select some variable X that has not been instantiated  For each value x of X  we add the assignment  X   x  to z and call split bnb recursively with the new value of z and our candidate solution q     Upon returning from the recursive call  we retract the assignment  X   x   and continue to the next value of X       REDUCING THE SEARCH SPACE  Consider now the following critical observation  Proposition   Let N be a Bayesian network  and let N   be the result of splitting nodes in N   If Z contains all variables that were split in N to produce N     then  MPE p  N  z    MPE p  N     z   z    where     Q  CC   C  and C are all the clones in N      According to this proposition  once we have instantiated in z all variables that were cloned  the resulting approximation is exact  This tells us that during our search  we need not instantiate every one of our network variables X  We need only instantiate variables in a smaller set of variables Z  X containing precisely the variables that were split in N to produce N     Once the bound on the MPE probability becomes exact  we know that we will not find a better solution by instantiating further variables  so we can stop and backtrack  This observation allows us to work in a reduced search space  rather than searching in a space whose size is exponential in the number of network variables X  we search in a space whose size is exponential only in the number of split variables  Moreover  if our variable splitting strategy seeks to minimize the number of split variables  rather than the number of clones introduced  we can potentially realize dramatic reductions in the size of the resulting search space  As we shall see in the following section  this can have a drastic effect on the efficiency of search    CHOI ET AL   In our experiments  we compared the splitting strategy based on a jointree  JT  with the strategy based on a greedy mini bucket elimination  MB   both described in Section    In particular  we asserted limits on the maximum cluster size for JT  and equivalently  the size of the largest factor for MB  We then compared the two strategies across a range of cluster and factor size limits from   to     where   corresponds to a fully disconnected network and    corresponds to exact inference  no splits   In all of our experiments  to emphasize the difference between splitting strategies  we make neutral decisions in the choice of a search seed  we use a trivial seed        variable ordering  random  and value ordering  as defined by the model   First  consider Figure    which compares the effectiveness of node splitting strategies in minimizing the number of variables split and the number of clones  Recall that the heuristic based on jointrees  JT  seeks to minimize the number of split variables  while the greedy mini bucket  MB  strategy would seek to minimize the number of clones  We see that in Figure    on    In particular  each network is associated with its own piece of evidence corresponding to a codeword received via transmission through a  simulated  noisy Gaussian channel  with standard deviations ranging from        to        in steps of       JT MB                JT MB                     log  max cluster size        log  max cluster size   Figure    Comparing splitting heuristics               JT MB                   log  max cluster size     search nodes  We begin with experiments on networks for decoding error correcting codes  see  e g   Frey   MacKay        Rish  Kask    Dechter         We first consider simpler networks  that correspond to codes containing    information bits and    redundant bits  Each of our plot points is an average of    randomly generated networks    networks for each of   levels of noise   Here  an MPE solution would recover the most likely word encoded prior to transmission  Our method for exact inference in the approximate model is based on compiling Bayesian networks  Chavira   Darwiche         an approach that has already been demonstrated to be effective in branch and bound search for MAP explanations  Huang  Chavira    Darwiche            of vars split  We present empirical results in this section to highlight the trade offs in the efficiency of search based on the quality of the bound resulting from different node splitting strategies  and the size of the resulting search space  We further illustrate how our framework allows for significant practical gains with relatively little effort  by employing state of the art algorithms for exact inference in the approximate  node split network  Thus  our goal here is  not to evaluate a completely specified system for MPE search  but to illustrate the benefits that our node splitting perspective can bring to existing systems         of clones created  EMPIRICAL OBSERVATIONS  search space size                               log  max cluster size     Figure    Evaluating the efficiency of search  On the right  the top pair searches in the full space  and the bottom pair searches in the reduced space   the left  our jointree  JT  method can split nearly half of the variables that the mini bucket  MB  strategy splits  On the other hand  we see that on the right  the mini bucket  MB  strategy is introducing fewer clones  Note that on both extremes  no splits and all split   MB and JT are identical  To see the impact that reducing the number of split variables has on the efficiency of search  consider Figure    On the left  we see that JT can get an order of magnitude savings over MB in the size of the reduced search space  which is exponential only in the number of split variables  see again Figure     Consider now  on the right  the number of nodes visited while performing split bnb search  The top pair plots the efficiency of search using the full search space  JTF and MB F   while the bottom pair plots the efficiency of using the reduced search space  JT R and MB R   We see that both JT R and MB R experience several orders of magnitude improvement when using the reduced search space versus the full search space  When we compare JT F and MB F  top pair   we see that MB F is in fact more efficient in terms of the number of nodes visited  In this setting  where both methods are searching in the same space  we see that the number of clones introduced appears to be the dominant factor in the efficiency of search  This is expected  as we expect that the upper bounds on the MPE probability should be tighter when fewer clones are introduced  When we now compare JT R and MB R  bottom pair   we see that the situation has       CHOI ET AL  JT MB    of clones created    of vars split                     log  max cluster size   JT MB                       log  max cluster size        JT MB  search nodes  search space size  Figure    Comparing splitting heuristics                                 log  max cluster size            log  max cluster size   Figure    Evaluating the efficiency of search   Table    Compilation versus Variable Elimination Network                                                                                   Search Nodes                                                        AC Time  s                            VE Time  s                                                   Imp                                           reversed  and that JT R is now outperforming MB R  Here  each method is performing search in their own reduced search spaces  A strategy based on reducing the number of split variables reduces the size of the search space  and this reduction now dominates the quality of the bound  Figures   and   depict similar results but for larger coding networks  in which we have a rate    code with    information bits and    redundant bits  Note that only the reduced space was used for search here  Our approach based on node splitting has another major advantage  which we have only briefly mentioned thus far  By formulating mini buckets as exact inference in an approximate network  the evaluation of the mini bucket approximation need not rely on any specific exact inference algorithm  We mention here that  the arithmetic circuit  AC  approach we have been using to compute the bound indeed has a key advantage over mainstream algorithms  in that it is able to effectively exploit certain types of local structure  Chavira   Darwiche         To highlight the extent to which using a different algorithm can be significant  we constructed another set of experiments  In each  we used a different grid network  first introduced in  Sang  Beame    Kautz         and constructed a single MPE query  Each grid network has treewidth in the low thirties  just out of reach for traditional algorithms for exact inference  We ran our search twice  each time using a different algorithm to compute the mini bucket bound  the first using AC and the second using standard variable elimination  that does not exploit local structure   Table   shows the results for each network  including the number of search nodes visited and  for each algorithm  the total search time  For each network  we performed two identical searches for each algorithm  the only difference being in how the bound was computed  Consequently  the dramatic differences we observe reflect the ability of the AC approach to exploit local structure  showing how advances in exact inference can be easily utilized to extend the reach of mini bucket approximations      CONCLUSION  We presented in this paper a new perspective on minibucket approximations  formulating it in terms of exact inference in an approximate network  but one found by splitting nodes  This perspective has led to a number of theoretical and practical insights  For one  it becomes apparent that a branch and bound search using a mini bucket bound may operate in a drastically reduced search space  This suggests a heuristic for identifying a mini bucket approximation that is explicitly based on minimizing this search space  rather than the quality of the resulting bound  Empirically  we observe that a reduced search space can have more impact than a better bound  in terms of the efficiency of branch and bound search  Moreover  as our approach is independent of the algorithm used for exact inference in the resulting approximate network  we can effortlessly employ state of the art algorithms for exact inference  including those that can exploit compilation and local structure   A  PROOFS  Lemma   Let N be a Bayesian network  and let N   be the result of splitting nodes in N   We then have  Pr  x    Pr    x   x      CHOI ET AL  Q Here    CC  C   where C is the set of clones in network N      Proof of Lemma   Note first that  x is an instantiation of only root variables  and that all clones have uniform priors  i e   c    C     We then have that Y Y  Pr     x    c    C           c x  CC   Since instantiation x is compatible with  x   where a variable and its clones are set to the same value  we  find in Pr    x    x   that clone variables act as selectors for the CPT values composing Pr  x   Thus    Pr  x   x     Pr    x    x  Pr     x     Pr  x        and we have Pr  x    Pr    x   x    as desired      Proof of Theorem   Suppose for contradiction that there exists an instantiation z  MPE  N  e  such that  Pr  z    MPE p  N     e   e    By Lemma    the instan  tiation z gives us   Pr  z    Pr    z   z     MPE p  N     e   e     contradicting the optimality of MPE p  N     e   e        Proposition   is in fact a generalization of Lemma   from a complete instantiation x to a partial instantiation z where Z contains all nodes that have been split in N     Note that splitting a node X when the value of X has already been fixed corresponds to a common preprocessing rule for Bayesian networks given evidence  In particular  when a given piece of evidence z fixes the value of variable Z  any edge Z  Y can be pruned and a selector node Z can be made a parent of Y   Node Z is then set to the value that instantiation z assigns to Z  This pruning process yields a simpler network which corresponds exactly to the original network for any query of the form    z   Proof of Proposition   From the correspondence to pruning edges outgoing instantiated variables  we know that queries of the form    z   including complete instantiations  x  z   are equivalent in N condi tioned on z and N   conditioned on  z   z    Thus the MPEs of each network must also be the same    Proof of Theorem   Given the trace of an instance of mbe N  e   algorithm split mbe N  e  returns a network N   and an ordering    of variables in N     We show  by induction  that each iteration of ve on N   mimics each iteration of mbe on N   We can then conclude that the product of factors returned by both must be the same  and further  that they are of the same time and space complexity  In particular        we show how ve N  e   e   mimics mbe N  e  first on Line    and then Lines      and    in Algorithms   and    For simplicity  we ignore the constant factor  that the clone CPTs contribute to the MPE value of N     On Line    iteration i       by construction  the CPTs in N are the same as the CPTs in N     after relabeling  For iterations i      assume for induction that the factors available to both ve and mbe are the same  On Line    if mbe picked variable X on Line    then algorithm ve picks variable X         i   which is either X or a clone X  by construction  Lines   and    of Algorithm     On Line    each factor in the set Si is either    a CPT mentioning X  or    a factor that is composed of a CPT mentioning X  The variables that these CPTs belong to are the variable set B  the basis of iteration i  Algorithm   decides to split  or not split   so that each variable in B will have a CPT in N   that mentions X         i   We know by induction  that all factors f selected by mbe are available for selection by ve in N     Since Algorithm   ensures that each of these factors f now mention X     and since ve picks all factors mentioning X     we know ve picks the same factors mbe picked  On Line    consider any variable Z mentioned in Si   Let j  i be the iteration where Z is eliminated in mbe  The relevant CPTs mentioning Z at iteration i are among the relevant CPTs of the basis at iteration j  Thus  Algorithm   ensures that they all mention the same instance of Z in N     Thus  the resulting product of factors fi must be the same after relabeling    A node split network also upper bounds Pr  e   The following theorem corresponds to a mini bucket bound on the probability of evidence  Dechter   Rish         Theorem   Let N be a Bayesian network  and let N   be the result of splitting nodes in N   We then have  Pr  e   Pr    e   e    Q Here     CC  C   where C is the set of clones in network N     Proof of Theorem   By Lemma    we know that  Pr  x    Pr    x   x    Therefore  X X  Pr  e    Pr  x     Pr    x   x  xe    X  xe   Pr  x     Pr    e   e         x  e  e  where x  is an instantiation of variables in N     but where the values of the original network variables are not necessarily compatible with the values of the clone  variables  as they are in x and  x            CHOI ET AL   B  LOOP CUTSET CONDITIONING  The loop cutset conditioning algorithm and split bnb search are closely related when our splitting strategy performs only full splits  see Definition     This correspondence reveals the difficulty of answering the following decision problem  D FS  Given k and   does there exist a set Z of size  k such that fully splitting nodes Z in network N results in an approximate network N   with treewidth    We now state the following negative result  Theorem   Decision problem D FS is NPcomplete  Hardness can be shown by reduction from the loopcutset problem  which is NPcomplete  Suermondt   Cooper         In particular  when we fully split enough variables Z to render N   a polytree  then Z also constitutes a loop cutset of N      If N is rendered a polytree  and we ignore the bound during split bnb search and further employ the reduced search space over split variables Z  then splitbnb reduces to loop cutset conditioning  More generally  when we split enough variables Z so that network N   has treewidth   split bnb reduces to cutset conditioning  Bidyuk   Dechter           Assuming that for exact inference in N   we use an algorithm that is exponential in the treewidth  of N     this correspondence tells us that the worst case time and space complexity of split bnb search is precisely that of cutset conditioning  In particular  say that n is the number of variables in N   value m is the number of variables cloned in N     and value  is the treewidth of network N     The worst case time complexity of split bnb search is thus O n exp    exp m     O n exp    m    since we spend O n exp    time at each of at most exp m  search nodes  Note that the space complexity of split bnb search is only O n exp     m    Choi  A     Darwiche  A       b   A variational approach for approximating Bayesian networks by edge deletion  In UAI  pp        Dechter  R          Bucket elimination  A unifying framework for probabilistic inference  In UAI  pp          Dechter  R   Kask  K     Mateescu  R          Iterative join graph propagation  In UAI  pp          Dechter  R     Rish  I          Mini buckets  A general scheme for bounded inference  J  ACM                   Frey  B  J     MacKay  D  J  C          A revolution  Belief propagation in graphs with cycles  In NIPS  pp          Huang  J   Chavira  M     Darwiche  A          Solving map exactly by searching on compiled arithmetic circuits  In AAAI  pp          Hutter  F   Hoos  H  H     Stutzle  T          Efficient stochastic local search for MPE solving  In IJCAI  pp          Kolmogorov  V     Wainwright   M  J          On the optimality of tree reweighted max product message passing  In UAI  Marinescu  R     Dechter  R          AND OR branchand bound for graphical models  In IJCAI  pp           Marinescu  R   Kask  K     Dechter  R          Systematic vs  non systematic algorithms for solving the MPE task  In UAI  pp          Park  J  D          Using weighted max sat engines to solve MPE  In AAAI IAAI  pp          Pearl  J          Probabilistic Reasoning in Intelligent Systems  Networks of Plausible Inference  Morgan Kaufmann Publishers  Inc   San Mateo  California  Rish  I   Kask  K     Dechter  R          Empirical evaluation of approximation algorithms for probabilistic decoding  In UAI  pp          Sang  T   Beame  P     Kautz  H          Solving Bayesian networks by weighted model counting  In AAAI  Vol     pp          AAAI Press  Suermondt  H  J     Cooper  G  F          Probabilistic inference in multiply connected networks using loop cutsets  IJAR             Wainwright  M  J   Jaakkola  T     Willsky  A  S          Map estimation via agreement on trees  messagepassing and linear programming  IEEE Transactions on Information Theory                      
