 Most causal discovery algorithms in the literature exploit an assumption usually referred to as the Causal Faithfulness or Stability Condition  In this paper  we highlight two components of the condition used in constraint based algorithms  which we call Adjacency Faithfulness and OrientationFaithfulness  We point out that assuming Adjacency Faithfulness is true  it is possible to test the validity of OrientationFaithfulness  Motivated by this observation  we explore the consequence of making only the Adjacency Faithfulness assumption  We show that the familiar PC algorithm has to be modified to be correct under the weaker  Adjacency Faithfulness assumption  The modified algorithm  called Conservative PC  CPC   checks whether OrientationFaithfulness holds in the orientation phase  and if not  avoids drawing certain causal conclusions the PC algorithm would draw  However  if the stronger  standard causal Faithfulness condition actually obtains  the CPC algorithm outputs the same pattern as the PC algorithm does in the large sample limit  We also present a simulation study showing that the CPC algorithm runs almost as fast as the PC algorithm  and outputs significantly fewer false causal arrowheads than the PC algorithm does on realistic sample sizes      MOTIVATION  FAITHFULNESS DECOMPOSED  Directed acyclic graphs  DAGs  can be interpreted both probabilistically and causally  Under the causal interpretation  a DAG G represents a causal structure such that A is a direct cause of B just in case there  Jiji Zhang Division of Humanities and Social Sciences California Institute of Technology Pasadena  CA       jiji hss caltech edu  is a directed edge from A to B in G  Under the probabilistic interpretation  a DAG G  also referred to as a Bayesian network  represents a probability distribution P that satisfies the Markov Property  each variable in G is independent of its non descendants conditional on its parents  The Causal Markov Condition is a bridge principle linking the causal interpretation of a DAG to the probabilistic interpretation   Causal Markov Condition  Given a set of variables whose causal structure can be represented by a DAG G  every variable is probabilistically independent of its non effects  non descendants in G  conditional on its direct causes  parents in G   The assumption that the causal structure can be represented by a DAG entails that there is no causal feedback  and that no common cause of any pair of variables in the DAG is left out  All DAG based causal discovery algorithms assume the causal Markov condition  and most of them  e g   those discussed in Pearl       Spirtes et al        Heckerman et al        also assume  if only implicitly  the converse principle  known as the Causal Faithfulness or Stability Condition  Causal Faithfulness Condition  Given a set of variables whose causal structure can be represented by a DAG  no conditional independence holds unless entailed by the Causal Markov Condition  Conditional independence relations entailed by the Markov condition are captured exactly by a graphical criterion called d separation  Neapolitan        defined as follows  Given a path p in a DAG  a nonendpoint vertex V on p is called a collider if the two edges incident to V on p are both into V   V    otherwise V is called a non collider on p  Definition    d separation   In a DAG  a path p between vertices A and B is active  d connecting  relative to a set of vertices C  A  B    C  if    For a more formal presentation of the notions mentioned in this section  see Spirtes et al            i  every non collider on p is not a member of C  ii  every collider on p is an ancestor of some member of C  Two sets of variables A and B are said to be dseparated by C if there is no active path between any member of A and any member of B relative to C  A well known important result is that for any three disjoint sets of variables A  B and C in a DAG G  A and B are entailed  by the Markov condition  to be independent conditional on C if and only if they are d separated by C in G  So the causal Faithfulness condition can be rephrased as saying that for every three disjoint sets of variables A  B and C  if A and B are not d separated by C in the causal DAG  then A and B are not independent conditional on C  Two simple facts about d separation are particularly relevant to our purpose  see e g  Neapolitan       pp     for proofs   Proposition    Two variables are adjacent in a DAG if and only if they are not d separated by any subset of other variables in the DAG  Call a triple of variables hX  Y  Zi in a DAG an unshielded triple if X and Z are both adjacent to Y but are not adjacent to each other  Proposition    In a DAG  any unshielded triple hX  Y  Zi is a collider if and only if all sets that dseparate X from Z do not contain Y   it is a noncollider if and only if all sets that d separate X from Z contain Y   Below we focus on two implications of the Causal Faithfulness Condition  easily derivable given Propositions   and    We call them Adjacency Faithfulness and Orientation Faithfulness  respectively  Implication    Adjacency Faithfulness   Given a set of variables V whose causal structure can be represented by a DAG G  if two variables X  Y are adjacent in G  then they are dependent conditional on any subset of V  X  Y     resented by a DAG G  let hX  Y  Zi be any unshielded triple in G   O   if X  Y  Z  then X and Z are dependent given any subset of V  X  Z  that contains Y    O   otherwise  X and Z are dependent conditional on any subset of V  X  Z  that does not contain Y   Orientation Faithfulness obviously serves to justify the step of identifying unshielded colliders  and unshielded non colliders   For any unshielded triple hX  Y  Zi resulting from the adjacency step  a conditioning set that renders X and Z independent must have been found  The Orientation Faithfulness condition then implies that the triple is an unshielded collider if and only if the conditioning set does not contain Y   This is in fact what the familiar PC algorithm checks  The rest of our paper is motivated by the following simple observation  assuming the AdjacencyFaithfulness condition is true  we can in principle test whether Orientation Faithfulness fails of a particular unshielded triple  Suppose we have a perfect oracle of conditional independence relations  which is in principle available for many parametric families in the large sample limit by performing statistical tests  Since the Adjacency Faithfulness is by assumption true  out of the oracle one can construct correct adjacencies and non adjacencies  and thus correct unshielded triples in the causal graph  For such an unshielded triple  say  hX  Y  Zi  if there is a subset of V  X  Z  containing Y that renders X and Z independent and a subset not containing Y that renders X and Z independent  then Orientation Faithfulness fails on this triple  This failing condition can of course be verified by the oracle  Note that this simple test of Orientation Faithfulness does not rely on knowing what the true causal DAG is  The reason why this test works is that a distribution that satisfies the Adjacency Faithfulness with respect to the true causal DAG but fails the above test is not Orientation Faithful to any DAG  and hence not Orientation Faithful to the true causal DAG   We call this condition Adjacency Faithfulness for the obvious reason that this is the part of the Faithfulness condition that is used to justify the step of recovering adjacencies in constraint based algorithms  Generically  this step proceeds by searching for a conditioning set that renders two variables independent  and by the causal Markov and Adjacency Faithfulness conditions  the two variables are not adjacent if and only if such a conditioning set is found   This suggests that theoretically we can relax the standard causal Faithfulness assumption and still have provably correct and informative causal discovery procedures  In fact  one main result we will establish in this paper is that the PC algorithm  though incorrect under the weaker  Adjacency Faithfulness condition  can be revised in such a way that the modified version  that we call CPC  conservative PC   is correct given the Adjacency Faithfulness condition  and is as informative as the standard PC algorithm if the Causal Faithfulness Condition actually obtains   Implication    Orientation Faithfulness   Given a set of variables V whose causal structure can be rep   In addition to the theoretical demonstration  we will present a simulation study comparing the CPC algo    rithm and the PC algorithm  The results show that the CPC algorithm runs almost as fast as the PC algorithm  which is known for its computational feasibility  More importantly  even when the standard Causal Faithfulness Condition holds  the CPC algorithm turns out to be more accurate on realistic sample sizes than the PC algorithm in that it outputs significantly fewer false causal arrowheads and  almost  as many true causal arrowheads      HOW PC ALGORITHM ERRS  Before we present our modification of the PC algorithm  it is helpful to explain how the PC algorithm can make mistakes under the causal Markov and Adjacency Faithfulness conditions  The relevant details of the PC algorithm are reproduced below  where we use ADJ G  X  to denote the set of nodes adjacent to X in a graph G  PC Algorithm S  Form the complete undirected graph U on the set of variables V  S  n     repeat For each pair of variables X and Y that are adjacent in  the current  U such that ADJ U  X   Y   or ADJ U  Y    X  has at least n elements  check through the subsets of ADJ U  X   Y   and the subsets of ADJ U  Y    X  that have exactly n variables  If a subset S is found conditional on which X and Y are independent  remove the edge between X and Y in U   and record S as Sepset X  Y    n   n      until for each ordered pair of adjacent variables X and Y   ADJ U  X   Y   has less than n elements  S  Let P be the graph resulting from step S   For each unshielded triple hA  B  Ci in P   orient it as A  B  C iff B is not in Sepset A  C   S  Execute the orientation rules given in Meek        If the input to the PC algorithm is a sample from a population distribution that is faithful to some DAG  then in the large sample limit  the output of the PC algorithm can be interpreted as a set of DAGs  all of   Details of the Meek orientation rules do not matter for the purposes of this paper  The rules are also described in Neapolitan       pp        which are d separation equivalent  that is  they imply exactly the same d separation relations   The dseparation equivalence class of DAGs output by the PC algorithm  and the score based GES algorithm as well  is represented by a graphical object called a pattern  or a PDAG  Chickering        A pattern is a mixture of directed and undirected edges  A DAG is represented by a pattern if it contains the same adjacencies as the pattern  every directed edge A  B in the pattern is oriented as A  B in the DAG  and if the DAG contains an unshielded collider  then so does the pattern  The output of the PC algorithm is correct given the Causal Markov and Faithfulness Conditions and a perfect conditional independence oracle  such as statistical tests in the large sample limit  in the sense that the true causal DAG is among the DAGs represented by the output pattern  The output of the PC algorithm is complete in the sense that if an edge A  B occurs in every DAG in the d separation equivalence class represented by the output pattern  then it is oriented as A  B in the output pattern   Meek       Spirtes et al         Two specific features of PC are worth noting  First  in S   the adjacency step  the PC algorithm essentially searches for a conditioning set for each pair of variables that renders them independent  which we henceforth call a screen off conditioning set  But it does this with two additional tricks      it starts with the conditioning set of size    i e   the empty set  and gradually increases the size of the conditioning set  and     it confines the search of a screen off conditioning set for two variables to within the potential parents  i e   the currently adjacent nodes  of the two variables  and thus systematically narrows down the space of possible screen off sets as the search goes on  These two tricks increase both computational and statistical efficiency in most real cases  and we will keep this step intact in our modification  Secondly  in S  the PC algorithm uses a very simple rule to identify unshielded colliders or non colliders  For any unshielded triple hX  Y  Zi  it simply checks whether or not Y is contained in the screen off set for X and Z found in the adjacency stage  Now if we assume the causal Markov and AdjacencyFaithfulness conditions are true  the adjacencies  and non adjacencies  resulting from the adjacency stage are asymptotically correct  However  these two conditions do not imply the truth of OrientationFaithfulness  and when the latter fails  the PC algorithm will err even in the large sample limit  Consider the simplest example A  B  C where A C and A C B  This is the case when  for example  causation fails to be transitive  an issue of great interest to philosophers of causality  In this situation   the causal Markov and Adjacency Faithfulness conditions are both satisfied  but Orientation Faithfulness is not true of the triple hA  B  Ci  Now  given the correct conditional independence oracle  the PC algorithm would remove the edge between A and C in S  because A C  and later in S  orient the triple as A  B  C because B is not in the screen off set found in S   i e   the empty set  Simple as it is  the example suffices to establish that the PC algorithm is not asymptotically correct  under the causal Markov and Adjacency Faithfulness assumptions      CONSERVATIVE PC  It is not hard  however  to modify the PC algorithm to retain correctness under the weaker assumption  Indeed a predecessor of the PC algorithm  called the SGS algorithm  Spirtes et al         is almost correct  The SGS algorithm decides whether an unshielded triple hX  Y  Zi is a collider or a non collider by literally checking whether  O   or  O   in the statement of Orientation Faithfulness is true  Theoretically all it lacks is a clause that acknowledges the failure of Orientation Faithfulness when neither  O   nor  O   passes the check  Practically  however  the SGS algorithm is a terribly inefficient algorithm  Computationally  it is best case exponential because it has to check dependence between X and Z conditional on every subset of V  X  Z   Statistically  tests of independence conditional on large sets of variables have very low power  and are likely to lead to errors  In addition the sheer number of conditional independence tests makes it exceedingly likely that some of them will err  and we suspect that almost every unshielded triple will be marked as unfaithful if we run the SGS algorithms on more than a few variables  Fortunately  the main idea of the PC algorithm comes to the rescue  A correct algorithm does not have to check every subset of V  X  Z  in order to test whether hX  Y  Zi is a collider  a non collider  or an unfaithful triple  It only needs to check subsets of the variables that are potential parents of X and Z  This trick  as we shall show shortly  is theoretically valid  and turns out to work well in simulations  The CPC algorithm replaces S  in PC with the following S   and otherwise remains the same  S  Let P be the graph resulting from step    For each unshielded triple hA  B  Ci  check all subsets of As potential parents and of Cs potential parents     By asymptotically correct we mean the probability of the output containing an error converges to zero in the large sample limit  no matter what the true probability distribution is    a  If B is NOT in any such set conditional on which A and C are independent  orient A  B C as A  B  C   b  if B is in all such sets conditional on which A and C are independent  leave A B C as it is  i e   a non collider   c  otherwise  mark the triple as unfaithful by underlining the triple  A B C   In S   The orientation rules that are applied to unshielded non colliders in the PC algorithm are  of course  applied only to unshielded non colliders in the CPC algorithm  in particular they are not applied to triples that are marked as unfaithful   The output of the CPC algorithm can also be interpreted as a set of DAGs  If the input to the CPC algorithm is a sample from a distribution that satisfies the Markov and Adjacency Faithfulness Assumptions  in the large sample limit  the output is an extended pattern  or e pattern for short  An e pattern contains a mixture of undirected and directed edges  as well as underlinings for unshielded triples that are unfaithful  A DAG is represented by an e pattern if it has the same adjacencies as the e pattern  every directed edge A  B in the e pattern is oriented as A  B in the DAG  and every unshielded collider in the DAG is either an unshielded collider or a marked unfaithful triple in the e pattern  These rules allow that an unfaithful triple in the e pattern can be oriented as either a collider or a non collider in a DAG represented by the e pattern  The set of DAGs represented by an e pattern may not be d separation equivalent  if the e pattern contains an unfaithful triple  For example  if A causes B  and B causes C  but the causation is not transitive  i e  I A  C B  and I A  C    the resulting epattern is A BC  because it is an unfaithful triple  The set of DAGs represented by A B C contains A  B  C  A  B  C  A  B  C  and A  B  C  The latter DAG is not d separation equivalent to the first three DAGs  Note that in this case the true distribution lies in the intersection of sets of distributions represented by non  d separation equivalent DAGs  The intersection would be ruled out as impossible by the standard Faithfulness assumption  At this point it should be clear why the modified PC algorithm is labeled conservative  it is more cautious than the PC algorithm in drawing unambiguous conclusions about causal orientations  A typical output of the CPC algorithm is shown in Figure    The conservativeness is of course what is needed to make the algorithm correct under the causal Markov and Adjacency Faithfulness assumptions    Figure    A typical output for CPC  Underlining  which in the figure looks like crossing  indicates unfaithful unshielded triples discovered by the algorithm            Time  seconds   Elapsed Time                 Dimension  Theorem    Correctness of CPC   Under the causal Markov and Adjacency Faithfulness assumptions  the CPC algorithm is correct in the sense that given a perfect conditional independence oracle  the algorithm returns an e pattern that represents the true causal DAG  Proof  Suppose the true causal graph is G  and all conditional independence judgments are correct  The Markov and Adjacency Faithfulness assumptions imply that the undirected graph P resulting from step S  has the same adjacencies as G does  Spirtes et al         Now consider step S     If S    a  obtains  then A  B  C must be a subgraph of G  because otherwise by the Markov assumption  either As parents or Cs parents d separate A and C  which means that there is a subset S of either As potential parents or Cs potential parents containing B such that A C S  contradicting the antecedent in S    a   If S    b  obtains  then A  B  C cannot be a subgraph of G  and hence the triple must be an unshielded noncollider   because otherwise by the Markov assumption  there must be a subset S of either As potential parents or Cs potential parents not containing B such that A C S  contradicting the antecedent in S    b   So neither S    a  nor S    b  will introduce an orientation error  It follows that every unshielded collider in G is either an unshielded collider or a marked triple in P   Trivially S    c  does not produce an orientation error  and it has been proven  in e g   Meek       that S  will not produce any  which implies that every directed edge in P is also in G  The theorem entails that the output e pattern     has the same adjacencies as the true causal DAG  and     all arrowheads and unshielded non colliders in the epattern are also in the true causal DAG  Theorem    together with the consistency of statistical tests of in   Figure    Average elapsed time  dependence  entails that the probability of the output containing an error approaches zero as the sample size approaches infinity  Note that a triple A B C or A B C may occur in cases where the triple was initially marked as unfaithful  but all later orientation rules provided further consistent orientation information  In those cases  the underlining serves no purpose  as ambiguity concerning collider vs  non collider is already dissolved  and can be removed  The remaining triples marked unfaithful by the CPC algorithm in the large sample limit are truly ambiguous in that either a collider or a non collider is compatible with the conditional independence judgments  We conjecture but cannot yet prove that the CPC algorithm is complete in the sense that for every undirected edge in an e pattern output by the CPC algorithm  there is a DAG represented by the e pattern that orients the edge in one direction and another DAG represented by the e pattern that orients the edge in the other direction  Finally  it is obvious that asymptotically the CPC algorithm and the PC algorithm produce the same output if the standard Faithfulness assumption actually obtains      SIMULATION RESULTS  The theoretical superiority of the CPC algorithm over the PC algorithm may not necessarily cash out in practice if the situations where the Adjacency Faithfulness but not the Orientation Faithfulness holds do not arise often  We will not try to make an argument to the contrary here  even though we believe such an argu    The simulations illustrate that the extra independence checks invoked in the CPC algorithms do not render CPC significantly slower than PC and that CPC is more accurate than PC in terms of arrow orientations  The explanation for the first point is that the main computational expense of the PC algorithm occurs in the adjacency stage  the number of independence checks added in CPC for orientation is small by comparison  The second point suggests that the PC algorithm too often infers that unshielded triples are colliders  and the CPC algorithm provides the right antidote to this by means of the extra checks it performs  Again  we expect that the CPC algorithm will do particularly better than the PC algorithm when the distribution generated is close to unfaithful to the true graph  a situation pointed out by several authors as a major obstacle to reliable causal inference  Robins et al        Zhang and Spirtes        To illustrate these points  the following simulations were performed on linear Gaussian models  with variations for sparser and denser graphs  with dimensions  numbers of variables  ranging from   to     variables  For the sparser case  for each dimension d from   to     in increments of    five random graphs were selected uniformly from the space of DAGs with at most d edges and with a maximum degree of     For each such graph  a random structural equation model was constructed by selecting edge coefficients randomly      of the time uniformly from                        and      of the time uniformly from                  Selection from the range                guarantees the presence of weak edges  which in turn often lead to al   Intuitively  almost violations of OrientationFaithfulness refer to situations where two variables  though entailed to be dependent conditional on some variables by the Orientation Faithfulness condition  are close to conditionally independent  How to quantify the closeness and just how close is close enough to cause trouble depend on distributional assumptions and sample sizes                  Arrows Added  Count  ment can be made  Instead we wish to show that the CPC algorithm in practice performs better than the PC algorithm  regardless of whether OrientationFaithfulness holds or not  That is  even when the data are generated from a distribution Markov and Faithful to the true causal graph  it pays to be conservative on realistic sample sizes  One possible rationale for this is that even though PC is correct in the large sample limit if Orientation Faithfulness is not violated  it is very liable to error on realistic sample sizes if Orientation Unfaithfulness is almost violated  Almost violations of Orientation Faithfulness can arise in several ways  for example  when a triple chain is almost non transitive  or more generally  when one of the edges in an unshielded triple is very weak                    Dimension  Figure    Average count of arrow false positives most violations of faithfulness   For each such model  a random data set of      cases was simulated  to which PC and CPC were applied with significance level         for each hypothesis test of conditional independence  tested using Fishers Z transformation of partial correlation  The output was compared to the pattern of the true DAG  the true pattern   not the true DAG itself  Performance statistics were recorded  including elapsed time and false positive and negative counts for arrows  unshielded colliders  unshielded non colliders  and adjacencies  For each number of variables  each performance statistic was averaged over the five random models constructed at that dimension  for PC and for CPC  This procedure was repeated for denser models with DAGs randomly selected uniformly from the set of DAGs with at most  d edges and a maximum degree of     Counting orientation errors when there are differences in adjacencies as well raises some subtle issues that we have chosen to resolve in the following way  An arrowhead removal error  false negative  occurs when the true pattern P  contains A  B  but the output P  either does not contain an edge between A and B or does contain an edge between A and B but there is no arrowhead on this edge at B  An analogous rule is used to count arrowhead addition errors  false positive   This has the consequence that if A and B are not adjacent in P    but A  B is in P    this is counted as an adjacency addition error  but not an arrowhead addition or removal error  In contrast  if A  B is in P    this is counted as an adjacency addition error and an arrowhead addition error  because of the arrowhead at B  The justification for this is that the A  B er                             Count         Noncolliders Added     Count  Arrows Removed     Dimension              Dimension  Figure    Average count of arrow false negatives  Figure    Average count of non collider false positive  ror leaves open whether there is an arrowhead at B  and does not lead to any errors in predicting the effects of manipulations  the effects of manipulation are unknown because the orientation of the edge is unknown   In contrast  the A  B error does definitively state that there is an arrowhead at B  and does lead to errors in predicting the effects of manipulations   false negative unshielded non colliders also matches that of PC  as shown in Figures   and    In a word  in no respect is PC noticeably better than CPC  whereas CPC is significantly better than PC in avoiding false positive causal arrowheads  the arguably most consequential type of errors   There is an unshielded non collider addition error for the triple hX  Y  Zi if they form an unshielded noncollider in P    but P  either has different adjacencies among X  Y   and Z  or the same adjacencies but is a collider  An unfaithful triple in G  does not count as an unshielded non collider or collider addition error  regardless of what is in G    Unshielded non collider removal errors are handled in an analogous fashion  In all the figures  PC statistics are represented by triangles and CPC statistics are represented by circles  sparser models use filled symbols  and denser models used unfilled symbols  The horizontal axis is the number of variables in the true DAG  Figure   shows that for both sparser and denser models  CPC is only slightly slower than PC  Figure   shows that for both sparser and denser models  the number of extra arrows introduced is far better controlled by CPC than by PC  For sparser models  the number is particularly well controlled  Figure   shows that for both sparser and denser models  the number of arrowhead removal errors committed by CPC is almost indistinguishable from the number of arrowhead removal errors committed by PC  The performance of CPC regarding false positive and  Figure   plots the percentage of unfaithful triples among the total number of unshielded triples output by CPC  For sparser models  the percentage of unfaithful triples is around    percent  for denser models  it rises to around    percent  This confirms our expectation that CPC is more conservative the denser the true graph  Similar simulations were carried out parameterizing random graphs using discrete Bayes nets with   to   categories per variable  but otherwise with identical setup to the sparser continuous simulations above  with similar results      CONCLUSION  The CPC algorithm we proposed in this paper is provably correct under the causal Markov assumption plus a weaker than standard Faithfulness assumption  the Adjacency Faithfulness assumption  It can be regarded as a conservative generalization of the PC algorithm in that it theoretically gives the same answer as the PC does under the standard assumptions  Perhaps more importantly  simulation results suggest that the CPC algorithm works much better than the PC algorithm in terms of avoiding false causal arrowheads  and achieves this without costing significantly more              Percent Unfaithful Triples  Percent             Count  Noncolliders Removed                     Dimension              Dimension  Figure    Average count of non collider false negatives  Figure    Percent of unfaithful triples among all unshielded triples  in running time or missing positive information  We do not claim that the evidence is conclusive  and we think it would be interesting to compare CPC and PC on real data sets   
 The presence of latent variables can greatly complicate inferences about causal relations between measured variables from statistical data  In many cases  the presence of latent variables makes it impossible to determine for two measured variables A and B  whether A causes B  B causes A  or there is some common cause  In this paper I present several theorems that state conditions under which it is possible to reliably infer the causal relation between two measured variables  regardless of whether latent variables are acting or not      Introduction  The problem of inferring causal relations from statistical data in the absence of experiments arises repeatedly in many scientific disciplines  including sociology  economics  epidemiology  and psychology  In addition  the building of expert systems could be expedited if background knowledge elicited from experts could be supplemented with automated techniques  Recently  efficient algorithms for determining causal structure  in the form of Bayesian networks  from statistical data when there are no unmeasured or  latent  variables have been proposed   See Spirtes  Glymour and Scheines       Spirtes and Glymour        Spines  Glymour  and Scheines forthcoming  Verma and Pearl       and Pearl and Verma         Inferring causal relations when unmeasured variables are also acting is a much more difficult problem  In many cases it is impossible to infer the structure among the latent variables from statistical relations among the measured variables  But the presence of latent variables can also make it difficult to infer the causal relations among the measured variables themselves  One important question for many policy decisions is  Does A cause B   Statistics about A and B alone do not suffice to answer these questions  When only two variables  A and B  have been measured  and there is a correlation between the two  this does not suffice to establish whether A caused B  B caused A  or there is a third variable causing both A and  B  Nevertheless  when more variables are measured  more knowledge about the causal relations between A and B is possible  Drawing upon recent results of Verma and Pearl  Verma and Pearl       and Pearl and Verma        I will prove in Theorem   that there are some circumstances in which it is possible to establish that A caused B  rather than that B caused A  or that a third variable caused both A and B  I will prove in Theorem   that there are other circumstances in which the possibility that A caused B can be eliminated  The proofs are given in the Appendix  I will also demonstrate that a recent proposal derived from Pearl and Verma        to establish more general conditions for causal pathways in the presence of unmeasured causes is incorrect      Results  Causal processes between a set of random variables V are represented by a directed acyclic graph over V  where there is an edge from A to B if and only if A is an immediate cause of B relative to V   For a discussion of the meaning of immediate causation  see Spirtes  Glymour and Scheines        If there is a directed path from A to B in the causal graph  I will say that A is a  possibly indirect  cause of B  If a distribution is placed over the exogenous variables in a causal process  variables of zero indegree in the causal graph   which in turn affect the values of other random variables  the result is a joint distribution over all of the random variables  In that case  I will say that the causal process generated the joint distribution  Following Pearl         I assume that the distribution generated by a causal process satisfies the Minimality and Markov conditions for the causal graph of that process  In Pearl s terminology  Pearl       the causal graph is a Bayes network of any distribution that it generates   In what follows  I will capitalize random variables  and boldface any sets of variables    Markov Condition  Let Descendant V  be the set of descendants of V in a graph G  and Parents V  be the set of parents of V in G  A graph G and a probability distribution P on the vertices V of G satisfy the Markov condition if and only if for every V in V  and every subset   Detecting Causal Relations in the Presence  X of V  V and X     V  u Descendants V  are independent conditional on Parents V   Minimality Condition  A graph G and a probability  distribution P satisfies the minimality condition if and only if G and P satisfy the Markov condition and for every graph H obtained by deleting an edge from G  H and P do not satisfy the Markov condition  If P satisfies the Markov condition for graph G  and every conditional independence true of the distribution P is entailed by the Markov condition  then we say that P is faithful to G  In a directed graph G  I will write X   Y if there is an edge from X toY in G  In an undirected graph U  I will write X  Y if there is an undirected edge between X and Y  X and Y are adjacent in a directed graph G if and only if either X   Y orY   X in G  Two edges are adjacent in an undirected graph U if and only if X Y in U  In a directed acyclic graph G  an undirected path U from X to Y is a sequence of vertices starting with X and ending with Y such that for every pair of variables A and B that are adjacent to each other in the path  A and B are adjacent in G  and no vertex occurs more than once in U  In a directed acyclic graph G  a directed path P from X to Y is a sequence of vertices starting with X and ending with Y such that for every pair of variables A and B that are adjacent to each other in the path in that order  the edge A    B occurs in G  and no vertex occurs more than once in P  An edge between X and Y occurs in a p ath P  directed or undirected  if and only if X and Y are adjacent in P  If an undirected path U contains an edge between X and Y  and an edge between Y and Z  the two edges collide at Z if and only if X   Y and Z   Y in G  On an undirected path U  Z is an unshielded collider if and only if there exist edges X   Y and Z   Y in U  and Z and X are not adjacent in G  X is an ancestor of Y and Y is a descendant of X if and only if there is a directed path from X toY  Pearl and Verma have shown how to calculate the conditional independence relations that are entailed by distributions satisfying the Markov condition for a graph G using the d separability relation  In graph G  variables X and Y are d separated by a set of vertices S not containing X orY if and only if there exists no undirected path U between X andY  such that  i  every collider on U has a descendent in S and  ii  no other vertex on U is in S  Disjoint sets of variables X and Y are d separated by S in G if and only if every member of X is d separated from every member of Y by S in G  If distribution P satisfies the Markov condition for graph G  then the Markov condition entails that X is independent of Y conditional on S if and only if X is d separated from Y by S in G  Pearl l      The following algorithm  Spirtes       Spirtes        reconstructs the set of all graphs of causal processes that could have generated a given probability distribution  under the assumptions that no latent variables are present  i e  every cause of a pair of measured variables is itself measured  and that the distribution is faithful to the graph of the causal process that generated it  The d separability  of Unmeasured Variables  relations of the graph can be determined either by performing tests of conditional independence on the generated distribution  or in the linear case  tests of zero partial correlations  Let Ac A B  denote the set of vertices adjacent to A or to B in graph C  except for A and B themselves  Let Uc A B  denote the set of vertices in graph C on  acyclic  undirected paths between A and B  except for A and B themselves   Since the algorithm is continually updating C  Ac A B  and Uc A B  are constantly changing as the algorithm progresses   PC Algorithm A   Form the complete undirected graph C on the vertex set V  B    n      repeat For each pair of variables A  B adjacent in C  if Ac A B  n Uc A B  has cardinality greater than or equal to n and A  B are d separated by any subset of Ac A B  n Uc A B  of cardinality n  delete A   B from C  n n l  until for each pair of vertices A  B that are adjacent in C  Ac A B  n Uc A B  is of cardinality less than n  C   Let F be the graph resulting from step B  For each triple of vertices A  B  C such that the pair A  B and the pair B  C are each adjacent in F but the pair A  C are not adjacent in F  orient A B   C as A   B    C if and only if A and C are not d separated by any subset of Ap A C  n UF A  C  containing B     D  repeat If there is a directed edge A    B  and an undirected edge B   C  and no edge of either kind connecting A and C  then orient B   C as B   C  If there is a directed path from A to B  and an undirected edge A   B  orient A B as A   B  until no more arrowheads can be added  We have run this algorithm on as many as    variables in randomly generated sparse graphs  In Monte Carlo simulations on linear models  for large sample sizes  on the order of several thousand  on sparse graphs  the percentage of errors of omission or commission for adjacencies is below     Under the same conditions  the percentage of arrowheads that are erroneously omitted is less than      and the percentage of arrowheads that are erroneously added is about       See Spirtes  Glymour  and Scheines forthcoming for the details of this Monte Carlo study   Following Pearl s terminology  the output of the algorithm when there are no unmeasured common causes is a mixture of directed and undirected edges called a             Spirtes  pattern  A pattern TI represents a set of directed acyclic graphs  A graph G is in the set of graphs represented by TI if and only if      G has the same adjacency relations as TI     If the edge between A and B is oriented A    B in TI  then it is oriented A   B in G     There are no unshielded colliders in G that are not also unshielded colliders in TI  A hybrid graph may contain contain bidirected edges edges  Such graphs may be used to represented the marginal structure on a set of measured variables when unmeasured common causes have edges that collide  in Verma and Pearl s terminology  are  head to head   with edges between measured variables  When there are unmeasured common causes  the output of the PC algorithm can include such bi directed edges  X   Y denotes that there is an undirected edge between X andY in a pattern TI  X o  Y denotes  hat X andY are adjacent and there is at least an arrowhead intoY  X   Y denotes X o   Y and not Y o   X  and X     Y denotes X o   Y and Y o   X  Two vertices are adjacent in a hybrid graph if X Y  X    Y  or X     Y  Two edges collide atY if and only if each edge has an arrowhead directed into Y  The definitions of directed path  undirected path  and d separability for hybrid graphs are the same as for directed graphs  Let  D be a directed acyclic graph with vertex set U of  which the subset   are observable  The pattern TI of D restricted to   is the hybrid graph that is the result of applying the PC algorithm to the d separability relations of D that involve just variables in    From the marginals over   of distributions fairhful to D  the PC algorithm can be used to construct the pattern of D   The method that Verma and Pearl use to construct the pattern of D is based upon an earlier algorithm described in Spirtes      for constructing graphs when no latent variables are present given fairhful input  It is equivalent in output to the PC algorithm  but is too slow to be used on large numbers of variables  and is less reliable in practice because it requires testing high order conditional independence relations    I will show the following   Theorem    Let G be a graph over a set of vertices U   and   be a subset of U containing X and Y  and TI the pattern of G over    If there exists a directed path A from X toY in G then TI contains a semi directed path B from X toY  Theorem   states that if the probability distribution on the set of measured variables is not compatible with a semi directed path B from X toY then if that distribution is the marginal of a distribution perfectly represented by a graph on a larger vertex set  the larger graph also contains no directed path from X to Y  In that case  it is possible to know that A is not a cause of B by examining the marginal over the observed variables  even if latent variables are present  The following theorem states under what conditions it is possible to infer from a pattern the existence of a directed path in a graph G In a directed acyclic graph or a pattern  X  Y  and Z form a triangle if and only if X is adjacent toY and Z  and Z is adjacent toY      Let   be a subset of vertices of G containing X and Z  and let the pattern of G for   contain a directed edge X    Z  no triangle containing X and Z   Theorem  and a variable C such that C o   X   Then in G there is a directed path from X to Z  Furthermore   Corollary    Let   be a subset of vertices of G containing X and Z  and let the pattern of G for   contain a vertex C such that C o   X   and a directed path A from X to Z such that for no adjacent pair U  W on A is there a triangle in TI containing both U and W  Then in G there is a directed path from X to Z  Theorem   and Corollary   state conditions under which it is possible to know that A is a  possibly indirect  cause of  B simply by examining the marginal over the observed variables  even if latent variables are present      Can Theorem   Be Strengthened   Theorem    Given a graph G over a set of variables U   Verma and Pearl have recently published a claim  Corollary   in Pearl       that entails a stronger version of Theorem    A consequence of their Corollary    translated into the language of this paper isl   a distribution P that satisfies the Markov condition for G  and some subset   of U   for X   Y   and C that are disjoint subsets of    the Markov condition entails that X is independent of Y conditional on C if and only if X and Y are d separated by C in the pattern of G over     Let   be a subset of vertices of G containing X and Z  and let the pattern TI of G for   contain a directed path P from X to Z  such that for every edge A    B in P there is a variable C such that C o   A  Then in G there is a directed path from X to Z   The following theorem is a corollary of a theorem proved by Verma and Pearl   In a pattern TI   a semi directed path P from X to Y is an undirected path from X to Y such that if A occurs before B in P  then in TI the edge from B to A docs not have an arrowhead into A  In other words  a semi directed path can contain both undirected edges and directed edges pointing in the direction from X to Y  but it cannot contain any bidirected edges or edges pointing in the direction fromY to X   This claim is false  It is stronger than Theorem   because it does not require that each edge in P not be part of a triangle in P order to infer the existence of a directed palh l The statement of their Corollary    is somewhat ambiguous  so it is  not immediately obvious that the correct interpretation entails the consequence stated here   However Pearl  personal communication   has confinned that the interpretation presented here is the intended one    Detecting Causal Relations in the Presence of Unmeasured Variables  from X to Z in G  The graph depicted in Figure I   suggested by the proof of Theorem    provides a counterexample  In G  there is no directed path fromX to Z  however in the pattern of G over the set of variables that does not include T  there is a uni directed edge fromX to Z  and an edge D   X   path PJ  X Y  in G from X to Y that induces an edge betweenX andY in IT  then eitherX    Y orX  Y in IT   Proof  The proof is a reductio  Assume that there is a directed path P in G from X to Y that induces and edge betweenX andY in IT  but neitherX    Y nor X  Y in IT  It follows thatY o  X in IT  By lemma    there is a vertex Z in   such that either Z is adjacent to X and not toY in IT  and both of the edges betweenX andY andX and Z are induced by paths pointing at X  or Z o   X in IT  andX is a descendant ofY in G  X is not a descendant ofY in G  becauseY is a descendant ofX in G  and G is    E  Graph G  acyclic  Suppose then that there is a vertex Z in   such that Z is adjacent toX and not toY in IT  and both of the edges betweenX andY andX and Z are induced by paths pointing atX  Let P  Z X  be an undirected path between X and Z that points intoX and induces an edge betweenX and Z in IT  and P  XY     be an undirected path betweenX andY that points intoX and induces an edge betweenX andY in IT  Let R be the first point of intersection of P X Z  with P  X Y   P  Z R  be the subpath of P  Z X  from Z to R  P  R Y   be the subpath of P  X Y   from R to Y  and P  Z Y   be the concatenation of P  Z R  and P  R Y   P  Z Y  is an undirected path because P  Z R  and P  R Y  by construction intersect only at R  and hence P  Z  R  and P  R Y  contain any given vertex at most once   Pattern of G over  C D X Z E   Figure       Appendix  Lemma     Verma and Pearl   Let   be a subset of U  D  a directed acyclic graph over U  and IT the pattern of D restricted to    Two variables A  B in   are adjacent in IT if and only if there exists a path P between A andB in D satisfying the following two conditions   Every vertex on P  Z Y   that is in   is a collider on P  Z Y   By lemma    every vertex in   on P  Z  R  except for Z and R are collidcrs on P  Z  R   and every vertex in   on P   R Y  except for R andY are colliders on P   R Y    because they are subpaths of paths that induce edges between Z andX  andX andY respectively   This shows that each vertex in   with the possible exception of R on P  Z Y  is a collider on P  Z Y   I will now show that if R is in    it is also a collider on P  Z Y   If R is equal to X  thenX is a collider on P  Z Y  because both P  Z X  and P XY    are intoX  If R is not equal to X  and R is in    then R is a collider on both P  Z X  and P X Y   hence R is a collider on P  Z Y   It follows that every vertex on P  Z Y   that is in   is a collider on P  Z Y        every observable node on P   except the endpoints  is a collider along P  and  Suppose first that every collider on P  Z Y   is a shieldable ancestor of either Z orY  By lemma    P  Z Y  induces an edge between Z andY in IT  It follows from lemma         every collider along P is a shieldable ancestor of either A orB   orientation  This contradicts the assumption   where an ancestor S of A is shieldable if and only if every directed path from S to A contains an observable other than A   Lemma     Verma and Pearl   For any pattern IT of D over    A o   B if and only if there is a node C such that either     C is adjacent to B and not A  in IT  and both edges A  B andB   C were induced by paths   of D  which ended pointing at B  or      C o   A in IT and B is a descendent of A in D   Lemma    Let G be a directed acyclic graph over a set of vertices U    be a subset of U containing X andY  and IT be the pattern of G restricted to    If there is a directed  that P  Z X  and P  X Y   do not induce a Y o   X  Suppose next that there is a collider on P  Z Y  that is not a shieldable ancestor of either Z or Y  and let W be the first such collidcr after Z  R is the only vertex on P  Z Y  that may be a collider on P  Z Y     but not a collider in either P  Z X   or P  XY      Hence W is either equal to R or a collider on P  Z X  or P  XY      In either caseY is a descendant of W  Suppose first that W is a collider on P  Z X  or P  XY      Because W is not a shieldable ancestor of either Z orY  by lemma    W is a shieldable ancestor ofX  X is an ancestor ofY  and W is an ancestor ofX  so W is an ancestor ofY  Suppose next that W is equal to R  In this case R is not equal to X  becauseX is an ancestor ofY andX is in    and henceX             Spirtes  is a shieldable ancestor ofY  Either X is a descendant of     R  Y is a descendant of R  or some collider along P  XY is a descendant of R  If X is a descendant of R  thenY is a descendant of R  because Y is a descendant of X  If some collider along P  X Y  is a descendant of R  thenY is a descendant of R because each collidcr on P  X Y  is a shieldable ancestor of either X orY  andY is a descendant of X  In any case  then  Y is a descendant of W  W is an ancestor ofY  but not a shicldable ancestor ofY  so there is a directed path P WY     from W to Y that contains no vertices in   other thanY  Let S be the first point of intersection of P ZY     with P WY      P Z  S  the subpath of P Z Y  from Z to S  P S Y  the subpath of P WY     from S toY  and P  ZY     the concatenation of P Z S  and P S Y   S is not a collidcr on P    Z   Y  because the first edge P SY     is not directed into S  Hence every collider on P Z  S  is a collidcr on P  Z  W   W is the first collider on P Z Y  that is not a shieldable ancestor of Z or Y  S either equals W or is before W on P Z Y   S is not a collidcr on P  Z Y    and P S Y  contains no colliders  hence every collidcr on P  Z Y  is a shieldable ancestor of either Z orY  Every vertex V on P   Z Y  that is in   is on P Z  S   V is not equal to S because S is not in    Ir V is not equal to S  then V is a collidcr on P Z  S   and hence a collider on P  Z Y   By lemma    P  Z Y  induces an edge between Z andY in P  It follows from lemma   that P Z X  and P  X Y  do not induce a Y o   X orientation  This contradicts the assumption  Theorem    Let G be a graph over a set of vertices U   and   be a subset of U containing X andY  and TI the pattern of G over    If there exists a directed path A from X toY in G then TI contains a semi directed path B from X toY  Proof  Break the path A in G into a series of subpaths  such that only the endpoints of the subpaths are in    Let U be the source and V be the sink of some such arbitrary subpath  There is an edge between U and Vin TI by lemma    U is prior to V on B  The concatenation of the edges induced by the subpaths arc an undirected path B from X toY inTI  By Lemma    it is not the case that V o   U  By definition of semi directed path  B is a semi directed path from X toY     Let   be a subset of vertices of G containing X and Z  and let the pattern TI of G for   contain a directed edge X    Z  no triangle containing X and Z  and a variable D such that D o   X  Then in G there is a directed path from X to Z   Theorem  Proof  Since X and Z are adjacent in TI there is a path A in G between X and Z such that every observable node on A is a collider and every collider on A is a shieldable ancestor of X or Z   If X    Z in pattern TI arises because of clause      of lemma    we are done because Z is a descendant of X in G  So suppose X    Z is oriented by condition     of  lemma    Then there is a path A in G that induces X    Z and A is into Z in G  If A contains no colliders and is not into X  then A is a directed path from X to Z  and we are done  Otherwise there are two cases  A contains a collider  or A is into X  First  we consider the case where A is into X  Then there is a path between X and Z that induces an edge in TI  and is into X  By assumption there is a vertex D in TI such that D o   X  By lemma    either there is a vertex C in n such that C is adjacent to X and not D  and both edges C X and D X are induced by paths of G which point at X  or C o   D in TI and X is a descendant of D in G  Suppose that the first disjunct is true  In that case  either C is adjacent to Z inTI or it isn t  If it is adjacent to Z  then there is a triangle in IT containing X and Z  If C is not adjacent to Z  then by clause     of lemma    Z o   X  contrary to our assumption  Suppose now that the second disjunct is true  Because X is a descendant of D in G  there is a directed path in G from some variable E in   to X that does not contain any variables in   other than X and E  This path induces an edge between E and X in IT  If E is adjacent to X in TI  then TI contains a triangle containing X and Z  if E is not adjacent to X inTI  then by clause     of lemma    Z o   X  contrary to our assumption  We now consider the case where A is not into X  but there is a collider on A  Let K be the first collider on A after X  Because A is not into X  then there is a directed path from X to K  and hence no directed path from K to X  This implies that K is not an ancestor  and hence not a shieldable ancestor of X  So by clause     of lemma    K is a shieldable ancestor of Z  The concatenation of the paths from X to K and from K to Z is a directed path from X to Z     Let   be a subset of vertices of G containing X and Z  and let the pattern TI of G for   contain a vertex C such that C o   X  and a directed path P from X to Z such that for no adjacent pair U  W on P is there a triangle in n containing both U and W  Then in G there is a directed path from X to Z  Corollary  Proof  Since there is a directed path from X to Z in TI    there i s a sequence of edges X    A    B        Z i n TI  By Theorem    there is a directed path from X to A in G  Since X    A in TI  Theorem   can next be applied to A    B  to show that there is a directed path from A to B in G  Repeating this process in turn for each edge on P implies that there is a directed path from X to Z in G   Acknowledgements  I wish to thank Clark Glymour for a number of useful discussions and suggestions on the topic of causal inference when latent variables are present  This research was supported in part by a graph with the Office of Naval Research  and the Naval Manpower Research and Development Center under Contract number N         J      Theorem   and a weaker version of Theorem   were reported in  Causal Structure among Measured   Detecting Causal Relations in the Presence of Unmeasured Variables  Variables Preserved with Unmeasured Variables   by Peter Spirtes and Clark Glymour  Laboratory for Computational Linguistics Technical Report No  CMU LCL       August         
  gorithm  an efficient search procedure over equivalence classes of DAGs  Meek       Chickering         Different directed acyclic graphs  DAGs  may be Markov equivalent in the sense that they entail the same conditional independence relations among the observed variables  Chickering        provided a transformational characterization of Markov equivalence for DAGs  with no latent variables   which is useful in deriving properties shared by Markov equivalent DAGs  and  with certain generalization  is needed to prove the asymptotic correctness of a search procedure over Markov equivalence classes  known as the GES algorithm   In many situations  however  we need also to consider DAGs with latent variables  Indeed there are cases where no DAGs can perfectly explain the observed conditional independence relations unless latent variables are introduced  see  e g   Figure     But it is often undesirable to work with latent variable DAG models  especially with respect to model search  For example  given a set of observed variables  there are infinitely many latent variable DAG models to search over  Besides  to fit and score a DAG with latent variables is usually difficult due to statistical issues such as identifiability  Fortunately  such latent variable DAG models can be represented by ancestral graphical models  Richardson and Spirtes        in that for any DAG with latent variables  there is a  maximal  ancestral graph that captures the exact observable conditional independence relations as well as some of the causal relations entailed by that DAG  Since ancestral graphs do not explicitly include latent variables  they are more amenable to search  Spirtes et al          For DAG models with latent variables  maximal ancestral graphs  MAGs  provide a neat representation that facilitates model search  However  no transformational characterization  analogous to Chickerings  of Markov equivalent MAGs is yet available  This paper establishes such a characterization for directed MAGs  which we expect will have similar uses as it does for DAGs      Peter Spirtes Department of Philosophy Carnegie Mellon University Institute for Human and Machine Cognition University of West Florida ps z andrew cmu edu  INTRODUCTION  Markov equivalence between directed acyclic graphs  DAGs  has been characterized in several ways  e g   Verma and Pearl       Chickering       Andersson et al         All of them have been found useful for various purposes  In particular  the transformational characterization provided by Chickering         that two Markov equivalent DAGs can be transformed to each other by a sequence of single edge reversals that preserve Markov equivalence  is useful in deriving properties shared by Markov equivalent DAGs  Moreover  when generalized  the transformational characterization implies the asymptotic correctness of the GES al   Markov equivalence for ancestral graphs has been characterized in ways analogous to the one given by Verma and Pearl        for DAGs  Spirtes and Richardson       Ali et al         However  no result is yet available that is analogous to Chickerings transformational characterization  In this paper we establish one for directed ancestral graphs  Specifically we show that two directed maximal ancestral graphs are Markov equivalent if and only if one can be transformed to the other by a sequence of single mark changes  adding or dropping an arrowhead  that preserve Markov equivalence  This characterization we expect will have similar uses as Chickerings does for DAGs  In particular  it is a step towards justifying the application of the GES algorithm to MAGs  and hence to latent variable DAG models  The paper is organized as follows  The remainder of this section introduces the relevant definitions and no    tations  We then present the main result in section    drawing on some facts proved in Zhang and Spirtes        and Ali et al          We conclude the paper in section   with a discussion of the potential application  limitation and generalization of our result       DIRECTED ANCESTRAL GRAPHS  In full generality  an ancestral graph can contain three kinds of edges  directed edge     bi directed edge    and undirected edge     In this paper  however  we will confine ourselves to directed ancestral graphs  which do not contain undirected edges  until section    where we explain why our result does not hold for general ancestral graphs  The class of directed ancestral graphs  due to its inclusion of bidirected edges  is suitable for representing observed conditional independence structures in the presence of latent confounders  see Figure     Without undirected edges  however  ancestral graphs cannot represent the presence of latent selection variables  By a directed mixed graph we denote an arbitrary graph that can have two kinds of edges  directed and bi directed  The two ends of an edge we call marks or orientations  So the two marks of a bi directed edge are both arrowheads      while a directed edge has one arrowhead and one tail    as its marks  Sometimes we say an edge is into  or out of  a vertex if the mark of the edge at the vertex is an arrowhead  or a tail   The meaning of the standard graph theoretical concepts  such as parent child   directed  path  ancestor descendant  etc   remains the same in mixed graphs  Furthermore  if there is a bi directed edge between two vertices A and B  A  B   then A is called a spouse of B and B a spouse of A  Definition    ancestral   A directed mixed graph is ancestral if  a   there is no directed cycle  and  a   for any two vertices A and B  if A is a spouse of B  i e   A  B   then A is not an ancestor of B  Clearly DAGs are a special case of directed ancestral graphs  with no bi directed edges   Condition  a   is just the familiar one for DAGs  Condition  a    together with  a    defines a nice feature of arrowheads  that is  an arrowhead implies non ancestorship  This motivates the term ancestral and induces a natural causal interpretation of ancestral graphs  Mixed graphs encode conditional independence relations by essentially the same graphical criterion as the well known d separation for DAGs  except that in mixed graphs colliders can arise in more edge configurations than they do in DAGs  Given a path u in a  mixed graph  a non endpoint vertex V on u is called a collider if the two edges incident to V on u are both into V   otherwise V is called a non collider  Definition    m separation   In a mixed graph  a path u between vertices A and B is active  mconnecting  relative to a set of vertices Z  A  B    Z  if i  every non collider on u is not a member of Z  ii  every collider on u is an ancestor of some member of Z  A and B are said to be m separated by Z if there is no active path between A and B relative to Z  The following property is true of DAGs  if two vertices are not adjacent  then there is a set of some other vertices that m separates  d separates  the two  This  however  is not true of directed ancestral graphs in general  For example  Figure   a  is an ancestral graph that fails this condition  C and D are not adjacent  but no subset of  A  B  m separates them   A  B  A  B  C  D  C  D   a    b   Figure     a  an ancestral graph that is not maximal   b  a maximal ancestral graph This motivates the following definition  Definition    maximality   A directed ancestral graph is said to be maximal if for any two nonadjacent vertices  there is a set of vertices that mseparates them  It is shown in Richardson and Spirtes        that every non maximal ancestral graph can be easily transformed to a unique supergraph that is ancestral and maximal by adding bi directed edges  This justifies considering only those ancestral graphs that are maximal  MAGs   From now on  we focus on directed maximal ancestral graphs  which we will refer to as DMAGs  A notion closely related to maximality is that of inducing path  Definition    inducing path   In an ancestral graph  a path u between A and B is called an inducing path if every non endpoint vertex on u is a collider and is an ancestor of either A or B    For example  in Figure   a   the path hC  A  B  Di is an inducing path between C and D  Richardson and Spirtes        proved that the presence of an inducing path is necessary and sufficient for two vertices not to be m separated by any set  So  to show that a graph is maximal  it suffices to demonstrate that there is no inducing path between any two non adjacent vertices in the graph  Given any DAG with  or without  latent variables  the conditional independence relations as well as the causal relations among the observed variables can be represented by a DMAG that includes only the observed variables  The DMAG is constructed as follows  for every pair of observed variables  Oi and Oj   put an edge between them if and only if they are not d separated by any set of other observed variables in the given DAG  and mark an arrowhead at Oi  Oj   on the edge if it is not an ancestor of Oj  Oi   in the given DAG  For example  Figure   a  is a DAG with latent variables  L   L   L    Figure   b  depicts the DMAG  G   resulting from the above construction  The mseparation relations in G  correspond exactly to the d separation relations over  X   X   X   X   X   in Figure   a   By contrast  no DAG without extra latent variables has the exact same d separation relations  Furthermore  the orientations in G  accurately represent the ancestor relationships  which  upon natural interpretations  are causal relationships  among the observed variables in   a    This  however  is not the case with G    X   X        MARKOV EQUIVALENCE  A DMAG represents the set of joint distributions that satisfy its global Markov property  i e   the set of distributions of which the conditional independence relations entailed by m separations in the DMAG hold  Hence  if two DMAGs share the same m separation structures  then they represent the same set of distributions  Definition    Markov equivalence   Two DMAGs G    G   with the same set of vertices  are Markov equivalent if for any three disjoint sets of vertices X  Y  Z  X and Y are m separated by Z in G  if and only if X and Y are m separated by Z in G    Figure   c   for example  is a DMAG Markov equivalent to   b   It is well known that two DAGs are Markov equivalent if and only if they have the same adjacencies and the same unshielded colliders  Verma and Pearl         A triple hA  B  Ci is said to be unshielded if A  B are adjacent  B  C are adjacent but A  C are not adjacent   The conditions are still necessary for Markov equivalence between DMAGs  but are not sufficient  For two DMAGs to be equivalent  some shielded colliders have to be present in both or neither of the graphs  The next definition is related to this  Definition    discriminating path   In a DMAG  a path between X and Y   u   hX       W  V  Y i  is a discriminating path for V if i  u includes at least three edges  i e   at least four vertices as specified    L   ii  V is adjacent to an endpoint Y on u  and L   X    a   X  X   X   L   X   X   iii  X is not adjacent to Y   and every vertex between X and V is a collider on u and is a parent of Y    Y  X  X   X   X   X   X X   X    b  G    c  G   Figure     a   A DAG with latent variables   b   A DMAG that captures both the conditional independence and causal relations among the observed variables represented by  a    c   A DMAG that entails the right conditional independence relations but not the right causal relations in  a    W  V  Figure    A discriminating path for V   the triple hW  V  Y i is discriminated to be a collider here  Discriminating paths behave similarly to unshielded triples in that if u   hX       W  V  Y i is discriminating for V   then hW  V  Y i is a  shielded  collider  See Figure    if and only if every set that m separates X and Y excludes V   it is a non collider if and only if every set that m separates X and Y contains V   The   following proposition is proved in Spirtes and Richardson                   explored the statistical significance of this fact for fitting bi directed graphs   Proposition    Two DMAGs over the same set of vertices are Markov equivalent if and only if  Another feature which will be particularly relevant to our argument is that between a DMAG and any of its LEGs  only one kind of difference is possible  namely  some bi directed edges in the DMAG are oriented as directed edges in its LEG  For a simple illustration  compare the graphs in Figure    where H  is a LEG of G   and H  is a LEG of G   This feature is important because it will be the condition for Theorem   in        e   They have the same adjacencies   e   They have the same unshielded colliders   e   If a path u is a discriminating path for a vertex B in both graphs  then B is a collider on the path in one graph if and only if it is a collider on the path in the other      TRANSFORMATION BETWEEN EQUIVALENT DMAGS  X   X  X   X   X   X   X   X        LOYAL EQUIVALENT GRAPH  Given a MAG G  a mark  or edge  in G is invariant if it is present in all MAGs Markov equivalent to G  Invariant marks are particularly important for causal inference because data alone usually cannot distinguish between members of a Markov equivalence class  An algorithm for detecting all invariant arrowheads in a MAG is given by Ali et al          and one for further detecting all invariant tails is presented in Zhang and Spirtes         The following is a special case of Corollary    in Zhang and Spirtes         Proposition    Given any DMAG G  there exists a DMAG H Markov equivalent to G such that all bidirected edges in H are invariant  and every directed edge in G is also in H  We will call H in Proposition   a Loyal Equivalent Graph  LEG  of G  In general a DMAG could have multiple LEGs  A distinctive feature of the LEGs is that they have the fewest bi directed edges among Markov equivalent DMAGs    Drton and Richardson   The conditions are also valid for maximal ancestral graphs that contain undirected edges    For general MAGs  Corollary    in Zhang and Spirtes        also asserts that the LEGs have the fewest undirected edges as well   X   H   G   We present the main result of the paper in this section  namely Markov equivalent DMAGs can be transformed to each other by a sequence of single mark changes that preserve Markov equivalence  We first describe in section     two corollaries from Zhang and Spirtes        and Ali et al         which our arguments will rely upon  Section     establishes sufficient and necessary conditions for a single mark change to preserve Markov equivalence  The theorems are then presented in section       X   X   X  X   X   X   X   X   X   X   X   G   H   Figure    A LEG of G   H   and a LEG of G   H   A directed edge in a DMAG is called reversible if there is another Markov equivalent DMAG in which the direction of the edge is reversed  To prove Theorem   in      we also need a fact that immediately follows from Corollary     in Ali et al          Proposition    Let A  B be any reversible edge in a DMAG G  For any vertex C  distinct from A and B   there is an invariant bi directed edge between C and A if and only if there is an invariant bi directed edge between C and B  In particular  if H is a LEG of a DMAG  then A  B being reversible implies that A and B have the same spouses  as every bi directed edge in H is invariant       LEGITIMATE MARK CHANGE  Eventually we will show that any two Markov equivalent DMAGs can be connected by a sequence of equivalence preserving mark changes  It is thus desirable to give some relatively simple graphical conditions under which a single mark change would preserve equivalence  Lemma   below gives necessary and sufficient conditions under which adding an arrowhead to a directed edge  i e   changing the directed edge to a bi directed one  preserves Markov equivalence  By symmetry  they are also the conditions for dropping   an arrowhead from a bi directed edge while preserving Markov equivalence  Lemma    Let G be an arbitrary DMAG  and A  B an arbitrary directed edge in G  Let G   be the graph identical to G except that the edge between A and B is A  B   In other words  G   is the result of simply changing A  B into A  B in G   G   is a DMAG and Markov equivalent to G if and only if  t   there is no directed path from A to B other than A  B in G   t   For any C  A in G  C  B is also in G  and for any D  A in G  either D  B or D  B is in G   t   there is no discriminating path for A on which B is the endpoint adjacent to A in G  Proof Sketch    We skip the demonstration of necessity because it is relatively easy and will not be used later  To prove sufficiency  suppose  t    t   are met  and we show that they guarantee G   is a DMAG and is Markov equivalent to G  To see that G   is ancestral  note that it only differs from G  an ancestral graph  regarding the edge between A and B  So the only way for G   to violate the definition of ancestral graph is for A to be an ancestor of B in G     which contradicts  t    To show that G   is maximal  we need to show that there is no inducing path  Definition    between any two non adjacent vertices  Suppose for contradiction that there is an inducing path u in G   between two nonadjacent vertices  D and E  Then u includes A  B and A is not an endpoint of u  for otherwise u would also be an inducing path in G  contradicting the fact that G is maximal  Also note that D is not a parent of B  otherwise D is an ancestor of E by definition    which easily leads to a contradiction given that G   has been shown to be ancestral  Suppose  without loss of generality  that D is the endpoint closer to A on u than it is to B  Let u D  A    hD   V         Vn   Ai be the subpath of u between D and A  Then some Vi is Bs spouse  in G   for otherwise we can show by induction  starting from Vn   that every Vi   and in particular V    D  is a parent of B  which is a contradiction  Let Vj be a spouse of B on u D  A   Replacing u Vj   B  on u with Vj  B yields an inducing path between D and E that does not include A  B  and hence an inducing path between D and E in G  a contradiction  So the initial supposition of nonmaximality is false  G   is also maximal    The full version of the paper can be found at www andrew cmu edu user jiji transformation pdf   Lastly  we verify that G and G   satisfy the conditions for Markov equivalence in Proposition    Obviously they have the same adjacencies  and share the same colliders except possibly A  But A will not be a collider in an unshielded triple  for condition  t   requires that any vertex that is incident to an edge into A is also adjacent to B  So the only worry is that a triple hC  A  Bi might be discriminated by a path  but  t   guarantees that there is no such path  We say a mark change is legitimate when the conditions in Lemma   are satisfied  Recall that for DAGs the basic unit of equivalence preserving transformation is  covered  edge reversal  Chickering        In the current paper we treat an edge reversal as simply a special case of two consecutive mark changes  That is  a reversal of A  B is simply to first add an arrowhead at A  to form A  B   and then to drop the arrowhead at B  to form A  B   An edge reversal is said to be legitimate if both of the two consecutive mark changes are legitimate  Given Lemma    it is straightforward to check the validity of the following conditions for legitimate edge reversal   We use PaG  SpG to denote the set of parents spouses of a vertex in G   Lemma    Let G be an arbitrary DMAG  and A  B an arbitrary directed edge in G  The reversal of A  B is legitimate if and only if PaG  B    PaG  A    A  and SpG  B    SpG  A   When there is no bi directed edge in G  that is  when G is a DAG  the condition in Lemma   is reduced to the familiar definition for covered edge  i e   PaG  B    PaG  A    A   Chickering        The condition given by Drton and Richardson        for an edge in a bidirected graph to be orientable in either direction  SpG  B    SpG  A   can be viewed as another special case of the above lemma       THE MAIN RESULT  We first state two intermediate theorems crucial for the main result we are heading for  The first one says if the differences between two Markov equivalent DMAGs G and G   are all of the following sort  a directed edge is in G while the corresponding edge is bi directed in G     then there is a sequence of legitimate mark changes that transforms one to the other  The second one says that if every bi directed edge in G and every bi directed edge in G   are invariant  then there is a sequence of legitimate mark changes  edge reversals  that transforms one to the other  The proofs follow the strategy of Chickerings proof for DAGs  Theorem    Let G and G   be two Markov equivalent DMAGs  If the differences between G and G   are all of the following sort  a directed edge is in G while the   corresponding edge is bi directed in G     then there is a sequence of legitimate mark changes that transforms one to the other  Proof Sketch  We prove that there is a sequence of transformation from G to G     the reverse of which will be a transformation from G   to G  Specifically we show that as long as G and G   are different  there is always a legitimate mark change that can eliminate a difference between them  The theorem then follows from a simple induction on the number of differences  Let Diff    y  there is a x such that x  y is in G and x  y is in G     By the antecedent condition  Diff exhausts all the differences there are between G and G     So the two graphs are identical if and only if Diff     We claim that if Diff is not empty  there is a legitimate mark change that eliminates a difference  Choose B  Diff such that no proper ancestor of B in G is in Diff   Let Diff B    x x  B is in G and x  B is in G     Since B  Diff   Diff B is not empty  Choose A  Diff B such that no proper descendant of A in G is in Diff B   The claim is that changing A  B to A  B in G is a legitimate mark change  that is  it satisfies the conditions stated in Lemma    The verifications of conditions  t   and  t   in Lemma   take advantage of the specific way by which we choose A and B  For example  if condition  t   were violated  i e   if there were a directed path d from A to B other than A  B  then in order for G   to be ancestral  d would not be directed in G     which implies that some edge on d would be bi directed in G     It is then easy to derive a contradiction to our choice of A or B in the first place  The verification of  t   is similarly easy  which uses the fact that G and G   have the same unshielded colliders   To show that  t   also holds  suppose for contradiction that there is a discriminating path u   hD       C  A  Bi for A in G  By Definition    C is a parent of B  It follows that the edge between A and C is not A  C  for otherwise A  C  B would be a directed path from A to B  which has been shown to be absent  Hence the edge between C and A is bidirected  C  A  because C  Definition    is a collider on u   Then the antecedent of the theorem implies that C  A is also in G     Moreover  the antecedent implies that every arrowhead in G is also in G     which entails that in G   every vertex between D and A is  still a collider on u  It is then easy to prove by induction that every vertex between D and A on u is also a parent of B in G    using the fact that G   is Markov equivalent to G   and hence u is also discriminating for A in G     But A is a collider on u in G   but not in G  which contradicts  e   in Proposition    Obviously a DMAG and any of its LEGs satisfy the antecedent of Theorem    so they can be transformed to each other by a sequence of legitimate mark changes  Steps      in Figure    for example  portraits a stepwise transformation from G  to H   Theorem    Let G and G   be two Markov equivalent MAGs  If every bi directed edge in G is invariant and every bi directed edge in G   is also invariant  then there is a sequence of legitimate mark changes that transforms one to the other  Proof Sketch  Without loss of generality  we prove that there is a transformation from G to G     Let Diff    y  there is a x such that x  y is in G and x  y is in G     By the antecedent  Diff exhausts all the differences there are between G and G     If Diff is not empty  we can choose an edge A  B by exactly the same procedure as that in the proof of Theorem    The claim is that reversing A  B is a legitimate edge reversal  that is  a couple of legitimate mark changes   i e   satisfies the conditions in Lemma    The verification is fairly easy  Note that A  B  by our choice  is a reversible edge in G  for A  B is in G     which is Markov equivalent to G   It follows directly from Proposition    and the assumption about bi directed edges in G  that SpG  B    SpG  A   The argument for PaG  B    PaG  A    A  is virtually the same as Chickerings proof for DAGs  Lemma    in particular  in Chickering        Note that after an edge reversal  no new bi directed edge is introduced  so it is still true of the new graph that every bi directed edge is invariant  Hence we can always identify a legitimate edge reversal to eliminate a difference in direction as long as the current graph and G   are still different  An induction on the number of differences between G and G   would complete the argument  Since a LEG  of any MAG  only contains invariant bidirected edges  two LEGs can always be transformed to each other via a sequence of legitimate mark changes according to the above theorem  For example  steps     in Figure   constitute a transformation from H   a   LEG of G   to H   a LEG of G    Note that Chickerings result for DAGs is a special case of Theorem    where bi directed edges are absent   X   X  X   X   X   X   We are ready to prove the main result of this paper  Theorem    Two DMAGs G and G   are Markov equivalent if and only if there exists a sequence of single mark changes in G such that    after each mark change  the resulting graph is also a DMAG and is Markov equivalent to G     after all the mark changes  the resulting graph is G    X   X   X   step    G    X   X  X   X   X   X   X   As a simple illustration  Figure   gives the steps in transforming G  to G  according to Theorem    That is  G  is first transformed to one of its LEGs  H   H  is then transformed to H   a LEG of G   Lastly  H  is transformed to G   Theorems   and    as they are currently stated  are special cases of Theorem    but their proofs actually achieve a little more than they claim  The transformations constructed in the proofs of Theorems   and   are efficient in the sense that every mark change in the transformation eliminates a difference between the current DMAG and the target  So the transformations consist of as many mark changes as the number of differences at the beginning  By contrast  the transformation constructed in Theorem   may take some detours  in that some mark changes in the way actually increase rather than decrease the difference between G and G      This is not the case in Figure    but if  for example  we chose different LEGs for G  or G   there would be detours   We believe that no such detour is really necessary  that is  there is always a transformation from G to G   consisting of as many mark changes as the number of differences between them  But we are yet unable to prove this conjecture   X   X   X   step    step    H   X   Proof  The if part is trivial  since every mark change preserves the equivalence  the end is of course Markov equivalent to the beginning  Now suppose G and G   are equivalent  We show that there exists such a sequence of transformation  By Proposition    there is a LEG H for G and a LEG H  for G     By Theorem    there is a sequence of legitimate mark changes s  that transforms G to H  and there is a sequence of legitimate mark changes s  that transforms H  to G     By Theorem    there is a sequence of legitimate mark changes s  that transforms H to H    Concatenating s    s  and s  yields a sequence of legitimate mark changes that transforms G to G      X   step    X   X   X   X   X   X   X   X   X   step    H    step    G    Figure    A transformation from G  to G      Conclusion  In this paper we established a transformational property for Markov equivalent directed MAGs  which is a generalization of the transformational characterization of Markov equivalent DAGs given by Chickering         It implies that no matter how different two Markov equivalent graphs are  there is a sequence of Markov equivalent graphs in between such that the adjacent graphs differ in only one edge  It could thus simplify derivations of invariance properties across a Markov equivalence class  in order to show two arbitrary Markov equivalent DMAGs share something in common  we only need to consider two Markov equivalent DMAGs with the minimal difference  Indeed  Chickering        used his characterization to derive that Markov equivalent DAGs have the same number of parameters under the standard CPT parameterization  and hence would receive the same score under the typical penalized likelihood type metrics   The discrete parameterization of DMAGs is currently under development    We think our result will prove useful to show similar facts once the discrete parameterization is available     Drton and Richardson        provide a parameterization for bi directed graphs with binary variables  for which the problem of parameter equivalence does not arise because no two different bi directed graphs are Markov equivalent    The property  however  does not hold exactly for general MAGs  which may also contain undirected edges    A simple counterexample is given in Figure    When we include undirected edges  the requirement of ancestral graphs is that the endpoints of undirected edges are of zero in degree  that is  if a vertex is an endpoint of an undirected edge  then no edge is into that vertex  see Richardson and Spirtes        for details   So  although the two graphs in Figure   are Markov equivalent MAGs  M  cannot be transformed to M  by a sequence of single legitimate mark changes  as adding any single arrowhead to M  would make it nonancestral  Therefore  for general MAGs  the transformation may have to include a stage of changing the undirected subgraph to a directed one in a wholesale manner   A  A  
 In a causal graphical model  an instrument for a variable  X  and its effect Y is a ran  dom variable that is a cause of  X  and in  dependent of all the causes of Y except  Pearl         X  h  For continuous variables  in    strumental variables can be used to estimate  r  how the distribution of an effect will respond to a manipulation of its causes  even in the presence of unmeasured common causes  con founders    f  In typical instrumental variable     s  z     x    Y  estimation  instruments are chosen based on domain knowledge   fy  There is currently no  statistical test for validating a continuous variable as an instrument   In this paper   we introduce the concept of semi instrument  which generalizes the concept of instrument   Figure    One Instrumental Variable  each instrument is a semi instrument  but the converse does not hold   We show that in  the framework of additive models  under cer tain conditions  we can test whether a vari able is semi instrumental  Moreover  adding some distribution assumptions  we can test whether two semi instruments are instrumen  tal   We give algorithms to test whether a   variable is semi instrumental  and whether two semi instruments are both instrumental  These algorithms can be used to test the ex perts  choice of instruments  or to identify in struments automatically   random variable on  X  X  is the cause of Y  then intervening  will change Y  but intervening on Y should not  change X    By regressing Y on effect of the intervention of  X  X   we can estimate the  on Y  up to a constant   if no unmeasured common cause of  X  and Y exists   However  this advantage becomes problematic if we know  or even suspect  that there are some unmea sured common causes of  X  and Y  in which case we  cannot estimate the direct effect of  X  on Y   Consider the the causal structure illustrated in figure    Suppose that Z is not observable for the moment   Key Words  Causality  Instrumental Variables  Among  X   Y  Ex  and fy  only  X andY  are observed  variables  The functional relations among them are      INTRODUCTION  One of the major advantages of knowing the causal relations among the variables we are interested in is that we can use the causal structure to predict the effects of interventions  For example  if we know that a    Here by intervention we mean the manipulation of the value of one random variable  with the assumption that this manipulation can affect other variables only through the manipulated variable  We also assume no feedback  i e   no directed cycles  and no reversible causal relations  in a causal graph  See Spirtes et al          CHU ET AL          UAI      PRIOR WORK  There has been intensive study of the use of instru ments in the econometric literature  Much work has focused on the study of the efficiency of an instrument  h   Nelson et al       Shea       and Zivot et al                  instrument  l  s  X  z  strument for  ey  r  f  Z  is an in X and Y  provided we already have an Z  for X and Y  Wu       Newey        There are studies of whether a variable  and Magdalinos        but how to find the first in strument is still an open problem   There is no test  available to find out whether a continuous variable is  y      t   an instrument  without knowing that some other vari    able is an instrument   In practice  people usually resort to domain knowledge  g  to determine whether a variable is an instrument  As some studies show  moreover  it is often very difficult to find instruments  and a wrong choice may signifi cantly affect the final result  Bartels       Heckman  Figure    One Common Cause        and Bound et al        Therefore  even in the case where we do have some domain knowledge to help us identify instruments  some kind of testing procedure is still useful in that it can serve as a double check  It  X Y E t Yitx  Note that  ey  X  and           f Z   ex s X   ey h Ex          E EyiX     X   The  are dependent  that is   will be a non constant function of  X   say   regression of Y on X will give   E YIXJ     s  X      E EyjX      s X    X  E XIZ    X  on  Z     APPROACH  W ithin the framework of linear models  the fact that a variable is an instrument imposes no constraint on the Consider the causal structure illustrated in figure     even up to a constant   However  with the help of variable Ex by regressing  variable is an instrument   joint marginal distribution of the observed variables   s  X       X   Because we have no way to estimate    X   we also have no way to identify  is the goal of this paper to find out whether  under certain reasonable conditions  we can test whether a  Z  we can estimate  where only  Z  X  and Y  to get an estimate of ex    Then  we can regress Y on X and EX to       s X   s X    E EY Ex     E t yiEx X     s X    h Ex      where f and  An additive regression method will give an estimate of and h Ex  simultaneously     s X  Here  Z  is called an instrumental variable for  Y  by which we mean that and  X   Z  Z  X  and  is a direct cause of  is independent of all other causes of  Y  X   except   Note that X must be a direct cause of Y    Here we use the fact that X and  conditional on figure     Ex   In  particular  we assume that   get   E YIX t x   are observable  Assume for  now that all the functional relations are linear   Ey  are independent  which is implied by the causal graph in   For the proof of the identifiability of constant  see Newey et a           s X    up to a  s are  X     fZ cx  Y     sX  t y           non zero constants   Now we construct another model based on the causal     Let g  t x  Let Z   structure illustrated in figure         s   g     and ey     Ey   y  g        c           z  X   X    Pearl        shows that  for discrete data  Z being an instrument for X and Y imposes some constraint  which is called by him as instrumental inequality  on the joint dis tribution of Z  X  andY  This inequality is not sufficient for Z to be an instrument  and  as pointed in this paper  it cannot be extended to the continuous models  This paper does not give an instrument testing procedure for discrete data    Y  gf Z    s  X    f  Clearly Y  Y  hence  Z X Y  has the same distribution as  Z  X  Y    However  we notice that in the first case  Z is an in strument for X and Y  while in the second case  Z  is not an instrument  but a common cause of X  and Y  Actually  we can set g  to be any value  and adjust s  X and Y  Y  This and E accordingly so that X  and              means that  a joint distribution implied by a model  where      CHU ET AL   UAI      Z  is an instrument could also be implied by  uncountably many other models where  Z  is not an in  strument  Based on the joint distribution  we cannot tell whether  Z  figure represents the correct model  Note that a linear model is a special case of the ad ditive model  Thus  given that we cannot determine whether a random variable is an instrument in a linear model  we could not  in general  determine whether it is an instrument in an additive model  Therefore  we should look for some other conditions  preferably a lit tle bit weaker than those required by an instrument  so that a random variable that satisfies these conditions can be identified in an additive model  One such candidate set of conditions is what we call  is an instrument   The above analysis suggests two possible approaches for instrument testing  First  we may extend the space of linear structural models to a larger space  The idea is that the space of linear structural models already im p oses strong constraints on each variable so that being an instrument does not impose any extra constraints   However  i n a larger space  it is possible that being an instrument does imply some conditions not neces sarily satisfied by all models in that space  Another approach is to get more candidates for instrumental variables  and see whether the instrument assumption imposes some constraint on the joint distribution of these candidate variables  In this section we shall try a combination of both ap  the semi instrumental conditions  A random variable is a  Z  semi instrument  for  X     Z is an exogenous variable in M     Z is the cause of X  and X is a cause of Y in M      If Z is also a cause of Y  then the direct effect of Z on Y is a linear function of the direct effect of Z on X  Note that if a random variable  ment variables  In the first stage of the test  we shall exclude some  but not necessarily all  non instruments   g Z    In the second stage of the test  we shall determine  X  say   whether all the remaining ones are instruments  pro  if the following     The joint distribution of Z  X  andY can be rep resented by an additive model M consisting only of Z  X  and Y as the observed variables   for X and  vided we still have at least two candidates left   Y  conditions are satisfied   proaches  That is  we shall propose a two stage proce dure  Suppose we are given a set of candidate instru  and  Y   Z is  a semi instrument  then the direct effect of  Z  on  Y   say   is a linear function of the direct effect of Z on  f Z   That is  there is a pair of real numbers that g Z    af  Z    b  We shall call a the linear coefficient of the semi instrument Z   a  b  such  It is easy to see that the semi instrumental assump      SEMI INSTRUMENTAL VARIABLES  tion is weaker than the instrumental assumption  All  FOR THE ADDITIVE  instruments are semi instruments  with a linear coeffi  NONPARAMETRIC MODELS  cient     but not all semi instruments are instruments  Moreover  in general  in a linear model  an exogenous  Here by an additive nonparametric model we mean  Z that is a common cause of X  that  for each endogenous variable  its expectation  be an instrument  is a semi instrument for X andY   and Y  which could not  given its parents is a linear combination of univari  because both its effect on X  i e   f  and its effect on  ate functions of its parents  plus some error term with  Y   unknown distribution  We further assume that all ex  ing the space of linear models to the space of additive  i e   g  are linear functions  Therefore  by extend  ogenous variables are independent  and that they are  models  we find that the instrument assumption does  independent of all the error terms  We do allow de  impose some constraints on the possible kinds of ef  pendence between the error terms of  X  and  Y   Later  fects of  Y  Z  on Y  Only models where the effect of  Z  on  nonparametric model interchangeably   compatible with the distribution implied by a model  Figure   and figure     give two very simple additive  models  We can take them  as  two alternative hypothe  ses about the underlying model that generates a sam ple with observable variables lem of testing whether  Z  Z   X  and Y  The prob  is an instrument for X and  Y is equivalent to the problem of determining which  where  is a linear function of the effect of  Z  in this paper we shall use additive model and additive  Z  on X are  is an instrument   To test whether a random variable  Z  is a semi  instrument for X and Y  theoretically we should check whether all the four conditions are satisfied   How  ever  it turns out that not all these four conditions are testable   For example  the second condition  i e   Z   CHU ET AL       UAI      is an exogenous variable  cannot be tested if we only have the joint distribution of  X  Y   and  Z   From the  joint distribution only  there is no way to tell whether  Z  and the error term associated with  X   say   t x   dependent  On the other hand  we do have many cases where it is reasonable to assume that the first three conditions are satisfied  For example  it is typical that when testing whether  Z  is an instrument for  X and  Ey  r  are  Y  we are deal     s     Y  ing with an additive model described by conditions   to    and our question is whether  Z  is also a direct  cause ofY  Assuming that the first three conditions are satisfied  under certain smoothness conditions  to test whether the fourth condition is also satisfied is equivalent to testing whether  E YjX  t x  is a linear combination of X and a univariate function  Figure  a univariate function of      Two Common Causes  of t x  Theorem    Consider the additive model given by fig ure    Suppose that  X  t x  has a joint density  and that all the functions  i e   J  g  s  and h  are differen tiable    Then E YIX  t x  is a linear combination of a univariate function of X and a univariate function oft x  and Var YjZ t x    Var YIE XIZ  t x   if and only if g Z    af Z    b for some pair of real numbers  a  b       If  The second step is the measurability test for the null hypothesis that Var   Y j Z  t x     Regress  X     Regress  Y on Z and t x  The above theorem suggests an algorithm  which we  semi instrument testing algorithm   will call the  Z      to test  test has two steps  the first step is the additivity test         Regress  to estimate  Yon X and Z  t x  X E XjZ   with a surface smoother   and score the fit of this model with some score function that penalizes model complexity       Regress  Y  on  X  and  t x   with a surface smoother   t x  with an additive  smoother  and score the fit of this model with some score function that penalizes model com plexity  s These two conditions are much stronger than what we need  Actually  we only need to assume the boundary of the support of the  X  t x   has no positive probability   see proof of Theorem     in Newey et a           and that all the functions are absolutely continuous  We choose these two conditions because they are more intuitive   Regress Y on  E XIZJ  and  t x  with a surface  If the regression of Y on  Z  and fX has a smaller  hypothesis  Otherwise  accept it          X on Z  and  sum of residuals  i e   RA   RN  reject the null  X and a univariate  t x   Regress  E XjZ   to estimate  uals   for the null hypothesis thatE  Yj X  t x  is a linear com bination of a univariate function of      Z  smoother  and let RN be the sum of the resid  is a semi instrument for a sample S gener  ated from an additive model given by figure    This  function of  on    Var YIE XIZJ  t x    and let RA be the sum of the residuals   For the proof  see the appendix   whether  the additive model has a better score  accept  the null hypothesis  Otherwise  reject it   TWO SEMI INSTRUMENTS  If the test for whether a random variable  say  zl  is a semi instrument for  X andY  gives a negative result   there is not much left to do with  Z    However  if the  test says that zl is a semi instrument  we will face another problem  Is  Z   an instrument   We have pointed out that this question cannot be an swered if we only have the joint distribution of  Z   X   andY  However  from the Bayesian point of view  with some further assumption  if there is a second semi instrument  say  z   we might be able to determine   Here we might also want to do a bootstrap estimation of the distribution of RN  RA  If we adopt this approach  we could generate bootstrap samples by adding permuta tions of the residuals obtained by regressing Y on E XID  and ex to the fitted values ofY obtained from the same regression        CHU ET AL   UAI      whether Z  and Z  are both instruments   If not both of them are instrument  we would not be able to tell whether both are non instrumental  or only one is non instrumental    The following theorem gives the condi tion when two semi instruments are both instruments almost surely  Let Z  and Z  be two independent ran dom variables that are both semi instruments for X and Y  Let a  and a  be the linear coefficients of Z  and Z  respectively  Suppose a  and az are indepen dent  and each has a distribution function that has one and only one point of discontinuity       If Z  and Z  have the same linear coefficients  then with probability    Z  and Z  are both instruments   Theorem    For the proof  see the appendix  Assume that the sample S was generated from the causal structure illustrated in figure    and that both Z  and Z  are semi instruments for X andY  We can use the following algorithm  called the double instru ments testing algorithm  to test whether Z  and Z  have the same linear coefficient       Create a new variable Z ft  Z     h Z    where ft Zt     E XIZ       Z     E XIZ          Test whether Z is a semi instrument  This algorithm is based on the following observation  Assume that g  and gz are differentiable  and that Z  and Z  have a joint density  Then g  ZI       Z     a ft Z     h  Z      b for some  a b  iff g  Z     aft  Z     b  and g  Z     af  Z     b  for some b    b   b     testing and double instrument testing  It turns out that  in order for the semi instrument testing to work  we need to find a better additive regression method that can handle dependent predictors  which seems currently not available  However  the double instruments testing algorithm does work for a subset of additive models  the models where the influence of X on Y is linear  This subset includes a very impor tant class of models  the linear models          SEMI INSTRUMENT TESTING  The semi instrument testing algorithm requires a sur face smoother and an additive smoother  We use the Splus functions loess as the surface smoother  and gam as the additive smoother  The gam function im plements the back fitting algorithm proposed in Hastie et al         The loess function is an implementation of the local polynomial regression  We use BIC score function to score the fitted models returned by gam and loess respectively  We generate   samples from the following   models   X   Z   t x   Y    X   ciZ  where E t ylt x       i   c      gam  and  SIMULATION  size                              We have done some simulation studies to estimate the performance of the two algorithms for semi instrument   Note that here by imposing a distribution on a  and a   which are actually parameters of our models  we have adopted a Bayesian perspective  Also  the conditions for the distribution are stronger than required  What we really  need is to ensure that the P at    a     P at   a            That is  we want to assume that it is possible that a    a   and if a    a   it is almost sure that a    a       which means that both Zt and Z  are instruments    Note that from the classical point of view  this algo rithm can be used to reject the null hypothesis that Zt and Z  are both instruments in the cases where Zt and Z  have different linear coefficients  However  to make it a testing algorithm for double instruments  a certain Bayesian as sumption about the prior distributions of the linear coeffi cients of Zt and Z   like the one proposed in Theorem    is required    The proof of this observation is similar to that of The orem      o      and c        These two models share the same Z  t x  X  and fy  differ in the effect of Z on Y  and hence differ in Y  In the first model  Z is an instrument  hence a semi instrument  In the second model  Z is not a semi    For each model  we generated   sam instrument  ples with sizes            and      respectively  Table       Ey  model              loe ss gam  models comparison  BIC                                             loess  BIC                                            From the above data  we can see that the gam model is     The second algorithm works for the models where the inftuence of X on Y is linear because in this case we can modify the algorithm so that we do not need to apply ad ditive regression method to models with dependent predic tors  For a detailed discussion  see section          The distribution of these variables are  Z is uniform  between   and    There is also a latent variable T that is uniform between   and    EX is the sum ofT and a normal noise with standard deviation      Ey is the sum of T  and a normal noise with standard deviation        CHU ET AL       always much worse than the loess model  no matter whether  Z  to get  is a semi instrument or not  This implies     Let Z  that no matter whether the null hypothesis is true   Z  i e    The most plausible explanation of  tx  and  X   the performance of gam is signif     with some further conditions  we could still make the double instruments testing algorithm work  Consider  Zr Z  are semi instruments  we further assume that s X    eX   d  i e   the direct effect of X on Y is a linear function in X  then we will be able to test whether Z  and Z  have the same linear coefficients  the rnodel given by figure    If besides assuming  and  a h Z     b    and do   eX d   ty     Y  Z   Z    where  Z   Z   Y     Let Z      t Zt    h Z     Var YIZr   Z        Var   ctx   ty      are  ar  ctx     models   E tyJtx     Ey    t  and Ct        e           and C          y   Z   tx  X  and z   and z   model  Z  and Z   but differ in the linear coefficients of  and hence differ in  Y   In the first  have the same linear coefficients  In the second model  there is a small difference in the linear coefficients  In     For  Table    Values of BICa  BlOt for the    samples sample size  Ct                  C          C                                                                                                            The entries of the above table are the values of BIC BIGa for each sample   Var  ar  t  Zt    a    Z  jZ    A positive  value means that the  null hypothesis is accepted for that sample  From the table  we can see that to detect the significant differ  a      ence between the linear coefficients of  a    double instruments testing algorithm   Zt  and  Z    a  sample of size    is sufficient  But when the difference  The following algorithm  which is called the  linear  is small  we need a sample size of       compares the  mean square error of regressing Y on  Z   Z   Z    and  with the mean square error of regressing Y on  DISCUSSION AND FUTURE     WORK  It can be used to test whether the two  semi instruments Z  and Z  have the same linear coef  ficients  assuming the direct effect of in     it is easy to see that   with the equality holds only when  E  X IZt   Z     have the same linear coefficients if  These three models share the same Z r   ty   do   Var YjZ      where     Var etx   ty     Z   Yi   X   CiZi  we further have    c   ai  ft Zr    h Z     and  X  z    Zi  Ex  r y   If Z  and Z  have the same linear coefficient  i e    a       using additive nonpara       and     respectively   independent of each other  and jointly independent of r x an  Z   each model  we generated   samples with sizes            r Z l        Z    is additive in  and  the third model  the difference is significant      c   at ft ZI     c   a  h Z     ctx   ty   do  That is   Z   from the following  Despite the lack of good additive regression method   Y  Z   on  To test the above algorithm  we generated    samples  DOUBLE INSTRUMENTS TESTING     Y  BIC    BICa   and in some cases fails to converge      Zr    atfr Zr    br  g  Z     br   b    d we have   Regress Yon Z using linear  BICa   back fitting algorithm often gets trapped in a plateau   Let  E XJZr  Z     metric regression  and compute the BIC score  icantly worse than that of loess  It seems that the             Regress  this phenomenon is that because of the dependence between  E XJZr  Z     regression  and compute the BIC score BIC    is a semi instrument  the test procedure will  always reject it   UAI      X  on Y is linear  X        THREE ASSUMPTIONS  The semi instrument testing algorithm assumes that the first three semi instrumental conditions are sat       Use additive regression to regress X on Zt and  Z       Note that by Theorem    under certain distribution assumptions  a    a  implies that a    a      w p l   i e    both zl and z  are instruments     The distribution of these variables are  Z  and Z  both  are uniform between   and    There is also a latent variable  T that is uniform between   and     X  is the sum ofT and  a normal noise with standard deviation      y is the sum of T  and a normal noise with standard deviation        UAI       CHU ET AL   isfied  While in general we cannot test whether a random variable satisfies all the first three semi instrumental conditions  it is interesting to know whether we can test for one of them  especially the second semi instrumental condition  Z is an exogenous variable  The answer is  in principle  this assumption can be tested by the method of instrument  if we have an instrument for Z and X  However  it is easy to see that this will lead to an infinite regression  Another assumption key to the double instruments testing algorithm is  The prior probability that Z is instrumental is positive  while the prior probability is   for a semi instrument to have linear coefficient a if a            This raises a question  Why does the value   have a special status in the range of possible values of the linear coefficients of a semi instrument  Here we want to give an argument for the plausibility of this assumption  If we take the set of possible causal structures among X  Y  Z  and Z  as a discrete sample space  it is reasonable to assign a positive prior prob ability to one element in the space  i e   the structure where both Z  and Z  are instruments  which means that both Z  and Z  have linear coefficients    On the other hand  if a semi instrument is not an instrument  there is no specific reason to believe that its linear co efficient should take any specific non zero value  We make a third assumption in an effort to modify the double instruments testing algorithm so that it has suf ficient power  We assume that the direct effect of X on Y is a linear function of X  We notice that this is a rather strong assumption for an additive model  More over  because currently we do not have a suitable addi tive regression method for the semi instrument testing  we also have to assume  without any testing  that Z  and Zz are semi instruments  Nevertheless  the mod ified double instruments testing algorithm is general enough to provide a double instruments test for linear models           FUTURE WORK  To make the semi instrument testing powerful  we will continue to look for some additive regression method that is suitable for the case where the predictors are dependent     Alternatively  we may also try to find some new ways of testing semi instruments where the problem of the dependence of the predictors will not significantly affect the test results     Tom Minka suggested that by letting the back fitting algorithm run for a sufficient number of iterations  it will eventually return a good fitted model  or we can use least  squares to the the best fitted model directly   without iter ations     We have yet to test whether this will work       Acknowledgments  This paper was supported by NSF grant DMS         Reference      Bartels  L           Instrumental and  Quasi Instrumental  Variables   in American Journal of Po litical Science                   Bound  J   Jaeger  D   Baker  R           Prob lems with Instrumental Variables Estimation When the Correlation Between the Instruments and the En dogenous Explanatory Variable is Weak   in Journal of the American Statistical Ass oc iat ion  Vol                  Fennessey  J   d Amico  R           Collinearity  Ridge Regression  and Investigator Judgment   in So ciological Methods and Research                 Gozalo  P   Linton             Testing Additiv ity in Generalized Nonparametric Regression Models with Estimated Parameters   forthcoming in Journal of Econometrics       Hastie  T   Tibshirani  R          Generalized Ad ditive Models  New York   Chapman and Hall      Heckman  J           Instrumental Variables  A Cautionary Tale   NBER Technical Working Paper No            Magdalinos  M           Testing Instrument Ad missibility  Some Refined Asymptotic Results   in Econometrica Vol                  Nelson  C   Startz  R           The Distribution of the Instrumental Variables Estimator and Its t Ratio when the Instrument is a Poor One   in Journal of Business  vol      S    S         Newey  W           Generalized Method of Mo ments Specification Testing   in Journal of Economet rics  Vol                    Newey  W   Powell  J   Vella  F            Nonpara metric Estimation of Triangular Simultaneous Equa tions Models   in Econometrica Vol                     Pearl  J          On the Testability of Causal Models with Latent and Instrumental Variables   in P  Besnard and S  Hanks  Eds    Uncertainty in Ar tificial Intelligence              San Francisco  CA  Morgan Kaufmann      Shea  J           Instrument Relevance in Multi variate Linear Models  A Simple Measure   in Review of Economics and Statistics Vol                    Spirtes  P   Glymour  C  and Scheines  R            CHU ET AL       Causation  Prediction  and Search    nd ed  New York   N Y   MIT Press        Wu  D           Alternative Tests oflndependence Between Stochastic Regressors and Disturbances   in Econometrica Vol                    Zivot  E   Startz  R   Nelson  C            Valid Confidence Intervals and Inference in the Presence of Weak Instruments   in International Economic Re view  Vol               Appendix Proof of Theorem      Because  X  Ex  has a joint density  and j  g  s   and h are differentiable  it immediately follows that E Y X   x  Ex   u  is differentiable with respect to X and t x with probability one  Also note that E Y X     x  Ex  u    E s X   g Z    t y X  x  EX   u     s x   E g Z  j Z      x    u      h u    because conditional on f Z   X Ex  x is independent of X  x and t x  u    u   g Z   It is easy to see that if g Z  aj Z   b  i e   the direct effect of Z on Y is a linear function of the direct effect of Z on X  then with probability one  E Y X  t x    s X   aX   h Ex   at x   b  i e   E Y X  t x  is a linear combination of a univariate function of X and a univariate function of t x  Moreover  we have     Var Y E X Z   Ex   Var s j Z   EX   g Z   Ey lf Z   Ex         Var t ylf Z   ex      Var t yjt x   Var Ey Z t x    Var Y Z t x   To show the converse  suppose  E Y X     x t x  u   Let g  x u         s   x   h  u    E g Z  f Z     x      we have   u     go x u    d    o x u  d  sl   x   s x   x ax          nb   x u  is constant in u                ub is a constant            go is a linear function  Note that the assumption that Var YjE XjZ   t x    Var Y Z ex  implies that Var g Z  j Z        which again implies that g Z  E g Z  j Z   w p l  There fore  we have     g Z      af Z   b  where a  bare constants   UAI      Proof of Theorem      Let L  be the linear coefficient of Z    L  the linear coefficient of Z   and f lL  the distribution of    Then   P L t P Lt              O Lt     L   L                     P Lt L          for  P L  L      fn   O  P L            lt df lL   lt           
 An important task in data analysis is the discovery of causal relationships between observed variables  For continuous valued data  linear acyclic causal models are commonly used to model the data generating process  and the inference of such models is a wellstudied problem  However  existing methods have significant limitations  Methods based on conditional independencies  Spirtes et al        Pearl       cannot distinguish between independence equivalent models  whereas approaches purely based on Independent Component Analysis  Shimizu et al        are inapplicable to data which is partially Gaussian  In this paper  we generalize and combine the two approaches  to yield a method able to learn the model structure in many cases for which the previous methods provide answers that are either incorrect or are not as informative as possible  We give exact graphical conditions for when two distinct models represent the same family of distributions  and empirically demonstrate the power of our method through thorough simulations      INTRODUCTION  In much of science  the primary focus is on the discovery of causal relationships between quantities of interest  The randomized controlled experiment is geared specifically to inferring such relationships  Unfortunately  in many studies it is unethical  technically extremely difficult  or simply too expensive to conduct such experiments  In such cases causal discovery must  Gustavo Lacerda Machine Learning Department Carnegie Mellon University Pittsburgh  PA  USA Shohei Shimizu Osaka University Japan  be based on uncontrolled  purely observational data combined with prior information and reasonable assumptions  In cases in which the observed data is continuousvalued  linear acyclic models  also known as recursive Structural Equation Models  have been widely used in a variety of fields such as econometrics  psychology  sociology  and biology  for some examples  see  Bollen        In much of this work  the structure of the models has been assumed to be known or  at most  only a few different models have been compared  During the past    years  however  a number of methods have been developed to learn the model structure in an unsupervised way  Spirtes et al        Pearl       Geiger and Heckerman       Shimizu et al         Nevertheless  all approaches so far presented have either required distributional assumptions or have been overly restricted in the amount of structure they can infer from the data  In this contribution we show how to combine the strenghts of existing approaches  yielding a method capable of inferring the model structure in many cases where previous methods give incorrect or uninformative answers  The paper is structured as follows  Section   precisely defines the models under study  and Section   discusses existing methods for causal discovery of such models  In Section   we formalize the discovery problem and give exact theoretical results on identifiability  Then  in Section   we introduce and analyze a method termed PClingam that combines the strenghts of existing methods and overcomes some of their weaknesses  and is  in the limit  able to estimate all identifiable aspects of the underlying model  Section   provides empirical demonstrations of the power of our method  Finally  Section   maps out future work and Section   provides a summary of the main points of the paper       a  LINEAR MODELS  In this paper  we assume that the observed data has been generated by the following process     The observed variables xi   i            n  can be arranged in a causal order  such that no later variable causes any earlier variable  We denote such a causal order by k i   That is  the generating process is recursive  Bollen        meaning it can be represented graphically by a directed acyclic graph  DAG   Pearl       Spirtes et al            The value assigned to each variable xi is a linear function of the values already assigned to the earlier variables  plus a disturbance  noise  term ei   and plus an optional constant term ci   that is X bij xj   ei   ci       xi   k j  k i   where we only include non zero coefficients bij in the equation     The disturbances ei are all continuous random variables with arbitrary densities pi  ei    and the ei are Q independent of each other  i e  p e            en     i pi  ei    This formulation neither requires the disturbances to be normally distributed nor does it require them to have non Gaussian  non normal  densities  In general  some of the distributions can be Gaussian and some not  and we do not a priori know which are which  We assume that we are able to observe a large number of data vectors x  which contain the variables xi    and each data vector is generated according to the above described process  with the same causal order k i   same coefficients bij   same constants ci   and the disturbances ei sampled independently from the same distributions  Note that the independence of the disturbances implies that there are no unobserved confounders  Pearl        Spirtes et al         call this the causally sufficient case  Finally  we assume that the observed distribution is faithful to the generating graph  Spirtes et al         i e  the model is stable in the terminology of Pearl         If the model parameters are in some sense randomly generated  this is not a strong assumption  as violations of faithfulness have Lebesgue measure   in the space of the linear coefficients  An example of such a model is given in Figure  a  Note that the full model consists of a directed acyclic graph over the variables  the connection strenghts bij   the constants ci   and the densities pi  ei    In this example we have chosen ci     for all i  so these are not shown   b x  c  d  x  x  x  x  x  y  y  y  y  y  z  z  z  z  z     y     z  Figure    An example case used to illustrate the concepts described in Sections      a  A linear  acyclic causal model for x  y and z  The data is generated as x    ex   y     x   ey   and z     y   ez   with ex and ey drawn from Gaussian distributions and ez from a non Gaussian distribution  and ex   ey and ez are all mutually independent  Note that we show variables with Gaussian disturbances using circles whereas variables with non Gaussian disturbances are marked by squares   b  The three directed acyclic graphs over x  y and z which all entail the same conditional independence relationships as the generating model   c  The three DAGs in  b  succintly represented as a dseparation equivalence pattern   d  The distributionequivalence pattern of the original model      EXISTING METHODS  Given our set of data vectors x  to what extent can we estimate the data generating process  Obviously  if the number N of data vectors is small  estimation may be quite unreliable  Therefore we will here mainly focus on the theoretical question  To what extent  and with what methods  can we identify the true model in the limit as N    The most well known approach to inference of this type of causal networks is based on  conditional  independencies between the variables  Spirtes et al        Pearl        When  as in our case  there are assumed to be no hidden confounding variables and no selection bias  one can in the large sample limit identify the set of networks which represent the same independencies as the true data generating model  To illustrate  in Figure  b we show all three DAGs which imply the set of independencies produced by the true model  This set is known as the d separation equivalence class  and is often represented in the form of a d separationequivalence pattern  a partly directed graph in which undirected edges represent edges for which both directions are present in the equivalence class  Spirtes et al         as illustrated in Figure  c  We want to emphasize that  using conditional independence information alone  it is impossible to distinguish between members inside a d separation equivalence class because these  by definition  represent the same set of conditional   independencies between the observed variables  Fortunately  in many cases there is additional information available that can be used to further distinguish between different DAGs  In particular  it can be shown  Shimizu et al        that if all  or all but one  distributions of the error variables are non Gaussian  it is in fact possible to identify the complete causal model  including all the parameters  This is possible using a method based on Independent Component Analysis  ICA   Hyvarinen et al         Unfortunately  however  when two or more disturbances are Gaussian the standard method based on ICA will fail  As an extreme example  when all disturbances are Gaussian  standard ICA based methods return nonsense and are not even able to find the correct d separationequivalence class  These considerations raise the question of whether it is possible to combine the methods so as to obtain robustness with respect to Gaussian distributions but not forgo the possibility of identifying the full model in favourable circumstances  Indeed  such a combination is possible and is presented in Section    Here  we simply note that the nave solution of first running some test and then selecting one of the two methods  will not be optimal  Consider  for instance  our example model in Figure  a  Because there is more than one Gaussian error variable the standard ICA based method  Shimizu et al        is not applicable  and hence one would have to settle for the d separation equivalence class  Figure  c  given by independence based methods  However  as we show in the next section  in this example we can actually reject one of the DAGs in the equivalence class and hence obtain a smaller set of possible generating models      DISTRIBUTION EQUIVALENCE  In general  an ngDAG D is instantiated by many different models M which differ in their connection strengths bij as well as in their distributions pi  ei    Next  we define the important concept of distributionequivalence between ngDAGs  which defines to what extent it is possible to infer the ngDAG which represents the true data generating causal model  from observational data alone  Definition   Two ngDAGs D  and D  are distribution equivalent if and only if for any linear acyclic causal model M  which instantiates D  there exists an instantiation M  of D  which yields the same joint observed distribution as M    and vice versa  Distribution equivalence partitions the set of ngDAGs into distribution equivalence classes  and these may be represented using simplified graphs  Definition   An ngDAG pattern representing an ngDAG D is a mixed graph  consisting of potentially both directed und undirected edges   obtained in the following way     Derive the d separation equivalence pattern corresponding to the DAG in D    Orient any unoriented edges which originate from  or terminate in  a node positively marked in ng of D  in the orientation given by the DAG in D    Finally  orient any edges which follow from the orientations given in the previous step and dseparation equivalence  according to the rules derived by Meek         We say that a mixed graph is an ngDAG pattern if it represents some ngDAG   First  we need to extend a DAG object to include information on the non Gaussianity of associated disturbance variables   An ngDAG pattern is similar in many respects to dseparation equivalence patterns  For example  we have the following result   Definition   An ngDAG is a pair  G  ng  where G is a directed acyclic graph over a set of variables V and ng is a binary vector of length  V    each element of which is associated with one of the variables of V    Lemma   An ngDAG pattern is a chain graph   Definition   We say that a linear acyclic causal model M instantiates an ngDAG D  alternatively  D represents M   if and only if the directed acyclic graph associated with M is equal to that specified in D  and further if the set of variables with non Gaussian disturbance variables in M is equal to the set of positive entries in the binary vector specified in D   The proof is given in the Appendix  Our main result connects ngDAG patterns with distribution equivalence in mixed Gaussian and nonGaussian models in the same way that d separationequivalence patterns are associated with distributionequivalence in purely Gaussian models  Theorem   Two ngDAGs are distribution equivalent if and only if they are represented by the same ngDAG pattern    The proof of this theorem is provided in the Appendix  The important point is that we now know exactly which models are indistinguishable from each other on the basis of observational data alone    c  Calculate the corresponding ICA objective function X   Uf    E f  ei     k       As a simple illustration  in Figure  d we show the ngDAG pattern representing the ngDAG corresponding to the generating model of Figure  a  Note that the ngDAG pattern is more informative than the dseparation equivalence pattern of Figure  c  Nevertheless  there are still two ngDAGs  leftmost two in Figure  b  which cannot be distinguished based on non experimental data   where k is the expected value of f applied to a zero mean  unit variance Gaussian variable  i e  k   E f  g    g  N         In the ICA literature  many different choices of f have been utilized  here we suggest simply taking the absolute value function f  ei      ei    giving    X  p     U  E  ei         Henceforth in the paper we shall use the terms ngDAG pattern and distribution equivalence pattern interchangably      PC LINGAM  Although an important goal in this study was to look at the theoretical aspects of identifying DAGs in mixed Gaussian   non Gaussian acyclic linear causal models  an equally significant objective is to give a practical method with which to infer models from a finite data set  Although there are a number of possible approaches  we here give a simple combination of independence based techniques and the ICA based method  The method  termed PClingam  consists of three steps     Use methods based on conditional independence tests to estimate the d separation equivalence class within which the generating model lies  In particular  we advocate using the PC algorithm  Spirtes et al        which is computationally efficient even for a large number of variables  Note that  for linear models  to obtain the d separationequivalence class it is sufficient to identify the zero partial correlations in the data  as these depend only on the linear coefficients and the variances of the disturbances  and not on non Gaussianity aspects of the distributions   However  since the data may well be signficantly non Gaussian  nonparametric tests should optimally be used to find the zero partial correlations     For each DAG G in the estimated d separationequivalence class   a  Estimate the coefficients bij using ordinary least squares regression   Note that this provides consistent estimates regardless of nonGaussianity of the variables    b  Calculate the corresponding residuals ei and rescale them to zero mean and unit variance for each i  i  i  Of course  since we only have samples we have to take the sample mean rather than the expectation     Select the highest scoring DAG Gopt from Step   and apply a statistical test for normality for each of the corresponding residuals ei   Using Definition    compute and return the ngDAG pattern representing the ngDAG  Gopt   ng  where ng is the vector indicating those residuals whose normality was rejected by the normality tests  The objective function U is commonly used in ICA as a measure of the non Gaussianity of a random variable  and it can be shown to give a consistent estimator for finding independent components under weak conditions  Hyvarinen et al         ICA estimation is closely related to choosing the right DAG because statistical independence of the estimated residuals is a necessary condition for the correct model  Any DAG for which the estimated residuals are not independent violates the assumptions of the model  see Section    and hence cannot be the data generating DAG  On the other hand  any DAG which results in statistically independent residuals represents one valid model that could have generated the data  Note that if we could disregard sampling effects  distribution equivalent models would attain exactly the same value of U   However  in the practical case of a finite sample this is not the case  thus Step   in the PClingam algorithm is required to identify the correct distribution equivalence class  The method as presented above has at least a couple of shortcomings  One is that  for any given function f  ei   used  there always exist distributions which are nonGaussian yet are not distinguished from the Gaussian by this measure  This is a well known issue in ICA which fortunately tends to have little practical significance since few such distributions are encountered in practice  If needed  non parametric Gaussianity measures could be used to remedy this potential problem    a  b                                               c  d              e                                                                Figure    One of the networks used in the simulations  Variables with non Gaussian disturbances are shown in squares  while those with Gaussian disturbances are plotted as circles   a  True data generating model   b  True d separation equivalence pattern   c  True distribution equivalence pattern   d  Estimated DAG Gopt    e  Estimated distribution equivalence pattern  See main text for details  Naturally  in some cases many of the disturbances may be slightly non Gaussian yet sufficiently close to Gaussian that the available samples may not be sufficient to distinguish the two and utilize the information for determining causal directions in the model  Of course  this is not a shortcoming of this particular method but is a more general phenomenon  Another important limitation is that the ICA objective function given above will only provide a proper comparison of different DAGs for which the residuals ei are linearly uncorrelated  This is guaranteed to be the case when the search is in the correct d separationequivalence class  but if in Step   of the procedure we select a too simple model  i e  containing too few edges  then the estimated disturbances may be linearly correlated and the objective function misleading  Thus  it might be wise to include a term penalizing linear correlations such as is used in maximum likelihood estimation of ICA  Hyvarinen et al         However  to keep our method as simple as possible  we have omitted such a penalty term in this paper      SIMULATIONS  In this section we report on simulations used to test the performance of the PClingam method  First  we tested the ability of the non Gaussianity objective function     of Step   and the normality tests of Step   of PClingam to identify the correct ngDAG pattern  distribution equivalence class  when the true d separation equivalence pattern was known  In other words  we tested how well the algorithm would function if Step   of the method worked flawlessly  Subsequently  we experimented with the full method incorporating the necessary estimation of the d separationequivalence class  Step     Figure  a displays one of the models used to test the procedure  The disturbance distributions of variables X  and X  were a standard Gaussian the values of which were squared  but keeping the original sign   while the disturbance of X  was produced in a similar way but instead raising the values to the third power  The disturbances of X    X    and X  were Gaussian  The disturbance variables were scaled such that their variances ranged from     to      A sample of      data vectors was generated from the model  Figure  b shows the true d separation equivalence pattern of the model in  a   The equivalence class consists of    different DAGs  However  the non Gaussianity of the disturbances of X    X    and X  means that there are actually only   DAGs which are distributionequivalent  these are represented by the distributionequivalence pattern of Figure  c  Figure  d shows the DAG Gopt found by Step   of PClingam from the data  when the true d separation equivalence class was given to the algorithm  An Anderson Darling test for normality  Anderson and Darling       gave the p values                                      and        for the corresponding residuals e  to e    Inferring a residual to be non Gaussian when p        in Step   of the method produced the ngDAG pattern of Figure  e  which turns out identical to the true ngDAG pattern in  c   This basic procedure was repeated    times  with the results summarized in Table  a  In each simulation  we randomly generated a linear acyclic causal model over   variables  with each variable randomly chosen to have either a Gaussian or a non Gaussian disturbance  The non Gaussian distributions used were those mentioned above as well as a Students t    degrees of freedom   a bimodal Mixture of Gaussians      N              N          a log normal distribution  exponentiated standard normal  and a uniform distribution  The true d separation equivalence pattern was input to the algorithm  to test the functioning of the PClingam method when the correct pattern is selected in Step    The panel shows how often a specific type of true edge  in the true distribution equivalence pattern  gave rise to a specific type of estimated edge  in the estimated distribution equivalence pattern   Rows cor    Table    Summary of the simulations employing various methods for inferring the d separation equivalence class in Step   of PClingam  Each table is a confusion matrix of arcs in the true distribution equivalence patterns vs arcs in the estimated distribution equivalence pattern  See main text for details   a  Using the true d sep equiv pattern  b  PC                                                                                                            c  d  CPC  GES                                                                                                               While the theoretical aspects of identifiability are solved  at least a couple of important issues regarding the estimation of the model from finite samples remain  First and foremost  non parametric methods for identifying zero partial correlations in non Gaussian settings should be used so as to obtain better estimates of the appropriate d separation equivalence class within which to search  Although the methods developed for Gaussian variables seem to work relatively well in our partly non Gaussian setting  it is likely they will be outperformed by methods that take into account the possibility of non Gaussian distributions  Another important question is how to make the procedure scalable to data involving many  tens or even hundreds of  variables  Although the current approach relies on a brute force enumeration of all DAGs in the d separation equivalence class  it would not be difficult to adapt the method to do a local search among DAGs in an equivalence class  The extent to which such a method would be hampered by local maxima is unknown      respond to the true edges  columns to estimated ones  Optimally all off diagonal elements would be zero  It can be seen that the results are close to perfect  the method misclassifies two undirected edges as directed  but correctly estimates all others  These simulations confirm that the PClingam method works well at least when the d separation equivalence class can reliably be estimated  But in practice  with finite datasets  there may be significant errors in inferring the d separation equivalence class  The degree to which this affects the algorithm is an important practical issue  Thus  in further simulations  we applied several different methods for learning d separation equivalence patterns from the simulated data  as Step   in the PClingam method  The methods we compared were the PC algorithm  Spirtes et al         the Conservative PC algorithm  Ramsey et al         and the GES algorithm  Chickering        Panels b d of Table   summarize the results  Although all of the methods assumed Gaussianity when learning the d separationequivalence pattern  the results are still quite encouraging  and a clear majority of edges were correctly estimated   FUTURE WORK  SUMMARY  The discovery of linear acyclic causal models is a topic which has been thoroughly investigated in the last two decades  Both the Gaussian and the fully nonGaussian special cases are well understood  but the general mixed case has not been previously discussed  In this paper we have provided a complete characterization of distribution equivalence and a practical estimation method in this setting  Acknowledgements The authors wish to thank Clark Glymour for helpful and stimulating discussions  P O H  was funded by a postdoctoral researcher grant from the University of Helsinki  
 Causal discovery from observational data in the presence of unobserved variables is challenging  Identification of so called Y substructures is a sufficient condition for ascertaining some causal relations in the large sample limit  without the assumption of no hidden common causes  An example of a Y substructure is A  C  B  C  C  D  This paper describes the first asymptotically reliable and computationally feasible scorebased search for discrete Y structures that does not assume that there are no unobserved common causes  For any parameterization of a directed acyclic graph  DAG  that has scores with the property that any DAG that can represent the distribution beats any DAG that cant  and for two DAGs that represent the distribution  if one has fewer parameters than the other  the one with the fewest parameter wins  In this framework there is no need to assign scores to causal structures with unobserved common causes  The paper also describes how the existence of a Y structure shows the presence of an unconfounded causal relation  without assuming that there are no hidden common causes      Introduction  Discovering causal relationships from observational data is challenging due to the presence of observed confounders    particularly  hidden  latent  confounders   Currently at the Department of Biomedical Informatics  Vanderbilt University  Nashville  TN               A node W is said to be a confounder of nodes A and B if there is a directed path from W to A and a directed path from W to B that does not traverse A  If W is observed  it is said to be a measured confounder  otherwise it is a hidden confounder   Gregory F  Cooper gfc cbmi pitt edu Center for Biomedical Informatics University of Pittsburgh Pittsburgh  PA        Furthermore  it is known that members of an independence  Markov  equivalence class of causal Bayesian network  CBN  models are indistinguishable using only probabilistic dependence and independence relationships among the observed variables  There are several algorithms for reliably identifying  some  causal effects in the large sample limit  given the assumptions of acyclicity  and the Causal Markov and Causal Faithfulness assumptions  explained below   Due to space limitations  we mention only two representative algorithms here  The Greedy Equivalence Search  Chickering        is a score based search that reliably identifies some causal effects in the large sample limit  but only under the additional assumption of no unobserved common causes  The FCI algorithm  Spirtes et al         reliably identifies some causal effects in the large sample limit  but it is a constraint based search  and such methods have several important disadvantages relative to score based searches  Heckerman et al            the inability to include prior belief in the form of structure probabilities      the output is based on the significance level used for the independence tests  and     there is no quantification of the strength of the hypothesis that is output  A constraint based search performs a sequence of conditional independence tests  A single conditional independence test may force the removal of an edge  or a particular orientation  even if that makes the rest of the model a poor fit  In contrast  a score gives a kind of overall evaluation of how all the conditional independence constraints are met by a directed acyclic graph  DAG   and would not decrease the overall fit to save one conditional independence  A score based search over models that explicitly include hidden variables faces several difficult problems  there are a potentially infinite number of such models  and there are both theoretical and computational difficulties in calculating scores for models with hidden variables  Richardson   Spirtes         Another approach is to use graphical models  mixed ances    tral graphs  that represent the marginal distributions of hidden variable models over the observed variables  but do not explicitly contain hidden variables  Richardson   Spirtes         Mixed ancestral graph models  unlike hidden variable models  are finite in number  and in the linear case  there are no theoretical or computational problems in scoring mixed ancestral graphs  However  there is no currently known way to score discrete mixed ancestral graphs  and there is no known algorithm for efficiently searching the space of mixed ancestral graphs  Finally  hidden variable models are known to entail non conditional independence constraints upon the marginal distribution  these constraints can be used to guide searches for hidden variable models that entail the constraints  Spirtes et al         Tian   Pearl         However  no such computationally feasible search is known to be guaranteed correct   per case and random variables by upper case letters italicized  Graphs are denoted by upper case letters such as G or M or by calligraphic letters such as G or M  Lower case letters such as x or z denote an assignment to variables X or Z   The search proposed here is the first computationally feasible score based search to reliably identify some causal effects in the large sample limit for both discrete and linear models  without assuming that there are no unobserved common causes  and without making any assumptions about the true causal structure  other than acyclicity   There is also no need to assign scores explicitly to causal structures with unobserved common causes in this framework  We identify a class of structures  Y structures  as sufficient for assigning causality  and provide the necessary theorems and proofs for the claim  The proofs presented in this paper depend only upon two features of the score    a DAG that cannot represent a distribution loses in the limit to one that can  and     if two DAGs with different numbers of parameters can represent the distribution  the one with more parameters loses to the DAG with fewer parameters   Viral infection in  ABC GFED X  early pregnancy                        Low birth  ABC GFED  ABC GFED X X  A Congenital   weight disease    Aheart   AA       AA      A       Infant  ABC Heart GFED  ABC GFED X  X  mortality  The remainder of the paper is organized as follows  Section   introduces the causal Bayesian network framework and presents the basic assumptions related to causal discovery  Section   describes the V and Y structures  Section   introduces the relevant theorems and proofs  Additional information on search and the score function is provided in Appendix A and Appendix B      Background  In this section we first introduce the CBN framework  Subsequently  we define d separation and d connectivity  independence equivalence  the causal sufficiency assumption  the causal Markov condition  and the causal Faithfulness condition  The following notational convention will be used throughout the paper  Sets of variables are represented in bold and up        Causal Bayesian network  A causal Bayesian network  CBN  is a directed acyclic graph  DAG  with the vertices representing observed variables in a domain and each arc is interpreted as a direct causal influence between a parent node   variable  and a child node  Pearl         For example  if there is a directed edge from A to B  A  B   node A is said to exert a causal influence on node B  Figure   illustrates the structure of a hypothetical causal Bayesian network structure  which contains five nodes  The states of the nodes and the probabilities that are associated with this structure are not shown   murmur  Figure    A hypothetical causal Bayesian network structure The causal Bayesian network structure in Figure   indicates  for example  that a Viral infection in early pregnancy can causally influence whether Congenital heart disease is present in the newborn  which in turn can causally influence Infant mortality and presence of a Heart murmur       d separation and d connectivity  Pearl         d separation is a graphical condition  Assume that A and B are vertices in a DAG G and C is a set of vertices in G such that A   C and B   C  The d separation theorem states that if a distribution P satisfies the Markov condition  each vertex is independent of its non descendants conditional on its parents  for a DAG G  and A is d separated from B conditional on C in G  then A is independent of B conditional on C in G  Consider the DAG G in Figure    A and B are said to be d separated given C iff the following property   If there is an arc from node A to node B in a CBN  A is said to be the parent of B  and B  the child of A    holds  there exists no undirected path  U between X and Y s t   Bayesian network  In a causal DAG for a causally sufficient set of variables  each variable is independent of its non descendants  i e   non effects  given just its parents  i e   its direct causes       every collider  on U is in C or has a descendant in C     no other vertex on U is in C  Likewise  if A and B are not in C  then A and B are d connected given C iff they are not d separated given C  In Figure   the nodes X  and X  are d separated by X    The nodes X  and X  are d connected given     Two disjoint sets of variables A and B are d separated conditional on C if and only if every vertex in A is dseparated from every vertex in B conditional on C  otherwise they are d connected  See  Pearl        for more details on d separation and d connectivity       Independence equivalence  Two Bayesian network structures S and S over a set of observed variables V are independence equivalent  Heckerman        iff S and S have the same set of d separation and d connectivity relationships between A and B conditional on C where A  V  B  V  and C  V and A   C and B   C  Independence equivalence is also referred to as Markov equivalence  For Gaussian or discrete distributions  it is also the case that two DAGs are independence equivalent if and only if the set of distributions that satisfy the Markov condition for one equals the set of distributions that satisfy the Markov condition for the other  i e  they represent the same set of distributions        The causal sufficiency assumption  The CMC represents the locality of causality  This implies that indirect  distant  causes become irrelevant when the direct  near  causes are known  The CMC is the fundamental principle relating causal relations to probability distributions  and is explicitly or implicitly assumed by all causal graph search algorithms       The causal Faithfulness condition  While the causal Markov condition specifies independence relationships among variables  the causal Faithfulness condition  CFC  specifies dependence relationships  In a causal DAG  two disjoint sets of variables A and B are dependent conditional on a third disjoint set of variables C unless the Causal Markov Condition entails that A and B are independent conditional on C  The CFC is related to the notion that causal events are typically correlated in observational data  The CFC relates causal structure to probabilistic dependence  The CFC can fail for certain parameter values  but for linear or discrete parameters  the Lebesgue measure of the set of parameters for which it fails is    It is at least implicitly assumed by both constraint based and score based causal DAG discovery algorithms   A set of variables S is causally sufficient if no variable that is a direct cause  of two variables in S is not in S  In Figure    S    X    X    is not causally sufficient because X  is a direct cause of X  and X    but is not in S  However  note that for the causal discovery approach based on Y structures introduced in this paper  we do not assume causal sufficiency   Based on the causal Markov condition each vertex of a CBN is independent of its non descendants given its parents  The independence map or I map of a CBN is the set of all independencies that follow from the causal Markov condition  The dependence map or Dmap of a CBN is the set of all dependencies that follow from the causal Faithfulness condition        The I map and D map of the causal network W   X  W  is as follows   The causal Markov condition  The causal Markov condition  CMC  gives the independence relationships that are specified by a causal    An undirected path between two vertices A and B in a graph G is a sequence of vertices starting with A and ending with B and for every pair of vertices X and Y in the sequence that are adjacent there is an edge between them  X  Y or X  Y    Spirtes et al         page      A node with a head to head configuration  C is a collider in A  C  B    The direct causation is relative to S    W  W    W    X  W    X W   W    X  W    X W    W    W   X      Markov Blanket  The Markov blanket  MB  of a node X in a causal Bayesian network G is the union of the set of parents of X  the children of X  and the parents of the children   of X  Pearl         Note that the MB is the minimal set of nodes when conditioned on  instantiated  that makes a node X independent of all the other nodes in the CBN      V and Y structures  In this section we first define the V and Y structures and discuss their potential role in causal discovery            Y structure   ABC GFED  ABC  ABC GFED  ABC  ABC GFED  ABC GFED GFED GFED  ABC W  GFED W  W  GFED W  W  W   ABC W  W        X   F                                                          X X X X O O                                           Z Z Z Z G   V structure  A V structure over variables W    W  and X is shown in DAG M  in Figure    There is a directed edge from W  to X and another directed edge from W  to X  There is no edge between W  and W    A V structure contains a collider and the node X is a collider in Figure    M    Since there is no arc between W  and W    X is also termed as an unshielded collider  Figure    M  is a model in which there is an arc between W  and W    and thus X is a shielded collider in this example   ABC GFED  ABC GFED W  C W  CC     C                 X   ABC GFED  ABC   GFED W  W  C CC     C                 X  M   M   Figure    X is an unshielded collider in M  and a shielded collider in M    M  is also referred to as a V structure   ABC GFED  ABC GFED W  W  C                             H KK sH KKK s s   sy s           X Figure    A model that has the same dependence independence relationships over W    W  and X as Figure    M    H denotes a hidden variable  The V structure is not sufficient for discovering that some variable W  or W  causes X if we do not make the assumption that M  is causally sufficient  On the other hand  even allowing for confounders  hidden and measured   we can conclude that X does not cause W  or W   Spirtes et al          Figure   shows a confounder for the pair  W    X  and the pair  W    X   In other words we can make an acausal discovery but not a causal one using the V structure   G   G   G   Figure    Several CBN models that contain four nodes  G  is a Y structure  We now introduce the concept of a Y structure  Let W   X  W  be a V structure  Note that X is an unshielded collider in this V structure since there is no arc between W  and W    If there is a node Z such that there is an arc from X to Z  and there are no arcs from W  to Z and W  to Z  then the nodes W    W    X and Z form a Y structure  A Y structure has interesting dependence and independence properties  If W    W    X  Z form a Y structure over a set of four variables V and the Y structure is represented by G   see Figure     there is no DAG that contains a superset of the variables in V  entails the same conditional independence relations over V  and in which there is a  measured or unmeasured  confounder of X and Z  or in which X is not an ancestor of Z  Robins et al         Spirtes et al         page      that is in the same independence equivalence class as G    In other words  if a Y structure is learned from data  the arc from X to Z represents an unconfounded causal relationship  Since G  also has the same set of independence dependence relationships over the observed variables W    W    and X as Figure     the arcs W   X and W   X cannot be interpreted as causal relationships      Y structure theorems  Definition    Complete table Bayesian network   A complete table Bayesian network is one that contains all discrete variables and for which the probabilities that define the Bayesian network are described by contingency tables with no missing values  Definition    Perfect map   A Bayesian network structure S is a perfect map of a distribution  if A and B are independent conditional on C in  iff A and B are d separated conditional on C in S  Suppose Bayesian network B defines a joint distribution  over all the variables in B  Let S be the structure   of B  If the Markov and Faithfulness conditions hold for B  then S is a perfect map of   In the results of this section  we will only be considering complete table Bayesian networks that satisfy the Markov and Faithfulness conditions  Definition    Y structure Bayesian network   A Y structure Bayesian network is a Bayesian network containing four variables that has the structure shown in Figure    where the node labels are arbitrary   ABC GFED  ABC GFED W  W  G GG ww GG w G   www           X             Z G  Figure    A Y structure   We will use the following notation in regard to an arbitrary complete table Bayesian network that satisfies the Markov and Faithfulness conditions and has a Y structure  By denotes the network  Sy denotes its structure  Vy denotes the four variables in the structure  Qy denotes its complete table parameters  and y denotes the correspondingly defined joint distribution over the four variables  Lemma    There is no other Bayesian network structure on the variables in Vy that is independence equivalent to Sy   Proof  The proof of Lemma   follows from Theorem   given below  Theorem    Two network structures Bs  and Bs  are independence equivalent iff they satisfy the following conditions  Verma   Pearl            Bs  and Bs  have the same set of vertices     Bs  and Bs  have the same set of adjacencies     If there is a configuration such as A  C  B where A and B are not adjacent  V structure  in Bs    the same pattern is present in Bs    and vice versa   Lemma    Let B be a Bayesian network that contains the fewest number of parameters that can represent the population distribution  Let B be a Bayesian network that either cannot represent the population distribution  or can but does not contain the fewest number of parameters  Let S and S be the network structures of  B and B   respectively  Let m denote the number of iid cases in D that have been sampled from the population distribution defined by B  P  S   D       m P  S  D   Then lim       Proof  The proof of Lemma   follows from the results in  Chickering         which in turn uses results in  Haughton         Theorem    Let B    S Q  be a complete table Bayesian network that contains n measured variables  where S and Q are the structure and parameters of B  respectively  Suppose that B defines a distribution  on the n variables  such that S is a perfect map of   Let D be a database containing m complete cases on the n variables in B  for which the cases are iid samples from distribution   Let B be a Bayesian network over the same n variables with structure S that is not independence equivalent to B  Suppose that P  S  D  and P  S   D  are computed using the BDe score with nonzero parameter and structure priors   The BDe score assigns the same score to members of the same Markov equivalent class when equal structural priors are assigned  see  Heckerman et al           P  S   D       m P  S  D   Then lim       Proof  If B cannot represent the generative distribution  then according to Lemma   the current theorem holds  Suppose B can represent the generative distribution  Since by assumption B is not independence equivalent to B  B must contain all the dependence relationships in B  plus additional dependence relationships  Therefore B contains more parameters than B  Chickering        Proposition     Thus it follows from Lemma   that the theorem holds  Theorem    Assume the notation and conditions in Theorem   and suppose the number of variables is four  n       If S is the data generating structure on the four variables and S is a Y structure  then in the large sample limit P  S  D    P  S   D  for all S    S  where S contains just the same   variables   Conversely  if S is the data generating structure and S is not equal to some Y structure  S   then in the large sample limit P  S  D    P  S   D   Proof  The proof follows from Theorem   and Lemma    Theorem   shows  under the conditions assumed  that in the large sample limit a Y structure will have the highest BDe score if and only if it is the structure of the data generating Bayesian network  Note that   Theorem   assumes that some DAG over the measured variables is a perfect map of the observed conditional independence and dependence relations  this is not in general true if there are unmeasured common causes  Lemma   can be strengthened using results in  Nishii        Theorem    to show that in the large sample limit  with probability   the ratio approaches    rather than merely approaching some positive number less than     Correspondingly  Theorem   can be strengthened to state that the data generating structure has posterior probability   and all other structures have probability   in the large sample limit    This strengthened version of Theorem   implies that in the large sample limit  model averaging using Equation   in the appendix will derive an arc X  Z as causal and unconfounded with probability    if and only if it  X  Z  is a causal and unconfounded arc within a Y structure of the data generating causal Bayesian network  The proof of sufficiency of the Y structure for ascertaining causality in the possible presence of hidden confounders requires an understanding of the common properties of DAGs in the same Markov equivalence class in the possible presence of such hidden variables  A class of structures that can represent the common properties of DAGs in the same Markov equivalence class are called partial ancestral graphs  PAGs   We next describe the theorems that make use of PAGs  A PAG has a richer representation compared to a directed acyclic graph  DAG  and makes use of the following types of edges          Partial ancestral graphs A Markov equivalence class of DAGs over a set of observed variables O is the set of all DAGs that contain at least the variables in O and that have the same set of d separation relations among the variables in O  i e   G  and G  are in the same Markov equivalence class over O if for all disjoint X  Y  Z  O  X is d separated from Y conditional on Z in G  iff X is d separated from Y conditional on Z in G     A PAG P over O is a graphical object with vertices O that represents the Markov equivalence class of DAGs M over O in two distinct ways     A PAG represents the d separation relations over O in M     The results in  Nishii        are based on almost surely convergent proofs  which guarantee that in the large sample limit the data will with probability   support the stated convergence    If there are several Bayesian networks that contain the fewest number of parameters that can represent the data generating distribution  then the result states that the sum of their posterior probabilities is equal to        A PAG represents the ancestor and non ancestor relations among variables in O common to every DAG in M  More specifically  it is possible to extend the concept of d separation in a natural way to PAGs so that if PAG P represents the Markov equivalence class M over O  then for all disjoint X  Y  Z  O  X is d separated from Y conditional on Z in P iff X is d separated from Y conditional on Z in every DAG in M  A PAG is formally defined as stated below  Definition    PAG   The PAG P that represents a Markov equivalence class M over O can be formed in the following way     A and B are adjacent in P iff A and B are d connected conditional on every subset of O  A  B      If A and B are adjacent in P  there is an   arrowtail  at the A end of the edge iff A is an ancestor of B in every member of M     If A and B are adjacent in P  there is an    arrowhead  at the B end of the edge iff B is not an ancestor of A in every member of M     If A and B are adjacent in P  an o at the A end of the edge entails that in some DAG in M  A is an ancestor of B and in some other DAG in M  A is not an ancestor of B   In  Richardson   Spirtes         this is called a maximally oriented PAG    ABC GFED  ABC GFED W  W      GG w    GG w w G   ww           X             Z G P Figure    A Y PAG  For example  suppose M is the Markov equivalence class of the Y structure  It can be shown that the PAG that represents a Y structure is in Figure    indicating that for every DAG in M  the following conditions hold   X is not an ancestor of W  or W     Z is not an ancestor of X   X is an ancestor of Z   W  and W  are ancestors of X in some members of M  and not ancestors of X in other members of M    Definition    DAG PAG   For a PAG P  if there is an assignment of arrowheads and arrowtails to the o endpoints in P such that the resulting graph is a DAG that has the same d separation relations as P  then P is a DAG PAG  For example  a Y PAG is a DAG PAG because the DAG in Figure    which we will call the Y DAG  has the same d separation relations as the PAG in Figure    A DAG PAG can be parameterized in the same way as a corresponding DAG  Every DAG PAG has the same d separations over the measured variables as the DAGs that it represents  Every DAG PAG can be assigned a score equal to the score of any of the DAGs that it represents  The reader is referred to  Spirtes et al         and  Spirtes et al         pages         for additional details about PAGs  The FCI algorithm is a constraint based algorithm that generates a PAG from data faithful to a DAG represented by the PAG  A listing of the PAGs over four variables that includes the Y PAG with a discussion of their causal implications is presented in  Richardson   Spirtes         Definition    Embedded pure Y structure   Let B be a causal Bayesian network with structure S  We say that B contains an embedded pure Y structure  EPYS  involving the variables W    W    X and Z  iff all and only the following d separation conditions hold among the variables W    W    X and Z  A  B C means that A and B are d separated conditioned on C    W    W       W    Z  X   W    Z  X  W     W    Z  X   W    Z  X  W    Context    Let B be a complete table Bayesian network involving the variables W    W    X and Z  Furthermore  let B be the data generating model for data on just W    W    X and Z  In general  B may contain other variables  which we consider as hidden with regard to the data being measured on these four variables  Let w w xz be the data generating distribution on the variables W    W    X and Z that is given by a marginal distribution of B  Suppose that every d separation condition among W    W    X and Z in B implies a corresponding independence according to w w xz  call this the marginal Markov condition     Suppose also that every d connection condition among W    W    X and Z implies a corresponding dependence according    The marginal Markov condition is entailed by the Markov condition  and is strictly weaker than the Markov condition   to w w xz  call this the marginal Faithfulness condition     To summarize     Let Sy denote the Y structure in Figure       Let VSy denote the variables in Sy      Let B be the Bayesian network generating the data     Let VB denote the variables in B     In general VSy  VB      Assume the marginal Markov and Faithfulness conditions hold for B with respect to w w xz    ABC GFED  ABC GFED W  C W  CC      C               X   ABC GFED  ABC GFED W  aC   W  CC     C              X  Figure    X is a non collider in both the models Definition    Non collider   A variable X is said to be a non collider on a path if it does not have two incoming arcs  arrowheads   See Figure   for examples  Lemma    If B contains an EPYS  then in the large sample limit  the Y PAG receives a higher score than any other DAG PAG over the same variables  with probability    Proof  If B contains an EPYS  then by the Marginal Faithfulness condition  the marginal population distribution is faithful to the Y PAG  and hence to the Y DAG  By Theorem    the Y PAG receives a higher score than any other DAG over the same variables  and hence any other DAG PAG  Let PAG B  be the PAG for Bayesian network B  over the set of measured variables O  Lemma    If B does not contain an EPYS  and the population distribution is faithful to PAG B   then in the large sample limit  with probability   there is a DAG PAG P over O that receives a higher score than the Y PAG  Proof  Suppose that B does not contain an EPYS  There are three cases  Suppose first that PAG B  contains any adjacency between some pair of observed vertices A and C that are not adjacent in the Y PAG  It follows that A    The marginal Faithfulness condition is entailed by the Faithfulness condition  and is strictly weaker than the Faithfulness condition    and C are d separated conditional on some subset of O in the Y PAG but not in PAG B   Hence  by the Marginal Faithfulness condition  in the population distribution A and C are dependent conditional on every subset of O  Hence the Y PAG cannot represent the marginal population distribution  Some DAG can represent the marginal population distribution  since a DAG in which every pair of vertices are adjacent can represent any distribution  Hence there is a DAG G with the fewest number of parameters that can represent the marginal population distribution  By Lemma    in the large sample limit with probability    the Y PAG receives a lower score than G  Suppose next that PAG B  contains the same adjacencies as the Y PAG  but different orientations  It is easy to see by exhaustively considering all of the possible PAGs with the same adjacencies as the Y PAG that each of them is a DAG PAG  Since the two different PAGs have the same d separation relations as some pair of different DAGs over O  we can reason about their d separation relations using two DAGs  Given that they have the same adjacencies  the Y DAG and the DAG G with the same d separation relations as PAG B  are not equivalent iff they have different unshielded colliders  Suppose that there is some unshielded collider A  D  C in the Y DAG  but not in G  It follows that A and C are d separated conditional on some subset of variables in the Y DAG  and that every set that d separates A and C in the Y DAG does not contain D  In G  A and C are d separated conditional only on subsets of variables that do contain D  Hence there is a d separation relation in the Y PAG that is not in G  The case where A  D  C is an unshielded collider in G  but not in the Y DAG  is analogous  Hence  by the marginal Faithfulness assumption the Y PAG cannot represent the marginal population distribution  By Lemma    in the large sample limit with probability    the Y PAG receives a lower score than some other DAG with the fewest parameters that does represent the population distribution  Finally  suppose that PAG B  contains a proper subset of the adjacencies in the Y PAG  Inspection shows that all of the PAGs with subsets of adjacencies of the Y PAG are DAG PAGs  Hence  by the Marginal Faithfulness condition  the population distribution is faithful to some such PAG  and hence faithful to a corresponding DAG  By Theorem   the DAG that the population distribution is faithful to receives a higher score than the Y PAG with probability   in the large sample limit   Let us consider the following example  Assume that B contains the substructure shown in Figure    We refer to this structure as Near Y DAG or N Y DAG  GFED  ABC  ABC GFED W  G W     GG w w w    GGG  www          X                          Z M  Figure    A Near Y structure   ABC GFED GFED  ABC W  W        GG w    G w    GG w  ww                  X                  Z M P Figure    A Near Y PAG   for short  The N Y DAG has an additional arc from W  to Z when compared to the Y DAG shown in Figure      Note that for the N Y DAG the d separation W    Z  X  does not hold and hence if B contains an N Y DAG  it will not be an embedded pure Y structure  According to Lemma   then with probability   there is a DAG PAG P over O that receives a higher score than the Y PAG  Such a DAG PAG P is shown in Figure    The N Y DAG also has interesting independence dependence properties  It is the only member of its independence class over the observed variables W    W    X  and Z  A causal claim can be made for the arc from X to Z in the N Y DAG and it can be estimated from P x   z  w   We plan to generate a formal proof of causality for the N Y DAG as part of our future work  Theorem    Assume that Context   holds  In the large sample limit  in scoring DAGs on VSy   Sy is assigned the highest score  iff B contains a corresponding EPYS  and if B contains such an EPYS  then X is an ancestor of Z in B  and there is no unmeasured common cause of X and Z  Proof  By Lemmas   and    Sy is assigned the highest score  iff B contains a corresponding EPYS  In  Spirtes et al         it is shown that if there is a directed edge from X to Z in a PAG  as in the Y PAG   then X is an ancestor of Z in every DAG represented by the    The N Y DAG can have either the extra arc from W  to Z or the arc from W  to Z  but not both    PAG  and there is no hidden common cause of X and Z in any such DAG  Theorem   indicates that local Bayesian causal discovery using Y structures is possible  under assumptions   even when the data generating process is assumed to be a causal Bayesian network with hidden variables      Discussion  The local Bayesian causal discovery based on Y structures may have practical applications on large data sets such as gene expression data sets with thousands of variables or large population based data sets with hundreds of thousands of records  As the Y structure represents an unconfounded causal influence of a variable X on variable Z  it can be used for performing planned interventions leading to desirable outcomes  When experimental studies are contra indicated  due to ethical  logistical  or cost considerations  causal discovery from observational data remains the only feasible approach  Moreover  in resource limited settings such methods can be initially used to generate plausible causal hypothesis that can then be tested using experimental methods resulting in better utilization of available resources  As part of our future work  we plan to relax some of our causal discovery assumptions  for example  the acyclicity assumption  and extend the proofs to more complex Y structures such as those with measured confounders   A  Appendix  Search  The proofs presented in this paper for the EPYS do not depend on a particular search heuristic for the identification of tetrasets  sets of four variables  for Y structure scoring  An obvious search method is to search for EPYS in all possible tetrasets of a domain  For different finite sample sizes the BLCD search method has been shown to be effective in simulation studies  The BLCD search first estimates the Markov blanket of a node X  and creates sets of four variables by choosing three nodes from the MB of X in addition to X  The tetrasets are scored using the Score function described in Appendix B  A preliminary version of the BLCD algorithm was published in  Mani   Cooper         The reader is referred to  Mani        for additional details  including results of an extensive set of simulation experiments   B  Appendix  Scoring measure  The Score function assigns a score to a model that represents the probability of the model given data  and prior knowledge  For scoring the DAGs  we use the Bayesian likelihood equivalent  BDe  scoring measure  Heckerman et al          We use uniform  noninformative priors in order to test the ability of the algorithm to discover causal relationships from data  rather than from a combination of data and prior knowledge  The latter introduces two sources of experimental variation  Note that for a Y structure  the causal claim is valid for only the arc from X to Z  We represent the Y structure using the notation X  Z  In the large sample limit under the causal Markov and causal Faithfulness assumptions  see Section     P X  Z D   D denotes the dataset  can be estimated using Equation    Score G   D  P    i   Score Gi  D        where Gi represents one of the     CBNs over V    W    W    X  Z   Note that P X  Z D  is a heuristic approximation to what would be obtained if we were to explicitly perform a full Bayesian scoring that includes hidden variables  The score for each of the     measured structures represents a score for both the measured structure itself  as well as the score for an infinite number of structures with hidden variables  Interpreting the score as a probability is a heuristic approximation borne out by simulation studies  Mani         but the theorems in the paper do not depend upon that approximation  The only probabilities being considered are the probability of being a Y structure and the probability of not being a Y structure  The theorems in the paper show the conditions under which P X  Z D  converges to the correct value in the large sample limit  In the large sample limit the posterior probability of G  will be greater relative to the other     models if indeed     X causally influences Z in an unconfounded manner      X is an unshielded collider of W  and W  in the distribution of the causal process generating the data  and     the Markov and Faithfulness conditions hold   Acknowledgements We thank the anonymous reviewers for helpful suggestions  We also thank Thomas Richardson for comments on an earlier version of this paper  This research was supported in part by grant IIS         from the National Science Foundation and by grant R   LM       from the National Library of Medicine awarded to Greg Cooper    
 We generalize Shimizu et als        ICA based approach for discovering linear non Gaussian acyclic  LiNGAM  Structural Equation Models  SEMs  from causally sufficient  continuous valued observational data  By relaxing the assumption that the generating SEMs graph is acyclic  we solve the more general problem of linear non Gaussian  LiNG  SEM discovery  LiNG discovery algorithms output the distribution equivalence class of SEMs which  in the large sample limit  represents the population distribution  We apply a LiNG discovery algorithm to simulated data  Finally  we give sufficient conditions under which only one of the SEMs in the output class is stable        Patrik O  Hoyer Dept  of Computer Science University of Helsinki Helsinki  Finland  The model  with an illustration  Let x be the random vector of substantive variables  e be the vector of error terms  and B be the matrix of linear coefficients for the substantive variables  Then the following equation describes the linear SEM model  x   Bx   e       For example  consider the model defined by  x    e  x       x      x    e  x     x    e        x    x    e  x     x    e  Note that the coefficient of each variable on the lefthand side of the equation is        Linear SEMs  Linear structural equation models  SEMs  are statistical causal models widely used in the natural and social sciences  including econometrics  political science  sociology  and biology       The variables in a linear SEM can be divided into two sets  the error terms  typically unobserved   and the substantive variables  For each substantive variable xi   there is a linear equation with xi on the left handside  and the direct causes of xi plus the corresponding error term on the right hand side  Each SEM with jointly independent error terms can be associated with a directed graph  abbreviated as DG  that represents the causal structure of the model and the form of the linear equations  The vertices of the graph are the substantive variables  and there is a directed edge from xi to xj just when the coefficient of xi in the structural equation for xj is non zero       Traditionally  SEMs with acyclic graphs are called re   Fig     Example   x can also be expressed directly as a linear combination of the error terms  as long as I  B is invertible  Solving for x in Eq    gives x    I  B   e  If we let A    I  B     then x   Ae  A is called the reduced form matrix  in the terminology of Independent cursive  and SEMs with cyclic graphs non recursive      We avoid this usage  and use acyclic or cyclic instead    Components Analysis  see Section       it is called the mixing matrix   The distributions over the error terms in a SEM  together with the linear equations  entail a joint distribution over the substantive variables  This joint distribution can be interpreted in terms of physical processes  as shown next       Interpretating linear SEMs  These equations  contained in matrix equation      can be given several different interpretations  Under one class of interpretations  they are a set of equations satisfied by a set of variables x in equilibrium  With some further assumptions  the B matrix in the simultaneous equations  a k a  equilibrium equations  also represents the coefficients in a set of dynamical equations describing a deterministic dynamical system  Fisher     gave one such interpretation as follows  There is a relatively long observation period of length    and a much shorter reaction lag of length      n  The observed variable is the vector x t   defined as the average of x over the observation period starting at t  n  X x t   k      x t   n  that do not contain any self loops  edges from a vertex to itself      i e  the B matrices output by our LiNG discovery algorithms have all zeros in the diagonal  This is because it is impossible to determine the values of the diagonal entries of the B matrix from equilibrium data alone  In the underlying dynamical equations  it may be that for some index a  xa  t    k      affects xa  t   k   i e  ba a        Our goal is to recover the coefficients that both represent the distribution of x and correctly predict the effects of manipulations  A manipulation of a variable xi to a distribution P is modeled by replacing the dynamical equation for xi by a new dynamical equation xi  t   k    e  i   where e  i has distribution P       For these purposes  the following argument sketches why the underdetermination of the diagonal of Bequil by the equilibrium data is not a problem  as long as ba a      in the underlying dynamical equations  The equation for xa has the form  xa   ba a xa    n X  ba k xk   ea       k  a k    If ba a       it is possible to rewrite this as   k    xa  ba a xa    Suppose that the underlying dynamical equations are   n X  ba k xk   ea       k  a k    x t   k    Bdyn x t    k        e       where e is constant over the observation period  but may differ for different units in the population  e g  different observation periods   Fisher showed that  in the limit as  approaches    there is a Bequil   Bdyn such that  x t    Bequil x t    e       if and only if the modulus of each eigenvalue of Bdyn is less than or equal to    and no eigenvalue is equal to    The assumptions underlying this model are fairly strong  but commonly made in econometrics  and defended by Fisher      A simpler  but similar interpretation with similar consequences is the one in which the observed value x is the state in which the dynamical system converged  rather than its average over an observation period         Dealing with self loops  The LiNG discovery algorithms presented in this paper  described in section    output a set of directed graphs   n X    xa      ba a  k  a k     ba k xk   ea     n X  b a k xk  e a  k        b a a       The modified system of equations conwhere taining Equation   is represented by a graph that has no self loops  and has a different underlying dynamical equation in which the coefficient for xa  t    k      in the equation for xa  t   k  is zero  Note that in the second equation  the error term ea has been rescaled by       ba a   to form a new error term e a and when  I  B   is taken to form the reduced form coefficients  the coefficients corresponding to ea in the first set of equations will be multiplied by     ba a    and the two changes cancel each other out  Now  if we consider the original dynamical system and the one that results from setting the diagonal of B to zero  as above   it is sometimes the case that one dynamical system satisfies the conditions for the dynamical equations to approach the simultaneous equations   Fisher argues that self loops are not realistic  but these arguments are not entirely convincing    in the limit  while the other one does not  because the magnitude of the coefficients in the equation for xa  t  are different  If both forms satisfy Fishers conditions  then the act of manipulating any variable to a fixed distribution for all t makes the two sets of dynamical equations have equivalent limiting simultaneous equations         Self loops with coefficient    Unfortunately  the case where ba a     cannot be handled in the same way  since       ba a   is infinite  If ba a      then there may be no equivalent form without a self loop  or more precisely  the corresponding equations without a self loop may require setting the variance of some error terms to zero   The case where ba a     is a genuine problem that we do not currently have a solution for  For the purposes of this paper  we assume that no self loops have a coefficient of    As Dash has pointed out      there are cases where the simultaneous equations have a different graph than the underlying dynamical equations  and hence the graph that represents the simultaneous equations cannot be used to predict the effects of a manipulation of the underlying dynamical system  In      Dash presents two such examples  In both of them  in effect  Bdyn has a   in the diagonal          The problem and its history The problem of DG causal discovery  Using the interpretations from      we can frame the problem as follows  given samples of the equilibrium distribution of a LiNG process whose observed variables form a causally sufficient set     find the set of SEMs that describe this distribution  under the assumption that it is non empty   the linear coefficients  and features common to those directed graphs  such as ancestor relations   The algorithm performs a series of statistical tests of zero partial correlations to construct the PAG  The set of zero partial correlations that is entailed by a linear SEM with uncorrelated errors depends only upon the linear coefficients  and not upon the distribution of the error terms  Under some assumptions    in the large sample limit  CCD outputs a PAG that represents the true graph  There are a number of limitations to this algorithm  First  the set of DGs contained in a PAG can be large  and while they all entail the same zero partial correlations  viz   those judged to hold in the population   they need not entail the same joint distribution or even the same covariances  Hence in some cases  the set represented by the PAG will include cyclic graphs that do not fit the data well  Therefore  even assuming that the errors are all Gaussian  it is possible to reduce the size of the set of graphs output by CCD  although in practice this can be intractable  For details on the algorithm  see           Shimizu et als approach for discovering LiNGAM SEMs  The LiNGAM algorithm      which uses Independent Components Analysis  ICA   reliably discovers a unique correct LiNGAM SEM  under the following assumptions about the data  the structural equations of the generating process are linear and can be represented by an acyclic graph  the error terms have non zero variance  the samples are independent and identically distributed  no more than one error term is Gaussian  and the error terms are jointly independent             Richardsons Cyclic Causal Discovery  CCD  Algorithm  While many algorithms have been suggested for discovering  equivalence classes of  directed acyclic graphs  DAGs  from data  for general linear directed graphs  DGs  only one provably correct algorithm was known  until now   namely Richardsons Cyclic Causal Discovery  CCD  algorithm  CCD outputs a partial ancestral graph  PAG  that represents both a set of directed graphs that entail the same set of zero partial correlations for all values of   A set V of variables is causally sufficient for a population if and only if in the population every common direct cause of any two or more variables in V is in V    For subtleties regarding this definition  see         Independent Components Analysis  ICA   Independent components analysis        is a statistical technique used for estimating the mixing matrix A in equations of the form x   Ae  e is often called sources and written s   where x is observed and e and A are not  ICA algorithms find the invertible linear transforma   The assumptions are  the samples are independent and identically distributed  no error term has zero variance  the statistical tests for zero partial correlations are consistent  linearity of the equations  the existence of a unique reduced form  faithfulness  i e  there are no zero partial correlations in the population that are not entailed for all values of the free parameters of the true graph   and that the error terms are uncorrelated    The error terms are typically not jointly independent if the set of variables is not causally sufficient    tion W   A  of the data X that makes the error distributions corresponding to the implied samples E of e maximally non Gaussian  and thus  maximally independent   The matrix A can be identified up to scaling and permutation as long as the observed distribution is a linear  invertible mixture of independent components  at most one of which is Gaussian      There are computationally efficient algorithms for estimating A           The LiNGAM discovery algorithm  If we run an ICA algorithm on data generated by a linear SEM  the matrix WICA obtained will be a rowscaled  row permuted version of I  B  where B is the coefficient matrix of the true model  this is a consequence of the derivation in Section       We are now left with the problem of finding the proper permutation and scale for the W matrix so that it equals I B   Fig     After removing the edges whose coefficients are statistically indistinguishable from zero   a  the raw WICA matrix output by ICA on a SEM whose graph is x   f matrix  obtained by x   x   b  the corresponding W permuting the error terms in WICA  Since the order of the error terms given by ICA is arbitrary  the algorithm needs to correctly match each error term ei to its respective substantive variable xi   This means finding the correct permutation of the rows of WICA   We know that the row permutation of WICA corresponding to the correct model cannot have a zero in the diagonal  we call such permutations inadmissible  because W   I  B  and the diagonal of B is zero  Since  by assumption  the data was generated by a DAG  there is exactly one row permutation of WICA that is admissible       To visualize this  this constraint says that there is exactly one way to reorder the error terms so that every ei is the target of a vertical arrow   In this example  swapping the first and second error terms is the only permutation that produces an admissible matrix  as seen in Fig    b    After the algorithm finds the correct permutation  it finds the correct scaling  i e  normalizing W by dividing each row by its diagonal element  so that the diagonal of the output matrix is all  s  i e  the coefficient of each error term is    as specified in Section     Bringing it all together  the algorithm computes B by f    W f  using B   I  W     where W     normalize W RowP ermute WICA   and WICA   ICA X   Besides the fact that it determines the direction of every causal arrow  another advantage of LiNGAM over conditional independence based methods      is that the correctness of the algorithm does not require the faithfulness assumption  For more details on the LiNGAM approach  see           Discovering LiNG SEMs  The assumptions of the family of LiNG discovery algorithms described below  abbreviated as LiNG D  are the same as the LiNGAM assumptions  replacing the assumption that the SEM is acyclic with the weaker assumption that the diagonal of Bdyn contains no  s  In this more general case  as in the acyclic case  candidate models are generated by finding all admissible matches of the error terms  ei s  to the observed variables  xi s   In other words  each candidate corresponds to a row permutation of the WICA matrix that has a zeroless diagonal  As in LiNGAM  the output is the set of admissible models  In LiNGAM  this set is guaranteed to contain a single model  thanks to the acyclicity assumption  If the true model has cycles  however  more than one model will be admissible  The remainder of this section addresses the problem of finding the admissible models  given that ICA has finite data to work with       Prune and solve Constrained n Rooks  These algorithms generate candidate models by testing which entries of WICA are zero  i e  pruning   and finding all admissible permutations based on that  i e  solving Constrained n Rooks  see Section         We call an algorithm local if  for each entry wi j of WICA   it makes a decision about whether wi j is zero using only wi j          Deciding which entries are zero     Another consequence of acyclicity is that there will be no right pointing arrows in this representation  provided that the xs are topologically sorted w r t  the DAG   There are several methods for deciding which entries of WICA to set to zero     Thresholding  the simplest method for estimating which entries of WICA are zero is to simply choose a threshold value  and set every entry of WICA smaller than the threshold  in absolute value  to zero  This method fails to account for the fact that different coefficients may have different spreads  and will miss all coefficients smaller than the threshold   Test the non zero hypothesis by bootstrap sampling  another method for estimating which entries of WICA are actually zero is to do bootstrap sampling  Bootstrap samples are created by resampling with replacement from the original data  Then ICA is run on each bootstrap sample  and each coefficient wi j is calculated for each bootstrap sample  This leads to a real valued distribution for each coefficient   Then  for each one  a non parametric quantile test is performed in order to decide whether   is an outlier  If it isnt  the coefficient is set to    i e  the corresponding edge is pruned     Use sparse ICA  Use an ICA algorithm that returns a sparse  i e  pre pruned  mixture  such as the one presented by Zhang and Chan       Unlike the other methods above  this is not a local algorithm         Constrained n Rooks  the problem and an algorithm  Once it is decided which entries are zero  the algorithm searches for every row permutation of WICA that has a zeroless diagonal  Each such row permutation corresponds to a placement of n rooks onto the non zero entries on an n  n chessboard such that no two rooks threaten each other  Then the rows are permuted so that all the rooks end up on the diagonal  thus ensuring that the diagonal has no zeros  To solve this problem  we use a simple depth first search that prunes search paths that have nowhere to place the next rook  In the worst case  every permutation is admissible  and the search must take O n       One needs to be careful when doing this  since each run of ICA may return a WICA in a different row permutation  This means that we first need to row permute each bootstrap WICA to match with the original WICA     One could object that  instead of a quantile test  the correct procedure would be to simulate under the null hypothesis  i e   edge is absent  using the estimated error terms  and then compare the obtained distribution of the ICA statistics with their distribution for the bootstrap  However  this raises issues and complexities that are tangential to the current paper        A non local algorithm  Local algorithms work under the assumption that the estimates of the wi j are independent of each other  which is in general false when estimating with finite samples  This motivates the use of non local methods  In the LiNGAM  acyclic  approach       a non local algorithm is presented for finding the single best rowpermutation of WICA   which minimizes a loss function that heavily penalizes entries in the diagonal that are close to zero  such as x     x    This is written as a linear assignment problem  i e  finding the best match between the ei s and xi s   which can be solved using the Hungarian algorithm     or others  For general LiNG discovery  however  algorithms that find the best linear assignment do not suffice  since there may be multiple admissible permutations  One idea is to use a k th best assignment algorithm      i e  the k th permutation with the least penalty on the diagonal   for increasing k  With enough data  all permutations corresponding to inadmissible models will score poorly  and there should be a clear separation between admissible and inadmissible models  The non local method presented above  like the thresholding method  fails to account for differences in spread among estimates of the entries of WICA   It would be straightforward to fix this by modifying the loss function to penalize diagonal entries for which the test fails to reject the null hypothesis  as described in the part about bootstrap sampling in Section         instead of penalizing them for merely being close to zero       Sample run  We generated       sample points using the SEM in Example   and error terms distributed according to a symmetric Gaussian squared distribution    Fig    shows the output of the local thresholding algorithm with the cut off set to       For the sake of reproducibility  our code with instructions is available from  www phil cmu edu  tetrad  cd     html       Theory       Notions of DG equivalence  There are a number of different senses in which the directed graphs associated with SEMs can be equivalent or indistinguishable given observational data     The distribution was created by sampling from the standard Gaussian      and squaring it  If the value sampled was negative  it was made negative again    the error terms are assumed to be Gaussian  then distribution equivalence entails  but is not entailed by  covariance equivalence  which entails  but is not entailed by  d separation equivalence   Fig     The output of LiNG D  Candidate    and Candidate     assuming linearity and no dependence between error terms   DGs G  and G  are zero partial correlation equivalent if and only if the set of zero partial correlations entailed for all values of the free parameters  non zero linear coefficients  distribution of the error terms  of a linear SEM with DG G  is the same as the set of zero partial correlations entailed for all values of the free parameters of a linear SEM with G    For linear models  this is the same as d separation equivalence        DGs G  and G  are covariance equivalent if and only if for every set of parameter values for the free parameters of a linear SEM with DG G    there is a set of parameter values for the free parameters of a linear SEM with DG G  such that the two SEMs entail the same covariance matrix over the substantive variables  and vice versa   DGs G  and G  are distribution equivalent if and only if for every set of parameter values for the free parameters of a linear SEM with DG G    there is a set of parameter values for the free parameters of a linear SEM with DG G  such that the two SEMs entail the same distribution over the substantive variables  and vice versa  Do not confuse this with the notion of distribution entailment equivalence between SEMs  two SEMs with fixed parameters are distribution entailment equivalent iff they entail the same distribution  It follows from well known theorems about the Gaussian case       and some trivial consequences of known results about the non Gaussian case       that the following relationships exist among the different senses of equivalence for acyclic graphs  If all of the error terms are assumed to be Gaussian  distribution equivalence is equivalent to covariance equivalence  which in turn is equivalent to d separation equivalence  If not all of  So for example  given Gaussian error terms  A  B and A  B are zero partial correlation equivalent  covariance equivalent  and distribution equivalent  But given non Gaussian error terms  A  B and A  B are zero partial correlation equivalent and covariance equivalent  but not distribution equivalent  So for Gaussian errors and this pair of DGs  no algorithm that relies only on observational data can reliably select a unique acyclic graph that fits the population distribution as the correct causal graph without making further assumptions  but for all  or all except one  nonGaussian errors there will always be a unique acyclic graph that fits the population distribution  While there are theorems about the case of cyclic graphs and Gaussian errors  we are not aware of any such theorems about cyclic graphs with non Gaussian errors with respect to distribution equivalence  In the case of cyclic graphs with all Gaussian errors  distribution equivalence is equivalent to covariance equivalence  which entails  but is not entailed by  dseparation equivalence       In the case of cyclic graphs in which at most one error term is non Gaussian  distribution equivalence entails  but is not entailed by  covariance equivalence  which in turn entails  but is not entailed by  d separation equivalence  However  given at most one Gaussian error term  the important difference between acyclic graphs and cyclic graphs is that no two different acyclic graphs are distribution equivalent  but there are different cyclic graphs that are distribution equivalent  Hence  no algorithm that relies only on observational data can reliably select a unique cyclic graph that fits the data as the correct causal graph without making further assumptions  For example  the two cyclic graphs in Fig    are distribution equivalent       The output of LiNG D is correct and as fine as possible  Theorem   The output of LiNG D is a set of SEMs that comprise a distribution entailment equivalence class  Proof  First  we show that any two SEMs in the output of LiNG D entail the same distribution  The weight matrix output by ICA is determined only up to scaling and row permutation  Intuitively  then  permuting the error terms does not change the mixture  Now  more formally    Let M  and M  be candidate models output by LiNGD  Then W  and W  are row permutations of WICA   W    P  WICA   W    P  WICA Likewise  for the error terms  E    P  E  E    P  E Then the list of samples X implied by M  is A  E       W     E     P  WICA     P  E    WICA P    P  E     WICA E  By the same argument  the list of samples X implied   by M  is also WICA E  Therefore  any two SEM models output by LiNG D entail the same distribution  Now  it remains to be shown that if LiNG D outputs one SEM that entails a distribution P   it outputs all SEMs that entail P   Suppose that there is a SEM S that represents the same distribution as some T   which is output by LiNG D  Then the reduced form coefficient matrices for S and T   AS and AT   are the same up to columnpermutation and scaling  Hence  I  BS and I  BT are also the same up to scaling and row permutation  by I  B   A     By the assumption that there are no self loops with coefficient    neither I  BT nor I  BS has zeros on the diagonal  Since I  BT is a scaled row permutation of WICA that has no zeros on the diagonal  so is I  BS   Thus S is also output by LiNG D    Theorem   If the simultaneous equations are linear and can be represented by a directed graph  the error terms have non zero variance  the samples are independently and identically distributed  no more than one error term is Gaussian  and the error terms are jointly independent  then in the large sample limit  LiNG D outputs all SEMs that entail the population distribution  Proof  ICA gives pointwise consistent estimates of A and W under the assumptions listed      This entails that there are pointwise consistent tests of whether an entry in the W matrix is zero  and hence  by definition  in the large sample limit  the limit of both type I and type II errors of tests of zero coefficients are zero  Given the correct zeroes in the W matrix  the output of the local version of the LiNG D algorithm is correct in the sense that the simultaneous equation describes the population distribution    In general  each candidate model B     I  W   has the structure of a row permutation of WICA   The structures can be generated by analyzing what happens when we permute the rows of W     Remember that edges in B    and thus W     are read column torow  Thus  row permutations of W   change the positions of the arrow heads  targets   but not the arrow   tails  sources   Richardson proved that the operation of reversing a cycle preserves the set of entailed zero partial correlations  but did not consider distribution equivalence            Adding the assumption of stability  In dynamical systems  stable models are ones in which the effects of one time noise dissipate  For example  a model that has a single cycle whose cycleproduct  product of coefficients of edges in the cycle  is    is unstable  while one that has a single cycle whose cycle product is between    and   is stable  On the other hand  if a positive feedback loop of cycle product   is counteracted by a negative loop with cycle product      then the model is stable  because the effective cycle product is      A general way to express stability is lim B t      t which is mathematically equivalent to  for all eigenvalues e of B   e       in which  z  means the modulus of z  This eigenvalues criterion is easy to compute  Given only the coefficients between different variables  it is impossible to measure the stability of a SEM without assuming something about the self loops  Therefore  in this section  it is assumed that the true model has no self loops  It is often the case that many of the SEMs output by LiNG D are unstable  Since in many situations  the variables are assumed to be in equilibrium  we are often allowed to rule out unstable models  In the remainder of this section  we will prove that if the SEM generating the population distribution has a graph in which the cycles are disjoint  then among the candidate SEMs output by LiNG D  at most one will be stable  Theorem   SEMs in the form of a simple cycle with a cycle product  such that       are unstable  Proof  Let k be the length of the cycle  Then B k   I  Then for all integers i  B ik    i I  So if        the entries of B ik do not get smaller than the entries of B as i increases  Thus  B t will not converge to   as t      Corollary    For SEMs in the form of a simple cycle  having a cycle product    is equivalent to having an eigenvalue     in modulus   which is equivalent to being unstable  Theorem   Suppose that there is a SEM M with disjoint cycles with coefficient matrix B and graph G that entails a distribution Q  and a SEM M     M with graph G    coefficient matrix B    which is an admissible permutation of M and also entails Q  Then G    also contains disjoint cycles  at least one of which is a reversal of a cycle C in G  whose cycle product is the inverse of the cycle product of C  Proof  Due to space limitations  the proof is just sketched here  Every permutation can be represented as a product of disjoint cyclic subpermutations of the form a  b        m  n  a  where a  b means a gets mapped onto b   Some cyclic subpermutations may be trivial  i e  contain a single object mapped onto itself   Hence it suffices to prove the theorem for a single admissible cyclic row permutation of B  It can be shown that if a cyclic row permutation of B  a  b        m  n  a is admissible  then G contains the cycle C equal to a  b        m  n  a  and G  contains the reversed cycle C equal to a  b        m  n  a  Moreover  if G  contains two cycles that touch  so does G  Consider BC   the submatrix of B that contains the coefficients of the edges in cycle C            bk      b                 BC              b                      Qk  Note that the cycle product C   bk   i   bi i     WC   I  BC   The reversal is the row permutation in which the first row gets rotated into the bottom   b                         b         RowP ermute WC                                  bk   Normalizing the diagonal to be all  s  we get WC     Computing BC     IQ WC     one can see that the cyclek      product C     bk   i   bi i       C     We will now show that for SEMs in which the cycles are disjoint  their stability only depends on the stability of the cycles  Theorem   A SEM in which the cycles are disjoint is stable if and only if it has no unstable cycles  Proof  Let be G be a SEM whose cycles are disjoint  Then BG can be written as a block triangular matrix where each diagonal block is a cycle  The set of eigenvalues of a block triangular matrix is the union of the sets of eigenvalues of the blocks in the diagonal  in this  case  the eigenvalues of the cycles   Suppose a cycle of G is unstable  Then it has an eigenvalue     in modulus   But since this is also an eigenvalue of BG   it follows that G is unstable  The other direction goes similarly    Theorem   If the true SEM is stable and has a graph in which the cycles are disjoint  then no other SEMs in the output of LiNG D will be stable  Proof  Suppose the true SEM is stable and has a graph in which the cycles are disjoint  Call it G  Since  by Theorem    the output of LiNG D are the admissible distribution entailment equivalent alternatives to the true SEM  it suffices to show that all other admissible candidates are unstable  By Theorem    all cycles in G are stable  Let H be an admissible alternative to G  such that H    G  By Theorem    H will have at least one cycle C reversed relative to G and this reversed cycle will have a cycle product that is the inverse of the cycle product of C  By Corollary    the reversed matrix is not stable  Thus  by Theorem    H is unstable  Therefore  the only stable admissible alternative to G is G itself    It follows that if the true models cycles are disjoint  then under the assumption that the true model is stable  we can fully identify it using a LiNG discovery algorithm  at most one SEM in the output of the LiNG discovery algorithm will be stable   For example  consider the two candidate models shown in Fig     By assuming that the true model is stable  one would select candidate     Since our simulation used a stable model  this is indeed the correct answer  see Fig      In general  however  there may be multiple stable models  and one cannot reliably select the correct one  When the cycles are not disjoint  it is easy to find examples for which there are multiple stable candidates  The condition of disjoint cycles is sufficient  but not necessary  it is easy to come up with SEMs where we have exactly one stable SEM in the distributionentailment equivalence class  despite intersecting cycles      Discussion  We have presented Shimizus approach for discovering LiNGAM SEMs  and generalized it to a method that discovers general LiNG SEMs  This improves upon the state of the art on cyclic linear SEM discovery by outputting only the distribution entailment equivalence class of SEMs  instead of the entire d separation equiv    alence class  and by relaxing the faithfulness assumption  We have also shown that stability can be a powerful constraint  sometimes narrowing the candidates to a single SEM       D  Dash          Restructuring Dynamic Causal Systems in Equilibrium  Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics  AIStats        There are a number of questions that remain open for future research       F  Fisher          A correspondence principle for simultaneous equation models  Econometrica                 The LiNG D algorithm generates all admissible permutations  The worst case time complexity of n Rooks is high  but can we do better than depthfirst search for random instances  Is there an algorithm to efficiently search for the stable models  without going through all candidates  In the case where the cycles are disjoint  it is possible to just find the correct permutation for each cycle independently  but no such trick is known in general   How can prior information be incorporated into the algorithm   How can the algorithm be modified to allow the assumption of causal sufficiency to be relaxed  For the acyclic case  see       How can the algorithm be modified to allow for mixtures of non Gaussian and Gaussian  or almost Gaussian  error terms  Hoyer et al     address this problem for the acyclic case   How could we integrate this method into mainstream dynamical systems research  Can the algorithm handle noisy dynamics and noisy observations  Could it be made to handle non linear dynamics  What about self loops of coefficient    How could one integrate this with methods that use non equilibrium time series data  Acknowledgements The authors wish to thank Anupam Gupta  Michael Dinitz and Cosma Shalizi  GL was partially supported by NSF Award No  REC           
