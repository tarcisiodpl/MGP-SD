  Many AI synthesis problems such as planning or scheduling may be modelized as constraint satisfaction problems  CSP   A CSP is typically defined as the problem of finding any consis tent labeling for a fixed set of variables satisfy ing all given constraints between these variables  However  for many real tasks such as job shop scheduling  time table scheduling  design       all these constraints have not the same significance and have not to be necessarily satisfied  A first distinction can be made between hard constraints  which every solution should satisfy and soft con straints  whose satisfaction has not to be certain  In this paper  we formalize the notion of possi bilistic constraint satisfaction problems that al lows the modeling of uncertainly satisfied con straints  We use a possibility distribution over labelings to represent respective possibilities of each labeling  Necessity valued constraints al low a simple expression of the respective cer tainty degrees of each constraint  The main advantage of our approach is its integra tion in the CSP technical framework  Most clas sical techniques  such as Backtracking  BT   arc consistency enforcing  AC  or Forward Checking have been extended to handle possibilistics CSP and are effectively implemented  The utility of our approach is demonstrated on a simple design problem     Introduction  There are a lot of publications about constraints  and more specifically in the CSP framework  but most of these papers try to tackle the higly combinatorial nature  NP Hard  of such problems only considerin g hard constraints     This paper gives a clear meaning to what could be a soft constraint  how it may be expressed and how soft constraint satisfaction problems may be solved  Our aim is not to  e mail  schiex cert  fr  or  schiex ir it  fr  give a  general  theoretical framework for expressing soft constraints  such approaches may be found in  Satoh    using first and second order logic to express preferences or in  Freuder     relying on a problem space and a general measure on this space   but to give a specific  and hopefully useful  meaning to such constraints leading to  efficient  solving techniques  Non standard logics are manyfold that allows the expression of probabilities  Nilsson     orpreferences  Shoham     In particular  zadeh  s possibility theory  zadeh    has already been successfully used for modeling uncertainty and pref erences in the frame of propositional and first order logic by Dubois  Prade and Lang leading to the so called  pos sibilistic logic   Lang  b   One of the desirable feature of possibilistic semantics is the tolerance to  partial  con sistency  which allows a sort of paraconsistent reasoning  Another interesting feature of possibilistic logic is the close relationships between necessity measures and Gardenfors  epistemic entrechment  relation  Gardenfors et al      The main idea is to encapsulate preferences  or respective certainty degree  among labelings in a  possibility distribu tion  over labelings  Such a distribution naturally induces two  possibility and necessity  measures over constraints  However  it is not clear how to simply express such a dis tribution  A possible answer is to express bounds on necessity  or possibility  measures of constraints  defining a set of possi bility distribution among labelings  One can then define a set of  most possible  Iabelings satisfying these bounds  The structure of the paper is as follows   the section     re calls how a Constraint Satisfaction Problem may be defined and which objects are involved   the section     presents how a possibility distribution implicitly defines measures on constraints   the section     show s how bounds on ne cessity measures over constraints define  best  labelings  The next section rapidly presents algorithmic issues for pos sibilistic CSP solving and shows how specific satisfaction  Backtrack  and consistency enforcing  Arc consistency  Mackworth     techniques may be built  taking into ac count the induced possibility distribution  In section    we give an example of application of possibilis    Possibilistic Constraint Satisfaction Problems and k  are satisfied  k     ki is simply defined as the set of every labeling   over v  U Vj such that I f  k  and IF  k    tic CSP to a simple design problem  Both representation and solving issues are addressed  Section   compares our results with related works and is followed by a presentation of further possible researchs        Possibilistic constraint satisfaction problems       IF kj   An informal meaning  Let us breafty recall the definition of a classical Constraint Satisfaction Problem as definitions may change among au thors   The usual problem is then to find a labeling of the domain variables in V that satisfies the conjunction of every con straint in C  Cryptograms  such as SEND MORE MONEY   LYNDON B JOHNSON     then queens problem     are typ  ical instances of CSP   A typical CSP involves a set X   xi        x    of n vari ables and a set D of n associated domains  Each variable x  takes its value in its associated domain d   In the following  we shall restrict ourself to finite domains   Domains and variables are intrinsically bound together to form what we will call a domain variable  The domain variable i is defined as a pair   x   d   where x  is a variable and d  its associated domain  We will call V the set of domain variables defined by X and D  The fact that some domain variables take some specific values in their domains will be represented by a labeling  A labeling lw of a set W of domain variables is simply defined as an application on W such that    ViEW  lw i  Ed  Alternatively  a labeling lw wil l be considered as its map  the set    x  lw x   x E W    Further  a set of constraints C is considered  Each con straint k  it       i     on the set of domain variables v     it        inJ is a set of labelings of v    lw of W satisfies a lw F  k   iff v  c W  We will say that a labeling  k  it I   k  ij                    i      noted i    fl c lw       Let  Let I  be the set of every possible constraints on any non empty subset of V  The possibility distribution     on Lv induces two functions on K called possibility and necessity measures respectively noted n   and N   defined as follows    K       o     Vk E K  n   k    Sup   r l    E Lv l f  k   n     E  Let us consider lA and IB two partiallabe  ings  A C V  B c V   We will say that IA is more defined than IB  noted  A  IB  iff IB C IA  partial labeling  I  typically represents the set of every    For any given constraint  k  i         in    we will note  Lv          Given two constraints  k  and k   we will note k     kJ Vj that is satisfied when both k   N   T  is obviously equal to   i e   ever satisfied constraints are satisfied    N           SN    r  which is generally not equalto   l This means that unsatisfiable constraint may be somewhat re quired to be satisfied  This is dependant upon the fact that the possibility distribution  r is not required to be nonnalized  This choice has been made to cope with partial inconsisten     the constraint on v  u        Let us denote by     any unsatisfi able constraint  there is no I E Lv    f   li e   lis a constraint that contains no labeling  and by T the ever satisfied constraint  i e  the set of alllabelings on V     k   i       i     the constraint on V  that is unsatisfied when k  is satisfied    k   it        in   is simply the complement of k  i         in   in the set Lv  of every labelings over v       u  N    K         I  Vk E K  N  k    Inf     r l    E Lv l f    k  U  l   N  k       n     k   complete labeling that are more defined than   We finally define the following algebra over constraints    be the finite set of all possible labeling of the  malized if and only if    E Lv such that  r l    I  We define the sub normalization degree of  r as the quantity SN  r       Sup   r   f  E L    Obviously SN  r      iff  r is normalized     Definition      Lv  domain variables in V  A possibility distribution on Lv is a function  r from Lv to          r is said to be nor  constraint and     Modeling soft constraint with possibility and necessity measures  In classical first order logic  soft constraint may be formal ized through interpretation ordering  Satoh     P ossibilis tic CSP  as indicated by their name  relies on a possibility distribution over labelings   We will say that a labeling is complete iff it is defined on V  it will be partial otherwise   A  Given two constraints k  and k   we will note k  V ki the constraint on V  u Vi that is satisfied when one of k  or ki are satisfied  k  V ki is simply defined as the set of every labeling I over V  u Vj such that I f  k  or  cies     Yk  k  E K  N   k   k    Inf  l          E  Lv l   ktl kz   u                   Schiex   r  l   l E Lv l Vo k   U l   r l  l E Lv l  kz  U          Inf  N   kJ   N   kz         Inf   l    The possibility n      k  represents what its name suggests i e   the possibility for the constraint k to be satisfied accord ing to the knowledge of reference  The necessity N    k  tends towards   when the possibility that k being unsatisfied tends toward    measuring to what extent the satisfaction of k is entailed by the knowledge of reference  given by  r   Clearly  possibilisticCSP  as ssibilistic logic  is not meant to express fuzzy cons n as measures are attached to precise constraints  The statement  It is     necessary that the product be delivered before the   th  may be translated in possibilistic CSP to something like N     D               where Dis the variable corresponding to the delivering day  Possibilistic CSP is not intended to modelize a statement such as  The product should be delivered not too late after the  lth    r  trai ts  Because of the min and max operators  the precise values of neces sity or possibility is not so important  The essen tial is the total pre order induced by them  Thus  necessity degrees express preference degrees  N    k    N    k   ex pressing that the satisfaction of k is preferred to the satisfac tion of k   Therefore  possibilistic CSP are closely related to Hierarchical CSP as described in the frame of Constraint Logic Programming in  Boming et al       The notion of  constraint satisfaction will now depend on a possibility distribution  r on Lv  Let us consider   k  a  a necessity valued constraint     We will say  k  a  is satisfied by  r  noted  r F  k  a   itT the necessity measure Nf  induced by  r on K verifies N   k       a  Considering the whole constraint set C  we will say that the CSP   V  C  is satisfied by a possibility measure  r iff the necessity measure induced by  r verifies    that  V  k  a  E C  N   k       Possibilistic CSP   definition and semantics      where ki  it                  ti            Vague relations seen  as  a fu u y set of authaurized labeling   See  MC   Dubois et al    Rosenfeld et al   J    In fact  such a predicate may be decomposed in a set of  crisp predicates  a cuts of the vague constraint   In our case  the domains being finite  the set of a cuts is finite and a given fuzzy constraint may be decomposed in a finite set of possibilistic constraints  However  this  possibly automatic  conversion may be heavy and the result is  from an expressive view point  quite distant from the original knowledge   C V  C    Sup c Sup eLv    r  l    Sup  Fc l  SN  r   Sup iELv rt l          II V C      C V C  Inf    c SN  r     Inf      c   N              nf I reLv J   l                      A typical possibilistic CSP is then defined by a finite set X of variables  a finite set D of associated finite domains  defining a set V of domain variables  and by a fi nite set C of necessity valued constraints   It will be noted either  X  D  C  or  V  C   The necessity valued constraint  k  a   ex pres ses that N     k       a i e that the satisfaction of k is at least a necessary  The necessity valued constraint  k     expresses that k should absolutely be satisfied  and therefore takes the usual meaning of a constraint in classical CSP    k      is totally useless as it expresses that the necessity measure of k should be at least    which is always true          te  degree     E  On another hand  if we consider a specific  complete  labeling    its compatibility with the knowledge of reference  noted rt     will be the maximum of  r   l  for  Thus the degree of consistency of the possibilis c CSP or qv  C  may be defined as the maximum of   SN    r for every  r which satisfies C or  equivalently  as the compat ibility of the most compatible labeling  Its inconsistency degree ll V C  will be the complement to I of its consis ncy    in    a     in   is a classical constraint and a  If we consider a specific distribution  r  the most possible labeling will have a possibility equal to     SN  r     every  r which satisfies C  Its incompatibility  noted J     will be its complement to I   The only difference between a classical and a possibilistic CSP is the introduction of necessity valued constraint in stead of simple constraint  A necessity valued constraint is a pair    ki il     a  Thus a possibilistic CSP has not a set of consistent labeling  but a set of possibility distributions on the set of all labelings on V              Thus  the inconsistency degree of a possibilistic CSP is equal to the smallest necessity degree of the unsatisfiable constraint l  for all possibility distribution satisfying C   ti  The computa on of the inconsistency degree of a possi bilistic CSP is made easier by the fact that one can define a maximal possibility distribution among all possibility dis tribution satisfying  V  C   Theorem     Let  P    V  C  be a possibilistic CSP  we define the possibility distribution  r p on Lv by    V l E  Lv    rp l      lnf l  a  ec          a      F ki  U        Thenfor any possibility distribution  r on Lv  if     s  ll p  Proof  ll   satisfies     iff  v   k  ai  E C   iff v  k   a    E  C   ll   satisfies  N   k          k   a   a    r  satisfies  P   Possibilistic Constraint Satisfaction Problems  obtained by extending the labeling lon             i  with a new variable i     and every possible label in di l The leafs of the trees are complete labelings that may  or not  satisfy every constraint  In a depth first exploration of the tree the first labeling that satisfies every constraint is retained   iff v  k  a   E C  Inf      r  l Jll    k   U          a  iff v  k  a   E C V ll    k   r l      a  iff v l E Lv   ll  l    nf k      c     a  ll    k   U       iff v l E Lv  ll  l     ll p l  D Corollary      We simply conclude that      It           p  l       J l                            C P   SupiELv  Tp l        SN  ri     Su plE L v        k  cri Ec       O i    F   k t  U        Proo f  The two first points are immediate  According to theorem       we know that       r that satisfies P   r    Kp  i e    v  r that satisfies P      E Lv       r l        v  ll  that satisfies P  SN lr       SN  ll p     So   The  corresponding  result  for  the  U  degree  Test and Generate  The next step towards sophistication  and efficiency  is the  test and generate  approach  often referred as the  Back track  algorithm  BT   The obvious idea is to cut each branch that will necessarily lead to complete labelings that do not satisfy every constraint  Each non terminal node corresponds to a partial labeling I  To possibly lead to a complete labeling that satisfies every constraint  a partial labeling should be consistent    If a partial labeling  looses its consistency property  every labeling I  more defined than I will also be non consistent  In the case of complete labeling  non consistency is equiv alent with non satisfaction             consistency         Definition     Given a classical CSP  V  C   a partial la beling lA on the set of domain variables A C V will be consistenti ffVki E C such that V  C A  lA f  kj        rp l    II P   Inf  l c SN  r     SN  ll p    lnf eLv  Sup k   ai ec   a   I f    k    The corresponding approach in possibilistic CSP will be an optimization problem on the same tree  For each leaf of the tree  we may compute the value of  Tp on the corresponding complete labeling  In a depth first exploration of the tree  we will retain the set of the labelings that maximize  Tp  l    is  immediate O  Then  computing the inconsistency degree of a CSP means computing the sub nonnalization degree of the distribu tion  r    The set of all labeling Lv being finite  we can define the set Lv of all Jabelings of V such that VI  E Ly  rp l      SN  r     Thiswill be called the set of the best labelings of  P  Its elements are the most com patible labelings with the CSP  P    V  C  among every labeling  The problem of finding a  best labeling  may be reduced to find a labeling r that solve any of the following equivalent Min Max optimisation problems     l depth first tree exploration  the property of consistency is simply checked at each node  Backtrack occurs when it is not verified  One should note that if a labeling I on             i  is consistent  each labelings I  on             i  i      is consistent iff it satisfies the constraints k  such that V  c            i  i      and V  rt           i     In t  e case of  In the framework of possibilistic CSP  we extend the notion of compatibility to partial labelings  Definition      The compatibility ofa partiallabeling l A on A is defined as the maximum of the compatibility of every complete labeling more defined than  A    C V C   SUPIELv Inf k  a  EC     o ifl F   ki  U  I    II V C    InfleLv Sup k  a  EC   ad I F   k   U       Such problems may be tackled through many classical tree search algorithms  namely Depth first Branch and Bound  DFBB   a       or SSS               Our aim will be to compute  for each node  i e  each partial labeling  an upper bound on the compatibility of the partial labeling  An easily computed upper bound of this value is given by   Extending classical CSP algorithms Generate and Test  The more obvious algorithm to solve classical CSP is the  generate and test  algorithm  It traverses the domain variables in a predetermined order             n   In the tree explored  each node corresponds to a labeling  The root of the tree is the empty labeling  the sons of a node l are  Infc c    cr     e c      l   ai  Vi  C  A  lA F     k   U       This bound is exact for complete labelings  Moreover  it may be incrementally computed as the tree is traversed downwards   if a labeling I on A has been granted an upper bound f   each labeling  I on A u     ci   di     more defined than l is granted the upper bound                   Schiex          lnf        E C  C AU   xj dj     A classical n ary CSP is said to be arc consistent iff for every domain variable j  the domain d  is not empty  and for every label v E di  for every constraint k  such that j E V   there is a labeling I on V   more defined than the labeling   j  v       that satisfies k    U       a     k   a   V   V  q  A  I  I   k     For the sake of clarity  given an explicit ordering                  n  on the domain variables  we will note cf   the set of the necessity valued constraints  k   a   such that V  c           j j      and V  ct           j   If we note    the upper bound previously computed on a labeling I on             j  and l  a labeling on            j       more defined than I  we may compute the upper bound     on the compatibility of I  via         Inf  iJ  U     a    k  a    E      CJ  I   I  I   k      lllis decreasing bound is used in a DFBB algorithm to compute one  or every  best labeling  The algorithm sim ply stans with the empty labeling and extends it according to the vertical ordering    It maintains two parameters of importance   the number a under which a cutoff should take place  increased each time a complete labeling with an augmented compatibility has been found  It should be initially set to zero to ensure optimality  a cutoff takes place as soon as     a  and the number    over which no im provement is possible  the bound on the compatibility of the current partial labeling  It should initially be set to     These two bounds offer a great deal of flexibility       Typically  if a labeling whose possibility is lower than a is considered as useless  the algorithm should be called with a set to a  allowing a more efficient pruning of the tree  On the opposite side  if a labeling of possibility b is considered as enough  the algorithm should be called with f  set to b  allowing the algorithm to stop as soon as a    consistent labeling has been found   Naturally  in the first case we may fail to find a best labeling if its consistency degree is lower than a   in the second case  we have no garantee that the best labeling has been found  Alternatively  one may stop the algorithm execution upon any event  time exhausted       and get the best labeling found up to the occurrence of the event  getting closer to an  anytime algorithm   Dean et al        The algorithm that converts a CSP in an equivalent  arc consistent CSP  if it exists  is usually embodied in a single procedure Revise  apply ing to a domain variable j and a constraint k   j E V    that suppresses every label in the domain di that does not satisfy the previous property  This procedure is applied repetedly on the whole CSP until sta bilization  ACl to AC    In our case  such a label may stil l be possible if the constraint k  is not   necessary  In general  the knowledge we may extract is an upper bound on the compatibility of the partial labelings that maps a single variable to a label  If we consider a variable  k  such that j  E  b    j  v    k       V   and  j and a  non unary  constraint if we note Uv  the set of unary  constraint on any of the variables in V   the upper bound  on the compatibility of the partial labeling    j  v      v E d   taking into account k  and every unary constraint in Uv  is equal to  Supi ELv   l t  j v     Inf k   a   E Vv  u    l   ao           an  f  F   kn  U        A possibilistic CSP will be said arc consistent if for every domain variable j  the domain di is not empty and for every label v E di  the compatibility of   j  v     with respect to the possibilistic CSP    j   U j   is strictly positive and equal to the minimum of the such that j E V    b     j  v    k    for every  k   More precisely  if we note Pi the CSP defined by    j   U j     a possibilistic CSP is   arc consistent if it is arc consistent  and          InfjEv SupvEdj ll j i    j  v        It may be shown that   is an upper bound on the overall consistency  C V  C   The main idea to convert a possibilistic CSP into an equivalent  arc consistent possibilistic CSP is then to add unary necessity valued constraints  rather than suppressing labels  reflecting this bound and to take these new unary constraints in account when the process is repetedly ap  Every usual vertical heuristic ordering  max  cardinality  max  degree         may be applied to this tree search  An horizontal heuristic is given by the current bounds obtained for the various labels  but its efficiency is y et to be evalu ated  it is welllrnown that horizontal ordering has a strong influence on the efficiency of Min Max problems solving  e g  in Alpha Beta algorithm applied to games  Pearl       The Revise   procedure we have defined not only filters out necessarily inconsistent labels  but also compute for each label v Edithe upper bound b    j  v      k   on the compat ibility of the partial labeling I that maps the variable j being          J rwo CSP     and     are equivalent if they have the same set of solutions  i e  VI  ll         II    z   Consistency enforcing  A further step is to extend the various local consis tency notions  node  arc  path and k consistency  Mack worth   Montanari     and their corresponding filtering al gorithms Mohr et al    Deville et al       plied   rt is precisely the compatibility of the labeling    j  v   in the CSP   j  U V    k   u Uv     Two possibilistic CSP     and     are equivalent if they have the same set of satisfying possibility distributions  i e  V r   r I  p     II          or equivalently  ll p    ll p    Possibilistic Constraint Satisfaction Problems  filtered to this label v taking into account the constraint k   This bound b may be simply encoded in the CSP by adding a simple unary constraint  on j indicating that this label is forbidden with a necessity     b l  ki     I I I         o            I I      The additionnal information obtained is taken into account in the tree search algorithm and may greatly enhance the performances of the algorithm  tighter bounds on partial in consistencies are obtained earlier   The termination  which is quite trivial  and complexity of the algorithm  the unicity of the problem obtained are yet to be formaly determined  Limited applications of the Reviser procedure during the tree search exploration  so called Forward Checking  or Partial Look Ahead  that are usual in CSP are immediately usable in possibilistic CSP and have been implemented   b  should be noted that a possibilistic CSP containing only HARD constraints is strictly equivalent to a classi cal CSP and that extended algorithms  tree exploration  arc consistency  behave exactly as corresponding classical CSP algorithms  Therefore  softness  costs  almost  noth ing when left unused  The only overhead is due to the manipulation of the floating point numbers     and     and the operators min max instead of boolean true and false and logical operators and or   It     appJe pie ice fruit  Figure    Gastronomic CSP hypergraph  A design problem  A great restaurant want to offer to its clients a computer aided menu designer  The system should integrate  know how  knowledge and customer desires to compose a  best menu composed of a drink  white or red wine  beer or water   an entrance  smoked salmon  caviar   foie gras   oysters or nothing   a main dish  fish of the day  leg of wild boar  sauerkraut  and a dessert  apple pie  strawberry ice  fruit or nothing    The sauerkraut should be accompanied by a beer   a          white whine may be possibly considered  b        or even water  c          Fish may not be eaten twice in the menu  caviar and oys  ters will be considered  as   fishes    d        and should be      I would like to eat some fish  n                 I would like to taste the sauerkraut  o         ate  Cf  figure     The basic constraints are represented by the continuous arcs  the client constraints are represented by dotted arcs   The tree explored with the previously out lined  DFBB  algorithm using the ordering  Dish  Drink  is given figure    Labels are given by their capitals  cutoffs are indicated by thick lines  The first  best menu  found  compatibility      is as follows    main dish   Fish of the day    Foie gras should be accompanied by a soft white wine  h     drink   White wine             entrance   Foie gras      dessert   Apple pie    Meat should  almost  certainly be eaten with a red wine  g             I surely do not want any oysters in my menu  m   Entrance  Dessert   accompanied with white wine  e       or water  f            The encoding in a   variables possibilistic CSP is immedi  We shall first consider the following knowledge    Our client now integrate its preferences    After the leg of wild boar  a strawbeny ice  as  very good  digestive effects  i          No entrance or no dessert is not appreciated  by the restau rant   j        having both no entrance and dessert is even    less appreciated   k         Having water  a drink is no good            as      ne may also define   weak arc consistency enforcing by limiting the Revise r to the inference of unary constraints whose  necessity is greater or equal than  Y   weak arc consistency leads to label suppression    weak arc consistency is possibilistic arc  consistency   The overall consistency degree of the CSP is therefore equal to      As the knowledge introduced makes no difference between a soft and a dry white wine  our customer will either drink its  foie gras  with a dry wine or its fish with a soft wine  Nobody is perfect     The size of this problem makes arc consistency enforcing and forward checking useless  Nevertheless  one may note that the problem is actually     arc consistent  As an exam ple  one of the unary constraint infered by arc consistency             Schiex  been extended to take in account fuzzy antagonist temporal constraints  F    A       I I I I I   aooO l WW RW B    o  s   WB  o     p oAP  s  aoo    ww p o    W  CFG  a     I  a         N          AP  Further researchs  We are currentl y working on the conversion of the possi bilistic AC  like algorithm to more sophisticated schemes as AC   Mohr et al      A matter of study is also the fix point semantics of the possibilisticarc consistency as is has been done for classical CSP  Gusgen et al      Several extension of possibilistic CSPs may be considered      l   a   P       a  o P  AP        J  Figure    DFBB search      enforcing is a constraint that forbids the label  white vine  with a     necessity  as b     drink  white wine      a          We are currently trying to apply these techniques in the frame of job shop scheduling  It is clear that the particular nature of the constraints that appears in this framework could  and should  be taken into account in the propagation process  as it may be done in the AC    Deville et al     algorithm in classical CSP     Related works  The obviously related work is   possibilistic logic   Lang  b  which has been a fundamental basis for pos sibilistic CSP definition  J  Lang  Lang  a  has applied propositional possibilistic logic to constraint satisfaction problems  In our opinion  our approach offers greater expressive power  let us recall that the encoding of the SEND MORE MONEY problem in propositional logic leads to no more than      clauses and    propositional vari ables  and more varied and powerful techniques  the only resolution technique used in propositional possibilistic logic being essentialy a  backjumping Iike  algorithm  Oxusoff et al    Gashnig      Other related works include Hierarchical Constraint Logic Programming  Boming et al     that allows the expression of prioritized constraints in the body of an Hom clause  Satoh  Satoh    proposes a formalisation of soft constraints based on an interpretations ordering but does not provide any algorithmic issue  The system GARI  Descottes et al     which is more ori ented towards production rules is very close to ours as it compute a solution that is the best compromise under a set of antagonist constraints  It is also close to the OPAL scheduling system  Bel et al    Bel et al     which has      Many CSP techniques  AC n  path or k consistency  backjumping  learning  tree clustering  cycle cutset  and useful properties  Freuder theorems  Freuder     should be adapted or extended to possibilistic CSP   The integration of fuzzy constraints  defined as a fuzzy set of authorized labelings  is almost immediate and leads to an even greater expressive power  As is has been shown for possibilistic logic  Dubois et al   b   the pre order induced by necessity valued constraints is a numerical  epistemic entrenchment  relation  Gardenfors et al      The consistency degree of a possibilistic CSP may be considered as an indica tor of the constraints that should be suppressed for the  contraction  of a CSP upon revision  However  as an anonymous referee pointed out  that means excluding every constraint below the inconsistency degree  This is somewhat too drastic  for some of these constraints may be not  involved  in inconsistencies  This could be corrected by an adequate redefinition of the label ing compatibility  or by complete redefinition of the mesure used  However  algorithmic issues will have to be reconsidered  Possibility and necessity measures may be seen as spe cific decomposable measures  Dubois et a       We think that most of this work could be easily extended to such measures  including probabilistic measures   Algorithmic issues will again have to be reconsidered  Possibilistic logic programming as been experimented in  Dubois et al   a   The integration of Possibilistic logic programming and possibilistic CSP is a first step toward Possibilistic Constraint Logic Programming   Acknowledgements  The author wants to thank Helene Fargier  Jerome Lang and the anonymous referees for their fruitful comments on previous releases of this paper  
 The Weighted Constraint Satisfaction Problem  WCSP  framework allows representing and solving problems involving both hard constraints and cost functions  It has been applied to various problems  including resource allocation  bioinformatics  scheduling  etc  To solve such problems  solvers usually rely on branch and bound algorithms equipped with local consistency filtering  mostly soft arc consistency  However  these techniques are not well suited to solve problems with very large domains  Motivated by the resolution of an RNA gene localization problem inside large genomic sequences  and in the spirit of bounds consistency for large domains in crisp CSPs  we introduce soft bounds arc consistency  a new weighted local consistency specifically designed for WCSP with very large domains  Compared to soft arc consistency  BAC provides significantly improved time and space asymptotic complexity  In this paper  we show how the semantics of cost functions can be exploited to further improve the time complexity of BAC  We also compare both in theory and in practice the efficiency of BAC on a WCSP with bounds consistency enforced on a crisp CSP using cost variables  On two different real problems modeled as WCSP  including our RNA gene localization problem  we observe that maintaining bounds arc consistency outperforms arc consistency and also improves over bounds consistency enforced on a constraint model with cost variables      Introduction The Weighted Constraint Satisfaction Problem  WCSP  is an extension of the crisp Constraint Satisfaction Problem  CSP  that allows the direct representation of hard constraints and cost functions  The WCSP defines a simple optimization  minimization  framework with a wide range of applications in resource allocation  scheduling  bioinformatics  Sanchez  de Givry    Schiex        Zytnicki  Gaspin    Schiex         electronic markets  Sandholm         etc  It also captures fundamental AI and statistical problems such as Maximum Probability Explanation in Bayesian nets and Markov Random Fields  Chellappa   Jain         As in crisp CSP  the two main approaches to solve WCSP are inference and search  This last approach is usually embodied in a branch and bound algorithm  This algorithm estimates at each node of the search tree a lower bound of the cost of the solutions of the sub tree  c      AI Access Foundation  All rights reserved    Zytnicki  Gaspin  de Givry   Schiex  One of the most successful approaches to build lower bounds has been obtained by extending the notion of local consistency to WCSP  Meseguer  Rossi    Schiex         This includes soft AC  Schiex         AC   Larrosa         FDAC   Larrosa   Schiex         EDAC   Heras  Larrosa  de Givry    Zytnicki         OSAC  Cooper  de Givry    Schiex        and VAC  Cooper  de Givry  Sanchez  Schiex    Zytnicki        among others  Unfortunately  the worst case time complexity bounds of the associated enforcing algorithms are at least cubic in the domain size and use an amount of space which is at least linear in the domain size  This makes these consistencies useless for problems with very large domains  The motivation for designing a local consistency which can be enforced efficiently on problems with large domains follows from our interest in the RNA gene localization problem  Initially modeled as a crisp CSP  this problem has been tackled using bounds consistency  Choi  Harvey  Lee    Stuckey        Lhomme        and dedicated propagators using efficient pattern matching algorithms  Thebault  de Givry  Schiex    Gaspin         The domain sizes are related to the size of the genomic sequences considered and can reach hundreds of millions of values  In order to enhance this tool with scoring capabilities and improved quality of localization  a shift from crisp to weighted CSP is a natural step which requires the extension of bounds consistency to WCSP  Beyond this direct motivation  this extension is also useful in other domains where large domains occur naturally such as temporal reasoning or scheduling  The local consistencies we define combine the principles of bounds consistency with the principles of soft local consistencies  These definitions are general and are not restricted to binary cost functions  The corresponding enforcing algorithms improve over the time and space complexity of AC  by a factor of d and also have the nice but rare property  for WCSP local consistencies  of being confluent  As it has been done for AC   by Van Hentenryck  Deville  and Teng        for functional or monotonic constraints  we show that different forms of cost functions  largely captured by the notion of semi convex cost functions  can be processed more efficiently  We also show that the most powerful of these bounds arc consistencies is strictly stronger than the application of bounds consistency to the reified representation of the WCSP as proposed by Petit  Regin  and Bessiere         To conclude  we experimentally compare the efficiency of algorithms that maintain these different local consistencies inside branch and bound on agile satellite scheduling problems  Verfaillie   Lematre        and RNA gene localization problems  Zytnicki et al         and observe clear speedups compared to different existing local consistencies      Definitions and Notations This section will introduce the main notions that will be used throughout the paper  We will define the  Weighted  Constraint Satisfaction Problems  as well as a local consistency property frequently used for solving the Weighted Constraint Satisfaction Problem  arc consistency  AC          Bounds Arc Consistency for Weighted CSPs      Constraint Networks Classic and weighted constraint networks share finite domain variables as one of their components  In this paper  the domain of a variable xi is denoted by D xi    To denote a value in D xi    we use an index i as in vi   vi        For each variable xi   we assume that the domain of xi is totally ordered by i and we denote by inf xi   and sup xi   the minimum  resp  maximum  values of the domain D xi    An assignment tS of a set of variables S    xi            xir   is a function that maps variables to elements of their domains  tS    xi   vi            xir  vir   with i   i            ir    tS  xi     vi  D xi    For a given assignment tS such that xi  S  we simply say that a value vi  D xi   belongs to tS to mean that tS  xi     vi   We denote by lS   the set of all possible assignments on S  Definition     A constraint network  CN  is a tuple P   hX   D  Ci  where X    x            xn   is a set of variables and D    D x             D xn    is the set of the finite domains of each variable  C is a set of constraints  A constraint cS  C defines the set of all authorized combinations of values for the variables in S as a subset of lS   S is called the scope of cS    S  is called the arity of cS   For simplicity  unary  arity    and binary  arity    constraints may be denoted by ci and cij instead of c xi   and c xi  xj   respectively  We denote by d the maximum domain size  n  the number of variables in the network and e  the number of constraints  The central problem on constraint networks is to find a solution  defined as an assignment tX of all variables such that for any constraint cS  C  the restriction of tX to S is authorized by cS  all constraints are satisfied   This is the Constraint Satisfaction Problem  CSP   Definition     Two CNs with the same variables are equivalent if they have the same set of solutions  A CN will be said to be empty if one of its variables has an empty domain  This may happen following local consistency enforcement  For CN with large domains  the use of bounds consistency is the most usual approach  Historically  different variants of bounds consistency have been introduced  generating some confusion  Using the terminology introduced by Choi et al          the bounds consistency considered in this paper is the bounds D  consistency  Because we only consider large domains defining intervals  this is actually equivalent to bounds Z  consistency  For simplicity  in the rest of the paper we denote this as bounds consistency  Definition      Bounds consistency  A variable xi is bounds consistent iff every constraint cS  C such that xi  S contains a pair of assignments  t  t    lS  lS such that inf xi    t and sup xi    t   In this case  t and t are called the supports of the two bounds of xi s domain  A CN is bounds consistent iff all its variables are bounds consistent  To enforce bounds consistency on a given CN  any domain bound that does not satisfy the above properties is deleted until a fixed point is reached        Zytnicki  Gaspin  de Givry   Schiex      Weighted Constraint Networks Weighted constraint networks are obtained by using cost functions  also referred as soft constraints  instead of constraints  Definition     A weighted constraint network  WCN  is a tuple P   hX   D  W  ki  where X    x            xn   is a set of variables and D    D x             D xn    is the set of the finite domains of each variable  W is a set of cost functions  A cost function wS  W associates an integer cost wS  tS        k  to every assignment tS of the variables in S  The positive number k defines a maximum  intolerable  cost  The cost k  which may be finite or infinite  is the cost associated with forbidden assignments  This cost is used to represent hard constraints  Unary and binary cost functions may be denoted by wi and wij instead of w xi   and w xi  xj   respectively  As usually for WCNs  we assume the existence of a zero arity cost function  w      k   a constant cost whose initial value is usually equal to    The cost of an assignment tX of all variables is obtained by combining the costs of all the cost functions wS  W applied to the restriction of tX to S  The combination is done using the function  defined as a  b   min k  a   b   Definition     A solution of a WCN is an assignment tX of all variables whose cost is less than k  It is optimal if no other assignment of X has a strictly lower cost  The central problem in WCN is to find an optimal solution  Definition     Two WCNs with the same variables are equivalent if they give the same cost to any assignments of all their variables  Initially introduced by Schiex         the extension of arc consistency to WCSP has been refined by Larrosa        leading to the definition of AC   It can be decomposed into two sub properties  node and arc consistency itself  Definition      Larrosa        A variable xi is node consistent iff   vi  D xi    w  wi  vi     k   vi  D xi   such that wi  vi        The value vi is called the unary support of xi   A WCN is node consistent iff every variable is node consistent  To enforce NC on a WCN  values that violate the first property are simply deleted  Value deletion alone is not capable of enforcing the second property  As shown by Cooper and Schiex         the fundamental mechanism required here is the ability to move costs between different scopes  A cost b can be subtracted from a greater cost a by the function  defined by a  b    a  b  if a    k and k otherwise  Using   a unary support for a variable xi can be created by subtracting the smallest unary cost minvi D xi   wi  vi   from all wi  vi   and adding it  using   to w  This operation that shifts costs from variables to w  creating a unary support  is called a projection from wi to w  Because  and  cancel out  defining a fair valuation structure  Cooper   Schiex         the obtained WCN is equivalent to the original one  This equivalence preserving transformation  Cooper and Schiex  is more precisely described as the ProjectUnary   function in Algorithm    We are now able to define arc and AC  consistency on WCN        Bounds Arc Consistency for Weighted CSPs  Algorithm    Projections at unary and binary levels                       Procedure ProjectUnary xi   min  minvi D xi    wi  vi      if  min      then return  foreach vi  D xi   do wi  vi    wi  vi    min   w  w  min      Find the unary support of xi    Procedure Project xi   vi   xj     Find the support of vi w r t  wij   min  minvj D xj    wij  vi   vj      if  min      then return  foreach vj  D xj   do wij  vi   vj    wij  vi   vj    min   wi  vi    wi  vi    min    Definition     A variable xi is arc consistent iff for every cost function wS  W such that xi  S  and for every value vi  D xi    there exists an assignment t  lS such that vi  t and wS  t       The assignment t is called the support of vi on wS   A WCN is AC  iff every variable is arc and node consistent  To enforce arc consistency  a support for a given value vi of xi on a cost function wS can be created by subtracting  using   the cost mintlS  vi t wS  t  from the costs of all assignments containing vi in lS and adding it to wi  vi    These cost movements  applied for all values vi of D xi    define the projection from wS to wi   Again  this transformation preserves equivalence between problems  It is more precisely described  for simplicity  in the case of binary cost functions  as the Project   function in Algorithm     Example     Consider the WCN in Figure   a   It contains two variables  x  and x     each with two possible values  a and b  represented by vertices   A unary cost function is associated with each variable  the cost of a value being represented inside the corresponding vertex  A binary cost function between the two variables is represented by weighted edges connecting pairs of values  The absence of edge between two values represents a zero cost  Assume k is equal to   and w is equal to    Since the cost w   x   a  is equal to k  the value a can be deleted from the domain of x   by NC  first property   The resulting WCN is represented in Figure   b   Then  since x  has no unary support  second line of the definition of NC   we can project a cost of   to w  cf  Figure   c    The instance is now NC  To enforce AC   we project   from the binary cost function w   to the value a of x  since this value has no support on w    cf  Figure   d    Finally  we project   from w  to w  as seen on Figure   e   Ultimately  we note that the value b of x  has no support  To enforce AC   we project a binary cost of   to this value and remove it since it has a unary cost of   which  combined with w reaches k             Zytnicki  Gaspin  de Givry   Schiex  w      k     x  x   w      k     x  x  a        a     a           b     b  a             b  b     a  a     b  b           x      a  a     b  b   e  find unary support using ProjectUnary x      d  find support for  x   b  using Project x    b  x     AC       a        b   c  find unary support using ProjectUnary x     NC    w      k     x  x   w      k     x  x      a     b  prune forbidden values  NC     a  original instance  b  a        b  w      k     x  x   w      k     x        a b   f  Arc consistency enforced  Figure    Enforcing Arc Consistency      Bounds Arc Consistency  BAC  In crisp CSP  the bounds consistency enforcing process just deletes bounds that are not supported in one constraint  In weighted CSP  enforcement is more complex  If a similar value deletion process exists based on the first node consistency property violation  whenever w  wi  vi   reaches k   additional cost movements are performed to enforce node and arc consistency  As shown for AC   these projections require the ability to represent an arbitrary unary cost function wi for every variable xi   This requires space in O d  in general since projections can lead to arbitrary changes in the original wi cost function  even if they have an efficient internal representation   To prevent this  we therefore avoid to move cost from cost functions with arity greater than one to unary constraints  Instead of such projections  we only keep a value deletion mechanism applied to the bounds of the current domain that takes into account all the cost functions involving the variable considered  For a given variable xi involved in a cost function wS   the choice of a given value vi will at least induce a cost increase of mintS lS  vi tS wS  tS    If these minimum costs  combined on all the cost functions involving xi   together with w  reach the intolerable cost of k  then the value can be deleted  As in bounds consistency  this is just done for the two bounds of the domain  This leads to the following definition of BAC  bounds arc consistency  in WCSP  Definition     In a WCN P   hX   D  W  ki  a variable xi is bounds arc consistent iff      X wS  tS     k w  min tS lS  inf xi  tS  wS W xi S  w   X  wS W xi S     min  tS lS  sup xi  tS    wS  tS     k  A WCN is bounds arc consistent if every variable is bounds arc consistent        Bounds Arc Consistency for Weighted CSPs  One can note that this definition is a proper generalization of bounds consistency since when k      it is actually equivalent to the definition of bounds D  consistency for crisp CSP  Choi et al          also equivalent to bounds Z  consistency since domains are defined as intervals   The algorithm enforcing BAC is described as Algorithm    Because enforcing BAC only uses value deletion  it is very similar in structure to bounds consistency enforcement  We maintain a queue Q of variables whose domain has been modified  or is untested   For better efficiency  we use extra data structures to efficiently maintain the combined cost associated with the domain bound inf xi    denoted winf  xi    For a cost function wS involving xi   the contribution of wS to this combined cost is equal to mintS lS  inf xi  tS wS  tS    This contribution is maintained in a data structure inf  xi   wS   and updated whenever the minimum cost may change because of value removals  Notice that  in Algorithm    the line    is a concise way to denote the hidden loops which initialize the winf   wsup   inf and sup data structures to zero  Domain pruning is achieved by function PruneInf   which also resets the data structures associated with the variable at line    and these data structures are recomputed when the variable is extracted from the queue  Indeed  inside the loop of line     the contributions inf  xi   wS   to the cost winf  xi   from the cost functions wS involving xj are reset  The Function pop removes an element from the queue and returns it  Proposition      Time and space complexity  For a WCN with maximum arity r of the constraints  enforcing BAC with Algorithm   is time O er  dr   and space O n   er   Proof  Regarding time  every variable can be pushed into Q at most d     times  once at the beginning  and when one of its values has been removed  As a consequence  the foreach loop on line    iterates O erd  times  and the foreach loop on line    iterates O er  d  times  The min computation on line    takes time O dr    and thus  the overall time spent at this line takes time O er  dr    PruneInf   is called at most O er  d  times  The condition on line    is true at most O nd  times and so  line    takes time O ed   resetting inf  xi     on line    hides a loop on all cost functions involving xi    The total time complexity is thus O er  dr    Regarding space  we only used winf   wsup and  data structures  The space complexity is thus O n   er     Note that exploiting the information of last supports as in AC      Bessiere   Regin        does not reduce the worst case time complexity because the minimum cost of a cost function must be recomputed from scratch each time a domain has been reduced and the last support has been lost  Larrosa         However  using last supports helps in practice to reduce mean computation time and this has been done in our implementation  Compared to AC   which can be enforced in O n  d    time and O ed  space for binary WCN  BAC can be enforced d times faster  and the space complexity becomes independent of d which is a requirement for problems with very large domains  Another interesting difference with AC  is that BAC is confluent  just as bounds consistency is  Considering AC   it is known that there may exist several different AC  closures with possibly different associated lower bounds w  Cooper   Schiex         Note that although OSAC  Cooper et al         is able to find an optimal w  at much higher       Zytnicki  Gaspin  de Givry   Schiex  Algorithm    Algorithm enforcing BAC                                                                                     Procedure BAC X   D  W  k  QX   winf         wsup         inf           sup           while  Q      do xj  pop Q    foreach wS  W  xj  S do foreach xi  S do   mintS lS  inf xi  tS wS  tS     winf  xi    winf  xi    inf  xi   wS       inf  xi   wS       if PruneInf xi   then Q  Q   xi       mintS lS  sup xi  tS wS  tS     wsup  xi    wsup  xi    sup  xi   wS       sup  xi   wS       if PruneSup xi   then Q  Q   xi     Function PruneInf xi     boolean if  w  winf  xi     k  then delete inf xi     winf  xi        inf  xi          return true  else return false  Function PruneSup xi     boolean if  w  wsup  xi     k  then delete sup xi     wsup  xi        sup  xi          return true  else return false         Bounds Arc Consistency for Weighted CSPs  computational cost   it is still not confluent  The following property shows that BAC is confluent  Proposition      Confluence  Enforcing BAC on a given problem always leads to a unique WCN  Proof  We will prove the proposition as follows  We will first define a set of problems which contains all the problems that can be reached from the original WCN through BAC enforcement  Notice that  at each step of BAC enforcement  in the general case  several operations can be performed and no specific order is imposed  Therefore  a set of problems can be reached at each step  We will show that the set of problems has a lattice structure and ultimately show that the closure of BAC is the lower bound of this lattice  and is therefore unique  which proves the property  This proof technique is usual for proving convergence of the chaotic iteration of a collection of suitable functions and has been used for characterizing CSP local consistency by Apt         During the enforcement of BAC  the original problem P   hX   D  W  ki is iteratively transformed into a set of different problems which are all equivalent to P  and obtained by deleting values violating BAC  Because these problems are obtained by value removals  they belong to the set    P   defined by   hX   D   W  ki   D  D   We now define a relation  denoted   on the set    P     P    P         P   P   P   i      n   D   xi    D   xi   It is easy to see that this relation defines a partial order  Furthermore  each pair of elements has a greatest lower bound glb and a least upper bound lub in    P   defined by   P    P         P   glb P    P      hX    D   xi    D   xi     i      n    W  ki     P  lub P    P      hX    D   xi    D   xi     i      n    W  ki     P  h   P   i is thus a complete lattice  BAC filtering works by removing values violating the BAC properties  transforming an original problem into a succession of equivalent problems  Each transformation can be described by the application of dedicated functions from    P  to    P   More precisely  there are two such functions for each variable  one for the minimum bound inf xi   of the domain of xi and a symmetrical one for the maximum bound  For inf xi    the associated function keeps the instance unchanged if inf xi   satisfies the condition of Definition     and it otherwise returns a WCN where inf xi   alone has been deleted  The collection of all those functions defines a set of functions from    P   to    P   which we denote by FBAC   Obviously  every function f  FBAC is order preserving   P    P         P   P   P   f  P     f  P    By application of the Tarski Knaster theorem  Tarski         it is known that every function f  FBAC  applied until quiescence during BAC enforcement  has at least one fixpoint  and that the set of these fixed points forms a lattice for   Moreover  the intersection of the lattices of fixed points of the functions f  FBAC   denoted by    P   is also       Zytnicki  Gaspin  de Givry   Schiex  a lattice     P  is not empty since the problem hX                 Wi is a fixpoint for every filtering function in FBAC      P  is exactly the set of fixed points of FBAC   We now show that  if the algorithm reaches a fixpoint  it reaches the greatest element of    P   We will prove by induction that any successive application of elements of FBAC on P yields problems which are greater than any element of    P  for the order   Let us consider any fixpoint P  of    P   Initially  the algorithm applies on P  which is the greatest element of    P   and thus P   P  This is the base case of the induction  Let us now consider any problem P  obtained during the execution of the algorithm  We have  by induction  P   P    Since  is order preserving  we know that  for any function f of FBAC   f  P      P   f  P     This therefore proves the induction  To conclude  if the algorithm terminates  then it gives the maximum element of    P   Since proposition     showed that the algorithm actually terminates  we can conclude that it is confluent    If enforcing BAC may reduce domains  it never increases the lower bound w  This is an important limitation given that each increase in w may generate further value deletions and possibly  failure detection  Note that even when a cost function becomes totally assigned  the cost of the corresponding assignment is not projected to w by BAC enforcement  This can be simply done by maintaining a form of backward checking as in the most simple WCSP branch and bound algorithm  Freuder   Wallace         To go beyond this simple approach  we consider the combination of BAC with another WCSP local consistency which  similarly to AC   requires cost movements to be enforced but which avoids the modification of unary cost functions to keep a reasonable space complexity  This is achieved by directly moving costs to w      Enhancing BAC In many cases  BAC may be very weak compared to AC  in situations where it seems to be possible to infer a decent w value  Consider for example the following cost function    D x     D x     E D x      D x              w      v    v      v    v  AC  can increase w by    by projecting a cost of   from w   to the unary constraint w  on every value  and then projecting these costs from w  to w by enforcing NC  However  if w   w    w      and k is strictly greater than     BAC remains idle here  We can however simply improve BAC by directly taking into account the minimum possible cost of the cost function w   over all possible assignments given the current domains  Definition     A cost function wS is  inverse consistent   IC  iff  tS  lS   wS  tS       Such a tuple tS is called a support for wS   A WCN is  IC iff every cost function  except w  is  IC  Enforcing  IC can always be done as follows  for every cost function wS with a non empty scope  the minimum cost assignment of wS given the current variable domains is       Bounds Arc Consistency for Weighted CSPs  computed  The cost  of this assignment is then subtracted from all the tuple costs in wS and added to w  This creates at least one support in wS and makes the cost function  IC  For a given cost function wS   this is done by the Project   function of Algorithm    In order to strengthen BAC  a natural idea is to combine it with  IC  We will call BAC the resulting combination of BAC and  IC  To enforce BAC  the previous algorithm is modified by first adding a call to the Project   function  see line    of Algorithm     Moreover  to maintain BAC whenever w is modified by projection  every variable is tested for possible pruning at line    and put back in Q in case of domain change  Note that the subtraction applied to all constraint tuples at line    can be done in constant time without modifying the constraint by using an additional wS data structure  similar to the  data structure introduced by Cooper and Schiex         This data structure keeps track of the cost which has been projected from wS to w  This feature makes it possible to leave the original costs unchanged during the enforcement of the local consistency  For example  for any tS  lS   wS  t  refers to wS  t   wS   where wS  t  denotes the original cost  Note that wS   which will be later used in a confluence proof  precisely contains the amount of cost which has been moved from wS to w  The whole algorithm is described in Algorithm    We highlighted in black the parts which are different from Algorithm   whereas the unchanged parts are in gray  Proposition      Time and space complexity  For a WCN with maximum arity r of the constraints  enforcing BAC with Algorithm   can be enforced in O n  r  dr     time using O n   er  memory space  Proof  Every variable is pushed at most O d  times in Q  thus the foreach at line     resp  line     loops at most O erd   resp  O er  d   times  The projection on line    takes O dr   time  The operation at line    can be carried out in O dr    time  The overall time spent inside the if of the PruneInf   function is bounded by O ed   Thus the overall time spent in the loop at line     resp  line     is bounded by O er  dr      resp  O er  dr     The flag on line    is true when w increases  and so it cannot be true more than k times  assuming integer costs   If the flag is true  then we spend O n  time to check all the bounds of the variables  Thus  the time complexity under the if is bounded by O min k  nd   n   To sum up  the overall time complexity is O er  dr     min k  nd   n   which is bounded by O n  r  dr      The space complexity is given by the   winf   wsup and wS data structures which sums up to O n   re  for a WCN with an arity bounded by r    The time complexity of the algorithm enforcing BAC is multiplied by d compared to BAC without  IC  This is a usual trade off between the strength of a local property and the time spent to enforce it  However  the space complexity is still independent of d  Moreover  like BAC  BAC is confluent  Proposition      Confluence  Enforcing BAC on a given problem always leads to a unique WCN  Proof  The proof is similar to the proof of Proposition      However  because of the possible cost movements induced by projections  BAC transforms the original problem P in more complex ways  allowing either pruning domains  BAC  or moving costs from cost       Zytnicki  Gaspin  de Givry   Schiex  Algorithm    Algorithm enforcing BAC                                                                                       Procedure BAC X   D  W  k  QX   winf         wsup         inf           sup           while  Q      do xj  pop Q    flag  false   foreach wS  W  xj  S do if Project wS   then flag  true   foreach xi  S do   mintS lS  inf xi  tS wS  tS     winf  xi    winf  xi    inf  xi   wS       inf  xi   wS       if PruneInf xi   then Q  Q   xi       mintS lS  sup xi  tS wS  tS     wsup  xi    wsup  xi    sup  xi   wS       sup  xi   wS       if PruneSup xi   then Q  Q   xi     if  flag  then foreach xi  X do if PruneInf xi   then Q  Q   xi     if PruneSup xi   then Q  Q   xi     Function Project wS     boolean   mintS lS wS  tS     if        then w  w     wS     wS        return true  else return false         Bounds Arc Consistency for Weighted CSPs  functions to w  The set of problems that will be considered needs therefore to take this into account  Instead of being just defined by its domains  a WCN reached by BAC is also characterized by the amount of cost that has been moved from each cost function wS to w  This quantity is already denoted by wS in Section    on page      We therefore consider the set    P  defined by     hX   D   W  ki   w   w  W     i      n   D  xi    D xi    w  W  w      k  We can now define the relation  on    P    w P   P     w  W  w          xi  X   D   xi    D   xi      This relation is reflexive  transitive and antisymmetric  The first two properties can be easily verified  Suppose now that  P    P         P  and that  P   P      P   P     We have thus  w  W  w   w   xi  X   D xi     D  xi     This ensures that the domains  as well as the amounts of cost projected by each cost function  are the same  Thus  the problems are the same and  is antisymmetric  Besides  h   P   i is a complete lattice  since   P    P         P   w glb P    P       hX    D   xi    D   xi     i      n    W  ki   max w           w  W   w lub P    P       hX    D   xi    D   xi     i      n    W  ki   min w           w  W    and both of them are in    P   Every enforcement of BAC follows from the application of functions from a set of functions FBAC  which may remove the maximum or minimum domain bound  same definition as for BAC  or may project cost from cost functions to w  For a given cost function w  W  such a function keeps the instance unchanged if the minimum  of w is   over possible tuples  Otherwise  if       the problem returned is derived from P by projecting an amount of cost  from w to w  These functions are easily shown to be order preserving for   As in the proof of Proposition      we can define the lattice    P   which is the intersection of the sets of fixed points of the functions f  FBAC       P  is not empty  since  hX                 W  ki   k          k   is in it  As in the proof of proposition      and since Algorithm   terminates  we can conclude that this algorithm is confluent  and that it results in lub    P          Exploiting Cost Function Semantics in BAC In crisp AC  several classes of binary constraints make it possible to enforce AC significantly faster  in O ed  instead of O ed     as shown by Van Hentenryck et al          Similarly  it is possible to exploit the semantics of the cost functions to improve the time complexity of BAC enforcement  As the proof of Proposition     shows  the dominating factors in this complexity comes from the complexity of computing the minimum of cost functions during projection at lines    and    of Algorithm    Therefore  any cost function property       Zytnicki  Gaspin  de Givry   Schiex  that makes these computations less costly may lead to an improvement of the overall time complexity  Proposition     In a binary WCN  if for any cost function wij  W and for any subintervals Ei  D xi    Ej  D xj    the minimum of wij over Ei  Ej can be found in time O d   then the time complexity of enforcing BAC is O n  d     Proof  This follows directly from the proof of Proposition      In this case  the complexity of projection at line    is only in O d  instead of O d     Thus the overall time spent in the loop at line    is bounded by O ed    and the overall complexity is O ed    n  d   O n  d       Proposition     In a binary WCN  if for any cost function wij  W and for any subintervals Ei  D xi    Ej  D xj    the minimum of wij over Ei  Ej can be found in constant time  then the time complexity of enforcing BAC is O n  d   Proof  This follows again from the proof of Proposition      In this case  the complexity of projection at line    is only in O    instead of O d     Moreover  the operation at line    can be carried out in time O    instead of O d   Thus  the overall time spent in the loop at line    is bounded by O ed  and the overall complexity is O ed   n  d    O n  d     These two properties are quite straightforward and one may wonder if they have non trivial usage  They can actually be directly exploited to generalize the results presented by Van Hentenryck et al         for functional  anti functional and monotonic constraints  In the following sections  we show that functional  anti functional and semi convex cost functions  which include monotonic cost functions  can indeed benefit from an O d  speedup factor by application of Proposition      For monotonic cost functions and more generally any convex cost function  a stronger speedup factor of O d    can be obtained by Proposition          Functional Cost Functions The notion of functional constraint can be extended to cost functions as follows  Definition     A cost function wij is functional w r t  xi iff    vi   vj    D xi    D xj    wij  vi   vj          with       k   vi  D xi    there is at most one value vj  D xj   such that wij  vi   vj        When it exists  this value is called the functional support of vi   We assume in the rest of the paper that the functional support can be computed in constant     if xi   xj     time  For example  the cost function wij is functional  In this case  the   otherwise functional support of vi is itself  Note that for k      functional cost functions represent functional constraints  Proposition     The minimum of a functional cost function wij w r t  xi can always be found in O d         Bounds Arc Consistency for Weighted CSPs  Proof  For every value vi of xi   one can just check if the functional support of vi belongs to the domain of xj   This requires O d  checks  If this is never the case  then the minimum of the cost function is known to be   Otherwise  it is    The result follows        Anti Functional and Semi Convex Cost Functions Definition     A cost function wij is anti functional w r t  the variable xi iff    vi   vj    D xi    D xj    wij  vi   vj          with       k   vi  D xi    there is at most one value vj  D xj   such that wij  vi   vj       When it exists  this value is called the anti support of vi       if xi    xj    The cost function wij   is an example of an anti functional cost function    otherwise In this case  the anti support of vi is itself  Note that for k      anti functional cost functions represent anti functional constraints  Anti functional cost functions are actually a specific case of semi convex cost functions  a class of cost functions that appear for example in temporal constraint networks with preferences  Khatib  Morris  Morris    Rossi         Definition     Assume that the domain D xj   is contained in a set Dj totally ordered by the order  j   A function wij is semi convex w r t  xi iff       k   vi  Di   the set  vj  Dj   wij  vi   vj       called the  support of vi   defines an interval over Dj according to  j   Semi convexity relies on the definition of intervals defined in a totally ordered discrete set denoted Dj   and ordered by  j   Even if they may be identical  it is important to avoid confusion between the order j over D xj    used to define interval domains for bounds arc consistency  and the order  j over Dj used to define intervals for semi convexity  In order to guarantee constant time access to the minimum and maximum elements of D xj   according to  j  called the  j  bounds of the domain   we assume that  j  j or  j  j     In this case  the  j  bounds and the domain bounds are identical  One can simply check that anti functional cost functions are indeed semi convex  in this case  the  support of any value is either the whole domain         reduced to one point          or to the empty set  otherwise   Another example is the cost function wij   x i  x j which is semi convex w r t  xi   Proposition     The minimum of a cost function wij which is semi convex w r t  one of its variables can always be found in O d   Proof  We will first show that  if wij is semi convex w r t  to one of its variables  let say xi    then for any value vi of xi   the cost function wij must be minimum at one of the  j  bounds of Dj      This restriction could be removed using for example a doubly linked list data structure over the values in D xj    keeping the domain sorted according to  j and allowing constant time access and deletion but this would be at the cost of linear space which we cannot afford in the context of BAC         Zytnicki  Gaspin  de Givry   Schiex  Assume xi is set to vi   Let b be the lowest cost reached on either of the two  j  bounds of the domain  Since wij is semi convex  then  vj  Dj   wij  vi   vj    b   is an interval  and thus every cost wij  vi   vj   is not less than b for every value of Dj   Therefore  at least one of the two  j  bounds has a minimum cost  In order to find the global minimum of wij   we can restrict ourselves to the  j  bounds of the domain of xj for every value of xi   Therefore  only  d costs need to be checked    From Proposition      we can conclude Corollary     In a binary WCN  if all cost functions are functional  anti functional or semi convex  the time complexity of enforcing BAC is O n  d    only      Monotonic and Convex Cost Functions Definition     Assume that the domain D xi    resp  D xj    is contained in a set Di  resp  Dj   totally ordered by the order  i  resp   j    A cost function wij is monotonic iff   vi   vi   vj   vj    Di   Dj    vi i vi  vj j vj  wij  vi   vj    wij  vi   vj        if xi  xj is an example of a monotonic cost function    otherwise Monotonic cost functions are actually instances of a larger class of functions called convex functions  The cost function   wij     Definition      A function wij is convex iff it is semi convex w r t  each of its variables  For example  wij   xi   xj is convex  Proposition      The minimum of a convex cost function can always be found in constant time  Proof  Since the cost function is semi convex w r t  each of its variable  we know from the proof of Proposition     that it must reach a minimum cost on one of the  j  bounds of the domain of xj and similarly for xi   There are therefore only four costs to check in order to compute the minimum cost    From Proposition      we conclude that Corollary      In a binary WCN  if all cost functions are convex  then the time complexity of enforcing BAC is O n  d  only  One interesting example for a convex cost function is wij   max xi  xj   cst      This type of cost function  which can be efficiently filtered by BAC  may occur in temporal reasoning problems and is also used in our RNA gene localization problem for specifying preferred distances between elements of a gene        Bounds Arc Consistency for Weighted CSPs     Comparison with Crisp Bounds Consistency Petit et al         have proposed to transform WCNs into crisp constraint networks with extra cost variables  In this transformation  every cost function is reified into a constraint  which applies on the original cost function scope augmented by one extra variable representing the assignment cost  This reification of costs into domain variables transforms a WCN in a crisp CN with more variables and augmented arities  As proposed by Petit et al   it can be achieved using meta constraints  i e  logical operators applied to constraints  Given this relation between WCNs and crisp CNs and the relation between BAC and bounds consistency  it is natural to wonder how BAC enforcing relates to just enforcing bounds consistency on the reified version of a WCN  In this section we show that BAC is in some precise sense stronger than enforcing bounds consistency on the reified form  This is a natural consequence of the fact that the domain filtering in BAC is based on the combined cost of several cost functions instead of taking each constraint separately in bounds consistency  We first define the reification process precisely  We then show that BAC can be stronger than the reified bounds consistency on one example and conclude by proving that it can never be weaker  The following example introduces the cost reification process  Example     Consider the WCN in Figure   a   It contains two variables x  and x    one binary cost function w     and two unary cost functions w  and w    For the sake of clarity  every variable or constraint in the reified hard model  described on Figure   b   will be indexed by the letter R  First of all  we model every cost function by a hard constraint  and express that assigning b to x  yields a cost of    We create a new variable x  C R   the cost variable of w    that stores the cost of any assignment of x    Then  we replace the unary cost function w  by a binary constraint c R that involves x  and x  C R   such that if a value v  is assigned to x    then x  C should take the value w  v    We do the same for the unary cost function w    The     R idea is the same for the binary cost function w     we create a new variable x   C R   and we replace w   by a ternary constraint c  R   that makes sure that for any assignment of x  and x  to v  and v  respectively  x   C R takes the value w    v    v     Finally  a global cost C constraint cR that states that the sum of the cost variables should be less than k is added  C C x  C R   x  R   x   R   k  This completes the description of the reified cost hard constraint network  We can now define more formally the reification process of a WCN  Definition     Consider the WCN P   hX   D  W  ki  Let reify P    hXR   DR   WR i be the crisp CN such that   the set XR contains one variable xi R for every variable xi  X   augmented with an extra cost variable xS C R per cost function wS  W   w    the domains DR are   DR  xiR     D xi   for the xiR variables  with domain bounds lbiR and ubiR   C C   lbS C R   ubS R         k     for the xS R variables         Zytnicki  Gaspin  de Givry   Schiex  x  C R  x  C R x R  x R       a  a  b  b              k   x   x   a        a  b        b  x   C R          cC R   a  a small cost function network   b  the reified constraint network  Figure    A small cost function network and its reified counterpart   the set WR of constraints contains   cS R     t  wS  t     t  lS   w  wS  t    k   with scope S   xS C R    for every cost function wS  W  P C  cC wS W xS R   k   an extra constraint that makes sure R is defined as  w  that the sum of the cost variables is strictly less than k  It is simple to check that the problem reify P  has a solution iff P has a solution and the sum of the cost variables in a solution is the cost of the corresponding solution  defined by the values of the xiR variables  in the original WCN  Definition     Let P be a problem  l and l two local consistency properties  Let l P  be the problem obtained after filtering P by l  l is said to be not weaker than l iff l  P  emptiness implies l P  emptiness  l is said to be stronger than l iff it is not weaker than l   and if there exists a problem P such that l  P  is not empty but l P  is empty  This definition is practically very significant since the emptiness of a filtered problem is the event that generates backtracking in tree search algorithms used for solving CSP and WCSP  Example     Consider the WCN defined by three variables  x    x  and x    and two binary cost functions  w   and w      D x       a  b  c  d   D x      D x       a  b  c   we assume that a  b  c  d   The costs of the binary cost functions are described in Figure    Assume that k     and w      One can check that the associated reified problem is already bounds consistent and obviously not empty  For example  a support of the minimum bound of the domain of x  R w r t  c   R is  a  a      a support of its maximum bound is  d  a      Supports of the maximum and minimum bounds of the domain of x   C R w r t  c  R are  b  a     and  a  a     respectively  Similarly  one can check that all other variable bounds are also supported on all the constraints that involve them        Bounds Arc Consistency for Weighted CSPs  a a    x    b   c     x    b c              a a    x    b   c    d         x    b c              d        Figure    Two cost matrices  However  the original problem is not BAC since for example  the value a  the minimum bound of the domain of x    does not satisfy the BAC property  w   X  wS W x  S     min  tS lS  atS    wS  tS     k  This means that the value a can be deleted by BAC filtering  By symmetry  the same applies to the maximum bound of x  and ultimately  the problem inconsistency will be proved by BAC  This shows that bounds consistency on the reified problem cannot be stronger than BAC on the original problem  We will now show that BAC is actually stronger than bounds consistency applied on the reified WCN  Because BAC consistency implies non emptiness  since it requires the existence of assignments of cost   in every cost function  we will start from any BAC consistent WCN P  therefore not empty  and prove that filtering the reified problem reify P  by bounds consistency does not lead to an empty problem  Lemma     Let P be a BAC consistent binary WCN  Then filtering reify P  by bounds consistency does not produce an empty problem  Proof  We will prove here that bounds consistency will just reduce the maximum bounds of the domains of the cost variables xS C R to a non empty set and leave all other domains unchanged  More precisely  the final domain of xS C R will become     max wS  t    t  lS   w wS  t    k    Note that this interval is not empty because the network is BAC consistent which means that every cost function has an assignment of cost    by  IC  and w   k  or else the bounds of the domains could not have supports and the problem would not be BAC   To prove that bounds consistency will not reduce the problem by more than this  we simply prove that the problem defined by these domain reductions only is actually bounds consistent  All the bounds consistency required properties apply to the bounds of the domains of the variables of reify P   Let us consider every type of variable in this reified reduced problem   reified variables xiR   Without loss of generality  assume that the minimum bound lbiR of xiR is not bounds consistent  the symmetrical reasoning applies to the maximum bound   This means it would have no support with respect to a given reified constraint       Zytnicki  Gaspin  de Givry   Schiex  cS R   xi  S  However  by BAC  we have w   min  tlS  lbiR t  wS  t    k  and so t  lS   lbiR  t   wS  t   max wS  t    t  lS   w  wS  t    k   which means that lbi R is supported w r t  cS R    cost variables  The minimum bound of all cost variables are always bounds consistent w r t  the global constraint cC R because this constraint is a less than inequality  Moreover  since the minimum bounds of the cost variables are set to    they are also consistent w r t  the reified constraints  by the definition of  inverse consistency  Consider the maximum bound ubS C R of a cost variable in the reduced reified problem  Remember it is defined as max wS  t    t  lS   w wS  t    k   and so w ubS C R   k  The minimum bounds of all other cost variables in the reified problem  which are    C C form a support of ubS C R w r t  the global constraint cR   So ubS R cannot be removed by bounds consistency    We will now prove the final assertion  Proposition     BAC is stronger than bounds consistency  Proof  Lemma     shows that BAC is not weaker than bounds consistency  Then  example     is an instance where BAC  and therefore BAC is actually stronger than bounds consistency after reification    A filtering related to BAC could be achieved in the reified approach by an extra shaving process where each variable is assigned to one of its domain bounds and this bound is deleted if an inconsistency is found after enforcing bounds consistency  Lhomme             Other Related Works The Definition     of BAC is closely related to the notion of arc consistency counts introduced by Freuder and Wallace        for Max CSP processing  The Max CSP can be seen as a very simplified form of WCN where cost functions only generate costs of   or    when the associated constraint is violated   Our definition of BAC can be seen as an extension of AC counts allowing dealing with arbitrary cost functions  including the usage of w and k  and applied only to domain bounds as in bounds consistency  The addition of  IC makes BAC more powerful  Dealing with large domains in Max CSP has also been considered in the Range Based Algorithm  again designed for Max CSP by Petit  Regin  and Bessiere         This algorithm uses reversible directed arc consistency  DAC  counts and exploits the fact that in Max CSP  several successive values in a domain may have the same DAC counts  The algorithm intimately relies on the fact that the problem is a Max CSP problem  defined by a set of constraints and actively uses bounds consistency dedicated propagators for the constraints in the Max CSP  In this case the number of different values reachable by the DAC counters of a variable is bounded by the degree of the variable  which can be much       Bounds Arc Consistency for Weighted CSPs  smaller than the domain size  Handling intervals of values with a same DAC cost as one value allows space and time savings  For arbitrary binary cost functions  the translation into constraints could generate up to d  constraints for a single cost function and makes the scheme totally impractical  Several alternative definition of bounds consistency exist in crisp CSPs  Choi et al          Our extension to WCSP is based on bounds D  or bounds Z  consistencies  which are equivalent on intervals   For numerical domains  another possible weaker definition of bounds consistency is bounds R  consistency  which is obtained by a relaxation to real numbers  It has been shown by Choi et al  that bounds R  consistency can be checked in polynomial time on some constraints whereas bounds D  or bounds Z  is NP hard  eg  for linear equality   The use of this relaxed version in the WCSP context together with intentional description of cost functions would have the side effect of extending the cost domain from integer to real numbers  Because extensional or algorithmical description of integer cost functions is more general and frequent in our problems  this possibility was not considered  Since cost comparison is the fundamental mechanism used for pruning in WCSP  a shift to real numbers for costs would require a safe floating number implementation both in the local consistency enforcing algorithms and in the branch and bound algorithm      Experimental Results We experimented bounds arc consistency on two benchmarks translated into weighted CSPs  The first benchmark is from AI planning and scheduling  It is a mission management benchmark for agile satellites  Verfaillie   Lematre        de Givry   Jeannin         The maximum domain size of the temporal variables is      This reasonable size and the fact that there are only binary cost functions allows us to compare BAC with strong local consistencies such as EDAC   Additionally  this benchmark has also been modeled using the reified version of WCN  thus allowing for an experimental counterpart of the theoretical comparison of Section    The second benchmark comes from bioinformatics and models the problem of the localization of non coding RNA molecules in genomes  Thebault et al         Zytnicki et al          Our aim here is mostly to confirm that bounds arc consistency is useful and practical on a real complex problem with huge domains  which can reach several millions      A Mission Management Benchmark for Agile Satellites We solved a simplified version described by de Givry and Jeannin        of a problem of selecting and scheduling earth observations for agile satellites  A complete description of the problem is given by Verfaillie and Lematre         The satellite has a pool of candidate photographs to take  It must select and schedule a subset of them on each pass above a certain strip of territory  The satellite can only take one photograph at a time  disjunctive scheduling   A photograph can only be taken during a time window that depends on the location photographed  Minimal repositioning times are required between two consecutive photographs  All physical constraints  time windows and repositioning times  must be met  and the sum of the revenues of the selected photographs must be maximized  This is equivalent to minimizing the rejected revenues of the non selected photographs        Zytnicki  Gaspin  de Givry   Schiex  Let N be the number of candidate photographs  We define N decision variables representing the acquisition starting times of the candidate photographs  The domain of each variable is defined by the time window of its corresponding photograph plus an extra domain value which represents the fact that the photograph is not selected  As proposed by de Givry and Jeannin         we create a binary hard constraint for every pair of photographs  resulting in a complete constraint graph  which enforces the minimal repositioning times if both photographs are selected  represented by a disjunctive constraint   For each photograph  a unary cost function associates its rejected revenue to the corresponding extra value  In order to have a better filtering  we moved costs from unary cost functions inside the binary hard constraints in a preprocessing step  This allows bounds arc consistency filtering to exploit the revenue information and the repositioning times jointly  possibly increasing w and the starting times of some photographs  To achieve this  for each variable xi   the unary cost function wi is successively combined  using   with each binary hard  defined constraint wij that involves xi   This yields N    new binary cost functions wij  as wij  t    wij  t   wi  t xi     having both hard     and soft weights  These binary  replace the unary cost function w and the N    original binary hard cost functions wij i constraints wij   Notice that this transformation has the side effect of multiplying all soft weights by N     This does preserve the equivalence with the original problem since all finite weights are just multiplied by the same constant  N      The search procedure is an exact depth first branch and bound dedicated to scheduling problems  using a schedule or postpone strategy as described by de Givry and Jeannin        which avoids the enumeration of all possible starting time values  No initial upper bound was provided  k       We generated     random instances for different numbers of candidate photographs  N varying from    to        We compared BAC  denoted by BAC  in the experimental results  with EDAC   Heras et al          denoted by EDAC    Note that FDAC  and VAC  applied in preprocessing and during search  in addition to EDAC   were also tested on these instances  but did not improve over EDAC   FDAC  was slightly faster than EDAC  but developed more search nodes and VAC was significantly slower than EDAC   without improving w in preprocessing   OSAC is not practical on this benchmark  for N       it has to solve a linear problem with         variables and about   million constraints   All the algorithms are using the same search procedure  They are implemented in the toulbar  C   solver    Finding the minimum cost of the previously described binary cost functions  which are convex if we consider the extra domain values for rejected photographs separately   is done in constant time for BAC  It is done in time O d    for EDAC   d         We also report the results obtained by maintaining bounds consistency on the reified problem using meta constraints as described by de Givry and Jeannin         using the claire Eclair C   constraint programming solver  de Givry  Jeannin  Josset  Mattioli  Museux    Saveant        developed by THALES  denoted by B consistency   The results are presented in Figure    using a log scale  These results were obtained on a   GHz Intel Xeon with   GB of RAM  Figure   shows the mean CPU time in seconds and the mean number of backtracks performed by the search procedure to find the optimum    These instances are available at http   www inra fr mia ftp T bep      See http   carlit toulouse inra fr cgi bin awki cgi ToolBarIntro         Bounds Arc Consistency for Weighted CSPs  Satellite benchmark        EDAC  B consistency BAC   Cpu time in seconds                                e                 Number of candidate photographs      Satellite benchmark  e     B consistency BAC  EDAC   Number of backtracks   e     e                                                 Number of candidate photographs  Figure    Comparing various local consistencies on a satellite benchmark  Cpu time  top  and number of backtracks  bottom  are given         Zytnicki  Gaspin  de Givry   Schiex  and prove optimality as the problem size increases  In the legends  algorithms are sorted by increasing efficiency  The analysis of experimental results shows that BAC was up to    times faster than EDAC  while doing only     more backtracks than EDAC   for N       no backtrack results are reported as EDAC  does not solve any instance within the time limit of   hours   It shows that bounds arc consistency can prune almost as many search nodes as a stronger local consistency does in much less time for temporal reasoning problems where the semantic of the cost functions can be exploited  as explained in Section    The second fastest approach was bounds consistency on the reified representation which was at least     worse than BAC in terms of speed and number of backtracks when N      This is a practical confirmation of the comparison of Section    The reified approach used with bounds consistency introduces Boolean decision variables for representing photograph selection and uses a criteria defined as a linear function of these variables  Contrarily to BAC  bounds consistency is by definition unable to reason simultaneously on the combination of several constraints to prune the starting times      Non coding RNA Gene Localization A non coding RNA  ncRNA  gene is a functional molecule composed of smaller molecules  called nucleotides  linked together by covalent bonds  There are four types of these nucleotides  commonly identified by a single letter  A  U  G and C  Thus  an RNA can be represented as a word built from the four letters  This sequence defines what is called the primary structure of the RNA molecule  RNA molecules have the ability to fold back on themselves by developing interactions between nucleotides  forming pairs  The most frequently interacting pairs are  a G interacts with a C  or a U interacts with an A  A sequence of such interactions forms a structure called a helix  Helices are a fundamental structural element in ncRNA genes and are the basis for more complex structures  The set of interactions is often displayed by a graph where vertices represent nucleotides and edges represent either covalent bonds linking successive nucleotides  represented as plain lines in Figure    or interacting nucleotide pairs  represented as dotted lines   This representation is usually called the molecules secondary structure  See the graph of a helix in Figure   a   The set of ncRNAs that have a common biological function is called a family  The signature of a gene family is the set of conserved elements either in the sequence or the secondary structure  It can be expressed as a collection of properties that must be satisfied by a set of regions occurring on a sequence  Given the signature of a family  the problem we are interested in involves searching for new members of a gene family in existing genomes  where these members are in fact the set of regions appearing in the genome which satisfy the signature properties  Genomic sequences are themselves long texts composed of nucleotides  They can be thousand of nucleotides long for the simplest organisms up to several hundred million nucleotides for the more complex ones  The problem of searching for an occurrence of a gene signature in a genomic sequence is NP complete for complex combinations of helix structures  Vialette         In order to find ncRNAs  we can build a weighted constraint network that scans a genome  and detects the regions of the genome where the signature elements are present       Bounds Arc Consistency for Weighted CSPs  A U A C  A G C G U C  G  U C G C A G  helix  C G A U  A G A C UA U U xi xj  loop   cost      b  pattern xi   xj   ACGUA  cost function    a  An helix with its loop   G  xi  U C  U A G  A G  xl  cost  xj  A G C  k  xk  cost       c  The helix xi   xj   xk   xl      cost function   The     xj  xi d  d   d   d    d  The cost profile spacer xi   xj   d    d    d    d    cost tion   of func   Figure    Examples of signature elements with their cost functions  and correctly positioned  The variables are the positions of the signature elements in the sequence  The size of the domains is the size of the genomic sequence  Cost functions enforce the presence of the signature elements between the positions taken by the variables involved  Examples of cost functions are given in Figure     The pattern xi   xj   p  function states that a fixed word p  given as parameter  should be found between the positions indicated by the variables xi and xj   The cost given by the function is the edit distance between the word found at xi  xj and the word p  see the cost function pattern with the word ACGUA in Figure   b     The helix xi   xj   xk   xl   m  function states that the nucleotides between positions xi and xj should be able to bind with the nucleotides between xk and xl   Parameter m specifies a minimum helix length  The cost given is the number of mismatches or nucleotides left unmatched  see the helix function with   interacting nucleotide pairs in Figure   c     Finally  the function  spacer xi   xj   d    d    d    d    specifies a favorite range of distances between positions xi and xj using a trapezoidal cost function as shown in Figure   d   See the work of Zytnicki et al         for a complete description of the cost functions  Because of the sheer domain size  and given that the complex pattern matching oriented cost functions do not have any specific property that could speedup filtering  BAC alone has been used for filtering these cost functions  Zytnicki et al          The exception is the       Zytnicki  Gaspin  de Givry   Schiex  piecewise linear spacer cost function  its minimum can be computed in constant time for BAC enforcement  The resulting C   solver is called DARN     Size   of solutions AC  Time   of backtracks BAC Time  sec     of backtracks    k       k        k        k      M        M       hour   min         hours                                                                                 Table    Searching all the solutions of a tRNA motif in Escherichia coli genome  A typical benchmark for the ncRNA localization problem is the transfer RNA  tRNA  localization  The tRNA signature  Gautheret  Major    Cedergren        can be modelled by    variables    nucleotide words    helices  and   spacers  DARN  searched for all the solutions with a cost strictly lower than the maximum cost k      Just to illustrate the absolute necessity of using bounds arc consistency in this problem  we compared bounds arc consistency enforcement with AC   Larrosa        on sub sequences of the genome of Escherichia coli  which is     million nucleotides long  Because of their identical space complexity and because they have not been defined nor implemented on non binary cost functions  helix is a quaternarycost function   DAC  FDAC or EDAC have not been tested  see the work of Sanchez et al         however for an extension of FDAC to ternary cost functions   The results are displayed in Table    For different beginning sub sequences of the complete sequence  we report the size of the sub sequence in which the signature is searched for    k is a sequence of        nucleotides   as well as the number of solutions found  We also show the number of backtracks and the time spent on a   GHz Intel Xeon with   GB  A   means the instance could not be solved due to memory reasons  and despite memory optimizations  BAC solved the complete sequence in less than   seconds  BAC is approximately           resp               times faster than AC  for the   k  resp    k  sub sequence  More results on other genomes and ncRNA signatures can be found in the work of Zytnicki et al          The reason of the superiority of BAC over AC  is twofold  First  AC  needs to store all the unary costs for every variable and projects costs from binary cost functions to unary cost functions  Thus  the space complexity of AC  is at least O nd   For very large domains  in our experiments  greater than         values   the computer cannot allocate a sufficient memory and the program is aborted  For the same kind of projection  BAC only needs to store the costs of the bounds of the domains  leading to a space complexity of O n   Second  BAC does not care about the interior values and focuses on the bounds of the domains only  On the other hand  AC  projects all the binary costs to all the interior values     DARN   and several genomic http   carlit toulouse inra fr Darn    sequences       and  family  signatures  are  available  at   Bounds Arc Consistency for Weighted CSPs  which takes a lot of time  but should remove more values and detect inconsistencies earlier  However  Table   shows that the number of backtracks performed by AC  and BAC are the same  This can be explained as follows  Due to the nature of the cost functions used in these problems  the supports of the bounds of the domains of the variables usually are the bounds of the other variables  Thus  removing the values which are inside the domains  as AC  does  do not help removing the bounds of the variables  As a consequence  the bounds founds by BAC are the same as those found by AC   This explains why enforcing of AC  generally does not lead to new domain wipe out compared to BAC  and finding the support inside the bounds of the domains is useless  Notice that the spacer cost functions dramatically reduce the size of the domains  When a single variable is assigned  all the other domain sizes are dramatically reduced  and the instance becomes quickly tractable  Moreover  the helix constraint has the extra knowledge of a maximum distance djk between its variables xj and xk  see Fig    c   which bounds the time complexity of finding the minimum cost w r t  djk and not the length of the sequence      Conclusions and Future Work We have presented here new local consistencies for weighted CSPs dedicated to large domains as well as algorithms to enforce these properties  The first local consistency  BAC  has a time complexity which can be easily reduced if the semantics of the cost function is appropriate  A possible enhancement of this property   IC  has also been presented  Our experiments showed that maintaining bounds arc consistency is much better than AC  for problems with large domains  such as ncRNA localization and scheduling for Earth observation satellites  This is due to the fact that AC  cannot handle problems with large domains  especially because of its high memory complexity  but also because BAC behaves particularly well with specific classes of cost functions  Similarly to bounds consistency  which is implemented on almost all state of the art CSP solvers  this new local property has been implemented in the open source toulbar  WCSP solver   BAC  BAC and  inverse consistency allowed us to transfer bounds consistency CSP to weighted CSP  including improved propagation for specific classes of binary cost functions  Our implementation for RNA gene finding is also able to filter non binary constraints  It would therefore be quite natural to try to define efficient algorithms for enforcing BAC  BAC or  inverse consistency on specific cost functions of arbitrary arity such as the soft global constraints derived from All Diff  GCC or regular  Regin        Van Hoeve  Pesant    Rousseau         This line of research has been recently explored by Lee and Leung         Finally  another interesting extension of this work would be to better exploit the connection between BAC and bounds consistency by exploiting the idea of Virtual Arc Consistency introduced by Cooper et al          The connection established by Virtual AC between crisp CNs and WCNs is much finer grained than in the reification approach considered by Petit et al         and could provide strong practical and theoretical results     Available at http   carlit toulouse inra fr cgi bin awki cgi ToolBarIntro         Zytnicki  Gaspin  de Givry   Schiex  
 There exist several architectures to solve influence diagrams using local computations  such as the Shenoy Shafer  the HUGIN  or the Lazy Propagation architectures  They all extend usual variable elimination algorithms thanks to the use of so called potentials  In this paper  we introduce a new architecture  called the Multi operator Cluster DAG architecture  which can produce decompositions with an improved constrained inducedwidth  and therefore induce potentially exponential gains  Its principle is to benefit from the composite nature of influence diagrams  instead of using uniform potentials  in order to better analyze the problem structure      INTRODUCTION  Since the first algorithms based on decision trees or arc reversal operations  Shachter         several exact methods have been proposed to solve influence diagrams using local computations  such as the ones based on the Shenoy Shafer  the HUGIN  or the Lazy Propagation architectures  Shenoy        Jensen et al         Madsen and Jensen         These methods have successfully adapted classical Variable Elimination  VE  techniques  which are basically designed to compute one type of marginalization on a combination of local functions with only one type of combination operator   in order to handle the multiple types of information  probabilities and utilities   the multiple types of marginalizations  sum and max   and the multiple types of combination   for probabilities    for utilities  involved in an influence diagram  The key mechanism used for such an extension consists in using elements known as potentials  Ndilikilikesha         In this paper  we define a new architecture  called the Multi operator Cluster DAG  MCDAG  architecture   Gerard Verfaillie ONERA Toulouse  France  which does not use potentials  but still relies on VE  Compared to existing schemes  MCDAGs actively exploit the composite nature of influence diagrams  We first present the potential based approach and motivate the need for a new architecture  Section     Then  MCDAGs are introduced  Section    and a VE algorithm is defined  Section     Finally  this work is compared with existing approaches  Section    and extended to other frameworks  Section     All proofs are available in  Pralet et al       b       MOTIVATIONS  Notations and definitions An influence diagram  Howard and Matheson        is a composite graphical model defined on three sets of variables organized in a Directed Acyclic Graph  DAG  G      a set C of chance variables x  C  for each of which a conditional probability distribution Px   pa x  on x given its parents in G is specified      a set D    D            Dq    indices represent the order in which decisions are made  of decision variables x  D  for each of which pa x  is the set of variables observed before decision x is made      a set  of utility variables u    each of which is associated with a utility function Upa u  on pa u   and utility variables are leaves in the DAG   We consider influence diagrams where the parents of a decision variable are parents of all subsequent decision variables  no forgetting   The set of conditional probability distributions  one for each x  C  is denoted P and the set of utility functions  one for each u    is denoted U   Each function   P  U holds on a set of variables sc   called its scope  and is consequently called a scoped function  sc Px   pa x       x   pa x  and sc Upa u      pa u    The set of chance variables observed before the first decision is denoted I    the set of chance variables observed between decisions Dk and Dk   is denoted Ik   and the set of chance variables unobserved before the last decision is denoted Iq   We use dom x  to denote the domain of a vari    able x  C Q  D  and by extension  for W  C  D  dom W     xW dom x    The usual problem associated with an influence diagram is to find decision rules maximizing the expected utility  a decision rule for a decision Dk is a function associating a value in dom Dk   with any assignment of the variables observed before making decision Dk   As shown in  Jensen et al          this is equivalent to computing optimal decision rules for the quantity      Y X X X X     Pi  Ui max max       I        D   Iq   Dq  Iq  Pi P  Ui U  THE POTENTIAL APPROACH  With this approach  Equation   is reformulated using so called potentials in order to use only one combination and one marginalization operator  A potential on a set of variables W is a pair W    pW   uW    where pW and uW are respectively a nonnegative real function and a real function  whose scopes are included in W   The initial conditional probability distributions Pi  P are transformed into potentials  Pi       whereas the initial utility functions Ui  U are transformed into potentials     Ui    On these potentials  a combination operation  and a marginalization  or elimination  operation  are defined   the combination of W     pW    uW    and W     pW    uW    is the potential on W   W  given by W   W     pW   pW    uW    uW      the marginalization  of W    pP W   uW   over   W   P W  W   pW u W P  with C equals W   W  p W   pW W   the convention           whereas the marginalization of W    pW   uW   over W   D is given W  by W    pW   maxW  uW     Solving the problem associated with an influence diagram is then equivalent to computing           CD Iq  Dq  Iq      D   I    where CD    Pi P  Pi         Ui U     Ui    is the combination of the initial potentials  As  and  satisfy the ShenoyShafer axioms defined in  Shenoy          can be computed using usual VE algorithms  Jensen et al          This explains why existing architectures like ShenoyShafer  HUGIN  or Lazy Propagation  LP    use potentials to solve influence diagrams       QUANTIFYING THE COMPLEXITY  In the case of influence diagrams  the alternation of sum and max marginalizations  which do not gener   The LP architecture actually uses potentials defined as pairs of set of functions  instead of pairs of functions    ally commute  prevents from eliminating variables in any order  The complexity of VE can then be quantified using constrained induced width  Jensen et al         Park and Darwiche         instead of inducedwidth  Dechter and Fattah          Definition    Let G    VG   HG   be a hypergraph  and let   be a partial order on VG   The constrained induced width of G with constraints on the elimination order given by    x  y stands for y must be eliminated before x  is a parameter denoted wG      It is defined as wG       minolin    wG  o   lin    being the set of linearizations of   to a total order on VG and wG  o  being the induced width of G for the elimination order o  i e  the size of the largest hyperedge created when eliminating variables in the order given by o   The constrained induced width can be used to give an upper bound on the complexity of existing potentialbased VE algorithms  Let Gp    C  D   sc     P  U    be the hypergraph corresponding to the untyped influence diagram  Let  p be the partial order defined by I  p D     Ik        Dk p Ik p Dk      and Dq p Iq   Finally  let d be the maximum size of the variables domains  Then  with classical approaches based on potentials and strong junction trees  Jensen et al          which are junction trees with constraints on the marginalization order  the theoretical complexity is O  P  U    d  wGp   p      the number of elements of a finite set E is denoted  E         DECREASING THE CONSTRAINED INDUCED WIDTH  The constrained induced width is a guideline to show how the complexity can be decreased  In this direction  one can work on the two parameters on which it depends  the partial order    and the hypergraph G  Weakening the partial order   Proposition    Let G    VG   HG   be a hypergraph and let         be two partial orders on VG such that  x  y   VG  VG    x    y    x    y      is weaker than       Then  wG        wG        Proposition   means that if one weakens    i e  if one reveals some extra freedoms in the elimination order  e g  by proving that some marginalizations with sum and max can commute   then the theoretical complexity may decrease  Though such a technique is known to be useless in contexts like Maximum A Posteriori hypothesis  Park and Darwiche         where there is only one alternation of max and sum marginalizations     This means that VG is the set of variables  or vertices   and HG is a set of hyperedges on VG   i e  a subset of  VG     it can lead to an exponential gain as soon as there are more than two levels of alternation  Indeed  assume that one wants P to compute P maxx       xn y maxxn   Py  Ux   y    in Uxi  xn      On one hand  using    defined by  x            xn     y   xn   provides us with the constrained inducedwidth wG         n  since xn   is then necessarily eliminated first  On the other hand  the scopes of the functions involved enable us to infer that with    defined by x    y  one is guaranteed to compute the same value  since y is linked only with x    The constrained induced width is then wG            e g  with the elimination order x   y  xn    xn         x    Therefore  the theoretical complexity decreases from O  n       dn     to O  n       d     thanks to the weakening of the partial order  the  n      factor corresponds to the number of scoped functions   Working on the hypergraph The second possible mechanism is to work on the hypergraph G  either by eliminating so called barren P variables  computing x Px   pa x  is useless because of normalization   or by better decomposing the problem  To illustrate the latter  P assume that one wants to compute maxx       xn y Py   Uy x         Uy xn    The basic hypergraph G      x            xn   y     y  x             y  xn      together with    defined by  x            xn     y  gives a theoretical complexity O  n       dwG              O  n       dn      However  one can write  P maxx       xn y Py   Uy x         Uy xn   P P    maxx  y Py  Uy x            maxxn y Py  Uy xn   Thus  an implicit duplication of y makes the complexity decrease to O  n     d      O  n     d  wG           where G  is the hypergraph defined by the variables  x            xn   y               y  n    and by the hyperedges   x    y                 xn   y  n      and where    is given by x    y          P     xn   y  n    This P method Pwhich uses the property S  U    U        S U P      S U     duplicates variables quantified with   so that computations become more local  Proposition   shows the possible exponential gain obtained by duplication  Proposition    Let x Si be a scoped function of scope  x   Si P for any i      m   The direct computation of x  x S         x Sm   always requires more sums than P the direct computation of P          x x S      x x Sm    Moreover  the comP putation of x  x S         x Sm   results in a complexity O m  d   S     Sm      whereas P the computation of the m quantities in the set   x x Si      i  m  results in a complexity O m  d  maxi   m   Si      Why not use potentials  Though weakening the constraints on the elimination order could be done  with potentials  the duplication mechanism cannot be used if potentials are  Indeed  one cannot write W  W   W   W   W     W     W   even if W   C      The duplication mechanism has actually already been proposed in the influence diagram litterature  Dechter        where it was applied on the fly during elimination  In this paper  the duplication is exploited in a global preliminary analysis which may reveal new degrees of freedom in the elimination order  in synergism with the application of other mechanisms  The new architecture we introduce  which does not use potentials to solve influence diagrams  is called the Multi operator Cluster DAG  MCDAG  architecture          THE MCDAG ARCHITECTURE MACROSTRUCTURING AN INFLUENCE DIAGRAM  The first step to build the MCDAG architecture is to analyze the macrostructure of the influence diagram  by detecting the possible reordering freedoms in the elimination order  while using the duplication technique and the normalization conditions on conditional probability distributions  This macrostructure is represented with a DAG of computation nodes  Definition    An atomic computation node n is a scoped function  in P  U   In this case  the value of n is val n      and its scope is sc n    sc    A computation node is either an atomic computation node or a triple n    Sov     N    where Sov is a sequence of operator variables pairs    is an associative and commutative operator with an identity  and where N is a set of computation nodes  In the latter  the value of n is given by val n    Sov   n  N val n      and its scope is given by sc n     n  N sc n       x   opx  Sov   Informally  a computation node  Sov     N   defines a sequence of marginalizations on a combination of computation nodes with a specific operator  It can be represented as in Figure    Given a set of computation nodes N   we define N  x  resp  N x   as the set of nodes of N whose scope contains x  resp  does not contain x   N  x    n  N   x  sc n    resp  N x    n  N   x    sc n     Sov   n   n        k nl  Figure    A computation node  Sov     N    where              k    resp   n            nl    is the set of atomic  resp  non atomic  computation nodes in N            From influence diagrams to computation nodes  Without loss of generality  we assume that U      if this is not the case  one can add U      to U     maxd P   Prr  r Ud r              P Rewriting rules for x When a sum marginalization must be performed  a decomposition rule D   a recomposition rule R   and two simplification rules     are used  These are illustrated in Figure    and S S which corresponds to the influence diagram example introduced in        P D  Sov  x            N     N  N           P  x  N  N Sov         N x  x     N R   Prec    S     S  sc N           N   N       P P P   SS       N   N      S     N      S       N        S   Prec   x    S  sc N       P P    x S     N  Px   pa x      S     N     P           N  S      N          Example In the example of Figure    the first rule to be applied is the decomposition rule D   which treats  P   Prr  r Ud r       maxd  P  P  P   Prr  r Ud         r     Ud r     P  Prr  r Ud r       r   P     r   Ud  P  Prr  r      D maxd  Macrostructuring the initial node  In order to exhibit the macrostructure of the influence diagram  we analyze the sequence of computations performed by n    To do so  we successively consider the eliminations in Sov  from the right to the left and use three types of rewriting rules  preserving nodes values  to make the macrostructure explicit      decomposition rules  which decompose the structure using namely the duplication technique      recomposition rules  which reveal freedoms in the elimination order      simplification rules  which remove useless computations from the architecture  by using normalization conditions  Rewriting rules are presented first for the case of sum marginalizations  and then for the case of max marginalizations  A rewriting rule may be preceded by preconditions restricting its applicability      r   r   D  Proposition P    Let Sov  P be the initial sequence P I  maxD        Iq  maxDq Iq of operator variables pairs defined by the influence diagram  The value of Equation   is equal to the value of the computation node n     Sov             P   Ui     Ui  U     For the influence P diagram associated with the computation of maxd r   r  Pr  Pr   r   Ud r   Ud r   Ud   n  corresponds to the first computation node in Figure     P     P            P  r   r   P  Prr  r Ud r       P  r    Ud r  P  r   P  r   Ud   P  Prr  r      R maxd   P  r   r            Ud  P r  Pr  Ud r  P Pr  Ud r  P r   r   Pr   r  r   r   P Pr   r  r   r      S  S   maxd      P  r    Pr  Ud r      P  r   r       Ud  Pr  Ud r  Pr   r   Figure    Application of rewriting rules for  P     P the operator variable pair r   Such a rule uses the duplication mechanism and the distributivity property of  over    It provides us with a DAG of computation nodes  It is a DAG since common computation nodes are merged  and it is not hard to detect such nodes when applying P the rules   Then  D can be applied again for r    One can infer from the obtained architecture that there is no reason for r  to be eliminated before r    Using the recomposition rule R makes this clear in the structure  Basically  R uses   the distributivity of  over    Last  applying S and   S   which use the normalization of conditional probability distributions  simplifies some nodes in the architecture  In the end  no computation involves more than P two variables if one eliminates r  first in the node   r   r       Pr    Pr   r    Ud r      whereas with a poten    tial based approach  it would be necessary to process three variables simultaneously  since r  would be involved in the potentials  Pr         Pr   r            Ud r    if eliminated first  and r  would be involved in the potentials  Pr   r            Ud r    if eliminated first    maxd   P  maxd   r   P r    Pr   r  U d   P  r      maxd   P r    Pr   r  Ud   d   P r    Pr   r  Ur   d   d   P r    Pr   r  Ur   d   Dmax Rewriting rules for maxx When a max marginalization must be performed  a decomposition rule Dmax and a recomposition rule Rmax are used  there is no simplification rule since there is no normalization condition to use for decision variables   These rules are a bit more complex than the previous ones and are illustrated in Figure P    which corresponds to the influP ence diagram maxd  r  maxd  r  maxd  Pr   Pr   r    Ud    Ud   d    Ur   d   d    Ur   d      maxd   Rmax   Prec    S    Ssc N   sc N         N   N  N   N        n  N    val n         maxS      N         N     maxS              N    N   N        r   maxd   P     r     Pr  Pr   r   P r    Pr   r  U d   P r    Pr   r  Ur   d   maxd      Ud   d     Ur   d   d   D maxd   Dmax   Prec   N  N x n  N x   val n        Sov maxx            N     N  N      Sov           N     N  N   if N x     Sov           N     N  Nx           N     maxx      N        otherwise   N    N N x N x where N          N  N      N  N x    P  P    U d  P  r   r   maxd         P r   Pr  r      P  maxd          Ud   d   r   Pr Ur  d  Pr   r             Ur   d   d   Dmax maxd   P     r        U d  P  P r  r   Pr  r    maxd          maxd      maxSS        N         N   N      N   N      Ud   d   Example In the example of Figure    one first applies the decomposition rule Dmax   in order to treat the operator variable pair maxd    Such a rule uses first the monotonicity of    max a   b  a   c    a   max b  c    and then both the distributivity of  over   and the monotonicity of   so as to write things like maxd    Pr   Pr   r   Ud   d       Pr   Pr   r   Ur   d   d       maxd   Ud   d   Ur   d   d      Then  D can be Pr  Pr   rP   used for r    and Dmax can be used for maxd    After those steps  the recomposition rule Rmax   which uses the monotonicities of  and    reveals that the elimination order between d  and d  is actually free  This was not obvious from the initial Sov sequence  The approach using potentials is unable to make such freedoms explicit  which may induce exponential increase in complexity as shown in       P  P  r       Pr Ur   d  Pr   r     Ur   d   d   Rmax  Rule application order A chaotic iteration of the rules does not converge  since e g   rules Dmax and Rmax may be infinitely alternately applied  Hence  we specify an order in which we apply rules to converge to a unique final DAG of computation nodes  we have  maxd    U d   r       maxd   d       Ud   d  P  r   P r   Pr  r            Ur   d   d  P  r     Pr Ur   d  Pr   r   Figure    Application of rewriting rules for max  the application of the rules may create nodes looking like       n    which perform no computations  these nodes can be eliminated at a final step    used this order in the previous examples   We successively consider each operator variable pair of the initial sequence Sov P   from the right to the left P  marginalizaP tions like x       xn can be split into x     xn    If the rightmost marginalization in the Sov sequence of P the root node is x   then rule D is applied once  It creates new grandchildren nodes for the root  for each   of which  we try to apply rule R in order to reveal freedoms in the elimination order  If R is applied  this creates new computation nodes  on each of which     simplification rules S and then S are applied  until they cannot be applied anymore   If the rightmost marginalization in the Sov sequence of the root node is maxx   then rule Dmax is applied once  This creates a new child and a new grandchild for the root  For the created grandchild  we try to weaken constraints on the elimination order using Rmax   Therefore  the rewriting rules are applied in a deterministic order  except from the freedom left when choosing the next variable in S to consider for P marginalizations like S or maxS   It can be shown that this freedom does not change the final structure  The soundness of the macrostructure obtained is provided by the soundness of the rewriting rules      Proposition    Rewriting rules D   R   S   S   Dmax and Rmax are sound  i e  for any of these rules n  n    if the preconditions are satisfied  then val n      val n    holds  Moreover  rules Dmax and Rmax leave the set of optimal decision rules unchanged   Complexity issues An architecture is usable only if it is reasonable to build it  Proposition   makes it possible to save some tests during the application of the rewriting rules  and Proposition   gives upper bounds on the complexity      the preconditions of Proposition    Except for S the rewriting rules are always satisfied   Proposition    The time and space complexity of the application of the rewriting rules are O  C  D    U          P       and O  C  D     U      P     respectively       TOWARDS MCDAGS  The rewriting rules enable us to transform the initial multi operator computation node n  into a DAG of mono operator P computation nodes looking like  maxS      N      S     N       P  N    or   P  U   For nodes  maxS      N   or   S     N    it is time to use freedoms in the elimination order  To do so  usual junction tree construction techniques can be used  since on one hand   R  max     and  R       are commutative semirings  and since on the other hand  there are no constraints on the elimination order inside each of these nodes  the only slight difference with usual junction trees is that only a subset of the variables involved in a computation node may have to be eliminated  but it is quite easy to cope with this   To obtain a goodPdecomposition for nodes n like  maxS      N   or   S     N    one can build a junction tree to eliminate S from the hypergraph G     sc N     sc n      n   N     The optimal induced width which can be obtained for n is w n    wG S   the induced width of G for the elimination of the variables in S   The induced width of the MCDAG architecture is then defined by wmcdag   maxnN w n   where P N is the set of nodes looking like  maxS      N   or   S     N    After the decomposition of each mono operator computation node  one obtains a Multi operator Cluster DAG  Definition    A Multi operator Cluster DAG is a DAG where every vertex c  called a cluster  is labeled with four elements  a set of variables V  c   a set of scoped functions  c  taking values in a set E  a set of son clusters Sons c   and a couple  c   c   of operators on E s t   E  c   c   is a commutative semiring  Definition    The value of a cluster c of a MCDAG is  given by val c       c V  c V  pa c   c  c   c c sSons c  val s    The value of a MCDAG is the value of its root node  Thanks to Proposition    working on MCDAGs is sufficient to solve influence diagrams  Proposition    The value of the MCDAG obtained after having decomposed the macrostructure is equal to the maximal expected utility  Moreover  for any decision variable Dk   the set of optimal decision rules for Dk in the influence diagram is equal to the set of optimal decision rules for Dk in the MCDAG       MERGING SOME COMPUTATIONS  There may exist MCDAG clusters performing exactly the same computations  even if the computation nodes they come from are Pdistinct  For instance  a computation node n      x y      Px   Py x   Uy z   may be decomposed into clusters c      x    Px   Py x               and c       y   P  Uy z     c             A computation node n      x y      Px   Py x   Uy t   may be decomposed into clusters c      x    Px   Py x             and c       y    Uy t    c              As c    c    some computations can be saved by merging clusters c  and c  in the MCDAG  Detecting common clusters is not as easy as detecting common computation nodes     For  maxS      N   nodes  which actually always look like  maxS            N      N    N    better decompositions can be obtained by using another hypergraph  In fact  for each N    N  there exists a unique n  N     denoted N    u   s t  n or its children involve a utility function  It is then better to consider the hypergraph  sc N     sc N    u     N    N    This enables to figure out that e g  only two variables  x and y  must be considered if one eliminates x first in a node like  maxxy      N      maxxy            Uy z          nz   Ux y           nz   Ux       since nz is a factor of both Ux y and Ux   We do not further develop this technical point    this notion is not used only for decision rules conciseness reasons  it is also used to reveal reordering freedoms  which can decrease the time complexity  Note that some of the ordering freedom here is obtained by synergism with the duplication   To sum up  there are three steps to build the architecture  First  the initial multi operator computation node is transformed into a DAG of mono operator computation nodes  via sound rewriting rules   Then  each computation node is decomposed with a usual junction tree construction  It provides us with a MCDAG  in which some clusters can finally be merged         Thanks to simplification rule S   the normalization conditions enable us not only to avoid useless computations  but also to improve the ar  may indirectly weaken chitecture structure  S some constraints on the elimination order   This is stronger than Lazy Propagation architectures  Madsen and Jensen         which use the first point only  during the message passing phase  Note that with MCDAGs  once the DAG of computation nodes is built  there are no remaining normalization conditions to be used   VE ALGORITHM ON MCDAGs  Defining a VE algorithm on a MCDAG is simple  The only difference with existing VE algorithms is the multi operator aspect for both the marginalization and the combination operators used  As in usual architectures  nodes send messages to their parents  Whenever a node c has received all messages val s  from its children  c can compute   its own value val c       c V  c V  pa c   c  c   c c sSons c  val s  and send it to its parents  As a result  messages go from leaves to root  and the value computed by the root is the maximal expected utility      COMPARISON WITH EXISTING ARCHITECTURES  Compared to existing architectures on influence diagrams  MCDAGs can be exponentially more efficient by strongly decreasing the constrained induced width  cf Section       thanks to     the duplication technique      the analysis of extra reordering freedoms  and     the use of normalizations conditions  One can compare these three points with existing works   The idea behind duplication is to use all the decompositions  independences  available in influence diagrams  An influence diagram actually expresses independences on one hand on the global probability distribution PC   D   and on the other hand on the global utility function  MCDAGs separately use these two kinds of independences  whereas a potential based approach uses a kind of weaker mixed independence relation  Using the duplication mechanism during the MCDAG building is better  in terms of induced width  than using it on the fly as in  Dechter           Weakening constraints on the elimination order can be linked with the usual notion of relevant information for decision variables  With MCDAGs     E g   for the quite simple influence diagram introduced in Section        the algorithm in  Dechter        gives   as an induced width  whereas MCDAGs give an inducedwidth    The reason is that MCDAGs allow to eliminate both x  before x  in the subproblem corresponding to Ud x  and x  before x  in the subproblem corresponding to Ud x     Compared to existing architectures  MCDAGs actually always produce the best decomposition in terms of constrained induced width  as Theorem   shows  Theorem    Let wGp   p   be the constrained inducedwidth associated with the potential based approach  cf Section       Let wmcdag be the induced width associated with the MCDAG  cf Section       Then  wmcdag  wGp   p    Last  the MCDAG architecture contradicts a common belief that using division operations is necessary to solve influence diagrams with VE algorithms      POSSIBLE EXTENSIONS  The MCDAG architecture has actually been developed in a generic algebraic framework which subsumes influence diagrams  This framework  called the Plausibility Feasibility Utility networks  PFUs  framework  Pralet et al       a   is a generic framework for sequential decision making with possibly uncertainties  plausibility part   asymmetries in the decision process  feasibility part   and utilities  PFUs subsume formalisms from quantified boolean formulas or Bayesian networks to stochastic constraint satisfaction problems  and even define new frameworks like possibilistic influence diagrams  This subsumption is possible because the questions raised in many existing formalisms often reduce to a sequence of marginalizations on a combination of scoped functions  Such sequences  a particular case of which is Equation    can be structured using rewriting rules as the ones previously presented  which actively exploit the algebraic properties of the operators at stake  Thanks to the generic nature of PFUs  extending the previous work to a possibilistic version of influence diagrams is trivial  If one uses the possibilistic   pessimistic expected utility  Dubois and Prade         the optimal utility can be defined by  the probability distributions Pi become possibility distributions  and the utilities Ui become preference degrees in               min max       min max min max max     Pi    min U   I   D   Iq  Dq  Iq  Pi P  Ui U  These eliminations can be structured via a MCDAG  The only difference in the rewriting rules is that  becomes max and   becomes min  The computation nodes then look like  min  max  N     max  min  N    or    max  N    and the MCDAG clusters use  c   c      min  max    max  min   or    max       CONCLUSION  To solve influence diagrams  using potentials allows one to reuse existing VE schemes  but may be exponentially sub optimal  The key point is that taking advantage of the composite nature of graphical models such as influence diagrams  and namely of the algebraic properties of the elimination and combination operators at stake  is essential to obtain an efficient architecture for local computations  The direct handling of several elimination and combination operators in a kind of composite architecture is the key mechanism which allows MCDAGs to always produce the best constrained induced width when compared to potential based schemes  The authors are currently working to obtain experimental results on MCDAGs in the context of the PFU framework  the construction of MCDAG architectures is currently implemented   Future directions could be first to adapt the MCDAG architecture to the case of Limited Memory Influence Diagrams  LIMIDs   Lauritzen and Nilsson         and then to use the MCDAG architecture in the context of approximate resolution  Acknowledgments We would like to thank the reviewers of this article for their helpful comments on related works  This work was partially conducted within the EU IP COGNIRON  The Cognitive Companion  funded by the European Commission Division FP  IST Future and Emerging Technologies under Contract FP          
  level  with the condition that violating however many formulas at a given level is always more acceptable  Penalty logic  introduced by Pinkas         than violating only one formula at a strictly higher  as  sociates to each formula of a knowledge base  level   the price to pay if this formula is violated    e    z   Penalties may be used as a criterion for se  apparently very appealing  besides it has already been  consistent knowledge base  thus inducing a  used several times in the literature  consists in weight  A pre  ing formulas with positive numbers called  cise formalization and the main properties of penalty logic and of its associated non  since they are  additive   ties of the rejected formulas  Moreover  inviolable  or unrejectable  formulas are given an infinite penalty  The additive combination of penalties leads to an in  pecially in the infinitesimal case   terpretation in terms of  itarist   Introduction appears when  the available knowledge base   KB for short    here a set of propositional formulas  is inconsistent  Most approaches come up with the inconsistency by select ing among the consistent subsets of KB some  preferred  subsets  the selection criterion generally makes use of uncertainty considerations  sometimes by using explic itly uncertainty measures  such as W ilson ferhat and Smets         expressed qualitatively as back to Rescher       and  priorities   the idea comes        Nebel         Benferhat  Cayrol  Dubois  Lang  Prade  Lehmann          Ben  has been developed by many  authors  among them Brewka                  or more often using measures  Cayrol       and  Although these priorities are gener  ally not given a semantics in terms of uncertainty mea sures  however see       for a comparative study of the  priority based and possibilistic approaches to inconsis tency handling   their intuitive interpretation is clearly in terms of gradual uncertainty  the least prioritary formulas    i e   the  ones which are most likely to be re  jected in case of inconsistency  are clearly the ones we are the least confident in  i e   the least certain ones   cost   thus this criterion is  util  contrarily to priority based approaches which  are rather  inconsistency handling  the global penalty for rejecting  a set of formulas is the sum of the elementary penal  first part  We also show that penalty logic and Dempster Shafer theory ate related  es  The problem of  penalties   Contrarily to priorities  penalties are compensatory  monotonic inference relation are given in the     non compensatory   An alternative approach  more or less empirical but  lecting preferred consistent subsets in an in non monotonic inference relation   thus these approaches are  levels never interact   egalitarist   This additive criterion is very in  tuitive  since rejecting a formula generally causes some  additive  trouble with the experts which provided the f  B with the formulas  or some real financial cost  or  another kind of additive cost  Note that a degenerate case of penalties  all penalties being equal to     prefers  subsets of maximum cardinality  Moreover  and as we will see later  these penalties can sometimes be inter preted as the  probability of fault  of the source which provided us with the information  all sources failing in dependently   up to a logarithmic transformation  In any case  these penalties can be viewed as measuring  uncertainty  since  again  the less expensive to reject   the more uncertain the piece of information  penalty logic  Thus   expresses uncertainty in terms of costs   However a formal connection of penalties with classical theories of uncertainty has not really been made  Penalty based approaches have been already used sev eral times in the literature  first by Pinkas           from  whom we borrowed the terminology  penalty   who uses them for inconsistency handling and for mod elling symmetric neural networks behavior  and also  All aforementioned priority based approaches consist  by Eiter and Gotlob         for cost based abduction  by Sandewall         for cost based minimization of  the set  Satisfaction Problems  Moreover  penalties associated  in ranking the f  B in n priority levels  assume that   is the highest priority and n the lowest  and maximize  or  the number of formulas satisfied at each  surprises in temporal reasoning and by Freuder and Wallace        for tackling inconsistencies in Constraint   Penalty Logic and its Link with Dempster Shafer Theory  to formulas have also been used for guiding the search in randomized algorithms dedicated to the satisfiabil ity problems  such as GSAT            Lastly  there  should clearly be a link between penalties and utility theory  the latter has been recently used in AI  espe cially in decision theoretic planning   see e g         however  in this paper we do not investigate this pos sible link        Since PK is a multi set of pairs  and not a  set   it  is possible for a pair      a   to appear several times in PI    for example  PK     a       a      is not equivalent to PK      a      since using PK  it costs   to delete a   and using P K   it costs only    However  as we will see in        if a formula    appears several times in PK then we may replace all the occur rences of the formula  P by only one occurrence of  P  In this paper we revisit penalties by giving a further  formalization of Pinkas  work  we also go further in the  annotated with the sum of the penalties associated to this formula in the previous base  The new knowledge  theoretical study of penalty based inconsistency han  base obtained is equivalent to the initial base   dling and non monotonic reasoning   We briefly give  a formalization in penalty logic of an additive problem  Lastly  we establish  a  O R   link between penalties  and Dempster Shafer theory  this link is twofold  first  the penalty function  on interpretations  is equivalent  up to a log  transformation  to a contour function  i e   the plausibility function restricted to singletons   then penalty functions on formulas coincide with plausibil ity functions of an infinitesimal t ersion of Dempst er Shafer theory      based on a finite number of propositional variables  T      wi ll be writt e n       j   etc  The set of interpretations attached to     will be de noted by n  and an interpretation by w   P F      and Formulas of   P f l lj  will represent logical consequence and logical equivalence between the formulas  I     P and     respectively   will also be used between an interpretation and a formula to denote satisfiahility  The set of models of a formula  P will be denoted by M    P   the set of for mulas of     satisfied by w  i e     P I w I    P  will be denoted by  w    A classical knowledge base    is a set of formulas of      A sub theory of    is a consistent subset of    A    is a consistent subset of    T  T U    P  is inconsistent  Given  maximal sub theory T of such that  if    E       a formula      T is said  ljl consistent iff T U    j   is c o n sistent  Tis maximal lj  consistent if it is   J consistent  and V P E     T  T U             is inconsistent  w  will be the union of the set of all the strictly positive real particular  if     oo   equipped with the usual order     oo then a     oo    a    in  A penalty knowledge base PK is a finite multi set of pairs   P   a     where      E     and a   E w   a  is the penalty associated to  Pi  it represents intuitively what we s hould pay in order to get rid of  Pi  if we pay the  requested price we do not need any longer to satisfy  Pi  so the larger  a    In particular  if a    move  the p en al ti es ex    Also  in the expressions sub theory of PK  subset of PK and PK  A we will refer to the set of  is  the more important  Pi is        Cost of an interpretation  Let PK  and  l will represent taut ology and contradiction re  numbers and  Lastly  we will say that PK E  JlJc is co nsistent if the set of formulas  Pi of PK is consistent   without mentioning            a    i            n   be a penalty knowl  edge base   following      will be a propositional language  spectively   violated           Formal definitions  In the  logic comes down to classical logic  no formula can be  formulas obtained from PK by ignoring the pena lties   Penalty logic         c will be the set of all the penalty knowledge bases  Note that when the penalties are all infinite  penalty   oo then it is forbidden to re  i p  from PK   Pi is inviolable    Definition    Pinkas          The cost of an in w E Q with respect to PK   denoted by kpK w    is equal to the sum of the penalties of the for  terpretation  mulas zn PK violated by   with  the  w   corn enfion L  P E  a        Definition   A PK preferred interpretation is an in terpretation of minimal cost w r t  PK  i e  an inter pretation minimizing  kpl    As an example  let us consider the following penalty k n owledge base P K     PI  a y   b v c y      b  P      c         oo                            Here are the corresponding interpretations costs   kpr       a  b  c   kpr          a  b        c   kpK   a    b    c   kpK   a b   c   kpK   a b c   kPK   a b c    kpK     a  b c      oo kpK     a  b  c      oo                      If the interpretations are decisions to make  for exam ple if the knowledge base is made of constraints con cerning the construction of a timetable   then a min imum cost interpretation corresponds to the cheapest        Dupin de Saint Cyr  Lang  and Schiex  decision  i e   the most interesting one  The cheapest interpretation is generally not unique  Besides  if the penalties are all equal to   then a cheapest interpreta tion satisfies a maximum consistent subset of PK w  r  t  cardinality         Cost of consistency of a formula    The cost of consistency of a formula r p with respect to PK  denoted by f PK cp   is the mini mum cost with respect to PK of an interpretation sat isfying r p  KpK C O   min kpK w   Definition  wi  P   with the convention min  kPK w       oo   Example   f PK   a   b  KpK  a  c  KPK    a      oo  KpK  f   of a formula r p  is the minimal price to pay in order to make PK consistent with cp  For example  in order to make PK  consistent with a    c  the least expensive way is to remove r o    oo and J PK   T     J PK l    minwEn kPK w    All proofs can be found  in French  in Dupin de Saint Cyr  Lang and Schiex        and in Dupin de Saint Cyr          KPK l      oo is easy to understand  because it is impossible to have PK consistent with      Let us note that   pK   T  is the cost of any PK preferred inter pretation  it is thus the mini mum cost to make PK consistent  Property    KpK T   is inconsistent       oo      cp   E  PK a       oo   This quantity KPK T  is important  because it mea sures the strength of the inconsistency of PK  i e   how expensive it will be to recover the consistency   If the penalties are all equal to  oo then KpK T  can only take two values    if and only if PK is consistent  and  oo if and only if PK is inconsistent  Example  J PK   T       the only minimum cost in terpretation is  a  b    c   To make PK  consistent  the least expensive solution is to take off  or to ignore  the formula  f   Property      pK T          Property    Vi p   E   f    cp f             KpJ   P       Vr o    E      KPK IO         max I pK  P   Kpr            KpK     V       mi n KP K   cp      KpK         f pK l         pK IO     f pK T   Note that  up to its interval of definition and its or dering convention w r t  Proposition          oo         instead of          S      pK is actually a possibil ity measure  Note also that Spohn s ordinal condi tional functions x  verify property      e  x   A U B    min x  A   x  B         Cost of a sub theory  Definition    Pinkas       J  The cost CPK  A  of a sub theory A of PK  is the sum of the penalties of the formulas of PK that are not in A     f        EPK A For instance  considering the knowledge base PK   given A     i pl i p   P   and A     r o  i p    we have CPK   Al    a      and CpK   A           a     oo  Definition     VA  B  PK   B   PK A  B is preferred to A  ifJCPK B  S CPK A    VA  B  PK   B  h A if and only if B   PK A and not A   PK B  Definition    Pinkas          A  PK ferred sub theory relatively to PK  is  a  pre   or   PK preferred  if and only if A is consistent and    B  PK   such that B is consistent and B  f K A   Note that there may be several preferred sub theories  in the previous example    cp    P    p   is the only one    PK  preferred sub theory   Property     VPK  E  fflc   If J pK T    f  oo  then any   PK  preferred sub theory is a maximal sub theory of PK w  r t  inclusion        PK is consistent   Indeed  if KpK T      then there is no need to delete any formula in order to make PK consistent  therefore PK is consistent  and conversely    KPK       Property              The cost  Property  This property is the monotonicity of K with respect to classical entailment     Let us notice that when KPK T     oo  every sub theory of PK has an infinite cost  therefore every sub theory of PK is   PK preferred  but ob viously every sub theory is not necessarily maxi mal w r t  inclusion  Besides  if PK is consistent  then I PK T       and then the only  pK preferred sub theory of PK is PK itself  its Zost is      Example  continued   A     r ol  f    P   is a   PK  preferred sub theory and it is maximal w r t  inclusion         Penalty Logic and its Link with Dempster Shafer Theory  But  although  cp  cp  cp   is a maximalsub t heory of PK   w r t  inclusion   it is not    PK  preferred  be cause its cost is infinite    If we add the formula   cps       a  a       oo   to  PK   Besides  we define a pre ordering relation   c as follows  Definition    then the subset of infinite cost formulas is inconsistent  therefore every sub theory has an infi ni t e cost  and a  every sub theory is  preferred  PK  interpretation cost of the sub  theory of PK composed of all the formulas satisfying  w   As  PK   an example   PK n      A is a maximal sub theory of PK Vw f  A  kpK w    CPK A   Corollary      KPK  P  is equal to the of a    consistent sub theory of PK   Corollary  of a formula  minim  m cost  cp wit h respect to the  base PK is the cost of a cp consist ent  sub theory of      min CrK A    A  PI  A     conHtent  Therefore  the cost  PK     rK preferred      VA  PK   Corollary  A  is  a       cf  corollary  Definition          PK  preferred sub theory  KPK T      with cp      Add PK  cp   CpJ  A    T      PK U     p   oo        ta b   ia   bl J  a  bl l      a       bl  Therefore  the cost to make the knowledge base consis tent with a given formula  can be computed by adding with  an  infinite penalty and then evalu  ating the cost of the new knowledge base consistency   Two  alent  Equivalence between penalty knowledge bases k n o w ledge bases are  semantically equiv if they i n d u ce the same cost function on    i e    penalty  Definition   VPK  PK  E  fie  PK c PK   PK J  PK  as                    and  follows   b                   the fol               So we have PK  c PK  and PK  c is not equivalent to PK     PI     but Pl    N B   the previous example shows th at it is impos sible to transform equivalently a penalty knowledgr  base containing several non equivalent formulas in a penalty knowledge base containing the conjunction of those formulas   But  if a knowledge base contains se  eral times the same form tla  or an eq  ivalent on e   it is possible to transform it equi alent y in a knowhdge base contain ing this formula only one time with a penalty equal to the sum of the penalties of this form ula in the prelious base  Property   VPK  PK  E  JlJc   PK              consider  c  PK       A cpiJ   E  PK  f ll    PiliPi  E  PK    The con verse is obviously false   Property    this formula  b     w     PKpreferred sub theory with respfCI to all the sub theories of PK     us  lowing        KpK rp   let  The cost func t i ons incluceu by those bases are  E     has a minimal cost U   r f  PK  a   w  is  less expensive than PK      kpK   the penalty knowledge bases defined PK    PKa   PI       a      a   a     b  w  is  E  Jic      kpK  a  Vw  PK   PK  sub theory   Property   The cost kpK w  of an w E   with respect to PK is equal to the  Corollary      c  VPK  PK   on      l K is semantically equivalent to PK           kpi       kpw   Inconsistency handling with penalty logic  Using penalties to handle inconsistency is a syntax based approach  in the sense of       which means that  the way a know ledge base be hav es is dependent on the syntax of the input  this is justified by the fact that each formula is considered as an independent piece of  information   for instance   p q      pY   q  will not be have as  p    q       p Y      q   since in the first  case we ca n re  move independently the formulas   p   p and q     p  q    p V   q  and   q       p Y      q  are the maximal sub theories   but in the second case we must rem ove or        whole fo rm ula p    q    p    q  and       p V   q  the maximal sub theories    keep the are  In order to deal with inconsistency  the basic idea de veloped with syntax ba  led approaches is to define a        Dupin de Saint Cyr  Lang  and Schiex  nonmonotonic inference relation as follows    J can be deduced nonmonotonicaly from a knowledge base iff all the maximal sub theories of this base entails  clas sically              Given  Nonmonotonic inference relation induced by a penalty knowledge base  PK  E  Definition  c      r cp   J  E           In this section  we will see that penalty logic is not only a tool for inconsistency handling but also a good way to represent  in a logical language  discrete opti mization problems  for instance issued from operation research   in which minimum cost interpretations cor respond to optimum solutions   We consider an undirected graph      v A  PK  if A is a    PI   preferred cp consistent  sub theory among all the cp consistent sub theories of PK  then AU  cp  f    J   sub graph  N B    r        L Property  In penalty logic we can represent it like this      J   s  that this vertex  f   K  J       v cp    J     if w f   to each vertex  sitional variable         E  E U  we can associate  s  a  propo  which truth assignation means  belongs to  the clique we are look  minimum of vertices  to each vertex we associate the penalty formula  s       f    K   J  cp  we are searching for a set of vertices which is max imum for cardinality  so we have to exclude the       and w is a    PI   preferred interpretation satisfying  p  then w f     J  E  every vertex is connected with every  ing for    P   r w  then A F    i e    other vertex   Finding a maximum cardinality clique is a classical N P ha rd problem in operational research   In particular  if cp   T  the definition becomes  f     K  J     if A is a    PK preferred sub theory among  PK   G  i e   a set of ver  tices U and a set of edges V connecting those vertices  A clique of G is a subset of V which define a complete  cp f    K  J  all the sub theories of  An application of penalty logic  maximum clique in a graph  the resulting set   x  y   graph  must be a clique so for each pair  of vertices that are not connected in the  G  i e    x  y   V   at least either  x  y  or  This property shows that the nonmonotonic inference  does not belong to the clique  In consequence  we  relation  can associate to each pair  f    K  belongs to the set of relations based on  preferential models in the sense of       As Pl  is a complete pre ordering  we immediately get the follow ing result   Property  Let  f    K      relation   Property  P     Given         zs  a  comparative  inference       x V   y   oo    formula  PK G        s  l  s  E   x  y    V the penalty  U U      xV    y  oo   x y     V        see      Every minimum cost inter pretation with respect to PI   G  corresponds to a max imum clique of G and conversely   Property PK E  c and cp    J  E      with  Example   For instance   let us consider the following penalty  knowledge base  e    a   l  b  l  c      d  l  e               c  The minimum cost interpretation       a f    I    al b   f   K  is    a  b  c  d    e    This example shows the ability of penalty logic to en  It can be checked that   f    I   c  b     c   a    a V c   oo       a V     d   oo    a V e   oo     b V e   oo       c V e   oo      oo       av   b        b           code discrete optimization problems  One could ar gue that  in operation research  algorithms for solv  c  c  ing classical problems  as maximum clique  minimum vertex cover      do already exist  Those algorithms are probably more efficient than the one consisting in  c  comparative inference relation      is a rational rela  tion      that also satisfies supraclassicality  if  P   Pb  K f     d  PK     a V b       a   A  a  F       then  finding the best interpretation oped in         in penalty logic  devel  However  the logical representation of this  kind of problems presents at least two advantages  the        Penalty Logic and its Link with Dempster Shafer Theory  great power of expression of logi c allows us to spec ify many complicated problems which could not easily be specified within the operational research language  and the best solution search method is independent of the given problem      Relating penalties to Dempster Shafer theory  In this section we are going to show   ties are used to induce a preference r el at ion on    first  that the cost of an interpretation kpK   rl      oo  induced by a penalty knowledge base PK c ons i s t ing of n weighted formulas corresponds ac tually to the contour function pl   rl             induced by Dempster s combination of n simple support functions  one for each formula rpi      th en   that moreover  the f unc tion Kp               f           oo  corresponds to a plausibility me as ur e in an infinitesimal version of Depmster Shafer the ory        Interpretation costs and contour functions  Let PK      rp   a    i          n  be a penalty kn owl edge base  Let us define  for each i  the body of evi dence m    m  rp         e a  m   T    e  a     By convention we take e     S ince a  E      oo   it can be seen that m  cpi  E        and m  T  E          note that lim     co m  cp        m  is called a simple support functi on       Let m   m  EfJ EBmn be the result of Dempster s combination of the m   s     without re normalization  The contour func tion pl   n        associated to m i s the restriction  M oreover     of  the      plausibility function to sin g le ton s    pl w  Now      P l     w        L  i e    m  rp   it is well known      that    IT Pl   w      II        IT  i wl          II  i wl   cp   i wl cp   e   port functions in order to rank interpretations can be  d on e a lt erna ti vely with penalty logic   This also brings to light  a relation bet ween penalties and      where each formula  Pi of the knowledge base is considered to be given by a distinct source  this source having the pro bability p  to be faulty  i e   the infor mation it provides us with is not pertinent   and all  so u rces being independent  which gives the simple sup port function m  cpi        p   and m  T    p     So if the task is only to find the most plausible interpre tation  as in      which i s the C o ns tr aint Satisfaction counterpart of        it can thus be done equivalently with penalti  Ps        Formula costs as infinitesimal plausibilities  Let us consider an infinitesimal version of Dempster  Shafer theory  where the masses involved are all in finitely close to   or to    Let c be an infinitely small quantity    Again  let PK     cp  a   i         n   Let us define  for each i  the infinitesimal body of evi dence m      i l  where P l  is the plausibility function induced by m   Moreover  Pl  w      if w f   Pi and Pl  w    e c   if w f     Pi  Thus   pl w   rl  a nd  then possibly to select one of the  or all   cheapest in t erpretat ion s   Namely  this is eno ugh for inducing the inference relation      K  for solving discrete op timization proble m s  and also for applying p en alties to constraint satisfaction problems or abduction  So  ha ndlin g penalties in su ch a purpose is nothing but performing Dempster s combination on s impl e sup port functions  Reciprocally  combining simple sup  m     r o         ca  m    T    ca   n  Pl  w    Therefore  kpK w     ln p  w    up to a logarithmic transformation  kPK is a contour function  or mo re pre cisely  the process consisting in computing kpr  corre sponds to applyin g Dempster s combination without re norma lization on simple support functions  Thi s equivalence does not extend to an equivalence between   pK and a plausibility function  see subsec ti on       but this result is already significant  since in most prac t ic al applications of penalty logic  only the contour function kpK is useful  this is the case when penal  e c    Lt  t     Pi O i  e kPK w   e Ct   Let m    m     tfl   ffi m  n be the result of Dempster s combination f the m   s      without re normalization  Let us show now that J PK has the same or d er of mag nitude  w r t  c   as ln P     where ln Pl   is the plau sibility function induced by m   Let us note that the set of focal elements of exactly    iEI Pi  It            n     m   is   More formally  this consists in considering a family of  e s tending towards    indeed what we are interested in is only the limit of the considered  We recall that ft e      f  I    when  h e  iff lim  o  I   tends to       i j                Dupin de Saint Cyr  Lang  and Schiex     Now  let us define R PK  w       I    l    n   J    p    i      w consistent   iEI  Now  II m   p    II m  T  i l  IER PK t J  iE   Pl   j    IER PK  j   iEJ  As c  is infinitely small and    a  R      therefore  PI    R    I  is always finite  f  E       ER PK t J    J  E  as  R PK   J    La  is minimum  i ll  and let r PK   J   Since  c  is     IRminpen PK       infinitely small  we have  PI    R     ERm      PK  P   r PK    j   maxiER PI         L er a  r PK   Used to handle inconsistency and perform non monotonic inferences  penalty logic has shown to have interesting properties  Using penalties for selecting preferred sub theories of an inconsistent knowledge base not only allows to distinguish between the degree of importance of various formulas  as usual priority based approaches do  but also to express possible com pensations between formulas  The non monotonic in ference relation defined satisfies the usual postulates      and is  logarithmically  related to an infinitesimal version of Dempster Shafer theory  Furthermore  the complexity of the penalty non monotonic deduction problem has been considered in     and is ranked as one of the most simple non monotonic inference problem  in     ER PK  f   i tl  Let us now define Rminpen PK       Conclusion    c miniER PK   Z v a   Now   Penalty logic may also been considered as a logical lan guage for expressing discrete optimiza tion problems  The search for a preferred interpretation has been im plemented using an A  like variant of Davis and Put nam p ro ced u re     and has been tested on small ex amples  Randomized search algorithms such as GSAT          could also be considered  but they do not guar antee that an optimum is actually reached    As shown in      solving the problem of searching a preferred interpretation allows to simply solve the non monotonic inference problem  without any restriction on the language of the formulas expressed   Any way  even the limited ll  complexity can be consid ered as excessive when faced to practical applications  A reasonable approach would then consist in defining a gradual inference relation and in trying only to solve an approximation of the resulting gradual inference problem  Among the other possible extensions of penalty logic   minJER PK tiJ   La  i l   min  BCPK  BAtjJ    c onsistent  min  BPK BAtjJ consitent  La    Pi lB  CpJ  B   one could consider associating many unrelated penal ties to a single formula  Partially ordered penalty vec tors would then replace penalties  Another possible ex tension consists in taking into account not only penal ties caused by violations but also profits associated to satisfactions  which could be expressed using negative penalties   Acknowledgements  Therefore   Note that r PK    does not depend on      and more r PK          So  up to a logarithmic transformation and a multiplicative constant  in other terms  if we consider only the orders of magnitude w  r  t  c     Kpl  is equivalent to an infinitesimal plausi bility function   We would like to express our thanks to Didier Dubois and Henri Prade for helpful suggestions concerning the link between penalties and Dempster Shafer the ory  and Michel Cayrol for having found an error in a preliminary version  This work has been partially supported by the ESPRIT BRA project DRUMS     over that     Using an ATMS for computing candidates and pre  ferred sub theories could also be considered  but the re sulting complexity is more important in the general case          Penalty Logi c and its Link w ith Dempster Shafer Theory  
