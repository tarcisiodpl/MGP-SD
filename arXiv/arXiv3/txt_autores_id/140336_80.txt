 Qualitative and infinitesimal probability schemes are consistent with the axioms of probability theory  but avoid the need for precise numerical probabilities  U sing qualitative probabilities could substantially reduce the effort for knowledge engineering and improve the robustness of results  We examine experimentally how well infinitesimal probabilities  the kappa calculus of Goldszmidt and Pearl  perform a diagnostic task troubleshooting a car that will not start   by comparison with a conventional numerical belief network  We found the infinitesimal scheme to be as good as the numerical scheme in identifying the true fault  The performance of the infinitesimal scheme worsens significantly for prior fault probabilities greater than       These results suggest that infinitesimal probability methods may be of substantial practical value for machine diagnosis with small prior fault probabilities  Keywords  Bayesian probabilities  kappa probabilities  diagnosis   networks  qualitative calculus  i nfinitesimal    BACKGROUND AND GOALS  Bayesian and decision theoretic methods have long  been criticized for an excessive need for quantification  They require many numerical probabilities and  Brendan Del Faverol  Gillian Sanders    oepartment of Engineering Economic Systems  Stanford University  CA        Section on Medical Informatics Stanford University  CA        utilities that are difficult to assess and are liable to judgmental biases  Some people claim that since human thinking is inherently qualitative  it is incompatible with quantitative schemes  These criticisms have fueled interest in alternative formalisms for reasoning and decision making under uncertainty that are intended to be easier to use and more compatible with human cognition  Among these alternative schemes are  various generalizations of decision theory  Edwards         Dempster Shafer belief functions  Shafer        generalizations of logic  including default and non monotonic logics  Ginsberg         fuzzy logic  Zadeh         possibility theory  Dubois and Prade         and fuzzy probabilities  If   however  our goal is simply to provide a qualitative basis for reasoning and decision making under uncertainty  there is no need to abandon Bayesian decision theory  The axioms of decision theory  indeed  assume only the ability to make qualitative judgments   that is  to order events by probability or outcomes by desirability  The quantification of probabilities and utilities can be based on purely qualitative judgments  Furthermore  several schemes have been developed that are purely qualitative  but are consistent with the axioms of decision theory  One such scheme is qualitative probabilities  originated by Wellman        Henrion   Druzdzel       Wellman   Henrion         A second approach to qualitative probabilities is the kappa calculus  Goldszmidt and Pearl         which represents all probabilities in a Bayesian belief network by e K  where is an integral power of E  The K  calculus is  K   Henrion  Prova n Del Favero  and Sanders          consistent with the axioms of probability where E       Events are ranked according to K  Events with larger K are assumed to be negligible relative to events with smaller K  The calculus provides a plausible set of events  those with the smallest  most probable  consistent with the observed findings  The calculus is sometimes called  qualitative probability   To avoid  confusion with other qualitative probability schemes  we call this representation Pearl  infinitesimal probabilities          has extended this scheme to handle  similar difficulties  Much current research on qualitative simulation is directed towards integrating quantitative information to resolve ambiguities  and the resultant combinatorial explosions of the search space   In this paper  we report the results of an initial experimental  study  comparing  the  diagnostic  performance on a specific belief network using     the K  calculus or infinitesimal probabilities  and       qualitative utilities to support decision making   numerical probabilities  Our goal is to examine how  The K calculus or infinitesimal probabilities can be  approximation to the numerical representation   well  the  infinitesimal  scheme performs as  an We  looked at in two ways   a  as providing a scheme for  start with a fully assessed numerical representation   non monotonic reasoning whose semantics are firmly  convert this into a kappa representation using finite e  grounded in probability and decision theory  or  b  as  values  and perform inference on a set of test cases   providing a simplification of belief networks with  We first explain the mappings we used to obtain  numerical probabilities  In this paper  we are focus on  infinitesimal  the second view  and examine the performance of  probabilities  and how we mapped back from the  infinitesimal probabilities as an approximation to numerical probabilities   or  K values  from  the  numerical  pos terior K values into probabilities for comparison of  From this perspective   performance  Then  we describe the experimental  proponents of infinitesimal probabilities may claim  design  including the sample network  the set of test  four possible advantages over traditional numerical  cases   belief networks   probabilities  the epsilon values used in mapping  and     It may be easier to express beliefs by partitioning  and  our  variations  of  the  prior  fault  the number of findings observations per case   The  number of sets of relative  infinitesimal scheme provides a set of the most  plausibility  that is values  than by assigning  plausible diagnoses for each case  In the results  we  events into a small  each event a precise numerical probabilities      Results from reasoning with infinitesimal  compare these plausible sets with the posterior probabilities for the diagnoses produced by the  probabilities are more robust and therefore more  numerical  trustworthy since they are based on less specific  implications of these results for the application of the  inputs   K calculus as a practical representation   scheme   Finally   we  discuss  the     Reasoning with infinitesimal probabilities is easier to understand and explain     Inference methods with infinitesimal probabilities  can be computationally more efficient   Initial analysis of the computational complexity of          suggests that  in general  it is of the same order as reasoning with numerical probabilities  that is NP hard   Cooper           There  may  be modest  computational savings from doing arithmetic with small integers instead of floating point numbers  Most  research  on  qualitative probabilities has  concentrated on developing the formalisms and efficient algorithms   AND INFINITESIMAL PROBABILITIES  Hitherto  these claims have been largely untested  reasoning infinitesimal probabilities Darwiche    MAPPINGS BETWEEN NUMERICAL  There has been little concerted  effort to demonstrate their application to real tasks and to evaluate their practicality  Initial studies of QPNs  Henrion and Druzdzel         Druzdzel and Henrion        Druzdzel        suggest that they are often  inconclusive for nontrivial cases  For example  QPNs give vacuous results in any case with conflicting evidence  Studies of qualitative simulation have found  In order to be a b le to apply  the  K  calculus to  probabilistic reasoning on a belief network with finite probabilities  we need to provide a mapping from probabilities into kappa values  In order to compare the results we need to map the kappa results back again into probabilities  Strictly  the K calculus is only valid as E   tO   We use an approximation for finite  values of E  For a finite E  the K calculus partitions the  real interval        into regions identified by integers   based on the smallest power of in the polynomial  This mapping is illustrated in Figure    More specifically  consider the real       interval I  which is the interval used by probability theory  and a discretized representation of I  which we call      is a set of non negative integers which the  calculus uses to represent probability measures in the interval I   We  wish to explore the mappings f  I   tS  i e   from numerical to infinitesimal probability  and g  S    t I   Nume rical and Qualitative Probabilistic Reasoning   i e   from infinitesimal to  numerical probability    Note that there is information loss in the mapping f  since it is not injective  Moreover  the mapping g is not surjective   Definition           K  map   Spohn       The mapping f  from probability measures to  K  values takes a  probability  r and a threshold probability e and  outputs a K  value K  e S such that    APPLICATION DOMAIN  WHY YOUR  CAR DOES NOT START The task is to troubleshoot why a car is not starting  given evidence on the status of the lights  battery  fuel   fan belt  and so on  Figure   shows the Bayesian belief  network displaying the causal and conditional independence relations   We are grateful to David  Heckerman for providing the original belief network and to Paul Dagum for lending us his expertise as a Figure   shows an example of a mapping for            car mechanic in adjusting some of the probabilities   All variables are binary  present or absent   except for battery charge which has three values  high  low  none   The initial network contains fully quantified  numerical conditional probability distributions for  Kappa  C X   each influence and prior probabilities for each fault  source variable       common  effect  Effects of multiple causes of a are combined  with noisy ORs   generalized where necessary  There are nine explicitly identified faults in this model  spark plugs bad distributor bad fuel line bad fuel pump bad gas tank empty                    Prd lability p X        starter bad battery bad  Figure    An example mapping giving kappa as a  fan belt loose  function of probability  for        alternator bad  Figure     Bayesian network representing the car diagnosis domain  Leak events represent all the  potential causes of a fault other than those shown explicitly  The number in each origin fault of a leak node represents its prior probability in the original network  The numbers attached to each influence arrow represent causal strengths  that is the probability that the successor is broken given that the predecessor is broken  and all other predecessors are normal         He nrion  Prova n Del Favero  and Sanders    We also identified three leaks  Each leak event represents all possible causes of an event that are not explicitly identified above  The probability of a leak is the probability that its associated effect will be observed even though none of its identified causes are present   from    to       Table   shows the mean and range of  the resulting prior odds we used   Table    The minimum  mean  and maximum prior fault probabilities  The top line shows the original  engine start other  The  network with larger probabilities  To do this  we multiplied the prior odds by an odds factor ranging  probabilities  Those below are derived by multiplying  engine tum over other  the odds of each prior by the odds factor and  charging system other  converting back to probabilities   leaky noisy  or model assigns a probability to each  leak  to handle the fact that the network is inevitably incomplete  In our adjusted network  the probability  Odds  of each leak was substantially smaller than the sum of  factor  the probabilities of the identified causes for each event   Minimum  Mean  Maximum                                                                                                                                                                                                There are    observable findings in the model  listed here in non decreasing order of expense to test      engine start    gas gauge    engine tum over     lights    radio    fan belt    battery age    distributor        spark plugs     alternator  Note that there are four findings that are also enumerated faults  namely fan belt  alternator  spark      Test Cases and quantity of evidence  plugs  and distributor   We expected that the performance of both numerical    EXPERIMENTAL DESIGN  function of the quantity of evidence  We also wished  We wish to investigate the effects of three factors on  relative  the diagnos tic probabilities   performance  and infinitesimal schemes would improve as a  of  inf initesimal  to examine the effect of the quantity of evidence on the performance  of  the  two  schemes   Accordingly  we needed a representative set of test cases with varying numbers of findings    a  The choice of the value of E on the mapping between numerical and infinitesimal probabilities    b  The range of prior fault probabilities  c  The quantity of evidence in the test cases  We have already discussed factor  a   Here  we will discuss our choice of each of these factors  and the conduct of the experiment   We generated a set of     test cases  in the following manner  For each of twelve faults  nine identified faults plus three leaks   we identified the most likely  modal  value for each of the ten observable findings  For each fault  we created a base  case  consisting of all  findings at their modal value  In four cases  the fault is itself a finding  which we omitted from the base test  case  since including the true fault as observed in the test case would be trivial  We then generated a second case for each fault by omitting the most expensive observation from the base case   Further cases were      Range of prior fault probabilities  generated by omitting the next most expensive  The numbers in Figure  finding that the engine does not start  In this way  we created a series of ten cases for eight faults  and nine    are the original prior fault  probabilities  To examine the effect of the magnitude of the priors on the relative performance of the infinitesimal calculus  we created versions of the  observation in tum   In all cases  we retained the   Numerical and Qualitative Probabilistic Reasoning  cases for the four faults that are observable  resulting in a total of     test cases in all             faults are clearly identifiable  having probabilities at  least an order of magnitude greater than those of all other faults  We found that this approach  as expected   gave very similar results to the exact IC calculus  Computation  inference using CNETS    To obtain results for the numerical probabilistic  scheme  we employed IDEAL  Srinivas and Breese          using the clustering algorithm from the I DEAL library  We applied each of the     test cases to the network using each of the six sets of priors  performing a total of     run  For each run we computed the posterior probability for each of the twelve faults resulting in      probabilities     RESULTS Our first goal was to examine the effect of E values on the performance of the infinitesimal probability scheme  We then selected the value of E that gave the best results and examined the effect of varying the  quantity of evidence on the performance of both  numerical and infinitesimal schemes   We also converted the original numerical probabilities into K values  using the three values e                     resulting in a total of      additional runs  We ran       calculus developed at  we might expect it to perform better for small  where  each case using CNETS  a full implementation of the K the  Rockwell  Palo Alto  Laboratory  Darwiche         producing posterior K values for each fault  For each run  we computed the  plausible set  that is the subset of faults with the minimal K value  Definition   Plausible Set      Consider a set  V    v  v    vm representing m possible hypotheses    Let  vmin       minvj by the minimum    value  J  probability interval           as shown in Figure    and  larger e  To investigate this we analyzed an initial set of    test cases usin g E values of                                 Figure   shows a graph of average probability against e  It is interested to note that the average score  identical fore      and e  To compare the infinitesimal scheme with the numerical one  we converted K values of diagnoses back to probabilities as follows  De fi n it ion     original probabilities  with less information lost   Accordingly  we might expect it to do better with  is identical for E        and E  Cll V   j vj  vminl   Pro b a bility  setV  v  v        vm r e presenting assigned a  the approximation will be mere exact  On the other  hand  a larger E provides rnore partitions to the  score assigned to the true diagnosis for these cases   The plausible set is given by  hypotheses   Since the kappa calculus is only strictly correct as E        consequently  it provides a finer discretization of the  in which each hypothesis has been assigned a value   Effect of E values  score   m  For              and also       Overall  there is an  improvement in performance with increasing E up to       Accordingly  we selected E         for use in our  remaining experiments   a  p o s s i b   e  in which each hypothes is has been     value  the corresponding probability  distribution is given by  ifvj vmax                                   I           s                   a              r                         otherwise  That is  the probability ni    n is assigned to the true faults if it is in the plausible set of size n  Otherwise  we assigned p                                                As an additional test  we also ran IDEAL using the  exact algorithm  but using fault probabilities mapped to O OlK for the values obtained from the mapping using the full set of K values   subset of    test cases   We applied this to a  In the results  the plausible  Figure    Effect of E o n the score  probability  assigned to the true fault  by the infinitesimal scheme   Henrion  Provan  Del Favero  and Sanders           Effect of Number of Findin gs on the  Plausible set                        As the quantity of evidence increases  we should expect the performance of both numerical and infinitesimal schemes to improve   Accordingly  we  classified the cases by the number of findings  Figure   graphs the average size of the plausible set  number of  Gl  I I          Cl     e  Cl          plausible faults  identified by the infinitesimal scheme as a function of the number of findings  These results summarize all    cases fore            As expected  the  average size of the plausible set of faults decreases with the number of findings  from   faults with   finding to      faults for    findings  With   findings   o                  Number of findings  this scheme provides almost complete specificity that is  the plausible set usually consists of just a single diagnosis        Ill  Figure    The probability assigned to the true fault for each scheme as a function of number of findings              Ill      Ill  I       liloo  What is  perhaps  surpnsmg is how closely the performance of the infinitesimal scheme tracks the performance of the numerical scheme           N           Indeed the  infinitesimal scheme appears to perform better than the numerical scheme for intermediate numbers of     findings  but this difference is not significant   Since  the infinitesimal representation is derived from the numerical one  we could not expect it to do better  on average  Note that  even with all ten findings  both schemes  o             r                Number of fi nd i ngs         average about     probability for the true diagnosis  This relatively poor performance arises because of the limited scope of the network  which does not provide the means to differentiate among several classes of  Figure    The average size of the plausible set  as a function of the number of findings in each case       Comparing the performance of infinitesimal and numerical schemes  Next  we compare how the number of findings affects the diagnostic performance for the infinitesimal and numerical schemes  Figure   graphs the performance in terms of the average probability each assigns to the true fault  as a function of the number of findings  For both schemes  as expected  the average probability assigned to the true fault increases with increasing evidence  from about      with   finding  to about       with    findings   fault       The magnitude of priors and the performance of infinitesimal  probabilities The infinitesimal probability scheme appears to perform very well relative to numerical probabilities for the original car network  in which the prior fault probabilities are very small  on average          To  examine if it performs equally well for larger priors  we multiplied the prior odds by five odds factors  as shown in Table      Figure   shows the average  probability assigned to the true diagnosis as a function of the average priors   Interestingly  the two schemes  are almost indistinguishable up to an average fault prior         Above that  the performance of the infinitesimal probability drops off sharply   that is  for average priors of       and         These results   Numerical and Qualitative Probabilistic Reasoning  confirm our ex pect ation that infinitesimal works well for small priors  but not so well for large pr i ors          Cl              c       ordering of diagnosis  A third  would be to evaluate  even more  the quality of decisions will be less rather than more sensitive to these differences in representation        While these findings are encouraging for the practical usefulness of infinitesimal pr oba b il ities  we should   CI c      a   them  Another way would be to compare the rank the quality of decisions based on the diagnosi s  In general  scoring rules based on ranks of diagnosis or                  remember that these initial results are on a single domain  This car model dom ain is simple  with few       loops and short chains   This kind of experiment  should be conducted on a wide range of types of network to see how far these initial results will hold        up  In the introduction  we distinguished view infinitesimal  o              o oo          o oo       Aver age prior fault probability Figure    Comparison of the average performance of infinitesimal and numerical probability schemes as a function of prior fault probabilities   probabilities   as  an   a   approach  of to  nonmonotonic reasoning  from view  b   as an approximation to numerical probabilities   We  reiterate that this paper  we focus on  b   and we are not attempting to evaluate its use as an approach to nonmonotonic logic   Conclusions about the former  have limited relevance to the latter  Infinitesimal pro babi lit ies are quite appealing as an alternative to numerical probab il ities  They should be    CONCLUSIONS  significantly easier to eli ci t from experts  Inference  We find these initial results very encouraging in terms  of the diagnostic performance of the infinitesimal probability scheme  For this example domain  we  found the best performance occurs using E     to      Performance for E      was slightly worse     may be more effjcient  And resulting inferences should be somewhat more robust to changes in probabilities  Some questions that need further investigation include      Performance of the infinitesimal scheme relative to the numerical  scheme  does  not  appear  to  Does the best choice of E vary with the domain   vary  significantly with the quantity of evi dence  The performance using infinitesimal probability is not  Does these results hold for larger networks  with more complex structures   noticeably worse than the numerical probabilities for prior fault probabilities up to about       For larger average fault probabilities  the relative perform ance of  Can this infinitesimal approximation be extended to utilities and decision making   infinitesimal probabilities starts to drop off sharply   This findings suggests that infinitesimal probabilities  Can we obtain a clearer analytic characterization  are more likely to be reliable for diagnosis tasks with  of when performance  very small prior fault probabilities  such as most machine and electronic devices  They may also work for some med ical domains  as long as the priors are less than  disease       we have used is very simple   In  addition   engineering  The mapping from K values back to probabilities that More sophistic ated  mappings are pos sible  making use of higher values   We should also point out that the scoring methods that we have used to evaluate performan ce have been based on posterior probability of the true diagnosis  which is perhaps the most exacting way to compare  will be or won t be  reliable  we  methods  need practical knowledge for  eliciting  infinitesimal  probabilities  We an ticipate that  in the long run  the  best p r actical tools will quantitative methods   combine qualitative and        Henrion  Provan  Del Favero  and Sanders  Intelligence Conference  Acknowledgments  M  Goldszmidt and J  Pearl  causal relations   This work was supported by the National Science Institute for Decision Systems Research  We would like to thank David Beckerman for use of the car  pages         Vermont        entropy approach to nonmonotonic reasoning   refining some of the probabilities   M   Shacter   The Logic of Conditionals   G F  Cooper  The Computational Complexity of Probabilistic Inference Using Belief Networks   Artificial Intelligence                    Darwiche  A symbolic generalization of probability theory  Ph D  dissertation  Computer Science Dept   Stanford University  Palo Alto  CA        M      Goldzmidt   CNETS   A  computational environment for generalized causal networks         this volume    M  Druzdzel and M  Henrion  Efficient reasoning in  Proceedings of the American Association for Artificial Intelligence Conference  pages          Washington D C         J  Druzdzel  Probabilistic Reasoning in Decision Support Systems  From Computation to Common Sense  PhD thesis  Department of Engineering and qualitative probabilistic networks  In  M   Public  Policy   Carnegie  Mellon  University   Pittsburgh  Pa         H  Prade  Possibility Theory  an Approach to Computerized Processing of Uncertainty  Plenum  D  Dubois and  Press  NY         Utility Theories  Measurements and Applications  Kluwer Academic        H  A  Geffner  Default Reasoning  Causal and Conditional  W  Edwards   Theories  M   MIT Press   Henrion   and  Cambridge  MA        M   Druzdzel   Qualitative  propagation and scenario based explanation of probabilistic reasoning   In M  Henrion and R  S h achter   editors   Intelligence       Uncertainty in Artificial  Elsevier  Science  B V    North  Holland         M   Hendon    Search based methods to bound  diagnostic probabilities in very large belief nets    M   In Proceedings of Conf on Uncertainty and Artificial Intelligence        Ginsberg  Readings in Nonmonotonic Reasoning  Morgan Kaufmann  San Mateo  CA         M  Goldszmidt and J  Pearl  System Z   A formalism for reasoning with variable strength defaults  In  Proceedings of American Association for Artificial  editors   Uncertainty in Artificial  Intelhgence  pages          Elsevier Science B V   D  Reidel   Dordrecht  Netherlands         D arwiche  IEEE Transactions on Pattern Analysis and Machine Intelligence                      He nrion  An Introduction to Algorithms for Inference in Belief Networks  In M  Henrion and R   
 Backward simulation is an approximate inference technique for Bayesian belief networks  It differs from existing simulation methods in that it starts simulation from the known evidence and works backward  i e   contrary to the direction of the arcs   The technique s focus on the evidence leads to improved convergence in situations where the posterior beliefs are dominated by the evidence rather than by the prior probabilities  Since this class of situations is large  the technique may make practical the application of approximate inference in Bayesian belief networks to many realworld problems      INTRODUCTION AND MOTIVATION  Because of its sound theoretical foundation in probability theory  the Bayesian belief network technology has become  in artificial intelligence  an important alternative architecture for reasoning to logicbased architectures  e g   rulebased systems   Although efficient exact inference techniques  Shachter        Lauritzen        Shachter        for Bayesian belief networks can and have provided excellent solutions to many realworld problems  their applicability is limited  because exact inference is NPcomplete  Cooper         Because of this  significant research has been focused on finding efficient approximate inference methods  Most previous research has emphasized simulation methods  Pearl        Fung        Shachter        Chavez        Shwe      a   which repeatedly draw sample values from the network s nodes using a sampling algorithm  and then use the relative frequencies of the sample values to estimate the probabilities of interest  Researchers try to find methods that converge quickly to the exact result  and to characterize the convergence properties of the simulation algorithms  There are two basic classes of simulation methods  forward simulation methods  Fung        Henrion        Shachter        and stochasticsimulation methods  Chavez        Pearl         Forwardsirnulation  methods start each trial of the simulation by instantiating the source nodes  i e   nodes with no predecessors  and then proceeding forward along the diagram arcs to instantiate each downstream node in tum  Because the sample values from one trial to the next are unrelated  the trials are independent  In stochasticsimulation methods  on the other hand  each trial begins by modifying the previous trial s instantiation  Each node s sample is chosen with respect to the current instantiations of neighboring nodes  Because they are driven by the prior probabilities of upstream nodes  rather than by the likelihood of the observed evidence  forwardsimulation methods converge slowly when faced with lowlikelihood evidence  evidence that has low prior likelihood   Because of the way samples depend on the current instantiation  stochasticsimulation methods as a group are inefficient when there are deterministic or neardeterministic relationships in a network  In this paper  we present the backwardsimulation method for performing approximate probabilistic inference i n Bayesian belief networks  Our method i s closely related to forwardsimulation methods  and is not susceptible to slow convergence in the presence of deterministic relationships as are stochastic simulation methods  In addition  the method is not as susceptible to slow convergence with lowlikelihood evidence  the main problem with other methods of its class  In Section    we present the notation used in this paper  In Section    we discuss forward simulation methods in detail  Section   provides the details on the backward  simulation method  In Section    we give a summary of the paper  and discuss directions for future research      NOTATION  A Bayesian belief network is a directed acyclic graph D with an associated probability distribution P  The set of nodes in a network is denoted by N  Individual nodes are denoted by capital letters  whereas general references to a node  such as  node i   are in lowercase italics  Each node i in the network has a corresponding variable X  in P and a set of parents Pa  i   If S is a set of nodes  then the        Fung and Del Favero  set X s is the set of variables corresponding to the nodes in  s   The variable  Xi has a corresponding set of conditioning variables X Pa i  called the parents of X   Each variable xi has a state space nj and an associated probability distribution P X   Xpa    The product of node probability  i  dis tributions in a network is the joint probability distribution P of the variables associated with the graph D      BACKWARD SIMULATION  The two defining features of backward simulation are the direction of simulation and the sampling method for drawing node values  The direction of sampling in the backward simulation method is outward from the evidence  In contrast  forward simulation methods that work forward from the source nodes  whereas stochastic simulation methods that have no particular direction of  We represent evidence in Bayesian belief networks by  simulation   setting the values of the appropriate nodes to their  methods  backward simulation permits many possible  observed states  The set of nodes whose values have been  orderings of the nodes to be sampled   observed is denoted by N e The nodes that are unobserved  i e   that are in  N Ne  are called state nodes   Backward simulation is a specialization of the importance sampling inference method discussed in  The inference task for Bayesian belief networks is to compute answers to queries of the  Like forward and stochastic simulation  form P X   XK   where  Section    Like forward simulation  backward simulation includes a node ordering step and a simulation step  The  all the observed evidence Ne is typically a subset of the  first step computes an ordering of network nodes  starting  nodes in K  Many exact and approximate algorithms have been developed for addressing such queries   general  an ordering will contain only a subset of the  Where there is little potential for confusion  the notation  P A  will be used to mean P XA         nodes in the network  In the second step  the simulation trials are performed  with network nodes sampled in the predetermined order         is a well known  technique for improving convergence in Monte Carlo simulation  It has been adapted for use in forward  simulation models  Shachter          It provides the  ability to instantiate the network from an arbitrary  distribution Ps instead of just from the joint distribution P as in logic sampling  To adjust for sampling from P s instead of from P  the weight Z associated with each trial is computed to be the ratio of the likelihood of the sample  based on the network distribution to the likelihood of the sample based on the sampling distribution    I  The network probability P XN  is always the product of node probabilities   A trial weight Z is computed for      ORDERING The  thr ee  requirements  for  node  ordering  in  back ward simulation are flexible and allow significant variation      A node must be instantiated before it is backward sampled      A node s predecessors must be instantiated before the node is forward sampled  and    Each node in the network must be either  a  a node in the ordering or  b  a direct predecessor of a node in the ordering that is backward sampled   Items   and   a  are the usual requirements for an or dering in forward simulation        Because evidence nodes are instantiated  they can always be backward sampled   but the sampling distribution can be arbitrary   In the  simplest form of the algorithm  the sampling distribution is the joint distribution over the unobserved nodes  the state nodes   In that case  where P XN   P  XN    the ratio in Equation   is just likelihood of the evidence   In  each trial  and is used to increment the counts of each distribution  i e   query  of interest   IMPORTANCE SAMPLING  Importance sampling  Rubinstein   from the evidence nodes and working outward   P XN e IXNs   the overall    An informal argument for why this method works is as follows  If a large number of trials is done  the frequency of Xi in the accumulated trials should be approximately Ps X    When multiplied by the weight we get the probability of X   P Xi   A formal proof of the conver gence of this method is presented in  Geweke          Leaf nodes also can always be  backward sampled  because a dummy evidence node with uniform likelihoods can be attached to a leaf node without the posterior distribution being changed  We shall use the simple network in Figure I to illustrate the ordering and sampling steps  Each node represents a  probabilistic variable that is conditionally dependent on the nodes at the ends of the arrows pointing into it  Thus  the network in Figure   represents this probabilistic  factorization   P ABCDE    P A  P BIA  P CIA  P DIBC  P E   Node  D is shaded to indicate that it is an evidence node    Backward Simulation in Bayesian Networks       simulation  the process driving the sampling   Indeed  in forward simulation  they are linked  forward simulation involves only forward sampling  However  the process of backward simulation does involve both backward and forward sampling          Backward Sampling  Backward sampling from a node s probability distribution instantiates those of the node s predecessors that are not already instantiated  We denote the instantiated value of node i by X   the parents of i that are uninstantiated by Pa ll i  and the instantiated parents by Pa  ci   Figure    Simple Network  Four different sampling orders for the example network shown in Figure   are  D B E    D E B    D E C   and   D C E   First  the evidence node D is sampled  instantiating nodes B and C  We then have two choices for instantiating node A  namely by backward sampling from either B or C  Finally  the value of E is determined by forward sampling from C  The nodes in the sampling order will be denoted by Ns  This set is composed of the nodes to backward sampled  N b and the nodes to be forward sampled  Nr In this example  if we take Ns to be   D B  E   then N b is   D B  andNf is  E    The backward sampling procedure instantiates the uninstantiated parent nodes according to the following probability distribution   ps  Backward sampling differs from forward sampling in each of these three aspects  First  sampling from a node s probability distribution occurs only after the node itself has been instantiated  Second  sampling from a node s probability distribution does not determine the node s value but rather the values of the node s predecessors  Third  the sampling is based not on a particular conditional probability distribution but rather on normalized likelihoods of the node s conditional probability distribution  Which of the two methods is used to sample a node depends on the network topology  For nodes with an evidence node as a descendent  sampling from a node s conditional probability occurs after the node has been instantiated  and it determines the values for the node s predecessors  For nodes with no downstream evidence nodes  forward sampling is used  Any of the algorithms developed for forward simulation can be used  We would like to emphasize the distinction between sampling  a selection from a set of choices  and  I         P x   xp    I XP     a     r  a            Norm  i      z E  N b       The numerator of the preceding expression is the likelihood of the current state of node i given a particular state of the parents of node i  The denominator  Norm i   is a normalization constant that ensures that the terms in this distribution sum to    Norm i  is computed as follows      u i l X  I y  XPa  Norrn i    LyeXP Pa      SAMPLING Two sampling methods are used in backward simulation  forward sampling and backward sampling  In forward sampling  a node s distribution is sampled only after all the node s predecessors have been sampled  Forward sampling a node sets the value for the node itself  The probability distribution on which the random sampling is based is determined by the values of the node s predecessors    Pa u       J    i  The set XP Pau i   is the set of all possible conditioning cases of the uninstantiated parents of node i  For nodes with n binary valued uninstantiated parents  this set is of size  n  Normalization constants can be precomputed if there is a fixed order  or they can be cached as computation proceeds          Forward Sampling  In forward sampling  the sampling distribution for a node is the same as the node s probability distribution           SCORING After all of the nodes have been instantiated  the weight for each trial can therefore be computed by combining Equations   through     Z x    e           P  xj I XPa j    rrjENb Norm J  n   NP xi lxPa i      rrjENf p   x  XPa j     I        Fung and Del Favero  Tills can be simplified to     Z x   Die N   N P x  I xPa i  s  Table   n  j eN b Norm        Backward Sampling Distribution for A           EXAMPLE Let us step through an example of backward simulation   Consider the five node network in Figure l  All of the nodes are binary valued  the values for A  for instance   are taken to be a  and a   whereas the possible joint values  or states  of nodes B and C are htc   b  c   b  c   and b  c  The value of the evidence node D is observed to  be d    Suppose that the sampling order   D B E  is used   Table   is the conditional probability table for node D  For instance  P D  d  I B b   C c     p        As before  the constant  Suppose that the sampling step selects state a   Finally  we would use forward sampling to set node E to  one of the states       e   according to the distribution given in Table    which is identical to the second column of Table     Table I  Probability distribution for D givenB and C  Table    Probability ofE given C  P  D IBC   Pm  Pm  P      P       P     P     P      P     Since D is observed to be  j  normalizes the terms   P E I C    d z  the sampling distribution for  the parents of D is based on the second row of Table     Table  The sampling distribution  over the states in XP Pa    OJ    b  c   b  c   b c   b c    is shown in Table     cz     Sampling Distribution forE  Table    Sampling Distribution forB and C In forward sampling  the sampling distribution is the same P tt   Pt    P       P      a      a  a  The constant a normalizes the terms to sum to    namely  a   Nonn D dz    P        P       P        P        as the probability distribution  so no normalization constant is necessary  the terms already sum to  Suppose that the sampling step setsE to e           This example trial has instantiated the network to the joint state a b  c d  e    The trial score that would be added to the beliefs of the currently selected states of each node  would be  Suppose that the sampling step chooses joint state b  c  and sets the states of B and C to these values  Next  we would sample nodeB to set node A to one of the  states   a   a   according to the distribution over these states  Table   shows the conditional probability distribution of B given A  whereas Table   shows the sampling distribution for A   Table    Probability ofB given A P B I A       CORRECTNESS AND CONVERGENCE Backward simulation meets the single constraint for a valid importance sampling procedure  no point in the  joint state space of the prior distribution with positive probability can have a probability of zero in the sampling distribution   As a form of importance sampling with likelihood  weighting  backward simulation inherits the convergence properties of importance sampling  Shachter         That  is  the beliefs generated by the simulation are guaranteed  to converge to their true values  with the errors decreasing in proportion to the square root of the number of trials         Backward Simulation in Bayesian Networks     DISCUSSION  S to s   and would set the trial score Z to e  a relatively small number  The belief corresponding to      BENEFITS AND COSTS          remain at zero  After many trials  on average  about  The backward simulation method is well suited to inference in situations with low likelihood evidence  By working from the evidence  the method focuses on instantiating those scenarios that are most compatible with the observed network state  rather than with the prior distribution  The effect   will be  augmented by this score  whereas the belief of s  will  of the prior distribution is taken  into account by the trial weights  If the prior probabilities are diffuse  compared to the evidence likelihoods  the  nodeS will be set to s  and the trial score Z would be   e   Now  because of this one trial  the belief of s   will  discovered  after much work  that s   is the most likely  explanation for the evidence observed  Table      ForwardSampling Distribution forS P S   weights also will be diffuse  and the inference method will converge to the correct solution much faster than would  for low likelihood evidence is illustrated by the two node network in Figure    NodeT has been observed at value t   and nodeS has two states  s  and           forward simulation methods  This benefit of backward sampling over forward sampling  be  much greater than the belief of s    we will have  Table    shows the backward sampling distribution for S   If we use backward sampling fromT to S  S will be set to state s  in the preponderance of trials  The belief of s   will be augmented in each trial by  Z o  and the belief of  s  by zero  Thus  from the start  the simulation is more in line with the most probable diagnosis  s    Table      BackwardSampling Distribution forS        The main cost of backward sampling is the computational resources required for computing the normalization Figure     Two node Network With Low likelihood Evidence   Suppose that the prior and conditional probabilities are as follows  with              I     P S    o s         o  P T   t  IS  st    t  e              P T  t   S s   e Using exact inference  Bayes s rule   we can show that  although the prior for state  Although in  general  the  costs  grow  exponentially with the number of predecessors  the costs can be reduced where there are special network structures such as invertible continuous functions or noisy or relationships  P S   st     constants    is much less than that for  state s   s  is the most likely explanation for the evidence   Backward simulation is related to the method of evidential integration  Chin          Fung          that has been  suggested for use with simulation methods  In evidential integration  arc reversals are used as a pre processing step to integrate the evidence into the network  to convert extremal  likelihoods  to less  extreme  likelihoods   Evidence integration is computationally expensive  backward  simulation  does part of what  evidence  integration does  when it computes the normalization constants   at a fraction of the cost  For networks in which the conditional probabilities do not change  the normalization constants can be precomputed and cached  taking much of the work out of each trial      e     o e   e             o e    e     P s  t     u e Table    shows the forward sampling distribution forS  If  we use forward sampling from S toT  most trials will set      EXPERIMENTS We have run some preliminary experiments comparing the performance of forward simulation with backward            The probabilities are given  simulation  They were based on the network in Figure as described in  Fung  inTable state e        D is observed in state  q and E is observed in        Fung and Del Favero  The test routines were written in Macintosh Common Lisp  We tested a forward simulation method against the Both use likelihood  backward simulation method  weighted scoring   evidence is not particularly unlikely  thus there is no particular benefit for backward sampling  Most importantly  Figures   and   show that backward simulation works as an inference method   Table    Probabilities for the Experimental Network  P A   P a           P BIA   P b  a           P b            P CIA   P c  a           P c        P DIBC   P d   b  c           P d   b c           P d lib tC              P d  b c                  P d   b c           P d  b  c    P EIC     I    LIJ                            B Cl           As was done in the previously cited paper  we perform a large number       of runs  In each run  we measure the             and      trials  using the absolute value error        Trials  accuracy of each simulation method at                function below               Figure    Standard Deviation vs  Trials   Forward  Diamonds  and Backward  Squares  Although both simulation methods will converge to the  same answer  they may do so at different speeds  This is the motivation of the next set of runs  which were  Here  Bk xiit  is the belief  probability estimate  for state j of node i on trial t of the kth run  The error  averaged over all runs  is presented in Figure     The  standard deviation of the errors in the runs is presented in  Figure     performed on  the  same  network  with  the  same  distributions  with the exception of the modified entries  listed in Table     The evidence in these runs is that node  D is set to d   These modifications make the observed evidence much less likely than before  Table     Modifications to Probabilities in Table   for Extreme probability Experiment       g                  P DIBC   P d   b c   P d  b c                   P d  b c               P d  b c                        We record the performance of each method at                  and     trials  The average error and standard deviation are presented in Figures   and              Figures   and                              The conclusion is that we can expect good performance of  Trials  backward simulation at a low numbers of trials in  Figure    Error vs  Trials for Two Simulation Methods  Forward  Diamonds  and Backward  Squares  Figures   and    show that backward and forward  simulation are performing equivalently well  with the backward s imulation method showing slightly lower  average error values   This is to be expected   given a  large enough number of trials  both methods will  converge to the same answer     show that at low trial numbers  backward  simulation is consistently closer to the true probability than forward simulation   In this example  the  networks with low l ikelihood evidence  Of course  much more work is necessary to characterize and understand the conditions under which each simulation method performs  better than the other  There are cases in which backward simulation would perform worse than forward simulation   backward simulation is subject to the proof of Dagum        that  in the worst case  all approximate  probabilistic methods are NP hard    Backward Simulation in Bayesian Networks  In backward simulation  on the other hand  the sampling order can affect the sampling distribution  This may have a significance in convergence properties of the simulation  For instance  in the example of Section      two possible sampling orders are   D E B and  D E C   Using the former  A is backward sampled from B  whereas using the latter  A is backward sampled from C  Depending on the structure of the joint distribution P ABC   the sampling distribution for A could be quite different between the two orders  One ordering may be better than the other in terms of simulation performance            j t                                            Trials Figure    Error vs  Trials for Second Experimental Run  Forward  Diamonds  and Backward  Squares              t   UJ            The added flexibility is exciting  in that it provides an opportunity to develop heuristics governing when to sample a node backward or forward  The hope is that with backward and forward simulation in the probabilistic tool chest  as well as evidence integration and other approximate methods  a simulation run can be optimized based on the particulars of the network being considered          There are many interesting avenues of research for backward simulation  Dynamic node ordering  i e   changing Ns within a single trial or between trials  is possible and may provide a way to improve performance  Also  it is possible to group nodes for sampling  For example  in Figure    B and C could be aggregated together for the purpose of sampling the value of A        E  The ordering  A B C E  presents a different type of flexibility  or ambiguity  it is valid under this order to instantiate C by forward sampling from A or by backward sampling from D  There may be a difference in simulation performance under the different interpretations of this ordering                                             Trials Figure    Standard Deviation vs  Trials for Second Exper imental Run  Forward  Diamonds  and Backward  Squares      THE SIGNIFICANCE OF NODE ORDERING Backward simulation provides a new flexibility in devising strategies for instantiating the network during a simulation triaL One strategy would be to use backward sampling wherever possible  Another would use forward sampling everywhere  this is just forward simulation   In between these extremes  there are many possibilities for mixing both forward and backward sampling  There are often multiple possible node ordering in forward simulation as welL However  node ordering has no impact on the sampling distribution for a particular node  the distribution is always based on the node s predecessors  which are always instantiated before the node itself is   FUTURE RESEARCH AND APPLICATIONS  One of the most promising areas of research is the combination of the backward simulation method with its dual  forward simulation  The combination would provide a complete probabilistic architecture that allows both data driven and causal reasoning  Such an architecture might have the promise of attacking such problems as natural language understanding or speech recognition  We are currently testing this architecture on two level networks with noisy or relationships between the nodes such as the QMR DT network  Shwe     Ib   This combination may make feasible the application of Bayesian belief networks to many real world settings that current techniques cannot handle  For example  it should be applicable to situations that have large and dynamic state spaces  and strong evidence  Many sensory situations  e g   vision  seem to have this flavor  It seems to match well with how people reason under uncertainty   by reasoning from evidence to conclusions  This has the promise of making explanation of the results of backward simulation more intuitive than for exact inference methods         Fung and Del Favero  Acknowledgments  This work was supported in part by NSF Grant IRI         This work has benefited greatly from discussions with Mark Peot and Kuo Chu Chang and from the comments of the reviewers of this paper  
