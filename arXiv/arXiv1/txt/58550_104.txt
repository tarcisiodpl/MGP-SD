Exact Inference Networks Discrete Children Continuous Parents real life domains mixture discrete continuous variables modeled hybrid Bayesian Networks Animportant subclass hybrid BNs conditional linear Gaussian CLG networks conditional distribution continuous variables assignment discrete variables multivariate Gaussian Lauritzen 's extension clique tree algorithm exact inference CLG networks domains include discrete variables depend continuous CLG networks allow dependencies berepresented exact inference algorithm proposed enhanced CLG networks paper generalize Lauritzen 's algorithm providing exact inference algorithm augmented CLG networks networks continuous nodes conditional linear Gaussians allow discrete children ofcontinuous parents algorithm exact sense computes exact distributions discrete nodes exact moments continuous accuracy numerical integration thealgorithm discrete children modeled softmax CPDs real world domains approximation continuous distributions using moments accurate algorithm simple implement comparable complexity Lauritzen 's algorithm empirically achieves accuracy previous approximate algorithms Expectation Propagation Continuous Time Bayesian Networks Continuous time Bayesian networks CTBNs describe structured stochastic processes finitely evolve continuous time CTBN directed cyclic dependency graph set variables represents finite continuous time Markov process transition model function parents exact inference CTBNs intractable address approximate inference allowing queries conditioned evidence continuous time intervals discrete time CTBNs parameterized exponential family insight develop message passing scheme cluster graphs allows apply expectation propagation CTBNs clusters cluster graph distributions cluster variables individual time distributions trajectories variables duration discrete time temporal models dynamic Bayesian networks adapt time granularity reason variables conditions Object-Oriented Bayesian Networks Bayesian networks provide modeling language associated inference algorithm stochastic domains applied variety medium-scale applications faced complex domain task modeling using Bayesian networks resemble task programming using logical circuits paper describe object-oriented Bayesian network OOBN language allows complex domains described terms inter-related objects Bayesian network fragment describe probabilistic relations attributes object attributes objects providing natural framework encoding part-of hierarchies Classes provide reusable probabilistic model applied multiple objects Classes support inheritance model fragments class subclass allowing common aspects classes defined language declarative semantics OOBN interpreted stochastic functional program uniquely specifies probabilistic model provide inference algorithm OOBNs structural encoded OOBN encapsulation variables object reuse model fragments contexts speed inference process Convex Estimation using Undirected Bayesian Transfer Hierarchies learning tasks naturally arranged hierarchy appealing approach coping scarcity instances transfer learning using hierarchical Bayes framework Bayesian computations difficult computationally demanding desirable posterior estimates facilitate efficient prediction hierarchical Bayes framework lend naturally maximum aposteriori goal propose undirected reformulation hierarchical Bayes relies priors form similarity measures introduce notion degree transfer weights components similarity measures automatically learned joint probabilistic framework Importantly reformulation convex objective learning facilitating optimal posterior estimation using standard optimization techniques addition require proper priors allowing flexible straightforward specification joint distributions transfer hierarchies framework effective learning models transfer hierarchies real-life tasks object shape modeling using Gaussian density estimation document classification Policy Iteration Factored MDPs MDPs represented compactly using dynamic Bayesian network structure value function retain structure process value functions factored MDPs approximated using decomposed value function linear combination <I> restricted </I> basis functions refers subset variables approximate value function policy computed using approximate dynamic programming approach produce approximation relative distance metric weighted stationary distribution current policy type weighted projection ill-suited policy improvement approach value determination simple closed-form computation directly compute least-squares decomposed approximation value function <I> weights </I> value determination algorithm subroutine policy iteration process reasonable restrictions policies induced factored value function compactly represented manipulated efficiently policy iteration process method computing error bounds decomposed value functions using variable-elimination algorithm function optimization complexity algorithms depends factorization system dynamics approximate value function Generating Beliefs previous BGHK92 BGHK93 studied random-worlds approach powerful method generating degrees belief i.e. subjective probabilities knowledge base consisting objective first-order statistical default allowing knowledge base objective limiting occasionally wish include degrees belief knowledge base contexts beliefs represent influence beliefs paper describe techniques extending method generates degrees belief objective degrees belief techniques bloused well-known approaches cross-entropy discuss connections techniques conceptually technically techniques answer applied random-worlds method Nonuniform Dynamic Discretization Hybrid Networks consider probabilistic inference hybrid networks include continuous discrete variables arbitrary topology reexamine question variable discretization hybrid network aiming minimizing loss induced discretization nonuniform partition variables opposed uniform partition variable separately reduces size data structures represent continuous function provide simple efficient procedure nonuniform partition represent nonuniform discretization computer memory introduce data structure call Binary Split Partition BSP tree BSP trees exponential factor data structures standard uniform discretization multiple dimensions BSP trees standard join tree algorithm accuracy inference process improved adjusting discretization evidence construct iterative anytime algorithm gradually improves quality discretization accuracy answer query provide empirical evidence algorithm converges Utilities Random Variables Density Estimation Structure Discovery Decision theory traditionally include uncertainty utility functions argue person 's utility value outcome treated treat domain attributes random variable density function values apply statistical density estimation techniques learn density function database partially elicited utility functions define Bayesian learning framework assuming distribution utilities mixture Gaussians mixture components represent statistically coherent subpopulations extend techniques discovering generalized additivity structure utility functions population define Bayesian model selection criterion utility function structure search procedure structures factorization utilities learned model generalization density estimation allows provide robust estimates utilities using utility elicitation questions experiment technique synthetic utility data real database utility functions domain prenatal diagnosis Bayesian Network Structure domains analyzing structure underlying distribution e.g. variable direct parent Bayesian model-selection attempts MAP model structure answer questions amount data modest models non-negligible posterior compute Bayesian posterior feature i.e. total posterior probability models paper propose approach task efficiently compute sum exponential networks consistent fixed network variables allows compute marginal probability data posterior feature result basis algorithm approximates Bayesian posterior feature approach Markov Chain Monte Carlo MCMC method orderings network structures space orderings regular space structures smoother posterior landscape empirical synthetic real-life datasets compare approach model averaging MCMC network structures non-Bayesian bootstrap approach Continuous Time Markov Networks central task applications reasoning processes change continuous time mathematical framework Continuous Time Markov Processes basic foundations modeling systems Nodelman al introduced continuous time Bayesian networks CTBNs allow compact representation continuous-time processes factored space paper introduce continuous time Markov networks CTMNs alternative representation language represents type continuous-time dynamics real life processes biological chemical systems dynamics process naturally described interplay forces tendency entity change fitness energy function entire system model force described continuous-time proposal process suggests local changes system rates force represented Markov network encodes fitness desirability proposed local change accepted probability function change fitness distribution fitness distribution stationary distribution Markov process representation characterization temporal process stationary distribution compact graphical representation allows naturally capture type structure complex dynamical processes evolving biological sequences describe semantics representation basic properties compares CTBNs provide algorithms learning models data discuss applicability biological sequence evolution Modeling Latent Variable Uncertainty Loss-based Learning consider parameter estimation using weakly supervised datasets training sample consists input partially annotation refer output missing annotation modeled using latent variables Previous methods overburden single distribution separate tasks modeling uncertainty latent variables training ii accurate predictions output latent variables testing propose novel framework separates demands tasks using distributions conditional distribution model uncertainty latent variables input-output pair ii delta distribution predict output latent variables input learning encourage agreement distributions minimizing loss-based dissimilarity coefficient approach generalizes latent SVM models uncertainty latent variables relying pointwise estimate ii allows loss functions depend latent variables greatly increases applicability demonstrate efficacy approach challenging object detection action detection using publicly datasets Probabilistic Models Agents Beliefs Decisions applications intelligent systems require reasoning mental agents domain reason agent 's beliefs including beliefs agents reason agent 's preferences beliefs preferences relate behavior define probabilistic epistemic logic PEL belief statements formal semantics provide algorithm asserting querying PEL formulas Bayesian networks reason agent 's behavior modeling decision process influence diagram assuming behaves rationally PEL reasoning agent 's observed actions conclusions aspects domain including unobserved domain variables agent 's mental Constrained Approximate Maximum Entropy Learning Markov Random Fields Parameter estimation Markov random fields MRFs difficult task inference network inner loop gradient descent procedure Replacing exact inference approximate methods loopy belief propagation LBP suffer poor convergence paper provide approach combining MRF learning Bethe approximation consider dual maximum likelihood Markov network learning maximizing entropy moment matching constraints approximate objective constraints optimization previous lines Teh Welling formulation allows parameter sharing features log-linear model parameter regularization conditional training piecewise training Sutton McCallum restricted special formulation study optimization strategies based single convex approximation repeated convex approximations real-world networks demonstrate algorithms outperform learning loopy piecewise provide framework analyzing trade-offs relaxations entropy objective constraints Ordering-Based Search Simple Effective Algorithm Learning Bayesian Networks basic tasks Bayesian networks BNs learning network structure data BN-learning NP-hard standard solution heuristic search approaches proposed task outperform baseline greedy hill-climbing tabu lists proposed algorithms complex hard implement paper propose simple easy-to-implement method addressing task approach based well-known network bounded in-degree consistent node efficiently propose search space structures space orderings selecting network consistent search space global search steps lower branching factor avoids costly acyclicity checks algorithm synthetic real data sets evaluating score network running time ordering-based search outperforms standard baseline competitive algorithms harder implement MAP Estimation Semi-Metric MRFs Hierarchical Graph Cuts consider task obtaining maximum posteriori estimate discrete pairwise random fields arbitrary unary potentials semimetric pairwise potentials propose accurate hierarchical move strategy move computed efficiently solving st-MINCUT previous move approaches e.g. widely a-expansion algorithm method obtains guarantees standard linear programming LP relaxation special metric labeling existing LP relaxation solvers e.g. interior-point algorithms tree-reweighted message passing method faster efficient st-MINCUT algorithm design Using synthetic real data experiments technique outperforms commonly algorithms Residual Belief Propagation Informed Scheduling Asynchronous Message Passing Inference probabilistic graphical models practical challenge domains commonly effective belief propagation BP algorithm generalizations converge applied hard real-life inference tasks widely recognized scheduling messages algorithms consequences issue remains unexplored address question schedule messages asynchronous propagation fixed reached faster reasonable asynchronous BP converges unique fixed conditions guarantee convergence synchronous BP addition convergence rate simple round-robin schedule synchronous propagation propose residual belief propagation RBP novel easy-to-implement asynchronous propagation algorithm schedules messages informed pushes bound distance fixed Finally demonstrate superiority RBP state-of-the-art methods variety challenging synthetic real-life RBP converges methods reduces running time convergence methods converge Update Rules Parameter Estimation Bayesian Networks paper re-examines parameter estimation Bayesian networks missing values hidden variables perspective on-line learning Kivinen Warmuth provide unified framework parameter estimation encompasses on-line learning model continuously adapted data arrive traditional batch learning pre-accumulated set samples one-time model selection process batch framework encompasses gradient projection algorithm EM algorithm Bayesian networks framework leads on-line batch parameter update schemes including parameterized version EM provide empirical theoretical indicating parameterized EM allows faster convergence maximum likelihood parameters standard EM Reasoning Time Granularity real-world dynamic systems composed components evolve rates traditional temporal graphical models dynamic Bayesian networks time modeled fixed granularity selected based rate fastest component evolves Inference performed fastest granularity computational cost Continuous Time Bayesian Networks CTBNs avoid time-slicing representation modeling system evolving continuously time expectation-propagation EP inference algorithm Nodelman al. vary inference granularity time granularity uniform system selected advance paper provide EP algorithm utilizes cluster graph architecture clusters distributions overlap space set variables time architecture allows system modeled time granularities current rate evolution provide information-theoretic criterion dynamically re-partitioning clusters inference tune level approximation current rate evolution avoids hand-select appropriate granularity allows granularity adapt transmitted network experiments demonstrating approach result computational savings SPOOK System Probabilistic Object-Oriented Knowledge Representation previous limitations standard Bayesian networks modeling framework complex domains proposed richly structured modeling language em Object-oriented Bayesian Netorks argued deal domains OOBNs expressive model aspects complex domains existence specific named objects arbitrary relations objects uncertainty domain structure aspects crucial real-world domains battlefield awareness paper SPOOK implemented system addresses limitations SPOOK implements expressive language allows represent battlespace domain naturally compactly inference algorithm utilizes model structure fundamental empirically achieves magnitude speedup existing approaches Probability Estimation Irrelevant paper consider aspect applying decision theory design agents learn decisions uncertainty aspect concerns agent estimate probabilities world limited observations committing decision naive application statistical tools improved agent determine observations truly relevant estimation hand framework determinations define estimation procedure framework suggests extensions additional knowledge improve tile estimation procedure Projected Subgradient Methods Learning Sparse Gaussians Gaussian Markov random fields GMRFs useful broad range applications paper tackle learning sparse GMRF high-dimensional space approach l1-norm regularization inverse covariance matrix utilize novel projected gradient method faster previous methods practice equal performing asymptotic complexity extend l1-regularized objective sparsifying entire blocks inverse covariance matrix methods generalize fairly easily methods demonstrate extensions generalization performance real domains biological network analysis 2D-shape modeling image task Expectation Maximization Complex Duration Distributions Continuous Time Bayesian Networks Continuous time Bayesian networks CTBNs describe structured stochastic processes finitely evolve continuous time CTBN directed cyclic dependency graph set variables represents finite continuous time Markov process transition model function parents address learning parameters structure CTBN partially observed data apply expectation maximization EM structural expectation maximization SEM CTBNs availability EM algorithm allows extend representation CTBNs allow richer class transition durations distributions phase distributions class highly expressive semi-parametric representation approximate duration distribution arbitrarily closely extension CTBN framework addresses main limitations CTBNs DBNs restriction exponentially geometrically distributed duration experimental real data set people 's life spans algorithm learns reasonable models structure parameters partially observed data phase distributions achieves performance DBNs