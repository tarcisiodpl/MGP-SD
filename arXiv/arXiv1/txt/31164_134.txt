Suboptimality Bounds Stochastic Shortest Path consider Bellman residual dynamic programming operator compute suboptimality bounds solutions stochastic shortest path bounds established special policies proper dynamic programming operator contraction easily computable limited special discounting condition transition costs positive suboptimality bounds easily computed policies proper restrictions transition costs analysis complex preliminary bounds Policy Iteration Decentralized Control Markov Decision Processes Coordination distributed agents required arising including multi-robot systems networking e-commerce formal framework decentralized partially observable Markov decision process DEC-POMDP optimal dynamic programming algorithms single-agent version optimal algorithms multiagent elusive main contribution paper optimal policy iteration algorithm solving DEC-POMDPs algorithm stochastic finite-state controllers represent policies solution include correlation device allows agents correlate actions communicating approach alternates expanding controller performing value-preserving transformations modify controller sacrificing value efficient value-preserving transformations reduce size controller improve value keeping size fixed Empirical demonstrate usefulness value-preserving transformations increasing value keeping controller size minimum broaden applicability approach heuristic version policy iteration algorithm sacrifices convergence optimality algorithm reduces size controllers step assuming probability distributions agents actions assumption hold helps produce quality solutions test Heuristic Search Approach Planning Continuous Resources Stochastic Domains consider optimal planning stochastic domains resource constraints resources continuous choice action step depends resource availability introduce HAO algorithm generalization AO algorithm performs search hybrid space modeled using discrete continuous variables continuous variables represent monotonic resources heuristic search algorithms HAO leverages knowledge start admissible heuristic focus computational effort space reached start optimal policy approach effective resource constraints limit space reachable Experimental demonstrate effectiveness domain motivates automated planning planetary exploration rovers Improving Scalability Optimal Bayesian Network Learning External-Memory Frontier Breadth-First Branch Bound Search Previous learning optimal structure Bayesian network formulated shortest path finding graph solved using search paper improve scalability approach developing memory-efficient heuristic search algorithm learning structure Bayesian network using propose frontier breadth-first branch bound search leverages layered structure search graph layers graph solution reconstruction stored memory time improve scalability algorithm stores graph external memory hard disk fit RAM Experimental algorithm solves larger current art Solving Multistage Influence Diagrams using Branch-and-Bound Search branch-and-bound approach solving influ ence diagrams proposed literature appears implemented evaluated difficulties computing effective bounds branch-and-bound search paper describe efficiently compute effective bounds develop practical implementa tion depth-first branch-and-bound search influence diagram evaluation outperforms existing methods solving influence diagrams multiple stages Sparse Stochastic Finite-State Controllers POMDPs Bounded policy iteration approach solving infinite-horizon POMDPs represents policies stochastic finite-state controllers iteratively improves controller adjusting parameters node using linear programming original algorithm size linear programs complexity policy improvement depends parameters node grows size controller practice parameters node non-zero values grow size controller Based observation develop version bounded policy iteration leverages sparse structure stochastic finite-state controller iteration improves policy amount original algorithm scalability