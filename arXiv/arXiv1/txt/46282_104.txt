Value-Directed Belief Approximation POMDPs consider belief-state monitoring purposes implementing policy partially-observable Markov decision process POMDP approximate belief schemes belief-state approximation e.g. based minimixing measures KL-diveregence true estimated appropriate POMDPs propose framework analyzing value-directed approximation schemes approximation quality determined expected error utility error belief propose heuristic methods finding projection schemes belief estimation exhibiting anytime characteristics POMDP value fucntion describe algorithms constructing bounds error decision quality expected utility associated acting belief approximation Hierarchical POMDP Controller Optimization Likelihood Maximization Planning simpli decomposing task tasks arranged hierarchically Charlin al. hierarchy discovery framed non-convex optimization inherent computational di culty solving optimization hard scale realworld Toussaint al. developed method solve planning maximumlikelihood estimation paper hierarchy discovery partially observable domains tackled using maximum likelihood approach technique rst transforms dynamic Bayesian network hierarchical structure naturally discovered optimizing policy Experimental demonstrate approach scales previous techniques based non-convex optimization Comparative Analysis Probabilistic Models Activity Recognition Instrumented Walker Rollating walkers popular mobility aids adults improve balance control automatically recognize activities performed walker users understand activity patterns mobility issues context falls happen design compare techniques recognize walker activities comprehensive evaluation control subjects walker users retirement community Value-Directed Sampling Methods POMDPs consider approximate belief-state monitoring using particle filtering purposes implementing policy partially-observable Markov decision process POMDP particle filtering widely-used tool AI monitoring dynamical systems scant attention paid context decision Assuming existence value function derive error bounds decision quality associated filtering using sampling describe adaptive procedure dynamically determine samples required meet specific error bounds Empirical evidence offered supporting technique profitable directing sampling effort distinguish policies Vector-space Analysis Belief-state Approximation POMDPs propose approach value-directed belief approximation POMDPs value-directed model allows choose approximation methods belief monitoring impact decision quality Using vector space analysis devise search procedures selecting approximation scheme computational properties existing methods provide looser error bounds empirically impact decision quality practice magnitude