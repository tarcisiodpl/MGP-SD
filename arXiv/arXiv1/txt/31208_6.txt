Reversible Jump MCMC Simulated Annealing Neural Networks propose novel reversible jump Markov chain Monte Carlo MCMC simulated annealing algorithm optimize radial basis function RBF networks algorithm enables maximize joint posterior distribution network parameters basis functions performs global search joint space parameters parameters thereby surmounting local minima calibrating Bayesian model classical AIC BIC MDL model selection criteria penalized likelihood framework Finally theoretically empirically algorithm converges modes posterior distribution efficient Bayesian Multi-Scale Optimistic Optimization Bayesian optimization powerful global optimization technique expensive black-box functions shortcomings requires auxiliary optimization acquisition function iteration auxiliary optimization costly hard carry practice creates serious theoretical concerns convergence assume exact optimum acquisition function paper introduce technique efficient global optimization combines Gaussian process confidence bounds treed simultaneous optimistic optimization eliminate auxiliary optimization acquisition functions experiments global optimization benchmarks novel application automatic extraction demonstrate technique efficient approaches draws inspiration theoretical analyses Bayesian optimization Gaussian processes finite-time convergence rate proofs require exact optimization acquisition function approach eliminates unsatisfactory assumption difficult NP-hard solved vanishing regret rates Predicting Parameters Deep Learning demonstrate redundancy parameterization deep learning models weight values feature accurately predict remaining values parameter values predicted learned train architectures learning weights predicting rest predict weights network drop accuracy Exploiting correlation budget constraints Bayesian multi-armed bandit optimization address finding maximizer nonlinear smooth function evaluated point-wise subject constraints permitted function evaluations fixed-budget arm identification multi-armed bandit literature introduce Bayesian approach empirically outperforms existing frequentist counterpart Bayesian optimization methods Bayesian approach emphasis detailed modelling including modelling correlations arms result perform situations arms larger allowed function evaluation frequentist counterpart inapplicable feature enables develop deploy practical applications automatic machine learning toolboxes paper comprehensive comparisons proposed approach Thompson sampling classical Bayesian optimization techniques Bayesian bandit approaches state-of-the-art arm identification methods comparison methods literature allows examine relative merits features Theoretical Analysis Bayesian Optimisation Unknown Gaussian Process Hyper-Parameters Bayesian optimisation gained popularity tool optimising parameters machine learning algorithms models ironically setting hyper-parameters Bayesian optimisation methods notoriously hard reasonable practical solutions advanced fail optima Surprisingly theoretical analysis crucial literature address derive cumulative regret bound Bayesian optimisation Gaussian processes unknown kernel hyper-parameters stochastic setting bound applies expected improvement acquisition function sub-Gaussian observation noise guidelines design hyper-parameter estimation methods simple simulation demonstrates guidelines Learning individuals statistics propose formulation informative binary multiple-instance learning setting instances described feature vectors estimates fraction positively-labeled instances task learn instance level classifier trying estimate unknown binary labels individuals knowledge statistics propose principled probabilistic model solve accounts uncertainty parameters unknown individual labels model trained efficient MCMC algorithm performance demonstrated synthetic real-world data arising object recognition Regret Bounds Deterministic Gaussian Process Bandits paper analyses Gaussian process GP bandits deterministic observations analysis branch bound algorithm UCB algorithm Srinivas al. GPs Gaussian observation noise variance strictly Srinivas al. proved regret vanishes approximate rate frac sqrt observations complement result attack deterministic attain faster exponential convergence rate regularity assumptions regret decreases asymptotically frac tau ln d/4 probability dimension search space tau constant depends behaviour objective function global maximum Distributed Parameter Estimation Probabilistic Graphical Models paper foundational theoretical distributed parameter estimation undirected probabilistic graphical models introduces condition composite likelihood decompositions models guarantees global consistency distributed estimators provided local estimators consistent Deep Architecture Semantic Parsing successful approaches semantic parsing build top syntactic analysis text distributional representations statistical models match parses ontology-specific queries paper novel deep learning architecture semantic parsing system union neural models language semantics allows generation ontology-specific queries natural language statements questions parsing suitable grammatically malformed syntactically atypical text tweets permitting development semantic parsers resource-poor languages Rao-Blackwellised Particle Filtering Dynamic Bayesian Networks Particle filters PFs powerful sampling-based inference/learning algorithms dynamic Bayesian networks DBNs allow treat principled type probability distribution nonlinearity non-stationarity appeared fields names condensation sequential Monte Carlo survival fittest paper exploit structure DBN increase efficiency particle filtering using technique Rao-Blackwellisation Essentially samples variables marginalizes rest exactly using Kalman filter HMM filter junction tree algorithm finite dimensional optimal filter Rao-Blackwellised particle filters RBPFs lead accurate estimates standard PFs demonstrate RBPFs non-stationary online regression radial basis function networks robot localization map building discuss potential application provide references finite dimensional optimal filters Adaptive Hamiltonian Riemann Manifold Monte Carlo Samplers paper address widely-experienced difficulty tuning Hamiltonian-based Monte Carlo samplers develop algorithm allows adaptation Hamiltonian Riemann manifold Hamiltonian Monte Carlo samplers using Bayesian optimization allows infinite adaptation parameters samplers sampling algorithms ergodic adaptive algorithms easy efficient samplers precluding complex solutions Hamiltonian-based Monte Carlo samplers widely excellent choice MCMC method aim paper remove key obstacle widespread samplers practice Variational MCMC propose class learning algorithms combines variational approximation Markov chain Monte Carlo MCMC simulation Naive algorithms variational approximation proposal distribution perform approximation tends underestimate true variance features data solve introducing sophisticated MCMC algorithms algorithms mixture MCMC kernels random walk Metropolis kernel blockMetropolis-Hastings MH kernel variational approximation proposaldistribution MH kernel allows locate regions probability efficiently Metropolis kernel allows explore vicinity regions algorithm outperforms variationalapproximations yields estimates considerably estimates moments covariances outperforms standard MCMC algorithms locates theregions probability speeding convergence demonstrate algorithm Bayesian parameter estimation logistic sigmoid belief networks Large-Flip Sampling propose Monte Carlo algorithm complex discrete distributions algorithm motivated N-Fold ingenious event-driven MCMC sampler avoids rejection moves specific N-Fold trapped cycles surmount modifying sampling process correction introduce bias bias subsequently corrected carefully engineered sampler Herded Gibbs Sampling Gibbs sampler popular algorithms inference statistical models paper introduce herding variant algorithm called herded Gibbs entirely deterministic prove herded Gibbs 1/T convergence rate models independent variables connected probabilistic graphical models Herded Gibbs outperform Gibbs tasks image denoising MRFs named entity recognition CRFs convergence herded Gibbs sparsely connected probabilistic graphical models Intracluster Moves Constrained Discrete-Space MCMC paper addresses sampling binary distributions constraints proposes MCMC method draw samples distribution set distance reference example reference vector zeros algorithm draw samples binary distribution constraint active variables 's motivate algorithm examples statistical physics probabilistic inference previous algorithms proposed sample binary distributions constraints algorithm allows moves space tends propose energetically favourable algorithm demonstrated Boltzmann machines varying difficulty ferromagnetic Ising model positive potentials restricted Boltzmann machine learned Gabor-like filters potentials challenging three-dimensional spin-glass positive negative potentials Practical N2 Monte Carlo Marginal Particle Filter Sequential Monte Carlo techniques useful estimation non-linear non-Gaussian dynamic models methods allow approximate joint posterior distribution using sequential sampling framework dimension target distribution grows time step introduce resampling steps ensure estimates provided algorithm reasonable variance applications marginal filtering distribution defined space fixed dimension Sequential Monte Carlo algorithm called Marginal Particle Filter operates directly marginal distribution avoiding perform sampling space growing dimension Using idea derive improved version auxiliary particle filter theoretic empirical demonstrate reduction variance conventional particle filtering techniques reducing cost marginal particle filter particles N2 logN Bayesian Optimization Billion Dimensions Random Embeddings Bayesian optimization techniques applied robotics planning sensor placement recommendation advertising intelligent user interfaces automatic algorithm configuration successes approach restricted moderate dimension workshops Bayesian optimization identified scaling high-dimensions holy grails field paper introduce novel random embedding idea attack Random EMbedding Bayesian Optimization REMBO algorithm simple invariance properties applies domains categorical continuous variables thorough theoretical analysis REMBO including regret bounds depend 's intrinsic dimensionality Empirical confirm REMBO effectively solve billions dimensions provided intrinsic dimensionality low REMBO achieves state-of-the-art performance optimizing discrete parameters popular mixed integer linear programming solver inference strategies solving Markov Decision Processes using reversible jump MCMC paper build previous inferences techniques Markov Chain Monte Carlo MCMC methods solve parameterized control propose modifications approach practical higher-dimensional spaces introduce target distribution incorporate reward sampled trajectories break strong correlations policy parameters sampled trajectories sample freely Finally incorporate techniques principled manner estimates optimal policy Nonparametric Bayesian Logic Bayesian Logic BLOG language developed defining first-order probability models worlds unknown objects handles AI including data association population estimation paper extends BLOG adopting generative processes function spaces nonparametrics Bayesian literature introduce syntax reasoning arbitrary collections objects properties intuitive manner exploiting exchangeability distributions unknown objects attributes cast Dirichlet processes resolve difficulties model selection inference caused varying objects demonstrate concepts application citation matching Exponential Regret Bounds Gaussian Process Bandits Deterministic Observations paper analyzes Gaussian process GP bandits deterministic observations analysis branch bound algorithm UCB algorithm Srinivas al GPs Gaussian observation noise variance strictly Srinivas al proved regret vanishes approximate rate sqrt observations complement result attack deterministic attain faster exponential convergence rate regularity assumptions regret decreases asymptotically frac tau ln d/4 probability dimension search space tau constant depends behaviour objective function global maximum Decentralized Adaptive Look-Ahead Particle Filtering decentralized particle filter DPF proposed increase level parallelism particle filtering decomposition space nested sets variables DPF particle filter sample set conditions sample generate set samples set variables DPF understood variant popular Rao-Blackwellized particle filter RBPF step carried using Monte Carlo approximations analytical inference result range applications DPF broader RBPF paper improve DPF derive Monte Carlo approximation optimal proposal distribution consequently design implement efficient look-ahead DPF decentralized filters initially designed capitalize parallel implementation look-ahead DPF outperform standard particle filter single machine propose bandit algorithms automatically configure space decomposition DPF Asymptotic Efficiency Deterministic Estimators Discrete Energy-Based Models Ratio Matching Pseudolikelihood Standard maximum likelihood estimation applied discrete energy-based models computation exact model probabilities intractable proposal estimators designed overcome intractability virtually theoretical properties paper generalized estimator unifies classical proposed estimators standard asymptotic theory M-estimators derive generic expression asymptotic covariance matrix generalized estimator apply study relative statistical efficiency classical pseudolikelihood recently-proposed ratio matching estimator