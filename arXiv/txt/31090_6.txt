Learning planning Bayes-optimal reinforcement learning Monte-Carlo tree search Bayes-optimal behavior well-defined difficult achieve advances Monte-Carlo tree search MCTS near-optimally Markov Decision Processes MDPs infinite spaces Bayes-optimal behavior unknown MDP equivalent optimal behavior belief-space MDP size belief-space MDP grows exponentially amount history retained infinite agent MCTS algorithm Forward Search Sparse Sampling FSSS efficient Bayes-optimally polynomial steps assuming FSSS efficiently underlying MDP Bayesian Sampling Approach Exploration Reinforcement Learning modular approach reinforcement learning Bayesian representation uncertainty models approach BOSS Sampled Set drives exploration sampling multiple models posterior selecting actions optimistically extends previous providing rule deciding resample combine models algorithm achieves nearoptimal reward probability sample complexity low relative speed posterior distribution converges learning demonstrate BOSS performs favorably compared state-of-the-art reinforcement-learning approaches illustrate flexibility pairing non-parametric model generalizes