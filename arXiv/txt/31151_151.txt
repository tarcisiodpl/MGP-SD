Studies Lower Bounding Probabilities Evidence using Markov Inequality Computing probability evidence error bounds NP-hard paper address hard settling easier propose approximation confidence lower bounds probability evidence guarantees terms relative absolute error proposed approximation randomized sampling scheme Markov inequality straight-forward application Markov inequality lead poor lower bounds propose heuristic measures improve performance practice Empirical evaluation scheme state-of the-art lower bounding schemes reveals promise approach AND/OR Sampling paper introduces AND/OR sampling probabilistic graphical models contrast sampling AND/OR sampling caches samples AND/OR space extracts sample stored samples prove AND/OR sampling lower variance sampling thereby providing theoretical justification preferring sampling empirical evaluation demonstrates AND/OR sampling accurate sampling Probabilistic Theorem Proving representation schemes combining first-order logic probability proposed Progress unifying logical probabilistic inference slower Existing methods variants lifted variable elimination belief propagation logical structure account propose method power graphical model inference first-order theorem proving finite domains Herbrand interpretations define probabilistic theorem proving generalization computing probability logical formula probabilities weights set formulas reduced lifted weighted model counting develop efficient algorithm prove correctness algorithm investigate properties generalizes previous approaches Experiments greatly outperforms lifted variable elimination logical structure Finally propose algorithm approximate probabilistic theorem proving greatly outperform lifted belief propagation Join-Graph Propagation Algorithms paper investigates parameterized approximate message-passing schemes based bounded inference inspired Pearl 's belief propagation algorithm BP start bounded inference mini-clustering algorithm move iterative scheme called Iterative Join-Graph Propagation IJGP combines iteration bounded inference Algorithm IJGP belongs class Generalized Belief Propagation algorithms framework allowed connections approximate algorithms statistical physics empirically surpass performance mini-clustering belief propagation state-of-the-art algorithms classes networks provide insight accuracy iterative BP IJGP relating algorithms classes constraint propagation schemes Approximate Inference Algorithms Hybrid Bayesian Networks Discrete Constraints paper consider Hybrid Mixed Networks HMN Hybrid Bayesian Networks allow discrete deterministic modeled explicitly form constraints approximate inference algorithms HMNs integrate adjust algorithmic principles Generalized Belief Propagation Rao-Blackwellised Sampling Constraint Propagation address complexity modeling reasoning HMNs demonstrate performance approximate inference algorithms randomly generated HMNs Approximation Quantization Inference graphical models consists repeatedly multiplying summing potentials intractable derived potentials exponentially Approximate inference techniques belief propagation variational methods combat simplifying derived potentials typically dropping variables propose alternate method simplifying potentials quantizing values Quantization potential value introduces context-specific independencies exploited represent potential compactly algebraic decision diagrams ADDs efficiently apply quantization ADD reduction variable elimination junction tree propagation yielding family bounded approximate inference schemes experimental tests schemes outperform state-of-the-art approaches benchmark instances Formula-Based Probabilistic Inference Computing probability formula probabilities weights associated formulas natural extension logical inference probabilistic setting Surprisingly received attention literature considering includes standard inference special paper propose algorithms formula decomposition conditioning exact method formula sampling approximate method knowledge application model counting approximate probabilistic inference conventional variable-based algorithms algorithms dual realm logical formulas Theoretically algorithms greatly improve efficiency exploiting structural formulas Empirically powerful achieving substantial performance gains state-of-the-art schemes Modeling Transportation Routines using Hybrid Dynamic Mixed Networks paper describes framework called Hybrid Dynamic Mixed Networks HDMNs Hybrid Dynamic Bayesian Networks allow representation discrete deterministic form constraints propose approximate inference algorithms integrate adjust algorithmic principles Generalized Belief Propagation Rao-Blackwellised Particle Filtering Constraint Propagation address complexity modeling reasoning HDMNs framework model person 's travel activity time predict destination routes current location preliminary empirical evaluation demonstrating effectiveness modeling framework algorithms using variants activity model