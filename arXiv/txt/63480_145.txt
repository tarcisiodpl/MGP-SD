Planning Prioritized Sweeping Backups Efficient planning plays crucial role model-based reinforcement learning Traditionally main planning operation backup based current estimates successor Consequently computation time proportional successor paper introduce planning backup current value single successor computation time independent successor backup call backup door class model-based reinforcement learning methods exhibit finer control planning process traditional methods empirically demonstrate increased flexibility allows efficient planning implementation prioritized sweeping based backups achieves substantial performance improvement classical implementations Dyna-Style Planning Linear Function Approximation Prioritized Sweeping consider efficiently learning optimal control policies value functions spaces online setting estimates interaction world paper develops explicitly model-based approach extending Dyna architecture linear function approximation Dynastyle planning proceeds generating imaginary experience world model applying model-free reinforcement learning algorithms imagined transitions main prove linear Dyna-style planning converges unique solution independent generating distribution natural conditions policy evaluation setting prove limit least-squares LSTD solution implication prioritized-sweeping soundly extended linear approximation preceding features preceding introduce versions prioritized sweeping linear Dyna illustrate performance empirically Mountain Car Boyan Chain Off-Policy Actor-Critic paper actor-critic algorithm off-policy reinforcement learning algorithm online incremental per-time-step complexity scales linearly learned weights Previous actor-critic algorithms limited on-policy setting advantage advances off-policy gradient temporal-difference learning Off-policy techniques Greedy-GQ enable target policy learned obtaining data behavior policy actor-critic methods practical action value methods Greedy-GQ explicitly represent policy consequently policy stochastic utilize action space paper illustrate practically combine generality learning potential off-policy learning flexibility action selection actor-critic methods derive incremental linear time space complexity algorithm includes eligibility traces prove convergence assumptions previous off-policy algorithms empirically comparable performance existing algorithms standard reinforcement-learning benchmark