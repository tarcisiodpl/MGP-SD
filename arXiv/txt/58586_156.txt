Field Variational Approximation Continuous-Time Bayesian Networks Continuous-time Bayesian networks natural structured representation language multicomponent stochastic processes evolve continuously time compact representation inference models intractable simple structured networks introduce field variational approximation product inhomogeneous Markov processes approximate distribution trajectories variational approach leads globally consistent distribution efficiently queried Additionally lower bound probability observations attractive learning tasks provide theoretical foundations approximation efficient implementation exploits wide range highly optimized ordinary differential equations ODE solvers experimentally explore characterizations processes approximation suitable applications large-scale realworld inference Continuous Time Markov Networks central task applications reasoning processes change continuous time mathematical framework Continuous Time Markov Processes basic foundations modeling systems Nodelman al introduced continuous time Bayesian networks CTBNs allow compact representation continuous-time processes factored space paper introduce continuous time Markov networks CTMNs alternative representation language represents type continuous-time dynamics real life processes biological chemical systems dynamics process naturally described interplay forces tendency entity change fitness energy function entire system model force described continuous-time proposal process suggests local changes system rates force represented Markov network encodes fitness desirability proposed local change accepted probability function change fitness distribution fitness distribution stationary distribution Markov process representation characterization temporal process stationary distribution compact graphical representation allows naturally capture type structure complex dynamical processes evolving biological sequences describe semantics representation basic properties compares CTBNs provide algorithms learning models data discuss applicability biological sequence evolution Gibbs Sampling Factorized Continuous-Time Markov Processes central task applications reasoning processes change continuous time Continuous-Time Bayesian Networks compact representation language multi-component continuous-time processes exact inference processes exponential components infeasible models develop novel Gibbs sampling procedure multi-component processes procedure iteratively samples trajectory components remaining perform exact sampling adapts natural time scale sampled process sampling procedure naturally exploits structure network reduce computational cost step procedure provide asymptotically unbiased approximation processes Incorporating Expressive Graphical Models Variational Approximations Chain-Graphs Hidden Variables Global variational approximation methods graphical models allow efficient approximate inference complex posterior distributions using simpler model choice approximating model determines tradeoff complexity approximation procedure quality approximation paper consider variational approximations based classes models richer standard Bayesian networks Markov networks mixture models classes allow tradeoffs spectrum approximations class models chain graphs capture distributions partially directed class models directed graphs Bayesian networks additional latent variables classes allow representation multi-variable dependencies easily represented Bayesian network