compact hierarchical Q-function decomposition Previous hierarchical reinforcement learning faced dilemma ignore values exit subroutine thereby risking suboptimal behavior represent values explicitly thereby incurring representation cost exit values refer nonlocal aspects world i.e. subsequent rewards paper avoid solution based recursively decomposing exit value function terms Q-functions levels hierarchy leads intuitively appealing runtime architecture parent subroutine passes child value function exit child reasons choices affect exit value identify structural conditions value function transition distributions allow concise representations exit distributions leading abstraction essence variables exit values considered parent cares child demonstrate utility algorithms series increasingly complex environments Gibbs Sampling Open-Universe Stochastic Languages Languages open-universe probabilistic models OUPMs represent situations unknown objects iden tity uncertainty wide range real-world appli cations existing purpose inference methods OUPMs efficient restricted lan guages model classes paper remedying deficit troducing proving correct generaliza tion Gibbs sampling partial worlds varying model structure ap proach draws extends previous generic OUPM inference methods aux iliary variable samplers nonparametric mixture models implemented BLOG well-known OUPM language Combined compile-time optimizations algorithm yields substan tial speedups existing methods sev eral test improves practicality OUPM languages Variational MCMC propose class learning algorithms combines variational approximation Markov chain Monte Carlo MCMC simulation Naive algorithms variational approximation proposal distribution perform approximation tends underestimate true variance features data solve introducing sophisticated MCMC algorithms algorithms mixture MCMC kernels random walk Metropolis kernel blockMetropolis-Hastings MH kernel variational approximation proposaldistribution MH kernel allows locate regions probability efficiently Metropolis kernel allows explore vicinity regions algorithm outperforms variationalapproximations yields estimates considerably estimates moments covariances outperforms standard MCMC algorithms locates theregions probability speeding convergence demonstrate algorithm Bayesian parameter estimation logistic sigmoid belief networks Rao-Blackwellised Particle Filtering Dynamic Bayesian Networks Particle filters PFs powerful sampling-based inference/learning algorithms dynamic Bayesian networks DBNs allow treat principled type probability distribution nonlinearity non-stationarity appeared fields names condensation sequential Monte Carlo survival fittest paper exploit structure DBN increase efficiency particle filtering using technique Rao-Blackwellisation Essentially samples variables marginalizes rest exactly using Kalman filter HMM filter junction tree algorithm finite dimensional optimal filter Rao-Blackwellised particle filters RBPFs lead accurate estimates standard PFs demonstrate RBPFs non-stationary online regression radial basis function networks robot localization map building discuss potential application provide references finite dimensional optimal filters temporally abstracted Viterbi algorithm Hierarchical abstraction applicable offer exponential reductions computational complexity Previous coarse-to-fine dynamic programming CFDP demonstrated possibility using abstraction speed Viterbi algorithm paper apply temporal abstraction Viterbi algorithm bounds derived analysis coarse timescales prune trellis finer timescales demonstrate improvements magnitude standard Viterbi algorithm speedups CFDP variables evolve widely differing rates Fine-Grained Decision-Theoretic Search Control Decision-theoretic control search basic unit computation generation evaluation complete set successors simplifies analysis lost opportunities pruning satisficing paper extends analysis value computation cover individual successor evaluations analytic techniques prove useful control reasoning settings formula developed expected value node successors evaluated formula estimate value expanding successors using formula value computation game-playing developed earlier exhibit improved version MGSS algorithm empirical game Othello Selecting Computations Theory Applications Sequential decision solvable simulating future action sequences em Metalevel decision procedures developed selecting em action sequences simulate based estimating expected improvement decision quality result simulation example using bandit algorithms control Monte Carlo tree search game paper develop theoretical basis metalevel decisions statistical framework Bayesian em selection arguing appropriate bandit framework derive basic applicable Monte Carlo selection including finite sampling bounds optimal policies provide simple counterexample intuitive conjecture optimal policy reach decision derive heuristic approximations Bayesian distribution-free settings demonstrate superiority bandit-based heuristics one-shot decision Improving Gradient Estimation Incorporating Sensor Data efficient policy search algorithm estimate local gradient objective function respect policy parameters trials policy search methods estimate gradient observing rewards policy trials theoretically empirically taking account sensor data gradient estimates faster learning reason rewards policy execution vary trial trial noise environment sensor data correlates noise partially correct variation estimatorwith lower variance Image Segmentation Video Sequences Probabilistic Approach Background subtraction technique finding moving objects video sequence example cars driving freeway idea subtracting current image timeaveraged background image leave nonstationary objects crude approximation task classifying pixel current image fails slow-moving objects distinguish shadows moving objects basic idea paper classify pixel using model pixel classes learn mixture-of-Gaussians classification model pixel using unsupervised technique efficient incremental version EM standard image-averaging approach automatically updates mixture component class likelihood membership slow-moving objects handled perfectly approach identifies eliminates shadows effectively techniques thresholding Application method Roadwatch traffic surveillance project expected result improvements vehicle identification tracking RAPID Reachable Anytime Planner Imprecisely-sensed Domains intractability generic optimal partially observable Markov decision process planning exist highly structured models Previous researchers insight construct efficient algorithms factored domains domains topological structure flat dynamics model motivated findings education community relevant automated tutoring consider exhibit form topological structure factored dynamics model Reachable Anytime Planner Imprecisely-sensed Domains RAPID leverages structure efficiently compute initial envelope reachable optimal MDP policy time linear variables RAPID performs partially-observable planning limited envelope slowly expands space considered time allows RAPID performs tutoring-inspired simulation variables corresponding flat space General-Purpose MCMC Inference Relational Structures Tasks record linkage multi-target tracking involve reconstructing set objects underlie observed data challenging probabilistic inference achieved efficient accurate inference using Markov chain Monte Carlo MCMC techniques customized proposal distributions Currently implementing system requires coding MCMC representations acceptance probability calculations specific application alternative approach pursue paper general-purpose probabilistic modeling language BLOG generic Metropolis-Hastings MCMC algorithm supports user-supplied proposal distributions algorithm gains flexibility using MCMC partial descriptions worlds provide conditions MCMC partial worlds yields correct answers queries context-specific Bayes net identify factors acceptance probability computed proposed move Experimental citation matching task general-purpose MCMC engine compares favorably application-specific system