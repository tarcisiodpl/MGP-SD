Speeding Planning Markov Decision Processes Automatically Constructed Abstractions paper consider planning stochastic shortest path SSP subclass Markov Decision MDP focus medium-size space enumerated numerous applications navigation planning uncertainty propose approach constructing multi-level hierarchy progressively simpler abstractions original computed hierarchy speed planning finding policy abstract level recursively refining solution original approach automated delivers speed-up magnitude state-of-the-art MDP solver sample returning near-optimal solutions prove theoretical bounds loss solution optimality abstractions Improved Variance Approximations Belief Net Responses Network Doubling Bayesian belief network models joint distribution directed acyclic graph representing dependencies variables network parameters characterizing conditional distributions parameters viewed random variables quantify uncertainty values Belief nets compute responses queries i.e. conditional probabilities query function parameters random variable Van Allen al. quantify uncertainty query delta method approximation variance develop accurate approximations query variance key idea extend query approximation doubled network involving independent replicates method assumes complete data applied discrete continuous hybrid networks provided discrete variables discrete parents analyze improvements provide empirical studies demonstrate effectiveness Learning Bayesian Nets Perform Bayesian net BN succinct encode probabilistic distribution corresponds function answer queries BN evaluated accuracy answers returns algorithms learning BNs attempt optimize criterion usually likelihood augmented regularizing term independent distribution queries posed paper takes performance criteria seriously considers challenge computing BN performance read accuracy distribution queries optimal aspects learning task difficult corresponding subtasks standard model Bayesian Error-Bars Belief Net Inference Bayesian Belief Network BN model joint distribution setof variables DAG structure represent dependenciesbetween variables set parameters aka CPTables represent thelocal conditional probabilities node assignment itsparents situations parameters random variables reflect uncertainty domain expert atraining sample estimate parameter values distribution overthese CPtable variables induces distribution response BNwill return Pr query paper investigates thevariance response asymptotically normal providing asymptotical variance aneffective algorithm computing variance samecomplexity simply computing value response variables effective treewidth Finally provide empirical evidence algorithm whichincorporates assumptions approximations effectively practice samples Generalized Loop Correction Method Approximate Inference Graphical Models Belief Propagation BP popular methods inference probabilistic graphical models BP guaranteed return correct answer tree structures incorrect non-convergent loopy graphical models approximate inference algorithms based cavity distribution proposed methods account loops incorporating dependency BP messages Alternatively region-based approximations lead methods Generalized Belief Propagation improve BP considering interactions clusters variables taking loops clusters account paper introduces approach Generalized Loop Correction GLC benefits types loop correction GLC relates families inference methods provide empirical evidence GLC effectively accurate correction schemes Training Restricted Boltzmann Machine Perturbation approach maximum likelihood learning discrete graphical models RBM introduced method Perturb Descend PD inspired ideas perturb MAP method sampling learning Contrastive Divergence minimization contrast perturb MAP PD leverages training data learn models allow efficient MAP estimation learning produce sample current model start training data descend energy landscape perturbed model fixed steps local optima reached RBM involves linear calculations thresholding fast amount perturbation closely temperature parameter regularize model producing robust features sparse hidden layer activation