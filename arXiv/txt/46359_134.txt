Online Stochastic Optimization Correlated Bandit Feedback paper consider online stochastic optimization locally smooth function bandit feedback introduce high-confidence tree HCT algorithm novel any-time mathcal armed bandit algorithm derive regret bounds matching performance existing state-of-the-art terms dependency steps smoothness factor main advantage HCT handles challenging correlated rewards existing methods require reward-generating process arm identically independent distributed iid random process HCT improves state-of-the-art terms memory requirement requiring weaker smoothness assumption mean-reward function compare previous anytime algorithms Finally discuss HCT applied policy search reinforcement learning report preliminary empirical Regret Bounds Reinforcement Learning Policy Advice reinforcement learning agent provided set input policies learned prior experience provided advisors reinforcement learning policy advice RLPA algorithm leverages input set learns policy set reinforcement learning task hand prove RLPA sub-linear regret tilde sqrt relative input policy regret computational complexity independent size action space empirical simulations support theoretical analysis suggests RLPA offer advantages domains prior policies provided RAPID Reachable Anytime Planner Imprecisely-sensed Domains intractability generic optimal partially observable Markov decision process planning exist highly structured models Previous researchers insight construct efficient algorithms factored domains domains topological structure flat dynamics model motivated findings education community relevant automated tutoring consider exhibit form topological structure factored dynamics model Reachable Anytime Planner Imprecisely-sensed Domains RAPID leverages structure efficiently compute initial envelope reachable optimal MDP policy time linear variables RAPID performs partially-observable planning limited envelope slowly expands space considered time allows RAPID performs tutoring-inspired simulation variables corresponding flat space CORL Continuous-state Offset-dynamics Reinforcement Learner Continuous spaces stochastic switching dynamics characterize rich realworld domains robot navigation varying terrain describe reinforcementlearning algorithm learning domains prove environments algorithm correct sample complexity scales polynomially state-space dimension Unfortunately optimal planning techniques exist fitted value iteration solve learned MDP include error approximate planning bounds Finally report experiment using robotic car driving varying terrain demonstrate dynamics representations adequately capture real-world dynamics algorithm efficiently solve