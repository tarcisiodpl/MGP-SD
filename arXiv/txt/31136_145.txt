PAC-Bayesian Policy Evaluation Reinforcement Learning Bayesian priors offer compact incorporating domain knowledge learning tasks correctness Bayesian analysis inference depends accuracy correctness priors PAC-Bayesian methods overcome providing bounds hold correctness prior distribution paper introduces PAC-Bayesian bound batch reinforcement learning function approximation bound perform model-selection transfer learning scenario empirical confirm PAC-Bayesian policy evaluation leverage prior distributions informative standard Bayesian RL approaches ignore misleading Online Learning Delayed Feedback Online learning delayed feedback received increasing attention applications distributed web-based learning paper provide systematic study topic analyze delay regret online learning algorithms surprisingly delay increases regret multiplicative adversarial additive stochastic meta-algorithms transform black-box fashion algorithms developed non-delayed handle presence delays feedback loop Modifications well-known UCB algorithm developed bandit delayed feedback advantage meta-algorithms implemented lower complexity Online Learning Markov Decision Processes Adversarially Chosen Transition Probability Distributions study learning Markov decision processes finite action spaces transition probability distributions loss functions chosen adversarially allowed change time introduce algorithm regret respect policy comparison class grows square root rounds game provided transition probabilities satisfy uniform mixing condition approach efficient comparison class polynomial compute expectations sample paths policy Designing efficient algorithm regret remains Randomized Mirror Descent Algorithm Scale Multiple Kernel Learning consider simultaneously learning linearly combine kernels learn predictor based learnt kernel kernels combined multiple kernel learning methods computational cost scales linearly intractable propose randomized version mirror descent algorithm overcome issue objective minimizing norm penalized empirical risk key achieve required exponential speed-up computationally efficient construction low-variance estimates gradient propose sampling based estimates ideal distribution samples coordinate probability proportional magnitude corresponding gradient surprising result learning coefficients polynomial kernel combinatorial structure base kernels combined allows implementation sampling distribution log time total computational cost method achieve epsilon optimal solution log epsilon thereby allowing method operate values Experiments simulated real data confirm algorithm computationally efficient state-of-the-art alternatives Statistical Linear Estimation Penalized Estimators Application Reinforcement Learning Motivated value function estimation reinforcement learning study statistical linear inverse i.e. coefficients linear system solved observed noise consider penalized estimators performance evaluated using matrix-weighted two-norm defect estimator measured respect true unknown coefficients objective functions considered depending error defect measured respect noisy coefficients squared unsquared propose simple novel theoretically well-founded data-dependent choices regularization parameters avoid data-splitting distinguishing feature analysis derive deterministic error bounds terms error coefficients allowing complete separation analysis stochastic properties errors lead insights bounds linear value function estimation reinforcement learning Analysis Kernel Matching Covariate Shift real supervised learning scenarios uncommon training test sample follow probability distributions rendering necessity correct sampling bias Focusing covariate shift derive probability confidence bounds kernel matching KMM estimator convergence rate depend regularity measure regression function capacity measure kernel comparing KMM natural plug-in estimator establish superiority provide concrete evidence/understanding effectiveness KMM covariate shift Apprenticeship Learning using Inverse Reinforcement Learning Gradient Methods paper propose novel gradient algorithm learn policy expert 's observed behavior assuming expert behaves optimally respect unknown reward function Markovian Decision algorithm 's aim reward function optimal policy matches expert 's observed behavior main difficulty mapping parameters policies nonsmooth highly redundant Resorting subdifferentials solves difficulty computing natural gradients tested proposed method artificial domains reliable efficient previous methods Adaptive Algorithm Finite Stochastic Partial Monitoring anytime algorithm achieves near-optimal regret instance finite stochastic partial monitoring algorithm achieves minimax regret logarithmic factors easy hard easy additionally achieves logarithmic individual regret importantly algorithm adaptive sense opponent strategy easy region strategy space regret grows easy implication reasonable additional assumptions algorithm enjoys sqrt regret Dynamic Pricing proven hard Bartok al. Dyna-Style Planning Linear Function Approximation Prioritized Sweeping consider efficiently learning optimal control policies value functions spaces online setting estimates interaction world paper develops explicitly model-based approach extending Dyna architecture linear function approximation Dynastyle planning proceeds generating imaginary experience world model applying model-free reinforcement learning algorithms imagined transitions main prove linear Dyna-style planning converges unique solution independent generating distribution natural conditions policy evaluation setting prove limit least-squares LSTD solution implication prioritized-sweeping soundly extended linear approximation preceding features preceding introduce versions prioritized sweeping linear Dyna illustrate performance empirically Mountain Car Boyan Chain Speeding Planning Markov Decision Processes Automatically Constructed Abstractions paper consider planning stochastic shortest path SSP subclass Markov Decision MDP focus medium-size space enumerated numerous applications navigation planning uncertainty propose approach constructing multi-level hierarchy progressively simpler abstractions original computed hierarchy speed planning finding policy abstract level recursively refining solution original approach automated delivers speed-up magnitude state-of-the-art MDP solver sample returning near-optimal solutions prove theoretical bounds loss solution optimality abstractions