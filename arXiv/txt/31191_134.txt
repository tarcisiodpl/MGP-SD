Bilinear Programming Approach Multiagent Planning Multiagent planning coordination common computationally hard wide range two-agent formulated bilinear programs successive approximation algorithm outperforms coverage set algorithm state-of-the-art method class multiagent algorithm formulated bilinear programs simpler implement algorithm terminated time and-unlike coverage set algorithm-it facilitates derivation useful online performance bound efficient average reducing computation time optimal solution magnitude Finally introduce automatic dimensionality reduction method improves effectiveness algorithm extending applicability domains providing analyze subclass bilinear programs Policy Iteration Decentralized Control Markov Decision Processes Coordination distributed agents required arising including multi-robot systems networking e-commerce formal framework decentralized partially observable Markov decision process DEC-POMDP optimal dynamic programming algorithms single-agent version optimal algorithms multiagent elusive main contribution paper optimal policy iteration algorithm solving DEC-POMDPs algorithm stochastic finite-state controllers represent policies solution include correlation device allows agents correlate actions communicating approach alternates expanding controller performing value-preserving transformations modify controller sacrificing value efficient value-preserving transformations reduce size controller improve value keeping size fixed Empirical demonstrate usefulness value-preserving transformations increasing value keeping controller size minimum broaden applicability approach heuristic version policy iteration algorithm sacrifices convergence optimality algorithm reduces size controllers step assuming probability distributions agents actions assumption hold helps produce quality solutions test Complexity Decentralized Control Markov Decision Processes Planning distributed agents partial considered decision theoretic perspective describe generalizations MDP POMDP models allow decentralized control agents finite-horizon corresponding models complete nondeterministic exponential time complexity illustrate fundamental difference centralized decentralized control Markov processes contrast MDP POMDP consider provably admit polynomial-time algorithms require doubly exponential time solve worst provided mathematical evidence corresponding intuition decentralized planning easily reduced centralized solved exactly using established techniques Optimizing Memory-Bounded Controllers Decentralized POMDPs memory-bounded optimization approach solving infinite-horizon decentralized POMDPs Policies agent represented stochastic finite controllers formulate optimizing policies nonlinear program leveraging powerful existing nonlinear optimization techniques solving existing solvers guarantee locally optimal solutions formulation produces quality controllers state-of-the-art approach incorporate shared source randomness form correlation device increase solution quality limited increase space time experimental nonlinear optimization provide quality concise solutions decentralized decision uncertainty Rollout Sampling Policy Iteration Decentralized POMDPs decentralized rollout sampling policy iteration DecRSPI algorithm multi-agent decision formalized DEC-POMDPs DecRSPI designed improve scalability tackle lack explicit model algorithm Monte Carlo methods generate sample reachable belief computes joint policy belief based rollout estimations policy representation allows represent solutions compactly key benefits algorithm linear time complexity agents bounded memory usage solution quality solve larger intractable existing planning algorithms Experimental confirm effectiveness scalability approach Anytime Planning Decentralized POMDPs using Expectation Maximization Decentralized POMDPs provide expressive framework multi-agent sequential decision fnite-horizon DECPOMDPs enjoyed signifcant success progress remains slow infnite-horizon inherent complexity optimizing stochastic controllers representing agent policies promising class algorithms infnite-horizon recasts optimization inference mixture DBNs attractive feature approach straightforward adoption existing inference techniques DBNs solving DEC-POMDPs supporting richer representations factored continuous actions derive Expectation Maximization EM algorithm optimize joint policy represented DBNs Experiments benchmark domains EM compares favorably state-of-the-art solvers Message-Passing Algorithms Quadratic Programming Formulations MAP Estimation Computing maximum posteriori MAP estimation graphical models inference applications message-passing algorithms quadratic programming QP formulations MAP estimation pairwise Markov random fields concave-convex procedure CCCP locally optimal algorithm non-convex QP formulation technique derive globally convergent algorithm convex QP relaxation MAP developed expectation-maximization EM algorithm QP formulation MAP derived CCCP perspective Experiments synthetic real-world confirm approach competitive max-product variations Compared CPLEX achieve order-of-magnitude speedup solving optimally convex QP relaxation