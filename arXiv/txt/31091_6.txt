CORL Continuous-state Offset-dynamics Reinforcement Learner Continuous spaces stochastic switching dynamics characterize rich realworld domains robot navigation varying terrain describe reinforcementlearning algorithm learning domains prove environments algorithm correct sample complexity scales polynomially state-space dimension Unfortunately optimal planning techniques exist fitted value iteration solve learned MDP include error approximate planning bounds Finally report experiment using robotic car driving varying terrain demonstrate dynamics representations adequately capture real-world dynamics algorithm efficiently solve Bayesian Sampling Approach Exploration Reinforcement Learning modular approach reinforcement learning Bayesian representation uncertainty models approach BOSS Sampled Set drives exploration sampling multiple models posterior selecting actions optimistically extends previous providing rule deciding resample combine models algorithm achieves nearoptimal reward probability sample complexity low relative speed posterior distribution converges learning demonstrate BOSS performs favorably compared state-of-the-art reinforcement-learning approaches illustrate flexibility pairing non-parametric model generalizes Learning planning Bayes-optimal reinforcement learning Monte-Carlo tree search Bayes-optimal behavior well-defined difficult achieve advances Monte-Carlo tree search MCTS near-optimally Markov Decision Processes MDPs infinite spaces Bayes-optimal behavior unknown MDP equivalent optimal behavior belief-space MDP size belief-space MDP grows exponentially amount history retained infinite agent MCTS algorithm Forward Search Sparse Sampling FSSS efficient Bayes-optimally polynomial steps assuming FSSS efficiently underlying MDP Incremental Pruning Simple Fast Exact Method Partially Observable Markov Decision Processes exact algorithms partially observable Markov decision processes POMDPs form dynamic programming piecewise-linear convex representation value function transformed examine variations incremental pruning method solving compare earlier algorithms theoretical empirical perspectives incremental pruning presently efficient exact method solving POMDPs Complexity Plan Existence Evaluation Probabilistic Domains examine computational complexity testing finding plans probabilistic planning domains succinct representations complete variety complexity classes NP co-NP NP co-NP PSPACE probabilistic classes NP special field uncertainty artificial intelligence deserving additional study fruitful direction future algorithmic development Exploring compact reinforcement-learning representations linear regression paper algorithm online linear regression efficiency guarantees satisfy requirements KWIK framework algorithm improves complexity bounds current state-of-the-art procedure setting explore applications algorithm learning compact reinforcement-learning representations KWIK linear regression learn reward function factored MDP probabilities action outcomes Stochastic STRIPS Object Oriented MDPs proven efficiently learnable RL setting combine KWIK linear regression KWIK learners learn larger portions models including experiments learning factored MDP transition reward functions Incremental Model-based Learners Formal Learning-Time Guarantees Model-based learning algorithms experience efficiently learning solve Markov Decision Processes MDPs finite action spaces computational cost repeatedly solving internal model inhibits large-scale propose method based real-time dynamic programming RTDP speed model-based algorithms RMAX MBIE model-based interval estimation computationally faster algorithms loss compared existing bounds learning algorithms RTDP-RMAX RTDP-IE considerably computational demands RMAX MBIE develop theoretical framework allows prove efficient learners PAC correct sense experimental evaluation algorithms helps quantify tradeoff computational experience demands Polynomial-time Nash Equilibrium Algorithm Repeated Stochastic Games polynomial-time algorithm approximate Nash equilibrium repeated two-player stochastic games algorithm exploits folk theorem derive strategy profile forms equilibrium buttressing mutually beneficial behavior threats component algorithm efficiently searches approximation egalitarian fairest pareto-efficient solution paper concludes applying algorithm set grid games illustrate typical solutions algorithm solutions compare favorably competing algorithms strategies social welfare guaranteed computational efficiency Graphical Models Game Theory introduce graphical modelsfor multi-player game theory powerful algorithms computing Nash equilibria n-player game undirected graph nodes set local matrices interpretation payoff player determined entirely actions player neighbors graph payoff matrix player indexed players view global n-player game composed interacting local games involving fewer players player 's action global impact occurs propagation local influences.Our main technical result efficient algorithm computing Nash equilibria underlying graph tree tree node mergings algorithm runs time polynomial size representation graph theassociated local game matrices distinct flavors version involves approximation step computes representation approximate Nash equilibria exponential version allows exact computation Nash equilibria expense weakened complexity bounds algorithm requires local message-passing nodes implemented players distributed manner analogy inference Bayes nets develop analysis algorithm involved polytree algorithm partially compute select exponential potential solutions discuss extensions computation equilibria desirable global properties e.g. maximizing global return directions Efficient Optimal-Equilibrium Algorithm Two-player Game Trees Two-player complete-information game trees simplest setting studying general-sum games computational finding equilibria games admit simple bottom-up algorithm finding subgame perfect Nash equilibria efficiently algorithm fail identify optimal equilibria maximize social welfare reason counterintuitively probabilistic action choices achieve maximum payoffs provide novel polynomial-time algorithm explicitly reasons stochastic decisions demonstrate example card game