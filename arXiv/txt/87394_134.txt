Selecting Computations Theory Applications Sequential decision solvable simulating future action sequences em Metalevel decision procedures developed selecting em action sequences simulate based estimating expected improvement decision quality result simulation example using bandit algorithms control Monte Carlo tree search game paper develop theoretical basis metalevel decisions statistical framework Bayesian em selection arguing appropriate bandit framework derive basic applicable Monte Carlo selection including finite sampling bounds optimal policies provide simple counterexample intuitive conjecture optimal policy reach decision derive heuristic approximations Bayesian distribution-free settings demonstrate superiority bandit-based heuristics one-shot decision VOI-aware MCTS UCT state-of-the art algorithm Monte Carlo tree search MCTS games Markov decision processes based UCB1 sampling policy Multi-armed Bandit MAB minimizes cumulative regret search differs MAB MCTS usually final arm pull actual move selection collects reward arm pulls paper MCTS sampling policy based Value VOI estimates rollouts suggested Empirical evaluation policy comparison UCB1 UCT performed random MAB instances Computer MCTS Based Simple Regret UCT state-of-the art algorithm Monte Carlo tree search MCTS games Markov decision processes based UCB sampling policy Multi-armed Bandit MAB minimizes cumulative regret search differs MAB MCTS usually final arm pull actual move selection collects reward arm pulls sense minimize simple regret opposed cumulative regret introducing policies multi-armed bandits lower finite-time asymptotic simple regret UCB using develop two-stage scheme SR+CR MCTS outperforms UCT empirically Optimizing sampling process metareasoning solution value VOI techniques theory VOI search exists applying MCTS non-trivial typical myopic assumptions fail Lacking complete VOI theory MCTS propose sampling scheme aware VOI achieving algorithm empirical evaluation outperforms UCT proposed algorithms