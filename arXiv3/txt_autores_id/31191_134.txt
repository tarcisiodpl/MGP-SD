
Multiagent planning and coordination problems are common and known to be computationally hard. We show that a wide range of two-agent problems can be formulated
as bilinear programs. We present a successive approximation algorithm that significantly
outperforms the coverage set algorithm, which is the state-of-the-art method for this class
of multiagent problems. Because the algorithm is formulated for bilinear programs, it is
more general and simpler to implement. The new algorithm can be terminated at any time
and–unlike the coverage set algorithm–it facilitates the derivation of a useful online performance bound. It is also much more efficient, on average reducing the computation time
of the optimal solution by about four orders of magnitude. Finally, we introduce an automatic dimensionality reduction method that improves the effectiveness of the algorithm,
extending its applicability to new domains and providing a new way to analyze a subclass
of bilinear programs.

1. Introduction
We present a new approach for solving a range of multiagent planning and coordination
problems using bilinear programming. The problems we focus on represent various extensions of the Markov decision process (MDP) to multiagent settings. The success of MDP
algorithms for planning and learning under uncertainty has motivated researchers to extend
the model to cooperative multiagent problems. One possibility is to assume that all the
agents share all the information about the underlying state. This results in a multiagent
Markov decision process (Boutilier, 1999), which is essentially an MDP with a factored
action set. A more complex alternative is to allow only partial sharing of information
among agents. In these settings, several agents–each having different partial information
about the world–must cooperate with each other in order to achieve some joint objective. Such problems are common in practice and can be modeled as decentralized partially
observable MDPs (DEC-POMDPs) (Bernstein, Zilberstein, & Immerman, 2000). Some refinements of this model have been studied, for example by making certain independence
assumptions (Becker, Zilberstein, & Lesser, 2003) or by adding explicit communication
actions (Goldman & Zilberstein, 2008). DEC-POMDPs are closely related to extensive
games (Rubinstein, 1997). In fact, any DEC-POMDP represents an exponentially larger
extensive game with a common objective. Unfortunately, DEC-POMDPs with just two
agents are intractable in general, unlike MDPs that can be solved in polynomial time.
Despite recent progress in solving DEC-POMDPs, even state-of-the-art algorithms are
generally limited to very small problems (Seuken & Zilberstein, 2008). This has motivated
the development of algorithms that either solve a restricted class of problems (Becker,
c 2009 AI Access Foundation. All rights reserved.

Petrik & Zilberstein

Lesser, & Zilberstein, 2004; Kim, Nair, Varakantham, Tambe, & Yokoo, 2006) or provide
only approximate solutions (Emery-Montemerlo, Gordon, Schneider, & Thrun, 2004; Nair,
Roth, Yokoo, & Tambe, 2004; Seuken & Zilberstein, 2007). In this paper, we introduce
an efficient algorithm for several restricted classes, most notably decentralized MDPs with
transition and observation independence (Becker et al., 2003). For the sake of simplicity,
we denote this model as DEC-MDP, although this is usually used to denote the model
without the independence assumptions. The objective in these problems is to maximize the
cumulative reward of a set of cooperative agents over some finite horizon. Each agent can
be viewed as a single decision-maker operating on its own “local” MDP. What complicates
the problem is the fact that all these MDPs are linked through a common reward function
that depends on their states.
The coverage set algorithm (CSA) was the first optimal algorithm to solve efficiently
transition and observation independent DEC-MDPs (Becker, Zilberstein, Lesser, & Goldman, 2004). By exploiting the fact that the interaction between the agents is limited
compared to their individual local problems, CSA can solve problems that cannot be solved
by the more general exact DEC-POMDP algorithms. It also exhibits good anytime behavior. However, the anytime behavior is of limited applicability because solution quality is
only known in hindsight, after the algorithm terminates.
We develop a new approach to solve DEC-MDPs–as well as a range of other multiagent
planning problems–by representing them as bilinear programs. We also present an efficient
new algorithm for solving these kinds of separable bilinear problems. When the algorithm
is applied to DEC-MDPs, it improves efficiency by several orders of magnitude compared
with previous state-of the art algorithms (Becker, 2006; Petrik & Zilberstein, 2007a). In
addition, the algorithm provides useful runtime bounds on the approximation error, which
makes it more useful as an anytime algorithm. Finally, the algorithm is formulated for
general separable bilinear programs and therefore it can be easily applied to a range of
other problems.
The rest of the paper is organized as follows. First, in Section 2, we describe the basic
bilinear program formulation and how a range of multiagent planning problems can be
expressed within this framework. In Section 3, we describe a new successive approximation
algorithm for bilinear programs. The performance of the algorithm depends heavily on the
number of interactions between the agents. To address that, we propose in Section 4 a
method that automatically reduces the number of interactions and provides a bound on
the degradation in solution quality. Furthermore, to be able to project the computational
effort required to solve a given problem instance, we develop offline approximation bounds
in Section 5. In Section 6, we examine the performance of the approach on a standard
benchmark problem. We conclude with a summary of the results and a discussion of future
work that could further improve the performance of this approach.

2. Formulating Multiagent Planning Problems as Bilinear Programs
We begin with a formal description of bilinear programs and the different types of multiagent
planning problems that can be formulated as such. In addition to multiagent planning
problems, bilinear programs can be used to solve a variety of other problems such as robotic
manipulation (Pang, Trinkle, & Lo, 1996), bilinear separation (Bennett & Mangasarian,
236

A Bilinear Programming Approach for Multiagent Planning

1992), and even general linear complementarity problems (Mangasarian, 1995). We focus on
multiagent planning problems where this formulation turns out to be particularly effective.
Definition 1. A separable bilinear program in the normal form is defined as follows:
maximize
w,x,y,z

T
T
T
T
f (w, x, y, z) = sT
1 w + r1 x + x Cy + r2 y + s2 z

subject to A1 x + B1 w = b1

(1)

A2 y + B2 z = b2
w, x, y, z ≥ 0

The size of the program is the total number of variables in w, x, y and z. The number of
variables in y determines the dimensionality of the program1 .
Unless otherwise specified, all vectors are column vectors. We use boldface 0 and 1
to denote vectors of zeros and ones respectively of the appropriate dimensions. This program specifies two linear programs that are connected only through the nonlinear objective
function term xT Cy. The program contains two types of variables. The first type includes
the variables x, y that appear in the bilinear term of the objective function. The second
type includes the additional variables w, z that do not appear in the bilinear term. As we
show later, this distinction is important because the complexity of the algorithm we propose
depends mostly on the dimensionality of the problem, which is the number of variables y
involved in the bilinear term.
The bilinear program in Eq. (1) is separable because the constraints on x and w are
independent of the constraints on y and z. That is, the variables that participate in the
bilinear term of the objective function are independently constrained. The theory of nonseparable bilinear programs is much more complicated and the corresponding algorithms
are not as efficient (Horst & Tuy, 1996). Thus, we limit the discussion in this paper to
separable bilinear programs and often omit the term “separable”. As discussed later in
more detail, a separable bilinear program may be seen as a concave minimization problem
with multiple local minima. It can be shown that solving this problem is NP-complete,
compared to polynomial time complexity of linear programs.
In addition to the formulation of the bilinear program shown in Eq. (1), we also use the
following formulation, stated in terms of inequalities:
maximize
x,y

xT Cy

subject to A1 x ≤ b1

x≥0

A2 y ≤ b2

y≥0

(2)

The latter formulation can be easily transformed into the normal form using standard
transformations of linear programs (Vanderbei, 2001). In particular, we can introduce slack
1. It is possible to define the dimensionality in terms of x, or the minimum of dimensions of x and y. The
issue is discussed in Appendix B.

237

Petrik & Zilberstein

variables w, z to obtain the following identical bilinear program in the normal form:
xT Cy

maximize
w,x,y,z

subject to A1 x − w = b1
A2 y − z = b2

(3)

w, x, y, z ≥ 0
We use the following matrix and block matrix notation in the paper. Matrices are
denoted by square brackets, with columns separated by commas and rows separated by
semicolons. Columns haveprecedence
over rows. For example, the notation [A, B; C, D]

corresponds to the matrix

A
C

B
.
D

As we show later, the presence of the variables w, z in the objective function may prevent
a crucial function from being convex. Since this has an unfavorable impact on the properties
of the bilinear program, we introduce a compact form of the problem.
Definition 2. We say that the bilinear program in Eq. (1) is in a compact form when s1
and s2 are zero vectors. It is in a semi-compact form if s2 is a zero vector.
The compactness requirement is not limiting because any bilinear program in the form
shown in Eq. (1) can be expressed in a semi-compact form as follows:
maximize
w,x,y,z,x̂,ŷ

sT
1w

+

r1T x

+

xT

subject to A1 x + B1 w = b1
x̂ = 1 ŷ =

 

 C 0
y
x̂
+ r2T y
ŷ
0 1
A2 y + B2 z = b2

(4)

sT
2z

w, x, y, z ≥ 0
Clearly, feasible solutions of Eq. (1) and Eq. (4) have the same objective value when ŷ is set
appropriately. Notice that the dimensionality of the bilinear term in the objective function
increases by 1 for both x and y. Hence, this transformation increases the dimensionality of
the program by 1.
The rest of this section describes several classes of multiagent planning problems that can
be formulated as bilinear programs. Starting with observation and transition independent
DEC-MDPs, we extend the formulation to allow a different objective function (maximizing
average reward over an infinite horizon), to handle interdependent observations, and to find
Nash equilibria in competitive settings.
2.1 DEC-MDPs
As mentioned previously, any transition-independent and observation-independent DECMDP (Becker et al., 2004) may be formulated as a bilinear program. Intuitively, a DECMDP is transition independent when no agent can influence the other agents’ transitions. A
DEC-MDP is observation independent when no agent can observe the states of other agents.
These assumptions are crucial since they ensure a lower complexity of the problem (Becker
238

A Bilinear Programming Approach for Multiagent Planning

et al., 2004). In the remainder of the paper, we use simply the term DEC-MDP to refer to
transition and observation independent DEC-MDP.
The DEC-MDP model has proved useful in several multiagent planning domains. One
example that we use is the Mars rover planning problem (Bresina, Golden, Smith, & Washington, 1999), first formulated as a DEC-MDP by Becker et al. (2003). This domain
involves two autonomous rovers that visit several sites in a given order and may decide to
perform certain scientific experiments at each site. The overall activity must be completed
within a given time limit. The uncertainty about the duration of each experiment is modeled by a given discrete distribution. While the rovers operate independently and receive
local rewards for each completed experiment, the global reward function also depends on
some experiments completed by both rovers. The interaction between the rovers is thus
limited to a relatively small number of such overlapping tasks. We return to this problem
and describe it in more detail in Section 6.
A DEC-MDP problem is composed of two MDPs with state-sets S1 , S2 and action sets
A1 , A2 . The functions r1 and r2 define local rewards for action-state pairs. The initial
state distributions are α1 and α2 . The MDPs are coupled through a global reward function
defined by the matrix R. Each entry R(i, j) represents the joint reward for the state-action
i by one agent and j by the other. Our definition of a DEC-MDP is based on the work of
Becker et al. (2004), with some modifications that we discuss below.
Definition 3. A two-agent transition and observation independent DEC-MDP with extended reward structure is defined by a tuple hS, F, α, A, P, Ri:
• S = (S1 , S2 ) is the factored set of world states
• F = (F1 ⊆ S1 , F2 ⊆ S2 ) is the factored set of terminal states.
• α = (α1 , α2 ) where αi : Si 7→ [0, 1] are the initial state distribution functions
• A = (A1 , A2 ) is the factored set of actions
• P = (P1 , P2 ), Pi : Si × Ai × Si 7→ [0, 1] are the transition functions. Let a ∈ Ai
be an action, then Pia : Si × Si 7→ [0, 1] is a stochastic transition matrix such that
Pi (s, a, s0 ) = Pia (s, s0 ) is the probability of a transition from state s ∈ Si to state
s0 ∈ Si of agent i, assuming it takes action a. The transitions from the final states
have 0 probability; that is Pi (s, a, s0 ) = 0 if s ∈ Fi , s0 ∈ Si , and a ∈ Ai .
• R = (r1 , r2 , R) where ri : Si × Ai 7→ R are the local reward functions and
R : (S1 × A1 ) × (S2 × A2 ) 7→ R is the global reward function. Local rewards ri are
represented as vectors, and R is a matrix with (s1 , a1 ) as rows and (s2 , a2 ) as columns.
Definition 3 differs from the original definition of transition and observation independent DEC-MDP (Becker et al. 2004, Definition 1) in two ways. The modifications allow
us to explicitly capture assumptions that are implicit in previous work. First, the individual MDPs in our model are formulated as stochastic shortest-path problems (Bertsekas &
Tsitsiklis, 1996). That is, there is no explicit time horizon, but instead some states are
terminal. The process stops upon reaching a terminal state. The objective is to maximize
the cumulative reward received before reaching the terminal states.
The second modification of the original definition is that Definition 3 generalizes the
reward structure of the DEC-MDP formulation, using the extended reward structure. The
joint rewards in the original DEC-MDP are defined only for the joint states (s1 ∈ S1 , s2 ∈ S2 )
239

Petrik & Zilberstein

s11

s21

s12

s22

s13

s23

s31

t1

s1

s2

s3
t2

s32

t3

s33

Figure 1: An MDP and its stochastic shortest path version with time horizon 3. The dotted
circles are terminal states.

visited by both agents simultaneously. That is, if agent 1 visits states s11 , s12 and agent 2
visits states s21 , s22 , then the reward can only be defined for joint states (s11 , s21 ) and (s12 , s22 ).
However, our DEC-MDP formulation with extended reward structure also allows the reward
to depend on (s11 , s22 ) and (s12 , s21 ), even when they are not visited simultaneously. As a result,
the global reward may depend on the history, not only on the current state. Note that this
reward structure is more general than what is commonly used in DEC-POMDPs.
We prefer the more general definition because it has been already implicitly used in
previous work. In particular, this extended reward structure arises from introducing the
primitive and compound events in the work of Becker et al. (2004). This reward structure
is necessary to capture the characteristics of the Mars rover benchmark. Interestingly,
this extension does not complicate our proposed solution methods in any way. Note that
the stochastic shortest path formulation (right side of Figure 1) inherently eliminates any
loops because time always advances when an action is taken. Therefore, every state in that
representation may be visited at most once. This property is commonly used when an MDP
is formulated as a linear program (Puterman, 2005).
The solution of a DEC-MDP is a deterministic stationary policy π = (π1 , π2 ), where
πi : Si 7→ Ai is the standard MDP policy (Puterman, 2005) for agent i. In particular, πi (si )
represents the action taken by agent i in state si . To define the bilinear program, we use
variables x(s1 , a1 ) to denote the probability that agent 1 visits state s1 and takes action a1
and y(s2 , a2 ) to denote the same for agent 2. These are the standard dual variables in MDP
formulation. Given a solution in terms of x for agent 1, the policy is calculated for s ∈ S1
as follows, breaking ties arbitrarily.
π1 (s) = arg max x(s, a)
a∈A1

The policy π2 is similarly calculated from y. The correctness of the policy calculation follows
from the existence of an optimal policy that is deterministic and depends only on the local
states of that agent (Becker et al., 2004).
The objective in DEC-MDPs in terms of x and y is then to maximize:
X
X
X X
r1 (s1 , a1 )x(s1 , a1 ) +
R(s1 , a1 , s2 , a2 )x(s1 , a1 )y(s2 , a2 ) +
r2 (s2 , a2 )y(s2 , a2 ).
s1 ∈S1
a1 ∈A1

s1 ∈S1 s2 ∈S2
a1 ∈A1 a2 ∈A2

s2 ∈S2
a2 ∈A2

The stochastic shortest path representation is more general because any finite-horizon MDP
can be represented as such by keeping track of time as part of the state, as illustrated
240

A Bilinear Programming Approach for Multiagent Planning

s11

s21

s12

r1

s13

s22

s14

s23
s24

s25

Figure 2: A sample DEC-MDP.

in Figure 1. This modification allows us to apply the model directly to the Mars rover
benchmark problem. Actions in the Mars rover problem may have different durations,
while all actions in finite-horizon MDPs take the same amount of time.
A DEC-MDP problem with an extended reward structure can be formulated as a bilinear mathematical program as follows. Vector variables x and y represent the state-action
probabilities for each agent, as used in the dual linear formulation of MDPs. Given the transition and observation independence, the feasible regions may be defined by linear equalities
A1 x = α1 and x ≥ 0, and A2 y = α2 and y ≥ 0. The matrices A1 and A2 are the same
as for the dual formulation of total expected reward MDPs (Puterman, 2005), representing
the following equalities for agent i:
X
a0 ∈Ai

X X

x(s0 , a0 ) −

Pi (s, a, s0 )x(s, a) = αi (s0 ),

s∈Si a∈Ai

for every s0 ∈ Si . As described above, variables x(s, a) represent the probabilities of visiting
the state s and taking action a by the appropriate agent during plan execution. Note that
for agent 2, the variables are y(s, a) rather than x(s, a). Intuitively, these equalities ensure
that the probability of entering each non-terminal state, through either the initial step or
from other states, is the same as the probability of leaving the state. The bilinear problem
is then formulated as follows:
maximize
x,y

r1T x + xT Ry + r2T y

subject to A1 x = α1

x≥0

A2 y = α 2

y≥0

(5)

In this formulation, we treat the initial state distributions αi as vectors, based on a fixed
ordering of the states. The following simple example illustrates the formulation.
Example 4. Consider a DEC-MDP with two agents, depicted in Figure 2. The transitions
in this problem are deterministic, and thus all the branches represent actions ai , ordered for
each state from left to right. In some states, only one action is available. The shared reward
r1 , denoted by a dotted line, is received when both agents visit the state. The local rewards
are denoted by the numbers above next to the states. The terminal states are omitted. The
241

Petrik & Zilberstein

agents start in states s11 and s21 respectively. The bilinear formulation of this problem is:
maximize
subject to

x(s14 , a1 ) ∗ r1 ∗ y(s24 , a1 )
x(s11 , a1 )
1
1
x(s2 , a1 ) + x(s2 , a2 ) − x(s11 , a1 )
x(s13 , a1 ) − x(s12 , a1 )
x(s14 , a1 ) − x(s12 , a2 )

=1
=0
=0
=0

y(s21 , a1 ) + y(s21 , a2 )
y(s22 , a1 ) − y(s21 , a1 )
y(s23 , a1 ) − y(s22 , a1 )
y(s24 , a1 ) − y(s22 , a1 )
y(s25 , a1 ) − y(s23 , a1 )

=1
=0
=0
=0
=0

While the results in this paper focus on two-agent problems, our approach can be extended to DEC-MDPs with more than two agents in two ways. The first approach requires
that each component of the global reward depends on at most two agents. The DEC-MDP
then may be viewed as a graph with vertices representing agents and edges representing
the immediate interactions or dependencies. To formulate the problem as a bilinear program, this graph must be bipartite. Interestingly, this class of problems has been previously
formulated (Kim et al., 2006). Let G1 and G2 be the indices of the agents in the two
partitions of the bipartite graph. Then the problem can be formulated as follows:
X
T
maximize
riT xi + xT
i Rij yj + rj yj
x,y

i∈G1 ,j∈G2

subject to Ai xi = α1

x i ≥ 0 i ∈ G1

Aj yj = α2

yj ≥ 0 j ∈ G2

(6)

Here, Rij denotes the global reward for interactions between agents i and j. This program is bilinear and separable because the constraints on the variables in G1 and G2 are
independent.
The second approach to generalize the framework is to represent the DEC-MDP as a
multilinear program. In that case, no restrictions on the reward structure are necessary.
An algorithm to solve, say a trilinear program, could be almost identical to the algorithm
we propose, except that the best response would be calculated using bilinear, not linear
programs. However, the scalability of this approach to more than a few agents is doubtful.
2.2 Average-Reward Infinite-Horizon DEC-MDPs
The previous formulation deals with finite-horizon DEC-MDPs. An average-reward problem may also be formulated as a bilinear program (Petrik & Zilberstein, 2007b). This
is particularly useful for infinite-horizon DEC-MDPs. For example, consider the infinitehorizon version of the Multiple Access Broadcast Channel (MABC) (Rosberg, 1983; Ooi &
Wornell, 1996). In this problem, which has been used widely in recent studies of decentralized decision making, two communication devices share a single channel, and they need to
periodically transmit some data. However, the channel can transmit only a single message
at a time. When both agents send messages at the same time, this leads to a collision, and
the transmission fails. The memory of the devices is limited, thus they need to send the
messages sooner rather than later. We adapt the model from the work of Rosberg (1983),
which is particularly suitable because it assumes no sharing of local information among the
devices.
242

A Bilinear Programming Approach for Multiagent Planning

The definition of average-reward two-agent transition and observation independent DECMDP is the same as Definition 3, with the exception of the terminal states, policy, and objective. There are no terminal states in average-reward DEC-MDPs, and the policy (π1 , π2 )
may be stochastic. That is, πi (s, a) 7→ [0, 1] is the probability of agent i taking an action
a in state s. The objective is to find a stationary infinite-horizon policy π that maximizes
the average reward, or gain, defined as follows.
Definition 5. Let π = (π1 , π2 ) be a stochastic policy, and Xt and Yt be random variables
that represent the probability distributions over the state-action pairs at time t of the two
agents respectively according to π. The gain G of the policy π is then defined for states
s1 ∈ S1 and s2 ∈ S2 as:
"N −1
#
X
1
G(s , s ) = lim
E(s1 ,π1 (s1 )),(s2 ,π2 (s2 ))
r1 (Xt ) + R(Xt , Yt ) + r2 (Yt ) ,
N →∞ N
1

2

t=0

where πi (si ) is the distribution over the actions in state si . Note that the expectation is
with respect to the initial states and action distributions (s1 , π1 (s1 )), (s2 , π2 (s2 )).
The actual gain of a policy depends on the agents’ initial state distributions α1 , α2
and may be expressed as α1T Gα2 , with G represented as a matrix. Puterman (2005), for
example, provides a more detailed discussion of the definition and meaning of policy gain.
To simplify the bilinear formulation of the average-reward DEC-MDP, we assume that
r1 = 0 and r2 = 0. The bilinear program follows.
maximize
p1 ,p2 ,q1 ,q2

τ (p1 , p2 , q1 , q2 ) = pT
1 Rp2

subject to pX
1 , p2 ≥ 0
∀s0 ∈ S1
p1 (s0 , a) −
∀s0 ∈ S1
∀s0 ∈ S2
∀s0 ∈ S2

a∈A
X1
a∈A
X1
a∈A
X2
a∈A2

p1 (s0 , a) +
p2 (s0 , a) −
p2 (s0 , a) +

X


p1 (s, a)P1a s, s0 = 0

s∈S
X1 ,a∈A1

q1 (s0 , a) −

a∈AX
1

X


q1 (s, a)P1a s, s0 = α1 (s0 )

s∈S1 ,a∈A1

p2 (s, a)P2a s, s0

s∈S
X2 ,a∈A2

q2 (s0 , a) −

a∈A2

X

(7)

=0


q2 (s, a)P2a s, s0 = α2 (s0 )

s∈S2 ,a∈A2

The variables in the program come from the dual formulation of the average-reward MDP
linear program (Puterman, 2005). The state sets of the MDPs is divided into recurrent
and transient states. The recurrent states are expected to be visited infinitely many times,
while the transient states are expected to be visited finitely many times. Variables p1 and
p2 represent the limiting distributions of each MDP, which is non-zero for all recurrent
states. The (possibly stochastic) policy πi of agent i is defined in the recurrent states by
the probability of taking action a ∈ Ai in state s ∈ Si :
pi (s, a)
.
0
a0 ∈Ai pi (s, a )

πi (s, a) = P

243

Petrik & Zilberstein

a11

a12

b12

b11

b11

b12

a21

a22

a21

a22

a23

a24

a23

a24

r1

r2

r3

r4

r5

r1

r7

r8

Figure 3: A tree form of a policy for a DEC-POMDP or an extensive game. The dotted
ellipses denote the information sets.

The variables pi are 0 in transient states. The policy in the transient states is calculated
from variables qi as:
qi (s, a)
πi (s, a) = P
.
0
a0 ∈Ai qi (s, a )
The correctness of the constraints follows from the dual formulation of optimal average
reward (Puterman 2005, Equation 9.3.4). Petrik and Zilberstein (2007b) provide further
details of this formulation.
2.3 General DEC-POMDPs and Extensive Games
The general DEC-POMDP problem and extensive-form games with two agents, or players,
can also be formulated as bilinear programs. However, the constraints may not be separable
because actions of one agent influence the other agent. The approach in this case may be
similar to linear complementarity problem formulation of extensive games (Koller, Megiddo,
& von Stengel, 1994), and integer linear program formulation of DEC-POMDPs (Aras
& Charpillet, 2007). The approach we develop is closely related to event-driven DECPOMDPs (Becker et al., 2004), but it is in general more efficient. Nevertheless, the size of
the bilinear program is exponential in the size of the DEC-POMDP. This can be expected
since solving DEC-POMDPs is NEXP-complete (Bernstein et al., 2000), while solving
bilinear programs is NP-complete (Mangasarian, 1995). Because the general formulation in
this case is somewhat cumbersome, we only illustrate it using the following simple example.
Aras (2008) provides the details of a similar construction.
Example 6. Consider the problem depicted in Figure 3, assuming that the agents are
cooperative. The actions of the other agent are not observable, as denoted by the information
sets. This approach can be generalized to any problem with any observable sets as long as
the perfect recall condition is satisfied. Agents satisfy the perfect recall condition when
they remember the set of actions taken in the prior moves (Osborne & Rubinstein, 1994).
Rewards are only collected in the leaf-nodes in this case. The variables on the edges represent
the probability of taking the action. Here, variables a denote the actions of one agent, and
244

A Bilinear Programming Approach for Multiagent Planning

variables b of the other. The total common reward received in the end is:
r = a11 b11 a21 r1 + a11 b11 a22 r2 + a11 b12 a21 r3 + a11 b12 a22 r4 +
a12 b11 a23 r5 + a12 b11 a24 r6 + a12 b12 a23 r7 + a12 b12 a24 r8 .
The constraints in this problem are of the following form: a11 + a12 = 1.
Any DEC-POMDP problem can be represented using the approach used above. It is
also straightforward to extend the approach to problems with rewards in every node. However, the above formulation is clearly not bilinear. To apply our algorithm to this class
of problems, we need to reformulate the problem in a bilinear form. This can be easily
accomplished in a way similar to the construction of the dual linear program for an MDP.
Namely, we introduce variables:
c11 = a11
c12 = a12
c21 = a11 a21
c22 = a11 a22
and so on for every set of variables on any path to a leaf node. Then, the objective may be
reformulated as follows:
r = c21 b11 r1 + c22 b11 r2 + c23 b12 r3 + c24 b12 r4 +
c25 b11 r5 + c26 b11 r6 + c27 b12 r7 + c28 b12 r8 .
Variables bij are replaced in the same fashion. This objective function is clearly bilinear.
The constraints may be reformulated as follows. The constraint a21 + a22 = 1 can be
multiplied by a11 and then replaced by c21 + c22 = c11 , and so on. That is, the variables in
each level have to sum to the variable that is their least common parent in the level above
for the same agent.
2.4 General Two-Player Games
In addition to cooperative problems, some competitive problems with 2 players may be
formulated as bilinear programs. It is known that the problem of finding an equilibrium for
a bi-matrix game may be formulated as a linear complementarity problem (Cottle, Pang,
& Stone, 1992). It has also been shown that a linear complementarity problem may be
formulated as a bilinear problem (Mangasarian, 1995). However, a direct application of
these two reductions results in a complex problem with a large dimensionality. Below, we
demonstrate how a general game can be directly formulated as a bilinear program. There
are many ways to formulate a game, thus we take a very general approach. We simply
assume that each agent optimizes a linear program, as follows.
maximize
x

maximize

d1 (x) = r1T x + xT C1 y

subject to A1 x = b1

y

(8)

d2 (y) = r2T y + xT C2 y

subject to A2 y = b2
y≥0

x≥0
245

(9)

Petrik & Zilberstein

In Eq. (8), the variable y is considered to be a constant and similarly in Eq. (9) the
variable x is considered to be a constant. For normal form games, the constraint matrices
A1 and A2 are simply rows of ones, and b1 = b2 = 1. For competitive DEC-MDPs, the
constraint matrices A1 and A2 are the same as in Section 2.1. Extensive games may be
formulated similarly to DEC-POMDPs, as described in Section 2.3.
The game specified by linear programs Eq. (8) and Eq. (9) may be formulated as a
bilinear program as follows. First, define the reward vectors for each agent, given a policy
of the other agent.
q1 (y) = r1 + C1 y
q2 (x) = r2 + C2T x.

These values are unrelated to those of Eq. (7). The complementary slackness values (Vanderbei, 2001) for the linear programs Eq. (8) and Eq. (9) are:


k1 (x, y, λ1 ) = q1 (y)T − λT
1 A1 x


k2 (x, y, λ2 ) = q2 (x)T − λT
A
2 2 y,
where λ1 and λ2 are the dual variables of the corresponding linear programs. For any
primal feasible x and y, and dual feasible λ1 and λ2 , we have that k1 (x, y, λ1 ) ≥ 0 and
k2 (x, y, λ2 ) ≥ 0. The equality is attained if and only if x and y are optimal. This can
be used to write the following optimization problem, in which we implicitly assume that
x,y,λ1 ,λ2 are feasible in the appropriate primal and dual linear programs:
0≤
=
=
=
=

min k1 (x, y, λ1 ) + k2 (x, y, λ2 )

x,y,λ1 ,λ2

T
T
min (q1 (y)T − λT
1 A1 )x + (q2 (x) − λ2 A2 )y

x,y,λ1 ,λ2

T T
T
min ((r1 + C1 y)T − λT
1 A1 )x + ((r2 + C2 x) − λ2 A2 )y

x,y,λ1 ,λ2

T T
min r1T x + r2T y + xT (C1 + C2 )y − xT AT
1 λ 1 − y A2 λ 2

x,y,λ1 ,λ2

T
min r1T x + r2T y + xT (C1 + C2 )y − bT
1 λ1 − b2 λ2 .

x,y,λ1 ,λ2

Therefore, any feasible x and y that set the right hand side to 0 solve both linear programs
in Eq. (8) and Eq. (9) optimally. Adding the primal and dual feasibility conditions to the
above, we get the following bilinear program:
minimize
x,y,λ1 ,λ2

T
r1T x + r2T y + xT (C1 + C2 )y − bT
1 λ1 − b2 λ2

subject to A1 x = b1

A2 y = b2

r1 + C1 y − AT
1 λ1 ≤ 0
r2 + C2T x − AT
2 λ2 ≤ 0
x≥0

y≥0
246

(10)

A Bilinear Programming Approach for Multiagent Planning

Algorithm 1: IterativeBestResponse(B)
x0 , w0 ← rand ;
i←1;
while yi−1 6= yi or xi−1 6= xi do
(yi , zi ) ← arg maxy,z f (wi−1 , xi−1 , y, z) ;
(xi , wi ) ← arg maxx,w f (w, x, yi , zi ) ;
6
i←i+1

1
2
3
4
5

7

return f (wi , xi , yi , zi )

The optimal solution of Eq. (10) is 0 and it corresponds to a Nash equilibrium. This
is because both the primal variables x, y and dual variables λ1 , λ2 are feasible and the
complementary slackness condition is satisfied. The open question in this example are the
interpretation of an approximate result and a formulation that would select the equilibrium.
It is not clear yet whether it is possible to formulate the program so that the optimal solution
will be a Nash equilibrium that maximizes a certain criterion. The approximate solutions
of the program probably correspond to -Nash equilibria, but this remain an open question.
The algorithm in this case also relies on the number of shared rewards being small
compared to the size of the problem. But even if this is not the case, it is often possible
that the number of shared rewards may be automatically reduced as described in Section 4.
In fact, it is easy to show that a zero-sum normal form game is automatically reduced to
two uncoupled linear programs. This follows from the dimensionality reduction procedure
in Section 4.

3. Solving Bilinear Programs
One simple method often used for solving bilinear programs is the iterative procedure shown
in Algorithm 1. The parameter B represents the bilinear program. While the algorithm
often performs well in practice, it tends to converge to a suboptimal solution (Mangasarian,
1995). When applied to DEC-MDPs, this algorithm is essentially identical to JESP (Nair,
Tambe, Yokoo, Pynadath, & Marsella, 2003)–one of the early solution methods. In the
following, we use f (w, x, y, z) to denote the objective value of Eq. (1).
The rest of this section presents a new anytime algorithm for solving bilinear programs.
The goal of the algorithm to is to produce a good solution quickly and then improve the
solution in the remaining time. Along with each approximate solution, the maximal approximation bound with respect to the optimal solution is provided. As we show below, our
algorithm can benefit from results produced by suboptimal algorithms, such as Algorithm 1,
to quickly determine tight approximation bounds.
3.1 The Successive Approximation Algorithm
We begin with an overview of a successive approximation algorithm for bilinear problems
that takes advantage of a low number of interactions between the agents. It is particularly
suitable when the input problem is large in comparison to its dimensionality, as defined in
Section 2. We address the issue of dimensionality reduction in Section 4.
247

Petrik & Zilberstein

We begin with a simple intuitive explanation of the algorithm, and then show how it
can be formalized. The bilinear program can be seen as an optimization game played by
two agents, in which the first agent sets the variables w, x and the second one sets the
variables y, z. This is a general observation that applies to any bilinear program. In any
practical application, the feasible sets for the two sets of variables may be too large to
explore exhaustively. In fact, when this method is applied to DEC-MDPs, these sets are
infinite and continuous. The basic idea of the algorithm is to first identify the set of best
responses of one of the agents, say agent 1, to some policy of the other agent. This is
simple because once the variables of agent 2 are fixed, the program becomes linear, which
is relatively easy to solve. Once the set of best-response policies of agent 1 is identified,
assuming it is of a reasonable size, it is possible to calculate the best response of agent 2.
This general approach is also used by the coverage set algorithm (Becker et al., 2004).
One distinction is that the representation used in CSA applies only to DEC-MDPs, while
our formulation applies to bilinear programs–a more general representation. The main
distinction between our algorithm and CSA is the way in which the variables y, z are chosen.
In CSA, the values y, z are calculated in a way that simply guarantees termination in
finite time. We, on the other hand, choose values y, z greedily so as to minimize the
approximation bound on the optimal solution. This is possible because we establish bounds
on the optimality of the solution throughout the calculation. As a result, our algorithm
converges more rapidly and may be terminated at any time with a guaranteed performance
bound. Unlike the earlier version of the algorithm (Petrik & Zilberstein, 2007a), the version
described in this paper calculates the best response using only a subset of the values of y, z.
As we show, it is possible to identify regions of y, z in which it is impossible to improve the
current best solution and exclude these regions from consideration.
We now formalize the ideas described above. To simplify the notation, we define feasible
sets as follows:
X = {(x, w) A1 x + B1 w = b1 }
Y

= {(y, z) A2 y + B2 z = b2 }.

We use y ∈ Y to denote that there exists z such that (y, z) ∈ Y . In addition, we assume that
the problem is in a semi-compact form. This is reasonable because any bilinear program
may be converted to semi-compact form with an increase in dimensionality of one, as we
have shown earlier.
Assumption 7. The sets X and Y are bounded, that is, they are contained in a ball of a
finite radius.
While Assumption 7 is limiting, coordination problems under uncertainty typically have
bounded feasible sets because the variables correspond to probabilities bounded to [0, 1].
Assumption 8. The bilinear program is in a semi-compact form.
The main idea of the algorithm is to compute a set X̃ ⊆ X that contains only those
elements that satisfy a necessary optimality condition. The set X̃ is formally defined as
follows:


∗
∗
∗ ∗
X̃ ⊆ (x , w ) ∃(y, z) ∈ Y f (w , x , y, z) = max f (w, x, y, z) .
(x,w)∈X

248

A Bilinear Programming Approach for Multiagent Planning

As described above, this set may be seen as a set of best responses of one agent to the
variable settings of the other. The best responses are easy to calculate since the bilinear
program in Eq. (1) reduces to a linear program for fixed w, x or fixed y, z. In our algorithm,
we assume that X̃ is potentially a proper subset of all necessary optimality points and
focus on the approximation error of the optimal solution. Given the set X̃, the following
simplified problem is solved.
maximize
w,x,y,z

f (w, x, y, z)

subject to (x, w) ∈ X̃

(11)

A2 y + B2 z = b2
y, z ≥ 0

Unlike the original continuous set X, the reduced set X̃ is discrete and small. Thus the
elements of X̃ may be enumerated. For a fixed w and x, the bilinear program in Eq. (11)
reduces to a linear program.
To help compute the approximation bound and to guide the selection of elements for
X̃, we use the best-response function g(y), defined as follows:
g(y) =

max

{w,x,z (x,w)∈X,(y,z)∈Y }

f (w, x, y, z) =

max

{x,w (x,w)∈X}

f (w, x, y, 0),

with the second equality for semi-compact programs only and feasible y ∈ Y . Note that
g(y) is also defined for y ∈
/ Y , in which case the choice of z is arbitrary since it does
not influence the objective function. The best-response function is easy to calculate using
a linear program. The crucial property of the function g that we use to calculate the
approximation bound is its convexity. The following proposition holds because g(y) =
max{x,w (x,w)∈X} f (w, x, y, 0) is a maximum of a finite set of linear functions.
Proposition 9. The function g(y) is convex when the program is in a semi-compact form.
Proposition 9 relies heavily on the separability of Eq. (1), which means that the constraints on the variables on one side of the bilinear term are independent of the variables on
the other side. The separability ensures that w, x are valid solutions regardless of the values
of y, z. The semi-compactness of the program is necessary to establish convexity, as shown
in Example 23 in Appendix C. The example is constructed using the properties described
in the appendix, which show that f (w, x, y, z) may be expressed as a sum of a convex and
a concave function.
We are now ready to describe Algorithm 2, which computes the set X̃ for a bilinear
problem B such that the approximation error is at most 0 . The algorithm iteratively adds
the best response (x, w) for a selected pivot point y into X̃. The pivot points are selected
hierarchically. At an iteration j, the algorithm keeps a set of polyhedra S1 . . . Sj which
represent the triangulation of the feasible space Y , which is possible based on Assumption 7.
For each polyhedron Si = (y1 . . . yn+1 ), the algorithm keeps a bound i on the maximal
difference between the optimal solution on the polyhedron and the best solution found so
far. This error bound on a polyhedron Si is defined as:
i = e(Si ) =

max

{w,x,y|(x,w)∈X,y∈Si }

f (w, x, y, 0) −

max
{w,x,y|(x,w)∈X̃,y∈Si }

249

f (w, x, y, 0),

Petrik & Zilberstein

Algorithm 2: BestResponseApprox(B, 0 ) returns (w, x, y, z)
1

2
3
4
5
6
7

8
9
10
11
12

13

14

// Create the initial polyhedron S1 .
S1 ← (y1 . . . yn+1 ), Y ⊆ S1 ;
// Add best-responses for vertices of S1 to X̃
X̃ ← {arg max(x,w)∈X f (w, x, y1 , 0), ..., arg max(x,w)∈X f (w, x, yn+1 , 0)} ;
// Calculate the error  and pivot point φ of the initial polyhedron
(1 , φ1 ) ← P olyhedronError(S1 ) ;
// Section 3.2,Section 3.3
// Initialize the number of polyhedra to 1
j←1;
// Continue until reaching a predefined precision 0
while maxi=1,...,j i ≥ 0 do
// Find the polyhedron with the largest error
i ← arg maxk=1,...,j k ;
// Select the pivot point of the polyhedron with the largest error
y ← φi ;
// Add the best response to the pivot point y to the set X̃
X̃ ← X ∪ {arg max(x,w)∈X f (w, x, y, 0)} ;
// Calculate errors and pivot points of the refined polyhedra
for k = 1, . . . , n + 1 do
j ←j+1 ;
// Replace the k-th vertex by the pivot point y
Sj ← (y, y1 . . . yk−1 , yk+1 , . . . yn+1 ) ;
(j , φj ) ← P olyhedronError(Sj ) ;
// Section 3.2,Section 3.3
// Take the smaller of the errors on the original and the refined
polyhedron. The error may not increase with the refinement,
although the bound may
j ← min{i , j } ;
// Set the error of the refined polyhedron to 0, since the region is
covered by the refinements
i ← 0 ;

(w, x, y, z) ← arg max{w,x,y,z
16 return (w, x, y, z) ;

15

(x,w)∈X̃,(y,z)∈Y }

f (w, x, y, 0) ;

where X̃ represents the current, not final, set of best responses.
Next, a point y0 is selected as described below and n + 1 new polyhedra are created
by replacing one of the vertices by y0 to get: (y0 , y2 , . . .), (y1 , y0 , y3 , . . .), . . . , (y1 , . . . , yn , y0 ).
This is depicted for a 2-dimensional set Y in Figure 4. The old polyhedron is discarded
and the above procedure is then repeatedly applied to the polyhedron with the maximal
approximation error.
For the sake of clarity, the pseudo-code of Algorithm 2 is simplified and does not address
any efficiency issues. In practice, g(yi ) could be cached, and the errors i could be stored in
a prioritized heap or at least in a sorted array. In addition, a lower bound li and an upper
bound ui is calculated and stored for each polyhedron Si = (y1 . . . yn+1 ). The function e(Si )
calculates their maximal difference on the polyhedron Si and the point where it is attained.
The error bound i on the polyhedron Si may not be tight, as we describe in Remark 13.
As a result, when the polyhedron Si is refined to n polyhedra S10 . . . Sn0 with online error
250

A Bilinear Programming Approach for Multiagent Planning

y3

y0
y1

y2

Figure 4: Refinement of a polyhedron in two dimensions with a pivot y0 .
bounds 01 . . . 0n , it is possible that for some k: 0k > i . Since S10 . . . Sn0 ⊆ Si , the true error
on Sk0 is less than on Si and therefore 0k may be set to i .
Conceptually, the algorithm is similar to CSA, but there are some important differences.
The main difference is in the choice of the pivot point y0 and the bounds on g. CSA does not
keep any upper bound and it evaluates g(y) on all the intersection points of planes defined
by the current solutions in X̃. That guarantees that g(y) is eventually known precisely
(Becker et al., 2004). A similar approach was also taken for POMDPs (Cheng, 1988). The
|X̃| 
upper bound on the number of intersection points in CSA is dim
Y . The principal problem
is that the bound is exponential in the dimension of Y , and experiments do not show a
slower growth in typical problems. In contrast, we choose the pivot points to minimize
the approximation error. This is more selective and tends to more rapidly reduce the error
bound. In addition, the error at the pivot point may be used to determine the overall
error bound. The following proposition states the soundness of the triangulation, proved
in Appendix A. The correctness of the triangulation establishes that in each iteration the
approximation error over Y is equivalent to the maximum of the approximation errors over
the current polyhedra S1 . . . Sj .
Proposition 10. In the proposed triangulation, the sub-polyhedra do not overlap and they
cover the whole feasible set Y , given that the pivot point is in the interior of S.

3.2 Online Error Bound
The selection of the pivot point plays a key role in the performance of the algorithm, in both
calculating the error bound and the speed of convergence to the optimal solution. In this
section we show exactly how we use the triangulation in the algorithm to calculate an error
bound. To compute the approximation bound, we define the approximate best-response
function g̃(y) as:
g̃(y) =
max
f (w, x, y, 0).
{x,w (x,w)∈X̃}

Notice that z is not considered in this expression, since we assume that the bilinear program is in the semi-compact form. The value of the best approximate solution during the
execution of the algorithm is:
max

f (w, x, y, 0) = max g̃(y).
y∈Y

{w,x,y,z (x,w)∈X̃,y∈Y }

251

Petrik & Zilberstein

This value can be calculated at runtime when each new element of X̃ is added. Then the
maximal approximation error between the current solution and the optimal one may be
calculated from the approximation error of the best-response function g(·), as stated by the
following proposition.
Proposition 11. Consider a bilinear program in a semi-compact form. Then let w̃, x̃, ỹ
be an optimal solution of Eq. (11) and let w∗ , x∗ , y ∗ be an optimal solution of Eq. (1). The
approximation error is then bounded by:
f (w∗ , x∗ , y ∗ , 0) − f (w̃, x̃, ỹ, 0) ≤ max (g(y) − g̃(y)) .
y∈Y

Proof.
f (w∗ , x∗ , y ∗ , 0) − f (w̃, x̃, ỹ, 0) = max g(y) − max g̃(y) ≤ max g(y) − g̃(y)
y∈Y

y∈Y

y∈Y

Now, the approximation error is maxy∈Y g(y) − g̃(y), which is bounded by the difference
between an upper bound and a lower bound on g(y). Clearly, g̃(y) is a lower bound on
g(y). Given points in which g̃(y) is the same as the best-response function g(y), we can use
Jensen’s inequality to obtain the upper bound. This is summarized by the following lemma.
Lemma
12.Let yi ∈ Y for i = 1, . . . , n + 1 such that g̃(yi ) = g(yi ). Then
P
Pn+1
Pn+1
n+1
g
i=1 ci yi ≤
i=1 ci g(yi ) when
i=1 ci = 1 and ci ≥ 0 for all i.
The actual implementation of the bound relies on the choice of the pivot points. Next
we describe the maximal error calculation on a single polyhedron defined by S = (y1 . . . yn ).
Let matrix T have yi as columns, and let L = {x1 . . . xn+1 } be the set of the best responses
for its vertices. The matrix T is used to convert any y in absolute coordinates to a relative
representation t that is a convex combination of the vertices. This is defined formally as
follows:


...
y = T t = y1 y2 . . . t
...
1 = 1T t
0 ≤ t
where the yi ’s are column vectors.
We can represent a lower bound l(y) for g̃(y) and an upper bound u(y) for g(y) as:
l(y) = max rT x + xT Cy
x∈L

u(y) = [g(y1 ), g(y2 ), . . .]T t = [g(y1 ), g(y2 ), . . .]T



T
1T

−1  
y
,
1

The upper bound correctness follows from Lemma 12. Notice that u(y) is a linear function,
which enables us to use a linear program to determine the maximal-error point.
252

A Bilinear Programming Approach for Multiagent Planning

Algorithm 3: PolyhedronError(B, S)
P ← one of Eq. (12), or (13), or (14), or (20) ;
t ← the optimal solution of P ;
 ← the optimal objective value of P ;
// Coordinates t are relative to the vertices of S, convert them to absolute
values in Y
4 φ ← Tt ;
5 return (, φ) ;

1
2
3

Remark 13. Notice that we use L instead of X̃ in calculating l(y). Using all of X̃ would
lead to a tighter bound, as it is easy to show in three-dimensional examples. However, this
also would substantially increase the computational complexity.
Now, the error on a polyhedron S may be expressed as:
e(S) ≤ max u(y) − l(y) = max u(y) − max rT x + xT Cy
y∈S

y∈S

T

x∈L

T

= max min u(y) − r x − x Cy.
y∈S x∈L

We also have


y ∈ S ⇔ y = T t ∧ t ≥ 0 ∧ 1T t = 1 .
As a result, the point with the maximal error bound may be determined using the following
linear program in terms of variables t, :
maximize
t,



subject to  ≤ u(T t) − rT x − xT CT t

∀x ∈ L

(12)

1T t = 1 t ≥ 0
Here x is not a variable. The formulation is correct because all feasible solutions are
bounded below the maximal error and any maximal-error solution is feasible.
Proposition 14. The optimal solution of Eq. (12) is equivalent to maxy∈S |u(y) − l(y)|.
We thus select the next pivot point to greedily minimize the error. The maximal difference is actually achieved in points where some of the planes meet, as Becker et al. (2004)
have suggested. However, checking these intersections is very similar to running the simplex
algorithm. In general, the simplex algorithm is preferable to interior point methods for this
program because of its small size (Vanderbei, 2001).
Algorithm 3 shows a general way to calculate the maximal error and the pivot point on
the polyhedron S. This algorithm may use the basic formulation in Eq. (12), or the more
advanced formulations in Eqs. (13), (14), and (20) defined in Section 3.3.
In the following section, we describe a more refined pivot point selection method that
can in some cases dramatically improve the performance.
253

Petrik & Zilberstein

20

15

h10
5
−6

Yh

−4

−2

0

2

4

Yh

6

Figure 5: The reduced set Yh that needs to be considered for pivot point selection.
3.3 Advanced Pivot Point Selection
As described above, the pivot points are chosen greedily to both determine the maximal
error in each polyhedron and to minimize the approximation error. The basic approach
described in Section 3.1 may be refined, because the goal is not to approximate the function
g(y) with the least error, but to find the optimal solution. Intuitively, we can ignore those
regions of Y that will not guarantee any improvement of the current solution, as illustrated
in Figure 5. As we show below, the search for the maximal error point could be limited to
this region as well.
We first define a set Yh ⊆ Y that we will search for the maximal error, given that the
optimal solution f ∗ ≥ h.
Yh = {y g(y) ≥ h, y ∈ Y }.
The next proposition states that the maximal error needs to be calculated only in a superset
of Yh .
Proposition 15. Let w̃, x̃, ỹ, z̃ be the approximate optimal solution and w∗ , x∗ , y ∗ , z ∗ be the
optimal solution. Also let f (w∗ , x∗ , y ∗ , z ∗ ) ≥ h and assume some Ỹh ⊇ Yh . The approximation error is then bounded by:
f (w∗ , x∗ , y ∗ , z ∗ ) − f (w̃, x̃, ỹ, z̃) ≤ max g(y) − g̃(y).
y∈Ỹh

Proof. First, f (w∗ , x∗ , y ∗ , z ∗ ) = g(y ∗ ) ≥ h and thus y ∗ ∈ Yh . Then:
f (w∗ , x∗ , y ∗ , z ∗ ) − f (w̃, x̃, ỹ, z̃) = max g(y) − max g̃(y)
y∈Yh

y∈Y

≤ max g(y) − g̃(y)
y∈Yh

≤ max g(y) − g̃(y)
y∈Ỹh

Proposition 15 indicates that the point with the maximal error needs to be selected only
from the set Yh . The question is how to easily identify Yh . Because the set is not convex in
general, a tight approximation of this set needs to be found. In particular, we use methods
254

A Bilinear Programming Approach for Multiagent Planning

that approximate the intersection of a superset of Yh with the polyhedron that is being
refined, using the following methods:
1. Feasibility [Eq. (13)]: Require that pivot points are feasible in Y .
2. Linear bound [Eq. (14)]: Use the linear upper bound u(y) ≥ h.
3. Cutting plane [Eq. (20)]: Use the linear inequalities that define YhC , where
YhC = R|Y | \ Yh is the complement of Yh .
Any combination of these methods is also possible.
Feasibility The first method is the simplest, but also the least constraining. The linear
program to find the pivot point with the maximal error bound is as follows:
maximize
,t,y,z



subject to  ≤ u(T t) − rT x + xT CT t

∀x ∈ L

1T t = 1 t ≥ 0

(13)

y = Tt
A2 y + B2 z = b2
y, z ≥ 0

This approach does not require that the bilinear program is in the semi-compact form.
Linear Bound The second method, using the linear bound, is also very simple to implement and compute, and it is more selective than just requiring feasibility. Let:
Ỹh = {y u(y) ≥ h} ⊇ {y g(y) ≥ h} = Yh .
This set is convex and thus does not need to be approximated. The linear program used to
find the pivot point with the maximal error bound is as follows:
maximize
,t



subject to  ≤ u(T t) − rT x + xT CT t

∀x ∈ L

1T t = 1 t ≥ 0

(14)

u(T t) ≥ h
The difference from Eq. (12) is the last constraint. This approach requires that the bilinear
program is in the semi-compact form to ensure that u(y) is a bound on the total return.
Cutting Plane The third method, using the cutting plane elimination, is the most computationally intensive one, but also the most selective one. Using this approach requires
additional assumptions on the other parts of the algorithm, which we discuss below. The
method is based on the same principle as α-extensions in concave cuts (Horst & Tuy, 1996).
We start with the set YhC because it is convex and may be expressed as:


T
T T
T
max sT
w
+
r
x
+
y
C
x
+
r
y
≤ h
(15)
1
1
2
w,x

255

A1 x + B1 w = b1

(16)

w, x ≥ 0

(17)

Petrik & Zilberstein

y3

f1
y1
y2
f2

Y − Yh

Figure 6: Approximating Yh using the cutting plane elimination method.
To use these inequalities in selecting the pivot point, we need to make them linear. But
there are two obstacles: Eq. (15) contains a bilinear term and is a maximization. Both
of these issues can be addressed by using the dual formulation of Eq. (15). The corresponding linear program and its dual for fixed y, ignoring constants h and r2T y, are:
maximize
w,x

T
T T
sT
1 w + r1 x + y C x

subject to A1 x + B1 w = b1

minimize
(18)

λ

bT
1λ

subject to AT
1 λ ≥ r1 + Cy

w, x ≥ 0

(19)

B1T λ ≥ s1

Using the dual formulation, Eq. (15) becomes:


T
T
min b1 λ + r2 y
≤ h
λ

AT
1 λ ≥ r1 + Cy
B1T λ ≥ s1
Now, we use that for any function φ and any value θ the following holds:
min φ(x) ≤ θ ⇔ (∃x) φ(x) ≤ θ.
x

Finally, this leads to the following set of inequalities.
r2T y ≤ h − bT
1λ
Cy ≤ AT
1 λ − r1
s1 ≤ B1T λ
The above inequalities define the convex set YhC . Because its complement Yh is not
necessarily convex, we need to use its convex superset Ỹh on the given polyhedron. This
is done by projecting YhC , or its subset, onto the edges of each polyhedron as depicted in
Figure 6 and described in Algorithm 4. The algorithm returns a single constraint which
cuts off part of the set YhC . Notice that only the combination of the first n points fk is
256

A Bilinear Programming Approach for Multiagent Planning

Algorithm 4: PolyhedronCut({y1 , . . . , yn+1 }, h) returns constraint σ T y ≤ τ
1
2

3
4
5
6
7
8
9

// Find vertices of the polyhedron {y1 , . . . , yn+1 } inside of YhC
I ← {yi yi ∈ YhC } ;
// Find vertices of the polyhedron outside of YhC
O ← {yi yi ∈ Yh } ;
// Find at least n points fk in which the edge of Yh intersects an edge of
the polyhedron
k←1;
for i ∈ O do
for j ∈ I do
fk ← yj + maxβ {β β(yi − yj ) ∈ (YhC )} ;
k ←k+1 ;
if k ≥ n then
break ;

Find σ and τ , such that [f1 , . . . , fn ]σ = τ and 1T σ = 1 ;
// Determine the correct orientation of the constraint to have all y in Yh
feasible
11 if ∃yj ∈ O, and σ T yj > τ then
// Reverse the constraint if it points the wrong way
12
σ ← −σ ;
13
τ ← −τ ;
10

14

return σ T y ≤ τ

used. In general, there may be more than n points, and any subset of points fk of size n can
be used to define a new cutting plane that constraints Yh . This did not lead to significant
improvements in our experiments. The linear program to find the pivot point with the
cutting plane option is as follows:
maximize
,t,y



subject to  ≤ u(T t) − rT x + xT CT t
1T t = 1 t ≥ 0

∀x ∈ L
(20)

y = Tt
σTy ≤ τ
Here, σ, and τ are obtained as a result of running Algorithm 4.
Note that this approach requires that the bilinear program is in the semi-compact form
to ensure that g(y) is convex. The following proposition states the correctness of this
procedure.
Proposition 16. The resulting polyhedron produced by Algorithm 4 is a superset of the
intersection of the polyhedron S with the complement of Yh .
Proof. The convexity of g(y) implies that YhC is also convex. Therefore, the intersection
Q = {y σ T y ≥ τ } ∩ S
257

Petrik & Zilberstein

is also convex. It is also a convex hull of points fk ∈ YhC . Therefore, from the convexity of
YhC , we have that Q ⊆ YhC , and therefore S − Q ⊇ Yh .

4. Dimensionality Reduction
Our experiments show that the efficiency of the algorithm depends heavily on the dimensionality of the matrix C in Eq. (1). In this section, we show the principles behind automatically
determining the necessary dimensionality of a given problem. Using the proposed procedure, it is possible to identify weak interactions and eliminate them. Finally, the procedure
works for arbitrary bilinear programs and is a generalization of a method we have previously
introduced (Petrik & Zilberstein, 2007a).
The dimensionality is inherently part of the model, not the problem itself. There may be
equivalent models of a given problem with very different dimensionality. Thus, procedures
for reducing the dimensionality are not necessary when the modeler can create a model
with minimal dimensionality. However, this is nontrivial in many cases. In addition, some
dimensions may have little impact on the overall performance. To determine which ones
can be discarded, we need a measure of their contribution that can be computed efficiently.
We define these notions more formally later in this section.
We assume that the feasible sets have bounded L2 norms, and assume a general formulation of the bilinear program, not necessarily in the semi-compact form. Given Assumption 7,
this can be achieved by scaling the constraints when the feasible region is bounded.
Assumption 17. For all x ∈ X and y ∈ Y , their norms satisfy kxk2 ≤ 1 and kyk2 ≤ 1.
We discuss the implications of and problems with this assumption after presenting Theorem 18. Intuitively, the dimensionality reduction removes those dimensions where g(y) is
constant, or almost constant. Interestingly, these dimensions may be recovered based on
the eigenvectors and eigenvalues of C T C. We use the eigenvectors of C T C instead of the
eigenvectors of C, because our analysis is based on L2 norm of x and y and thus of C.
The L2 norm kCk2 is bounded by the largest eigenvalue of C T C. In addition, a symmetric
matrix is required to ensure that the eigenvectors are perpendicular and span the whole
space.
Given a problem represented using Eq. (1), let F be a matrix whose columns are all the
eigenvectors of C T C with eigenvalues greater than some λ̄. Let G be a matrix with all the
remaining eigenvectors as columns. Notice that together, the columns of the matrices span
the whole space and are real-valued, since C T C is a symmetric matrix. Assume without
loss of generality that the eigenvectors are unitary. The compressed version of the bilinear
program is then the following:
maximize
w,x,y1 ,y2 ,z

T
T
f˜(w, x, y1 , y2 , z) = r1T x + sT
2 w + x CF y1 + r2 F

subject to A1 x + B1 w = b
 
 y1
A2 F G
+ B2 z = b2
y2
w, x, y1 , y2 , z ≥ 0
258

 
 y1
G
+ sT
2z
y2
(21)

A Bilinear Programming Approach for Multiagent Planning

Notice that the program is missing the element xT CGy2 , which would make its optimal
solutions identical to the optimal solutions of Eq. (1). We describe a more practical approach
to reducing the dimensionality in Appendix B. This approach is based on singular value
decomposition and may be directly applied to any bilinear program. The following theorem
quantifies the maximum error when using the compressed program.
Theorem 18. Let f ∗ and f˜∗ be optimal solutions of Eq. (1) and Eq. (21) respectively. Then:
p
 = |f ∗ − f˜∗ | ≤ λ̄.
Moreover, this is the maximal linear dimensionality reduction possible with this error without
considering the constraint structure.
√
Proof. We first show that indeed the error is at most λ̄ and that any linearly compressed
problem with the given error has at least f dimensions. Using a mapping that preserves
the feasibility of both programs, the error is bounded by:

  

 
 y1
y
 ≤ f w, x, F G
, z − f˜ w, x, y1 , 2
= xT CGy2 .
y2
z
Denote the feasible region of y2 as Y2 . From the orthogonality of [F, G], we have that
ky2 k2 ≤ 1 as follows:
 
 y1
y = F G
y2
 
 T
y1
F
y =
y2
GT
GT y = y2
kGT yk2 = ky2 k2
Then we have:
 ≤
≤

max max xT CGy2 ≤ max kCGy2 k2
y2 ∈Y2
q
q
p
λ̄
max y2T GT C T CGy2 ≤ max y2T Ly2 ≤

y2 ∈Y2 x∈X

y2 ∈Y2

y2 ∈Y2

The result follows from Cauchy-Schwartz inequality, the fact that C T C is symmetric, and
Assumption 17. The matrix L denotes a diagonal matrix of eigenvalues corresponding to
eigenvectors of G.
Now, let H be an arbitrary matrix that satisfies the preceding error inequality for G.
Clearly, H ∩ F = ∅, otherwise ∃y, kyk2 = 1, such that kCHyk2 > . Therefore, we have
|H| ≤ n − |F | ≤ |G|, because |H| + |F | = |Y |. Here | · | denotes the number of columns of
the matrix.
Alternatively, the bound can be proved by replacing the equality A1 x + B1 w = b1 by
kxk2 = 1. The bound can then be obtained by Lagrange necessary optimality conditions. In
these bounds we use L2 -norm; an extension to a different norm is not straightforward. Note
259

Petrik & Zilberstein

Y

ŷ

kyk2 ≤ 1

Figure 7: Approximation of the feasible set Y according to Assumption 17.
also that this dimensionality reduction technique ignores the constraint structure. When
the constraints have some special structure, it might be possible to obtain an even tighter
bound. As described in the next section, the dimensionality reduction technique generalizes
the reduction that Becker et al. (2004) used implicitly.
The result of Theorem 18 is based on an approximation of the feasible set Y by kyk2 ≤ 1,
as Assumption 17 states. This approximation may be quite loose in some problems, which
may lead to a significant multiplicative overestimation of the bound in Theorem 18. For
example, consider the feasible set depicted in Figure 7. The bound may be achieved in a
point ŷ, which is far from the feasible region. In specific problems, a tighter bound could be
obtained by either appropriately scaling the constraints, or using a weighted L2 with a better
precision. We partially address this issue by considering the structure of the constraints.
To derive this, consider the following linear program and corresponding theorem:
maximize
x

cT x

subject to Ax = b

(22)

x≥0

Theorem 19. The optimal solution of Eq. (22) is the same as when the objective function
is modified to
cT (I − AT (AAT )−1 A)x,
where I is the identity matrix.
Proof. The objective function is:
max

{x Ax=b, x≥0}

cT x =
=

max

{x Ax=b, x≥0}

cT (I − AT (AAT )−1 A)x + cT AT (AAT )−1 Ax

= cT AT (AAT )−1 b +

max

{x Ax=b, x≥0}

cT (I − AT (AAT )−1 A)x.

The first term may be ignored because it does not depend on the solution x.
260

A Bilinear Programming Approach for Multiagent Planning

The following corollary shows how the above theorem can be used to strengthen the
dimensionality reduction bound. For example, in zero-sum games, this stronger dimensionality reduction splits the bilinear program into two linear programs.
Corollary 20. Assume that there are no variables w and z in Eq. (1). Let:
T −1
Qi = (I − AT
i (Ai Ai ) Ai )),

i ∈ {1, 2},

where Ai are defined in Eq. (1). Let C̃ be:
C̃ = Q1 CQ2 ,
where C is the bilinear-term matrix from Eq. (1). Then the bilinear programs will have
identical optimal solutions with either C or C̃.
Proof. Using Theorem 19, we can modify the original objective function in Eq. (1) to:
T −1
T
T −1
T
f (x, y) = r1T x + xT (I − AT
1 (A1 A1 ) A1 ))C(I − A2 (A2 A2 ) A2 ))y + r2 y.

For the sake of simplicity we ignore the variables w and z, which do not influence the bilinear
T −1
term. Because both (I − AT
i (Ai Ai ) Ai ) for i = 1, 2 are orthogonal projection matrices,
none of the eigenvalues in Theorem 18 will increase.
The dimensionality reduction presented in this section is related to the idea of compound events used in CSA. Allen, Petrik, and Zilberstein (2008a, 2008b) provide a detailed
discussion of this issue.

5. Offline Bound
In this section we develop an approximation bound that depends only on the number of
points for which g(y) is evaluated and the structure of the problem. This kind of bound
is useful in practice because it provides performance guarantees without actually solving
the problem. In addition, the bound reveals which parameters of the problem influence the
algorithm’s performance. The bound is derived based on the maximal slope of g(y) and the
maximal distance among the points.
Theorem 21. To achieve an approximation error of at most , the number of points to be
evaluated in a regular grid with k points in every dimension must satisfy:

√ n
kCk2 n
n
k ≥
,

where n is the number of dimensions of Y .
The theorem follows using basic algebraic manipulations from the following lemma.
Lemma 22. Assume that for each y1 ∈ Y there exists y2 ∈ Y such that ky1 − y2 k2 ≤ δ and
g̃(y2 ) = g(y2 ). Then the maximal approximation error is:
 = max g(y) − g̃(y) ≤ kCk2 δ.
y∈Y

261

Petrik & Zilberstein

Proof. Let y1 be a point where the maximal error is attained. This point is in Y , because
this set is compact. Now, let y2 be the closest point to y1 in L2 norm. Let x1 and x2 be
the best responses for y1 and y2 respectively. From the definition of solution optimality we
can derive:
T
T
T
r1T x1 + r2T y2 + xT
1 Cy2 ≤ r1 x2 + r2 y2 + x2 Cy2

r1T (x1 − x2 ) ≤ −(x1 − x2 )T Cy2 .
The error now can be expressed, using the fact that kx1 − x2 k2 ≤ 1, as:
T
T
T
 = r1T x1 + r2T y1 + xT
1 Cy1 − r1 x2 − r2 y1 − x2 Cy1

= r1T (x1 − x2 ) + (x1 − x2 )T Cy1
≤ −(x1 − x2 )T Cy2 + (x1 − x2 )T Cy1
≤ (x1 − x2 )T C(y1 − y2 )
(x1 − x2 )T
(y1 − y2 )
≤ ky1 − y2 k2
C
k(x1 − x2 )k2 ky1 − y2 k2
≤ ky1 − y2 k2

max

max

{x kxk2 ≤1} {y kyk2 ≤1}

xT Cy

≤ δkCk2
The above derivation follows from Assumption 17, and the bound reduces to the matrix
norm using Cauchy-Schwartz inequality.
Not surprisingly, the bound is independent of the local rewards and transition structure
of the agents. Thus it in fact shows that the complexity of achieving a fixed approximation
with a fixed interaction structure is linear in the problem size. However, the bounds are
still exponential in the dimensionality of the space. Notice also that the bound is additive.

6. Experimental Results
We now turn to an empirical analysis of the performance of the algorithm. For this purpose
we use the Mars rover problem described earlier. We compared our algorithm with the
original CSA and with a mixed integer linear program (MILP), derived for Eq. (1) as Petrik
and Zilberstein (2007b) describe. Although Eq. (1) can also be modeled as a linear complementarity problem (LCP) (Murty, 1988; Cottle et al., 1992), we do not evaluate that
option experimentally because LCPs are closely related to MILPs (Rosen, 1986). We expect
these two formulations to exhibit similar performance. We also do not compare to any of
the methods described by Horst and Tuy (1996) and Bennett and Mangasarian (1992) due
to their very different nature and high complexity, and because some of these algorithms
do not provide any optimality guarantees.
In our experiments, we applied the algorithm to randomly generated problem instances
with the same parameters that Becker et al. (2003, 2004) used. Each problem instance
includes 2 rovers and 6 sites. At each site, the rovers can decide to perform an experiment
or to skip the site. Performing experiments takes some time, and all the experiments must
be performed in 15 time units. The time required to perform an experiment is drawn from
a discrete normal distribution with the mean uniformly chosen from 4.0-6.0. The variance
262

A Bilinear Programming Approach for Multiagent Planning

Algorithm 5: MPBP: Multiagent Planning with Bilinear Programming

6

Formulate DEC-MDP M as a bilinear program B ;
// [Section 2.1]
B 0 ← ReduceDimensionality(B) with  ≤ 10−4 ;
// [Section 4, Appendix B]
Convert B 0 to a semi-compact form ;
// [Definition 2]
h ← −∞ ;
// Presolve step: run Algorithm 1 θ times with random initialization
for i ∈ {1 . . . θ} do
h ← max{h, IterativeBestResponse(B 0 )} ;
// [Algorithm 1]

7

BestResponseApprox(B 0 , 0 ) ;

1
2
3
4

5

// [Algorithm 2]

is 0.4 of the mean. The local reward for performing an experiment is selected uniformly
from the interval [0.1,1.0] for each site and it is identical for both rovers. The global reward,
received when both rovers perform an experiment on a shared site, is super-additive and is
1/2 of the local reward. The experiments were performed with sites {1, 2, 3, 4, 5} as shared
sites. Typically, the performance of the algorithm degrades with the number of shared sites.
Because the problem with fewer than 5 shared sites–as used in the original CSA paper–were
too easy to solve, we only present results for problems with 5 shared sites. Note that CSA
was used on this problem with an implicit dimensionality reduction due to the use of the
compound events.
In these experiments, the naive dimensionality of Y in Eq. (5) is 6 ∗ 15 ∗ 2 = 180.
This dimensionality can be reduced to be one per each shared site using the automatic
dimensionality reduction procedure. Each dimension then represents the probability that
an experiment on a shared site is performed regardless of the time. Therefore, the dimension
represents the sum of the individual probabilities. Becker et al. (2004) achieved the same
compression using compound events, where each compound event represents the fact that
an experiment is performed on some site regardless of the specific time.
The complete algorithm–Multiagent Planning with Bilinear Programming (MPBP)–is
summarized in Algorithm 5. The automatic dimensionality reduction reduces Y to 5 dimensions. Then, reformulating the problem to a semi-compact form increases the dimensionality
to 6. We experimented with different configurations of the algorithm that differ in the way
the refinements of the pivot point selection is performed. The different methods, described
in Section 3.3, were used to create six configurations as shown in Figure 8. The configuration
C1 corresponds to an earlier version of the algorithm (Petrik & Zilberstein, 2007a).
We executed the algorithm 20 times with each configuration on every problem, randomly
generated according to the distribution described above. The results represent the average
over the random instances. The maximum number of iterations of the algorithm was 200.
Due to rounding errors, we considered any error less than 10−4 to be 0. The algorithm
is implemented in MATLAB release 2007a. The linear solver we used is MOSEK version
5.0. The hardware configuration was Intel Core 2 Duo 1.6 GHz Low Voltage with 2GB
RAM. The time to perform the dimensionality reduction is negligible and not included in
the result.
A direct comparison with CSA was not possible because CSA cannot solve problems
with this dimensionality within a reasonable amount of time. However, in a very similar
263

Petrik & Zilberstein

Configuration

Feasible
[Eq. (13)]

C1

Linear bound
[Eq. (14)]

Cutting plane
[Eq. (20)]

0

√

C2

√

C3

0

√

√

C4

0

√

√

C6

0

√

√

C5

Presolve [θ]

10

√

10

Figure 8: The six algorithm configurations that were evaluated. Feasible, linear bound, and
cutting plane refer to methods used to determine the optimal solution.

problem setup with at most 4 shared sites, CSA solved only 76% of the problems, and
the longest solution took approximately 4 hours (Becker et al., 2004). In contrast, MPBP
solved all 200 problems with 4 shared sites optimally in less than 1 second on average, about
10000 times faster. In addition, MPBP returns solutions that are guaranteed to be close to
optimal in the first few iterations. While CSA also returns solutions close to optimal very
rapidly, it takes a very long time to confirm that.
Figure 9 shows the average guaranteed ratio of the optimal solution, achieved as a
function of the number of iterations, that is, points for which g(y) is evaluated. This figure,
as all others, shows the result of the online error bound. This value is guaranteed and is not
based on the optimal solution. This compares the performance of the various configurations
of the algorithm, without using the presolve step. While the optimal solution was typically
discovered in the first few iterations, it takes significantly longer to prove its optimality.
The average of absolute errors in both linear and log scale are shown in Figure 10. These
results indicate that the methods proposed to eliminate the dominated region in searching
for the pivot point can dramatically improve performance. While requiring that the new
pivot points are feasible in Y improves the performance, it is much more significant with
1

Fraction Optimal

C1
0.95

C2

0.9

C3
C4

0.85
0.8
0.75
0.7
0.65
0.6
0

50

100
Iteration

150

200

Figure 9: Guaranteed fraction of optimality according to the online bound.
264

A Bilinear Programming Approach for Multiagent Planning

1

5

10

C1

C1

C2

4

C

2

0

10

C

C3

C4
3

2

Absolute Error

Absolute Error

3

4

−2

10

−3

1

0
0

C
−1

10

10

−4

50

100
Iteration

150

10

200

0

50

100
Iteration

150

200

Figure 10: Comparison of absolute error of various region elimination methods.

a better approximation of Yh . As expected, the cutting plane elimination is most efficient,
but also most complex.
To evaluate the tradeoffs in the implementation, we also show the average time per
iteration and the average total time in Figure 11. These figures show that the time per
iteration is significantly larger when the cutting plane elimination is used. Overall, the
algorithm is faster when the simpler linear bound is used.

0.03

12

0.025

10

0.02

8

Total Seconds

Seconds per Iteration

This trend is most likely problem specific. In problems with higher dimensionality, the
more precise cutting plane algorithm may be more efficient. Implementation issues play a
significant role in this problem too, and it is likely that the implementation of Algorithm 4
can be further improved.

0.015
0.01

4
2

0.005
0

6

C1

C2

C3

0

C4

C1

C2

C3

C4

Figure 11: Time per iteration and the total time to solve. With configurations C1 and C2 ,
the optimal value is not reached with 200 iterations . The figure only shows the
time to compute up to 200 iterations.

265

Petrik & Zilberstein

−1

10

C3
C

4

C5
Absolute Error

−2

C6

10

−3

10

−4

10

0

10

20
Iteration

30

40

Figure 12: Influence of the presolve method.
Figure 12 shows the influence of using the presolve method. The plots of C3 and C4 are
identical to the plots of C5 and C6 respectively, indicating that the presolve method does
not have any significant influence. This also indicates that a solution that is very close to
optimal is obtained when the values of the initial points are calculated.
We also performed experiments with CPLEX–a state-of-the-art MILP solver on the direct MILP formulation of the DEC-MDP. CPLEX was not able to solve any of the problems
within 30 minutes, no matter how many of the sites were shared. The main reason for this
is that it does not take any advantage of the limited interaction. Nevertheless, it is possible
that some specialized MILP solvers may perform better.

7. Conclusion and Further Work
We present an algorithm that significantly improves the state-of-the-art in solving two-agent
coordination problems. The algorithm takes as input a bilinear program representing the
problem, and solves the problem using a new successive approximation method. It provides
a useful online performance bound that can be used to decide when the approximation is
good enough. The algorithm can take advantage of the limited interaction among the agents,
which is translated into a small dimensionality of the bilinear program. Moreover, using
our approach, it is possible to reduce the dimensionality of such problems automatically,
without extensive modeling effort. This makes it easy to apply our new method in practice.
When applied to DEC-MDPs, the algorithm is much faster than the existing CSA method,
on average reducing computation time by four orders of magnitude. We also show that a
variety of other coordination problems can be treated within this framework.
Besides multiagent coordination problems, bilinear programs have been previously used
to solve problems in operations research and global optimization (Sherali & Shetty, 1980;
White, 1992; Gabriel, Garca-Bertrand, Sahakij, & Conejo, 2005). Global optimization
deals with finding the optimal solutions to problems with multi-extremal objective function.
Solution techniques often share the same idea and are based on cutting plane methods. The
main idea is to iteratively restrict the set of feasible solutions, while improving the incumbent
266

A Bilinear Programming Approach for Multiagent Planning

solution. Horst and Tuy (1996) provide an excellent overview of these techniques. These
algorithms have different characteristics and cannot be directly compared to the algorithm
we developed. Unlike these traditional algorithms, we focus on providing quickly good
approximate solutions with error bounds. In addition, we exploit the small dimensionality
of the best-response space Y to get tight approximation bounds.
Future work will address several interesting open questions with respect to the bilinear
formulation as well as further improvement of the efficiency of the algorithm. With regard to
the representation, it is yet to be determined whether the anytime behavior can be exploited
when applied to games. That is, it is necessary to verify that an approximate solution to
the bilinear program is also a meaningful approximation of the Nash equilibrium. It is also
important to identify the classes of extensive games that can be efficiently formulated as
bilinear programs.
The algorithm we present can be made more efficient in several ways. In particular, a
significant speedup could be achieved by reducing the size of the individual linear programs.
The programs are solved many times with the same constraints, but a different objective
function. The objective function is always from a small-dimensional space. Therefore, the
problems that are solved are all very similar. In the DEC-MDP domain, one option would
be to use a procedure similar to action elimination. In addition, the performance could be
significantly improved by starting with a tight initial triangulation. In our implementation,
we simply use a single large polyhedron that covers the whole feasible region. A better
approach would be to start with something that approximates the feasible region more
tightly. A tighter approximation of the feasible region could also improve the precision of
the dimensionality reduction procedure. Instead of the naive ellipsis used in Assumption 7,
it is possible to use one that approximates the feasible region as tightly as possible. It is
however very encouraging to see that even without these improvements, the algorithm is
very effective compared with existing solution techniques.

Acknowledgments
We thank Chris Amato, Raghav Aras, Alan Carlin, Hala Mostafa, and the anonymous
reviewers for useful comments and suggestions. This work was supported in part by the
Air Force Office of Scientific Research under Grants No. FA9550-05-1-0254 and FA955008-1-0181, and by the National Science Foundation under Grants No. IIS-0535061 and
IIS-0812149.

Appendix A. Proofs
Proof of Proposition 10 The proposition states that in the proposed triangulation, the
sub-polyhedra do not overlap and they cover the whole feasible set Y , given that the pivot
point is in the interior of S.
Proof. We prove the theorem by induction on the number of polyhedron splits that were
performed. The base case is trivial: there is only a single polyhedron, which covers the
whole feasible region.
For the inductive case, we show that for any polyhedron S the sub-polyhedra induced
by the pivot point ŷ cover S and do not overlap. The notation we use is the following: T
267

Petrik & Zilberstein

denotes the original polyhedron and ŷ = T c is the pivot point, where 1T c = 1 and c ≥ 0.
Note that T is a matrix and c, d, ŷ are vectors, and β is a scalar.
We show that the sub-polyhedra cover the original polyhedron S as follows. Take any
a = T d such that 1T d = 1 and d ≥ 0. We show that there exists a sub-polyhedron that
contains a and has ŷ as a vertex. First, let
 
T
T̂ =
1T
This matrix is square and invertible, since the polyhedron is non-empty. To get a representation of a that contains ŷ, we show that there is a vector o such that for some i,
o(i) = 0:
 

a
= T̂ d = T̂ o + β ŷ
1
o ≥ 0,
for some β > 0. This will ensure that a is in the sub-polyhedron with ŷ with vertex i
replaced by ŷ. The value o depends on β as follows:
 
−1 ŷ
.
o = d − β T̂
1
This can be achieved by setting:
β = min
i

d(i)
(T̂ −1 ŷ)(i)

.

Since both d and c = T̂ −1 ŷ are non-negative. This leaves us with an equation for the
sub-polyhedron containing the point a. Notice that the resulting polyhedron may be of a
smaller dimension than n when o(j) = 0 for some i 6= j.
To show that the polyhedra do not overlap, assume there exists a point a that is common to the interior of at least two of the polyhedra. That is, assume that a is a convex
combination of the vertices:
a = T3 c1 + h1 ŷ + β1 y1
a = T3 c2 + h2 ŷ + β2 y2 ,
where T3 represents the set of points common to the two polyhedra, and y1 and y2 represent
the disjoint points in the two polyhedra. The values h1 , h2 , β1 , and β2 are all scalars, while
c1 and c2 are vectors. Notice that the sub-polyhedra differ by at most one vertex. The
coefficients satisfy:
c1 ≥ 0

c2 ≥ 0

h1 ≥ 0

h2 ≥ 0

β1 ≥ 0

β2 ≥ 0

T

T

1 c1 + h1 + β1 = 1

1 c2 + h2 + β2 = 1
268

A Bilinear Programming Approach for Multiagent Planning

Since the interior of the polyhedron is non-empty, this convex combination is unique.
First assume that h = h1 = h2 . Then we can show the following:
a = T3 c1 + hŷ + β1 y1 = T3 c2 + hŷ + β2 y2
T3 c1 + β1 y1 = T3 c2 + β2 y2
β1 y1 = β2 y2
β1 = β2 = 0
This holds since y1 and y2 are independent of T3 when the polyhedron is nonempty and
y1 6= y2 . The last equality follows from the fact that y1 and y2 are linearly independent.
This is a contradiction, since β1 = β2 = 0 implies that the point a is not in the interior of
two polyhedra, but at their intersection.
Finally, assume WLOG that h1 > h2 . Now let ŷ = T3 ĉ + α1 y1 + α2 y2 , for some scalars
α1 ≥ 0 and α2 ≥ 0 that represent a convex combination. We get:
a = T3 c1 + h1 ŷ + β1 y1 = T3 (c1 + h1 ĉ) + (h1 α1 + β1 )y1 + h1 α2 y2
a = T3 c2 + h2 ŷ + β2 y2 = T3 (c2 + h2 ĉ) + h2 α1 y1 + (h2 α2 + β2 )y2 .
The coefficients sum to one as shown below.
1T (c1 + h1 ĉ) + (h1 α1 + β1 ) + h1 α2 = 1T c1 + β1 + h1 (1T ĉ + α1 + α2 ) = 1T c1 + β1 + h1 = 1
1T (c2 + h2 ĉ) + α1 + (h2 α2 + β2 ) = 1T c2 + β2 + h2 (1T ĉ + α1 + α2 ) = 1T c2 + β2 + h2 = 1
Now, the convex combination is unique, and therefore the coefficients associated with each
vertex for the two representations of a must be identical. In particular, equating the coefficients for y1 and y2 results in the following:
h1 α1 + β1 = h2 α1

h1 α2 = h2 α2 + β2

β1 = h2 α1 − h1 α1

β2 = h1 α2 − h2 α2

β1 = α1 (h2 − h1 ) > 0

β2 = α2 (h1 − h2 ) < 0

We have that α1 > 0 and α2 > 0 from the fact that ŷ is in the interior of the polyhedron S.
Then, having β2 ≤ 0 is a contradiction with a being a convex combination of the vertices
of S.

Appendix B. Practical Dimensionality Reduction
In this section we describe an approach to dimensionality reduction that is easy to implement. Note that there are at least two possible approaches to take advantage of reduced
dimensionality. First, it is possible to use the dimensionality information to limit the algorithm to work only in the significant dimensions of Y . Second, it is possible to modify the
bilinear program to have a small dimensionality. While changing the algorithm may be more
straightforward, it limits the use of the advanced pivot point selection methods described
in Section 3.3. Here, we show how to implement the second option in a straightforward way
using singular value decomposition.
269

Petrik & Zilberstein

The dimensionality reduction is applied to the following bilinear program:
maximize
w,x,y,z

T
T
T
r1T x + sT
1 w + x Cy + r2 y + s2 z

subject to A1 x + B1 w = b1
A2 y + B2 z = b2

(23)

w, x, y, z ≥ 0
Let C = SV T T be a singular value decomposition. Let T = [T1 , T2 ], such that the
singular value of vectors ti in T2 is less than the required . Then, a bilinear program with
reduced dimensionality may be defined as follows:
maximize
w,x,ȳ,y,z

T
T
T
r1T x + sT
1 w + x SV T1 ȳ + r2 y + s2 z

subject to T1 ȳ = y
A1 x + B1 w = b1

(24)

A2 y + B2 z = b2
w, x, y, z ≥ 0
Note that ȳ is not constrained to be non-negative. One problematic aspect of reducing the
dimensionality is how to define the initial polyhedron that needs to encompass all feasible
solutions. One option is to make it large enough to contain the set {y kyk2 = 1}, but this
may be too large. Often in practice, it may be more efficient to first triangulate a rough
approximation of the feasible region, and then execute the algorithm on this triangulation.

Appendix C. Sum of Convex and Concave Functions
In this section we show that the best-response function g(y) may not be convex when the
program is not in a semi-compact form. The convexity of the best-response function is
crucial in bounding the approximation error and in eliminating the dominated regions.
We show that when the program is not in a semi-compact form, the best-response
function can we written as a sum of a convex function and a concave function. To show
that consider the following bilinear program.
maximize
w,x,y,z

T
T
T
f = r1T x + sT
1 w + x Cy + r2 y + s2 z

subject to A1 x + B1 w = b1
A2 y + B2 z = b2
w, x, y, z ≥ 0
This problem may be reformulated as:
f

=
=

max

max

max

g 0 (y) + sT
2 z,

{y,z (y,z)∈Y } {x,w (x,w)∈X}
{y,z (y,z)∈Y }

T
T
T
r1T x + sT
1 w + x Cy + r2 y + s2 z

270

(25)

A Bilinear Programming Approach for Multiagent Planning

where
g 0 (y) =

max

{x,w (x,w)∈X}

T
T
r1T x + sT
1 w + x Cy + r2 y.

Notice that function g 0 (y) is convex, because it is a maximum of a set of linear functions.
Since f = max{y (y,z)∈Y } g(y), the best-response function g(y) can be expressed as:
g(y) =

max

{z (y,z)∈Y }
0

0
g 0 (y) + sT
2 z = g (y) +

max

{z (y,z)∈Y }

sT
2z

= g (y) + t(y),
where
t(y) =

max

{z A2 y+B2 z=b2 , y,z≥0}

sT
2 z.

Function g 0 (y) does not depend on z, and therefore could be taken out of the maximization.
The function t(y) corresponds to a linear program, and its dual using the variable q is:
(b2 − A2 y)T q

minimize
q

subject to B2T q ≥ s2

(26)

Therefore:
t(y) =

(b2 − A2 y)T q,

min

{q B2T q≥s2 }

which is a concave function, because it is a minimum of a set of linear functions. The
best-response function can now be written as:
g(y) = g 0 (y) + t(y),
which is a sum of a convex function and a concave function, also known as a d.c. function (Horst & Tuy, 1996). Using this property, it is easy to construct a program such that
g(y) will be convex on one part of Y and concave on another part of Y , as the following
example shows. Note that in semi-compact bilinear programs t(y) = 0, which guarantees
the convexity of g(y).
Example 23. Consider the following bilinear program:
maximize
x,y,z

−x + xy − 2z

subject to −1 ≤ x ≤ 1
y−z ≤2
z≥0
A plot of the best response function for this program is shown in Figure 13.

271

(27)

Petrik & Zilberstein

2

maxx f(x,y)

1.5
1
0.5
0
−0.5
−1
−1

0

1

2

3

4

y

Figure 13: A plot of a non-convex best-response function g for a bilinear program, which is
not in a semi-compact form.



Coordination of distributed agents is required for problems arising in many areas, including multi-robot systems, networking and e-commerce. As a formal framework for such
problems, we use the decentralized partially observable Markov decision process (DECPOMDP). Though much work has been done on optimal dynamic programming algorithms
for the single-agent version of the problem, optimal algorithms for the multiagent case have
been elusive. The main contribution of this paper is an optimal policy iteration algorithm
for solving DEC-POMDPs. The algorithm uses stochastic finite-state controllers to represent policies. The solution can include a correlation device, which allows agents to correlate
their actions without communicating. This approach alternates between expanding the
controller and performing value-preserving transformations, which modify the controller
without sacrificing value. We present two efficient value-preserving transformations: one
can reduce the size of the controller and the other can improve its value while keeping the
size fixed. Empirical results demonstrate the usefulness of value-preserving transformations
in increasing value while keeping controller size to a minimum. To broaden the applicability of the approach, we also present a heuristic version of the policy iteration algorithm,
which sacrifices convergence to optimality. This algorithm further reduces the size of the
controllers at each step by assuming that probability distributions over the other agents’
actions are known. While this assumption may not hold in general, it helps produce higher
quality solutions in our test problems.

1. Introduction
Markov decision processes (MDPs) provide a useful framework for solving problems of
sequential decision making under uncertainty. In some settings, agents must base their
decisions on partial information about the system state. In that case, it is often better to use
the more general framework of partially observable Markov decision processes (POMDPs).
Even more general are problems in which a team of decision makers, each with its own
c 2009 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Bernstein, Amato, Hansen, & Zilberstein

local observations, must act together. Domains in which these types of problems arise
include networking, multi-robot coordination, e-commerce, and space exploration systems.
The decentralized partially observable Markov decision process (DEC-POMDP) provides
an effective framework to model such problems. Though this model has been recognized for
decades (Witsenhausen, 1971), there has been little work on provably optimal algorithms
for it.
On the other hand, POMDPs have been studied extensively over the past few decades
(Smallwood & Sondik, 1973; Simmons & Koenig, 1995; Cassandra, Littman, & Zhang, 1997;
Hansen, 1998a; Bonet & Geffner, 2000; Poupart & Boutilier, 2003; Feng & Zilberstein, 2004;
Smith & Simmons, 2005; Smith, Thompson, & Wettergreen, 2007). It is well known that
a POMDP can be reformulated as an equivalent belief-state MDP. A belief-state MDP
cannot be solved in a straightforward way using MDP methods because it has a continuous
state space. However, Smallwood and Sondik showed how to implement value iteration by
exploiting the piecewise linearity and convexity of the value function. This work opened the
door for many algorithms, including approximate approaches and policy iteration algorithms
in which the policy is represented using a finite-state controller.
Extending dynamic programming for POMDPs to the multiagent case is not straightforward. For one thing, it is not clear how to define a belief state and consequently form a
belief-state MDP. With multiple agents, each agent has uncertainty about the observations
and beliefs of the other agents. Furthermore, the finite-horizon DEC-POMDP problem
with just two agents is complete for a higher complexity class than the single-agent version
(Bernstein, Givan, Immerman, & Zilberstein, 2002), indicating that these are fundamentally
different problems.
In this paper, we describe an extension of the policy iteration algorithm for single agent
POMDPs to the multiagent case. As in the single agent case, our algorithm converges
in the limit, and thus serves as the first nontrivial optimal algorithm for infinite-horizon
DEC-POMDPs. A few optimal approaches (Hansen, Bernstein, & Zilberstein, 2004; Szer,
Charpillet, & Zilberstein, 2005) and several approximate algorithms have been developed for
finite-horizon DEC-POMDPs (Peshkin, Kim, Meuleau, & Kaelbling, 2000; Nair, Pynadath,
Yokoo, Tambe, & Marsella, 2003; Emery-Montemerlo, Gordon, Schnieder, & Thrun, 2004;
Seuken & Zilberstein, 2007), but only locally optimal algorithms have been proposed for
the infinite-horizon case (Bernstein, Hansen, & Zilberstein, 2005; Szer & Charpillet, 2005;
Amato, Bernstein, & Zilberstein, 2007).
In our algorithmic framework, policies are represented using stochastic finite-state controllers. A simple way to implement this is to give each agent its own local controller. In
this case, the agents’ policies are all independent. A more general class of policies includes
those which allow agents to share a common source of randomness without sharing observations. We define this class formally, using a shared source of randomness called a correlation
device. The use of correlated stochastic policies in the DEC-POMDP context is novel. The
importance of correlation has been recognized in the game theory community (Aumann,
1974), but there has been little work on algorithms for finding correlated policies.
Each iteration of the algorithm consists of two phases. These are exhaustive backups,
which add nodes to the controller, and value-preserving transformations, which change the
controller without sacrificing value. We first provide a novel exposition of existing single90

Policy Iteration for DEC-POMDPs

agent algorithms using this two-phase view, and then we go on to describe the multiagent
extension.
There are many possibilities for value-preserving transformations. In this paper, we
describe two different types, both of which can be performed efficiently using linear programming. The first type allows us to remove nodes from the controller, and the second
allows us to improve the value of the controller while keeping its size fixed. Our empirical
results demonstrate the usefulness of value-preserving transformations in obtaining high
values while keeping controller size to a minimum.
We note that this work serves to unify and generalize previous work on dynamic programming for DEC-POMDPs. The first algorithm for the finite-horizon case (Hansen et al., 2004)
can be extended to the infinite-horizon case and viewed as interleaving exhaustive backups
and controller reductions. The bounded policy iteration algorithm for DEC-POMDPs (Bernstein et al., 2005), which extends a POMDP algorithm proposed by Poupart and Boutilier
(2003), can be viewed through the lens of our framework as repeated application of a specific
value-preserving transformation.
Because the optimal algorithm will not usually be able to return an optimal solution
in practice, we also introduce a heuristic version of the policy iteration algorithm. This
approach makes use of initial state information to focus policy search and further reduces
controller size at each step. To accomplish this, a forward search from the initial state
distribution is used to construct a set of belief points an agent would visit assuming the
other agents use given fixed policies. This search is conducted for each agent and then policy
iteration takes place while using the belief points to guide the removal of controller nodes.
The assumption that other agents use fixed policies causes the algorithm to no longer be
optimal, but it performs well in practice. We show that more concise and higher-valued
solutions can be produced compared to the optimal method before resources are exhausted.
The remainder of the paper is organized as follows. Section 2 introduces the formal
models of sequential decision making. Section 3 contains a novel presentation of existing
dynamic programming algorithms for POMDPs. In section 4, we present an extension of
policy iteration for POMDPs to the DEC-POMDP case, along with a convergence proof.
We discuss the heuristic version of policy iteration in section 5, followed by experiments
using policy iteration and heuristic policy iteration in section 6. Finally, section 7 contains
the conclusion and a discussion of possible future work.

2. Formal Model of Distributed Decision Making
We begin with a description of the formal framework upon which our work is based. This
framework extends the well-known Markov decision process to allow for distributed policy
execution. We also define an optimal solution for this model and discuss two different
representations for these solutions.
2.1 Decentralized POMDPs
A decentralized partially observable Markov decision process (DEC-POMDP) is defined for~ T, R, Ω,
~ Oi, where
mally as a tuple hI, S, A,
• I is a finite set of agents.
91

Bernstein, Amato, Hansen, & Zilberstein

Agent
Agent
a

s, r
System

a1

Agent
a

System

o, r
System

o1, r

a2

o2, r
Agent

(a)

(b)

(c)

Figure 1: (a) Markov decision process. (b) Partially observable Markov decision process.
(c) Decentralized partially observable Markov decision process with two agents.

• S is a finite set of states, with distinguished initial state s0 .
~ = ×i∈I Ai is a set of joint actions, where Ai is the set of actions for agent i.
• A
~ → ∆S is the state transition function, defining the distributions of states
• T : S×A
that result from starting in a given state and each agent performing an action.
~ → < is the reward function for the set of agents for each set of joint actions
• R : S ×A
and each state.
~ = ×i∈I Ωi is a set of joint observations, where Ωi contains observations for agent i.
• Ω
~ × S → ∆Ω
~ is an observation function, defining the distributions of observations
• O:A
for the set of agents that result from each agent performing an action and ending in
a given state.
The special case of a DEC-POMDP in which there is only one agent is called a partially
observable Markov decision process (POMDP).
In this paper, we consider the case in which the process unfolds over an infinite sequence
of stages. At each stage, all agents simultaneously select an action, and each receives the
global reward based on the reward function and a local observation based on the observation
function. Thus, the transitions, rewards and observations depend on the actions of all
agents, but each agent must act based only on local observations. This is illustrated in
Figure 1. The objective of the agents is to maximize the expected discounted sum of
rewards that are received, thus it is a cooperative framework. We denote the discount
factor β and require that 0 ≤ β < 1.
In a DEC-POMDP, the decisions of each agent affect all the agents in the domain, but
due to the decentralized nature of the model each agent must choose actions based solely on
local information. Because each agent receives a separate observation that does not usually
provide sufficient information to efficiently reason about the other agents, solving a DECPOMDP optimally becomes very difficult. For example, each agent may receive a different
92

Policy Iteration for DEC-POMDPs

(a)

(b)

Figure 2: A set of horizon three policy trees (a) and two node stochastic controllers (b) for
a two agent DEC-POMDP.

piece of information that does not allow a common state estimate or any estimate of the
other agents’ decisions to be calculated. These single estimates are crucial in single agent
problems, as they allow the agent’s history to be summarize concisely, but they are not
generally available in DEC-POMDPs. This is seen in the complexity of the finite-horizon
problem with at least two agents, which is NEXP-complete (Bernstein et al., 2002) and
thus in practice may require double exponential time. Like the infinite-horizon POMDP,
optimally solving an infinite-horizon DEC-POMDP is undecidable as it may require infinite
resources, but our method is able to provide a solution within  of the optimal with finite
time and memory. Nevertheless, introducing multiple decentralized agents causes a DECPOMDP to be significantly more difficult than a single agent POMDP.
2.2 Solution Representations
A local policy for an agent is a mapping from local action and observation histories to actions
while a joint policy is a set of policies, one for each agent in the problem. As mentioned
above, an optimal solution for a DEC-POMDP is the joint policy that maximizes the
expected sum of rewards that are received over the finite or infinite steps of the problem.
In infinite-horizon problems, the rewards are discounted to maintain a finite sum. Thus, an
optimal solution is a joint policy that provides the highest value starting at the given initial
state of the problem.
For finite-horizon problems, local policies can be represented using a policy tree as seen
in Figure 2a. Actions are represented by the arrows or stop figures (where each agent can
move in the given direction or stay where it is) and observations are labeled “wl” and “wr”
for seeing a wall on the left or the right respectively. Using this representation, an agent
takes the action defined at the root node and then after seeing an observation, chooses the
next action that is defined by the respective branch. This continues until the action at a
leaf node is executed. For example, agent 1 would first move left and then if a wall is seen
on the right, the agent would move left again. If a wall is now seen on the left, the agent
does not move on the final step. A policy tree is a record of the the entire local history for
an agent up to some fixed horizon and because each tree is independent of the others it can
93

Bernstein, Amato, Hansen, & Zilberstein

be executed in a decentralized manner. While this representation is useful for finite-horizon
problems, infinite-horizon problems would require trees of infinite height.
Another option used in this paper is to condition action selection on some internal
memory state. These solutions can be represented as a set of local finite-state controllers
(seen in Figure 2b). The controllers operate in a very similar way to the policy trees in
that there is a designated initial node and following the action selection at that node, the
controller transitions to the next node depending on the observation seen. This continues for
the infinite steps of the problem. Throughout this paper, controller states will be referred
to as nodes to help distinguish them from system states.
An infinite number of nodes may be required to define an optimal infinite-horizon DECPOMDP policy, but we will discuss a way to produce solutions within  of the optimal
with a fixed number of nodes. While deterministic action selection and node transitions are
sufficient to define this -optimal policy, when memory is limited stochastic action selection
and node transition may be beneficial. A simple example illustrating this for POMDPs
is given by Singh (1994), which can be easily extended to DEC-POMDPs. Intuitively,
randomness can help an agent to break out of costly loops that result from forgetfulness.
A formal description of stochastic controllers for POMDPs and DEC-POMDPs is given
in sections 3.2.1 and 4.1.1 respectively, but an example can be seen in Figure 2b. Agent 2
begins at node 1 and moves up with probability 0.89 and stays in place with probability
0.11. If the agent stayed in place and a wall was then seen on the left (observation “wl”), on
the next step, the controller would transition to node 1 and the agent would use the same
distribution of actions again. If a wall was seen on the right instead (observation “wr”),
there is a 0.85 probability that the controller will transition back to node 1 and a 0.15
probability that the controller will transition to node 2 for the next step. The finite-state
controller allows an infinite-horizon policy to be represented compactly by remembering
some aspects of the agent’s history without representing the entire local history.

3. Centralized Dynamic Programming
In this section, we cover the main concepts involved in dynamic programming for the single agent case. This will provide a foundation for the multiagent dynamic programming
algorithm described in the following section.
3.1 Value Iteration for POMDPs
Value iteration can be used to solve POMDPs optimally. This algorithm is more complicated
than its MDP counterpart, and does not have efficiency guarantees. However, in practice
it can provide significant leverage in solving POMDPs.
We begin by explaining how every POMDP has an equivalent MDP with a continuous
state space. Next, we describe how the value functions for this MDP have special structure
that can be exploited. These ideas are central to the value iteration algorithm.
3.1.1 Belief State MDPs
A convenient way to summarize the observation history of an agent in a POMDP is through
a belief state, which is a distribution over system states. As it receives observations, the
94

Policy Iteration for DEC-POMDPs

agent can update its belief state and then remove its observations from memory. Let b
denote a belief state, and let b(s) represent the probability assigned to state s by b. If an
agent chooses action a from belief state b and subsequently observes o, each component of
the successor belief state obeys the equation
P
P (o|a, s0 ) s∈S P (s0 |s, a)b(s)
0 0
b (s ) =
,
P (o|b, a)
where

"
P (o|b, a) =

X

#
0

P (o|a, s )

s0 ∈S

X

0

P (s |s, a)b(s) .

s∈S

Note that this is a simple application of Bayes’ rule.
It was shown by Astrom (1965) that a belief state constitutes a sufficient statistic for
the agent’s observation history, and it is possible to define an MDP over belief states as
follows. A belief-state MDP is a tuple hΠ, A, T, Ri, where
• Π is the set of distributions over S.
• A is the set of actions (same as before).
• T (b, a, b0 ) is the transition function, defined as
X
T (b, a, b0 ) =
P (b0 |b, a, o)P (o|b, a).
o∈O

• R(b, a) is a reward function, defined as
R(b, a) =

X

b(s)R(s, a).

s∈S

When combined with belief-state updating, an optimal solution to this MDP can be used
as an optimal solution to the POMDP from which it was constructed. However, since the
belief state MDP has a continuous, |S|-dimensional state space, traditional MDP techniques
are not immediately applicable.
Fortunately, dynamic programming can be used to find a solution to the belief state
MDP. The key result in making dynamic programming practical was proved by Smallwood
and Sondik (1973), who showed that the Bellman operator preserves piecewise linearity and
convexity of a value function. Starting with a piecewise linear and convex representation of
V t , the value function V t+1 is piecewise linear and convex, and can be computed in finite
time.
To represent a piecewise linear and convex value function, one need only store the value
of each facet for each system state. Denoting the set of facets Γ, we can store |Γ| |S|dimensional vectors of real values.PFor any single vector, γ ∈ Γ, we can define its value at
the belief state b with V (b, γ) = s∈S b(s)γ(s). Thus, to go from a set of vectors to the
value of a belief state, we use the equation
X
V (b) = max
b(s)γ(s).
γ

s∈S

95

Bernstein, Amato, Hansen, & Zilberstein

s1

s1

s2
(a)

s2
(b)

Figure 3: A piecewise linear and convex value function for a POMDP with two states (a)
and a non-minimal representation of a piecewise linear and convex value function
for a POMDP (b).

Figure 3a shows a piecewise linear and convex value function for a POMDP with two states.
Smallwood and Sondik proved that the optimal value function for a finite-horizon
POMDP is piecewise linear and convex. The optimal value function for an infinite-horizon
POMDP is convex, but may not be piecewise linear. However, it can be approximated
arbitrarily closely by a piecewise linear and convex value function, and the value iteration
algorithm constructs closer and closer approximations, as we shall see.
3.1.2 Pruning Vectors
Every piecewise linear and convex value function has a minimal set of vectors Γ that represents it. Of course, it is possible to use a non-minimal set to represent the same function.
This is illustrated in Figure 3b. Note that the removal of certain vectors does not change
the value of any belief state. Vectors such as these are not necessary to keep in memory.
Formally, we say that a vector γ is dominated if for all belief states b, there is a vector
γ̂ ∈ Γ \ γ such that V (b, γ) ≤ V (b, γ̂).
Because dominated vectors are not necessary, it would be useful to have a method for
removing them. This task is often called pruning, and has an efficient algorithm based
on linear programming. For a given vector γ, the linear program in Table 1 determines
whether γ is dominated. If variables can be found to make  positive, then adding γ to the
set improves the value function at some belief state. If not, then γ is dominated.
This gives rise to a simple algorithm for pruning a set of vectors Γ̃ to obtain a minimal
set Γ. The algorithm loops through Γ̃, removes each vector γ ∈ Γ̃, and solves the linear
program using γ and Γ̃ \ γ. If γ is not dominated, then it is returned to Γ̃.
It turns out that there is an equivalent way to characterize dominance that can be useful.
Recall that for a vector to be dominated, there does not have to be a single vector that has
value at least as high for all states. It is sufficient for there to exist a set of vectors such
that for all belief states, one of the vectors in the set has value at least as high as the vector
in question.
96

Policy Iteration for DEC-POMDPs

Variables: , b(s)
Objective: Maximize .
Improvement constraints:
X

∀γ̂

b(s)γ̂(s) +  ≤

s

X

b(s)γ(s)

s

Probability constraints:
X

∀s

b(s) = 1,

b(s) ≥ 0

s

Table 1: The linear program for testing whether a vector γ is dominated.

γ1

γ2
convex combination
γ3

s1

s2

Figure 4: The dual interpretation of dominance. Vector γ3 is dominated at all belief states
by either γ1 or γ2 . This is equivalent to the existence of a convex combination of
γ1 and γ2 which dominates γ3 for all belief states.

It can be shown that such a set exists if and only if there is some convex combination
of vectors that has value at least as high as the vector in question for all states. This is
shown graphically in Figure 4. If we take the dual of the linear program for dominance
given in the previous section, we get a linear program for which the solution is a vector of
probabilities for the convex combination. This dual view of dominance was first used in a
POMDP context by Poupart and Boutilier (2003), and is useful for policy iteration, as will
be explained later.
3.1.3 Dynamic Programming Update
In this section, we describe how to implement a dynamic programming update to go from a
value function Vt to a value function Vt+1 . In terms of implementation, our aim is to take
a minimal set of vectors Γt that represents Vt and produce a minimal set of vectors Γt+1
that represents Vt+1 .
97

Bernstein, Amato, Hansen, & Zilberstein

Each vector that could potentially be included in Γt+1 represents the value of an action a
and assignment of vectors in Γt to observations. A combination of an action and transition
rule will hereafter be called a one-step policy. The value vector for a one-step policy can
be determined by considering the action taken, the resulting state transitioned to and
observation seen and the value of the assigned vector at step t. This is given via the
equation
X
γit+1 (s) = R(s, α(i)) + β
P (s0 |s, α(i))P (o|α(i), s0 )γτt (i,o) (s0 ),
s0 ,o

where i is the index of the vector, α(i) is its action, and τ (i, o) is the index of the vector in
Γt to which to transition upon receiving observation o and β is the discount factor. More
details on the derivation and use of this formula are provided by Zhang and Zhang (2001).
There are |A||Γt ||Ω| possible one-step policies. A simple way to construct Γt+1 is to
evaluate all possible one-step policies and then apply a pruning algorithm such as Lark’s
method (Lark III, 1990). Evaluating the entire set of one-step policies will hereafter be
called performing an exhaustive backup. It turns out that there are ways to perform a
dynamic programming update without first performing an exhaustive backup. Below we
describe two approaches to doing this.
The first approach uses the fact that it is simple to find the optimal vector for any
particular belief state. For a belief state b, an optimal action can be determined via the
equation
"
#
X
P (o|b, a)V t (T (b|a, o)) .
α = argmaxa∈A R(b, a) + β
o∈Ω

For each observation o, there is a subsequent belief state, which can be computed using
Bayes’ rule. To get an optimal transition rule, τ (o), we take the optimal vector for the
belief state corresponding to o.
Since the backed-up value function has finitely many vectors, there must be a finite set
of belief states for which backups must be performed. Algorithms which identify these belief
states include Smallwood and Sondik’s “one-pass” algorithm (1973), Cheng’s linear support
and relaxed region algorithms (Cheng, 1988), and Kaelbling, Cassandra and Littman’s
Witness algorithm (1998).
The second approach is based on generating and pruning sets of vectors. Instead of
generating all vectors and then pruning, these techniques attempt to prune during the generation phase. The first algorithm along these lines was the incremental pruning algorithm
(Cassandra et al., 1997). Recently, improvements have been made to this approach (Zhang
& Lee, 1998; Feng & Zilberstein, 2004, 2005).
It should be noted that there are theoretical complexity barriers for DP updates. Littman
et al. (1995) showed that under certain widely believed complexity theoretic assumptions,
there is no algorithm for performing a DP update that is worst-case polynomial in all the
quantities involved. Despite this fact, dynamic programming updates have been successfully implemented as part of the value iteration and policy iteration algorithms, which will
be described in the subsequent sections.
98

Policy Iteration for DEC-POMDPs

3.1.4 Value Iteration
To implement value iteration, we simply start with an arbitrary piecewise linear and convex
value function, and proceed to perform DP updates. This corresponds to value iteration in
the equivalent belief state MDP, and thus converges to an -optimal value function after a
finite number of iterations.
Value iteration returns a value function, but a policy is needed for execution. As in the
MDP case, we can use one-step lookahead, using the equation
"
#
X
X
δ(b) = argmaxa∈A
R(s, a)b(s) + β
P (o|b, a)V (τ (b, o, a)) ,
s∈S

o∈Ω

where τ (b, o, a) is the belief state resulting from starting in belief state b, taking action a,
and receiving observation o. We note that a state estimator must be used as well to track
the belief state. Using the fact that each vector corresponds to a one-step policy, we can
extract a policy from the value of the vectors:
!
X
δ(b) = α argmaxk
b(s)γk (s)
s

While the size of the resulting set of dominant vectors may remain exponential, in many
cases it is much smaller. This can significantly simplify computation.
As in the completely observable case, the Bellman residual provides a bound on the
distance to optimality. Recall that the Bellman residual is the maximum distance across all
belief states between the value functions of successive iterations. It is possible to find the
maximum distance between two piecewise linear and convex functions in polynomial time
with an algorithm that uses linear programming (Littman et al., 1995).
3.2 Policy Iteration for POMDPs
With value iteration, a POMDP is viewed as a belief-state MDP, and a policy is a mapping
from belief states to actions. An early policy iteration algorithm developed by Sondik used
this policy representation (Sondik, 1978), but it was very complicated and did not meet
with success in practice. We shall describe a different approach that has performed better
on test problems. With this approach, a policy is represented as a finite-state controller.
3.2.1 Finite-State Controllers
Using a finite-state controller, an agent has a finite number of internal states. Its actions
are based only on its internal state, and transitions between internal states occur when
observations are received. Internal states provide agents with a kind of memory, which can
be crucial for difficult POMDPs. Of course, an agent’s memory is limited by the number
of internal states it possesses. In general, an agent cannot remember its entire history of
observations, as this would require infinitely many internal states. An example of a finitestate controller can be seen by considering only one agent’s controller in Figure 2b. The
operation of a single controller is the same as that for each agent in the decentralized case.
We formally define a controller as a tuple hQ, Ω, A, ψ, ηi, where
99

Bernstein, Amato, Hansen, & Zilberstein

• Q is a finite set of controller nodes.
• Ω is a set of inputs, taken to be the observations of the POMDP.
• A is a set of outputs, taken to be the actions of the POMDP.
• ψ : Q → ∆A is an action selection function, defining the distribution of actions
selected at each node.
• η : Q × A × Ω → ∆Q is a transition function, defining the distribution of resulting
nodes for each initial node and action taken.
For each state and starting node of the controller, there is an expected discounted sum
of rewards over the infinite horizon. It can be computed using the following system of linear
equations, one for each s ∈ S and q ∈ Q:


X
X
V (s, q) =
P (a|q) R(s, a) + β
P (o, s0 |s, a)P (q 0 |q, a, o)V (s0 , q 0 ) .
s0 ,o,q 0

a

Where P (a|q) is the probability action a will be taken in node q and P (q 0 |q, a, o) is the
probability the controller will transition to node q 0 from node q after action a was taken
and o was observed.
We sometimes refer to the value of the controller at a belief state. For a belief state b,
this is defined as
X
V (b) = max
b(s)V (s, q).
q

s

Thus, it is assumed that, given an initial state distribution, the controller is started in the
node which maximizes value from that distribution. Once execution has begun, however,
there is no belief state updating. In fact, it is possible for the agent to encounter the same
belief state twice and be in a different internal state each time.
3.2.2 Algorithmic Framework
We will describe the policy iteration algorithm in abstract terms, focusing on the key components necessary for convergence. In subsequent sections, we present different possibilities
for implementation.
Policy iteration takes as input an arbitrary finite-state controller. The first phase of an
iteration consists of evaluating the controller, as described above. Recall that value iteration
was initialized with an arbitrary piecewise linear and convex value function, represented by
a set of vectors. In policy iteration, the piecewise linear and convex value function arises
out of evaluation of the controller. Each controller node has a value when paired with each
state. Thus, each node has a corresponding vector and thus a linear value function over
belief state space. Choosing the best node for each belief state yields a piecewise linear and
convex value function.
The second phase of an iteration is the dynamic programming update. In value iteration, an update produces an improved set of vectors, where each vector corresponds to
a deterministic one-step policy. The same set of vectors is produced in this case, but the
100

Policy Iteration for DEC-POMDPs

Input: A finite state controller, and a parameter .
1. Evaluate the finite-state controller by solving a system of linear equations.
2. Perform a dynamic programming update to add a set of deterministic nodes to the
controller.
3. Perform value-preserving transformations on the controller.
4. Calculate the Bellman residual. If it is less than (1 − β)/2β, then terminate.
Otherwise, go to step 1.
Output: An -optimal finite-state controller.
Table 2: Policy Iteration for POMDPs.
actions and transition rules for the one-step policy cannot be removed from memory. Each
new vector is actually a node that gets added to the controller. All of the probability distributions for the added nodes are deterministic. That is, a exhaustive backup in this context
creates a new node for each possible action and possible combinations of observations and
deterministic transitions into the current controller. This results in the same one-step policies being considered as in the dynamic programming update described above. As there
are |A||Γt ||Ω| possible one-step polices, this number also defines the number of new nodes
added to the controller after an exhaustive backup.
Finally, additional operations are performed on the controller. There are many such
operations, and we describe two possibilities in the following section. The only restriction
placed on these operations is that they do not decrease the value for any belief state. Such
an operation is denoted a value-preserving transformation.
The complete algorithm is outlined in Table 2. It is guaranteed to converge to a finitestate controller that is -optimal for all belief states within a finite number of steps. Furthermore, the Bellman residual can be used to obtain a bound on the distance to optimality,
as with value iteration.
3.2.3 Controller Reductions
In performing a DP update, potential nodes that are dominated do not get added to the
controller. However, after the update is performed, some of the old nodes may have become
dominated. These nodes cannot simply be removed, however, as other nodes may transition
into them. This is where the dual view of dominance is useful. Recall that if a node is
dominated, then there is a convex combination of other nodes with value at least as high
from all states. Thus, we can remove the dominated node and merge it into the dominating
convex combination by changing transition probabilities accordingly. This operation was
proposed by Poupart and Boutilier (2003) and built upon earlier work by Hansen (1998b).
Formally, a controller reduction attempts to replace a node q ∈ Q with a distribution
P (q̂) over nodes q̂ ∈ Q \ q such that for all s ∈ S,
X
V (s, q) ≤
P (q̂)V (s, q̂).
q̂∈Q\q

101

Bernstein, Amato, Hansen, & Zilberstein

Variables: , x(γ̂)
Objective: Maximize 
Improvement constraints:
∀s

V (s, γ) +  ≤

X

x(γ̂)V (s, γ̂)

γ̂

Probability constraints:
X

∀γ̂

x(γ̂) = 1,

x(γ̂) ≥ 0

γ̂

Table 3: The dual linear program for testing dominance for the vector γ. The variable x(γ̂)
represents P (γ̂).

This can be achieved by solving the linear program in Table 3. As nodes are used rather
than vectors, we replace x(γ̂) with x(q̂) in the dual formulation which provides a probability distribution of nodes which dominate node q. Rather than transitioning into q, this
distribution can then be used instead. It can be shown that if such a distribution is found
and used for merging, the resulting controller is a value-preserving transformation of the
original one.
3.2.4 Bounded Backups
In the previous section, we described a way to reduce the size of a controller without
sacrificing value. The method described in this section attempts to increase the value of
the controller while keeping its size fixed. It focuses on one node at a time, and attempts to
change the parameters of the node such that the value of the controller is at least as high
for all belief states. The idea for this approach originated with Platzman (1980), and was
made efficient by Poupart and Boutilier (2003).
In this method, a node q is chosen, and parameters for the conditional distribution
P (a, q 0 |q, o) are to be determined. Determining these parameters works as follows. We
assume that the original controller will be used from the second step on, and try to replace
the parameters for q with better ones for just the first step. In other words, we look for
parameters which satisfy the following inequality:


X
X
V (s, q) ≤
P (a|q) R(s, a) + β
P (q 0 |q, a, o)P (o, s0 |s, a)V (s0 , q 0 )
a

s0 ,o,q 0

for all s ∈ S. Note that the inequality is always satisfied by the original parameters.
However, it is often possible to get an improvement.
The new parameters can be found by solving a linear program, as shown in Table 4.
Note that the size of the linear program is polynomial in the sizes of the POMDP and the
controller. We call this process a bounded backup because it acts like a dynamic programming
102

Policy Iteration for DEC-POMDPs

Variables: , x(a), x(a, o, q 0 )
Objective: Maximize 
Improvement constraints:

∀s

V (s, q) +  ≤

X



x(a)R(s, a) + β

x(a) = 1,

∀a, o

X

x(a, o, q 0 ) = x(a)

q0

a

∀a

x(a, o, q 0 )P (o, s0 |s, a)V (s0 , q 0 )

s0 ,o,q 0

a

Probability constraints:
X

X

x(a) ≥ 0,

∀a, o, q 0

x(a, o, q 0 ) ≥ 0

Table 4: The linear program to be solved for a bounded backup. The variable x(a) represents P (a|q), and the variable x(a, o, q 0 ) represents P (a, q 0 |q, o).

backup with memory constraints. To see this, consider the set of nodes generated by a DP
backup. These nodes dominate the original nodes across all belief states, so for every
original node, there must be a convex combination of the nodes in this set that dominate
the original node for all states. A bounded backup finds such a convex combination.
It can be shown that a bounded backup yields a value-preserving transformation. Repeated application of bounded backups can lead to a local optimum, at which none of the
nodes can be improved any further. Poupart and Boutilier (2003) showed that a local optimum has been reached when each node’s value function is touching the value function
produced by performing a full DP backup. This is illustrated in Figure 5.

4. Decentralized Dynamic Programming
In the previous section, we presented dynamic programming for POMDPs. A key part of
POMDP theory is the fact that every POMDP has an equivalent belief-state MDP. No
such result is known for DEC-POMDPs, making it difficult to generalize value iteration to
the multiagent case. This lack of a shared belief-state requires a new set of tools to be
developed for solving DEC-POMDP. As a step in this direction, we were able to develop an
optimal policy iteration algorithm for DEC-POMDPs that includes the POMDP version as
a special case. This algorithm is the focus of the section.
We first show how to extend the definition of a stochastic controller to the multiagent
case. Multiagent controllers include a correlation device, which is a source of randomness
shared by all the agents. This shared randomness increases solution quality while minimally
increasing representation size without adding communication. As in the single agent case,
policy iteration alternates between exhaustive backups and value-preserving transforma103

Bernstein, Amato, Hansen, & Zilberstein

value function after
DP update
value function for
controller

s1

s2

Figure 5: A local optimum for bounded backups. The solid line is the value function for the
controller, and the dotted line is the value function for the controller that results
from a full DP update.

tions. A convergence proof is given, along with efficient transformations that extend those
presented in the previous section.
4.1 Correlated Finite-State Controllers
The joint policy for the agents is represented using a stochastic finite-state controller for
each agent. In this section, we first define a type of controller in which the agents act
independently. We then provide an example demonstrating the utility of correlation, and
show how to extend the definition of a controller to allow for correlation among agents.
4.1.1 Local Finite-State Controllers
In a local controller, the agent’s node is based on the local observations received, and the
agent’s action is based on the current node. These local controllers are defined in the same
way as the POMDP controllers above, with each agent possessing its own controller that
operates independently of the others. As before, stochastic transitions and action selection
are allowed.
We formally define a local controller for agent i as a tuple hQi , Ωi , Ai , ψi , ηi i, where
• Qi is a finite set of controller nodes.
• Ωi is a set of inputs, taken to be the local observations for agent i.
• Ai is a set of outputs, taken to be the actions for agent i.
• ψi : Qi → ∆Ai is an action selection function for agent i, defining the distribution of
actions selected at each node of that agent’s controller.
• ηi : Qi × Ai × Ωi → ∆Qi is a transition function for agent i, defining the distribution
of resulting nodes for each initial node and action taken of that agent’s controller.
The functions ψi and ηi parameterize the conditional distribution P (ai , qi0 |qi , oi ) which represents the combined action selection and node transition probability for agent i. When
104

Policy Iteration for DEC-POMDPs

AB
BA
BB

AA

AA
AB
BA

+R

–R

–R

s1

s2

+R

BB

Figure 6: A DEC-POMDP for which a correlated joint policy yields more reward than the
optimal independent joint policy.

taken together, the agents’ controllers determine the conditional distribution P (~a, ~q 0 |~q, ~o).
This is denoted an independent joint controller. In the following subsection, we show that
independence can be limiting.
4.1.2 The Utility of Correlation
The joint controllers described above do not allow the agents to correlate their behavior
via a shared source of randomness. We will use a simple example to illustrate the utility
of correlation in partially observable domains where agents have limited memory. This
example generalizes the one given by Singh (1994) to illustrate the utility of stochastic
policies in partially observable settings containing a single agent.
Consider the DEC-POMDP shown in Figure 6. This problem has two states, two agents,
and two actions per agent (A and B). The agents each have only one observation, and
thus cannot distinguish between the two states. For this example, we will consider only
memoryless policies.
Suppose that the agents can independently randomize their behavior using distributions
P (a1 ) and P (a2 ). If the agents each choose either A or B according to a uniform distribution,
then they receive an expected reward of − R2 per time step, and thus an expected long-term
−R
reward of 2(1−β)
. It is straightforward to show that no independent policy yields higher
reward than this one for all states.
Next, let us consider the even larger class of policies in which the agents may act in a
correlated fashion. In other words, we consider all joint distributions P (a1 , a2 ). Consider
the policy that assigns probability 21 to the pair AA and probability 12 to the pair BB. This
yields an average reward of 0 at each time step and thus an expected long-term reward of
0. The difference between the rewards obtained by the independent and correlated policies
can be made arbitrarily large by increasing R.
105

Bernstein, Amato, Hansen, & Zilberstein

4.1.3 Correlated Joint Controllers
In the previous subsection, we established that correlation can be useful in the face of
limited memory. In this subsection, we extend our definition of a joint controller to allow for
correlation among the agents. To do this, we introduce an additional finite-state machine,
called a correlation device, that provides extra signals to the agents at each time step. The
device operates independently of the DEC-POMDP process, and thus does not provide
agents with information about the other agents’ observations. In fact, the random numbers
necessary for its operation could be determined prior to execution time and made available
to all agents.
Formally, a correlation device is a tuple hQc , ψc i, where Qc is a set of nodes and ψc :
Qc → ∆Qc is a state transition function. At each step, the device undergoes a transition,
and each agent observes its state.
We must modify the definition of a local controller to take the state of the correlation
device as input. Now, a local controller for agent i is a conditional distribution of the
form P (ai , qi0 |qc , qi , oi ). The correlation device together with the local controllers form a
joint conditional distribution P (~a, ~q 0 |~q, ~o), where ~q = hqc , q1 , . . . , qn i. We will refer to this
as a correlated joint controller. Note that a correlated joint controller with |Qc | = 1 is
effectively an independent joint controller. Figure 7 contains a graphical representation of
the probabilistic dependencies in a correlated joint controller.
The value function for a correlated joint controller can be computed by solving the
~
following system of linear equations, one for each s ∈ S and ~q ∈ Q:

V (s, ~q) =

X



P (~a|~q) R(s, ~a) + β

X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)V (s0 , ~q 0 ) .

s0 ,~
o,~
q0

~a

We sometimes refer to the value of the controller for an initial state distribution. For a
distribution b, this is defined as
V (b) = max
q~

X

b(s)V (s, ~q).

s

It is assumed that, given an initial state distribution, the controller is started in the joint
node which maximizes value from that distribution.
It is worth noting that correlation can increase the value of a set of fixed-size controllers,
but this same value can be achieved by a larger set of uncorrelated controllers. Thus,
correlation is a way to make better use of limited representation size, but is not required
to produce a set of optimal controllers. This is formalized by the following theorem, which
is proved in Appendix A. The theorem asserts the existence of uncorrelated controllers;
determining how much extra memory is needed to replace a correlation device remains an
open problem.
Theorem 1 Given an initial state and a correlated joint controller, there always exists
some finite-size joint controller without a correlation device that produces at least the same
value for the initial state.
106

Policy Iteration for DEC-POMDPs

a1

q1

q2

a2

q’2

o2

qc
o1

q’1

Figure 7: A graphical representation of the probabilistic dependencies in a correlated joint
controller for two agents.

In the example above, higher value can be achieved with two node uncorrelated controllers for each agent. If the problem starts in s1 , the first node for each agent would choose
A and transition to the second node which would choose B. The second node would then
transition back to the first node. The resulting policy consists of the agents alternating
R
between choosing AA and BB, producing an expected long-term reward of 1−β
which is
higher than the correlated one node policy value of 0. Thus, doubling memory for each
agent in this problem is sufficient to remove the correlation device.
4.2 Policy Iteration
In this section, we describe the policy iteration algorithm. We first extend the definitions of
exhaustive backup and value-preserving transformation to the multiagent case. Following
that, we provide a description of the complete algorithm, along with a convergence proof.
4.2.1 Exhaustive Backups
We introduced exhaustive backups in the section on dynamic programming for POMDPs.
We stated that one way to implement a DP update was to perform an exhaustive backup,
and then prune dominated nodes that were created. More efficient implementations were
described thereafter. These implementations involved interleaving pruning with node generation.
For the multiagent case, it is an open problem whether pruning can be interleaved
with node generation. Nodes can be removed, as we will show in a later subsection, but for
convergence we require exhaustive backups. We do not define DP updates for the multiagent
case, and instead make exhaustive backups a central component of our algorithm.
An exhaustive backup adds nodes to the local controllers for all agents at once, and
leaves the correlation device unchanged. For each agent i, |Ai ||Qi ||Ωi | nodes are added
to the
Q local controller, one for each one-step policy. Thus, the joint controller grows by
|Qc | i |Ai ||Qi ||Oi | joint nodes.
Note that repeated application of exhaustive backups amounts to a brute force search
in the space of deterministic policies. This converges to optimality, but is obviously quite
inefficient. As in the single agent case, we must modify the joint controller in between
107

Bernstein, Amato, Hansen, & Zilberstein

adding new nodes. For convergence, these modifications must preserve value in a sense that
will be made formal in the following section.
4.2.2 Value-Preserving Transformations
We now extend the definition of a value-preserving transformation to the multiagent case.
In the following subsection, we show how this definition allows for convergence to optimality
as the number of iterations grows.
The dual interpretation of dominance is helpful in understanding multiagent valuepreserving transformations. Recall that for a POMDP, we say that a node is dominated if
there is a convex combination of other nodes with value at least as high for all states. Though
we defined a value-preserving transformation in terms of the value function across belief
states, we could have equivalently defined it so that every node in the original controller
has a dominating convex combination in the new controller.
For the multiagent case, we do not have the concept of a belief state MDP, so we take
the second approach mentioned above. In particular, we require that dominating convex
combinations exist for nodes of all the local controllers and the correlation device. A transformation of a controller C to a controller D qualifies as a value-preserving transformation
if C ≤ D, where ≤ is defined below.
~ and R,
~ respectively. We
Consider correlated joint controllers C and D with node sets Q
say that C ≤ D if there exist mappings fi : Qi → ∆Ri for each agent i and fc : Qc → ∆Rc
such that
X
V (s, ~q) ≤
P (~r|~q)V (s, ~r)
~
r

~ Note that this relation is transitive as further value-preserving
for all s ∈ S and ~q ∈ Q.
transformations of D will also be value-preserving transformations of C.
~ → ∆R.
~ Examples
We sometimes describe the fi and fc as a single mapping f : Q
of efficient value-preserving transformations are given in a later section. In the following subsection, we show that alternating between exhaustive backups and value-preserving
transformations yields convergence to optimality.
4.2.3 Algorithmic Framework
The policy iteration algorithm is initialized with an arbitrary correlated joint controller. In
the first part of an iteration, the controller is evaluated via the solution of a system of linear
equations. Next, an exhaustive backup is performed to add nodes to the local controllers.
Finally, value-preserving transformations are performed.
In contrast to the single agent case, there is no Bellman residual for testing convergence to -optimality. We resort to a simpler test for -optimality based on the discount
rate and the number of iterations so far. Let |Rmax | be the largest absolute value of an
immediate reward possible in the DEC-POMDP. Our algorithm terminates after iteration
t+1 |R
max |
t if β 1−β
≤ . At this point, due to discounting, the value of any policy after step t is
less than . Justification for this test is provided in the convergence proof. The complete
algorithm is sketched in Table 5.
Before proving convergence, we state a key lemma regarding the ordering of exhaustive
backups and value-preserving transformations. Its proof is deferred to the Appendix.
108

Policy Iteration for DEC-POMDPs

Input: A correlated joint controller, and a parameter .
1. Evaluate the correlated joint controller by solving a system of linear equations.
2. Perform an exhaustive backup to add deterministic nodes to the local controllers.
3. Perform value-preserving transformations on the controller.
t+1

|Rmax |
4. If β 1−β
≤ , where t is the number of iterations so far, then terminate. Else go
to step 1.

Output: A correlated joint controller that is -optimal for all states.
Table 5: Policy Iteration for DEC-POMDPs.
Lemma 1 Let C and D be correlated joint controllers, and let Ĉ and D̂ be the results of
performing exhaustive backups on C and D, respectively. Then Ĉ ≤ D̂ if C ≤ D.
Thus, if there is a value-preserving transformation mapping controller C to D and both are
exhaustively backed up, then there is a value-preserving transformation mapping controller
Ĉ to D̂. This allows value-preserving transformations to be performed before exhaustive
backups, while ensuring that value is not lost after the backup. We can now state and prove
the main convergence theorem for policy iteration.
Theorem 2 For any , policy iteration returns a correlated joint controller that is -optimal
for all initial states in a finite number of iterations.
Proof: Repeated application of exhaustive backups amounts to a brute force search in
the space of deterministic joint policies. Thus, after t exhaustive backups, the resulting
controller is optimal for t steps from any initial state. Let t be an integer large enough that
β t+1 |Rmax |
≤ . Then any possible discounted sum of rewards after t time steps is small
1−β
enough that optimality over t time steps implies -optimality over the infinite horizon.
Now recall the above lemma, which states that performing value-preserving transformations before a backup provides at least as much value as just performing a backup. By an
inductive argument, performing t steps of policy iteration is a value-preserving transformation of the result of t exhaustive backups. We have argued that for large enough t, the value
of the controller resulting from t exhaustive backups is within  of optimal for all states.
Thus, the result of t steps of policy iteration is also within  of optimal for all states. 2
4.3 Efficient Value-Preserving Transformations
In this section, we describe how to extend controller reductions and bounded backups
to the multiagent case. We will show that both of these operations are value-preserving
transformations.
4.3.1 Controller Reductions
Recall that in the single agent case, a node can be removed if for all belief states, there is
another node with value at least as high. The equivalent dual interpretation is that a node
109

Bernstein, Amato, Hansen, & Zilberstein

can be removed is there exists a convex combination of other nodes with value at least as
high across the entire state space.
Using the dual interpretation, we can extend this to a rule for removing nodes in the
multiagent case. The rule applies to removing nodes either from a local controller or from the
correlation device. Intuitively, in considering the removal of a node from a local controller
or the correlation device, we consider the nodes of the other controllers to be part of the
hidden state.
More precisely, suppose we are considering removing node qi from agent i’s local controller. To do this, we need to find a distribution P (q̂i ) over nodes q̂i ∈ Qi \ qi such that for
all s ∈ S, q−i ∈ Q−i , and qc ∈ Qc ,
V (s, qi , q−i , qc ) ≤

X

P (q̂i )V (s, q̂i , q−i , qc ).

q̂i

where Q−i represents the set of nodes for the other agents. Finding such a distribution
can be formulated as a linear program, as shown in Table 6a. In this case, success is
finding parameters such that  ≥ 0. The linear program is polynomial in the sizes of the
DEC-POMDP and controllers, but exponential in the number of agents.
If we are successful in finding parameters that make  ≥ 0, then we can merge the
dominated node into the convex combination of other nodes by changing all incoming links
to the dominated controller node to be redirected based on the distribution P (q̂i ). At this
point, there is no chance of ever transitioning into qi , and thus it can be removed.
The rule for the correlation device is very similar. Suppose that we are considering the
removal of node qc . In this case, we need to find a distribution P (q̂c ) over nodes q̂c ∈ Qc \ qc
~
such that for all s ∈ S and ~q ∈ Q,
V (s, ~q, qc ) ≤

X

P (q̂c )V (s, ~q, q̂c ).

q̂c

~ for the set of tuples of local controller nodes,
Note that we abuse notation here and use Q
excluding the nodes for the correlation device. As in the previous case, finding parameters
can done using linear programming. This is shown in Table 6b. This linear program is also
polynomial in the the sizes of the DEC-POMDP and controllers, but exponential in the
number of agents.
We have the following theorem, which states that controller reductions are value-preserving
transformations.
Theorem 3 Any controller reduction applied to either a local node or a node of the correlation device is a value-preserving transformation.
Proof: Suppose that we have replaced an agent i node qi with a distribution over nodes
in Qi \ qi . Let us take fi to be the identity map for all nodes except qi , which will map to
the new distribution. We take fc to be the identity map, and we take fj to be the identity
map for all j 6= i. This yields a complete mapping f . We must now show that f satisfies
the condition given in the definition of a value-preserving transformation.
110

Policy Iteration for DEC-POMDPs

(a) Variables: , x(q̂i )
Objective: Maximize 
Improvement constraints:
∀s, q−i , qc

V (s, qi , q−i , qc ) +  ≤

X

x(q̂i )V (s, q̂i , q−i , qc )

q̂i

Probability constraints:
X

∀q̂i

x(q̂i ) = 1,

x(q̂i ) ≥ 0

q̂i

(b) Variables: , x(qc )
Objective: Maximize 
Improvement constraints:
∀s, ~q V (s, ~q, qc ) +  ≤

X

x(q̂c )V (s, ~q, q̂c )

qˆc

Probability constraints:
X

∀q̂c

x(q̂c ) = 1,

x(q̂c ) ≥ 0

q̂c

Table 6: (a) The linear program to be solved to find a replacement for agent i’s node qi .
The variable x(q̂i ) represents P (q̂i ). (b) The linear program to be solved to find a
replacement for the correlation node qc . The variable x(q̂c ) represents P (q̂c ).

Let Vo be the value function for the original controller, and let Vn be the value function
for the controller with qi removed. A controller reduction requires that
Vo (s, ~q) ≤

X

P (~r|~q)Vo (s, ~r)

~
r

~ Thus, we have
for all s ∈ S and ~q ∈ Q.

Vo (s, ~q) =

X

P (~a|~q) R(s, a) + β


X

P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)Vo (s0 , ~q 0 )

s0 ,~
o,~
q0

~a


≤

X
~a

P (~a|~q) R(s, a) + β


X

P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)

s0 ,~
o,~
q0

111

X
~
r0

P (~r|~q)Vo (s, ~r 0 )

Bernstein, Amato, Hansen, & Zilberstein


=

X

P (~a|~q) R(s, a) + β


X

P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)P (~r|~q)Vo (s, ~r 0 )

s0 ,~
o,~
q 0 ,~
r0

~a

~ Notice that the formula on the right is the Bellman operator
for all s ∈ S and ~q ∈ Q.
for the new controller, applied to the old value function. Denoting this operator Tn , the
system of inequalities implies that Tn Vo ≥ Vo . By monotonicity, we have that for all k ≥ 0,
Tnk+1 (Vo ) ≥ Tnk (Vo ). Since Vn = limk→∞ Tnk (Vo ), we have that Vn ≥ Vo . This is sufficient
for f to satisfy the condition in the definition of value-preserving transformation.
The argument for removing a node of the correlation device is almost identical to the
one given above. 2
4.3.2 Bounded Dynamic Programming Updates
In the previous section, we described a way to reduce the size of a controller without
sacrificing value. Recall that in the single agent case, we could also use bounded backups
to increase the value of the controller while keeping its size fixed. This technique can
be extended to the multiagent case. As in the previous section, the extension relies on
improving a single local controller or the correlation device, while viewing the nodes of the
other controllers as part of the hidden state.
We first describe in detail how to improve a local controller. To do this, we choose an
agent i, along with a node qi . Then, for each oi ∈ Ωi , we search for new parameters for the
conditional distribution P (ai , qi0 |qi , oi ).
The search for new parameters works as follows. We assume that the original controller
will be used from the second step on, and try to replace the parameters for qi with better
ones for just the first step. In other words, we look for parameters satisfying the following
inequality:


X
X
V (s, ~q) ≤
P (~a|~q) R(s, a) + β
P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)V (s0 , ~q 0 )
~a

s0 ,~
o,~
q0

for all s ∈ S, q−i ∈ Q−i , and qc ∈ Qc . The search for new parameters can be formulated as a
linear program, as shown in Table 7a. Its size is polynomial in the sizes of the DEC-POMDP
and the joint controller, but exponential in the number of agents.
The procedure for improving the correlation device is very similar to the procedure for
improving a local controller. We first choose a device node qc , and consider changing its
parameters for just the first step. We look for parameters satisfying the following inequality:


X
X
P (~a|~q) R(s, a) + β
P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)V (s0 , ~q 0 )
V (s, ~q) ≤
~a

s0 ,~
o,~
q0

~
for all s ∈ S and ~q ∈ Q.
As in the previous case, the search for parameters can be formulated as a linear program.
This is shown in Table 7b. This linear program is also polynomial in the sizes of the DECPOMDP and joint controller, but exponential in the number of agents.
The following theorem states that bounded backups preserve value.
112

Policy Iteration for DEC-POMDPs

(a) Variables: , x(qc , ai ), x(qc , ai , oi , qi0 )
Objective: Maximize 
Improvement constraints:
∀s, q−i , qc

X

V (s, ~q, qc ) +  ≤

P (a−i |qc , q−i )[x(qc , ai )R(s, ~a) +

~a

β

X

0
x(c, ai , oi , qi0 )P (q−i
|qc , q−i , a−i , o−i )

s0 ,~
o,~
q 0 ,qc0

· P (~o, s0 |s, ~a)P (qc0 |qc )V (s0 , ~q 0 , qc0 )]

Probability constraints:
X
∀qc
x(qc , ai ) = 1,

∀qc , ai , oi

x(qc , ai , oi , qi0 ) = x(qc , ai )

qi0

ai

∀qc , ai

X

x(qc , ai ) ≥ 0,

∀qc , ai , oi , qi0

x(qc , ai , oi , qi0 ) ≥ 0

(b) Variables: , x(qc0 )
Objective: Maximize 
Improvement constraints:
∀s, ~q V (s, ~q, qc ) +  ≤

X

P (~a|qc , ~q)[R(s, ~a) + β

X

P (~q 0 |qc , ~q, ~a, ~o)

s0 ,~
o,~
q 0 ,qc0

~a
0

· P (s , ~o|s, ~a)x(qc0 )V (s0 , ~q 0 , qc0 )]

Probability constraints:
∀qc0

X

x(qc0 ) = 1,

∀qc0

x(qc0 ) ≥ 0

qc0

Table 7: (a) The linear program used to find new parameters for agent i’s node qi . The
variable x(qc , ai ) represents P (ai |qi , qc ), and the variable x(qc , ai , oi , qi0 ) represents
P (ai , qi0 |qc , qi , oi ). (b) The linear program used to find new parameters for the
correlation device node qc . The variable x(qc0 ) represents P (qc0 |qc ).

113

Bernstein, Amato, Hansen, & Zilberstein

Theorem 4 Performing a bounded backup on a local controller or the correlation device
produces a new correlated joint controller which is a value-preserving transformation of the
original.
Proof: Consider the case in which some node qi of agent i’s local controller is changed.
We define f to be a deterministic mapping from nodes in the original controller to the
corresponding nodes in the new controller.
Let Vo be the value function for the original controller, and let Vn be the value function
for the new controller. Recall that the new parameters for P (ai , qi0 |qc , qi , oi ) must satisfy
the following inequality for all s ∈ S, q−i ∈ Q−i , and qc ∈ Qc :


X
X
Vo (s, ~q) ≤
P (~a|~q) R(s, a) + β
P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)Vo (s0 , ~q 0 ) .
~a

s0 ,~
o,~
q0

Notice that the formula on the right is the Bellman operator for the new controller, applied
to the old value function. Denoting this operator Tn , the system of inequalities implies
that Tn Vo ≥ Vo . By monotonicity, we have that for all k ≥ 0, Tnk+1 (Vo ) ≥ Tnk (Vo ). Since
Vn = limk→∞ Tnk (Vo ), we have that Vn ≥ Vo . Thus, the new controller is a value-preserving
transformation of the original one.
The argument for changing nodes of the correlation device is almost identical to the one
given above. 2
4.4 Open Issues
We noted at the beginning of the section that there is no known way to convert a DECPOMDP into an equivalent belief-state MDP. Despite this fact, we were able to develop
a provably convergent policy iteration algorithm. However, the policy iteration algorithm
for POMDPs has other desirable properties besides convergence, and we have not yet been
able to extend these to the multiagent case. Two such properties are described below.
4.4.1 Error Bounds
The first property is the existence of a Bellman residual. In the single agent case, it
is possible to compute a bound on the distance to optimality using two successive value
functions. In the multiagent case, policy iteration produces a sequence of controllers, each
of which has a value function. However, we do not have a way to obtain an error bound
from these value functions. For now, to bound the distance to optimality, we must consider
the discount rate and the number of iterations completed.
4.4.2 Avoiding Exhaustive Backups
In performing a DP update for POMDPs, it is possible to remove certain nodes from
consideration without first generating them. In Section 3, we gave a high-level description
of a few different approaches to doing this. For DEC-POMDPs, however, we did not define
a DP update and instead used exhaustive backups as the way to expand a controller. Since
exhaustive backups are expensive, it would be useful to extend the more sophisticated
pruning methods for POMDPs to the multiagent case.
114

Policy Iteration for DEC-POMDPs

Input: A joint controller, the desired number of centralized belief points k, initial state
b0 and fixed policy for each agent πi .
1. Starting from b0 , sample a set of k belief points for each agent assuming the other
agents use their fixed policy.
2. Evaluate the joint controller by solving a system of linear equations.
3. Perform an exhaustive backup to add deterministic nodes to the local controllers.
4. Retain nodes that contribute the highest value at each of the belief points.
5. For each agent, replace nodes that have lower value than some combination of other
nodes at each belief point.
6. If controller sizes and parameters do not change then terminate. Else go to step 2.
Output: A new joint controller based on the sampled centralized belief points.
Table 8: Heuristic Policy Iteration for DEC-POMDPs.

Unfortunately, in the case of POMDPs, the proofs of correctness for these methods all
use the fact that there exists a Bellman equation. Roughly speaking, this equation allows us
to determine whether a potential node is dominated by just analyzing the nodes that would
be its successors. Because we do not currently have an analog of the Bellman equation for
DEC-POMDPs, we have not been able to generalize these results.
There is one exception to the above statement, however. When an exhaustive backup
has been performed for all agents except one, then a type of belief state space can be
constructed for the agent in question using the system states and the nodes for the other
agents. The POMDP node generation methods can then be applied to just that agent. In
general, though, it seems difficult to rule out a node for one agent before generating all the
nodes for the other agents.

5. Heuristic Policy Iteration
While the optimal policy iteration method shows how a set of controllers with value arbitrarily close to optimal can be found, the resulting controllers may be very large and many
unnecessary nodes may be generated along the way. This is exacerbated by the fact that
the algorithm cannot take advantage of an initial state distribution and must attempt to
improve the controller for any initial state. As a way to combat these disadvantages, we
have developed a heuristic version of policy iteration that removes nodes based on their
value only at a given set of centralized belief points. We call these centralized belief points
because they are distributions over the system state that in general could only be known
by full observability of the problem. As a result, the algorithm will no longer be optimal,
but it can often produce more concise controllers with higher solution quality for a given
initial state distribution.
115

Bernstein, Amato, Hansen, & Zilberstein

5.1 Directed Pruning
Our heuristic policy iteration algorithm uses sets of belief points to direct the pruning process of our algorithm. There are two main advantages of this approach: it allows simultaneous pruning for all agents and it focuses the controller on certain areas of the belief space.
We first discuss the benefits of simultaneous pruning and then mention the advantages of
focusing on small areas of the belief space.
As mentioned above, the pruning method used by the optimal algorithm will not always
remove all nodes that could be removed from all the agents’ controllers without losing
value. Because pruning requires each agent to consider the controllers of other agents, after
nodes are removed for one agent, the other agents may be able to prune other nodes. Thus
pruning must cycle through the agents and ceases when no agent can remove any further
nodes. This is both time consuming and causes the controller to be much larger than it
needs to be.
Like the game theoretic concept of incredible threats1 , a set of suboptimal policies for
an agent may be useful only because other agents may employ similarly suboptimal policies. That is, because pruning is conducted for each agent while holding the other agents’
policies fixed, polices that are useful for any set of other agent policies are retained, no
matter the quality of these other agent policies. Some of an agent’s policies may only be
retained because they have the highest value when used in conjunction with other suboptimal policies of the other agents. In these cases, only by removing the set of suboptimal
policies simultaneously can controller size be reduced while at least maintaining value. This
simultaneous pruning could further reduce controller sizes and thus increase scalability and
solution quality. While it may be possible to define a value-preserving transformation for
these problems, finding a nontrivial automated way to do so while maintaining the optimality of the algorithm remains an open question.
The advantage of considering a smaller part of the state space has already been shown
to produce drastic performance increases in POMDPs (Ji, Parr, Li, Liao, & Carin, 2007;
Pineau, Gordon, & Thrun, 2003) and finite-horizon DEC-POMDPs (Seuken & Zilberstein,
2007; Szer & Charpillet, 2006). For POMDPs, a problem with many states has a belief
space with large dimensionality, but many parts may never be visited by an optimal policy.
Focusing on a subset of belief states can allow a large part of the state space to be ignored
without significant loss of solution quality.
The problem of having a large state space is compounded in the DEC-POMDP case.
Not only is there uncertainty about the state, but also about the policies of the other agents.
As a consequence, the generalized belief space which includes all possible distributions over
states of the system and current policies of the other agents must be considered to guarantee
optimality. This results in a huge space which contains many unlikely states and policies.
The uncertainty about which policies other agents may utilize does not allow belief updates
to normally be calculated for DEC-POMDPs, but as we showed above, it can be done by
assuming a probability distribution over actions of the other agents. This limits the number
of policies that need to be considered by all agents and if the distributions are chosen well,
may permit a high-valued solution to be found.
1. An incredible threat is an irrational strategy that the agent knows it will receive a lower value by choosing
it. While it is possible the agent will choose the incredible threat strategy, it is irrational to do so.

116

Policy Iteration for DEC-POMDPs

Variables: , x(q̂i ) and for each belief point b
Objective: Maximize 
Improvement constraints:

∀b, q−i

X

X

b(s)
x(q̂i )V (q̂i , q−i , s) − V (~q, s) ≥ 

s

X

Probability constraints:

q̂i

x(q̂i ) = 1 and ∀q̂i x(qi ) ≥ 0

q̂i

Table 9: The linear program used to determine if a node q for agent i is dominated at
each point b and all initial nodes of the other agents’ controllers. As node q
may be dominated by a distribution of nodes, variable x(q̂i ) represents P (q̂i ), the
probability of starting in node q̂ for agent i.

5.2 Belief Set Generation
As mentioned above, our heuristic policy iteration algorithm constructs sets of belief points
for each agent which are later used to evaluate the joint controller and remove dominated
nodes. To generate the belief point set, we start at the initial state and by making assumptions about the other agents, we can calculate the resulting belief state for each action and
observation pair of an agent. By fixing the policies for the other agents, this belief state update can be calculated in a way very similar to that described for POMDPs in section 3.1.1.
This procedure can be repeated from each resulting belief state until a desired number of
points is generated or no new points are visited.
More formally, we assume the other agents have a fixed distribution of action choice for
each system state. That is, if we know P (~a−i |s) then we can determine the probability any
state results given a belief point and an agent’s action and observation. The derivation of
the likelihood of state s0 , given the belief state b, and agent i’s action ai and observation oi
is shown below.

P (s0 |ai , oi , b) =

X

P (s0 , ~a−i , ~o−i , s|ai , oi , b)

~a−i ,~
o−i ,s

o|s, b, ~a, s0 )P (s0 , s, ~a, b)
~a−i ,~
o−i ,s P (~

P
=

P (oi , ai , b)
o|s, ~a, s0 )P (s0 |s, ~a, b)P (~a, s, b)
~a−i ,~
o−i ,s P (~

P
=

P (oi , ai , b)
o|s, ~a, s0 )P (s0 |s, ~a)P (~a−i |a, s, b)P (~a, s, b)
~a−i ,~
o−i ,s P (~

P
=

P (oi , ai , b)
0 )P (s0 |s, ~
P
(~
o
|s,
~
a
,
s
a)P (~a−i |ai , s, b)P (s|ai , b)P (ai , b)
~a−i ,~
o−i ,s

P
=

P (oi , ai , b)
117

Bernstein, Amato, Hansen, & Zilberstein

P
=

o|s, ~a, s
~a−i ,~
o−i ,s P (~

0 )P (s0 |s, ~
a)P (~a−i |s)b(s)

P (oi |ai , b)

where
X

P (oi |ai , b) =

P (~o|s, ~a, s0 )P (s0 |s, ~a)P (~a−i |s)b(s)

a−i ,o−i ,s,s0

Thus, given the action probabilities for the other agents, −i, and the transition and observation models of the system, a belief state update can be calculated.
5.3 Algorithmic Framework
We provide a formal description of our approach in Table 8. Given the desired number
of belief points, k, and random action and observation selection for each agent, the sets
of points are generated as described above. The search begins at the initial state of the
problem and continues until the given number of points is obtained. If no new points are
found, this process can be repeated to ensure a diverse set is produced. The arbitrary initial
controller is evaluated and the value at each state and for each initial node of any agent’s
controller is retained. The exhaustive backup procedure is exactly the same as the one used
in the optimal algorithm, but updating the controller takes place in two steps. First, for
each of the k belief points, the highest-valued set of initial nodes is found. To accomplish
this, the value of beginning at each combination of nodes for all agents is calculated for each
of these k points and the best combination is kept. This allows nodes that do not contribute
to any of these values to be simultaneously pruned. Next, each node of each agent is pruned
using the linear program shown in Table 9. If a distribution of nodes for the given agent has
higher value at each of the belief points for any initial nodes of the other agents’ controllers,
it is pruned and replaced with that distribution. The new controllers are then evaluated
and the value is compared with the value of the previous controller. This process of backing
up and pruning continues while the controller parameters continue to change.
Similar to how bounded policy updates can be used in conjunction with pruning in the
optimal policy iteration algorithm, a nonlinear programming approach (Amato et al., 2007)
can be used to improve solution quality for the heuristic case. To accomplish this, instead of
optimizing the controller for just the initial belief state of the problem, all the belief points
being considered are used. A simple way to achieve this is to maximize over the sum of
the values of the initial nodes of the controllers weighted by the probabilities given for each
point. This approach can be used after each pruning step and may further improve value
of the controllers.

6. Dynamic Programming Experiments
This section describes the results of experiments performed using policy iteration. Because
of the flexibility of the algorithm, it is impossible to explore all possible ways of implementing
it. However, we did experiment with a few different implementation strategies to gain an
idea of how the algorithm works in practice. All of these experiments were run on a 3.40GHz
Intel Pentium 4 with 2GB of memory. Three main sets of experiments were performed on
a single set of test problems.
118

Policy Iteration for DEC-POMDPs

Our first set of experiments focused on exhaustive backups and controller reductions.
The results confirm that value improvement can be obtained through iterated application of
these two operations. Further improvement is demonstrated by also incorporating bounded
updates. However, because exhaustive backups are expensive, the algorithm was unable to
complete more than a few iterations on any of our test problems.
In the second set of experiments, we addressed the complexity issues by using only
bounded backups, and no exhaustive backups. With bounded backups, we were able to
obtain higher-valued controllers while keeping memory requirements fixed. We examined
how the sizes of the initial local controllers and the correlation device affected the value of
the final solution.
The third set of experiments examined the complexity issues caused by exhaustive backups by using the point-based heuristic. This allowed our heuristic policy iteration algorithm
to complete more iterations than the optimal algorithm and in doing so, increased solution
quality of the largest solvable controllers. By incorporating Amato et al.’s NLP approach,
the heuristic algorithm becomes slightly less scalable than with heuristic pruning alone, but
the amount of value improvement per step increases. This causes the resulting controllers
in each domain to have the highest value of any approach.
6.1 Test Domains
In this section, we describe three test domains, ordered by the size of the problem representation. For each problem, the transition function, observation function, and reward
functions are described. In addition, an initial state is specified. Although policy iteration
does not require an initial state as input, one is commonly assumed and is used by the
heuristic version of the algorithm. A few different initial states were tried for each problem,
and qualitatively similar results were obtained. In all domains, a discount factor of 0.9 was
utilized.
As a very loose upper bound, the centralized policy was calculated for each problem in
which all agents share their observations with a central agent and decisions for all agents are
made by the central agent. This results in a POMDP with the same number of states, but
the action and observation sets are Cartesian products of the agents action and observation
sets. The value of this POMDP policy is provided below, but because DEC-POMDP policies
are more constrained, the optimal value may be much lower.
Two Agent Tiger Problem
The two agent tiger problem consists of 2 states, 3 actions and 2 observations (Nair et al.,
2003). The domain includes two doors, one of which leads to a tiger and the other to a large
treasure. Each agent may open one of the doors or listen. If either agent opens the door
with the tiger behind it, a large penalty is given. If the door with the treasure behind it is
opened and the tiger door is not, a reward is given. If both agents choose the same action
(i.e., both opening the same door) a larger positive reward or a smaller penalty is given to
reward cooperation. If an agent listens, a small penalty is given and an observation is seen
that is a noisy indication of which door the tiger is behind. While listening does not change
the location of the tiger, opening a door causes the tiger to be placed behind one of the
119

Bernstein, Amato, Hansen, & Zilberstein

door with equal probability. The problem begins with the tiger equally likely to be located
behind either door. The optimal centralized policy for this problem has value 59.817.
Meeting on a Grid
In this problem, with 16 states, 5 actions and 4 observations, two robots must navigate on a
two-by-two grid. Each robot can only sense whether there are walls to its left or right, and
their goal is to spend as much time as possible on the same square as the other agent. The
actions are to move up, down, left, or right, or to stay on the same square. When a robot
attempts to move to an open square, it only goes in the intended direction with probability
0.6, otherwise it either goes in another direction or stays in the same square. Any move
into a wall results in staying in the same square. The robots do not interfere with each
other and cannot sense each other. The reward is 1 when the agents share a square, and
0 otherwise. The initial state places the robots diagonally across from each other and the
optimal centralized policy for this problem has value 7.129.
Box Pushing Problem
This problem, with 100 states, 4 actions and 5 observations consists of two agents that
get rewarded by pushing different boxes (Seuken & Zilberstein, 2007). The agents begin
facing each other in the bottom corners of a four-by-three grid with the available actions of
turning right, turning left, moving forward or staying in place. There is a 0.9 probability
that the agent will succeed in moving and otherwise will stay in place, but the two agents
can never occupy the same square. The middle row of the grid contains one large box in the
middle of two small boxes. The small boxes can be moved by a single agent, but the large
box can only be moved by both agents pushing at the same time. The upper row of the
grid is considered the goal row, which the boxes are pushed into. The possible deterministic
observations for each agent consist of seeing an empty space, a wall, the other agent, a small
box or the large box. A reward of 100 is given if both agents push the large box to the
goal row and 10 is given for each small box that is moved to the goal row. A penalty of -5
is given for each agent that cannot move and -0.1 is given for each time step. Once a box
is moved to the goal row, the environment resets to the original start state. The optimal
centralized policy for this problem has value 183.936.
6.2 Exhaustive Backups and Controller Reductions
In this section, we present the results of using exhaustive backups together with controller
reductions. For each domain, the initial controllers for each agent contained a single node
with a self loop, and there was no correlation device. For each problem, the first action
of the problem description was used. This resulted in the repeated actions of opening the
left door in the two agent tiger problem, moving up in the meeting on a grid problem and
turning left in the box pushing problem. The reason for starting with the smallest possible
controllers was to see how many iterations we could complete before running out of memory.
On each iteration, we performed an exhaustive backup, and then alternated between
agents, performing controller reductions until no more nodes could be removed. For bounded
dynamic programming results, after the reductions were completed bounded updates were
also performed for all agents. For these experiments, we attempted to improve the nodes of
120

Policy Iteration for DEC-POMDPs

Iteration
0
1
2
3

Two Agent Tiger, |S| = 2, |Ai | = 3, |Ωi | = 2
Exhaustive Sizes
Controller Reductions
Bounded Updates
(1, 1)
-150 (1,1 in 1s)
-150 (1,1 in 1s)
(3, 3)
-137 (3,3 in 1s)
-20 (3,3 in 12s)
(27, 27)
-117.8 (15, 15 in 7s)
-20 (15, 15 in 89s)
(2187, 2187)
-98.9 (255, 255 in 1301s) -20* (255, 255 in 3145s)

Iteration
0
1
2

Meeting on a Grid, |S| = 16, |Ai | = 5, |Ωi | = 4
Exhaustive Sizes
Controller Reductions
Bounded Updates
(1, 1)
2.8 (1,1 in 1s)
2.8 (1,1 in 1s)
(5, 5)
3.4 (5,5 in 7s)
3.8 (5,5 in 145s)
(3125, 3125)
3.7 (80,80 in 821s)
4.78* (125,125 in 1204s)

Iteration
0
1
2

Box Pushing, |S| = 100, |Ai | = 4, |Ωi | = 5
Exhaustive Sizes
Controller Reductions
Bounded Updates
(1, 1)
-2 (1,1 in 4s)
-2 (1,1 in 53s)
(4, 4)
-2 (2,2 in 108s)
6.3 (2,2 in 132s)
(4096, 4096)
12.8 (9,9 in 755s)
42.7* (16,17 in 714s)

Table 10: Results of applying exhaustive backups, controller reductions and bounded updates to our test problems. The second column contains the sizes of the controllers
if only exhaustive backups had been performed. The third column contains the
resulting value, sizes of the controllers, and time required for controller reductions
to be performed on each iteration. The fourth column displays these same quantities with bounded updates also being used. The * denotes that a backup and
pruning were performed, but bounded updates exhausted the given resources.

each agent in turn until value could not be improved for any node of any agent. For each
iteration, we recorded the sizes of the controllers produced, and noted what the sizes would
be if no controller reductions had been performed. In addition, we recorded the value from
the initial state and the total time taken to reach the given result.
The results are shown in Table 10. Because exhaustive backups add many nodes, we
were unable to complete many iterations without exceeding memory limits. As expected,
the smallest problem led to the largest number of iterations being completed. Although
we could not complete many iterations before running out of memory, the use of controller
reductions led to significantly smaller controllers compared to the approach of just applying
exhaustive backups. Incorporating bounded updates requires some extra time, but is able
to improve the value produced at each step, causing substantial improvement in some cases.
It is also interesting to notice that the controller sizes when using bounded updates are
not always the same as when only controller reductions are completed. This can be seen
after two iterations in both the meeting on a grid and box pushing problems. This can
occur because the bounded updates change node value and thus change the number and
location of the nodes that are pruned. In the box pushing problem, the two agents also
121

Bernstein, Amato, Hansen, & Zilberstein

have different size controllers after two steps. This can occur, even in symmetric problems,
when a set of actions is only necessary for a single agent.
6.3 Bounded Dynamic Programming Updates
As we saw from the previous experiments, exhaustive backups can fill up memory very
quickly. This leads naturally to the question of how much improvement is possible without
exhaustive backups. In this section, we describe an experiment in which we repeatedly
applied bounded backups, which left the size of the controller fixed. We experimented with
different starting sizes for the local controllers and the correlation device.
We define a trial run of the algorithm as follows. At the start of a trial run, a size is
chosen for each of the local controllers and the correlation device. The action selection and
transition functions are initialized to be deterministic, with the outcomes drawn according
to a uniform distribution. A step consists of choosing a node uniformly at random from the
correlation device or one of the local controllers, and performing a bounded backup on that
node. After 200 steps, the run is considered over. In practice, we found that values often
stabilized in fewer steps.
We varied the sizes of the local controllers while maintaining the same number of nodes
for each agent, and we varied the size of the correlation device from 1 to 2. For each domain,
we increased number of nodes until the required number of steps could not be completed
in under four hours. In general, runs required significantly less time to terminate. For each
combination of sizes, we performed 20 trial runs and recorded the best value over all runs.
For each of the three problems, we were able to obtain solutions with higher value than
with exhaustive backups. Thus, we see that even though repeated application of bounded
backups does not have an optimality guarantee, it can be competitive with an algorithm that
does. However, it should be noted that we have not performed an exhaustive comparison.
We could have made different design decisions for both approaches concerning the starting
controllers, the order in which nodes are considered, and other factors.
Besides comparing to the exhaustive backup approach, we wanted to examine the effect
of the sizes of the local controllers and the correlation device on value. Figure 8 shows
a graph of best values plotted against controller size. We found that, for the most part,
the value increases when we increase the size of the correlation device from one node to
two nodes (essentially moving from independent to correlated). It is worth noting that the
solution quality had somewhat high variance in each problem, showing that setting good
initial parameters is important for high-valued solutions.
For small controllers, the best value tends to increase with controller size. However, for
very large controllers, this not always the case. This can be explained by considering how a
bounded backup works. For new node parameters to be acceptable, they must not decrease
the value for any combination of states, nodes for the other controllers, and nodes for the
correlation device. This becomes more difficult as the numbers of nodes increase, and thus
it is easier to get stuck in a local optimum. This can be readily seen in the two agent tiger
problem and to some extent the meeting on a grid problem. Memory was exhausted before
this phenomenon takes place in the box pushing problem.
122

Policy Iteration for DEC-POMDPs

(a)

(b)

(c)
Figure 8: Best value per trial run plotted against the size of the local controllers, for (a)
the two agent tiger problem, (b) the meeting in a grid problem and (c) the box
pushing problem. The solid line represents independent controllers (a correlation
device with one node), and the dotted line represents a joint controller including a
two-node correlation device. Times ranged from under 1s for one node controllers
without correlation to four hours for the largest controller found with correlation
in each problem.

6.4 Heuristic Dynamic Programming Updates
As observed above, the optimal dynamic programming approach can only complete a small
number of backups before resources are exhausted. Similarly, using bounded updates with
fixed size controllers can generate high value solutions, but it can be difficult to pick the
correct controller size and initial parameters. As an alternative to the other approaches, we
also present experiments using our heuristic dynamic programming algorithm.
Like the optimal policy iteration experiments, we initialized single node controllers for
each agent with self loops and no correlation device. The same first actions were used as
above and backups were performed until memory was exhausted. The set of belief points
for each problem was generated given the initial state distribution and a distribution of
actions for the other agents. For the meeting on a grid and box pushing problems, it was
123

Bernstein, Amato, Hansen, & Zilberstein

assumed that all agents chose any action with equal probability regardless of state. For the
two agent tiger problem, it was assumed that for any state agents listen with probability 0.8
and open each door with probability 0.1. This simple heuristic policy was chosen to allow
more of the state space to be sampled by our search. The number of belief points used for
the two agent tiger and meeting on a grid problems was ten and twenty points were used
for the box pushing problem.
For each iteration, we performed an exhaustive backup and then pruned controllers as
described in steps four and five of Table 8. All the nodes that contributed to the highest
value for each belief point were retained and then each node was examined using the linear
program in Table 9. For results with the NLP approach, we also improved the set of
controllers after heuristic pruning by optimizing a nonlinear program whose objective was
the sum of the values of the initial nodes weighted by the belief point probabilities. We
report the value produced by the optimal and heuristic approaches for each iteration that
could be completed in under four hours and with the memory limits of the machine used.
The nonlinear optimization was performed on the NEOS server, which provides a set of
machines with varying CPU speeds and memory limitations.
The values for each iteration of each problem are given in Figure 9. We see the heuristic policy iteration (HPI) methods are able to complete more iterations than the optimal
methods and as a consequence produce higher values. In fact, the results from HPI are
almost always exactly the same as those for the optimal policy iteration algorithm without
bounded updates for all iterations that can be completed by the optimal approach. Thus,
improvement occurs primarily due to the larger number of backups that can be performed.
We also see that while incorporating bounded updates improves value for the optimal
algorithm, incorporating the NLP approach into the heuristic approach produces even higher
value. Optimizing the NLP requires a small time overhead, but substantially increases
value on each iteration. This results in the highest controller value in each problem. Using
the NLP also allows our heuristic policy iteration to converge to a six node controller for
each agent in the two agent tiger problem. Unfortunately, this solution is known to be
suboptimal. As an heuristic algorithm, this is not unexpected, and it should be noted that
even suboptimal solutions by the heuristic approach outperform all other methods in all
our test problems.
6.5 Discussion
We have demonstrated how policy iteration can be used to improve both correlated and
independent joint controllers. We showed that using controller reductions together with
exhaustive backups is more efficient in terms of memory than using exhaustive backups
alone. However, due to the complexity of exhaustive backups, even that approach could
only complete a few iterations on each of our test problems.
Using bounded backups alone provided a good way to deal with the complexity issues.
With bounded backups, we were able to find higher-valued policies than with the previous
approach. Through our experiments, we were able to understand how the sizes of the local
controllers and correlation device affect the final values obtained.
With our heuristic policy iteration algorithm, we demonstrated further improvement by
dealing with some of the complexity issues. The heuristic approach is often able to continue
124

Policy Iteration for DEC-POMDPs

(a)

(b)

(c)
Figure 9: Comparison of the dynamic programming algorithms on (a) the two agent tiger
problem, (b) the meeting in a grid problem and (c) the box pushing problem.
The value produced by policy iteration with and without bounded backups as
well as our heuristic policy iteration with and without optimizing the NLP were
compared on each iteration until the time or memory limit was reached.

improving solution quality past the point where the optimal algorithm exhausts resources.
More efficient use of this limited representation size is achieved by incorporating the NLP
approach as well. In fact, the heuristic algorithm with NLP improvements at each step
provided results that are at least equal to the highest value obtained in each problem and
sometimes were markedly higher than the other approaches. Furthermore, as far as we
know, these results are the highest published values for all three of the test domains.

7. Conclusion
We present a policy iteration algorithm for DEC-POMDPs. The algorithm uses a novel policy representation consisting of stochastic finite-state controllers for each agent along with
a correlation device. We define value-preserving transformations and show that alternating
between exhaustive backups and value-preserving transformations leads to convergence to
125

Bernstein, Amato, Hansen, & Zilberstein

optimality. We also extend controller reductions and bounded backups from the single agent
case to the multiagent case. Both of these operations are value-preserving transformations
and are provably efficient. Finally, we introduced a heuristic version of our algorithm which
is more scalable and produces higher values on our test problems. Our algorithm serves as
the first nontrivial exact algorithm for DEC-POMDPs, and provides a bridge to the large
body of work on dynamic programming for POMDPs.
Our work provides a solid foundation for solving DEC-POMDPs, but much work remains
in addressing more challenging problem instances. We focused on solving general DECPOMDPs, but the efficiency of our approaches could be improved by using structure found
in certain problems. This would allow specialized representations and solution techniques
to be incorporated. Below we describe some key challenges of our general approach, along
with some preliminary algorithmic ideas to extend our work on policy iteration.
Approximation with Error Bounds Often, strict optimality requirements cause computational difficulties. A good compromise is to search for policies that are within some
bound of optimal. Our framework is easily generalized to allow for this.
Instead of a value-preserving transformation, we could define an -value-preserving transformation, which insures that the value at all states decreases by at most . We can perform
such transformations with no modifications to any of our linear programs. We simply need
to relax the requirement on the value for  that is returned. It is easily shown that using
an -value-preserving transformation at each step leads to convergence to a policy that is
β
within 1−β
of optimal for all states.
For controller reductions, relaxing the tolerance may lead to smaller controllers because
some value can be sacrificed. For bounded backups, it may help in escaping from local
optima. Though relaxing the tolerance for a bounded backup could lead to a decrease in
value for some states, a small “downward” step could lead to higher value overall in the
long run. We are currently working on testing these hypotheses empirically.
General-Sum Games In a general-sum game, there is a set of agents, each with its own
set of strategies, and a strategy profile is defined to be a tuple of strategies for all agents.
Each agent assigns a payoff to each strategy profile. The agents may be noncooperative, so
the same strategy profile may be assigned different values for each agent.
The DEC-POMDP model can be extended to a general-sum game by allowing each
agent to have its own reward function. In this case, the strategies are the local policies, and
a strategy profile is a joint policy. This model is often called a partially observable stochastic
game (POSG). Hansen et al. (2004) presented a dynamic programming algorithm for finitehorizon POSGs. The algorithm was shown to perform iterated elimination of dominated
strategies in the game. Roughly speaking, it eliminates strategies that are not useful for an
agent, regardless of the strategies of the other agents.
Work remains to be done on extending the notion of a value-preserving transformation
to the noncooperative case. One possibility is to redefine value-preserving transformations
so that value is preserved for all agents. This is closely related to the idea of Pareto
optimality. In a general-sum game, a strategy profile is said to be Pareto optimal if there
does not exist another strategy profile that yields higher payoff for all agents. It seems that
policy iteration using the revised definition of value-preserving transformation would tend
to move the controller in the direction of the Pareto optimal set. Another possibility is
126

Policy Iteration for DEC-POMDPs

to define value-preserving transformations with respect to specific agents. As each agent
transforms its own controller, the joint controller should move towards a Nash equilibrium.
Handling Large Numbers of Agents The general DEC-POMDP representation presented in this paper grows exponentially with the number of agents, as seen in the growth
of the set of joint actions and observations as well as the transition, reward and observation
functions. Thus this representation is not feasible for large numbers of agents. However,
a compact representation is possible if each agent interacts directly with just a few other
agents. We can have a separate state space for each agent, factored transition probabilities,
and a reward function that is the sum of local reward functions for clusters of agents. In
this case, the problem size is exponential only in the maximum number of agents interacting
directly. This idea is closely related to recent work on graphical games (La Mura, 2000;
Koller & Milch, 2003).
Once we have a compact representation, the next question to answer is whether we
can adapt policy iteration to work efficiently with the representation. This indeed seems
possible. With the value-preserving transformations we presented, the nodes of the other
agents are considered part of the hidden state of the agent under consideration. These
techniques modify the controller of the agent to get value improvement for all possible
hidden states. When an agent’s state transitions and rewards do not depend on some other
agent, it should not need to consider that agent’s nodes as part of its hidden state. A
specific compact representation along with extensions of different algorithms was proposed
by Nair et al. (2005).

Acknowledgments
We thank Martin Allen, Marek Petrik and Siddharth Srivastava for helpful discussions of
this work. Marek and Siddharth, in particular, helped formalize and prove Theorem 1. The
anonymous reviewers provided valuable feedback and suggestions. Support for this work
was provided in part by the National Science Foundation under grants IIS-0535061 and
IIS-0812149, by NASA under cooperative agreement NCC-2-1311, and by the Air Force
Office of Scientific Research under grants F49620-03-1-0090 and FA9550-08-1-0181.

Appendix A. Proof of Theorem 1
A correlation device produces a sequence of values that all the agents can observe. Let X
be the set of all possible infinite sequences that can be generated by a correlation device.
Let Vx (~q0 , s0 ) be the value of the correlated joint controller with respect to some correlation
sequence x ∈ X, initial nodes ~q0 of the agent controllers, and initial state s0 of the problem.
We will refer to Vx (~q0 , s0 ) simply as Vx – the value of some sequence x, given the controllers
for the agents. We define a regular sequence as a sequence that can be generated by a
regular expression. Before we prove Theorem 1, we establish the following property.
Lemma 2 The value of any sequence, whether regular or non-regular, can be approximated
within any  by some other sequence.
Proof: The property holds thanks to the discount factor used in infinite-horizon DECPOMDPs. Given a sequence x with value Vx , we can determine another sequence x0 such
127

Bernstein, Amato, Hansen, & Zilberstein

that |Vx0 − Vx | < . The sequence x0 is constructed by choosing the first k elements of x,
and then choosing an arbitrary regular or non-regular sequence for the remaining elements.
kR
max
As long as k is chosen such that  ≥ β(1−β)
, then |Vx0 − Vx | < . 2
Theorem 1 Given an initial state and a correlated joint controller, there always exists
some finite-size joint controller without a correlation device that produces at least the same
value for the initial state.
Proof: Let E represent the expected value of the joint controller with the correlation
device. Let V = {Vx | x ∈ X} be the set of values produced by all the possible correlation
device sequences. Let inf and sup represent the infimum and supremum of V respectively.
We break the proof into two cases, depending on the relation of the expectation versus
the supremum. We show in each case that a regular sequence can be found that produces
at least the same value as E. Once such a regular sequence is found, then that sequence can
be generated by a finite-state controller that can be embedded within each agent. Thus, a
finite number of nodes can be added to the agents’ controllers to provide equal or greater
value, without using a correlation device.
Case (1) inf ≤ E < sup
Based on Lemma 2, there is some regular sequence x that can approximate the supremum
within . If we choose  = sup −E, then Vx ≥ sup − = E.
Case (2) E = sup
If there is a regular sequence, x, for which Vx = E, we can choose that sequence. If no
such regular sequence exists, we will show that E 6= sup. We give a somewhat informal
argument, but this can be more formally proven using cylinder sets as discussed by Parker
(2002). We begin by first choosing some regular sequence. We can construct a neighborhood around this sequence (as described in Lemma 2) by choosing a fixed length prefix
of
A prefixP
of length k has a well-defined probability that is defined as
P the sequence.
P
0)
1 |q 0 ) . . .
k−1 |q k−2 ) where P (q 0 ) is the probability distribution
P
(q
P
(q
c
c c
c
c
qc0
qc1
qck−1 P (qc
of initial node of the correlation device and P (qci |qci−1 ) represents the probability of transitioning to correlation device node qci from node qci−1 . The set of sequences that possess this
prefix has probability equal to that of the prefix. Because we assumed there exists some
regular sequence which has value less than the supremum, we can always choose a prefix
and length such that the values of the sequences in the set are less than the supremum.
Because the probability of this set is nonzero and the value of these sequences is less than
the supremum, then E 6= sup, which is a contradiction.
Therefore, some regular sequence can be found that provides at least the same value as
the expected value of the correlated joint controller. This allows some uncorrelated joint
controller to produce at least the same value as a given correlated one. 2

Appendix B. Proof of Lemma 1
For ease of exposition, we prove the lemma under the assumption that there is no correlation
device. Including a correlation device is straightforward but unnecessarily tedious.
128

Policy Iteration for DEC-POMDPs

Lemma 1 Let C and D be correlated joint controllers, and let Ĉ and D̂ be the results of
performing exhaustive backups on C and D, respectively. Then Ĉ ≤ D̂ if C ≤ D.
Proof: Suppose we are given controllers C and D, where C ≤ D. Call the sets of joint
~ and R,
~ respectively. It follows that there exists a function
nodes for these controllers Q
~
fi : Qi → ∆Ri for each agent i such that for all s ∈ S and ~q ∈ Q
V (s, ~q) ≤

X

P (~r|~q)V (s, ~r).

~
r

We now define functions fˆi to map between the two controllers Ĉ and D̂. For the old
nodes, we define fˆi to produce the same output as fi . It remains to specify the results of fˆi
applied to the nodes added by the exhaustive backup. New nodes of Ĉ will be mapped to
distributions involving only new nodes of D̂.
To describe the mapping formally, we need to introduce some new notation. Recall that
the new nodes are all deterministic. For each new node ~r in controller D̂, the node’s action
is denoted ~a(~r), and its transition rule is denoted ~r 0 (~r, ~o). Now, the mappings fˆi are defined
such that
P (~r|~q) = P (~a(~r)|~q)

YX

P (~q 0 |~q, ~a(~r), ~o)P (~r 0 (~r, ~o)|~q 0 )

q~ 0

~
o

for all ~q in controller Ĉ and ~r in controller D̂.
We must now show that the mapping fˆ satisfies the inequality given in the definition
of a value-preserving transformation. For the nodes that were not added by the exhaustive
backup, this is straightforward. For the new nodes ~q of the controller Ĉ, we have for all
s ∈ S,

V (s, ~q) =

X

P (~a|~q) R(s, ~a) +


X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)V (s0 , ~q 0 )

~
o,s0 ,~
q0

~a




≤

X

P (~a|~q) R(s, ~a) +

X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)

~
o,s0 ,~
q0

~a

X

P (~r 0 |~q 0 )V (s0 , ~r 0 )

~
r0


=

X

P (~a|~q) R(s, ~a) +


X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)P (~r 0 |~q 0 )V (s0 , ~r 0 )

~
o,s0 ,~
q 0 ,~
r0

~a


=

X

=

X

P (~r|~q) R(s, ~a(~r)) +


X

P (s0 , ~o|s, ~a(~r))V (s0 , ~r 0 (~r, ~o))

~
o,s0

~
r

P (~r|~q)V (s, ~r).

~
r

2
129

Bernstein, Amato, Hansen, & Zilberstein



Planning for distributed agents with partial
state information is considered from a decision­
theoretic perspective. We describe generaliza­
tions of both the MDP and POMDP models
that allow for decentralized control. For even a
small number of agents, the finite-horizon prob­
lems corresponding to both of our models are
complete for nondeterministic exponential time.
These complexity results illustrate a fundamen­
tal difference between centralized and decentral­
ized control of Markov processes. In contrast to
the MDP and POMDP problems, the problems
we consider provably do not admit polynomial­
time algorithms and most likely require doubly
exponential time to solve in the worst case. We
have thus provided mathematical evidence corre­
sponding to the intuition that decentralized plan­
ning problems cannot easily be reduced to cen­
tralized problems and solved exactly using estab­
lished techniques.

1

Introduction

Among researchers in artificial intelligence, there has been
growing interest in problems with multiple distributed
agents working to achieve a common goal (Grosz & Kraus,
1996; Lesser, 1998; desJardins et al., 1999; Durfee, 1999;
Stone & Veloso, 1999). In many of these problems, intera­
gent communication is costly or impossible. For instance,
consider two robots cooperating to push a box (Mataric,
1998). Communication between the robots may take time
that could otherwise be spent performing physical actions.
Thus, it may be suboptimal for the robots to communi­
cate frequently. A planner is faced with the difficult task
of deciding what each robot should do in between com­
munications, when it only has access to its own sensory
information. Other problems of planning for distributed
agents with limited communication include maximizing the

throughput of a multiple access broadcast channel (Ooi &
Womell, 1996) and coordinating multiple spacecraft on a
mission together (Estlin et al., 1999). We are interested
in the question of whether these planning problems are
computationally harder to solve than problems that involve
planning for a single agent or multiple agents with access
to the exact same information.
We focus on centralized planning for distributed agents,
with the Markov decision process (MDP) framework as
the basis for our model of agents interacting with an envi­
ronment. A partially observable Markov decision process
(POMDP) is a generalization of an MDP in which an agent
must base its decisions on incomplete information about
the state of the environment (White, 1993). We extend
the POMDP model to allow for multiple distributed agents
to each receive local observations and base their decisions
on these observations. The state transitions and expected
rewards depend on the actions of all of the agents. We
call this a decentralized partially observable Markov de­
cision process (DEC-POMDP). An interesting special case
of a DEC-POMDP satisfies the assumption that at any time
step the state is uniquely determined from the current set
of observations of the agents. This is denoted a decen­
tralized Markov decision process (DEC-MDP). The MDP,
POMDP, and DEC-MDP can all be viewed as special cases
of the DEC-POMDP. The relationships among the models
are shown in Figure 1.
There has been some related work in AI. Boutilier (1999)
studies multi-agent Markov decision processes (MMDPs),
but in this model, the agents all have access to the same in­
formation. In the framework we describe, this assumption
is not made. Peshkin et al. (2000) use essentially the DEC­
POMDP model (although they refer to it as a partially ob­
servable identical payoff stochastic game (POIPSG)) and
discuss algorithms for obtaining approximate solutions to
the corresponding optimization problem. The models that
we study also exist in the control theory literature (Ooi
et al., 1997; Aicardi et al., 1987). However, the compu­
tational complexity inherent in these models has not been
studied. One closely related piece of work is that of Tsit-

33

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

agent given that it chose action a in state s.
There are several different ways to define "long-term re­
ward" and thus several different measures of optimality. In
this paper, we focus on finite-horizon optimality, for which
the aim is to maximize the expected sum of rewards re­
ceived over T time steps. Formally, the agent should maximize
Figure 1: The relationships among the models.

siklis and Athans (1985), in which the complexity of non­
sequential decentralized decision problems is studied.
We discuss the computational complexity of finding opti­
mal policies for the finite-horizon versions of these prob­
lems. It is known that solving an MDP is P-complete
and that solving a POMDP is PSPACE-complete (Papadim­
itriou & Tsitsiklis, 1987). We show that solving a DEC­
POMDP with a constant number, m ;::: 2, of agents is com­
plete for the complexity class nondeterministic exponen­
tial time (NEXP). Furthermore, solving a DEC-MDP with
a constant number, m ;::: 3, of agents is NEXP-complete.
This has a few consequences. One is that these problems
provably do not admit polynomial-time algorithms. This
trait is not shared by the MDP problems nor the POMDP
problems. Another consequence is that any algorithm for
solving either problem will most likely take doubly expo­
nential time in the worst case. In contrast, the exact al­
gorithms for finite-horizon POMDPs take "only" exponen­
tial time in the worst case. Thus, our results shed light
on the fundamental differences between centralized and de­
centralized control of Markov decision processes. We now
have mathematical evidence corresponding to the intuition
that decentralized planning problems are more difficult to
solve than their centralized counterparts. These results can
steer researchers away from trying to find easy reductions
from the decentralized problems to centralized ones and to­
ward completely different approaches.
A precise categorization of the two-agent DEC-MDP prob­
lem presents an interesting mathematical challenge. The
extent of our present knowledge is that the problem is
PSPACE-hard and is contained in NEXP.

2

Centralized Models

A Markov decision process (MDP) models an agent acting
in a stochastic environment to maximize its long-term re­
ward. The type of MDP that we consider contains a finite
setS of states, withs0 ES as the start state. For each state
s E S, As is a finite set of actions available to the agent.
P is the table of transition probabilities, where P(s'Js, a)
is the probability of a transition to state s' given that the
agent performed action a in states. R is the reward func­
tion, where R(s, a) is the expected reward received by the

where r(st, at) is the reward received at time step t. A

policy <5 for a finite-horizon MDP is a mapping from each
states and timet to an action <5(s,t). This is called a non­
stationary policy. The decision problem corresponding to
a finite-horizon MDP is as follows: Given an MDP M, a
positive integer T, and an integer K, is there a policy that
yields total reward at least K?
An MDP can be generalized so that the agent does not nec­
essarily observe the exact state of the environment at each
time step. This is called a partially observable Markov de­
cision process (POMDP). A POMDP has a state setS, a
start stateso E S, a table of transition probabilities P, and
a reward function R, just as an MDP does. Additionally, it
contains a finite set n of observations, and a table 0 of ob­
servation probabilities, where O(oJa,s') is the probability
that o is observed, given that action a was taken and led to
state s'. For each observation o E 11, Ao is a finite set of
actions available to the agent. A policy <5 is now a mapping
from histories of observations o , ..., Ot to actions in Ao, .
1
The decision problem for a POMDP is stated in exactly the
same way as for an MDP.

3

Decentralized Models

A decentralized partially observable Markov decision pro­
cess (DEC-POMDP) is a generalization of a POMDP to
allow for distributed control by m agents that may not
be able to observe the exact state.
A DEC-POMDP
contains a finite set S of states, with so E S as the
start state. The transition probabilities P(s' Is, a1, ..., am)
and expected rewards R(s, a1, ..., am) depend on the ac­
tions of all agents. ni is a finite set of observations
for agent i, and 0 is a table of observation probabilities,
where O(o1, ..., omJa1, ..., am,s') is the probability that
o1, ..., om are observed by agents 1, . . . , m respectively,
given that the action tuple (a1, ..., am) was taken and led
to state s'. Each agent i has a set of actions A� for each
observation oi E Oi. Notice that this model reduces to a
POMDP in the one-agent case.
For each a1, ..., am, s', let w(a1,
, am,s') denote the
set of observation tuples that have a nonzero chance of
occurring given that the action tuple (a1, ..., am) was
taken and led to state s'. To form a decentralized Markov
decision process (DEC-MDP), we add the requirement
• • .

34

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

that for each a1,.. .,am,s', and each (o1, . . . ,om) E
w(a 1,...,am,s') the state is uniquely determined by
(o1,...,om). In the one-agent case, this model is essentially the same as an MDP.
We define a local policy, oi, to be a mapping from local
histories of observations of, ... , o� to actions ai E A�,.
A joint policy, o = (81,...,om), is defined to be a tu­
ple of local policies. We wish to find a joint policy that
maximizes the total expected return over the finite hori­
zon. The decision problem is stated as follows: Given a
DEC-POMDP M, a positive integer T, and an integer K,
is there a joint policy that yields total reward at least K?
Let DEC-POMDP and DEC-MDPm denote the deci­
m
sion problems for them-agent DEC-POMDP and them­
agent DEC-MDP, respectively.

4

Complexity Results

It is necessary to consider only problems for which T <
lSI. If we place no restrictions on T, then the upper
bounds do not necessarily hold. Also, we assume that
each of the elements of the tables for the transition prob­
abilities and expected rewards can be represented with a
constant number of bits. With these restrictions, it was
shown in (Papadimitriou & Tsitsiklis, 1987) that the de­
cision problem for an MDP is P-complete. In the same
paper, the authors showed that the decision problem for a
POMDP is PSPACE-complete and thus probably does not
admit a polynomial-time algorithm. We prove that for all
m 2: 2, DEC-POMDP is NEXP-complete, and for all
m
m 2: 3, DEC-MDP is NEXP-complete, where NEXP =
m
c
NTIME ( 2n ) (Papadimitriou, 1994). Since P ::J. NEXP, we
can be certain that there does not exist a polynomial-time
algorithm for either problem. Moreover, there probably is
not even an exponential-time algorithm that solves either
problem.
For our reduction, we use a problem called TILING (Pa­
padimitriou, 1994), which is described as follows: We
are given a set of square tile types T = {to, ... , tk }, to­
gether with two relations H, V � T x T (the horizontal
and vertical compatibility relations, respectively). We are
also given an integer n in binary. A tiling is a function
f: {O,. . . , n - 1} x {O,. . . , n - 1 } -t T. A tiling f
is consistent if and only if (a) f(O,0) = to, and (b) for all
i,j (f(i,j),f(i+1,j)) E H, and (f(i,j),f(i,j+1)) E V.
The decision problem is to tell, given T, H, V, and n ,
whether a consistent tiling exists. It is known that TILING
is NEXP-complete.
Theorem 1

complete.

For all m

>

2, DEC-POMDP

m

is

NEXP­

Proof. First, we will show that the problem is in NEXP. We
can guess a joint policy o and write it down in exponential

time. This is because a joint policy consists of m map­
pings from local histories to actions, and since T < lSI,
all histories have length less than lSI. A DEC-POMDP
together with a joint policy can be viewed as a POMDP to­
gether with a policy, where the observations in the POMDP
correspond to the observation tuples in the DEC-POMDP.
In exponential time, each of the exponentially many possi­
ble sequences of observations can be converted into belief
states. The transition probabilities and expected rewards
for the corresponding "belief MDP" can be computed in
exponential time (Kaelbling et al., 1998). It is possible to
use dynamic programming to determine whether the policy
yields expected reward at least K in this belief MDP. This
takes at most exponential time.
Now we show that the problem is NEXP-hard. For sim­
plicity, we consider only the two-agent case. Clearly, the
problem with more agents can be no easier. We are given
an arbitrary instance of TILING. From it, we construct a
DEC-POMDP such that the existence of a joint policy that
yields a reward of at least zero is equivalent to the existence
of a consistent tiling in the original problem. Furthermore,
T < lSI in the DEC-POMDP that is constructed. Intu­
itively, a local policy in our DEC-POMDP corresponds to
a mapping from tile positions to tile types, i.e., a tiling, and
thus a joint policy corresponds to a pair of tilings. The pro­
cess works as follows: In the position choice phase, two
tile positions are randomly "chosen" by the environment.
Then, at the tile choice step, each agent sees a different
position and must use its policy to determine a tile to be
placed in that position. Based on information about where
the two positions are in relation to each other, the environ­
ment checks whether the tile types placed in the two posi­
tions could be part of one consistent tiling. Only if the nec­
essary conditions hold do the agents obtain a nonnegative
reward. It turns out that the agents can obtain a nonnega­
tive expected reward if and only if the conditions hold for
all pairs of positions the environment can choose, i.e., there
exists a consistent tiling.
We now present the construction in detail. During the posi­
tion choice phase, each agent only has one action available
to it, and a reward of zero is obtained at each step. The
states and the transition probability matrix comprise the
nontrivial aspect of this phase. Recall that this phase intu­
itively represents the choosing of two tile positions. First,
let the two tile positions be denoted (i1,jt) and ( i2 ,jz),
where 0 ::; i 1,i2,j1,jz ::; n - 1. There are 4log n steps in
this phase, and each step is devoted to the choosing of one
bit of one of the numbers. (We assume that n is a power
of two. It is straightforward to modify the proof to deal
with the more general case.) The order in which the bits
are chosen is important, and it is as follows: The bits of
i1 and i2 are chosen from least significant up to most sig­
nificant, alternating between the two numbers at each step.
Then j1 and jz are chosen in the same way. As the bits of

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

35

the numbers are being determined, information about the
relationships between the numbers is being recorded in the
state. How we express all of this as a Markov process is
explained below.

6) Vertically Adjacent Tile Positions
This component is used to check whether the first tile posi­
tion is directly above the second one. Its regular expression
is as follows:

Each state has six components, and each component rep­
resents a necessary piece of information about the two tile
positions being chosen. We describe how each of the com­
ponents changes with time. A time step in our process
can be viewed as having two parts, which we refer to as
the stochastic part and the deterministic part. During the
stochastic part, the environment "flips a coin" to choose
either the number 0 or the number 1, each with equal prob­
ability. After this choice is made, the change in each com­
ponent of the state can be described by a deterministic finite
automaton that takes as input a string of O's and 1 's (the en­
vironment's coin flips). The semantics of the components,
along with their associated automata, are described below:

(11 + 00) ...(11 + 00)(10)*(01)(11 + 00)*.

1) Bit Chosen in the Last Step

This component of the state says whether 0 or 1 was just
chosen by the environment. The corresponding automaton
consists of only two states.
2) Number of Bits Chosen So Far
This component simply counts up to 4logn, in order to
determine when the position choice phase should end. Its
automaton consists of 4logn + 1 states.

3) Equal Tile Positions
After the 4logn steps, this component tells us whether the
two tile positions chosen are equal or not. For this automa­
ton, along with the following three, we need to have a no­
tion of an accept state. Consider the following regular ex­
pression:
(00 + 11)*.
Note that the DFA corresponding to the above expression,
on an input of length 4logn, ends in an accept state if and
only if (i1,i1) = (iz,jz).
4) Upper Left Tile Position

This component is used to check whether the first tile posi­
tion is the upper left comer of the grid. Its regular expres­
sion is as follows:
(0(0 + 1))*.
The corresponding DFA, on an input of length 4logn, ends
in an accept state if and only if ( i1,j1) = (0,0).
5) Horizontally Adjacent Tile Positions
This component is used to check whether the first tile po­
sition is directly to the left of the second one. Its regular
expression is as follows:

(10)*(01)(11 + 00)* (11 + 00) ...(11 + 00) .

logn

The corresponding DFA, on an input of length 4logn, ends
in an accept state if and only if(i1,j1 + 1) = (iz,jz).
So far we have described the six automata that determine
how each of the six components of the state evolve based on
input (0 or 1) from the environment. We can take the cross
product of these six automata to get a new automaton that
is only polynomially bigger and describes how the entire
state evolves based on the sequence of O's and 1 's chosen
by the environment. This automaton, along with the en­
vironment's "coin flips," corresponds to a Markov process.
The number of states of the process is polylogarithmic inn,
and hence polynomial in the size of the TILING instance.
The start state s0 is a tuple of the start states of the six au­
tomata. The table of transition probabilities for this process
can be constructed in time polylogarithmic in n.
We have described the states, actions, state transitions, and
rewards for the position choice phase, and we now describe
the observation function. In this DEC-POMDP, the obser­
vations are uniquely determined from the state. For the
states after which a bit of i1 or i1 has been chosen, agent
one observes the first component of the state, while agent
two observes a dummy observation. The reverse is true for
the states after which a bit of i2 or jz has been chosen. Intu­
itively, agent one "sees" only(i1, jl), and agent two "sees"
only ( iz,)z) .
When the second component of the state reaches its limit,
the tile positions have been chosen, and the last four com­
ponents of the state contain information about the tile po­
sitions and how they are related. Of course, the exact tile
positions are not recorded in the state, as this would require
exponentially many states. This marks the end of the posi­
tion choice phase. In the next step, which we call the tile
choice step, each agent has k + 1 actions available to it,
corresponding to each of the tile types, to, ... , tk. We de­
note agent one's choice t1 and agent two's choice t2. No
matter which actions are chosen, the state transitions de­
terministically to some final state. The reward function for
this step is the nontrivial part. After the actions are chosen,
the following statements are checked for validity:

(i2 , j2 ), then t1 = t2.
2)If(i1,jl) (0,0), then t1 = t0.
3)If(il + 1,jl) = (iz,jz) , then (t1 , t2 )
4)If(i1,i1 + 1) (iz,jz) , then (tl , t2 )
1)If(h,jl)

=

=

=

logn

The corresponding DFA, on an input of length 4logn, ends
in an accept state if and only if ( i1 + 1, j1) = ( iz,jz) .

E
E

H.
V.

If all of these are true, then a reward of 0 is received. Oth­
erwise, a reward of -1 is received. This reward function
can be computed from the TILING instance in polynomial

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

36

time. To complete the construction, the horizon T is set
to 4log n (exactly the number of steps it takes the process
to reach the tile choice step, and fewer than the number of
states lSI).
Now we argue that the expected reward is zero if and only
if there exists a consistent tiling. First, suppose a consis­
tent tiling exists. This tiling corresponds to a local policy
for an agent. If each of the two agents follows this policy,
then no matter which two positions are chosen by the en­
vironment, the agents choose tile types for those positions
so that the conditions checked at the end evaluate to true.
Thus, no matter what sequence of O's and 1's the environ­
ment chooses, the agents receive a reward of zero. Hence,
the expected reward for the agents is zero.
For the converse, suppose the expected reward is zero.
Then the reward is zero no matter what sequence of O's
and 1's the environment chooses, i.e., no matter which two
tile positions are chosen. This implies that the four condi­
tions mentioned above are satisfied for any two tile posi­
tions that are chosen. The first condition ensures that for
all pairs of tile positions, if the positions are equal, then the
tile types chosen are the same. This implies that the two
agents' tilings are exactly the same. The last three condi­
tions ensure that this tiling is consistent. 0
Theorem 2

For all m

2:

3, DEC-MDPm is NEXP­

complete.
Proof. (Sketch) Inclusion in NEXP follows from the fact

that a DEC-MDP is a special case of a DEC-POMDP. For
NEXP-hardness, we can reduce a DEC-POMDP with two
agents to a DEC-MDP with three agents. We simply add a
third agent to the DEC-POMDP and impose the following
requirement: The state is uniquely determined by just the
third agent's observation, but the third agent always has just
one action and cannot affect the state transitions or rewards
received. It is clear that the new problem qualifies as a
DEC-MDP and is essentially the same as the original DEC­
POMDP. 0
The reduction described above can also be used to con­
struct a two-agent DEC-MDP from a POMDP and hence
show that DEC-MDP2 is PSPACE-hard. However, this
technique is not powerful enough to prove the NEXP­
hardness of the problem. In fact, the question of whether
DEC-MDP2 is NEXP-hard remains open. Note that in
the reduction in the proof of Theorem 1, the observa­
tion function is such that there are some parts of the state
that are hidden from both agents. This needs to some­
how be avoided in order to reduce to a two-agent DEC­
MDP. A simpler task may actually be to derive a better up­
per bound for the problem. For example, it may be pos­
sible that DEC-MDP2 E co-NEXP, where co-NEXP =
{LIL E NEXP } . Regardless of the outcome, the problem
provides an interesting mathematical challenge.

5

Discussion

Using the tools of worst-case complexity analysis, we
analyzed two models of decision-theoretic planning for
distributed agents.
Specifically, we proved that the
finite-horizon m-agent DEC-POMDP problem is NEXP­
complete form 2: 2 and the finite-horizonm-agent DEC­
MDP problem is NEXP-complete form 2: 3.
The results have some theoretical implications. First, un­
like the MDP and POMDP problems, the problems we
studied provably do not admit polynomial-time algorithms,
since P # NEXP. Second, we have drawn a connection be­
tween work on Markov decision processes and the body
of work in complexity theory that deals with the exponen­
tial jump in complexity due to decentralization (Peterson
& Reif, 1979; Babai et al., 1991). Finally, the two-agent
DEC-MDP case yields an interesting open problem. The
solution of the problem may imply that the difference be­
tween planning for two agents and planning for more than
two agents is a significant one in the case where the state is
collectively observed by the agents.
There are also more direct implications for researchers try­
ing to solve problems of planning for distributed agents.
Consider the growing body of work on algorithms for ob­
taining exact or approximate solutions for POMDPs (e.g.,
Jaakkola et al., 1995; Cassandra et al., 1997; Hansen,
1998). It would have been beneficial to discover that a
DEC-POMDP or DEC-MDP is just a POMDP "in dis­
guise," in the sense that it can easily be converted to a
POMDP and solved using established techniques. We have
provided evidence to the contrary, however. The complex­
ity results do not answer all of the questions surrounding
how these problems should be attacked, but they do sug­
gest that the fundamentally different structure of the de­
centralized problems may require fundamentally different
algorithmic ideas.
Finally, consider the infinite-horizon versions of the afore­
mentioned problems. It has recently been shown that the
infinite-horizon POMDP problem is undecidable (Madani
et al., 1999) under several different optimality criteria.
Since a POMDP is a special case of a DEC-POMDP, the
corresponding DEC-POMDP problems are also undecid­
able. In addition, because it is possible to reduce a POMDP
to a two-agent DEC-MDP, the DEC-MDP problems are
also undecidable.
Acknowledgments

The authors thank Micah Adler, Andy Barto, Dexter
Kozen, Victor Lesser, Frank McSherry, Ted Perkins, and
Ping Xuan for helpful discussions. This work was sup­
ported in part by the National Science Foundation under
grants IRI-9624992, IRI-9634938, and CCR-9877078 and
an NSF Graduate Fellowship to Daniel Bernstein.

37

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Proceedings of the Sixteenth National Conference on Arti­




eas include distributed robot control, networking and
e-commerce.

We present a memory-bounded optimization
approach for solving infinite-horizon decentralized POMDPs. Policies for each agent
are represented by stochastic finite state controllers. We formulate the problem of optimizing these policies as a nonlinear program,
leveraging powerful existing nonlinear optimization techniques for solving the problem.
While existing solvers only guarantee locally
optimal solutions, we show that our formulation produces higher quality controllers than
the state-of-the-art approach. We also incorporate a shared source of randomness in
the form of a correlation device to further
increase solution quality with only a limited
increase in space and time. Our experimental results show that nonlinear optimization
can be used to provide high quality, concise
solutions to decentralized decision problems
under uncertainty.

Although there has been some recent work on exact
and approximate algorithms for DEC-POMDPs (Nair
et al., 2003; Bernstein et al., 2005; Hansen et al., 2004;
Szer et al., 2005; Szer and Charpillet, 2005; Seuken
and Zilberstein, 2007), only two algorithms (Bernstein et al., 2005; Szer and Charpillet, 2005) are able
to find solutions for the infinite-horizon case. Such
domains as networking and robot control problems,
where the agents are in continuous use are more appropriately modeled as infinite-horizon problems. Exact
algorithms require an intractable amount of space for
all but the smallest problems. This may occur even if
an optimal or near-optimal solution is concise. DECPOMDP approximation algorithms can operate with a
limited amount of memory, but as a consequence may
provide poor results.

Introduction

Markov decision processes (MDPs) have been widely
used to study single agent sequential decision making
with full observability. Partially observable Markov
decision processes (POMDPs) have had success modeling the more general situation in which the agent has
only partial information about the state of the system. The decentralized partially observable Markov
decision processes (DEC-POMDP) is an even more
general framework which extends the POMDP model
to mutiagent settings. In a DEC-POMDP each agent
must make decisions based on uncertainty about the
other agents as well as imperfect information of the
system state. The agents seek to maximize a shared
total reward using solely local information in order to
act. Some examples of DEC-POMDP application ar-

In this paper, we propose a new approach that addresses the space requirement of DEC-POMDP algorithms while maintaining a principled method based
on the optimal solution. This approach formulates
the optimal memory-bounded solution for the DECPOMDP as a nonlinear program (NLP), thus allowing a wide range of powerful nonlinear optimization
techniques to be applied. This is done by optimizing
the parameters of fixed-size independent controllers for
each agent, which when combined, produce the policy
for the DEC-POMDP. While no existing NLP solver
guarantees finding an optimal solution, our new formulation facilitates a more efficient search of the solution
space and produces high quality controllers of a given
size.
We also discuss the benefits of adding a shared source
of randomness to increase the solution quality of our
memory-bounded approach. This allows a set of independent controllers to be correlated in order to produce higher values, without sharing any local information. Correlation adds another mechanism in efforts
to gain the most possible value with a fixed amount of

2

AMATO ET AL.

space. This has been shown to be useful in order to
increase value of fixed-size controllers (Bernstein et al.,
2005) and we show that is also useful when combined
with our NLP approach.
The rest of the paper is organized as follows. We first
present some background on the DEC-POMDP model
and the finite state controller representation of their
solution. We then describe the current infinite-horizon
algorithms and describe some of their flaws. As an
alternative, we present a nonlinear program that represents the optimal fixed-size solution. We also incorporate correlation into the NLP method and discuss
its benefits. Lastly, experimental results are provided
comparing the nonlinear optimization methods with
and without correlation and the current state-of-theart DEC-POMDP approximation algorithm. This is
done by using an off-the-shelf, locally optimal nonlinear optimization method to solve the NLPs, but
more sophisticated methods are also possible. For a
range of domains and controller sizes, higher-valued
controllers are found with the NLP and using correlation further increases solution quality. This suggests
that high quality, concise controllers can be found in
many diverse DEC-POMDP domains.

2

DEC-POMDP model and solutions

We first review the decentralized partially observable
Markov decision process (DEC-POMDP) model. For
clarity, we present the model for two agents as it is
straightforward to extend it to n agents.
A two agent DEC-POMDP can be defined with the
tuple: M = hS, A1 , A2 , P, R, Ω1 , Ω2 , O, T i
• S, the finite set of states
• A1 and A2 , the finite sets of actions for each agent
• P , the set of state transition probabilities:
P (s0 |s, a1 , a2 ), the probability of transitioning
from state s to s0 when actions a1 and a2 are
taken by agents 1 and 2 respectively
• R, the reward function: R(s, a1 , a2 ), the immediate reward for being in state s and agent 1 taking
action a1 and agent 2 taking action a2
• Ω1 and Ω2 , the finite sets of observations for each
agent
• O, the set of observation probabilities:
O(o1 , o2 |s0 , a1 , a2 ), the probability of agents
1 and 2 seeing observations o1 and o2 respectively
given agent 1 has taken action a1 and agent 2
has taken action a2 and this results in state s0
Since we are considering the infinite-horizon DECPOMDP, the decision making process unfolds over an
infinite sequence of stages. At each step, every agent

chooses an action based on their local observation histories, resulting in an immediate reward and an observation for each agent. Note that because the state
is not directly observed, it may be beneficial for the
agent to remember the observation history. A local
policy for an agent is a mapping from local observation histories to actions while a joint policy is a set of
policies, one for each agent in the problem. The goal is
to maximize the infinite-horizon total cumulative reward, beginning at some initial distribution over states
called a belief state. In order to maintain a finite sum
over the infinite-horizon, we employ a discount factor,
0 ≤ γ < 1.
As a way to model DEC-POMDP policies with finite
memory, finite state controllers provide an appealing
solution. Each agent’s policy can be represented as a
local controller and the resulting set of controllers supply the joint policy, called the joint controller. Each
finite state controller can formally be defined by the
tuple hQ, ψ, ηi, where Q is the finite set of controller
nodes, ψ : Q → ∆A is the action selection model for
each node, and η : Q × A × O → ∆Q represents the
node transition model for each node given an action
was taken and an observation seen. For n agents, the
value for starting in agent 1’s nodes ~q and at state s
is given by

n
XY
X
P (s0 |~a, s)·
V (~q, s) =
P (ai |qi ) R(s, ~a) + γ
0
i
s
~
a

n
X
XY
0
0
0
~
O(~o|s , ~a)
P (qi |qi , ai , oi )V (q , s )
~
o

q~0

i

This is also referred to as the Bellman equation. Note
that the values can be calculated offline in order to
determine controllers for each agent that can then be
executed online for distributed control.

3

Previous work

As mentioned above, the only other algorithms that
we know of that can solve infinite-horizon DECPOMDPs are those of Bernstein et al. (2005) and
Szer and Charpillet (2005). Bernstein et al.’s approach, called bounded policy iteration for decentralized POMDPs (DEC-BPI), is an approximate algorithm that also uses stochastic finite state controllers.
Szer and Charpillet’s approach is also an approximate
algorithm, but it uses deterministic controllers.
DEC-BPI improves a set of fixed-size controllers by
using linear programming. This is done by iterating
through the nodes of each agent’s controller and attempting to find an improvement. A linear program
searches for a probability distribution over actions and
transitions into the agent’s current controller that increases the value of the controller for any initial state

AMATO ET AL.
For variables: x(~q, ~a), y(~q, ~a, ~o, q~0 ) and z(~q, s)
Maximize
X

3

b0 (s)z(q~0 , s)

s

Given the Bellman constraints:
∀~q, s z(~q, s) =

X



X
X
X
0
0
0
0
0
~
~
y(~q, ~a, ~o, q )z(q , s )
O(~o|s , ~a)
P (s |s, ~a)
x(~q, ~a) R(s, ~a) + γ
s0

~
a

~
o

q~0

For each agent i and set of agents, −i, apart from i,
Independence constraints:
∀ai , ~q

X

X

x(~q, ~a) =

a−i

∀~a, ~q, ~o, qi0

X

y(~q, ~a, ~o, q~0 ) =

0
q−i

f
, ~a)
x(qi , q−i

a−i

X

f
y(qi , q−i
, ai , af−i , oi , of−i , q~0 )

0
q−i

And probability constraints:
X
X
f
f
∀qi
x(qi , q−i
, ~a) = 1 and ∀qi , oi , ai
y(qi , q−i
, ai , ac−i , oi , of−i , q~0 ) = 1
~
a

q~0

∀~q, ~a x(~q, ~a) ≥ 0 and ∀~q, ~o, ~a y(~q, ~a, ~o, q~0 ) ≥ 0
Table 1: The nonlinear program representing the optimal fixed-size controller. Variable x(~q, ~a) represents P (~a|~q),
variable y(~q, ~a, ~o, q~0 ) represents P (q~0 |~q, ~a, ~o), variable z(~q, s) represents V (~q, s), q~0 represents the initial controller
f
node for each agent. Superscripted f ’s such as q−i
represent arbitrary fixed values.
and any initial node of the other agents’ controllers.
If the improvement is discovered, the node is updated
based on the probability distributions found. Each
node for each agent is examined in turn and the algorithm terminates when no agent can make any further
improvements.
This algorithm allows memory to remain fixed, but
provides only a locally optimal solution. This is due to
the linear program considering the old controller values
from the second step on and the fact that improvement
must be over all possible states and initial nodes for
the controllers of the other agents. As the number of
agents or size of controllers grows, this later drawback
is likely to severely hinder improvement.
Szer and Charpillet have developed a best-first search
algorithm that finds deterministic finite state controllers of a fixed size. This is done by calculating a
heuristic for the controller given the known deterministic parameters and filling in the remaining parameters
one at a time in a best-first fashion. They prove that
this technique will find the optimal deterministic finite
state controller of a given size, but its use remains limited. This approach is very time and memory intensive
and is restricted to deterministic controllers.

4

Nonlinear optimization approach

Due to the high space complexity of finding an optimal solution for a DEC-POMDP, fixed-size solutions
are very appealing. Fixing memory balances optimality and computational concerns and should allow high
quality solutions to be found for many problems. Using Bernstein et al.’s DEC-BPI method reduces problem complexity by fixing controller size, but solution
quality is limited by a linear program that requires
improvement across all states and initial nodes of the
other agents. Also, each agent’s controller is improved
separately without consideration for the knowledge of
the initial problem state, thus reducing solution quality. Both of these limitations can be eliminated by
modeling a set of optimal controllers as a nonlinear
program. By setting the value as a variable and using
constraints to maintain validity, the parameters can be
updated in order to represent the globally optimal solution over the infinite-horizon of the problem. Rather
than the the iterative process of DEC-BPI, the NLP
improves and evaluates the controllers of all agents at
once for a given initial state in order to make the best
possible use of the controller size.
Compared with other DEC-POMDP algorithms, the
NLP approach makes more efficient use of memory

4

AMATO ET AL.

than the exact methods, and using locally optimal
NLP algorithms provides an approximation technique
with a search based on the optimal solution of the
problem. Rather than adding nodes and then attempting to remove those that will not improve the controller, as a dynamic programming approach might do,
we search for the best controllers of a fixed size. The
NLP is also able to take advantage of the start distribution, thus making better use of its size.
The NLP approach has already shown promise in
the POMDP case. In a previous paper (Amato et
al., 2007), we have modeled the optimal fixed-size
controller for a given POMDP as an NLP and with
locally optimal solution techniques produced consistently higher quality controllers than a current stateof-the-art method. The success of the NLP in the single agent case suggested that an extension to DECPOMDPs could also be successful. To construct this
NLP, extra constraints are needed to guarantee independent controllers for each agent, while still maximizing the value.
4.1

Nonlinear problem model

The nonlinear program seeks to optimize the value
of fixed-size controllers given a initial state distribution and the DEC-POMDP model. The parameters
of this problem in vector notation are the joint action selection probabilities at each node of the controllers P (~a|~q), the joint node transition probabilities
P (q~0 |~q, ~a, ~o) and the values of each node in each state,
V (~q, s). This approach differs from Bernstein et al.’s
approach in that it explicitly represents the node values as variables. To ensure that the values are correct
given the action and node transition probabilities, nonlinear constraints must be added to the optimization.
These constraints are the Bellman equations given the
policy determined by the action and transition probabilities. Constraints are also added to ensure distributed action selection and node transitions for each
agent. We must also ensure that all probabilities are
valid numbers between 0 and 1.
Table 1 describes the nonlinear program that defines
the optimal controller for an arbitrary number of
agents. The value of designated initial local nodes is
maximized given the initial state distribution and the
necessary constraints. The independence constraints
guarantee that action selection and transition probabilities can be summed out for each agent by ensuring
that they do not depend on any information that is
not local.
Theorem 1 An optimal solution of the NLP results
in optimal stochastic controllers for the given size and
initial state distribution.

Proof sketch. The optimality of the controllers follows from the NLP constraints and maximization of
given initial nodes at the initial state distribution.
The Bellman equation constraints restrict the value
variables to valid amounts based on the chosen probabilities, the independence constraints guarantee distributed control and the maximum value is found for
the initial nodes and state. Hence, this represents optimal controllers.
4.2

Nonlinear solution techniques

There are many efficient algorithms for solving large
NLPs. When the problem is non-convex, as in our
case, there are multiple local maxima and no NLP
solver guarantees finding the optimal solution. Nevertheless, existing techniques proved useful in finding
high-quality results for large problems.
For this paper, we used a freely available nonlinearly
constrained optimization solver called filter (Fletcher
et al., 2002) on the NEOS server (http://wwwneos.mcs.anl.gov/neos/). Filter finds solutions by a
method of successive approximations called sequential
quadratic programming (SQP). SQP uses quadratic
approximations which are then more efficiently solved
with quadratic programming (QP) until a solution to
the more general problem is found. A QP is typically
easier to solve, but must have a quadratic objective
function and linear constraints. Filter adds a “filter”
which tests the current objective and constraint violations against those of previous steps in order to
promote convergence and avoid certain locally optimal
solutions. The DEC-POMDP and nonlinear optimization models were described using a standard optimization language AMPL.

5

Incorporating correlation

Bernstein et al. also allow each agent’s controller to be
correlated by using a shared source of randomness in
the form of a correlation device. As an example of one
such device, imagine that before each action is taken, a
coin is flipped and both agents have access to the outcome. Each agent can then use that new information
to affect their choice of action. Along with stochasticity, correlation is another means of increasing value
when memory is limited.
A correlation device provides extra signals to the
agents and operates independently of the DECPOMDP. That is, the correlation device is a tuple
hC, ψi, where C is a set of states and ψ : C → ∆C
is a stochastic transition function that we will represent as P (c0 |c). At each step of the problem, the device
transitions and each agent can observe its state.

AMATO ET AL.

5

For variables: w(c, c0 ), x(~q, ~a, c), y(~q, ~a, ~o, q~0 , c) and z(~q, s, c)
Maximize
X
b0 (s)z(q~0 , s)
s

Given the Bellman constraints:


X
X
X
X
X
0
0
0
0
0
0
~
~
w(c, c )z(q , s , c)
y(~q, ~a, ~o, q , c)
O(~o|s , ~a)
P (s |s, ~a)
x(~q, ~a, c) R(s, ~a) + γ
∀~q, s z(~q, s, c) =
~
a

s0

~
o

q~0

c0

Table 2: The nonlinear program representing the optimal fixed-size controller including a correlation device.
Variable x(~q, ~a, c) represents P (~a|~q, c), variable y(~q, ~a, ~o, q~0 , c) represents P (q~0 |~q, ~a, ~o, c), variable z(~q, s, c) represents V (~q, s, c), q~0 represents the initial controller node for each agent and w(c, c0 ) represents P (c0 |c). The other
constraints are similar to those above with the addition of a sum to one constraint for the correlation device.
size
DEC-BPI DEC-BPI corr NLO NLO-corr
The independent local controllers defined above can be
modified to make use of the correlation device. This
1
4.687
6.290
9.1
9.1
is done by making the parameters dependent on the
2
4.068
7.749
9.1
9.1
signal from the correlation device. For agent i, ac3
8.637
7.781
9.1
9.1
tion selection is then P (ai |qi , c) and node transition is
4
7.857
8.165
9.1
9.1
P (qi0 |qi , ai , oi , c). For n agents, the value of the correTable 3: Broadcast problem values using NLP methlated joint controller beginning in nodes ~q, state s and
ods and DEC-BPI with and without a 2 node correlacorrelation device state c is defined as V (~q, s, c) =

n
tion device
X
X
XY
P (s0 |~a, s)
O(~o|s0 , ~a)·
P (ai |qi , c) R(s, ~a) + γ
i
s0
~
o
~
a
size
DEC-BPI DEC-BPI corr
NLO
NLO-corr

n
XY
X
0
1
< 1s
< 1s
1s
2s
P (qi |qi , ai , oi , c)
P (c |c)V (q~0 , s0 , c0 )
0
2
<
1s
2s
3s
8s
i
c
0
q~
3
2s
7s
764s
2119s
Our NLP can be extended to include a correlation
4
5s
24s
4061s
10149s
device. This optimization problem, the first part of
which is shown in Table 2, is very similar to the preTable 4: Broadcast problem mean optimization times
vious NLP. A new variable is added for the transiusing NLP methods and DEC-BPI with and without
tion function of the correlation device and the other
a 2 node correlation device
variables now include the signal from the device. The
Bellman equation incorporates the new correlation device signal at each step, but the other constraints reEach NLP and DEC-BPI algorithm was run until conmain the same. A new probability constraint is also
vergence was achieved with ten different random deadded to ensure that the transition probabilities for
terministic initial controllers, and the mean values and
each state of the correlation device sum to one.
times are reported. The times reported for each NLP

6

Experimental results

We tested our nonlinear programming approach in
three DEC-POMDP domains. In each experiment, we
compare Bernstein et al.’s DEC-BPI with NLP solutions using filter for a range of controller sizes. We
also implemented each of these approaches with a correlation device of size two. We do not compare with
Szer and Charpillet’s algorithm because the problems
presented in that work are slightly different than those
used by Bernstein et al. Nevertheless, on the problems
that we tested, our approach can and does achieve
higher values than Szer and Charpillet’s algorithm for
all of the controller sizes for which that the best-first
search is able to find a solution.

method can only be considered estimates due to running each algorithm on external machines with uncontrollable load levels, but we expect that they vary by
only a small constant. Note that our goal in these experiments is to demonstrate the benefits of our formulation when used in conjunction with an “off the shelf”
solver such as filter. The formulation is very general
and many other solvers may be applied. Throughout
this section we will refer to our nonlinear optimization
as NLO and the optimization with the correlation device with two states as NLO-corr.
6.1

Broadcast problem

A DEC-POMDP used by Bernstein et al. was a simplified two agent networking example. This problem

6

AMATO ET AL.

Figure 1: Recycling robots values using NLP methods
and DEC-BPI with and without a 2 node correlation
device
has 4 states, 2 actions and 5 observations. At each
time step, each agent must choose whether or not to
send a message. If both agents send, there is a collision
and neither gets through. A reward of 1 is given for
every step a message is successfully sent over the channel and all other actions receive no reward. Agent 1
has a 0.9 probability of having a message in its queue
on each step and agent 2 has only a 0.1 probability.
The domain is initialized with only agent 1 possessing
a message and a discount factor of 0.9 was used.
Table 3 shows the values produced by DEC-BPI and
our nonlinear programming approach with and without a correlation device for several controller sizes.
Both nonlinear techniques produce the same value, 9.1
for each controller size. In all cases this is a higher
value than that produced by Bernstein et al.’s independent and correlated approaches. As 9.1 is the maximum value that any approach that we tested receives
for the given controller sizes, it is likely that it is optimal for these sizes.
The time used by each algorithm is shown in Table
4. As expected, the nonlinear optimization methods
require more time to find a solution than the DECBPI methods. As noted above, solution quality is also
higher using nonlinear optimization. Either NLP approach can produce a higher valued one node controller
in an amount of time similar to or less than each DECBPI method. Therefore, for this problem, the NLP
methods are able to find higher valued, more concise
solutions given a fixed amount of space or time.
6.2

Recycling robots

As another comparison, we have extended the Recycling Robot problem (Sutton and Barto, 1998) to the
multiagent case. The robots have the task of picking
up cans in an office building. They have sensors to

Figure 2: Recycling robots graphs for value vs time
for the NLP and DEC-BPI methods with and without
the correlation device.
find a can and motors to move around the office in
order to look for cans. The robots are able to control a gripper arm to grasp each can and then place it
in an on-board receptacle. Each robot has three high
level actions: (1) search for a small can, (2) search for
a large can or (3) recharge the battery. In our two
agent version, the larger can is only retrievable if both
robots pick it up at the same time. Each agent can
decide to independently search for a small can or to
attempt to cooperate in order to receive a larger reward. If only one agent chooses to retreive the large
can, no reward is given. For each agent that picks up
a small can, a reward 2 is given and if both agents
cooperate to pick the large can, a reward of 5 is given.
The robots have the same battery states of high and
low, with an increased likelihood of transitioning to a
low state or exhausting the battery after attempting
to pick up the large can. Each robot’s battery power
depends only on its own actions and each agent can
fully observe its own level, but not that of the other
agent. If the robot exhausts the battery, it is picked
up and plugged into the charger and then continues to
act on the next step with a high battery level. The two
robot version used in this paper has 4 states, 3 actions
and 2 observations. A discount factor of 0.9 was used.
We can see in Figure 1 that in this domain higher
quality controllers are produced by using nonlinear optimization. Both NLP methods permit higher mean
values than either DEC-BPI approach for all controller
sizes. Also, correlation is helpful for both the NLP and
DEC-BPI approaches, but becomes less so for larger
controller sizes. For the nonlinear optimization cases,
both approaches converge to within a small amount of
the maximum value that was found for any controller
size tested. As controller size grows, the NLP methods
are able to reliably find this solution and correlation
is no longer useful.

AMATO ET AL.

7

The running times of each algorithm follow the same
trend as above in which the nonlinear optimization
approaches required much more time as controller size
increases. The ability for the NLP techniques to produce smaller, higher valued controllers with similar or
lesser running time also follows the same trend.
Figure 2 shows the values that can be attained for each
method based on the mean time necessary for convergence. Results are included for NLP techniques up to
four nodes with the correlation device and five nodes
without it while DEC-BPI values are given for fourteen nodes with the correlation device and eighteen
without it. This graph demonstrates that even if we
allow controller size to continue to grow and examine
only the amount of time that is necessary to achieve a
solution, the NLP methods continue to provide higher
values. Although the values of the controllers produced by the DEC-BPI methods are somewhat close
to those of the NLP techniques as controller size grows,
our approaches produce that value with a fraction of
the controller size.
6.3

Figure 3: Multiagent Tiger problem values using NLP
methods and DEC-BPI with and without a 2 node
correlation device.

Multiagent tiger problem

Another domain with 2 states, 3 actions and 2 observations called the multiagent tiger problem was introduced by Nair et al. (Nair et al., 2003). In this
problem, there are two doors. Behind one door is a
tiger and behind the other is a large treasure. Each
agent may open one of the doors or listen. If either
agent opens the door with the tiger behind it, a large
penalty is given. If the door with the treasure behind it is opened and the tiger door is not, a reward is
given. If both agents choose the same action (i.e., both
opening the same door) a larger positive reward or a
smaller penalty is given to reward this cooperation. If
an agent listens, a small penalty is given and an observation is seen that is a noisy indication of which door
the tiger is behind. While listening does not change
the location of the tiger, opening a door causes the
tiger to be placed behind one of the door with equal
probability. A discount factor of 0.9 was used.
Figure 3 shows the values attained by each NLP and
DEC-BPI method for the given controller sizes. Figure 4 shows the values of just the two NLP methods.
These graphs show that not only do the NLP methods significantly outperform the DEC-BPI approaches,
but correlation greatly increases the value attained by
the nonlinear optimization. The individual results for
this problem suggest the DEC-BPI approach is more
dependent on the initial controller and the large penalties in this problem result in several results that are
very low. This outweighs the few times that more reasonable value is attained. Nevertheless, the max value
attained by DEC-BPI for all cases is still less than the

Figure 4: Multiagent Tiger problem values using just
the NLP methods with and without a 2 node correlation device.

Figure 5: Multiagent Tiger problem graphs for value
vs. time for the NLP methods with and without the
correlation device.
mean value attained by the NLP methods. Again for
this problem, more time is needed for the NLP approaches, but one node controllers are produced with
higher value than any controller size for the DEC-BPI

8

AMATO ET AL.

methods and require very little time.
The usefulness of the correlation device is illustrated
in Figure 5. For given amounts of time, the nonlinear
optimization that includes the correlation device produces much higher values. The DEC-BPI methods are
not included in this graph as they were unable to produce mean values greater than -50 for any controller
size up to 22 for which mean time to convergence was
over 5000 seconds. This shows the importance of correlation in this problem and the ability of our NLP
technique to take advantage of it.

7

Conclusion

We introduced a novel approach to solving decentralized POMDPs by using a nonlinear program formulation. This memory-bounded stochastic controller formulation allows a wide range of powerful nonlinear
programming algorithms to be applied to solve DECPOMDPs. The approach is easy to implement as it
mostly involves reformulating the problem and feeding it into an NLP solver.
We showed that by using an off-the-shelf locally optimal NLP solver, we were able to produce higher valued
controllers than the current state-of-the-art technique
for an assortment of DEC-POMDP problems. Our experiments also demonstrate that incorporating a correlation device as a shared source of randomness for
the agents can further increase solution quality. While
the time taken to find a solution to the NLP can be
higher, the fact that higher values can be found with
smaller controllers by using the NLP suggests adopting more powerful optimization techniques for smaller
controllers can be more productive in a given amount
of time. The combination of start state knowledge
and more advanced optimization allows us to make efficient use of the limited space of the controllers. These
results show that this method can allow compact optimal or near-optimal controllers to be found for various
DEC-POMDPs.
In the future, we plan to conduct a more exhaustive
analysis of the NLP representation and explore more
specialized algorithms that can be tailored for this optimization problem. While the performance we get
using a standard nonlinear optimization algorithm is
very good, specialized solvers might be able to further
increase solution quality and scalability. We also plan
to characterize the circumstances under which introducing a correlation device is cost effective.
Acknowledgements
An earlier version of this paper without improvements
such as incorporating a correlation device was pre-

sented at the AAMAS-06 Workshop on Multi-Agent
Sequential Decision Making in Uncertain Domains.
This work was supported in part by the Air Force Office of Scientific Research (Grant No. FA9550-05-10254) and by the National Science Foundation (Grant
No. 0535061). Any opinions, findings, conclusions
or recommendations expressed in this manuscript are
those of the authors and do not reflect the views of the
US government.


We present decentralized rollout sampling policy iteration (DecRSPI) — a new algorithm
for multi-agent decision problems formalized as
DEC-POMDPs. DecRSPI is designed to improve scalability and tackle problems that lack
an explicit model. The algorithm uses MonteCarlo methods to generate a sample of reachable
belief states. Then it computes a joint policy for
each belief state based on the rollout estimations.
A new policy representation allows us to represent solutions compactly. The key benefits of the
algorithm are its linear time complexity over the
number of agents, its bounded memory usage and
good solution quality. It can solve larger problems that are intractable for existing planning algorithms. Experimental results confirm the effectiveness and scalability of the approach.

1

Introduction

Planing under uncertainty in multi-agent settings is a challenging computational problem, particularly when agents
with imperfect sensors and actuators, such as autonomous
rovers or rescue robots, must reason about a large space of
possible outcomes and choose a plan based on their incomplete knowledge. The partially observable Markov decision process (POMDP) has proved useful in modeling and
analyzing this type of uncertainty in single-agent domains.
When multiple cooperative agents are present, each agent
must also reason about the decisions of the other agents and
how they may affect the environment. Since each agent can
only obtain partial information about the environment and
sharing all the local information among the agents is often
impossible, each agent must act based solely on its local
information. These problems can be modeled as decentralized POMDPs (DEC-POMDPs) [2].
When a complete model of the domain is available, DECPOMDPs can be solved using a wide range of optimal or

Xiaoping Chen
School of Computer Science
University of Sci. & Tech. of China
Hefei, Anhui 230027 China
xpchen@ustc.edu.cn

approximate algorithms, particularly MBDP [18] and its
descendants [1, 8, 17]. Unfortunately, these algorithms are
quite limited in terms of the size of the problems they can
tackle. This is not surprising given that finite-horizon DECPOMDPs are NEXP-complete [2]. Intuitively, the main
reason is that it is hard to define a compact belief state and
compute a value function for DEC-POMDPs, as is often
done for POMDPs. The state and action spaces blow-up
exponentially with the number of agents. Besides, it is very
difficult to search over the large policy space and find the
best action for every possible situation.
Another key challenge is modeling the dynamics of the entire domain, which may include complex physical systems.
Existing DEC-POMDP algorithms assume that a complete
model of the domain is known. This assumption does not
hold in some real-world applications such as robot soccer.
Incomplete domain knowledge is often addressed by reinforcement learning algorithms [19]. However, most cooperative multi-agent reinforcement learning algorithms assume that the system state is completely observable by all
the agents [6]. Learning cooperative policies for multiagent partially-observable domains is extremely challenging due to the large space of possible policies given only
the local view of each agent.
In reinforcement learning, a class of useful techniques such
as Monte-Carlo methods allows agents to choose actions
based on experience [19]. These methods require no prior
knowledge of the dynamics, as long as sample trajectories
can be generated online or using a simulator of the environment. Although a model is required, it must only provide
enough information to generate samples, not the complete
probability distributions of all possible transitions that are
required by planning algorithms. In many cases it is easy
to generate samples by simulating the target environment,
but obtaining distributions in explicit form may be much
harder. In the robot soccer domain, for example, there exist
many high-fidelity simulation engines. It is also possible to
put a central camera on top of the field and obtain samples
by running the actual robots.
This paper introduces the decentralized rollout sampling

policy iteration (DecRSPI) algorithm for finite-horizon
DEC-POMDPs. Our objective is to compute a set of cooperative policies using Monte-Carlo methods, without having an explicit representation of the dynamics of the underlying system. DecRSPI first samples a set of reachable belief states based on some heuristic policies. Then
it computes a joint policy for each belief state based on the
rollout estimations. Similar to dynamic programming approaches, policies are constructed from the last step backwards. A new policy representation is used to bound the
amount of memory. To the best of our knowledge, this is
the first rollout-based learning algorithm for finite-horizon
DEC-POMDPs. DecRSPI has linear time complexity over
the number of agents and it can solve much larger problems
compared to existing planning algorithms.
We begin with some background on the DEC-POMDP
model and the policy structure we use. We then describe
each component of the rollout sampling algorithm and analyze its properties. Finally, we examine the performance
of DecRSPI on a set of test problems, and conclude with a
summary of related work and the contributions.

2

Decentralized POMDPs

Formally, a finite-horizon DEC-POMDP can be defined as
a tuple hI, S, {Ai }, {Ωi }, P, O, R, b0 , T i, where
• I is a collection of agents, identified by i ∈ {1 . . . m},
and T is the time horizon of the problem.
• S is a finite state space and b0 is the initial belief state
(i.e., a probability distribution over states).
• Ai is a discrete action space for agent i. We denote
by ~a = ha1 , a2 , · · · , am i a joint action where ai ∈ Ai
~ = ×i∈I Ai is the joint action space.
and A
• Ωi is a discrete observation space for agent i. Similarly ~o = ho1 , o2 , · · · , om i is a joint observation
~ = ×i∈I Ωi is the joint obserwhere oi ∈ Ωi and Ω
vation space.
~ → ∆(S) is the state transition function and
• P : S ×A
P (s0 |s, ~a) denotes the probability of the next state s0
when the agents take joint action ~a in state s.
~ → ∆(Ω)
~ is an observation function and
• O : S×A
0
O(~o|s , ~a) denotes the probability of observing ~o after
taking joint action ~a with outcome state s0 .
~ → R is a reward function and R(s, ~a) is
• R : S×A
the immediate reward after agents take ~a in state s.
In a DEC-POMDP, each agent i ∈ I executes an action ai
based on its policy at each time step t. Thus a joint action ~a
of all the agents is performed, followed by a state transition
of the environment and an identical joint reward obtained
by the team. Then agent i receives its private observation oi
from the environment and updates its policy for the next execution cycle. The goal of each agent is to choose a policy

that maximizes thePaccumulated reward of the team over
T
the horizon T , i.e. t=1 E[R(t)|b0 ].
Generally, a policy qi is a mapping from agent i’s observation history to an action ai and a joint policy ~q =
hq1 , q2 , · · · , qm i is a vector of policies, one for each agent.
The value of a fixed joint policy ~q at state s can be computed recursively by the Bellman equation:
X
P (s0 |s, ~a)O(~o|s0 , ~a)V (s0 , ~q~o )
V (s, ~q) = R(s, ~a) +
s0 ,~
o

where ~a is the joint action specified by ~q, and ~q~o is the joint
sub-policy of ~q after observing ~o. Given a state distribution
b ∈ ∆(s), the value of a joint policy ~q can be computed by
X
V (b, ~q) =
b(s)V (s, ~q)
(1)
s∈S

Note that in a DEC-POMDP, each agent can only receive its
own local observations when executing the policy. Therefore the policy must be completely decentralized, which
means the policy of an agent must be guided by its own
local observation history only. It is not clear how to maintain a sufficient statistic, such as a belief state in POMDPs,
based only on the local partial information of each agent.
Thus, most of the works on multi-agent partially observable
domains are policy-based and learning in DEC-POMDP
settings is extremely challenging. While the policy execution is decentralized, planning or learning algorithms can
operate offline and thus may be centralized [11, 18].
The policies for finite-horizon DEC-POMDPs are often
represented as a set of local policy trees [1, 8, 11, 17, 18].
Each tree is defined recursively with an action at the root
and a subtree for each observation. This continues until the
horizon is reached at a leaf node. A dynamic programming
(DP) algorithm was developed to build the policy trees optimally from the bottom up [11]. In this algorithm, the policies of the next iteration are enumerated by an exhaustive
backup of the current trees. That is, for each action and
each resulting observation, a branch to any of the current
trees is considered. Unfortunately, the number of possible trees grows double-exponentially over the horizon. Recently, memory-bounded techniques have been introduced.
These methods keep only a fixed number of trees at each
iteration [1, 8, 17, 18]. They use a fixed amount of memory
and have linear complexity over the horizon.
There are many possibilities for constructing policies with
bounded memory. In this work we use a stochastic policy
for each agent. It is quite similar to stochastic finite state
controllers (FSC), used to solve infinite-horizon POMDPs
[16] and DEC-POMDPs [3]. But our stochastic policies
have a layered structure, one layer for each time step. Each
layer has a fixed number of decision nodes. Each node is labeled with an action and includes a node selection function.
The selection function is a mapping from an observation to

start node
a1

1
o1
0.9

0.4

0.6

0.2

0.8

a2
o1

T

o1

o2
0.1

2

o2
0.7

a3
o2

a3

Algorithm 1: Rollout Sampling Policy Iteration

a2

o1

0.3

o2

a1

Figure 1: An agent’s stochastic policy with two observations and two decision nodes in each layer.
a probability distribution over the nodes of the next layer.
In this paper, we denote by Qti the set of decision nodes of
agent i ∈ I at time step t ∈ 1..T . Also, N denotes the
predetermined size of Qti and π(qi0 |oi ) is the probability of
selecting the node of the next layer qi0 after observing oi .
An example of such stochastic policies is shown in Figure 1. In the planning phase, a set of stochastic polices
are constructed offline, one for each agent. When executing the policy, each agent executes the action in the current
node and then transitions to the next node based on the received observation as well as the node selection function.
We show how the stochastic node selection function can
be optimized easily by our policy improvement technique.
The following sections describe the algorithm in details.

3

The Rollout Sampling Method

In this section, we propose a new rollout sampling policy
iteration (DecRSPI) for DEC-POMDPs that heuristically
generates stochastic policies using an approximate policy
improvement operator trained with Monte-Carlo simulation. The approximate operator performs policy evaluation
by simulation, evaluating a joint policy ~q at state s by drawing K sample trajectories of ~q starting at s. Then, the operator performs policy improvement by constructing a series
of linear programs with parameters computed from samples and then solving the linear programs to induce a new
improved approximate policy. Similar to MBDP, DecRSPI generates policies using point-based dynamic programming, which builds policies according to heuristic state
distributions from the bottom up. The key difference is
that DecRSPI improves the policies by simulation without
knowing the exact transition function P , observation function O and reward function R of the DEC-POMDP model.
Note that DecRSPI is performed offline in a centralized
way, but the computed policies are totally decentralized.
The use of simulation assumes that the state of the environment can be reset and the system information (state, reward
and observations) are available after executing a joint action by the agents. In the planning phase, this information
is often available. In large real-world systems, modeling

~ given T, N
generate a random joint policy Q
sample a set of beliefs B for t ∈ 1..T, n ∈ 1..N
for t=T to 1 do
for n=1 to N do
~ tn
b ← Bnt , ~
q←Q
repeat
foreach agent i ∈ I do
keep the other agents’ policies q−i fixed
foreach action ai ∈ Ai do
Φi ← estimate the parameter matrix
build a linear program with Φi
πi ← solve the linear program
∆i ← ∆i ∪ {hai , πi i}
hai , πi i∗ ← arg max∆i Rollout(b, hai , πi i)
update agent i’s policy qi by hai , πi i∗
until no improvement in all agents’ policies
~
return the joint policy Q

the exact DEC-POMDP is extremely challenging and even
the representation itself is nontrivial for several reasons.
First, the system may be based on some complex physical models and it may be difficult to compute the exact P ,
O and R. Second, the state, action and observation spaces
may be very large, making it hard to store the entire transition table. Fortunately, simulators of these domains are
often available and can be modified to compute the policies
as needed.
We activate DecRSPI by providing it with a random joint
policy and a set of reachable state distributions, computed
by some heuristics. The joint policy is initialized by assigning a random action and random node selection functions
for each decision node from layer-1 to layer-T . Policy iteration is performed from the last step t=T backward to
the first step t=1. At each iteration, we first choose a state
distribution and an unimproved joint policy. Then we try
to improve the joint policy based on the state distribution.
This is done by keeping the policies of the other agents
fixed and searching for the best policy of one agent at a
time. We continue to alternate between the agents until no
improvement is achievable for the current policies of all the
agents. This process is summarized in Algorithm 1.
3.1

Belief Sampling

In this paper, the belief state b ∈ ∆(S) is a probability distribution over states. We use it interchangeably with the
state distribution with the same meaning. Generally, given
belief state bt at time t, we determine ~at , execute ~at and
make a subsequent observation ~o t+1 , then update our belief state to obtain bt+1 . In single-agent POMDPs, this belief state is obtained via straightforward Bayesian updating,
by computing bt+1 = P r(S|bt , ~at , ~o t+1 ). Unfortunately,
even if the transition and observation functions are available, the belief update itself is generally time-consuming

Algorithm 2: Belief Sampling
for n=1 to N do
Bnt ← ∅ for t ∈ 1..T
h ← choose a heuristic policy
for k=1 to K do
s ← draw a state from b0
for t=1 to T do
θt ← θt ∪ {btk (s)}
~a ← select a joint action based on h
s0 ← simulate the model with s, ~a
s ← s0
for t=1 to T do
bt ← compute the belief by particle set θt
Bnt ← Bnt ∪ {bt }
return the belief set B

because each belief state is a vector of size |S|. To approximate belief states by simulation, consider the following
particle filtering procedure. At any time step t, we have a
collection θt of K particles. The particle set θt , t ∈ 1..T
represents the following state distribution:
PK
{1 : btk (s) ∈ θt }
t
b (s) = k=1
, ∀s ∈ S
(2)
K
where btk (s) is the k th particle of θt . As mentioned above,
the significance of this method lies in the fact that, for many
applications, it is easy to sample successor states according
to the system dynamics. But direct computation of beliefs
is generally intractable especially when the dynamics specification is unavailable.
Another key question is how to choose the heuristic policies. In fact, the usefulness of the heuristics and, more importantly, the computed belief states, are highly dependent
on the specific problem. Instead of just using one heuristic, a whole portfolio of heuristics can be used to compute
a set of belief states. Thus, each heuristic is used to select
a subset of the policies. There are a number of possible
alternatives. Our first choice is the random policy, where
agents select actions randomly from a uniform distribution
at each time step. Another choice is the policy of the underlying MDP. That is, agents can learn an approximate MDP
value function by some MDP learning algorithms and then
select actions greedily based on that value function. In specific domains such as robot soccer, where learning the MDP
policy is also hard, hand-coded policies or policies learned
by DecRSPI itself with merely random guidance are also
useful as heuristics. The overall belief sampling method is
detailed in Algorithm 2.
3.2

Policy Improvement

In multi-agent settings, agents with only local information
must reason about all the possible choices of the others and
select the optimal joint policy that maximizes the team’s
expected reward. One straightforward method for finding

Φi (oi , qi0 )x(oi , qi0 )
P
subject to ∀oi ,qi0 x(oi , qi0 ) ≥ 0, ∀oi q0 x(oi , qi0 ) = 1.
Maximize x

P

oi ∈Ωi

P

qi0 ∈Qt+1
i

i

Table 1: Linear program to improve agent i’s policy where
the variable x(oi , qi0 ) = π(qi0 |oi ) is the node selection table.
the optimal joint policy is to simply search over the entire
space of possible policies, evaluate each one, and select the
policy with the highest value. Unfortunately, the number
T
of possible joint policies is O((|Ai |(|Ωi | −1)/(|Ωi |−1) )|I| ).
Instead of searching over the entire policy space, dynamic
programming (DP) constructs policies from the last step
up to the first one and eliminates dominated policies at the
early stages [11]. However, the exhaustive backup in the
DP algorithm at t still generates agent i’s policies of the order O(|Ai ||Qit−1 ||Ωi | ). Memory-bounded techniques have
been developed to combine the top-down heuristics and the
bottom-up dynamic programming together, keeping only a
bounded number of policies at each iteration [18]. This results in linear complexity over the horizon, but the one-step
backup operation is still time-consuming [17].
Our algorithm is based on the MBDP algorithm [18], but
it approximates the backup operation with an alternating
maximization process. As shown in Algorithm 1, the basic idea is to choose each agent in turn and compute the
best-response policy, while keeping the policies of the other
agents fixed. This process is repeated until no improvement is possible for all agents. That is, the process ends
when the joint policy converges to a Nash equilibrium. This
method was first introduced by Nair et al. [14] and later
refined by Bernstein et al. [3]. The differences are: Nair
et al. use the method to reformulate the problem as an augmented POMDP; Bernstein et al. use it to optimize the
controllers of infinite-horizon DEC-POMDPs. In contrast,
when an agent is chosen, our algorithm approximates the
best-response policy that maximizes the following value:
X
Y
V (b, ~q) = R(b, ~a)+
P r(s0 , ~o|b, ~a)
π(qi0 |oi )V (s0 , ~q 0 )
s0 ,~
o,~
q0

i

(3)
P
0
where P r(sP
, ~o|b, ~a) = s∈S b(s)P (s0 |s, ~a)O(~o|s0 , ~a) and
R(b, ~a) = s∈S b(s)R(s, ~a). This value function is similar to Equation 1, but for a stochastic joint policy.
Notice that our algorithm is designed to work when an explicit form of system dynamics is not available. Our solution, as shown in Algorithm 1, is two-fold: first we find the
best node selection function πi for every action ai ∈ Ai
and generate a set of stochastic policies ∆i ; then we evaluate the policy qi ∈ ∆i for the given belief point b, choose
the best one and update the current policy of agent i with
it. In order to find the best πi that maximizes the value
function of Equation 3 given ai and other agents’ policies
q−i , we use the linear program shown in Table 1. Note that

Algorithm 3: Parameter Estimation

Algorithm 4: Rollout Evaluation

Input: b, ai , q−i
a−i ← get actions from q−i
for k=1 to K do
s ← draw a state from b
s0 , ~o ← simulate the model with s, ~a
ωoi (s0 , o−i ) ← ωoi (s0 , o−i ) + 1
normalize ωoi for ∀oi ∈ Ωi
foreach oi ∈ Ωi , qi0 ∈ Qt+1
do
i
for k=1 to K do
s0 , o−i ← draw a sample from ωoi
0
q−i
← get other agents’ policy π(·|q−i , o−i )
Φi (oi , qi0 )k ← Rollout(s0 , ~
q 0)
P
K
0
1
Φi (oi , qi0 ) ← K
k=1 Φi (oi , qi )k
return the parameter matrix Φi

Input: t, s, ~q t
for k=1 to K do
vk ← 0
while t ≤ T do
~a ← get the joint action from ~q t
s0 , r, ~o ← simulate the model with s, ~a
vk ← vk + r, ~q t+1 ← π(·|~q t , ~o)
s ← s0 , t ← t + 1
PK
1
Ṽ ← K
k=1 vk
return the average value Ṽ

R(b, ~a) is a constant given b, ai , q−i and is thus omitted.
The matrix Φi of the linear program is defined as follows:
X
0
Φi (oi , qi0 ) =
P r(s0 , ~o|b, ~a)π(q−i
|o−i )V (s0 , ~q 0 )
0
s0 ,o−i ,q−i

Q
0
where π(q−i
|o−i ) = k6=i π(qk0 |ok ). Since the dynamics
is unknown, Algorithm 3 is used to estimate Φi . It first estimates P r(s0 , ~o|b, ~a) by drawing K samples from one-step
simulation. Then it estimates each element of Φi by an0
other K samples with π(q−i
|o−i ). The value of V (s0 , ~q 0 )
is approximated by the rollout operation as follows.
3.3

Rollout Evaluation

The rollout evaluation is a Monte-Carlo method to estimate
the value of a policy ~q at a state s (or belief state b), without
requiring an explicit representation of the value function as
the DP algorithm does. A rollout for hs, ~qi simulates a trajectory starting from state s and choosing actions according
to policy ~q up to the horizon T . The observed total accumulated reward is averaged over K rollouts to estimate the
value V (s, ~q). If a belief state b is given, it is straightforward to draw a state s from b and perform this simulation.
The outline of the rollout process is given in Algorithm 4.
The accuracy of the expected value estimate improves with
the number of rollouts. Intuitively, the value starting from
hs, ~qi can be viewed as a random variable whose expectation is V (s, ~q). Each rollout term vk is a sample of this
random variable and the average of these Ṽ is an unbiased
estimate of V (s, ~q). Thus, we can apply the following Hoeffding bounds to determine the accuracy of this estimate.
Property 1 (Hoeffding inequality). Let V be a random
variable in [Vmin , Vmax ] with V̄ = E[V ], observed values
PK
1
v1 , v2 , · · · , vK of V , and Ṽ = K
k=1 vk . Then

P r(Ṽ ≤ V̄ + ε) ≥ 1 − exp −2Kε2 /V∆2

P r(Ṽ ≥ V̄ − ε) ≥ 1 − exp −2Kε2 /V∆2
where V∆ = Vmax − Vmin is the range of values.

Given a particular confidence threshold δ and a size of
samples K, we can produce a PAC-style error bound ε on
the accuracy of our estimate Ṽ :
s
V∆2 ln ( 1δ )
(4)
ε=
2K
Property 2. If the number of rollouts K is infinitely large,
the average value returned by the rollout algorithm Ṽ will
converge to the expected value of the policy V̄ .
The required sample size given error tolerance ε and
confidence threshold δ for the estimation of Ṽ is:
K(ε, δ) =

V∆2 ln ( 1δ )
2ε2

(5)

It is difficult to compute a meaningful error bound for the
overall algorithm. There are several reasons: (1) DecRSPI is an MBDP-based algorithm and MBDP itself has no
guarantee on the solution quality since the belief sampling
method is based on domain-dependent heuristics; (2) the
local search technique — which updates one agent’s policy
at a time — could get stuck in a suboptimal Nash equilibrium; and (3) the error may accumulate over the horizon,
because the policies of the current iteration depend on the
policies of previous iterations. Thus, we demonstrate the
performance and benefits of DecRSPI largely based on experimental results.
3.4

Complexity Analysis

Note that the size of each agent’s policy is predetermined
with T layers and N decision nodes in each layer. At each
iteration, DecRSPI chooses an unimproved joint policy and
tries to improve the policy parameters (actions and node
selection functions) of each agent. Thus, the amount of
space is of the order O(mT N ) for m agents. Several rollouts are performed in the main process of each iteration.
The time per rollout grows linearly with T . Therefore,
the total time with respect to the horizon is on the order
of 1 + 2 + · · · + T = (T 2 + T )/2, i.e. O(T 2 ).
Theorem 3. The DecRSPI algorithm has linear space and
quadratic time complexity with respect to the horizon T .

700
600
500

140
120

400
300

60
40
20

-100

(a) Meeting in a 3×3 Grid
25000

0
10 20 30 40 50 60 70 80 90 100
Horizon

10 20 30 40 50 60 70 80 90 100
Horizon

(b) Cooperative Box Pushing

(c) Stochastic Mars Rover

140

Box Pushing
Meeting Grid
Mars Rover

120
100
Value

20000
15000
10000

Box Pushing
Meeting Grid
Mars Rover

80
60
40

5000

20

0

0
10 20 30 40 50 60 70 80 90 100
Horizon

(d) Horizon vs. Runtime

10 20 30 40 50 60 70 80 90 100
Number of Trials

(e) Trials vs. Value

Time (s)

30000

80

100
0

DecRSPI
DGD
PBIP-IPG

100

200

10 20 30 40 50 60 70 80 90 100
Horizon

Time (s)

160

DecRSPI
DGD
PBIP-IPG
Value

DecRSPI
DGD
PBIP-IPG
Value

Value

100
90
80
70
60
50
40
30
20
10
0

180
160
140
120
100
80
60
40
20
0

Box Pushing
Meeting Grid
Mars Rover

10 20 30 40 50 60 70 80 90 100
Number of Trials

(f) Trials vs. Runtime

Figure 2: Experimental results for standard benchmark problems.
Clearly, the amount of space grows linearly with the number of agents. At each iteration, the main loop chooses
N joint policies. For each joint policy, the improvement
process selects agents alternatively until no improvement
is possible. In practice, we set thresholds both for the
minimum improvement (e.g. 10−4 ) and the maximum repeat count (e.g. 100). The improvement process terminates
when one of these bounds is reached. Theoretically, the
runtime of a rollout inside the improvement process is independent of the number of agents. However in practice,
systems with more agents will take significantly more time
to simulate, thereby increasing the time per rollout. But
this is due to the complexity of domains or simulators, not
the complexity of the DecRSPI algorithm.
Theorem 4. Ignoring system simulation time, the DecRSPI algorithm has linear time and space complexity with
respect to the number of agents |I|.

4

Experiments

We performed experiments on several common benchmark
problems in the DEC-POMDP literature to evaluate the solution quality and runtime of DecRSPI. A larger distributed
sensor network domain was used to test the scalability of
DecRSPI with respect to the number of agents.
4.1

Benchmark Problems

We first tested DecRSPI on several common benchmark
problems for which the system dynamics — an explicit representation of the transition, observation and reward functions — is available. To run the learning algorithm, we implemented a DEC-POMDP simulator based on the dynamics and learned the joint policy from the simulator. We used
two types of heuristic policies to sample belief states: the

random policy that randomly chooses an action with a uniform distribution, and the MDP-based policy that chooses
an action according to the global state (which is known during the learning phase). For the benchmark problems, we
solved the underlying MDP models and used the policies
for sampling. DecRSPI selects a heuristic each time with a
chance of 0.55 acting randomly and 0.45 for MDP policies.
There are few work on learning policies in the general DEC-POMDP setting. In the experiments, we compared our results with the distributed gradient descent
(DGD) [15] with different horizons. The DGD approach
performs the gradient-descent algorithm for each agent independently to adapt the parameters of each agent’s local
policy. We also present the results of PBIP-IPG [1] — the
best existing planning algorithm — for these domains. Notice that PBIP-IPG computes the policy based on an explicit
model of system dynamics. Thus, the values of PBIP-IPG
can be viewed as upper bounds for learning algorithms.
Due to the randomness of Monte-Carlo methods, we ran
the algorithm 20 times per problem and reported average
runtimes and values. The default number of policy nodes
N is 3 and the number of samples K is 20.
We experimented with three common DEC-POMDP
benchmark problems, which are also used by PBIPIPG [1]. The Meeting in a 3×3 Grid problem [3] involves
two robots that navigate in a 3×3 grid and try to stay as
much time as possible in the same cell. We adopted the
version used by Amato et al. [1], which has 81 states, 5
actions and 9 observations per robot. The results for this
domain with different horizons are given in Figure 2(a).
The Cooperative Box Pushing problem [17] involves two
robots that cooperate with each other to push boxes to their
destinations in a 3×4 grid. This domain has 100 states, 4
actions and 5 observations per robot. The results are given
in Figure 2(b). The Stochastic Mars Rover problem [1] is

80

600

DecRSPI-Value
DGD-Value
DecRSPI-Time
DecRSPI+SIM-Time

60
40

500

Figure 3: The distributed sensor network domain.

400

0
300
-20
-40

Time (s)

Value

20

200

-60
100

a larger domain with 2 robots, 256 states, 6 actions and 8
observations per robot. The results are given in Figure 2(c).
In all three domains, DecRSPI outperforms DGD with
large margins. In the Meeting in Grid and Mars Rover
domains, the learning results of DecRSPI are quite close
to the planning values of PBIP-IPG. Note that PBIP-IPG
is also an MBDP-based algorithm whose values represent
good upper bounds on the learning quality of DecRSPI. Being close to the value of PBIP-IPG means that DecRSPI
does learn good policies given the same heuristics and policy sizes. In the Cooperative Box Pushing domain, the gap
between DecRSPI and PBIP-IPG is a little bit larger because this problem has more complex interaction structure
than the other two domains. Interestingly, in this domain,
the value of DGD decreases with the horizon.
We also present timing results for each domain with different horizons (T ) in Figure 2(d), which shows the same
property (quadratic time complexity) as stated in Theorem 3. In Figure 2(e), we test DecRSPI with different number of trials (K) and a fixed horizon of 20. The value of the
Meeting in Grid and Mars Rover domains becomes stable
when the number of trials is larger than 10. But the Box
Pushing problem needs more trials (about 40) to get to a
stable value, which is very close to the value of PBIP-IPG.
In Figure 2(f), we show that runtime grows linearly with
the number of trials in all three domains. It is worthwhile to
point out that in these experimental settings, DecRSPI runs
much faster than PBIP-IPG. For example, in the Stochastic
Mars Rover domain with horizon 20, PBIP-IPG may take
14947s while DecRSPI only needs 49.8s.
4.2

Distributed Sensor Network

The distributed sensor network (DSN) problem, adapted
from [20], consists of two chains with identical number
of sensors as shown in Figure 3. The region between the
chains is split into cells and each cell is surrounded by four
sensors. The two targets can move around in the place, either moving to a neighboring cell or staying in place with
equal probability. Each target starts with an energy level
of 2. A target is captured and removed when it reaches 0.
Each sensor can take 3 actions ( track-left, track-right and
none) and has 4 observations (left and right cells are occupied or not), resulting in joint spaces of 3|I| actions and
4|I| observations (e.g. 320 ≈ 3.5 × 109 joint actions and
420 ≈ 1.1 × 1012 joint observations for the 20 agents case).
Each track action has a cost of 1. The energy of a target
will be decreased by 1 if it is tracked by at least three of the

-80
-100

0
8

10

12
14
16
Number of Agents

18

20

Figure 4: Value and runtime of DSN (T =10, N =3, K=20).
four surrounding sensors at the same time. When a target is
captured, the team gets a reward of 10. When all targets are
captured, the DSN restarts with random target positions.
This domain is designed to demonstrate the scalability of
DecRSPI over the number of agents. Most of the planning algorithms for general DEC-POMDPs have only been
tested in domains with 2 agents [1, 8, 11, 17, 18]. It is very
challenging to solve a general DEC-POMDP problems
with many agents because the joint action and observation
spaces grow exponentially over the number of agents. Our
results in the DSN domain with horizon 10 and random
heuristics are shown in Figure 4. Again, DecRSPI achieves
much better value than DGD. The value decreases with the
growing number of agents because there are more cells for
the targets to move around and greater chance of sensor
miscoordination. Note that DecRSPI solves this problem
without using any specific domain structure and the learned
policies are totally decentralized, without any assumption
of communication or global observability. The figure also
shows two measures of timing results: DecRSPI-Time —
the runtime of DecRSPI, and DecRSPI+SIM-Time — the
overall runtime including domain simulation time. The two
measures of runtime grow with the number of agents. As
stated in Theorem 4, DecRSPI-Time grows linearly, which
shows that it scales up very well with the number of agents.

5

Related Work

Several policy search algorithms have been introduced to
learn agents’ policies without the system dynamics. The
distributed gradient descent (DGD) algorithm performs
gradient-based policy search independently on each agent’s
local controller using the experience data [15]. Zhang et al.
[20] proposed an online natural actor-critic algorithm using
conditional random fields (CRF). It can learn cooperative
policies with CRFs, but it assumes that agents can communicate and share their local observations at every step.
Melo [13] proposed another actor-critic algorithm with natural gradient, but it only works for transition-independent
DEC-POMDPs. In contract, our algorithm learns cooperative policies for the general DEC-POMDP setting without
any assumption about communication.

The rollout sampling method has been introduced to learn
MDP policies without explicitly representing the value
function [9, 10, 12]. The main idea is to produce training
data through extensive simulation (rollout) of the previous
policy and use a supervised learning algorithm (e.g. SVM)
to learn a new policy from the labeled data. The rollout
technique is also widely used to perform lookahead and estimate the value of action in online methods [4, 5, 7]. Our
algorithm uses rollout sampling to estimate the parameters
of policy improvements and select the best joint policy.

6

Conclusion

We have presented the decentralized rollout sampling policy iteration (DecRSPI) algorithm for learning cooperative
policies in partially observable multi-agent domains. The
main contribution is the ability to compute decentralized
policies without knowing explicitly the system dynamics.
In many applications, the system dynamics is either too
complex to be modeled accurately or too large to be represented explicitly. DecRSPI learns policies from experience obtained by merely interacting with the environment.
The learned policies are totally decentralized without any
assumption about communication or global observability.
Another advantage of DecRSPI is that it focuses the computation only on reachable states. As the experiments show,
little sampling is needed for domains where agents have
sparse interaction structures, and the solution quality calculated by a small set of samples is quite close to the best existing planning algorithms. Most importantly, DecRSPI has
linear time complexity over the number of agents. Therefore DecRSPI can solve problems with up to 20 agents as
shown in the experiments. Additionally, DecRSPI bounds
memory usage as other MBDP-based algorithms. In the future, we plan to further exploit the interaction structure of
agents and make even better use of samples, which will be
helpful for large real-world domains.

Acknowledgments
This work was supported in part by the Air Force Office
of Scientific Research under Grant No. FA9550-08-1-0181,
the National Science Foundation under Grant No. IIS0812149, the Natural Science Foundations of China under
Grant No. 60745002, and the National Hi-Tech Project of
China under Grant No. 2008AA01Z150.



Decentralized POMDPs provide an expressive framework for multi-agent sequential decision making. While finite-horizon DECPOMDPs have enjoyed significant success,
progress remains slow for the infinite-horizon
case mainly due to the inherent complexity of
optimizing stochastic controllers representing
agent policies. We present a promising new
class of algorithms for the infinite-horizon
case, which recasts the optimization problem
as inference in a mixture of DBNs. An attractive feature of this approach is the straightforward adoption of existing inference techniques in DBNs for solving DEC-POMDPs
and supporting richer representations such
as factored or continuous states and actions.
We also derive the Expectation Maximization
(EM) algorithm to optimize the joint policy represented as DBNs. Experiments on
benchmark domains show that EM compares
favorably against the state-of-the-art solvers.

1

Introduction

Decentralized partially observable MDPs (DECPOMDPs) have emerged in recent years as an important framework for modeling sequential decision
making by a team of agents [5]. Their expressive
power makes it possible to tackle coordination problems in which agents must act based on different partial information about the environment and about each
other to maximize a global reward function. Applications of DEC-POMDPs include coordinating the
operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor
agents [13]. However, the rich model comes with a
price–optimally solving a finite-horizon DEC-POMDP

Shlomo Zilberstein
Computer Science Dept.
University of Massachusetts, Amherst
shlomo@cs.umass.edu

is NEXP-Complete [5]. In contrast, finite-horizon
POMDPs are PSPACE-complete [12], a strictly lower
complexity class that highlights the difficulty of solving DEC-POMDPs.
Recently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon
DEC-POMDPs [11, 8, 16]. However, unlike their
point-based counterparts in POMDPs ([15, 17]), they
cannot be easily adopted for the infinite-horizon case
due to a variety of reasons. For example, POMDP algorithms represent the policy compactly as α-vectors,
whereas all DEC-POMDP algorithms explicitly store
the policy as a mapping from observation sequences
to actions, making them unsuitable for the infinitehorizon case. In POMDPs, the Bellman equation
forms the basis of most point-based solvers, but as
Bernstein et. al. [4] highlight, no analogous equation
exists for DEC-POMDPs.
To alleviate such problems, most infinite-horizon algorithms represent agent policies as finite-state controllers [1, 4]. So far, only two algorithms have
shown promise for effectively solving infinite-horizon
DEC-POMDPs–decentralized bounded policy iteration (DEC-BPI) [4] and a non-linear programming
based approach (NLP) [1]. However, both of these
algorithms have significant drawbacks in terms of the
representative class of problems that can be handled.
For example, solving DEC-POMDPs with continuous
state or action spaces is not supported by either of
these approaches. Scaling up to structured representations such as factored or hierarchical state-space is
difficult due to convergence issues in DEC-BPI and
a potential increase in the number of non-linear constraints in the NLP solver. Further, none of the above
approaches have been shown to work for more than
2 agents, a significant bottleneck for solving practical
problems.
To address these shortcomings, we present a promising new class of algorithms which amalgamates planning with probabilistic inference and opens the door

to the application of rich inference techniques to solving infinite-horizon DEC-POMDPs. Our technique is
based on Toussaint et. al.’s approach of transforming the planning problem to its equivalent mixture
of dynamic Bayes nets (DBNs) and using likelihood
maximization in this framework to optimize the policy
value [20, 19]. Earlier work on planning by probabilistic inference can be found in [2]. Such approaches have
been successful in solving MDPs and POMDPs [19].
They also easily extend to factored or hierarchical
structures [18] and can handle continuous action and
state spaces thanks to advanced probabilistic inference
techniques [10]. We show how DEC-POMDPs, which
are much harder to solve than MDPs or POMDPs,
can also be reformulated as a mixture of DBNs. We
then present the Expectation Maximization algorithm
(EM) to maximize the reward likelihood in this framework. The EM algorithm naturally has the desirable
anytime property as it is guaranteed to improve the
likelihood (and hence the policy value) with each iteration. We also discuss its extension to large multiagent systems. Our experiments on benchmark domains show that EM compares favorably against the
state-of-the-art algorithms, DEC-BPI and NLP-based
optimization. It always produces better quality policies than DEC-BPI and for some instances, it nearly
doubles the solution quality of the NLP solver. Finally, we discuss potential pitfalls, which are inherent
in the EM based approach.

This added uncertainty about other agents in the system make DEC-POMDPs NEXP complete [5].
We are concerned with solving infinite-horizon DECPOMDPs with a discount factor γ. We represent
the stationary policy of each agent using a fixed
size, stochastic finite-state controller (FSC) similar to [1]. An FSC can be described by a tuple
hN, π, λ, νi. N denotes a finite set of controller nodes
n; π : N → ∆A represents the actions selection model
or the probability πan = P (a|n); λ : N × Y → ∆N
represents the node transition model or the probability λn0 ny = P (n0 |n, y); ν : N → ∆N represents
the initial node distribution νn = P (n). We adopt
the convention that nodes of agent 1’s controller are
denoted by p and agent 2’s by q. Other problem
parameters such as observation function P (y, z|s, a, b)
are represented using subscripts as Pyzsab . The value
for starting the controllers in nodes hp, qi at state s is
given by:
h
X
V (p, q, s) =
πap πbq Rsab +
i
X a,b X
X
γ
Ps0 sab
Pyzs0 ab
λp0 py λq0 qz V (p0 , q 0 , s0 ) .
s0

p0 ,q 0

y,z

The goal is to set the parameters hπ, λ, νi of the agents’
controllers (of some given size) that maximize the expected discounted reward for the initial belief b0 :
X
V (b0 ) =
νp νq b0 (s)V (p, q, s)
p,q,s

2

The DEC-POMDP model

In this section, we introduce the DEC-POMDP model
for two agents [5]. Note that finite-horizon DECPOMDPs are NEXP complete even for two agents.
The set S denotes the set of environment states, with
a given initial state distribution b0 . The action set
of agent 1 is denoted by A and agent 2 by B. The
state transition probability P (s0 |s, a, b) depends upon
the actions of both the agents. Upon taking the joint
action ha, bi in state s, agents receive the joint reward
R(s, a, b). Y is the finite set of observations for agent 1
and Z for agent 2. O(s, ab, yz) denotes the probability
P (y, z|s, a, b) of agent 1 observing y ∈ Y and agent 2
observing z ∈ Z when the joint action ha, bi was taken
and resulted in state s.
To highlight the differences between a single agent
POMDP and a DEC-POMDP, we note that in a
POMDP an agent can maintain a belief over the environment state. However, in a DEC-POMDP, an agent
is not only uncertain about the environment states but
also about the actions and observations of the other
agent. Therefore in a DEC-POMDP a belief over the
states cannot be maintained during execution time.

3

DEC-POMDPs as mixture of DBNs

In this section, we describe how DEC-POMDPs can
be reformulated as a mixture of DBNs such that maximizing the reward likelihood (to be defined later) in
this framework is equivalent to optimizing the joint
policy. Our approach is based on the framework proposed in [19, 20] to solve Markovian planning problems
using probabilistic inference. First we informally describe the intuition behind this reformulation (for details please refer to [19]) and then we describe in detail
the steps specific to DEC-POMDPs.
A DEC-POMDP can be described using a single DBN
where the reward is emitted at each time step. However, in our approach, it is described by an infinite
mixture of a special type of DBNs where reward is
emitted only at the end. For example, Fig. 1(a) describes the DBN for time t = 0. The key intuition
is that for the reward emitted at any time step T ,
we have a separate DBN with the general structure
as in Fig. 1(b). Further, to simulate the discounting of rewards, probability of time variable
T is set as
P∞
P (T = t) = γ t (1 − γ). This ensures that t=0 pt = 1.
In addition, the random variable r shown in Fig. 1(a,b)

p0

p0

p1
a0

a0

y1

a1

y1z1
s0

r

s1
b0

q0

(a)

pT

y2

yT

y2z2

yT zT

p0

p1

s2
z1
q1

b1

pT

aT

a0
s0

b0
q0

p2

sT

y1

a1

yT

aT

r

z2

zT

q2

qT

(b)

bT
s0

s1

sT

r

(c)

Figure 1: a) DEC-POMDP DBN for time step 0. b) for time step T . c) POMDP DBN for time step T

is a binary variable with its conditional distribution
(for any time T ) described using the normalized immediate reward as R̂sab = P (r = 1|sT = s, aT = a, bT =
b) = (Rsab − Rmin )/(Rmax − Rmin ). This scaling of
the reward is the key to transforming the optimization
problem from the realm of planning to likelihood maximization as stated below. θ denotes the parameters
hπ, λ, νi for each agent’s controller.
Theorem 1. Let the1 CPT of binary rewards r be such
that R̂sab ∝ Rsab and the discounting time
prior be set
1
as P (T ) = γ T (1 − γ). Then, maximizing the likelihood
Lθ = P (r = 1; θ) in the mixture of DBNs is equivalent
to optimizing the DEC-POMDP policy. Furthermore,
the joint-policy value relates linearly P
to the likelihood
as V θ = (Rmax − Rmin )Lθ /(1 − γ) + T γ T Rmin
The proof is omitted as it is very similar to that of
MDPs and POMDPs [19]. Before detailing the EM algorithm, we describe the DBN representation of DECPOMDPs–the basis for any inference technique.
The DBN for any time step T is shown in Fig. 1(b).
Every node is a random variable with subscripts indicating time. pi denotes controller nodes for agent 1
and qi for agent 2. The remaining nodes represent the
states, actions, and observations. There are four kinds
of dependencies induced by the DEC-POMDP model
that the DBN must represent:
• State transitions: State transitions as a result
of the joint action of both agents and the previous
state, shown by the DBN’s middle layer.
• Controller node transitions (λ): These transitions depend on the last controller state and the
most recent individual observation received. They
are shown in the top and bottom layers.
• Action probabilities (π): The action taken at
any time step t depends on the current controller
state. The links between controller nodes (pi or qi )
and action nodes (ai or bi ) model this.
• Observation probabilities: First, the probability of receiving joint observation yi zi depends on
the joint action of both agents and the domain

state. This relationship is modeled by the DBN
nodes labeled yi zi . Second, the individual observation each agent receives is a deterministic function of the joint observation. That is Pyy0 z0 =
P (y|y 0 z 0 ) = 1 if y = y 0 else 0. This is modeled
by a link between yi zi and the nodes yi and zi .
To highlight the differences from a POMDP, Fig. 1(c)
shows the DBN for a POMDP. The sheer scale of interactions present in a DEC-POMDP DBN become clear
1
from this comparison, also highlighting
the difficulty
of solving DEC-POMDPs even approximately. In a
POMDP, an agent receives the observation which is
affected by the environment state, whereas in a DECPOMDP agents only perceive the individual part of
the joint observation yi zi . Such differences in the interaction structure make the E and M steps of a DECPOMDP EM very different from that of a POMDP,
despite sharing the same high-level principles.

4

EM algorithm for DEC-POMDPs

This section describes the EM algorithm [7] for maximizing the reward likelihood in the mixture of DBNs
representing DEC-POMDPs. In the corresponding
DBNs, only the binary reward is treated as observed
(r = 1); all other variables are latent. While maximizing the likelihood, EM yields the DEC-POMDP
joint-policy parameters θ. EM also possesses the desirable anytime characteristic as the likelihood (and the
policy value which is proportional to the likelihood) is
guaranteed to increase per iteration until convergence.
We note that EM is not guaranteed to converge to
the global optima. However, in the experiments we
show that EM almost always achieves similar values
as the state-of-the-art NLP based solver [1] and much
better than DEC-BPI [4]. The main advantage of using EM lies in its ability to easily generalize to much
richer representations than currently possible for DECPOMDPs such as factored or hierarchical controllers,
continuous state and action spaces. Another important advantage is the ability to generalize the solver to
larger multi-agent systems with more than 2 agents.

The E step we derive next is generic as any probabilistic inference technique can be used.
4.1

E-step

p,q,s

In the E-step, for the fixed parameter θ, forward messages α and backward messages β are propagated.
First, we define the following Markovian transitions on
the (p, q, s) state in the DBN of Fig. 1(b). These transitions are independent of the time t due to the stationary joint policy. We also adopt the convention that for
any random variable v, v 0 refers to the next time slice
and v̄ refers to the previous time slice. For any group
of variables v, Pt (v, v0 ) refers to P (vt = v, vt+1 = v0 ).
P (p0, q 0, s0 |p, q, s) =
X
λp0 py0 λq0 qz0 Py0 z0 abs0 πap πbq Ps0 sab

(1)

aby 0 z 0

α0 (p, q, s)
0

= νp νq b0 (s)
X
=
P (p0 , q 0 , s0 |p, q, s)αt−1 (p, q, s)

0

αt (p , q , s )

p,q,s

Intuitively, α messages compute the probability of visiting a particular (p, q, s) state in the DBN as per the
current policy. The β messages are similar to computing the value of starting the controllers in nodes hp, qi
at state s using dynamic programming. They are propagated backwards and are defined as Pt (r = 1|p, q, s).
However, this particular definition would require separate inference for each DBN as for T and T 0 step
DBN, βt will be different due to difference in the
time-to-go (T − t and T 0 − t). To circumvent this
problem, β messages are indexed backward in time as
βτ (p, q, s) = PT −τ (r = 1|p, q, s) using the index τ such
that τ = 0 denotes the time slice t = T . Hence we get:
X
β0 (p, q, s) =
Rsab πap πbq
ab

βτ (p, q, s)

If both α and
β messages are propagated for k steps
P2k−1
and Lθ2k  T =0 γ T LθT , then the message propagation can be stopped.
4.1.1

=

X

βτ −1 (p0 , q 0 , s0 )P (p0 , q 0 , s0 |p, q, s)

p0 ,q 0 ,s0

Based on the α and β messages
P we also calculate two
more quantities α̂(p, q, s) = t P (T = t)α(p, q, s) and
P
β̂(p, q, s) = t P (T = t)β(p, q, s), which will be used
in the M-step. The cut-off time for message propagation can either be fixed a priori or be more flexible
based on the likelihood accumulation. If α messages

Complexity

Calculating the Markov transitions on the (p, q, s)
chain has complexity O(N 4 S 2 A2 Y 2 ), where N is the
maximum number of nodes for a controller. The
message propagation has complexity O(Tmax N 4 S 2 ).
Techniques to effectively reduce this complexity without sacrificing accuracy will be discussed later.
4.2

αt is defined as Pt (p, q, s; θ). It might appear that
we need to propagate α messages for each DBN separately, but as pointed out in [19], only one sweep is
required as the head of the DBN is shared among all
the mixture components. That is, α2 is the same for
all the T-step DBNs with T ≥ 2. We will omit using
θ as long as it is unambiguous.

0

are propagated for t-steps and β-messages for τ steps,
then the likelihood for T = t + τ is given by
X
Lθt+τ = P (r = 1|T = t + τ ; θ) =
αt (p, q, s)βτ (p, q, s)

M-step

In the DBNs of Fig. 1(a,b) every variable is hidden except the reward variable. After each M-step, EM provides better estimates of these variables, improving the
likelihood Lθ and hence the policy value. For details
of EM, we refer to [7]. The parameters to estimate are
hπ, λ, νi for each agent. For a particular DBN for time
T , let L̃ = (P, Q, A, B, S) denote the latent variables,
where each variable denotes a sequence of length T .
That is, P = p0:T . EM maximizes the following expected complete log-likelihood for the DEC-POMDP
DBN mixture. θ denotes the previous parameters and
θ? denotes new parameters.
XX
Q(θ, θ? ) =
P (r = 1, L̃, T ; θ) log P (r = 1, L̃, T ; θ? )
T

L̃

In the rest of the section, all the derivations refer to
the general DBN structure of the DEC-POMDP as in
Fig. 1(b). The joint probability of all the variables is:
T


Y
P (r = 1, L̃, T ; θ) = P (T ) Rsab t=T
πap πbq Pss̄āb̄


t=1
Pyyz Pzyz Pyzsāb̄ λpp̄y λqq̄z πap πbq νp νq b0 (s) t=0
(2)

where

 brackets indicate the time slices, i.e.,
Rsab t=T = R(sT , aT , bT ). Taking the log, we get:
log P (r = 1, L̃, T ) = . . . +
+

T
X
t=1

log λpt pt−1 yt +

T
X
t=0
T
X

log πat pt +

T
X

log πbt qt

t=0

log λqt qt−1 zt

t=1

+ log νp0 + log νq0

(3)

where the missing terms represents the quantities independent of θ. As all the policy parameters hπ, λ, νi get
separated out for each agent in the log above, we first
derive the action updates for an agent by substituting
Eq. 3 in Q(θ, θ? )

4.2.1

Action updates

?
The update for action parameters πap
for agent 1 can
?
be derived by simplifying Q(θ, θ ) as follows:

Q(θ, θ? ) =

∞
X

P (T )

T X
X


?
P (r = 1, a, p|T ; θ) t log πap
t=0 a,p

T =0

By breaking the above summation between t = T and
t = 0 to T − 1, we get
∞
X

P (T )

T =0

X

?
Rsab πap πbq αT (p, q, s) log πap
+

X

p0 q 0 s0 y 0 z 0

b

The above expression is maximized by setting the pa?
rameter πap
to be:

P (T )

T =0

apqbs
T
−1
X

∞
X

The product P (s0 |a, q, s)P (y 0 z 0 |a, q, s0 ) can be further
simplified by marginalizing out over actions b of agent
2 as follows:
X
X
X
γ
?
=
πap log πap
α̂(p, q, s)
Rsab πbq +
1
−
γ
ap
qs
b

X
X
0 0 0
0
0
0
0
0
0
0
0
β̂(p , q , s )λp py λq qz
Py z s ab πbq Ps sab

X
πap X
γ
α̂(p, q, s)
Rsab πbq +
Cp qs
1−γ
b

X
X
β̂(p0 , q 0 , s0 )λp0 py0 λq0 qz0
Py0 z0 s0 ab πbq Ps0 sab (4)
?
πap
=

?
βT −t−1 (p0 , q 0 , s0 )Pt (a, p, p0 , q 0 , s0 ) log πap

t=0 app0 q 0 s0

p0 q 0 s 0 y 0 z 0

In the above equation, we marginalized the last time
slice over the variables (q, b, s). For the intermediate
time slice t, we condition upon the variables (p0 , q 0 , s0 )
in the next time slice t + 1. We now use the definition
of α̂ and move the summation over time T inside for
the last time slice and further marginalize over the
remaining variables (q, s) in the intermediate slice t:
=

X

?
Rsab πap πbq α̂(p, q, s) log πap
+

b

where Cp is a normalization constant. The action pa?
rameters πbq
of the other agent can be found similarly
by the analogue of the previous equation.
4.2.2

Controller node transition updates

The update for controller node transition parameters
λpp̄y for agent 1 can be found by maximizing Q(θ, θ? )
w.r.t λ?pp̄y as follows.

a,p,q,b,s
∞
X

P (T )

T
−1 X
X

?
log πap

0

0

0

βT −t−1 (p , q , s )πap

Upon further marginalizing over the joint observations
y 0 z 0 and simplifying we get:
X
XX
?
=
πap log πap
Rsab πbq α̂(p, q, s) +
qs

X

∞
X

p0 q 0 s0 y 0 z 0 T =0

P (T )

b
T
−1
X

∞
X

P (T )

T =0

P (p , q , s |a, p, q, s)αt (p, q, s)

ap

Q(θ, θ? ) =

p0 q 0 s0 sq
0 0 0

t=0 ap

T =0

X

0

0

0

0

βT −t−1 (p , q , s )P (s |a, q, s)

t=0

λp0 py0 λq0 qz0 P (y 0 z 0 |a, q, s0 )αt (p, q, s)

=

∞
X
T =0

P (T )

T X
X

log λ?pp̄y βT −t (p, q, s)Pt (p, p̄, y, s, q|T ; θ)

t=1 pp̄ysq

By further marginalizing over the variables (s̄, q̄) for
the previous time slice of t and over the observations
z of the other agent, we get
=

X

λpp̄y log λ?pp̄y

pp̄y

p0 q 0 s 0 y 0 z 0

t=1 pp̄y

By marginalizing over the variables (q, s) for the current time slice t, we get



We resolve the above time summation, as in [19], based
P∞ PT −1
on the fact that
t=0 f (T − t − 1)g(t) can be
T =0
P∞ P
∞
rewritten as t=0 T =t+1 fP
(T − t − 1)g(t)
P∞ and then
∞
setting τ = T − t − 1 to get t=0 g(t) τ =0 f (τ ).
Finally we get:
X
X
X
γ
?
=
πap log πap
α̂(p, q, s)
Rsab πbq +
1
−
γ
ap
qs
b

X
0 0 0
0
0 0
0
β̂(p , q , s )λp0 py0 λq0 qz0 P (s |a, q, s)P (y z |a, q, s )

T X
X


P (r = 1, p, p̄, y|T ; θ) t log λ?pp̄y

∞
X

P (T )

T X
X

βT −t (p, q, s)λqq̄z

t=1 sqs̄q̄z

T =0

P (yz|p̄, q̄, s)P (s|p̄, q̄, s̄)αt−1 (p̄, q̄, s̄)
The above equation can be further simplified by
marginalizing the product P (yz|p̄, q̄, s)P (s|p̄, q̄, s̄) over
actions a and b of both the agents as follows:
=

X
pp̄y

λpp̄y log λ?pp̄y

∞
X
T =0

P (T )

T X
X

βT −t (p, q, s)λqq̄z

t=1 sqs̄q̄z

αt−1 (p̄, q̄, s̄)

X
ab

Pyzsab Pss̄ab πap̄ πbq̄

Upon resolving the time summation as before, we get
the final M-step estimate:
λ?pp̄y =

λpp̄y X
α̂(p̄, q̄, s̄)β̂(p, q, s)λqq̄z
Cp̄y sqs̄q̄z
X
Pyzsab Pss̄ab πap̄ πbq̄

The parameters λ?qq̄z for the other agent can be found
in an analogous way.
Initial node distribution

The initial node distribution ν for controller nodes of
agent 1 and 2 can be updated as follows. We do not
show the complete derivation as it is similar to that of
the other parameters.
νp X
β̂(p, q, s)νq Ps b0 (s)
(6)
νp? =
Cp qs
4.2.4

Complexity and implementation issues

The complexity of updating all action parameters is
O(N 4 S 2 AY 2 ). Updating node transitions requires
O(N 4 S 2 Y 2 + N 2 S 2 Y 2 A2 ). This is relatively high
when compared to the POMDP updates requiring
O(N 2 S 2 AY ) mainly due to the scale of the interactions present in DEC-POMDPs.
In our experimental settings, we observed that having a relatively small sized controller (N ≤ 5) suffices to yield good quality solutions. The main contributor to the complexity is the factor S 2 as we
experimented with large domains having nearly 250
states. The good news is that the structure of the
E and M-step equations provides a way to effectively
reduce this complexity by significant factor without
sacrificing accuracy. For a given state s, joint action
ha, bi and joint observation hy, zi, the possible next
states can be calculated as follows: succ(s, a, b, y, z) =
{s0 |P (s0 |s, a, b)P (y, z|s0 , a, b) > 0}. For most of the
problems, the size of this set is typically a constant
k < 10. Such simple reachability analysis and other
techniques could speed up the EM algorithm by more
than an order of magnitude for large problems. The
effective complexity reduces to O(N 4 SAY 2 k) for the
action updates and O(N 4 SY 2 k+N 2 SY 2 A2 k) for node
transitions. Other enhancements of the EM implementation are discussed in Section 6.

5

DEC-BPI
4.687
4.068
8.637
7.857

NLP
9.1
9.1
9.1
9.1

EM
9.05
9.05
9.05
9.05

DEC-BPI
< 1s
< 1s
2s
5s

EM
< 1s
< 1s
1.7s
4.62s

Table 1: Broadcast channel: Policy value, execution time

(5)

ab

4.2.3

Size
1
2
3
4

Experiments

We experimented with several standard 2-agent DECPOMDP benchmarks with discount factor 0.9. Complete details of these problems can be found in [1, 4].

We compare our approach with the decentralized
bounded policy iteration (DEC-BPI) algorithm [4] and
a non-convex optimization solver (NLP) [1]. The
DEC-BPI algorithm iteratively improves the parameters of a node using a linear program while keeping
the other nodes’ parameters fixed. The NLP approach
recasts the policy optimization problem as a non-linear
program and uses an off-the-shelf solver, Snopt [9], to
obtain a solution. We implemented the EM algorithm
in JAVA. All our experiments were on a Mac with
4GB RAM and 2.4GHz CPU. Each data point is an
average of 10 runs with random initial controller parameters. In terms of solution quality, EM is always
better than DEC-BPI and it achieves similar or higher
solution quality than NLP. We note that our current
implementation is mainly a proof-of-concept; we have
not yet implemented several enhancements (discussed
later) that could improve the performance of the EM
approach. In contrast, the NLP solver [9] is an optimized package and therefore for larger problems is
currently faster than the EM approach. The fact that
a crude implementation of the EM approach works so
well is very encouraging.
Table 1 shows results for the broadcast channel problem, which has 4 states, 2 actions per agent and 5 observations. This is a networking problem where agents
must decide whether or not to send a message on a
shared channel and must avoid collision to get a reward. We tested with different controller sizes. On
this problem, all the algorithms compare reasonably
well, with EM being better than DEC-BPI and very
close in value to NLP. The time for NLP is also ≈ 1s.
Fig. 2(a) compares the solution quality of the EM approach against DEC-BPI and NLP for varying controller sizes on the recycling robots problem. In this
problem, two robots have the task of picking up cans
in an office building. They can search for a small can, a
big can or recharge the battery. The large item is only
retrievable by the joint action of the two robots. Their
goal is to coordinate their actions to maximize the joint
reward. EM(2) and NLP(2) show the results with controller size 2 for both agents in Fig. 2(a). For this
problem, EM works much better than both DEC-BPI
and the NLP approach. EM achieves a value of ≈ 62
for all controller sizes, providing nearly 12% improvement over DEC-BPI (= 55) and 20% improvement
over NLP (= 51). Fig. 2(b) shows the time comparisons for EM with different controller sizes. Both the
NLP and DEC-BPI take nearly 1s to converge. EM

8

7

7

55

6

6

45
40
EM(2)
EM(4)
NLP(2)
NLP(4)
DEC-BPI(2)
DEC-BPI(4)

35
30
25
20
0

(a)

50

100

150 200
Iteration

250

300

5
4
3

4
3
2

1

1

EM(2)
EM(4)
0

50

100

150
200
Iteration

250

300

(b)

|S| = 3, |A| = 3, |Y | = 2

0
350

50

5

2

0
350

60

Time (sec)

50

Policy Value

8

60

Time (sec)

Policy Value

65

EM(2)
EM(3)
NLP(2)
NLP(3)
0

(c)

50

100

150
200
Iteration

250

40
30
20
10

300

350

|S| = 16, |A| = 5, |Y | = 4

0

EM(2)
EM(3)
0

50

100

150 200
Iteration

250

300

350

(d)

Figure 2: Solution quality and runtime for recycling robots (a) & (b) and meeting on a grid (c) & (d)

with controller size 2 has comparable performance, but
as expected, EM with 4-node controllers takes longer
as the complexity of EM is proportional to O(N 4 ).
Fig. 2(c) compares the solution quality of EM on the
meeting on a grid problem. In this problem, agents
start diagonally across in a 2 × 2 grid and their goal
is to take actions such that they meet each other (i.e.,
share the same square) as much as possible. As the figure shows, EM provides much better solution quality
than the NLP approach. EM achieves a value of ≈ 7,
which nearly doubles the solution quality achieved by
NLP (= 3.3). DEC-BPI results are not plotted as it
performs much worse and achieves a solution quality of
0, essentially unable to improve the policy at all even
for large controllers. Both DEC-BPI and NLP take
around 1s to converge. Fig. 2(d) shows the time comparison for EM versions. EM with 2-node controllers
is very fast and takes < 1s to converge (50 iterations).
Also note that in both the cases, EM could run with
much larger controller sizes (≈ 10), but the increase in
size did not provide tangible improvement in solution
quality.
Fig. 3 shows the results for the multi-agent tiger problem, involving two doors with a tiger behind one door
and a treasure behind the other. Agents should coordinate to open the door leading to the treasure [1].
Fig. 3(a) shows the quality comparisons. EM does
not perform well in this case; even after increasing the
controller size, it achieves a value of −19. NLP works
better with large controller sizes. However, this experiment presents an interesting insight into the workings
of EM as related to the scaling of the rewards. Recalling the relation between the likelihood and the policy
value from Theorem 1, the equation for this problem
0

0.85

-10

0.75
Likelihood

Policy Value

0.8

-20
-30

EM(2)
EM(4)
EM(10)
NLP(2)
NLP(5)
NLP(10)

-40
-50

(a)

0

100

200
Iteration

300

0.7
0.65
0.6
0.55
0.5
0.45

400

|S| = 2, |A| = 3, |Y | = 2

0.4

EM(2)
EM(10)
0

100

200
Iteration

300

400

(b)

Figure 3: Solution quality (a) and likelihood (b) for “tiger”

is: V θ = 1210Lθ − 1004.5. For EM to achieve the
same solution as the best NLP setting (= −3), the
likelihood should be .827. Fig. 3(b) shows that the
likelihood EM converges to is .813. Therefore, from
EM’s perspective, it is finding a really good solution.
Thus, the scaling of rewards has a significant impact
(in this case, adverse) on the policy value. This is a
potential drawback of the EM approach, which applies
to other Markovian planning problems too when using
the technique of [19]. Incidently, DEC-BPI performs
much worse on this problem and gets a quality of −77.
Fig. 4 shows the results for the two largest DECPOMDP domains–box pushing and Mars rovers. In
the box pushing domain, agents need to coordinate
and push boxes into a goal area. In the Mars rovers
domain, agents need to coordinate their actions to perform experiments at multiple sites. Fig. 4(a) shows
that EM performs much better than DEC-BPI for every controller size. For controller size 2, EM achieves
better quality than NLP with comparable runtime
(Fig. 4(b), 500 iterations). However, for the larger controller size (= 3), it achieves slightly lower quality than
NLP. For the largest Mars rovers domain (Fig. 4(c)),
EM achieves better solution quality (= 9.9) than NLP
(= 8.1). However, EM also takes many more iterations
to converge than for previous problems and hence, requires more time than NLP. EM is also much better
than DEC-BPI, which achieves a quality of −1.18 and
takes even longer to converge (Fig. 4(d)).

6

Conclusion and future work

We present a new approach to solve DEC-POMDPs
using inference in a mixture of DBNs. Even a simple
implementation of the approach provides good results.
Extensive experiments show that EM is always better
than DEC-BPI and compares favorably with the stateof-the-art NLP solver. The experiments also highlight
two potential drawbacks of the EM approach: the adverse effect of reward scaling on solution quality and
slow convergence rate for large problems. We are currently addressing the runtime issue by parallelizing the
algorithm. For example, α and β can be propagated
in parallel. Even updating each node’s parameters can

10000

10
0

EM(2)
EM(3)
NLP(2)
NLP(3)
DEC-BPI(2)
DEC-BPI(3)

-10
-20
-30

(a)

0

200

400

600 800 1000 1200 1400
Iteration

|S| = 100, |A| = 4, |Y | = 5

1000

5

100
10
EM(2)
EM(3)
NLP(2)
NLP(3)
DEC-BPI(2)

1
0.1
0

200 400

600 800 1000 1200 1400
Iteration

(b)

Policy Value

20

Time (sec, logscale)

Policy Value

30

100000

10

40

Time (sec, logscale)

50

0
-5
-10
EM(2)
NLP(2)
DEC-BPI(2)

-15
-20

(c)

0

1000

2000
3000
Iteration

4000

5000

|S| = 256, |A| = 6, |Y | = 8

10000
1000
100
10
1

EM(2)
NLP(2)
DEC-BPI(2)
0

1000

2000
3000
Iteration

4000

5000

(d)

Figure 4: Solution quality and runtime for box pushing (a) & (b) and Mars rovers (c) & (d)

be done in parallel for each iteration. Furthermore, the
structure of EM’s update equations is very amenable
to Google’s Map-Reduce paradigm [6], allowing each
parameter to be computed by a cluster of machines in
parallel using Map-Reduce. Such scalable techniques
will certainly make our approach many times faster
than the current serial implementation. We are also
investigating how a different scaling of rewards affects
the convergence properties of EM.
The main benefit of the EM approach is that it opens
up the possibility of using powerful probabilistic inference techniques to solve decentralized planning problems. Using a graphical DBN structure, EM can easily
generalize to richer representations such as factored or
hierarchical controllers, or continuous state and action
spaces. Unlike the existing techniques, EM can easily
extend to larger multi-agent systems with more than 2
agents. The ND-POMDP model [13] is a class of DECPOMDPs specifically designed to support large multiagent systems. It makes some restrictive yet realistic assumptions such as locality of interaction among
agents, and transition and observation independence.
EM can naturally exploit such independence structure
in the DBN and scale to larger multi-agent systems,
something that current infinite-horizon algorithms fail
to achieve. Hence the approach we introduce offers
great promise to overcome the shortcomings of the prevailing approaches to multi-agent planning.

Acknowledgments
Support for this work was provided in part by the
National Science Foundation Grant IIS-0812149 and
by the Air Force Office of Scientific Research Grant
FA9550-08-1-0181.



Computing maximum a posteriori (MAP) estimation in graphical models is an important
inference problem with many applications.
We present message-passing algorithms for
quadratic programming (QP) formulations of
MAP estimation for pairwise Markov random
fields. In particular, we use the concaveconvex procedure (CCCP) to obtain a locally optimal algorithm for the non-convex
QP formulation. A similar technique is used
to derive a globally convergent algorithm for
the convex QP relaxation of MAP. We also
show that a recently developed expectationmaximization (EM) algorithm for the QP formulation of MAP can be derived from the
CCCP perspective. Experiments on synthetic and real-world problems confirm that
our new approach is competitive with maxproduct and its variations. Compared with
CPLEX, we achieve more than an order-ofmagnitude speedup in solving optimally the
convex QP relaxation.

1

INTRODUCTION

Probabilistic graphical models provide an effective
framework for compactly representing probability distributions over high dimensional spaces and performing complex inference using simple local update procedures. In this work, we focus on the class of undirected
models called Markov random fields (MRFs) [Wainwright and Jordan, 2008]. A common inference problem in this model is to compute the most probable assignment to variables, also called the maximum a posteriori (MAP) assignment. MAP estimation is crucial
for many practical applications in computer vision and
bioinformatics such as protein design [Yanover et al.,
2006; Sontag et al., 2008] among others. Computing

Shlomo Zilberstein
Department of Computer Science
University of Massachusetts Amherst
shlomo@cs.umass.edu

MAP exactly is NP-hard for general graphs. Thus approximate inference techniques are often used [Wainwright and Jordan, 2008; Sontag et al., 2010].
Recently, several convergent algorithms have been developed for MAP estimation such as tree-reweighted
max-product [Wainwright et al., 2002; Kolmogorov,
2006] and max-product LP [Globerson and Jaakkola,
2007; Sontag et al., 2008]. Many of these algorithms
are based on the linear programming (LP) relaxation
of the MAP problem [Wainwright and Jordan, 2008].
A different formulation of MAP is based on quadratic
programming (QP) [Ravikumar and Lafferty, 2006;
Kumar et al., 2009]. The QP formulation is an attractive alternative because it provides a more compact
representation of MAP: In a MRF with n variables, k
values per variable, and |E| edges, the QP has O(nk)
variables whereas the LP has O(|E|k 2 ) variables. The
large size of the LP makes off-the-shelf LP solvers
impractical for several real-world problems [Yanover
et al., 2006]. Another significant advantage of the QP
formulation is that it is exact. However, the QP formulation is non-convex, making global optimization hard.
To remedy this, Ravikumar and Lafferty [2006] developed a convex QP relaxation of the MAP problem.
Our main contribution is the analysis of the QP formulations of MAP as a difference of convex functions
(D.C.) problem, which yields efficient, graph-based
message-passing algorithms for both the non-convex
and convex QP formulations. We use the concaveconvex procedure (CCCP) to develop the message passing algorithms [Yuille and Rangarajan, 2003]. Motivated by geometric programming [Boyd et al., 2007],
we present another QP-based formulation of MAP
and solve it using the CCCP technique. The resulting algorithm is shown to be equivalent to a recently
developed expectation-maximization (EM) algorithm
that provides good performance for large MAP problems [Kumar and Zilberstein, 2010]. The CCCP approach, however, is more flexible than EM and makes
it easy to incorporate additional constraints that can

tighten the convex QP [Kumar et al., 2009]. All the developed CCCP algorithms are guaranteed to converge
to a local optimum for non-convex QPs, and to the
global optimum for convex QPs. All the algorithms
also provide monotonic improvement in the objective.

Nonetheless, for several problems, a local optimum of
this QP provides a good solution as we will show empirically. This was also observed by Kumar and Zilberstein [2010].

We experiment on synthetic benchmarks and realworld protein-design problems [Yanover et al., 2006].
Against max-product [Pearl, 1988], CCCP provides
significantly better solution quality, sometimes more
than 45% for large Ising graphs. On the real-world
protein design problems, CCCP achieves near-optimal
solution quality for most instances, and is significantly
faster than the max-product LP method [Sontag et al.,
2008]. Ravikumar and Lafferty [2006] proposed to
solve the convex QP relaxation using standard QP
solvers. Our message-passing algorithm for this case
provides more than an order-of-magnitude speedup
against the state-of-the-art QP solver CPLEX.

2.1

2

QP FORMULATION OF MAP

A pairwise Markov random field (MRF) is described
using an undirected graph G = (V, E). A discrete
random variable xi with a finite domain is associated
with each node i ∈ V of the graph. Associated with
each edge (i, j) ∈ E is a potential function θij (xi , xj ).
The complete assignment x has the probability:
X

p(x; θ) ∝ exp
θij (xi , xj )
ij∈E

The MAP problem consists of finding the most probable assignment to all the variables under p(x; θ). This
is equivalent to finding the assignment
x that maxP
imizes the function f (x; θ) =
ij∈E θij (xi , xj ). We
assume w.l.o.g. that each θij is nonnegative, otherwise
a constant can be added to each θij without changing
the optimal solution. Let pi be the marginal probability associated with each MRF node i ∈ V . The MAP
quadratic programming (QP) formulation [Ravikumar
and Lafferty, 2006] is given by:
X X
max
pi (xi )pj (xj )θij (xi , xj ) (1)
p

subject to

ij∈E xi ,xj

X

pi (xi ) = 1, pi (xi ) ≥ 0 ∀i ∈ V

xi

The above QP is compact even for large graphical
models and has simple linear constraints: O(nk) variables and n normalization constraints where n = |V |
and k is the domain size. Ravikumar and Lafferty
[2006] also show that this formulation is exact. That
is, the global optimum of the above QP will maximize the function f (x; θ) and an integral MAP assignment can be extracted from it. However this formulation is non-convex, making global optimization hard.

The Concave Convex Procedure

The concave-convex procedure (CCCP) [Yuille and
Rangarajan, 2003] is a popular approach to optimize a
general non-convex function expressed as a difference
of two convex functions. We use this method to obtain message-passing algorithms for QP formulations
of MAP. We describe it here briefly.
Consider the optimization problem:
min{g(x) : x ∈ Ω}

(2)

where g(x) = u(x) − v(x) is an arbitrary function with
u , v being real-valued convex functions and Ω being a
convex set. The CCCP method provides an iterative
procedure that generates a sequence of points xl by
solving the following convex program:
xl+1 = arg min{u(x) − xT ∇v(xl ) : x ∈ Ω}

(3)

Each iteration of CCCP decreases the objective
g(x) and is guaranteed to converge to a local optimum [Sriperumbudur and Lanckriet, 2009].
2.2

Solving MAP QP Using CCCP

We first show how the CCCP framework can be used
to solve the QP in Eq. (1). We adopt the convention
that a MAP QP always refers to the QP in Eq. (1);
the convex variant of this QP shall be explicitly differentiated when addressed later. Consider the following
functions u, v:
u(p) =

X X θij (xi , xj )

p2i (xi ) + p2j (xj )
2
ij x x

(4)

X X θij (xi , xj )
2
pi (xi ) + pj (xj )
2
ij x x

(5)

i

v(p) =

i

j

j

The above functions are convex because the quadratic
functions f (z) = z 2 and f (y, z) = (y + z)2 are convex,
and the nonnegative weighted sum of convex functions
is also convex [Boyd and Vandenberghe, 2004, Ch. 3].
It can be easily verified that the QP in Eq. (1) can be
written as minp {u(p) − v(p)} with normalization and
nonnegativity constraints defining the constraint set
Ω. Intuitively, we used the simple identity −2xy =
(x2 + y 2 ) − (x + y)2 . We also negated the objective
function to convert maximization to minimization. For
simplicity, we denote the gradient ∂v/∂p(xi ) by ∇xi v.
X X
X X
∇xi v = pi (xi )
θij (xi , xj )+
θij (xi , xj )pj (xj )
j∈Ne(i) xj

j∈Ne(i) xj

The first part of the above equation involves a local
computation associated with an MRF node and the
second part defines the messages δj from neighbors
j ∈ N e(i) of node i. It can be made explicit as follows:
X X
X
θ̂(xi )=
θij (xi , xj ); δj (xi ) =
θij (xi , xj )pj (xj )
j∈Ne(i) xj

xj

X

∇xi v= pi (xi )θ̂(xi ) +

δj (xi )

(6)

j∈N e(i)

CCCP iterations: Each iteration of CCCP involves
solving the convex program of Eq. (3). First we write
the Lagrangian function involving only the normalization constraints, later we address the nonnegativity
inequality constraints. ∇l v denotes the gradient from
the previous iteration l.
X X θij (xi , xj ) 
p2i (xi ) + p2j (xj )
L(p, λ) =
2
ij xi xj
XX
X X
−
pi (xi )∇lxi v +
λi (
pi (xi ) − 1) (7)
i

xi

xi

i

Solving for the first order optimality conditions
∇p L(x? , λ? ) and ∇λ L(x? , λ? ), we get the solution:
pl+1
i (xi ) =

∇lxi v

− λi

(8)

θ̂(xi )

λi = P

1

1
xi θ̂(xi )

X
xi

∇lxi v
θ̂(xi )


−1

(9)

Nonnegativity constraints: Nonnegativity constraints in the MAP QP are inequality constraints
which are harder to handle as the Karush-KuhnTucker (KKT) conditions include the nonlinear complementary slackness condition µ?j p?j (xj ) = 0 [Boyd
and Vandenberghe, 2004]. We can use interior-point
methods, but they lose the efficiency of graph based
message passing. Fortunately, we show that for MAP
QP, the KKT conditions are easily satisfied by incorporating an inner-loop in the CCCP iterations.
Alg. 1 shows the complete message-passing procedure
to solve MAP QPs. Each outer loop corresponds to
solving the CCCP iteration of Eq. (3) and is run until the desired number of iterations is reached. The
messages δ are used for computing the gradient ∇v as
in Eq. (6). The inner loop corresponds to satisfying
the KKT conditions including the nonnegativity inequality constraints. Intuitively, the strategy to handle
inequality constraints is as highlighted in [Bertsekas,
1999, Sec. 3.3.1] – considering all possible combinations of inequality constraints being active (pi (xi ) = 0)
or inactive (pi (xi ) > 0) and solving the resulting KKT
conditions, which is easier as they become linear equations. If the resulting solution satisfies the KKT conditions of the original problem, then we have a valid solution for the original optimization problem. Of course,

1: Graph-based message passing for MAP estimation
input: Graph G = (V, E) and potentials θij per edge
//outer loop starts
repeat
foreach node i ∈
PV do
δi→j (xj ) ← xi pi (xi )θij (xi , xj )
Send message δi→j to each neighbor j ∈ Ne(i)
foreach node i ∈ V do
zeros ← φ
//inner loop starts
repeat
Set pi (xi ) ← 0 ∀xi ∈ zeros
Calculate pi (xi ) using Eq. (8) ∀ xi ∈
/ zeros
zeros ← zeros ∪ {xi : pi (xi ) < 0}
until all beliefs pi (xi ) ≥ 0
until stopping criterion is satisfied
return: The decoded complete integral assignment

this is highly inefficient for the general case. But fortunately for the MAP QP, we show that the inner loop of
Alg. 1 recovers the correct solution and the Lagrange
multipliers are computed efficiently for the convex program of Eq. (3). We describe it below.
The inner loop includes local computation to each
MRF node i and does not require message passing.
Intuitively, the set zeros tracks all the settings x0i of
the variable xi for which pi (x0i ) was negative in any
previous inner loop iteration. It then clamps all such
beliefs to 0 for all future iterations. Then the beliefs
for the rest of the settings of xi are computed using
Eq. (8). The new Lagrange multiplier λi (which corre?
?
sponds to the condition
P ∇λ L(x , λ ) = 0) is calculated
using the equation xi \x0 pi (xi ) = 1.
i

Lemma 1. The inner loop of Alg. 1 terminates with
worst case complexity O(k 2 ), and yields a feasible point
for the convex program of Eq. (3).
Proof. The size of the set zeros increases with each
iteration, therefore the inner loop must terminate as
each variable’s domain is finite. With the domain size
of a variable being k, the inner loop can run for at
most k iterations. Computing new beliefs within each
inner loop iteration also requires O(k) time. Thus the
worst case total complexity is O(k 2 ).
The inner loop can terminate only in two ways – before iteration k or at iteration k. If it terminates before
iteration k, then it implies that all the beliefs pi (xi )
must be nonnegative. The normalization constraints
are always enforced by the Lagrange multipliers λi ’s.
If it terminates during iteration k, then it implies that
k − 1 settings of the variable xi are clamped to zero as
the size of the set zeros will be exactly k − 1. The size
cannot be k because that would make all the beliefs
equal to zero, making it impossible to satisfy the normalization constraint; λi will not allow this. The size

cannot be smaller than k − 1 because the set zeros
grows by at least one element during each previous iteration. Therefore the only solution during iteration
k is to set the single remaining setting of the variable
xi to 1 to satisfy the normalization and nonnegativity
constraints simultaneously. Therefore the inner loop
always yields a feasible point upon termination.
Empirically, we observed that even for large protein
design problems with k = 150, the number of required
inner loop iterations is below 20 – far below the worst
case complexity. For a fixed outer loop l, let the inner
loop iterations be indexed by r.
Lemma 2. The Lagrange multiplier corresponding to
the normalization constraint for a MRF variable xi
always increases with each inner loop iteration.
Proof. Each inner loop iteration r computes
P a new Lagrange multiplier λri for the constraint i pi (xi ) = 1
using Eq. (8). We show that λr+1
> λri . For the inner
i
loop iteration r, some of the computed beliefs must be
negative, otherwise the inner loop must have terminated. Let x0i denote those settings of variable xi for
which pi (x0i ) < 0 in iteration r. From the normalization constraint for iteration r, we get:
X ∇lx v − λri
i
xi

θ̂(xi )

=1

(10)

We used the explicit representation of pi (xi ) from
Eq. (8). Since pi (x0i ) are negative, we get:
X ∇lx v − λri
i
xi \x0i

θ̂(xi )

>1

(11)

The belief for all such x0i will become zero for the next
inner loop iteration r + 1. From the normalization
constraint for iteration r + 1, we get:
X ∇lx v − λr+1
i
i
xi \x0i

θ̂(xi )

=1

(12)

We used a slight simplification in the above equations
as we ignored the effect of previous iterations, before
iteration r. However, it will not change the conclusion as all the beliefs that were clamped to zero earlier
(before iteration r) shall remain so for all future iterations. Note that ∇xi and θ̂ do not depend on the inner
loop iterations. Subtracting Eq. 12 from Eq. 11:
X 1
(λr+1
− λri )
>0
(13)
i
θ̂(xi )
xi \x0
i

Since we assumed that all potential functions θij are
nonnegative, we must have (λr+1
− λri ) > 0. Hence
i
r+1
r
λi > λi and the lemma is proved.

Theorem 3. The inner loop of Alg. 1 correctly recovers all the Lagrange multipliers for the equality and inequality constraints for the convex program of Eq. (3),
thus solving it exactly.
Proof. Lemma 1 shows that the inner loop provides a
feasible point of the convex program. We now show
that this point also satisfies the KKT conditions, thus
is the optimal solution. The KKT conditions for the
normalization constraints are always satisfied during
the belief updates (see. Eq. (8)). The main task is to
show that for the inequality constraint −pi (xi ) ≤ 0,
the KKT conditions hold. That is, if pi (xi ) = 0, then
the Lagrange multiplier µi (xi ) ≥ 0, and if −pi (xi ) < 0,
then µi (xi ) = 0.
By using the KKT condition ∇pi (xi ) L(p? , λ? , µ? ) = 0,
we get:
X X
θij (xi , xj )pi (xi )−∇lxi v + λi − µi (xi ) = 0
j∈N e(i) xj

(14)
The main focus of the proof is on the beliefs for elements in the set zeros. Let us focus on the end of an
inner loop iteration r, when a new element x0i is added
to zeros because its computed belief pi (x0i ) < 0. Using Eq. (8), we know that pi (x0i ) =
pi (x0i )

∇lx0 v−λri
i

θ̂(x0i )

. Because

< 0 we get:
λri > ∇lx0i v

(15)

For all future iterations of the inner loop, pi (x0i ) will be
set to zero. Therefore the KKT condition for iteration
r + 1 mandates that µr+1
(x0i ) ≥ 0. Setting pi (x0i ) = 0
i
in Eq. (14), we get:
µr+1
(x0i ) = λr+1
− ∇lx0i v
i
i

(16)

We know from Lemma 2 that λr+1
> λri . Combining
i
r+1 0
this fact with Eq. (15), we get µi (xi ) > 0, thereby
satisfying the KKT condition. Note that the only component depending on the inner loop in the above condition is λri ; ∇x0i is fixed during each inner loop. Furthermore, for all future inner loop iterations, the KKT
conditions for all elements x0i ’s in the set zeros will be
met due to the increasing nature of the multiplier λi .
Therefore, when the inner loop terminates, we shall
have correct Lagrange multipliers µ satisfying µ ≥ 0
for all the elements of the set zeros. For the rest of
the elements, the multiplier µ = 0, satisfying all the
KKT conditions. As the first order KKT conditions
are both necessary and sufficient for optimality in convex programs [Bertsekas, 1999, Sec. 3.3.4], the inner
loop solves exactly the convex program in Eq. (3).

2.3

Solving Convex MAP QP Using CCCP

Because the previous QP formulation of MAP is
nonconvex, global optimization is hard. To remedy
this, Ravikumar and Lafferty [2006] developed a convex QP relaxation for MAP, which performed well
on their benchmarks. Recently, Kumar et al. [2009]
showed that the convex QP relaxation is also equivalent to the second order cone programming (SOCP)
relaxation. Ravikumar and Lafferty [2006] proposed
to solve such QP using standard QP solvers. We show
using CCCP that this QP relaxation can be solved efficiently using graph-based message passing, and the
resulting algorithm converges to the global optimum of
the relaxed QP. Experimentally, we found the resulting
message-passing algorithm to be highly efficient even
for large graphs, outperforming CPLEX by more than
an order-of-magnitude. The relaxed QP is described
as follows:
XX
XX
pi (xi )di (xi )+
pi (xi )pj (xj )θij (xi , xj )
max
p

ij xi ,xj

xi

i

−

XX
i

p2i (xi )di (xi )

(17)

xi

The relaxation is based on adding a diagonal term,
di (xi ), for each variable xi . Note that under the integrality assumption pi (xi ) = p2i (xi ), thus the first and
last terms cancel out, resulting in the original MAP
QP. The diagonal term is given by:
di (xi ) =

X X |θij (xi , xj )|
j∈N e(i) xj

2

Consider the convex function u(p) represented as:
X X θij (xi , xj )
 X 2
pi (xi )di (xi )
p2i (xi ) + p2j (xj ) +
2
i,x
ij x x
i

j

i

and the convex function v(p) represented as:
X X θij (xi , xj )
ij xi xj

2

2 X
pi (xi )+pj (xj ) +
pi (xi )di (xi )
i,xi

The above two functions are the same as the original
QP formulation, except for the added diagonal terms
di (xi ). It can be easily verified that the relaxed QP
objective can be written as minp {u(p) − v(p)} subject
to normalization and nonnegativity constraints. Note
that the maximization of the relaxed QP is converted
to minimization by negating the objective. The gradient required by CCCP is given by:
X X
∇xi v = pi (xi )θ̂(xi ) +
θij (xi , xj )pj (xj ) + di (xi )
j∈N e(i) xj

Notice the close similarity with the MAP QP case
in Eq. (6). The only additional term is di (xi ), which

needs to be computed only once before message passing begins. The messages for the relaxed QP case are
exactly the same as the δ messages for the MAP QP.
The Lagrangian corresponding to the convex program
of Eq. (3) is similar to thePMAP QP case (see Eq. (7))
with an additional term i,xi p2i (xi )di (xi ). The constraint set Ω includes the normalization and nonnegativity constraints as for the MAP QP case.
Solving for the optimality conditions ∇p L(p? , λ? ) and
∇λ L(p? , λ? ), we get the new beliefs as follows:
pil+1 (xi ) =

∇lxi v − λi
2di (xi ) + θ̂(xi )

(18)

The Lagrange multiplier λi for the normalization
constraint
can be calculated by using the equation
P
xi pi (xi ) = 1. The only difference from the corresponding Eq. (8) for the MAP QP is the additional
term di (xi ) in the denominator.
Thanks to these strong similarities, we can show that
Alg. 1 also works for the convex MAP QP with minor modifications. First, we calculate the diagonal
terms di (xi ) once for each variable xi of the MRF.
The message-passing procedure for each outer loop iteration remains the same. The second difference lies
in the inner loop that enforces the nonnegativity constraints. The inner loop now uses Eq. (18) instead
of Eq. (8) to estimate new beliefs pi (xi ). The proof
is omitted being very similar to the MAP QP case.
Theorem 4. The CCCP message-passing algorithm
converges to a global optimum of the convex MAP QP.
The result is based on the fact that CCCP converges to
a stationary point of the given constrained optimization problem that satisfies the KKT conditions [Sriperumbudur and Lanckriet, 2009]. Because the KKT
conditions are both necessary and sufficient for convex
optimization problems with linear constraints [Bertsekas, 1999], the result follows. We also highlight that
a global optimum of the convex QP may not solve the
MAP problem exactly, as the convex QP is a variational approximation to MAP that may not be tight.
Nonetheless, it has shown to perform well in practice [Ravikumar and Lafferty, 2006].
2.4

GP Based Reformulation of MAP QP

We now present another formulation of the MAP QP
problem based on geometric programming (GP). A GP
is a type of mathematical program characterized by
objectives and constraints that have a special form.
For details, we refer to [Boyd et al., 2007]. While the
QP formulation of MAP in Eq. (1) is not exactly a GP,
it bears a close resemblance. This allows us to transfer
some ideas from GP, which we shall describe. We start
with some basic concepts of GP.

Definition 1. Let x1 , . . . , xn denote real positive variables. A real valued monomial function has the form
f (x) = cxa1 1 xa2 2 · · · xann , where c > 0 and ai ∈ <. A
posynomial function
is a sum of one or more monoPK
mials: f (x) = k=1 ck x1a1k xa2 2k · · · xannk
In a GP, the objective function and all inequality
constraints are posynomial functions. The MAP QP
(see Eq. (1)) satisfies this requirement – the potential function θij corresponds to ck and is positive (the
θij = 0 case can be excluded for convenience); node
marginals pi (xi ) are real positive variables. Since we
already assumed marginals pi to be positive, nonnegativity inequality constraints are not required. In a
GP, the equality constraints can only be monomial
functions. This is not satisfied in MAP QP as normalization constraints are posynomials. Nonetheless,
we proceed as in GP by converting the original problem using certain optimality-preserving transformations. The first change is to let pi (xi ) = eyi (xi ) , where
yi (xi ) is an unconstrained variable. This is allowed as
all marginals must be positive. The second change is
to take the log of the objective function; because log is
a monotonic function, this will not change the optimal
solution. The reformulated MAP QP is shown below:
X X


min: − log
exp yi (xi )+yj (xj )+log θij (xi , xj )
ij xi ,xj

subject to:

X

eyi (xi ) = 1 ∀i ∈ V

xi

This nonlinear program has the same optimal solutions
as the original MAP QP. As log-sum-exp is convex, the
objective function of the above problem is concave.
Note that we are minimizing a concave function that
can have multiple local optima. We again solve it using
CCCP. Consider the function u(y) = 0 and v(y) as
the objective of the above program, but without the
negative sign. The gradient required by CCCP is:

P
P
j∈N e(i) xjθij (xi , xj ) exp yi (xi )+yj (xj )
l
 (19)
∇yi v = P P
ij
xi ,xj θij (xi , xj ) exp yi (xi )+yj (xj )
The Lagrangian corresponding to Eq. (3) with constraint set including only normalization constraints is:
X
X X
L(y, λ) = −
yi (xi )∇lyi v +
λi (
eyi (xi ) − 1)
i,xi

i

xi

Using the first order optimality condition, we get:
 ∇lyi v
exp yi (xi ) =
λi

(20)

We note that the denominator of Eq. (19) is a constant
for each yi (xi ). Therefore we represent it using cl . Re-

substituting pi (xi ) = eyi (xi ) and ∇lyi v in Eq. (20):
P
P
j∈N e(i)
xj θij (xi , xj )pi (xi )pj (xj )
?
pi (xi ) =
cl λi
where p?i (xi ) is the new parameter for the current iteration, and parameters without asterisk (on the R.H.S.)
are from the previous iteration. Since cl λi is also a constant, we can replace them by a normalization constant
Ci to get the final update equation:
P
P
j∈N e(i)
xj θij (xi , xj )pj (xj )
?
pi (xi ) = pi (xi )
Ci
The message-passing process for this version of CCCP
is exactly the same as that for MAP QP and convex
QP. This version does not require an inner loop as
all the node marginals remain positive using such updates. This update process is also identical to the recently developed message-passing algorithm for MAP
estimation that is based on expectation-maximization
(EM) rather than CCCP [Kumar and Zilberstein,
2010]. However, CCCP provides a more flexible framework in that it handled the nonconvex and convex QP
in a similar way as shown earlier. Furthermore, the
CCCP framework allows for additional constraints to
be added to the convex QP to make it tighter [Sriperumbudur and Lanckriet, 2009].
In sum, we have shown that the concave-convex procedure provides a unifying framework for the various quadratic programming formulations of the MAP
problem. Each iteration of CCCP can be easily implemented using graph-based message passing. Interestingly, the messages exchanged for all the QP formulations we discussed remain exactly the same; the differences lie in how new node marginals are computed
using such messages.

3

EXPERIMENTS

We now present an empirical evaluation of the CCCP
algorithms. We first report results on synthetic
graphs generated using the Ising model from statistical physics [Baxter, 1982]. We compare max-product
(MP) and the CCCP message-passing algorithm for
the QP formulation of MAP. We generated 2D nearest
neighbor grid graphs for a number of grid sizes (ranging between 10 × 10 to 50 × 50) and varying values
of the coupling parameter. All the variables were binary. The node potentials were generated by sampling
from the uniform distribution U[−0.05, 0.05]. The coupling strength, dcoup , for each edge was sampled from
U[−β, β] following the mixed Ising model. The binary
edge potential θij was defined as follows:
(
dcoup xi = xj
θij (xi , xj ) =
−dcoup xi 6= xj

400

400

400
300

400
300

400
1400
1400
1400
3000
3000
3000
400
1400
1200
1400 1200
1400 1200
300
3000 3000
3000
1200
1200 1000
1200 1000
1000
2000
2000
2000
300
300
300
800
800
800
1000 1000
1000
200
200
200
2000 2000
2000
800
800
800
600
600
600
1000
1000
1000
200
200
200
CCCP
CCCP
600
400
400
400
600
600 CCCP
100
100
100
MP
MP
MP
1000 1000
1000
CCCP
CCCP
CCCP
400
400
400 MP 200
200
200
0
0
0
100
100
100
MP
MP
1
21
321
432
543
541
521
321
432
543
541
521
321
432
543
54
5
200
0
200
0
200
0
(a)
(a)
Ising423(a)
! 100
Ising
(b)
Ising423(b)
! 400
Ising
Ising423!
(c)900
Ising534! 900 45
12 100
5(b)
12 400
5(c)
12900
5
1
21 Ising 3!
534! 100 45 1
21 Ising 3!
534! 400 45 1
21 Ising 3!(c)
(a)
(b)
(c)
Ising ! 100
Ising ! 400
Ising ! 900
Ising(a)
! 100
Ising(b)
! 400
(c)900
Ising(c)
! 900
(a) Ising (a)
! 100
(b) Ising (b)
! 400
(c) Ising !
6000
6000
6000
100
100
100
100
6000 6000
100
6000
100
7500
7500
7500
95
95
95
4000
4000
4000
7500 7500
7500
95
95
95
4000 4000
4000
5000
5000
5000
90
90
90
90
2000
2000
2000
5000 5000
90
5000
90
85
85
85
2500
2500
2500
2000 2000
2000
CCCP
CCCP
CCCP
85
85
85
2500 2500
2500
0
0
0
80
80
80
CCCP
CCCP
CCCP
1
21
321
432
543
541
521
321
432
543
540
205 0 4020 0604020
806040
1008060 10080 100
0
80
0
80
0
80
(d)
(d)
Ising 42(d)
!3 1600
Ising534! 160045 1
(e)
Ising 42(e)
!3 2500
Ising534! 250045 0(f) Protein
Protein
(f)
instances
design
Protein
instances
design
5(e)
5 0(f)design
20 8060
40
60instances
80 100
200 60
40
100
1
21Ising !
3121600
21Ising !
3122500
20
40
10080
Ising ! 1600
Ising ! 2500
(f)instances
Protein
design instances
Ising(d)
! 1600
Ising(e)
! 2500
(f)design
Protein
design instances
(d) Ising (d)
! 1600
(e) Ising (e)
! 2500
(f) Protein

(d)

(e)

(f)

Figure 1: (a) – (e) show quality comparison between max-product (MP) and CCCP for Ising graphs with varying number
of nodes (100 – 2500). The x-axis denotes the coupling parameter β, y-axis shows solution quality. (f) shows the solution
quality CCCP achieves as a percentage of the optimal value (y-axis) for different protein design instances (x-axis).

For every grid size and each setting of the coupling
strength parameter β, we generated 10 instances by
sampling dcoup per edge. For each instance, we considered the best solution quality of 10 runs for both
max-product and CCCP. We then report the average
quality of the 10 instances achieved for each parameter
β. Both max-product and CCCP were implemented
in JAVA and ran on a 2.4GHz CPU. Max-product was
allowed 1000 iterations and often did not converge,
whereas CCCP converged within 500 iterations.
Fig. 1(a–e) show solution quality comparisons between
MP and CCCP. For 10 × 10 graphs (Fig. 1(a)), both
CCCP and MP achieve similar solution quality. The
gain in quality provided by CCCP increases with the
size of the grid graph. For 20 × 20 grids, the average gain in solution quality, ((QCCCP − QM P )/QM P ),
for each coupling strength parameter β is over 20%.
For 30 × 30 (Fig. 1(c)) grids, the gain is above 30%
for each parameter β; for 40 × 40 grids it is 36% and
for 50 × 50 grids it is 43%. Overall, CCCP provides
much better performance than max-product over these
Ising graphs. And unlike max-product, CCCP monotonically increases solution quality and is guaranteed
to converge. A detailed performance evaluation of the
convex QP is provided [Ravikumar and Lafferty, 2006].
As such Ising graphs have relatively small QP representation, the CCCP message passing method and
CPLEX had similar runtime for the convex QP.
We also experimented on the protein design benchmark (total of 97 instances) [Yanover et al., 2006].
In these problems, the task is to find a sequence of
amino acids that is as stable as possible for a given
backbone structure of protein. This problem can be
modeled using a pairwise Markov random field. These
problems are particularly hard and dense with up to
170 variables, each with a large domain size of up

to 150 values. Fig. 1(f) shows the % of the optimal
value CCCP achieves against the best upper bound
provided by the LP based approach MPLP [Sontag
et al., 2008]. MPLP has been shown to be very effective in solving exactly the MAP problem for several real-world problems. However for these protein
design problems, due to the large variable domains,
its reported mean running time is 9.7 hours [Sontag
et al., 2008]. As Fig. 1(f) shows, CCCP achieves nearoptimal solutions, on average within 97.7% of the optimal value. A significant advantage of CCCP is its
speed: it converges within 1200 iterations for all these
problems and requires ≈ 403 seconds for the largest
instance, much faster than MPLP. The mean running
time of CCCP was ≈170 seconds for this dataset. Thus
CCCP can prove to be quite effective when fast, approximate solutions are desired. The main reason for
this speedup is that CCCP’s messages are easier to
compute than MPLP’s as also highlighted in [Kumar
and Zilberstein, 2010]. Compared to the EM approach
of [Kumar and Zilberstein, 2010], CCCP provides better solution quality: EM achieved 95% of the optimal
value on average, while CCCP achieves 97.7%. The
overhead of the inner loop in CCCP is small against
EM which takes ≈ 352 seconds for the largest instance,
while CCCP takes ≈ 403 seconds.
We also tested CCCP on the protein prediction
dataset [Yanover et al., 2006]. The problems in this
dataset are much smaller than those in the protein
design dataset, and both max-product and MPLP
achieve good solution quality. CCCP’s performance
was worse in this case, partly due to the local optima
present in the nonconvex QP formulation of MAP. The
convex QP formulation was not tight in this case.
Fig. 2(a) shows runtime comparison of CCCP against
CPLEX for the convex QP for the 25 largest protein

15%

0.025

relaxation, CCCP provided more than an order-ofmagnitude speedup over the state-of-the-art QP solver
CPLEX. These results offer a powerful new way for
solving efficiently large MAP estimation problems.

0.02
10%
0.015

Acknowledgments

0.01
5%

Support for this work was provided in part by the NSF
Grant IIS-0812149 and by the AFOSR Grant FA955008-1-0181.

0.005

0%

0

10
20
(a) Time comparison

0

0

10
20
(b) Quality comparison

Figure 2: (a) Time comparison of CCCP for convex
QP against CPLEX for the largest 25 protein design instances (x-axis). The y-axis denotes TCCCP /TCP LEX as
a percentage. (b) denotes the signed quality difference
QCCCP − QCP LEX , a higher value is better.

design problems w.r.t. the number of graph edges.
After trying different QP solver options available in
CPLEX, we chose the barrier method which provided
the best performance. As CPLEX was quite slow, we
let CPLEX use 8 CPU cores with 8GB RAM, while
CCCP only used a single CPU. As this figure shows,
CCCP is more than an order-of-magnitude faster than
CPLEX even when it uses a single core. The longest
CPLEX took was 3504 seconds, whereas CCCP only
took 99 seconds for the same instance. The mean running time of CPLEX was 1914 seconds; for CCCP,
it was 96 seconds. Surprisingly, CCCP converges in
only 15 iterations to the optimal solution for all 25
problems. Fig. 2(b) shows the signed quality difference
between CCCP and CPLEX for the convex QP objective. CPLEX provides the optimal solution within
some non-zero  (we used the default setting). This
figure shows that even within 15 iterations, CCCP
achieved a slightly better solution. The decoded solution quality provided by the convex QP was decent,
within 80% of the optimal value, but not as high as
the CCCP method for the nonconvex QP.

4

CONCLUSION

We presented new message-passing algorithms for various quadratic programming formulations of the MAP
problem. We showed that the concave-convex procedure provides a unifying framework for different QP
formulations of the MAP problem represented as a difference of convex functions. The resulting algorithms
were shown to be convergent – to a local optimum
for the nonconvex QP and to the global optimum of
the convex QP. Empirically, the CCCP algorithm was
shown to work well on Ising graphs and real-world
protein design problems. The CCCP approach provided much better solution quality than max-product
for Ising graphs and converged significantly faster than
max-product LP for protein design problems, while
providing near optimal solutions. For the convex QP


