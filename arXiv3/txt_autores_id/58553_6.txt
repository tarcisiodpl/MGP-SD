
The Bayesian Logic (BLOG) language was recently developed for defining first-order probability models over worlds with unknown numbers of objects. It handles important problems
in AI, including data association and population
estimation. This paper extends BLOG by adopting generative processes over function spaces —
known as nonparametrics in the Bayesian literature. We introduce syntax for reasoning about
arbitrary collections of objects, and their properties, in an intuitive manner. By exploiting
exchangeability, distributions over unknown objects and their attributes are cast as Dirichlet processes, which resolve difficulties in model selection and inference caused by varying numbers of
objects. We demonstrate these concepts with application to citation matching.

1

Introduction

Probabilistic first-order logic has played a prominent role
in recent attempts to develop more expressive models in
artificial intelligence [3, 4, 6, 8, 15, 16, 17]. Among these,
the Bayesian logic (BLOG) approach [11] stands out for
its ability to handle unknown numbers of objects and data
association in a coherent fashion, and it does not assume
unique names and domain closure.
A BLOG model specifies a probability distribution over
possible worlds of a typed, first-order language. That is,
it defines a probabilistic model over objects and their attributes. A model structure corresponds to a possible world,
which is obtained by extending each object type and interpreting each function symbol. Objects can either be “guaranteed”, meaning the extension of a type is fixed, or they
can be generated from a distribution. For example, in the
aircraft tracking domain [11] the times and radar blips are
known, and the number of unknown aircraft may vary in
possible worlds. BLOG as a case study provides a strong
argument for Bayesian hierarchical methodology as a basis
for probabilistic first-order logic.

BLOG specifies a prior over the number of objects. In
many domains, however, it is unreasonable for the user to
suggest such a proper, data-independent prior. An investigation of this issue was the seed that grew into our proposal for Nonparametric Bayesian Logic, or NP-BLOG,
a language which extends the original framework developed in [11]. NP-BLOG is distinguished by its ability to
handle object attributes that are generated by unbounded
sets of objects. It also permits arbitrary collections of attributes drawn from unbounded sets. We extend the BLOG
language by adopting Bayesian nonparametrics, which are
probabilistic models with infinitely many parameters [1].
The statistics community has long stressed the need for
models that avoid commiting to restrictive assumptions regarding the underlying population. Nonparametric models
specify distributions over function spaces — a natural fit
with Bayesian methods, since they can be incorporated as
prior information and then implemented at the inference
level via Bayes’ theorem. In this paper, we recognize that
Bayesian nonparametric methods have an important role to
play in first-order probabilistic inference as well. We start
with a simple example that introduces some concepts necessary to understanding the main points of the paper.
Consider a variation of the problem explored in [11]. You
have just gone to the candy store and have bought a box
of Smarties (or M&Ms), and you would like to discover
how many colours there are (while avoiding the temptation
to eat them!). Even though there is an infinite number of
colours to choose from, the candies are coloured from a finite set. Due to the manufacturing process, Smarties may
be slightly discoloured. You would like to discover the unknown (true) set of colours by randomly picking Smarties
from the box and observing their colours. After a certain
number of draws, you would like to answer questions such
as: How many different colours are in the box? Do two
Smarties have the same colour? What is the probability
that the first candy you select from a new box is a colour
you have never seen before?
The graphical representation of the BLOG model is shown
in Fig. 1a. The number of Smarties of different colours,
n(Smartie), is chosen from a Poisson distribution with

it is unreasonable to expect a domain expert to implement
nonparametrics considering the degree of effort required
to grasp these abstract notions. We show that Bayesian
nonparametrics lead to sophisticated representations that
can be easier to implement than their parametric counterparts. We formulate a language that allows one to specify
nonparametric models in an intuitive manner, while hiding
complicated implementation details from the user. Sec. 3
formalizes our proposed language extension as a set of
rules that map code to a nonparametric generative process.
We emphasize that NP-BLOG is an extension to the BLOG
language, so it retains all the functionality specified in [11].

Figure 1: (a) The BLOG and (b) NP-BLOG graphical models for counting Smarties. The latter implements a Dirichlet
process mixture. The shaded nodes are observations.
mean γSmartie . A colour for each Smartie s is drawn
from the distribution HColourDist . Then, for every draw d,
zSmartieDrawn [d] is drawn uniformly from the set of Smarties {1, . . . , n(Smartie)}. Finally, we sample the observed,
noisy colour of each draw conditioned on zSmartieDrawn [d]
and the true colours of the Smarties.
The NP-BLOG model for the same setting is shown in
Fig. 1b. The true colours of an infinite sequence of Smarties s are sampled from HColourDist . πSmartie is a distribution over the choice of coloured Smarties, and is sampled from a uniform Dirichlet distribution with parameter
αSmartie . Once the Smarties and their colours are generated, the true Smartie for draw d, represented by the indicator zSmartieDrawn [d] = s, is sampled from the distribution
of Smarties πSmartie . The last step is to sample the observed
colour, which remains the same as in the BLOG model.
One advantage of the NP-BLOG model is that it determines
a posterior over the number of Smarties colours without
having to specify a prior over n(Smartie). This is important since this prior is difficult to specify in many domains.
A more significant advantage is that NP-BLOG explicitly
models a distribution over the collection of Smarties. This
is not an improvement in expressiveness — one can always
reverse engineer a parametric model given a target nonparametric model in a specific setting. Rather, nonparametrics
facilitate the resolution of queries on unbounded sets, such
as the colours of Smarties. This plays a key role in making inference tractable in sophisticated models with object
properties that are themselves unbounded collections of objects. This is the case with the citation matching model in
Sec. 3.1, in which publications have collections of authors.
The skeptic might still say, despite these advantages, that

We focus on an important class of nonparametric methods,
the Dirichlet process (DP), because it handles distributions
over unbounded sets of objects as long as the objects themselves are infinitely exchangeable, a notion formalized in
Sec. 3.4. The nonparametric nature of DPs makes them
suitable for solving model selection problems that arise
in the face of identity uncertainty and unknown numbers
of objects. Understanding the Dirichlet process is integral to understanding NP-BLOG, so we devote a section
to it. Sec. 3.5 shows how DPs can characterize collections of objects. Models based on DPs have been shown
to be capable of solving a variety of difficult tasks, such
as topic-document retrieval [2, 21]. Provided the necessary
expert knowledge, our approach can attack these applications, and others. We conduct a citation matching experiment in Sec. 4, demonstrating accurate and efficient probabilistic inference in a real-world problem.

2

Dirichlet processes

A Dirichlet process G | α, H ∼ DP (α, H), with parameter
α and base measure H, is the unique probability measure
defined G on the space of all probability measures (Φ, B),
where Φ is the sample space, satisfying
(G(β1 ), ..., G(βK)) ∼ Dirichlet(αH(β1 ), ..., αH(βK)) (1)
for every measurable partition β1 , . . . , βK of Φ. The base
measure H defines the expectation of each partition, and α
is a precision parameter. One can consider the DP as a generalization of the Dirichlet distribution to infinite spaces.
In Sec. 3.4, we formalize exchangeability of unknown objects. In order to explain the connection between exchangeability and the DP, it is instructive to construct DPs with
the Pólya urn scheme [5]. Consider an urn with balls of
K possible colours, in which the probability of the first
ball being colour k is given by Hk . We draw a ball
from the urn, observe its colour φ1 , then return it to the
urn along with another ball of the same colour. We then
make another draw, observing its colour with probability
p(φ2 = k|φ1 ) = (αHk +δ(α1 = k))/(α+1). After N observations, the colour of the next ball is distributed as
αHk
+
P (φN +1 = k|φ1:N ) =
α+N

PN

δ(φi = k)
.
α+N

i=1

(2)

The marginal P (φ1:N ) of this process, obtained by applying the chain rule to successive predictive distributions, can
be shown to satisfy the infinite mixture representation
!
Z
K
PN
Y
δ(φ
=k)
i
P (φ1:N ) =
DPα,H (dπ), (3)
πk i=1
M(Φ) k=1

where the πk are multinomial success rates of each colour
k. This result, a manifestation of de Finetti’s theorem, establishes the existence and uniqueness of the DP prior for
the Pólya urn scheme [5]. In the Pólya urn setting, observations φi are infinitely exchangeable and independently
distributed given the measure G. Thus, what we have established here in a somewhat cursory fashion is the appropriateness of the DP for the case when the observations φi
are infinitely exchangeable.
Analogously, if the urn allows for infinitely many colours,
then for any measurable interval β of Φ we have
p(φN +1 ∈ β|φ1:N ) =

N
1 X
αH(β)
+
δ(φi ∈ β).
α+N
α + N i=1

The first term in this expansion corresponds to prior knowledge and the second term corresponds to the empirical distribution. Larger values of α indicate more confidence
in the prior H. Note that, as N increases, most of the
colours will be repeated. Asymptotically, one ends up sampling colours from a possibly large but finite set of colours,
achieving a clustering effect. Nonetheless, there is always
some probability of generating a new cluster.
DPs are essential building blocks in our formulation of nonparametric first-order logic. In the literature, these blocks
are used to construct more flexible models, such as DP mixtures and hierarchical or nested DPs [2, 21]. Since observations are provably discrete, DP mixtures add an additional
layer xi ∼ P (xi |φi ) in order to model continuous draws xi
from discrete mixture components φi .
In the Pólya urn scheme, G is integrated out and the φi ’s are
sampled directly from H. Most algorithms for sampling
DPs are based on this scheme [2, 13, 21]. In the hierarchies
constructed by our language, however, we rely on an explicit representation of the measure G since it is not clear
we can always integrate it out, even when the measures
are conjugate. This compels us to use the stick-breaking
construction [19], which establishes that i.i.d. sequences
wk ∼ Beta(1, α) and φk ∼ H can be used
P∞to construct
the equivalent empirical distribution G = k=1 πk δ(φk ),
Qk−1
where the stick-breaking weights πk = wk j=1 (1 − wj )
and can be shown to sum to unity. We abbreviate the sampling of the weights as πk ∼ Stick(α). This shows that G
is an infinite sum of discrete values. The DP mixture due
to the stick-breaking construction is
φi | H ∼ H
zi | π ∼ π

π | α ∼ Stick(α)
xi | φi , zi ∼ p(xi |φzi ),

(4)

where zi = k indicates that sample xi belongs to component k. The Smarties model (Fig. 1b) is in fact an example
of a DP mixture, where the unbounded set of colours is Φ.
By grounding on the support of the observations, the true
number of colours K is finite. At the same time, the DP
mixture is open about seeing new colours as new Smarties
are drawn. In NP-BLOG, the unknown objects are the mixture components.
NP-BLOG semantics (Sec. 3) define arbitrary hierarchies
of Dirichlet process mixtures. By the stick-breaking construction (4), every random variable xi has a countable set
of ancestors (the unknown objects), hence DP mixtures preserve the well-definedness of BLOG models.
To infer the hidden variables of our models, we employ
the efficient blocked Gibbs sampling algorithm developed
in [7] as one of the steps in the overall Gibbs sampler. One
complication in inference stems from the fact that a product
of Dirichlets is difficult to simulate. Teh et al. [21] provide
a solution using an auxiliary variable sampling scheme.

3

Syntax and semantics

This section formalizes the NP-BLOG language by specifying a procedure that takes a set of statements LΨ in the
language and returns a model Ψ. A model comprises a set
of types, function symbols, and a distribution over possible
worlds ω ∈ ΩΨ . We underline that our language retains all
the functionality of BLOG. Unknown objects must be infinitely exchangeable, but this trivially the case in BLOG.
Sec. 3.4 elaborates on this.
We illustrate the concepts introduced in this section with
an application to citation matching. Even though our citation matching model doesn’t touch upon all the interesting
aspects of NP-BLOG, the reader will hopefully find it instrumental in understanding the semantics.
3.1 Citation matching
One of the main challenges in developing an automated citation matching system is the resolution of identity uncertainty: for each citation, we would like to recover its true
title and authors. For instance, the following citations from
the CiteSeer database probably refer to the same paper:
Kozierok, Robin, and Maes, Pattie, A Learning Interface Agent
for Meeting Scheduling, Proceedings of the 1993 International
Workshop on Intelligent user Interfaces, ACM Press, NY.
R. Kozierok and P. Maes. A learning interface agent for
scheduling meetings. In W. D. Gray, W. E. Heey, and D. Murray, editors, Proc. of the Internation al Workshop on Intelligent
User Interfaces, Orlando FL, New York, 1993. ACM Press.

Even after assuming the title and author strings have been
segmented into separate fields (an open research problem
itself!), citation matching still exhibits serious challenges:
two different strings may refer to the same author (e.g.
J.F.G. de Freitas and Nando de Freitas) and, conversely,
the same string may refer to different authors (e.g. David
Lowe in vision and David Lowe in quantum field theory).

01 type Author; type Pub; type Citation;
02 guaranteed Citation;
03 #Author ∼ NumAuthorsDist();
04 #Pub ∼ NumPubsDist();
05 Name(a) ∼ NameDist();
06 Title(p) ∼ TitleDist();
07 NumAuthors(p) ∼ NumAuthorsDist();
08 RefAuthor(p, i) if Less(i, NumAuthors(p))
then ∼ Uniform(Author a);
09 RefPub(c) ∼ Uniform(Pub p);
10 CitedTitle(c) ∼ TitleStrDist(Title(RefPub(c)));
11 CitedName(c, i) if Less(i, NumAuthors(RefPub(c)))
then ∼ NameStrDist(Name(RefAuthor(RefPub(c), i)));

Figure 2: BLOG model for citation matching [10].
01 type Author; type Pub;
02 type Citation; type AuthorMention;
03 guaranteed Citation; guaranteed AuthorMention;
04 Name(a) ∼ NameDist{};
05 Title(p) ∼ TitleDist{};
06 CitedTitle(c) ∼ TitleStrDist{Title(RefPub(c))};
07 RefAuthor(u) ∼ PubAuthorsDist(RefPub(CitedIn(u)));
08 CitedName(u) ∼ NameStrDist{Name(RefAuthor(u))};

Figure 3: NP-BLOG model for citation matching.
There are a number of different approaches to this problem.
Pasula et al. incorporate unknown objects and identity uncertainty into a probabilistic relational model [14]. Wellner
et al. resolve identity uncertainty by computing the optimal graph partition in a conditional random field [22]. We
elaborate on the BLOG model presented in [10] in order to
contrast it with the one we propose. The BLOG model is
shown in Fig. 2 with cosmetic modifications and the function declaration statements omitted.
The BLOG model describes a generative sampling process.
Line 1 declares the object types, and line 2 declares that
the citations are guaranteed (hence are not generated by a
number statement). Lines 3 and 4 are number statements,
and lines 5-11 are dependency statements; their combination defines a generative process. The process starts by
choosing a certain number of authors and publications from
their respective prior distributions. Then it samples author names, publication titles and the number of authors
per publication. For each author string i in a citation, we
choose the referring author from the set of authors. Finally,
the properties of the citation objects are chosen. For example, generating an interpretation of CitedTitle(c) for citation
c requires values for RefPub(c) and estimates of publication
titles. TitleStrDist(s) can be interpreted as a measure that
adds noise in the form of perturbations to input string s.
The NP-BLOG model in Fig. 3 follows a similar generative
approach, the key differences being that it samples collections of unknown objects from DPs, and it allows for uncertainty in the order of authors in publications. But what
do we gain by implementing nonparametrics? The advan-

Figure 4: Three representations of lines 5-6 in Fig. 3: as
an NP-BLOG program, as a generative process, and as a
graphical model. Darker, hatched nodes are fixed or generated from other lines and shaded nodes are observed. Note
the similarity between the graphical model and Fig. 1b.
Lines 5-6 describe a DP mixture (4) over the publications
p, where the base measure is φTitleDist , πTitle is the hidden
distribution over publication objects, the indicators are the
true publications zRefPub [c] corresponding to the citations c,
and the continuous observations are the titles φCitedTitle [c].
tage lies in the ability to capture sophisticated models of
unbounded sets of objects in a high-level fashion, and the
relative ease of conducting inference, since nonparametrics
can deal gracefully with the problem of model selection.
One can view a model such as the automatic citation
matcher from three perspectives: it is a set of statements
in the language that comprise a program; from a statistician’s point of view, the model is a process that samples
the defined random variables; and from the perspective of
machine learning, it is a graphical model. Fig. 3 interprets
lines 5-6 of Fig. 4 in three different ways. The semantics,
as we will see, formally unify all three perspectives.
Both BLOG and NP-BLOG can answer the following
queries: Is the referring publication of citation c the same
as the referring publication of citation d? How many authors are there in the given citation database? What are
the names of the authors of the publication referenced by
citation c? How many publications contain the author a,
where a is one of the authors in the publication referenced
by citation c? And what are the titles of those publications?
However, only NP-BLOG can easily answer the following
query: what group of researchers do we expect to be authors in a future, unseen publication?
3.2 Objects and function symbols
This section is largely devoted to defining notation so that
we can properly elaborate on NP-BLOG semantics in Sections 3.3 to 3.5. The notation as it appears in these sections
makes the connection with both first-order logic and the
Dirichlet process mixture presented in Sec. 2.

The set of objects of a type τ is called the extension of
τ , and is denoted by [τ ]. In BLOG, extensions associated
with unknown (non-guaranteed) types can vary over possible worlds ω, so we sometimes write [τ ]ω . The size of [τ ]ω
is denoted by nω (τ ).1 Note that objects may be unknown
even if there is a fixed number of them. Guaranteed objects
are present in all possible worlds. We denote ΩΨ to be the
set of possible worlds for model Ψ.
A model introduces a set of function symbols indexed by
the objects. For conciseness, we treat predicates as Boolean
functions and constants as zero-ary functions. For example, the citation matching model (Fig. 3) has the function
symbols Name and CitedTitle, among others, so there is a
Name(a) for every author a and CitedTitle(c) for every citation c. By assigning numbers to objects as they are generated, we can consider logical variables a and c to be indices
on the set of natural numbers. Since BLOG is a typed language, the range of interpretations of a function symbol f
is specified by its type signature. For example, the interpretation of RefAuthor(u), for each u ∈ [AuthorMention] =
{1, 2, . . . , n(AuthorMention)}, takes a value on the range
[Author]. Likewise, Title(p) ranges over the set of strings
[String]. Figures 2 and 3 omit function declaration statements, which specify type signatures. Nonetheless, this
should not prevent the reader from deducing the type signatures of the functions via the statements that generate them.
Nonparametric priors define distributions over probability
measures, so we need function symbols that uniformly refer to them. Letting X and Y be object domains (e.g.
X = [Author]), we define MD (X | Y) to be the set of conditional probability densities p(x ∈ X | y ∈ Y) following
the class of parameterizations D. We can extend this logic,
denoting MD′ (MD (X | Y) | Z) to be the set of probability
measures p(d ∈ D | z ∈ Z) over the choice of parameterizations d ∈ D, conditioned on Z. And so on. For peace of
mind, we assume each class of distributions D is defined on
a measurable σ-field and the densities are integrable over
the range of the sample space. Note that Y or Z, but not
X , may be a Cartesian product over sets of objects. BLOG
does not allow return types that are tuples of objects, so
we restrict distributions of objects accordingly. One can
extend the above reasoning to accommodate distributions
over multiple unknown objects by adopting slightly more
general notation involving products of sets of objects.
We assign symbols to all the functions defined in the language LΨ . For instance, the range of NameDist in Fig. 3
is M([String]) for some specified parameterization class.
Since NameDist is not generated in another line, it must
be fixed over all possible worlds. For each publication p,
the interpretation of symbol PubAuthorsDist(p) is assigned
a value on the space MMultinomial ([Author]). That is, the
1
Even though the DP imposes a distribution over an infinite set
of unknown objects, nω (τ ) is still finite since it refers to the estimated number of objects in ω. n(τ ) corresponds to the random
variables of the DP mixture, as explained in Sec. 3.5.

function symbol refers to a distribution over author objects.
How the model chooses the success rate parameters for this
multinomial distribution, given that it is not on the left side
of any generating statement, is the subject of Sec. 3.5.
NP-BLOG integrates first-order logic with Bayesian nonparametric methods, but we have left out one piece of the
puzzle: how to specify distributions such as NameDist, or
classes of distributions. This is an important design decision, but an implementation level detail, so we postpone it
to future work. For the time being, one can think of parameterizations as object classes in a programming language
such as Java that generate samples of the appropriate type.
We point out that there is already an established language
for constructing hierarchical Bayesian models, BUGS [20].
The truth of any first-order sentence is determined by a
possible world in the corresponding language. A possible
world ω ∈ ΩΨ consists of an extension [τ ]ω for each type τ
and an interpretation for each function symbol f . Sec. 3.5
details how NP-BLOG specifies a distribution over ΩΨ .
3.3 Dependency statements for known objects
The dependency statement is the key ingredient in the specification of a generative process. We have already seen several examples of dependency statements, and we formalize
them here. It is well explained in [11], but we need to extend the definition in the context of nonparametrics.
In BLOG, a dependency statement looks like
f (x1 , . . . , xL ) ∼ g(t1 , . . . , tN );

(5)

where f is a function symbol, x1 , . . . , xL is a tuple of
logical variables representing arguments to the function,
g is a probability density conditioned on the arguments
t1 , . . . , tN , which are terms or formulae in the language
LΨ in which the logical variables x1 , . . . , xL may appear.
The dependency statement carries out a generative process. For an example, let’s look at the dependency statement on line 10 of Fig. 2. Following the rules of semantics [11], line 10 generates assignments for random variables φCitedTitle [c], for c = 1, . . . , n(Citation), from probability density g conditioned on values for zRefPub [c] and
φTitle [p], for all p = 1, . . . , n(Pub). As in [11], we use
square brackets to index random variables, instead of the
statistics convention of using subscripts.
In NP-BLOG, the probability density g is itself a function
symbol, and the dependency statement is given by
f (x1 , . . . , xL ) ∼ g(t1 , . . . , tM ){tM +1 , . . . , tM +N }; (6)
where f and g are function symbols, and t1 , . . . , tM +N
are formulae of the language as in (5). For this to be a
valid statement, g(t1 , . . . , tM ) must be defined on the range
M(X | Y), where X is the range of f (x1 , . . . , xL ) and Y is
the domain of the input arguments within the curly braces.
The first M terms inside the parentheses are evaluated in
possible world ω, and their resulting values determine the

choice of measure g. The terms inside the curly braces
are evaluated in ω and the resulting values are passed to
distribution g(t1 , . . . , tM ). When all the logical variables
x1 , . . . , xL refer to guaranteed objects, the semantics of the
dependency statement are given by [11]. The curly brace
notation is used to disambiguate the two roles of input argument variables. The arguments inside parentheses are
indices to function symbols (e.g. the c in RefPub(c) in
Fig. 3), whereas the arguments inside curly braces serve
as input to probability densities (e.g. the term inside the
curly braces in TitleStrDist{Title(RefPub(c))}). This new
notation is necessary when a distribution takes both types
of arguments. We don’t have such an example in citation
matching, so we borrow one from an NP-BLOG model in
the aircraft tracking domain:2
State(a, t) if t = 0 then ∼ InitState{}
else ∼ StateTransDist(a){State(a, t − 1)};

The state of the aircraft a at time t is an R6Vector object
which stores the aircraft’s position and velocity in space.
When t > 0, the state is generated from the transition distribution of aircraft a given the state at the previous time
step. StateTransDist(a) corresponds to a measure on the
space M([R6Vector] | [R6Vector]).
For example, in line 6 of Fig. 3, the citation objects are
guaranteed. Following the rules of semantics, line 6 defines
a random variable φCitedTitle [c] corresponding to the interpretation of function symbol CitedTitle(c) for every citation
c. Given assignments to φTitleStrDist , zRefPub [c] (we use z
to be consistent with the notation of the semantics used in
this paper, although it makes no difference in BLOG) and
φTitle [p] for all p ∈ [Pub] — assignments that are either
observed or generated from other statements — the dependency statement defines the generative process
φCitedTitle [c] ∼ φTitleStrDist (φTitle [p]) s.t. p = zRefPub [c].
BLOG allows for contingencies in dependency statements.
These can be subsumed within our formal
framework
by defining a new measure φ′ (c, t) =
P
i δ(ci )φi (ti,1 , ti,2 , . . .), where δ(·) is the indicator function, ci is the condition i which must be satisfied in order to
sample from the density φi , c and t are the complete sets of
terms and conditions, and the summation is over the number of clauses. Infinite contingencies and their connection
to graphical models are discussed in [12].
3.4 Exchangeability and unknown objects
Unknown objects are precisely those which are not guaranteed. In this section, we formalize some important properties of generated objects in BLOG. We adopt the notion
of exchangeability [1] to objects in probabilistic first-order
logic. We start with some standard definitions.
2

In which aircraft in flight appear as blips on a radar screen,
and the objectives are to infer the number of aircraft and their
flight paths and to resolve identity uncertainty, arising because
a blip might not represent any aircraft or, conversely, an aircraft
might produce multiple detections [10].

Definition 1. The random variables x1 , . . . , xN are
(finitely) exchangeable under probability density function
p if p satisfies p(x1 , . . . , xN ) = p(xπ(1) , . . . , xπ(N ) ) for
all permutations π on {1, . . . , N } [1].
When n is finite, the concept of exchangeability is intuitive:
the ordering is irrelevant since possible worlds are equally
likely. The next definition extends exchangeability to unbounded sequences of random variables.
Definition 2. The random variables x1 , x2 , . . . are infinitely exchangeable if every finite subset is finitely exchangeable [1].
Exchangeability is useful for reasoning about distributions
over properties on sets of objects in BLOG. From Definitions 1 and 2, we have the following result.
Proposition 1. It is possible to define g in the dependency
statements (5) and (6) such that the sequence of objects
x1 , . . . , xL is finitely exchangeable if and only if the terms
t1 , . . . , tM +N do not contain any statements referring to a
particular xl .
For example, the distribution of hair colours of two people,
Eric and Mike, is not exchangeable given evidence that Eric
is the father of Mike. What about sequences of objects such
as time? As long as we do not set the predecessor function
beforehand, any sequence is legally exchangeable.
In this paper, models are restricted to infinitely exchangeable unknown objects. We can interpret this presupposition
this way: if we reorder a sequence of objects, then their
probability remains the same. If we add another object to
the sequence at some arbitrary position, both the original
and new sequence with one more object are exchangeable.
We can then appeal to de Finetti’s theorem (3), and hence
the Dirichlet process. Therefore, the order of unknown objects is not important, and we can reason about set of objects rather than sequences. While there are many domains
in which one would like to infer the presence of objects
that are not infinitely exchangeable, this constraint leaves
us open to modeling a wide range of interesting domains.
Unknown or non-guaranteed objects are assigned non-rigid
designators; a symbol in different possible worlds does not
necessarily refer to the same object, and so it does not
make sense to assign it a rigid label. This consideration
imposes a constraint: we can only refer to a publication
p via a guaranteed object, such as a citation c that refers
to it. While we cannot form a query that addresses a specific unknown object, or a subset of unknown objects, we
can pose questions about publications using existential and
universal quantifiers (resolved using Skolemization, for instance). We could ask, for instance, how many publications
have three or more authors.
3.5 Dependency statements for unknown objects
Sec. 3.2 formalized the notion of type extensions and function symbols in NP-BLOG programs. Sec. 3.3 served up
the preliminaries of syntax and semantics in dependency

statements. The remaining step to complete the full prescription of the semantics as a mapping from the language
LΨ to a distribution over possible worlds. This is accomplished by constructing a Bayesian hierarchical model over
random variables {φ, n, γ}, such that the set of random
variables φ is in one-to-one correspondence with the set of
function interpretations, n refers to the sizes of the type extensions,
and γ is a set of auxiliary random variables such
R
that p(φ, n, γ)dγ = p(φ, n). One might wonder why we
don’t dispense of function symbols entirely and instead describe everything using random variables, as in [18]. The
principal reason is to establish the connection with firstorder logic. Also, we want to make it clear that some random variables do not map to any individual in the domain.
What follows is a procedural definition of the semantics.
We now define distributions over the random variables, and
their mapping to the symbols of the first-order logic.
In order to define the rules of semantics, we collect dependency and number statements according to their input
argument types. If the collection of statements includes
a number statement, then the rules of semantics are given
in [11]. Otherwise, we describe how the objects and their
properties are implicitly drawn from a DP. Consider a set of
K dependency statements such that the generated functions
f1 , . . . , fK all require a single input of type υ, and [υ]ω can
vary over possible worlds ω. We denote x to be the logical
variable that ranges over [υ]. (The output types of the fk ’s
are not important.) The K dependency statements look like
f1 (x) ∼ g1 (t1,1 , . . . , t1,M1 ){t1,M1 +1 , . . . , t1,M1 +N1 };
..
..
(7)
.
.
fK (x) ∼ gK (tK,1 , ..., tK,MK){tK,MK+1 , ..., tK,MK+NK};
where Mk and Nk are the number of input arguments to
gk (·) and gk {·}, respectively, and tk,i is a formula in the
language in which x may appear. As in BLOG, each fk (x)
is associated with a random variable φfk [x]. The random
variables φg1 , . . . , φgK , including all those implicated in
the terms, must have been generated by other lines or are
observed. Overloading the notation, we define the terms
tk,i to be random variables that depend deterministically
on other generated or observed random variables. The set
of statements (7) defines the generative process
πυ ∼ Stick(αυ )
(8)
φfk [x] ∼ φgk [tk,1 , ..., tk,Mk ](tk,Mk+1 , ..., tk,Mk+Nk ), (9)
for k = 1, . . . , K, x = 1, . . . , ∞, where αυ is the userdefined DP concentration parameter and πυ is a multinomial distribution such that each success rate parameter πυ,x
determines the probability of choosing a particular object
x. NP-BLOG infers a distribution π over objects of type
υ following the condition of infinite exchangeability. For
example, applying rules (8,9) to line 4 of Fig. 3, we get
πAuthor ∼ Stick(αAuthor )
φName [a] ∼ φNameDist , for a = 1, . . . , ∞

If an object type does not have any dependency or number
statements, then no distribution over its extension is introduced (e.g. strings in the citation matching model).
The implementation of the DP brings about an important
subtlety: if x takes on a possibly infinite different set of values, how do we recover the true number of objects n(τ )?
The idea is to introduce a bijection from the subset of positive natural numbers that consists only of active objects to
the set {1, . . . , n(τ )}. An object is active in possible world
ω if and only if at least one random variable is assigned to
that object in ω. In the above example, n(Author) is the
number of author objects that are mentioned in the citations. Of course, in practice we do not sample an infinite
series of random variables φName [a].
If we declare a function symbol f with a return type τ ranging over a set of unknown objects, then there exists the default generating process
zf [x] ∼ πτ .

(10)

We use zf [x] instead of φf [x] to show that the random variables are the indicators of the DP mixture (4). For example,
each zRefPub [c] in line 6 in Fig. 3 is independently drawn
from the distribution of publications πPub . We can view
line 6 as constructing a portion of the hierarchical model,
as shown in Fig. 4. The number of publications n(Pub) is
set to the number of different values assigned to zRefPub [c].
NP-BLOG allows for the definition of a symbol f that corresponds to a multinomial distribution over [τ ], so its range
is MMultinomial ([τ ]). It exhibits the default prior
φf [x] ∼ Dirichlet(αf πτ ),

(11)

analogous to (10). αf is a user-defined scalar. We define
nf [x] to be the true number of objects associated with collection f (x). This is useful for modeling collections of objects such as the authors of a publication. Applying rules
(8,9,11) to the statements in Fig. 3 involving publication
objects, we arrive at the generative process
πPub ∼ Stick(αPub )
φTitle [p] ∼ φTitleDist , for p = 1, . . . , n(Pub)
φPubAuthorsDist [p] ∼ Dirichlet(αPubAuthorsDist πAuthor ) .
Most of the corresponding graphical model is shown in
Fig. 4. Only the φPubAuthorsDist [p]’s are missing, and
they are shown in Fig. 5. The true number of authors
nPubAuthorsDist [p] in publication p comes from the support
of all random variables that refer to it, and n(Pub) is determined by nPubAuthorsDist . While this paper focuses on the
Dirichlet process, our framework allows for other classes
of nonparametric distributions. One example can be found
in the aircraft tracking domain from Sec. 3.2, in which the
generation of aircraft transition tables might be specified
with the statement StateTransDist(a) ∼ StateTransPrior{}.
In both cases (10) and (11), one can override the defaults
by including appropriate dependency statements for f , in

Num. citations
Num. papers
Phrase matching
RPM+MCMC
CRF-Seg (N = 9)
NP-BLOG

Figure 5: The white nodes are the portion of the graphical
model generated in lines 7 and 8 of Fig. 3. See Fig. 4 for
an explanation of the darkened nodes.
which case we get φf [x] ∼ φg , following rule (9). For example, lines 7 and 8 in Fig. 3 specify the generative process
for the author mention objects,
zRefAuthor [u] ∼ φPubAuthorsDist [p]
φCitedName [u] ∼ φNameStrDist (φName [a]) ,
s.t. p = zRefPub [c], c = φCitedIn [u], a = zRefAuthor [u].
Fig. 5 shows the equivalent graphical model.
The generative process (8,9) is a stick-breaking construction over the unknown objects and their attributes. When
the objects x range over the set of natural numbers, (8,9) is
equivalent to the Dirichlet process
Gυ ∼ DP (αυ , Hυ,1 × · · · × Hυ,K ) ,
(12)
P∞
where Gυ , x=1 πυ,x δ(φf1 [x]) × · · · × δ(φfK [x]), and
Hυ,k is the base measure over the assignments to φfk , defined by gk conditioned on the terms tk,1 , . . . , tk,Mk +Nk .
Since BLOG is a typed, free language, we need to allow
for the null assignment to φf [x] when it is implicitly drawn
from πτ in (10). We permit the clause
f (x) ∼ if cond then null;

(13)

which defines φf [x] ∼ δ(null)δ(cond) + πτ (1 − δ(cond)).
This statement is necessary to take care of the situation
when an object’s source can be of different types, as in the
aircraft tracking domain with false alarms [10].
Next, we briefly describe how to extend the rules of semantics to functions with multiple input arguments. Let’s
consider the case of two inputs with an additional logical
variable y ∈ [ν]. Handling an additional input argument
associated with known (guaranteed) objects is easy. We
just duplicate (8,9) for every instance of y in the guaranteed type extension. This is equivalent to adding a finite
series of plates in the graphical model. Otherwise, we assume the unknown objects are drawn independently. That
is, π(υ,ν) = πυ πν . Multiple unknown objects as input does
cause some superficial complications with the interpretation of (8,9) as a DP, principally because we need to define
new notation for products of measures over different types.

Face Reinforce. Reason. Constraint
349
406
514
295
246
149
301
204
0.94
0.79
0.86
0.89
0.97
0.94
0.96
0.93
0.97
0.94
0.94
0.95
0.93
0.84
0.89
0.86

Table 1: Citation matching results for the Phrase Matching [9], RPM [14], CRF-Seg [22] and NP-BLOG models.
Performance is measured by counting the number of publication clusters that are recovered perfectly. The NP-BLOG
column reports an average over 1000 samples.
The DP determines an implicit distribution of unknown, infinitely exchangeable objects according to their properties.
That is, the DP distinguishes unknown objects solely by
their attributes. However, this is not always desirable —
for instance, despite being unable to differentiate the individual pieces, we know a chess board always has eight
black pawns. This is precisely why we retain the original
number statement syntax of BLOG which allows the user
to specify a prior over the number of unknown objects, independent of their properties. In the future, we would like
to experiment with priors that straddle these two extremes.
This could possibly be accomplished by setting a prior on
the Dirichlet concentration parameter, α.
By tracing the rules of semantics, one should see that only
thing the citation matching model does not generate is values for CitedIn(u). Therefore, they must be observed. We
can also provide observations from any number of object
attributes, such as CitedTitle(c) and CitedName(u), which
would result in unsupervised learning. By modifying the
set of evidence, one can also achieve supervised or semisupervised learning. Moreover, the language can capture both generative and discriminative models, depending
whether or not the observations are generated.
To summarize, the rules given by (7-11,13), combined
with the number statement [11], construct a distribution
p(φ, z, n, γ) such that the set of auxiliary variables is
γ = {π, α}, {φ, z} is in one-to-one correspondence with
the interpretations of the function symbols, the n are the
sizes of the [τ ], and an assignment to {φ, z, n} completely
determines the possible world ω ∈ Ω. The rules of semantics assemble models that are arbitrary hierarchies of DPs.

4

Experiment

The purpose of this experiment is to show that the NPBLOG language we have described realizes probabilistic
inference on a real-world problem. We simulate the citation matching model in Fig. 3 on the CiteSeer data set [9],
which consists of manually segmented citations from four
research areas in AI.
We use Markov Chain Monte Carlo (MCMC) to simulate
possible worlds from the model posterior given evidence in
the form of cited authors and titles. Sec. 2 briefly describes

There is much future work on this topic. An important direction is the development of efficient, flexible and on-line
inference methods for hierarchies of Dirichlet processes.

Figure 6: Estimated (solid blue) and true (dashed red line)
number of publications for the Face and Reasoning data.

Acknowledgements
This paper wouldn’t have happened without the help of
Brian Milch. Special thanks to Gareth Peters and Mike
Klaas for their assistance, and to the reviewers for their
time and effort in providing us with constructive comments.



First-order probabilistic models combine representational power of first-order logic with graphical models. There is an ongoing effort to design lifted inference algorithms for first-order
probabilistic models. We analyze lifted inference from the perspective of constraint processing and, through this viewpoint, we analyze and
compare existing approaches and expose their
advantages and limitations. Our theoretical results show that the wrong choice of constraint
processing method can lead to exponential increase in computational complexity. Our empirical tests confirm the importance of constraint
processing in lifted inference. This is the first
theoretical and empirical study of constraint processing in lifted inference.

1

INTRODUCTION

Representations that mix graphical models and first-order
logic—called either first-order or relational probabilistic
models—were proposed nearly twenty years ago (Breese,
1992; Horsch and Poole, 1990) and many more have
since emerged (De Raedt et al., 2008; Getoor and Taskar,
2007). In these models, random variables are parameterized by individuals belonging to a population. Even for
very simple first-order models, inference at the propositional level—that is, inference that explicitly considers every individual—is intractable. The idea behind lifted inference is to carry out as much inference as possible without
propositionalizing. An exact lifted inference procedure for
first-order probabilistic directed models was originally proposed by Poole (2003). It was later extended to a broader
range of problems by de Salvo Braz et al. (2007). Further
work by Milch et al. (2008) expanded the scope of lifted
inference and resulted in the C-FOVE algorithm, which is
currently the state of the art in exact lifted inference.
First-order models typically contain constraints on the pa-

rameters (logical variables typed with populations). Constraints are important for capturing knowledge regarding
particular individuals. In Poole (2003), each constraint
is processed only when necessary to continue probabilistic inference. We call this approach splitting as needed.
Conversely, in de Salvo Braz et al. (2007) all constraints
are processed at the start of the inference (this procedure
is called shattering), and at every point at which a new
constraint arises. Both approaches need to use constraint
processing to count the number of solutions to constraint
satisfaction problems that arise during the probabilistic inference. Milch et al. (2008) adopt the shattering procedure,
and avoid the need to use a constraint solver by requiring
that the constraints be written in normal form.
The impact of constraint processing on computational efficiency of lifted inference has been largely overlooked.
In this paper we address this issue and compare the approaches to constraint processing listed above, both theoretically and empirically. We show that, in the worst case,
shattering may have exponentially worse space and time
complexity (in the number of parameters) than splitting as
needed. Moreover, writing the constraints in normal form
can lead to computational costs with a complexity that is
even worse than exponential. Experiments confirm our theoretical results and stress the importance of informed constraint processing in lifted inference.
We introduce key concepts and notation in Section 2 and
give an overview of constraint processing during lifted inference in Section 3. In Section 4 we discuss how a specialized #CSP solver can be used during lifted inference.
Theoretical results are presented in Section 5. Section 6
contains results of experiments.

2

PRELIMINARIES

In this section we introduce a definition of parameterized random variables, which are essential components of
first-order probabilistic models. We also define parfactors (Poole, 2003), which are data structures used during
lifted inference.

294
2.1

KISYNSKI & POOLE

UAI 2009
g(x1)

PARAMETERIZED RANDOM VARIABLES

If S is a set, we denote by |S| the size of the set S.

g(A)

A population is a set of individuals. A population corresponds to a domain in logic.
A parameter corresponds to a logical variable and is typed
with a population. Given parameter X , we denote its population by D(X). Given a set of constraints C , we denote
a set of individuals from D(X) that satisfy constraints in C
by D(X) : C .
A substitution is of the form {X1 /t1 . . . . , Xk /tk }, where the
Xi are distinct parameters, and each term ti is a parameter
typed with a population or a constant denoting an individual from a population. A ground substitution is a substitution, where each ti is a constant.
A parameterized random variable is of the form
f (t1 , . . . ,tk ), where f is a functor (either a function symbol
or a predicate symbol) and ti are terms. Each functor has a
set of values called the range of the functor. We denote the
range of the functor f by range( f ). A parameterized random variable f (t1 , . . . ,tk ) represents a set of random variables, one for each possible ground substitution to all of its
parameters. The range of the functor of the parameterized
random variable is the domain of random variables represented by the parameterized random variable.
Let v denote an assignment of values to random variables;
v is a function that takes a random variable and returns its
value. We extend v to also work on parameterized random
variables, where we assume that free parameters are universally quantified.
Example 1. Let A and B be parameters typed with a population D(A) =D(B) ={x1 , . . . , xn }. Let h be a functor with
range {true, f alse}. Then h(A, B) is a parameterized random variable. It represents a set of n2 random variables
with domains {true, f alse}, one for each ground substitution {A/x1 , B/x1 }, {A/x1 , B/x2 }, . . . ,{A/xn , B/xn }. A parameterized random variable h(x1 , B) represents a set of
n random variables with domains {true, f alse}, one for
each ground substitution {B/x1 }, . . . ,{B/xn }. Let v be an
assignment of values to random variables. If v(h(x1 , B))
equals true, each of the random variables represented by
h(x1 , B), namely h(x1 , x1 ), . . . , h(x1 , xn ), is assigned the
value true by v.
2.2

PARAMETRIC FACTORS

A factor on a set of random variables represents a function
that, given an assignment of a value to each random variable from the set, returns a real number. Factors are used
in the variable elimination algorithm (Zhang and Poole,
1994) to store initial conditional probabilities and intermediate results of computation during probabilistic inference
in graphical models. Operations on factors include mul-

g(xn)

h(x1, x1)

h(A, B)

h(x1, xn)

B
A

FIRST-ORDER

h(xn , x1)

h(xn , xn)

PROPOSITIONAL

Figure 1: A parameterized belief network and its equivalent
belief network.
tiplication of factors and summing out random variables
from a factor.
Let v be an assignment of values to random variables and
let F be a factor on a set of random variables S. We extend
v to factors and denote by v(F) the value of the factor F
given v. If v does not assign values to all of the variables in
S, v(F) denotes a factor on other variables.
A parametric factor or parfactor is a triple hC , V, Fi where
C is a set of inequality constraints on parameters, V is a set
of parameterized random variables and F is a factor from
the Cartesian product of ranges of parameterized random
variables in V to the reals.
A parfactor hC , V, Fi represents a set of factors, one for
each ground substitution G to all free parameters in V that
satisfies the constraints in C . Each such factor FG is a factor on the set of random variables obtained by applying a
substitution G. Given an assignment v to random variables
represented by V, v(FG ) = v(F).
Parfactors are used to represent conditional probability distributions in directed first-order models and potentials in
undirected first-order models as well as intermediate computation results during inference in first-order models.
In the next example, which extends Example 1, we use parameterized belief networks (PBNs) (Poole, 2003) to illustrate representational power of parfactors. The PBNs are
a simple first-order directed probabilistic model, we could
have used parameterized Markov networks instead (as did
de Salvo Braz et al. (2007) and Milch et al. (2008)). Our
discussion of constraint processing in lifted inference is not
limited to PBNs, it applies to any model for which the joint
distribution can be expressed as a product of parfactors.
Example 2. A PBN consists of a directed acyclic graph
where the nodes are parameterized random variables, an
assignment of a range to each functor, an assignment of
a population to each parameter, and a probability distribution for each node given its parents. Consider the PBN
graph presented in Figure 1 using plate notation (Buntine,

UAI 2009

KISYNSKI & POOLE

1994). Let g be a functor with range {true, f alse}. Assume
we do not have any specific knowledge about instances of
g(A), but we have some specific knowledge about h(A, B)
for case where A = x1 and for case where A 6= x1 and A = B.
The probability P(g(A)) can be represented with a parfactor h0,
/ {g(A)}, Fg i, where Fg is a factor from range(h) to
the reals. The conditional probability P(h(A, B)|g(A)) can
be represented with a parfactor h0,
/ {g(x1 ), h(x1 , B)}, F1 i, a
parfactor h{A 6= x1 }, {g(A), h(A, A)}, F2 i, and a parfactor
h{A 6= x1 , A 6= B}, {g(A), h(A, B)}, F3 i, where F1 , F2 , and
F3 are factors from range(g) × range(h) to the reals.
Let C be a set of inequality constraints on parameters and
X be a parameter. We denote by EXC the excluded set for X,
that is, the set of terms t such that (X 6= t) ∈ C . A parfactor
hC , V, FF i is in normal form (Milch et al., 2008) if for each
inequality (X 6= Y ) ∈ C , where X and Y are parameters, we
have EXC \{Y } = EYC \{X}.
In a normal form parfactor, for all parameters X of parameterized random variables in V, | D(X): C | = | D(X) |−| EXC |.
Example 3. Consider the parfactor h{A 6= x1 , A 6=
B},{g(A), h(A, B)}, F3 i from Example 2. Let C denote
a set of constraints from this parfactor. The set C contains only one inequality between parameters, namely
A 6= B. We have EAC = {x1 , B} and EBC = {A}. As
EAC \{B} 6= EBC \{A}, the parfactor is not in normal form.
Recall that D(A) =D(B) ={x1 , . . . , xn }. The size of the
set D(A) : C depends on the parameter B. It is equal
n − 1 when B = x1 and n − 2 when B 6= x1 . Other parfactors from Example 2 are in normal form as they do
not contain constraints between parameters. Consider
a parfactor h{X 6= Y, X 6= a,Y 6= a}, {e(X), f (X,Y )}, Fe f i,
where D(X) = D(Y ) and | D(X) | = n. Let C 0 denote a set
0
0
of constraints from this parfactor. As EXC \{Y } = EYC \{X},
the parfactor is in normal form and | D(X) : C 0 | = n − 2 and
| D(Y ) : C 0 | = n − 2.

3

LIFTED INFERENCE AND
CONSTRAINT PROCESSING

In this section we give an overview of exact lifted
probabilistic inference developed in (Poole, 2003),
(de Salvo Braz et al., 2007), and (Milch et al., 2008) in
context of constraints. For more details on other aspects of
lifted inference we refer the reader to the above papers.
Let Φ be a set of parfactors. Let J (Φ) denote a factor
equal to the product of all factors represented by elements
of Φ. Let U be the set of all random variables represented
by parameterized random variables present in parfactors in
Φ. Let Q be a subset of U. The marginal of J (Φ) on Q,
denoted JQ (Φ), is defined as JQ (Φ) = ∑U\Q J (Φ).
Given Φ and Q, the lifted inference procedure computes the
marginal JQ (Φ) by summing out random variables from

295

Q, where possible in a lifted manner. Evidence can be handled by adding to Φ additional parfactors on observed random variables.
Before a (ground) random variable can be summed out, a
number of conditions must be satisfied. One is that a random variable can be summed out from a parfactor in Φ only
if there are no other parfactors in Φ involving this random
variable. To satisfy this condition, the inference procedure
may need to multiply parfactors prior to summing out.
Multiplication has a condition of its own: two parfactors
hC1 , V1 , F1 i and hC2 , V2 , F2 i can be multiplied only if for
each parameterized random variable from V1 and for each
parameterized random variable from V2 , the sets of random
variables represented by these two parameterized random
variables in respective parfactors are identical or disjoint.
This condition is trivially satisfied for parameterized random variables with different functors.
Example 4. Consider the PBN from Figure 1 and set Φ
containing parfactors introduced in Example 2. Assume
that we want to compute the marginal of J (Φ) on instances
of h(A, B), where A 6= x1 and A 6= B. We need to sum
out random variables represented by g(A) from parfactor
h{A 6= x1 , A 6= B}, {g(A), h(A, B)}, F3 i, but as they are also
among random variables represented by g(A) in parfactor
h0,
/ g(A), Fg i, we have to first multiply these two parfactors. Sets of random variables represented by g(A) in these
two parfactors are not disjoint and are not identical and the
precondition for multiplication is not satisfied.
3.1

SPLITTING

The precondition for parfactor multiplication may be satisfied through splitting parfactors on substitutions.
Let Φ be a set of parfactors. Let p f = hC , V, FF i ∈ Φ.
Let {X/t} be a substitution such that (X 6= t) ∈
/ C and
term t is a constant such that t ∈ D(X), or a parameter
such that D(t) = D(X). A split of p f on {X/t} results in
two parfactors: p f [X/t] that is a parfactor p f with all occurrences of X replaced by term t, and a parfactor p fr =
hC ∪{X 6= t}, V, FF i. We have J (Φ) = J (Φ \ {p f }∪
{p f [X/t], p fr }). We call p fr a residual parfactor.
Given two parfactors that need to be multiplied, substitutions on which splitting is performed are determined by analyzing constraint sets C and sets of parameterized random
variables V in these parfactors.
Example 5. Let us continue Example 4.
A split
of h0,
/ {g(A)}, Fg i on {A/x1 } results in h0,
/ {g(x1 )}, Fg i
and residual h{A 6= x1 }, {g(A)}, Fg i. The first parfactor
can be ignored because it is not relevant to the query,
while the residual needs to be multiplied by a parfactor
h{A 6= x1 , A 6= B}, {g(A), h(A, B)}, F3 i. The precondition
for multiplication is now satisfied as g(A) represents the
same set of random variables in both parfactors.

296
3.2

KISYNSKI & POOLE
MULTIPLICATION

Once the precondition for parfactor multiplication is satisfied, multiplication can be performed in a lifted manner.
This means that, although parfactors participating in a multiplication as well as their product represent multiple factors, the computational cost of parfactor multiplication is
limited to the cost of multiplying two factors. The only additional requirement is that the lifted inference procedure
needs to know how many factors each parfactor involved
in the multiplication represents and how many factors their
product will represent. These numbers can be different because the product parfactor might involve more parameters
than a parfactor participating in the multiplication. In such
a case, a correction to values of a factor inside appropriate
parfactors participating in multiplication is necessary. Detailed description of this correction is beyond the scope of
this paper. For more information see Example 6 below and
a discussion of the fusion operation in de Salvo Braz et al.
(2007). For our purpose, the key point is that the lifted inference procedure needs to compute the number of factors
represented by a parfactor.
Given a parfactor hC , V, Fi, the number of factors it represents is equal to the number of solutions to the constraint
satisfaction problem formed by constraints in C . This
counting problem is written as #CSP (Dechter, 2003). If
a parfactor is in normal form, each connected component
of the underlying constraint graph is fully connected (see
Proposition 1) and it is easy to compute the number of factors represented by the considered parfactor. If a parfactor
is not in normal form, a #CSP solver is necessary to compute the number of factors the parfactor represents.
Example 6. In Example 5 the parfactor h{A 6= x1 },
{g(A)}, Fg i represents n − 1 factors, and needs to be multiplied by a parfactor h{A 6= x1 , A 6= B}, {g(A), h(A, B)}, F3 i,
which represents (n − 1)2 factors. Their product p f∗ =
hA 6= x1 , A 6= B, {g(A), h(A, B)}, F∗ i, where F∗ is a factor
from range(g) × range(h) to the reals, represents (n − 1)2
factors. Let v be an assignment of values to g(A) and
(n−1)/(n−1)2

h(A, B). We have v(F∗ ) = v(Fg )
v(F3 ). Now
we can sum out g(A) from p f∗ . The result needs to be
represented with a counting formula (Milch et al., 2008),
which is outside of the scope of this paper. What is important for the paper is that a parfactor involving counting
formulas needs to be in normal form. Since p f∗ is not in
normal form, we first split it on substitution {B/x1 } and
then sum out g(A) from the two parfactors obtained through
splitting.
3.3

SUMMING OUT

During lifted summing out, a parameterized random variable is summed out from a parfactor hC , V, FF i, which
means that a random variable is eliminated from each factor
represented by the parfactor in one inference step. Lifted

UAI 2009

inference will perform summing out only once on the factor F. If some parameters only appear in the parameterized variable that is being eliminated, the resulting parfactor will represent fewer factors than the original one. As
in the case of parfactor multiplication, the inference procedure needs to compensate for this difference. It needs to
compute the size of the set X = (D(X1 ) × · · · × D(Xk )) : C ,
where X1 , . . . , Xk are parameters that will disappear from
the parfactor. This number tells us how many times fewer
factors the result of summing out represents compared to
the original parfactor.
If the parfactor is in normal form, |X | does not depend on
values of parameters remaining in the parfactor after summing out. The problem reduces to #CSP and, as we mentioned in Section 3.2, it is easy to solve. If the parfactor is
not in normal form, |X | may depend on values of remaining
parameters and a #CSP solver is necessary to compute all
the sizes of the set X conditioned on values of parameters
remaining in the parfactor.
Example 7. Assume that we want to sum out f (X,Y )
from a parfactor h{X 6= Y,Y 6= a}, {e(X), f (X,Y )}, Fe f i,
where Fe f is a factor from range(e) × range( f ) to the reals. Let D(X) = D(Y ) and | D(X) | = n. The parfactor represents (n − 1)2 factors. Note that the parfactor is
not in normal form and | D(Y ) : {X 6= Y,Y 6= a}| equals
n − 1 if X = a and n − 2 if X 6= a. A #CSP solver could
compute these numbers for us (see Example 9). After
f (X,Y ) is summed out, Y is no longer among parameters of random variables and X remains the sole parameter. To represent the result of summation we need two
parfactors: h0,
/ e(a), Fe1 i and h{X 6= a}, e(X), Fe2 i, where
Fe1 and Fe2 are factors from range(e) to the reals. Let
y ∈ range(e), then Fe1 (y) = (∑z∈range( f ) Fe f (y, z))n−1 and
Fe2 (y) = (∑z∈range( f ) Fe f (y, z))n−2 .
3.4

PROPOSITIONALIZATION

During inference in first-order probabilistic models it may
happen that none of lifted operations (including operations
that are not described in this paper) can be applied. In such
a situation the inference procedure substitutes appropriate
parameterized random variables with random variables represented by them. This may be achieved through splitting
as we demonstrate in an example below. Afterward, inference is performed, at least partially, at the propositional
level. As it has a negative impact on the efficiency of inference, propositionalization is avoided as much as possible
during inference in first-order models.
Example 8. Consider a parfactor h0,
/ {g(A)}, Fg i from Example 2. Assume we need to propositionalize g(A). Recall that D(A) ={x1 , . . . , xn }. Propositionalization results
in a set of parfactors h0,
/ {g(x1 )}, Fg i, h0,
/ {g(x2 )}, Fg i, . . . ,
h0,
/ {g(xn−1 )}, Fg i, h0,
/ {g(xn )}, Fg i. Each parameterized
random variable in the above parfactors represents just one

UAI 2009
10

10

KISYNSKI & POOLE

100w

10w

5w

W

W
w

3

=

2w

W

=

X

ϖw

5

10

297

Y

X

Z

=

=

Y

=

X=Y
=

=

=

=

=

Z

=

Z
0

10

0

5

10

15
w

20

25

30

Figure 2: Comparison of ϖw and exponential functions.
random variable and each parfactor represents just one factor. The set could be produced by a sequence of splits.
The above informal overview of lifted inference, together
with simple examples, shows that constraint processing is
an integral, important part of lifted inference.

4

#CSP SOLVER AND LIFTED
INFERENCE

In Section 3, we showed when a #CSP solver can be used
during lifted probabilistic inference. A solver that enumerates all individuals from domains of parameters that
form a CSP would contradict the core idea behind lifted
inference, that is, performing probabilistic inference without explicitly considering every individual. In our experiments presented in Section 6 we used a solver (Kisyński
and Poole, 2006) that addresses the above concern. It is a
lifted solver based on the variable elimination algorithm for
solving #CSP of Dechter (2003) and is optimized to handle
problems that contain only inequality constraints. It is not
possible to describe the algorithm in detail in this paper, but
below we provide some intuition behind the solver.

(a)

(b)

Figure 3: Constraint graph with a cycle (a) and the two
cases that need to be analyzed (b).
us to immediately solve the corresponding #CSP: we can
assign the value to W in n ways, and are left with n − 1 possible values for X, and n − 1 possible values for Y and Z.
Hence, there are n(n − 1)3 solutions to this CSP instance.
Consider a set of constraints {W 6= X,W 6= Y, X 6= Z,Y 6=
Z}, where all parameters have the same population of size
n. The underlying graph has a cycle (see Figure 3 (a)),
which makes the corresponding #CSP more difficult to
solve than in the previous example. We can assign the value
to W in n ways, and are left with n − 1 possible values for
X and Y . For Z we need to consider two cases: X = Y and
X 6= Y (see Figure 3 (b)). In the X = Y case, Z can take
n − 1 values, while in the X 6= Y case, Z can have n − 2 different values. Hence, the number of solutions to this CSP
instance is n(n − 1)2 + n(n − 1)(n − 2)2 . The first case corresponds to a partition {{X,Y }} of the set of parameters
{X,Y }, while the second case corresponds to a partition
{{X}, {Y }} of this set.

First, we need to introduce a concept of a set partition. A
partition of a set S is a collection {B1 , . . . , Bw } S
of nonempty,
pairwise disjoint subsets of S such that S = wi=1 Bi . The
sets Bi are called blocks of the partition. Set partitions
are intimately connected to equality. For any consistent set
of equality assertions on parameters, there is a partition in
which the parameters that are equal are in the same block,
and the parameters that are not equal are in different blocks.
If we consider a semantic mapping from parameters to individuals in the world, the inverse of this mapping, where
two parameters that map to the same individual are in the
same block, forms a partition of the parameters. The number of partitions of the set of size w is equal to the w-th
Bell number ϖw . Bell numbers grow faster than any exponential function (see Lovász (2003)), but for small w’s
they stay much smaller than exponential functions with a
moderate base (see Figure 2).

In general, to perform this kind of reasoning, we need
to triangulate the constraint graph. This can be naturally
achieved with a variable elimination algorithm. Each new
edge adds two cases, one in which the edge corresponds to
the equality constraint and one in which it corresponds to
the inequality constraint. Some cases are inconsistent and
can be ignored. When we have to analyze a fully connected
subgraph of w new edges, we need to consider ϖw cases.
This is because each such case corresponds to a partition of
the parameters from the subgraph; those parameters in the
same block of the partition are equal, and parameters in different blocks are not equal. The number of such partitions
is equal to ϖw . The lifted #CSP solver analyzes ϖw partitions of parameters, rather than nw ground substitutions of
individuals. Since we do not care about empty partitions,
we will never have to consider more partitions than there
are ground substitutions. As w corresponds to the induced
width of a constraint graph (which we do not expect to be
large) and n corresponds to the population size (potentially
large), the difference between ϖw and nw can be very big
(see Figure 2).

Consider a set of constraints {W 6= X, X 6= Y, X 6= Z}, where
all parameters have the same population of size n. The underlying constraint graph has a tree structure, which allows

In practice, parameters can be typed with different populations (from the very beginning as well as because of unary
constraints). In such a situation, we can apply the above

298

KISYNSKI & POOLE

UAI 2009

reasoning to any set of individuals that are indistinguishable as far as counting is concerned. For example, the intersection of all populations is a set of individuals for which
we only need the size; there is no point in reasoning about
each individual separately. Similarly, the elements from the
population of a parameter that do not belong to the population of any other parameter can be grouped and treated
together. In general, any individuals that are in the populations of the same group of parameters can be treated identically; all we need is to know how many there are.

two parfactors are about to be multiplied and the precondition for multiplication is not satisfied.

Example 9. In Example 7 we need to know the number | D(Y ) : {X 6= Y,Y 6= a}|, where D(X) = D(Y ) and
| D(X) | = n. Let a1 denote set {a} and a2 denote set
D(X) \{a}. The following factor has value 1 for substitutions to parameters X,Y that are solutions to the above
CSP and 0 otherwise:

Shattering simplifies design and implementation of lifted
inference procedures, in particular, construction of elimination ordering heuristics. Unfortunately, as we show in
Theorem 1, it may lead to creation of large number of parfactors that would not be created by following the splitting
as needed approach.

Y
a2
a2
a2

X
a1
a2
a2

Partition(s)
{{X}} {{Y }}
{{X,Y }}
{{X}, {Y }}

1 .
0
1

After we eliminate Y from the above factor we obtain:
X
a1
a2

Partition(s)
{{X}}
{{X}}

n−1 .
n−2

Numbers n − 1 and n − 2 are obtained through analysis
of partitions of X and Y present in the original factor and
knowledge that a1 represents 1 individual and a2 represents
n − 1 individuals. From the second factor we can infer that
| D(Y ) : {X 6= Y,Y 6= a}| equals n − 1 if X = a and n − 2 if
X 6= a.
If we assume that all populations of parameters forming a
#CSP are sorted according to the same (arbitrary) ordering, sets of indistinguishable individuals can be generated
through a single sweep of the populations. Each such set
can be represented by listing all of its elements or by listing all elements from the corresponding population that do
not belong to it. For each set we choose a more compact
representation.

An alternative, called shattering, was proposed by
de Salvo Braz et al. (2007). They perform splitting at the
beginning of the inference by doing all the splits that are
required to ensure that for any two parameterized random
variables present in considered parfactors the sets of random variables represented by them are either identical or
disjoint.1 Shattering was also used in Milch et al. (2008).

Theorem 1. Let Φ be a set of parfactors. Let Q be a subset of the set of all random variables represented by parameterized random variables present in parfactors in Φ.
Assume we want to compute the marginal JQ (Φ). Then:
(i) if neither of the algorithms performs propositionalization, then every split on substitution {X/t}, where t is
a constant, performed by lifted inference with splitting
as needed is also performed by lifted inference with
shattering (subject to a renaming of parameters);
(ii) lifted inference with shattering might create exponentially more (in the maximum number of parameters in
a parfactor) parfactors than lifted inference with splitting as needed.
Proof. We present a sketch of a proof of the first statement
and a constructive proof of the second statement.
(i) Assume that lifted inference with splitting as needed
performs a split. We can track back the cause of this to
the initial set of parfactors Φ. Further analysis shows that
shattering the set Φ would also involve this split.
(ii) Consider the following set of parfactors:
Φ = {h0,
/ {gQ (), g1 (X1 , X2 , . . . , Xk ),

The answer from the solver needs to be translated to sets of
substitutions and constraints accompanying each computed
value. Standard combinatorial enumeration algorithms can
do this task.

5

THEORETICAL RESULTS

g2 (X2 , X3 , . . . , Xk ),
...,
gk (Xk ))}, F0 i,

[0]

h0,
/ {g1 (a, X2 , . . . , Xk )}, F1 i,

[1]

h0,
/ {g2 (a, X3 , . . . , Xk )}, F2 i,

[2]

...,
In this section we discuss consequences of different approaches to constraint processing in lifted inference.
5.1

SPLITTING AS NEEDED VS. SHATTERING

Poole (2003) proposed a scheme in which splitting is performed “as needed” through the process of inference when

h0,
/ {gk−1 (a, Xk )}, Fk−1 i,

[k − 1]

h0,
/ {gk (a)}, Fk i,

[k]

and let Q be gQ ().
1 Shattering might also be necessary in the middle of inference
if propositionalization has been performed.

UAI 2009

KISYNSKI & POOLE

For i = 1, . . . , k, a set of random variables represented by
a parameterized random variable gi (Xi , . . . , Xk ) in a parfactor [0] is a proper superset of a set of random variables represented by a parameterized random variable
gi (a, Xi+1 , . . . , Xk ) in a parfactor [i]. Therefore lifted inference with shattering needs to perform several splits. Since
the order of splits during shattering does not matter here,
assume that the first operation is a split of the parfactor [0]
on a substitution {X1 /a} which creates a parfactor
h0,
/ {gQ (), g1 (a, X2 , . . . , Xk ), g2 (X2 , X3 , . . . , Xk ),
. . . , gk (Xk ))}, F0 i

[k + 1]

and a residual parfactor
h{X1 6= a}, {gQ (), g1 (X1 , X2 , . . . , Xk ),
g2 (X2 , X3 , . . . , Xk ), . . . , gk (Xk ))}, F0 i.

[k + 2]

In both newly created parfactors, for i = 2, . . . , k, a set of
random variables represented by a parameterized random
variable gi (Xi , . . . , Xk ) is a proper superset of a set of random variables represented by a parameterized random variable gi (a, Xi+1 , . . . , Xk ) in a parfactor [i] and shattering proceeds with further splits of both parfactors. Assume that in
next step parfactors [k + 1] and [k + 2] are split on a substitution {X2 /a}. The splits result in four new parfactors. The
result of the split of the parfactor [k + 1] on {X2 /a} contains a parameterized random variable g1 (a, a, . . . , Xk ) and a
parfactor [1] needs to be split on a substitution {X2 /a}. The
shattering process continues following a scheme described
above. It terminates after 2k+1 − k − 2 splits and results in
2k+1 − 1 parfactors (each original parfactor [i], i = 0, . . . , k,
is shattered into 2k−i parfactors). Assume that lifted inference proceeds with an elimination ordering g1 , . . . , gk
(this elimination ordering does not introduce counting formulas, other do). To compute the marginal JgQ () (Φ), 2k
lifted multiplications and 2k+1 − 2 lifted summations are
performed.
Consider lifted inference with splitting as needed. Assume
it follows an elimination ordering g1 , . . . , gk . A set of random variables represented by a parameterized random variable g1 (X1 , . . . , Xk ) in a parfactor [0] is a proper superset
of a set of random variables represented by a parameterized random variable g1 (a, X2 , . . . , Xk ) in a parfactor [1] and
the parfactor [0] is split on a substitution {X1 /a}. The
split results in parfactors identical to the parfactors [k + 1]
and [k + 2] from the description of shattering above. The
parfactor [k + 1] is multiplied by the parfactor [1] and all
instances of g1 (a, X1 , . . . , Xk ) are summed out from their
product while all instances of g1 (X1 , X2 , . . . , Xk ) (subject
to a constraint X1 6= a) are summed out from the parfactor
[k + 2]. The summations create two parfactors:
h0,
/ {gQ (), g2 (X2 , X3 , . . . , Xk ), . . . , gk (Xk ))}, FFk+3 i,

[k + 3]

h0,
/ {gQ (), g2 (X2 , X3 , . . . , Xk ), . . . , gk (Xk ))}, FFk+4 i .

[k + 4]

299

Instances of g2 are eliminated next. Parfactors [k + 3] and
[k + 4] are split on a substitution {X2 /a}, the results of the
splits and a parfactor [2] are multiplied together and the
residual parfactors are multiplied together. Then, all instances of g2 (a, X3 , . . . , Xk ) are summed out from the first
product product while all instances of g2 (X2 , . . . , Xk ) (subject to a constraint X2 6= a) are summed out from the second
product. The elimination of g3 , . . . , gk looks the same as for
g2 . In total, 2k − 1 splits, 3k − 2 lifted multiplications and
2k lifted summations are performed. At any moment, the
maximum number of parfactors is k + 3.
The above theorem shows that shattering approach is never
better and sometimes worse than splitting as needed. It is
worth pointing out that splitting as needed approach complicates the design of an elimination ordering heuristic.
5.2

NORMAL FORM PARFACTORS VS. #CSP
SOLVER

Normal form parfactors were introduced by Milch et al.
(2008) in the context of counting formulas. Counting formulas are parameterized random variables that let us compactly represent a special form of probabilistic dependencies between instances of a parameterized random variable. Milch et al. (2008) require all parfactors to be in normal form to eliminate the need to use a separate constraint
solver to solve #CSP. The requirement is enforced by splitting parfactors that are not in normal form on appropriate
substitutions. While parfactors that involve counting formulas must be in normal form, that is not necessary for
parfactors without counting formulas. It might actually be
quite expensive as we show in this section.
Proposition 1. Let hC , V, Fi be a parfactor in normal
form. Then each connected component of the constraint
graph corresponding to C is fully connected.
Proof. Proposition 1 is trivially true for components with
one or two parameters. Let us consider a connected component with more than two parameters. Suppose, contrary to our claim, that there are two parameters X and Y
with no edge between them. Since the component is connected, there exists a path X, Z1 , Z2 ..., Zm ,Y . As C is in normal form, EZCi \{Zi+1 } = EZCi+1 \{Zi }, i = 1, . . . , m − 1 and
EZCm \{Y } = EYC \{Zm }. We have X ∈ EZC1 , and consequently
X ∈ EYC . This contradicts our assumption that there is no
edge between X and Y .
While the above property simplifies solving #CSP for a set
of constraints from a parfactor in normal form it also has
negative consequences. If a parfactor is not in normal form,
conversion to normal from might require several splits. For
example we need three splits to convert a parfactor with
the set of constraints shown in Figure 3 (a) to a set of four
parfactors in normal form. The resulting sets of constraints

300

KISYNSKI & POOLE
W

W
W =Z
=
X =Y

=

=

W =Z

X=Y

=

=

X

=

X

=

=

Y

Y

=
=
=

=

=
Z

Z

Figure 4: Constraint graphs obtained through a conversion
to normal form.
are presented in Figure 4. If the underlying graph is sparse,
conversion might be very expensive as we show in the example below.
Example 10. Consider a parfactor h{X0 6= a, X0 6=
X1 , . . . , X0 6= Xk }, {g0 (X0 ), g1 (X1 ), . . . , gn (Xk )}, Fi, where
D(X0 ) = D(X1 ) = · · · = D(Xk ). Let C denote a set of constraints from this parfactor. We have EXC0 = {a, X1 , . . . , Xk }
and EXCi = {X0 }, i = 1, . . . , k. The parfactor is not in normal form because EXC0 \{Xi } =
6 EXCi \{X0 }, i = 1, . . . , k. As
a result the size of the set D(X0 ) : C depends on other parameters in the parfactor. For instance, it differs for X1 = a
and X1 6= a or for X1 = X2 and X1 6= X2 . A conversion of
the considered parfactor to set of parfactors in normal form
involves 2k − 1 splitson substitutions of the form {Xi /a},
1 ≤ i ≤ k and ∑ki=2 ki (ϖi − 1) splits on substitutions of the

form {Xi /X j }, 1 ≤ i, j ≤ k. It creates ∑ki=0 ki ϖi parfactors
in normal form. In Example 11 we analyze how this conversion affects parfactor multiplication compared to the use
of a #CSP solver.
From the above example we can clearly see that the cost of
converting a parfactor to normal form can be worse than exponential. Moreover, converting parfactors to normal form
may be very inefficient when analyzed in context of parfactor multiplication (see Section 5.2.1) or summing out a
parameterized random variable from a parfactor (see Section 5.2.1). Our empirical tests (see Section 6.2) confirm
this observation.
Note that splitting as needed can be used together with a
#CSP solver (Poole, 2003), or with normal form parfactors.
Shattering can be used with a #CSP solver (de Salvo Braz
et al., 2007) or with normal form parfactors (Milch et al.,
2008). The cost of converting parfactors to normal form
might be amplified if it is combined with shattering.
5.2.1

Multiplication

In the example below we demonstrate how the normal form
requirement might lead to a lot of, otherwise unnecessary,
parfactor multiplications.
Example 11. Assume we would like to multiply the
parfactor from Example 10 by a parfactor p f =
h0,
/ {g1 (X1 )}, F1 i. First, let us consider how it is done
with a #CSP solver. A #CSP solver computes the num-

UAI 2009

ber of factors the parfactor from Example 10 represents,
(| D(X0 ) | − 1)k+1 . Next the solver computes the number
of factors represented by the parfactor p f , which is trivially | D(X1 ) |. A correction is applied to values of the factor F1 to compensate for the difference between these two
numbers. Finally the two parfactors are multiplied. The
whole operation involved two calls to a #CSP solver, one
correction and one parfactor multiplication. Now, let us
see how it can be done without the use of #CSP solver.

The first parfactor is converted to a set Φ of ∑ki=0 ki ϖi
parfactors in normal form, as presented in Example 10.
Some of the parfactors in Φ contain a parameterized random variable g1 (a), the rest contains a parameterized random variable g1 (X) and a constraint X1 6= a, so the parfactor p f needs to be split on a substitution {X1 /a}. The
split results in a parfactor h0,
/ {g1 (a)}, F1 i and a residual
h{X1 6= a}, {g1 (X1 )}, F1 i. Next, each parfactor from Φ is
multiplied by either the result of the split or the residual.
Thus ∑ki=0 ki ϖi parfactor multiplications need to be performed and most of these multiplication require a correction prior to the actual parfactor multiplication.
There is an opportunity for some optimization, as factor
components of parfactors multiplications for different corrections could be cached and reused instead of being recomputed. Still, even with such a caching mechanism, multiple parfactor multiplications would be performed compared to just one multiplication when a #CSP solver is used.
5.2.2

Summing Out

Examples 7 and 9 demonstrate how summing out a parameterized variable from a parfactor that is not in normal form
can be done with a help of a #CSP solver. In the example below we show how this operation would look if we
convert the parfactor to a set of parfactors in normal form
which does not require a #CSP solver.
Example 12. Assume that we want to sum out f (X,Y )
from the parfactor h{X 6= Y,Y 6= a}, {e(X), f (X,Y )}, Fe f i
from the Example 7. First, we convert it to a set of
parfactors in normal form by splitting on a substitutions {X/a}. We obtain two parfactors in normal form:
h{Y 6= a}, {e(a), f (a,Y )}, Fe f i, which represents n−1 factors, and h{X 6= Y, X 6= a,Y 6= a}, {e(X), f (X,Y )}, Fe f i,
which represents (n − 1)(n − 2) factors. Next we sum out
f (a,Y ) from the first parfactor and f (X,Y ) from the second
parfactor. In both cases a correction will be necessary, as Y
will no longer among parameters of random variables and
the resulting parfactors will represent fewer factors than the
original parfactors.
In general, as illustrated by Examples 7, 9 and 12, conversion to normal form and #CSP solver create the same number of parfactors. The difference is, that the first method,
computes a factor component for the resulting parfactors
once and then applies a different correction for each result-

UAI 2009

KISYNSKI & POOLE

ing parfactor based on the answer from the #CSP solver.
The second method computes the factor component multiple times, once for each resulting parfactor, but does not
use a #CSP solver. As these factor components (before applying a correction) are identical, redundant computations
could be eliminated by caching. We successfully adopted a
caching mechanism in our empirical test (Section 6.2), but
expect it to be less effective for larger problems.

1.8
1.6





1.4



1.2
1

As in the case of splitting as needed, it might be difficult
to design an efficient elimination ordering heuristic that
would work with a #CSP solver. This is because we do not
known in advance how many parfactors will be obtained as
a result of summing out. We need to run a #CSP solver to
obtain this information.

6

10

 
 

EXPERIMENTS

We used Java implementations of tested lifted inference
methods. Tests were performed on an Intel Core 2 Duo
2.66GHz processor with 1GB of memory made available
to the JVM.
6.1

SPLITTING AS NEEDED VS. SHATTERING

In the first experiment we checked to what extent the overhead of the shattering approach can be minimized by using
intensional representations and immutable objects that are
shared whenever possible. We ran tests on the following
set of parfactors:
Φ = { h0,
/ {gQ (), g1 (a)}, F0 i,

[0]

h{X 6= a}, {gQ (), g1 (X)}, F1 i,

[1]

h0,
/ {g1 (X), g2 (X)}, F2 i,

[2]

h0,
/ {g2 (X), g3 (X)}, F3 i,

[3]

...,
h0,
/ {gk−1 (X), gk (X)}, Fk i,

[k]

h0,
/ {gk (X)}, Fk+1 i}

[k + 1] .

All functors had the range size 10 and we set Q to the instance of gQ (). We computed the marginal JQ (Φ). Lifted
inference with shattering first performed total of k splits,
then proceeded with 2k + 1 multiplications and 2k summations regardless of the elimination ordering. Lifted inference with splitting as needed performed 1 split, k + 2 multiplications and k + 1 summations (for the experiment we
used the best elimination ordering, that is gk , gk−1 , . . . , g1 ).
Figure 5 shows the results of the experiment where we varied k from 1 to 100. Even though lifted inference with shattering used virtually the same amount of memory as lifted
inference with splitting, it was slower because it performed
more arithmetic operations.

20

40

60

80

100

Figure 5: Speedup of splitting as needed over shattering.



6

301

CONV−NFM−SUM
NFM−SUM
#CSP−SUM

4

10

2

10

1

10

2

10

  

3

10

Figure 6: Summing out with and without a #CSP solver.

6.2

NORMAL FORM PARFACTORS VS. #CSP
SOLVER

For experiment in this section we randomly generated sets
of parfactors. There were up to 5 parameterized random
variables in each parfactor with range sizes varying from 2
to 10. Constraints sets contained very few (and very often
zero) constraints and formed sparse CSPs. Most of parfactors were in normal form, which allowed us to account
for #CSP solver overhead. There were up to 10 parameters present in each parfactor. Parameters were typed with
the same population. We varied the size of this population
from 5 to 1000 to verify how well #CSP solver scaled for
larger populations.
In this experiment we summed out a parameterized random variable from a parfactor. We compared summing
out with a help of a #CSP solver (#CSP-SUM) to summing out achieved by converting a parfactor to a set of
parfactors in normal form and summing out a parameterized random variable from each obtained parfactor without
a #CSP solver. (We cached factor components as suggested
in Section 5.2.2). For each population size we generated
100 parfactors and reported a cumulative time. For the second approach, we reported time including (CONV-NFMSUM) and excluding (NFM-SUM) conversion to normal
form. Results presented on Figure 6 show significant cost
of conversion to normal form and advantage of #CSP solver
for larger population sizes.

302

7

KISYNSKI & POOLE

CONCLUSIONS AND FUTURE WORK

In this paper we analyzed the impact of constraint processing on the efficiency of lifted inference, and explained
why we cannot ignore its role in lifted inference. We
showed that a choice of constraint processing strategy has
big impact on efficiency of lifted inference. In particular,
we discovered that shattering (de Salvo Braz et al., 2007)
is never better—and sometimes worse—than splitting as
needed (Poole, 2003), and that the conversion of parfactors
to normal form (Milch et al., 2008) is an expensive alternative to using a specialized #CSP solver. Although in this
paper we focused on exact lifted inference, our results are
applicable to approximate lifted inference. For example,
see the recent work of (Singla and Domingos, 2008) that
uses shattering.
It is difficult to design an elimination ordering heuristic
that works well with the splitting as needed approach and a
#CSP solver. We plan to address this problem in our future
research.
Acknowledgments
The authors wish to thank Brian Milch for discussing the CFOVE algorithm with us. Peter Carbonetto, Michael Chiang and Mark Crowley provided many helpful suggestions
during the preparation of the paper. This work was supported by NSERC grant to David Poole.

