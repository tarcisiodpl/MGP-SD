
In this paper we introduce a class of Markov decision processes that arise as a natural model for
many renewable resource allocation problems.
Upon extending results from the inventory control literature, we prove that they admit a closed
form solution and we show how to exploit this
structure to speed up its computation.
We consider the application of the proposed
framework to several problems arising in very
different domains, and as part of the ongoing effort in the emerging field of Computational Sustainability we discuss in detail its application to
the Northern Pacific Halibut marine fishery. Our
approach is applied to a model based on real
world data, obtaining a policy with a guaranteed
lower bound on the utility function that is structurally very different from the one currently employed.

1

Introduction

The problem of devising policies to optimally allocate resources over time is a fundamental decision theoretic problem with applications arising in many different fields. In
fact, such decisions may involve a variety of different resources such as time, energy, natural and financial resources, in allocation problems arising in domains as diverse as natural resources management, crowdsourcing,
supply chain management, QoS and routing in networks,
vaccine distribution and pollution management.
A particularly interesting class of such problems involves
policies for the allocation of renewable resources. A key
and unique aspect of such a resource type is the fact that, by
definition, its stock is constantly replenished by an intrinsic
growth process. The most common example are perhaps
living resources, such as fish populations or forests, that increase constantly by natural growth and reproduction, but
less conventional resources such as users in a social com-

Carla Gomes, Bart Selman
Department of Computer Science
Cornell University
{gomes,selman}@cs.cornell.edu

munity or in a crowdsourcing project share the same intrinsic growth feature due to social interactions.
A common feature of the growth processes presented is that
they are density dependent, in the sense that the growth
rate depends on the amount of resource available. This fact
creates a challenging management problem when the aim
of the intervention is to optimally use the resource, for instance by harvesting a fish population or by requiring some
effort from a crowdsourcing community, especially when
economic aspects are factored in. We face a similar challenge in vaccine distribution problems, where the growth
rate of infections is again density dependent and the objective is to reduce its spreading.
This study, in particular, has been motivated by the alarming consideration that many natural resources are endangered due to over-exploitation and generally poorly managed. For instance, the Food and Agricultural Organization
estimates in their most recent report that 7% of marine fish
stocks are already depleted, 1% are recovering from depletion, 52% are fully exploited and 17% are overexploited
([1]).
One of the most fundamental aspects of the problem seems
to be the lack of an effective way to handle the uncertainty
affecting the complex dynamics involved. While in most of
the works in the literature [6, 7] these growth processes are
modeled with deterministic first-order difference or differential equations, this approach often represents an oversimplification. In fact their intrinsic growth is often affected
by many variables and unpredictable factors. For example,
in the case of animal populations such as fisheries, both
weather and climate conditions are known to affect both
the growth and the mortality in the population. Other variable ecological factors such as the availability of food or
the interaction with other species also influence their natural dynamics to the point that it is very difficult even to
obtain reliable mathematical models to describe their dynamics.
On the other hand, stochastic differential equations can easily incorporate these variable factors and therefore represent a more robust description. However, obtaining a prob-

abilistic description of such systems is far from easy. In
fact, even if in principle uncertainty could be reduced by
collecting and analyzing more data, it is generally believed
that complex and stochastic systems, such a marine environments, could never become predictable (to the point
that the authors of [13] believe that “predictability of anything as complex as marine ecosystem will forever remain
a chimera”).
Moreover, there are situations of “radical uncertainty” ([8])
or ambiguity where a stochastic description is not feasible
because the probabilities are not quantifiable. For instance,
many fundamental environmental issues that we are facing, such as those surrounding the climate change debate,
involve ambiguity in the sense of scientific controversies or
irreducible beliefs that cannot be resolved.
In the context of stochastic optimization, there are two
main ways to deal with uncertainty. The first one involves
a risk management approach, where it is assumed that the
probabilities of the stochastic events are known a priori or
are learned from experience through statistical data analysis. Within this framework, decisions are taken according
to stochastic control methods. Using tools such as risksensitive Markov decision processes ([12, 15]), it is also
possible to encode into the problem the attitude towards
risk of the decision maker by using an appropriate utility
function. In particular the degree of risk aversion can be
controlled by sufficiently penalizing undesirable outcomes
with the utility function. When a fine grained stochastic description is not available, worst-case game theoretic frameworks, that are inherently risk averse, play a fundamental role because it is often crucial to devise policies that
avoid catastrophic depletion. This type of approach, where
the problem of data uncertainty is addressed by guaranteeing the optimality of the solution for the worst realizations
of the parameters, is also known in the literature as robust
optimization ([3, 5]), and has been successfully applied to
uncertain linear, conic quadratic and semidefinite programming.
In this paper, we present a class of Markov decision processes that arise as a natural model for many resource management problems. Instead of formulating the optimization problem in a traditional form as a maximization of an
expected utility, we tackle the management problems in a
game theoretic framework, where the optimization problem
is equivalent to a dynamic game against nature. This formulation is a particular type of Markov game [14] (sometimes called a stochastic game [16]) where there are only
two agents (the manager and nature) and they have diametrically opposed goals.
As mentioned before, although this formulation is more
conservative, it also eliminates the very difficult task of estimating the probabilities of the stochastic events affecting
the system. In a context where the emphasis in the literature has traditionally been on the study of expected utilities,

this approach represents a new perspective. Moreover, the
policies thus obtained provide a lower bound on the utility
that can be guaranteed to be achieved, no matter the outcomes of the stochastic events. For this class of problems,
we are able to completely characterize the optimal policy
with a theoretical analysis that extends results from the inventory control literature, obtaining a closed form solution
for the optimal policy.
As part of the new exciting research area of Computational
Sustainability ([10]), where techniques from computer science and related fields are applied to solve the pressing sustainability challenges of our time, we present an application
of the proposed framework to the Northern Pacific Halibut
fishery, one of the largest and most lucrative fisheries of the
Northwestern coast. In particular, our method suggests the
use of a cyclic scheme that involves periodic closures of the
fishery, a policy that is structurally different from the one
usually employed, that instead tries to maintain the stock
at a given size with appropriate yearly harvests. However,
this framework is interesting in its own right and, as briefly
mentioned before, it applies to a variety of other problems
that share a similar mathematical structure and that arise
in very different domains. For example, we can apply our
framework to pollution problems, where a stock of pollutants is evolving over time due to human action, and the
objective is to minimize the total costs deriving from the
presence of a certain stock of pollutants and the costs incurred with cleanups, but also to crowdsourcing and other
problems.

2

MDP Formulation

In this section, we will formulate the optimization problem
as discrete time, continuous space Markov decision process. Whenever possible, we will use a notation consistent
with the one used in [4]. Even if we will consider only a
finite horizon problem, the results can be extended to the
infinite horizon case with limiting arguments. To make the
description concrete, the model will be mostly described
having a natural resource management problem in mind.
We consider a dynamical system evolving over time according to
xn+1 = f (xn − hn , wn ),
(1)
where xn ∈ R denotes the stock of a renewable resource
at time n. By using a discrete time model we implicitly assume that replacement or birth processes occur in regular,
well defined “breeding seasons”, where f (·) is a reproduction function that maps the stock level at the end of one
season to the new stock level level at the beginning of the
next season. The control or decision variable at year n is
the harvest level hn (occurring between two consecutive
breeding seasons), that must satisfy 0 ≤ hn ≤ xn .
As mentioned in the introduction, the function f (·) cap-

tures the intrinsic replenishment ability of renewable resources, that in many practical applications (such as fisheries or forestry) is density dependent: growth rate is high
when the habitat is underutilized but it decreases when
the stock is larger and intraspecific competition intensifies.
Specific properties of reproduction functions f (·) will be
discussed in detail later, but we will always assume that
there is a finite maximum stock level denoted by m.
To compensate for the higher level description of the complex biological process we are modeling, we introduce uncertainty into the model through wn , a random variable that
might capture, for example, the temperature of the water,
an uncontrollable factor that influences the growth of the
resource. Given the worst case framework we are considering, we will never make assumptions on the probability distribution of wn but only on its support (or, in other words,
on the possible outcomes). In fact in an adversarial setting
it is sufficient to consider all possible scenarios, each one
corresponding to an action that nature can take against the
policy maker, without assigning them a weight in a probabilistic sense.
Given the presence of stochasticity, it is convenient to consider closed loop optimization approaches, where decisions
are made in stages and the manager is allowed to gather information about the system between stages. In particular,
we assume that the state of the system xn ∈ R is completely observable. For example, in the context of fisheries
this means that we assume to know exactly the level of the
stock xn when the harvest level hn is to be chosen. In
this context, a policy is a sequence of rules used to select
at each period a harvest level for each possible stock size.
In particular, an admissible policy π = {µ1 , . . . , µN } is a
sequence of functions, each one mapping stocks sizes x to
harvests h, so that for all x and for all i
0 ≤ µi (x) ≤ x.

(2)

We assume that the marginal harvesting cost g(x) increases
as the stock size x decreases. We include time preference
into the model by considering a fixed discount factor α =
1/(1 + δ) ( 0 ≤ α ≤ 1), where δ > 0 is a discount rate.
For any given horizon length N , we consider the problem
of finding an admissible policy π = {µi }i∈[1,N ] that maximizes
π
(x) =
CN

min
w1 , . . . , wN
wi ∈ W (xi )

N
X

αn (R(xn ) − R(xn − hn ) − Kδ0 (hn ))

n=1

where xn is subject to (1) and hn = µn (xn ), with initial
condition x1 = x and

1 if x > 0,
δ0 (x) =
0 otherwise.
This is a Max-Min formulation of the optimization problem, where the goal is to optimize the utility in a worst-case
scenario. As opposed to the maximization of an expected
utility ([17, 18]), this formulation is inherently risk averse.
An advantage of this formulation is that there is no need to
characterize the probability distribution of the random variables wk explicitly, but only to determine their support. In
fact, one should consider all the possible scenarios, without
worrying about the probabilities of their occurrence.

3

Main Results

3.1

Minimax Dynamic Programming

π
A policy π is called an optimal N -period policy if CN
(x)
attains its supremum over all admissible policies at π for
all x. We call
π
CN (x) = sup CN
(x),

2.1

π∈Π

Resource Economics

We now consider the economic aspects of the model. We
suppose that the revenue obtained from a harvest h is proportional to h through a fixed price p, and that harvesting
is costly. In particular we assume that there is

the optimal value function, where Π represents the set of
all admissible policies.
As a consequence of the principle of optimality([4]), the
dynamic programming equation for this problem reads:
C0 (x)
Cn (x)

• a fixed set-up cost K each time a harvest is undertaken
• a marginal harvest cost g(x) per unit harvested when
the stock size is x
It follows that the utility derived from a harvest h from an
initial stock x is
Z x
g(y)dy − K , R(x) − R(x − h) − K, (3)
ph −

R(x) = px −

Z

x

g(y)dy.
0

0,
max

min R(xn ) − R(xn − hn )

0≤hn ≤x wn ∈W

−Kδ0 (hn ) + αCn−1 (f (x − hn , wn ))
for all n > 0. The latter equation can be rewritten in terms
of the remaining stock z = x − hn (the post decision state)
as

x−h

where

=
=



Cn (x) = α max
0≤z≤x

R(x) − R(z) − Kδ0 (x − z) + min Cn−1 (f (z, wn )) .
wn ∈W

(4)

This formulation of the problem is effectively analogous
to a game against nature in the context of a two-person
zero-sum game. The objective is in fact devising the value
of z that maximizes the utility, but assuming that nature
is actively playing against the manager with the opposite
intention.
It can be shown (see [4]) that Cn (x), the revenue function
associated with an optimal policy, is the (unique) solution
to equation (4). From equation (4) we see that an optimal
policy, when there are n periods left and the stock level is
x, undertakes a harvest if and only if there exists 0 ≤ z ≤ x
such that

• If β(·) is nondecreasing and concave on I
and ψ(·) is nondecreasing and K-concave on
[inf x∈I β(x), supx∈I β(x)] then the composition
ψ ◦ β is K-concave on I.
• Let β1 (x), . . . , βN (x) be a family of functions such
that βi (x) is Ki -concave. Then γ(x) = mini βi (x) is
(maxi Ki )-concave.
• If β(·) is a continuous, K-concave function on the interval [0, m], then there exists scalars 0 ≤ S ≤ s ≤ m
such that
– β(S) ≥ β(q) for all q ∈ [0, m].
– Either s = m and β(S) − K ≤ β(m) or s < m
and β(S) − K = β(s) ≥ β(q) for all q ∈ [s, m).
– β(·) is a decreasing function on [s, m].
– For all x ≤ y ≤ s, β(x) − K ≤ β(y).

R(x) − R(z) − K + α min Cn−1 (f (z, wn )) >
wn ∈W

α min Cn−1 (f (x, wn )).
wn ∈W

In fact, an action should be taken if and only if its associated benefits are sufficient to compensate the fixed cost
incurred. By defining
(5)

The proof is not reported here for space reasons, but can
be found in [9]. Similar results for K-convex functions are
proved in [4].

we have that an optimal policy, when there are n periods
left and the stock level is x, undertakes a harvest if and
only if there exists 0 ≤ z ≤ x such that

In the following section we will prove by induction the Kconcavity of the functions Pn (x), n = 1, . . . , N . This will
allow us to characterize the structure of the optimal policy
by using the last assertion of Lemma 1.

Pn (x) = −R(x) + α min Cn−1 (f (x, wn )),
wn ∈W

Pn (z) − K > Pn (x).

(6)

To examine this kind of relationship it is useful to introduce
the notion of K-concavity, a natural extension of the Kconvexity property originally introduced by Scarf in [19]
to study inventory control problems.
3.2

Preliminaries on K-concavity

A function β(·) is K-concave if given three points x < y <
z, β(y) exceeds the secant approximation to β(y) obtained
using the points β(x) − K and β(z). Therefore for K = 0
no slack is allowed and one recovers the standard definition
of concavity. Formally
Definition 1. A real valued function β(·) is K-concave if
for all x, y, x < y, and for all b > 0
β(x) − β(y) − (x − y)

β(y + b) − β(y)
≤ K.
b

(7)

We state some useful results concerning K-concavity:
Lemma 1. The following properties hold:
• A concave function is 0-concave and hence Kconcave for all K ≥ 0 .
• If β1 (q) and β2 (q) are respectively K1 -concave and
K2 -concave for constants K1 ≥ 0 and K2 ≥ 0,
then aβ1 (q) + bβ2 (q) is (aK1 + bK2 )-concave for any
scalars a > 0 and b > 0.

3.3

On the Optimality of (S − s) policies

Suppose that we can prove that Pn (x) is continuous and
strictly K-concave. Then by Lemma 1 there exists Sn , sn
with the properties proved in the last point of the Lemma.
It is easy to see that condition (6) is satisfied only if x > s,
in which case the optimal value of the remaining stock
z would be precisely Sn . In conclusion, if we can prove
the continuity and K-concavity of the functions Pn (x),
n = 1, . . . , N , then following feedback control law, known
as a nonstationary (S − s) policy, is optimal:
At period n, a harvest is undertaken if and only if the
current stock level is greater than sn ; in that case the stock
is harvested down to Sn .
This policy is known in the inventory control literature as a
nonstationary (S −s) policy 1 , because the levels Sn and sn
are time dependent. Since it is assumed that the marginal
harvest cost g(x) is a non increasing function, we define x0
to be the zero profit level such that g(x0 ) = p. If g(x) < p
for all x, we define x0 = 0. As a consequence for all
x > x0 we have that R′ (x) ≥ 0 so that R (defined in
equation (3)) is non decreasing. Moreover if the marginal
harvest cost g(x) is a non increasing function, then R is
convex.
1
For the sake of consistency, we call sn the threshold value
that governs the decision, even if in our case Sn ≤ sn .

We also need to make an assumption on the concavity of
R(·). In particular the marginal cost function g is allowed
to decrease but not by too much. Let m Rbe an upper bound
x
on the possible values of x and G(x) = 0 g(t)dt, then we
need


1−α
,
(8)
τ = G(m) − mg(m) < K
α

nondecreasing, consider the case 0 ≤ x1 < x2 ≤ sn+1 :

α

Cn+1 (x2 ) − Cn+1 (x1 ) =

min Cn (f (x2 , wn )) − min Cn (f (x1 , wn )) .

wn ∈W

min

wn ∈W (x2 )

The main result is the following theorem, where we show
that if some assumptions are satisfied, the optimal policy is
of (S − s) type. The key point of this inductive proof is
to show that the K-concavity property is preserved by the
Dynamic Programming operator.
Theorem 1. For any setup cost K > 0 and any positive
integer N , if f (·, w) is nondecreasing and concave for any
w and if g is non increasing and satisfies condition (8),
then the functions Pn (x) defined as in (5) are continuous
and K-concave for all n = 1, . . . , N . Hence there exists a
non-stationary (S − s) policy that is optimal. The resulting optimal present value functions Cn (x) are continuous,
nondecreasing and K-concave for all n = 1, . . . , N .
Proof. From equation (8) we know that there exists a number k such that
(9)

The proof is by induction on N . The base case N = 0
is trivial because C0 (x) = 0 for all x, and therefore it
is continuous, nondecreasing and k-concave. Now we
assume that Cn (x) is continuous, nondecreasing and kconcave, and we show that Pn+1 (x) is continuous and Kconcave, and that Cn+1 (x) is continuous, nondecreasing
and k-concave.
Since f (·, w) is nondecreasing and concave for all w,
Cn (f (z, wn )) is K-concave by Lemma (1). By Lemma
1

wn ∈W

If for all x2 > x1 ≥ 0,

a condition that implies the τ -concavity of R.

(K + τ )α < k < K.



f (x2 , wn ) ≥

min

wn ∈W (x1 )

f (x1 , wn ),

then Cn+1 (x2 ) − Cn+1 (x1 ) ≥ 0 because Cn (x) is nondecreasing. For the case sn+1 < x1 < x2 and sn+1 ≥ x0 :
Cn+1 (x2 ) − Cn+1 (x1 ) = α(R(x2 ) − R(x1 )) ≥ 0,
because R is nondecreasing on that interval. It must be the
case that Sn+1 > x0 because harvesting below x0 is not
profitable and reduces the marginal growth of the stock, so
given that sn+1 ≥ Sn+1 ≥ x0 we conclude that Cn+1 (x)
is nondecreasing. It remains to show that Cn+1 (x) is kconcave, and by equation (9) it is sufficient to show that it
is (K + τ )α-concave. To show that definition (7) holds for
Cn+1 (x), we consider several cases.
When x < y ≤ sn+1 , according to equation (10) we have
that Cn+1 (x) = α(Pn+1 (x) + R(x)) and therefore equation (7) holds by Lemma 1 because Pn+1 is K-concave and
R(·) is τ -concave. Similarly when sn+1 < x < y, equation (7) holds because R(·) is τ -concave.
When x ≤ sn+1 < y equation (7) reads
Cn+1 (y + b) − Cn+1 (y)
≤
Cn+1 (x) − Cn+1 (y) − (x − y)
b


R(y + b) − R(y)
≤
α K + R(x) − R(y) − (x − y)
b
α(K + τ ).
because Pn+1 (x) ≤ Pn+1 (Sn+1 ) and R(·) is τ -concave.

4

Consistency and Complexity

min Cn−1 (f (z, wn ))

wn ∈W

Even if Theorem 1 completely describes the structure of the
optimal policy, in general there is no closed form solution
for the values of Sn and sn , that need to be computed numerically. In order to use the standard dynamic programming approach, the state, control and disturbance spaces
must be discretized, for instance using an evenly spaced
grid. Since we are assuming that those spaces are bounded,
we obtain in this way discretized sets with a finite number
of elements. We can then write DP like equations for those

α(Pn+1 (x) + R(x))
if x ≤ sn+1 , points, using an interpolation of the value function for the
Cn+1 (x) =
α(Pn+1 (Sn+1 ) + R(x) − K) if x > sn+1 . points that are not on the grid. The equations can be then
solved recursively, obtaining the semi-optimal action to be
(10)
taken for each point of the grid, that can then be extended
The continuity of Cn+1 (x) descends from the continuby interpolation to obtain an approximate solution to the
ity of Pn+1 (x) and because by definition Pn+1 (sn+1 ) +
original problem.
R(sn+1 ) = Pn+1 (Sn+1 ) + R(sn+1 ) − K. To show it is

is also K-concave. Again using Lemma 1, if −R(x)
is concave, then by equation (5) Pn+1 (x) is K-concave.
The continuity of Pn+1 (x) is implied by the continuity of
Cn (x) and R(x).
Given that Pn+1 (x) is K-concave and continuous, the optimal action is to harvest down to Sn+1 if and only if the
current stock level is greater than sn+1 , so we have

The standard dynamic programming algorithm involves
O(|X||W ||U ||T |) arithmetic operations, where |X| is the
number of discretized states, |W | the number of possible
outcomes of the (discretized) uncontrollable events, |U | the
maximum number of possible discretized actions that can
be taken in any given state and T is the length of the time
horizon. However, the priori knowledge of the structure of
the optimal policy can be used to speed up the computation. In fact it is sufficient to find s (for example by bisection) and compute the optimal control associated with any
state larger than s to completely characterize the policy for
a given time step. The complexity of this latter algorithm
is O(|W ||U ||T | log |X|).

5

Case Study: the Pacific Halibut

As part of the ongoing effort in the emerging field of Computational Sustainability, we consider an application of our
framework to the Pacific Halibut fishery.
The commercial exploitation of the Pacific halibut on the
Northwestern coastline of North America dates back to the
late 1800s, and it is today one of the region’s largest and
most profitable fisheries.The fishery developed so quickly
that by the early 20th century it was starting to exhibit
signs of overfishing. After the publication of scientific reports which demonstrated conclusively a sharp decline of
the stocks, governments of the U.S. and Canada signed a
treaty creating the International Pacific Halibut Commission (IPHC) to rationally manage the resource. The IPHC
commission controls the amount of fish caught annually by
deciding each year’s total allowable catch (TAC), that is
precisely the decision variable hn of our optimization problem.
5.1

Management Problem Formulation

To develop a bioeconomic model of the fishery, we have
extracted data 2 from the IPHC annual reports on estimated
biomass xt , harvest ht and effort Et (measured in thousands of skate soaks) for Area 3A (one of the major regulatory areas in which waters are divided) for a 33 years period
from 1975 to 2007. To model the population dynamics, we
2

Data is available from the authors upon request.

200
Effort H1000 sk. soaksL , Stock H10^6 poundsL

As with all discretization schemes, we need to discuss the
consistency of the method. In particular, we would like
(uniform) convergence to the solution of the original problem in the limit as the discretization becomes finer. It is
well known that in general this property does not hold.
However in this case Theorem 1 guarantees the continuity
of Cn , that in turn implies the consistency of the method,
even if the policy itself is not continuous as a function of
the state([4]). Intuitively, discrepancies are possible only
around the threshold sn , so that they tend to disappear as
the discretization becomes finer.

hist. stock
150

est. stock
hist. effort
est. effort

100

50

0
1975

1980

1985

1990

1995

2000

2005

Year

Figure 1: Fitted models (11) and (13) compared to historical data (in bold).
consider the Beverton-Holt model that uses the following
reproduction function
xn+1 = f (sn ) = (1 − m)sn +

r0 sn
,
1 + sn /M

(11)

where sn = xn −hn is the stock remaining after fishing (escapement) in year n. This model can be considered as a discretization of the continuous-time logistic equation. Here,
parameter m represents a natural mortality coefficient, r0
can be interpreted as a reproduction rate and M (r0 −m)/m
is the carrying capacity of the environment. The (a priori)
mortality coefficient we use is m = 0.15, that is the current
working value used by the IPHC. The values of r0 and M
are estimated by ordinary least square fitting to the historical data. Estimated values thus obtained are reported in
table 1, while the fitted curve is shown in figure 1.
Parameter
q
b
p
K
c
δ
m
M
r0

Value
9.07979 10−7
2.55465
4, 300, 000$ / (106 pounds)
5, 000, 000$
200, 000$ / 1000 skate soaks
0.05
0.15
196.3923 106 pounds
0.543365

Table 1: Base case parameter set.
Following [18], we suppose that the system is affected by
stochasticity in the form of seasonal shocks wn that influence only the new recruitment part
xn+1 = f (sn , wn ) = (1 − m)sn + wn

r0 sn
. (12)
1 + sn /M

Instead of assuming an a priori probability distribution for
wn or trying to learn one from data (that in our case would
not be feasible given current scarce data availability), we
will make use of the framework developed in the previous
sections. In particular we will (a priori) assume that wn are
random variables all having the same finite support that we
will learn from data, but we will not make any assumption
on the actual weight distribution. With our data, we obtain
that wn ∈ [1 − 0.11, 1 + 0.06] = Iw .
For the economic part of the model, we start by modeling
the relationship between a harvest ht that brings the population level from xt to xt − ht and the effort Et needed to
accomplish this result. We will a priori assume that there is
a marginal effort involved, so that

5.2

Optimal Policy

By using the dynamic programming approach on the problem discretized with a step size of 0.25 × 106 pounds,
we compute the optimal policy for a management horizon of N = 33 years, that is the length of our original
time series. As predicted by Theorem 1, the optimal policy
π ∗ = {µ1 , . . . , µN } for the model we constructed for area
3A is a non stationary (S − s) policy. In figure 2(a) we plot
the function µ1 (·) to be used in the first year (the values of
S1 and s1 are 133 and 176.75 respectively). In words, the
optimal policy dictates that at period n a harvest is to be
undertaken if and only if the current stock level is greater
than sn ; in that case the stock is harvested down to Sn .
Optimal policy and escapement
500

Et =

Z

xt
xt −ht

1
dy
qy b

harvest
escapement

450

(13)

400

6

stock (10 pounds)

350
300
250
200
150

S

100
50
0

s
0

100

200

300

400

500

600

stock (106 pounds)

(a) Optimal rule for selecting harvests in the first year.
Optimal state and control trajectories
200
180
160
140

6

stock (10 pounds)

for some q and b. This is inspired by the fact that less effort is required when the stock is abundant, and can also
be interpreted as an integral of infinitesimal Cobb-Douglas
production functions (a standard economic model for productivity) where b and g are the corresponding elasticities.
Estimated values obtained by least squares fitting are reported in table 1, while the resulting curve is compared with
historical data in figure 1.
Costs involved in the Halibut fishery are divided into two
categories: fixed costs and variable costs. Fixed costs include costs that are independent of the number and the duration of the trips a vessel makes (therefore generically independent from the effort Et ). For example, vessel repairs
costs, license and insurance fees, mooring and dockage fees
are typically considered fixed costs. We will denote with
K the sum of all the fixed costs, that will be incurred if and
only if a harvest is undertaken.
Variable costs include all the expenses that are dependent
on the effort level. Variable costs typically include fuel,
maintenance, crew wages, gear repair and replacement. We
assume that the total variable costs are proportional to the
effort Et (measured in skate soaks) according to a constant
c. Parameter c is set to 200, 000$ for 1000 skate soaks
(200$/skate) as estimated in [2]. Following the analysis of
the historical variable and fixed costs for the halibut fishery
carried on in [11], we assume K = 5, 000, 000$ for area
3A. The unit price p for the halibut is set to 4, 300, 000$/
106 pounds, as in [2].
If we further assume a fixed discount rate δ = 0.05, we
obtain a formulation of management problem for the Halibut fishery in Area 3A that fits into the framework described in the previous section. In particular, the problem for an N years horizon is that of finding an admissible policy π = {µi }i∈[1,N ] that maximizes the revenue
π
CN
(x) where xRn is subject to (12), hn = µn (xn ) and
x
R(x) = px − c 0 qy1b dy.

stock
harvest

120
100
80
60
40
20
0

0

5

10

15
20
time (years)

25

30

35

(b) Stock trajectory and corresponding optimal harvests.

Figure 2: The optimal policy.
The trajectory of the system when it is managed using the
optimal policy is shown in figure 2, together with the corresponding optimal harvests. As we can see, the optimal policy is pulsing, in the sense that it involves periodic closures
of the fishery, when no harvest should be undertaken so that

the fish stock has time to recover. Of course, this kind of
policy could be acceptable in practice only in combination
with some rotation scheme among the different Areas, so
that a constant yearly production can be sustained.

Optimal state and control trajectories with rolling horizon
200
180
160
140

To see the advantage of the optimal (S − s) policy, we
compare it with the historical harvest proportions and with
a CPP policy that uses the historical average harvest rate
a = 0.1277. Table 2 summarizes the discounted revenues
corresponding to an initial stock size x1 = 90.989 million
pounds, that is the estimated stock size in 1975.
Policy
Optimal S − s
Historical rates
Average CPP
Rolling Horizon

Disc. revenue ($)
9.05141 × 108
7.06866 × 108
6.51849 × 108
8.73605 × 108

Loss ($)
−
1.98275 × 108
2.53292 × 108
3.1536 × 107

Table 2: Policy Comparison
Compared to the historical policy or the CPP policy, revenues for the optimal (S − s) policy are about 35% higher,
as reported in table 2. Notice that the comparison is done
assuming a worst case realization of the stochasticity, or in
other words that the nature is actively playing against the
manager.
Notice that the large harvest prescribed by the optimal
(S − s) policy in the last year is an artifact of the finite
horizon effect, caused by the fact that there is no reason
not to exhaust the resource at the end of the management
horizon (as long as it is profitable to harvest it). However
it does not affect the comparison significantly due to the
discount rate. In fact the (discounted) revenue for the entire last large harvest only accounts for less than 8% of the
total revenue. This is confirmed by looking at the results
obtained with a rolling horizon strategy that always picks
the optimal action with a 33-years long management horizon in mind. As shown in figure 3, this (suboptimal) strategy is not affected by the finite horizon effect. The rolling
horizon strategy still involves periodic closures of the fishery and significantly outperforms the historical policies, as
reported in table 2.
To further clarify that the pulsing nature of the optimal harvests is not an artifact of the finite horizon, it is also interesting to notice that the theoretical results on the optimality of (S − s) policies and the corresponding pulsing

stock (106 pounds)

This scheme is very different from the Constant Proportional Policy (CPP) that has been traditionally used to manage the Halibut fishery. In fact a CPP works by choosing
the yearly TAC as a fixed fraction of the current stock level
xt , and is aimed at maintaining the exploited stock size
(the escapement) at a given fixed level. This policy can
be seen as a simplified version of an (S − s) policy where
the two levels do not depend on the stage n and coincide,
thus defining the target stock size.

stock
harvest

120
100
80
60
40
20
0

0

5

10

15
20
time (years)

25

30

35

Figure 3: Harvests and stock trajectory with the rolling
horizon strategy.

harvests can be carried over to the infinite horizon case via
limiting arguments. The high level argument is that the optimal value function Cn (x) converges uniformly to C(x)
as n → ∞, while Pn (x) converges uniformly to a function
P (x) as n → ∞. Given that by Theorem 1 Pn (x) is continuous and K-concave for all n, we have that P (x) must be
also continuous and K-concave. Using an argument similar to the one developed in section 3.3 and by using Lemma
1, one can show that there exists S and s such that the optimal stationary policy for the infinite horizon problem is an
(S − s) policy.

6

Conclusions

In this paper, we have analyzed the optimality of (S−s) polices for a fairly general class of stochastic discrete-time resource allocation problems. When a non stationary (S − s)
policy is used, a harvest is undertaken at period n if and
only if the current stock level is greater than sn ; in that case
the stock is harvested down to Sn . The framework developed is quite general and can be applied to problems arising
in very different domains, such as natural resource management, crowdsourcing, pollution management. When assumptions of Theorem 1 are met, we have shown that there
exists a non stationary (S − s) policy that maximizes the
utility in a worst case scenario.
A fundamental advantage of the game theoretic approach
is that it completely avoids the problem of evaluating the
probability distributions of the random variables describing the uncertainty affecting those systems, a task that is
difficult or even impossible to accomplish in many practical circumstances. Given the consensus reached by the scientific community on the importance of understanding the
role of uncertainty when dealing with renewable resources,
we believe that worst-case scenario frameworks such as the

one described here provide new insights and will become
increasingly important.
To contribute to the effort of the Computational Sustainability community in tackling the fundamental sustainability challenges of our time, we consider an application of
our model to a marine natural resource. This type of natural resources are in fact widely believed to be endangered
due to over exploitation and generally poorly managed. Using Gulf of Alaska Pacific halibut data from the International Pacific halibut Commission (IPHC) annual reports,
we formulated a real world case study problem that fits into
our framework. In particular, our approach defines a policy
with a guaranteed lower bound on the utility function that is
structurally very different from the one currently employed.
As a future direction, we plan to study the effects of partial observability on the optimal policies by moving into a
POMDP framework. Moreover, we aim at extending the
results presented here to the multidimensional case by extending the theory on the so-called (σ, S) policies from the
inventory control literature.

7

Acknowledgments

This research is funded by NSF Expeditions in Computing
grant 0832782.


. According to the Erdős discrepancy conjecture, for any infinite ±1
sequence, there exists a homogeneous arithmetic progression of unbounded discrepancy. In other words, for any ±1 sequence
(x1 , x2 , ...) and a discrepancy C,
P
there exist integers m and d such that | m
i=1 xi·d | > C. This is an 80-year-old
open problem and recent development proved that this conjecture is true for discrepancies up to 2. Paul Erdős also conjectured that this property of unbounded
discrepancy even holds for the restricted case of completely multiplicative sequences (CMSs), namely sequences (x1 , x2 , ...) where xa·b = xa · xb for any
a, b ≥ 1. The longest CMS with discrepancy 2 has been proven to be of size
246. In this paper, we prove that any completely multiplicative sequence of size
127, 646 or more has discrepancy at least 4, proving the Erdős discrepancy conjecture for CMSs of discrepancies up to 3. In addition, we prove that this bound is
tight and increases the size of the longest known sequence of discrepancy 3 from
17, 000 to 127, 645. Finally, we provide inductive construction rules as well as
streamlining methods to improve the lower bounds for sequences of higher discrepancies.

Introduction
Discrepancy theory addresses the problem of distributing points uniformly over some
geometric object, and studies how irregularities inevitably occur in these distributions.
For example, this subfield of combinatorics aims to answer the following question: for
a given set U of n elements, and a finite family S = {S1 , S2 , . . . , Sm } of subsets of U ,
is it possible to color the elements of U in red or blue, such that the difference between
the number of blue elements and red elements in any subset Si is small?
Important contributions in discrepancy theory include the Beck-Fiala theorem [1]
and Spencer’s Theorem [2]. The Beck-Fiala theorem guarantees that if each element
appears at most t times in the sets of S, the elements can be colored so that the imbalance, or discrepancy, is no more than 2t
p− 1. According to the Spencer’s theorem, the
discrepancy of S grows at most as Ω( n log(2m/n)). Nevertheless, some important
questions remain open.
According to Paul Erdős himself, two of his oldest conjectures relate to the discrepancy of homogeneous arithmetic progressions (HAPs) [3]. Namely, a HAP of length
k and of common difference d corresponds to the sequence (d, 2d, . . . , kd). The first
conjecture can be formulated as follows:
*Submitted on April 14, 2014 to the 20th International Conference on Principles and Practice
of Constraint Programming.

2

Conjecture 1. Let (x1 , x2 , ...) be an arbitrary ±1 sequence. The discrepancy of x w.r.t.
HAPs mustPbe unbounded, i.e. for any integer C there is an integer m and an integer d
m
such that | i=1 xi·d | > C.

This problem has been open for over eighty years, as is the weaker form according
to which one can restrict oneself to completely multiplicative functions. Namely, f is
a completely multiplicative function if f (a · b) = f (a) · f (b) for any a, b. The second
conjecture translates to:
Conjecture 2. Let (x1 , x2 , ...) be an arbitrary completely multiplicative ±1 sequence.
The discrepancy of xP
w.r.t. HAPs must be unbounded, i.e. for any integer C there is a
m
m and a d such that | i=1 xi·d | > C.

Hereinafter, when non-ambiguous, we refer to the discrepancy of a sequence as its
discrepancy with respect
P to homogeneous arithmetic progressions. Formally, we denote
disc(x) = maxm,d | m
i=1 xi·d |. We denote E1 (C) the length for which any sequence
has discrepancy at least C + 1, or equivalently, one plus the maximum length of a
sequence of discrepancy C. Similarly, we define E2 (C) the length for which any completely multiplicative sequence has discrepancy at least C + 1. 1
A proof or disproof of these conjectures would constitute a major advancement in
combinatorial number theory [4]. To date, both conjectures have been proven to hold
for the case C ≤ 2. The values of E1 (1), E2 (1), and E2 (2) have been long proven to be
12, 10, and 247 respectively, while recent development proved E1 (2) = 1161 [5]. Konev
and Lisitsa [5] also provide a new lower bound for E1 (3). After 3 days of computation,
a SAT solver was able to find a satisfying assignment for a sequence of length 13, 000.
Yet, it would fail to find a solution of size 14, 000 in over 2 weeks of computation. They
also report a solution of length 17, 000, the longest known sequence of discrepancy 3.
In this paper, we substantially increase the size of the longest sequence of discrepancy
3, from 17, 000 to 127, 645. In addition, we claim that E2 (3) = 127, 646, making this
bound tight, as Plingeling was able to prove unsat and Lingeling generated an
UNSAT proof in DRUP format [6].
This paper is organized as follows. The next section formally defines the Erdos
discrepancy problems (for the general case and the multiplicative case) and presents
SAT encodings for both problems. We then investigate streamlined search techniques to
boost the search for lower bounds of these two problems, and to characterize additional
structures that appear in a subset of the solutions. Furthermore, in a subsequent section,
we provide construction rules that are based on these streamliners and allow to generate
larger sequences of limited discrepancy from smaller ones. The last section presents the
results of these approaches.

Problem Formulation
In this section, we first formally define the two conjectures as decision problems and
then propose encodings for these problems.
1

Note that, if Conjecture 1 (resp. Conjecture 2) were to be rejected, E1 (C) (resp. E2 (C) ) would
correspond to infinity.

3

Definition 1 (EDP1 ). Given
Pm two integers n and C, does there exist a ±1 sequence
(x1 , . . . , xn ) such that | i=1 xi·d | ≤ C for any 1 ≤ d ≤ n, m ≤ n/d.

Konev and Lisitsa [5] provide a SAT encoding for this problem that uses an automaton accepting any sub-sequence of discrepancy exceeding C. A state sj of the
automaton corresponds to the sum of the input sequence, while the accepting state
sB captures whether the sequence has exceeded the discrepancy C. A proposition
Pm−1
(m,d)
is true whenever the automaton is in state i=1 xi·d after reading the sequence
sj
(xd , . . . , x(m−1)d ). Let pi be the proposition corresponding to xi = +1. A proposition
that tracks the state of the automaton for an input sequence (xd , x2d , . . . , x⌊n/d⌋d ) can
be formulated as:
n/d

φ(n, C, d) =

(1,d)
s0

^ 

m=1

(m+1,d) 

(m,d)

∧ pid → sj+1

(m,d)

∧ pid → sj+1

^

sj

^

sj

−C≤j<C

∧

(m+1,d) 

−C<j≤C

∧

(m,d)


∧ pid → sB ∧


(m,d)
s−C ∧ pid → sB
sC

(1)

In addition, we need to encode that the automaton is in exactly one state at any point
in time. Formally, we define this proposition as:

χ(n, C) =

^

1≤d≤n/C,1≤m≤n/d



_

(i,d)

sj

^

∧

−C≤j≤C

(i,d)

sj1

−C≤j1 ,j2 ≤C



(i,d) 

∨ sj2

(2)

Finally, we can encode the Erdős Discrepancy Problem as follows:
EDP1 (n, C) : sB ∧ χ(n, C) ∧

n
^

φ(n, C, d)

(3)

d=1
(m,d)

of the automaton do not
Furthermore, as the authors of [5], the actual states sj
require 2C + 1 binary variables to represent the 2C + 1 values of the states. Instead,
one can modify this formulation and use ⌈log2 (2C + 1)⌉ binary variables to encode the
automaton states.
For the completely multiplicative case, we introduce additional constraints to capture the multiplicative property of any element of the sequence, i.e. xid = xi xd for any
1 ≤ d ≤ n, 1 ≤ i ≤ n/d. With respect to the boolean variables pi , pd and pid , such a
constraint acts as XNOR gate of input pi and pd and of output pid . Formally, we denote
this proposition M(i, d) and define:
M(i, d) = (pi ∨ pd ∨ pid ) ∧ (pi ∨ pd ∨ pid ) ∧ (pi ∨ pd ∨ pid ) ∧ (pi ∨ pd ∨ pid ) (4)

4

Importantly, for completely multiplicative sequences, the discrepancy of the subsequence (xd , ..., xmd ) of length m and common difference d will bePthe same as
m
the
discrepancy of the P
subsequence (x
Pm
P1m, ..., xm ). Indeed , we have | i=1 xi·d | =
m
| i=1 xi xd | = P
|xd | · | i=1 xi | = | i=1 xi |. Therefore, one needs only check that
m
the partial sums i=1 xi , 1 ≤ m ≤ n never exceed C nor go below −C. Furthermore,
note that a completely multiplicative sequence is entirely characterized by the values it
takes at prime positions, i.e. {xp |p is prime}. In addition, if there exists a completely
multiplicative sequence sequence (x1 , ..., xp−1 ) of discrepancy C with p prime, then
1Pm
the sequence (x1 , ..., xp−1 , (−1) i=1 xi ≥0 ) will also be a CMS of discrepancy C. As
a result, E 2 (C) cannot be a prime number.
Overall, for the completely multiplicative case, we obtain:
^
M(i, d)
(5)
EDP2 (n, C) : sB ∧ χ(n, C) ∧ φ(n, C, 1)
1≤d≤n,1≤i≤n/d

Streamlined Search
The encoding of EDP1 given in the previous section has successfully led to prove a
tight bound for the case C = 2 [5]. On an Intel Core i5-2500K CPU, it takes about 800
seconds for Plingeling [7] to find a satisfying assignment for EDP1 (1160, 2) and
less than 6 hours for Glucose [8] to generate a proof of E1 (2) = 1, 161. Nevertheless,
for the case C = 3, it requires more than 3 days of computation for Plingeling to
find a sequence of size n = 13, 000, and fails to find a sequence of size 14, 000 in over
two weeks of computation.
In this section, in order to improve this lower bound and acquire a better understanding of the solution space, we explore streamlining techniques that identifies additional
structure occurring in a subset of the solutions. Among the solutions of a combinatorial problem, there might be solutions that possess regularities beyond the structure of
the combinatorial problem itself. Streamlining [9] is an effective combinatorial search
strategy that exploits these additional regularities. By intentionally imposing additional
structure to a combinatorial problem, it focuses the search on a highly structured subspace and triggers increased constraint reasoning and propagation. This search technique is sometimes referred to as “tunneling” [10]. In other words, a streamlined search
consists in adding specific desired or observed regularities, such as a partial pattern that
appears in a solution, to the combinatorial solver. These additional regularities boost
the solver that may find more effectively larger solutions that contain these regularities.
If no solution is found, the observed regularities were likely accidental. Otherwise, one
can analyze these new solutions and suggests new regularities. This methodology has
been successfully applied to find efficient constructions for different combinatorial objects, such as spatially-balanced Latin squares [11], or graceful double-wheel graphs
[12].
When analyzing solutions of EDP1 (n, 2) for n ∈ [1, 1160], there is a feature that
visually stands out of the solutions. When looking at a solution as a 2D-matrix with
entries in {−, +} and changing the dimensions of the matrix, there seems to be clear
preferred matrix dimensions (say m-by-p) such that the m rows are mostly identical for

5

the columns 1 to p − 1, suggesting that xi = xi mod p for 1 ≤ i ≤ p − 1. We denote
period(x, p, t) the streamliner that enforces this observation and define:
period(x, p, t) : xi = xi mod p ∀1 ≤ i ≤ t, i 6≡ 0 mod p

(6)

First, while this observation by itself did not allow to improve the current best lower
bound for E1 (3), it led to the formulation of the construction of the next section. Second,
it also led to the re-discovery of the so-called ’improved Walters sequence’ [13], defined
as follows:


if i is 1 mod 3
+1,
µ3 (i) = −1,
(7)
if i is 2 mod 3


−µ3 (i/3), otherwise.

In the following, we denote walters(x, w) the streamliner imposing that the first w
elements of a sequence x follow the improved Walters sequence, i.e.:
walters(x, w) : xi = µ3 (i) ∀1 ≤ i ≤ w

(8)

One can easily see that the improved Walters sequence is a special case of the periodic sequence defined previously. Namely, for any sequence x where walters(x, w)
holds true, then we have period(x, 9, w).
Finally, another striking feature of the solutions of EDP1 (n, 2) is that they tend to
follow a multiplicative sequence. Interesting, EDP2 restricts EDP1 to the special case
of multiplicative functions and we observe for the case C = 2 that this restriction
substantially impacts the value of the best bound possible (i.e. E1 (2) = 1, 161 whereas
E2 (2) = 247). Nevertheless, the solutions of EDP1 (n, 2) exhibit a partial multiplicative
property and we define:
mult(x, m, l) : xi·d = xi xd ∀2 ≤ d ≤ m, 1 ≤ i ≤ n/d, i ≤ l

(9)

In the experimental section, we show the speed-ups that are triggered using these
streamliners, and how the best lower bound for EDP1 (n, 2) gets greatly improved.

Construction Rule
In this section, we show how we used insights from the period(x, p, t) streamliner in
order to generate an inductive construction rule for sequences of discrepancy C from
sequences of lower discrepancy.
Consider a sequence x that is periodic of period p, as defined in the previous section,
i.e. period(x, p, |x|) holds true, and is of length n = p ∗ k. Then, the sequence x can be
written as:
x = (y1 , y2 , . . . , yp−2 , yp−1 , z1
y1 , y2 , . . . , yp−2 , yp−1 , z2
...
y1 , y2 , . . . , yp−2 , yp−1 , zk )

(10)

6
′
Let C be
discrepancy
Pthe
Pmof z = (z1 , z2 , ..., zk ) and C the discrepancy of (y1 , ..., yp−1 ).
m
Given that i=1 xip = i=1 zi for any 1 ≤ m ≤ k, we have disc(x) ≥ C. Note that
if x was completely multiplicative, then it would hold disc(x) = C. We study the general case where x is not necessarily multiplicative, and investigate the conditions under
which disc(x) is guaranteed to be less or equal to C + C ′ .
For a given common difference d and length m, we consider the subsequence
p
. Given the definition 10 of x, the subsequence
(xd , x2d , ..., xmd ). Let q = gcd(d,p)
(xd , x2d , ..., xmd ) corresponds to:

(yd mod p , y2d mod p , ..., y(q−1)d mod p , zq ,
yd mod p , y2d mod p , ..., y(q−1)d mod p , z2q ,
yd mod p , ...)

(11)
(12)
(13)

Note that if p divides d or d divides p, this subsequence becomes (zq , z2q , ..., zqm )
and is of discrepancy at most C. As a result, a sufficient condition for x to be of discrepancy at most C + C ′ is to have yd mod p , y2d mod p , ..., y(q−1)d mod p of discrepancy
C ′ and summing to 0. We say that such a sequence has a discrepancy modp of C ′ .
Formally, we define the problem of finding such sequences as follows:
Definition 2 (Discrepancy mod p). Given two integers p and C ′ , does there exist a ±1
sequence (y1 , . . . , yp−1 ) such that:
|

m
X

yi·d mod p | ≤ C ′ , ∀1 ≤ d ≤ n, m <

i=1

p
gcd(d, p)

(14)

p
−1
gcd(d,p)

X

yi·d mod p = 0, ∀1 ≤ d ≤ n

(15)

i=1

Notice that, given the equation 15, p should be odd for such a sequence to exist.
We encode this problem as a Constraint Satisfaction Problem (CSP) in a natural way
from the problem definition. We provide the experimental results in the next section.

Results
All experiments were run on a Linux (version 2.6.18) cluster where each node has an
Intel Xeon Processor X5670, with dual-CPU, hex-core @2.93GHz, 12M Cache, 48GB
RAM. Unless otherwise noted, the results were obtained using the parallel SAT solver
Plingeling, version ats1 for the SAT encodings, and using IBM ILOG CPLEX
CP Optimizer, release 12.5.1 for the CP encodings.
First, we evaluate the proposed streamliners for the two problems. Table 1 reports
the length of the sequences that were successfully generated, as well as the computation
time. The first clear observation is that, for EDP1 , the streamlined search based on the
partial multiplicative property significantly boosts the search and allows to generate
solutions that appear to be out of reach of the standard search approach. For example,
while it takes about 10 days to find a solution of length 13, 900 without streamliners, the

7

streamlined search generates a substantially-large satisfying assignment of size 31, 500
in about 15 hours. Next, we study streamliners that were used for EDP2 , i.e. partially
imposing the walters sequence. The results clearly show the speed up triggered by the
combination of the new encoding for EDP2 with the walters streamliners. Interestingly,
the longest walters sequence of discrepancy 3 is of size 819. Nevertheless, one can
successfully impose the first 800 elements of the walters sequence and still expand it to
a sequence of length 108, 000. Furthermore, when imposing walters(730), it takes less
than 1 hour and an half to find a satisfying assignment for a sequence of size 127, 645.
Moreover, without additional streamliners, it takes about 60 hours to prove unsat for the
case 127, 646 and allows us to claim that this bound is tight. Nevertheless, the solver
generates a DRUP proof of size 335GB, which lies beyond the reach of traditional
checkers [6].

Encoding

EDP1

EDP2

Streamliners

Size of sequence Runtime (in sec)

mult(120,2000)
mult(150,2000)
mult(200,1000)
mult(700,10000)
mult(700,20000)

13,000
13,500
13,900
15,600
18,800
23,900
27,000
31,500

286,247
560,663
770,122
4,535
8,744
12,608
45,773
51,144

walters(800)
walters(800)
walters(700)
walters(730)

81,000
108,000
112,000
127,645

1,364
4,333
5,459
4,501

Table 1: Solution runtimes of searches with and without streamliners. The streamlined
search leads to new lower bounds for the 2 EDP problems.

In terms of the inductive construction described in the previous section, we can generate sequences whose discrepancy modp is 1, for p in 1, 3, 5, 7, and 9, while it also
generates sequences of discrepancy modp equal to 2 for p in 11, 13, 15, 17, 25, 27, 45,
and 81. Overall, this proves that one can take any sequence x of length |x| and discrepancy C and generate one of length 9|x| and of discrepancy C + 1, or of length
81|x| and of discrepancy C + 2. As a result, this provides a new bound for the case
of discrepancy 4, and proves E1 (4) > 9 ∗ 127645 = 1, 148, 805. Interestingly, such a
long sequence suggests that the proof of the Erdos conjecture for C > 3 may require
additional insights and analytical proof, beyond the approach proposed in this work.

Conclusions
In this paper, we address the Erdos discrepancy problem for general sequences as well
as for completely multiplicative sequences. We adapt a SAT encoding previously pro-

8

posed and investigate streamlining methods to speed up the solving time and understand additional structures that occur in some solutions. Overall, we substantially improve the best known lower bound for discrepancy 3 from 17, 001 to 127, 646. In addition, we claim that this bound is tight, as suggested by the unsat proof generated
by Lingeling. Finally, we propose construction rules to inductively generate longer
sequences of limited discrepancy.

Acknowledgments
This work was supported by the National Science Foundation (NSF IIS award, grant
1344201). The experiments were run on an infrastructure supported by the NSF Computing research infrastructure for Computational Sustainability grant (grant 1059284).



UCT has recently emerged as an exciting
new adversarial reasoning technique based on
cleverly balancing exploration and exploitation in a Monte-Carlo sampling setting. It
has been particularly successful in the game
of Go but the reasons for its success are not
well understood and attempts to replicate its
success in other domains such as Chess have
failed. We provide an in-depth analysis of
the potential of UCT in domain-independent
settings, in cases where heuristic values are
available, and the effect of enhancing random
playouts to more informed playouts between
two weak minimax players. To provide further insights, we develop synthetic game tree
instances and discuss interesting properties
of UCT, both empirically and analytically.

1

INTRODUCTION

The recent introduction of the Upper Confidence
bounds applied to Trees (UCT) method for adversarial game playing significantly improved the standard of
computer Go programs (Gelly and Silver, 2007, 2008).
In fact, it now appears that we may reach human-level
performance in Go within the next decade, which is
substantially sooner than anyone had predicted just
a few years ago. The current developments are especially surprising given that the traditional minimax
game tree search, which has yielded world-class play
in Chess and many other games, does not scale to the
domain of Go. Two issues hamper the application of
minimax search to Go: a very high branching factor
and the lack of a high-quality board evaluation function. A good board evaluation function is key in game
tree search when one cannot reach terminal states in
the game tree. UCT provides an effective way to address both these issues.

The UCT algorithm (Kocsis and Szepesvári, 2006) is
derived from a highly effective approach to solving
the multi-armed bandit problem called UCB1 (Auer
et al., 2002). The UCT sampling strategy strikes a
provably optimal balance between exploration of new
game states and exploitation, where lines of play that
appear promising are repeatedly searched to deeper
levels. This novel approach means that UCT can reach
regions of the search space that are much deeper than
the conventional iterative deepening minimax search,
which has been the “gold standard” for Chess and
other games. When UCT encounters a non-terminal
leaf node, a random (or weakly informed) playout is
typically used to provide some indication of the value
of the state. As the designers of UCT for Go have observed, it is somewhat counter-intuitive that there is
any useful information to be gained from having two
weak players play out the game to completion from
some intermediate state. After all, any real game between competent players will follow a very different
overall trajectory than one between weak players.
In Go, these properties of UCT have been very useful
and clearly alleviate some of the difficulties of doing a
standard minimax search: the more focused search can
go much deeper than any kind of iterative deepening
minimax search given the high branching factor of Go,
and the playouts provide useful board evaluation information, given that a good general board evaluation
function for Go is not known.
The success of UCT in Go raises the natural question
of whether UCT is also effective in other adversarial reasoning domains. We address this question by
studying UCT in the context of Chess as well as synthetic instances designed to highlight the key aspects
of UCT. We chose Chess as one of our evaluation domains mainly because standard minimax search works
so well for it. We can therefore study the behavior
of UCT and its two key new search concepts in detail
by comparing its performance with traditional minimax results as the “gold standard”. As we will see,

UCT per se is not competitive in Chess. However,
there are promising aspects of UCT that may be used
to complement more traditional search. We will also
identify what causes difficulties for UCT in Chess style
domains. Our results are applicable to any adversarial reasoning domain that has the characteristics we
identify.
After discussing the basics of UCT in Section 2, we
will empirically show in Section 3 that in domainindependent settings, UCT can easily outperform minimax search with a comparable amount of computational power. We will then describe in Section 4 how
the performance of UCT can be significantly boosted
when heuristic information is available and is used in
place of random playouts. The way we use heuristics is
much more direct than the “bootstrapping” approach
often used to initialize UCT leaf values. However, even
with a high-quality heuristic, UCT does not perform
well on Chess compared to a shallow minimax search.
This suggests that the different success rates of UCT
on Chess and Go is perhaps explained not so much by
the lack of good heuristics as by the intricate properties of the two underlying search spaces.

tify active nodes at the next critical level, and so on.
We study the “averaging” backup strategy employed
by UCT and show how it can make recovering from
early poor choices very tricky and expensive. This
suggests that other backup strategies may work better, but designing one needs further study. We also
allude to differences between single agent search as in
UCB1 (Auer et al., 2002), which has been the motivation for the multi-agent UCT algorithm, and multiagent scenarios. For example, while single agent sampling based search can easily break ties between several
good moves and “freeze” to one such good move, in
two-player minimax settings, the opponent constantly
keeps switching in the hope of finding a better move,
thus preventing the search from “freezing” onto a single principal variation. This results in exponential
scaling of UCT in two-player games that would not
occur in single-agent search.

2

BACKGROUND

Monte Carlo sampling techniques have been successfully applied in the past to produce expert-level play in
games of incomplete information such as Bridge (Ginsberg, 1999) and Scrabble (Sheppard, 2002). However,
they have seldom outperformed traditional adversarial planning techniques such as the minimax algorithm
in deterministic 2-player game settings such as Chess.
This changed recently with the emergence of UCT,
which was used to produce the first program capable
of master level play in 9x9 Go (Gelly and Silver, 2007,
2008), a domain which had thus far proven to be challenging for minimax presumably due to a large branching factor and lack of good heuristics. UCT has also
proved promising in new domains such as Kriegspiel
that were beyond the scope of any traditional planning techniques (Ciancarini and Favini, 2009), and in
general game playing (Finnsson and Björnsson, 2008).

We will then return to playouts and demonstrate
that playouts between slightly more informed players
than random players can lead to discovering information that is available only to a much deeper minimax
search. Not being able to discover such information
can lead to UCT falling into what we call soft traps; we
will show that soft traps are pervasive even in grandmaster games of Chess and will provide a concrete
example. Finally, we will turn our attention to the
case of synthetic instances designed to provide insights
into the behavior of UCT in practice, complementing known theoretical results about bandit-based sampling and UCT that provide worst case exponential
time convergence guarantees in the limit. We focus,
in contrast to existing analysis (e.g., Auer et al., 2002;
Gelly and Silver, 2007; Coquelin and Munos, 2007),
on simple cases such as binary trees with implanted
winning strategies of low complexity (i.e., few critical
moves) where UCT does work in practice, and provide a methodology to analyze the behavior of UCT
on such trees.

Selection: The algorithm selects an action a that
maximizes an upper confidence
bound
q on the action

n(s)
where
value: π(s) = arg maxa Q(s, a) + c log
n(s,a)

This part of the paper highlights and formalizes several
subtle aspects of UCT. For example, we show, both
empirically and analytically, that the time to convergence scales exponentially with the depth of the critical choice points in a winning strategy. In fact, we
provide an expression capturing the fact that the runtime of UCT can be decomposed additively into the
time it spends identifying certain “active” nodes at
the first critical level, then the time it needs to explore the sub-trees from these active nodes to iden-

Q(s, a) is the current estimate of the value of taking
action a at state s, n(s) is the total number of visits to state s over past iterations and n(s, a) is the
number of times action a was selected in past visits to
s. If n(s, a) = 0 for any action a, it is selected before
any other actions are re-sampled. The opposing player
symmetrically selects an action that minimizes a lower
confidence bound. The constant c determines how the
agent trades-off exploiting known good moves and exploring under-sampled ones; in our experiments with
Chess, this constant was fixed at 0.4 which produced

For two-player games, a single iteration of UCT starting at a state s comprises the following steps:

a good balance between the two strategies.
Estimation: The selection operator is repeatedly applied until a previously unvisited state is reached. If
this state is non-terminal, a default policy is typically
used to play out the game from the current position to
a terminal state with reward R (R could alternately
be a heuristic board evaluation) and the new state is
added to the tree. Thus, on each iteration, the size
of the tree grows by 1 node. In our experiments, the
default policy selects uniformly at random from the
available actions (unless noted otherwise).
Value Backup: The reward R from the current UCT
episode is used to update the values of all state-action
pairs on the path from the root to the fringe of the tree
by incrementing both n(s) and n(s, a) by 1, and incrementing Q(s, a) by (R − Q(s, a))/n(s, a). This update
assigns to each state-action pair the average reward
accrued from every episode that passed through it.

3

DOMAIN-INDEPENDENT
SETTINGS

We begin by exploring the extent to which UCT-style
search methods can compete with minimax search
in a fully domain-independent setting. This situation arises, for instance, in reasoning about quantified
Boolean formulas (QBF) where all we have as input
is a formula, without any information about the semantics of the variables or the specifics of the problem
domain the formula is encapsulating. This also happens in the general game playing setting (Finnsson and
Björnsson, 2008).
For our empirical exploration of the behavior of UCT
and minimax, we use the setting of Chess but modify
minimax to avoid using any Chess-specific heuristic
information, pretending that the domain is unknown.
Specifically, for k ≥ 1, MM-k-R denotes the minimax
player that performs a minimax search of depth k, uses
±1 values at a leaf if it corresponds to a terminal state,
and uses the outcome of a single random rollout if the
leaf corresponds to a non-terminal state. This produces a player that is aware of winning (losing) positions within its search horizon, but otherwise has the
same rollout style information as is available to UCT.
Experimental Setup. The results are reported in
Table 1, which gives the success rate of the column player against the row player. The success rate,
throughout this paper, is computed by assigning a
score of 0 to each game lost, 1 to each game won,
and 0.5 to each game that resulted in a draw. Note
that if m games are played between two players, the
sum of the success rates of the two players will be precisely m. Further, if each of players A and A’ wins 3/4

Table 1: UCT and a purely Random player compared
against minimax without domain knowledge. Table
reports the success rate of the column player against
the row (minimax) player.
Minimax
depth
#nodes
MM-2-R
1,000
MM-4-R
10,000
MM-6-R 200,000

UCT

Random

74%
94%
96%

6%
0%
0%

of the non-drawn games against B but A draws fewer
games, then the success rate of A will be higher than
A’ — a desirable property. In this and all experiments,
unless otherwise stated, we report the average success
rate over a total of 100 games played from the default
starting position of Chess, with 50 played as White
and 50 as Black. The variation amongst the games is
induced by the stochastic nature of (at least one of)
the players.
The players used for comparison are UCT with random playouts (UCT) and the “random” player that
simply selects a legal move uniformly at random. The
UCT player is given roughly the same amount of computation power, measured using the number of nodes
explored (rather than runtime, in order to discount any
implementation differences), as the minimax player it
is competing against. We observe that even though
MM-k-R acts without much information in many situations, it is far from a trivial player as evidenced by its
clear success against the random player. Also, searching deeper improves the performance of MM-k-R; not
only is the success rate of MM-6-R against the random
player higher than that of MM-2-R, in a direct playoff
(not shown in the table), MM-6-R has a success rate
of 66% against MM-2-R. Finally, UCT significantly
outperforms MM-k-R, demonstrating the potential of
UCT in completely knowledge-free settings.

4

BOOSTING UCT WITH
HEURISTIC INFORMATION

We now consider the setting where we do have prior
domain knowledge. We are interested in the extent to
which this can be exploited to enhance UCT. Heuristics have already provided promising results for Go.
Typically the heuristic value is used to initialize the
value of leaf nodes to bias the selection process in the
early iterations of the search. Nonetheless, since current heuristics in Go are not very strong, UCT is set up
to fairly quickly override the heuristic value with playout values once the node has been visited sufficiently
many (typically a few dozen) times. In contrast, for
Chess, we have heuristics that are much more pow-

erful, and we explore how much they can boost the
performance of UCT.
To evaluate this, we consider the player UCT-H that
uses the board evaluation heuristic of gnuchess at the
leaves visited by UCT, rather than the {−1, 0, +1} values obtained from random playouts; in other words, we
fully replace playouts with heuristic evaluations. This
still preserves the convergence properties of UCT, i.e.,
with sufficiently many iterations, UCT-H will converge
to the true minimax value of each node. We carefully rescaled the heuristic value to fall in the range
[−1, +1] by resetting the default checkmate valuation
of gnuchess to the observed maximum heuristic value
of a non-terminal node (6, 500). Out of other candidate rescaling schemes including sigmoidal functions,
this simple scheme worked the best.
Against a UCT player (with random playouts) that
was given 10, 000 iterations for convergence, we found
that UCT-H had a success rate of 55.0%, 85.5%, and
96.5% with only 50, 100, and 1, 000 iterations, respectively. Thus, not only is UCT-H significantly
faster then UCT per iteration (because it does not do
playouts and thus avoids relatively expensive repeated
move generation), it needs drastically fewer iterations
to be competitive with UCT.
A natural question to ask, then, is how well does UCTH actually compete as a player against minimax? Unfortunately, for games such as Chess where minimax
is the successful strategy, even UCT-H doesn’t fare
too well. We found that even with 50,000 iterations,
UCT-H is only about as powerful as MM-2, a 2-level
minimax search with the gnuchess heuristic. This suggests that the difference in the performance of UCT
in Go vs. Chess is not only due to the quality of the
heuristic but perhaps more importantly, due to the different nature of the two underlying search spaces and
how “winning” is defined in the two settings. Any successful sampling-based player for Chess must therefore
take these aspects into account.

5

ENHANCING RANDOM
PLAYOUTS

We now focus our attention on one of the two key aspects of UCT, random playouts, and ask whether such
playouts can provide useful information in domains
such as Chess where we already have well-designed
state evaluation heuristics. An interesting question in
the context of playouts is, is it at all possible to obtain useful information about a strong player by doing
several playouts between two weak players? We find
that random playouts tend not to provide any more
information than Chess heuristics themselves, but a

slightly more powerful playout—namely a playout between two MM-2 players—can, surprisingly, reveal information that is often visible only to a significantly
deeper minimax player such as MM-8. We quantify
this in terms of a strong correlation between move
rankings obtained by the two players.
Such information, visible only to relatively deep and
systematic minimax searches, can take the form of
traps as recently studied by us (Ramanujan et al.,
2010), where making the “wrong” move leads to a
state from which the opponent has a relatively simple winning strategy; such traps, even at surprisingly
shallow depths, were found to be abundant even in
grandmaster games of Chess. More generally, we consider here the notion of soft traps, where a wrong
move takes one to a game state from which the opponent has a guaranteed strategy for gaining significant
“advantage” in the game. This advantage could be
measured in terms of an evaluation function h for the
states. In our analysis of 50 complete grandmaster
games, we discovered that 52% of them had at least
one occurrence of a soft trap, i.e., a position where
an MM-8 and MM-2 search had a significant disagreement over the valuation of the best move. We now
make the notion of soft traps precise.
As a generalization of k-move winning strategies (Ramanujan et al., 2010), consider a heuristic state evaluation function h and a parameter ∆. Define a k-move
(h, ∆) advantage strategy starting from the current
state s as a length-k action sequence that results in a
board state s0 such that h(s0 ) ≥ h(s) + ∆. Note that
when ∆ is sufficiently large, this becomes a k-move
winning strategy.
Definition 1. Let G be a 2-player game with a heuristic evaluation function h, and ∆ > 0 be a constant.
The current player p at state s of G is said to be at
risk of falling into a soft trap if there exists a move
m from state s such that after executing m, the opponent of p has k-move (h, ∆) advantage strategy. The
state of the game after executing m is referred to as a
soft level-k search trap for p.
In Figure 1, we explore how good heuristics and various kinds of playouts are in obtaining information that
is visible to a strong player, such as a deep MM-k
player (for experimental purposes, we use MM-8 as
the gold standard). For this evaluation, we consider
boards taken from grandmaster games and compute
the ranking from best to worst (1 being the best) of
the possible moves as given by an MM-8 evaluation of
each resulting state. Note that during actual gameplay, only the relative ordering of moves matters; it is
for this reason that we choose to study the correlation
of the move rankings rather than their raw estimated
values. This also helps circumvent the problem of com-

Move Rank Correlation
25

MM−8H Rank

20

15

10

5

0
0

5

10

15

MM−2 Playout Rank

20

25

Move Rank Correlation
Gnuchess Heuristic
10k Random Playouts
1k Heuristic Playouts

25

MM−8 Rank

20

Figure 2: A board where playouts with MM-2 players
are able to discover a soft trap visible at depth 9 while
complete MM-2 search misses it.

15

10

5

0
0

5

10

15

Estimated Rank

20

25

Figure 1: Correlation of move rankings of various players (x-axis) against MM-8 rankings (y-axis). Top:
playouts using MM-2. Bottom: gnuchess heuristic,
random playouts, heuristic playouts.
paring leaf value estimation methods whose outputs do
not map to the same range of values. For each kind of
estimation method, we apply smoothing by considering estimates within some  of each other as ties and
assigning them the same rank.
Figure 1 shows the results for a typical grandmaster
board 16 moves (31 plys) deep into the play. In the
top panel, we compare for each child, its MM-8 ranking
(y-axis) against the ranking obtained based on playouts using two MM-2 players (x-axis). The points being almost on the diagonal shows that the two rankings are very well correlated, especially in the region
of most interest—the bottom-left region, representing
moves that are considered very good by both players. In contrast, the lower pane of the figure shows
that the rankings obtained using the gnuchess heuristic, random playouts, or playouts between heuristic
players (x-axis) are much more loosely correlated with
MM-8 rankings (y-axis). For example, points in the
top left corner represent moves that MM-8 thinks are
very poor but the other player thinks are quite good—
indicative of traps or soft traps missed by the weaker
player. Similarly, points in the bottom right corner
indicate good moves, as identified by MM-8, that are
dismissed as bad moves by the weaker player.
Overall, this demonstrates that playouts between
slightly informed players, namely MM-2 players in this
case, can have a strong correlation with information

that is usually visible only to a much stronger player,
namely MM-8 in this case. A natural question to
ask at this stage, how does the ranking induced by an
MM-2 search itself compare to that induced by a playout between two MM-2 players? We have discovered
that there are in fact situations in which a playout of
two MM-2 players uncovers information that an MM-2
search does not. Example 1 describes a concrete occurrence of this phenomenon.
Example 1. Consider the Chess board shown in Figure 2. We will follow the standard algebraic chess notation in our discussion, where rows (ranks) are labeled 1-8 and columns (files) are labeled a-h, with a1
being the bottom left corner. In the given state, the
Black king is in check with Black on move and an
MM-2 search recommends that the king be moved to
h8. However, this allows White a devastating countermove: moving its pawn on file f to f5 and thereby
trapping Black’s rook. Black can stall for two moves
by using its bishop to place the White king in check,
and subsequently freeing its rook to escape up file e. In
this case, White simply moves its own rook to the same
rank as the Black rook. This sets up a situation where
Black is at minimum forced to trade its queen and
rook for the White queen. Sub-optimal sequences of
play result in much costlier piece exchanges for Black.
The correct move in the original position is for Black
to move its pawn on file g to g6, thereby nullifying
White’s pawn threat—this is the move prescribed by
a complete MM-8 search, as well as an MM-2 playout.

6

INSIGHTS INTO UCT:
SYNTHETIC SEARCH SPACES

While UCT is easy to describe, it has a rich and
complex behavior on adversarial search spaces such
as those of Chess and Go. In order to better understand its behavior, we consider synthetic adversarial
search spaces where we vary, in a controlled manner,
key properties that affect the performance of UCT.

1000
5

10

600
400
200
0
20
10

Depth of Critical Node 2 (d2)

0

20

15

10

5

0

Depth of Critical Node 1 (d1)

Depth of Critical Node 1 (d1) = 0

Depth of Critical Node 2 (d2) = 22
# Iterations to Convergence (logscale)

800

# Iterations to Convergence

# Iterations to Convergence (x 102)

Convergence of UCT vs. Strategy Complexity

4

10

3

10

2

10

1

10

0

5

10

15

Depth of Critical Node 2 (d2)

20

25

4.9

10

4.8

10

0

5

10

15

Depth of Critical Node 1 (d1)

20

Figure 3: UCT convergence time as a function of the depths of the critical nodes. left: 3-D contour; middle: slice
with a fixed depth of critical node 1, in logscale; right: slice with a fixed depth of critical node 2, in logscale.
We study game trees with implanted winning strategies for the max player (denoted Max) who is on
move at the root node. The winning strategies are
parametrized by the number of critical decision nodes
and their depths. If Max makes the correct action
choice at every critical node, then regardless of the
actions chosen by either player at all other nodes, the
payoff at the end of the game is +1. If Max chooses an
incorrect action at any of the critical nodes, then the
payoff at the end of the game is drawn uniformly from
{−1, 0, +1}. This simple model captures the notion of
winning plans that exist in many tactical games like
Chess, where from a given state, a player can force a
win by executing a sequence of a few clever moves.
In these experiments, we are interested in the time
UCT takes to “discover” the winning strategy for Max,
which we define in terms of the utility assigned by
UCT to the root node. Once UCT has settled on a
winning sequence of moves for Max (i.e., a principal
variation), it will exploit it on subsequent iterations
and this will force the utility of the root node to approach +1. A subtle point is that the min player (denoted Min) might keep forcing Max to different principal variations; nonetheless, the paths will be equally
good for Max and the value of the root will still approach +1.
Formally, let v(t) be the utility assigned to the root
node of the search tree after t iterations of UCT. For
a single UCT search, we define the τ -convergence
point t∗ as the smallest t such that v(t) ≥ τ for all
t ≥ t∗ . We say that UCT has τ -converged if the
current iteration number is at least t∗ . Unless otherwise specified, we will simply use the term converged
to imply τ -convergence at the root with τ = 0.7.
6.1

EMPIRICAL OBSERVATIONS

Figure 3 illustrates how the time UCT takes to converge in the presence of 2-step winning strategies (i.e.,

strategies with 2 critical nodes) in a 24-level binary
tree varies as a function of the depths of the two critical nodes (hereafter referred to as d1 and d2 , with
d2 > d1 ). Note that in the mesh plot, the area of interest lies beyond the d1 = d2 line, towards the back
of the plot. The middle and right-most panels depict
slices of this surface obtained by fixing d1 and d2 , resp.
As seen in the middle panel, for a fixed d1 , the convergence time of UCT is essentially exponential in d2 .
The dependence of the convergence time on d1 is more
intriguing—with a fixed d2 , UCT appears to perform
best when d1 is slightly more than half of d2 . This
“dip” in the curve is captured by the following expression for the runtime of UCT, which we explain below:
U CT (d1 , d2 ) = a · C d1 /2 + b · 2d1 /2 · C (d2 −d1 )/2

(1)

where 2 < C < 3 (empirically 2.37) and a, b > 0 are
small constants. This expression fits the mesh plot
in Figure 3 very closely and highlights a key property
of UCT in the presence of multi-step winning strategies: The runtime of UCT can be decomposed additively into the time spent between consecutive critical
levels. Specifically, UCT first explores roughly 2d1 /2
“active” nodes at level d1 in time O(C d1 /2 ) , then explores each of the roughly 2d1 /2 subtrees below these
active nodes at level d1 in time O(C (d2 −d1 )/2 ) each to
identify roughly 2(d2 −d1 )/2 active nodes at level d2 in
each subtree, and so on down to other critical decision
levels. The quantity 2d1 /2 (in general, 2(di −di−1 )/2 per
subtree) representing “active” nodes is nothing but the
minimum number of nodes that Min can continually
force Max to explore until Max has figured out a winning sequence from all of these nodes. In general, we
can extend this reasoning to k critical decision levels,
suggesting that the runtime of UCT is captured by:
U CT (d1 , d2 , . . . , dk ) =
a · C d1 /2 + b · 2d1 /2 · U CT (d2 , . . . , dk )

(2)

Note that Max takes C d1 /2 iterations, and not 2d1 /2 , to

x1
A

B
y2

x2
y2
1-step
winning strategy
for Max

2-step
winning strategy
for Min

Figure 4: Synthetic binary trees with implanted winning strategies for both Max and Min.
identify the 2d1 /2 active nodes at level d1 that Min can
force it to. This is because, although Max ideally has
the choice to “freeze” to any one of its equally good
children, the exploration constant forces Max to explore to some extent the other child as well, especially
during the initial few visits to that node. Nevertheless,
the overall time is much less than the size of the full
search tree till this level, which is 2d1 or 4d1 /2 .
In our second experiment, we study a more complex
scenario where both Max and Min have implanted
strategies and a few initial samples provide incorrect
guidance at the root (see Figure 4). In particular, we
study binary trees of depth 20 where Max has critical
nodes at depths (x1 , x2 ) where x1 = 0, 2 ≤ x2 ≤ 18,
and x2 is even, and Min has critical nodes at (y1 , y2 )
where 1 ≤ y1 , y2 ≤ 19 and y1 , y2 are odd. In order to
win, Max must move left at the root and again at level
x2 ; if Max goes right at the root, then Min can force
a win by going left at levels y1 and y2 (i.e., the right
child of the root is a trap state for Max). Let A and B
be the subtrees rooted at the left and right children of
the root node respectively. We bias the values of the
leaves that are not on a winning path for either player
such that the average of the values of the leaves in A is
0, while the average of the values of the leaves in B is
0.5. Thus, the B subtree, though ultimately a losing
proposition for Max (assuming optimal play by Min)
will look more promising with limited sampling. We
now ask the question, how do the depths of the strategies for the two players influence UCT’s convergence
time?
Table 2 presents our findings based on an average of
100 UCT runs on a fixed tree. On its first few iterations, UCT receives extremely noisy estimates of the
utilities of its two children at the root. In the best or
“favorable” case, these initial estimates correlate correctly with the true utilities of the children and Max
chooses to explore subtree A first. In the unfavorable
case, the child rankings are reversed and Max chooses
to explore subtree B first. Note that any ties will eventually resolve one way or the other, and at that point,

Table 2: Effects of the depths of Max and Min’s strategies on UCT’s convergence time. ‘F’ and ’U’ denote
instances with favorable and unfavorable initial estimates, respectively.
Max’s Strategy
Depth
Shallow
Mid-level
Deep

Min’s Strategy
Shallow Mid-level
F
U
F
U
36 148 77 610
950 1000 1500 1900
16k 16k 17k 17k

Depth
Deep
F
U
560 3800
7900 13k
30k 33k

Table 3: Effects of the depths of Max and Min’s strategies on the distribution of visits to the right subtree.
Max’s Strategy
Depth
Shallow
Mid-level
Deep

Min’s
Shallow
F
U
33% 21%
3% 4%
0.2% 0.3%

Strategy Depth
Mid-level
Deep
F
U
F
U
34% 30% 40% 36%
14% 16% 31% 31%
1.8% 2% 16% 16%

we fall back on one of these two cases.
There are a number of interesting trends in Table 2.
First, when estimates are unfavorable at the root,
the time to convergence is greater as UCT initially
“wastes” time in subtree B until it (at least partially)
uncovers Min’s winning strategy. Second, this gap in
convergence time is most pronounced when either Max
has a shallow winning strategy or Min has a deep winning strategy. This too makes sense; in the former
case, UCT can uncover Max’s strategy very quickly if
given the chance, and hence the time “wasted” in subtree B counts relatively much more; in the latter case,
UCT simply needs to work harder to uncover Min’s
winning strategy and switch to subtree A.
Finally, we note that increasing the depth of Min’s
strategy slows down UCT’s convergence even in the
favorable instances. The data in Table 3, which shows
the average percentage of time UCT spends in subtree B during the runs presented in Table 2, helps
explain this phenomenon. As Min’s strategy is implanted deeper down in the tree, UCT spends more
time exploring subtree B. A by-product of this repeated sampling from B is that the estimated utility
of the root node is now heavily biased by the samples
drawn from B; when UCT eventually switches to subtree A and discovers Max’s winning strategy, it needs
to work extra hard to overcome this bias and reinforce
the true utility of the root (we will formalize this in
Section 6.2, equation (3)). This is illustrated by the
fact that when both Max and Min have shallow strategies, when UCT converges, the root node of subtree A

T

has a typical utility estimate of 0.720; when both have
deep strategies, the root node of subtree A needs to
reach a much higher target value of 0.850.
This highlights an important shortcoming of UCT,
namely that it can be overly optimistic in its estimates
of state utilities, that lead it on wild goose chases. By
the time it discovers that an action it has been exploring is sub-optimal, nodes higher up the tree have been
reinforced with so many samples that it faces an uphill
task in changing these estimates. In the face of computational constraints (for example, in a timed gameplaying setting such as Blitz Chess), this is particularly
troublesome for it means that UCT could easily have
spent its time exploring sub-optimal moves and thus
faces a very real risk of falling into a trap state.
6.2

ANALYTICAL INSIGHTS

While a few attempts have been made to analyze bandit based sampling methods in general and UCT in
particular (e.g., Auer et al., 2002; Gelly and Silver,
2007; Coquelin and Munos, 2007), these analyses are
based on the worst case scenario and, in essence, boil
down to showing that an exponential (or even superexponential (Coquelin and Munos, 2007)) number of
iterations are necessary and sufficient for UCT to converge to true minimax values. These exponential time
convergence results, while intricate and interesting, do
not explain the success of UCT in practice in domains
such as Go, with a practically limited number of iterations available during game play. In contrast, our goal
in this section is to provide a methodology for analyzing some simple scenarios where UCT does work, and
obtain insights into its runtime behavior. Specifically,
we will consider 2-step winning strategies implanted in
binary trees.
We highlight three take-away messages, some of which
have previously been observed empirically and are derived here analytically: (a) the averaging backups of
UCT can make recovering from poor early choices very
costly; (b) UCT in two-player settings scales exponentially with the depth of the critical choice points,
whereas in single-player settings, all that matters is the
number of critical choice points, not their depth; and
(c) the tension between exploration and exploitation
as controlled by the exploration constant.
In order to make the analysis easier while still retaining
the key aspects of UCT, we work with a modified version of the algorithm in this section. Instead of implementing the UCB1 exploration-exploitation strategy,
we will use an -greedy version of the algorithm, where
 ∈ [0, 1] is a constant determining how often subobtimal moves are explored. Specifically, when exploring a node for the first few times, UCT simply visits

T

  T R
  
  
 

TL

all +1

p fraction
+1’s
(a)





 













 























 














 



 
 





 
 


q fraction
+1’s

+1
(b)

Figure 5: Synthetic binary trees with implanted winning strategies for Max. (a) 1-step winning strategy.
(b) 2-step winning strategy.
all children once (a “round”), as usual. However, after
this round, it selects an optimal branch (breaking ties
at random) with probability 1 −  and a sub-optimal
branch (breaking ties at random) with probability .
Auer et al. (2002) showed that this simpler variant
of UCB1 also has similar good convergence properties
(in the limit), as long as  decreases linearly with the
number of times the node is visited.
We make one further modification, where instead of
dealing with tie-breaking, we assume that rounds similar to the first round are repeated (i.e., all children explored in each round) until ties are broken. For binary
trees, which will be the main focus of this section, this
modification does not make a significant difference.
6.2.1

Scenario A

For ease of illustration, we start with the simplest case
and build upon it. Consider a binary game tree T with
Max on play at the top node. Let T L and T R denote
the left and right subtrees, respectively, of T . Suppose
that all leaves of T L are labeled +1, i.e., Max has a
sure win if he makes the left move. Suppose also that
a p fraction of the leaves, where p ∈ [0, 1), of T R are
labeled +1 and the rest are labeled −1. This tree is
depicted in Figure 5(a), with bold edges corresponding
to winning strategy moves. How long does it take for
UCT to identify the left branch as the winning move?
In a given round at the root node of T , a playout from
the left child always leads to +1 while a playout from
the right child leads to +1 with probability p. Therefore, we have a tie with probability p and it follows
that the expected number of rounds needed to break
the tie is 1/(1 − p). Hence, the total number of visits needed to the root node of T in expectation equals
2/(1 − p) (as there are 2 visits per round) plus the
time it takes for UCT to converge at the left child after the tie is broken. Note that the only way for the
tie to be broken in this tree is to have all +1 playouts

on the left and exactly one −1 playout on the right,
implying that the left move will necessarily be identified as the optimal move when the tie is broken. (This
will not be the case in general, as we discuss later.)
From this point on, T L will be visited a 1 −  fraction of the times the root node of T is visited. Let
C(τ, value, iter ) denote the number of visits needed to
the winning strategy node (in this case the root node
of T L ) for UCT to τ -converge at the root node of T ,
where value denotes the current value of the node and
iter denotes the number of visits already made to the
node; due to the “averaging” backups of UCT, the
current state of the node significantly affects the time
to convergence even after a winning strategy has been
identified, and we will quantify this shortly. The number of visits needed to the root of T is therefore roughly
2/(1 − p) + C(τ, value, iter )/(1 − ).
How do we determine C(τ, value, iter )? In the unlikely
case that the current value, value, is already at least
as good as τ (i.e., value ≥ τ for Max), this quantity is
0. Otherwise, assuming subsequent visits explore the
identified winning strategy, resulting in all +1 playout
values, the averaging nature of backups dictates that:
C(τ, value, iter ) × 1) + (iter × value)
=τ
C(τ, value, iter ) + iter
=⇒

τ − value
C(τ, value, iter ) = iter ×
1−τ

(3)

In our case, iter ≈ 1/(1 − p) and value = 1 − 2/iter
as all but the very last round should result in playout
values of +1. Plugging these values in, the number of
visits to the root node of T till convergence is roughly:
2
1 + τ − 2p
+
1 − p (1 − )(1 − p)(1 − τ )
Remark 1. Equation (3) points out an interesting
limitation of UCT that we have already encountered
near the end of Section 6.1, namely, that the averaging
backups of UCT can make recovering from poor early
choices very expensive. In particular, if iter is high
and value is too low (for Max), then UCT will take a
long time to make up for its mistakes before it reaches
τ . This suggests there might be other backup strategies, although finding an effective alternative backup
strategy requires further study because natural choices
such as simple minimaxing tend to be very brittle.
6.2.2

Scenario B

We now explore the “tension” between having a small
value for the exploration constant, , and a large value.
This example will also illustrate that the depth of the
critical nodes of a winning strategy exponentially influences the number of iterations needed for convergence.

This is in stark contrast to k-step winning strategies
in single-player settings, where it is easy to argue that
the depth of the critical choice points is immaterial,
and all that matters is the number of critical choice
points. Intuitively, the difference between the single
player and two players settings is that in the former
case, since all choices look equally good (or bad) at
non-critical points, the player can arbitrarily “freeze”
on one of them and keep exploiting it, while in the two
player setting, the opponent prevents this freezing by
continually forcing the winning player to different areas of the search space in the hope of avoiding defeat.
For example, for a depth d winning strategy, the losing
player can force the other player to explore precisely
2d/2 paths.
Suppose that T is modified so that the strategy embodied by T L in Scenario A is actually hidden deeper
and that Max needs to make one good move to get to
this strategy. Specifically, we now have a 2-step winning strategy for Max, with critical moves at levels 0
and 2, with the subtrees at level 2 being identical to
the ones in Scenario A. Also, let us suppose that the
right subtree of the root node has a fraction q of +1
leaves, which will affect tie breaking at the root. This
is depicted in Figure 5(b).
Given the expression derived above for Figure 5(a) for
the number of times we need to visit each of these subtrees at level 2 in order to identify the winning strategy
from there on, how many times do we need to visit the
root node of the tree to achieve this? First, consider
a node X one level above a winning strategy at level
2. Min is on move at X, which means that as soon
as Max begins to identify the winning strategy on the
left branch of X, Min has an incentive to switch to
the right branch of X (i.e., what’s good for Max is
bad for Min). In other words, Min will keep switching between the two choices until Max has figured out
the winning strategy under both choices of Min. This
means that the number of visits to X that we need
is twice the number of visits to each of T L ; in general, when Max’s winning strategy is at depth d, the
number of visits needed will be 2d times the number
of visits to any single “winning” subtree at level d—
hence the exponential scaling with the depth of the
winning strategy.
Further, the tie at the root node of T may now be
broken in favor of the right child as well, as there are
leaves labeled −1 on both sides. If the tie breaks in
favor of the left child (the “favorable” case), then the
number of iterations needed after breaking the tie is:


2
C(τ, value, iter )
2
×
+
D(favorable) ≈
1−
1−p
1−
the latter part of which is similar to Scenario A, mul-

tiplied by 2 for twice the work that needs to be done
due to Min’s choice at level 2, and divided by (1 − )
since in the favorable case we will visit the left subtree
of T this fraction of the times we visit the root node
of T .
More interestingly, when the tie at the root is incorrectly broken in favor of the right hand side child at
the root (the “unfavorable” case), the left subtree is
visited only an  fraction of the time, implying that
many more visits to the root node are needed in order
to achieve the same number of visits as before to the
strategy nodes at level 2. Specifically, the number of
iterations needed after breaking the tie is:
2
D(unfavorable) = ×




2
C(τ, value, iter )
+
1−p
1−

(f(x))

num iterations in unlucky case

350
300
250
200
150
100
50
0

0

0.2

0.4

0.6

0.8

1

epsilon

Figure 6: The effect of varying  on convergence time.
Additionally, we must consider the time to break the
tie at the root node, which is slightly more complex
than in Scenario A. The fraction of +1 labeled leaves
on the right is q and on the left is p0 = 3p/4. Therefore,
the probability of a tie is p0 q (when both playouts yield
+1) plus (1−p0 )(1−q) (when both playouts yield −1),
giving p0 +q−2p0 q. Thus, the expected number of visits
before the tie is broken is 2/(p0 + q − 2p0 q). Further,
when this happens, the tie is broken in favor of the
left subtree with probability p0 (1 − q) and in favor of
the right subtree with probability (1 − p0 )q. Putting
all this together, we have the following expression for
the rough number of visits needed to the root node:
p0 (1 − q) × D(favorable)
2
+
p0 + q − 2p0 q
p0 + q − 2p0 q
(1 − p0 )q × D(unfavorable)
+
p0 + q − 2p0 q

CONCLUSION

This work provides insights into the behavior of UCT
and extends its analysis to complement known worst
case (super-)exponential convergence results. We
studied UCT in domains such as Chess where traditional minimax search is very effective. Our results demonstrate that UCT consistently beats minimax in domain-independent settings, that it can be
significantly boosted by incorporating a state evaluation function, and that more informed playouts can
enhance performance. Finally, our results on synthetic
instances with implanted strategies revealed an interesting pattern in the convergence behavior of UCT.



This illustrates, in a concrete fashion, the tension between small and large values of , when the goal is
to minimize the number of visits to the root node to
achieve convergence; see Figure 6 for an illustration
where p = 0.5 and the C value is taken to be 10.
400

7

Acknowledgments
Supported by NSF (Expeditions in Computing award for
Computational Sustainability, 0832782; IIS grant 0514429)
and IISI, Cornell Univ. (AFOSR grant FA9550-04-1-0151).


— Many tasks in human environments require performing a sequence of navigation and manipulation steps
involving objects. In unstructured human environments, the
location and configuration of the objects involved often change
in unpredictable ways. This requires a high-level planning
strategy that is robust and flexible in an uncertain environment.
We propose a novel dynamic planning strategy, which can be
trained from a set of example sequences. High level tasks are
expressed as a sequence of primitive actions or controllers (with
appropriate parameters). Our score function, based on Markov
Random Field (MRF), captures the relations between environment, controllers, and their arguments. By expressing the environment using sets of attributes, the approach generalizes well
to unseen scenarios. We train the parameters of our MRF using
a maximum margin learning method. We provide a detailed
empirical validation of our overall framework demonstrating
successful plan strategies for a variety of tasks.1

I. I NTRODUCTION
When interacting with a robot, users often under-specify
the tasks to be performed. For example in Figure 5, when
asked to pour something, the robot has to infer which cup
to pour into and a complete sequence of the navigation and
manipulation steps—moving close, grasping, placing, and so
on.
This sequence not only changes with the task, but also
with the perceived state of the environment. As an example,
consider the task of a robot fetching a magazine from a desk.
The method to perform this task varies depending on several
properties of the environment: for example, the robot’s
relative distance from the magazine, the robot’s relative
orientation, the thickness of the magazine, and the presence
or the absence of other items on top of the magazine. If
the magazine is very thin, the robot may have to slide the
magazine to the side of the table to pick it up. If there is
a mug sitting on top of the magazine, it would have to
be moved prior to the magazine being picked up. Thus,
especially when the details of the manipulation task are
under-specified, the success of executing the task depends
on the ability to detect the object and on the ability to
sequence the set of primitives (navigation and manipulation
controllers) in various ways in response to the environment.
In recent years, there have been significant developments
in building low-level controllers for robots [34] as well as
in perceptual tasks such as object detection from sensor data
[20, 11, 35]. In this work, our goal is to, given the environment and the task, enable robots to sequence the navigation
Jaeyong Sung, Bart Selman and Ashutosh Saxena are with the Department of Computer Science, Cornell University, Ithaca, NY. Email:

{jysung,selman,asaxena}@cs.cornell.edu
1 A preliminary version of this work was presented at ICML workshop
on Prediction with Sequential Models, 2013 [33].

Fig. 1.
Figure showing our Kodiak PR2 in a kitchen with different
objects labeled with attributes. To accomplish the under-defined task of
pour(obj17), it has to first find the mug (obj13) and carry it to the table
(obj05) since it is dangerous to pour liquid in a tight shelf. Once the mug
is on the table, it has to bring the liquid by the container (obj19) and then
finally pour it into the mug.

and manipulation primitives. Manually sequencing instructions is not scalable because of the large variety of tasks and
situations that can arise in unstructured environments.
In this work, we take an attribute-based representation of
the environment, where each object is represented with a set
of attributes, such as their size, shape-related information,
presence of handles, and so forth. For a given task, there
are often multiple objects with similar functions that can be
used to accomplish the task, and humans can naturally reason
and choose the most suitable object for the given task [17].
Our model, based on attribute representation of objects, is
similarly capable of choosing the most suitable object for
the given task among many objects in the environment.
We take a dynamic planning approach to the problem
of synthesizing, in the right order, the suitable primitive
controllers. The best primitive to execute at each discrete
time step is based on a score function that represents the
appropriateness of a particular primitive for the current state
of the environment. Conceptually, a dynamic plan consists
of a loop containing a sequence of conditional statements
each with an associated primitive controller or action. If
the current environment matches the conditions of one of

the conditional statements, the corresponding primitive controller is executed, bringing the robot one step closer to
completing the overall task (example in Section III). We
will show how to generalize sequencing of primitives to
make them more flexible and robust, by switching to an
attribute-based representation. We then show how to unroll
the loop into a graph-based representation, isomorphic to a
Markov Random Field. We then train the parameters of the
model by maximum margin learning method using a dataset
comprising many examples of sequences.
We evaluated our model on 127 controller sequences for
five under-specified manipulation tasks generated from 13
environments using 7 primitives. We show that our model
can predict suitable primitives to be executed with the correct
arguments in most settings. Furthermore, we show that, for
five high-level tasks, our algorithm was able to correctly
sequence 70% of the sequences in different environments.
The main contributions of this paper are:
•
•
•

using an attribute-based representation of the environment for task planning,
inferring the sequence of steps where the goals are
under-specified and have to be inferred from the context,
a graph-based representation of a dynamic plan by
unrolling the loop into a Markov Random Field.
II. R ELATED W ORK

There is a large body of work in task planning across
various communities. We describe some of them in the
following categories.
Manual Controller Sequencing. Many works manually
sequence different types of controllers to accomplish specific
types of tasks. Bollini et al. [4] develop an end-to-end system
which can find ingredients on a tabletop and mix them
uniformly to bake cookies. Others used pre-programmed
sequences for tea serving and carrying humans in healthcare
robotics [25, 24]. These approaches however cannot scale
to large number of tasks when each task requires its own
complicated rules for sequencing controllers and assumes a
controlled environment, which is very different from actual
human households, where objects of interest can appear anywhere in the environment with a variety of similar objects.
Beetz et al. [2] retrieve a sequence for “making a pancake”
from online websites but assumes an environment with
correct labels and a single choice of object for the task.
Human experts can generate finite state machines for robots
but this again requires explicit labels (e.g. AR tags) [27]. Our
work addresses these problems by representing each object
in the environment as a set of attributes which is more robust
than labeling the individual object [7, 6, 22]. In our recent
work [23], we learn a sequence given a natural language
instruction and object labels, where the focus is to learn the
grounding of the natural language into the environment.
Learning Activities from Videos. In the area of computer
vision, several works [37, 38, 32, 19] consider modeling the
sequence of activities that humans perform. These works are
complementary to ours because our problem is to infer the
sequence of controllers and not to label the videos.

Symbolic Planning. Planning problems often rely on symbolic representation of entities as well as their relations. This
has often been formalized as a deduction [9] or satisfiability
problem [16]. A plan can also be generated hierarchically by
first planning abstractly, and then generating a detailed plan
recursively [15]. Such approaches can generate a sequence of
controllers that can be proven to be correct [14, 3]. Symbolic
planners however require encoding every precondition and
effect of each operation, which will not scale in human
environments where there are large variations. Such planners
also require domain description for each planning domain
including the types of each object (e.g., pallet crate - surface,
hoist surface - locatable) as well as any relations (e.g., on
x:crate y:surface, available x:hoist). The preconditions and
effects can be learned directly from examples of recorded
plans [36, 39] but this method suffers when there is noise in
the data [39], and also suffers from the difficulty of modeling
real world situations with the PDDL representation [36].
Such STRIPS-style representation also restricts the environment to be represented with explicit labels. Though
there is a substantial body of work on labeling human
environments [20, 21], it still remains a challenging task.
A more reliable way of representing an environment is
representing through attributes [7, 6]. An attribute-based
representation even allows classification of object classes that
are not present in the training data [22]. Similarly, in our
work, we represent the environment as a set of attributes,
allowing the robot to search for objects with the most suitable
attributes rather than looking for a specific object label.
Predicting Sequences. Predicting sequences has mostly been
studied in a Markov Decision Process framework, which
finds an optimal policy given the reward for each state.
Because the reward function cannot be easily specified
in many applications, inverse reinforcement learning (IRL)
learns the reward function from an expert’s policy [26].
IRL is extended to Apprenticeship Learning based on the
assumption that the expert tries to optimize an unknown
reward function [1]. Most similar to our work, the MaxMargin Planning frames imitation learning as a structured
max-margin learning problem [28]. However, this has only
been applied to problems such as 2D path planning, grasp
prediction and footstep prediction [29], which have much
smaller and clearer sets of states and actions compared to
our problem of sequencing different controllers. Co-Active
Learning for manipulation path planning [10], where user
preferences are learned from weak incremental feedback,
does not directly apply to sequencing different controllers.
Both the model-based and model-free methods evaluate
state-action pairs. When it is not possible to have knowledge
about all possible or subsequent states (full backup), they
can rely on sample backup which still requires sufficient
sample to be drawn from the state space [8]. However, when
lots of robot-object interactions are involved, highly accurate
and reliable physics-based robotic simulation is required
along with reliable implementation of each manipulation
controllers. Note that each of the manipulation primitives
such as grasping are still not fully solved problems. For
example, consider the scenario where the robot is grasping

the edge of the table and was given the instruction of
follow traj pour(table,shelf). It is unclear what
should occur in the environment and becomes challenging to
have reliable simulation of actions. Thus, in the context of
reinforcement learning, we take a maximum margin based
approach to learning the weight for wT φ(s, a) such that it
maximizes the number of states where the expert outperforms other policies, and chooses the action that maximizes
wT φ(s, a) at each time step. The key in our work is representing task planning as a graph-based model and designing
a score function that uses attribute-based representation of
environment for under-specified tasks.
III. O UR A PPROACH
We refer to a sequence of primitives (low-level navigation
and manipulation controllers) as a program. To model the
sequencing of primitives, we first represent each object in
the environment with a set of attributes as described in
Section IV-B. In order to make programs generalizable,
primitives should have the following two properties. First,
each primitive should specialize in an atomic operation such
as moving close, pulling, grasping, and releasing. Second, a
primitive should not be specific to a single high-level task.
By limiting the role of each primitive and keeping it general,
many different manipulation tasks can be accomplished with
the same small set of primitives, and our approach becomes
easily adaptable to different robots by providing implementation of primitives on the new robot.
For illustration, we write a program for “throw garbage
away” in Program 1. Most tasks could be written in such a
format, where there are many if statements inside the loop.
However, even for a simple “throw garbage away” task, the
program is quite complex. Writing down all the rules that
can account for the many different scenarios that can arise
in a human environment would be quite challenging.

more robust alternative. At each step, the current state of the
environment is considered and the next appropriate action
is selected by one of the conditional statements in the main
loop. A well-constructed dynamic plan will identify the next
step required to bring the robot closer to the overall goal in
any possible world state. In complex domains, dynamic plans
may become too complicated. However, we are considering
basic human activities, such as following a recipe, where
dynamic plans are generally quite compact and can effectively lead the robot to the goal state. Moreover, as we will
demonstrate, we can learn the dynamic plan from observing
a series of action sequences in related environments.
In order to make our approach more general, we introduce
a feature based representation for the conditions of if
statements. We can extract some features from both the
environment and the action that will be executed in the
body of if statement. With extracted features φ and some
weight vector w for each if statement, the same conditional
statements can be written as wT φ, since the environment will
always contain the rationale for executing certain primitive.
Such a feature-based approach allows us to re-write Program
1 in the form of Program 2.
Program 2 “throw garbage away.”
Input: environment e, trash a1
gc = f ind garbage can(e)
repeat
et = current environment
if w1T φ(et ,release(a1 )) > 0 then
release(a1 )
else if w2T φ(et ,move close(gc)) > 0 then
move close(gc)
..
.

else if wnT φ(et ,move close(a1 )) > 0 then
move close(a1 )
end if
until a1 inside gc

Program 1 “throw garbage away.”
Input: environment e, trash a1
gc = f ind garbage can(e)
repeat
if a1 is in hand & gc is close then
release(a1 )
else if a1 is in hand & far from gc then
move close(gc)
else if a1 is close & a1 not in hand
& nothing on top of a1 then
grasp(a1 )
..
.
else if a1 is far then
move close(a1 )
end if
until a1 inside gc

Program 1 is an example of what is commonly referred
to as reactive or dynamic planning [31, 18]. In traditional
deliberative planning, a planning algorithm synthesizes a
sequence of steps that starts from the given state and reaches
the given goal state. Although current symbolic planners can
find optimal plan sequences consisting of hundreds of steps,
such long sequences often break down because of unexpected
events during the execution. A dynamic plan provides a much

Now all the if statements have the same form, where
the same primitive along with same arguments are used in
both the condition as well as the body of the if statement.
We can therefore reduce all if statements inside the loop
further down to a simple line which depends only on a single
weight vector and a single joint feature map, as shown in
Program 3, for finding the most suitable pair of primitive p̂t
and its arguments (â1,t , â2,t ).
Program 3 “throw garbage away.”
Input: environment e, trash ga1
repeat
et = current environment
(p̂t , â1,t , â2,t ) :=
arg max

wT φ(et , pt (a1,t , a2,t ))

pt ∈P,a1,t ,a2,t ∈E

execute p̂t (â1,t , â2,t )
until p̂t = done

The approach taken in Program 3 also allowed removing
the function f ind garbage can(e). Both Program 1 and
Program 2 require f ind garbage can(e) which depends on
semantic labeling of each object in the environment. The
attributes of objects will allow the program to infer which
object is a garbage can without explicit encoding.

Program 3 provides a generic representation of a dynamic
plan. We will now discuss an approach to learning a set of
weights. To do so, we will employ a graph-like representation
obtained by “unrolling” the loop representing discrete time
steps by different layers. We will obtain a representation that
is isomorphic to a Markov Random Field (MRF) and will
use a maximum margin based approach to training the weight
vector. Our MRF encodes the relations between the environment, primitive and its arguments. Our empirical results show
that such a framework is effectively trainable with a relatively
small set of example sequences. Our feature-based dynamic
plan formulation therefore offers an effective and general
representation to learn and generalize from action sequences,
accomplishing high-level tasks in a dynamic environment.
IV. M ODEL F ORMULATION
We are given a set of possible primitives P (navigation and
manipulation controllers) to work with (see Section V) and
an environment E represented by a set of attributes. Using
these primitives, the robot has to accomplish a manipulation
task g ∈ T . The manipulation task g is followed by the
arguments ga1 , ga2 ∈ E which give a specification of the
task. For example, the program “throw garbage away” would
have a single argument which would be the object id of the
object that needs to be thrown away.
At each time step t (i.e., at each iteration of the loop
in Program 3), our environment et will dynamically change,
and its relations with the primitive is represented with a joint
set of features. These features include information about the
physical and semantic properties of the objects as well as
information about their locations in the environment.
Now our goal is to predict the best primitive pt ∈ P to
execute at each discrete time step, along with its arguments:
pt (a1,t , a2,t ). We will do so by designing a score function
S(·) that represents the correctness of executing a primitive
in the current environment for a task.
S(g(ga1 , ga2 ), et ,pt (a1,t , a2,t )) =
wT φ(g(ga1 , ga2 ), et , pt (a1,t , a2,t ))
In order to have a parsimonious representation, we decompose our score function using a model isomorphic to
a Markov Random Field (MRF), shown in Figure 2. This
allows us to capture the dependency between primitives,
their arguments, and environments which are represented by
set of attributes. In the figure, the top node represents the
given task and its arguments (g, ga1 , ga2 ). The second layer
from the top represents the sequence of primitives, and the
layer below represents the arguments associated with each
primitive. And, the bottom node represents the environment
which is represented with set of attributes. Note that we
also take into account the previous two primitives in the
past, together with their arguments: pt−1 (a1,t−1 , a2,t−1 ) and
pt−2 (a1,t−2 , a2,t−2 ).
Now the decomposed score function is:
prim-task

prim-args-env

prim-args-args(prev)-env

z}|{
z}|{
z }| {
S = Sae + Spt + Saet + Spae + Sppt + Spaae
|{z}
|{z}
|{z}
args-env

args-env-task

prim-prim(prev)-task

Fig. 2. Markov Random Field representation of our model at discrete
time step t. The top node represents the given task g, ga1 , ga2 . The second
layer from the top represents the sequence of primitives, and the layer below
represents the arguments associated with each primitive. And, the bottom
node represents the environment represented with set of attributes.

The terms associated with an edge in the graph are defined
as a linear function of its respective features φ and weights
w:
Sae = wae1 T φae (a1,t , et ) + wae2 T φae (a2,t , et )
Spt = wpt T φpt (pt , g)

Similarly, the terms associated with a clique in the graph
are defined as a linear function of respective features φ and
weights w:
Saet = waet1 T φaet (a1,t , et , g) + waet2 T φaet (a2,t , et , g)
Spae = wpae1 T φpae (pt , a1,t , et ) + wpae2 T φpae (pt , a2,t , et )
Sppt = wppt1 T φppt (pt−1 , pt , g) + wppt2 T φptt (pt−2 , pt , t)
X
Spaae =
wpaaeijk T φpaae (pt , ai,k , aj,t , et )
i,j∈(1,2),k∈(t−2,t−1)

Using these edge and clique terms, our score function S can be simply written in the following form,
which we have seen in Program 3 with an extra
term g for the task: S(g(ga1 , ga2 ), et , pt (a1,t , a2,t )) =
wT φ(g(ga1 , ga2 ), et , pt (a1,t , a2,t )).
A. Features
In this section, we describe our features φ(·) for the
different terms in the previous section.
Arguments-environment (φae ): The robot should be aware
of its location and the current level of its interaction with
objects (e.g., grasped), which are given as possible primitive
arguments a1,t , a2,t . Therefore, we add two binary features
which indicate whether each primitive argument is already
grasped and two features for the centroid distance from the
robot to each primitive arguments.
For capturing spatial relation between two objects a1,t and
a2,t , we add one binary feature indicating whether primitive
arguments a1,t , a2,t are currently in collision with each other.
Arguments-environment-task (φaet ): To capture relations
between the objects of interest (task arguments) and objects
of possible interest (primitive arguments), we build a binary
vector of length 8. First four represents the indicator values of
whether the objects of interest are identical as the objects of
possible interest, and the last four represents spatial relation
of whether they overlap from top view.

It is important to realize the type of object that is below
the objects of interests, and the desired property (e.g., bowllike object or table-like object) may differ depending on the
situation. We create two feature vectors, each of length l.
If the robot is holding the object, we store its extracted
attributes in the first vector. Otherwise, we store them in the
second vector. If the primitive has two arguments, we use
the first primitive argument since it often has higher level of
interaction with the robot compared to the second argument.
Finally, to capture correlation between the high-level task
and the types of object in primitive argument, we take a
tensor product of two vectors: an attribute vector of length
2l for two objects and a binary occurrence vector of length
|T |. The matrix of size 2l × |T | is flattened to a vector.
Primitive-task (φpt ): The set of primitives that are useful
may differ depending on the type of the task. We create a
|T |×|P| binary co-occurrence matrix between the task g and
the primitive pt that has a single non-zero entry in the current
task’s (g th ) row and current primitive’s (pt th ) column.
Primitive-arguments-environment (φpae ): Some primitives
such as hold above require one of the objects in arguments to be grasped or not to be grasped to execute correctly.
We create a |P| × 2 matrix where the row for the current
primitive (pt th row) contains two binary values indicating
whether each primitive argument is in the manipulator.
Primitive-primitive(previous)-task (φppt ): The robot makes
different transitions between primitives for different tasks.
Thus, a binary co-occurrence matrix of size |T | × |P|2
represents transition occurrence between the primitives for
each task. In this matrix, we encode two transitions for the
current task g, from t − 2 to t and from t − 1 to t.
Primitive-arguments-arguments(previous)-environment
(φpaae ): For a certain primitive in certain situations, the
arguments may not change between time steps. For example,
pour(A,B) would often be preceded by hold above
(A,B). Thus, the matrix of size |P| × 8 is created, with the
pt th row containing 8 binary values representing whether
the two primitive arguments at time t are the same as the
two arguments at t − 1 or the two arguments at t − 2.
B. Attributes.
Every object in the environment including tables and the
floor is represented using the following set of attributes:
height h, max(width(w),length(l)), min(w, l), volume(w ∗
l ∗ h), min(w, l, h)-over-max(w, l, h), median(w, l, h)-overmax(w, l, h), cylinder-shape, box-shape, liquid, container,
handle, movable, large-horizontal-surface, and multiplelarge-horizontal-surface. Attributes such as cylinder-shape,
box-shape, container, handle, and large-horizontal-surface
can be reliably extracted from RGB or RGBD images, and
were shown to be useful in several different applications
[7, 6, 22, 20]. We study the effects of attribute detection
errors on our model in Section V.
C. Learning
We use a max-margin approach to train a single model
for all tasks. This maximum margin approach fits our formulation, since it assumes that the discriminant function is a

Fig. 3. Figure showing two of our 13 environments in our evaluation
dataset using 43 objects along with PR2 robot.

linear function of a weight vector w and a joint feature map
φ(g(ga1 , ga2 ), et , pt (a1,t , a2,t )), and it has time complexity
linear with the number of training examples when solved using the cutting plane method [13]. We formalize our problem
as a “1-slack” structural SVM optimization problem:
i

n
l
1
C XX i
min wT w +
ξt
w,ξ≥0 2
l i=1 t=1

s.t.
T

w

for 1 ≤ i ≤ n, for each time step t :

∀p̂ ∈ P, ∀â1 , â2 ∈ E :

i

i
i
i
i
[φ(g (ga1
, ga2
), eit , pit (ai1,t , ai2,t ))−φ(g i (ga1
, ga2
), eit , p̂(â1 , â2 ))]
i
i
i
i
≥ ∆({pt , a1,t , a2,t }, {p̂, â1 , â2 }) − ξt

where n is the number of example sequences, li is the length
of the ith sequence, and l is the total length combining all
sequences. The loss function is defined as:
∆({p, a1 , a2 }, {p̂, â1 , â2 }) = 1(p 6= p̂)+1(a1 6= â1 )+1(a2 6= â2 )

With a learned w, we choose the next action in sequence
by selecting a pair of primitive and arguments that gives the
largest discriminant value:
arg max

wT φ(g(ga1 , ga2 ), et , pt (a1,t , a2,t ))

pt ∈P,a1,t ,a2,t ∈E

V. E XPERIMENTS
Dataset. We considered seven primitives (low-level
controllers): move close (A), grasp (A), release
(A), place above (A,B), hold above (A,B),
follow traj circle (A) and follow traj pour
(A,B). Depending on the environment and the task, these
primitives could be instantiated with different arguments.
For example, consider an environment that contains a bottle
(obj04) containing liquid (obj16) and an empty cup (obj02)
placed on top of the shelf, among other objects. If, say from
a recipe, our task is to pour the liquid, then our program
should figure out the correct sequence of primitives with
correct arguments (based on the objects’ attributes, etc.):
{pour(obj16); env2} →
{move close(obj02); grasp(obj02); move close(obj04);
place above(obj02,obj26); release(obj02); grasp(obj04);
hold above(obj04,obj02); follow traj pour(obj04,obj02)}

Note that the actual sequence does not directly interact with
the liquid (obj16)—the only object specified by the task—
but rather with a container of liquid (obj04), an empty cup
(obj02), and a table (obj26), while none of these objects are
specified in the task arguments. As seen in this example, the
input for our planning problem is under-specified.
For evaluation, we prepared a dataset where the goal
was to produce correct sequences for the following tasks
in different environments:
• stir(A): Given a liquid A, the robot has to identify a
stirrer of ideal size (from several) and stir with it. The

TABLE I
R ESULT OF BASELINES , OUR MODEL WITH VARIATIONS OF FEATURE SETS , AND OUR FULL MODEL ON OUR DATASET CONSISTING OF 127
SEQUENCES . T HE “ PRIM ” COLUMNS REPRESENT PERCENTAGE OF PRIMITIVES CORRECTLY CHOSEN REGARDLESS OF ARGUMENTS , AND “ARGS ”
COLUMNS REPRESENT PERCENTAGE OF A CORRECT PAIR OF PRIMITIVE AND ARGUMENTS . T HE LAST COLUMN SHOWS AVERAGE PERCENTAGE OF
SEQUENCES CORRECT OVER THE FIVE PROGRAMS EVALUATED .

chance
multiclass
symb-plan-svm
symb-plan-manual
Only edge features
Only clique features
Ours - full

•

•

•

•

move
prim
14.3
99.6
99.6
99.6
23.5
99.6
99.3

close
arg
1.1
82.5
85.4
15.3
1.9
82.8

grasp
prim
arg
14.3
1.1
90.4
94.2
72.4
94.2
76.3
56.4
45.5
96.8
82.7
96.8
84.0

release
prim
arg
14.3
1.1
95.7
67.4
63.0
67.4
63.0
93.5
93.5
90.2
90.2
97.8
97.8

place above
prim
arg
14.3
0.1
68.5
60.9
43.5
60.9
50.0
0.0
0.0
72.8
15.2
89.1
79.3

liquid may be located on a tight shelf where it would
be dangerous to stir the liquid, and the robot should
always stir it on top of an open surface, like a table. The
robot should always only interact with the container of
the liquid, rather than the liquid itself, whenever liquid
needs to be carried or poured. Our learning algorithm
should learn such properties.
pick and place(A,B): The robot has to place A on
top of B. If A is under some other object C, the object
C must first be moved before interacting with object A.
pour(A): The robot has to identify a bowl-like object
without object labels and pour liquid A into it. Note
again that liquid A cannot be directly interacted with,
and it should not be poured on top of a shelf.
pour to(A,B): The liquid A has to be poured into
the container B. (A variant of the previous task where
the container B is specified but the model should be
able to distinguish two different tasks.)
throw away(A): The robot has to locate a garbage
can in the environment and throw out object A.

In order to learn these programs, we collected 127 sequences for 113 unique scenarios by presenting participants
the environment in simulation and the task to be done. We
considered a single-armed mobile manipulator robot for these
tasks. In order to extract information about the environment
at each time frame of every sequence, we implemented
each primitive using OpenRAVE simulator [5]. Though most
of the scenarios had a single optimal sequence, multiple
sequences were introduced when there were other acceptable
variations. The length of each sequence varies from 4 steps
to 10 steps, providing a total of 736 instances of primitives.
To ensure variety in sequences, sequences were generated
based on the 13 different environments shown in Figure 3,
using 43 objects each with unique attributes.
Baseline Algorithms. We compared our model against following baseline algorithms:
•
•

•

chance: At each time step, a primitive and its arguments
are selected at random.
multiclass: A multiclass SVM [13] was trained to predict primitives without arguments, since the set of possible arguments changes depending on the environment.
symbolic-plan-svm: A PDDL-based symbolic planner
[36, 39] requires a domain and a problem definition.
Each scenario was translated to symbolic entities and
relations. However, the pre-conditions and effects of

hold above
prim
arg
14.3
0.1
79.7
76.6
73.4
76.6
76.6
18.8
9.4
87.5
15.6
96.9
92.2

•

traj circle
prim
arg
14.3
1.1
100.0
96.7
76.7
96.7
96.7
100.0
100.0
96.7
96.7
100.0
100.0

traj pour
prim
arg
14.3
0.1
14.7
97.1
91.2
97.1
97.1
50.0
44.1
100.0
97.1
97.1
94.1

Average
prim
arg
14.3
0.7
78.4
84.6
71.8
84.6
77.9
48.9
44.0
91.9
57.0
96.7
90.0

Sequence
prim
arg
0
0
58.4
49.6
58.4
54.9
0
0
45.0
0
91.6
69.7

each action in domain definition were hand-coded, and
each object was labeled with attributes using predicates.
Unlike our model that works on an under-specified
problem, each symbolic planning problem requires an
explicit goal state. In order to define these goal states,
we have trained ranking SVMs [12] in order to detect
a ‘stirrer’, an ‘object to pour into’ and a ‘garbage can’
for stir, pour, and throw away, respectively. Each
symbolic planning instance was then solved by reducing
to a satisfiability problem [16, 30].
symbolic-plan-manual: Based on the same method as
symbolic-plan-svm, instead of training ranking SVMs,
we provided ground-truth goal states. Even after providing lots of hand-coded rules, it is still missing some
rules due to the difficulty of representation using PDDL
[36, 39], These missing rules include the fact that liquid
needs to be handled through its container and that
objects should not be manipulated on top of the shelf.

Evaluation and Results. We evaluated our algorithm
through 6-fold cross-validation, computing accuracies over
primitives, over primitives with arguments, and over the
full sequences. Figure 4(a) shows the confusion matrix for
prediction of our seven primitives. We see that our model is
quite robust for most primitives.
With our dataset, our model was able to correctly predict
pairs of primitives and arguments 90.0% of the time and full
sequences 69.7% of the time (Table I). Considering only the
primitives without arguments, it was able to predict primitive
96.7% of the time and full sequence 91.6% of the time. The
last column of Table I shows the performance with respect
to whether the complete sequence was correct or not. For
example, for “pouring”, our model has learned not only to
bring a cup over to the table, but also to pick out the cup
when there are multiple other objects like a pot, a bowl, or
a can that may have similar properties.
How do baselines perform for our under-specified planning problem? The results of various baseline algorithms
are shown in Table I. If the primitive and arguments pairs
are predicted at random, none of the sequences would be
correct because of the large search space of arguments. Multiclass predicted well for some of the primitives but suffered
greatly on primitives like place above, hold above and
follow traj pour, which drastically impacts constructing overall sequences, even with correct arguments selected.
The symbolic planner based approaches, symbolic-plan-

(a) Confusion matrix for the seven primitives
in our dataset. Our dataset consist of 736 instances of seven primitives in 127 sequences
on five manipulation tasks.

(b) Percentage of programs correct. Without (c) Percentage of programs correct for 12
any feedback in completely autonomous mode, high-level tasks such as making sweet tea. In
the accuracy is 69.7%. With feedback (number of completely autonomous mode, the accuracy is
feedbacks on x-axis), the performance increases. 75%. With feedback (number of feedbacks on
This is on full 127 sequence dataset.
x-axis), the performance increases.
Fig. 4. Results with cross-validation. (a) On predicting the correct primitive individually. (b) On predicting programs, with and without user intervention.
(c) On performing different tasks with the predicted sequences.

svm and symbolic-plan-manual, suffered greatly from underspecified nature of the problem. The planners predicted correctly 49.6% and 54.9% of the times, respectively, compared
to our model’s performance of 69.7%. Even though both
planners made use of heavily hand-coded domain definitions
of the problem, due to the nature of the language used by
symbolic planners, rules such as that liquid should not be
handled on top of shelves were not able to be encoded. Even
if the language were capable of encoding these rules, it would
require a human expert in planning language to carefully
encode every single rule the expert can come up with.
Also, by varying the set of features, it is evident that
without very robust primitive-level accuracies, the models
are unable to construct a single correct sequence.
How important is attribute representation of objects?
For 113 unique scenarios in our dataset, we have randomly
flipped binary attributes and observed the effects of detection
errors on correctness for the full sequence (Figure 6). When
there is no error in detecting attributes, our model performs at
69.7%. With 10% detection error, it performs at 55.8%, and
with 40% detection errors, it performs at 38.1%. Since the
attribute detection is more reliable than the object detection
[7, 6, 22], our model will perform better than planners based
on explicit object labels.
How can the robot utilize learned programs? These
learned programs can form higher level tasks such as making a recipe found online. For example, serving sweet tea
would require the following steps: pouring tea into a cup,
pouring sugar into a cup, and stirring it (Figure 5). We have
tested each of the four tasks, serve-sweet-tea, serve-coffeewith-milk, empty-container-and-throw-away, and serve-andstore, in three environments. Each of the four tasks can be
sequenced in following manner by programs respectively:
pour → pour to → stir, pour to → pour to, pour
→ throw away, and pour → pick and place. Out
of total 12 scenarios, our model was able to successfully
complete the task for 9 scenarios.
Does the robot need a human observer? In an assistive
robotics setting, a robot will be accompanied by a human
observer. With help from the human, performance can be
greatly improved. Instead of choosing a primitive and argument pair that maximizes the discriminant function, the robot

Fig. 6. Effect of attribute perception error. Figure showing percentage
of programs correct with attribute labeling errors for binary attributes. For
113 unique scenarios, binary attributes were randomly flipped.

can present the top 2 or 3 primitive and argument pairs to the
observer, who can simply give feedback on the best option
among those choices. At the initial time step of the sequence,
with only a single piece of feedback, given 2 or 3 choices,
performance improves to 74.1% and 75.6% respectively from
69.7% (Figure 4(b)). If feedback was provided through whole
sequence with the top 2 or 3 choices, it further improves to
76.7% and 81.4%. Furthermore, the four higher level tasks
(recipes) considered earlier also shows that with a single
feedback at the initial time step of each program, the results
improve from 75% to 100% (Figure 4(c)).
Robotic Experiments. Finally, we demonstrate that our
inferred programs can be successfully executed on our
Kodiak PR2 robot for a given task in an environment.
Using our implementation of the primitives discussed in
Section V, we show our robot performing the task of
“serving sweet tea.” It comprises executing three programs
in series – pour, pour to and stir – which in total
required sequence of 20 primitives with correct arguments.
Each of these programs (i.e., the sequence of primitives and
arguments) is inferred for this environment. Figure 5 shows
a few snapshots and the full video is available at:
http://pr.cs.cornell.edu/learningtasksequences

VI. C ONCLUSION
In this paper, we considered the problem of learning
sequences of controllers for robots in unstructured human
environments. In an unstructured environment, even a simple
task such as pouring can take variety of different sequences
of controllers depending on the configuration of the environment. We took a dynamic planning approach, where we

Fig. 5. Few snapshots of learned sequences forming the higher level task of serving sweet tea, which takes the sequence of pouring tea into a cup,
pouring sugar into a cup, and then stirring it.

represent the current state of the environment using a set of
attributes. To ensure that our dynamic plans are as general
and flexible as possible, we designed a score function that
captures relations between task, environment, primitives, and
their arguments, and we trained a set of parameters weighting
the various attributes from example sequences. By unrolling
the program, we can obtain a Markov Random Field style
representation, and use a maximum margin learning strategy.
We demonstrated on a series of example sequences that our
approach can effectively learn dynamic plans for various
complex high-level tasks.
ACKNOWLEDGEMENTS
This work was supported in part by ONR Grant N0001414-1-0156, and Microsoft Faculty Fellowship and NSF Career award to Saxena.



We describe research and results centering on
the construction and use of Bayesian mod­
els that can predict the run time of problem
solvers. Our efforts are motivated by observa­
tions of high variance in the time required to
solve instances for several challenging prob­
lems. The methods have application to the
decision-theoretic control of hard search and
reasoning algorithms. We illustrate the ap­
proach with a focus on the task of predict­
ing run time for general and domain-specific
solvers on a hard class of structured con­
straint satisfaction problems. We review the
use of learned models to predict the ultimate

length of a trial, based on observing the be­
havior of the search algorithm during an early
phase of a problem session. Finally, we dis­
cuss how we can employ the models to inform
dynamic run-time decisions.
1

Introduction

The design of procedures for solving difficult problems
relies on a combination of insight, observation, and it­
erative refinements that take into consideration the be­
havior of algorithms on problem instances. Complex,
impenetrable relationships often arise in the process of
problem solving, and such complexity le ads to uncer­
tainty about the basis for observed efficiencies and in­
effi.ciences associated with specific problem instances.
We believe that recent advances in Bayesian methods
for learning predictive models from data offer valuable
tools for designing, controlling, and understanding au­
tomated reasoning methods.
We focus on using machine learning to characterize
variation in the run time of instances observed in in­
herently exponential search and reasoning problems.
Predictive models for run time in this domain could

Design, real-rime control,

World, Context

j

Contex.tual

evidence

insights

Run time

Structural
evidence

Ex.ecution
evidence

GQlliJ

Feature refinement, insights

Figure 1: Bayesian approach to problem solver design
and optimization. We seek to learn predictive mod­
els to refine and control computational procedures as
well as to gain insights about problem structure and
hardness.

provide the basis for more optimal decision making at
the microstructure of algorithmic activity as well as
inform higher-level policies that guide the allocation
of resources.
Our overall methodology is highlighted in Fig. 1. We
seek to develop models for predicting execution time
by considering dependencies between execution time
and one or more classes of observations. Such classes
include evidence about the nat ure of the generator that
has provided instances, about the structural properties
of instances noted before problem solving, and about
the run-time behaviors of solvers as they struggle to
solve the instances.
The research is fundamentally iterative in nature. We
exploit learning methods to identify and continue to

refine observational variables and models, balancing
the predictive power of multiple observations with the
cost of the real-time evaluation of such evidential dis-

HORVITZ ET AL.

236

tinctions. We seek ultimately to harness the learned
models to optimize the performance of automated rea­
soning procedures. Beyond this direct goal, the overall
exploratory process promises to be useful for providing
new insights about problem hardness.
We first provide background on the problem solving
domains we have been focusing on. Then, we describe
our efforts to instrument problem solvers and to learn
predictive models for run time. We describe the for­
mulation of variables we used in data collection and
model construction and review the accuracy of the in­
ferred models. Finally, we discuss opportunities for
exploiting the models. We focus on the sample appli­
cation of generating context-sensitive restart policies
in randomized search algorithms.
2

Hard Search Problems

UAI2001

distinct symbols in which some cells may be empty
but no row or column contains the same element twice.
The Quasigroup Completion Problem (QCP) can be
stated as follows: Given a partial quasigroup of order
n can it be completed to a quasigroup of the same
order?

n

Figure 2: Graphical representation of the quasigroup
problem. Left: A quasigroup instance with its comple­
tion. Right: A balanced instance with two holes per
row/column.

We have focused on applying learning methods to char­

acterize run times observed in backtracking search pro­
cedures for solving NP-complete problems encoded as
constraint satisfaction (CSP) and Boolean satisfiabil­
ity (SAT). For these problems, it has proven extremely
difficult to predict the particular sensitivities of run
time to changes in instances, initialization settings,
and solution policies. Numerous studies have demon­
strated that the probability distribution over run times
exhibit so-called heavy-tails [1 0 ]. Restart strategies
have been used in an attempt to find settings for an
instance that allow it to be solved rapidly, by avoiding
costly journeys into a long tail of run time. Restarts
are introduced by way of a parameter that terminates
the run and restarts the search from the root with a
new random seed after some specified amount of time
passes, measured in choices or backtracks.
Progress on the design and study of algorithms for
SAT and CSP has been aided by the recent devel­
opment of new methods for generating hard random
problem instances. Pure random instances, such as
k-Sat, have played a key role in the development of al­
gorithms for propositional deduction and satisfiability
testing. However, they lack the structure that char­
acterizes real world domains. Gomes and Selman [9]
introduced a new benchmark domain based on Quasi­
groups, the Quasigroup Completion Problem (QCP) .
QCP captures the structure that occurs in a variety of
real world problems such as timetabling, routing, and
statistical experimental design.
A quasigroup is a discrete structure whose multipli­
cation table corresponds to a Latin Square. A Latin
Square of order n is an n x n array in which n dis­
tinct symbols are arranged so that each symbol occurs
once in each row and column. A partial quaisgroup (or
Latin Square) of order n is an n x n array based on

QCP is an NP-complete problem [5] and random in­
stances have been found to exhibit a peak in prob­
lem hardness as a function of the ratio of the number
of uncolored cells to the total number of cells. The
peak occurs over a particular range of values of this
parameter, referred to as a region of phase transition
[9, 2]. A variant of the QCP problem, Quasigroup with
Holes (QWH) [ 2] , includes only satisfiable instances.
The QWH instance-generation procedure essentially
inverts the completion task: it begins with a randomly­
generated completed Latin square, and then erases col­
ors or "pokes holes." Completing QWH is NP-Hard
[2]. A structural property that affects hardness of in­
stances significantly is the pattern of the holes in row
and columns. Balancing the number holes in each row
and column of instances has been found to significantly
increase the hardness of the problems [1].
3

Experiments with Problem Solvers

We performed a number of experiments with Bayesian
learning methods to elucidate previously hidden dis­
tinctions and relationships in SAT and CSP reason­
ers. We experimented with both a randomized SAT
algorithm running on Boolean encodings of the QWH
and a randomized CSP solver for QWH. The SAT al­
gorithm was Satz-Rand [11], a randomized version of
the Satz system of Li and Anbulagan [20]. Satz is the
fastest known complete SAT algorithm for hard ran­
dom 3-SAT problems, and is well suited to many inter­
esting classes of structured satisfiability problems, in­
cluding SAT encodings of quasigroup completion prob­
lems [10] and planning problems [17]. The solver is a
version of the classic Davis-Putnam (DPLL) algorithm
[7] augmented with one-step lookahead and a sophisti-

UAI2001

cated variable

HORVITZ ET AL.

choice heuristic. The lookahead opera­

3.1

237

Formulating Evidential Variables

tion is invoked at most choice points and finds any

choices that would immediately lead
contradiction after unit propagation; for these,
the opposite variable assignment can be immediately
made. The variable ch oice heuristic is based on picking
a variable that if set would cause the greatest number
of ternary clauses to be reduced to binary clauses. The
variable choice set was enlarged by a noise parameter
of 30%, and value selection was performed determin­
istically by always branching on 'true' first.

variable/value
to a

The second backtrack search algorithm we studied is
randomized version of a specialized CSP solver for
quasigroup completion problem s, written using the
ILOG solver constraint programming library. The
backtrack search algorithm uses as a variable choice
heuristic a variant of the Brelaz heuristic. Further­
more, it uses a sophisticated propagation method to
enforce the constraints that assert that all the colors
in a row/ column must be different. We refer to such
a constraint as alldiff. The propagation of the alldiff
constraint corresponds to solving a matching problem
on a bipartite graph using a network-flow algorithm
[9, 26, 24].
a

learned predictive models for run-time, motivated
two different classes of target problems. For the
first class of problem, we assume that a solver is chal­
lenged by a n instance and must solve that specific
problem as quickly as possible. We term this the Sin­
gle Instance problem. In a second class of problem,
we draw cases from a distribution of instances and are
required to solve any instance as soon as possible, or
as many instances as possible for any amount of time
allocated. We call these challenges Multiple Instance
problems, and the subproblems as the Any Instance
and Max Instances problems , respectively.
We

by

We collected evidence and built models for CSP and
Satz solvers applied to the QWH problem for both
the Single In st an ce and Multiple Instances challenge.
We shall refer to the four problem-solving experiments
as CSP-QWH-Single, CSP-QWH-Multi, Satz -QW H ­
Single, and S atz-Q WH- Multi. Building predictive
Bayesian models for the CSP- Q WH-S ingle and Satz­
QWH-Single problems centered on gathering data on
the probabilistic relationships between observational
variables and run time for single instances with ran­
domized restarts. Experiments for the CSP-QWH­
Multi and S atz -Q WH- Multi problems centered on per­
forming single runs on multiple instances drawn from
the same instance generator.

We worked to define variables that we believed could
provide information on problem-solving progress for a
period of observation in an early phase of runs that we
refer to as th e observation horizon. The defin iti on of
variables was initially guided by intuition. However,
results from our early experiments helped us to refine
sets of variables and to propose additional candidates.
We initially explored a large number of variables, in­
cluding those that were difficult to compute. Although
we planned ultimately to avoid the use of costly ob­
servations in real-time forecasting settings, we were
interested in probing the predictive power and inter­
dependencies among features regardless of cost. Un­
der st andin g such informational dependencies promised
to be useful in understanding the potential losses in
predictive power with the removal of costly features,
or substitution of expensive evidence with less expen­
sive, approximate observations. We eventually limited
the features explored to those that could be computed
with low (constant) overhead.
We sought to collect information about base values as
well as several variants and combinations of these val­
ues. For example, we formulated features that could
capture higher-l evel patterns and dynamics of the state
of a prob l em solver that could serve as useful probes
of solution progress. Beyond exploring base observa­
tions about the program state at particular points in
a case, we defined new families of observations such as
first and second derivatives of the base variables, and
summaries of the status of variables over time.

Rather than include a separate variable in the model
for each feature at each choice point-which would
have led to an explosion in the number of variables
and severely limited generalization-features and their
dynamics were represented by variables for their sum­
mary statistics over the observation horizon. The sum­
mary statistics included initial, final, average, mini­
mum, and maximum values of the features during the
observation period. For example, at each choice point,
the SAT solver recorded the current number of binary
clauses. The training data would thus included a vari­
able for the average first derivative of t he number of
binary clauses during the observation period. Finally,
for several of the features, we also computed a sum­
mary statistic that measured the number of times the
sign of the feature changed from negative to positive
or vice-versa.
We developed distinct sets of observational var iables
for the CSP and Satz solvers. The features for the
CSP solver included some that were generic to any
constraint satisfaction problem, such as the number
of backtracks, the depth of the search tree, and the

HORVITZ ET AL.

238

average domain size of the unbound CSP variables.
Other features, such as the variance in the distribution
of unbound CSP variables between different columns
of the square, were specific to Latin squares. As we
will see below, the inclusion of such domain-specific
features was important in learning strongly predictive
models. The CSP solver recorded 18 basic features
at each choice point which were summarized by a to­
tal of 135 variables. The variables that turned out
to be most informative for prediction are described in
Sec. 4.1 below.
The features recorded by Satz-Rand were largely
generic to SAT. We included a feature for the num­
ber of Boolean variables that had been set positively;
this feature is problem specific in the sense that under
the SAT encoding we used, only a positive Boolean
variable corresponds to a bound CSP variable (i.e. a
colored squared). Some features measured the current
problem size (e.g. the number of unbound variables),
others the size of the search tree, and still others the
effectiveness of unit propagation and lookahead.
We also calculated two other features of special note.
One was the logarithm of the total number of possible
truth assignments (models) that had been ruled out
at any point in the search; this quantity can be effi­
ciently calculated by examining the stack of assumed
and proven Boolean variable managed by the DPLL
algorithm. The other is a quantity from the theory of
random graphs called .\, that measures the degree of
interaction between the binary clauses of the formula
(23]. In all Satz recorded 25 basic features that were
summarized in 127 variables.

4

Collecting Run-Time Data

For all experiments, observational variables were col­
lected over an observational horizon of 1000 solver
choice points. Choice points are states in search pnr
cedures where the algorithm assigns a value to vari­
ables heuristically, per the policies implemented in the
problem solver. Such points do not include the cases
where variable assignment is forced via propagation of
previous set values, as occurs with unit propagation,
backtracking, lookahead, and forward-checking.
For the studies described, we represented run time as a
binary variable with discrete states short versus long.
We defined short runs as cases completed before the
median of the run times for all cases in each data set.
Instances with run times shorter than the observation
horizon were not considered in the analyses.

Models and Results

We employed Bayesian structure learning to infer pre­
dictive models from data and to identify key variables
from the larger set of observations we collected. Over
the last decade, there has been steady progress on
methods for inferring Bayesian networks from data
[6, 27, 12, 13]. Given a dataset, the methods typically
perform heuristic search over a space of dependency
models and employ a Bayesian score to identify mod­
els with the greatest ability to predict the data. The
Bayesian score estimates p(modelldata) by approxi­
mating p( data lmodel)p( model). Chickering, Hecker­
man and Meek [4] show how to evaluate the Bayesian
score for models in which the conditional distributions
are decision trees. This Bayesian score requires a prior
distribution over both the parameters and the struc­
ture of the model. In our experiments, we used a uni­
form parameter prior. Chickering et al. suggest using
a structure prior of the form: p(model) r;,fP, where
0 < ,.. :::; 1 and fp is the number of free parameters in
the model. Intuitively, smaller values of r;, make large
trees unlikely a priori, and thus ,.. can be used to help
avoid overfitting. We used this prior, and tuned r;, as
described below.
=

We employed the methods of Chickering et a!. to infer
models and to build decision trees for run time from
the data collected in experiments with CSP and Satz
problem solvers applied to QWH problem instances.
We shall describe sample results from the data col­
lection and four learning experiments, focusing on the
CSP-QWH-Single case in detail.
4.1

3.2

UAI2001

CSP-QWH-Single Problem

For a sample CSP-QWH-Single problem, we built a
training set by selecting nonbalanced QWH problem
instance of order 34 with 380 unassigned variables. We
solved this instance 4000 times for the training set and
1000 times for the test data set, initiating each run
with a random seed. We collected run time data and
the states of multiple variables for each case over an
observational horizon of 1000 choice points. We also
created a marginal model, capturing the overall run­
time statistics for the training set.
We optimized the r;, parameter used in the structure
prior of the Bayesian score by splitting the training set
70/30 into training and holdout data sets, respectively.
We selected a kappa value by identifying a soft peak
in the Bayesian score. This value was used to build a
dependency model and decision tree for run time from
the full training set. We then tested the abilities of the
marginal model and the learned decision tree to pre­
dict the outcomes in the test data set. We computed
a classification accuracy for the learned and marginal

UAI2001

HORVITZ ET AL.

models to characterize the power of these models. The
classification accuracy is the likelihood that the classi­
fier will correctly identify the run time of cases in the
test set. We also computed an average log score for
the models.
Fig. 3 displays the learned Bayesian network for this
dataset. The figure highlights key dependencies and
variables discovered for the data set. Fig. 4 shows the
decision tree for run time.
The classification accuracy for the learned model is
0.963 in contrast with a classification accuracy of 0.489
for the marginal model. The average log score of the
learned model is -0.134 a.nd the average log score of
the marginal model was -0.693.
Because this was both the strongest and most com­
pact model we learned, we will discuss the features it
involves in more detail. Following Fig. 4 from left to
right, these are:
VarRowColumn measures the variance in the number
of uncolored cells in the QWH instance across rows
and across columns. A low variance indicates the open
cells are evenly balanced throughout the square. As
noted earlier, balanced instances are harder to solve
than unbalanced ones [1]. A rather complex summary
statistic of this quantity appears at the root of the de­
cision tree, namely the minimum of the first derivative
of this quantity during the observation period. In fu­
ture work we will be examining this feature carefully
in order to determine why this particular statistic was
most relevant.
AvgColumn measures the ratio of the number of uncol­
ored cells and the number of columns or rows. A low
value for this feature indicates that the quasigroup is
nearly complete. The decision tree shows that a run is
likely to be fast if the min i mum value of this quantity
over the entire observation period is small.
MinDepth is the minimum depth of all leaves of the
search tree, and the summary statistic is simply the fi­
nal value of this quantity. The third and fourth nodes
of the decision tree show that short runs are associ­
ated with high minimum depth and long runs with
low minimum depth. This may be interpreted as in­
dicating the search trees for the shorter runs have a
more regular shape.
AvgDepth is the average depth of a node in the search
tree. The model discovers that short runs are associ­
ated with a high frequency in the change of the sign
of the first derivative of the average depth. In other
words, frequent fluctuations up and down in the aver­
age depth indicate a short run. We do not yet have an
intuitive explanation for this phenomena.

239

VarRowColumn appears again as the last node in the
decision tree. Here we see that if the maximum vari­
ance of the number of uncolored cells in the QWH
instance across rows and columns is low (i.e., the prob­
lem remains balanced) then the run is long, as might
be expected.
4.2

CSP-QWH-Multi Problem

For a CSP-QWH-Multi problem, we built training and
test sets by selecting instances of nonbalanced QWH
problems of order 34 with 380 unassigned variables.
We collected data on 4000 instances for the training
set and 1000 instances for the test set.
As we were running instances of potentially different
fundamental hardnesses, we normalized the feature
measurements by the size of the instance (measured in
CSP variables) after the instances were initially sim­
plified by forward-checking. That is, although all the
instances originally had the same number of uncolored
cells, polynomial time preprocessing fills in some of the
cells, thus revealing the true size of the instance.
We collected run time data for each instance over
an observational horizon of 1000 choice points. The
learned model was found to have a classification accu­
racy of 0.715 in comparison to the marginal model ac­
curacy of 0.539. The average log score for the learned
model was found to be -0.562 and the average log score
for the marginal model was -0.690.
4.3

Satz-QWH-Single Problem

We performed analogous studies with the Satz solver.
In a study of the Satz-QWH-Single problem, we stud­
ied a single QWH instance (bqwh-34-410-16). We
found that the learned model had a classification ac­
curacy of 0.603, in comparison to a classification accu­
racy of 0.517 for the marginal model. The average log
score of the learned model was found to be -0.651 and
the log score of the marginal model was -0.693.
The predictive power of the SAT model was less than
that of the corresponding CSP model. This is reason­
able since the CSP model had access to features that
more precisely captured special features of quasigroup
problems (such as balance). The decision tree was still
relatively small, containing 12 nodes that referred to
10 different summary variables.
Observations that turned out to be most relevant for
the SAT model included:
•

The maximum number of variables set to 'true'
during the observation period. As noted earlier,
this corresponds to the number of CSP variables
that would be bound in the direct CSP encoding.

HORVITZ ET AL.

240

UAl 2001

Figure 3: The learned Bayesian network for a sample CSP-QWH-Single problem. Key dependencies and variables
are highlighted.

I:Y'· •I
/

6

/-

/

Nat< -5.03 (1174)

Nat< 233 (187]

Not< 19.S (543)

. } ,:,1
/
........

< -5.03 (22�)

Nat < 3 .33 (897)

<3.33(1

�

< 19.S (354]

Not< 18.9 (322)

�

•18.9[

1.3(�)

<21.3

�

Figure 4: The decision tree inferred for run time from data gathered in a CSP-QWH-Single experiment. The
probability of a short run is captured by the light component of the bargraphs displayed at the leaves.

UAI2001

HORVITZ ET AL.

•

The number of models ruled out.

•

The number of unit propagations performed.

•

•

4.4

The number of variables eliminated by Satz's
lookahead component: that is, the effectiveness
o f lookahead.
The quantity ..\ described in Sec. 3.1 above, a mea­
sure of the constrainedness of the binary clause
subproblem.
Satz-QWH-Multi Problem

For the experiment with the Satz-QWH-Multi prob­
lem, we executed single runs of QWH instances
with the same parameters as the instance studied in
the Satz-QWH-Single Problem (bqwh-34-410) for the
training and test sets. Run time and observational
variables were normalized in the same manner as for
the CSP-QWH-Multi problem. The classification ac­
curacy of the learned model was found to be 0. 715.
The classification accuracy of the marginal model was
found to be 0.526. The average log score for the model
was -0.557 and the average log score for the marginal
model was -0.692.
4.5

Toward Larger Studies

For broad application in guiding computational prob­
lem solving, it is important to develop an understand­
ing of how results for sample instances, such as the
problems described in Sections 4.1 through 4.4, gener­
alize to new instances within and across distinct classes
of problems. We have been working to build insights
about generalizability by exploring the statistics of the
performance of classifiers on sets of problem instances.
The work on studies with larger numbers of data sets
has been limited by the amount of time required to
generate data sets for the hard problems being stud­
ied. With our computing platforms, several days of
computational effort were typically required to pro­
duce each data set.
As an example of our work on generalization, we re­
view the statistics of model quality and classification
accuracy, and the regularity of discriminatory features
for additional data sets of instances in the CSP-QWH­
Single problem class.
We defined ten additional nonbalanced QWH problem
instances, parameterized in the same manner as the
CSP problem described in Section 4.1 (order 34 with
380 unassigned variables). We employed the same data
generation and analysis procedures as before, building
and testing ten separate models. Generating data for
these analyses using the ILOG libary executed on an

241

Intel Pentium III (running at 600 Mhz) required ap­
proximately twenty-four hours per 1000 runs. Thus,
each CSP dataset required approximately five days of
computation.
In summary, we found significant boosts in classi­
fication accuracy for all of the instances. For the
ten datasets, the mean classification accuracy for the
learned models was 0.812 with a standard deviation of
0.101. The average log score for the models was -0.388
with a standard deviation of 0.167. The predictive
power of the learned models stands in contrast to the
classification accuracy of using background statistics;
the mean classification accuracy of the marginal mod­
els was 0.497 with a standard deviation of 0.025. The
average log score for the marginal models was -0.693
with a standard deviation of 0.001. Thus, we observed
relatively consistent predictive power of the methods
across the new instances.
We observed variation in the tree structure and dis­
criminatory features across the ten learned models.
Nevertheless, several features appeared as valuable
discriminators in multiple models, including statistics
based on measures of VarRowColumn, AvgColumn,
AvgDepth, and MinDepth. Some of the evidential fea­
tures recurred for different problems, showing signifi­
cant predictive value across models with greater fre­
quency than others. For example, measures of the
maximum variation in the number of uncolored cells
in the QWH instance across rows and columns (Max­
VarRowColumn) appeared as being an important dis­
criminator in many of the models.
5

Generalizing Observation Policies

For the experiments described in Sections 3 and 4, we
employed a policy of gathering evidence over an obser­
vation horizon of the initial 1000 choice points. This
observational policy can be generalized in several ways.
For example, in addition to harvesting evidence within
the observation horizon, we can consider the amount
of time expended so far during a run as an explicit
observation. Also, evidence gathering can be general­
ized to consider the status of variables and statistics
of variables at progressively later times during a run.
Beyond experimenting with different observational
policies, we believe that there is potential for harness­
ing value-of-information analyses to optimize the gath­
ering of information. For example, there is opportu­
nity for employing affine analysis and optimization to
generate tractable real-time observation policies that
dictate which evidence to evaluate at different times
during a run, conditioned on evidence that has already
been observed during that run.

242

5.1

HORVITZ ET AL.

Time Expended

as

Evidence

In the process of exploring alternate observation
policies, we investigated the value of extending the
bounded-horizon policy described in Section 3, with
a consideration of the status of time expended so far
during a run. To probe potential boosts with inclusion
of time expended, we divided several of the data sets
explored in Section 4.5 into subsets based on whether
runs with the data set had exceeded specific run-time
boundaries. Then, we built distinct run-time-specific
models and tested the predictive power of these models
on test sets containing instances of appropriate mini­
mal length. Such time-specific models could be used
in practice as a cascade of models, depending on the
amount of time that had already been expended on a
run.
We typically found boosts in the predictive power of
models built with such temporal decompositions. As
we had expected, the boosts are greatest for models
conditioned on the largest amounts of expended time.
As an example, let us consider one of the data sets
generated for the study in Section 4.5. The model
that had been built previously with all of the data
had a classification accuracy of 0. 793. The median
time for the runs represented in the set was nearly
18,000 choice points. We created three separate sub­
sets of the complete set of runs: the set of runs that
exceeded 5,000 choice points, the set that exceeded
8,000 choice points, and the set that had exceeded
11,000 choice points. We created distinct predictive
models for each training set and tested these mod­
els with cases drawn from test sets containing runs of
appropriate minimal length. The classification accu­
racies of the models for the low, medium, and high
time expenditure were 0.779, 0.799, and 0.850 respec­
tively. We shall be continuing to study the use of time
allocated as a predictive variable.
6

Application: Dynamic Restart
Policies

A predictive model can be used in several ways to
control a solver. For example, the variable selection
heuristic used to decompose the problem instance can
be designed to minimize the expected solution time
of the subproblems. Another application centers on
building distinct models to predict the run time as­
sociated with different global strategies. As an ex­
ample, we can learn to predict the relative perfor­
mance of ordinary chronological backtrack search and
dependency-directed backtracking with clause learn­
ing [16]. Such a predictive model could be used to
decide whether the overhead of clause learning would
be worthwhile for a particular instance.

UA12001

Problem and instance-specific predictions of run time
can also be used to drive dynamic cutoff decisions on
when to suspend a current case and restart with a new
random seed or new problem instance, depending on
the class of problem. For example, consider a greedy
analysis, where we deliberate about the value of ceas­
ing a run that is in progress and performing a restart
on that instance or another instance, given predictions
about run time. The predictive models described in
this paper can provide the expected time remaining
until completion of a current run. Initiating a new
run will have an expected run time provided by the
statistics of the marginal model. From the perspec­
tive of a single-step analysis, when the expected time
remaining for the current instance is greater than the
expected time of the next instance, as defined by the
background marginal model, it is better to cease ac­
tivity and perform a restart. More generally, we can
construct richer multistep analyses that provide the
fastest solutions to a particular instance or the highest
rate of completed solutions with computational effort.
We can also use the predictive models to perform com­
parative analyses with previous policies. Luby et al.
[21] have shown that the optimal restart policy, as­
suming full knowledge of the distribution, is one with
a fixed cutoff. They also provide a universal strat­
egy ( using gradually increasing cutoffs) for minimizing
the expected cost of randomized procedures, assum­
ing no prior knowledge of the probability distribution.
They show that the universal strategy is within a log
factor of optimal. These results essential settle the
distribution-free case.
Consider now the following dynamic policy: Observe a
run for 0 steps. If a solution is not found, then predict
whether the run will complete within a total of L steps.
If the prediction is negative, then immediately restart;
otherwise continue to run for up to a total of L steps
before restarting if no solution is found.
An upper bound on the expected run of this policy can
be calculated in terms of the model accuracy A and the
probability Pi of a single run successfully ending in i
or fewer steps. For simplicity of exposition we assume
that the model's accuracy in predicting long or short
runs is identical. The expected number of runs until
a solution is found is E(N)
1/(A(PL- Po)+ Po).
An upper bound on the expected number of steps in
a single run can be calculated by assuming that runs
that end within 0 steps take exactly 0 steps, and that
runs that end in 0+ 1 to L steps take exactly L steps.
The probability that the policy continues a run past 0
steps (i.e., the prediction was positive) is APL + (1A) ( 1- PL). An upper bound on the expected length of
a single run is Eub(R) 0 + (L- O)(APL + (1- A) (1PL)). Thus, an upper bound on the expected time to
=

=

UAI2001

solve a

HORVITZ ET AL.

proble m

E(N)Eub(R).

using the policy is

It is important to note that the expected time depends
on both the accuracy of the model and the prediction
point L; in general, one would want to vary L in or­
der to optimize the solution time.

Furthermore, in

general, it would be better to design more sophisti­
cated dynamic policies that made use of all informa­
tion gathered over a run, rather than just during the
first 0 steps. But even a non-optimized policy based
directly on the models discussed in this paper can out­
perform the optimal fixed policy. For example, in the
CSP-QWH-single problem case, the optimal fixed pol­
icy has an expected solution

time of 38,000 steps, while

the dynamic policy has an expected solution time of
only

27,000

steps. Optimizing the choice of L should

provide about an order of magnitude further improve­
ment.

243

c1s1ons about the partition of resources

formulation and inference.
and Klein

[14]

between

re­

In other work, Horvitz

constructed Bayesian models consid­

ering the time expended so far in theorem proving.
They monitored the progress of search in a proposi­
tional theorem prover and used measures of progress
in updating the probability of truth or falsity of as­

sertions. A Bayesian model was harnessed to update
belief about different outcomes as a function of the
amount of time that problem solving continued with­
out halting. Stepping back to view the larger body of
work on the decision-theoretic control of computation,
measures of

expected value of computation [15,

8,

25],

employed to guide problem solving, rely on forecasts
of the refinements of partial results with future com­
putation. More generally, representations of problem­
solving progress have been central in research on flex­
ible or anytime methods-procedures that exhibit a

While it may not be surprising that a dynamic policy

relatively smooth surface of performance

can outperform the optimal fixed policy, it is interest­

location of computational resources.

with the al­

ing to note that this can occur when the observation
time 0 is

greater

than the fixed cutoff.

That is, for

proper values of L and A, it may be worthwhile to ob­
serve each run for

1000 steps

even if the optimal fixed

strategy is to cutoff after 500 st eps. These and other

issues concerning applications of prediction models to
restart policies are examined in detail in a forthcoming
paper.

8

Future Work and Directions

This work represents a vector in a space of ongoing re­
search. We are pursuing several lines of research with
the goals of enhancing the power and generalizing the
applicability of the predictive methods. We are explor­
ing the modeling of run time at a finer grain through
the use of continuous variables and prototypical named

7

distributions. We are also exploring the value of de­

Related Work

composing the learning problem into models that pre­

Learning methods have been employed in previous re­
search in a attempt to enhance the performance opti­
mize reasoning systems. In work on "speed-up learn­
ing," investigators have attempted to increase plan­
ning efficiency by learn i ng goal-specific preferences for

plan operators

[22, 19].

Khardon and Roth explored

the offline reformulation of representations based on
experiences with problem solving in an environment
to enhance run-time efficiency

[18].

Our work on using

probabilistic models to learn about algorithmic perfor­
m ance and to guide problem solving is most c losely re­

lated to research on flexible computation and decision­
theoretic control. Related work in this arena focused
on the use of predictive models to control computa­
tion, Breese and Horvitz [3] collected data about the

dict the average execution times seen with multiple
runs and models that predict how well a particular in­
stance will do relative to the overall hardness of the
problem.

In other extensions, we are exploring the

feasibility of inferring the likelihood that an instance
is solvable versus unsolvable and building models that
forecast the overall expected run time to completion
by conditioning on each situation. We are also inter­
ested in pursuing more general, dynamic observational
policies and in harnessing the value of information to
identify a set of conditional decisions about the pattern
and timing of monitoring. F inally, we are continuing
to investigate the formulation and testing of ideal poli­
cies for harnessing the predictive models to optimize
restart policies.

progress of search for graph cliquing and of cutset anal­
ysis for use in minimizing the time of probabilistic in­

ference with Bayesian networks.

9

Summary

The work was mo­

tivated by the challenge of identifying the ideal time

We presented a methodology for characterizing the run

for preprocessing graphical models for faster inference
before initiating inference, trading off reformulation

time of problem instances for randomized backtrack­
style search algorithms that have been developed to

time for inference time.

Trajectories of progress as

solve a hard class of structured constraint-satisfaction

a function of

of Bayesian network prob­

parameter s

lem instances were learned for use in dynamic de-

problems. The methods are motivated

by recent suc­

cesses with using fixed restart policies to address the

HORVITZ ET AL.

244

UAI 2001

high variance in running time typically exhibited by

[12]

backtracking search algorithms. We described two dis­
tinct formulations of problem-solving goals and b uilt

D. Beckerman , J. Breese, and K . Rommelse. Decision­
theoretic troubleshooting. CA CM, 38:3:49-57, 1995.

[13]

D . Beckerman, D . M . Chickering, C. Meek, R. Roun­
thwaite, and C. Kadie. Dependency networks for den­
sity estimation, collaborative filtering, and data visu­
alization. In Proceedings of UA I-2000, Stanford, CA,
pages 82-88. 2000.

[14]

E. Horvitz and A. Klein. Reasoning, metareasoning,
and mathematical truth: Studies of theorem proving
under limited resources. In Proceedings of UA I- 95,
pages 306-314, Montreal, Canada, August 1995. Mor­
gan Kaufmann, San Francisco.

[15]

E . J . Horvitz. Reasoning under varying and uncer­
tain resource constraints. In Proceedings of A A A I-88,
pages 1 1 1-1 16. Morgan Kaufmann, San Mateo, CA,
August 1988.

butions and feedback.

[16]



Stochastic algorithms are among the best for
solving computationally hard search and rea­
soning problems. The runtime of such pro­
cedures is characterized by a random vari­
able. Different algorithms give rise to differ­
ent probability distributions. One can take
advantage of such differences by combining
several algorithms into a portfolio, and run­
ning them in parallel or interleaving them
on a single processor.
We provide a de­
tailed evaluation of the portfolio approach
on distributions of hard combinatorial search
problems. We show under what conditions
the portfolio approach can have a dramatic
computational advantage over the best tra­
ditional methods.

1

Introduction

Randomized algorithms are among the best current
algorithms for solving computationally hard problem.
Most local search methods for solving combinatorial
optimization problems have a stochastic component,
both to generate an initial candidate solution, as well
as to choose among good local improvements during
the search. Complete backtrack-style search methods
often also use an element of randomness in their value
and variable selection in case of ties. The runtime of
these algorithms varies per run on the same problem
instance, and therefore can be characterized by a prob­
ability distribution. The performance of algorithms
can also vary dramatically among different problem
instances. In this case, we want to consider the per­
formance profile of the algorithm over a spectrum of
problem instances.
Carla P. Gomes works for Rome Laboratory
search Associate.

as

a Re­

Given the diversity in performance profiles among
algorithms, various approaches have been developed
to combine different algorithms to take into account
the computational resource constraints and to opti­
mize the overall performance. These considerations
led to the development of anytime algorithms (Dean
and Boddy 1988), decision theoretic metareasoning
and related approaches (Horvitz and Zilberstein 1996;
Russell and Norvig 1995), and algorithm portfolio de­
sign (Huberman et al. 1997). Despite the numer­
ous results obtained in these areas, so far they have
not been exploited much by the traditional commu­
nities that study hard computational problems, such
as operations research (OR), constraint satisfaction
(CSP), theorem proving, and the experimental algo­
rithms community.
In order to bridge this gap, we study the possibility of
combining algorithms in the context of the recent re­
sults concerning the inherent complexity of computa­
tionally hard search and reasoning problems. We will
provide a rigorous empirical study of the performance
profiles of several of the state-of-the-art search meth­
ods on a distribution of hard search problems. Our
search problems are based on the so-called quasigroup
completion task, defined below. For this particular
combinatorial search problem, we can vary the compu­
tational difficulty and the amount of inherent problem
structure in a controlled manner. This enables us to
study different aspects of the algorithm performance
profiles.
Our studies reveal that in many cases the performance
of a single algorithm dominates all others, on the prob­
lem class under consideration. This may be due to
the fact that heuristics are often highly tuned for par­
ticular problem domains. Having a single algorithm
that dominates over the whole spectrum of problem in­
stances prevents any possible payoff of combining dif­
ferent algorithms. However, we also identify several in­
teresting problem classes where no single method dom­
inates. We will show that on those problem classes,

Algorithm Portfolio Design: Theory vs. Practice

191

designing a portfolio of several algorithms gives a dra­

to the original problem of finding an arbitrary latin

matic improvement in terms of overall performance.

square.

In addition, we also show that a good strategy for de­

values is as a set of additional problem constraints to

signing a portfolio is to combine many short runs of

the basic structure of the quasigroup.

the same algorithm.

The effectiveness of such port­

folios explains the common practice of "restarts" for
stochastic procedures, where the same algorithm is run
repeatedly with different initial seeds for the random
number generator. (For related work on the effective­
ness of restarts, see e.g., Aldous and Vazirani 1994;
Ertel 1991; Selman and Kirkpatrick 1996.)

Another way to look at these pre-assigned

There is a natural formulation of the problem as a
Constraint Satisfaction Problem. We have a variable

for each of the N2 entries in the multiplication table of
the quasigroup, and we use constraints to capture the
requirement of having no repeated values in any row or
column. All variables have the same domain, namely
the set of elements Q of the quasigroup. Pre-assigned

Our results suggest that the various ideas on flexible

values are captured by fixing the value of some of the

computation can indeed play a significant role in al­

variables.

gorithm design, complementing the more traditional
methods for computationally hard search and reason­
ing problems.

Colbourn (1983) showed the quasigroup completion
problem to be NP-complete.

In previous work, we

identified a clear phase transition phenomenon for the

The paper is organized as follows.

In the next sec­

quasigroup completion problem (Gomes and Selman

tion, we introduce our benchmark problem domain:

1997).

the quasigroup completion problem. We also discuss

observe that the costs peak roughly around the same

See Figures 1 and 2.

From the figures, we

the theoretical complexity of the problem. In section

ratio (approximately 42% pre--assignment) for differ­

3, we give the performance distribution profiles for sev­

ent values of N. (Each data point is generated using

eral complete stochastic search methods on our prob­

1,000 problem instances. The pre-assigned values were

lem domain. Section 4, we design and evaluate various

randomly generated.) This phase transition with the

algorithm portfolios. In section 5, we summarize our

corresponding cost profile allows us to tune the diffi­

results and discuss future directions.

culty of our problem class by varying the percentage
of pre-assigned values.

2

A Structured Hard Search Problem

An interesting application area of latin squares is the
design of statistical experiments. The purpose of latin
squares is to eliminate the effect of certain system­

In order to study the performance profile of differ­

atic dependency among the data (Denes and Keedwell

ent search strategies, we derive generic distributions

197 4). Another interesting application is in scheduling

of hard combinatorial search problems from the do­

and timetabling. For example, latin squares are useful

main of finite algerbra. In particular, we consider the

in determining intricate schedules involving pairwise

quasigroup domain. A quasigroup is an ordered pair

meetings among the members of a group (Anderson

( Q,

·

)

,

where Q is a set and

()

on Q such that the equations

·

a

is a binary operation
·

x =

b and

y

·

a

::: b

a, b in
Q. The order N of the quasigroup is the cardinality of
the set Q. The best way to understand the structure
of a quasigroup is to consider the N by N multipli­

are uniquely solvable for every pair of elements

cation table as defined by its binary operation. The
constraints on

a

quasigroup are such that its multipli­

cation table defines a Latin square. This means that in
each row of the table, each element of the set Q occurs

1985).

The natural perturbation of this problem is

the problem of completing a schedule given a set pre­
assigned meetings.
The quasigroup domain has also been extensively used
in the area of automated theorem proving.

In this

community, the main interest in this domain has been
driven by questions regarding the existence and nonex­
istence of quasigroups with additional mathematical
properties (Fujita et al. 1993; Lam et al. 1989).

exactly once; similarly, in each column, each element
occurs exactly once (Denes and Keedwell 1974).
An incomplete

or

partial latin square

3

Computational Cost Profiles

P is a partially

filled N by N table such that no symbol occurs twice

We will now consider the computational cost of solv­

in a row or a column.

ing the completion problem for different search strate­

The Quasigroup Completion

Problem is the problem of determining whether the

gies. As our basic search procedure, we use a complete

remaining entries of the table can be filled in such a

backtrack-style search method.

way that we obtain a complete latin square, that is, a

such procedures can vary dramatically depending on

full multiplication table of a quasigroup. We view the

the way one selects the next variable to branch on (the

pre-assigned values of the latin square as a perturbation

"variable selection strategy") and in what order the

The performance of

192

Gomes and Selman

crdu- 11-+­
order 12 -+-­
order lJ. ·B··
order U ..��:.­
order lS ..-...•

lOGO

100

'·'
0.5
f.r.action. of prEo-asaigned eluents

Figure

1:

0.7

0.6

The Complexity of Quasigroup Completion

oL-�--�--�--��---L--�_j
0

0 .•

10

lS.
20
25
30
JS
-40
tl.l.Wbollr of backtra-cks for first solution

45

50

.-------.---..--.---r---,

(Log Scale)

.-·

order 12 +­
order 'LJ -+-··

order H ·B··
order 15 ·IC·"''

0.8

.

�

i

0.6

'

�
0

�

�

0.1

o L-----�----L--�--�
0

'
.
nuab.lt:r of ba.ektra.ek$ fQt :fit.:;:t soll.ltiQt\

10

0.2

Figure 3: Finding quasigroups of order 20 (no pre­
0.1

0.2

0.3
0' -4
0. 5
0. �
faction of proe-assiqned el1111ents

0. 7

0'

�

0. 9

assigned values).

1979).

Figure 2: Phase Transition for the Completion Prob­

called the Brelaz heuristics (Brelaz

lem

laz heuristic was originally introduced for graph color­

The Bre­

ing procedures. It provides one of the most powerful
possible values are assigned to a variable (the "value
selection strategy"). There is a large body of work in

graph-coloring and general CSP heuristics (Trick and
Johnson

1996).

both the CSP and OR communities exploring different

The Brelaz heuristic specifies a way for breaking ties in

search strategies.

the First-fail rule: If two variables have equally small

One of the most effective strategies is the so-called
First-Fail heuristic.1

In the First-Fail heuristic, the

next variable to branch on is the one with the small­
est remaining domain (i.e., in choosing a value for the
variable during the backtrack search, the search pro­
cedure has the fewest possible options left to explore
- leading to the smallest branching factor). We con­
sider a popular extension of the First-Fail heuristic,
1It's really a prerequisit for any reasonable bactrack­
In theorem proving and Boolean
style search method.
satisfiability, the rule corresponds to the powerful unit­
propagation heuristic.

remaining domains, the Brelaz heuristic proposes to
select the variable that shares constraints with the
largest number of the remaining unassigned variables.
A natural variation on this tie-breaking rule is what we
call the "reverse Berlaz" heuristic, in which preference
is given to the variable that shares constraints with
the smallest number of unassigned variables. Any re­
maining ties after the (reverse) Brelaz rule are resolved
randomly. One final issue left to specify in our search
procedure is the order in which the values are assigned
to a variable. In the standard Brelaz, value assignment
is done in lexicographical order

(i.e.,

systematic). In

our experiments, we consider four stragies:

193

Algorithm Portfolio Design: Theory vs. Practice

0.9
0.8
o.1

...
o.s

..

�:::=======::·=··�----·---

yrr

brel.tzs .......
brela.z:t
rbrelus
rbrel4zr

-+-··
·�··

·EJ··

0.1
0.1
0.2
0.1

. �--��--�--�
{'I

10

15
20
2S30
JS
-to
nwWer of be.c:hrac:k:s for Ur5t solution

45

1000
')-QO
numbe:r of ba-ck:tra-:ks fQr

SO

0.1

...

first

I

1500
solution

"

/

o.J

0.2

.

'

number of �cktuclcs for hrst sohotic>n

10

0 �--��--�---L--�
'
8
Hl
12
14
U.
!8
20
0
nll;JUlll!r (lf b«cku·ac:ks for first solutiQ:n

F igure 4: Find ing quasigroups of order 20 with 10%

Fi gure 5: Finding quasigroups of order 20 with 20%

pre-assigned values.

pre-assigned values.

•

Berlaz-S

- Berlaz with systematic value selec­

tion,

part of the p rofile .

•

Berlaz-R-- Berlaz with random value selection,

•

R-Berlaz-S

•

the overall profile; the bottom part gives the initial

- Reverse Berlaz with systematic

First, we note that that R-Brelaz-R dominates R­
Brelaz-S over the full profile. In other words, the cu­
mulative relative frequency curve for R-Brelaz-R lies

value selection, and

above that of R-Brelaz-S at every point along the x­

R-berlaz-R- Reverse Brelaz with random value

As we will see below, we often encounter such pat­

selecti on .

terns, where one strategy simply consistently outper­

axis. R-Berlaz-S, in turn, strictly dominates Brelaz-R.

forms strategies.
Figure 3, shows the performance profile of our four

strategies for the problem of finding a quasigroup of
order 20 (no pre-assigned values).

Each curve gives

the cumulative distribution obtained for each strat­

egy by solving the problem 10,000 times.

The cost

(horizontal axis) is measured in number of backtracks,
which is directly proportional to the total runtime of
our strategies. For exampl e, the figure shows that R­
Berlaz-R, finished roughly 80% of the 10,000 runs in 15
b ackt racks or less. The top panel of the figure shows

Unfortunately, this leaves no room
for combining strategies: one simply picks the best
strategy.

This may explain why some of the ideas

about combining algorithms has not received as much
attention in the traditional communities that deal with
hard computational problems.2
From the perspective of combining algorithms, what is
most interesting, however, is that in the initial part of
2There is still the issue of multiple runs with the same
method. We'll retum to this below.

194

Gomes and Selman

showing the inconsistency of a quasigroup comple­
tion problem. The instance in question has 43% pre­
assigned values. Here we again obeserve that Brelaz-S

br-elus ......_
brelaz::r -+-­
rbrel4Z.S ·El··
rbroel.,.-zr ·-M

0.9.0
o.u

is somewhat better at finding inconsistencies quickly
but again R-Brelaz-R dominates for most of the pro­
file. Again, the good initial performance of Brelaz-S

0.9.2

can be exploited by combining many short runs, as we

0.9

will see below.

o0.88
'0.86
0.84
0.82
0.8

brel.'!.%5 +-r-brelazr -t--·

Q.g

,_.

t.......--'---�----1.--�_j

0

500

1000
1500
.2Q{l{l
numbe:r of N-::k.tr-!!1-cks for tint �olution

3000

2500

0.7

g

!
0.7

b-tli!h.n:
brl!laH
rbrehc;s
rbrehzr

0.6

0.!
0.5
0.4

-+­
-+-·
-O·
-)1(�

0.3
0.2

0.1

0.3

nuaber ot backtrae.ks

0.3

1000

0.1
0 '-----�----�--�
10
t
'
0
n>aber of b•cktncks for first solution

1

0.15

.

.<>
�
0

Figure 6: Finding quasigroups of order 10 at the phase
transition.

1

<
.
..,

�

0.1

0.0'5

the profile (see bottom panel, Figure 3), Brelaz-S dom­
inates the R-Brelaz-R. Intuitively, Brelaz-S is better
than R-Berlaz-R at finding solutions quickly.

How­

ever, in the latter part of the cumulative distribution

ntmber of backt:rac:ks

(for more than five backtracks), R-Brelaz-R dominates
Brelaz-S. In a sense, R-Brelaz-R gets relatively better
when the search gets harder. As we will see in the next
section, we can exploit this in our algorithm portfolio

Figure 7: Showing inconsistency of quasigroups com­
pletion (order 10 with 43% preassigned values).

design.
Figure 4, shows the performance profiles for quasi­
groups with 10% pre-assigned values.

We see essen­

4

Portfolio Design

A

portfolio of algorithms is

tially the same pattern as in Figure 3, but the re­
gion where Brelaz-S dominates is relatively smaller.
When we increase the percentage of pre-assigned val­
ues (20% pre-assigned, Figure

5), we see that R-Brelaz­

a collection of different al­

gorithms and/or different copies of the same algorithm
running on different processors.3 Here we consider the

R completely dominates the other strategies over the

case of independent runs without interprocess commu­

whole problem spectrum. This pattern continues for

nication.

the higher numbers of pre-assigned values (Figure 6, at
the phase transition with roughly 40% pre-assigned).
Finally, Figure 7 gives the performance profile for

30ne can also consider the somewhat more general case
of interleaving the execution of algorithms on one or more
processors.

195

Algorithm Portfolio Design: Theory vs. Practice
Portfolio for 2 processors

" " ,-----,---,--.--,---.----T---,----,---.--.---,
2

bnl�zs.

{l

Portfolio for 2{l

0•

processors

., ,------.--,---.--.,--..--,
0

rbu-la r

1400

br�lazs,

20 r-

@'1 zr

0.<

�

1200

O.JS

]
�

lODO

900

�'

'"

��

<00

O.J

O.lS

�

1

iOO

O.i
btelazs,

1 rbrel4zr

0.15

L----L.--'--�---'--�--'---'--�---'--...__j
___
20

o.:t6

lns,

o.38

0 rbnlaz:r

ru.

{l.42

standard deviation {risk!

o.u

Figure 8: Portfolio for two processors combining Bre­

Figure

laz and R-Brelaz-R.

Brelaz and R-Brelaz-R.
Portfolio for

5

\l.t6

o.u

..t•n-dll.rd d"vi�tion

processors5

o.s

o.s�

o.s4

o.s-s

o.ss

lriskJ

10: Portfolio for twenty processors combining

within a set that is the best, both in terms of expected
br�ltJ:u,

r-br� a:z r

value and risk. This set of portfolios corresponds to the
efficient set or efficient frontier, following terminology

'·'

used in the theory of mathematical finance.

Within

this set, in order to minimize the risk, one has to dete­
riorate the expected value or, in order to improve the
expected value of the portfolio, one has to increase the
risk.
In this context, where we characterize a portfolio in
'(l

bt�laz�,

'i

terms of its mean and variance, combining different

rbrdll.zr

algorithms into a portfolio only makes sense if they
exhibit different probability profiles and none of them

2 brel<!�zs, J rbr-elatr
I L--����-�-�-�-�-�-�
0

'SO

lOti

l'i-D

JC.D

250

standar-d deviation

300.

�risk.l

J'SO

400

.t'iO

Figure 9: Portfolio for five processors combining Bre­
laz and R-Brelaz-R.

dominates the others over the whole spectrum of prob­
lem instances.

As noted earlier, algorithm

bution of algorithm

We are considering Las Vegas type algorithms, i.e.,

A domi­

nates algorithm B if the cumulative frequency distri­

A lies above the cumulative fre­

quency distribution of algorithm B for all points.4

stochastic algorithms that always return a model sat­

Let us consider a set of two algorithms, algorithm 1

isfying the constraints of the search problem or demon­

and algorithm 2. Let us associate a random variable

strate that no such model exists (Motwani and Ragha­

with each algorithm: AI -the number of backtracks

van 1995). The computational cost of the portfolio is

that algorithm 1 takes to find the first solution or to

therefore a random variable. The expected computa­

prove that a solution does not exist; A2 -the number

tional cost of the portfolio is simply the expected value

of backtracks that algorithm 2 takes to find the first

of the random variable associated with the portfolio

solution or to prove that a solution does not exist.

and its standard deviation is a measure of the "disper­
sion" of the computational cost obtained when using
the portfolio of algorithms.

In this sense, the stan­

dard deviation is a measure of the risk inherent to the
portfolio.

Let us assume that we have

N processors and that we

design a portfolio using n1 processors with algorithm
1 and n2 processors with algorithm 2. So,

N

=

nl +

n2. Let us define the random variable associ�ted with
this portfolio: X - the number of backtracks that the

The main motivation to combine different algorithms

portfolio takes to find the first solution or to prove that

into a portfolio is to improve on the performance of the

a solution does not exist.

component algorithms, mainly in terms of expected
computational cost but also in terms of the overall risk.
As we will show, some portfolios are strictly preferrable
to others, in the sense that they provide a lower risk
and also a lower expected computational cost.

How­

ever, in some cases, we cannot identify any portfolio

The probability distribution of X is a "weighted" prob­
ability distribution of the probability distributions of
algorithm 1 and algorithm 2.
4 Another

More precisely, the

criterion for combining algorithms into a port­

folio is given by the algorithm covariance.

196

Gomes and Selman

probability that X = x is given by the probability
that one processor takes exactly x backtracks and all
the other ones take x or more backtracks to find a
solution or to prove that a solution does not exist.
Let us assume that we have N processors and our port­
folio consists of N copies of algorithm 1. In this case,
P[X=x] is given by the probability that one proces­
sor take exactly x backtracks and the other N
1
take more than x backtracks, plus the probability that
two processors take exactly x backtracks and the other
-

(N-2) one takes more than x backtracks, etc., plus the
probability that all the processors take exactly x back­
tracks to find a solution or to prove that a solution does
not exist. The following expression gives the probabil­
ity function for such a portfolio.

N and n2

Given N processors, and let nl
P[X=x] is given by

{; ( � ) P[Al
N

=

x]i P[Al

>

0.

x](N-i)

To consider two algorithms, we have to generalize the
above expression, considering that X = x can occur
just within the processors that use algorithm 1, or just
within the processors that use algorithm 2 or within
both. As a result, the probability function for a port­
folio with two algorithms, is given by the following
expressiOn:
Given N processors, n1 such that 0 <= nl <= N ,
and n2
N - nl, P[X=x] is given by
=

�} P[Al
EE ( )
N

nl

( 7,;) P[A2

=

xfP[Al > x](nl-i')x

=

xf P[A2 > x](n2-i")j

The value of i11 is given by i11 = i i', and the term in
the summation is 0 whenever i11 < 0 or i11 > n2.
-

In the case of a portfolio involving two algorithms the
probability distribution of the portfolio is a summation
of a product of two expressions, each one correspond­
ing to one algorithm. In the case of a portfolio com­
prising M different algorithms, this probability func­
tion can be easily generalized, by having a summation
of a product of M expressions, each corresponding to
an algorithm.
Once we derive the probability distribution for the ran­
dom variable associated with the portfolio, the calcu­
lation of the its expected valt.�e and standard deviation
is straightforward.

4.1

Empirical results for portfolio design

We now design different portfolios based on our perfor­
mance profiles from Section 3. We focus on the case of
finding a quasigroup of order 20 with no-preassigned
values. The performance profiles are given in Figure
3. Note that this is an interesting case from the port­
folio design perspective because Brelaz-S dominates in
the initial part of the distribution, whereas R-Brelaz-R
dominates in the latter part.
Figures 8, 9, and 10 give the expected values and the
standard deviations of portfolios for 2, 5, and 20 pro­
cessors, respectively. (Results derived using the for­
mula given above.) We see that for 2 processors (Fig­
ure 8), the portfolio consisting of two copies of the
R-Brelaz-R has the best expected value and the low­
est standard deviation. This portfolio dominates the
two other 2-processor portfolios.
When we increase the number of processors, we ob­
serve an interesting shift in the optimal portfolio mix.
For example, for 5 processors, using 2 Brelaz-S gives a
better expected value at only a slight increase in the
risk (standard deviation) compared to zero Brelaz-S.
In this case, the efficient set comprises three portfo.­
lios. One with 5 R-Brelaz-R, one with 1 Brelaz-S and
4 R-Brelaz-R, and one with 2 Brelaz-S and 3 R-Brelaz­
R. The situation changes even more dramatically if
we go to yet more processors. In particular, with 20
processors (Figure 10), the best portfolio corresponds
to using all processors to run the Brelaz-S strategy
(the lowest expected value and the lowest standard
deviation). The intuitive explanantion for this is that
by running many copies of Brelaz-S, we have a good
chance that at least one of them will find a solution
quickly. This result is consistent with the common
use of "random restarts" in stochastic search methods
in practical applications. Our portfolio analysis also
gives the somewhat counter-intuitive result that, even
when given two stochastic algorithms, where neither
strictly dominates the other, running multiple copies
of a single algorithm is preferrable to a mix of algo­
rithms (Figure 8 and Figure 10).

5

Conclusions and Future Work

We have provided concrete empirical results showing
the computational advantage of a portfolio approach
for dealing with hard combinatorial search and rea­
soning problems as compared to the best more tra­
ditional single algorithm methods. Our analysis also
showed what properties of the problem instance distri­
butions lead to the largest payoff for using a portfolio
approach in practice. Finally, we saw how the use of
random restarts of a good stochastic method is often

Algorithm Portfolio Design: Theory vs. Practice

the optimal strategy. These results suggest that ideas
developed in the flexible computation community can
play a significant role in practical algorithm design.
Acknowledgments

We would like to thank Karen Alguire for developing
an exciting tool for experimenting with the quasigroup
completion problem. We also would like to thank Nort
Fowler for many useful suggestions and discussions,
and Neal Glassman for suggesting the domain of com­
binatorial design as a potential benchmark domain.
The first author is a research associate with Rome
Laboratory and is funded by the Air Force Office of
Scientific Research, under the New World Vistas Ini­
tiative (F30602-97-C-0037 and AFOSR NWV project
2304, LIRL 97RL005N25).

Horvitz, E. and Klein, A. (1995) Reasoning, metareason­
ing, and mathematical truth: studies of theorem prov­
ing under limited resources.
Proc. of the Eleventh
Conference on Uncertainty in Artificial Intelligence
(UAI-95}, August 1995.
Horvitz, E. and Z ilberstein S. (1996) ( Eds. ) Proceedings of
Flexible Computation, AAAI Fall Symposium, Cam­
bridge, MA, 1996.
Huberman, B.A., Lukose, R.M., and Hogg, T. (1997). An
economics approach to hard computational problems.
Science, 265, 51-54.
Hogg, T., Huberman, B.A., and W illiams , C.P. (Eds.)
(1996). Phase Transitions and Complexity. Artificial
Intelligence, 81 (Spec. Issue; 1996)
Kirkpatrick, S. and Selman, B. (1994) Critical Behavior
in the Satisfiability of Random Boolean Expressions.
Science, 264 (May 1994) 1297-1301.
Lam, C., Thiel, L., and Swiercz, S. (1989) Can. J. Math.,
Vol. XLI, 6, 1989, 1117-1123.



Survey propagation (SP) is an exciting new
technique that has been remarkably successful at solving very large hard combinatorial
problems, such as determining the satisfiability of Boolean formulas. In a promising
attempt at understanding the success of SP,
it was recently shown that SP can be viewed
as a form of belief propagation, computing
marginal probabilities over certain objects
called covers of a formula. This explanation was, however, shortly dismissed by experiments suggesting that non-trivial covers
simply do not exist for large formulas. In
this paper, we show that these experiments
were misleading: not only do covers exist for
large hard random formulas, SP is surprisingly accurate at computing marginals over
these covers despite the existence of many
cycles in the formulas. This re-opens a potentially simpler line of reasoning for understanding SP, in contrast to some alternative
lines of explanation that have been proposed
assuming covers do not exist.

1

INTRODUCTION

Survey Propagation (SP) is a new exciting algorithm
for solving hard combinatorial problems. It was discovered by Mezard, Parisi, and Zecchina (2002), and
is so far the only known method successful at solving
random Boolean satisfiability (SAT) problems with 1
million variables and beyond in near-linear time in the
hardest region. The SP method is quite radical in that
it tries to approximate certain marginal probabilities
related to the set of satisfying assignments. It then
iteratively assigns values to variables with the most
∗

Research supported by Intelligent Info. Systems Instt.
(IISI), Cornell Univ., AFOSR grant FA9550-04-1-0151.

extreme probabilities. In effect, the algorithm behaves like the usual backtrack search methods for SAT
(DPLL-based), which also assign variable values incrementally in an attempt to find a satisfying assignment.
However, quite surprisingly, SP almost never has to
backtrack. In other words, the “heuristic guidance”
from SP is almost always correct. Note that, interestingly, computing marginals on satisfying assignments
is actually believed to be much harder than finding
a single satisfying assignment (#P-complete vs. NPcomplete). Nonetheless, SP is able to efficiently approximate certain marginals and uses this information
to successfully find a satisfying assignment.
SP was derived from rather complex statistical physics
methods, specifically, the so-called cavity method developed for the study of spin glasses. Close connections
to belief propagation (BP) methods were subsequently
discovered. In particular, it was discovered by Braunstein and Zecchina (2004) (later extended by Maneva,
Mossel, and Wainwright (2005)) that SP equations are
equivalent to BP equations for obtaining marginals
over a special class of combinatorial objects, called
covers. Intuitively, a cover provides a representative
generalization of a cluster of satisfying assignments.
The discovery of a close connection between SP and
BP via the use of covers laid an exciting foundation
for explaining the success of SP. Unfortunately, subsequent experimental evidence suggested that hard random 3-SAT formulas have, with high probability, only
one (trivial) cover (Maneva et al., 2005). This would
leave all variables effectively in an undecided state, and
would mean that marginals on covers cannot provide
any useful information on how to set variables. Since
SP clearly sets variables in a non-trivial manner, it
was conjectured that there must be another explanation for the good behavior of SP; in particular, one
that is not based on the use of marginal probabilities
of variables in the covers.
In this paper, we revisit the claim that hard random 3SAT formulas do not have interesting non-trivial cov-

218

KROC ET AL.

ers. In fact, we show that such formulas have large
numbers of non-trivial covers. The main contribution
of the paper is the first clear empirical evidence showing that in random 3-SAT problems near the satisfiability and hardness threshold, (1) a significant number of non-trivial covers exist; (2) SP is remarkably
good at computing variable marginals based on covers; and (3) these cover marginals closely relate to solution marginals at least in the extreme values, where
it matters the most for survey inspired decimation. As
a consequence, we strongly suspect that explaining SP
in terms of covers may be the correct path after all.
Note that (2) above is quite surprising for random
3-SAT formulas because such formulas have many
loops. The known formal proof that SP computes
cover marginals only applies to tree-structured formulas, which in fact have only a single (trivial) cover.
Further, it’s amazing that while SP computes such
marginals in a fraction of a second, the next best methods of computing these marginals that we know of (via
exact enumeration, or sampling followed by “peeling”)
require over 100 CPU hours.
Our experiments also indicate that cover marginals
are more “conservative” than solution marginals in the
sense that variables that are extreme with respect to
cover marginals are almost certainly also extreme with
respect to solution marginals, but not vice versa. This
sheds light on why it is safe to set variables with extreme cover marginals in an iterative manner, as is
done in the survey inspired decimation process for finding a solution using the marginals computed by SP.
In addition to these empirical results, we also revisit
the derivation of the SP equations themselves, with the
goal of presenting the derivation in an insightful form
purely within the realm of combinatorial constraint
satisfaction problems (CSPs). We describe how one
can reformulate in a natural step-by-step manner the
problem of finding a satisfying assignment into one of
finding a cover, by considering related factor graphs
on larger state spaces. The BP equations for this reformulated problem are exactly the SP equations for
the original problem, as shown in the Appendix.

2

COVERS OF CNF FORMULAS

We start by introducing the notation and the basic concepts that we use throughout the paper. We
are concerned with Boolean formulas in Conjunctive
Normal Form or CNF, that is, formulas of the form
F ≡ (l11 ∨ . . . ∨ l1k1 ) ∧ . . . ∧ (lm1 ∨ . . . ∨ lmkm ), where
each lik (called a literal ) is a Boolean variable xj or
its negation ¬xj . Each conjunct of F , which itself is
a disjunction of literals, is called a clause. In 3-CNF
or 3-SAT formulas, every clause has 3 literals. Ran-

dom 3-SAT formulas over n variables are generated by
uniformly randomly choosing a pre-specified number
of clauses over these n variables. The Boolean satisfiability problem is the following: Given a CNF formula F over n variables, find a truth assignment σ for
the variables such that every clause in F evaluates to
true; σ is called a satisfying assignment or a solution
of F . We identify true with 1 and false with 0.
A truth assignment to n variables can be viewed as a
string of length n over the alphabet {0, 1}, and extending this alphabet to include a third letter “∗” leads to
a generalized assignment. A variable with the value ∗
can be interpreted as being “undecided,” while variables with values 0 or 1 can be interpreted as being
“decided” on what they want to be. We will be interested in certain generalized assignments called covers.
Our formal definition of covers follows the one given by
Achlioptas and Ricci-Tersenghi (2006). Let variable x
be called a supported variable under a generalized assignment σ if there is a clause C such that x is the
only variable that satisfies C and all other literals of
C are false. Otherwise, x is called unsupported.
Definition 1. A generalized assignment σ ∈ {0, 1, ∗}n
is a cover of a CNF formula F iff
1. every clause of F has at least one satisfying literal
or at least two literals with value ∗ under σ, and
2. σ has no unsupported variables assigned 0 or 1.
The first condition ensures that each clause of F is
either already satisfied by σ or has enough undecided
variables to not cause any undecided variable to be
forced to decide on a value (no “unit propagation”).
The second condition says that each variable that is
assigned 0 or 1 is set that way for a reason: there
exists a clause that relies on this setting in order to
be satisfied. For example, consider the formula F ≡
(x ∨ ¬y ∨ ¬z) ∧ (¬x ∨ y ∨ ¬z) ∧ (¬x ∨ ¬y ∨ z). F has
exactly two covers: 111 and ∗ ∗ ∗. This can be verified
by observing that whenever some variable is 0 or ∗,
then all non-∗ variables are unsupported. Notice that
the string of all ∗’s always satisfies the conditions in
Definition 1; we refer to this string as the trivial cover.
Covers were introduced by Maneva et al. (2005) as
a useful concept to analyze the behavior of SP, but
their combinatorial properties are much less known
than those of solutions. A cover can be thought of as
a partial assignment to variables, where the variables
assigned ∗ are considered unspecified. In this sense,
each cover is a representative of a potentially large set
of complete truth assignments, satisfying as well as not
satisfying. This motivates further differentiation:
Definition 2. A cover σ ∈ {0, 1, ∗}n of F is a true
cover iff there exists a satisfying assignment τ ∈
{0, 1}n of F such that σ and τ agree on all values where

KROC ET AL.
σ is not a ∗, i.e., ∀i ∈ {1, . . . , n}(σi 6= ∗ =⇒ σi = τi ).
Otherwise, σ is a false cover.
A true cover thus generalizes at least one satisfying
assignment. True covers are interesting to study when
trying to satisfy a formula, because if there exists a
true cover with variable x assigned 0 or 1, then there
must also exist a satisfying assignment with the same
setting of x.
One can construct a true cover σ ∈ {0, 1, ∗}n of F by
starting with any satisfying assignment τ ∈ {0, 1}n of
F and generalizing it using a simple procedure called
∗-propagation.1 The procedure starts by initially
setting σ = τ . It then repeatedly chooses an arbitrary
variable unsupported under σ and turns it into a ∗,
until there are no more unsupported variables. The resulting string σ is a true cover, which can be verified as
follows. The satisfying assignment τ already satisfies
the first condition in Definition 1, and ∗-propagation
does not destroy this property. In particular, a variable on which some clause relies is never turned into a
∗. The second condition in Definition 1 is also clearly
satisfied when ∗-propagation halts, so that σ must be
a cover. Moreover, since σ generalizes τ , it is a true
cover. Note that ∗-propagation can, in principle, be
applied to an arbitrary generalized assignment. However, unless we start with one that satisfies the first
condition in the cover definition, ∗-propagation may
not lead to a cover.
We end with a discussion of two insightful properties
of covers. The first relates to “self-reducibility” and
the second to covers for tree-structured formulas.
No self-reducibility. Consider the relation between
the decision and search versions of the problem of finding a solution of a CNF formula F . In the decision version, one needs an algorithm that determines whether
or not F has a solution, while in the search version,
one needs an algorithm that explicitly finds a solution. The problem of finding a solution for F is selfreducible, i.e., given an oracle for the decision version,
one can efficiently solve the search version by iteratively fixing variables to 1 or 0, testing whether there
is still a solution, and continuing in this way. Somewhat surprisingly, this strategy does not work for the
problem of finding a cover. In other words, an oracle
for the decision version of this problem does not immediately provide an efficient algorithm for finding a
cover. (The lack of self-reducibility makes it very hard
to find covers as we will see below.) As a concrete
example, consider the formula F described right after
Definition 1. To construct a cover of F , we could ask
1
This was introduced under different names as the peeling procedure or coarsening, e.g., by Maneva et al. (2005).

219

whether there exists a cover with x set to 1. Since
111 is a cover (yet unknown to us), the decision oracle
would say yes. We could then fix x to 1, simplify the
formula to (y ∨ ¬z) ∧ (¬y ∧ z), and ask whether there is
a cover with y set to 0. This residual formula indeed
has 00 as a cover, and the oracle would say yes. With
one more query, we will end up with 100 as the values
of x, y, z, which is in fact not a cover of F .
Tree-structured formulas. For tree-structured
formulas without unit clauses, i.e., formulas whose factor graph does not have a cycle, the only cover is the
trivial all-∗ cover. We argue this using the connection between covers and SP shown by Braunstein and
Zecchina (2004), which says that when generalized assignments have a uniform prior, SP on a tree formula F
provably computes probability marginals of variables
being 0, 1, and ∗ in covers of F . Moreover, it can be
verified from the iterative equations for SP that with
no unit clauses, zero marginals for any variable being
0 or 1, and full marginals for any variable being a ∗ is
a fixed point of SP. Since SP provably has exactly one
fixed point on tree formulas, it follows that the only
cover of such formulas is the trivial all-∗ cover.

3

PROBLEM REFORMULATION:
FROM SOLUTIONS TO COVERS

We now show that the concept of covers can be quite
naturally arrived at when trying to find solutions of
a CNF formula, thus motivating the study of covers
from a purely generative perspective. Starting with
a CNF formula F , we describe how F is transformed
step-by-step into the problem of finding covers of F ,
motivating each step.
Although our discussion applies to any CNF formula
F , we will be using the following example formula with
3 variables and 4 clauses to illustrate the steps:
(x ∨ y ∨ ¬z) ∧ (¬x ∨ y) ∧ (¬y ∨ z) ∧ (x ∨ ¬z)
|
{z
} | {z } | {z } | {z }
a

b

c

d

Let N denote the number of variables, M the number
of clauses, and L the number of literals of F .
Original problem. The problem is to find an asN
signment in the space {0, 1} that satisfies F . The factor graph for F has N variable nodes and M function
nodes, corresponding directly to the variables x, y, . . .
and clauses a, b, . . . in F (see e.g. Kschischang et al.
(2001)). The factor graph for the example formula
is depicted below. Here factors Fa , Fb , . . . represent
predicates ensuring that the corresponding clause has
at least one satisfying literal.

220

KROC ET AL.
x

Fa

y

z

Fb

The solutions to this modified problem do not necessarily correspond directly to solutions of the original
one. In particular, if there are no unit clauses and all
variables are set to ∗, the problem is already “solved”
without providing any useful information.

Fc

Fd

Variable occurrences. The first step in the transformation is to start treating every variable occurrence
xa , xb , ya , yb , . . . in F as a separate unit that can be either 0 or 1. This allows for more flexibility in the process of finding a solution, since a variable can decide
what value to assume in each clause separately. Of
course, we need to add constraints to ensure that the
occurrence values are eventually consistent: for every
variable x in F , we add a constraint Fx that all occurrences of x have the same value. Now the search space
is {0, 1}L, and the corresponding factor graph contains
L variable nodes and M + N function nodes (the original clause factors Fa , Fb , . . . and the new constraints
Fx , Fy , . . .).
xa

Fx

xb

Fa

xd

ya

Fb

yb

Fy

yc

za

Fc

zc

Fd

zd

Fz

At this point, we have not relaxed solutions to the
original problem F : solutions to the modified problem
correspond precisely to the original solutions, because
variable occurrences are forced to be consistent. However, we moved this consistency check from the syntactic level (variables could not be inconsistent simply
by the problem definition) to the semantic level (we
have special constraints to guarantee consistency).
Relaxing assignments. The next step is to relax
the problem by allowing variable nodes to assume the
special value “∗”. The semantics of ∗ is “undecided,”
meaning that the variable node is set neither to 0
L
nor to 1. The new search space is {0, 1, ∗} , and we
must specify how our constraints handle the value ∗.
Variable constraints Fx , . . . have the same meaning as
before, namely, all variable nodes xa , xb , . . . have the
same value for every variable x. Clause constraints
Fa , . . . now have a modified meaning: a clause is satisfied if it contains at least one satisfying literal or at
least two literals with the value ∗. The motivation here
is to either satisfy a clause or leave enough “freedom”
in the form of at least two undecided variables. (A
single undecided variable would be forced to take on a
particular value if all other literals in the clause were
falsified.) With this transformation, the factor graph
remains structurally the same, while the set of possible
values for variable nodes changes.

Reducing freedom of choice. To distinguish variables that could assume the value ∗ from those that
truly need to be fixed to either 0 or 1, we require that
every non-∗ variable has a clause that needs the variable to be 0 or 1 in order to be satisfied. The search
space does not change, but we need to add constraints
to implement the reduction in the freedom of choice.
Notice that this requirement is equivalent to “no unsupported variables” in the definition of a cover, and
that the first requirement in that definition is fulfilled by the clause constraints. Therefore, we are now
searching for covers of F . A natural way to represent
the “no unsupported variable” constraint in the factor graph is to add for each variable x a new function
node Fx0 , connected to the variable nodes for x as well
as for all other variables sharing a clause with x. This,
of course, creates many new links and introduces additional short cycles, even if the original factor graph
was acyclic. The following transformation step alleviates this issue.
Reinterpreting variable nodes. As the final step,
we change the semantics of the variable nodes’ values and of the constraints so that the “no unsupported variable” condition can be enforced without additional function nodes. The reasoning is that the simple {0, 1, ∗} domain creates a bottleneck for how much
information can be communicated between nodes in
the factor graph. By altering the semantics of the
variable nodes’ values, we can improve on this.
The new value of a variable node xa will be a pair
(ra→x , wx→a ) ∈ {(0, 0), (0, 1), (1, 0)}, so that the size
of the search space is still 3L . We interpret the value
ra→x as a request from clause a to variable x with the
meaning that a relies on x to satisfy it, and the value
wx→a as a warning from variable x to clause a that x is
set such that it does not satisfy a. The values 1 and 0
indicate presence and absence, resp., of the request or
warning. We can recover the original {0, 1, ∗} values
from these new values as follows: if ra→x = 1 for some
a, then x is set to satisfy clause a; if there is no request
from any clause where x appears, then x is undecided
(a value of ∗ in the previous interpretation). The variable constraints Fx , . . . not only ensure consistency of
the values of xa , xb , . . . as before, but also ensure the
second cover condition as described below. The clause
constraints Fa , . . . remain unchanged.
The variable constraint Fx is a predicate ensuring that

KROC ET AL.
the following two conditions are met:
1. if ra→x = 1 for any clause a where x appears,
then wx→b = 0 for all clauses b where x appears
with the same sign as in a, and wx→b = 1 for all
b where x appears with the opposite sign. Since x
must be set to satisfy a, this ensures that clauses
that are unsatisfied by x do receive a warning.
2. if ra→x = 0 for all clauses a where x appears, then
wx→a = 0 for all of them, i.e., no clause receives
a warning from x.
To evaluate Fx , values (ra→x , wx→a ) are needed only
for clauses a in which x appears, which is exactly the
set of variable nodes the factor Fx is connected to. Notice that the case (ra→x , wx→a ) = (1, 1) cannot happen
due to condition 1 above. The conditions also imply
that the variable occurrences of x are consistent, and in
particular that two clauses where x appears with opposite signs (say a and b) cannot simultaneously request
to be satisfied by x. This is because either ra→x = 0
or rb→x = 0 must hold due to condition 1.
The clause constraint Fa is a predicate stating that
clause a issues a request to its variable x if and only if it
receives warnings from all its other variables: ra→x = 1
iff wy→a = 1 for all variables y 6= x in a. Again, Fa
can be evaluated using exactly values from the variable
nodes it is connected to.
When clause a issues a request to variable x (i.e.,
ra→x = 1), x must be set to satisfy a, thus providing a
satisfying literal for a. If a does not issue any request,
then according to the condition of Fa , at least two of
a’s variables, say x and y, must not have sent a warning. In this case, Fx and Fy state that each of x and
y is either undecided or satisfies a. Thus the first condition in the cover definition holds in any solution of
this new constraint satisfaction problem. The second
condition also holds, because every variable x that is
not undecided must have received a request from some
clause a, so that x is the only literal in a that is not
false. Therefore x is supported.
Let us denote this final constraint satisfaction problem
by P (F ). (It is a function of the original formula F .)
Notice that the factor graph of P (F ) has the same
topology as the factor graph of F . In particular, if
F has a tree factor graph, so does P (F ). Further, by
the construction of P (F ) described above, its solutions
correspond precisely to the covers of F .
3.1

INFERENCE OVER COVERS

This section discusses an approach for solving the
problem P (F ) with probabilistic inference using belief
propagation (BP). It arrives at the survey propagation
equations for F by applying BP equations to P (F ).

221

Since the factor graph of P (F ) can be easily viewed
as a Bayesian Network (cf. Pearl, 1988), one can compute marginal probabilities over the set of satisfying
assignments of the problem, defined as
Pr[xa = v | all constraints of P (F ) are satisfied]
for each variable node xa and v ∈ {(0, 0), (0, 1), (1, 0)}.
The probability space here is over all assignments to
variable nodes with uniform prior.
Once these solution marginals are known, we know
which variables are most likely to assume a particular
value, and setting these variables simplifies the problem. A new set of marginals can be computed on this
simplified formula, and the whole process repeated.
This method of searching for a satisfying assignment
is called the decimation procedure. The problem,
of course, is to compute the marginals (which, in general, is much harder than finding a satisfying assignment). One possibility for computing marginals is to
use the belief propagation algorithm (cf. Pearl, 1988).
Although provably correct essentially only for formulas
with a tree factor graph, BP provides a good approximation of the true marginals in many problem domains
in practice (Murphy et al., 1999). Moreover, as shown
by Maneva et al. (2005), applying the BP algorithm to
the problem of searching for covers of F results in the
SP algorithm. Thus, on formulas with a tree factor
graph, the SP algorithm provably computes marginal
probabilities over covers of F , which are equivalent to
marginals over satisfying assignments of P (F ). When
the formula contains loops, SP computes a loopy approximation to the cover marginals. Specific details of
the derivation of SP equations from the problem P (F )
are deferred to the Appendix.

4

EXPERIMENTAL RESULTS

This section presents our main contributions. We begin by demonstrating that non-trivial covers do exist in large numbers in random 3-SAT formula, and
then explore connections between SP, BP, and variable marginals computed from covers as well as solutions, showing in particular that SP approximates
cover marginals surprisingly well.
4.1

EXISTENCE OF COVERS

Motivated by theoretical results connecting SP to covers of formulas, Maneva et al. (2005) suggested an
experimental study to test whether non-trivial covers
even exist in random 3-SAT formulas. They proposed
a seemingly good way to do this (the “peeling experiment”), namely, start with a uniformly random satisfying assignment of a formula F and, while it has unsupported variables, ∗-propagate the assignment. When

1000
200

400

600

800

Solutions leading to the trivial cover
Solutions leading to non−trivial covers

0

Number of unsupported variables

0

1000

2000

3000

Number of stars

4000

5000

Figure 1: The peeling experiment, showing the evolution of the number of stars as ∗-propagation is performed.
To understand this issue better, we ran the same peeling experiment on a 5000 variable random 3-SAT formula at clause-to-variable ratio 4.2 (which is close to
the hardness threshold for random 3-SAT problems),
but used SampleSat (Wei et al., 2004) to obtain samples, which is expected to produce fairly uniform samples. Figure 1 shows the evolution of the number of
unsupported variables at each stage as ∗-propagation
is performed starting from a solution. Here, the
x-axis shows the number of stars, which monotonically increases by ∗-propagation. The y-axis shows
the number of unsupported variables present at each
stage. As one moves from left to right following the
∗-propagation process, one hits a cover if the number of unsupported variables drops to zero (so that
∗-propagation terminates). The two curves in the plot
correspond to solutions that ∗-propagated to the trivial cover and those that did not. In our experiment,
out of 500 satisfying assignments used, nearly 74% led
to the trivial cover; their average is represented by the
top curve. The remaining 26% of the sampled solutions actually led to non-trivial covers; their average
is represented by the bottom curve. Thus, when solutions are sampled near-uniformly, a substantial fraction of them lead to non-trivial covers.2
2
That this was not observed by Maneva et al. (2005)
can be attributed to the fact that SP was used to find
satisfying assignments (Mossel, 2007), resulting in highly
non-uniform samples.

100
40

60

80

90 vars
70 vars
50 vars

0

20

1.0
0.8
0.6
0.4

P[non−trivial cover]

90 vars
70 vars
50 vars

0.2

the process terminates, one obtains a (true) cover of
F . Unfortunately, what they observed is that this process repeatedly hits the trivial all-∗ cover, from which
they concluded that non-trivial covers most likely do
not exist for such formulas. However, it is known that
near-uniformly sampling solutions of such formulas to
start with is a hard problem in itself and that most
sampling methods obtain solutions in a highly nonuniform manner (Wei et al., 2004). Consequently, one
must be careful in drawing conclusions from relatively
few and possibly biased samples.

Number of non−trivial covers

KROC ET AL.

0.0

222

2.0

2.5

3.0

3.5

4.0

Clause−to−variable ratio

4.5

2.0

2.5

3.0

3.5

4.0

Clause−to−variable ratio

4.5

Figure 2: Non-trivial covers in random formulas. Left:
existence probability. Right: average number.
An alternative method of finding covers is to create a
new Boolean formula G whose solutions correspond go
the covers of F . It turned out to be extremely hard
to solve G to find any non-trivial cover using state-ofthe-art SAT solvers for number of variables as low as
150. So we confined our experiments to small formulas, with 50, 70 and 90 variables. We found all covers
for such formulas with varying clause-to-variable ratios α. The results are shown in Figure 2, where each
data point corresponds to statistics obtained from 500
formulas. The left pane shows the probability that a
random formula, for a given clause-to-variable ratio,
has at least one non-trivial cover (either true or false).
The figure shows a nice phase transition where covers appear, at around α = 2.5, which is surprisingly
sharp given the small formula sizes. Also, the region
where covers surely exist is widening on both sides
as the number of variables increases, supporting the
claim that non-trivial covers exist even in large formulas. The right pane of Figure 2 shows the actual
number of non-trivial covers, with a clear trend that
the number increases with the size of the formula, for
all values of the clause-to-variable ratio. It is worth
noting that the number of covers is very small compared to the number of satisfying assignments; e.g.
for 90 variables and α = 4.2, the expected number of
satisfying assignments is 150, 000, while there are only
8 covers on average. Somewhat surprisingly, the number of false covers is almost negligible, around 2 at the
peak, and does not seem to be growing nearly as fast
as the total number of covers. This might explain why
SP, although approximating marginals over all covers,
is successful in finding satisfying assignments.
We also consider how the number of solutions that lead
to non-trivial covers changes for larger formulas, as the
number of variables N increases from 200 to 4000. The
left pane of Figure 3 shows that this number, in fact,
grows exponentially with N . The number is computed
by averaging over 20,000 sampled solutions from 200
formulas at ratio 4.2 for each N , estimating the fraction p(N ) of these that lead to a non-trivial cover,
and scaling it up by the expected number of solutions,

KROC ET AL.

0.5
0.4
0.3
0.2

P[non trivial cover]

0.0

0.1

1e+144
1e+56
1e+12

E[#sols w/ nontr. cover]
(log scale)

0.6

which is (2 × (7/8)4.2 )N ≈ 1.1414N . (The number of
solutions for such formulas at ratio 4.2 is known to
be highly concentrated around its expectation.) The
resulting number, p(N ) × 1.1414N , is plotted on the yaxis of the left pane, with N on the x-axis. The right
pane of Figure 3 shows the data used to estimate the
fraction p(N ) along with its fit on the y-axis, with N
on the x-axis again.

200

500

1000

2000

Number of Vars. (log scale)

1000

2000

3000

Number of Variables

4000

Figure 3: Left: Expected number of solutions leading
to non-trivial covers (log-log scale). Right: Probability
of a solution leading to a non-trivial cover.
Notice that the left pane is in log-scale for both axes,
and clearly increases faster than a linear function.
This shows that the expected number of solutions that
lead to non-trivial covers grows super-polynomially. In
fact, performing a best fit for this curve suggests that
this number grows exponentially, roughly as 1.1407N .
This number is indeed a vanishingly small fraction of
the expected number of solutions (1.1414N ) as observed by Maneva et al. (2005), but nonetheless exponentially increasing. The existence of covers for
random 3-SAT also aligns with what Achlioptas and
Ricci-Tersenghi (2006) recently proved for k-SAT with
k ≥ 9.
4.2

SP, BP, AND MARGINALS

We now study the behavior of SP and BP on a random formula in relation to solutions and covers of
that formula. While theoretical work has shown that
SP, viewed as BP on a related combinatorial problem,
provably computes cover marginals on tree-structured
formulas, we demonstrate that even on random 3-SAT
instances, which are far from tree-like, SP approximates cover marginals surprisingly well. We also show
that cover marginals, especially in the extreme range,
are closely related to solution marginals in an intriguing “conservative” fashion. The combination of these
two effects, we believe, plays a crucial role in the success of SP. Our experiments also reveal that BP performs poorly at computing any marginals of interest.
Given marginal probabilities, we define the magnetization of a variable to be the difference between the
marginals of the variable being positive and it being

223

negative. For the rest of our experiments, we start
with a random 3-SAT formula F with 5000 variables
and 21000 clauses (clause-to-variable ratio of 4.2), and
plot the magnetization of the variables of F in the
range [−1, +1].3 The marginals for magnetization are
obtained from four different sources, which are compared and contrasted against each other: (1) by running SP on F till the iterations converge; (2) by running BP on F but terminating it after 10,000 iterations because the equations do not converge; (3) by
sampling solutions of F using SampleSat and computing an estimate of the positive and negative marginals
from the sampled solutions (the solution marginals);
and (4) by sampling solutions of F using SampleSat,
∗-propagating them to covers, and computing an estimate of the positive and negative marginals from these
covers (the cover marginals). Note that in (4), we are
sampling true covers and obtaining an estimate. An
alternative approach is to use SP itself on F to try to
sample covers of F , but the issue here is that the problem of finding (non-trivial) covers is not self-reducible
to the decision problem of whether covers exist, as
shown in Section 2. Therefore, it is not clear whether
SP can be used to actually find a cover, despite it approximating the cover marginals very well.
Recall that the SP-based decimation process works by
identifying variables with extreme magnetization, fixing them, and iterating. We will therefore be interested mostly in what happens in the extreme magnetization regions in these plots, namely, the lower left
corner (−1, −1) and the upper right corner (+1, +1).
In the left pane of Figure 4 we plot the magnetization
computed by SP on the x-axis and the magnetization
obtained from cover marginals on the y-axis. The scatter plot has exactly 5000 data points, with one point
for each variable of the formula F . If the magnetizations on the two axes matched perfectly, all points
would fall on a single diagonal line from the bottomleft corner to the top-right corner. The plot shows that
SP is highly accurate at computing cover marginals, especially in the extreme regions at the bottom-left and
top-right.
The middle pane of Figure 4 compares the magnetization based on cover marginals with the magnetization based on solutions marginals. This will provide
an intuition for why it might be better to follow cover
marginals rather than solution marginals when looking
for a satisfying assignment.4 We see an interesting “s3

For clarity, the plots show magnetizations for one such
formula, although the trend is generic.
4
Of course, if solution marginals could be computed
perfectly, this would not be an issue. In practice, however, the best we can hope is to approximately estimate
marginals.

−1.0

−0.5

0.0

SP Magnetization

0.5

1.0

0.5
0.0

Solution Magnetization

−1.0

−0.5

1.0
0.5
0.0

Solution Magnetization

−1.0

−0.5

0.5
0.0
−1.0

Cover Magnetization

−0.5

1.0

KROC ET AL.

1.0

224

−1.0

−0.5

0.0

0.5

Cover Magnetization

1.0

−1.0

−0.5

0.0

0.5

1.0

BP Magnetization

Figure 4: Magnetization plots. Left: SP vs. covers. Middle: covers vs. solutions. Right: BP vs. solutions.
shape” in this plot, which can be interpreted as follows:
fixing variables with extreme cover magnetizations is
more conservative compared to fixing variables with
extreme solution magnetizations. Which means that
variables that are extreme w.r.t. cover-based magnetization are also extreme w.r.t. solution-based magnetization (but not necessarily vice-versa). Recall that
the extreme region is exactly where decimation-based
algorithms, that often fix a small set of extreme variables per iteration, need to be correct. Thus, etimates
of cover marginals provide a safer heuristic for fixing
variables than estimates of solution marginals.
As a comparison with BP, the right pane of Figure 4
shows BP magnetization vs. magnetization based on
solution marginals for the same 5000 variable, 21000
clause formula. Since BP almost never converges on
such formulas, we terminated BP after 10,000 iterations (SP took roughly 50 iterations to converge) and
used the partially converged marginals obtained so far
for computing magnetization. The plot shows that BP
provides very poor estimates for the magnetizations
based on solution marginals. (The points are equally
scattered when BP magnetization is plotted against
cover magnetization.) In fact, BP appears to identify
as extreme many variables that have the opposite solution magnetization. Thus, when magnetization obtained from BP is used as a heuristic for identifying
variables to fix, mistakes are often made that eventually lead to a contradiction, i.e. unsatisfiable reduced
formula.

5

DISCUSSION

A comparison between left and right panes of Figure 4
suggests that approximating statistics over covers (as
done by SP) is much more accurate than approximating statistics over solutions (as done by BP). This
appears to be because covers are much more coarse
grained than solutions; indeed, even an exponentially
large cluster of solutions will have only a single cover

as its representative. This cover still captures critical
properties of the cluster necessary for finding solutions,
such as backbone variables, which is what SP appears
to exploit.
We also saw that the extreme magnetization based on
cover marginals is more conservative than that based
on solution marginals (as seen in the “s-shape” of
the plot in the middle pane of Figure 4). This suggests that while SP, based on approximating cover
marginals, may miss some variables with extreme magnetization, when it does find a variable to have extreme magnetization, it is quite likely to be correct.
This provides an intuitive explanation of why the decimation process based on extreme SP magnetization
succeeds with high probability on random 3-SAT problems without having to backtrack, while the decimation process based on BP magnetizations more often
fails to find a satisfying assignment in practice.
We also note that BP and SP have been proven to compute exact marginals on solutions and covers, respectively, only for tree-structured formulas (with some
simple exceptional cases like formulas with a single cycle). For BP, solution marginals on tree formulas are
already non-trivial, and it is reasonable to expect it to
compute a fair approximation of marginals on loopy
networks (formulas). However, for SP, cover marginals
on tree formulas are trivial: the only cover here is the
all-∗ cover. Cover marginals become interesting only
when one goes to loopy formulas, such as random 3SAT. In this case, as seen in our experiments, it is remarkable that the SP computes a good approximation
of non-trivial cover marginals for non-tree formulas.
We hope that our results have convincingly demonstrated that the study of the covers of formulas is very
fruitful and may well lead to a correct explanation of
the success of SP.

KROC ET AL.


