. We consider the problem of using a heuristic policy to improve the value approximation by the Upper Confidence Bound applied
in Trees (UCT) algorithm in non-adversarial settings such as planning
with large-state space Markov Decision Processes. Current improvements
to UCT focus on either changing the action selection formula at the internal nodes or the rollout policy at the leaf nodes of the search tree. In
this work, we propose to add an auxiliary arm to each of the internal
nodes, and always use the heuristic policy to roll out simulations at the
auxiliary arms. The method aims to get fast convergence to optimal values at states where the heuristic policy is optimal, while retaining similar
approximation as the original UCT in other states. We show that bootstrapping with the proposed method in the new algorithm, UCT-Aux,
performs better compared to the original UCT algorithm and its variants in two benchmark experiment settings. We also examine conditions
under which UCT-Aux works well.

1

Introduction

Monte Carlo Tree Search (MCTS) [1], or more specifically Upper Confidence
Bound applied in Trees (UCT) [2], is a state-of-the-art approach to solving large
state-space planning problems. Example applications of the UCT algorithm in
the games domain include Go [1,3,4], General Game Playing [5], Real-Time
Strategy Game [6], etc.
The algorithm estimates the value of a state by building a search tree using
simulated episodes, or rollouts, via interactions with the simulator. Instead of
sampling every branch equally, the goal is to focus samplings in tree branches
that are more promising. In particular, UCT achieves that by choosing the action, or arm if its parent node is regarded as a multi-armed bandit, with the
highest estimated upper bound to simulate at every internal node, and randomly
selects actions after leaving the tree to finish the rollout.
Because UCT uses random sampling to discover nodes with good return,
it could take a long time to achieve good performance. To address this problem, many enhancements have been used to improve the search control of the
algorithm by either (1) tweaking the action selection formula at the internal
nodes [7,3,5,4,8], or/and (2) designing better-informed rollout policies in place
of random sampling at the leaf nodes [3,4].

We consider the problem of using a heuristic function to improve the approximated value function computed by UCT. Taking the approaches above, the first
method is to initialize new tree nodes with heuristic values and the second is to
use the chosen heuristic to roll out simulations at the leaf nodes. As intended,
these two methods could greatly influence the search control by guiding it into
more promising regions that are determined by the heuristic. However, when
the heuristic function does not accurately reflect the prospect of the states, it
could feed the algorithm with false information, thereby leading the search into
regions that should be kept unexplored otherwise.
In this work, we propose a novel yet simple enhancement method. Given a
heuristic in the form of an imperfect policy œÄ, the method adds an additional
arm at every internal node of the search tree. This special arm is labeled by
the action suggested by œÄ and once selected, rolls out the rest of the sampling
episode using œÄ. If the policy œÄ works well at a state, we expect it to quickly
give a good estimate of the value of the state without relying too much on the
other arms. The method aims to get fast convergence to optimal values at states
where the heuristic policy is optimal, while retaining similar approximation as
the original UCT in other states.
We compared this method with two aforementioned techniques in two domains, namely Obstructed Sailing, an extension of the original Sailing problem
previously used to measure UCT‚Äôs performance in [2], and Sheep Savior, a large
state-space MDP that characterizes a generic two-player collaborative puzzle
game. The results showed that UCT-Aux the new algorithm (Aux for auxiliary
arms) significantly outperforms its competitors when coupled with reasonable
heuristics.
One nice property of this method is that it does not affect the implementation of other bootstrapping techniques: No modification of the action selection
formula nor the rollout policy at any leaf nodes except for the added arms is required. This allows different sources of bootstrapping knowledge to be combined
into one algorithm for more performance boost.
The rest of the paper is structured as follows. We first give a brief overview
of MDP, UCT and its popular enhancements before presenting UCT-Aux. Next,
we describe two experimental setups for comparing the agents‚Äô performance and
analyze the results. We also identify the common properties of the heuristics
used in two experimental domains and provide some insights on why UCTAux works well in those cases. Finally, we conclude the paper by discussing the
possible usage of UCT-Aux.

2
2.1

Background
Markov Decision Process

A Markov Decision Process characterizes a planning problem with tuple (S, A, T, R),
in which
‚Äì S is the set of all states,

‚Äì A is the set of all available actions,
‚Äì Ta (s, s0 ) = P (st+1 = s0 |st = s, at = a) is the probability that action a ‚àà A
in state s ‚àà S at time t will lead to state s0 ‚àà S at time t + 1.
‚Äì Ra (s, s0 ) is the immediate reward received after the state transition from s
to s0 triggered by action a.
An action policy œÄ is a function, possibly stochastic, that returns an action
œÄ(s) for every state s ‚àà S. In infinite-horizon discounted MDPs, the objective is
to choose an action policy œÄ ‚àó that maximizes some cumulative
of the
P‚àû function
t
‚àó
received rewards, typically the expected discounted sum
t=0 Œ≥ Ra (st , st+1 )
with 0 ‚â§ Œ≥ < 1 being the discount factor. An MDP can be effectively solved
using different methods, one of which is the value iteration algorithm based
on the Bellman‚Äôs equation [9]. The algorithm maintains a value function V (s),
where s is a state, and iteratively updates the value function using the equation
!
X
0
0
0
Vt+1 (s) = max
Ta (s, s )(Ra (s, s ) + Œ≥Vt (s )) .
a

s0

This value iteration algorithm is guaranteed to converge to the optimal value
function V ‚àó (s), which gives the optimal expected cumulative reward of running
the optimal policy from state s.
The optimal value function V ‚àó can be used to construct
the optimal policy
P
by taking action a‚àó in state s such that a‚àó = argmaxa { s0 Ta (s, s0 )V ‚àó (s0 )}. The
optimal Q-function is constructed from V ‚àó as follows:
X
Ta (s, s0 )(Ra (s, s0 ) + Œ≥V ‚àó (s0 )).
Q‚àó (s, a) =
s0

Q‚àó (s, a) denotes the maximum expected long-term reward of an action a when
executed in state s.
One key issue that hinders MDPs and Value Iteration from being widely used
in real-life planning tasks is the large state space size (usually exponential in the
number of state variables) that is often required to model realistic problems.
2.2

Upper Confidence Bound Applied to Trees (UCT)

UCT [2] is an anytime algorithm that approximates the state-action value in real
time using Monte Carlo simulations. It was inspired by Sparse Sampling [10],
the first near-optimal policy whose runtime does not depend on the size of the
state space. The approach is particularly suitable for solving planning problems
with very large or possibly infinite state spaces.
The algorithm searches forward from a given starting state, building up a
tree whose nodes alternate between reachable future states and state-action pairs
(Figure 1). State nodes are called internal if their child state-action pairs have
been expanded and leaf otherwise. Starting with a root state, the algorithm
iteratively rolls out simulations from this root node; each time an internal node
is encountered, it is regarded as a multi-armed bandit and UCB1 [11] is used to

a0

S0	
 ¬†

S0,a0	
 ¬†

a0

S1	
 ¬†

a1

a1
S0,a1	
 ¬†

S2	
 ¬†

S3	
 ¬†
a0

a1

S1,a0	
 ¬†

S1,a1	
 ¬†

S3,a0	
 ¬†

S3,a1	
 ¬†

S4	
 ¬†

S5	
 ¬†

S6	
 ¬†

S7	
 ¬†

Fig. 1: A sample UCT search tree with two valid actions a0 and a1 at any state.
Circles are state nodes and rectangles are state-action nodes; solid state nodes
are internal while dotted are leafs.

determine the action or arm to sample, i.e., the edge to traverse. In particular,
at an internal node s, the algorithm selects an action according to
s
)
(
log n(s)
,
(1)
œÄU CT (s) = argmax QU CT (s, a) + 2Cp
n(s, a)
a
in which
‚Äì QU CT (s, a) is the estimated value of state-action pair (s, a), taken to be the
weighted average of its children‚Äôs values.
‚Äì Cp > 0 is a suitable hand-picked constant.
‚Äì n(s) is the total number of rollouts starting from s.
‚Äì n(s, a) is the number of rollouts that execute a at s.
At the chosen child state-action node, the simulator is randomly sampled for
a next state with accompanying reward; new states automatically become leaf
nodes. From the leaf nodes, rollouts are continued using random sampling until a
termination condition is satisfied, such as reaching terminal states or simulation
length limit. Once finished, the returned reward propagates up the tree, with the
value at each parent node being the weighted average of its child nodes‚Äô values;
suppose the rollout executes action a at state s and accumulates reward R(s, a)
in the end.
‚Äì at state-action nodes, n(s, i) = n(s, i) + 1 and QU CT (s, a) = QU CT (s, a) +
1
n(s,a) (R(s, a) ‚àí QU CT (s, a))
‚Äì at state nodes, n(s) = n(s) + 1.
Typically one leaf node is converted to internal per rollout, upon which its child
state-action nodes are generated. When the algorithm is terminated, the root‚Äôs
arm with highest QU CT (s, a) is returned1 .
1

In practice, returning the arm with highest n(s, a) is also a common choice.

When used for infinite-horizon discounted MDPs, the search can be cut off at
an 0 -horizon. Given any  > 0, with 0 small enough, the algorithm is proven to
converge to the arm whose value is within the -vicinity of the optimal arm [2].
2.3

Enhancement methods

In vanilla UCT, new state-action nodes are initialized with uninformed default
values and random sampling is used to finish the rollout when leaving the tree.
Given a source of prior knowledge, Gelly and Silver [3] proposed two directions
to bootstrap UCT:
1. Initialize new action-state nodes with n(s, a) = nprior (s, a) and QU CT (s, a) =
Qprior (s, a), and
2. Replace random sampling by better-informed exploration guided by œÄprior .
We refer to these two algorithms as UCT-I (UCT with new nodes initialized
to heuristic values) and UCT-S (UCT with simulations guided by œÄprior ); UCTIS is the combination of both methods. UCT-I and UCT-S can be further tuned
using domain knowledge to mitigate the flaw of a bad heuristic and amplify the
influence of a good one by adjusting the dependence of the search control on the
heuristic at internal nodes. In this work, we do not investigate the effect of such
tuning to ensure a fair comparison between techniques when employed as is.
In the same publication [3], the authors proposed another bootstrapping technique, namely Rapid Action Value Estimation (RAVE), which we do not examine
in this work. The technique is specifically designed for domains in which an action from a state s has similar effect regardless of when it is executed, either at s
or after many moves. RAVE uses the All-Moves-As-First (AMAF) heuristic [12]
instead of QU CT (s, a) in Equation 1 to select actions. Many board games such as
Go or Breakthrough [5] have this desired property. In our experiment domains,
RAVE is not applicable, because the actions are mostly directional movements,
e.g., {N, E, S, W }, thus tied closely to the state they are performed at.

3

UCT-Aux: Algorithm

Given an added policy œÄ, we propose a new algorithm UCT-Aux that follows
the same search control as UCT except for two differences.
1. At every internal node s, besides |A(s)| normal arms with A(s) being the
set of valid actions at state s, an additional arm labeled by the action œÄ(s)
is created (Figure 2).
2. When this arm is selected by Equation 1, it stops expanding the branch but
rolls out a simulation using œÄ; value update is carried out from the auxiliary
arm up to the root as per normal.
The method aims to better manage mixed-quality heuristics. If the heuristic
œÄ‚Äôs value estimation at a state is good, we expect the added arm to dominate

S0	
 ¬†

a0
S0,a0	
 ¬†

a1

œÄ(s0)

S0,a1	
 ¬†

S0, œÄ(s0)	
 ¬†
	
 ¬†

a0
S1,a0	
 ¬†

S1	
 ¬†
a1
S1,a1	
 ¬†

œÄ(s1)

S2	
 ¬†

S1, œÄ(s1)	
 ¬†

S3	
 ¬†
a0

a1

œÄ(s3)

S3,a0	
 ¬†

S3,a1	
 ¬†

S3, œÄ(s3)	
 ¬†
	
 ¬†

	
 ¬†

S4	
 ¬†

S5	
 ¬†

S6	
 ¬†

S7	
 ¬†

Fig. 2: Sample search tree of UCT-Aux.

the distribution of rollouts and quickly give a good estimate of the state‚Äôs value
without the need to inspect other arms. Otherwise, the search control will focus
rollouts in ordinary arms, thus retaining similar approximation as vanilla UCT.
For stochastic heuristic policies, at every internal node, not one but Œ∫ auxiliary arms are added, with Œ∫ being the number of actions aœÄ such that P (œÄ(s) =
aœÄ ) > 0. As such, the number of arms at internal nodes is bounded by 2|A| since
Œ∫ ‚â§ |A|.
Convergence Analysis
We will show that regardless of the added policy‚Äôs quality, UCT-Aux converges in
finite-horizon MDPs2 . The proof follows closely that of UCT analysis by treating
the auxiliary arms as any other ordinary arms. As a recap, UCT convergence
analysis revolves around the analysis of non-stationary multi-armed bandits with
reward sequences satisfying some drift conditions, which is proven to be the
case for UCT‚Äôs internal nodes with appropriate choice of bias sequence Cp 3 . In
particular, the drift conditions imposed on the payoff sequences go as follows:
Pn
‚Äì The expected values of the averages X in = n1 t=1 Xit must converge for
all arms i with n being the number of pulls and Xit the payoff of pull t. Let
¬µin = E[X in ] and ¬µi = limn‚Üí‚àû ¬µin .
‚Äì Cp > 0 can be chosen such that the tail inequalities P (X i,n(i) ‚â• ¬µi +ct,n(i) ) ‚â§
q
ln t
t‚àí4 and P (X i,n(i) ‚â§ ¬µi ‚àí ct,n(i) ) ‚â§ t‚àí4 are satisfied for ct,n(i) = 2Cp n(i)
with n(i) being the number of times arm i is pulled up to time t.
Firstly, we will show that all internal nodes of UCT-Aux have arms yielding
rewards satisfying the drift conditions. Suppose the horizon of the MDP is D, the
number of actions per state is K and the heuristic policy is deterministic (Œ∫ = 1);
2

3

As mentioned in [2], for use with discounted infinite-horizon MDPs, the search tree
can be cut off at the effective 0 -horizon with 0 being the desired accuracy at root.
Empirically, Cp is often chosen to be an upper bound of the accumulated reward
starting from the current state.

this can be proven using induction on D. Note that i.i.d. payoff sequences satisfy
the drift conditions trivially due to Hoeffding‚Äôs inequality.
‚Äì D = 1: Suppose the root has already been expanded, i.e., become internal.
It has K + 1 arms, which either lead to leaf nodes (ordinary arms) or return
i.i.d. payoffs (auxiliary arm). Since leaf nodes have i.i.d. payoffs, all arms
satisfy drift conditions.
‚Äì D > 1: Assume that all internal state nodes under the root have arms
satisfying the drift conditions, e.g., s1 and s3 in Figure 2. Consider any
ordinary arm of the root node (the added arm‚Äôs payoff sequence is already
i.i.d.), for instance, (s0 , a1 ). Its payoff average is the weighted sum of payoff
sequences in all leafs and state-action nodes on the next two levels of the
subtree, i.e., leaf s2 , arms (s3 , a0 ), (s3 , a1 ) and (s3 , œÄ(s3 )), all of which satisfy
drift conditions due to either the inductive hypothesis or producing i.i.d.
payoffs. Theorem 4 in [2] posits that the weighted sum of payoff sequences
conforming to drift conditions also satisfies drift conditions; therefore, all
arms originating from the root node satisfy drift conditions.
As a result, the theorems on non-stationary bandits in [2] hold for UCTAux‚Äôs internal nodes as well. Therefore, we can obtain similar results to Theorem
6 of [2], with the difference being statistical measures related to the auxiliary
arms such as ¬µaux and ‚àÜaux , i.e., the new algorithm‚Äôs probability of selecting a
suboptimal arm converges to zero as the number of rollouts tends to infinity.

4

Experiments

We compare the performance of UCT-Aux against UCT, UCT-I, UCT-S and
UCT-IS in two domains: Obstructed Sailing and Sheep Savior. Obstructed Sailing extends the benchmark Sailing domain by placing random blockage in the
map; the task is to quickly move a boat from one point to a destination on a
map, disturbed by changing wind, while avoiding obstacles. Sheep Savior features a two-player maze game in which the players need to herd a sheep into its
pen while protecting it from being killed by two ghosts in the same environment.

5

Obstructed Sailing

The Sailing domain, originally used to evaluate the performance of UCT [2],
features a control problem in which the planner is tasked to move a boat from a
starting point to a destination under certain disturbing wind conditions. In our
version, there are several obstacles placed randomly in the map (see Figure 3a).
In this domain, the state is characterized by tuple hx, y, b, wprev , wcurr i with
(x, y) being the current boat position, b the current boat posture or direction,
wprev the previous wind direction and wcurr the current wind direction. Directions take values in {N, NE, E, SE, S, SW, W, NW}, i.e. clockwise starting from
North. The controller‚Äôs valid action set includes all but the directions against

(a) Obstructed Sailing sample map

(b) SailTowardsGoal

Fig. 3: Obstructed Sailing domain; (a) a randomized starting configuration,
(b) SailTowardsGoal heuristic produces near-optimal estimates/policies in good
cases but misleads the search control in others.

wcurr , out of the map or into an obstacle. After each time step, the wind has
roughly equal probability to remain unchanged, switch to its left or its right [13].
Depending on the relative angle between the action taken and wcurr , a cost
from 1 to 4 minutes is incurred. Additionally, changing from a port to a starboard
tack or vice versa causes a tack delay of 3 minutes. In total, an action can cost
anywhere from 1 to 7 minutes, i.e., Cmin = 1 and Cmax = 7 [13]. We model the
problem as an infinite-horizon discounted MDP with discount factor Œ≥ = 0.99.
5.1

Choice of heuristic policies

A simple heuristic for this domain is to select a valid action that is closest to the
direction towards goal position regardless of the cost, thereafter referred to as
SailTowardsGoal. For instance, in the top subfigure of Figure 3b, at the starting
state marked by ‚ÄúS‚Äù, if current wind is not SW, SailTowardsGoal will move the
boat in the NE direction; otherwise, it will execute either N or E.
This heuristic is used in UCT-I and UCT-IS by initializing new state-action
nodes with values
nST G (s, a) ‚Üê 1
0

1 ‚àí Œ≥ d(s ,g)+1
QST G (s, a) ‚Üê C(s, a) + Cmin
1‚àíŒ≥
with C(s, a) being the cost of executing action a at state s and d(s0 , g) the
minimum distance between next state s0 and goal position g. The initialized

value can be seen as the minimum cost incurred when all future wind directions
are favorable for desired movement. For UCT-S, the random rollouts are replaced
by œÄ(s) = argmaxa QST G (s, a).
Heuristic quality. This heuristic works particularly well for empty spaces,
producing near-optimal plans if there are no obstacles. However, it could be
counterproductive when encountering obstacles. In the bottom subfigure of Figure 3b, if a rollout from the starting position is guided by SailTowardsGoal, it
could be stuck oscillating among the starred tiles, thus giving inaccurate estimation of the optimal cost.
5.2

Setup and results

The trial map size is 30 by 30, with fixed starting and goal positions at respectively (2, 2) and (27, 27) (Figure 3a). We generated 100 randomized instances of
the map, where obstacles are shuffled by giving each grid tile p = 0.4 chance
to be blocked 4 . Each instance is tried five times, each of which with different
starting boat postures and wind directions.

Fig. 4: Performance comparison of UCT, UCT-S, UCT-I, UCT-IS and UCT-Aux
when coupled with the heuristic SailTowardsGoal; y-axis is the reward average
with error bars being the standard errors of the means.

All UCT variants (UCT, UCT-I, UCT-S, UCT-IS and UCT-Aux) use the
same Cp = Cmax /(1 ‚àí Œ≥) = 700 and the search horizon5 is set to be 300; an
4

5

We tried with different values of p ‚àà {0.05, 0.1, 0.2, 0.3, 0.5} and they all yield similar
results as Figure 4; the detailed charts are not presented due to space constraint.
The search horizon is chosen to be long enough so that the cost accumulated after
the horizon has small effect to the total cost.

optimal path should not be very far from 60 steps as most actions move the boat
closer to the goal. The exact optimal policy is obtained using Value Iteration.
Note that the performance of Optimal agent varies because of the randomization
of starting states (initial boat and wind direction) and map configurations.
Given the same number of samplings, UCT-Aux outperforms all competing
UCT variants, despite the mixed quality of the added policy SailTowardsGoal
when dealing with obstacles (Figure 4). Note that without parameter tuning,
both UCT-I and UCT-S are inferior to vanilla UCT, but between UCT-I and
UCT-S, UCT-I shows faster performance improvement when the number of samplings increases. The reason is because when SailTowardsGoal produces inaccurate heuristic values, UCT-I only suffers at early stage while UCT-S endures
misleading guidance until the search reaches states where the policy yields more
accurate heuristic values. The heuristic‚Äôs impact is stronger in UCT-S than UCTI: UCT-IS‚Äôs behavior is closer to UCT-S than UCT-I.

6

Sheep Savior

This domain is an extension of the Collaborative Ghostbuster game introduced
in [14] as the testbed for their assistance framework for collaborative games.
The game features two players (a shepherd and a dog) whose task is to herd
a sheep into its pen while avoiding it to be killed by two ghosts in a maze-like
environment. All non-player characters (NPCs) run away from the players within
a certain distance, otherwise the ghosts chase the sheep and the sheep runs away
from ghosts. Since ghosts can only be shot by the Shepherd, the dog‚Äôs role is
strictly to gather the NPCs (Figure 5).
Both protagonists have 5 movement actions (no move, N, S, E and W) while
Shepherd has an additional action to inflict damage on a nearby ghost, hence a
total of 30 compound actions. The two players are given rewards for successfully
killing ghosts (5 points) or herding sheep into its pen (10 points). If the sheep
is killed, the game is terminated with penalty -10. The discount factor in this
domain is set to be 0.99.
6.1

Choice of heuristic policies

The game can be seen as having three subtasks, each of which is the task of
catching a single ghost or herding a single sheep, as shown in Figure 5. Each of
these subtasks consists of only two players and one NPC, hence has manageable
complexity and can be solved exactly offline using Value Iteration.
A heuristic Q-value can be obtained by taking the average of all individual
subworlds‚Äô, or subtasks‚Äô, Q-values, as an estimate for one state-action pair‚Äôs
value. Specifically, at state s the policy, GoalAveraging, yields
nGA (s, a) ‚Üê 1
m

QGA (s, a) =

1 X
Qi (si , a)
m i=1

Fig. 5: Task decomposition in Sheep Savior.

in which si is the projection of s in subtask i, m is the number of subtasks,
i.e. three in this case, and Qi (si , a) are subtasks‚Äô Q-values. The corresponding
heuristic policy can be constructed as œÄGA (s) = argmaxa QGA (s, a).
Heuristic quality. GoalAveraging works well in cases when the sheep is
well-separated from ghosts. However, when these creatures are close to each
other, the policy‚Äôs action estimation is no longer valid and could yield deadly
results. The under-performance is due to the fact that the heuristic is oblivious
to the interactivity between subtasks, in this case, ghost-killing-sheep scenarios.
6.2

Setup and results

The map shown in Figure 5 is tried 200 times, each of which with a different randomized starting configurations. We compare the means of discounted rewards
produced by the following agents: Random, GoalAveraging, UCT, UCT-I, UCTS, and UCT-Aux. The optimal policy in this domain is not computed due to the
prohibitively large state space, i.e., 1045 ‚àó 32 ‚âà 1011 since each ghost has at
most two health points. All UCT variants have a fixed planning depth of 300.
In our setup, one second of planning yields roughly 200 rollouts on average, so
we do not run simulations with higher numbers of rollouts than 10000 due to
time constraint. Moreover, in this game-related domain, the region of interest is
in the vicinity of 200 to 500 rollouts for practical use.
As shown in Figure 6, UCT-Aux outperforms the other variants, especially
early on with small numbers of rollouts. UCT-S takes advantage of GA better
than UCT-I, which yields even worse performance than vanilla UCT. Observing
the improvement rate of UCT-S we expect it to approach UCT-Aux much sooner
than others, although asymptotically all of them will converge to the same optimal value when enough samplings are given and the search tree is sufficiently
expanded; the time taken could be prohibitively long though.

Fig. 6: Performance comparison of Random, GoalAveraging, UCT, UCT-S,
UCT-I, and UCT-Aux when coupled with Goal Averaging.

7

Discussions

Although UCT-Aux shows superior performances in the experiments above, we
observe that the chosen heuristic policies share a common property that is crucial
for UCT-Aux‚Äôs success: they show behaviors of ‚Äúmake it or break it‚Äù. In other
words, at most states s, their action value estimate QœÄ (s, a) is either near-optimal
or as low as that of a random policy Qrand (s, a).
Specifically, in Obstructed Sailing, if following SailTowardsGoal can bring
the boat from a starting state to goal position, e.g., when the line of sight
connecting source and destination points lies entirely in free space, the resultant
course of actions does not deviate much from the optimal action sequences.
However when the policy fails to reach the goal, it could be stuck fruitlessly. For
instance, Figure 3b depicts one such case; once the boat has reached either one
of three starred tiles underneath the goal position, unless at least three to five
wind directions in a row are E, SailTowardsGoal results in oscillating the boat
among these starred tiles. The resultant cost is therefore very far from optimal
and could be as low as the cost incurred by random movement.
In contrast, an example for heuristics that are milder in nature is the policy StochasticOptimal.0.2 which issues optimal actions with probability 0.2 and
random actions for the rest. This policy is also suboptimal but almost always
yields better estimation than random movement; it is not as ‚Äúextreme‚Äù as SailTowardsGoal. Figure 7, which charts the performance histograms of StochasticOptimal.0.2 alongside with SailTowardsGoal, shows that a majority of runs with
SailTowardsGoal yield costs that are either optimal or worse than Random‚Äôs.
Similarly, GoalAveraging in Sheep Savior is also extreme: By ignoring the
danger of Ghosts when around Sheep, it is able to quickly herd the Sheep in the
Pen or kill nearby Ghosts (good), or end the game prematurely by forcing the

Fig. 7: Performance histograms of heuristics in Obstructed Sailing. The returned
costs of a heuristic are allocated relatively into bins that equally divide the cost
difference between Random and Optimal agents; x-axis denotes the bin number
and y-axis the frequency.

Sheep into Ghosts‚Äô zones (bad). We hypothesize that one way to obtain extreme
heuristics is by taking near-optimal policies of the relaxed version of the original planning problem, in which aspects of the environment that cause negative
effects to the accumulated reward are removed. For instance, SailTowardsGoal
is in spirit the same as the optimal policy for maps with no obstacle, while
GoalAveraging should work well if the ghosts do not attack sheep.
As UCT-Aux is coupled with heuristic policies with this ‚Äúextreme‚Äù characteristic, rollouts are centralized at auxiliary arms of states where œÄ(s) is nearoptimal, and distributed to ordinary arms otherwise. Consequently, the value
estimation falls back to the default random sampling where œÄ produces inaccurate estimates instead of relying entirely on œÄ as does UCT-S.
7.1

When does UCT-Aux not work?

Figure 8 charts the worst-case behavior of UCT-Aux when the coupled heuristic‚Äôs
estimate is mostly better than random sampling but much worse than that of the
optimal policy, e.g. the heuristic StochasticOptimal.0.2 in Obstructed Sailing.
The reason behind UCT-Aux‚Äôs flop is the same as that of UCT, i.e., due
to the overly ‚Äúoptimism‚Äù of UCB1, as described in [8]. At each internal node,
samplings are directed into suboptimal arms that appear to perform the best so
far, exponentially more than the rest (Theorem 1 [2]) when convergence has not
started. Even though each arm is guaranteed to be sampled an infinite number
of times when the number of samplings goes to infinity (Theorem 3 [2]), the subpolynomial rate means only a tiny fraction of samplings are spent on attempting
bad-looking arms. As a result, in a specially designed binary tree search case,
UCT takes at least ‚Ñ¶(exp(exp(...exp(2)...))) samplings before the optimal node
is discovered; the term is a composition of D ‚àí 1 exponential functions with D
being the number of actions in the optimal sequence.

Fig. 8: Bad case of UCT-Aux when coupled with StochasticOptimal.0.2.
Samplings
UCT
UCT-S
UCT-I
UCT-IS
UCT-Aux

500
268.4
268.9
279.2
278.6
159.2

1000
555.4
558.6
583
586
256.7

2000
1134.6
1157.3
1200.7
1209.2
447.8

5000
2694.5
2733.2
2805
2834.8
889.5

10000
5395.7
5424.7
5620
5657.1
1341.7

20000
11041.7
11212.7
11519.4
11704.4
1981.3

50000
27107.4
27851.2
28123.8
28872.5
3117.9

100000
53235.9
54382.2
55393.7
56401.2
4317.1

200000
107107
109308
111418
113682
5970.11

Table 1: The average number of tree nodes for UCT variants in Obstructed
Sailing when coupled with StochasticOptimal.0.2.

UCT-Aux falls into this situation when coupled with suboptimal policies
whose estimates are better than random sampling: At every internal node, it
artificially creates an arm that is suboptimal but produces preferable reward
sequences when compared to other arms with random sampling. As a result,
the auxiliary arms are sampled exponentially more often while not necessarily
prescribing a good move. Table 1 shows some evidence of this behavior: Given the
same number of samplings, UCT-Aux constructs a search tree with significantly
less nodes than other variants (up to 20 times). That means many samplings
have ended up in non-expansive auxiliary arms because they were preferred.
7.2

Combination of UCT-Aux, UCT-I and UCT-S

UCT-Aux bootstraps UCT in an orthogonal manner to UCT-I and UCT-S, thus
allowing combination with these common techniques for further performance
boost when many heuristics are available. Figure 9 charts the performance of

such combinations in Obstructed Sailing. UCT-Aux variants use SailTowardsGoal at the auxiliary arms while UCT-I/S variants use StochasticOptimal.0.2
at the ordinary arms. UCT-Aux-S outperforms both UCT-Aux and UCT-S at
earlier stage, and matches the better performer among the two, i.e. UCT-Aux,
in a long run.

Fig. 9: Combination of UCT-Aux with UCT-I/S/IS in Obstructed Sailing.

8

Conclusion

In this work, we have introduced a novel yet simple technique to bootstrap UCT
with an imperfect heuristic policy in a popular non-adversarial domain, i.e.,
planning in large-state space MDPs. It is shown to be able to leverage on the
well-performing region while avoiding the bad regions of the policy, empirically
outperforming other state-of-the-art bootstrapping methods when coupled with
the right policy, i.e, the ‚Äúextreme‚Äù kind. Our conclusion is that if such property is
known before hand about a certain heuristic, UCT-Aux can be expected to give
a real boost over the original UCT, especially in cases with scarce computational
resource; otherwise, it would be safer to employ the currently prevalent methods
of bootstrapping. As such, a different mentality can be employed when designing
heuristics specifically for UCT-Aux: instead of safe heuristics that try to avoid
as many flaws as possible, the designer should go for greedier and ‚Äúriskier‚Äù ones.
Lastly, since UCT-Aux is orthogonal to other commonly known enhancements,
it is a flexible tool that can be combined with others, facilitating more options

when incorporating domain knowledge into the vanilla MCTS algorithm. In the
future, we plan to examine how to adapt the method to adversarial domains.

9

Acknowledgments

This research is partially supported by a GAMBIT grant ‚ÄùTools for Creating
Intelligent Game Agents‚Äù, no. R-252-000-398-490 from the Media Development
Authority and an Academic Research Grant no. T1 251RES1005 from the Ministry of Education in Singapore.



We apply decision theoretic techniques to construct nonplayer characters that are able to assist a human player in collaborative games. The method is based on solving Markov
decision processes, which can be difficult when the game
state is described by many variables. To scale to more complex games, the method allows decomposition of a game task
into subtasks, each of which can be modelled by a Markov
decision process. Intention recognition is used to infer the
subtask that the human is currently performing, allowing the
helper to assist the human in performing the correct task. Experiments show that the method can be effective, giving nearhuman level performance in helping a human in a collaborative game.

Introduction
Traditionally, the behaviour of Non-Player Characters
(NPCs) in games is hand-crafted by programmers using
techniques such as Hierarchical Finite State Machines (HFSMs) and Behavior Trees (Champandard 2007). These techniques sometimes suffer from poor behavior in scenarios
that have not been anticipated by the programmer during
game construction. In contrast, techniques such as Hierarchical Task Networks (HTNs) or Goal-Oriented Action
Planner (GOAP) (Orkin 2004) specify goals for the NPCs
and use planning techniques to search for appropriate actions, alleviating some of the difficulties of having to anticipate all possible scenarios.
In this paper, we study the problem of creating NPCs that
are able to help players play collaborative games. The main
difficulties in creating NPC helpers are to understand the intention of the human player and to work out how to assist
the player. Given the successes of planning approaches to
simplifying game creation, we examine the application of
planning techniques to the collaborative NPC creation problem. In particular, we extend a decision-theoretic framework
Copyright c 2011, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

for assistance used in (Fern and Tadepalli 2010) to make it
appropriate for game construction.
The framework in (Fern and Tadepalli 2010) assumes that
the computer agent needs to help the human complete an unknown task, where the task is modeled as a Markov decision
process (MDP) (Bellman 1957). The use of MDPs provide
several advantages such as the ability to model noisy human
actions and stochastic environments. Furthermore, it allows
the human player to be modelled as a noisy utility maximization agent where the player is more likely to select actions
that has high utility for successfully completing the task. Finally, the formulation allows the use of Bayesian inference
for intention recognition and expected utility maximization
in order to select the best assistive action.
Unfortunately, direct application of this approach to
games is limited by the size of the MDP model, which grows
exponentially with the number of characters in a game. To
deal with this problem, we extend the framework to allow
decomposition of a task into subtasks, where each subtask
has manageable complexity. Instead of inferring the task
that the human is trying to achieve, we use intention recognition to infer the current subtask and track the player‚Äôs intention as the intended subtask changes through time.
For games that can be decomposed into sufficiently small
subtasks, the resulting system can be run very efficiently in
real time. We perform experiments on a simple collaborative
game and demonstrate that the technique gives competitive
performance compared to an expert human playing as the
assistant.

Scalable Decision Theoretic Framework
We will use the following simple game as a running example, as well as for the experiments on the effectiveness of
the framework. In this game, called Collaborative Ghostbuster, the assistant (illustrated as a dog) has to help the
human kill several ghosts in a maze-like environment. A
ghost will run away from the human or assistant when they
are within its vision limit, otherwise it will move randomly.
Since ghosts can only be shot by the human player, the dog‚Äôs

role is strictly to round them up. The game is shown in Figure 1. Note that collaboration is often truly required in this
game - without surrounding a ghost with both players in order to cut off its escape paths, ghost capturing can be quite
difficult.

This algorithm is guaranteed to converge to the optimal
value function V ‚àó (s), which gives the expected cumulative
reward of running the optimal policy from state s.
The optimal value function V ‚àó can be used to construct
the optimal actionsPby taking action a‚àó in state s such that
a‚àó = argmaxa { s0 Ta (s, s0 )V ‚àó (s0 )}. The optimal Qfunction is constructed from V ‚àó as follows:
X
Q‚àó (s, a) =
Ta (s, s0 )(Ra (s, s0 ) + Œ≥V ‚àó (s0 )).
s0

The function Q‚àó (s, a) denotes the maximum expected longterm reward of an action a when executed in state s instead
of just telling how valuable a state is, as does V ‚àó .

Figure 1: A typical level of Collaborative Ghostbuster. The
protagonists, Shepherd and Dog in the bottom right corner,
need to kill all three ghosts to pass the level.

Markov Decision Processes
We first describe a Markov decision process and illustrate
it with a Collaborative Ghostbuster game that has a single
ghost.
A Markov decision process is described by a tuple
(S, A, T, R) in which
‚Ä¢ S is a finite set of game states. In single ghost Collaborative Ghostbuster, the state consists of the positions of the
human player, the assistant and the ghost.
‚Ä¢ A is a finite set of actions available to the players; each
action a ‚àà A could be a compound action of both players.
If each of the human player and the assistant has 4 moves
(north, south, east and west), A would consist of the 16
possible combination of both players‚Äô moves.
‚Ä¢ Ta (s, s0 ) = P (st+1 = s0 |st = s, at = a) is the probability that action a in state s at time t will lead to state s0
at time t + 1. The human and assistant move deterministically in Collaborative Ghostbuster but the ghost may
move to a random position if there are no agents near it.
‚Ä¢ Ra (s, s0 ) is the immediate reward received after the state
transition from s to s0 triggered by action a. In Collaborative Ghostbuster, a non-zero reward is given only if the
ghost is killed in that move.
The aim of solving an MDP is to obtain a policy
maximizes the expected cumulative reward
P‚àûœÄ that
t
t=0 Œ≥ RœÄ(st ) (st , st+1 ) where 0 < Œ≥ < 1 is the discount
factor.
Value Iteration. An MDP can be effectively solved using a simple algorithm proposed by Bellman in 1957 (Bellman 1957). The algorithm maintains a value function V (s),
where s is a state, and iteratively updates the value function
using the equation
!
X
0
0
0
Vt+1 (s) = max
Ta (s, s )(Ra (s, s ) + Œ≥Vt (s )) .
a

s0

Intractability. One key issue that hinders MDPs from being widely used in real-life planning tasks is the large state
space size (usually exponential in the number of state variables) that is often required to model realistic problems.
Typically in game domains, a state needs to capture all essential aspects of the current configuration and may contain
a large number of state variables. For instance, in a Collaborative Ghostbuster game with a maze of size m (number
of valid positions) consisting of a player, an assistant and n
ghosts, the set of states is of size O(mn+2 ), which grows
exponentially with the number of ghosts.

Subtasks
To handle the exponentially large state space, we decompose
a task into smaller subtasks and use intention recognition to
track the current subtask that the player is trying to complete.

Figure 2: Task decomposition in Collaborative Ghostbuster.
In Collaborative Ghostbuster, each subtask is the task of
catching a single ghost, as shown in Figure 2. The MDP for
a subtask consists of only two players and a ghost and hence
has manageable complexity.

Human Model of Action Selection In order to assist effectively, the AI agent must know how the human is going to
act. Without this knowledge, it is almost impossible for the
AI to provide any help. We assume that the human is mostly
rational and use the Q-function to model the likely human
actions.
Specifically, we assume
maxaAI Q‚àó
i (si ,ahuman ,aAI )

P (ahuman |wi , si ) = Œ±.e
(1)
where Œ± is the normalizing constant, wi represents subtask i
and si is the state in subtask i. Note that we assume that the
human player knows the best response from the AI sidekick
and plays his part in choosing the action that matches the
most valued action pair. However, the human action selection can be noisy, as modelled by Equation (1).

Intention Recognition and Tracking
We use a probabilistic state machine to model the subtasks
for intention recognition and tracking. At each time instance, the player is likely to continue on the subtask that
he or she is currently pursuing. However, there is a small
probability that the player may decide to switch subtasks.
This is illustrated in Figure 3, where we model a human
player who tends to stick to his chosen sub-goal, choosing
to solve the current subtask 80% of the times and switching
to other sub-tasks 20% of the times. The transition probability distributions of the nodes need not be homogeneous, as
the human player could be more interested in solving some
specific subtask right after another subtask. For example, if
the ghosts need to be captured in a particular order, this constraint can be encoded in the state machine. The model also
allows the human to switch back and forth from one subtask
to another during the course of the game, modelling change
of mind.

where T (wj ‚Üí wi ) is the switching probability from subtask j to subtask i.
Next, we compute the posterior belief distribution using
Bayesian update, after observing the human action a and
subtask state si,t at time t, as follows:
Bt (wi |at = a, st , Œ∏t‚àí1 ) = Œ±.Bt (wi |Œ∏t‚àí1 ).P (at = a|wi , si,t )
(3)
where Œ± is a normalizing constant. Absorbing current human action a and current state into Œ∏t‚àí1 gives us the game
history Œ∏t at time t.
Complexity This component is run in real time, and thus
its complexity dictates how responsive our AI is. We are
going to show that it is at most O(k 2 ), with k being the
number of subtasks.
The first update step as depicted in Equation 2 is executed
for all subtasks, thus of complexity O(k 2 ).
The second update step as of Equation 3 requires the computation of P (at = a|wi , si ) (Equation 1), which takes
O(|A|) with A being the set of compound actions. Since
Equation 3 is applied for all subtasks, that sums up to
O(k|A|) for this second step.
In total, the complexity of our real-time Intention Recognition component is O(k 2 + k|A|), which will be dominated
by the first term O(k 2 ) if the action set is fixed.

Decision-theoretic Action Selection
Given a belief distribution on the players targeted subtasks
as well as knowledge to act collaboratively optimally on
each of the subtasks, the agent chooses the action that maximizes its expected reward.
(
)
X
i
‚àó
Bt (wi |Œ∏t )Qi (st , a)
a = argmaxa
i

CAPIR: Collaborative Action Planner with
Intention Recognition
We implement the scalable decision theoretic framework
as a toolkit for implementing collaborative games, called
Collaborative Action Planner with Intention Recognition
(CAPIR).
Figure 3: A probabilistic state machine, modeling the transitions between subtasks.
Belief Representation and Update The belief at time t,
denoted Bt (wi |Œ∏t ), where Œ∏t is the game history, is the conditional probability of that the human is performing subtask
i. The belief update operator takes Bt‚àí1 (wi |Œ∏t‚àí1 ) as input
and carries out two updating steps.
First, we obtain the next subtask belief distribution, taking into account the probabilistic state machine model for
subtask transition T (wk ‚Üí wi )
X
Bt (wi |Œ∏t‚àí1 ) =
T (wj ‚Üí wi )Bt‚àí1 (wj |Œ∏t‚àí1 )
(2)
j

CAPIR‚Äôs Architecture
Each game level in CAPIR is represented by a GameWorld
object, which consists of two Players and multiple SubWorld
objects, each of which contains only the elements required
for a subtask (Figure 4). The game objective is typically to
interact with these NPCs in such a way that gives the players the most points in the shortest given time. The players
are given points in major events such as successfully killing
a monster-type NPC or saving a civilian-type NPC ‚Äì these
typically form the subtasks.
Each character in the game, be it the NPC or the protagonist, is defined in a class of its own, capable of executing
multiple actions and possessing none or many properties.
Besides movable NPCs, immobile items, such as doors or

Figure 4: GameWorld‚Äôs components.
shovels, are specified by the class SpecialLocation. GameWorld maintains and updates an internal game state that captures the properties of all objects.
At the planning stage, for each SubWorld, an MDP is generated and a collaboratively optimal action policy is accordingly computed (Figure 5). These policies are used by the AI
assistant at runtime to determine the most appropriate action
to carry out, from a decision-theoretic viewpoint.







   
  

   

   





  
   

 

   

!  "#

$ $     

"#   

(
'


%

&

Figure 5: CAPIR‚Äôs action planning process. (a) Offline subtask Planning, (b) in-game action selection using Intention
Recognition.

busters. We chose five levels (see Appendix) with roughly
increasing state space size and game play complexity to assess how the technique can scale with respect to these dimensions.
The participants were requested to play five levels of the
game as Shepherd twice, each time with a helping Dog controlled by either AI or a member of our team, the so-called
human expert in playing the game. The identity of the dog‚Äôs
controller was randomized and hidden from the participants.
After each level, the participants were asked to compare
the assistant‚Äôs performance between two trials in terms of
usefulness, without knowing who controlled the assistant at
which turn.
In this set of experiments, the player‚Äôs aim is to kill three
ghosts in a maze, with the help of the assistant dog. The
ghosts stochastically1 run away from any protagonists if they
are 4 steps away. At any point of time, the protagonists could
move to an adjacent free grid square or shoot; however, the
ghosts only take damage from the ghost-buster if he is 3
steps away. This condition forces the players to collaborate
in order to win the game. In fact, when we try the game with
non-collaborative dog models such as random movement,
the result purely relies on chance and could go on until the
time limit (300 steps) runs out, as the human player hopelessly chases ghosts around obstacles while the dog is doing
some nonsense at a corner. Oftentimes the game ends when
ghosts walk themselves into dead-end corners.
The twenty participants are all graduate students at our
school, seven of whom rarely play games, ten once to twice
a week, and three more often.
When we match the answers back to respective controllers, the comparison results take on one of three possible
values, being AI assistant performing ‚Äúbetter‚Äù, ‚Äúworse‚Äù or
‚Äúindistinguishable‚Äù to the human counterpart. The AI assistant is given a score of 1 for a ‚Äúbetter‚Äù, 0 for an ‚Äúindistinguishable‚Äù and -1 for a ‚Äúworse‚Äù evaluation.
Qualitative evaluation For simpler levels 1, 2 and 3, our
AI was rated to be better or equally good more than 50%
the times. For level 4, our AI rarely got the rating of being
indistinguishable, though still managed to get a fairly competitive performance. Subsequently, we realized that in this
particular level, the map layout is confusing for the dog to
infer the human‚Äôs intention; there is a trajectory along which
the human player‚Äôs movement could appear to aim at any
one of three ghosts. In that case, the dog‚Äôs initial subtask belief plays a crucial role in determining which ghost it thinks
the human is targeting. Since the dog‚Äôs belief is always initialized to a uniform distribution, that causes the confusion.
If the human player decides to move on a different path, the
AI dog is able to efficiently assist him, thus getting good ratings instead. In level 5, our AI gets good ratings only for
less than one third of the times, but if we count ‚Äúindistinguishable‚Äù ratings as satisfactory, the overall percentage of
positive ratings exceeds 50%.

Experiment and Analysis
In order to evaluate the performance of our AI system, we
conducted a human experiment using Collaborative Ghost-

1
The ghosts run away 90% of the times and perform some random actions in the remaining 10%.

12	
 ¬†
10	
 ¬†
8	
 ¬†
-¬≠‚Äê1	
 ¬†

6	
 ¬†

0	
 ¬†
1	
 ¬†

4	
 ¬†
2	
 ¬†
0	
 ¬†
1	
 ¬†

2	
 ¬†

3	
 ¬†

4	
 ¬†

5	
 ¬†

Figure 6: Qualitative comparison between CAPIR‚Äôs AI assistant and human expert. The y-axis denotes the number of
ratings.
120	
 ¬†
100	
 ¬†
80	
 ¬†

AI	
 ¬†

60	
 ¬†

Human	
 ¬†

40	
 ¬†
20	
 ¬†
0	
 ¬†
1	
 ¬†

2	
 ¬†

3	
 ¬†

4	
 ¬†

5	
 ¬†

Figure 7: Average time, with standard error of the mean as
error bars, taken to finish each level when the partner is AI
or human. The y-axis denotes the number of game turns.
Quantitative evaluation Besides qualitative evaluation,
we also recorded the time taken for participants to finish
each level (Figure 7). Intuitively, a well-cooperative pair
of players should be able to complete Collaborative Ghostbuster‚Äôs levels in shorter time.
Similar to our qualitative result, in levels 1, 2 and 3, the
AI controlled dog is able to perform at near-human levels in
terms of game completion time. Level 4, which takes the
AI dog and human player more time on average and with
higher fluctuation, is known to cause confusion to the AI
assistant‚Äôs initial inference of the human‚Äôs intention and it
takes a number of game turns before the AI realizes the true
target, whereas our human expert is quicker in closing down
on the intended ghost. Level 5, larger and with more escape
points for the ghosts but less ambiguous, takes the protagonist pair (AI, human) only 4.3% more on average completion
time.

Related Work
Since plan recognition was identified as a problem on its
own right in 1978 (Schmidt, Sridharan, and Goodson 1978),
there have been various efforts to solve its variant in different domains. In the context of modern game AI research,
Bayesian-based plan recognition has been inspected using

different techniques such as Input Output Hidden Markov
Models (Gold 2010), Plan Networks (Orkin and Roy 2007),
text pattern-matching (Mateas and Stern 2007), n-gram and
Bayesian networks (Mott, Lee, and Lester 2006) and dynamic Bayesian networks (Albrecht, Zukerman, and Nicholson 1998). As far as we know, our work is the first to use
a combination of precomputed MDP action policies and online Bayesian belief update to solve the same problem in a
collaborative game setting.
Related to our work in the collaborative setting is the work
reported by Fern and Tadepalli (Fern and Tadepalli 2010)
who proposed a decision-theoretic framework of assistance.
There are however several fundamental differences between
their targeted problem and ours. Firstly, they assume the task
can be finished by the main subject without any help from
the AI assistant. This is not the case in our game, which
presents many scenarios in which the effort from one lone
player would amount to nothing and a good collaboration
is necessary to close down on the enemies. Secondly, they
assume a stationary human intention model, i.e. the human
only has one goal in mind from the start to the end of one
episode, and it is the assistant‚Äôs task to identify this sole intention. In contrary, our engine allows for a more dynamic
human intention model and does not impose a restriction on
the freedom of the human player to change his mind mid
way through the game. This helps ensure our AI‚Äôs robustness when inferring the human partner‚Äôs intention.
In a separate effort that also uses MDP as the game AI
backbone, Tan and Cheng (Tan and Cheng 2010) model the
game experience as an abstracted MDP - POMDP couple.
The MDP models the game world‚Äôs dynamics; its solution
establishes the optimal action policy that is used as the AI
agent‚Äôs base behaviors. The POMDP models the human play
style; its solution provides the best abstract action policy
given the human play style. The actions resulting from the
two components are then merged; reinforcement learning is
applied to choose an integrated action that has performed
best thus far. This approach attempts to adapt to different
human play styles to improve the AI agent‚Äôs performance. In
contrast, our work introduces the multi-subtask model with
intention recognition to directly tackle the intractability issue of the game world‚Äôs dynamics.

Conclusions
We describe a scalable decision theoretic approach for constructing collaborative games, using MDPs as subtasks and
intention recognition to infer the subtask that the player is
targeting at any time. Experiments show that the method is
effective, giving near human-level performance.
In the future, we also plan to evaluate the system in more
familiar commercial settings, using state-of-the-art game
platforms such as UDK or Unity. These full-fledged systems offer development of more realistic games but at the
same time introduce game environments that are much more
complex to plan. While experimenting with Collaborative
Ghostbuster, we have observed that even though Value Iteration is a simple naive approach, in most cases, it suffices,
converging in reasonable time. The more serious issue is the

state space size, as tabular representation of the states, reward and transition matrices takes much longer to construct.
We plan to tackle this limitation in future by using function
approximators in place of tabular representation.

Appendix
Game levels used for our experiments.

Acknowledgments
This work was supported in part by MDA GAMBIT grant R252-000-398-490 and AcRF grant T1-251RES0920 in Singapore. The authors would like to thank Qiao Li (NUS),
Shari Haynes and Shawn Conrad (MIT) for their valuable
feedbacks in improving the CAPIR engine, and the reviewers for their constructive criticism on the paper.



