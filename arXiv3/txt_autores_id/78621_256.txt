
The paper describes aHUGIN, a tool for cre­
ating adaptive systems. aHUGIN is an exten­
sion of the HUG IN shell, and is based on t he
methods repo r ted by Spiegelhalter and Lau­
r itzen {1990a). The adaptive systems resu lt­
ing from aHUGIN are able to adj u s t the con­
ditional probabi lities in the modeL A short
analysis of the adap tation task is gi ven and
the fe atures of aHUGIN are des cribed. Fi­
nally a sess i on with experiments is reported
and the results are discussed.

1

Introduction

With the revival of Bayesian methods in decision sup­

port systems ( Shachter 1986; Pearl 1 988; Shafer and
Pearl 1990; Andreassen ei al. 1991b) mainly due to

the construction of e ffi cien t methods for belief re vi­

sion in causal probabilistic networks (Pearl 1988; L au­
ritzen and Spiegelhalter 1988; Andersen d a/. 1989;

et al. 1990; Shenoy and Shafer 1990), the
process of knowledge acq uisit ion under the Bayesi an
Jensen

paradigm has become increasingly important. Wh en
construc ti ng causal probabilistic network models, var­
ious sources may be used, ranging from ignorance over
experts' subjective assessments to well established sci­
entific theories and statistical models based on large
database s . Very often a mod el is a mixtu re of contri­
butions from sources of different epistemological char­
acter.

Sometimes these contributions do not coin ci de , and

the model is a mediation between them; sometimes the
(ignorance has, for ex­
ample, forced crude 'guesses' on certain distributions);
sometimes the model must vary with contexts which
cannot be specified beforehand; sometimes the domain
is drifting ov er time - requiring the mo del to drift

r esu lt in g model is incomplete

along with i t, and sometimes the model quite s i mply
does not reflect the real world pr op erly.
All the proble ms listed above call for pr ocedures which
enable the sy stem to mod ify the mo d el t h ro ugh ex p eri-

Finn V . .Tens en

ence. We call such an a ctiv ity adaptation, and sy stems

p erfo rm in g automatic adaptation
tems.
we

Note that

we

call adaptwe

sys­

have chosen t.o distinguish adaptation

which we usc to describe the activity
or creat in g models by hatch-pmccssing of large data.
from training,

( 10D2)

bases. In Spiegelhalter el rd

called

leaming and they

arc

both activities are

dist-inguished as sequent-i a l

learning and batch learning.

W h en using adaptat.ion

we <He us i ng the analogy to tlw n ot i on

of adaptive
reg u lato rs in control t h eo r y . llopefully, t h is abundance
of te rm i n o logy will not. confuse the reader completely.
The present paper descri hes all UG IN, a tool for cre­
ating adaptive systems. Tlw system, which is an ex­
tension of

IJUGIN ( Andersen

ct a/.

198\J), is based

on methods reported in Spiegellta.lt.er <Hid Lau ntzen
(l!.l!.lOa), see also Spiegelhaltcr and Lamit.zcn (1090b),

and th e adapt ive s y s te ms result in g from aliUGIN arc
able to adjust the con d i t i on a l probabilities in
mo d el .

sented

the

In a l l UGIN the mo d el is compactly repre­

by

a

contingency

table of i m a gi nary counts,

the adaptation p roced u re is
counts in this table.

a

and

proccs� of 111odifying

the

In se ction 2 we give a short analysis of the adaptation
discuss various simple adapt.ation methods
lead ing up to a dcscri ption of the one used in all U GIN.

task and

In section 3

we

d cscri h e the f"caturcs ofalllJCJN, a11d

in section '1 a session with t·xpcrillwnts
the

results

ar<'

disciiSSt'd.

i� r···portr·d

and

( 1 \JU2), r�nd Cowell ( I\J\J2) clt'­
difl'erent
ma.in difl'ercnre bctii'('CII t. h is sys­

Spiegclhalt.cr and Cc)\\"cll

scribe

a similar systt'lll a.nd rt'sult.s of s l ig h tly

experiments.

The

tem and t he i r system, is that. we allow an extra facilit.y,
\ailed fadwy, that. makf's the systcrn forget. t.h<' past at.
an ex p onenti a l rate, t.herel>y 111aking them 11101-e prone

to adapt in changing cnvir-onmcnt.s.

2

Analysis of adaptation

CPN m o dels have both a quantit.at.ive a n d a. qualita­
tive aspect. Througl1 t.hc eli reeled aTcs, the networ k
reflects the only ways in which variables mny have im-

224

Olesen, Lauritzen, and Jensen
pact on each other. The st.rengt.h of the impact is mod­
e ll ed through conditional probability tables. We shall
here describe how the probabil ity tables are modifi ed
in the adaptation process.
So, consider a causal probabilistic network into which
information on the state of the variables can be en­
t ered
If the state of only some of the variables is
known, then the probability distributions for the re­
maining variables are calculated. Each time this is
done, we have described a case. Now, a large number
of cases is at hand, an d we want to improve the model
.

by adjusting the conditional probability tables to the
set of cases.
2.1

Direct modelling of table uncertRinty

In Fig u re 1 ( a ) , the state of the variable A is influenced
direct ly by the states of B and C, and the strength

of this influence is modelled by P(AIB, C). If the
this may, for exam­
stren gth is subject to doubt
ple, be due to d ifferent estimates from experts, or it
may be due to a context influence not mod elled (like
soil quality of corn fields or ge ne t i c disposition for a
disease)
then this doubt may be modelled directly
by introducing an extra parent, T, for A ( F igu re 1
( b)) . This variable can be c onsi d e r ed as a ty pe vari­
able modelling, for example, types of context or differ­
ent experts' assessments. To reflect credibility of the
experts or frequencies of the co ntext types, a prior dis­
tribution forT could be given. When a case is entered
-

-

ma.y be neces,;ar·y. These <lrffi•renl. t.ypes rnay !w lw:n·­
ily interdependent (for example, assessments from Vilr­
ious intersecting sets of experts) and it. may he neces­
sar y to construct a COirlplcx t.ype nel.work with t.he risk

A simplifying assump­
ti o n would be global independence: the context depen­
dence for the conditional probabilities are mutually in­
dependent. In that case, each variable can he given its
own parent of ty p es and retrieval and dissemination
are completely lo c al (perfonned as above). However,
the pro ce d u re is still v u l nera b l e t.o combinatorial PX­
plosion. Take for instance the variahlc A in Figure I.
For each parent configurat.ion a type dcpcnclcncc on
the distribu tion for A shall be described. These de­
pe nden c ies may vary a lot with tile p a rticu la r parent
con figu rations and all kinds of intcr-dcpcndrncics rnay

of a combinatorial explosion.

,

be prcsellt.

Tlw1·d'or<'

WI'

('orc1�d

""'Y lw

in<'l'\';1"''

to

the p mbabil i ty tables by a. factor which is the prodtrct
of the number of states in l11c parents. So, a further

simplification wo uld he lorn/ independence: tlw ron­
text dependence for the various p<HCII1 coni\gtrratinns
ar e mutually independent.

Indirect moddliug of table 111H:1�rtaint.y

2.2

If nothing is known

a p ossibl e

011

t.!w structure of llw

inadequacy of the rnodello

the

GlliSI�S

case

for

set, the

uncertainty can not he represented directly through
a

network of discr e t e types,

and we rnust. leave

roorn

for all kinds of types witlt <dl kinds of distrdartions.

The learning process h e re is

as

everywhere else in t.he

B ayesi a n p ar a d igm : Specif"y a prior distribution

of the

ty pes and calculate the posteri or given the case ob­
served . It remai ns to find a natural way of spccifyi11g
,

and

such a p robabil i ty model. Sricgelh a ltc r

(Hl90a)

give

a

Lauritu·n

range of pos;,ihi\ities, incl11ding normal­

l og istic models.

The simplest probability n1odel which is convenient. f"or
cue I

(a)

case 2
(b)

Figure 1: Ad apt ation th ro ugh a type variable.
to the CPN, the calculation of updated probabilities
will yield a new distribution on T, an d we may say that
the change of these probabi litie s reflects what we have
learnt from the case. This process is called retrieval
of experience. The new dist ri bution may now be used
as prior probabilities for the next case and its impact
on the conditional probabilities found by summing out
the type variable. This process is called dzssemination
of experience. The technique has, for example, been
used in Andreassen et al. (199la), where the system
con tain s a model for metab ol i sm in patients suffering
from diabetes. Through a type variable, the syst em
adapts to the characteristics of the individual patient.
Several conditional probabilities in the CPN may be
context dependent, and a whole set of type variables

th is

situation assu rncs that each set of entries i 11 the

conditional probability tahlcs for

configuration follows

( .Johnson

and Kotz

distribution has �:
sit y

f(pt,····Pk-ll
f•or

p; > ()

I

atl<

The simplicity is
p ro p er )

a

so-called

a

part.icular parent

/Jmchlr!-rhslnh·ulnm

l!li2). A k:-dilll<"nsiOJd Dirichl,'t­
paJ<It\\cV•rs, (n1, . . . , rq.) <�ttd d<'ll­

(X

(

k-J

1-

LJil

)"k-1

•=I

k-1

II;;;··-!.
•=1

(I)

"\'"'k- I
L,=1 p, <I.
in

the

distribution with

itt\.crprcl.a.tion�
tri

=

0 for all

lf

i

the (im­

is

consid­

specified
may be interpreted a.� representing past exper ie nce as a
coni.ingency iable (cl'J, .. . . nk) of counl.� of pas! casrs.

ered

a

noninformativc prior, the distribution

s = L; rt; i:; thcr·eforc refet·red to as 1/ic
equivalent sample size. The updating p roced ure con­

The quanti ty

sists of modifying the cou nt.s as new cases
observed.

We shall

not give details,

hut. just

state

arc

that

bci 11g

the frac-

aHUGIN: A System Creating Adaptive Causal Probabilistic Networks

tion a;/ s = m; is t he mean for the ith outcome, and
for each i the variance of the probability for the ith
outcome is

m;(l- m;)

v;= --'---.:...

(2)

s +l

Hence v; is a measure of the uncertainty of the prob­
ability m;.
Using this interpretation we also have a tool to model
expert opinion s of the type "the probability is some­
where between p and q, but I believe it is about 1·" . In
the case of two states a and b, consider, for example,
the statement that the probability of a is between 0.3
an d 0.4 and that it is about .35. If we, as in Spiegel­
halter et al. ( 1990), interpret the statement so that
the mean is 0.35 and the standard deviation is 0.05,
then it can be modelled by a 2-dimensional Dirichlet­
distribution (which is called a Beta-distribution). We
then have to determine two counts na an d Ob whi ch
satisfy the equations
Oa
--- =

aa+ab

0.35 and

0.35

·

0.65

aa+ab+l

=

0.0025

(3)

which we solve to get Oa = 31.5 and Ob = 58.5. This
can be an attractive alternative to modelling second
order u n certainty by intervals of lower and upper prob­
abilities.
learning: Let (m1, . . . , mn) and a sample size
s be a given specification of the conditional probabil­
ity table P(Aib, c) . We can then act as if we had a
contingency table of counts (sm1, .. . , smn)· lf we ge t
a case in the configuration (b, c ) and a;, then retrieval
qu ite simply consist in adding 1 to the count for a;,
and dissemination is just to calculate the new frequen­
cies. If global and local independence can be assumed
the scheme is applicable to all tables.
Back to

The scheme only works if both the states of A an d its
parents are known. In general we may anticipate that
the provided evidence, E, may leave uncertainty on
both the states of A and of its parents.
A nai ve approach in the general case could be to add a
count of P( a;, b, cl E) to the counts for a;. This scheme
is known as fractional updating (Titterington 1976).
However, the scheme has several drawbacks. For ex­
ample, if P(A I b, c) = P(A I E) then the scheme may
give unjustified counts yielding a false accuracy. If, for
example, E = (b, c), then nothing can be learned on
the distribution of A, but nevertheless the sample size
will be increased by one. See further discussion of this
issue in Spiegelhalter and Lauritzen (1990a) as well as
in Spiegelhalter and Cowell (1992).
A mathematically correct updating of the distribu­
tions under our interpretation results in a mixture of
Dirichlet-distributions rather than in a si ngl e one (a
mixture is a linear combination with non-negative co­
efficients summing to 1). This complicates the calcu­
lations intractably - in particular when adapting from
the next case where mixtures of Dirichlet-distributions
are to be updated. Eventually the process will yet

agam result in a combinatorial explosion. Instead,
the correct distribution is approximated by a single
Dirichlet-distribution (keeping the approach of modi­
fying counts). First of all, we want the approximated
distribution to have the correct means, and the new set
of probabilities (mj, ... , m;) is set to be the means of
the correct distribution. Secondly, it would be prefer­
able also to give the distrihu t.ion t.he c orrect varian cPs.
However, this is not possible since only one free parant­
eter is left, namely the equivalent sam ple size. Instead,
the equivalent sample size is give n a value such t.h<-tt.
t he

'average v<�riancc'

v =

L

111.1: v;

i=l

(" )

ts correct. The r csu lt.i ng »chcrnc, wh ich is used i11
aHUGIN, is t.hc followittg: 1'- in;t. the ltwans art' chang('d
as if a full count W<\.� oht.aitwd:

mi

=

m;s

P(a;, b,c II,· ')+ w;{ I- P(b, c I!�')}

+

s+l

The last term

(rl)

be undcrst.oocl so that it clistr ibut .cs
that. fJ and C arc not in st.ates (b, c)
according to their present p ro babili t ies .
may

the probability

over the

a;

's

Next, the sam pie size is determined:
s

•

=

L�-l rn;-" (1- mi) - 1
""k
. • •
L..- i=l 7ll.; V;

((i)

where v; is the variance of l'i in t.he mixture (the for­
mulae may be found in Spiegelhalt.er and Lauritzen
(19!)0a)). The new r.ounts iii'C s"mi.
3

Features of aHUGIN

The program a ii U G IN , wltich is currently 1111der int­
plementation, is an ex t.cnsion of II U GIN (A nders<'n
et at. 1989). JIUGIN is a shell which allows the user
to edit CPNs over finit.e st.at.c variahlcs, and wh en t.ll<'
CPN is specified, TIUG!N creates <1. runtime sysl<'llt
for entering findings and ttpd<�t.inr; prohahilit.i<�s of tlw
variables in the network.
In aiJUGIN each variable lllilY be declared t.o be in
adaptation mode. If, for cx<�mplc, the variab le A with
states a1, . . . , an has parents B, . . . , C, th e n the con·
ditiona! probability table P( A I B, . .. , C) is modified
by declaring A of ndapt.at.ion t.y p e . The t.ahle is i n ­
terpreted as a contingency table such that for ead1
parent configuration b, .. . , c, the set P(Ajb, ... , c) is
interpreted as a set of frequencies based on a sampl e
of cases. Therefore the user will for each p arent config­
uration be prompted t.o specify F:QUIVALENT SAMPLE
SIZE. The l arger the ESS, t.hc more conservative t.hc
adaptation will be. The default val l !!! of ESS is Gk·,
where k is the number of states in 11.

Alternatively t h e ttscr will he M>ked t.o specify an in­
terval for cac\1 of the prohahililics in t ���� conditional

225

226

Olesen, Lauritzen, and Jensen

probability tables. These in terva ls will th en be trans­
lated to sample sizes using the equivalent of (3). The
ESS used for the given parent configur at io n will now
be chosen as the minimum of the t r anslate d sam p le
sizes for the individual entries .
3.1

Fading

Variables in ada p tati on mode have an extra feature,
fading, which makes them tend to ignore th in gs they
have learnt a long time ago, considering them as less
relevant. Each time a new case is taken into account,
the equivalent sample si ze is discounted by a fading
factor q, which is a re al number less than one but typ­
ically close to one. From the expression ( 1) for the
Dirichlet density, it is seen that the fading scheme es­
sentially corresponds to flattening the density by r ais­
ing it to the power q, known as power-steady d ynamic
modelling (Smith 1979; Smith 1981).

SIZE. In the case o f a change from accumulating to
fading the EQUIVALENT SAMPLE SlZE is kept but the
MAXIMAL SAMPLE SIZE provided by the user

ually claim its influence.

4

Experiments with aHUGIN

To

in v es tigate

the

strengths

and

will grad­

limitations

of

se r ies of experiments were carried out..
The investigation was designed as a complete fa<"lo­
rial simulation experilllent 011 t.IH' now classical "Chest

aiiUGIN, a

clinic" example (Figure 2) originat.ing frorn

and Spiegelhalt.er

( 1088).

Laurit.�•�n

Each experiment simulates

If s is the initial ESS, then the maximal ESS after
adaptation from a case is qs + 1. Running n cases will
result in a maximal ESS of
1- q"
q"s + ---

1- q

This gives that 1/(1 - q) is the maximal sam ple size
in the long run.
Therefore the user is given the choice between ACCU­
MULATING (fading factor 1) and FADING. If fa din g is
selected, the user is prompted for MAXIMAL SAMPLE
SIZE, MSS, and the fading factor is then computed as
(MSS- 1)/MSS. Defau lt value is lOOm, w h e re m is the
number of entries in the table.
Note: The result of fad in g is not only that the sample

size is reduced. Consider namely an entry with count
a- and with samp le size s, an d suppose that ret r ie va l of
a case results in an increase of the sample size by 1 and
o f the count by x. Without fading the ratio between
counts from present and past is x ( a-, but with fading
the ratio is xjqa-. This tells us that with fading the

present counts are given more weight. This can also be
seen by assuming that the entry wilt never receive more
counts. Without fadi n g the p roba bili ty will vanis h at
the speed of a-f(s + n ) while wi th fadi ng, the speed of
vanishing is in the order of a-f(s + q-").
3.2

Runtime mode

Figure 2:
10,000

cases,

The

"Ciwst clinic" •'X<llllpk.

and fo ur factors, dc110I.l'd ll, 0, P

are considered. Tine•� r<�ndom

ated from

Rl: Probabilities

close to the

R2: Probabilities v<�ry

san1pks (R)

o rigi n a l

are

and

L.

�enn­

ones.

difk.renL from the migina! o1ws.

R3: Probabilities "drifting over time", starting as the
original ones.

To control difTerences due t.o chance va riat ions , the
samples are reused . Thus, for example, all experimcTit.s
with probabilities as in Rl are based on identical dat.a.
Two different observational schemes ( 0) are investi­
gated, the first one is lll<lillly included for control pur­
poses.

01: Complete observations.

The ad a pta tion starts with the CPN in the initial con­
figuration. Findings are entered, and wh en all infor­
mation on the case has been entered , the adaptation
takes place changing the tables for the variables of

02: Data observed only on the variables "Visit. t.o
Asia?", "Smoker?", "Positive X - r ay ? " and " Dys­

adaptation type.

The P factor des c ribes difl'erent weights on the prior
distributions, expressed as v a ryi n g eq11ivalcnt. sR.rnpk
sizes. Two cases are consid•�rcd

At any time between two cases the user can choose
to change the adaptation type of any variable. When
the adaptation type of a variable has been chan ge d ,
the user is prompt ed for p ossibl e mi s s i n g information
on EQUIVALENT SAMPLE SIZE and MAXIMAL SAMPLE

pnoea?".

PI: Low precision,
P2: ll igh pn�cision,

ESS
ESS

=

=

10.
I Oll

aHUGIN: A System Creating Adaptive Causal Probabilistic Networks

Finally, three different learning schemes (L) are inves­

the random s amp le .

tigated

11: All variables except
ac c u mu latin g mode .

"Th ber c ulos i

s or cancer" in

Experiment ,22�

a

12: As 11 for the first 1000 cases, then t he mod e is

postet ior probability intervals �or p(bls)

"'\...:··----.--- ..

1000).

�

13: A s 12, b ut with short memory (MSS = 100).
"Tuberculosis or can ce r " is always in fixed mode as
it is a pure logical tran sit ion . As can be seen, the
whole in ves tigati on consists of 3 x 2 x 2 x 3 = 36
experiments . For each experiment a plot is generate d ,
sho w in g the current value of th e conditional probabil­
ities after each case has been processed, t ogether with
ap proximate 95% p oste ri o r probability intervals.

Results in accumulating mode

95%

!\

change d to fading, with long memory (MSS

4.1

·

· •'--__ -..---'--'---"- �. :: :_::. : :d:._

2000

•ooo

Experiment 1222

b

6000

.

sooo

I

,0000

95% posterior probability intervals tor p(bls)

These experiments are very similar to those performed

by Spie gelhalter and Cowell (1992). However, we al­
low uncertainty on all conditional probability tables.
In general our results show the same pattern. For com­

plete data the correct values are obtained quite fast,
and the influence of the initial specifications vanishes
after a f e w hundred cases.
Figures 3 {a) and 4 (a) show an interesting phe­

nomenon w h en learning from incomplete data ( 02).
In these experiments, it can only be observed from t he
given data that a maj ori ty of smokers suffer f rom dys­
pnoea (shortness of breath). It can not be inferred
f rom the data whether this correlation is due to the
presence or absence of bronchitis. In the fi rst exper­
iment, where all variables are in accumulating mode,
th e frequency of bronchitis is overestimated (Figure 3
(a)). To compensate for th is , the condition al pr oba­
bility for dy spn oe a given bronchitis and none of the
other diseases, is underestimated ( F igure 4(a)). Thus
t he correlation b etw een what can a ct u ally be observed
in the data is determined correctly, but the intermedi­
ate expl an ation is slightly incorrect.
From these experiments we conclude, not su rpri singly ,
that the method has difficulties learning about con­

ce pts on which dat a are indirect. In such situations the
system rel ies str ongly on p ri or k nowledge . This con­
clusion was also reached by Spiegelhalter and Cowell

(1992).

4.2

Results in fading mode

Figures 3 (b)-(c) and 4 (b)-(c) dis p l ay the results for
the same exp er i m ent s as in F ig ures 3 (a) and 4 (a), but
with the variab l es ch anged to fading with l ong memory
after the first 1000 cases. The same effect on esti mat­
ing intermediate variables can be observed. Note also,
that the two curves vary syn ch rono u sly. Most proba­
bly this is a r e s u l t of variations in freq uenc ies due to

" -------r--�--�
,.,.,
0000

Experimenl I 223

c

·

95% poslerior probabilily

intervals tor p(bjs)

� -

..

i

"

.

0

.
0

;: ·_�---r-----�----.-------,�· · ,__._j
8000

'0000

Figure 3: Exp erimen ts with in c ompl ete data. The con­
diti on al probability of bt·onchitis given the p a t.ient. is a
smo k e r is learnt in (a) <lccurnulating mode; (b) fading
m ode with long memory: (c) fading mode with short.
memory.
ln the

third experiment

(Figures 3

(c) and

4 (c))

the

maximal samplesizes are reduced to 100. ThisexJwri­
ment reveals the
mode. Figure 4

limit of the applicability of the frtdin)!;
(c) shows t.hat t.lw dat.<1 <He lwst. l'X­
ass u ming t.hat. <�.II pa.tients wi t. h bronchit.is

plained by
suffer from dyspnoea. To 1naint.ain t.he consistP.ncy
with the d ata , the frequency of s1nokcrs sufT'ering from
b ronchitis is

underestimated acc o r dingly. This pat.t.ern

227

22H

Olesen,

Lauritzen, and

Jensen

is general for fading with short memory for high and
low probabilities. We conclude that special attention
must be directed towards systematically m issi ng data
and the choice of MSS if such variables are fading.
Figure 5 shows a series of experiments with a declin­

Experiment 3221

a

��

ing probability of being a smoker. The first 1000 cases
are identical for the three plots, the variable being in

-

95% posterior probability intervals tor p(s)

accumulating mode. In Figure 5 (a) the variable re­

mains in this mode and it is seen how the probability
is becomin g increasingly conservative as the ESS m­
creases.

Experiment 1221

a

. 95% posterior probabili!y 1ntervals for p(djnot e.b)

2000

b

Experiment 3222- 956/o posterJor probabilrty interva!s for p(s}

c

Exp&rimen\ '3223- 9-5°/.g poslerior probability ln•ervals tor p(s)

eooo

Experiment 1222 · 95% posterio r probability intervals lor p(djnot e.b)

b

I

�-��!�'""""'�-��

(

r

6

gl

Q �-----,----�--,---�--�
2000

c

Experiment 1223

10000

·

95% posterior

probabili!y intervals for p(dlnot e,b)

10000

Figure 5: Learning about.
bei ng a smoker.

. ��

J .

a

declining probability of

·0 ;

I

;:I
� �-----,----.--,---,�
10000

Figure 4: The same experiment as in Figure 3 but for
"Dyspnoea" given the patient has bronchitis but none
of the other diseases.

In Fig u re 5 (b) the variable is changed to fading with
lo ng memory (MSS = lOOO) after t.he first 1000 ca�es.
This i n creases the dynami<: behaviour of the system
an d an almost correct adaptation is obtained. De­

creasing the MSS to 100 (Figure 5 (c)) increases the
dynamic behaviour further, re!>ult.ing in stronger fluc­
tuations around the correct value. The general expe­
r i e nc e is that the !V1SS shonld not he sd too low, nnd

that the experiments confirm th<.:
of aiiUGIN.

expected behaviour

aHUGIN: A System Creating Adaptive Causal Probabilistic Networks

To s u mmarize, aH U G I N seems to be able to adapt
to chang i n g environments, thereby extending H U G I N
with a valuable fun ctional i ty . Howeve r , special atten­
tion must be directed to the choice of M SS and to
var i ables with systematically missing d ata.

Andersen, S . K . , O lesen , K. G . , Jensen , F . V . , and
J ensen, F. ( 1 989). H U G I N - A she l l for building
Bayesian belief u ni verses for expert systems. In
Proceedings of t h e 1 1 t h int ern ational joint confe r­
ence o n artificial intellig e n ce , p p .

reprinted i n S hafer and Pearl

1080-5.
( 1 990).

Also

Andreassen , S., Benn , J . J . , Hovorka, R . , Olesen ,
K . G . , and Carso n , E . R. ( 19 9 1 a ) . A probabilis­
tic approach to glucose prediction and i nsulin dose
adj us t ment . Techn i c al report , Inst i t u te for Elec­
tronic Sys tems , A a l b org U n i versity .

A n d reassen, S . , J ensen, F. V . , and O lesen , K . G .
( 1991 b) . Medical expert systems b ased on causal
probabilistic networks. In ternational Journ al of
Bi omedical Computation, 2 8 ,

1 -30 .

Cowell, R. G . ( 1992). BAlES - a probab i l i s t i c ex­
pert system shell with qual i tative and quantita­
tive learning . In Bayesian st a tistics 4, ( ed . J . M.
Bern ardo , J . 0. Berger, A . P. D awi d , and A . F. M .
Smith) , p . i n press. Clarendon Press, Oxford , UK .
Jensen, F . V . , Lauri tzen , S . L . , and Olese n , K . G .
( 1990) . Bayesian up dat i n g i n causal probabil istic
networks by local c o mpu tations . Co m p u t a t ion a l
St atistics Q u a rt e rly, 4,

Johnson , N . L . and Kotz ,

269-82.
S . ( 1972).

Distri b u t ions i n

statistics. Co ntinu ous multivariate dis t ri b u ti o n s .

J oh n Wiley and Sons, New York .
Lauritzen, S . L . and S p iegel h a l t er , D. J . ( 1988). Lo­
cal computations with probabilities on graphical
st r u ctures and their appli cation to expert systems
( with discussi on ) . Journ al of the Royal Sta tistical
Society, Series B, 50,

( 1988 ) .

1 57-224 .

Probab ilis t i c inference

m

i n t ellig e n t

syste ms. Morgan Kaufmann , San Mateo.

Shachter, R. D. ( 1986). Eval u at i n g influence d i agrams.
Opera t i o ns Research, 34,

87 1 -82.
( 1990).

Sh afer , G . R. and Pearl , J . (ed . )

Rea dmg s

in u n certain reas on ing. Morgan Kaufm an n , San

M ateo , Califor n i a.
Shenoy, P. P. and Shafer , G . R. ( 1 990 ) . Axioms for
probab i l i ty and belief-fu nction propagation . In
Uncert a inty in artificial intelligence I V, ( ed . R. D .
Shachter, T . S . Le v i t t , L. N . K an a! , an d J . F . Lem­
mer) , pp . 1 69-98. North - Hol l and , Amsterdam .
Smith, J . Q. ( 1979 ) . A general ization of the Bayesian
steady forecasting mo d el . Journal of t h e Royal
Statistical So ci e t y, Series B, 4 1 ,

Smith, J .
model .

Q. ( 1 98 1 ) .

3 75-87.

The m u l ti parameter st ead y

Journal of t h e Ro yal St atistical Society,

Series B, 4 3 ,

Spiegel halter, D . and Lau r i tzen , S . L. ( 1 9906 ) . Tech­
n i ques fo r Bayesi a n a n alysis i n e x p e r t. syste m s .
A n n als of




We present a new approach to the solu­
tion of decision problems formulated as in­
fluence diagrams. The approach converts the
influence diagram into a simpler structure,
the Limited Memory Influence Diagram
(LIMID), where only the requisite informa­
tion for the computation of optimal policies is
depicted. Because the requisite information
is explicitly represented in the diagram, the
evaluation procedure can take advantage of
it. In this paper we show how to convert an
influence diagram to a LIMID and describe
the procedure for finding an optimal strategy.
Our approach can yield significant savings of
memory and computational time when com­
pared to traditional methods.

1

INTRODUCTION

Influence Diagrams (IDs) were introduced by Howard
and Matheson (1981) as a compact representation of
decision problems. Since then, various authors have
attempted to formalize their approach and develop al­
gorithms for evaluating IDs.
Olmsted (1983) and Shachter (1986) initiated research
in this direction. Their methods operate directly on
the ID and consist of eliminating nodes from the di­
agram through a series of value preserving transfor­
mations. During the transformations the policies for
the decisions are computed. Later Shachter and Ndi­
likilikesha (1993) and Ndilikilikesha (1994) proposed a
similar, but more efficient approach.
Other algorithms evaluate IDs by converting them into
different structures. Cooper (1988) described an ap­
proach where the evaluation of IDs is transformed into
inference problems for Bayesian networks. Several im­
provements of this method were later proposed by

Shachter and Peot (1992) and Zhang (1998). Shenoy
(1992) presented a method where the ID is converted
into a valuation network, and the optimal strategy is
computed through the removal of nodes from this dia­
gram by fusing the valuations bearing on the node to
be removed. Jensen et al. (1994) compiled the ID into
a secondary structure, the strong junction tree, and
solved the decision problem by the passage of messages
towards the root of the tree.
Our work relies on a property that has already been
stressed by Shachter (1998, 1999), and Nielsen and
Jensen (1999). Namely that in decision problems rep­
resented as IDs there may be information which is not
requisite for computing the policies. Going further, we
transform the ID into a similar, but simpler, structure
termed Limited Memory Influence Diagram (LIMID)
where the requisite information is explicitly depicted,
and present a simple algorithm for finding the opti­
mal strategy using this reduced structure. This can
result in significant gains in efficiency compared to tra­
ditional methods for solving IDs.
Section 2 gives a basic description of LIMIDs as de­
veloped in Lauritzen and Nilsson (1999). For proofs
not given in the present paper, the reader is referred
to this source.
2

LIMIDS

LIMIDs are represented by directed acyclic graphs
(DAGs) with three types of nodes. Chance nodes,
shown as circles, represent random variables. D ecision
nodes, shown as squares, represent choices or actions
available to the decision maker. Finally, value nodes,
shown as diamonds, represent local utility functions.
The arcs in a LIMID have a different meaning based on
their target. Arcs pointing to utility or chance nodes
represent probabilistic or functional dependence. Arcs
into decision nodes indicate which variables are known
to the decision maker at the time of decision. Thus
they in particular imply time precedence.

437

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

In contrast with traditional IDs, the LIMID can repre­
sent decision problems that violates the assumption of
no forgetting saying that variables known at the time
of one decision must also be known when all later de­
cisions are made.
The following fictitious decision problem borrowed
from Lauritzen and Nilsson (1999) illustrates a typical
decision situation which is well described by a LIMID.
A pig breeder is growing pigs for a pe­
riod of four months and subsequently selling
them. During this period the pig may or may
not develop a certain disease. If the pig has
the disease at the time when it must be sold,
the pig must be sold for slaughtering. On the
other hand, if it is disease free, its expected
market price as a breeding animal is higher.
Once a month, a veterinary doctor sees the
pig and makes a test for presence of the dis­
ease. The test result is not fully reliable and
will only reveal the true condition (hi) of the
pig with a certain probability. Based on the
test result (ti), the doctor decides whether
treating the pig for the disease (di).

Associated with every chance node r (connoting
random variable) is a non-negative function Pr on
Xr X Xpa(r) such that
(1)
where the sum is over Xr. The term Pr does not in
general correspond to a true conditional distribution
but rather a family of probability distributions for r
parametrized by the states of pa(r).
Each value node u E Y is associated with a real func­
tion Uu defined on Xpa(u).
2.2

POLICIES A N D STRATEGIES

A policy for decision node d can be regarded as a pre­
scription of alternatives in xd for each possible obser­
vation in Xpa(d). To allow for the possibility of ran­
domizing between alternatives, we formally define a
policy as follows. A policy Jd for d is a non-negative
function on xd X Xpa(d) which indicates a probabil­
ity distribution over alternative choices for each pos­
sible value of pa( d). They must also satisfy the rela­
tion (1) as above. A strategy is a collection of policies
{Jd : d E 6.}, one for each decision.
A strategy q = {Jd : d E 6.} determines a joint distri­
bution of all the variables in V as
Jq

11 Pr 11 Jd,
rEI' dEtl.

=

(2)

and Pr and Jd are indeed true conditional distributions
w.r.t. fq·
The expected utility of the strategy q is given by
The diagram above represents the LIMID correspond­
ing to the situation where the pig breeder does not
keep individual records for his pigs and has to make
his decision knowing only the given test result. The
memory has been limited to the extreme of only re­
membering the present. In the LIMID, the util­
ity nodes u1, u2, u3 represent the potential treatment
costs, whereas u4 is the (expected) market price of the
pig as determined by its health at the fourth month.
2.1

SPECIFICATION OF LIMIDS

Suppose we are given a LIMID C with decision nodes
6. and chance nodes r. We let V = 6. U r. The set of
value nodes is denoted Y.
For a node n we let pa(n) denote its parents. Each
node n E V is associated with a variable which we
likewise denote by n, that takes a value in a finite set
Xn. For W � V we write Xw = XnEWXn. Typical
elements in Xw are denoted by lower case values such
as xw, abbreviating xv to x.

EU(q)

= L Jq(x)U(x),
X

where U = �uEY Uu is the total utility. We are
searching for an optimal strategy ij satisfying
EU(ij) 2: EU(q) for all strategies q.
Such an optimal strategy is termed a global maximum
strategy in Lauritzen and Nilsson (1999).
2.3

SOLUBLE LIMIDS

The complexity of finding optimal strategies within
LIMIDs is in general prohibitive. This task, however,
becomes feasible for LIMIDs that have a certain struc­
ture. For that reason they are termed soluble. In this
section we formally define soluble LIMIDs and present
a simple and efficient algorithm for evaluating them.
For a strategy q

= {Jd d E 6.} and any do E 6. we let
Q-do = q \ {Jdo}
:

438

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

be the partially specified strategy obtained by retract­
ing the policy at do.
A local maximum policy for a strategy q at d0, is a
policy J�0 which satisfies

So, J�0 is a local maximum policy for q at do if and only
if the expected utility does not increase by changing
the policy J�0 given the other policies are as in q. The
following lemma gives a method to find a local maxi­
mum policy. Here, f q d is defined through (2) and the
partial strategy obtained from q by retracting Jd. Let­
ting the fa mily of n be defined by fa(n ) pa(n) U { n}
we now have

_

=

Lemma 1 A policy Jd is a local maximum policy for
a strategy q at d if and only if for all Xfa(d) with
Jd(xd I Xpa(d)) > 0 we have

Xd

=

Zd

fq_d(xv\d,zd)U(xv\d,zd) ·

L

arg max

XV\fa(d)

As we shall see in Theorem 1, an important instance of
Lemma 1 is when the strategy q is the unifor m strategy.
Here the uniform strategy ij is defined as the strategy
ij
{Jd: d E �}, where
=

Jd(Xd I Xpa(d))
Letting
f

=

=

1/IXdl·

IT Pr,
rEr

we now have the following special case of Lemma

(3)

1.

policy Jd is a local maximum policy
for the unifor m strategy at d if an d only if for all Xfa(d)
with Jd(xd I Xpa(d)) > 0 we have
=

arg max

Zd

L

For a node n we let de(n) denote the descendants of
n. We say that a decision node do is extremal in the
LIMID £ if
u_l_c

(

U

{ fa(d ) : d

E �\ { do } }

) l fa(do )

for every utility node u E de(d0).
Theorem 1 establishes the connection between opti­
mum policies and extremal decision nodes.
Theorem 1

If decision node d is extremal in the
L IM ID £, then
•

•

d has an optimum policy;
any local maximum policy for the uniform strategy
at d is an optimum policy for d.

Suppose decision node d is extremal in the LIMID £.
Then Theorem 1 ensures that d has an optimum pol­
icy Jd. We can now implement Jd by converting d into
a chance node with Jd as the associated conditional
probability distribution to obtain a new LIMID £*. It
is easily seen that every optimal strategy q* for £* then
generates an optimal strategy for £ as q q• U { Jd}
Thus, if £* again has an extremal decision node, we
can yet again find an optimum policy and convert £*
as above. If the process can continue until all deci­
sion nodes have become chance nodes, we have clearly
obtained an optimal strategy for £.
=

Corollary 1 A

xd

denote that A and Bare d-separated bySin the DAG
formed by all the nodes in the LIMID £, i.e. including
the utility nodes.

f(xv\d,zd)U(xv\d,zd) ·

Xv\fa(d)

P roof: For the uniform strategy ij we have from (2)
and (3) that f il d ex f. Now the corollary follows from
•
Lemma 1.

-

·

We thus define an exact solution ordering d1,... , dk of
the decision nodes in £ as an ordering with the prop­
erty that for all i, di is extremal in the LIMID where
di+l,. .. , dk have been converted into chance nodes.
A LIMID £ is said to be soluble if it admits an exact
solution ordering.
Accordingly, computing an optimal strategy for a sol­
uble LIMID £ can be done using the following routine:
A lgorithm SINGLE POLICY UPDATING

A soluble LIMID £ with exact solution order­
ing d1 , ... , dk.

Input:

An optimum policy for do in the LIMID £ is a policy
which is a local maximum policy at d0 for all strategies
q in £. Evidently some decision nodes may not have an
optimum policy. However, in the following we present
a method for (graphically) identifying decision nodes
that have an optimum policy. For this purpose we let
the symbolic expression

For

i

=

k, . . ., 1 do:

1. Compute an optimum policy Jd; for di;
2. Convert di into a chance node with Jd; as its
associated conditional probability function.
Return:

The policies

{ Jdk,... , Jd1}.

439

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Note that the policy Jd; computed in step 1 is only
optimum for di in the LIMID where decision nodes
di+1, ..., dk are converted into chance nodes. The al­
gorithm is well-defined since, as described above, the
solubility of £ guarantees that it is always possible to
compute an optimum policy for di in step 1. Thus the
collection { Jdk, ... , Jd1} constitutes an optimal strat­
egy for £.
3

EVALUATING INFLUENCE
DIAGRAMS USING LIMIDS

Suppose we are given a decision problem represented
by an ID and wish to evaluate it using the algorithm
SINGLE POLICY UPDATING. Then one first needs to
transform the ID into an 'equivalent' LIMID. This is
an easy task: The ID requires a linear temporal order
on the decision nodes and, in addition it assumes 'no
forgetting', i.e. all variables known at the time of one
decision are assumed to be known when subsequent
decisions are made. Thus, for an ID with decision
nodes d1, ..., dk (where their index indicate the order
of the decisions), the no forgetting assumption can be
made explicit by drawing arcs from fa( dj) into di for
all i and for all j < i. We call the diagram produced
in this way the L IM ID version of the ID. In Fig. 1-2,
an ID and its LIMID version are shown. Now we have
Theorem 2

The L IM ID version of an ID is soluble.

P roof: Suppose we are given an ID with decision
nodes d1, ..., dk. For the LIMID version £ of the ID
we have

for all i, so di is clearly extremal after making
di+1, ..., dk into chance nodes. Thus £ is soluble with
•
exact solution ordering d1, ..., dk.

3.1

REDUCING SOLUBLE LIMIDS

Starting from a soluble LIMID £ we now present a
method for identifying parents of decision nodes that
are non-requisite for the computation of optimum poli­
cies. Similar methods for IDs have been produced by
Nielsen and Jensen (1999) and Shachter (1999) and
when a LIMID is representing an ID their mehod iden­
tifies the same requisite parents as ours, but the sub­
sequent use of SINGLE POLICY UPDATING exploits this
reduction to obtain lower complexity of the computa­
tions.
As for IDs the key to simplification of computational
problems for LIMIDs is the notion of irrelevance as
expressed through the notion of d-separation (Pearl

1986). We say that a node n E pa( d ) in £ is non­
requisite for d if
ul.cn I (fa(d) \ {n}),

for every utility node u E de( d). If the above condition
is not satisfied, then n is said to be requisite for d.
A reduction of £ is a LIMID obtained by successive
removals of arcs from non-requisite parents of deci­
sion nodes. It can be shown that any LIMID £ has
a unique minimal reduction, denoted Lmin, obtained
by reducing £ as much as possible. Thus in Lmin all
parents of decision nodes are requisite (cf. Theorem 4
in Lauritzen and Nilsson (1999)).
Reducing a soluble LIMID to its minimal reduction
can be done by applying the following routine. Note
that the algorithm runs in time O(k(graph size)).
A lgorithm Reducing Soluble LIMIDs

A soluble LIMID with exact solution ordering
d l, . . . ' dk.

Input:

For

i k,. . . , 1 do: Remove arcs from non-requisite
parents of decision node di.
=

Note that in the above algorithm the decision nodes
are visited in the reverse order starting from dk. This
ordering is important: If we chose some other order­
ing there is no guarantee that the reduced LIMID is
minimal. For a discussion of this issue the reader is
referred to Lauritzen and Nilsson (1999).
Fortunately, the maximum expected utility is pre­
served under reduction, i.e. if c' is a reduction of
£, then the optimal strategy in c' and the optimal
strategy in £ have the same expected utility. In addi­
tion, solubility is preserved under reduction, i.e. any
reduction of a soluble LIMID £ is itself soluble. The
reader interested in the details and proofs is referred
the above source; here we shall use the following the­
orem.
Theorem 3

If the L IM ID £ is soluble, then

1.

its minimal reduction Lmin is soluble;

2.

any optimal st rategy for Lmin is an optimal strat­
egy fo r £.

Example 1 Regard the ID in Fig. 1, and its LIMID
version depicted in Fig 2. The latter diagram is the
starting point for reducing the decision problem using
Procedure 1:

First one notes that u2 and u4 are the only utility
nodes that are descendants of d4. Furthermore

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

440

Figure 1: An influence diagram.

Figure 4: The moralized graph of the LIMID in Fig. 3

Figure 2: The LIMID version of the ID in Fig. 1.

Figure 5: The triangulated graph of the moral graph in
Fig. 4. The elimination order used in the triangulation
process was d1, r3, d3, r4, d2, r1, r2, d4.

so d1 and d3 are non-requisite parents of d4. So the
arcs from d1 and d3 into d4 are removed and we let £1
denote the reduced LIMID. Now one notes that in £1,
u1 is the only utility node that is a descendant of d3
and since

while not affecting the correctness of the algorithm,
the arcs from non-requisite parents introduce unnec­
essary computations.

d1 is non-requisite for d3 and the arc from d1 into d3
is removed. In the reduced LIMID it can be seen that
d1 (which is the only parent of d2) is requisite for d2.
Finally, d1 has no parents so no further reduction is
possible and therefore the reduced LIMID, shown in
Fig. 3, is minimal.
3.2

CONSTRUCTION OF JUNCTION
TREES

As we shall see, computing optimum policies for the
decisions during SINGLE POLICY UPDATING can be
done by message passing in a so-called junction tree.
In the present section we describe how to compile a
soluble LIMID into the junction tree. Clearly it is al­
ways advantageous to start with a minimal LIMID:

Figure 3: The minimal reduction of the LIMID in Fig.
2

The transformation from a LIMID C to a junction tree
starts by adding undirected edges between all nodes
with a common child (including children that are de­
cision nodes). Then we drop the directions on all arcs
and remove all value nodes to obtain the moral graph.
Next, edges are added to the moral graph to form a
triangulated graph and the cliques are subsequently
organized into a junction tree. This can be done in a
number of ways; we refer to Cowell et al. (1999) for de­
tails. It is important to note that, in contrast with the
local computation method described by Jensen et al.
(1994) the triangulation does not need to respect any
specific partial ordering of the nodes, but the trian­
gulation can simply be chosen to minimize the com­
putational costs, for example as described in Kjrerulff
(1992)
0

Example 2 Fig. 4 shows the moral graph of the min­
imal LIMID in Fig. 3, and Fig. 5 displays the triangu­
lation of the moral graph. The elimination order used
in the triangulation process is chosen to minimize the

Figure 6: The junction tree of the triangulated graph
in Fig. 5.

441

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Here we have used the convention that 0/0
Two potentials 1rw
(PW, u}v) and 1r�
are considered equal, and we write 7rW
xw we have
=

=

Figure 7: The strong junction tree of the original ID
represented in Fig. 1. The rightmost clique is the
strong root.

•
•

size of the cliques; in particular the ordering does not
respect the partial ordering of the nodes in the minimal
LIMID. The junction tree for the triangulated graph
is given in Fig. 6.
For comparison we have shown the strong junction tree
in Fig. 7. Even though the latter has fewer cliques
than our junction tree, the largest clique in the strong
junction tree contains six variables whereas the largest
clique in our junction tree only contains four variables.
This is important since the largest clique in the junc­
tion tree mainly determines the complexity of message
passing in the junction tree.
3.3

LIMID P OTENTIALS

In our junction tree we represent the quantitative el­
ements of a LIMID through entities called L IM ID­
potentials, or just potentials for short.
Let W � V.

(pw,uw) where

A potential on W is a pair

71W

=

pw is a non-negative real function defined on Xw;

•

uw is a real function defined on Xw.

(p�,u�)
1r� if for all
=

p}v(xw) p�(xw) and
uw(xw) u�(xw) whenever p}v(xw) > 0.
=

=

This identification of two potentials is needed to prove
that marginalization and combination satisfy the ax­
ioms of Shenoy and Shafer (1990) (cf. Lemma 2-4
in Lauritzen and Nilsson (1999)). This in turn estab­
lishes the correctness of the message passing scheme
presented in Section 3.5.
3.4

INITIA LIZATION

To initialize the junction tree T, one assigns a vacuous
potential to each clique C E C. Then for each chance
node r in the LIMID C one multiplies the conditional
probability function Pr onto the probability part of
any clique containing fa(r). When this has been done,
one takes each value node u, and adds the local utility
function Uu to the utility part of the potential of any
clique containing pa(u). The moralization process has
ensured the existence of such cliques.
Let 1rc (pc, uc) be the potential on clique C after
initialization. The joint potential 1rv on T is equal to
the combination of all potentials and satisfies:
1fv

So a potential consists of two parts where the first
part pw is called the p robability part and the second
part uw is called the utility part. A potential is called
vacuous if its probability part is equal to unity and
its utility part is equal to zero. To evaluate the de­
cision problem in terms of potentials we define two
basic operations of combination and marginalization.
This notion of operations is similar to what is used in
Shenoy (1992), Jensen et al. (1994), and Cowell et al.
(1999).
The combination of two potentials 1rw1
and 1rw2
(pw2, uw2), denoted 7rW1 @
potential on wl u w2 given by

,

(Pw1, uw1)
1rw2 is the

=

=

1fW1@ 1fW2

=

(pw1PW2, uw1 + uw2).

The marginalization of the potential 1rw (pw, uw)
onto wl � w' denoted 7fw1 is the potential on wl
given by
=

-(

_

0.

=

•

-l-W1
1fw

=

"'"' w, LW\W1 Pwuw
L....J P
"'
LJW\WtPW
W\Wt

)

.

=

@cEC7fC

=

(rr

rEr

Pr,

L

uEY

uu)

=

( J , U)'

(4)

where f is defined in (3) and U is the total utility.
3.5

PA SSAGE OF MESSAGES

Let { 1rc : C E C} be a collection of potentials on the
junction tree T, and let 1rv
®{ 1rc : C E C} be
the joint potential on T. Suppose we wish to find the
marginal 1rtR for some clique R E C. To achieve our
purpose we present a propagation scheme where mes­
sages are passed via a mailbox placed on each edge of
the junction tree. If the edge connects cl and c2, the
mailbox can hold messages in the form of potentials on
C1 n C2. So when a message is passed from C1 to C2
or vice versa, the message is inserted into the mailbox.
=

Imagine for the moment that we direct all the edges in
T towards the 'root-clique' R. Then each clique passes
a message to its child after having received messages
from all its other neighbours. The structure of a mes­
sage 7fC1-tC2 from clique C1 to its neighbour C2 is
given by

7fCt-+C2

=

C2
(1rc 1@ (®cEne(CI)\{C2}1fC-tC1)),J. ,

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

442

where ne(C1) are the neighbours of C1 in T

2

In words, the message which C1 sends to its neighbour
C2 is the combination of all the messages that C1 re­
ceives from its other neighbours together with its own
potential, suitably marginalized.

3.

·

messages towards a ' root-clique' R as described above.
When R has received a message from each of its neigh­
bours ne(R), the combination of all messages with its
own potential is equal to the ma rginalization of 71'V
onto R:

3.6

=

Contract:

- (7r* )tfa(d)
Compute 7r*fa(d)R

Compute the contraction

4.

·

Cfa(d)

of

Optimize: Define Jd(Xpa(d)) for all Xpa(d) as the
distribution degenerate at a point x;'t satisfying
(cf. Corollary 1)

Suppose we start with a joint potential

71'V on a junction tree T with cliques C, and pass

71'tR

'

7f'fa(d)'

The following result follows from the fact that the two
mappings, combination ( Q9) and marginalization (.!.)
obey the Shafer-Shenoy axioms.
Theorem 4

Marginalize·

(0CEC7rC)tR

=

71'R Q9 (®cEne(R)'lf'C-tR) ·

COMPUTING OPTIMUM POLICIES

Note that all the computations apart from the second
step are local in the root clique R.
Recall that, in SINGLE POLICY UPDATING, when an
optimum policy Jd for d has been computed, d is con­
verted into a chance node with Jd as its associated
probability function. To make an equivalent conver­
sion in our junction tree, we simply multiply Jd onto
the probability part of any clique containing fa(d).
3.7

BY MESSAGE PA SSING

COMPUTING THE OPTIMAL
STRATEGY B Y PARTIA L COLLECT

This section is concerned with showing how to find op­
timum policies for extremal decision nodes by message
passing in the junction tree T.
Let 71' W (Pw, uw) be a potential. The contraction of
'lf'W, denoted cont(71'W), is defined as the real function
on Xw given by
=

cont(7rw)

=

pwuw.

Accordingly, for the joint potential 71'V defined by (4)
we have
(5)
cont(7rv) f(x)U(x).
=

It is easily shown that for a potential 'lf'W on W and
W1 � W we have
cont(7rt:'1)

=

L

(6)

cont(7rw).

W\Wt

To compute an optimum policy for an extremal deci­
sion node d, one first note that by (5) and (6)

L

f(x)U(x)

=

cont(7ri;!a(d)).

XV\fa(d)

Consequently, an optimum policy for d can be found as
follows. First one identifies a clique, say R, that con­
tains fa(d). The compilation of a LIMID £ to a junc­
tion tree T guarantees the existence of such a clique.
Then the following steps are carried out (cf. Corol­
lary 1 and Theorem 1) :
1.

Collect to R to obtain 71'R
Theorem 4.
Collect:

=

71'tR

PROPAGATIONS

Suppose we have transformed a soluble LIMID £ with
exact solution ordering d1, . . . , dk into a junction tree
T. The propagation scheme presented here can be
used to compute the optimum policies during SINGLE
POLICY UPDATING.

As an initial step messages are collected towards any
root clique Rk which contains fa(dk)· Then we com­
pute an optimum policy for dk, as described in the
previous subsection, and the obtained policy is multi­
plied onto the probability part of Rk.
In a a similar manner the policy for dk_1 can be com­
puted: First, we identify a new root clique Rk-1 which
contains fa(dk-d· Then we could collect messages to
Rk-1 as above; however, this usually involves a great
deal of duplication. Instead we only need to pass mes­
sages along the (unique) path from the old root clique
Rk to Rk-1• This is done by first emptying the mail­
boxes on the path and then passing the messages. Note
that after this 'partial' collection of messages, Rk-1
has received messages from all its neighbours. Now,
an optimum policy for dk-I can be computed and the
potential on Rk-I is changed appropriately.
Proceeding in this way by successively collecting mes­
sages to cliques containing the families of the decisions
we eventually compute all the optimum policies and
thus the optimal strategy.
Example 3

as in

Fig. 8 shows how the propagation scheme
works on our junction tree. For simplicity of exposition
we have omitted the mailboxes in the junction tree.

443

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

Using (6) we have
cont

.J.fa(d)
( 7rR 0 ns)

(

)

L

cont(nR 0 ns)

L

PRPs(uR + us)

R\fa(d)
R\fa(d)

and
Figure 8: Passage of messages in the junction tree.
The number attached to the arcs indicate the order
that the messages are passed.
In our propagation scheme we successively collect
messages towards cliques that contain the variables
fa(d4),...,fa(d1) respectively. So we begin by collect­
ing messages to clique {r1,d4,d2,r4} since it contains
fa(d4)
{d4,r4,d2}. Then we compute an optimum
policy for d4 and modify the probability part on the
clique by multiplying it with the obtained policy for
d4. Now we partial collect messages towards clique
{d2,r2,d3} because it contains fa(d3). After comput­
ing an optimum policy for d3 and modifying the poten­
tial appropriately we partial collect messages towards
clique {d1,r1,d2}. Note that this clique not only con­
tains fa(d2) but it also contains fa(dl), and thus we
need not pass any more messages.
=

4

REFINEMENT OF THE
ALGORITHM

Suppose, at some stage in the algorithm, that the pol­
icy for decision d is to be computed, and let R be
any clique containing fa(d). In order to compute an
optimum policy for d we collect messages towards R.
The following theorem states a condition for when a
message from a neighbour of R is superfluous.
Theorem 5

Let C be a neighbour of clique R. Then,
whenever S = C n R � pa(d), the optimum policy for
d can be computed without the message from C.

Let 7rR (pR, uR) be the potential on R after
combining it with the messages from all its neighbours
except C. Further, suppose S R n C � pa(d) and
let ns (ps, us) be the message from C. We need to
show that for computation of the optimum policy for
d as described in Section 3.6, the message ns is not
needed.
=

=

=

(

.j.fa(d)
nR

)

=

""""'

� PRUR.

R\fa(d)

Clearly, asS� pa(d) � fa(d) we have that

L

R\fa(d)

where co

= Ps

PRPSUR =co

(

cont

n

:i[a(dl ) ,

(7)

2: 0 depends on Xpa(d) only.

Since f ex fq, where f and /q are given in (2) and (3)
we have

L

R\fa(d)

PRPSUS

us

L

V\fa(d)

cus

2:

f

V\fa(d)

fij,

where cis a constant. Because

is constant for fixed

Xpa(d),

L

Because multiple collect operations are performed in
T, we may pass many messages in the course of the
evaluation of all the decisions. In the present section
we give a condition for certain collect operations being
unnecessary.

P roof:

cont

R\fa(d)

where c1 depends on

this yields

PRPSUS

Xpa(d)

=

(8)

C!,

only.

Combining (7) and (8) now yields for fixed

Xpa(d)

where c0 2: 0 , i.e. for each fixed Xpa(d), the quantities
to be optimized with and without the message from
R are linearly and positively related. This completes
•
the proof.
The following example shows an application of Theo­
rem 5.
Example 4 The ID displayed in Fig. 9 was intro­
duced by Jensen et al. (1994). Fig. 10 shows the min­
imal reduction of the LIMID version of the ID and
Fig. 11 shows the junction tree of the minimal reduc­
tion.

In order to compute the optimum policy for d4 we
collect flows towards clique c4 since c4 contains fa(d4)·

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

444

Figure 11: The junction tree for the LIMID in Fig. 10.
Figure 9: An ID
4

t
4

t

Figure 10: The minimal reduction of the LIMID ver­
sion of the ID in Fig. 9.
However, an application of Theorem 5 gives that the
message from c3 is unneccessary: c3 n c4 is a subset
of pa(d4) in the minimal reduction in Fig. 10. Thus,
we will only need the message from C5. Furthermore,
to compute the optimum policy for d3 we collect flows
towards clique C9 since it contains fa(d3). But the
flow from C8 to C9 is unnecessary because Cs n Cg is
contained in pa(d3). Thus, only the flow from Cw to
C9 is needed because it has not been computed earlier.
Continuing in this way, it turns out that only one flow
along every edge in T is needed for the evaluation of
the decision problem (see Fig. 12). So, by applying
Theorem 5 we only need to pass 10 flows which is half
the flows we would have passed using the partial prop­
agation scheme presented earlier.
5

DISCUSSION

The method presented here transforms decision prob­
lems formulated as IDs into simpler structures, termed
minimal LIMIDs, having the property that all requi­
site information for the computation of the optimal
strategy is explicitly represented. It uses recursion to
solve the decision problem by exploiting that the en­
tire decision problem can be partitioned into a number
of smaller decision problems each of which having one
decision node only. A one-off process of compilation

Figure 12: Flow of messages for the computation of
the optimum policies using Theorem 5. The number
attached to the arcs indicate the order that the mes­
sages are passed.

is then performed on the LIMID to produce a higher
level graphical structure, the junction tree, that is par­
ticular well suited for efficient evaluation of each of the
small decision problems.
The use of recursion is inspired by the well-known
trick of Cooper (1988) and differs from methods in
e.g. Shenoy (1992), and Jensen et al. (1994). By using
recursion we do not require the storage of potentially
large tables of intermediate results (see for instance
Example 2).
As a consequence, our junction tree can always be
made as small as the strong junction tree (Jensen et al.
1994), and in some cases our method can result in
considerable reduction of evaluation time and mem­
ory. This reduction happens at two levels. At the
first level, we obtain a smaller junction tree because
we work in the reduced structure that only includes
requisite information. At the second level, we obtain
a smaller junction tree because we can triangulate the
structure obtained without obeying order constraints.
On the other hand, our method typically passes more
messages than the strong junction tree method, partly
because our junction tree have more (and smaller)
cliques, partly because we perform several collect prop­
agations. As the size of the maximal clique most often
is crucial for the efficiency of local computation algo­
rithms, our algorithm should generally be fast com-

445

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

pared to traditional algorithms.

Shachter, R. (1986). Evaluating influence diagrams. Oper­

There are many opportunities to refine and extend this
research. In particular it should be possible to reduce
the number of messages that are passed in the junc­
tion tree. We have presented one such condition for a
message being redundant, but a deeper insight in the
partial propagation algorithm may reveal other redun­
dant computations, and improve the efficiency of the
algorithm. The new method presented here also opens
the possibility of evaluating large and computationally
prohibitive decision problems by approximating them
with soluble LIMIDs. Work regarding this issue is de­
scribed in Lauritzen and Nilsson (1999) and is still in
progress.
Acknowledgements

This research was supported by DINA (Danish Infor­
matics Network in the Agricultural Sciences), funded
by the Danish Research Councils through their PIFT
programme.

Shachter, R. (1998).

Bayes-ball:

The rational pasttime

(for determining irrelevance and requisite information
in belief networks and influence diagrams). In Proceed­
ings of the Fourteenth Annual Conference on Uncer­
tainty in Artificial Intelligence {UAI-g8), pp. 48-487.
Morgan Kaufmann Publishers, San Francisco, CA.
Shachter, R. ( 1999). Efficient value of information com­
putation. In Proceedings of the 15th Annual Con­
ference on Uncertainty in Artificial Intelli gence, (ed.
K. Laskey and H. Prade), pp. 594-601. Morgan Kauf­
mann Publishers, San Francisco, CA.
Shachter, R. and Ndilikilikesha, P. (1993). Using influence
diagrams for probabilistic inference and decision mak­
ing. In Proceedings of the Ninth Conference on Uncer­
tainty in Artificial Intelli gence, (ed. D. Heckermann
and A. Mamdani), pp. 276-83. Morgan Kaufmann,
Stanford, California.
Shachter, R. and Peot, M. A. (1992). Decision making
using probabilistic inference methods. In Proceedings
of the Eighth Annual Conference on Uncertainty in
Artificial Intelligence (UAI-92), pp. 276-83. Morgan
Kaufmann Publishers, San Francisco, CA.
Shenoy, P. P. (1992). Valuation-based sy stems for Bayesian
decision analysis. Operati ons Research, 40, 463-84.



We describe an expert system, Maies, under development for analysing forensic identification problems involving DNA mixture
traces using quantitative peak area information. Peak area information is represented by conditional Gaussian distributions,
and inference based on exact junction tree
propagation ascertains whether individuals,
whose profiles have been measured, have contributed to the mixture. The system can also
be used to predict DNA profiles of unknown
contributors by separating the mixture into
its individual components. The use of the
system is illustrated with an application to a
real world example. The system implements
a novel MAP (maximum a posteriori) search
algorithm that is briefly described.

1

Introduction

Probabilistic expert systems (PES) for evaluating
DNA evidence were introduced in [1]. This paper is
concerned with describing the current status of a computer software system called Maies (Mixture Analysis
in Expert Systems) that analyses mixed traces where
several individuals may have contributed to a DNA
sample left at a scene of crime. In [2] it was shown how
to construct a PES using information about which alleles were present in the mixture, and we refer to this
article for a general description of the problem and for
genetic background information. (A brief summary to
genetic terminology is given in Appendix A.)
The results of a DNA analysis are usually represented
as an electropherogram (EPG) measuring responses in
relative fluorescence units (RFU) and the alleles in the
mixture correspond to peaks with a given height and
area around each allele, see Figure 1. The band intensity around each allele in the relative fluorescence

Julia Mortera
Dipartimento di Economia
Università Roma Tre
Via Ostiense, 139
00154 Roma, Italy.

units represented, for example, through their peak areas, contains important information about the composition of the mixture.

Figure 1: An electropherogram (EPG) of marker VWA
from a mixture. Peaks represent alleles at 15, 17 and
18 and the areas and height of the peaks express the
quantities of each. Since the peak around allelic position 17 is the highest this indicates that the 17 allele
is likely to be a homozygote or a shared allele between
two heterozygotes. This image is supplied courtesy of
LGC Limited, 2004.
The main focus of the present paper is to describe the
current status of a computer package called Maies,
which automatically builds Bayesian network models
for mixture traces based on conditional Gaussian distributions [3] for the peak areas, given the composition
of the true DNA mixtures. Currently the program only
considers a DNA mixture from exactly two contributors, which seems to be the most common scenario in
forensic casework [4], and the program ignores other
important complications such as stutter, dropout alleles, etc.

We distinguish two types of calculations that Maies
can perform. One type is evidential calculation, in
which a suspect with known genotype is held and we
want to determine the likelihood ratio for the hypothesis that the suspect has contributed to the mixture
vs. the hypothesis that the contributor is a randomly
chosen individual. We distinguish two cases: the other
contributor could be a victim with a known genotype
or a contaminator with an unknown genotype, possibly without a direct relation to the crime. This could
be a laboratory contamination or any other source
of contamination from an unknown contributor. The
other type calculation that Maies can perform is the
separation of profiles, i.e. identifying the genotype of
each of the possibly unknown contributors to the mixture, the evidential calculation playing a secondary
role. Both types of calculation are illustrated in § 5.

of DNA in a mixture sample. The model is idealized in
that it ignores complicating artefacts such as stutter,
drop-out alleles and so on, and assumes that the mixture is made up of DNA from two people, who we refer
to as p1 and p2. Typically, prior to amplification in a
laboratory, a DNA mixture sample will contain an unknown number of cells from p1 and a further unknown
number of cells from p2. Hence there is an unknown
common fraction, or proportion, across the markers
of the amount of DNA from p1, that we denote by
θ. In an ideal amplification apparatus, during each
amplification cycle the proportion of alleles of each allelic type would be preserved without error. We model
departures from this ideal as random variation using
the Gaussian distributions, and we introduce an additional variance term to represent other measurement
error, represented by ω 2 .

Previous related work includes that of [5] and [6] who
respectively developed numerical methods known as
Linear Mixture Analysis (LMA) and Least Square Deconvolution (LSD) for separating mixture profiles using peak area information. Both methods are based
on least squares heuristics that assume the mixture
proportion of the contributors’ DNA in the sample is
constant across markers. A computer program has
been written [7] for estimating the proportion of the
individual contributions in two-person mixtures and to
rank the genotype combinations based on minimizing
a residual sum of squares. More recently, [8] describes
PENDULUM, a computer package to automate guidelines in [9] and [7]. None of the methods described
above utilizing peak area information are probabilistic in nature, nor do they use information about allele
frequency. In contrast, the methodology proposed in
[10] combines a model using the gene frequencies with
a model describing variability in scaled peak areas to
calculate likelihood ratios and study their sensitivity
to assumptions about the mixture proportions.

The post-amplification proportions of alleles for each
marker are represented in the peak area information,
which we include in the analysis through the relative
peak weight. The (absolute) peak weight wa of an allele
with repeat number a is defined by scaling the peak
area with the repeat number as wa = aαa , where αa
is the peak area around allele a. Multiplying the area
with the repeat number is a crude way of correcting for
the fact that alleles with a high repeat number tend to
be less amplified than alleles with a low repeat number.
For issues concerning heterozygous imbalance see [11].

The plan of the rest of the paper is as follows. In
the following section we describe the mathematical
assumptions underlying the Bayesian networks that
Maies generates for analysing two-person DNA mixtures. We then describe the components that Maies
uses to build up the networks. This is followed by a
description of a simple MAP search algorithm, implemented in Maies for separation of profiles. We then
illustrate the use of Maies on a real life example, and
then summarize future work required to make Maies
into a tool for routine casework.

To avoid the arbitrariness in scaling used to measure
the areas, we consider the observed relative peak weight
ra , obtained by scaling with the total peak weight as
X
ra = wa /w+ , w+ =
wa ,

2

The mathematical model

Our PES is a probabilistic model for relating the preamplification and post-amplification relative amounts

We further assume that
• The peak weight for an allele is approximately
proportional to the amount of DNA of that allelic
type;
• The peak weight for an allele possessed by both
contributors is the sum of the corresponding
weights for the two contributors.

a

so that then

P

a ra

= 1.

For the relative peak weight, denoted by the random
variable Ra , we assume a Gaussian error distribution
Ra ∼ N (µa , τa2 ),

(2)
µa = {θn(1)
a + (1 − θ)na }/2, (1)

where θ is the proportion, or fraction, of DNA in the
(i)
mixture originating from the first contributor, na is
the number of alleles with repeat number a possessed
(i)
by person i. Note that na ∈ {0, 1, 2} and hence µa ∈
[0, 1].

We assume an error variance for τa2 of the form
τa2
2

2

= σ µa (1 − µa ) + ω

2

(2)

for U.S. Caucasians for the analysis in § 5 of data taken
from [9]2

2

where σ and ω are variance factors for the contributions to the variation from the amplification and measurement processes.1 Note that if µa = 0 then τa2 = ω 2
and Ra ∼ N (0, ω 2 ). The interpretation of this is that
if there are no alleles of type a in the mixture prior
to amplification, then any detected post-amplification
can be ascribed to measurement error. Similarly if
µa = 1 , which means that for the given marker all
alleles are of type a in the mixture before amplification, then Ra ∼ N (1, ω 2 ), that is post amplification
all alleles for the given marker are of type a, up to
measurement error.
However the Bayesian networks that Maies constructs
uses the variance structure
τa2 = σ 2 µa + ω 2 .

(3)

The reason is that we need to consider the correlation
between weights due to the fact that they must add
up to unity. It turns out, perhaps surprisingly, that
the
P likelihood obtained using (3) when conditioned on
a ra = 1 has precisely the form as would be obtained
using the likelihood based on (2) used in our model if
we ignore measurement error by setting ω 2 = 0, and
2
the likelihoods are
P very close numerically for small ω .
This constraint a ra = 1 is imposed when we enter
the complete set of observed peak weights as evidence
in our networks. The proof, too long for this paper,
may be found in [12].
In our example in § 5 we used σ 2 = 0.01 and ω 2 =
0.001, corresponding approximately to a standard
deviation
for the observed relative weight of about
p
0.01/4 + 0.001 = 0.06 for µa = 0.5 substituted into
(2). These parameter values imply that when amplifying DNA from one heterozygous individual (for which
µa = 0.5), an ra value at two standard deviations from
the mean would give a value of 0.38/0.62 = 0.61 for
the ratio of the minor to the major peak area; this is
about the limit of variability in peak imbalance that
has been reported in the literature [13], and suggests
that our chosen parameter values are perhaps conservative.
In general the variance factors may depend on the
marker and on the amount of DNA analysed, but for
simplicity we use the values above. (Our PES model is
robust to small changes in these parameter estimates.)
Finally, we assume known gene frequencies of single
STR alleles; in particular we use those reported in [14]

3

Maies

The basic form of our Bayesian network models is
fairly straightforward, but the networks can grow large
when modelling the ten or so markers typical in a
mixture problem. (In the example in § 5 the network
has 237 nodes.) One way to manage this complexity is to use object-oriented Bayesian network software, as we describe in detail in [12]. Here we describe Maies, a purpose built program that, after entering peak area information and available genetic information (if available) about the potential contributors, automatically constructs a single conditionalGaussian Bayesian network on which the probability
calculations are performed. Maies implements the local propagation scheme of [15]. Peak areas are automatically converted to normalized weights and entered
as evidence in the relevant nodes by the program.
An example of a network generated for a single marker
with two alleles observed in the mixture is shown in
Figure 2. The figure illustrates the repetitive modular
structure that makes it possible for Maies to create
the much larger Bayesian networks required to analyse
mixtures on several markers. We now describe these
various structures and how they interrelate, working
from top to bottom in Figure 2.
u1mg

u1pg

smg

u1gt

spg

vmg

sgt

vpg

u2mg

vgt
p1 = s?

p1gt

u2pg

u2gt

p2 = v?

target

p2gt

jointgt
p1 8

p1 9

p1 x

p2 8

8 inmix ?

p2 9

p2 x

x inmix ?
9 inmix ?

8 weight

9 weight

x weight

9 weightobs

x weightobs

p1 frac
8 weightobs

sym

Figure 2: The structure of a Bayesian network generated by MAIES for a single marker, in which two allele
peaks (8 and 9) were observed.

1

The first term in the variance structure in (2) can be
seen as a second order approximation to a more sophisticated model based on gamma distributions for the absolute
scaled peak weights to be discussed elsewhere.

2
This dataset has an observed allele 36 of the marker
D21. As none of the 302 subjects in [14] had this allele, we
chose to use 1/604=0.00166 as its frequency.

3.1

Top level people

Maies currently models mixtures only for DNA from
two individuals. Thus it sets up nodes for four individuals who are paired up, prefixed by s (for suspect),
v (for victim), and u1 and u2 representing two unspecified persons from the population. Corresponding
to each of these individuals is a triple of nodes representing their genotype (gt) on the marker, and the
individuals’ paternal (pg) and maternal (mg) genes.
The probability tables associated with the maternal
and paternal genes contain the allele frequencies of the
observed alleles, whilst the conditional probability table associated with the genotype node is the logical
combination of the maternal and paternal gene.
3.2

3.4

Repeat number nodes

On the level below the allele counting nodes are the repeat number nodes, labelled 8 inmix?, 9 inmix? and
x inmix?. These are (yes,no) binary valued nodes
representing whether or not the particular alleles are
present in the mixture: thus for example allele 8 is
present in the mixture if either of the allele counting nodes p1 8 or p2 8 takes a non-zero value. For
the node x inmix? the x refers to all of the alleles
in the marker that are not observed. When using repeat number information as evidence the repeat number nodes present in the mixture will be given the value
yes and x inmix? will be given the value no.

Actual contributors to the mixture

The genotypes on the marker of the two individuals
p1 and p2 whose DNA is in the mixture are the nodes
labelled p1gt and p2gt. Node p1gt has incoming arrows from nodes u1gt, sgt and a (yes,no) valued
binary node labelled p1 = s?. The function of this
latter node is to set the genotype of node p1gt to be
that of sgt if p1 = s? takes the value yes, otherwise
to set the genotype of node p1gt to be that of u1gt.
An equivalent relationship holds between the genotype
nodes p2gt, vgt, u2gt and p2 = v?. Uniform priors
are placed on the nodes p1 = s? and p2 = v?.
The node labelled target represents the four possible combinations of values of the two nodes p1 = s?
and p2 = v?, with a conditional probability table of
zeros and ones representing the logical identities. The
marginal posterior distribution of this node is used to
calculate likelihood ratios in evidential calculations.
The network also has a node representing the joint
genotypes of individuals p1 and p2, which is labelled
jointgt, with incoming arrows from p1gt and p2gt;
the (quite big) conditional probability table associated
with this node has entries that are either of zero or one.
The most likely configuration of the marginal distribution in all joint genotype nodes across all markers is
required for separating the mixture, and is found using
the MAP search algorithm described in § 4.
3.3

(i)

These nodes model the na variables introduced in (1).

Allele counting nodes

On the level below the genotype nodes for p1 and p2
is a set of nodes representing the number of alleles
(taking the value of 0, 1 or 2) of a certain type in
each individual. Thus, for example, the node p1 8
counts the number of alleles of repeat number 8 in the
genotype of individual p1 for the given marker: this
value only depends upon the genotype of the individual
p1 and hence there is an arrow from p1gt to p1 8.

3.5

True and observed weight nodes

These nodes are represented by the elliptical shapes.
The nodes 8 weight, 9 weight and x weight represent the true relative peak weights r8 , r9 and rx respectively of the alleles 8, 9 and x in the amplified DNA
sample. Each true-weight node is given a conditionalGaussian distribution as in (1), where the fraction θ
of DNA from p1 in the mixture is modelled in the
network by a discrete distribution in the node labelled
p1 frac. The variance is taken to be σ 2 µa . The nodes
8 weightobs, 9 weightobs and x weightobs represent the measured weights. The observed weight is
given a conditional-Gaussian distribution with mean
the true weight, and measurement variance ω 2 , hence
leading to the variance (3).
When using peak area information as evidence the
nodes representing the observed weights will have their
values set to the relative peak weights. The sym node
is only used for separating a mixture of two unknown
contributors, to break the symmetry between p1 and
p2 (see § 5.2).
3.6

Networks with more than one marker

The network displayed in Figure 2 generated by
Maies is for a single marker; for mixture problems
involving several markers the structure is similar but
more complex because the number of nodes grows with
the number of markers. In such a network the nodes
shaded in Figure 2 occur only once. The unshaded
nodes are replicated once for each marker, with each
node having text in their labels to identify the marker
that the allele or genotype nodes refer to. There will
also be extra repeat number, allele counting and allele
weight nodes in each marker having more than two
observed alleles in the mixture, extending the pattern
for the one-marker network in the obvious manner.

4

A simple MAP search algorithm

It is well known that the most likely configuration of a
set of discrete variables is not necessarily the same as
found by picking the most likely states in the individual marginals of the variables (see for example [16]).
The basis of the MAP search algorithm in Maies is to
assume that this is close.
Specifically, after entering and propagating evidence,
one finds the individual marginals of the set of MAP
variables M of interest. There are then two variants
of the MAP algorithm, batch and sequential.
In the batch variant, one finds for some reasonable
number n (say n = 5000) the top n most likely configurations of the joint probability given by the product of the individual marginals of MAP variables.
This is done by constructing a disconnected graph
in the MAP variables, and using the efficient algorithm of [17]. These configurations are stored in an
list (c1 , c2 , . . . , cn ) ordered by decreasing probability
according to the independence graph. Now returning to the original Bayesian network, one propagates
the available evidence E and finds the normalization
constant P (E) and stores this in rp, say (short for
“remainder probability”). One then processes the
(c1 , c2 , . . . , cn ) configurations as additional evidence in
the original Bayesian network and finds from the normalization constant each of their probabilities P (ci , E).
After processing each configuration, one keeps track
of the highest probability configuration found and its
probability, bp. One also subtracts p(ci , E) from rp,
so that if it ever happens bp > rp then the MAP
has been found. If one stores all of the probabilities
p(ci , E), i = 1, . . . , k for all of the configurations that
have been processed, then perhaps the second, third
etc. most likely configurations may
Pk be identified if
their probabilities exceed P (E) − i=1 P (ci , E). The
sequential variant proceeds similarly, the difference is
that the candidates c1 , c2 , . . ., are generated one at a
time as required. The following is pseudo-code for the
sequential variant for finding the MAP.

the second, third, fourth etc., most likely configurations.

5

A criminal case example

Our example is taken from Appendix B of [9] and illustrates the use of the amelogenin marker in the analysis
of DNA mixtures when the individual contributors are
of opposite sex.
Peak area analysis of the amelogenin marker in DNA
recovered from a condom used in a rape attack indicated an approximate 2:1 ratio for the amount of female to male DNA contributing to the mixture. Peak
area information was available on six other markers,
the information is shown in Table 1; we shall refer to
this as the Clayton data. (Further examples are illustrated in [12] and [18].)
Table 1: Clayton data of [9] showing mixture composition, peak areas and relative weights together with
the DNA profiles of both victim (v) and suspect (s).
For the marker D21 the allele designation in brackets
is as given in [9] using the labelling convention of [19]
Marker

Alleles

Amelogenin

X
Y
13
14
15
14
15
16
18
(61)
(65)
(70)
(77)
22
23
5
7
15
16
17
19

D8
D18

D21

FGA
THO
VWA

• Initialize: i = j = 1, bp = 0, and rp = P (E).
• While bp < rp do:
– Find ci and P (ci , E);
– If p(ci , E) > bp set bp = P (ci , E) and j = i;
– Set rp := rp − P (ci , E) and i := i + 1;
• cj is the MAP configuration.
For purely discrete networks, this algorithm does not
appear to be as efficient as that described in [16]. However it is neither clear that the latter can be applied to
finding the MAP of a set of discrete variables in a conditional Gaussian network, nor that it could identify

5.1

28
30
32.2
36

Peak
area
1277
262
3234
752
894
1339
1465
2895
2288
373
590
615
356
534
2792
5735
10769
1247
1193
2279
2000

Relative
weight
0.8298
0.1702
0.6372
0.1596
0.2032
0.1462
0.1714
0.3612
0.3212
0.1719
0.2913
0.3259
0.2109
0.1547
0.8453
0.2756
0.7244
0.1633
0.1667
0.3383
0.3318

s

v

X
Y

X
13

14
15
14
15
16
18
28
30
32.2
36
22
23
7
15
16

23
5
7
17
19

Evidential calculation

One possible use of the system to the Clayton data
would be to compare the two hypotheses:
• H0 : the suspect and victim both contributed to
the mixture
• H1 : the victim and an unknown contributor contributed to the mixture

In a courtroom setting, the null hypothesis, H0 , would
be a prosecution’s case, whilst H1 would represent the
defence’s case. (It is standard procedure in court for
likelihood ratios of these hypotheses to be reported.)

of genotypes of the two contributors to the mixture,
but other less likely but also plausible combinations.
Maies achieves this with the MAP search algorithm
described in § 4.

To do this calculation in Maies, evidence is entered
in the observed relative peak area nodes, the repeat
number nodes, and information on the suspect and
victim genotypes. After propagating the evidence the
marginal on the target is examined. This has the
following values (taken from Maies):

For separating a mixture, we may or may not have genetic information about one of the contributors. For
our rape example, suppose that we have the genotype
of the victim. Then using Maies we may enter as evidence the victim’s genotype, the relative peak areas
and the repeat number information. We also select the
value yes in the p2 = v? node. We then select the set
of joint genotype nodes, and perform the MAP search.
Maies returns two configuration, the most likely having posterior probability 0.997594, with the genotype
p1 matching our suspect profile in Table 1. The second
most likely combination has a posterior probability of
0.00239796, and differs from the true profile in the
marker FGA where a homozygous (22, 22) genotype
is predicted. All remaining possible genotype combinations have a total probability mass of less than
8 × 10−6 .

u1 & u2
v and u
s and u
s and v

4.2701211814389×10−21,
4.1040333719867×10−11,
3.660791624072×10−11,
0.99999999992235.

From this the likelihood ratio of H0 to H1 is calculated
to be P (s and v | E)/P (s and u | E) = 2.73 × 1010 ,
where E denotes the complete set of evidence.
(Note that because we have placed
uniform priors on the nodes p1 = s?
and
p2 = v?, then P (E | s and v)/P (E | s and u)
=
P (s and v | E)/P (s and u | E).)
It may be that only DNA from a suspect is available,
but not from a victim. In such a situation we could
use Maies to compare the following two hypotheses:
• H0 : the suspect and an unknown contributor contributed to the mixture
• H1 : two unknown contributors contributed to the
mixture
Again in a courtroom setting these could represent
prosecution and defence cases respectively. The calculation proceeds as before, but with the victim profile
omitted from the evidence. This time the marginal on
the target node is given by
u1 & u2
v and u
s and u
s and v

5.8322374221768×10−11,
5.8322374221768×10−11,
0.49999999994168,
0.49999999994168,

and the likelihood ratio of H0 vs. H1 is given by
P (s and u | E)/P (u1 and u2 | E) = 8.57 × 109 .
5.2

Mixture separation calculations

The other type of calculation that may be performed
with Maies is that of separating a two-person mixture
into genotypes of the contributors. The output from
such a decomposition could be used to find a match
in a DNA database search. For such a search it is
useful to have not just the most likely combination

The second possibility is that no genotypic information is available on either contributor to the mixture.
To do this calculation, evidence on the observed relative peak areas and the repeat number information
is entered. To overcome the symmetry in the network
between p1 and p2, we enter evidence on the sym node
that p1 frac is ≥ 0.5. Then, selecting the joint genotype nodes as before, we perform a MAP search. The
result is shown in Table 2. All markers are correctly
identified. Note in particular that the genotypes for
the marker THO are identified correctly. In [9] this
was only possible to do so after the victim’s profile
was taken into account.
Table 2: Most likely genotype combination of both
contributors for Clayton data. The victim (here p1)
and male suspect (p2) is correctly identified on every
marker. The final column indicates the marginal probabilities for the genotype pairs on individual markers, with the figure in parenthesis the product of these
marginals.
Marker
Amelogenin
D8
D18
D21
FGA
THO
VWA
joint

Genotype Genotype
p1
p2
XX
XY
13 13
14 15
16 18
14 15
30 32.2
28 36
23 23
22 23
57
77
17 19
15 16
0.701988

Posterior
probability
0.983115
0.903013
0.993166
0.945235
0.989090
0.845031
0.992738
(0.691517)

For this example, the Maies MAP search algorithm
identifies the next three most likely combinations

0.05

Posterior density

0.10

0.15

sumed correct. Such methods could also be useful for
calibrating the variance parameters σ 2 and ω 2 . We
are pursuing ways that this could be accomplished using an EM estimation algorithm. (Bayesian methods
for estimating the variance parameters could also be
developed.) Nevertheless, despite these many issues,
we feel that the present framework provides a sound
foundation in which these and other matters can be
be addressed and incorporated into Maies.

0.00

Acknowledgements
0.50

0.55

0.60

0.65

0.70

0.75

0.80

Proportion of DNA from the major contributor.

Figure 3: Posterior distribution of mixture proportion
from Clayton data using no genotypic information.
of genotypes, these have probabilities of 0.120049,
0.0583912 and 0.0227133 respectively. (The network
has 183 discrete nodes and 54 continuous nodes. The
total state space of the 7 joint genotype nodes is approximately 5.9 × 1012 . Identifying the 15 most likely
combinations of these nodes took approximately 38
seconds on a 1.6GHz laptop with 256Kb memory.)
Finally for this example, Figure 3 shows the posterior
distribution of the mixture proportion; the peak at
around 0.65 corresponds to a mixture ratio of 1.86:1,
in line with the approximate 2:1 estimated in [9].

6

Discussion

We have described a software system, Maies, for
analysing DNA mixtures using peak area information,
yielding a coherent way of predicting genotypes of unknown contributors and assessing evidence for particular individuals having contributed to the mixture, and
applied it to a real life example. A simple MAP search
algorithm allows a set of most plausible genotypes
to be generated, perhaps for use in a DNA database
search for a suspect.
There are a number of issues that would need addressing before the system could be used in routine analysis
of casework, for example, complications such as more
than two potential contributors, multiple traces, indirect genotypic evidence, stutter, etc. In addition,
preliminary investigations seem to indicate that the
variance factor depends critically on the total amount
of DNA available for analysis. As this necessarily is
varying from case to case, a calibration study should
be performed to take this properly into account. Methods for diagnostic checking and validation of the model
should be developed based upon comparing observed
weights to those predicted when genotypes are as-

This research was supported by a Research Interchange Grant from the Leverhulme Trust. We are indebted to participants in the above grant and to Sue
Pope and Niels Morling for constructive discussions.
We thank Caryn Saunders for supplying the EPG image used in Figure 1.


