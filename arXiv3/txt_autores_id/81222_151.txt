

without employing extensive terminology, thus mak­
ing the algorithms accessible to researchers working

This paper describes a class of probabilistic
approximation algorithms based on bucket
elimination which offer adjustable levels of
accuracy and efficiency. We analyze the ap­
proximation for several tasks: finding the
most probable explanation, belief updat­

in diverse areas.

More important, their uniformity

facilitates the transfer of ideas, applications, and
solutions between disciplines.

Indeed, all bucket­

elimination algorithms are similar enough to make
any improvement to a single algorithm applicable to
all others in its class.

ing and finding the maximum a posteriori

Normally, the input to a bucket-elimination algo­

hypothesis.

rithm consists of a knowledge-base theory specified

We identify regions of com­

pleteness and provide preliminary empiri­

by a collection of functions or relations, (e.g., clauses

cal evaluation on randomly generated net­

for propositional satisfiability, constraints, or condi­

works.

tional probability matrices for belief networks) . In

its first step, the algorithm partitions the functions

1

into buckets, each associated with a single variable.

Overview

Given a variable ordering, the bucket of a partic­
ular variable contains the functions defined on that

Bucket elimination, is a unifying algorithmic frame­

variable, provided the function is not defined on vari­

work that generalizes dynamic programming to en­

ables higher in the ordering. Next, buckets are pro­

able many complex problem-solving and reasoning
activities.

Among the algorithms that can be ac­

commodated within this framework are directional
resolution for propositional satisfiability, adaptive
consistency for constraint satisfaction, Fourier and

cessed from top to bottom. When the bucket of vari­
able

X

in its bucket. The result is a new function defined
over all the variables mentioned in the bucket, ex­

Gaussian elimination for linear equalities and in­

cluding

equalities, and dynamic programming for combina­

of

torial optimization

[7].

Many algorithms for proba­

bilistic inference, such as belief updating, finding the

is processed, an elimination procedure or an

inference procedure is performed over the functions

X

X.

This function summarizes the "effect"

on the remainder of the problem.

The new

function is placed in a lower bucket. For illustration
we include algorithm elim-mpe, a bucket-elimination

most probable explanation, finding the maximum a

algorithm for computing the maximum probable ex­

posteriori hypothesis, and calculating the maximum

planation in a belief network

expected utility, also can be expressed as bucket­
elimination algorithms

[3].

The main virtues of this framework are simplic­
ity and generality.

By simplicity, we mean that

complete specification of these algorithms is feasible
Thls work was partially supported by NSF grant
IRI-9157636 and by Air Force Office of Scientific Re­

( Figure 1) [3].

An important property of variable elimination algo­
rithms is that their performance can be predicted
using a graph parameter called induced width

( also called

[5},

tree-width[l ]) , which is the largest clus­

ter in an optimal tree-embedding of the graph. In
general, a given theory and its query can be asso­
ciated with an interaction graph describing various

search grant, AFOSR 900136, Rockwell International

dependencies between variables. The complexity of

and Amada of America.

bucket-elimination algorithms is time and space

ex-

133

A scheme for approximating probabilistic inference
ponential in the induced width of the problem's in­
teraction graph.

Depending on the variable order­

ing, the size of the induced width will vary and this
leads to different performance guarantees.
When a problem has a large induced width bucket­
elimination is unsuitable because of its extensive
memory demand. Approximation algorithms should
be attempted instead. We present here a collection
of parameterized approximation algorithms for prob­
abilistic inference that approximate bucket elimina­
tion with varying degrees of accuracy and efficiency.
In a companion paper

[4],

we presented a similar ap­

proach for dynamic programming algorithms, solv­
ing combinatorial optimization problems, and belief
updating. Here we focus on two tasks: finding the
most probable explanation and finding the maxi­
mum a posteriori hypothesis. We also show under
what conditions the approximations are complete
and provide preliminary empirical evaluation of the
algorithms on randomly generated networks.
After some preliminaries ( section

2),

we develop the

approximation scheme for the most probable expla­
nation task

4), and for
(section 5).

( section 3 ) ,

for belief updating (section

the maximum a posteriori hypothesis
We summarize the results of our em­

pirical evaluation in section

6.

Related work and

conclusions are presented in section

2

7.

Preliminaries

Definition 2.2 (belief networks)

Let X = {X1,... , Xn} be a set of random variables
over multivalued domains D1, ..., Dn. A belief net­
work (BN) is a pair (G, P) where G is a directed
acyclic graph and P = {P;}. P; = {P(X,Ipa(X,))}
are the conditional probability matrices associated
with X,. An a11signment (X1 = :llt, .. .,Xn = :!ln }
can be. abbreviated to :z: = ( :ct. . . .,:cn)· The BN
represents a probability distribution P(zt,. .. . , :en ) =
n�=lP(:z:,l:z:pa(x,)), where, Zs i!l the projection of z
over a sub11et S. if u is a tuple over a &ubset X,
then us denotes that assignment, which is restricted
to the variables in S n X. An evidence set e is an
instantiated subset of variables. We use (us, :cp) to
denote the tuple us appended by a value :z:P of Xp,
where Xp is not in S. We define i, = (:c1,...,z,)
and i{ = (z;,:z:i+l• ... , Zj).
Definition 2.3 (elimination functions) Given a
function h defined over subset of variables S,
where X E S, the functions (minxh), (ma:cxh),
(meanxh), and CL:x h) are defined over U
u,
For every U
S - {X} as follows.

mi� h(u,z), (ma:z:xh)(u)
(minxh)(u)
I: .. h(u, :z:), and
h(u,:z:), (L:x h)(u)
(meanxh)(u) = L: .. h�ij), where lXI i6 the car­
dinality of X 's domain. Given a 6et of functions
ht,..., h; defined over the 11ubset6 81, . . . , S1, the prod­
uct function (Tij hi) and L: 1 h1 are defined over U =
UjSj. For every U = u, (TI1h1)(u) = TI1hi(us3) and
(L:1 hj)(u) = Lj hj(us;)·

m�

Definition 2.1 (graph concepts)

A directed graph is a pair, G

A poly-tree is an acyclic directed graph whose under­
lying undirected graph (ignoring the arrows) has no
loops. The moral graph of a directed graph G is the
undirected graph obtained by connecting the parents
of all the node11 in G and then removing the arrows.

=

{V,E}, where

V = {X1, ... , Xn} is a set of elements and E =
{(X,,Xi)IX,,Xj E V } is the set of edges. If
(X;,Xj) E E, we say that X; points to Xi. For each
variable X;, pa(X;) or pa;, is the 11et of variable11
pointing to X, in G , while the. 11et of child node11 of
X,, denoted ch(X,), compri11e!l the variables that X1
point11 to. The family of X;, F,, includes X1 and its

child variable11. A directed graph is acyclic if it ha11
no directed cycle11. An ordered graph i!l a pair (G, d)
where G i11 an undirected graph and d = X11 ... ,Xn is
an ordering of the nodes. The width of a node in an
ordered graph i!l the number of the node 18 neig hbors
that precede it in the ordering. The width of an or­
dering d, denoted w(d), is the maximum width over
all nodes. The induced width of an ordered graph,
w * (d), is the width of the induced ordered graph ob­
tained by processing the node!l from last to first,· when
node X is p rocessed, all its neighbor11 that precede it
in the ordering are connected. The induced width of
a graph, W*, is the minimal induced width over all
it!l ordering!lj it is al!lo known as the tree-width [1].

Definition

2.4

(probabilistic tasks)

The mo11t probable explanation (mpe) task is to find
an anignment :z:0 = ( :z:011 . . . , :Z:0n) such that p(:z:0) =
ma.x.z,. Ilf=1 P(:c,, el:z:p4;). The belief assessment task
of X = :z: is to find bel(:z:) = P(X = :z:le) . Given
a set of hypothe11ize.d variables A = {At, ..., A�:},
A � X, the maximum a posteriori hypothesis (map)
task is to find an as6ignment a0 = (a01, ..., a0�c ) such
that p(a0) = ma.x;,11 L:'".x-A Ilf:::::1P(:z:;l:cpa,,e ) .
3

Approximating the mpe

Figure

mpe [3]

1

shows bucket-elimination algorithm

for computing mpe.

elim­

Given a variable or­

dering and a partitioning of the conditional proba­
bilities into their respective buckets, the algorithm

134

Dechter and Rish

Algorithm elim-mpe
Input: A belief network BN = {P1, ..., Pn}; an or­
dering of the variables, d; observations e.
Output: The most probable assignment.
1. Initialize: Partition BN into buckeh, ..., bucketn,
where bucket; contains all matrices whose highest vari­
able is X,. Put each observed variable into its appro­
priate bucket. Let S1, .. . , S; be the subset of variables
in the processed bucket on which matrices (new or old)
are defined.
2. Backward: For p +- n downto 1, do
for h1,h3, ...,h; in bucket,, do
• (bucket with observed variable) If bucket, contains
X, = :c,, BIIBign X, = a:, to each h; and put each
resulting function into its appropriate bucket.
.
• Else, generate the functions h": h" = ma:cx ll�
,. =l h;
and :r:; = argm.azx,.h". Add h" to the bucket of the

UL,

largest-index variable in U, +S;- {X,}.
3. Forward: Assign values in the ordering d using
the recorded functions :c0 in each bucket.

Figure 1: Algorithm

bottom. When processing the bucket of Xp, a new
function is generated by taking the maximum rel­
ative to Xp, over the product of functions in that
bucket. The resulting function is placed in the ap­
propriate lower bucket. The complexity of the al­
gorithm which is determined by the complexity of

2},

is time and space

exponential in the number of variables in the bucket
(namely the bucket's variable induced-width) and
is, therefore, time and space exponential in the
induced-width

w•

of the network's moral graph

[3].

Since the complexity of processing a bucket is tied
to the arity of the functions being recorded, we pro­
pose to approximate these functions by a collection
of smaller arity functions. Let

ht,..., h;

be the func­

tions in the bucket of Xp, and let St, ... , Sj be the
variable subsets on which those functions are d�
fined. When

elim-mpe processes the bucket of Xp, it
hP: hJ' = mazx,.II{=1�. One

computes the function

brute-force approximation method involves generat­
ing, instead, by migrating the maximization oper­
ator inside the multiplication, a new function

gP

=

rr{=lmazx,.hs.

Since each function

product of hJ' is replaced by

uct defining

gP, hJ' � gP.

mazx,.hs

hs

gP:

in the

in the prod­

We see that

gP has

a

product form in which the maximizing elimination
operator

gP's

mazx,. hs

This idea can be generalized to yield a collection
of parameterized approximation algorithms having
varying degrees of accuracy and efficiency. Instead
of applying the elimination operator (i.e., multiplica­
tion and maximization) to each singleton function in
a bucket as suggested in our brute-force approxima­
tion above, it can be applied to a more coerced par­
titioning of the buckets into mini-buckets. Let

{Qt, ...,Q,.}

Ql

=

be a partitioning into mini-buckets of

ht, ... , hi in Xp's bucket. The mini­
Q, contains the functions h11, ..., hlr· Algo­
rithm elim-mpe computes hP: (l index the mini­
buckets) hP = rnazx,.IIf=t� = mazx,.II/=1II,,h,,.
the functions

bucket

By migrating the maximization operator into each
mini-bucket, we get:

�'

=

II1=1 mazx,.II,,h,,.

As

the partitionings are more coerced, both the com­
plexity and the accuracy of the algorithm increase.

elim-mpe

starts processing buckets successively from top to

processing each bucket (step

mpe.

is applied separately to each of

component functions. The resulting functions

will never have dimensionality higher than

�.

and

each of these functions is moved, separately, to a

Definition 3.1 Partitioning Q' i! a refinement of
Q" iff for every !et A E Q' there ea:i!U a !et B E Q"
such that A C B.
Proposition 3.2 I'! in the bucket of Xp, Ql i! a
refinement of Q", then hP� �, � �·. 0
Algorithm

approz-mpe{i,m) is described

in Figure

2.

It is parameterized by two indexes that control the
partitionings.

Let H be a collection of function.
ht. ..., hi defined on subseu of variables, 81, ... , Si.
A partitioning of H u canonical if any function
whose argumenu are sub!umed by another function
belong! to a bucket with one of those !ubsuming
function.. A partitioning Q into mini-buckeu u
an (i, m)-partitioning iff 1. it u canonical, 2. at
most m nomubsumed function! participate in each
mini-bucket, 3. the total number of variables in a
mini-bucket doe! not ezceed i, and 4. the partition­
ing is refinement-maximal, namely, there is no other
( i,m)-partitioning that it refines.
Definition 3.3

If indez i is at least as large a! a
family size, then there ea:i!t an (i, m)-partitioning of
each bucket. 0

Proposition 3.4

Algorithm approz-mpe(i,m) com­
putes an upper bound to the mpe in time O(m
ezp(2i)) and space O(m e:z:p(i)), where i � n and
m� 2i.
Theorem 3.5

·

·

as m

lower bucket. When the algorithm reaches the first

Clearly, in general,

variable, it has computed an upper bound on the

accurate approximations.

and

i

increase we get more

135

A scheme for approximating probabilistic inference

Algorithm approx-rnpe(i,rn)
Input: A belief network BN =

{Pt, .. . ,Pn}i

ordering of the variables, d;
Output: An upper bound on the most probable as­

signment, given evidence e.
1. Initialize: Partition into buclcet1, ..., bucketn,
where bucket; contains all matrices whose highest vari­

able is X;. Let St, ... , S; be the subset of variables in
bucketp on which matrices old or new are defined.

2.
•

)

(

For p f- n downto 1, do
lfbucketp contains Xp = Zp, assign Xp =

(Backward)

h; and put each in appropriate bucket.
• el s e, for h1, h,, .. . ,h; in buclcetp, do:
Generate an

{Q�, ... , Qr}.

Zp

to each

(i, m)-mini-bucket-partitioning, Q'

For each

do,

Qr

E

Q

1

containing hr11

=

hr,

•••

1
1
1
Generate function h , h = mazxPII!=1hr,. Add h
to the bucket of the largest-index variable in Ur f-

ULl s,, - {Xp}•

3. (Forward) For i = 1 to n do, given Zt, •.• ,:z: p-1
choose a value :&p of Xp that maximizes the product
of all the functions in Xp's bucket.

Figure

2:

algorithm

In
maznP(H[E, F) h1(H, G), and &o on.
bucket(B) obtain the mpe value mazBP(B). hc(B),
and then can generate the mpe tuple while going for­
ward. If we proceu by approx-mpe ( n,l ) in6tead,
we get (we denote by 'Y the function& computed by
appro:z:..elim{n, 1} that differ from tho6e generated by
elim-mpe):
bucket(!) = P(IIH, G)
bucleet(H} = P(H\E, F), h1(H,G)
bucket{ G} = P(G[E , D),"fH(G)
bucket{F) = P(F[B), 'YH (E,F),
bucket{E) = P(E\C,B),y(E),-yG (E,D)
bucleet{D) = P(D\C),"fE(D)
bucket{C) = P(C),-yE(C,B),'YD(C)
bucket(B) = P(B),-yc(B),"fF (B).
Algorithm& elim-mpe and approx-mpe ( n,l ) fir6t dif­
fer in their processing of bucket( G). There, in6tead of recording a function on three variable6,
hH(E, F, G), iu6t like elim-mpe, appro:z:..mpe(n,l)
record6 two function6, one on G alone and one on
E and F. Once approz-mpe{n,l} ha6 proceued all
bucket!, we can generate a tuple in a greedy fa6h­
ion a6 in elim-mpe: we choo6e the value of B tiWJt
marimize6 the product of function6 in B 's bucket,
then a value of C marimizing the product-function6
in bucket(C}, and so on.
·

and an

appro:c-mpe{i,m)

There is no guarantee on the quality of the tuple
we generate. Nevertheless, we can bound the error

3: A belief network P(i,h,g,e,d,c,b)=
P(i\h, g )P( h\e , f )P(gje,d)P (e \c, b)P(d\c)P(b)P(c)

Figure

of

appro:z:-mpe

by evaluating the probability of the

generated tuple against the derived upper bound,
since the tuple generated provides a lower bound on
the mpe.

Example 3.6 Con,ider the network in Figure 3.
A.uume we we the ordering (B, C, D, E,F, G,H,I)
to which we apply both algorithm elim-mpe and it!
6imple6t approrimation where m = 1 and i = n. Ini­
tially the bucket of each variable will have at mod
one conditional probability: bucket{!) = P(I \H,G),
bucleet(H) = P(H \E, F), bucket( G) = P(G\E,D),
bucleet(F) = P(F \B ) , bucket{E) = P(E\C,B),
bucket(D) = P(D \ C), bucket(C) = P(C) , bucleet(B}
= P(B). Proce66ing the buckeb from top to bottom
by elim-mpe generate6 functiom that we denote by h
function,: buclcet(I) = P(I\H, G)
bucket(H) = P(H \E, F), h1(H,G)
bucleet(G) = P(GIE,D),hH(E,F,G)
bucket(F) = P(F\B),hG(E, F, D)
bucket(E) = P(E\C, B), h F(E,B, D)
bucket(D) = P(D\C),hE(C,B,D)
bucket{C) = P(C),hn(C, B)
bucleet{B) = P(B),hc(B)
Where h1(H,G) = mazrP(I\H,G), hH(E,F,G) =

Alternatively, we can use the recorded bound in each
bucket as heuristics in subsequent search.
the functions computed by

approz-mpe{i, m}

Since
are al­

ways upper bounds of the exact quantities, they
can be viewed

as

over-estimating heuristic func­

tions in a maximization problem. We can associate

ip-1 = {z1, ... , :l:p-t )
f(i,_l) = (g h)(z,_1)
rrr.:� P(zi\Zpo.) and h(ip-1) =

with each partial assignment
an evaluation function

·

g(ip-1) =
IT;ebuclcetp_1h;. It is easy to see that the evaluation
function f provides an upper bound on the mpe re­
stricted to the assignment i,_1• Consequently, we
where

can conduct a best first search using this heuristic
evaluation function.

From the theory of best first

search we know that

(1)

when the algorithm ter­

minates with a complete assignment, it has found
an optimal solution;

(2)

the sequence of evaluation

functions of expanded nodes are non-increasing;
as

(3)

the heuristic function becomes more accurate,

fewer nodes will be expanded; and

(4)

if we use the

136

Dechter

and Rish

full bucket-elimination algorithm, best first search
will become a greedy and complete algorithm for the
mpe task [10].

3.1

Cases of completeness

Clearly, approz-mpe(n, n) is identical to elim-mpe
because a full bucket is always a refinement-maximal
(n, n)-partitioning. There are additional cases for i
and m where the two algorithms coincide, and in
such cases approz-mpe(i, m} is complete. One case
is when the ordering d used by the algorithm has
induced width less than i. Formally,
Theorem 3. 7

Algorithm appro:z:.mpe(i, n} i! com­
plete for ordered network! having w• (d) � i.

Another interesting case is when m = 1. Algo­
rithm approz-mpe(n, 1) under some minor modifica­
tions and if applied to a poly-tree along some legal
orderings coincides with Pearl's poly-tree algorithm
[11]. A legal ordering of a poly-tree is one in which
observed variables appear last in the ordering and
otherwise, each child node appears before its par­
ents, and all the parents of the same family are con­
secutive.
Algorithm approz-mpe{n, 1} will solve
the mpe task on poly-trees with a legal variable­
ordering in time and space O(ezp(IFI)), where IFI
is the cardinality of the maximum family size. In
other words, it is complete for poly-trees and, like
Pearl's algorithm, it is tractable. Note, however,
that Pearl's algorithm records only unary functions
on a single variable, while ours records intermediate
results whose arity is at most the size of the fam­
ily. To restrict space needs, we modify elim-mpe
and approz-mpe{i, m} as follows. Whenever the al­
gorithm reaches a set of consecutive buckets from the
same family, all such buckets are combined into one
!uper-bucket indexed by a.ll the constituent buckets'
variables. In summary,

in variable X1, namely, to compute P(:z: 1, e) =
L-z=:!;" II�1P(:z:i, e[:z:pa.).
When processing each
bucket, we multiply all the bucket's matrices,
At. ... , >..i> defined over subsets S1, ... , S;, and then
eliminate the bucket's variable by summation [3].
In [4] we presented the mini-bucket approximation
scheme for belief updating. For completeness, we
summarize this scheme next. Let Ql = {Q1, , Q,.}
be a partitioning into mini-buckets of the func­
tions >.1, ... , >.; in Xp 's bucket. Algorithm elim­
bel com.putes )..P: (l index the mini-buckets) )..P =
Lx, II�=l >..i = l:x, III=1 lit, >..1,. Separating the
processing of one mini-bucket (call it first) from
the rest, we get ).P
L:x, (ITt, >.1,) (W==ziTt,>.t.),
and migrating the summation into each mini-bucket
yields, �, = rrr=l Lx, rr,,>..l,. This, however,
amounts to computing an unnecessarily bad upper
bound on P because the product IIt,>..l, for i > 1
is bounded by .L:x, II,,>.,,. Instead of bounding a
function of X by 1ts sum over X, we can bound
by its maximizing function, which yields �' =
.L:x,.((Iltl>.h) rrr=2ma:z:x,.III,AI.;)· Clearly, for ev­
ery partitioning Q, )..P::; yq · In summary, an upper
bound gP of )..P can be obtamed by processing one of
Xp 's mini� buckets by summation, and then process­
ing the rest of Xp's mini-buckets by maximization.
In addition to approximating by an upper bound, we
can approximate by a lower bound by applying the
min operator to each mini-bucket or by computing
a mean-value approximation using the mean-value
operator in each mini-bucket. Algorithm, appro:c­
bel-maz(i, m) , that uses the maximizing elimination
operator is described in [4]. In analogy to the mpe
task, we can conclude that, approz-bel-ma:c(i,m} has
time complexity O(m · ezp(2i)), is complete when,
(1) w"(d) � i, and, (2) when m = 1 and i = n, if
given a poly-tree.
•••

=

·

0

Prop o s ition 3.8 Algorithm approz-mpe{n,I} with

5

4

The bucket-elimination algorithm for computing the
map, elim-map, presented in [3] is a combination
of elim-mpe and elim-bel; some of the variables are
eliminated by summation, others by maximization.
Consequently, its mini-bucket approximation is com­
posed of approz-mpe{i,m} and appro:c-bel-ma:c{i,m).

the 8uper-bucket modification, applied along a legal
ordering, i! complete for poly-tree! and i! identical
to Pearl'! poly-tree algo'l'ithm for mpe. The modi­
fied algorithm'! complexity i! time ezponential in the
family 1ize, but it require! only linear 1pace. D
Approximating belief updating

The algorithm for belief assessment, elim-bel, is iden­
tical to elim-mpe with one change: it uses sum­
mation rather than maximization. Given some
evidence e,
the probl em is to assess the belief

Approximating the map

Given a belief network BN = {Pt, . .. . , P,,J, a. sub­
set of hypothesis variables A :::: {A1, ... , Ar.}, and
some evidence e, the problem is to find an assign­
ment to the hypothesized variable that maximizes

A scheme for approximating probabilistic inference

137

their probability. Formally, we wish to compute

m axP(a�ole) = (max L rr�=lP(:z:;,el:z:pa.))/P(e)
a � ..
4 ..
:l!k+l

when :z: = (a1, ... , a�o, :Z:Jo+l• ... , :Z:n ) · Algorithm elim­
map, the bucket-elimination algorithm for map, as­
sumes only orderings in which the hypothesized vari­
ables appear first. The algorithm has a backward
and a forward phase, but its forward phase is only
The ap­
relative to the hypothesized variables.
plication of the mini-bucket scheme to elim-map is
a straightforward extension of approz-mpe(i,m} and
approz-bel-maz(i,m}. We partition each bucket into
mini-buckets as before. If the bucket's variable is
a summation variable, we apply the rule we have
in appro:c-bel-maz(i,m}, that is, one mini-bucket is
approximated by summation and the rest by maxi­
mization. When the algorithm reaches the buckets
with hypothesized variables, their processing is iden­
tical to that of approz-mpe(i,m). Algorithm approz­
map(i,m} is described in Figure 4.

Theorem 5.1 Algorithm approz-map(i, m} com­
pute"' an upper bound of the map, in time 0( e:cp(m ·
e:cp(2i})) and &pace O(e:cp(m ezp(i))). Algorithm
approx-map(i, n} i.! complete when w * (d) :S i,
and algorithm approx-map(n, 1} u complete for poly­
tree&. D
·

Consider a belief network appropriate for decoding
a multiple turbo-code, that has M code fragments
(see Figure 5, which is taken from Figure 9 in [2]).
In this example, the Ufs are the information bits, the
X;'s are the code fragments, and the Yi's and Y,,'s
are the output of the channel. The task is to assess
the most likely values for the U's given the observed
Y's. Here, the X's are summation variables, while
the U's are maximization variables. After the ob­
servation's buckets are processed, (lower case char­
acters denoted observed variables) we process the
first three buckets by summation and the rest by
maximization using appro:c-map(n, 1}, we get that
all mini-buckets are full buckets due to subsurnp­
tion. The resulting buckets are:
bucket(X1) = P (y1IX1), P (X1I Ut, U:z:, U3, U4)
bucket(X2)
P (y:z:IX2), P (X2I Ut, U2, U3, U4)
f3x1 (Ut, U:z:, U3, U4)
bucket(X3)
P (y3IX2), P (X2IU11 U2, U3),
/3x2(Ull U2, U3, U4)
bucket(Ut) = P (Ut ), P(y,11Ut), f3x• (U11 U2, U3, U4)
bucket(U:z:) = P (U2), P(y,. IU2 ), {3u1 (U2 , U3, U4)
u
bucket(U3) = P (U3), P (y,,IU3), {3 2 (U3, U4)
u
bucket(U4) = P (U4), P(y,�IU4), f3 •(U4) ,
Therefore, approz-map(n, 1} coincides with elim­
map for this network.

Algorithm approx-map(i,m)
Input: A belief network BN

{Pt, ... ,P,.}; a sub­
,A�o}; an ordering of the
=

set of variables

A

=

{At1

•••

variables, d, in which the A's are first in the ordering;

evidence

e.

Output:

An upper bound maximum a posteriori hy­

pothesis, A = a.
1. Initialize: Partition BN into

bucket1, ••• , bucket,.,

where bucket, contains all matrices whose highest vari­
able is X,.

2. Backward: For p <f- n downto 1, do
in bucketp, do
for all the matrices f' t , f:Jl ,
• (bucket with observed variable) if bucketp contains
the observation Xp = zp1 then assign Xp = Xp to each
{3; and put each resulting function into its appropriate
bucket.

... ,{3;

A, for {3111 •••1{3; in bucketp1 do
'
(i, m)-partitiorung q of the matrices fl•
into miru-buckets Q,, ... 1 Qr.
'
(processing first bucket) For Q1 first in q containing
•

else, if Xp is not in

generate an

f3t17 ···� f' t; 1 do

{31

II1=1 {31,.

Add {:J1
=
x
,.
<f­
to the bucket of the largest-index variable in
•

generate function

L:

U1
{Xp}
•
U1=l St,
I
1f3r; 1 do
• For ea� Qr 1 l > 1 in Q contairung f3•1,
{:J1
functions
the
Generate
{Xp}·
Sr,
Ur � U� =l
•••

nf=1{:J,,.

Add
maxx,.
index variable in U,.
•

else, if

{31

to the bucket of the largest­

Xp E A, for f3t,f3l, ...

an
{Qt , ...1Qr}·

1{3j in bucketp1 do

(i, m )-miru-bucket-partitiorung

generate

For each

do
generate function

Q,

{:J11 {:J1

E
=

'
q

contairung

f:J111

maxx,.m=1{:Jr,.

3.

s,,

- {Xp}•

�orward:

'
q
•••

=

,{:Jr.,

Add

to the bucket of the largest-index variable in

U1=

=

U,

Assign values, in the ordering d

{31

�
=

At, ... , A1o using the information recorded in each
bucket.

Figure 4: Algorithm approz-map-maz(i,m)

Figure 5: Belief network for decoding multiple turbo
codes

138
6

Dechter and Rish

Experimental evaluation

diagnosis purposes, we also recorded the maximum
family size in the input network, F;,, and the max­

Our preliminary empirical evaluation is focused on
the trade-off between accuracy and efficiency of the
approximation algorithms for the mpe task. We wish

1.

to understand

the sensitivity of the approxima­

tions to the parameters

i

and m,

2.

The effective­

ness of the approximations on sparse networks vs
dense networks, and on uniform probability tables
vs.

structured ones (e.g., noisy-ORs), and

3,

the

extent to which a practitioner can tailor the approx­
imation level to his own application.

approz­
approz:-mpe{m), as­

We focused on two extreme schemes of

mpe {i, m):

the first one, called

i and varying m, while the sec­
approz:-mpe{i}, assumes unbounded
i.

sumes unbounded
ond one, called
m and varying
Given

the

values

of

i

and

m,

many

(i, m)­

partitionings are feasible, and preferring a particu­
lar one may have a significant impact on the quality
of the result.

Instead of trying to optimize parti­

tioning, we settled on a simple strategy.

We first

created a canonical partitioning in which subsumed
functions are combined into mini-buckets.

appro:rJ-mpe{m)

Then,

combines each m successive mini­

buckets into one mini-bucket, while

approz-mpe{i)

generates an i.-partitioning by processing the canoni­
cal mini-bucket list sequentially, merging the current
mini-bucket with a previous one provided that the
resulting number of variables in the resulting mini­
bucket does not exceed

i.

imum arity of the recorded functions,

F0•

We also

report the maximum number of mini-buckets that
occurred in any bucket during processing

6.1

(mb).

Results

We report on four sets of uniform random networks
(we had experimented with more sets and observed
similar behavior):

30

having

a.

set of

80

nodes and

instances having

60

200

hundred instances

1),

edges (set

nodes and

90

200
2), a

a set of

edges (set

100 instances having 100 nodes and 130 edges
3) and a set of 100 instances having 100 nodes
200 edges (set 4). The first and the forth sets

set of
(set
and

represent dense networks while the second and the
third represent sparse networks. For noisy-OR net­

30
100 edges; set 5 has 90 instances and uses
one evidence, set 6 has 140 instances and uses three
evidence nodes and set 7 has 130 instances and uses
works we experimented with three sets having

nodes and

ten evidence nodes.

6.1.1

Uniform random networks

On the relatively small networks (sets
we applied

elim-mpe and

1

and

2)

compared its performance

with the approximations. The results on these two

1-3.

sets appear in Tables

Table

1 reports averages,
i. Rather than

where the first column depicts m or

displaying the mpe, the lower bound, and the upper
bound (often, these values are very small, of order

The algorithms were evaluated on belief networks

10-6

generated randomly. The random acyclic-graph gen­

accuracy of the approximation. Thus, the second

n,

column displays Ml1, the ratio between the value of

erator, takes as an input the number of nodes,
and the number of edges,
ates

e

e,

and randomly gener­

directed edges, ensuring no cycles, no parallel

and less), we report ratios which capture the

an mpe tuple

(Ma:z:)

(Upper) and Ma:z:; and the fourth col­
time ratio, TR between the CPU
running times for elim-mpe and appro:rJ-mpe(m) or
approx- mpe(i). The next column gives the CPU
time, T,., of appro:rJ-mpe(m) or approz:-mpe(i). Fi­
nally, F,, F0 and mb, are reported.

edges, and no self-loops. Once the graph is available,

upper bound

for each node :z:;, a conditional probability function

umn contains the

P(z•l:z:,.,..) is generated. For uniform random net­
works the tables were created by selecting a random
number between 0 and 1 for each combination of val­
ues of :z:, and :z:,.,.,, and then normalizing. For ran­
dom noisy- OR networks the conditional probability
functions were generated as noisy-OR gates by se­

lecting a random probability q1c for each "inhibitor".
Algorithm

approz-mpe(i,m)

computes

an

upper

and the lower bound (Lower);

the third column shows the U IM ratio between the

Table

2

gives an alternative summary for the same

appro�-mpe{m} only. Three
UIM ratios, vs. the Time Ratio, are

two sets, focusing on
statistics MIL,

reported. For each bound and for each m, we display

200)

bound and a lower bound on the mpe. The latter is

the percent of instances (out of total

provided by the probability of the generated tuple.

the corresponding ratio (MI1 for the lower bound,

For each problem instance, we computed the mpe by

U IM for the upper bound) belongs to the interval

elim-mpe,

[E- 1, E] where the threshold value, E, changes from

the upper bound and the lower bound by

the approximation (either

approx-mpe{m} or approx­

mpe(i}) , and the running time ofthe algorithms.

For

1 to 4.

on which

We also display the corresponding mean T R.

For example, from Table 2's first few lines we see

A scheme for approximating probabilistic inference

that 8.5 % instances out of the 200 were solved by
appro:tJ-mpe(m=1} with accuracy factor of 2 or less,
48% achieved this accuracy with m = 2. The speed­
up over m= 1 instances was 176 while the speed-up
for m= 2 was 20.8.
Table 1: elim-mpe vs. appro:tJ-mpe(i,m) on 200 in­
stances of random networks with 30 nodes, 80 edges,
and with 60 nodes, 90 edges
_Ln•tance•
4ppNe-mpt(m form- 1,�.3

Mean value• on 200

•lim-mp• v•·

m

MJL

1
2
3

43.2
t.O
1.3

1
2
3

9.9
1.a
1.0

MfL

3

55.5
29.2
1'1'.3
15.0

12
3

I
II

12

UJM

I
I

41.2
3.3
1.1
21.'1'
2.a
1.1

elim-m.pe

i

I
II

I

!0

V• •

UfM

I

TR

I

I
I

80 nod••·
2118.1
25.0
1.t
eo nod••·
131.5
27.9
1.3

I
1

,.
T

for

ma>:
mb

max

4
2
1

9

3
2

5
5

1

1

-

I� I ':.�

�

9

5

31 t5 1 II 1:1

�

max
P.
n
n
n

12
n

12
ma>:
P.

0.1
0.1
0.2
0.8

4
3
3
2

9
9
9
9

12
12
12
12

0.1
0.1
0.2
0.5

3
2
2
2

5
5
5
5

12
12

I1 I
I

309.2
254.8
1111.0
U.3
80 node11 90 ed1e11
18.5
138.2
1.1
112.8
8.1
2.8
'11.'1
2.1
1.9
24.2
1.8
1.4

"'•

max
P,
.,
...-aluel per node

T

node11 AO edae•1

41.4
20,'1'
'1'.5
3.0

I
I

ao •da••
0.1
2.2
21.4
90 •dsu
0 .1
0.1
11.9

CpprOit-fi'I,J'8 ,;
TR

I

Yalue• per node

12

12

From these runs we observe a considerable efficiency
gain (2-3 orders of magnitude) relative to elim-mpe
for 50% of the probelm instances for which the ac­
curacy factor obtained was bounded by 4. We also
observe that, as expected, sparser networks require
lower levels of approximations than those required
by dense networks, in order to get similar levels of
accuracy. In particular, the performance of appro:tJ­
mpe(i=a) gave a 1-2 orders of magnitude perfor­
mance speedup while accompanied with an accuracy
factor bounde by 4, to 80 percent of the instances on
dense networks, and to 97 percent of the sparse net­
works. From table 1 we also observe that controlling
the approximation by i provides a better handle on
accuracy vs efficiency tradeoff. Finally, we observe
that approz-mpe(m=l} can be quite bad for arbi­
trary networks.
We experimented next with larger networks (sets
3 and 4), on which running the complete elimina­
tion algorithm was sometimes computationally pro­
hibitive. The results are reported in Tables 4 and
5. Since we did not run the complete algorithm on
those networks, we report the ratio U/L. We see that
the approximation is still effective (a factor of accu­
racy bounded by 10 achieved very effectively) for
sparse networks (set 3). However, on set 4, appro:tJ-

139

Table 2: Summary of the results: M/L, U /M and
TR statistics for the algorithm approz - mpe(m)
with m = 1, 2, 3 on random networks
[� -l,�J

Random network• w1th

��·�l
[2,3]
[3,4]

[i, oO]

��·�l

[2,3]
\3,
r.. ·��
""
l1
· �!
[2, 3]
(3,4}
[t, oO]

!�·

�l

(2, 3]

4., 00�l
r!M

!�· �!

[2, 3]
\3, " l
r4, �
""

!�· �!
[:1, 3]

[3,t]
[4 , oO]

30

1.5!'
9.0%
1.6%
'14%

2
2
2
3
3
3
3

11%
'1'.6%
211.5%
92,..
5%
1%
3%

tU'o

1'1'8.4.
339.11
221.3
313.1
20.1
211.'1'
113.1
25.3
1.4
2.0
1.2
1.1

Random notworko W1th eo
m

26.11:'18%
9Yo

2
2
2
2
3
3
3
3

'1'9.5,..
10%
6.5%
15%
1ooy;
o%
o%
0%

U.IIY,

10

edc••

Upper bound
UfM
Mean TR

0!'
o%
0%
100%
211.5,..
2'1.11%
17%
21%
9'1'7i
3%
1%
0"'
nodoo, 90 •da••

Lower bound
M ...uTR

MfL

1
1
1
1

nodes,

Lower bound
Moan TK
MfL

1
1
1
1

2

1, •J

L•

m

1'1'2.8
84.3
43.5
H7.5
:1 1 . 1
:11.0
-!:1.4
40.5
1.3
1.0
0.0
0.0

o.o
o.o
0.0
291.1
10.11
22.2
22.1
41.0
1.4
4.9
1.3
o.o

Upper bound
M...uTR

UfM
o,.,
oYo
!Yo
119%
41�
31%
14%
U%
100yj
1Yo
oYo
OYo

o.o
o.o
17.4
132.'1'
:11.2
!:I.e
24.t
40.3
1.3
1.0
0.0
0.0

mpe(m} was too expensive to run for m = 3, 4, and
too inaccurate for m = 1, 2. For this difficult class,
an acceptable accuracy was not obtained.
6.1.2

Noisy-OR networks

We experimented with several sets of random noisy­
OR networks and we report on three sets with 30
variables and 100 edges.
The results are summa­
rized in Figure 6 and Table 6. In the first, we display
all instances of set 5 plating the accuracy (M/L and
U /M) vs T R, for all 90 instances. In the second
we display the results on sets 6 and 7 in a manner
similar to Table 2. T.,1 gives the time of elim-mpe.
The results for the noisy-OR networks are much
more impressive than for the uniform random net­
works. The approximation algorithms often get a
correct mpe while still accompanied by 1-2 orders of
magnitue of speed-up (see cases when i = 12 and
i = 15.) Although the mean values of U /M and
M/1 can be large on average due to rare instances
(see Figure 6), in many of the cases both ratios are
close or equal to 1.
In summary, for random uniform and noisy-OR net­
works, 1. we observe that very efficient approxi­
mation algorithms can obtain good accuracy for a
considerable number of instances, 2. appro:tJ-mpe(i)
allows a more gradual control of the accuracy vs. ef-

Dechter and Rish

140

Table

3:

Summary of the results: M/L, U /M and

TR statistics for the algorithm appr ox -mpe ( i) with

i = 3, 6, 9, 12
[•

1 , •l

��
�·
�[2
, 3]
[3 , 4]
[4, oo]
!1• 2!
[2, 3)
(3 , 4)

[!, ..;]

!�·
�!]
[2, 3

�3 , 4�
[4 , 001

!•

1, •J

�
��·
[2 , 3�]
[3 , 4]
r:· . �
�
��·
��
[2, 3]
[3 , 4]
[4 , oo)
��· ��
[2 , 3]
(3, -!J
[4 , oo]

on random networks

Random net....orkl Wlth 30 nodelt eo edge•
Lower bound
I
Upper bound

MfJ..

Mean

111.5 ,,
9%
6 .5%
69%
31!'i
10%
10.5%
41.6%
61 !'!
15%
11%
23%

6
6
6
3

9
9

9
9

n
n

12
6

210.1
266.6
2-18.7
250.1
150.1
1 00.5
114.7
169.8
4 1 .3
41.3
69.2
u.s
•ith 60

Random network•

Lower bound

I

MfJ..

o.s,,
o%
0.&%
99%
2.6'ro
7%
12.6%
78%
29!'i
32%
IT%
22%

UfM.

3,.
15.5%
17.!i%
64%
3S.&!'i
2&%
21"
u.s%
I I ,.

13.&%
&%
0.5 %

3
6
9
12
1&
18
21

1 1 .3
0.0
14.4
256 .11
33.0
1 0 1 .1
132.9
162.1
27.0
60.5
U.4
60.6

T.,

1350t27.6
234561.7
9064,4
2 5 U .9
7]{.1
•o1.a

0.�
0.3
0.&
1.8
10.6
75.3

99.6

mb

max

Pi

"

7
T

3
3
3
3
3

5&0.2

'T

7
7

1
1

2

Accuracy vs efficiency:

�

;:;>
.,
•
..

�

c MIL
• UIM

1000
00

100 '9

0 0

10

500

H. O

ficiency relative to the complete elimination by one

max

MIL and U/M vs TR, I :12

2&.&
71.0
57.2
U2.0
35.9
72.0
96.3
124.5
23.5
29.1
3 7 .3

approx-mpe(m); 3. on random
appro:c-mpe(i} obtains a good ap­
M/L < 1.5) while still improving ef­

100 ln•tance•

UfL

10000 ��------�

mean .....,.

noisy-OR networks

TR

1000

1 500

6: Time Ratio versus M/L and U /M bounds
appro:c-mpe{m) with i = 12 on noisy-OR ne­
towrks with 30 nodes, 100 edges, and one evidence
Figure

for

node

or two orders of magnitude.

7

Mean vlllne• on

'

:R

ficiency tradeoff than
proximation (

on

Upp er bound

91.4
1511.3
U.3
157.2
54.9
88.9
27.4
158.-!
2-!.4
29.7
1 1 .4
21.1

15%
9%
1&.5%
ao,,
1 1 .5%
3%
5.5%
au;:'!
u.s%
0.5%
2.&%

Me•n

edgea

node11 SID

M.ean .... .,.

&1.�:'·

6
6
6
6
9
9
9
9
12
12
12
12

UfM

:R

5: elim-mpe vs. approz- mpe{i) for i = 3 - 2 1
100 instances of random networks with 100 nodes
and 200 edges

Table

z1

=

1

elimination framework, both the parameterized al­

Conclusions and related work

The paper describes a collection of parameterized
algorithms that approximate bucket elimination al­
gorithms.

gorithms and their approximations will apply uni­
formly across many areas. We presented and ana­

Due

to the

generality of the bucket-

lyzed the approximation algorithms in the context
of several probabilistic tasks.

We identified regions

of completeness and provided preliminary empirical
evaluations on randomly generated networks.
Our empirical evaluations have interesting negative

4: elim-mpe vs. appro:c-mpe(i, m) on 100 in­
stances of random networks with 100 nodes and 130
Table

edges

Mean vala••
m

on 100 l�•tan.aea

�mj_

tdim-m.pe v•. e&ppr-'rt:.-mp
UfL

T•

mn
mb

1
2
3
4
i

7111.1
10.t
1.2
1.0

0.1

3.4

13�.5
T..

12

''T&.S
36.3
14.5
1.1

11!

l.T

0.1
0.2
0.3
o.s
3.7
H.ll

I

u

1

1
209.5
elim.-m.pa v•. 4ppPoc-mpe i
UfL

3
6

3
�

3.0

max
mb

3
2
l
2
l
1

max

.!1'.;

5

5

6

5

max
.!1'.;

5
5
5
&
5
5

and positive results.

On the negative side, we see

that when the approximation algorithm coincides
with Pearl's poly-tree propagation algorithm (i.e.,
when we use

approz-mpe(m=l}) ,

it can produce ar­

bitrarily bad results, which contrasts recent suc­
cesses with Pearl's poly-tree algorithm when ap­
plied to examples coming from coding problems

9].

[2;

On the positive side, we see that on many prob­

lem instances the approximations can be quite good.
As theory dictates, we observe substantial improve­
ments in approximation quality as we increase the
parameters

(m

or

i).

This allows the user to an­

alyze in advance, based on memory considerations
and given the problem's graph, what would be the
best

m and i he can effort to use.

In addition, the ac-

A scheme for approxim ating probabilistic inference

Table 6: Summary of the results: M/1, U /M and
TR statistics for the appro:t!-mpe(i) on noisy-OR net­
works with 30 nodes, 100 edges
3
ranae

1
[ 1 , 2]
[2 , 3]
[ 3 , 4]

[,, .,.;]
1
[ 1 , 2]
(2, 3]
(3 , 4 ]

[i, .,.;]
1
[ 1 , 2]
[2 , 3]
[ 3 , i]

[,, .,.;]
1
[ 1 , 2]
[2 , 3]
[3 , 4 ]

[,, .,.;]
ran1e

1

[1, 2]

[ 2 , 3]
[ 3 , i]
[i , .,.;]
1
[1 , 2]
[2 , 3]
( 3 , i]
[, , .,.;]
1
[1 , 2]
[ 2 , 3)
[3, 4]
[4 , oo]
1
[1, 2]
[2 , 3]
[3 , 4)
[4, oo)

I

i

10

I

8
I

I

6
9
9
9
9
9
12
n

12
12
12
1&
1&
15

1&
15

upper and lower bounds approximations can be de­
rived for sigmoid belief networks. Specifically, each
Sigmoid function in a bucket, is approximated by a
Gaussian function.

e.,.tdenee node11 l of O problem 1nd•nce1

M,IL

Lower bound

5
6
6
5
6
9
9
9
9
9
12
12
12
1:1
1:1
1&
1&
1&
1&
1&

141

20.79�
10.0%
&.0%
4.3%
60.0%
46.4"
1&.0%
10.0%
1 .4%
27.1 Yo
70.7Y,
10.7%
4.3%
2.9%
11.4%
16.4,.,
10.0%
1.4%
0.7%
2.1"

rR

&07.9
6&4.2
494.1
730.&
929.1
461.0
&23.2
431.4
411.1
&3&.3
129.0
202.6
12.4
69.1
224.2

22.7
21.0
2&.&
34.1
43.2
33.7
39.0
34.3
27.2
40.0
32.0
&0.1
2 1 .2
17.2
58.7
34.1
4&.0
17.9
13.1
62.5

:n.t

36.0
13.2
9.0

u.r

e·ndence node•,

130

Lower bound
'.1:11.

MfL

28.5!!
17.3%
6.0%
6.8%
43.8%
39.1'Jio
19.&%
9.1%
6.0%
2&.8%
&4.9!!
19.&%
3 .8%
4.&%
17.3%
73.7'Jio
12.1%
3.0%
3.1%
7,&%

423.0
312.1
43!.9
436.0
454.1
311.1
212.8
2U.2
222.7
260.&
87.2
82.1
74.1
1 11 . 1

12.9
18.6
18.3
18.1
21.4
31.11

r,,

UfM

Upper bound

'!

1.4
16.4%
1 7.1 %
10.0%
&&.0%
1.4'!
40.7%
22.1"
1&.0%
20.7%

u.sy,

&6.4%
1 1 .<1%
7.1%
6.4%
40.7%
&2.9%
&.0%
0.7%
1.4%

rR

&21.2
616.1
611.6
421.&
939.0
&10.1
319.4
&12.3
730.&
402.1
11&.5
1&1.1
11&.3
1111.7
149.1
11.7
38.8
28.0
1 1.11
35.6

T,l

24.3
21.2
34.1
11.1
i ].7
40.9
21.\1
n.2
&&.&
30.7
25.1
u. s

31.6

u.s

37.&
22.2
47.1
34.7
17.1
31.5



We consider the problem of diagnosing faults
in a system represented by a Bayesian network,
where diagnosis corresponds to recovering the
most likely state of unobserved nodes given the
outcomes of tests (observed nodes). Finding
an optimal subset of tests in this setting is intractable in general. We show that it is difficult
even to compute the next most-informative test
using greedy test selection, as it involves several
entropy terms whose exact computation is intractable. We propose an approximate approach
that utilizes the loopy belief propagation infrastructure to simultaneously compute approximations of marginal and conditional entropies on
multiple subsets of nodes. We apply our method
to fault diagnosis in computer networks, and
show the algorithm to be very effective on realistic Internet-like topologies. We also provide theoretical justification for the greedy test selection
approach, along with some performance guarantees.

1 Introduction
The problem of fault diagnosis appears in many places under various guises. Examples include medical diagnosis,
computer system troubleshooting, decoding messages sent
through a noisy channel, etc. In recent years, diagnosis
has often been formulated as an inference problem on a
Bayesian network, with the goal of assigning most likely
states to unobserved nodes based on outcome of test nodes.
An important issue in diagnosis is the trade-off between
the cost of performing tests and the achieved accuracy of
diagnosis. It is often too expensive or even impossible to
perform all tests. In this paper, we concentrate on the problem of active diagnosis, in which tests are selected sequentially to minimize the cost of testing. We use entropy as

Alina Beygelzimer
IBM T.J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532
beygel@us.ibm.com

the cost function and select a set of tests providing maximum information, or minimum conditional entropy, about
the unknown variables.
However, exact computation of conditional entropies in a
general Bayesian network can be intractable. While much
existing research has addressed the problem of efficient and
accurate probabilistic inference, other probabilistic quantities, such as conditional entropy and information gain,
have not received nearly as much attention. There is a
vast amount of literature on value-of-information and mostinformative test selection [10, 4, 9, 11], but none of the previous work appears to focus on the computational complexity of most-informative test selection in a general Bayesian
network setting.
We propose an approximation algorithm for computing
marginal conditional entropy. The algorithm is based on
loopy belief propagation, a successful approximate inference method. We illustrate the algorithm at work in the setting of fault diagnosis for distributed computer networks,
and demonstrate promising empirical results. We also apply existing theoretical results on the optimality of certain
greedy algorithms to our test selection problem, and analyze the effect of approximation error on the expected cost
of active diagnosis. Our method is general enough to apply
to other applications of Bayesian networks that require the
computation of information gain and conditional entropies
of subsets of nodes. In our application, it can efficiently
compute the information gain for all candidate tests simultaneously.
The paper is structured as follows. Section 2 introduces
necessary background and definitions. In section 3, we describe the general problem of active diagnosis and the computational complexity issue thereof. We propose a solution
to this problem in section 4. Section 5 discusses an application of our approach in the context of distributed computer system diagnosis, while section 6 presents empirical
results. We survey related work in section 7, and conclude
in section 8.

2 Background and Definitions

process could diverge; convergence is guaranteed only for
polytrees.

Let X = {X1 , X2 , . . . , XN } denote a set of N discrete random variables and x a possible realization of X. A Bayesian
network is a directed acyclic graph (DAG) G with nodes
corresponding to X 1 , X2 , . . . , XN and edges representing
direct dependencies [16]. The dependencies are quantified
by associating each node X i with a local conditional probability distribution P (xi | pai ), where pai is an assignment
to the parents of X i (nodes pointing to X i in the Bayesian
network). The set of nodes {x i , pai } is called a family.
The joint probability distribution function (PDF) over X is
given as product

Let a denote a factor node and i one of its variable nodes.
Let N (a) represent the neighbors of a, i.e., the set of variable nodes connected to that factor. Let N (i) denote the
neighbors of i, i.e., the set of factor nodes to which variable node i belongs. The BP message from node i to factor
a is defined as (see, e.g., [12])

mc→i (xi ),
(3)
ni→a (xi ) :=

P (x) =

N


P (xi | pai ).

(1)

i=1

We use E ⊆ X to denote a possibly empty set of evidence
nodes for which observation is available.
For ease of presentation, we will also use the terminology
of factor graphs [6], which unifies directed and undirected
graphical representations of joint PDFs. A factor graph
is an undirected bipartite graph that contains factor nodes
(usually shown as squares) and variable nodes (shown as
circles). (See Fig. 1 for an example.) There is an edge
between a variable node and a factor node if and only if
the variable participates in the potential function of the corresponding factor. The joint distribution is assumed to be
written in a factored form
P (x) =

1 
fa (xa ),
Z a

(2)

where Z is a normalization constant called the partition
function, and the index a ranges over all factors f a (xa ),
defined on the corresponding subsets X a of X.
The computation complexity of many probabilistic inference problems can be related to graphical properties. Exact
inference algorithms require time and space exponential in
the treewidth [16] of the graph, which is defined to be the
size of the largest clique induced by inference, and can be
as large as the size of the graph. Many common probabilistic inference problems are NP-complete. [1] This includes our problem of probabilistic diagnosis, which can
be formulated as a Maximum A Posteriori (MAP) probability problem: given a set of observations, find the most
likely states of unobserved variables.
Although probabilistic inference can be intractable in general, there exists a simple linear-time approximate inference algorithm known as belief propagation (BP) [ 16]. BP
is provably correct on polytrees (i.e. Bayesian networks
with no undirected cycles), and can be used as an approximation on general networks. In belief propagation, probabilistic messages are iterated between the nodes. The

c∈N (i)\a

and the message from factor a to node i is defined as


fa (xa )
nj→a (xj ).
ma→i (xi ) :=
xa \xi

(4)

j∈N (a)\i

Based on these messages, we can compute the beliefs for
each node and the probability potential for each factor:

ma→i (xi ),
(5)
bi (xi ) ∝
a∈N (i)

ba (xa ) ∝ fa (xa )



ni→a (xi ).

(6)

i∈N (a)

Observations are incorporated into the process via δfunctions as local potentials for the evidence nodes. In
that case, bi (xi ) becomes the approximation of the posterior probability P (x i | e).

3 The Active Test Selection Problem
In many diagnosis problems, the user has an opportunity
to actively select tests in order to improve the accuracy of
diagnosis. For example, in medical diagnosis, doctors face
the experiment design problem of choosing which medical
tests to perform next.
Let S = {S1 , S2 , . . . , SN } denote a set of unobserved
random variables we wish to diagnose, and let T =
{T1 , T2 , . . . , TM } denote the available set of tests. Our
objective is to maximize diagnostic quality while minimizing the cost of testing. The diagnostic quality of a
subset of tests T∗ can be measured by the amount of uncertainty about S that remains after observing T ∗ . From
the information-theoretic perspective, a natural measurement of uncertainty is the conditional entropy H(S | T ∗ ).
Clearly, H(S | T) ≤ H(S | T∗ ) for all T∗ ⊆ T. Thus
the problem is to find T ∗ ⊆ T which minimizes both
H(S | T∗ ) and the cost of testing. When all tests have
equal cost, this is equivalent to minimizing the number of
tests.
This problem is known to be NP-hard [19]. A simple
greedy approximation is to choose the next test to be T ∗ =
arg minT H(S | T, T ), where T is the currently selected

test set. The expected number of tests produced by the
greedy strategy is known to be within a O(log N ) factor
from optimal (see Appendix). The same result holds for
approximations (within a constant multiplicative factor) to
the greedy approach. Furthermore, our empirical results
show that the approach works well in practice.
We make a distinction between off-line test selection and
online test selection. In online selection, previous test outcomes are available when selecting the next test. Off-line
test selection attempts to plan a suite of tests before any
observations have been made. We will focus on the online approach, sometimes called active diagnosis, which is
typically much more efficient in practice than its off-line
counterpart [19].
Active Test Selection Problem: Given the observed outcome t of previously selected sequence of tests T  , select
the next test to be arg minT H(S | T, t ).
In a Bayesian network, the joint entropy H(X) can be decomposed into sum of entropies over the families and thus
can be easily computed using the input potential functions.
Conditional marginal entropies, on the other hand, do not
generally have this property. Under certain independence
conditions they decompose into functions over the families. But computing those functions will require inference.
(See Appendix for proofs.)
Lemma 1. Given a Bayesian network representing a joint
PDF P (X), the joint entropy H(X) can be decomposed
into the sum of entropies over the families: H(X) =
N
i=1 H(Xi | Pai ).
Lemma 2. Given a Bayesian network representing a joint
PDF P (S, T), where ∀i : paTi ⊆ S (i.e. tests Ti and Tj
are independent given a subset of S), the observation t  of
previously selected test set, and a candidate test T , the conditional marginal entropy H(S | T, t  ) can be written as

H(S | T, t ) = −
P (spaT , t | t ) log P (t | spaT )
t,spaT

+



P (t | t ) log P (t | t ) + const, (7)

t

where const is a constant expression.

4 BP for Entropy Approximation
Let us consider the problem of computing the conditional
marginal entropy

P (xa | e) log P (xa | e),
(8)
H(Xa | e) = −
xa



where P (xa | e) =
x\xa P (x | e), x\xa representing
variable nodes not in x a . The trick is to replace the marginal
posterior P (xa | e) with its factorized BP approximation,
and make use of the BP message passing mechanism to
perform the summation over x a . We call this process Belief
Propagation for Entropy Approximation (BPEA).
Pick any node X 0 from Xa and designate it as the root
node. We modify the final message passed to X 0 as follows:

ma→0 (x0 ) := −
(9)
b̃a (xa ) log b̃a (xa ).
xa \x0

Here, b̃a (xa ) is the unnormalized
belief of X a (i.e.,

b̃a (xa ) = σba (xa ), where σ = xa b̃a (xa )).
Plugging in b̃a (xa ) in place of P (x a | e) in Eqn. 8, we
see that it only remains to sum over the root node X 0 and
normalize properly.

h̃(Xa | e) :=
ma→0 (x0 ),
(10)
x0

h(Xa | e)

:=

h̃(Xa | e)
+ log σ.
σ

(11)

It follows immediately that BPEA is exact whenever BP is
exact.
The normalization constant σ is already computed during
normal BP iterations. The computation of b̃a (·), ma→i , and
h̃(·) can all be piggy-backed onto the same BP infrastructure, and therefore does not impact its overall complexity.
Furthermore, due to the local and parallel message update
procedure in BP, we can compute the marginal posterior
entropies of multiple families in one single sweep. This is
an important advantage for the active probing setup.

Minimizing conditional entropy is a particular instance of
value-of-information (VOI) analysis [9], where tests are selected to minimize the expected value of a certain cost function c(s, t, t ). The result of Lemma 2 can be generalized
to this case if the cost function is decomposable over the
families. See Lemma 3 in the Appendix for details.

It is also easy to show that the approach is extendible beyond the entropy computation, to an arbitrary cost function decomposable over families (see Lemma 3 in the Appendix). The cost function replaces the negative logarithm
in Eqns. (8) and (9).

Since observations of test outcome correlate the parent
nodes, the exact computation of all the posterior probabilities in Eqn. (7) is intractable. We can certainly use an existing approximation method to compute P (s paT , t | t ) and
P (t | t ). But a more efficient approach is possible if we
exploit the belief propagation infrastructure.

5 Application: Fault Diagnosis in Computer
Networks
Suppose we wish to monitor a system of networked computers. Let S represent the binary state of N network elements. Si = 0 indicates that the element is in normal

is the cross entropy between the posterior probability of T
and its parents, and the conditional probability of T given
its parents. The second term in Eqn. (7) is simply the negative conditional entropy −H(T | t  ).

···
S1

S1

S3

T1

T1

···

SM

···

TN

Figure 1: Factor graph of the fault diagnostic Bayes net.
operation mode, and S i = 1 indicates that the element is
faulty. We can take S i to be any system component whose
state can be measured using a suite of tests. If the system
is large, it is often impossible to test each individual component directly. A common solution is to test a subset of
components with a single test probe. If all the test components are okay, the test would return a 0. Otherwise the test
would return 1, but it does not reveal which components
are faulty.

We deal with the two entropy terms separately. For H(T |
t ), we may use approximation methods such as BP or GBP
to calculate the belief b(t | t ), which can then be used
to directly compute H(T | t  ). (Note that the summation over values of T is simple since T is binary-valued.)
To calculate A(T, SpaT | t ), we use the entropy approximation method BPEA, as described in Section 4. Because BP message updates are done locally, we can compute A(T, SpaT | t ) for all unobserved T nodes during a
single application of BP. Thus, picking the next probe requires only one run of the BPEA approximation algorithm.
For each candidate probe, we designate the probe node T
itself as the root node. The unnormalized belief has the
form

b̃t (t, spaT ) := P (t | spaT )
nj→t (sj ).
(15)
j∈paT

We assume there are machines designated as probe stations, which are instrumented to send out probes to test the
response of the network elements represented by S. Let T
denote the available set of probes. A probe can be as simple as a ping request, which detects network availability. A
more sophisticated probe might be an e-mail message or a
webpage-access request. In the absence of noise a probe
is a disjunctive test: it fails if an only if there is at least
one failed node on its path. More generally, it is a noisyOR test [16]. The joint PDF of all tests and network nodes
forms the well-known QMR-DT model [13]:

This is used to calculate the modified message m a→t (t)
(cf. Eqn. (9)). However, since A(T, S paT | t ) is a cross
entropy term, we do not take the log of b̃, but rather take
the logarithm of the known probabilities P (t | s paT ). This
simplifies the normalization step described in Eqn. (11)
to A(T, SpaT | t ) = Ã(T, SpaT | t )/σ, where σ =

t,spa (T ) b̃t (t, spaT ).

P (sj ) = (αj )sj (1 − αj )(1−sj ) ,
 s
P (ti = 0 | spai ) = ρi0
ρijj ,

We conduct experiments on network topologies built by
the INET generator [20], which simulates an Internet-like
topology at the Autonomous Systems level. Our dataset includes a set of networks of 485 nodes, where the number
of probe stations varies from 1 to 50.

P (s, t) =


i

(12)
(13)

j∈pai

P (ti | spai )



P (sj ).

(14)

j

Here, αj := P (sj = 1) is the prior fault probability, ρ ij is
the so-called inhibition probability, and (1−ρ i0 ) is the leak
probability of an omitted faulty element. The inhibition
probability is a measurement of the amount of noise in the
network. Fig. 1 shows a factor graph representation of our
model.
As discussed in Section 3, we adopt the active probing framework for fault diagnosis, sequentially selecting
probes to minimize the conditional entropy. Our previous
work [17] makes the single-fault assumption, which effectively reduces S to one random variable with N +1 possible
states. In general, however, multiple faults could exist in
the system simultaneously, which requires the more complicated conditional entropy given in Eqn. ( 7).
Let A(T, SpaT | t ) denote the first term in Eqn. (7). This

6 Empirical Results

The connections between probe nodes and network nodes
are generated with two goals in mind: detection and diagnosis. A detection probe set needs to cover all network
components, so that at least one probe has a positive probability of returning 1 when a component fails. A diagnosis
probe set needs to further distinguish between faulty components. Optimal probe set design is NP-hard for either
detection or diagnosis. For the datasets used here, we first
use a greedy approach to obtain a probe set that covers all
network components, then augment this set with additional
probes in order to guarantee single-fault diagnosis. Interested readers may find detailed discussions of probe set design for diagnostic Bayesian networks in [11, 18].
In our experiments, we measure the effects of prior fault
probability α and inhibition probability ρ on approximation and diagnostic quality. We compare the approximate
entropy values and the quality of the selected probe set

0.02

0

//

//

600

0.01

0.01
0.05
0.1
0.3

100
0

0

//

0.1
0.05

0

0.01

0.05 0.1 0.2 0.4

inhibition prob

//

//

800

400

200

0.15

0

0.05 0.1 0.2 0.4

0.01
0.05
0.1
0.3

0.2

inhibition prob
(c) Test set entropy

500

300

//

0.01

0.05 0.1 0.2 0.4

inhibition prob
(d) Test set size

0.01
0.05
0.1
0.3
0

//

0.01

0.05 0.1 0.2 0.4

inhibition prob

Figure 2: Approximation errors and diagnostic quality for
an augmented detection network. Each curve represents a
different prior fault probability.
against the ground truth, which is obtained via the junction tree exact inference algorithm. In subsection 6.3, we
also summarize how the type of network may effect computational efficiency. Since all measurements depend on the
particular set of probe outcomes, we repeat all experiments
on 10 different samples of the Bayes net.
We use the diagnostic quality of the probe set to determine
when to stop the probe selection process: when the reduction in entropy for the past 5 iterations is no more than
0.00001, the selection process is deemed to converge. Otherwise we continue until all probes have been picked.
6.1 Approximation accuracy
First, we look at approximation accuracy. Recall that at
each time step of the active probing process, we obtain a
vector of approximate entropy values, one for each candidate probe T . We average the relative error between the
approximate values and the exact values for all candidate
probes, and further average over all time steps and samples. Let M denote the total number of probes, n the number of selected probes, h ij the approximate value for probe
j at the ith time step of probe selection, and H ij the corresponding exact value. We compute
R(h, H) :=

200
150
100
50
diag 1 10 20 30 40 50
Network type

1
0.8
0.6
0.4

diag
1
10
20
30
40
50

0.2
0

−9 −7 −5 −3 −1 1 3
# seconds saved

5

Figure 3: Efficiency of approximate method. (a) Average number of BP iterations saved by re-using messages;
(b) CDF of speed-up (in CPU seconds) compared to exact
method.

400

0

250

0

600

200

(b)

300
cumulative distribution

0.04

0

reduction in bit entropy

ave relative abs error

0.06

size of final probe set

ave relative abs error

0.08

(a)

(b) Second term approx errors
0.25

0.01
0.05
0.1
0.3

# iters saved per node

(a) First term approx errors

//

0.1

n−1
M−i
1  1  |hij − Hij |
.
n i=0 M − i j=1
|Hij |

(16)

We conduct this experiment on the detection network with
10 probe stations, augmented with single-node probes.
Fig. 2(a-b) contains plots of the average, the minimum, and
the maximum approximation errors, taken over 10 samples
of probe outcomes. Relative error values are shown separately for the first term, A(T, S paT | t ), and the second
term, H(T | t ). For both terms, the approximation errors

are generally lower at lower α values. The average errors
do not exceed 2%, with the only exception being the BP
error for term two at α = 0.3 and ρ = 0, which reaches up
to 10%. BP approximaton errors of the second term seem
to be generally higher than BPEA approximations of the
first term. At the maximum, the approximation error never
exceeds 10% for term one, and 20% for term two. BP errors for term two does not seem to contain any linear trends
with respect to ρ. However, BPEA’s approximation quality
of term one does seem to become slightly worse at higher
levels of the inhibition probability.
6.2 Diagnostic quality
The quality of diagnosis is taken to be the reduction in conditional bit entropy of the hidden states. If t  represents
the observed outcomes of the final set
of selected probes,

we
measure
H(S)
−
H(S
|
t
)
=
−
s P (s) log2 P (s) +



P
(s
|
t
)
log
P
(s
|
t
).
2
s
Fig. 2(c) compares the diagnostic quality of approximate
and exact algorithms on the augmented detection network
with 10 probe stations. Overall, the reduction in bit entropy
is larger for higher values of α. This is due to the fact that
H(S) is higher when α is larger. The quality of the exact
algorithm is almost identical to that of the approximate algorithm. The two are virtually indistinguishable, except at
α = 0.1 and ρ = 0.3. There is an outlier at this combination. For one of the samples, the value of the entropy
H(S | t ) plateaued unusually early during the active probing process, fooling the algorithm into believing that it had
converged, even though the amount of reduction in entropy
is still very small. Fig. 2(d) shows that the process terminated after selecting only a small set of tests. This outlier is
an artifact of our convergence criterion, not of the approximate algorithm itself.
Fig. 2(d) looks at the size of the final selected probe set
when active probing converges. Here again, the two algorithms have almost identical behavior. The value of α does
not have much impact on the number of selected tests, except when ρ = 0 (i.e., no noise in the tests), in which case

fewer tests are needed for diagnosis at lower levels of α.
These results demonstrate that, while the approximated entropy values may deviate from the truth, the diagnostic
quality of the approximate method is virtually identical to
that obtained using the exact method. Combined with its
speed advantages as described in the next section, these results make a strong case for why the approximate method
is preferable over the exact one.
6.3 Implementation and speed
We use the junction tree inference engine in Kevin Murphy’s Bayes Net Toolbox [15] for Matlab to obtain exact
singleton posterior probabilities. The approximate method
is implemented on top of the belief propagation C++/mex
code developed by Yair Weiss and Talya Meltzer. Additionally, we speed up the approximate active probing process by re-using BP messages at the start of each round of
test selection, thereby maintaining BP’s state from the end
of the selection round. We find that BP converges in substantially fewer iterations this way.
Fig. 3(a) plots the average, maximum, and minimum number of BP iterations that we save by re-using BP messages.
The results are aggregated over 5 samples of the Bayes net.
The x-axis denotes the type of network used. The label
diag represents the diagnosis network with 1 probe station, and the rest are detection networks with various numbers of probe stations. In the detection network with 50
probe stations, we save up to 269 iterations per test node
at the maximum. On average, re-using messages shortens
the BP convergence time by 40-50 iterations per test. If
active probing selected 100 tests, say, then re-using messages would require 4000 to 5000 fewer iterations of belief
propagation.
Fig. 3(b) is a plot of the empirical cumulative distribution
of the speed-up using the approximate method. For all of
the detection networks, the approximate method is at least
1 CPU second faster than the exact method for 75% of the
test nodes. The speed-up is even higher for the diagnostic network, where for 78% of all test nodes the approximate method saves at least 2 CPU seconds per node. This
amounts to substantial savings over the entire active probing process. Also keep in mind that, for networks with large
tree-width, the exact method is not even computationally
feasible. Hence, approximation may be the only realistic
option.

7 Related Work
The problem of most-informative test selection was previously addressed in various areas including diagnosis, decision analysis, and feature selection in machine
learning. Given a cost function, a common decisiontheoretic approach is to compute the expected value-of-

information [10] of a candidate test, i.e., the expected cost
of making a decision after observing the test outcome.
When entropy is used as the cost function, the approach is
called most-informative test selection. In particular, mostinformative test selection was considered in the context of
model-based diagnosis [4] and probabilistic diagnosis [16].
Previous research [9, 8] on VOI analysis has made various simplifying assumptions such as binary hypothesis
and direct observations. An interesting but tangential approach was taken in [9], which proposes to select a set of
tests based on a law-of-large-numbers approximation of the
VOI. Up to now, however, no one seems to have addressed
the efficiency of computing single-test information gain in
a generic Bayesian network.
Most-informative test selection is quite similar to the optimal coding problem [2]. Namely, the hidden state vector
S is the input message, and the test outcomes T the output message from some noisy channel. The goal of mostinformative test selection is to minimize the number of bits
sent through the channel while still accurately decoding the
input message. There is, however, an important difference
between the two. In the coding domain, one may separate
source coding from channel coding. Fault diagnosis, on
the other hand, has to deal with a combination of the two,
represented by the conditional probability P (T i | Spai ).
We may have no control over the source coding function,
but we can still aim to select the smallest, most informative
subset of tests.
In the context of probing, optimal test selection is very similar to the group testing problem [5]. Given a set of Boolean
variables, the objective of group testing is to find all ’failed’
objects by using a sequence of disjunctive tests. Particularly, sequential test selection is known as adaptive group
testing [5]. (There is also a direct connection between
adaptive group testing and Golomb codes [ 7].) Note that
group testing assumes no constraints on the tests (i.e., any
subset of objects can be tested together), while in Bayesian
networks the tests can be only selected from a fixed set.
Even in a less restrictive case of probe selection, we are still
constrained by the network topology. Theoretical analysis
of constrained group testing is difficult.

8 Conclusions
We propose an entropy approximation method based on
loopy belief propagation, and examine its behavior on the
application of active probing for fault diagnosis in a networked computer system. The level of approximation error
varies slightly with the level of noise. But even so, the diagnosis quality is practically identical to that of the exact
method. Furthermore, the approximate method can handle
larger networks than the exact method, and is almost always faster on the smaller ones. This highlights a promising direction for active probing and fault diagnosis, as well

as for entropy approximation on Bayesian networks in general.


