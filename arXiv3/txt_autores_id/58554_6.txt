

It is well known that conditional indepen­
dence can be used to factorize a joint prob­
ability into a multiplication of conditional
probabilities. This paper proposes a con­
structive definition of intercausal indepen­
dence, which can be used to further factorize
a conditional probability. An inference algo­
rithm is developed, which makes use of both
conditional independence and intercausal in­
dependence to reduce inference complexity in
Bayesian networks.
Key words: Bayesian networks, intercausal indepen­
dence (definition, representation, inference)
1

INTRODUCTION

In one interpretation of Bayesian networks, arcs are
viewed as indication of causality; the parents of a ran­
dom variable are considered causes that jointly influ­
ence the variable (Pearl 1988). The concept intercausal
independence refers to situations where the mechanism
by which a cause influences a variable is independent of
the mechanisms by which other causes influence that
variable. The noisy OR-gate and noisy adder models
(Good 1961, Pearl 1988) are examples of intercausal
independence.
Special cases of intercausal independence such as the
OR-gate model have been utilized to reduce the
complexity of knowledge acquisition (Pearl 1988, Hen­
rion 1987) as well as the complexity of inference (Kim
and Pearl 1983). Beckerman (1993) is the first re­
searcher to try to formally define intercausal indepen­
dence. His definition is temporal in nature. Based
on this definition, a graph-theoretic representation of
intercausal independence has been proposed.

noisy

This paper attempts a constructive definition. Our
definition is based on the following intuition about in­
tercausal independence: a number of causes contribute
independently to an effect and the total contribution
is a combination of the individual contributions. The

definition allows us to represent intercausal indepen­
dence by factorization of conditional probability, in a
way similar to that conditional independence can be
represented by factorization of joint probability.
The advantages of our factorization-of-conditional­
probability representation of intercausal independence
over Beckerman's graph-theoretic representation are
twofold. Firstly, the symmetric nature of intercausal
independence is retained in our representation. Sec­
ondly and more importantly, our representation allows
one to make full use of intercausal independence to re­
duce inference complexity.
While Heckerman uses intercausal independencies to
alter the topologies of Bayesian networks, we follow
Pearl (1988) (section 4.3.2) to exploit intercausal in­
dependencies in inference. While Pearl only deals with
the case of singly connected networks, we deal with the
general case.

The rest of this paper is organized as follows. A
constructive definition of intercausal independence is
given in Section 2. Section 3 discusses factorization of
a joint probability into a multiplication of conditional
probabilities, and points out intercausal independence
allows one to further factorize conditional probabili­
ties into "even-smaller" factors. The fact that those
"even-smaller" factors might be combined by opera­
tors other than multiplication leads to the concept of
heterogeneous factorization (HF). After some technical
preparations (Sections 4 and 5), the formal definition
of HF is given in section 6. Section 7 discusses how
to sum out variables from an HF. An algorithm for
computing marginals from an HF is given in Section
8, which is illustrated through an example in Section
9. Related work is discussed in Section 10.
2

CONSTRUCTIVE INTERCAUSAL
INDEPENDENCE

This sections gives a constructive definition of inter­
causal independence. This definition is based on the
following intuition: a number of causes c1, c2, . . . , Cm
contribute independently to an effect e and the total

607

Intercausal Independence and Heterogeneous Factorization

contribution is a combination of the individual contri­
butions.
Let us begin with an example - the noisy OR-gate
model (Good 1961, Pearl 1988, Beckerman 1993). In
this model, there is a random binary variable �i in
correspondence to each c;, which is also binary. �;
depends on c; and is conditionally independent of any
other �i given the c; 's. e is 0 if and only if all the �; 's
are 0, and is 1 otherwise. In formula, e=6 V . . . V �m·
Consider the case when m=2 and consider the condi­
tional probability P(eict, c2). For any value /3; of c;
(i::::1:: , 2), we have

P(e=Oic1 =f3t, c2=fJ2)
= P(�t V�2=0ict:::::/31, c2=/12)
= P(6 :::::Oict =f31)P(6=0ic2=f32),
and

Define ft(e=a:bct=f3t)=deJP(6=a:tict=f3t) and de­
fine !2(e:::::a:2, c2=.B2)=d•J P(6=a:2Jc2=.62). We can
rewrite the above two equations in a more compact
form as follows:
P(e=a:lcl=f3t, c2=.62) =
ft(e=o:t, Ct=f3dh(e=a:2, c2=.B2),

(1)

a1va-,=a

where o:, a:11 and a:2 can be either 0 or
motivates the following definitions.

1.

This example

Let e be a discrete variable and let *• be an
commutative and associative binary operator over
the frame n. - the set of possible values of e. In the previous example, *• is the logic
OR operator V. Let f(e, x1, . .. ,xr,Yt, ... ,y,) and
g( e, Xt, . . . , Xr, Zt, ... , Zt) be two functions, where the
Yi 's are different from the Zj 's. Then, the combination
!®.g off and g is defined as follows: for any value a:
of e,
/(i)eg(e:::::a:, XI, ... , Xr, Yt, . .. , y,, Zt, ..., Zt)
= dej L [f(e=o:t,XJ1 . .. ,Xr,YI, .. . ,y,)

where ®e is the *e-induced combination operator. The
right hand of the equation makes sense because ®e
is commutative and associative. When c1, . . . , em
contribute independently to e, we call e a bastard
variable1• A non-bastard variable is said to be nor­
mal. We also say that f;(e, c;) is (a description of) the
contribution by c; to e.
Intuitively, the base combination operator (e.g. V)
determines how contributions from different sources
are combined, while the induced combination operator
is the reflection of the base operator at the level of
conditional probability.

P(e=llcFf3t, c2=P2)
P(€1 V6=1!c1 =f3t, c2=f32)
P(6=1 let=f3t)P(6=0Jcz=f32)
+P(�t =OJct =f3t)P(6=lic2=f32)
+P(6=1ict=f3t)P(6=llc:�=f3:�).

2:::

Here is our constructive definition of intercausal in­
dependence. We say that c1, ... , Cm contribute in­
dependently to e or e receives contributions indepen­
dently from Ct, ••• , Cm if there exists a commutative
and associative binary operator *• over the frame of
e and real-valued non-negative functions It (e, c1 ), ...,
f m (e, Cm) such that

X

Because of equation (1), the noisy-OR gate model is
an example of constructive intercausal independence,
with the logic OR Vas the base combination operator.
As another example, consider the noisy adder model
(Beckerman 1993). In this model, there is a random
Variable �i in correspondence to each Cj j e; depends
On Cj and is Conditionally independent of any other ej
given the c,'s. The �;'s are combined by the addition
operator "+" to result in e, i.e. e =6 + .. . +em.
To see that e is a bastard variable in this model, let
the base combination operator *e be simply "+" and
let the description of individual contribution /;(e, c;)
be as follows: for any value o: of e and any value f3 of
C; '

/; (e==o:, c;=f3)=def P(e;=aJc;=,B).
Then it is easy to verify

that

equation (3) is

satisfied.

It is interesting to notice the similarity between equa­
tion (3) and the following property of conditional in­
dependence: if a variable x is independent of another
variable z given a third variable y, then there exist
non-negative functions f(x, y) and g(y, z) such that
the joint probability P(x, y, z ) is given by
P(x,y, z)=f(x, y)g(y, z).

(4)

1Those who are familiar with clique tree propagation

We shall refer to *• as the base combination operator
and ®e as the *e-induced combination operator. We
would like to alert the reader that *e combines values
of e, while ®e combines functions of e. It is easy to
see that the induced operator ®e is also commutative
and associative.

may remember that the :first thing to do in constructing

a clique tree from a Bayesian network is to "marry" the

(

parents of each node variable)

1988).

( Lauritzen and Spiegehalter

As implies by the word "bastard", the parents of

a bastard node will not be married. This is because the
conditional probability of a bastard node is factorized into
a bunch of factors, each involving only one parent.

608

Zhang and Poole

In (4) conditional independence allows us to factorize
a joint probability into factors that involve less vari­
ables, while in (3) intercausal independence allows us
to factorize a conditional probability into a bunch of
factors that involve less variables. The only difference
lies in the way the factors are combined.
Conditional independence has been used to reduce in­
ference complexity in Bayesian networks. The rest of
this paper investigates how to use intercausal indepen­
dence for the same purpose.
3

FACTORIZATION OF JOINT
PROBABILITIES

This section discusses factorization of joint probabili­
ties and introduces the concept of heterogeneous fac­
torization (HF).
A fundamental assumption under the theory of proba­
bilistic reasoning is that a joint probability is adequate
for capturing experts' knowledge and beliefs relevant
to a reasoning-under-uncertainty task. Factorization
and Bayesian networks come into play because joint
probability is difficult, if not impossible, to directly
assess, store, and reason with.

Let P(xb x2, ..., xn) be a joint probability over variables x1, x2, ... , Xn. By the chain rule of probabilities,
we have
P(x1, X2,
, Xn)
=P(xi)P(x2lxt) ... P(xnlxl, ..., Xn-1)·
.

•

.

(5)

For any i, there might be a subset 71'; � {x1, ..., :r;_I}
such that X& is conditionally independent of all the
other variables in {x1, ... , X&-d given the variables in
11';, i.e P(x;lxb ..., x;_I)=P(x;l7r;). Equation (5) can
hence be rewritten as
n

P(x1,x2, . . . , Xn)= IT P(x&l7r;).

(6)

&=1

Equation
(6)
factorizes
the
joint proba­
bility P(z 1 , x2, ..., z,. ) into a multiplication of factors
P(xd1Ti)· While the joint probability involves all then
variables, the factors usually involves less than n vari­
ables. This fact implies savings in assessing, storing,
and reasoning with probabilities.
A Bayesian network is constructed from the factoriza­
tion as follows: construct a directed graph with nodes
x1, x2, . . . , :r,. such that there is an arc from :rj to
x; if and only if Xj E 71';, and associate the conditional
probability P(x;l7r;) with the node x;. P(x1,...,Xn) is
said to be the joint probability of the Bayesian network
so constructed. Also nodes in 11'& are called parents of
:r;.

The above factorization is homogeneous in the sense
that all the factors are combined in the same way, i.e
by multiplication.

Figure 1: A Bayesian network, where e1 and e2 re­
ceive contribution independently from their respective
parents.
Let x;1, ... , x;m, be the parents of x;. If x; is a bastard
variable with base combination operator *i, then the
conditional probability P(xd7r;) can be further factor­
ized by

where®; is the *;-induced combination operator. The
fact that ®; might be other than multiplication leads
to the concept of heterogeneous factorization (HF).
The word heterogeneous reflects the fact that differ­
ent factors might be combined in different manners.
As an example, consider the Bayesian network in Fig­
ure 1. The network indicates that P(a, b, c, e1,e2,ea, y)
can be factorized into a multiplication of P(a),
P(b), P(c), P(e1la,b), P(e2la, b, c), P ( eale1 , e2) , and
P(ylea).
Now if the e;'s are bastard variables, then there exist
base combination operators *i (i=l, 2, 3) such that
the conditional probabilities of the e; 's can be further
factorized as follows:
P(e1la, b)
P(e2la,b,c)
P(ealet, e2)

fu(el, a)®dt2(el> b)
/21(e2, a)®2!n(e2, b)®2/2a(e2, c)
!at(ea,el)®a!a2(e3,el)

where fu(et, a), for instance, denotes the contribution
by a to e1, and where the ®i's are the combination
operators respectively induced by the *i's.
The factorization of P(a, b, c, et, e2 , e3, y) into the
factors:
P(a), P(b), P(c), P(ylea), fu(el, a),
(
,
b),
e2, a), /22(e2, b), ha(e2 , c), fat(ea, et),
h1(
!t2 e1
and !a2 (e3 , e2) is called the HF in correspondence to
the Bayesian network in Figure 1. We shall call the
fii 's heterogeneous factors since they might be com­
bined by operators other than multiplication. On the
other hand, we shall say that the factors P(a), P(b),
P(c), and P(yle3) are normal.

Intercausal Independence and Heterogeneous Factorization

609

To prevent I(e1, eD from being mistaken to be the con­
tribution by ei to e1, we shall always make it explicit
that I(e1, e�) is a normal factor, not a heterogeneous
factor.

COMBININ G FACTORS THAT

5

IN V OLVE M OR E THAN ONE
BAS TARD VARIABLE
Even though deputation guarantees that every hetero­

Figure 2: The Bayesian network in Figure
deputation of bastard nodes.

4

1

after the

geneous factor involves only one bastard variable at
the beginning, inference may give rise to factors that
involve more than one bastard variable. In Figure 2,
for instance, summing out the variable a results in a
factor that involves both e1 and e2. This section in­
troduces an operator for combining such factors.
Suppose e1, ... , e�o are bastard variables with base
opera­
combination
tor *t, . . , *k· Let f(et, ...,e,.,xt, ...,xr, Yl, ..., y.)
and g ( et,... ,e�:,xt,····x.
, ,zt, .. . ,zt ) be two func­
tions, where the xi's are normal variables and the yj's

DEPUTATION OF BAS TARD

.

NODES

'

Consider the heterogeneous factor h1( es, e1) from the
previous example. It contains two bastard variables e1
to e3. As we shall see later, it is desirable for every
heterogeneous factor to contain at most one bastard
variable. The concept of deputation is introduced to
guarantee this.

are different from the zr s (they can be bastard as well
as normal variables). Then, the combination f®g of
f and g is defined as follows: for any particular value
a; of e; ,

f®g(el =a:1, .. . ,e�o=O:i:, Xt,.. ., Xr,
Yl,- . . , y,, Zt, . . . , Zt )

Let e be a bastard node in a Bayesian network. The
deputation of e is the following operation: make a copy
e' of e, make the children of e to be children of e',
'
make e a child of e, and set the conditional probability
P(e'le) as follows:

P(e'Ie ) =

{0
1

if e = e'
otherwise

(8)

We shall call e' the deputy of e. We shall also call
P(ele') the deputing function, and rewrite it as I(e, e')
since P(ele') ensures that e and e' be the same.
The Bayesian network in Figure 1 becomes the one in
Figure 2 after the deputation of aU the bastard nodes.
We shall call the latter a a deputation Bayesian net­

work.

Proposition 1 Let N' be a Bayesian network, and let
N' is the Bayesian network obtained from N' by the

deputation of all bastard nodes. Then the joint proba­
bility of N can be obtained from that of N' by summing
out all the deputy variables. 0

In Figure 1, we have the heterogeneous factors
h1(es,el) and f32(es,e2), which involves two bastard
variables. This may cause confusions and is undersir­
able for other reasons, as we shall see soon. After dep­
utation, each heterogeneous factor involves only one
bastard variable. As a matter of fact, fst(es, et) and
fs2(es,e2) have become fst(es,eD and fs2(es,e�).

[f(et =au, . . . , e�o=akl, x1,..., Xr, Yl, , Ys) X
g(e1 =0:121 , ek:::0:1:21 Xt, .. ., Xr1 Zt 1 • • • 1 Zt)]. (9)
.

•

•

.

•

•

A few notes are in order. First, fixing a list of bas­
tard variables and their base combination operators,
one can use the operator® to combined two arbitrary
functions. In the following, we shall always work im­
plicitly with a fixed list of bastard variables, and we
shall refer to ® as the general combination operator.
Second, when

(2).

k

=

1 th is definition reduces to equation

Third, since the base combination operators are com­
mutative and associative, the operator ® is also com­
mutative and associative.
Fourth, when
off and g.

5.1

k

==

0, f®g

is simply the multiplication

Combining all the Heterogeneous Factors
in a Bayesian networks

Equipped with the general combination operator ®,
we now consider combining all the heterogeneous fac­
tors of the Bayesian network in F igure 2. Because of
the third note above, we can combine them in any
order. Let us first combine fu(et, a ) with !t2(e2, b),
!21(e2, a) with h2(e2, b) and hs(e2, c), and fst(es, eD

610

Zhang and Poole

:F

In the following, we shall also say that the
of the function F(X).

We now combine the resulting conditional probabili­
ties. Because of the fourth note, the combination of
P(etia, b), P(e2 la , b, c) , and P(e3lei,e2) is their multi­
plication. So, the combination of all the heterogeneous
factors of the Bayesian network in Figure 2 is simply
the multiplication ofthe conditional probabilities of all
the bastard variables. This is true in general.

Suppose N is a deputation Bayesian network. Sup­
pose :F is the HF that corresponds to N. :F has two
interesting properties.

In a deputation Bayesian network,
the multiplication of the conditional probabilities of all
the bastard variables is the same as the result of com­
bining of all the heterogeneous factors. D

Proposition 2

6.1

is an

HF

with /32(e3, e2). Because of the second note, we have
/H®/t2(et,a,b ) = P(eda,b),
P(e2la,b, c) ,
ht®h2®!23(e2,a,b, c)
P(e31e2,eD.
ht®/32(e3,e2,eD

HF's in Correspondence to Deputation
Bayesian Networks

First, according to Proposition 2 the combination of all
the heterogeneous factors is the multiplication of the
conditional probabilities of all the bastard variables.
Thus, the joint of :F is simply the joint probability of

N.

The joint of the HF that corresponds
to a deputation Bayesian network N is the same as
the joint probability of N.

Proposition 3

Note that in Figure 1, since ht(e3,e1) and h2(e3, e2)
involve two bastard variables, the combination
fn(et,a) ® ... ® f23(e2,c) ® ht(e3,et) ® /a2(e3 ,e2)
would not the same as the multiplication of the condi­
tional probabilities of the bastard variables.

To reveal the second interesting property, let us first
define the concept of tidness. An HF is tidy if for each
bastard variable e, there exists at most one normal
factor that involves e. Moreover, this factor, if exists,
involves only one other variable in addition to e itself.

This is why we need deputation; deputation allows us
to combine the heterogeneous factors by a single com­
bination operator ®, which opens up the possibility of
combining the heterogeneous factors in any order we
choose. This flexibility turns out to be the key to the
method of utilizing intercausal independence we are
proposing in this paper.

An HF that corresponds to a deputation Bayesian net­
work is tidy. For each bastard variable e, I(e, e') is the
only one normal factor that involves e, and this factor
involves only one other variable, namely e'.

6

HETEROGENEOUS
FACTORIZATION

We now formally define the concept of heterogeneous
factorization. Let X be a set of discrete variables. A
heterogeneous factorization (HF) F over X consists of
1. A list e1, .. . , em of variables in X that are said
to be bastard variables. Associated with each bas­
tard variable ei is a base combination operator *i,
which is commutative and associative,
2. A set :Fo of heterogeneous factors, and
3. A set :F1 of normal factors.
We shall write an HF as a quadruplet :F =(X,
{(e1, *t ) , . . . , (em, *m ) } , :Fo, Ft). Variables that are
not bastard are called normal.
In an HF, the combination F0 of all the heterogeneous
factors is given by
(10)
The joint F(X) of an HF is the multiplication of Fa
and all the normal factors. In formula
F=deJ(®Je:F0f)

IT
g€1"1

g.

(11)

Tidy HF's do not have to be in correspondence to a
deputation Bayesian network. As a matter of fact, we
shall start with a tidy HF that corresponds to a dep­
utation Bayesian network, and then sum out variables
from the HF. We shall sum out variables in such a
way such that the tidness is retained. Even though
the HF we start out with corresponds to a deputation
Bayesian network, after summing out some variables,
the resulting tidy HF might no longer correspond to
any deputation Bayesian network.
However, we shall continue to use the terms deputy
variable and deputing function.
7

SUMM ING OUT VARIABLES
FROM TIDY HF'S

Let F(X) be a function. Suppose A is a subset of X.
The projection F(A) of F(X) onto A is obtained from
F(X) by summing out all the variables in X -A. In
formula
(12)
F(A)=d•J I: F(X).
X-A

When F(X)
probability.

is

a joint probability, F(A) is a marginal

Summing variables out directly from F(X) usually re­
quire too many additions. Suppose X contains n vari­
ables and suppose all variables are binary. One needs
to perform 2n - 1 additions to sum out one variable.

Intercausal Independence and Heterogeneous Factorization

A better idea is to sum out variables from an factoriza­
tion of F(X) if there is one. This section investigates
how to sum out variables from tidy HF's. The follow­
ing two lemmas are of fundamental importance, and
they readily follow the definition of the general com­
bination operator @.
Both m'llltiplication and @ are distributive
w. r. t summation.
More specifically, s'llppose f and g
are two functions and variable x appears in f and not
in g . Then

Lemma 1

1.

'Er(fg) = ('Er f)g, and

summing out z does not affect the deputing functions.
Therefore, :F' remains tidy.
When z is a bastard variable, summing out z will not
affect the deputing functions of any other bastard vari­
ables. Therefore, :F' also remains tidy. 0
general, a variable can appear in more than one nor­
mal and heterogeneous factors. The next proposition
reduces the general case to the case where the variable
appear in at most two factors, one normal and one
heterogeneous.

In

F(X), and let z
be a variable in X. Let It , ... , fm be all the heteroge­
neous factors that involve z and let 91, . . , Un be all
the normal factors that involve z. Define

Proposition 5 Let :F be an HF of

2. E., (I®g)= CEr f)® g.

.

0

f=aej 0?;1 /; ,

The following lemma spells out two conditions under
which multiplication and ® are associative with each
other.
Lemma 2
1.

h{f®g}={hf} ®g.

n

g=aeJ IT Ui·
j=l

Let f and g be two functions.

If h is a function that involves no bastard vari­
ables, then

(13)

Let :F' be the HF obtained from :F by removing the fi 's
and the Ui 's, and by adding a new heterogeneous factor
f and a new normal factor g. Then
1. :F' is also an HF of F(X), and f and

g are the
only two factors that involve z. In particular,
when either m=O or n=O, there is only one factor
in :F' that involves z.

2. If h is a function such that all the bastard variables
in

h

f and not in g,
h{f®g } ={hf}®g.

appear only in

then

(14)

0

We now proceed to consider the problem of summing

out variables from a tidy HF in such a way that the tid­
ness is retained. First of all the following proposition
deals with the case when the variable to be summed
out appears in only one factor.
4 Let :F be an HF of F(X) and is tidy.
Suppose z is a variable that appears only in one factor
!(A), normal or heterogeneous. Define h

Proposition

h(A - {z} )=d�J Lf(A).
z

Let :F' be the HF obtained from :F by replacing

f with

h 2• Then, :F' is a HF of F(X- { z } )
the projection
of F(X) onto X-{z} . Moreover if z is not a dep'llty
-

variable, then :F' remains tidy.

Proof:

611

The first part of proposition follows from

Lemma 1.

For the second part, since z is not a deputy variable, it
can be either a non-deputy normal variable or a bas­
tard variable. When z is a non-deputy normal variable,
2The factor h is heterogeneous or normal if and only if
f is.

2.

If z

is not a dep'llty variable, then when :F is tidy,
so is :F'.

Proof: The first part of the proposition follows from

the commutativity and associativity of multiplication
and of the general combination operator @.
For the second part, since z is not a deputy variable, it
can either be a non-deputy normal variable or a bas­
tard variable. When z is a non-deputy normal vari­
ables, the operations performed by the proposition do
not affect the deputing functions. Thus, :F' remains
tidy.
When z is a bastard variable, the deputing functions
are not affect either. Because for each bastard variable
e, its deputing functions is the only normal factor that
involves e. So, :F' also remains tidy. D.
The following proposition merges a normal factor into
a heterogeneous factor.
Proposition 6 Let :F be an HF of F(X) and is tidy.

Suppose z is a variable that appears in only one normal
factor g and only one heterogeneous factor f. Define
h by

h=aeJfg.
Let :F' be the HF obtained from :F by removing g and
f, and by adding a heterogeneous judor h. If z is not

612

Zhang and Poole

a deputy variable, then the joint of :F' is also F(X)
and :F' is tidy. Moreover, h is only one factor in :F'

that involves

z.

Proof: We first consider the case when z is a non­
deputy normal variable. Because the tidness of :F, g
involves no bastard variables. According to Lemmas 2

(1), the joint ofF' is also F.

Since g is not a deputing function, the operation of
combining f and g into one factor does not affect the
deputing functions. Hence, :F' remains tidy.
Let us now consider the case when z is a bastard vari­
able. Since :F is tidy, g must be the deputing function
of z. Since f is the only heterogeneous factor that in­
volves z, all other heterogeneous factors do not involve
z. According Lemma 2 (2), the joint of :F' is also F.
After combining f and g into a heterogeneous factor,
there is no normal factor that involve z. Also, the
deputing functions of the other bastard variables are
not affected. Hence, :F' remains tidy. D.
The

in Bayesian networks. To this end, we need only con­

sider deputing functions I(e, e') such that I(e, e') = 1
if e = e' and I(e, e') = 0 otherwise. Let us say such
deputing functions are identifying. Since for any func­
tion f(e, e', x1, ... , xn ) ,

LI(e, e')f(e, e',

Procedure PROJECTION ( :F, A,
•

8

AN ALGORITHM

This section presents an algorithm for computing pro­
jections of a function F(X) by summing variables from
a tidy HF of F(X). Because of Proposition 3, the al­
gorithm can be used to compute marginal probabili­
ties, and hence posterior probabilities, in Bayesian net­
works.
To sum out the variables in X- A, an ordering needs
to be specified (Lauritzen and Spiegehalter 1988). In
the literature, such an ordering is called an elimi­
nation ordering, which can be found by heuristics
such as the maximum cardinality search (Tarjan and
Yannakakis 1984) or the maximal intersection search
(Zhang 1993).
At the end of the last section, we said that a deputy
variable should be summed out only after the corre­
sponding bastard variable has been summed out. If e
is a bastard variable in A, what should we do with its
deputy variable e'?
The paper is concerned with intercausal independence

p)

Input:
1. :F - A tidy HF of a certain func­
tion F(X) such that all the deputing
functions are identifying,

a tidy HF, bastard variables and non-deputy normal
variables. You may ask: how about deputy variables?
As it turns out, after summing out a bastard variable
e, its deputy e1 becomes a non-deputy normal variable.
So, we can also sum out deputy variables; we just have
to make sure to sum out a deputy variable after the
corresponding bastard variable has been summed out.
variable e' needs to be summed out after the corre­
sponding bastard variable e. As a matter of fact, sum­
ming out e ' before e is the inverse of the deputation of
e. But we have shown at the end the Section 5 that
deputation is necessary.

) = f(e, e, x 1 , .. . , Xn),

we can handle the deputies of bastard variables in A as
follows: wait till after all the other variables outside
A have been summed out and all the heterogeneous
factors have been combined, then simply remove all
the deputing functions, replace each occurrence of a
deputy variable with the corresponding bastard vari­
able. This operation can be viewed as the inverse of
deputation.

above three propositions allow us to sum out, from

It is possible to intuitively understand why a deputy

Xt, ... , Xn

e'

2. A- A subset of X,

3. p - An elimination ordering consist­
ing all the variables other than the
variables A and their deputies. In
p, a deputy variable e� comes right
after the corresponding bastard vari­
able e;.

•

Output:
onto A.

F(A) - The projection of F

1. If p is empty, combine all the het­
erogeneous factors by using the gen­
eral combination operator ®, resulting
in f; remove all the deputing functions
and replace each occurrence of a deputy
variable with the corresponding bastard
variable; multiply f together with all the
normal factors; output the resulting fac­
tion; and exit.
2. Remove the first variable

z

from the or­

dering p.

3. Remove from :F all the heterogeneous
factors ft, .. . , fl� that involve z, and
set

f=dej

®f=l k

Let B be the set of all the variables that
appear in f.
4. Remove from :F, all the normal factors
91 , . .., Om that involve z, and set

m

D= deJ

IT 9j·

j= l

Let C be the set of all the variables that
appear in g.

Intercausal Independence and Heterogeneous Factorization

5. If k=O, define a function

The bastard variable e3 appears in heterogeneous fac­
tors /31 (e3ei) and /a2(e 3, e�), and in the normal factor
I3(e3,e �). After summing out eg the factors become:

h by

h(C-{z})=def Lg(C),
Add h into F

as

613

{T,bt(eL e�,e�), fu(el,a),
Fo
!21(e2,a), /22(e2, b), ha(e2,c)};
F1={P(a), P(b), P(c), P( yie�),
h(e2, e�)},

•

a normal factor,

6. Else if m=O, define a function h by

•

h(B-{z})=deJ L f(B),

/t 2(e1, b),
l1(et,ei),

where
Add h into F as a heterogeneous factor,

7. Else define a function h by

h(BUC-{z} )=de/ L f(B)g(C),
Add h into F as a heterogeneous factor.
Endif
8. Recursively call PROJECTION(F,

A, p)

The correctness of PROJECTION is guaranteed by
Propositions 4, 5, and 6.

summing out a variable re­
quires combinin g only the factors that involve the vari­
able. This is why PROJECTION allows one to ex­

Note that in the algorithm

ploit intercausal independencies for efficiency gains. If
one ignores intercausal independencies, to sum out one
variable one needs combine all the conditional proba­
bilities that involve the variable. There is a gain in effi­
ciency by using PROJECTION because intercausal in­
dependence allows one to further factorize conditional
probabilities into factors that involve less variables. In
Figure 1, for instance, summing out a requires com­
bining P(e tla, b) and P(e2la, b, c) when intercausal in­
dependencies are ignored; there are five variables in­
volved here. By using PROJECTION, one needs to
combine f11 (e 1 , a) and !21(e2,a); there are only three
variables involved in the case.
Finally, we would like to remark that the algorithm
is an extension to a simple algorithm for computing
marginal probabilities from a homogeneous factoriza­
tion (Zhang and Poole 1994).

tPt(e�, e�,e3)=def L(/31 (ea,eD®faz(e3, e� ))I3(e3, e3).
"�

Now e� is the next to sum out. e� appears in the
heterogeneous factor t/J1 and the normal factor P(yle�),
After summing out e�, the factors become:
•

•

where

?f>2(e�, e�, y)=def L if>1 (e� ,e�,e3)P(yle3).
e;

Next, summing out
•

•

Suppose the elimination ordering pis:

e�, c.
•

•

e3, e�, a, b, e1,

Initially, the factors are as follows:

:Fo = {/u(et,a), /t2(e1,b), ht(e2, a ) , /22(e2,b)
h3(e2,c), h1(e3,eD, h2(e3,e�)};
:F1 = {P(a), P(b), P(c), P(yle�), h(el,eD,
l2(e2,e�), l3(ea, e�)}.

tfi2(e�,e�,y),
Fa={ tP3(e1 ,e2),
h2(e2, b), /23(e2,c)};

!t2(e1,b),

F1={P(b), P(c), I1(e11 ei), I2(e2, e�)},

a

Then, summing out

•

work N shown in Figure 2. Since P(e2ly=O) can
be readily obtained from the marginal probability
P(e2,y), we shall show how PROJECTION computes
the latter.

gives us:

a

•

To illustrate PROJECTION, consider computing the
conditional probability P(e2ly=O ) in the Bayesian net­

a

where

An example

9

:Fo=N2(eL e�,y), /u(et,a), !t2(e1,b), !21(e2,a),
!22(e2,b), h3(e2, c)};
:F1= {P(a), P(b), P(c), It(et, ei), I2(e2,e�)},

b gives

us:

Fo={ 'tj.>4(e1, e2),
!23(e2,c)};
Ft= {P(c), lt(et,eD, I2(e2,e�)},

where

if>4(et,e2)

=def
=

L P(b)[/t2(et,b)®f22(e2,b)]
L P(b)!t2(e1,b)h2(e2, b).
b

The next variable on p is e1, which appears in hetero­
geneous factors 'tj.>3 (e1,e2) and 'tj.>4 (e 1, e2) and normal
factor h (e1, ei). After summing out e1 the factors be­
come:

614

Zhang and Poole

together with conditional independencies, to further
reduce inference complexity.

where

t/Js(e2, e�)=deJ

2: It (et, eD[t/Ja(e1, e2)®t/J4(e1, e2)].
"1

Due to space limit, we have to discontinue the example
here. Hopefully, the following two points shoul be be
clear now. F irst, in summig out one variable, PRO­
JECTION combines only the factors that involve the
variable.
Second, since

not have

e1

is a bastard variable, we usually do

Acknowledgement
The authors are grateful for the three anonymous re­
viewers for their valuable comments and suggestions.
Research is supported by NSERC Grant OGP0044121
and by a travel grant from Hong Kong University of
Science and Technology.




There is much interest in providing proba-·
bilistic semantics for defaults but most ap­
proaches seem to suffer from one of two prob­
lems: either they require numbers, a problem
defaults were intended to avoid, or they gen­
erate peculiar side effects.
Rather than provide semantics to defaults,
we address the original problem that defaults
were intended to solve: that of reasoning un­
der uncertainty where numeric probability
distributions are not available. We describe
a non-numeric formalism called an inference
graph based on standard probability theory,
conditional independence and sentences of
confirmation, where a confirms b = coni( a, b)
= p(a!b) > p(a).
The formalism seems to handle the exam­
ples from the nonmonotonic literature. Most
importantly, the sentences of our system can
be verified by performing an appropriate ex­
periment in the semantic domain.
1

should not be thought of as having any mean­
ing in. the sense of "most" or "typical"; they
are statements the user is prepared to accept
as part of an explanation as to why some­
thing may be true.
What, then, does a default mean? Within
the default logic camp, we know of no work
which provides a semantics for defaults, in
the sense that an experiment is described
that can be performed in the semantic do­
main to verify the truth of a default. It is
therefore compelling to view defaults as qual­
itative probabilistic statements where nu­
meric distributions are unavailable. We sur­
vey some of these views but note most re­
quire numbers, something default reasoning
intended to avoid, or have side effects.
Rather than "add semantics" to defaults,
we construct a sound non-numeric proba­
bilistic formalism called an inference graph.
We explore its mathematical properties, then
apply it to the standard examples. We con­
clude with a brief description of the imple­
mentation.

Introduction

Though default r�asoning involves reasoning under conditions of uncertainty, some
argue it is not probabilistic reasoning. Reiter and Crisculo [21] distinguish the two by
suggesting different interpretations for the
word "most" . Probabilistic reasoning gives
"most" a statistical connotation, whereas default logic gives it a prototypical sense. On
the other hand, Poole (18] claims defaults

275

2

What's in a default?

Poole et al [20] attempt to put both default
reasoning and diagnosis under a single urn­
brella by constructing a system containing a
set of facts F known to be true, a set � of
defaults, and g, a set of (possible) observa­
tions which are goals to be proved. Here we
assume F, �and g are propositional.

;

· ..

I
If D

is a subset of

FU D

I= g, and

�

Pearl shows p(flbe) � 0. Then, from

such that

FU D

is consistent.

p(flb) = p(fleb)P(elb) + p(ll-.eb)p(-.elb)

then D is an ezplanation of g. This system is
based on a theorem prover and can be used
in two ways. If g consists of observations
known to be true, we interpret g as querying
t»hy g?, and Dis a diagnosis of g. If g is not
known to be true, then g is interpreted as
querying t»hether g?, and g is a prediction of
FUD. The problem default logic runs into is
that there _is typically another D' such that
FuD' predicts -.g; this is known as the mul­
tiple eztension prob lem and is discussed be­
low. (This is a very abbreviated presentation
of default reasoning; for details on implemen­
tation and application see [15,14).)
AJJ pointed out in the introduction, de­
faults appear to have no semantics, and
many researchers study the relationship be­
tween default reasoning and uncertainty.
Rich [22) advocates adding certainty factors
to possible hypotheses to fine-tune a default
reasoning syst�m and concludes "default rea­
soning is likelihood reasoning and treating it
that way simplifies it" . While some argue
with her treatment, her conclusions seem to
be widely held. Ginsberg [4] pursues this ap­
proach.
At the 1987 Workshop on Uncertainty in
AI, Groaof suggested defaults are interval­
valued probabilities on the entire unit inter­
val. Default inference thus becomes closely
related to Kyburg's theory of interval-valued
probabilities [8,7).
In [9]; McCarthy states non-monotonic
sentences can represent statements of in­
finitesimal probability, but does not go into
detail. Pearl explores this in [12]. This inter­
pretation has some problems. Let e = emu,
b -:- bird, I = fly. If

it follows p( elb) � 0. But since a prior is
always bounded by its conditionals on any
evidence and the negation of that evidence,
we can show p(e) � 0.
Default logic also has this "property":
from no knowledge at all, we can prove -.emu
by cases from fly and -.fly using the contra­
positive forms C?f the defaults. This intro­
duces the following variant of the "lottery
paradox" (8].
Suppose kangaroos (k) are
exceptional because they have a marsupial
birth and platypusses (p) are exceptional be­
cause they lay eggs but dingos (d) have no
such exceptional traits. If
Example 2.1

ozzie-animal => e Vk V p V d,
then p( dl ozzie-animal) is close to one since
the disjunction of the other three is close to
zero. Default reasoning and circumscription
(16] suffer the same problem. Poole [17,3]
solves this by explicitly pruning the proof
tree with a set of sentences called constraints.
However, to do this, you need to know the
right answers in advance.

Besides making subclasses vanish, Pearl's
£-semantics suffers another problem: in gen­
eral, it is impossible to go out into a real
problem domain and find a set of conditional
probabilities infinitely close to one.
Bacchus[1] addresses this issue of practi­
cality and argues for thresholding, that is,
that a possible hypothesis stands for a prob­
ability greater than some threshold k > 1/2.
His system allows only a single defeasible in­
ference, since p(bla) > k and p(clb) > k do
not in general constrain the value of p (cia)
p(flb) � 1, p(fle) � O,p(ble) � 1
to be greater thank.
There seems to be no end to different prob­
(i.e., there are some non-bird emus) then
abilistic
semantics that might be added to
from
defaults or inference rules that might be in­
vented to come up with the right answers for
p(fle)_· p(flbe)p(ble) + p(fl-.be)p(-.ble)
·

276

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

the various examples. We claim it is neces­
sary to ask again what were the original goals
of formalisms such as default reasoning, in­
heritance hierarchies and semantic nets. We
should reconsider the original objections to
standard probability, and ask whether we
solve the problem in a principled way, with­
out tlie invention of new formalisms.

tic formalism based on standard probabil­
ity theory, conditional independence and sen­
rules for

Rather than give

accepting uncertain conclusions, the

inference graph allows us to make inferences
about

3.1

shifts in

contains four kinds of

inference graph

-f+. Links with double
logical links and the others
probabilistic links. Each node is labelled

links, =*, -, :;¢- and

arrows are called
are

with a name or set of names in lower case '
for example,

quaker or pacifist.

Links are at-

tached to a name or its negation at either

An inference graph is a strictly probabilis­

tences of confirmation.

An

Syntax

endpoint.

Inference graphs

3

3.2

belief.

3.3

Semantics

Nodes in an inference graph denote events.
Generally events have two mutually exclu­
sive outcomes, for example fly or

Occa­

--.fly.

sionally an event may have several mutually
exclusive outcomes, (not all of which need be
specified), for example

{hawk, dove}.

Sentences about confirmation1 are repre­

Confirmation

sented by the four kinds of links in an infer­

An interesting mathematical property of log­

ence graph:

sequent increases belief in the antecedent.

a- b

means

a=* b

means 1

a -f+ b

means

ical implication is that knowledge of the con­
That is, a =*
Rosenkrantz

mation,

b implies that p(ajb) ;:::. p(a).
(23) calls this property confir­

and we will see it has many of

the same useful computational properties as

other probabilistic formalisms.
Confirmation describes a

shift in

belief; it

seems to be the weakest probabilistic prop­
erty a default

ought to have.

This provides an

interesting venue to explore: rather than use
knowledge of the form "birds are more likely
to fly than not" , we consider knowledge of
the form "an individual is more likely to fly
once we learn that it is a bird" .

Consider Nutter's example ( 10], where in

springtime it is not true that most birds fly,
since most birds are flightless nestlings. Yet,
the information that an individual is a bird
inclines us to shift belief in favour of flying.
This

also admits an interesting kind of sen­

tence. If we say "Irish Canadians have red
hair" , we do not mean mor� than half or al­
most all Irish Canadians have red hair, even
though the stereotype is widely held.

a:;¢- b means

(Note

=

p(bla) > p(b).

p(-.b!a) > p(-.b).
1

=

p(-.b!a) > p(-.b).

that we insist on strict inequality.

This means that links such as

sky-is-blue
graph.

2+2

=

3

=*

cannot appear on an inference

For the same reason, we also insist

all events are possible. )
The topology of the inference graph carries
information about independence of events.

Definition 3.1 If

p(a!b) = (a), a
unconditionally independent.

Definition 3.2 If

and

p(a!bc) = p(a!b), a
ditionally independent of c, given b .

is

b

ar

e

con­

1Poaaibly confirmation i s too strong a term where
logical implication is not involved.

We use confir­

mation here in the sense of partial confirmation,_ or
relevance.

277

p(bja) > p(b).

I
If a

is a node, and b1, ..., bn are the nodes
directed into a, then a is conditionally inde­
pendent of all the predecessors of the b, given
the outcomes of the bi.
Thus, an inference graph may be seen as a
non-numeric influence diagram[25]. We next
explore the kinds of inferences about confir­
mation that we can make.
4

The confirmation relation

Definition 4..1 If

write conf{a,b).
4.1

p(ajb)

>

p(a), we also

Symmetry

Lemma 4..2 If

Proof:
Rule. 0

coni(a, b), then conj(b, a).

Follows immediately from Bayes'

This allows our system to be reversible; if
we observe sneeze we can confirm has-cold.
Alternately, if we know that someone has a
cold we can predict they will sneeze. Thus
we can use the same. formalism for prediction
and diagnosis.
4.2

Negation

Lemma 4..3 If coni(a, b), then

coni(-.a, -.b).

By definition, p(ajb) > p(a). Negat­
ing both sides yields p(-.ajb) < p(-.a). Then
p(bj-.a) < p(b) by Lemma 4.2 and negating
again yields p(-.bl..:.,a) > p(-.b). Another ap­
plication of Lemma 4.2 yields the desired re­
sult. 0
Thus, not only does bird increase belief in
fly, -.bird increases belief in -.fly. An inter­
esting intermediate result is that the "contra­
positive" form of a link yields a valid infer­
ence, so long as it is made from a single link.
This means use of the contrapositive form of
a link is valid, bu.t the context of such an
inference must be carefully restricted. Infer­
ence graphs also explain why default reason­
ers based on a theorem prover sometime run
Proof:

into difficulties when they apply the contra­
positive: they viola.te independence assump­
tions.
4.3

Logical Inferences

coni(a, c) and conj(b, d)
where c and d are outcomes of the same ran­
dom variable, and a f= b, then conf{c,ab).

Lemma 4..4. If

p(cjab) p(cja) > p(c), since sen­
tences of probability hold for logically equiv­
alent propositions. 0
Default reasoners produce separate argu­
ments for c and d and attempt to choose
among the arguments by appeal to "speci­
ficity" . Poole [19] calls it preferring the most
specific theory and Kirby [6] calls it choosing
the most specific extension.
W hile the default logic view seems to be
to prefer the conclusion based on the most
specific knowledge, we remark that there is
not universal agreement on this in the prob­
abilist community when statistics are not
good. Kyburg [7,8] suggests we make an
inference based on the narrowest reference
class for which we have adequate statistics.
Some Bayesians suggest that data from var­
ious subclasses be combined [2]2•

Proof:

=

I
I
I
I
I
I
I
I
I
I

b f= a, but a � b then

I

Proof:

This generalizes the property of
logical links to the rest of the graph. 0

I

4. 4

I

Lemma 4..S If

conf(b,a).

Transitive inference

Default proofs consist of more than a single
inference; part of the appeal of such reason­
ers is that they appear to create and argu­
ment by making inferences towards a goal. In
general, if a - b and b - c are links on an in­
ference graph, we cannot conclude conj(c, a).
However, if c is conditionally independent of
a given b, it can be shown that conj(c, a). In
2

Thia reference

Cheeseman.

was

pointed out to us by Peter

I
I
I
I

278

I

I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

Proof: p( albc) = p(alc) > p(a), from the
fact, conditional independence gives us much
more than transitivity. Not only can we re­ definition of conditional independence. 0
verse the inference, we can perform transduc­
tion, inferring evidence from other evidence. Lemma 4.9 (Relevance) Suppose conf(a,c)
and conf(b,c} and a is conditionally indepen­
We can also confirm certain conjunctions.
dent of b given c. Then conf(ab,c).
Lemma 4.6 (Probabilistic Resolution} If
there exists c such conf(a,c) and conf(b,c) Proof: conf(a,b) follows from Lemma 4.6,
and a is conditionally independent of b given and p(alb) < p(a, c) from Lemma 4.7. Then
c, then conf(a,b).
p(ablc) = p(alc)p(blc) < p(alb)p(b). 0
Proof: By contradiction. Suppose p(alb) �
p(a). From the premises and an identity of 4.5 Other inferences
probability it follows p(alb) � p(al-.,b). Then,
The following two lemmas address situations
that prove to be useful in Section 5.3. The
p(alb) p(alcb)p(clb) + p(al-.,cb)p(-.,clb).
( I
( I
proofs are straightforward and we omit them.
I
I
I
p a -.,b) p a c..., b)p(c ....,b)+p(a -,c...,b)p(...,c ....,b)
Simplify using the conditional independence LeiDDla 4.10 If coni( ...,a, b), coni(a, c), and
b � c, then coni(a, -,be).
knowledge, then subtract to obtain
=

=

0

•

p(al-.,b)- p(alb)
- (p(alc)- p(al...,c))(p(cl-.,b)- p(clb)).
�

Lemma 4.11 If

r

decessors of g, and

and e are the direct pre­

1. r I= e,
2. a I= e,
3. r is unconditionally independent
of a,
4. conj(g, e),
5. conj(-.g, r) ,

But then both terms must be positive, con­
tradicting the premise that conj(b, c). 0
Unsurprisingly, each such inference results
in a dilution of confirmation. This lemma is
needed for later results.

conf{a,b} and conf{b,c) and
then conf(g, a).
a is conditionally independent of c given b,
then p(alc) < p(alb).
Lemma 4.7 If

5

Proof:

p(alc)

p(albc)p(blc) + p(al-.,bc)p(-.,blc)
= p(alb)p(blc) + p(al-.,b)p(-.,blc)
p(alb)p(blc) + p(alb)p(-.,blc)
<
- p(alb)(p(blc) + p(-.,blc))
p(alb)o

5.1

Examples
Birds fly

=

=

I
I
I
I

The next two lemmas yield two ways of
confirming conjunctions of events.

This inference graph aims to capture a lot
of information. If something is a bird, we
Lemma 4.8 (Irrelevance) If conf(a,c) and believe it is more likely that it will fly, and
a is conditionally independent of b given c, if it flies, it is more likely to be airborn. If
something is an emu, we are less likely to
then conf(a,bc).

279

I
I

believe it flies, but if it is a flemu (flying emu)
we again change our belief. We are inclined

to believe that birds have feathers.

I

W hat

inferences can we make from this graph?

Birds fty,

emus don't

conj(fty, bird)
single

More

links

and

coni{-.fly, emu)

containing

importantly,

this

we

from the

information.

cannot

conf(fly,emu}, i.e., we do not have the
tiple extension"

Emus don't vanish
is true: we show

We

problem.

Lemma 4.4 to conclude

I

We can prove

Our

quaker in­
creases belief in dove and republican increases
belief in hawk. Since the graph contains no

prove
"mul­

can use

that

information· about the joint distribution, we

coni{-.fly, b ird/\ emu).

do not conclude either

hawk or dove if Nixon

is both.

Exactly the opposite

coni{ emu, bird).

system concludes

However,

Note that

ers

we do not accept conclusions, just increase

and

we want to

Republicans

conclude

are

political.

exelusion (S ].

Default reasoners simply add

all the links with the result that
In default reason­

.true given

ers based on theorem provers, there is typi­
cally a single proof of
the default "birds

airbom from emu using
fly". Poole (13] solves this

by accepting only what is true in every ex­

tension. The semantics of extensions are not
very well understood; it is trivial to gener­
ate probability distributions where proposi­
tions true in every extension are less likely
than those that are. not. Thus the meaning
of this conclusion is unclear, We conclude an
individual is less likely to be airborn given it
is an emu because

coni(-.airbom, -.fly)

airbom

coni( emu, -.fly)

and

independent of

emu given fly.

Feathered things fty
we can show

and

is conditionally

conj{feathers, fly) using c =

4.6,
b ird.

dove

or

-.dove[13].

political

is

It is possible

to prove that an object about which nothing
is known is a political non-emu!
We solve the problem in this formalism
by making

hawk

and

dove

mutually exclu­

sive but not necessarily exhaustive outcomes

Below is the historic example of

not

ing to draw an inference. H Dick is both a

show that

-.hawk

is confirmed by

quaker. H
po­
quaker.

we allow this, then we can confirm both

litical and

its negation given

Royal and African Elephants

This appears in

[24,5].

Intuitively, the graph

is suppose to show elephants are typically
gray, but Royal elephants are not. H Clyde is

elephants are gray. If

phant

royal is

true, then

ele­

is true and the conditional indepen­

Quaker and a Republican, we do not want to

dence assumptions shield

conclude he is a hawk or dove.

of

280

I
I
I

We use Lemma 4.11 to conclude African

want­

I

and transitive inference is not being able to

dove, we can make the desired inference using
Lemma 4.6. T he price we pay for consistency

are we to conclude about grayness?

Modified Nixon Diamond

I

I

Since

conditionally independent of

both and African and a Royal elephant what

5.2

I

political is
quaker given

of some random event.

5.3

W ith Lemma

In­

heritance sy stems cannot represent mutual

our belief in them.

Emus are not airborn

Quak­

I

african and

gray from the effect
we conclude conf(-.gray,{royal

I
I
I
I
I
I
I

I
I
I

african]). This would not be true if the links sists of a straightforward transcription of the
from royal and african to elephant were prob­ Lemmas in Section 4 into Prolog and the sys­
tem prints a readable proof of the probabilis­
abilistic.
tic inferences it makes.

I

Acknowledgements

I
I
I
I

Horty et al and Sandewall disagree on this.
We claim there are no "right" answers to
this question and we build different graphs
to model domains with different properties.
5.4.

Naive diagnosis

Consider the diagnostic dual to the "birds
fly" problem.

I
I
I




used by Recommender Systems such as Amazon.com

Collaborative filtering is the process of mak­
ing recommendations regarding the potential
preference of a user, for example shopping on

the Internet, based on the preference ratings

of the user and a number of other users for
various items. This paper considers collabo­
rative filtering bMed on explicit multi-valued
ratings. To evaluate the algorithms, we con­
sider only pure collaborative filtering, using
ratings

exclusively, and no other information

about the people or items.

-a book store on the web, CDNow.com- a CD store
on the web, and MovieFinder .com - a movie site on

the internet [Schafer, Konstan and Riedl, 1999].
Collaborative filtering (CF) is the process of making

predictions whether a user will prefer a particular item,

given his or her ratings on other items and given other
people's ratings on various items including the one in
question. CF relies on the fact that people's prefer­
ences are not randomly distributed; there are patterns
within the preferences of a person and among simi­

lar groups of people, creating correlation. The user
for whom we are predicting a rating is called the ac­

Our approach is to predict a user's prefer­

tive user. In collaborative filtering, the main premise

ences regarding a particular item by using

is that the active user will prefer items which like­

other people who rated that item and other

minded people prefer, or even that dissimilar people

items rated by the user as noisy sensors. The

don't prefer.

noisy sensor model uses Bayes' theorem to

a set of ratings for various user-item pairs, predict a

The problem can be formalized: given

compute the probability distribution for the

rating for a new user-item pair. It is interesting that

user's rating of a new item.

the abstract problem is symmetric between users and

We give two

variant models: in one, we learn a classical
normal linear regression model of how users
rate items; in another, we assume different
users rate items the same, but the accuracy
of the sensors needs to be learned. We com­
pare these variant models with state-of-the­
art techniques and show how they are sig­

nificantly better, whether a user has rated
only two items or many. We report empir­
ical results using the EachMovie database

1

of movie ratings. We also show that by con­
sidering items similarity along with the users
similarity, the accuracy of the prediction in­
creases.

items.
Collaborative filtering has been an active area of re­
search in recent years.

Several collaborative filter­

ing algorithms have

suggested, ranging from bi­

been

nary to non-binary rating, implicit and explicit rating.
Initial collaborative filtering algorithms were based
on statistical methods using correlation between user
preferences [Resnick, Iacovou, Suchak, Bergstrom and
Riedl, 1994; Shardanand and Maes, 1995]. These cor­
relation based algorithms predict the active user rat­
ings as a similarity-weighted sum of the other users
ratings.

These algorithms are also referred to as

memory based algorithms [Breese, Heckerman and
Kadie, 1998]. Collaborative filtering is different to the
standard supervised learning task because there are

1

Introduction

Collaborative filtering is a key technology used to build
Recommender Systems on the Internet. It has been

1http:/ /research.compaq.com/SRC/eachmovie/

only two attributes, each with a large domain; it is
the structure within the domains that are important
to the prediction, but this structure is not provided ex­
plicitly. Recently, some researchers have used machine
learning methods [Breese et al., 1998; Ungar and Fos-

SHARMA & POOLE

UAI2001

ter, 1998) for collaborative filtering algorithms. These
methods essentially discover the hidden attributes for
users and items, which explain the similarity between
users and items.
Breese et al. [Breese et a!., 1998] proposed and eval­
uated two probabilistic models for model based col­
laborative filtering algorithms: cluster models and
Bayesian networks. In the cluster model, users with
similar preferences are clustered together into classes.
The model's parameters, the number of clusters, and
the conditional probability of ratings given a class are
estimated from the training data. In the Bayesian net­
work, nodes correspond to items in the database. The
training data is used to learn the network structure
and the conditional probabilities.
Lawrence and Giles,
collaborative filtering algorithm

Pennock et al.[Pennock, Horvitz,

2000] proposed

a

called personality diagnosis (P D) and showed that
PD makes better predictions than other memory and
model based algorithms. This algor ithm is b ased on a
probabilistic model of how people rate items, which is
similar to our noisy sensor model approach.

In this paper we propose and evaluate a probabilis­
tic approach based on a noisy sensor model, which is
symmetric between users and items. Our approach is
based on the idea that to predict an active user's rating
for a particular item, we can use all those people who
rated that item and other items rated by the active
user as the noisy sensors. We view the noisy sensor
model as a belief network. The conditional probabil­
ity table associated with each sensor reflects the noise
in the sensor.
To model how another user (user u) can act as a noisy
sensor for the active user a's rating, we need to find a
relationship between their preferences. Unfortunately,
there is usually very little data, so we need to make a
priori some assumptions about the relationship. Here
we give two variants of the general idea for learning
the noisy sensor model for explicit multi-valued rat­
ing data: one, where we learn a classical normal lin­
ear regression model of how users rate items ( Noisyl) ;
and another, where we assume that the different users
rate items the same and learn the accuracy of the sen­
sor(Noisy2).
In

order to avoid a perfect fit wi th sparse data we add
some dummy points before fitting the relationship. We
use hierarchical prior to distribute the effect of dummy
points over all possible rating pairs.
After learning the noisy sensor model (i.e. the con­
ditional probability table associated with each sensor
node), we use Bayes' theorem to compute the proba­
bility distribution of the user a's rating of a new item.

489

We evaluate both Noisyl and Noisy2 on the Each­
Movie database of movie ratings and compare them
to the state-of-the-art techniques. We also show that
symmetric collaborative filtering, which employs both
user and item similarity, offers better accuracy than
asymmetric collaborative filtering.
Filtering Problem and

2

Mathematical Notation

Let N be the number of users and M be the total num­
ber of items in the database. S is an N x M matrix of
all user's ratings for all items; Sui is the rating given
by user u to item i. Let the ratings be on a cardi­
nal scale with m values that we denote v1, v2, .. . , Vm·
Then each rating Sui has a domain of possible values
(v1,v2,
, vm ) · In collaborative filtering, S, the user­
item matrix, is ge ner ally very sparse since each user
will only have rated a small percentage of the total
number of items. Under this formulation, the collabo­
rative filtering problem becomes predicting those Sui
which are not defined in S, the user -item matrix.
• . .

Collaborative Filtering Using the

3

Noisy Sensor Model

We propose a simple probabilistic approach for sym­
metric collaborative filtering using the noisy sensor
model for predicting the rating by user a (active user)
of an item j. We use as noisy sensors:
•

all users who rated the item j

•

all items rated by user

a

The sensor model is depicted as a naive Bayesian net­
work in Figure 1. The direction of the arrow shows
that the prediction of user a for item j causes the sen­
sor u to take on a particular prediction for item j. The
sensor model is the conditional probability table asso­
ciated with each sensor node. The noise in the sensor
is reflected by the probability of incorrect prediction;
that is, by the conditional probability table associated
with it. To keep the model simple we use the indepen­
dence assumption that the prediction of any sensor for
item j is independent of others, given the prediction
of user a for item j.
We need the following probabilities for Figure 1:
P r (SuiiSaj) : the probability of user u's prediction
for item j, given the prediction of user a for item j.
Pr

(SakiSaj)

for item

k,

: the probability of user a's prediction
given the prediction of user a for item j.

SHARMA & POOLE

490

UAI2001

to find a straight line which best fits these points. We
assume that the mean of y can be expressed as a lin­
ear function of independent variable x. Since a model
based on an independent variable cannot in general
predict exactly the observed values of y, it is neces ­
sary to introduce the error e;. For the ith observation,
we have the following:
y;

= a +

+ e;

(3x;

We assume that unobserved errors (e;) are indepen­
dent and normally distributed with a mean of zero

variance (]"2•

and the
If Yi is the linear function of e;,
which is normally distributed, then y; is also normally
distributed. We as s ume the same variance for all the
observations. T h e mean and variance of y; are given
thus:
Figure 1: Naive Bayesian Network Semantics for the
Noisy Sensor Model

Pr (Saj) : the prior probabil ity of active

user a's pre­

diction for item j.

We

the prior probability distribution
of user a's rating for item j by the
fraction of rating vi in the training data set, where
ViE (vt,V2, . ..,vm ) ·

E (yi)

(3x;

= a +

var (y,)

==

(]"2

For th e ith observation, the probability distribution
function of y which is normally distributed can be writ­
ten thus:

compute

Pr (Saj

=vi )

p (y
where

=

y;Jx

,Ui

=

==

x;)

a+

==

(3x,

V2�a2 exp [ ;:.� (y;- ,u;)2]

Given the conditional probabilities table for all sen­
sors, we can comp ute the probability distribution for
user a's rating of an unseen item j, using the noisy
sensor model as described in Figure 1. By applying
Bayes' rule we can have the following:

The joint probability distribution function ( or the like­
lihood function denoted by LF (a, (3, a2)) is the pr od­
uct of the individual P (y;lx;) over all observations.

Pr (Saj I (Slj, ... , SNj)

LF (a,j3,a2)

ex:

Pr (Saj)

N

. II

u=l

Pr

1\

(Sal, ... , SaM))
M

(Suj ISaj). II Pr (SakI Saj)

(1)

n
=

IT P(y;Jx;)

=

k=l

To use the noisy sensor model for collaborative fil­
tering we need the probability table for probabilities:
Pr(SuilSa.i) and Pr(Sa.k/Sa.J)·
Consider first the problem of estimating Pr (Sui ISai ) ,
which is the problem of estimating user u's rating for
item j given user a's rating of it. There is typically
sparse data for the m x m probability table and we
need to make some prior assumptions about the rela­
tionship. We assume that there is a linear relation­
ship with Gaussian error between the preferences of
users and, similarly, between the ratings received by
the items. Suppose the rating of user a (the indepen­
dent variable) is denoted by x, and t hat the rating of
user u (the dependent variable) is denoted by y. Sup­
pose that user a and user u co- rated n it ems and their
ratings over n co-rated items are denoted by n pairs of
observations (xl,yl),(x2,Y2), ... ,(xn,Yn)- We want

We apply the maximum likelihood method [Gujarati,
1995] to estimate unknown parameters (a, (3, (]"2). The
likelihood is maximum at the following values of the
parameters:
a==

1/n(LYi- f32:x;)

After calculating the parameters a, (3 and (]"2 we can
write the expression of the probability distribution of
user u's preference for item j given the user a's pref­
erence for it as follows:
Pr

(Suj

= Xuj ISaj

=

Xaj)

=

P

(y

=

Xuj Jx

==

Xa.j)

UAI2001

SHARMA

1

=

� exp
v 27fcr2

To estimate

[-1 (Xuj- (a+ /3Xaj))2]
2

u

Pr (SakiSaj),

We use the prior distribution of rating pairs for dis­
we use the same model as

case the independent variable

denotes the rating re­

x

while dependent variable

. In this

y denotes

And, the n pairs of

the rating received by item k.

{xl,Yl), (x2,Y2), . . . , (xn,Yn) are the rat­
j and k by those n users, who have
co-rated both items j and k.
observations

ings over item

After

computing

Pr (SujiSaj) for
and Pr (SakiSaj)

the

probability

all users

(u)

for all items

(k)

distribution

who rated item

j,

rated by user a, we

can compute the probability distribution of the user
a rating for item j using Equation
we need to compute
and (!2

),

We compute the prior distribution of

each rating pair by its frequency in the training data.

Pr (Suj ISaj)

j,

491

rating pairs.

2

described above for computing
ceived by item

& POOLE

3 * ( u + i)

(1).

In this model

free parameters

(a,

{3,

where u is the number of users who rated

the item j and i is the number of items rated by user

a.

tributing the effect of K dummy points over all rating
pairs like hierarchical

prior.

be distributed over all possible rating pairs. We have
experimented with parameter K, and we found that

Noisyl

performs better with K = 1. For subsequent

experiments we, therefore, chose K = 1 for

3.2

Noisyl.

S e lec t i ng Noisy Sensors

For determining the reliability of the noisy sensors,
we consider the

goodness of fit of

the fitted regression

line to a set of observations. We use the coefficient of
determination

r2

[Gujarati, 1995], a measure that tells

how well the regression line fits the observations. This
coefficient measures the proportion or percentage of
the total variation in the dependent variable explained
by the regression model.

When the linear relationship exceeds the maximum

This, however, reduces

our ability to guarantee the ratings for K items will

r2

is calculated as follows

[Gujarati, 1995}:

value of the rating scale, we use the maximum value;
when it is lower than the minimum value of the rating
scale, we use the minimum value.

=

To predict a rating (for example, to compare it with

1-

"'"'e2
L., �

� (Yi- fJ)2

other algorithms that predict ratings), we predict the

where y is the mean of the ratings of user u.

expected value of the rating.

The value of

The expected value of

the rating is defined as follows:

E (Saj) =

is, e;

(Slj, ... , SNi) I\ (Sal, ... , SaM).

lies b etween 0 and 1 ( 0 ::; r2 ::; 1) .

=

0 for each observation (co-rated item).

r2

=

On

0, it means there is no linear

is horizontal line going through the mean y. We order

find a perfect linear relationship, even though the sen­
sor isn't perfect. If there is a perfect fit between users
a and u, then the variance will be zero according to the
above calculations. Therefore, the sensor u's predic­

tion for item j will be perfect, or deterministic; that is,

the conditional probability table associated with sen­
We do not want

this for our noisy sensor model because a determinis­
tic sensor will discount the effects of other sensors. For
example, often one or two co-rated items always have
a perfect fit, even though such a user is not a good
sensor.

the user and item noisy sensors according to

r2.

We

use the best U user noisy sensors and best I item noisy
sensors for making the predictions.

The parameter

settings for U and I are explained in the next section.

3.3

Variant Noisy2 of Noisyl

The problem with

Noisyl is that

we must often fit lin­

ear relationships with very little data (co-rated items).
It may be better to assume a priori the linear model
and then simply learn the noise. The algorithm

Noisy2

is based on the idea that different users rate items the
same and, similarly, different items receive the same
rating. We assume that the preferences of user a and

We hypothesize that this problem can be overcome if
we add K dummy observations along with n obser­
vations (co-rated items). We assume that user
u

r2

1, there exists a perfect linear relationship

relationship between users a and u and the best fit line

While trying to fit Jines with sparse data, we often

user

=

the other hand, if

K Dummy Observations

sor u will be purely deterministic.

r2

between the preferences of user a and user u; that

�;:1 Pr (Saj =Vi IX)* Vi

where X=

3.1

If

a

and

user u are the same; that is, the expected value of user

u's preference of any item is equal to active user a's
preference for th at item.

give ratings over K dummy items (K > 0) in

E

such a way that their ratings for K dummy items are
distributed over all possible rating pairs. For m scale

rating data there are

m2

possible combination of the

(Yilx = x;)

= 11 =

x;

We learn the variance of user u's prediction. The al­
gorithm

Noisy2 can be

derived from algorithm

Noisyl

492

SHARMA & POOLE

by making the regression coefficients a = 0 and /3 = 1.
In this model we need to compute (u + i) free param­
eters (a2), where u is the number of users who rated
the item j and i is the number of items rated by user
a. We also add the K dummy observations because
the same problem (as discussed in Subsection 3.1) can
arise in Noisy2. We have experimented with parame­
ter K, and we found that Noisy2 also performs better
with K
1. For subsequent experiments we, there­
fore, chose K = 1 for Noisy2 also.
=

In Noisy2 we are not fitting the relationship between
user a and user u, but we assume an equal relation­
ship. So, it doesn't make sense to use the coefficient of
determination r2 for finding the reliability of the noisy
sensors. Rather, to find the reliability of the noisy sen­
sor, we use the variance; the less variance, the more
reliable the noisy sensor. We use the best U user noisy
sensors and best I item noisy sensors for making the
predictions. The parameter settings for U and I are
explained in the next section.

4

Evaluation Framework

To evaluate the accuracy of collaborative filtering al­
gorithms we used the training and test set approach.
In this approach, the dataset of users (and their rat­
ings) is divided into two: a training set and a test set.
The training set is used as the collaborative filtering
dataset. The test set is used to evaluate the accuracy
of the collaborative filtering algorithm. We treat each
user from the test set as the active user. To carry out
testing, we divide the ratings by each test user into two
sets: Ia and Pa. The set Ia contains ratings that we
treat as observed ratings. The set Pa contains the rat­
ings that we attempt to predict using a CF algorithm
and observed ratings (Ia) and training set.
To evaluate the accuracy of the collaborative filtering
algorithm we use the average absolute deviation met­
ric, as it is the most commonly used metric [Breese
et al., 1998]. The lower the average absolute deviation,
the more accurate the collaborative filtering algorithm
is. For all users in the test set we calculate the average
absolute deviation of the predicted rating against the
actual rating of items. Let the number of predicted
ratings in the test set for the active user be na; then
the average absolute deviation for a user is given as
follows:

Sa=

,;, LjEPa IPa,j- ra,jl,

where Pai is user a's observed rating for item j and raj
is user a's predicted rating for item j.
These absolut e deviations are
users in the test set.

then averaged over all

4.1

UAI2001

Data and protocols

We compared both versions of our noisy sensor model
to PD (Personality Diagnosis) [Pennock et al., 2000]
and Correlation (Pearson Correlation) [Resnick et al.,
1994]. To compare the performance we used the same
subset of the EachMovie database as used by Breese et
al. [Breese et al., 1998] and Pennock et al. [Pennock
et al., 2000], consisting of 1,623 movies, 381,862 rat­
ings, 5,000 users in the training set, and 4,119 users
in the test set. In EachMovie database the ratings are
elicited on a integer scale from zero to five. We also
tested the algorithms on other subsets to verify that
we are not finding peculiarities of the subset.
We ran experiments with different amounts of ratings
in set Ia to understand the effect of the amount of
the observed ratings on the prediction accuracy of
the collaborative filt ering algorithm. As in [Breese
et al., 1998] for the AllBut1 Protocol, we put a sin­
gle randomly selected rating for each test user in the
test set Pa and the rest of the ratings in the observed
set I<+. As in [Breese et al., 1998] for each GivenX Pro­
tocol, we place X ratings randomly for each test user
in the observed set Ia, and the rest of the ratings in
the test set Pa. We did the experiments for X = 2, 5
and 10.
4.2

Selecting Noisy Sensors

After learning the noisy sensor model we determine
which noisy sensors should be used in making the pre­
dic tion for the active user. Figure 2 shows the varia­
tion of average absolute deviation with best user noisy
sensors for different numbers of best item noisy sensors
for Noisy1. We used the dataset as described above
but the test rating and the observed ratings for each
user of the test set were selected randomly.
Figute 2 shows that the average absolute deviation de­

creases with the increase in number of item sensors.
There is no significant improvement in accuracy when
the number of item sensors is more than twenty. It
also shows that the average absolute deviation first
decreases with the increase in number of user sensors
and then increases as more user sensors are used for
prediction. This is because the large number of user
sensors results in too much noise for those user sensors
that have good reliability.
From the experiments, we found that both algo­
rithms give better performances with ten-to-twenty
item noisy sensors and forty-to-seventy user noisy sen­
sors 2• For the experiments reported in the following
section, we use the best fifty user noisy sensors and
2We didn't use the test set for finding the number of
user and item noisy sensors.

UAI2001

SHARMA &

125

.

- �

+

10 item nol.ay sen:Kit1
15 item nol:&y sensors

20 ilem noisy sensors
25 ilem no!!l_sensors

1.1 5

1

�

5 '

''
1;\

1

'

•

'

09 5

'
·.li-�lj:O
•

9

0

10

20

30

�lf \1 �y� � 11�� il-ily
40

so

60
70
60
90
user noisy sensors

100

���

i-

�

il

-<11-

130

140

�

�

110

120

493

agnosis match exactly with those reported in Pennock
et al. [Pennock et al., 2000]. We took the results
for Correlation directly from Pennock et al. [Pennock
et al., 2000].

0 Item noisy senson;;
S IWI'n noisy sensors

0

2

POOLE

150

Figure 2: The average absolute deviation as a function
of the number of best user noisy sensors for different
numbers of best item noisy sensors.

Noisyl performed better than PD and Correlation
for AllButl and Givenl0 protocols. For GivenS and
Given2 protocols Noisyl performance is better than
Correlation but not better than PD. We believe that
Noisy1's poor performance can be explained by the
fact the lines that are fitted to very small data sets
are often poor fit to the actual relationship. The algo­
rithm Noisy2, based on an equal relationship between
users, doesn't suffer from the same problem, and out­
performed all algorithms under all protocols.

Table
Average absolute deviation scores on the
EachMovie data for Noisy1, Noisy2, PD and Corre­
lation (note: lower scores are better).

1:

Algorithm

best twenty item noisy sensors (i.e. U
50 and I
20) for both Noisyl and Noisy2. The parameters U
and I depend on the database. In the case of the Each­
Movie database, the number of users are more than the
movies, also each user has rated only few movies. Due
to this possibility more best user noisy sensors (U) are
selected than the b est item noisy sensors (I).
=

=

From Figure 2 we also see that the minimum aver­
age absolute deviation is .936 when we use both user
and item noisy sensors (with sixty user and twenty
item noisy sensors). It is .964 when we use only the
user noisy sensors, shown by the zero item noisy sen­
sors case, and 1.027 when we use only the item noisy
sensors, shown on the y-axis for ten item noisy sen­
sors. This indicates that when we include the item
noisy sensors along with the user noisy sensors, the
quality of the prediction improves considerably. It
also shows that if we use only the item noisy sensors
for prediction, then the average absolute deviation be­
comes greater than when we use only user noisy sen­
sors. Therefore, symmetric collaborative filtering of­
fers better accuracy than asymmetric collaborative fil­
tering.
4.3

Comparison

with Other Methods

We compared the algorithms Noisyl, Noisy2, Corre­
lation and PD using the same training and test set as
Pennock et al. [Pennock et al., 2000]. For each test
user in the test set we use the same set of observed
(Ia) and test ratings (Pa) as Pennock et al.
The results·of comparing Noisyl, Noisy2, Correlation
and PD are shown in Table 1. We re-implemented
Personality Diagnosis. Our results for Personality Di-

Noisy2
Noisyl
PD
Carrel

AllButl

Protocol
Given10 GivenS

.893

.943

.974

1.012

.943
.964
.999

.983
.986
1.069

1.021
1.016
1.145

1.196
1.039
1.296

Given2

Shardanand and Maes [Shardanand and Maes, 1995]
and Pennock et al. [Pennock et al., 2000] proposed
that the accuracy of a collaborative filtering algorithm
should be evaluated on extreme ratings (very high or
very low ratings) for items. The supposition is that,
most of the time, people are interested in suggestions
about items they might like or dislike, but not about
items they are unsure of. Pennock et al. [Pennock
et al., 2000] defined the extreme ratings as those which
are 0.5 above and 0.5 below the average rating in the
subset. To compare the performance of algorithms
with extreme ratings we computed the predicted rat­
ings for those test cases from the test set Pa of all
protocols, whose observed rating is less than R- 0.5
or greater than R + 0.5, where R is the overall average
rating in the subset.
Table 2: Average absolute deviation scores on Each­
Movie data for Noisyl, Noisy2, PD and Correlation
for extreme ratings.
Algorithm
Noisy2
Noisyl
PD
Carrel

AllButl
1.001
.997

1.029
1.108

Protocol
Given10 GivenS
1.057

1.087

1.063
1.087
1.127

1.125
1.128
1.167

Given2
1.124

1.249
1.163
1.189

SHARMA

494

The results for the extreme ratings are shown in Ta­
ble 2. The results for extreme ratings show that
Noisy1 performs better than Noisy2 for AllBut1 proto­
col. It also performs better than PD and Correlation
over Given10 and GivenS protocols. Noisy2 performs
better than the other three algorithms over Given1 0,
GivenS and Given2 protocols.
Table 3: Significance levels of the differences in aver­
age absolute deviation between Noisy1 and PD, and
between Noisy2 and PD, on EachMovie data (note:
low significance levels indicate that the differences in
results are unlikely to be coincidental).
Protocol
AllBut1
AllBut1
(extreme)
Given10
Given10
(extreme)
Given5

Given5
(extreme)
Given2
Given2
(extreme)

vs

Noisy1

PD vs.

Noisy2

PD

vs. PD

Noisy1

vs. PD

Noisy2

.0006

.0003

.9994
.9997

.0001
.0127

.0211

.8623
.9789

.0009

.9064

.0936

.0043

.2897

.7103

.0001

.9999
.9999

.0001

.0019

.0001

.0001

.137 7

.0001

.

.9999
.9873
.9999
.9991
99 57
.9999
.

.9981
.9999

To determine the statistical significance of these re­
sults, we computed the significance levels for the dif­
ferences in average absolute deviation between Noisy1
and PD, PD and Noisy1, Noisy2 and PD, and PD and
Noisy2for all protocols. To do this, we divided the test
set for all protocols into 60 samples of equal size and
used randomization paired sample test of differences of
means [Cohen, 1995]. This method calculates the sam­
pling distribution of the mean difference between two
algorithms by repeatedly shuffling and recalculating
the mean difference in 10,000 different permutations.
The shuffling reverses the sign of the difference score
for each sample with a probability of .5.
The statistical significance results of the EachMovie
data results are shown in Table 3; it shows the prob­
ability of achieving a difference less than or equal to
the mean difference. That is, it shows the probability
of incorrectly rejecting the n ull hypothesis that both
algorithms' deviation scores arise from the same dis­
tribution.
5

Conclusion

In this paper, we have concerned ourselves with sym­
metric collaborative filtering based on explicit ratings

& POOLE

UAI2001

used for making recommendations to a user based on
ratings of various items by a number of people, and
the user's ratings of various items.
We have described a new probabilistic approach for
symmetric collaborative filtering based on a noisy sen­
sor model. We have shown that the noisy sensor model
makes better predictions than other state-of-the-art
techniques. The results for Noisy2 are highly statis­
tically significant. We have also shown that by includ­
ing the items similarity along with users similarity, the
accuracy of prediction increases. This paper has only
considered the accuracy of the noisy sensor model, not
on the computational issues involved. It is beyond the
scope of this paper to consider the trade-off between
off-line and online computation and effective indexing
to find the best matches.
6

Acknowledgments

We thank Compaq Equipment Corporation and David
M. Pennock for providing the EachMovie database
and subsets used in this study. We also thank Holger
Hoos for providing the valuable comments. This work
was supported by Institute for Robotics and Intelli­
gent Systems project BOU and the Natural Sciences
and Engineering R esearch Council of Canada Operat­
ing Grant OGP0044121.


The Bayesian Logic (BLOG) language was recently developed for defining first-order probability models over worlds with unknown numbers of objects. It handles important problems
in AI, including data association and population
estimation. This paper extends BLOG by adopting generative processes over function spaces —
known as nonparametrics in the Bayesian literature. We introduce syntax for reasoning about
arbitrary collections of objects, and their properties, in an intuitive manner. By exploiting
exchangeability, distributions over unknown objects and their attributes are cast as Dirichlet processes, which resolve difficulties in model selection and inference caused by varying numbers of
objects. We demonstrate these concepts with application to citation matching.

1

Introduction

Probabilistic first-order logic has played a prominent role
in recent attempts to develop more expressive models in
artificial intelligence [3, 4, 6, 8, 15, 16, 17]. Among these,
the Bayesian logic (BLOG) approach [11] stands out for
its ability to handle unknown numbers of objects and data
association in a coherent fashion, and it does not assume
unique names and domain closure.
A BLOG model specifies a probability distribution over
possible worlds of a typed, first-order language. That is,
it defines a probabilistic model over objects and their attributes. A model structure corresponds to a possible world,
which is obtained by extending each object type and interpreting each function symbol. Objects can either be “guaranteed”, meaning the extension of a type is fixed, or they
can be generated from a distribution. For example, in the
aircraft tracking domain [11] the times and radar blips are
known, and the number of unknown aircraft may vary in
possible worlds. BLOG as a case study provides a strong
argument for Bayesian hierarchical methodology as a basis
for probabilistic first-order logic.

BLOG specifies a prior over the number of objects. In
many domains, however, it is unreasonable for the user to
suggest such a proper, data-independent prior. An investigation of this issue was the seed that grew into our proposal for Nonparametric Bayesian Logic, or NP-BLOG,
a language which extends the original framework developed in [11]. NP-BLOG is distinguished by its ability to
handle object attributes that are generated by unbounded
sets of objects. It also permits arbitrary collections of attributes drawn from unbounded sets. We extend the BLOG
language by adopting Bayesian nonparametrics, which are
probabilistic models with infinitely many parameters [1].
The statistics community has long stressed the need for
models that avoid commiting to restrictive assumptions regarding the underlying population. Nonparametric models
specify distributions over function spaces — a natural fit
with Bayesian methods, since they can be incorporated as
prior information and then implemented at the inference
level via Bayes’ theorem. In this paper, we recognize that
Bayesian nonparametric methods have an important role to
play in first-order probabilistic inference as well. We start
with a simple example that introduces some concepts necessary to understanding the main points of the paper.
Consider a variation of the problem explored in [11]. You
have just gone to the candy store and have bought a box
of Smarties (or M&Ms), and you would like to discover
how many colours there are (while avoiding the temptation
to eat them!). Even though there is an infinite number of
colours to choose from, the candies are coloured from a finite set. Due to the manufacturing process, Smarties may
be slightly discoloured. You would like to discover the unknown (true) set of colours by randomly picking Smarties
from the box and observing their colours. After a certain
number of draws, you would like to answer questions such
as: How many different colours are in the box? Do two
Smarties have the same colour? What is the probability
that the first candy you select from a new box is a colour
you have never seen before?
The graphical representation of the BLOG model is shown
in Fig. 1a. The number of Smarties of different colours,
n(Smartie), is chosen from a Poisson distribution with

it is unreasonable to expect a domain expert to implement
nonparametrics considering the degree of effort required
to grasp these abstract notions. We show that Bayesian
nonparametrics lead to sophisticated representations that
can be easier to implement than their parametric counterparts. We formulate a language that allows one to specify
nonparametric models in an intuitive manner, while hiding
complicated implementation details from the user. Sec. 3
formalizes our proposed language extension as a set of
rules that map code to a nonparametric generative process.
We emphasize that NP-BLOG is an extension to the BLOG
language, so it retains all the functionality specified in [11].

Figure 1: (a) The BLOG and (b) NP-BLOG graphical models for counting Smarties. The latter implements a Dirichlet
process mixture. The shaded nodes are observations.
mean γSmartie . A colour for each Smartie s is drawn
from the distribution HColourDist . Then, for every draw d,
zSmartieDrawn [d] is drawn uniformly from the set of Smarties {1, . . . , n(Smartie)}. Finally, we sample the observed,
noisy colour of each draw conditioned on zSmartieDrawn [d]
and the true colours of the Smarties.
The NP-BLOG model for the same setting is shown in
Fig. 1b. The true colours of an infinite sequence of Smarties s are sampled from HColourDist . πSmartie is a distribution over the choice of coloured Smarties, and is sampled from a uniform Dirichlet distribution with parameter
αSmartie . Once the Smarties and their colours are generated, the true Smartie for draw d, represented by the indicator zSmartieDrawn [d] = s, is sampled from the distribution
of Smarties πSmartie . The last step is to sample the observed
colour, which remains the same as in the BLOG model.
One advantage of the NP-BLOG model is that it determines
a posterior over the number of Smarties colours without
having to specify a prior over n(Smartie). This is important since this prior is difficult to specify in many domains.
A more significant advantage is that NP-BLOG explicitly
models a distribution over the collection of Smarties. This
is not an improvement in expressiveness — one can always
reverse engineer a parametric model given a target nonparametric model in a specific setting. Rather, nonparametrics
facilitate the resolution of queries on unbounded sets, such
as the colours of Smarties. This plays a key role in making inference tractable in sophisticated models with object
properties that are themselves unbounded collections of objects. This is the case with the citation matching model in
Sec. 3.1, in which publications have collections of authors.
The skeptic might still say, despite these advantages, that

We focus on an important class of nonparametric methods,
the Dirichlet process (DP), because it handles distributions
over unbounded sets of objects as long as the objects themselves are infinitely exchangeable, a notion formalized in
Sec. 3.4. The nonparametric nature of DPs makes them
suitable for solving model selection problems that arise
in the face of identity uncertainty and unknown numbers
of objects. Understanding the Dirichlet process is integral to understanding NP-BLOG, so we devote a section
to it. Sec. 3.5 shows how DPs can characterize collections of objects. Models based on DPs have been shown
to be capable of solving a variety of difficult tasks, such
as topic-document retrieval [2, 21]. Provided the necessary
expert knowledge, our approach can attack these applications, and others. We conduct a citation matching experiment in Sec. 4, demonstrating accurate and efficient probabilistic inference in a real-world problem.

2

Dirichlet processes

A Dirichlet process G | α, H ∼ DP (α, H), with parameter
α and base measure H, is the unique probability measure
defined G on the space of all probability measures (Φ, B),
where Φ is the sample space, satisfying
(G(β1 ), ..., G(βK)) ∼ Dirichlet(αH(β1 ), ..., αH(βK)) (1)
for every measurable partition β1 , . . . , βK of Φ. The base
measure H defines the expectation of each partition, and α
is a precision parameter. One can consider the DP as a generalization of the Dirichlet distribution to infinite spaces.
In Sec. 3.4, we formalize exchangeability of unknown objects. In order to explain the connection between exchangeability and the DP, it is instructive to construct DPs with
the Pólya urn scheme [5]. Consider an urn with balls of
K possible colours, in which the probability of the first
ball being colour k is given by Hk . We draw a ball
from the urn, observe its colour φ1 , then return it to the
urn along with another ball of the same colour. We then
make another draw, observing its colour with probability
p(φ2 = k|φ1 ) = (αHk +δ(α1 = k))/(α+1). After N observations, the colour of the next ball is distributed as
αHk
+
P (φN +1 = k|φ1:N ) =
α+N

PN

δ(φi = k)
.
α+N

i=1

(2)

The marginal P (φ1:N ) of this process, obtained by applying the chain rule to successive predictive distributions, can
be shown to satisfy the infinite mixture representation
!
Z
K
PN
Y
δ(φ
=k)
i
P (φ1:N ) =
DPα,H (dπ), (3)
πk i=1
M(Φ) k=1

where the πk are multinomial success rates of each colour
k. This result, a manifestation of de Finetti’s theorem, establishes the existence and uniqueness of the DP prior for
the Pólya urn scheme [5]. In the Pólya urn setting, observations φi are infinitely exchangeable and independently
distributed given the measure G. Thus, what we have established here in a somewhat cursory fashion is the appropriateness of the DP for the case when the observations φi
are infinitely exchangeable.
Analogously, if the urn allows for infinitely many colours,
then for any measurable interval β of Φ we have
p(φN +1 ∈ β|φ1:N ) =

N
1 X
αH(β)
+
δ(φi ∈ β).
α+N
α + N i=1

The first term in this expansion corresponds to prior knowledge and the second term corresponds to the empirical distribution. Larger values of α indicate more confidence
in the prior H. Note that, as N increases, most of the
colours will be repeated. Asymptotically, one ends up sampling colours from a possibly large but finite set of colours,
achieving a clustering effect. Nonetheless, there is always
some probability of generating a new cluster.
DPs are essential building blocks in our formulation of nonparametric first-order logic. In the literature, these blocks
are used to construct more flexible models, such as DP mixtures and hierarchical or nested DPs [2, 21]. Since observations are provably discrete, DP mixtures add an additional
layer xi ∼ P (xi |φi ) in order to model continuous draws xi
from discrete mixture components φi .
In the Pólya urn scheme, G is integrated out and the φi ’s are
sampled directly from H. Most algorithms for sampling
DPs are based on this scheme [2, 13, 21]. In the hierarchies
constructed by our language, however, we rely on an explicit representation of the measure G since it is not clear
we can always integrate it out, even when the measures
are conjugate. This compels us to use the stick-breaking
construction [19], which establishes that i.i.d. sequences
wk ∼ Beta(1, α) and φk ∼ H can be used
P∞to construct
the equivalent empirical distribution G = k=1 πk δ(φk ),
Qk−1
where the stick-breaking weights πk = wk j=1 (1 − wj )
and can be shown to sum to unity. We abbreviate the sampling of the weights as πk ∼ Stick(α). This shows that G
is an infinite sum of discrete values. The DP mixture due
to the stick-breaking construction is
φi | H ∼ H
zi | π ∼ π

π | α ∼ Stick(α)
xi | φi , zi ∼ p(xi |φzi ),

(4)

where zi = k indicates that sample xi belongs to component k. The Smarties model (Fig. 1b) is in fact an example
of a DP mixture, where the unbounded set of colours is Φ.
By grounding on the support of the observations, the true
number of colours K is finite. At the same time, the DP
mixture is open about seeing new colours as new Smarties
are drawn. In NP-BLOG, the unknown objects are the mixture components.
NP-BLOG semantics (Sec. 3) define arbitrary hierarchies
of Dirichlet process mixtures. By the stick-breaking construction (4), every random variable xi has a countable set
of ancestors (the unknown objects), hence DP mixtures preserve the well-definedness of BLOG models.
To infer the hidden variables of our models, we employ
the efficient blocked Gibbs sampling algorithm developed
in [7] as one of the steps in the overall Gibbs sampler. One
complication in inference stems from the fact that a product
of Dirichlets is difficult to simulate. Teh et al. [21] provide
a solution using an auxiliary variable sampling scheme.

3

Syntax and semantics

This section formalizes the NP-BLOG language by specifying a procedure that takes a set of statements LΨ in the
language and returns a model Ψ. A model comprises a set
of types, function symbols, and a distribution over possible
worlds ω ∈ ΩΨ . We underline that our language retains all
the functionality of BLOG. Unknown objects must be infinitely exchangeable, but this trivially the case in BLOG.
Sec. 3.4 elaborates on this.
We illustrate the concepts introduced in this section with
an application to citation matching. Even though our citation matching model doesn’t touch upon all the interesting
aspects of NP-BLOG, the reader will hopefully find it instrumental in understanding the semantics.
3.1 Citation matching
One of the main challenges in developing an automated citation matching system is the resolution of identity uncertainty: for each citation, we would like to recover its true
title and authors. For instance, the following citations from
the CiteSeer database probably refer to the same paper:
Kozierok, Robin, and Maes, Pattie, A Learning Interface Agent
for Meeting Scheduling, Proceedings of the 1993 International
Workshop on Intelligent user Interfaces, ACM Press, NY.
R. Kozierok and P. Maes. A learning interface agent for
scheduling meetings. In W. D. Gray, W. E. Heey, and D. Murray, editors, Proc. of the Internation al Workshop on Intelligent
User Interfaces, Orlando FL, New York, 1993. ACM Press.

Even after assuming the title and author strings have been
segmented into separate fields (an open research problem
itself!), citation matching still exhibits serious challenges:
two different strings may refer to the same author (e.g.
J.F.G. de Freitas and Nando de Freitas) and, conversely,
the same string may refer to different authors (e.g. David
Lowe in vision and David Lowe in quantum field theory).

01 type Author; type Pub; type Citation;
02 guaranteed Citation;
03 #Author ∼ NumAuthorsDist();
04 #Pub ∼ NumPubsDist();
05 Name(a) ∼ NameDist();
06 Title(p) ∼ TitleDist();
07 NumAuthors(p) ∼ NumAuthorsDist();
08 RefAuthor(p, i) if Less(i, NumAuthors(p))
then ∼ Uniform(Author a);
09 RefPub(c) ∼ Uniform(Pub p);
10 CitedTitle(c) ∼ TitleStrDist(Title(RefPub(c)));
11 CitedName(c, i) if Less(i, NumAuthors(RefPub(c)))
then ∼ NameStrDist(Name(RefAuthor(RefPub(c), i)));

Figure 2: BLOG model for citation matching [10].
01 type Author; type Pub;
02 type Citation; type AuthorMention;
03 guaranteed Citation; guaranteed AuthorMention;
04 Name(a) ∼ NameDist{};
05 Title(p) ∼ TitleDist{};
06 CitedTitle(c) ∼ TitleStrDist{Title(RefPub(c))};
07 RefAuthor(u) ∼ PubAuthorsDist(RefPub(CitedIn(u)));
08 CitedName(u) ∼ NameStrDist{Name(RefAuthor(u))};

Figure 3: NP-BLOG model for citation matching.
There are a number of different approaches to this problem.
Pasula et al. incorporate unknown objects and identity uncertainty into a probabilistic relational model [14]. Wellner
et al. resolve identity uncertainty by computing the optimal graph partition in a conditional random field [22]. We
elaborate on the BLOG model presented in [10] in order to
contrast it with the one we propose. The BLOG model is
shown in Fig. 2 with cosmetic modifications and the function declaration statements omitted.
The BLOG model describes a generative sampling process.
Line 1 declares the object types, and line 2 declares that
the citations are guaranteed (hence are not generated by a
number statement). Lines 3 and 4 are number statements,
and lines 5-11 are dependency statements; their combination defines a generative process. The process starts by
choosing a certain number of authors and publications from
their respective prior distributions. Then it samples author names, publication titles and the number of authors
per publication. For each author string i in a citation, we
choose the referring author from the set of authors. Finally,
the properties of the citation objects are chosen. For example, generating an interpretation of CitedTitle(c) for citation
c requires values for RefPub(c) and estimates of publication
titles. TitleStrDist(s) can be interpreted as a measure that
adds noise in the form of perturbations to input string s.
The NP-BLOG model in Fig. 3 follows a similar generative
approach, the key differences being that it samples collections of unknown objects from DPs, and it allows for uncertainty in the order of authors in publications. But what
do we gain by implementing nonparametrics? The advan-

Figure 4: Three representations of lines 5-6 in Fig. 3: as
an NP-BLOG program, as a generative process, and as a
graphical model. Darker, hatched nodes are fixed or generated from other lines and shaded nodes are observed. Note
the similarity between the graphical model and Fig. 1b.
Lines 5-6 describe a DP mixture (4) over the publications
p, where the base measure is φTitleDist , πTitle is the hidden
distribution over publication objects, the indicators are the
true publications zRefPub [c] corresponding to the citations c,
and the continuous observations are the titles φCitedTitle [c].
tage lies in the ability to capture sophisticated models of
unbounded sets of objects in a high-level fashion, and the
relative ease of conducting inference, since nonparametrics
can deal gracefully with the problem of model selection.
One can view a model such as the automatic citation
matcher from three perspectives: it is a set of statements
in the language that comprise a program; from a statistician’s point of view, the model is a process that samples
the defined random variables; and from the perspective of
machine learning, it is a graphical model. Fig. 3 interprets
lines 5-6 of Fig. 4 in three different ways. The semantics,
as we will see, formally unify all three perspectives.
Both BLOG and NP-BLOG can answer the following
queries: Is the referring publication of citation c the same
as the referring publication of citation d? How many authors are there in the given citation database? What are
the names of the authors of the publication referenced by
citation c? How many publications contain the author a,
where a is one of the authors in the publication referenced
by citation c? And what are the titles of those publications?
However, only NP-BLOG can easily answer the following
query: what group of researchers do we expect to be authors in a future, unseen publication?
3.2 Objects and function symbols
This section is largely devoted to defining notation so that
we can properly elaborate on NP-BLOG semantics in Sections 3.3 to 3.5. The notation as it appears in these sections
makes the connection with both first-order logic and the
Dirichlet process mixture presented in Sec. 2.

The set of objects of a type τ is called the extension of
τ , and is denoted by [τ ]. In BLOG, extensions associated
with unknown (non-guaranteed) types can vary over possible worlds ω, so we sometimes write [τ ]ω . The size of [τ ]ω
is denoted by nω (τ ).1 Note that objects may be unknown
even if there is a fixed number of them. Guaranteed objects
are present in all possible worlds. We denote ΩΨ to be the
set of possible worlds for model Ψ.
A model introduces a set of function symbols indexed by
the objects. For conciseness, we treat predicates as Boolean
functions and constants as zero-ary functions. For example, the citation matching model (Fig. 3) has the function
symbols Name and CitedTitle, among others, so there is a
Name(a) for every author a and CitedTitle(c) for every citation c. By assigning numbers to objects as they are generated, we can consider logical variables a and c to be indices
on the set of natural numbers. Since BLOG is a typed language, the range of interpretations of a function symbol f
is specified by its type signature. For example, the interpretation of RefAuthor(u), for each u ∈ [AuthorMention] =
{1, 2, . . . , n(AuthorMention)}, takes a value on the range
[Author]. Likewise, Title(p) ranges over the set of strings
[String]. Figures 2 and 3 omit function declaration statements, which specify type signatures. Nonetheless, this
should not prevent the reader from deducing the type signatures of the functions via the statements that generate them.
Nonparametric priors define distributions over probability
measures, so we need function symbols that uniformly refer to them. Letting X and Y be object domains (e.g.
X = [Author]), we define MD (X | Y) to be the set of conditional probability densities p(x ∈ X | y ∈ Y) following
the class of parameterizations D. We can extend this logic,
denoting MD′ (MD (X | Y) | Z) to be the set of probability
measures p(d ∈ D | z ∈ Z) over the choice of parameterizations d ∈ D, conditioned on Z. And so on. For peace of
mind, we assume each class of distributions D is defined on
a measurable σ-field and the densities are integrable over
the range of the sample space. Note that Y or Z, but not
X , may be a Cartesian product over sets of objects. BLOG
does not allow return types that are tuples of objects, so
we restrict distributions of objects accordingly. One can
extend the above reasoning to accommodate distributions
over multiple unknown objects by adopting slightly more
general notation involving products of sets of objects.
We assign symbols to all the functions defined in the language LΨ . For instance, the range of NameDist in Fig. 3
is M([String]) for some specified parameterization class.
Since NameDist is not generated in another line, it must
be fixed over all possible worlds. For each publication p,
the interpretation of symbol PubAuthorsDist(p) is assigned
a value on the space MMultinomial ([Author]). That is, the
1
Even though the DP imposes a distribution over an infinite set
of unknown objects, nω (τ ) is still finite since it refers to the estimated number of objects in ω. n(τ ) corresponds to the random
variables of the DP mixture, as explained in Sec. 3.5.

function symbol refers to a distribution over author objects.
How the model chooses the success rate parameters for this
multinomial distribution, given that it is not on the left side
of any generating statement, is the subject of Sec. 3.5.
NP-BLOG integrates first-order logic with Bayesian nonparametric methods, but we have left out one piece of the
puzzle: how to specify distributions such as NameDist, or
classes of distributions. This is an important design decision, but an implementation level detail, so we postpone it
to future work. For the time being, one can think of parameterizations as object classes in a programming language
such as Java that generate samples of the appropriate type.
We point out that there is already an established language
for constructing hierarchical Bayesian models, BUGS [20].
The truth of any first-order sentence is determined by a
possible world in the corresponding language. A possible
world ω ∈ ΩΨ consists of an extension [τ ]ω for each type τ
and an interpretation for each function symbol f . Sec. 3.5
details how NP-BLOG specifies a distribution over ΩΨ .
3.3 Dependency statements for known objects
The dependency statement is the key ingredient in the specification of a generative process. We have already seen several examples of dependency statements, and we formalize
them here. It is well explained in [11], but we need to extend the definition in the context of nonparametrics.
In BLOG, a dependency statement looks like
f (x1 , . . . , xL ) ∼ g(t1 , . . . , tN );

(5)

where f is a function symbol, x1 , . . . , xL is a tuple of
logical variables representing arguments to the function,
g is a probability density conditioned on the arguments
t1 , . . . , tN , which are terms or formulae in the language
LΨ in which the logical variables x1 , . . . , xL may appear.
The dependency statement carries out a generative process. For an example, let’s look at the dependency statement on line 10 of Fig. 2. Following the rules of semantics [11], line 10 generates assignments for random variables φCitedTitle [c], for c = 1, . . . , n(Citation), from probability density g conditioned on values for zRefPub [c] and
φTitle [p], for all p = 1, . . . , n(Pub). As in [11], we use
square brackets to index random variables, instead of the
statistics convention of using subscripts.
In NP-BLOG, the probability density g is itself a function
symbol, and the dependency statement is given by
f (x1 , . . . , xL ) ∼ g(t1 , . . . , tM ){tM +1 , . . . , tM +N }; (6)
where f and g are function symbols, and t1 , . . . , tM +N
are formulae of the language as in (5). For this to be a
valid statement, g(t1 , . . . , tM ) must be defined on the range
M(X | Y), where X is the range of f (x1 , . . . , xL ) and Y is
the domain of the input arguments within the curly braces.
The first M terms inside the parentheses are evaluated in
possible world ω, and their resulting values determine the

choice of measure g. The terms inside the curly braces
are evaluated in ω and the resulting values are passed to
distribution g(t1 , . . . , tM ). When all the logical variables
x1 , . . . , xL refer to guaranteed objects, the semantics of the
dependency statement are given by [11]. The curly brace
notation is used to disambiguate the two roles of input argument variables. The arguments inside parentheses are
indices to function symbols (e.g. the c in RefPub(c) in
Fig. 3), whereas the arguments inside curly braces serve
as input to probability densities (e.g. the term inside the
curly braces in TitleStrDist{Title(RefPub(c))}). This new
notation is necessary when a distribution takes both types
of arguments. We don’t have such an example in citation
matching, so we borrow one from an NP-BLOG model in
the aircraft tracking domain:2
State(a, t) if t = 0 then ∼ InitState{}
else ∼ StateTransDist(a){State(a, t − 1)};

The state of the aircraft a at time t is an R6Vector object
which stores the aircraft’s position and velocity in space.
When t > 0, the state is generated from the transition distribution of aircraft a given the state at the previous time
step. StateTransDist(a) corresponds to a measure on the
space M([R6Vector] | [R6Vector]).
For example, in line 6 of Fig. 3, the citation objects are
guaranteed. Following the rules of semantics, line 6 defines
a random variable φCitedTitle [c] corresponding to the interpretation of function symbol CitedTitle(c) for every citation
c. Given assignments to φTitleStrDist , zRefPub [c] (we use z
to be consistent with the notation of the semantics used in
this paper, although it makes no difference in BLOG) and
φTitle [p] for all p ∈ [Pub] — assignments that are either
observed or generated from other statements — the dependency statement defines the generative process
φCitedTitle [c] ∼ φTitleStrDist (φTitle [p]) s.t. p = zRefPub [c].
BLOG allows for contingencies in dependency statements.
These can be subsumed within our formal
framework
by defining a new measure φ′ (c, t) =
P
i δ(ci )φi (ti,1 , ti,2 , . . .), where δ(·) is the indicator function, ci is the condition i which must be satisfied in order to
sample from the density φi , c and t are the complete sets of
terms and conditions, and the summation is over the number of clauses. Infinite contingencies and their connection
to graphical models are discussed in [12].
3.4 Exchangeability and unknown objects
Unknown objects are precisely those which are not guaranteed. In this section, we formalize some important properties of generated objects in BLOG. We adopt the notion
of exchangeability [1] to objects in probabilistic first-order
logic. We start with some standard definitions.
2

In which aircraft in flight appear as blips on a radar screen,
and the objectives are to infer the number of aircraft and their
flight paths and to resolve identity uncertainty, arising because
a blip might not represent any aircraft or, conversely, an aircraft
might produce multiple detections [10].

Definition 1. The random variables x1 , . . . , xN are
(finitely) exchangeable under probability density function
p if p satisfies p(x1 , . . . , xN ) = p(xπ(1) , . . . , xπ(N ) ) for
all permutations π on {1, . . . , N } [1].
When n is finite, the concept of exchangeability is intuitive:
the ordering is irrelevant since possible worlds are equally
likely. The next definition extends exchangeability to unbounded sequences of random variables.
Definition 2. The random variables x1 , x2 , . . . are infinitely exchangeable if every finite subset is finitely exchangeable [1].
Exchangeability is useful for reasoning about distributions
over properties on sets of objects in BLOG. From Definitions 1 and 2, we have the following result.
Proposition 1. It is possible to define g in the dependency
statements (5) and (6) such that the sequence of objects
x1 , . . . , xL is finitely exchangeable if and only if the terms
t1 , . . . , tM +N do not contain any statements referring to a
particular xl .
For example, the distribution of hair colours of two people,
Eric and Mike, is not exchangeable given evidence that Eric
is the father of Mike. What about sequences of objects such
as time? As long as we do not set the predecessor function
beforehand, any sequence is legally exchangeable.
In this paper, models are restricted to infinitely exchangeable unknown objects. We can interpret this presupposition
this way: if we reorder a sequence of objects, then their
probability remains the same. If we add another object to
the sequence at some arbitrary position, both the original
and new sequence with one more object are exchangeable.
We can then appeal to de Finetti’s theorem (3), and hence
the Dirichlet process. Therefore, the order of unknown objects is not important, and we can reason about set of objects rather than sequences. While there are many domains
in which one would like to infer the presence of objects
that are not infinitely exchangeable, this constraint leaves
us open to modeling a wide range of interesting domains.
Unknown or non-guaranteed objects are assigned non-rigid
designators; a symbol in different possible worlds does not
necessarily refer to the same object, and so it does not
make sense to assign it a rigid label. This consideration
imposes a constraint: we can only refer to a publication
p via a guaranteed object, such as a citation c that refers
to it. While we cannot form a query that addresses a specific unknown object, or a subset of unknown objects, we
can pose questions about publications using existential and
universal quantifiers (resolved using Skolemization, for instance). We could ask, for instance, how many publications
have three or more authors.
3.5 Dependency statements for unknown objects
Sec. 3.2 formalized the notion of type extensions and function symbols in NP-BLOG programs. Sec. 3.3 served up
the preliminaries of syntax and semantics in dependency

statements. The remaining step to complete the full prescription of the semantics as a mapping from the language
LΨ to a distribution over possible worlds. This is accomplished by constructing a Bayesian hierarchical model over
random variables {φ, n, γ}, such that the set of random
variables φ is in one-to-one correspondence with the set of
function interpretations, n refers to the sizes of the type extensions,
and γ is a set of auxiliary random variables such
R
that p(φ, n, γ)dγ = p(φ, n). One might wonder why we
don’t dispense of function symbols entirely and instead describe everything using random variables, as in [18]. The
principal reason is to establish the connection with firstorder logic. Also, we want to make it clear that some random variables do not map to any individual in the domain.
What follows is a procedural definition of the semantics.
We now define distributions over the random variables, and
their mapping to the symbols of the first-order logic.
In order to define the rules of semantics, we collect dependency and number statements according to their input
argument types. If the collection of statements includes
a number statement, then the rules of semantics are given
in [11]. Otherwise, we describe how the objects and their
properties are implicitly drawn from a DP. Consider a set of
K dependency statements such that the generated functions
f1 , . . . , fK all require a single input of type υ, and [υ]ω can
vary over possible worlds ω. We denote x to be the logical
variable that ranges over [υ]. (The output types of the fk ’s
are not important.) The K dependency statements look like
f1 (x) ∼ g1 (t1,1 , . . . , t1,M1 ){t1,M1 +1 , . . . , t1,M1 +N1 };
..
..
(7)
.
.
fK (x) ∼ gK (tK,1 , ..., tK,MK){tK,MK+1 , ..., tK,MK+NK};
where Mk and Nk are the number of input arguments to
gk (·) and gk {·}, respectively, and tk,i is a formula in the
language in which x may appear. As in BLOG, each fk (x)
is associated with a random variable φfk [x]. The random
variables φg1 , . . . , φgK , including all those implicated in
the terms, must have been generated by other lines or are
observed. Overloading the notation, we define the terms
tk,i to be random variables that depend deterministically
on other generated or observed random variables. The set
of statements (7) defines the generative process
πυ ∼ Stick(αυ )
(8)
φfk [x] ∼ φgk [tk,1 , ..., tk,Mk ](tk,Mk+1 , ..., tk,Mk+Nk ), (9)
for k = 1, . . . , K, x = 1, . . . , ∞, where αυ is the userdefined DP concentration parameter and πυ is a multinomial distribution such that each success rate parameter πυ,x
determines the probability of choosing a particular object
x. NP-BLOG infers a distribution π over objects of type
υ following the condition of infinite exchangeability. For
example, applying rules (8,9) to line 4 of Fig. 3, we get
πAuthor ∼ Stick(αAuthor )
φName [a] ∼ φNameDist , for a = 1, . . . , ∞

If an object type does not have any dependency or number
statements, then no distribution over its extension is introduced (e.g. strings in the citation matching model).
The implementation of the DP brings about an important
subtlety: if x takes on a possibly infinite different set of values, how do we recover the true number of objects n(τ )?
The idea is to introduce a bijection from the subset of positive natural numbers that consists only of active objects to
the set {1, . . . , n(τ )}. An object is active in possible world
ω if and only if at least one random variable is assigned to
that object in ω. In the above example, n(Author) is the
number of author objects that are mentioned in the citations. Of course, in practice we do not sample an infinite
series of random variables φName [a].
If we declare a function symbol f with a return type τ ranging over a set of unknown objects, then there exists the default generating process
zf [x] ∼ πτ .

(10)

We use zf [x] instead of φf [x] to show that the random variables are the indicators of the DP mixture (4). For example,
each zRefPub [c] in line 6 in Fig. 3 is independently drawn
from the distribution of publications πPub . We can view
line 6 as constructing a portion of the hierarchical model,
as shown in Fig. 4. The number of publications n(Pub) is
set to the number of different values assigned to zRefPub [c].
NP-BLOG allows for the definition of a symbol f that corresponds to a multinomial distribution over [τ ], so its range
is MMultinomial ([τ ]). It exhibits the default prior
φf [x] ∼ Dirichlet(αf πτ ),

(11)

analogous to (10). αf is a user-defined scalar. We define
nf [x] to be the true number of objects associated with collection f (x). This is useful for modeling collections of objects such as the authors of a publication. Applying rules
(8,9,11) to the statements in Fig. 3 involving publication
objects, we arrive at the generative process
πPub ∼ Stick(αPub )
φTitle [p] ∼ φTitleDist , for p = 1, . . . , n(Pub)
φPubAuthorsDist [p] ∼ Dirichlet(αPubAuthorsDist πAuthor ) .
Most of the corresponding graphical model is shown in
Fig. 4. Only the φPubAuthorsDist [p]’s are missing, and
they are shown in Fig. 5. The true number of authors
nPubAuthorsDist [p] in publication p comes from the support
of all random variables that refer to it, and n(Pub) is determined by nPubAuthorsDist . While this paper focuses on the
Dirichlet process, our framework allows for other classes
of nonparametric distributions. One example can be found
in the aircraft tracking domain from Sec. 3.2, in which the
generation of aircraft transition tables might be specified
with the statement StateTransDist(a) ∼ StateTransPrior{}.
In both cases (10) and (11), one can override the defaults
by including appropriate dependency statements for f , in

Num. citations
Num. papers
Phrase matching
RPM+MCMC
CRF-Seg (N = 9)
NP-BLOG

Figure 5: The white nodes are the portion of the graphical
model generated in lines 7 and 8 of Fig. 3. See Fig. 4 for
an explanation of the darkened nodes.
which case we get φf [x] ∼ φg , following rule (9). For example, lines 7 and 8 in Fig. 3 specify the generative process
for the author mention objects,
zRefAuthor [u] ∼ φPubAuthorsDist [p]
φCitedName [u] ∼ φNameStrDist (φName [a]) ,
s.t. p = zRefPub [c], c = φCitedIn [u], a = zRefAuthor [u].
Fig. 5 shows the equivalent graphical model.
The generative process (8,9) is a stick-breaking construction over the unknown objects and their attributes. When
the objects x range over the set of natural numbers, (8,9) is
equivalent to the Dirichlet process
Gυ ∼ DP (αυ , Hυ,1 × · · · × Hυ,K ) ,
(12)
P∞
where Gυ , x=1 πυ,x δ(φf1 [x]) × · · · × δ(φfK [x]), and
Hυ,k is the base measure over the assignments to φfk , defined by gk conditioned on the terms tk,1 , . . . , tk,Mk +Nk .
Since BLOG is a typed, free language, we need to allow
for the null assignment to φf [x] when it is implicitly drawn
from πτ in (10). We permit the clause
f (x) ∼ if cond then null;

(13)

which defines φf [x] ∼ δ(null)δ(cond) + πτ (1 − δ(cond)).
This statement is necessary to take care of the situation
when an object’s source can be of different types, as in the
aircraft tracking domain with false alarms [10].
Next, we briefly describe how to extend the rules of semantics to functions with multiple input arguments. Let’s
consider the case of two inputs with an additional logical
variable y ∈ [ν]. Handling an additional input argument
associated with known (guaranteed) objects is easy. We
just duplicate (8,9) for every instance of y in the guaranteed type extension. This is equivalent to adding a finite
series of plates in the graphical model. Otherwise, we assume the unknown objects are drawn independently. That
is, π(υ,ν) = πυ πν . Multiple unknown objects as input does
cause some superficial complications with the interpretation of (8,9) as a DP, principally because we need to define
new notation for products of measures over different types.

Face Reinforce. Reason. Constraint
349
406
514
295
246
149
301
204
0.94
0.79
0.86
0.89
0.97
0.94
0.96
0.93
0.97
0.94
0.94
0.95
0.93
0.84
0.89
0.86

Table 1: Citation matching results for the Phrase Matching [9], RPM [14], CRF-Seg [22] and NP-BLOG models.
Performance is measured by counting the number of publication clusters that are recovered perfectly. The NP-BLOG
column reports an average over 1000 samples.
The DP determines an implicit distribution of unknown, infinitely exchangeable objects according to their properties.
That is, the DP distinguishes unknown objects solely by
their attributes. However, this is not always desirable —
for instance, despite being unable to differentiate the individual pieces, we know a chess board always has eight
black pawns. This is precisely why we retain the original
number statement syntax of BLOG which allows the user
to specify a prior over the number of unknown objects, independent of their properties. In the future, we would like
to experiment with priors that straddle these two extremes.
This could possibly be accomplished by setting a prior on
the Dirichlet concentration parameter, α.
By tracing the rules of semantics, one should see that only
thing the citation matching model does not generate is values for CitedIn(u). Therefore, they must be observed. We
can also provide observations from any number of object
attributes, such as CitedTitle(c) and CitedName(u), which
would result in unsupervised learning. By modifying the
set of evidence, one can also achieve supervised or semisupervised learning. Moreover, the language can capture both generative and discriminative models, depending
whether or not the observations are generated.
To summarize, the rules given by (7-11,13), combined
with the number statement [11], construct a distribution
p(φ, z, n, γ) such that the set of auxiliary variables is
γ = {π, α}, {φ, z} is in one-to-one correspondence with
the interpretations of the function symbols, the n are the
sizes of the [τ ], and an assignment to {φ, z, n} completely
determines the possible world ω ∈ Ω. The rules of semantics assemble models that are arbitrary hierarchies of DPs.

4

Experiment

The purpose of this experiment is to show that the NPBLOG language we have described realizes probabilistic
inference on a real-world problem. We simulate the citation matching model in Fig. 3 on the CiteSeer data set [9],
which consists of manually segmented citations from four
research areas in AI.
We use Markov Chain Monte Carlo (MCMC) to simulate
possible worlds from the model posterior given evidence in
the form of cited authors and titles. Sec. 2 briefly describes

There is much future work on this topic. An important direction is the development of efficient, flexible and on-line
inference methods for hierarchies of Dirichlet processes.

Figure 6: Estimated (solid blue) and true (dashed red line)
number of publications for the Face and Reasoning data.

Acknowledgements
This paper wouldn’t have happened without the help of
Brian Milch. Special thanks to Gareth Peters and Mike
Klaas for their assistance, and to the reviewers for their
time and effort in providing us with constructive comments.



First-order probabilistic models combine representational power of first-order logic with graphical models. There is an ongoing effort to design lifted inference algorithms for first-order
probabilistic models. We analyze lifted inference from the perspective of constraint processing and, through this viewpoint, we analyze and
compare existing approaches and expose their
advantages and limitations. Our theoretical results show that the wrong choice of constraint
processing method can lead to exponential increase in computational complexity. Our empirical tests confirm the importance of constraint
processing in lifted inference. This is the first
theoretical and empirical study of constraint processing in lifted inference.

1

INTRODUCTION

Representations that mix graphical models and first-order
logic—called either first-order or relational probabilistic
models—were proposed nearly twenty years ago (Breese,
1992; Horsch and Poole, 1990) and many more have
since emerged (De Raedt et al., 2008; Getoor and Taskar,
2007). In these models, random variables are parameterized by individuals belonging to a population. Even for
very simple first-order models, inference at the propositional level—that is, inference that explicitly considers every individual—is intractable. The idea behind lifted inference is to carry out as much inference as possible without
propositionalizing. An exact lifted inference procedure for
first-order probabilistic directed models was originally proposed by Poole (2003). It was later extended to a broader
range of problems by de Salvo Braz et al. (2007). Further
work by Milch et al. (2008) expanded the scope of lifted
inference and resulted in the C-FOVE algorithm, which is
currently the state of the art in exact lifted inference.
First-order models typically contain constraints on the pa-

rameters (logical variables typed with populations). Constraints are important for capturing knowledge regarding
particular individuals. In Poole (2003), each constraint
is processed only when necessary to continue probabilistic inference. We call this approach splitting as needed.
Conversely, in de Salvo Braz et al. (2007) all constraints
are processed at the start of the inference (this procedure
is called shattering), and at every point at which a new
constraint arises. Both approaches need to use constraint
processing to count the number of solutions to constraint
satisfaction problems that arise during the probabilistic inference. Milch et al. (2008) adopt the shattering procedure,
and avoid the need to use a constraint solver by requiring
that the constraints be written in normal form.
The impact of constraint processing on computational efficiency of lifted inference has been largely overlooked.
In this paper we address this issue and compare the approaches to constraint processing listed above, both theoretically and empirically. We show that, in the worst case,
shattering may have exponentially worse space and time
complexity (in the number of parameters) than splitting as
needed. Moreover, writing the constraints in normal form
can lead to computational costs with a complexity that is
even worse than exponential. Experiments confirm our theoretical results and stress the importance of informed constraint processing in lifted inference.
We introduce key concepts and notation in Section 2 and
give an overview of constraint processing during lifted inference in Section 3. In Section 4 we discuss how a specialized #CSP solver can be used during lifted inference.
Theoretical results are presented in Section 5. Section 6
contains results of experiments.

2

PRELIMINARIES

In this section we introduce a definition of parameterized random variables, which are essential components of
first-order probabilistic models. We also define parfactors (Poole, 2003), which are data structures used during
lifted inference.

294
2.1

KISYNSKI & POOLE

UAI 2009
g(x1)

PARAMETERIZED RANDOM VARIABLES

If S is a set, we denote by |S| the size of the set S.

g(A)

A population is a set of individuals. A population corresponds to a domain in logic.
A parameter corresponds to a logical variable and is typed
with a population. Given parameter X , we denote its population by D(X). Given a set of constraints C , we denote
a set of individuals from D(X) that satisfy constraints in C
by D(X) : C .
A substitution is of the form {X1 /t1 . . . . , Xk /tk }, where the
Xi are distinct parameters, and each term ti is a parameter
typed with a population or a constant denoting an individual from a population. A ground substitution is a substitution, where each ti is a constant.
A parameterized random variable is of the form
f (t1 , . . . ,tk ), where f is a functor (either a function symbol
or a predicate symbol) and ti are terms. Each functor has a
set of values called the range of the functor. We denote the
range of the functor f by range( f ). A parameterized random variable f (t1 , . . . ,tk ) represents a set of random variables, one for each possible ground substitution to all of its
parameters. The range of the functor of the parameterized
random variable is the domain of random variables represented by the parameterized random variable.
Let v denote an assignment of values to random variables;
v is a function that takes a random variable and returns its
value. We extend v to also work on parameterized random
variables, where we assume that free parameters are universally quantified.
Example 1. Let A and B be parameters typed with a population D(A) =D(B) ={x1 , . . . , xn }. Let h be a functor with
range {true, f alse}. Then h(A, B) is a parameterized random variable. It represents a set of n2 random variables
with domains {true, f alse}, one for each ground substitution {A/x1 , B/x1 }, {A/x1 , B/x2 }, . . . ,{A/xn , B/xn }. A parameterized random variable h(x1 , B) represents a set of
n random variables with domains {true, f alse}, one for
each ground substitution {B/x1 }, . . . ,{B/xn }. Let v be an
assignment of values to random variables. If v(h(x1 , B))
equals true, each of the random variables represented by
h(x1 , B), namely h(x1 , x1 ), . . . , h(x1 , xn ), is assigned the
value true by v.
2.2

PARAMETRIC FACTORS

A factor on a set of random variables represents a function
that, given an assignment of a value to each random variable from the set, returns a real number. Factors are used
in the variable elimination algorithm (Zhang and Poole,
1994) to store initial conditional probabilities and intermediate results of computation during probabilistic inference
in graphical models. Operations on factors include mul-

g(xn)

h(x1, x1)

h(A, B)

h(x1, xn)

B
A

FIRST-ORDER

h(xn , x1)

h(xn , xn)

PROPOSITIONAL

Figure 1: A parameterized belief network and its equivalent
belief network.
tiplication of factors and summing out random variables
from a factor.
Let v be an assignment of values to random variables and
let F be a factor on a set of random variables S. We extend
v to factors and denote by v(F) the value of the factor F
given v. If v does not assign values to all of the variables in
S, v(F) denotes a factor on other variables.
A parametric factor or parfactor is a triple hC , V, Fi where
C is a set of inequality constraints on parameters, V is a set
of parameterized random variables and F is a factor from
the Cartesian product of ranges of parameterized random
variables in V to the reals.
A parfactor hC , V, Fi represents a set of factors, one for
each ground substitution G to all free parameters in V that
satisfies the constraints in C . Each such factor FG is a factor on the set of random variables obtained by applying a
substitution G. Given an assignment v to random variables
represented by V, v(FG ) = v(F).
Parfactors are used to represent conditional probability distributions in directed first-order models and potentials in
undirected first-order models as well as intermediate computation results during inference in first-order models.
In the next example, which extends Example 1, we use parameterized belief networks (PBNs) (Poole, 2003) to illustrate representational power of parfactors. The PBNs are
a simple first-order directed probabilistic model, we could
have used parameterized Markov networks instead (as did
de Salvo Braz et al. (2007) and Milch et al. (2008)). Our
discussion of constraint processing in lifted inference is not
limited to PBNs, it applies to any model for which the joint
distribution can be expressed as a product of parfactors.
Example 2. A PBN consists of a directed acyclic graph
where the nodes are parameterized random variables, an
assignment of a range to each functor, an assignment of
a population to each parameter, and a probability distribution for each node given its parents. Consider the PBN
graph presented in Figure 1 using plate notation (Buntine,

UAI 2009

KISYNSKI & POOLE

1994). Let g be a functor with range {true, f alse}. Assume
we do not have any specific knowledge about instances of
g(A), but we have some specific knowledge about h(A, B)
for case where A = x1 and for case where A 6= x1 and A = B.
The probability P(g(A)) can be represented with a parfactor h0,
/ {g(A)}, Fg i, where Fg is a factor from range(h) to
the reals. The conditional probability P(h(A, B)|g(A)) can
be represented with a parfactor h0,
/ {g(x1 ), h(x1 , B)}, F1 i, a
parfactor h{A 6= x1 }, {g(A), h(A, A)}, F2 i, and a parfactor
h{A 6= x1 , A 6= B}, {g(A), h(A, B)}, F3 i, where F1 , F2 , and
F3 are factors from range(g) × range(h) to the reals.
Let C be a set of inequality constraints on parameters and
X be a parameter. We denote by EXC the excluded set for X,
that is, the set of terms t such that (X 6= t) ∈ C . A parfactor
hC , V, FF i is in normal form (Milch et al., 2008) if for each
inequality (X 6= Y ) ∈ C , where X and Y are parameters, we
have EXC \{Y } = EYC \{X}.
In a normal form parfactor, for all parameters X of parameterized random variables in V, | D(X): C | = | D(X) |−| EXC |.
Example 3. Consider the parfactor h{A 6= x1 , A 6=
B},{g(A), h(A, B)}, F3 i from Example 2. Let C denote
a set of constraints from this parfactor. The set C contains only one inequality between parameters, namely
A 6= B. We have EAC = {x1 , B} and EBC = {A}. As
EAC \{B} 6= EBC \{A}, the parfactor is not in normal form.
Recall that D(A) =D(B) ={x1 , . . . , xn }. The size of the
set D(A) : C depends on the parameter B. It is equal
n − 1 when B = x1 and n − 2 when B 6= x1 . Other parfactors from Example 2 are in normal form as they do
not contain constraints between parameters. Consider
a parfactor h{X 6= Y, X 6= a,Y 6= a}, {e(X), f (X,Y )}, Fe f i,
where D(X) = D(Y ) and | D(X) | = n. Let C 0 denote a set
0
0
of constraints from this parfactor. As EXC \{Y } = EYC \{X},
the parfactor is in normal form and | D(X) : C 0 | = n − 2 and
| D(Y ) : C 0 | = n − 2.

3

LIFTED INFERENCE AND
CONSTRAINT PROCESSING

In this section we give an overview of exact lifted
probabilistic inference developed in (Poole, 2003),
(de Salvo Braz et al., 2007), and (Milch et al., 2008) in
context of constraints. For more details on other aspects of
lifted inference we refer the reader to the above papers.
Let Φ be a set of parfactors. Let J (Φ) denote a factor
equal to the product of all factors represented by elements
of Φ. Let U be the set of all random variables represented
by parameterized random variables present in parfactors in
Φ. Let Q be a subset of U. The marginal of J (Φ) on Q,
denoted JQ (Φ), is defined as JQ (Φ) = ∑U\Q J (Φ).
Given Φ and Q, the lifted inference procedure computes the
marginal JQ (Φ) by summing out random variables from

295

Q, where possible in a lifted manner. Evidence can be handled by adding to Φ additional parfactors on observed random variables.
Before a (ground) random variable can be summed out, a
number of conditions must be satisfied. One is that a random variable can be summed out from a parfactor in Φ only
if there are no other parfactors in Φ involving this random
variable. To satisfy this condition, the inference procedure
may need to multiply parfactors prior to summing out.
Multiplication has a condition of its own: two parfactors
hC1 , V1 , F1 i and hC2 , V2 , F2 i can be multiplied only if for
each parameterized random variable from V1 and for each
parameterized random variable from V2 , the sets of random
variables represented by these two parameterized random
variables in respective parfactors are identical or disjoint.
This condition is trivially satisfied for parameterized random variables with different functors.
Example 4. Consider the PBN from Figure 1 and set Φ
containing parfactors introduced in Example 2. Assume
that we want to compute the marginal of J (Φ) on instances
of h(A, B), where A 6= x1 and A 6= B. We need to sum
out random variables represented by g(A) from parfactor
h{A 6= x1 , A 6= B}, {g(A), h(A, B)}, F3 i, but as they are also
among random variables represented by g(A) in parfactor
h0,
/ g(A), Fg i, we have to first multiply these two parfactors. Sets of random variables represented by g(A) in these
two parfactors are not disjoint and are not identical and the
precondition for multiplication is not satisfied.
3.1

SPLITTING

The precondition for parfactor multiplication may be satisfied through splitting parfactors on substitutions.
Let Φ be a set of parfactors. Let p f = hC , V, FF i ∈ Φ.
Let {X/t} be a substitution such that (X 6= t) ∈
/ C and
term t is a constant such that t ∈ D(X), or a parameter
such that D(t) = D(X). A split of p f on {X/t} results in
two parfactors: p f [X/t] that is a parfactor p f with all occurrences of X replaced by term t, and a parfactor p fr =
hC ∪{X 6= t}, V, FF i. We have J (Φ) = J (Φ \ {p f }∪
{p f [X/t], p fr }). We call p fr a residual parfactor.
Given two parfactors that need to be multiplied, substitutions on which splitting is performed are determined by analyzing constraint sets C and sets of parameterized random
variables V in these parfactors.
Example 5. Let us continue Example 4.
A split
of h0,
/ {g(A)}, Fg i on {A/x1 } results in h0,
/ {g(x1 )}, Fg i
and residual h{A 6= x1 }, {g(A)}, Fg i. The first parfactor
can be ignored because it is not relevant to the query,
while the residual needs to be multiplied by a parfactor
h{A 6= x1 , A 6= B}, {g(A), h(A, B)}, F3 i. The precondition
for multiplication is now satisfied as g(A) represents the
same set of random variables in both parfactors.

296
3.2

KISYNSKI & POOLE
MULTIPLICATION

Once the precondition for parfactor multiplication is satisfied, multiplication can be performed in a lifted manner.
This means that, although parfactors participating in a multiplication as well as their product represent multiple factors, the computational cost of parfactor multiplication is
limited to the cost of multiplying two factors. The only additional requirement is that the lifted inference procedure
needs to know how many factors each parfactor involved
in the multiplication represents and how many factors their
product will represent. These numbers can be different because the product parfactor might involve more parameters
than a parfactor participating in the multiplication. In such
a case, a correction to values of a factor inside appropriate
parfactors participating in multiplication is necessary. Detailed description of this correction is beyond the scope of
this paper. For more information see Example 6 below and
a discussion of the fusion operation in de Salvo Braz et al.
(2007). For our purpose, the key point is that the lifted inference procedure needs to compute the number of factors
represented by a parfactor.
Given a parfactor hC , V, Fi, the number of factors it represents is equal to the number of solutions to the constraint
satisfaction problem formed by constraints in C . This
counting problem is written as #CSP (Dechter, 2003). If
a parfactor is in normal form, each connected component
of the underlying constraint graph is fully connected (see
Proposition 1) and it is easy to compute the number of factors represented by the considered parfactor. If a parfactor
is not in normal form, a #CSP solver is necessary to compute the number of factors the parfactor represents.
Example 6. In Example 5 the parfactor h{A 6= x1 },
{g(A)}, Fg i represents n − 1 factors, and needs to be multiplied by a parfactor h{A 6= x1 , A 6= B}, {g(A), h(A, B)}, F3 i,
which represents (n − 1)2 factors. Their product p f∗ =
hA 6= x1 , A 6= B, {g(A), h(A, B)}, F∗ i, where F∗ is a factor
from range(g) × range(h) to the reals, represents (n − 1)2
factors. Let v be an assignment of values to g(A) and
(n−1)/(n−1)2

h(A, B). We have v(F∗ ) = v(Fg )
v(F3 ). Now
we can sum out g(A) from p f∗ . The result needs to be
represented with a counting formula (Milch et al., 2008),
which is outside of the scope of this paper. What is important for the paper is that a parfactor involving counting
formulas needs to be in normal form. Since p f∗ is not in
normal form, we first split it on substitution {B/x1 } and
then sum out g(A) from the two parfactors obtained through
splitting.
3.3

SUMMING OUT

During lifted summing out, a parameterized random variable is summed out from a parfactor hC , V, FF i, which
means that a random variable is eliminated from each factor
represented by the parfactor in one inference step. Lifted

UAI 2009

inference will perform summing out only once on the factor F. If some parameters only appear in the parameterized variable that is being eliminated, the resulting parfactor will represent fewer factors than the original one. As
in the case of parfactor multiplication, the inference procedure needs to compensate for this difference. It needs to
compute the size of the set X = (D(X1 ) × · · · × D(Xk )) : C ,
where X1 , . . . , Xk are parameters that will disappear from
the parfactor. This number tells us how many times fewer
factors the result of summing out represents compared to
the original parfactor.
If the parfactor is in normal form, |X | does not depend on
values of parameters remaining in the parfactor after summing out. The problem reduces to #CSP and, as we mentioned in Section 3.2, it is easy to solve. If the parfactor is
not in normal form, |X | may depend on values of remaining
parameters and a #CSP solver is necessary to compute all
the sizes of the set X conditioned on values of parameters
remaining in the parfactor.
Example 7. Assume that we want to sum out f (X,Y )
from a parfactor h{X 6= Y,Y 6= a}, {e(X), f (X,Y )}, Fe f i,
where Fe f is a factor from range(e) × range( f ) to the reals. Let D(X) = D(Y ) and | D(X) | = n. The parfactor represents (n − 1)2 factors. Note that the parfactor is
not in normal form and | D(Y ) : {X 6= Y,Y 6= a}| equals
n − 1 if X = a and n − 2 if X 6= a. A #CSP solver could
compute these numbers for us (see Example 9). After
f (X,Y ) is summed out, Y is no longer among parameters of random variables and X remains the sole parameter. To represent the result of summation we need two
parfactors: h0,
/ e(a), Fe1 i and h{X 6= a}, e(X), Fe2 i, where
Fe1 and Fe2 are factors from range(e) to the reals. Let
y ∈ range(e), then Fe1 (y) = (∑z∈range( f ) Fe f (y, z))n−1 and
Fe2 (y) = (∑z∈range( f ) Fe f (y, z))n−2 .
3.4

PROPOSITIONALIZATION

During inference in first-order probabilistic models it may
happen that none of lifted operations (including operations
that are not described in this paper) can be applied. In such
a situation the inference procedure substitutes appropriate
parameterized random variables with random variables represented by them. This may be achieved through splitting
as we demonstrate in an example below. Afterward, inference is performed, at least partially, at the propositional
level. As it has a negative impact on the efficiency of inference, propositionalization is avoided as much as possible
during inference in first-order models.
Example 8. Consider a parfactor h0,
/ {g(A)}, Fg i from Example 2. Assume we need to propositionalize g(A). Recall that D(A) ={x1 , . . . , xn }. Propositionalization results
in a set of parfactors h0,
/ {g(x1 )}, Fg i, h0,
/ {g(x2 )}, Fg i, . . . ,
h0,
/ {g(xn−1 )}, Fg i, h0,
/ {g(xn )}, Fg i. Each parameterized
random variable in the above parfactors represents just one

UAI 2009
10

10

KISYNSKI & POOLE

100w

10w

5w

W

W
w

3

=

2w

W

=

X

ϖw

5

10

297

Y

X

Z

=

=

Y

=

X=Y
=

=

=

=

=

Z

=

Z
0

10

0

5

10

15
w

20

25

30

Figure 2: Comparison of ϖw and exponential functions.
random variable and each parfactor represents just one factor. The set could be produced by a sequence of splits.
The above informal overview of lifted inference, together
with simple examples, shows that constraint processing is
an integral, important part of lifted inference.

4

#CSP SOLVER AND LIFTED
INFERENCE

In Section 3, we showed when a #CSP solver can be used
during lifted probabilistic inference. A solver that enumerates all individuals from domains of parameters that
form a CSP would contradict the core idea behind lifted
inference, that is, performing probabilistic inference without explicitly considering every individual. In our experiments presented in Section 6 we used a solver (Kisyński
and Poole, 2006) that addresses the above concern. It is a
lifted solver based on the variable elimination algorithm for
solving #CSP of Dechter (2003) and is optimized to handle
problems that contain only inequality constraints. It is not
possible to describe the algorithm in detail in this paper, but
below we provide some intuition behind the solver.

(a)

(b)

Figure 3: Constraint graph with a cycle (a) and the two
cases that need to be analyzed (b).
us to immediately solve the corresponding #CSP: we can
assign the value to W in n ways, and are left with n − 1 possible values for X, and n − 1 possible values for Y and Z.
Hence, there are n(n − 1)3 solutions to this CSP instance.
Consider a set of constraints {W 6= X,W 6= Y, X 6= Z,Y 6=
Z}, where all parameters have the same population of size
n. The underlying graph has a cycle (see Figure 3 (a)),
which makes the corresponding #CSP more difficult to
solve than in the previous example. We can assign the value
to W in n ways, and are left with n − 1 possible values for
X and Y . For Z we need to consider two cases: X = Y and
X 6= Y (see Figure 3 (b)). In the X = Y case, Z can take
n − 1 values, while in the X 6= Y case, Z can have n − 2 different values. Hence, the number of solutions to this CSP
instance is n(n − 1)2 + n(n − 1)(n − 2)2 . The first case corresponds to a partition {{X,Y }} of the set of parameters
{X,Y }, while the second case corresponds to a partition
{{X}, {Y }} of this set.

First, we need to introduce a concept of a set partition. A
partition of a set S is a collection {B1 , . . . , Bw } S
of nonempty,
pairwise disjoint subsets of S such that S = wi=1 Bi . The
sets Bi are called blocks of the partition. Set partitions
are intimately connected to equality. For any consistent set
of equality assertions on parameters, there is a partition in
which the parameters that are equal are in the same block,
and the parameters that are not equal are in different blocks.
If we consider a semantic mapping from parameters to individuals in the world, the inverse of this mapping, where
two parameters that map to the same individual are in the
same block, forms a partition of the parameters. The number of partitions of the set of size w is equal to the w-th
Bell number ϖw . Bell numbers grow faster than any exponential function (see Lovász (2003)), but for small w’s
they stay much smaller than exponential functions with a
moderate base (see Figure 2).

In general, to perform this kind of reasoning, we need
to triangulate the constraint graph. This can be naturally
achieved with a variable elimination algorithm. Each new
edge adds two cases, one in which the edge corresponds to
the equality constraint and one in which it corresponds to
the inequality constraint. Some cases are inconsistent and
can be ignored. When we have to analyze a fully connected
subgraph of w new edges, we need to consider ϖw cases.
This is because each such case corresponds to a partition of
the parameters from the subgraph; those parameters in the
same block of the partition are equal, and parameters in different blocks are not equal. The number of such partitions
is equal to ϖw . The lifted #CSP solver analyzes ϖw partitions of parameters, rather than nw ground substitutions of
individuals. Since we do not care about empty partitions,
we will never have to consider more partitions than there
are ground substitutions. As w corresponds to the induced
width of a constraint graph (which we do not expect to be
large) and n corresponds to the population size (potentially
large), the difference between ϖw and nw can be very big
(see Figure 2).

Consider a set of constraints {W 6= X, X 6= Y, X 6= Z}, where
all parameters have the same population of size n. The underlying constraint graph has a tree structure, which allows

In practice, parameters can be typed with different populations (from the very beginning as well as because of unary
constraints). In such a situation, we can apply the above

298

KISYNSKI & POOLE

UAI 2009

reasoning to any set of individuals that are indistinguishable as far as counting is concerned. For example, the intersection of all populations is a set of individuals for which
we only need the size; there is no point in reasoning about
each individual separately. Similarly, the elements from the
population of a parameter that do not belong to the population of any other parameter can be grouped and treated
together. In general, any individuals that are in the populations of the same group of parameters can be treated identically; all we need is to know how many there are.

two parfactors are about to be multiplied and the precondition for multiplication is not satisfied.

Example 9. In Example 7 we need to know the number | D(Y ) : {X 6= Y,Y 6= a}|, where D(X) = D(Y ) and
| D(X) | = n. Let a1 denote set {a} and a2 denote set
D(X) \{a}. The following factor has value 1 for substitutions to parameters X,Y that are solutions to the above
CSP and 0 otherwise:

Shattering simplifies design and implementation of lifted
inference procedures, in particular, construction of elimination ordering heuristics. Unfortunately, as we show in
Theorem 1, it may lead to creation of large number of parfactors that would not be created by following the splitting
as needed approach.

Y
a2
a2
a2

X
a1
a2
a2

Partition(s)
{{X}} {{Y }}
{{X,Y }}
{{X}, {Y }}

1 .
0
1

After we eliminate Y from the above factor we obtain:
X
a1
a2

Partition(s)
{{X}}
{{X}}

n−1 .
n−2

Numbers n − 1 and n − 2 are obtained through analysis
of partitions of X and Y present in the original factor and
knowledge that a1 represents 1 individual and a2 represents
n − 1 individuals. From the second factor we can infer that
| D(Y ) : {X 6= Y,Y 6= a}| equals n − 1 if X = a and n − 2 if
X 6= a.
If we assume that all populations of parameters forming a
#CSP are sorted according to the same (arbitrary) ordering, sets of indistinguishable individuals can be generated
through a single sweep of the populations. Each such set
can be represented by listing all of its elements or by listing all elements from the corresponding population that do
not belong to it. For each set we choose a more compact
representation.

An alternative, called shattering, was proposed by
de Salvo Braz et al. (2007). They perform splitting at the
beginning of the inference by doing all the splits that are
required to ensure that for any two parameterized random
variables present in considered parfactors the sets of random variables represented by them are either identical or
disjoint.1 Shattering was also used in Milch et al. (2008).

Theorem 1. Let Φ be a set of parfactors. Let Q be a subset of the set of all random variables represented by parameterized random variables present in parfactors in Φ.
Assume we want to compute the marginal JQ (Φ). Then:
(i) if neither of the algorithms performs propositionalization, then every split on substitution {X/t}, where t is
a constant, performed by lifted inference with splitting
as needed is also performed by lifted inference with
shattering (subject to a renaming of parameters);
(ii) lifted inference with shattering might create exponentially more (in the maximum number of parameters in
a parfactor) parfactors than lifted inference with splitting as needed.
Proof. We present a sketch of a proof of the first statement
and a constructive proof of the second statement.
(i) Assume that lifted inference with splitting as needed
performs a split. We can track back the cause of this to
the initial set of parfactors Φ. Further analysis shows that
shattering the set Φ would also involve this split.
(ii) Consider the following set of parfactors:
Φ = {h0,
/ {gQ (), g1 (X1 , X2 , . . . , Xk ),

The answer from the solver needs to be translated to sets of
substitutions and constraints accompanying each computed
value. Standard combinatorial enumeration algorithms can
do this task.

5

THEORETICAL RESULTS

g2 (X2 , X3 , . . . , Xk ),
...,
gk (Xk ))}, F0 i,

[0]

h0,
/ {g1 (a, X2 , . . . , Xk )}, F1 i,

[1]

h0,
/ {g2 (a, X3 , . . . , Xk )}, F2 i,

[2]

...,
In this section we discuss consequences of different approaches to constraint processing in lifted inference.
5.1

SPLITTING AS NEEDED VS. SHATTERING

Poole (2003) proposed a scheme in which splitting is performed “as needed” through the process of inference when

h0,
/ {gk−1 (a, Xk )}, Fk−1 i,

[k − 1]

h0,
/ {gk (a)}, Fk i,

[k]

and let Q be gQ ().
1 Shattering might also be necessary in the middle of inference
if propositionalization has been performed.

UAI 2009

KISYNSKI & POOLE

For i = 1, . . . , k, a set of random variables represented by
a parameterized random variable gi (Xi , . . . , Xk ) in a parfactor [0] is a proper superset of a set of random variables represented by a parameterized random variable
gi (a, Xi+1 , . . . , Xk ) in a parfactor [i]. Therefore lifted inference with shattering needs to perform several splits. Since
the order of splits during shattering does not matter here,
assume that the first operation is a split of the parfactor [0]
on a substitution {X1 /a} which creates a parfactor
h0,
/ {gQ (), g1 (a, X2 , . . . , Xk ), g2 (X2 , X3 , . . . , Xk ),
. . . , gk (Xk ))}, F0 i

[k + 1]

and a residual parfactor
h{X1 6= a}, {gQ (), g1 (X1 , X2 , . . . , Xk ),
g2 (X2 , X3 , . . . , Xk ), . . . , gk (Xk ))}, F0 i.

[k + 2]

In both newly created parfactors, for i = 2, . . . , k, a set of
random variables represented by a parameterized random
variable gi (Xi , . . . , Xk ) is a proper superset of a set of random variables represented by a parameterized random variable gi (a, Xi+1 , . . . , Xk ) in a parfactor [i] and shattering proceeds with further splits of both parfactors. Assume that in
next step parfactors [k + 1] and [k + 2] are split on a substitution {X2 /a}. The splits result in four new parfactors. The
result of the split of the parfactor [k + 1] on {X2 /a} contains a parameterized random variable g1 (a, a, . . . , Xk ) and a
parfactor [1] needs to be split on a substitution {X2 /a}. The
shattering process continues following a scheme described
above. It terminates after 2k+1 − k − 2 splits and results in
2k+1 − 1 parfactors (each original parfactor [i], i = 0, . . . , k,
is shattered into 2k−i parfactors). Assume that lifted inference proceeds with an elimination ordering g1 , . . . , gk
(this elimination ordering does not introduce counting formulas, other do). To compute the marginal JgQ () (Φ), 2k
lifted multiplications and 2k+1 − 2 lifted summations are
performed.
Consider lifted inference with splitting as needed. Assume
it follows an elimination ordering g1 , . . . , gk . A set of random variables represented by a parameterized random variable g1 (X1 , . . . , Xk ) in a parfactor [0] is a proper superset
of a set of random variables represented by a parameterized random variable g1 (a, X2 , . . . , Xk ) in a parfactor [1] and
the parfactor [0] is split on a substitution {X1 /a}. The
split results in parfactors identical to the parfactors [k + 1]
and [k + 2] from the description of shattering above. The
parfactor [k + 1] is multiplied by the parfactor [1] and all
instances of g1 (a, X1 , . . . , Xk ) are summed out from their
product while all instances of g1 (X1 , X2 , . . . , Xk ) (subject
to a constraint X1 6= a) are summed out from the parfactor
[k + 2]. The summations create two parfactors:
h0,
/ {gQ (), g2 (X2 , X3 , . . . , Xk ), . . . , gk (Xk ))}, FFk+3 i,

[k + 3]

h0,
/ {gQ (), g2 (X2 , X3 , . . . , Xk ), . . . , gk (Xk ))}, FFk+4 i .

[k + 4]

299

Instances of g2 are eliminated next. Parfactors [k + 3] and
[k + 4] are split on a substitution {X2 /a}, the results of the
splits and a parfactor [2] are multiplied together and the
residual parfactors are multiplied together. Then, all instances of g2 (a, X3 , . . . , Xk ) are summed out from the first
product product while all instances of g2 (X2 , . . . , Xk ) (subject to a constraint X2 6= a) are summed out from the second
product. The elimination of g3 , . . . , gk looks the same as for
g2 . In total, 2k − 1 splits, 3k − 2 lifted multiplications and
2k lifted summations are performed. At any moment, the
maximum number of parfactors is k + 3.
The above theorem shows that shattering approach is never
better and sometimes worse than splitting as needed. It is
worth pointing out that splitting as needed approach complicates the design of an elimination ordering heuristic.
5.2

NORMAL FORM PARFACTORS VS. #CSP
SOLVER

Normal form parfactors were introduced by Milch et al.
(2008) in the context of counting formulas. Counting formulas are parameterized random variables that let us compactly represent a special form of probabilistic dependencies between instances of a parameterized random variable. Milch et al. (2008) require all parfactors to be in normal form to eliminate the need to use a separate constraint
solver to solve #CSP. The requirement is enforced by splitting parfactors that are not in normal form on appropriate
substitutions. While parfactors that involve counting formulas must be in normal form, that is not necessary for
parfactors without counting formulas. It might actually be
quite expensive as we show in this section.
Proposition 1. Let hC , V, Fi be a parfactor in normal
form. Then each connected component of the constraint
graph corresponding to C is fully connected.
Proof. Proposition 1 is trivially true for components with
one or two parameters. Let us consider a connected component with more than two parameters. Suppose, contrary to our claim, that there are two parameters X and Y
with no edge between them. Since the component is connected, there exists a path X, Z1 , Z2 ..., Zm ,Y . As C is in normal form, EZCi \{Zi+1 } = EZCi+1 \{Zi }, i = 1, . . . , m − 1 and
EZCm \{Y } = EYC \{Zm }. We have X ∈ EZC1 , and consequently
X ∈ EYC . This contradicts our assumption that there is no
edge between X and Y .
While the above property simplifies solving #CSP for a set
of constraints from a parfactor in normal form it also has
negative consequences. If a parfactor is not in normal form,
conversion to normal from might require several splits. For
example we need three splits to convert a parfactor with
the set of constraints shown in Figure 3 (a) to a set of four
parfactors in normal form. The resulting sets of constraints

300

KISYNSKI & POOLE
W

W
W =Z
=
X =Y

=

=

W =Z

X=Y

=

=

X

=

X

=

=

Y

Y

=
=
=

=

=
Z

Z

Figure 4: Constraint graphs obtained through a conversion
to normal form.
are presented in Figure 4. If the underlying graph is sparse,
conversion might be very expensive as we show in the example below.
Example 10. Consider a parfactor h{X0 6= a, X0 6=
X1 , . . . , X0 6= Xk }, {g0 (X0 ), g1 (X1 ), . . . , gn (Xk )}, Fi, where
D(X0 ) = D(X1 ) = · · · = D(Xk ). Let C denote a set of constraints from this parfactor. We have EXC0 = {a, X1 , . . . , Xk }
and EXCi = {X0 }, i = 1, . . . , k. The parfactor is not in normal form because EXC0 \{Xi } =
6 EXCi \{X0 }, i = 1, . . . , k. As
a result the size of the set D(X0 ) : C depends on other parameters in the parfactor. For instance, it differs for X1 = a
and X1 6= a or for X1 = X2 and X1 6= X2 . A conversion of
the considered parfactor to set of parfactors in normal form
involves 2k − 1 splitson substitutions of the form {Xi /a},
1 ≤ i ≤ k and ∑ki=2 ki (ϖi − 1) splits on substitutions of the

form {Xi /X j }, 1 ≤ i, j ≤ k. It creates ∑ki=0 ki ϖi parfactors
in normal form. In Example 11 we analyze how this conversion affects parfactor multiplication compared to the use
of a #CSP solver.
From the above example we can clearly see that the cost of
converting a parfactor to normal form can be worse than exponential. Moreover, converting parfactors to normal form
may be very inefficient when analyzed in context of parfactor multiplication (see Section 5.2.1) or summing out a
parameterized random variable from a parfactor (see Section 5.2.1). Our empirical tests (see Section 6.2) confirm
this observation.
Note that splitting as needed can be used together with a
#CSP solver (Poole, 2003), or with normal form parfactors.
Shattering can be used with a #CSP solver (de Salvo Braz
et al., 2007) or with normal form parfactors (Milch et al.,
2008). The cost of converting parfactors to normal form
might be amplified if it is combined with shattering.
5.2.1

Multiplication

In the example below we demonstrate how the normal form
requirement might lead to a lot of, otherwise unnecessary,
parfactor multiplications.
Example 11. Assume we would like to multiply the
parfactor from Example 10 by a parfactor p f =
h0,
/ {g1 (X1 )}, F1 i. First, let us consider how it is done
with a #CSP solver. A #CSP solver computes the num-

UAI 2009

ber of factors the parfactor from Example 10 represents,
(| D(X0 ) | − 1)k+1 . Next the solver computes the number
of factors represented by the parfactor p f , which is trivially | D(X1 ) |. A correction is applied to values of the factor F1 to compensate for the difference between these two
numbers. Finally the two parfactors are multiplied. The
whole operation involved two calls to a #CSP solver, one
correction and one parfactor multiplication. Now, let us
see how it can be done without the use of #CSP solver.

The first parfactor is converted to a set Φ of ∑ki=0 ki ϖi
parfactors in normal form, as presented in Example 10.
Some of the parfactors in Φ contain a parameterized random variable g1 (a), the rest contains a parameterized random variable g1 (X) and a constraint X1 6= a, so the parfactor p f needs to be split on a substitution {X1 /a}. The
split results in a parfactor h0,
/ {g1 (a)}, F1 i and a residual
h{X1 6= a}, {g1 (X1 )}, F1 i. Next, each parfactor from Φ is
multiplied by either the result of the split or the residual.
Thus ∑ki=0 ki ϖi parfactor multiplications need to be performed and most of these multiplication require a correction prior to the actual parfactor multiplication.
There is an opportunity for some optimization, as factor
components of parfactors multiplications for different corrections could be cached and reused instead of being recomputed. Still, even with such a caching mechanism, multiple parfactor multiplications would be performed compared to just one multiplication when a #CSP solver is used.
5.2.2

Summing Out

Examples 7 and 9 demonstrate how summing out a parameterized variable from a parfactor that is not in normal form
can be done with a help of a #CSP solver. In the example below we show how this operation would look if we
convert the parfactor to a set of parfactors in normal form
which does not require a #CSP solver.
Example 12. Assume that we want to sum out f (X,Y )
from the parfactor h{X 6= Y,Y 6= a}, {e(X), f (X,Y )}, Fe f i
from the Example 7. First, we convert it to a set of
parfactors in normal form by splitting on a substitutions {X/a}. We obtain two parfactors in normal form:
h{Y 6= a}, {e(a), f (a,Y )}, Fe f i, which represents n−1 factors, and h{X 6= Y, X 6= a,Y 6= a}, {e(X), f (X,Y )}, Fe f i,
which represents (n − 1)(n − 2) factors. Next we sum out
f (a,Y ) from the first parfactor and f (X,Y ) from the second
parfactor. In both cases a correction will be necessary, as Y
will no longer among parameters of random variables and
the resulting parfactors will represent fewer factors than the
original parfactors.
In general, as illustrated by Examples 7, 9 and 12, conversion to normal form and #CSP solver create the same number of parfactors. The difference is, that the first method,
computes a factor component for the resulting parfactors
once and then applies a different correction for each result-

UAI 2009

KISYNSKI & POOLE

ing parfactor based on the answer from the #CSP solver.
The second method computes the factor component multiple times, once for each resulting parfactor, but does not
use a #CSP solver. As these factor components (before applying a correction) are identical, redundant computations
could be eliminated by caching. We successfully adopted a
caching mechanism in our empirical test (Section 6.2), but
expect it to be less effective for larger problems.

1.8
1.6





1.4



1.2
1

As in the case of splitting as needed, it might be difficult
to design an efficient elimination ordering heuristic that
would work with a #CSP solver. This is because we do not
known in advance how many parfactors will be obtained as
a result of summing out. We need to run a #CSP solver to
obtain this information.

6

10

 
 

EXPERIMENTS

We used Java implementations of tested lifted inference
methods. Tests were performed on an Intel Core 2 Duo
2.66GHz processor with 1GB of memory made available
to the JVM.
6.1

SPLITTING AS NEEDED VS. SHATTERING

In the first experiment we checked to what extent the overhead of the shattering approach can be minimized by using
intensional representations and immutable objects that are
shared whenever possible. We ran tests on the following
set of parfactors:
Φ = { h0,
/ {gQ (), g1 (a)}, F0 i,

[0]

h{X 6= a}, {gQ (), g1 (X)}, F1 i,

[1]

h0,
/ {g1 (X), g2 (X)}, F2 i,

[2]

h0,
/ {g2 (X), g3 (X)}, F3 i,

[3]

...,
h0,
/ {gk−1 (X), gk (X)}, Fk i,

[k]

h0,
/ {gk (X)}, Fk+1 i}

[k + 1] .

All functors had the range size 10 and we set Q to the instance of gQ (). We computed the marginal JQ (Φ). Lifted
inference with shattering first performed total of k splits,
then proceeded with 2k + 1 multiplications and 2k summations regardless of the elimination ordering. Lifted inference with splitting as needed performed 1 split, k + 2 multiplications and k + 1 summations (for the experiment we
used the best elimination ordering, that is gk , gk−1 , . . . , g1 ).
Figure 5 shows the results of the experiment where we varied k from 1 to 100. Even though lifted inference with shattering used virtually the same amount of memory as lifted
inference with splitting, it was slower because it performed
more arithmetic operations.

20

40

60

80

100

Figure 5: Speedup of splitting as needed over shattering.



6

301

CONV−NFM−SUM
NFM−SUM
#CSP−SUM

4

10

2

10

1

10

2

10

  

3

10

Figure 6: Summing out with and without a #CSP solver.

6.2

NORMAL FORM PARFACTORS VS. #CSP
SOLVER

For experiment in this section we randomly generated sets
of parfactors. There were up to 5 parameterized random
variables in each parfactor with range sizes varying from 2
to 10. Constraints sets contained very few (and very often
zero) constraints and formed sparse CSPs. Most of parfactors were in normal form, which allowed us to account
for #CSP solver overhead. There were up to 10 parameters present in each parfactor. Parameters were typed with
the same population. We varied the size of this population
from 5 to 1000 to verify how well #CSP solver scaled for
larger populations.
In this experiment we summed out a parameterized random variable from a parfactor. We compared summing
out with a help of a #CSP solver (#CSP-SUM) to summing out achieved by converting a parfactor to a set of
parfactors in normal form and summing out a parameterized random variable from each obtained parfactor without
a #CSP solver. (We cached factor components as suggested
in Section 5.2.2). For each population size we generated
100 parfactors and reported a cumulative time. For the second approach, we reported time including (CONV-NFMSUM) and excluding (NFM-SUM) conversion to normal
form. Results presented on Figure 6 show significant cost
of conversion to normal form and advantage of #CSP solver
for larger population sizes.

302

7

KISYNSKI & POOLE

CONCLUSIONS AND FUTURE WORK

In this paper we analyzed the impact of constraint processing on the efficiency of lifted inference, and explained
why we cannot ignore its role in lifted inference. We
showed that a choice of constraint processing strategy has
big impact on efficiency of lifted inference. In particular,
we discovered that shattering (de Salvo Braz et al., 2007)
is never better—and sometimes worse—than splitting as
needed (Poole, 2003), and that the conversion of parfactors
to normal form (Milch et al., 2008) is an expensive alternative to using a specialized #CSP solver. Although in this
paper we focused on exact lifted inference, our results are
applicable to approximate lifted inference. For example,
see the recent work of (Singla and Domingos, 2008) that
uses shattering.
It is difficult to design an elimination ordering heuristic
that works well with the splitting as needed approach and a
#CSP solver. We plan to address this problem in our future
research.
Acknowledgments
The authors wish to thank Brian Milch for discussing the CFOVE algorithm with us. Peter Carbonetto, Michael Chiang and Mark Crowley provided many helpful suggestions
during the preparation of the paper. This work was supported by NSERC grant to David Poole.


We introduce a challenging real-world planning
problem where actions must be taken at each location in a spatial area at each point in time. We
use forestry planning as the motivating application. In Large Scale Spatial-Temporal (LSST)
planning problems, the state and action spaces
are defined as the cross-products of many local state and action spaces spread over a large
spatial area such as a city or forest. These
problems possess state uncertainty, have complex utility functions involving spatial constraints
and we generally must rely on simulations rather
than an explicit transition model. We define
LSST problems as reinforcement learning problems and present a solution using policy gradients. We compare two different policy formulations: an explicit policy that identifies each location in space and the action to take there; and
an abstract policy that defines the proportion of
actions to take across all locations in space. We
show that the abstract policy is more robust and
achieves higher rewards with far fewer parameters than the elementary policy. This abstract policy is also a better fit to the properties that practitioners in LSST problem domains require for
such methods to be widely useful.

1

INTRODUCTION

In some real world planning problems there are many actions to be taken in parallel over a spatial area. This is
the case, for example, in urban planning when zoning different areas of a city for different uses. In infectious disease control, decisions need to be made about allocating
medicine to thousands or millions of people spread across
space based on need, cost, transportation or any number of
other variables. In forestry planning, decisions come down
to whether to cut each tree or not, or to perform some other

David Poole
Computer Science Department
University of British Columbia
cs.ubc.ca/∼poole

activity at every point in the forest. We call problems of
this form Large Scale Spatial-Temporal (LSST) planning
problems.
After further motivating the problem with details from the
example of forestry planning we introduce a general definition of LSST planning as a reinforcement learning (Sutton
and Barto, 1998) problem and discuss the properties a solution needs to possess. We demonstrate how policy gradients (Williams, 1992) can satisfy many of these properties
for LSST problems. We compare two policy formulations:
an explicit policy that identifies each location in space with
parameters for controlling the actions taken and an abstract
policy that represents the proportion of actions that will be
taken across the entire space. We show that this abstract
policy produces better results with far fewer parameters and
we argue that the level of abstraction it uses more closely
matches the level needed by human planners in forestry or
other LSST domains that would utilize this method to aid
in planning.

2

FORESTRY BACKGROUND

Forestry planning as it is practiced in British Columbia
will be used as our motivating example throughout this
paper. Forestry is a very important industry in British
Columbia generating 12% of the province’s GDP and employing around 200,000 people. Large regions of forest of
up to several hundred thousand hectares are licensed by the
government to forestry companies to cut trees and sell lumber. The government places many constraints on management activities: setting a maximum annual allowable cut,
specifying areas that are off limits, specifying spatial constraints to avoid a high density of cut areas and to protect
wildlife migration routes and habitats. Violations of these
constraints are enforced with large fines and possible revocation of licence.
Suppose you are the head forester in charge of planning
for one of these companies. Your interest is to maximize
your return and minimize your fines incurred. This can be
achieved by providing a steady supply of logs over the long

UAI 2009

CROWLEY ET AL.

term and maintaining a healthy forest. The forest is divided up into many small spatial regions we will call cells
and you must decide for every cell whether to clear-cut the
whole cell, cut a portion of the trees or do nothing in that
cell this year.
One major impact on forest health is insect infestation such
as the Mountain Pine Beetle (MPB)(Eng et al., 2004). MPB
are tiny beetles that burrow under the bark of pine trees laying eggs, cutting off nutrients and leaving a deadly blue
fungus that kills the tree. MPB are an endemic species
however, in recent decades, a lack of cold winters and the
large number of older trees resulting from years of forest
fire suppression have provided the MPB population with
the conditions they need for an explosive epidemic. Cutting down trees before a brood spread can kill the beetles
but the rice-sized beetles are hard to detect until a year or
two after an attack. That is when the thousands of killed
trees are easily spotted by their distinctive red color. The
infestation is devastating the forests of British Columbia,
wiping out over 50% of the harvestable pine in the past 15
years and shows no sign of stopping at the provincial or
national borders.
2.1

SOLUTION CONSIDERATIONS

In a complex domain such as forestry there are many
researchers who have developed sophisticated simulation
models for different elements of forestry planning from tree
growth to MPB growth. The explicit transition models underlying these simulations are too complex and varied to
be used as conditional probabilities so we must rely on the
simulations themselves as black boxes that provide a future
forest state given some proposed set of actions and the current state. Generating simulations is expensive, so we need
to treat all simulation data as a precious resource to be used
as effectively as possible.
In many LSST planning domains a distinction is made between strategic, tactical and operational planning. Operational planning refers to the immediate implementation of
a low level plan (eg. which particular trees to cut in which
order). Tactical planning covers mid-sized regions over
medium timescales of less than 20 years. Tactical plans
take into account local conditions (eg. where to build roads
to access the forest, assigning workers, scheduling cutting
in different areas). Strategic planning takes place at the
highest levels, focussing on properties of the entire landscape, spatial constraints and total rewards into the long
term future over decades or centuries. Some strategic considerations in forestry are : maintaining the proportion of
trees within an age class, balancing employment between
regions, satisfying spatial constraints, reducing overall pest
levels. The total number of cells in a landscape can range
from 100-100,000. In this paper we focus on strategic
planning, as this is the level where effective use of large
amounts of data can have the greatest impact and it is an
important area of research in Forestry planning.

127

The result of strategic planning is a strategic policy which
is concerned primarily with the proportions of actions taken
across the landscape and their impact on long term value.
The strategic policy does not express which actions to take
in particular cells in the landscape. This is due to the
fact that over the level of the entire landscape there are
many states that the reward function does not distinguish
between. Consider a reward function based purely on the
number of trees cut and constrained by a maximum allowable cut. There are many ways to achieve the maximum
value that involve different assignments of actions to particular cells. These distinctions are not relevant as long
as a policy can be defined to achieve the maximum value.
In practice, the person designing the strategic policy often
does not even have the authority to specify the lowest level
action choices (eg. “Clear-cut cell 1582”) as these choices
are made by experts on the ground based on their local context in accordance with the strategic policy.
We thus have two major requirements for a good LSST
planning solution:
1. the method can efficiently find a high value policy
without having an explicit transition model for the
simulation
2. we want a strategic policy that does not commit to
more detail than necessary in order to maximize reward
In the following sections we give a general definition of
LSST planning problems and show how to use policy gradients to achieve these requirements.
2.2

CURRENT SOLUTIONS IN FORESTRY

Many existing planning solutions in forestry have relied
heavily on assuming spatial independence between cells in
a forest. The deterministic optimization models often used,
such as linear programming, break down when faced with
uncertainty about the state of the forest and cannot use any
information about spatial relations between cells. This is a
problem in forestry (J.P.Kimmins et al., 2005) since MPB
breaks the assumption of spatial independence and adds uncertainty to the problem. MPB can fly between nearby cells
in the forest, so the immediate neighbourhood is always relevant when planning which trees to cut in order to reduce
the spread of the pests and quickly salvage trees killed by
them.
Other solution methods commonly used in forestry planning are simulation modelling and meta-heuristics. Simulation modelling is an interactive approach where the
user specifies maps, constraints and preferences for various actions and the software carries out a simulation
while choosing actions consistent with the constraints and
preferences. Some example simulation tools are ATLAS
(http://www.forestry.ubc.ca/atlas-simfor) and SELES (Fall

128

CROWLEY ET AL.

et al., 2003). The results from these simulation planners are
often then fed through other tools for analysis after which
the user can alter the parameters of the simulation and run
it once again. These simulations will be a useful black box
to be used by higher level RL planning techniques.
The final set of methods in common use are stochastic local search methods such as tabu search, genetic algorithms
and simulated annealing (Pukkala and Kurttila, 2005). The
general approach is to predefine fixed plans that could be
applied to a cell over the entire time horizon. The search
proceeds to assign one of these plans to each cell, evaluate
the outcome and make local improvement steps. Simulated
annealing has offerred the best hope for integrating spatial
relations of these methods but uncertainty is generally not
dealt with in a significant way.
Uncertainty is also introduced to the otherwise relatively
predictable growth of trees by the fact that the exact location and severity of the MPB infestation is unknown until
a year or two after an attack. Increasingly, there are efforts in forestry planning to improve modelling of uncertainty and complex, dynamic processes in the forest, such
as fire or pest infestations (Baskent and Keles, 2005). This
work contributes to those efforts by translating this specific domain into a general planning problem that can be
approached with recent advances from the artificial intelligence community.

3

LSST DEFINITIONS

A landscape is partitioned spatially into a set of cells C. We
assume these cells are disjoint and completely cover the
area of the landscape. Cell partitioning remains constant
over the planning horizon T .
Each cell, c ∈ C, at each timestep t ∈ [0, T ], has a state,
s ∈ S. Each cell-state is a column vector of real numbers
s[f ] for all cell-features f ∈ F . These features describe
different aspects of the cell such as elevation, the number
of trees in the cell, the number of trees per age class and
the number of MPB present in the cell. It is also possible to model spatial features in each cell which take into
account attributes of neighbouring cells. Such spatial features provide a simple way to include some relevant spatial
information in local features of a cell. One such feature
we will use in our experiments is an aggregated count of
the number of MPB that will be invading the cell from all
neighbouring cells.

UAI 2009

A which in this simplified forestry problem consists of:
“clear-cut”, “thin trees” and “do nothing”. Similarly to
states, the landscape-action is a function (a : C → A)
representing the combined actions in all cells in the landscape and referred to for a particular timestep t and cell c
as at [c].
An LSST planning problem is defined as an MDP
$S, A, r, P % where S and A are the sets of all possible landscape states and actions, r is a reward function and P is the
state transition model. The transition model P (st+1 |st , at )
will generally not be available in LSST problems in any
explicit form. Instead, we assume there is access to simulations of the domain developed by domain experts. These
external simulators return a new state when given the current landscape-state and landscape-action. By making a series of calls to the simulator we can construct a trajectory,
k, of states and actions across all timesteps2 ,
k = $sk0 , ak0 , sk1 , ak1 , . . .%.

The reward function, r(st , at , st+1 ) returns a real number representing the reward received for the actions taken
across the landscape st . The reward may contain local cell
components (eg. the value of cut trees), spatial components
(eg. constraints on the number of contiguously cut cells)
and even landscape-wide components (eg. a penalty for the
total number of MPB present). Other important constraints
in forestry are the annual allowable cut (AAC), as well as
upper and lower bounds on the proportion of the forest in
a different age classes (eg. no more than 20% of the forest
is less than 10 years old).
! The total discounted reward of a
trajectory is R(k) = t γ t r(skt , akt , skt+1 ) with a constant
discount factor γ ∈ [0, 1].
A partially observable MDP (POMDP) is defined as above
except that the states are now hidden and there is an additional set, O, of observations about the states. The probability distribution P (ot+1 |at , st+1 ) models how likely the
observation is given the most recent action and the state that
resulted from that action. Although LSST planning problems are partially observable in general, for this paper we
focus on the fully observed problem.
3.1

POLICY DEFINITION

A cell-policy, π, is a distribution over actions for a given
cell-state. The probability of taking action a in a cell that
is in state s is given by π(s, a, θ). The policy parameters,
θ, are an A × F matrix of real numbers used to define the
policy as a Gibbs distribution of weighted state features:

The landscape-state, s, represents the combined state1 of
all of the cells in the landscape as a function, (s : C → S).
The landscape-state at a particular timestep t and cell c is
denoted st [c].

eθ[a]s
π(s, a, θ) = "
eθ[b]s

Each cell has an action, a, taken from the set of cell-actions

Where θ[a] is a vector of feature weights combined as a dot
product with the cell-state feature vector, s.

1
Variables or functions refering to the entire landscape of cells
will be set in bold.

2
When it is not relevant, the trajectory k will be dropped from
state and actions names.

(1)

b∈A

UAI 2009
3.2

CROWLEY ET AL.

LANDSCAPE POLICY

A general landscape-policy, π(s, a, θ), can be defined in
terms of π as the joint probability of choosing all of the
cell-actions in a given a set of cell-states, s. The landscapeparameters θ define parameters for each local cell-policy.
The landscape-policy is computed as the product of the
probabilities for all cells given by the appropriate cellpolicies. We will provide two parameterizations for the
landscape-policy, π C and π 1 , shown below. The first formulation, π C , defines an explicit policy by maintaining
separate parameters, θ C : C → θ, for each cell and each
time step.
π C (s, a, θ C ) =

#

π(s[c], a[c], θ C [c])

(2)

c∈C

The second formulation, shown in (3), describes an abstract
policy where a single set of parameters, θ 1 , is used for all
cells in the landscape at that timestep.
1

π (s, a, θ ) =
1

#

1

π(s[c], a[c], θ )

As stated earlier, in LSST planning problems we will generally be given a black box simulator rather than the transition model P . However, it turns out that computing the
gradient of the value function with respect to the policy parameters, ∇θ V θ , does not require knowing V θ or P (Riedmiller et al., 2007; Sutton et al., 2000):
&
&
θ
∇θ V = ∇θ p(k|θ)R(k)dk = ∇θ p(k|θ)R(k)dk
&
= p(k|θ)∇θ log p(k|θ)R(k)dk
≈

=

Policy gradient (PG) methods seek to find optimal policies
by following the gradient with respect to the policy parameters of a function describing the value of the current policy. PG researchers have recently achieved significant gains
in the types of reinforcement learning problems that can
be solved (Kersting and Driessens, 2008; Riedmiller et al.,
2007; Sutton et al., 2000; Williams, 1992). PG methods
require stochastic, parameterized policies and work well
when the state space is very large and the transition dynamics are not available. These properties match well with
LSST planning problems so PG methods seem a promising
place to start looking for solutions.
Policy gradient algorithms are founded on the observation
that the expected value of a stochastic policy can be computed using previously sampled trajectories by weighting
the rewards actually received during each trajectory by the
probability of that trajectory given the current policy (Sutton et al., 2000) (Riedmiller et al., 2007):
$
% & ∞
θ
V = E p(k|θ)R(k) =
p(k|θ)R(k)dk (4)
0

Where the probability of a trajectory k is:
#
t

"
t

POLICY GRADIENTS

P (skt |skt−1 , akt−1 )π(skt , akt , θ t )

"

t

log π(skt , akt , θ t )

t

∇θ log π(skt , akt , θ t )

(7)

If we choose to use π C as the landscape policy then the
final gradient of the policy value becomes:
1 ""
∇θ V θ ≈
∇θ log π C (skt , akt , θ C
t )R(k)
|K|
t
k
#
1 ""
=
∇θ log
π(skt [c], akt [c], θ C
t [c])R(k)
|K|
t
c
k
1 """
=
∇θ log π(skt [c], akt [c], θ C
t [c])R(k)
|K|
t
c
k

(8)

The basic policy gradient algorithm then involves two main
steps, generating samples and updating the policy. First,
a new sample trajectory, k, is generated using the current
policy parameters, θ, and this trajectory is used to compute
∇θ V θ . Then, the policy parameters are updated by following the gradient of the policy value:
θ $ = θ + µ∇θ V θ

(9)

Where µ is a learning rate that controls the size of the policy update steps. This learning rate is notoriously difficult
to choose as it needs to scale with the magnitude of the
derivative. Riedmiller et al. (2007) describe a few techniques called optimal base-lining and Rprop to counteract
these difficulties which we use. We describe these methods
briefly here.
4.1

(5)

(6)

k∈K

+ ∇θ

(3)

These two formulations will be used within a general policy
gradient algorithm and compared.

p(k|θ) = p(s0 )

1 "
∇θ log p(k|θ)R(k)
|K|

Where K is the finite set of sampled trajectories. The log
trajectory likelihood, ∇θ log p(k|θ), needed in (6) can be
computed using (5) to be:
"
∇θ log p(k|θ) = ∇θ log p(s0 ) + ∇θ
log P (skt |skt−1 , akt−1 )

c∈C

4

129

REDUCING THE VARIANCE OF ∇θ V θ

The varying magnitude of R(k) can lead to high variance in
the estimate of ∇θ V θ , which will impede learning. Part of

130

CROWLEY ET AL.

UAI 2009

the variance can be removed by subtracting a constant baseline b from each occurrence
' of R(k) in equations (4) and
(8). This is valid since ∇θ p(k|θ)dk = ∇θ 1 = 0 (Riedmiller et al., 2007). The optimal baseline for our problem
(shown here for π C )is computed for each policy parameter
θ[α, f ] for every α ∈ A and f ∈ F as follows:

4.3

bt [α, f ] =
! $! !

Algorithm: LSST-PG(s0 )
initialize θ randomly
∆θ = 0.1; K = ∅
repeat maxSamples times
// Sample new trajectory
$s, a, R% = generateTrajectory(s0 , θ)
K = K ∪ $s, a, R%
// Update policy
update b as in!
(10)!
1
k
k
∇θ V θ = |K|
k
t (R(k) − bt )∇θ log π(st , at , θ t )
update ∆θ using ∇θ V θ as in sec 4.1
θ = θ + ∆θ
return θ

%2
C
k
k
R(k)
k
t
c ∇αf log π(st [c], at [c], θ t [c])
%2
! $! !
C
k
k
k
t
c ∇αf log π(st [c], at [c], θ t [c])

(10)

Another technique used to improve policy gradient performance is called Rprop (Riedmiller et al., 2007) which replaces scaled updating using the full gradient as in eq (9)
with an update-value, ∆θ , which has the same direction
as ∇θ V θ but a magnitude that is unrelated to the gradient. The magnitude of ∆θ is similar to the magnitude of
the parameters in θ and is updated incrementally based on
the progress of the policy search. To update the policy we
compute θ $ = θ + ∆θ and then increment the value of
∆θ in the appropriate direction based on the gradient. See
(Riedmiller et al., 2007) for more details.
4.2

SOLVING LSST PROBLEMS USING POLICY
GRADIENTS

The formulation for ∇θ V θ in (8) requires us to know
∇ log π(s, a, θ). Our choice of policy parameterization allows us to express this analytically. We compute the partial
derivative ∇αf V θ with respect to parameter θ[α, f ] for every α ∈ A and f ∈ F :
(
)
eθ[a]s
∇αf log π(s, a, θ) = ∇αf log !
θ[b]s
b∈A e
"
= ∇αf log eθ[a]s − ∇αf log
eθ[b]s
b

! θ[b]s
∇αf
e
= ∇αf θ[a]s − ! bθ[b]s
e
b
! θ[b]s
e
∇αf θ[b]s
= ∇αf θ[a]s − b ! θ[b]s
be

This partial derivative will be different depending on
whether the action for this cell, a, matches the action associated with parameter being differentiated, α. Since all
the policy parameters are independent we know that for any
action a ∈ A and feature g ∈ F :
*
1 if α = a and f = g
∇αf θ[a, g] =
(11)
0 otherwise
This allows us to simplify ∇αf log π(s, a, θ) to:
s[f ](1 − π(s, α, θ))
−s[f ]π(s, α, θ)

: if α = a
: if α *= a

(12)
(13)

LSST POLICY GRADIENT ALGORITHM

Combining all of these elements together we arrive at the
policy gradient algorithm that iteratively generates new trajectories and updates the policy based on the current set of
trajectories:

generateTrajectory(s0 , θ)
R=0
for t = 0 to T do
$at , rt , st+1 % = runSim(st , θ t )
R = R + γ t rt
return $s, a, R%
runSim(st , θ t )
foreach c in C do
// sample action distribution
at [c] ∼ π(st [c], θ C
(or π(st [c], θ 1t ))
t [c])
st+1 = externalSimulator(st , at )
rt = reward(st , at , st+1 )
return $at , rt , st+1 %
We set the initial value of the gradient update-value, ∆θ to
a value of 0.1 which has been found to be reasonable for
many problems (Riedmiller et al., 2007). The main loop
repeats until maxSamples is reached which is simply an upper bound on the number of trajectories to sample. Some
other condition could easily be used such as a measure of
the current convergence of the gradient. Note also that the
sample and update steps are independent and could be run
varying numbers of times or in parallel.

5

TWO ALGORITHM VARIANTS

In section 2.1, we outlined two major requirements of a
good LSST planning solution, dealing with the lack of an
explicit transition model and defining a strategic policy that
does not overcommit to too much low level detail.
Policy gradients provide a way to satisfy the first requirement as they do not require a model and only follow the

UAI 2009

CROWLEY ET AL.

gradient of the policy value. One fairly obvious approach
is to define a landscape policy using π C which maintains
parameters for every aspect of the state space with θ C
t [c]
defined for each and every cell. We will call this algorithm
LSST-PGC and it is simply the LSST-PG algorithm shown
above where π C fulfills the function of π in the code.
5.1

ABSTRACT ACTIONS

The algorithm LSST-PGC has two major problems. First, it
gives us an enormous number of parameters to search over
with |A|×|F |×T ×|C| dimensions. As we mentioned earlier, |C| could be on the order of 100,000 whereas |A|, |F |
and T are generally less than 100. This enormous space
makes convergence to an optimal policy very difficult.
The second problem is that LSST-PGC does not give us
a strategic policy but instead a very low level operational
policy. The optimal strategic policy should not distinguish
between particular cells. The policy should treat cells interchangeably and define a pattern of actions that gives the
proportion of cells each action will be applied to across the
landscape. We do not want to require a commitment to particular actions for particular cells in our strategic policy.
The policy π C is, in essence, too focussed on the ‘trees’ to
ever see the ‘forest’ and find these patterns of actions. To
achieve a strategic policy we instead propose to use a single, stochastic action for the entire landscape and a single
set of parameters θ 1t for each step in time. This is the policy
π 1 shown in equation (3). We define a second algorithm,
LSST-PG1 , where the role of π in LSST-PG is fulfilled by
π 1 instead of π C . We also need to alter the sampling line
in the third method runSim to at [c] ∼ π(st [c], θ 1t ).
This shift to one set of parameters seems minor, but its impact is profound. By optimizing π 1 we will be learning
how to act on abstract cells that could occur anywhere in
the landscape. While the policy is still defined for each
cell, it now does not distinguish between cells based on
their identity. All cells are treated equally based on their
state features. Recall that cell features can also take into
account information from neighbouring cells such as MPB
spread.
One way to think about the difference between π C and π 1
is by an analogy to time. A policy can be stationary or
nonstationary with respect to time. A stationary policy defines one set of parameters for all timesteps. Similarly, π 1
is stationary with respect to space. This spatially stationary policy defines a distribution over actions based on cell
features that apply to any cell in the landscape. Note that
this spatially stationary policy is well-defined for any number of cells and thus has arbitrary scale much as a stationary policy has arbitrary scale in terms of planning horizons.
A spatially stationary policy would have many advantages
during planning, allowing us to easily change scale or apply a learned policy to different subregions of the landscape
without modification.

6

131

EXPERIMENTS

The goal of our research is to develop a planning algorithm
that can utilize existing simulations from LSST domains in
a scalable way to find high value strategic policies. There
are a great variety of simulators in forestry that each require extensive expertise to set up and integrate with. We
decided for this stage of our research to develop our own
simple forest simulator to evaluate the performance of gradient descent on this problem. Our simulator includes state
features for the distribution of tree species and age classes,
the level of MPB in a cell and its neighbouring cells. The
dynamics include tree birth, growth and death, replanting
of young trees after clearcutting, killing of trees by MPB
and the spread of MPB to nearby cells year to year.
We implemented the two algorithms, LSST-PG1 and LSSTPGC , in Matlab and ran all tests on a dual processor Pentium 4 3.2GHz PC with 2GB RAM running Windows XP.
The initial landscape states were varied randomly around
representative values for state features based on common
distributions present in data for BC forests for tree species,
tree age, MPB presence and other features.
The reward function assigns value to individual trees cut
and penalizes various properties of the landscape state,
such as, a quadratic penalty on the deviation from a desired tree density for the entire landscape, linear penalties for overcutting and for the number of trees killed by
MPB, and base costs for maintaining the forest (salaries,
license fees, etc.) to inhibit a strategy of no cutting at all.
Note that the reward function is not equally well defined
at all points. The “Do Nothing” policy $DoN othing =
1.0, ClearCut = 0.0, T hin = 0.0% is a bad policy to follow in our model, as it is all cost and no revenue, but it is
actually much worse than the reward indicates. Modellers
are not willing to even assign a utility to situations where
the entire industry ceases to exist or, similarly, where all
of the trees are cut down and the ecosystem is totally destroyed. Rewards can be defined accurately within “reasonable” regions of policy space, but they must still be defined
at all points to serve as a signal to be used during policy
search.

7

RESULTS

Figure 1 shows a typical result for the total reward received
by the two algorithms. The reward is shown for each trajectory sample and is averaged over 20 trials for a small
problem with 5 cells and 5 timesteps. Each trial sampled
200 trajectories and updated the policy after every 5 samples using all trajectories sampled up to that point. The initial policy for each trial was specified by uniform weights
across all state features combined with an initial action
distribution for the action components of the parameters:
$DoN othing = 1.0, ClearCut = 0.0, T hin = 0.0%. Ini-

132

CROWLEY ET AL.
6

rithm LSST-PG1 does not have this problem since there are
fewer deterministic policies in which to get stuck and they
all have very low reward (all “Do Nothing, all “Cut” etc.).

Total Reward Received for Each Sample

x 10
!4
!5

Figure 3 shows the gradients of the combined parameters
for each action in a policy for the same trial as in figure 2. After the LSST-PGC policy converges to a narrow range of rewards the gradient begins converging. For
LSST-PG1 , the variation in the gradient drops significantly
once a ’good’ policy is found.

!6

Total Reward R(k)

!7

LST!PGTC
LST!PGT0

!8
!9
!10
!11
!12
!13
0

20

40

60
80
100
120
140
Number of Sampled Trajectories k

UAI 2009

160

180

200

Figure 1: Total reward received average over 20 trials for
LSST-PGC and LSST-PG1 on 5 cells with 5 timesteps
after 200 samples with policy updates every five samples. Initial policy was $DoN othing = 1.0, ClearCut =
0.0, T hin = 0.0%.
tially all timesteps (and cells) will have the same action
distribution before they begin diverging.
LSST-PG1 consistently finds higher value policies than
LSST-PGC . The abstract policy of LSST-PG1 is more robust across multiple trials whereas LSST-PGC fixes onto a
deterministic set of action assignments to particular cells
that is tuned to the start state for each trial.
Figure 2 shows the initial and final policies for the two algorithms on a single trial of a 20 cell planning problem.
The initial policy in this trial was set to $DoN othing =
.8, ClearCut = .15, T hin = .05% for both algorithms.
The actions that are actually taken are not exactly the same
even with identical initial parameters because of the different policy structures and stochastic choices being made but
the final policies are very different. LSST-PG1 has found
a policy that does even less cutting than the initial policy
but that cuts more in timesteps seven and eight to achieve a
higher value.
The LSST-PGC algorithm has found a policy with a much
higher proportion of cutting. This policy is deterministic,
each cell at each timestep always has the same action taken
over many different trajectory samples. LSST-PGC cannot
break out of this policy, even though it is incurring major
penalties for overcutting, because the value of a policy is
based on weighting rewards by the liklihood of past trajectories under the current policy. This makes a deterministic
policy that doesn’t change between samples very attractive.
Once a deterministic policy is found, diverging on some
cell will only lower the expected value of the policy. Algo-

The LSST-PG1 algorithm runs about three times faster than
LSST-PGC on the same number of trials and trajectories.
This is not surprising since both algorithms sample actions
for every cell and timestep while LSST-PGC uses more
memory and time managing the large number of parameters. The actual runtimes for LSST-PG1 range from 2 minutes for a 5 cell, 5 timestep problem, up to 230 minutes
for a 10 timestep, 30 cell problem. With the current implementation we estimate that solving a problem with a couple
thousand cells would take about two days. Our implementation has a lot of room for efficiency improvements but
other advances will be needed to improve speeds even further. For realistic problem sizes of hundreds of thousands
of cells, forestry planners currently expect runtimes of tens
of minutes (for linear problems) or up to several hours (for
’meta-heuristic’ solutions).

8

RELATED AND FUTURE WORK

We have used our own forest simulator here to keep implementation simple. To improve realism it would be best to
switch to a simulator in use by forestry planners. The tools
used for simulation planning such as those discussed in section 2.2 could be used for this. We are working with researchers in forestry to integrate our algorithm with more of
their own data and simulations to experiment at the larger
scales needed for results to be useful for Forestry planning
experts.
Our work builds upon research on model-free reinforcement learning (RL)(Sutton and Barto, 1998) and policy
gradient methods (Riedmiller et al., 2007). Sutton et al.
(2000) described the basis for using policy gradients within
an RL framework. The algorithm presented here includes
some extensions to basic PG such as reward baselining and
Rprop. More advanced PG techniques are available such
as natural gradients, which have been shown by Riedmiller
et al. (2007) to significantly improve performance by computing the reward baseline using the Fisher information matrix of the gradient. Another obvious next step is to fully
model the uncertainty about the current state and use policy
gradients to solve a POMDP version of the LSST problem.
Also, the independence of the update policy and generate
trajectory steps would make parallelization of the algorithm
straightforward.
Outside of policy gradient methods there is Least Square

UAI 2009

CROWLEY ET AL.

133

Number of Cells Assigned that are Each Action in Initial and Final Policies

Number of Cells

Initial Policy for !1

Initial Policy for !C

Final Policy for !1

Final Policy for !C

20

20

20

20

15

15

15

15

10

10

10

10

5

5

5

5

0

2

4
6
8
Timestep

10

0

2

4
6
8
Timestep

10

Do Nothing

0

2

Clear Cut

4
6
Timestep

8

10

0

2

4
6
8
Timestep

10

Thin Trees

Figure 2: In each cell the available actions are $“DoN othing $$ , “ClearCut$$ , “T hinT rees$$ %. The number of cells assigned each of these actions are shown as areas. The initial and final policies after optimization for both LSST-PG1 and
LSST-PGC are shown. This trial used 200 sampled trajectories using, 10 timesteps and 20 cells. The initial policy was set
to $DoN othing = .8, ClearCut = .15, T hin = .05% for both algorithms.
Policy Iteration (LSPI) (Lagoudakis and Parr, 2001), another RL approach that uses a parameterized policy and
learns without a transition model by using a stored history
of sampled trajectories. As the policy changes the value of
the policy can be recomputed based on these trajectories.
LSST problems also have many similarities to multi-agent
planning problems as each cell can be viewed as an agent
while we are seeking to optimize a joint reward based on
the actions of all agents. Guestrin et al. (2002) applied
LSPI to multi-agent problems to find an initial estimate
of the value function and specify a policy in proportion
to the values of each action. New research into decentralized (PO)MDPs (Seuken and Zilberstein, 2008) brings
together various threads in coordinated multi-agent planning into one language. LSST planning problems could be
a rich problem domain for this new field. However, many
multi-agent methods assume that the value function is a linear combination of the local value functions of individual
agents. In LSST problems this assumption does not always
hold as there are nonlocal constraints on actions across the
landscape such as landscape cut quotas.

Spatial relations between cells are a major component of
LSST problems which we have addressed here with simple
aggregation features from neighbouring cells. This method
could be expanded with more complex relational aggregators or by adding new variables modelling relations between groups of cells and represented in the reward and
policy functions. Recently, Kersting and Driessens (2008)
introduced a non-parametric policy gradient approach that
might be useful for LSST planning. Their method uses a
gradient tree boosting approach for learning policies in relational domains.
The long timescales used in LSST problems ensure that
long term plans will not be followed blindly for any length
of time. Thus, planning over time periods of varying
lengths, such as is done in the SMDP literature (Barto and
Mahadevan, 2003), could be useful. There may be enormous gains available to be made by dynamically allowing
greater abstraction of policies, models and time granularity
as time progresses.

9

CONCLUSIONS

C

The explicit policy, π , is overly detailed and unwieldy for
real world planning. The abstract policy, π 1 , is at a more
appropriate, strategic level of abstraction but it is merely
the other extreme end of a spectrum of policies. In between are varying levels of abstraction that could be defined by using more than one set of policy parameters applied to groups of cells. These groups could be learned
from clustering of features or by hierarchical decomposition of cell-state space by iteratively adding new features
to define groups of similar cells. Another idea along these
lines to explore is using multiple weighted policies where
the weights determine which policies are applied to which
cells and these weights are part of the learning process.

In this paper we have introduced Large Scale SpatialTemporal planning problems, which are very challenging
instances of general planning where states and actions are
spatially divided into components. We have described how
to apply RL techniques to these problems and demonstrated
one way to use policy gradient methods to find good policies in a simulated forestry planning problem. We showed
that use of a spatially stationary policy formulation greatly
reduces the parameter space to be searched and improves
the value of the resulting policy.
We hope that raising awareness about this particular set of
problems will benefit both the UAI research community

134

CROWLEY ET AL.

Total !" V" for LSST!PGC per action

9

x 10

6

1.5

4

1

2

0.5

0

x 10

0

!2

!0.5

!4

!1

Do Nothing
Clear!cut
Thin

!6
!8
0

5

10

15
20
25
Number of Policy Updates

30

Total !" V" for LSST!PG1 per action

8

2

!" V"

!" V"

8

UAI 2009

35

Do Nothing
Clear!cut
Thin

!1.5

40

(a) Algorithm LSST-PGC

!2
0

5

10

15
20
25
Number of Policy Updates

30

35

40

(b) Algorithm LSST-PG1

Figure 3: Policy gradients for parameters relating to each action, summed across all cells and timesteps for one trial with
200 sampled trajectories, 20 cells and 10 timesteps.
and the many researchers and planners in real-world planning domains with LSST structures who are looking for a
way to make their very complex problems more manageable.
Acknowledgements
We would like to thank Matt Hoffman for his help with
PG methods and feedback received from Peter Carbonetto,
Michael Chiang, Albert Jiang, Jacek Kisyński and the very
helpful advice from the anonymous reviewers.


